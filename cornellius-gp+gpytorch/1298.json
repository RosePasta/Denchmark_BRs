{"BR": {"BR_id": "1298", "BR_author": "samuelstanton", "BRopenT": "2020-10-01T17:58:48Z", "BRcloseT": "2020-10-05T21:28:22Z", "BR_text": {"BRsummary": "[Bug]", "BRdescription": "\n <denchmark-h:h1>\ud83d\udc1b Bug</denchmark-h>\n \n Batched fixed noise GPs fail when the preconditioning threshold is reached. The concatenation in this line fails with a shape error.\n <denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/added_diag_lazy_tensor.py#L126>https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/added_diag_lazy_tensor.py#L126</denchmark-link>\n \n <denchmark-h:h2>To reproduce</denchmark-h>\n \n ** Code snippet to reproduce **\n import torch\n import gpytorch\n \n class BatchFixedNoiseGP(gpytorch.models.GP):\n     def __init__(self, init_x, init_y, noise, batch_shape):\n         super().__init__()\n         self.mean_module = gpytorch.means.ZeroMean()\n         self.covar_module = gpytorch.kernels.RBFKernel(batch_shape=batch_shape)\n         self.likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise)\n         \n     def forward(self, inputs):\n         mean = self.mean_module(inputs)\n         covar = self.covar_module(inputs)\n         return gpytorch.distributions.MultivariateNormal(mean, covar)\n \n batch_shape = [2]\n train_x = torch.randn(*batch_shape, 101, 3)\n train_y = torch.randn(*batch_shape, 101)\n train_noise = torch.rand(*batch_shape, 101)\n \n gp = BatchFixedNoiseGP(train_x, train_y, train_noise, batch_shape)\n mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\n \n with gpytorch.settings.max_cholesky_size(100), gpytorch.settings.min_preconditioning_size(100):\n     train_dist = gp(train_x)\n     loss = -mll(train_dist, train_y).sum()\n ** Stack trace/error message **\n <denchmark-code>RuntimeError                              Traceback (most recent call last)\n <ipython-input-4-9e151e2de37a> in <module>\n      24 with gpytorch.settings.max_cholesky_size(100), gpytorch.settings.min_preconditioning_size(100):\n      25     train_dist = gp(train_x)\n ---> 26     loss = -mll(train_dist, train_y).sum()\n \n ~/Code/gpytorch/gpytorch/module.py in __call__(self, *inputs, **kwargs)\n      26 \n      27     def __call__(self, *inputs, **kwargs):\n ---> 28         outputs = self.forward(*inputs, **kwargs)\n      29         if isinstance(outputs, list):\n      30             return [_validate_module_outputs(output) for output in outputs]\n \n ~/Code/gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py in forward(self, function_dist, target, *params)\n      49         # Get the log prob of the marginal distribution\n      50         output = self.likelihood(function_dist, *params)\n ---> 51         res = output.log_prob(target)\n      52 \n      53         # Add additional terms (SGPR / learned inducing points, heteroskedastic likelihood models)\n \n ~/Code/gpytorch/gpytorch/distributions/multivariate_normal.py in log_prob(self, value)\n     138 \n     139         # Get log determininat and first part of quadratic form\n --> 140         inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n     141 \n     142         res = -0.5 * sum([inv_quad, logdet, diff.size(-1) * math.log(2 * math.pi)])\n \n ~/Code/gpytorch/gpytorch/lazy/lazy_tensor.py in inv_quad_logdet(self, inv_quad_rhs, logdet, reduce_inv_quad)\n    1069             probe_vectors,\n    1070             probe_vector_norms,\n -> 1071             *args,\n    1072         )\n    1073 \n \n ~/Code/gpytorch/gpytorch/functions/_inv_quad_log_det.py in forward(ctx, representation_tree, dtype, device, matrix_shape, batch_shape, inv_quad, logdet, probe_vectors, probe_vector_norms, *args)\n      65         lazy_tsr = ctx.representation_tree(*matrix_args)\n      66         with torch.no_grad():\n ---> 67             preconditioner, precond_lt, logdet_correction = lazy_tsr._preconditioner()\n      68 \n      69         ctx.preconditioner = preconditioner\n \n ~/Code/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _preconditioner(self)\n      84                 )\n      85                 return None, None, None\n ---> 86             self._init_cache()\n      87 \n      88         # NOTE: We cannot memoize this precondition closure as it causes a memory leak\n \n ~/Code/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _init_cache(self)\n     107             self._init_cache_for_constant_diag(eye, batch_shape, n, k)\n     108         else:\n --> 109             self._init_cache_for_non_constant_diag(eye, batch_shape, n)\n     110 \n     111         self._precond_lt = PsdSumLazyTensor(RootLazyTensor(self._piv_chol_self), self._diag_tensor)\n \n ~/Code/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _init_cache_for_non_constant_diag(self, eye, batch_shape, n)\n     125         # With non-constant diagonals, we cant factor out the noise as easily\n     126         # eye = eye.expand(*batch_shape, -1, -1)\n --> 127         self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self / self._noise.sqrt(), eye)))\n     128         self._q_cache = self._q_cache[..., :n, :] / self._noise.sqrt()\n     129 \n \n RuntimeError: Tensors must have same number of dimensions: got 3 and 2\n </denchmark-code>\n \n <denchmark-h:h2>Expected Behavior</denchmark-h>\n \n Everything works fine until the preconditioning threshold is reached. Obviously one would hope that it would continue to work.\n <denchmark-h:h2>System information</denchmark-h>\n \n Please complete the following information:\n GPyTorch Version: 1.2.0\n PyTorch Version:  '1.6.0.dev20200522'\n OS: Ubuntu 16.04 LTS\n <denchmark-h:h2>Additional context</denchmark-h>\n \n This appears to fix the problem\n <denchmark-code>    def _init_cache_for_non_constant_diag(self, eye, batch_shape, n):\n         # With non-constant diagonals, we cant factor out the noise as easily\n         eye = eye.expand(*batch_shape, -1, -1)\n         self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self / self._noise.sqrt(), eye), dim=-2))\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "samuelstanton", "commentT": "2020-10-05T12:33:42Z", "comment_text": "\n \t\tThanks for the catch. I'll take a look\n \t\t"}}}, "commit": {"commit_id": "192200f98d841661160f2bd83354866a853239e0", "commit_author": "Geoff Pleiss", "commitT": "2020-10-05 13:20:43+00:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "gpytorch\\lazy\\added_diag_lazy_tensor.py", "file_new_name": "gpytorch\\lazy\\added_diag_lazy_tensor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "149,152", "deleted_lines": null, "method_info": {"method_name": "_init_cache_for_non_constant_diag", "method_params": "self,eye,batch_shape,n", "method_startline": "147", "method_endline": "155"}}, "hunk_1": {"Ismethod": 1, "added_lines": "71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,93,94,95,96,97,111", "deleted_lines": null, "method_info": {"method_name": "_preconditioner", "method_params": "self", "method_startline": "70", "method_endline": "117"}}, "hunk_2": {"Ismethod": 1, "added_lines": "127", "deleted_lines": "126", "method_info": {"method_name": "_init_cache", "method_params": "self", "method_startline": "119", "method_endline": "134"}}, "hunk_3": {"Ismethod": 1, "added_lines": "111", "deleted_lines": null, "method_info": {"method_name": "_preconditioner.precondition_closure", "method_params": "tensor", "method_startline": "110", "method_endline": "115"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "test\\examples\\test_white_noise_regression.py", "file_new_name": "test\\examples\\test_white_noise_regression.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "56,78", "deleted_lines": "56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85", "method_info": {"method_name": "test_posterior_latent_gp_and_likelihood_without_optimization", "method_params": "self,cuda", "method_startline": "56", "method_endline": "85"}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "87,88,89,90", "method_info": {"method_name": "test_posterior_latent_gp_and_likelihood_without_optimization_cuda", "method_params": "self", "method_startline": "87", "method_endline": "90"}}, "hunk_2": {"Ismethod": 1, "added_lines": "100,101,102", "deleted_lines": "92,114", "method_info": {"method_name": "test_posterior_latent_gp_and_likelihood_with_optimization", "method_params": "self,cuda", "method_startline": "92", "method_endline": "134"}}, "hunk_3": {"Ismethod": 1, "added_lines": "100,101", "deleted_lines": null, "method_info": {"method_name": "test_posterior_latent_gp_and_likelihood_with_optimization_with_cg", "method_params": "self", "method_startline": "100", "method_endline": "101"}}, "hunk_4": {"Ismethod": 1, "added_lines": "56,78", "deleted_lines": "56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92", "method_info": {"method_name": "test_posterior_latent_gp_and_likelihood_with_optimization", "method_params": "self,cuda,cg", "method_startline": "56", "method_endline": "98"}}}}}}}