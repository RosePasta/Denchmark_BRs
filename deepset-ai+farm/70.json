{"BR": {"BR_id": "70", "BR_author": "JulianGerhard21", "BRopenT": "2019-08-28T06:14:45Z", "BRcloseT": "2019-08-30T15:18:37Z", "BR_text": {"BRsummary": "'NERProcessor' has no attribute 'tokenizer'", "BRdescription": "\n Python: 3.6.8\n OS: Windows 10\n FARM: 0.2.0\n Hi guys,\n if I want to run FARM to do some NER finetuning, I tried both approaches:\n experiments = load_experiments(\"conll_de_config.json\")\n and the stick-your-blocks-together one. Both of them ended with the following error:\n <denchmark-code>08/28/2019 07:47:35 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n 08/28/2019 07:47:35 - INFO - pytorch_transformers.tokenization_utils -   loading file https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt from cache at C:\\Users\\JulianGerhard\\.cache\\torch\\pytorch_transformers\\da299cdd121a3d71e1626f2908dda0d02658f42e925a3d6abd8273ec08cf41a6.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d\n 08/28/2019 07:47:35 - INFO - farm.data_handler.data_silo -   \n Loading data into the data silo ... \n               ______\n                |o  |   !\n    __          |:`_|---'-.\n   |__|______.-/ _ \\-----.|       \n  (o)(o)------'\\ _ /     ( )      \n  \n 08/28/2019 07:47:35 - INFO - farm.data_handler.data_silo -   Loading train set from: data/conll03-de\\train.txt \n 08/28/2019 07:47:35 - INFO - farm.data_handler.utils -    Couldn't find data/conll03-de\\train.txt locally. Trying to download ...\n 08/28/2019 07:47:35 - INFO - farm.data_handler.utils -   downloading and extracting file C:\\Users\\JulianGerhard\\PycharmProjects\\word_embeddings\\ner_finetuning\\data\\conll03-de to dir \n 08/28/2019 07:47:39 - INFO - farm.data_handler.processor -   Got ya 8 parallel workers to fill the baskets with samples (chunksize = 1000)...\n   0%|          | 0/24000 [00:00<?, ?it/s]multiprocessing.pool.RemoteTraceback: \n \"\"\"\n Traceback (most recent call last):\n   File \"C:\\Users\\JulianGerhard\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 119, in worker\n     result = (True, func(*args, **kwds))\n   File \"C:\\Users\\JulianGerhard\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 44, in mapstar\n     return list(map(*args))\n   File \"c:\\users\\juliangerhard\\pycharmprojects\\farm\\farm\\data_handler\\processor.py\", line 310, in _multiproc_sample\n     samples = cls._dict_to_samples(dict=basket.raw, all_dicts=all_dicts)\n   File \"c:\\users\\juliangerhard\\pycharmprojects\\farm\\farm\\data_handler\\processor.py\", line 555, in _dict_to_samples\n     tokenized = tokenize_with_metadata(dict[\"text\"], cls.tokenizer, cls.max_seq_len)\n AttributeError: type object 'NERProcessor' has no attribute 'tokenizer'\n \"\"\"\n \n The above exception was the direct cause of the following exception:\n \n Traceback (most recent call last):\n   File \"C:/Users/JulianGerhard/PycharmProjects/word_embeddings/ner_finetuning/farm_experimen t.py\", line 6, in <module>\n     run_experiment(experiments[0])\n   File \"c:\\users\\juliangerhard\\pycharmprojects\\farm\\farm\\experiment.py\", line 87, in run_experiment\n     distributed=distributed,\n   File \"c:\\users\\juliangerhard\\pycharmprojects\\farm\\farm\\data_handler\\data_silo.py\", line 39, in __init__\n     self._load_data()\n   File \"c:\\users\\juliangerhard\\pycharmprojects\\farm\\farm\\data_handler\\data_silo.py\", line 47, in _load_data\n     self.data[\"train\"], self.tensor_names = self.processor.dataset_from_file(train_file)\n   File \"c:\\users\\juliangerhard\\pycharmprojects\\farm\\farm\\data_handler\\processor.py\", line 366, in dataset_from_file\n     self._init_samples_in_baskets()\n   File \"c:\\users\\juliangerhard\\pycharmprojects\\farm\\farm\\data_handler\\processor.py\", line 304, in _init_samples_in_baskets\n     zip(samples, self.baskets), total=len(self.baskets)\n   File \"C:\\Users\\JulianGerhard\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tqdm\\_tqdm.py\", line 1034, in __iter__\n     for obj in iterable:\n   File \"C:\\Users\\JulianGerhard\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 320, in <genexpr>\n     return (item for chunk in result for item in chunk)\n   File \"C:\\Users\\JulianGerhard\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 735, in next\n     raise value\n AttributeError: type object 'NERProcessor' has no attribute 'tokenizer'\n   0%|          | 0/24000 [00:06<?, ?it/s]\n \n </denchmark-code>\n \n I installed twice - from repo and with pip.\n After investigating this shortly (I need to leave), I think it has something to do with your NERProcessor's classmethod implementation. I can confirm that the tokenizer is set properly:\n  <farm.modeling.tokenization.BertTokenizer object at 0x00000238DE551080>\n and the cls instance, _dict_to_samples is receiving is of type:\n <class 'farm.data_handler.processor.NERProcessor'>\n Any ideas?\n Regards\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "JulianGerhard21", "commentT": "2019-08-28T06:58:19Z", "comment_text": "\n \t\tI just did a quick check and couldn't reproduce this issue on my linux machine. Both modes seem to work for NER.\n I suspect your error is due to how Windows handles multiprocessing (see <denchmark-link:https://stackoverflow.com/questions/42148344/python-multiprocessing-linux-windows-difference>https://stackoverflow.com/questions/42148344/python-multiprocessing-linux-windows-difference</denchmark-link>\n ). Since the tokenizer is a class attribute of Processor and not an instance attribute, this might cause problems in inidividual processes.\n We will investigate this further ...\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "JulianGerhard21", "commentT": "2019-08-30T15:18:37Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/JulianGerhard21>@JulianGerhard21</denchmark-link>\n . This issue has been resolved by <denchmark-link:https://github.com/deepset-ai/FARM/pull/76>#76</denchmark-link>\n .\n \t\t"}}}, "commit": {"commit_id": "ab0226d62a3f4a95c679d04b7cbeb6f904f648f8", "commit_author": "Tanay Soni", "commitT": "2019-08-30 17:17:28+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "farm\\data_handler\\processor.py", "file_new_name": "farm\\data_handler\\processor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "290,291,292,293,295,296,297,298,299,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321", "deleted_lines": "289,290,292,293,299,300,301,302,303,304,306,307,308,309,319,320,321", "method_info": {"method_name": "_init_samples_in_baskets", "method_params": "self", "method_startline": "289", "method_endline": "321"}}, "hunk_1": {"Ismethod": 1, "added_lines": "74", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,tokenizer,max_seq_len,label_list,metrics,train_filename,dev_filename,test_filename,dev_split,data_dir,label_dtype,multiprocessing_chunk_size,max_processes,share_all_baskets_for_multiprocessing,use_multiprocessing", "method_startline": "59", "method_endline": "74"}}, "hunk_2": {"Ismethod": 1, "added_lines": "332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356", "deleted_lines": null, "method_info": {"method_name": "_featurize_samples", "method_params": "self", "method_startline": "331", "method_endline": "362"}}}}}}}