{"BR": {"BR_id": "42364", "BR_author": "dakl", "BRopenT": "2020-08-14T12:52:54Z", "BRcloseT": "2020-09-01T21:55:44Z", "BR_text": {"BRsummary": "`tf.data.experimental.snapshot()` hangs when using GCS paths", "BRdescription": "\n Please make sure that this is a bug. As per our\n GitHub Policy,\n we only address code/doc bugs, performance issues, feature requests and\n build/installation issues on GitHub. tag:bug_template\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.12 stretch\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n TensorFlow installed from (source or binary): binary\n TensorFlow version (use command below): 2.3\n Python version: 3.5.7\n Bazel version (if compiling from source): NA\n GCC/Compiler version (if compiling from source): NA\n CUDA/cuDNN version: CUDA 10.1\n GPU model and memory: Nvidia Tesla T4\n \n You can collect some of this information using our environment capture\n <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>\n \n You can also obtain the TensorFlow version with:\n \n TF 1.0: python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n TF 2.0: python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\n \n Describe the current behavior\n tf.data.experimental.snapshot() hangs when using a Google Storage path.\n Describe the expected behavior\n tf.data.experimental.snapshot() works.\n Standalone code to reproduce the issue\n Provide a reproducible test case that is the bare minimum necessary to generate\n the problem. If possible, please share a link to Colab/Jupyter/any notebook.\n <denchmark-code>import tensorflow as tf\n for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/my-path')): break\n </denchmark-code>\n \n hangs. Using a local path\n <denchmark-code>for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break\n </denchmark-code>\n \n works as expected.\n Other info / logs Include any logs or source code that would be helpful to\n diagnose the problem. If including tracebacks, please include the full\n traceback. Large logs and files should be attached.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dakl", "commentT": "2020-08-14T12:53:28Z", "comment_text": "\n \t\tping <denchmark-link:https://github.com/piwell>@piwell</denchmark-link>\n  <denchmark-link:https://github.com/carlthome>@carlthome</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dakl", "commentT": "2020-08-18T01:34:05Z", "comment_text": "\n \t\tConfirming that I was able to reproduce this in Colab and with a GCP deep learning vm instance. Although it was hanging, the metadata files did get written to the bucket. Did you see the same?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dakl", "commentT": "2020-08-21T12:28:35Z", "comment_text": "\n \t\tYes, <denchmark-link:https://github.com/nikitamaia>@nikitamaia</denchmark-link>\n , we saw the same behavior with files popping up on the bucket.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dakl", "commentT": "2020-08-21T12:29:55Z", "comment_text": "\n \t\t\n Using a local path\n for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break\n \n \n Ping <denchmark-link:https://github.com/dakl>@dakl</denchmark-link>\n , the MCVE is not using a local path. Edit the OP?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "dakl", "commentT": "2020-08-29T05:39:27Z", "comment_text": "\n \t\tThanks for reporting this issue! I've investigated and the underlying cause is gcs_file_system.cc not handling calls to NewAppendableFile() correctly when the file does not yet exist.\n The deadlock happens due to the following sequence of events:\n The error is propagated all the way back to AsyncWriter::WriterThread, which then returns with the error code.\n However, by this time, the entire writing process has finished and the main iterator thread calls SignalEOF from GetNextInternal. SignalEOF acquires the WriterThread's mu_ lock and then tries to clear() writers_ vector, containing all the AsyncWriters.\n To clear the vector, C++ needs to call AsyncWriter's default destructor, which then blocks on the thread_ within AsyncWriter finishing.\n Now, the AsyncWriter thread is still trying to call the done() function that was passed in, which tries to acquire the same mu_ lock that SignalEOF was already holding.\n This results in a deadlock where the main thread calling writers_.clear() cannot proceed because the AsyncWriter thread has not terminated, but the AsyncWriter thread is blocked trying to acquire the mu_ lock held by the main thread.\n Will fix the underlying problem and the deadlock in an upcoming commit.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "dakl", "commentT": "2020-09-01T21:55:45Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "532966cad34471bded2b0483737e8e8d23bc4720", "commit_author": "Frank Chen", "commitT": "2020-09-01 14:51:39-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\data\\experimental\\snapshot_dataset_op.cc", "file_new_name": "tensorflow\\core\\kernels\\data\\experimental\\snapshot_dataset_op.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "677,678,679,680,681,682,690,691,692,693,709,710", "deleted_lines": "676,677,678,686,702", "method_info": {"method_name": "tensorflow::data::experimental::SnapshotDatasetV2Op::Dataset::Iterator::Writer::GetNextInternal", "method_params": "ctx,out_tensors,end_of_sequence", "method_startline": "651", "method_endline": "721"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\platform\\cloud\\gcs_file_system.cc", "file_new_name": "tensorflow\\core\\platform\\cloud\\gcs_file_system.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1297,1298,1299", "deleted_lines": null, "method_info": {"method_name": "tensorflow::GcsFileSystem::NewAppendableFile", "method_params": "fname,token,result", "method_startline": "1277", "method_endline": "1340"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\platform\\cloud\\gcs_file_system_test.cc", "file_new_name": "tensorflow\\core\\platform\\cloud\\gcs_file_system_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474", "deleted_lines": null, "method_info": {"method_name": "tensorflow::TEST", "method_params": "GcsFileSystemTest,NewAppendableFile_ObjectDoesNotExist", "method_startline": "1445", "method_endline": "1474"}}}}}}}