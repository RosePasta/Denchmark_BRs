{"BR": {"BR_id": "6766", "BR_author": "jrosti", "BRopenT": "2017-01-10T12:06:06Z", "BRcloseT": "2018-01-23T21:12:41Z", "BR_text": {"BRsummary": "softmax_cross_entropy_with_logits aborts the process, if a tensor with zero first dimension is passed as an argument", "BRdescription": "\n <denchmark-h:h3>Environment info</denchmark-h>\n \n Operating System: Ubuntu 16.04\n Installed version of CUDA and cuDNN: CUDA-8.0, CUDNN 5.1.5\n Tensorflow version: 0.12.1 installed from\n <denchmark-link:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl>https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl</denchmark-link>\n \n Reproduced also using tf-0.11.0, CUDA-7.5, CUDNN-5.1.3\n <denchmark-h:h3>Minimal reproducible example</denchmark-h>\n \n <denchmark-code>import tensorflow as tf\n y = tf.placeholder(\"int64\", [None], \"y\")\n one_hot_y=tf.one_hot(y,10)\n ce = tf.nn.softmax_cross_entropy_with_logits(one_hot_y, one_hot_y)\n sess = tf.Session()\n sess.run(ce, {y: []})\n </denchmark-code>\n \n Result on GPU:\n <denchmark-code>E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\n W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\n F tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.\n Aborted (core dumped)\n </denchmark-code>\n \n Result on CPU:\n <denchmark-code>array([], dtype=float32)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jrosti", "commentT": "2017-01-11T05:10:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>\n  Here's a GPU memory alloc issue. Reported both here and on Stack Overflow: <denchmark-link:http://stackoverflow.com/questions/41530966/memory-error-with-eigenallocator>http://stackoverflow.com/questions/41530966/memory-error-with-eigenallocator</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jrosti", "commentT": "2017-04-09T09:34:00Z", "comment_text": "\n \t\tHi, <denchmark-link:https://github.com/jrosti>@jrosti</denchmark-link>\n \n Have this problem been solved?\n I met the same problem in \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jrosti", "commentT": "2017-04-19T17:20:12Z", "comment_text": "\n \t\tI still have this problem which seems to arise when using tensorflow fold.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jrosti", "commentT": "2017-06-02T04:06:18Z", "comment_text": "\n \t\tStill getting this issue, as of 06/02/2017 when using tf.gather_nd(...) and softmax_cross_entropy_with_logits(...) together.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jrosti", "commentT": "2017-06-24T23:02:07Z", "comment_text": "\n \t\tAnyone looking into this? I'm also experiencing this issue using TensorFlow 1.2.0 (v1.2.0-rc2-21-g12f033d). In my case, a tf.while_loop is being used (in conjunction with tf.TensorArray); however, the same error occurs if I swap out the tf.while_loop for a while statement.\n Backtrace (abbr):\n <denchmark-code>2017-06-24 05:42:28.894580: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\n 2017-06-24 05:42:28.894638: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\n 2017-06-24 05:42:28.894647: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes\n 2017-06-24 05:42:28.894657: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes\n 2017-06-24 05:42:28.895314: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\n 2017-06-24 05:42:28.895406: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr\n 2017-06-24 05:42:28.896185: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\n          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\n 2017-06-24 05:42:28.896209: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\n          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\n .\n .\n .\n 2017-06-24 05:42:28.905449: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\n          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\n 2017-06-24 05:42:28.905669: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for\n          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/Reshape, while/Reshape_1)]]\n </denchmark-code>\n \n A bit further down,\n <denchmark-code>Caused by op 'while/SoftmaxCrossEntropyWithLogits', defined at:\n   ...\n   File \"scripts/gpu_experiment.py\", line 400, in build_backend\n     backend['o'] = build_outputs(config, backend.get('o', None))\n   File \"scripts/gpu_experiment.py\", line 363, in build_outputs\n     loop_vars = tf.while_loop(loop_cond, _build_outputs, loop_vars)\n     ...\n     cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jrosti", "commentT": "2017-07-25T09:14:01Z", "comment_text": "\n \t\tI'm also experiencing this issue and error message is same as  <denchmark-link:https://github.com/j-wilson>@j-wilson</denchmark-link>\n    . I am fine tuning object detction api on faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017 model.ckpt.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jrosti", "commentT": "2017-08-24T11:27:15Z", "comment_text": "\n \t\tAlso having the same problem in tf 1.3. Would be great if it threw a more descriptive exception.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "jrosti", "commentT": "2017-09-17T17:38:02Z", "comment_text": "\n \t\tI hit this issue.  After some investigating I realized I was feeding in an empty tensor.  Not sure if that is the only thing that can cause it, but that was my problem at least.\n It was easy to fix, but it certainly would be nice if it threw a more descriptive error.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "jrosti", "commentT": "2017-11-02T12:02:43Z", "comment_text": "\n \t\tMassive thanks. Empty tensor is the cause of my problem too. <denchmark-link:https://github.com/metachi>@metachi</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "jrosti", "commentT": "2017-12-20T19:25:48Z", "comment_text": "\n \t\tIt has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "jrosti", "commentT": "2018-01-04T19:18:11Z", "comment_text": "\n \t\tIt has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "jrosti", "commentT": "2018-01-10T18:25:03Z", "comment_text": "\n \t\tI'm facing same problem even checking if tensor is empty or not:\n <denchmark-code>\n  loss = K.switch(tf.size(y_true) > 0,\n                     tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),\n                     tf.constant(0.0))\n </denchmark-code>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "jrosti", "commentT": "2018-01-10T18:43:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/filipetrocadoferreira>@filipetrocadoferreira</denchmark-link>\n  Have you tried to wrap the conditional branches into lambda functions?\n <denchmark-code>loss = K.switch(tf.size(y_true) > 0,\n                     lambda: tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),\n                     lambda: tf.constant(0.0))\n </denchmark-code>\n \n This should allow a lazy execution of the branches. (see <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/cond>tf.cond</denchmark-link>\n )\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "jrosti", "commentT": "2018-01-11T09:19:23Z", "comment_text": "\n \t\twow, this seems to work. Can you link to an explanation?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "jrosti", "commentT": "2018-01-11T21:24:41Z", "comment_text": "\n \t\tAdded PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/16051>#16051</denchmark-link>\n  for a fix.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "jrosti", "commentT": "2018-01-11T22:42:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/filipetrocadoferreira>@filipetrocadoferreira</denchmark-link>\n  I can\u2019t find any link with an extensive explanation (I tought it was explained in tf.cond \u2019s documentation but actually it is not). In short, if you don\u2019t define the branches as functions, they will be both executed regardless of the condition. This explains why you still had the problem despite checking the tensor\u2019s size.\n \t\t"}}}, "commit": {"commit_id": "07e6ca0ac7cdfcb6105fb3410fc68355c04df1d5", "commit_author": "Yong Tang", "commitT": "2018-01-23 13:12:40-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\xent_op.cc", "file_new_name": "tensorflow\\core\\kernels\\xent_op.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "70,71,72,73,74,75", "deleted_lines": "70,71,72,73", "method_info": {"method_name": "tensorflow::SoftmaxXentWithLogitsOp::Compute", "method_params": "context", "method_startline": "41", "method_endline": "76"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\kernel_tests\\xent_op_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\xent_op_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "243,244,245,246,247,248,249,250,251", "deleted_lines": null, "method_info": {"method_name": "testZeroDimension", "method_params": "self", "method_startline": "243", "method_endline": "251"}}}}}}}