{"BR": {"BR_id": "6717", "BR_author": "lwohlhart", "BRopenT": "2017-01-08T00:29:44Z", "BRcloseT": "2017-01-24T01:57:33Z", "BR_text": {"BRsummary": "Incorrect gradient for categorical distribution entropy", "BRdescription": "\n The Categorical distribution class provides an awesome entropy operator but apparently the gradient calculation w.r.t. the input operators doesn't work.\n logits = tf.Variable(initial_value=[[1., 2., 3.], [2., 5., 1.]])\n \n probabilities = tf.nn.softmax(logits)\n log_probabilities = tf.nn.log_softmax(logits)\n entropy = - tf.reduce_sum(probabilities * log_probabilities, axis=-1)\n \n # using the actual distribution would be nicer but gradients seem buggy\n categorical_distribution = tf.contrib.distributions.Categorical(p=probabilities)\n categorical_distribution_entropy = categorical_distribution.entropy()\n \n # initialize\n init = tf.global_variables_initializer()\n sess = tf.Session()\n sess.run(init)\n \n # works\n print(sess.run(entropy))\n print(sess.run(tf.gradients(entropy, [logits])))\n \n # apparently loses gradient information\n print(sess.run(categorical_distribution_entropy))\n print(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))\n In the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.\n <denchmark-code>[ 0.83239555  0.27431309]\n [array([[ 0.14181709,  0.14077036, -0.28258738],\n        [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]\n [ 0.83239555  0.27431309]\n [array([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]], dtype=float32)]\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lwohlhart", "commentT": "2017-01-09T02:25:16Z", "comment_text": "\n \t\tI suspect this a numerical stability issue. <denchmark-link:https://gist.github.com/yaroslavvb/97504b8221a8529e7a51a50915206d68>Looking</denchmark-link>\n  at your \"working\" graph, it uses . Whereas in the categorical distribution, it computes  and combines it with  <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n \n <denchmark-link:https://cloud.githubusercontent.com/assets/23068/21756041/dd734ea4-d5d0-11e6-82d0-001d9fab5bea.png></denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lwohlhart", "commentT": "2017-01-18T16:37:43Z", "comment_text": "\n \t\tTaking a look.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lwohlhart", "commentT": "2017-01-18T19:33:38Z", "comment_text": "\n \t\tFor numerical stability, we use tf.nn.softmax_cross_entropy_with_logits to calculate the entropy.  I just found out that this operation does not perform backprop with respect to the \"labels\" argument (the exponentiated probabilities term of the entropy).  Thus the zeros.\n Probably the best way to fix this is for us to implement the numerically stable equivalent of your reduce_sum.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lwohlhart", "commentT": "2017-01-18T19:53:16Z", "comment_text": "\n \t\tFix going into master hopefully today / tomorrow.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lwohlhart", "commentT": "2017-01-24T01:56:15Z", "comment_text": "\n \t\tthank you for the fix !\n i guess we can close this then\n \t\t"}}}, "commit": {"commit_id": "b39773478542bf812a12715f7f753f9a88e5e86c", "commit_author": "Eugene Brevdo", "commitT": "2017-01-18 15:50:31-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\contrib\\distributions\\python\\kernel_tests\\categorical_test.py", "file_new_name": "tensorflow\\contrib\\distributions\\python\\kernel_tests\\categorical_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175", "deleted_lines": null, "method_info": {"method_name": "testEntropyGradient", "method_params": "self", "method_startline": "151", "method_endline": "175"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\contrib\\distributions\\python\\ops\\categorical.py", "file_new_name": "tensorflow\\contrib\\distributions\\python\\ops\\categorical.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "239,240,241", "deleted_lines": null, "method_info": {"method_name": "_kl_categorical_categorical", "method_params": "a,b,name", "method_startline": "223", "method_endline": "241"}}, "hunk_1": {"Ismethod": 1, "added_lines": "212,213", "deleted_lines": "212,213", "method_info": {"method_name": "_entropy", "method_params": "self", "method_startline": "211", "method_endline": "213"}}}}}}}