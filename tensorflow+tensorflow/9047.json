{"BR": {"BR_id": "9047", "BR_author": "taion", "BRopenT": "2017-04-07T16:08:11Z", "BRcloseT": "2017-04-13T20:27:35Z", "BR_text": {"BRsummary": "beta2_power is applied incorrectly in Adam optimizer", "BRdescription": "\n In adam.py and in the ApplyAdam op, the denominator is effectively:\n <denchmark-code>(tf.sqrt(v_t) + epsilon_t) / tf.sqrt(1 - beta2_power)\n </denchmark-code>\n \n However, this appears incorrect \u2013 per the paper, the correct EMA adjustment should give:\n <denchmark-code>tf.sqrt(v_t / (1 - beta2_power)) + epsilon_t\n </denchmark-code>\n \n Otherwise, when epsilon_t is large relative to tf.sqrt(v_t), the effective epsilon used in the denominator is also scaled up by the correction factor, which doesn't match what's in the paper.\n Does this seem right, or am I missing something here?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "taion", "commentT": "2017-04-07T22:39:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/georgedahl>@georgedahl</denchmark-link>\n  <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>\n  <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>\n  : Any comments on this (or know whom to redirect to?)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "taion", "commentT": "2017-04-07T23:24:47Z", "comment_text": "\n \t\tJust so I'm clear, you do agree that the TensorFlow implementation matches the expression just before Section 2.1, with the \"epsilon hat\" in it? It's just that \"epsilon hat\" is a scaled version of the epsilon in Algorithm 1 in the paper, right?\n Is there a use-case for having the un-scaled epsilon as an option?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "taion", "commentT": "2017-04-07T23:25:20Z", "comment_text": "\n \t\tI think this is correct. If we look in the paper <denchmark-link:https://arxiv.org/pdf/1412.6980.pdf>https://arxiv.org/pdf/1412.6980.pdf</denchmark-link>\n  at the bottom of section 2 (Algorithm), it says \"the efficiency of algorithm 1 can, at the expense of clarity, be improved...\" and lists an update like what we are doing, no?\n <denchmark-link:https://cloud.githubusercontent.com/assets/994930/24822992/edac7cae-1bc7-11e7-9a15-0f4ca0250a5e.png></denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "taion", "commentT": "2017-04-07T23:32:10Z", "comment_text": "\n \t\tAh, the \"epsilon hat\" vs the regular epsilon is what I was missing (though \"epsilon hat\" is never defined that I can see, I assume it is a scaled version). I agree this is probably a bug.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "taion", "commentT": "2017-04-07T23:49:17Z", "comment_text": "\n \t\tOops, yeah \u2013 I think it should be something like:\n epsilon_hat = epsilon_t * tf.sqrt(1 - beta2_power)\n Otherwise without this scaling the effective epsilon is scaled by 1. / tf.sqrt(1 - beta2_power).\n With the default settings it probably barely matters (so you get an epsilon of ~3e-7 instead of 1e-8... big deal), but if you follow the recommendations in the comments and e.g. set epsilon to 1 or 0.1, then you effectively will barely update the weights for the first few iterations until beta2_power gets closer to 0 \u2013 like if you set epsilon to 1, then the effective \"epsilon\" you'd use for the first iteration would be ~31.\n I can't really think of a reason why you'd want to incorrectly scale the epsilon in this case.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "taion", "commentT": "2017-04-08T00:40:56Z", "comment_text": "\n \t\tOn the other hand, the recommendation in that comment is definitely referring to epsilon_hat being 1 or 0.1, not epsilon (i.e. the person who wrote it was using TensorFlow's implementation of Adam).\n <denchmark-link:https://github.com/skywaLKer518>@skywaLKer518</denchmark-link>\n : Any thoughts on epsilon vs. epsilon_hat? It looks like you authored that bit of advice on large values of epsilon for Adam.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "taion", "commentT": "2017-04-11T17:25:39Z", "comment_text": "\n \t\tIt is very possible that the optimizer gets modified after I tested and added the comments (I'm not exactly sure now -- the code is definitely at least re-organized and I do not recall the use of epsilon_hat clearly) so that the comment might not apply any more. But at the time we test it on Inception (summer 2015), we observe much better performance with large epsilon (e.g. 1.0) than the default values like 1e-8 (which sometimes even causes objective divergence if I remember correctly).\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "taion", "commentT": "2017-04-11T17:45:46Z", "comment_text": "\n \t\tepsilon and epsilon_hat end up approximately the same after a few thousand iterations anyway \u2013 not using epsilon_hat just means that training will start out very slow at the beginning when using a large value of epsilon.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "taion", "commentT": "2017-04-11T17:49:43Z", "comment_text": "\n \t\tInteresting. I'm inclined to keep behavior as-is, since it probably doesn't matter for the on-label use of avoiding short-term numerical instability due to near-zero gradients. In fact we'd need to be careful that a correction didn't introduce numerical issues.\n The off-label advice for making long-term training more stable has developed for epsilon_hat, so we'd make people re-tune their hyperparameters for questionable benefit by changing the default (i.e. maybe very small updates for the first few thousand iterations is desirable).\n However, the documentation should certainly be updated to let people know that it's epsilon_hat rather than epsilon that they're setting.\n <denchmark-link:https://github.com/taion>@taion</denchmark-link>\n : Does that sound reasonable? I'm happy to put together the documentation changes.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "taion", "commentT": "2017-04-11T18:00:53Z", "comment_text": "\n \t\tJust checked a few other packages \u2013\n \n Keras uses the same incorrect formulation as here: https://github.com/fchollet/keras/blob/7f58b6fbe702c1936e88a878002ee6e9c469bc77/keras/optimizers.py#L389-L400\n PyTorch uses the same incorrect formulation as here: https://github.com/pytorch/pytorch/blob/f17cfe42936310a2e3fd573e1f4dec8c684d4003/torch/optim/adam.py#L68-L72\n Lasagne uses the correct implementation, but in the unoptimized form: https://github.com/Lasagne/Lasagne/blob/45bb5689f0b2edb7114608e88305e8074d29bbe7/lasagne/updates.py#L620-L622\n \n I mean, I don't know. The correct version seems unlikely to cause issues for the on-label case, or people using Lasagne would have had problems.\n I don't really want to train an InceptionNet from scratch to see what this does for the off-label case; I will just note that when I tried using a larger epsilon a while ago, my model seemed to not train at all even when I used a higher learning rate, which would suggest that the current implementation makes things worse in the off-label case of using a higher epsilon.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "taion", "commentT": "2017-04-12T17:47:02Z", "comment_text": "\n \t\tThank you for taking a look at other frameworks! That's an interesting list.\n In terms of numerical issues, I'm particularly worried about float16 users. AdamOptimizer already requires tuning the default epsilon_hat of 1e-8 to at least 1e-7 to avoid errors. If we naively divided it by anything more than 3, it would underflow. Specifying epsilon_hat directly at least makes reasoning about precision issues a bit easier.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "taion", "commentT": "2017-04-12T18:45:51Z", "comment_text": "\n \t\tOh! I didn't think of the float16 case.\n Maybe the first-best would be to rename the current epsilon to epsilon_hat (with a deprecation notice for the moment), then possibly add a proper epsilon later on?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "taion", "commentT": "2017-04-12T20:55:49Z", "comment_text": "\n \t\tThe current formulation is sufficiently useful for numerical stability that we'll want to keep it around. I'll do the documentation fix and mark this as closed once that propagates. Feel free to open a feature request for a second epsilon parameter (\"epsilon_nohat\"?), or work on a pull request, but I don't think it's going to be a priority without experimental evidence that it's useful.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "taion", "commentT": "2017-04-12T21:04:37Z", "comment_text": "\n \t\tSounds good, thanks.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "taion", "commentT": "2017-04-12T21:06:36Z", "comment_text": "\n \t\tFix is submitting, should be synced within a day or so. Thank you for the report!\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "taion", "commentT": "2017-04-13T20:32:42Z", "comment_text": "\n \t\tThanks!\n \t\t"}}}, "commit": {"commit_id": "1926b9d3b91375f5a4433303a27b49b4da53f64e", "commit_author": "A. Unique TensorFlower", "commitT": "2017-04-12 15:04:53-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\python\\training\\adam.py", "file_new_name": "tensorflow\\python\\training\\adam.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "64,65,66,67,84,85,86", "deleted_lines": "64,81"}}}}}}