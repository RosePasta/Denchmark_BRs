{"BR": {"BR_id": "3624", "BR_author": "sbrodeur", "BRopenT": "2016-08-03T15:32:28Z", "BRcloseT": "2016-10-25T18:04:29Z", "BR_text": {"BRsummary": "Basic Element-wise Complex Number Calculations Not Available On GPU", "BRdescription": "\n Basic element-wise addition, subtraction, multiplication or division for any Tensor of type tf.complex64 is not implemented on GPU.\n <denchmark-h:h3>Environment info</denchmark-h>\n \n Operating System: Centos 7,  3.10.0-327.22.2.el7.x86_64\n Installed version of CUDA and cuDNN:  CUDA 7.5 and cuDNN 7.0-v4\n -rw-r--r--. 1 root root 189170 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudadevrt.a\n lrwxrwxrwx. 1 root root     16 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\n lrwxrwxrwx. 1 root root     19 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n -rwxr-xr-x. 1 root root 311596 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n -rw-r--r--. 1 root root 558020 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart_static.a\n Tensorflow installed from source:\n \n Commit hash 00700f0\n Bazel information:\n Build label: 0.3.0-2016-07-22 (@ca36b06)\n Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n Build time: Fri Jul 22 19:23:10 2016 (1469215390)\n Build timestamp: 1469215390\n Build timestamp as int: 1469215390\n \n <denchmark-h:h3>Steps to reproduce</denchmark-h>\n \n \n Add, subtract, multiply or divide any Tensor of type tf.complex64. A code example is shown here for element-wise addition:\n \n <denchmark-code>import tensorflow as tf\n \n if __name__ == '__main__':\n \n     with tf.device('/gpu:0'):\n         N = 100\n         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n         c = a + b\n \n         with tf.Session() as sess:\n             c = sess.run(c)\n </denchmark-code>\n \n The code returns the following output if run on GPU (works well on CPU):\n I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally\n I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally\n I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally\n I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally\n I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\n name: Tesla K40c\n major: 3 minor: 5 memoryClockRate (GHz) 0.745\n pciBusID 0000:02:00.0\n Total memory: 12.00GiB\n Free memory: 11.90GiB\n W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x5168890\n I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\n name: GeForce GT 610\n major: 2 minor: 1 memoryClockRate (GHz) 1.62\n pciBusID 0000:01:00.0\n Total memory: 1023.19MiB\n Free memory: 396.98MiB\n I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\n I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\n I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1\n I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N\n I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\n I tensorflow/core/common_runtime/gpu/gpu_device.cc:814] Ignoring gpu device (device: 1, name: GeForce GT 610, pci bus id: 0000:01:00.0) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.5.\n E tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n [[Node: add = Add[T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex, Complex_1)]]\n Traceback (most recent call last):\n File \"test_div_gpu_prob.py\", line 12, in \n c = sess.run(c)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 382, in run\n run_metadata_ptr)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 655, in _run\n feed_dict_string, options, run_metadata)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n target_list, options, run_metadata)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n [[Node: add = Add[T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex, Complex_1)]]\n Caused by op u'add', defined at:\n File \"test_div_gpu_prob.py\", line 9, in \n c = a + b\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 755, in binary_op_wrapper\n return func(x, y, name=name)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 70, in add\n result = _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n op_def=op_def)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n original_op=self._default_original_op, op_def=op_def)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1232, in init\n self._traceback = _extract_stack()\n <denchmark-h:h3>What have you tried?</denchmark-h>\n \n \n Implementation using builtin Tensorflow functions works, if the real and imaginary parts are separated. See the code below:\n \n <denchmark-code>import numpy as np\n import tensorflow as tf\n \n def complex_add(x, y):\n     xr, xi = tf.real(x), tf.imag(x)\n     yr, yi = tf.real(y), tf.imag(y)\n     return tf.complex(xr + yr, xi + yi)\n \n def complex_sub(x, y):\n     xr, xi = tf.real(x), tf.imag(x)\n     yr, yi = tf.real(y), tf.imag(y)\n     return tf.complex(xr - yr, xi - yi)\n \n def complex_mul(x, y):\n     xr, xi = tf.real(x), tf.imag(x)\n     yr, yi = tf.real(y), tf.imag(y)\n     return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)\n \n def complex_div(x, y):\n     xr, xi = tf.real(x), tf.imag(x)\n     yr, yi = tf.real(y), tf.imag(y)\n     d = tf.square(yr) + tf.square(yi)\n     return tf.complex((xr*yr+xi*yi)/d, (xi*yr-xr*yi)/d)\n \n if __name__ == '__main__':\n \n     with tf.device('/gpu:0'):\n         N = 100\n         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n \n         with tf.Session() as sess:\n \n             a_, b_, c = sess.run([a,b,complex_add(a,b)])\n             assert np.allclose(c, a_ + b_)\n \n             a_, b_, c = sess.run([a,b,complex_sub(a,b)])\n             assert np.allclose(c, a_ - b_)\n \n             a_, b_, c = sess.run([a,b,complex_mul(a,b)])\n             assert np.allclose(c, a_ * b_)\n \n             a_, b_, c = sess.run([a,b,complex_div(a,b)])\n             assert np.allclose(c, a_ / b_)\n </denchmark-code>\n \n It would be nice to have such functions transparent with the built-in CPU implementations.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sbrodeur", "commentT": "2016-08-03T18:17:08Z", "comment_text": "\n \t\tNote: implementations using built-in Tensorflow functions as show above doesn't solve gradient issues caused by the handling of complex numbers:\n <denchmark-code>import tensorflow as tf\n \n def complex_mul(x, y):\n     xr, xi = tf.real(x), tf.imag(x)\n     yr, yi = tf.real(y), tf.imag(y)\n     return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)\n \n if __name__ == '__main__':\n \n     with tf.device('/gpu:0'):\n         N = 100\n         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n         c = complex_mul(a, b)\n \n         grad = tf.gradients([c], [a])\n \n         with tf.Session() as sess:\n             grad = sess.run(grad)\n </denchmark-code>\n \n This code will fail with the following error:\n E tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n [[Node: gradients/Shape = Shape<denchmark-link:Complex_2>T=DT_COMPLEX64, _device=\"/device:GPU:0\"</denchmark-link>\n ]]\n Traceback (most recent call last):\n File \"test_div_gpu_grad_prob.py\", line 19, in \n grad = sess.run(grad)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 382, in run\n run_metadata_ptr)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 655, in _run\n feed_dict_string, options, run_metadata)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n target_list, options, run_metadata)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n [[Node: gradients/Shape = Shape<denchmark-link:Complex_2>T=DT_COMPLEX64, _device=\"/device:GPU:0\"</denchmark-link>\n ]]\n Caused by op u'gradients/Shape', defined at:\n File \"test_div_gpu_grad_prob.py\", line 16, in \n grad = tf.gradients([c], [a])\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 367, in gradients\n grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 230, in _DefaultGradYs\n array_ops.shape(y),\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 131, in shape\n return gen_array_ops.shape(input, name=name)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1922, in shape\n result = _op_def_lib.apply_op(\"Shape\", input=input, name=name)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n op_def=op_def)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n original_op=self._default_original_op, op_def=op_def)\n File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1232, in \n self._traceback = _extract_stack()\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sbrodeur", "commentT": "2016-08-03T22:53:42Z", "comment_text": "\n \t\tIt seems that support for complex64 types is piecemeal, by op-type and device-type.  Bringing in <denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>\n  for a comment on the policy.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sbrodeur", "commentT": "2016-08-03T22:57:51Z", "comment_text": "\n \t\tYes, this is a good feature request bordering on a bug. Please check the Op registrations of the affected ops, and you'll probably find that the templates of many of them are not specialized for complex data types. It is a relatively simple thing to fix, and I'd love PRs that do it.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "sbrodeur", "commentT": "2016-08-08T12:56:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>\n : Are you currently working on this?\n If not, I could go ahead and attempt a fix.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "sbrodeur", "commentT": "2016-08-08T14:02:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ibab>@ibab</denchmark-link>\n : I did not yet attempt a fix. I've looked a at little at Eigen:\n \"\"\n [https://eigen.tuxfamily.org/dox-devel/TopicCustomizingEigen.html]\n Thus, for the simple calculations here, should I expect Eigen to provide compatible functors, e.g. :\n <denchmark-code>template <typename T>\n struct add : base<T, Eigen::internal::scalar_sum_op<T> > {\n   static const bool use_bcast_optimization = true;\n };\n </denchmark-code>\n \n This code is in file cwise_ops.h\n Does this means the fix is similar to <denchmark-link:https://github.com/tensorflow/tensorflow/pull/2263>#2263</denchmark-link>\n , i.e. just adding the complex64 type when we register the kernels?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "sbrodeur", "commentT": "2016-08-08T14:14:47Z", "comment_text": "\n \t\tYes, you won't have to implement the operations themselves, you just need to enable them.\n For example, you can look at the supported types for the addition op here:\n \n \n \n tensorflow/tensorflow/core/kernels/cwise_op_add.cc\n \n \n          Line 22\n       in\n       32bd3d0\n \n \n \n \n \n \n  REGISTER4(BinaryOp, GPU, \"Add\", functor::add, float, Eigen::half, double, \n \n \n \n \n \n You would need to add complex64 and complex128 to the macro (and change it into REGISTER6).\n You should make sure that the GPU tests are enabled for complex64 and complex128 for each op that has been extended, for example here: \n \n \n tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py\n \n \n          Line 362\n       in\n       32bd3d0\n \n \n \n \n \n \n  def testComplex64Basic(self): \n \n \n \n \n .\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "sbrodeur", "commentT": "2016-08-08T14:27:15Z", "comment_text": "\n \t\tThanks for the information <denchmark-link:https://github.com/ibab>@ibab</denchmark-link>\n ! I will attempt a fix myself and send a PR soon!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "sbrodeur", "commentT": "2016-08-12T16:43:15Z", "comment_text": "\n \t\tSo far, I can make it work with some operations (add, sub) by simply adding the complex data types when registering the kernels: e.g. \n \n \n tensorflow/tensorflow/core/kernels/cwise_op_add.cc\n \n \n          Line 22\n       in\n       32bd3d0\n \n \n \n \n \n \n  REGISTER4(BinaryOp, GPU, \"Add\", functor::add, float, Eigen::half, double, \n \n \n \n \n \n <denchmark-code>DEFINE_BINARY6(add, Eigen::half, float, double, int64, complex64, complex128);\n </denchmark-code>\n \n Compilation errors however occur for multiplication (and division), as seen below.\n Searching the web, I found here that CUDA may not support std::complex because of STL incompatibilities:\n <denchmark-link:https://forum.kde.org/viewtopic.php?f=74&t=123919>https://forum.kde.org/viewtopic.php?f=74&t=123919</denchmark-link>\n \n It seems to solve this problem, people have been using reimplementations of the std:complex type (e.g. from thrust, cuda_complex or cusp) so that it can be used in device code:\n <denchmark-link:https://github.com/thrust/thrust/blob/2ef13096187b40a35a71451d09e49b14074b0859/thrust/complex.h>https://github.com/thrust/thrust/blob/2ef13096187b40a35a71451d09e49b14074b0859/thrust/complex.h</denchmark-link>\n \n <denchmark-link:https://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp>https://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp</denchmark-link>\n \n <denchmark-link:https://github.com/cusplibrary/cusplibrary/blob/master/cusp/complex.h>https://github.com/cusplibrary/cusplibrary/blob/master/cusp/complex.h</denchmark-link>\n \n Would the Eigen library implementing something similar to what thrust uses solve the issue in Tensorflow?\n <denchmark-h:h3>Compilation output</denchmark-h>\n \n INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_mul.cu.cc:\n nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,\n from /usr/local/cuda-7.5/include/cuda_runtime.h:76,\n from :0:\n /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n <denchmark-h:h1>warning _FORTIFY_SOURCE requires compiling with optimization (-O)</denchmark-h>\n \n <denchmark-code>^\n </denchmark-code>\n \n In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,\n from /usr/local/cuda-7.5/include/cuda_runtime.h:76,\n from :0:\n /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n <denchmark-h:h1>warning _FORTIFY_SOURCE requires compiling with optimization (-O)</denchmark-h>\n \n <denchmark-code>^\n </denchmark-code>\n \n In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,\n from /usr/local/cuda-7.5/include/cuda_runtime.h:76,\n from :0:\n /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\n <denchmark-h:h1>warning _FORTIFY_SOURCE requires compiling with optimization (-O)</denchmark-h>\n \n <denchmark-code>^\n </denchmark-code>\n \n nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n 12 errors detected in the compilation of \"/tmp/tmpxft_0000430f_00000000-12_cwise_op_gpu_mul.cu.compute_35.cpp2.i\".\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "sbrodeur", "commentT": "2016-08-13T02:37:33Z", "comment_text": "\n \t\tStrange, your errors seem to be caused by the fact that Eigen is trying to assign values from an int Tensor into a complex Tensor.\n I don't think that's supposed to happen usually.\n I've tried enabling complex mul and div just now, and it successfully compiled and ran when I restricted the ops to run on the GPU.\n I'm also using CUDA 7.5, not sure what else could be different between our setups.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "sbrodeur", "commentT": "2016-08-13T15:40:17Z", "comment_text": "\n \t\tHere is my configuration:\n GPU: Tesla K40c\n Operating System: CentOS Linux 7 (Core)\n Kernel: Linux 3.10.0-327.22.2.el7.x86_64\n Architecture: x86-64\n C Compiler: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\n CUDA Compiler: Cuda compilation tools, release 7.5, V7.5.17\n I will try with a more recent gcc (e.g. 4.9.2) to see if the compilation problem disappears.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "sbrodeur", "commentT": "2016-08-13T16:30:20Z", "comment_text": "\n \t\tI'm also using  and.\n Not sure what could be causing this if these two are the same.\n I've uploaded my changes to <denchmark-link:https://github.com/ibab/tensorflow/commit/8c3baae08bd449d25f80e9af8ae4830eb7ae2670>ibab@8c3baae</denchmark-link>\n , so you might want to compare these with yours.\n If that doesn't help we should try rebasing to the same TensorFlow commit.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "sbrodeur", "commentT": "2016-08-13T18:30:58Z", "comment_text": "\n \t\tSadly, I obtain the same errors if I clone and compile the fork <denchmark-link:https://github.com/ibab/tensorflow/commit/8c3baae>ibab/tensorflow@8c3baae</denchmark-link>\n  without any modifications.\n <denchmark-link:https://github.com/ibab>@ibab</denchmark-link>\n  - What is your Linux distribution?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "sbrodeur", "commentT": "2016-08-13T18:42:39Z", "comment_text": "\n \t\tI'm running Scientific Linux 6, which should be virtually identical to Red Hat 6.\n I get my compiler toolchain from anaconda, though.\n I'll try to make sure it's not something weird on my end.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "sbrodeur", "commentT": "2016-08-13T18:54:49Z", "comment_text": "\n \t\tOn my side, I'll try a build on my laptop which runs the latest Debian 8 (Jessie). I don't have a Nvidia GPU but I should nevertheless be able to compile with CUDA.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "sbrodeur", "commentT": "2016-08-13T19:11:38Z", "comment_text": "\n \t\tOkay, I've rebuilt tensorflow after a bazel clean --expunge to make extra sure that I'm using the right Eigen version, as I've changed it around a few times previously, but it still built successfully.\n Edit: Btw, do you also get compiler warnings about calling __host__ functions from device code as in the link you posted above?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "sbrodeur", "commentT": "2016-08-14T14:50:00Z", "comment_text": "\n \t\tI do not get compiler warnings about calling host functions from device code.\n I just tried to build on my laptop (Debian 8, up-to-date) with configuration;\n gcc (Debian 4.9.2-10) 4.9.2\n Cuda compilation tools, release 7.5, V7.5.17\n I obtained the same errors, so it does not seem related to gcc or distribution. I also tried to build with the latest eigen (3782cd1de9c4) on the Centos 7 machine, and that did not help either. I will try building with CUDA 8, after which I will be clueless about those compilation issues.\n Edit:  same errors with CUDA 8.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "sbrodeur", "commentT": "2016-08-14T16:10:20Z", "comment_text": "\n \t\tI've tried compiling with different compute capabilities, but it still compiled without errors.\n The fact that you reproduced it on two different systems makes me think that it's a problem with my setup, though.\n Unfortunately nvcc doesn't give us a lot of information in the error message (like which template instantiations we are dealing with) :(\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "sbrodeur", "commentT": "2016-08-23T22:17:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ibab>@ibab</denchmark-link>\n  , <denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>\n  , thank you so much for working on this. I think it would really speed up one of my projects. Is there any new progress? Are you planning to include this on the next release of TensorFlow? What about basic math functions such as , ?\n Thanks again!\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "sbrodeur", "commentT": "2016-08-24T14:31:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/iportillo>@iportillo</denchmark-link>\n  - I will give it another try today. It would also significantly accelerate my experiments, since everything could run on the GPU. I'll try to see if it would be easy to use CUDABlas directly (rather than Eigen) for the basic math functions on complex numbers.\n tf.complex_abs is easy to implement on GPU right now:\n <denchmark-code>def complex_abs(x):\n     return tf.sqrt(tf.square(tf.real(x)) + tf.square(tf.imag(x)))\n </denchmark-code>\n \n By tf.exp(), do you mean converting from the Cartesian to the complex exponential form (angle and norm)? To calculate the angle, this means implementing the atan2 function (for complex x + iy):\n <denchmark-code>def atan2(y, x):\n     angle = tf.select(tf.greater(x,0.0), tf.atan(y/x) + np.pi, tf.zeros_like(x))\n     angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)\n     angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)\n     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)\n     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)\n     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)\n     return angle\n \n def complex_arg(x):\n     return atan2(tf.imag(x), tf.real(x))\n </denchmark-code>\n \n It's not optimized but works well on GPU.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "sbrodeur", "commentT": "2016-08-24T15:12:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/benoitsteiner>@benoitsteiner</denchmark-link>\n : We're having some problems with implementing the product and div ops for  using the Eigen Tensor library.\n Do you see why we would get an error like the following when enabling them in TensorFlow?\n <denchmark-code>external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type \"int\" cannot be assigned to an entity of type \"_ZNSt7complexIfE9_ComplexTE\"\n </denchmark-code>\n \n <denchmark-code>$ c++filt _ZNSt7complexIfE9_ComplexTE\n std::complex<float>::_ComplexT\n </denchmark-code>\n \n Maybe we would need to switch to something like  as <denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>\n  suggested?\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "sbrodeur", "commentT": "2016-08-24T21:41:53Z", "comment_text": "\n \t\tI made some progress! I can make multiplication and division ops work for complex numbers if I specialized the templates in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops.h#L432>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops.h#L432</denchmark-link>\n \n <denchmark-code>template <typename T>\n struct mul : base<T, Eigen::internal::scalar_product_op<T> > {};\n \n template <typename T>\n struct multiply_complex {\n   typedef std::complex<T> result_type;\n   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex<T> a,\n                                                                std::complex<T> b) const {\n     return std::complex<T>(a.real()*b.real() - a.imag()*b.imag(),\n                            a.real()*b.imag() + a.imag()*b.real());\n   }\n };\n \n template <>\n struct mul<std::complex<float> > : base<std::complex<float>, multiply_complex<float> > {};\n \n template <>\n struct mul<std::complex<double> > : base<std::complex<double>, multiply_complex<double> > {};\n \n </denchmark-code>\n \n It seems more like a hack, but it doesn't involve changes in Eigen for now.\n Not sure what is wrong with nvcc using scalar_product_op in Eigen for complex numbers:\n <denchmark-link:https://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/functors/BinaryFunctors.h#L76>https://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/functors/BinaryFunctors.h#L76</denchmark-link>\n \n However, it seems tightly related to using built-in * and / operators for std:complex types.\n For instance, this fails with the same errors as in the previous posts:\n <denchmark-code>template <typename T>\n struct mul : base<T, Eigen::internal::scalar_product_op<T> > {};\n \n template <typename T>\n struct multiply_complex {\n   typedef std::complex<T> result_type;\n   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex<T> a,\n                                                                std::complex<T> b) const {\n     return a*b;\n   }\n };\n \n template <>\n struct mul<std::complex<float> > : base<std::complex<float>, multiply_complex<float> > {};\n \n template <>\n struct mul<std::complex<double> > : base<std::complex<double>, multiply_complex<double> > {};\n \n </denchmark-code>\n \n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "sbrodeur", "commentT": "2016-08-24T23:26:17Z", "comment_text": "\n \t\tI can confirm that with the above trick, I can make work a lot of very useful functions for complex numbers on GPU (e.g. square, neg, div, mul, abs) This brings support for complex gradient computation on GPU:\n <denchmark-code>import tensorflow as tf\n \n if __name__ == '__main__':\n \n     with tf.device('/gpu:0'):\n         N = 100\n         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n         c = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))\n \n         d = c * tf.neg(tf.square(a + b))\n \n         grad = tf.gradients([d], [a])\n \n         with tf.Session() as sess:\n             grad = sess.run(grad)\n </denchmark-code>\n \n Should I make a PR or should we investigate further the handling of std::complex by nvcc?\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "sbrodeur", "commentT": "2016-08-29T15:23:06Z", "comment_text": "\n \t\tI'm not a maintainer, but I think a PR would definitely be a good idea \ud83d\udc4d\n Maybe you can split it into two PRs, one that requires the extra scalar_prod_op specialization, and one that doesn't?\n In the long term, it would probably be best to get the specialization into Eigen itself, or to find another fix (like switching to thrust::complex).\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "sbrodeur", "commentT": "2016-09-13T17:39:21Z", "comment_text": "\n \t\tI've been privately writing GPU-based complex-valued ops for TF and decided to make my repository public. I think that more general support for computation of complex numbers on the GPU will be valuable to the community. However since my repository is in the early stages and isn't well tested, I think I'd like to develop it as a separate project and then port it as a TF pull request when it's more mature.   Feel free to make contributions and/or suggestions.\n <denchmark-link:https://github.com/woodshop/complex_tf>https://github.com/woodshop/complex_tf</denchmark-link>\n \n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "sbrodeur", "commentT": "2016-09-14T20:16:29Z", "comment_text": "\n \t\tIn C++14,  std::complex methods are marked as constexpr. This will ensure that they can be used inside cuda kernels even though they're not marked as __device__ functions provided that we compile with the --relaxed-constexpr flag (which TensorFlow has been doing for some time now).\n Unfortunately nvcc doesn't yet support c++14, but we can ask nvidia to start adding partial support for it starting with complex numbers.\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "sbrodeur", "commentT": "2016-09-15T19:26:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/iportillo>@iportillo</denchmark-link>\n  ComplexAbs (and a few others) added here: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/f21642013092b53186491064335053a9e02ce010>f216420</denchmark-link>\n \n and the corresponding Eigen change:\n <denchmark-link:https://bitbucket.org/eigen/eigen/commits/6d4cd6e5cdd9c750b10cc4c6a374e4c513b267ed>https://bitbucket.org/eigen/eigen/commits/6d4cd6e5cdd9c750b10cc4c6a374e4c513b267ed</denchmark-link>\n \n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "sbrodeur", "commentT": "2016-09-28T00:58:56Z", "comment_text": "\n \t\tAfter adding a workaround to Eigen:\n <denchmark-link:https://bitbucket.org/eigen/eigen/commits/27f6140fa81c9fe83167d87e7aeb23031b42f344>https://bitbucket.org/eigen/eigen/commits/27f6140fa81c9fe83167d87e7aeb23031b42f344</denchmark-link>\n \n We were able to enable addition, subtraction, division, and multiplication kernels for complex types on GPU: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/93f15d4cde2f08057819f1194e5a4771f0d391ff>93f15d4</denchmark-link>\n \n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "sbrodeur", "commentT": "2016-10-11T15:48:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>\n  Does TensorFlow now support all the operations you need on complex, or are there additional improvements we need to make ?\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "sbrodeur", "commentT": "2016-10-25T16:54:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/benoitsteiner>@benoitsteiner</denchmark-link>\n  Tensorflow now supports everything I need for handling complex numbers.\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "sbrodeur", "commentT": "2016-10-25T18:04:29Z", "comment_text": "\n \t\tThanks, closing the issue.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "sbrodeur", "commentT": "2017-07-14T13:38:14Z", "comment_text": "\n \t\tIs it possible to calculate a complex number divide a float number without type cast?\n \t\t"}}}, "commit": {"commit_id": "53af29cb8503c7ed55a23d22090dd39ce0056a7a", "commit_author": "Olivia Nordquist", "commitT": "2016-09-14 16:31:25-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\core\\kernels\\cwise_op_add.cc", "file_new_name": "tensorflow\\core\\kernels\\cwise_op_add.cc", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,23", "deleted_lines": "22,23"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\core\\kernels\\cwise_op_gpu_add.cu.cc", "file_new_name": "tensorflow\\core\\kernels\\cwise_op_gpu_add.cu.cc", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22", "deleted_lines": "22"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\kernel_tests\\cwise_ops_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\cwise_ops_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "743", "deleted_lines": null, "method_info": {"method_name": "testComplex128Basic", "method_params": "self", "method_startline": "730", "method_endline": "743"}}, "hunk_1": {"Ismethod": 1, "added_lines": "728", "deleted_lines": null, "method_info": {"method_name": "testComplex64Basic", "method_params": "self", "method_startline": "715", "method_endline": "728"}}}}}}}