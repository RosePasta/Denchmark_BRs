{"BR": {"BR_id": "22013", "BR_author": "mpekalski", "BRopenT": "2018-09-02T21:09:33Z", "BRcloseT": "2018-10-12T20:17:50Z", "BR_text": {"BRsummary": "tf.scatter_nd_update - Segmentation fault (core dumped)", "BRdescription": "\n <denchmark-h:hr></denchmark-h>\n \n <denchmark-h:h3>System information</denchmark-h>\n \n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\n yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n Linux Ubuntu 16.04\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n TensorFlow installed from (source or binary):\n source\n TensorFlow version (use command below):\n \n TF checkpoint I have built\n <denchmark-code>/tmp/tensorflow# git log   \n commit 09792df012c22622324f085f46edde33006c7355\n Author: A. Unique TensorFlower <gardener@tensorflow.org>\n Date:   Sun Aug 26 02:07:11 2018 -0700\n \n     compat: Update forward compatibility horizon to 2018-08-26\n     \n     PiperOrigin-RevId: 210266798\n </denchmark-code>\n \n <denchmark-code>== cat /etc/issue ===============================================\n Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n VERSION=\"16.04.5 LTS (Xenial Xerus)\"\n VERSION_ID=\"16.04\"\n VERSION_CODENAME=xenial\n \n == are we in docker =============================================\n Yes\n \n == compiler =====================================================\n c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\n Copyright (C) 2015 Free Software Foundation, Inc.\n This is free software; see the source for copying conditions.  There is NO\n warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n \n \n == uname -a =====================================================\n Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n \n == check pips ===================================================\n numpy (1.14.5)\n protobuf (3.6.1)\n tensorflow (1.10.0)\n \n == check for virtualenv =========================================\n False\n \n == tensorflow import ============================================\n tf.VERSION = 1.10.0\n tf.GIT_VERSION = b'unknown'\n tf.COMPILER_VERSION = b'unknown'\n Sanity check: array([1], dtype=int32)\n \n == env ==========================================================\n LD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\n DYLD_LIBRARY_PATH is unset\n \n == nvidia-smi ===================================================\n Wed Aug 29 19:57:14 2018       \n +-----------------------------------------------------------------------------+\n | NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\n |-------------------------------+----------------------+----------------------+\n | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n |===============================+======================+======================|\n |   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n |  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\n +-------------------------------+----------------------+----------------------+\n                                                                                \n +-----------------------------------------------------------------------------+\n | Processes:                                                       GPU Memory |\n |  GPU       PID   Type   Process name                             Usage      |\n |=============================================================================|\n +-----------------------------------------------------------------------------+\n \n == cuda libs  ===================================================\n /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\n /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\n \n </denchmark-code>\n \n Bazel version\n <denchmark-code>$ bazel version\n WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\n Build label: 0.16.0\n Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n Build time: Tue Jul 31 17:01:24 2018 (1533056484)\n Build timestamp: 1533056484\n Build timestamp as int: 1533056484\n </denchmark-code>\n \n CUDNN version:\n <denchmark-code>$ nvcc --version\n nvcc: NVIDIA (R) Cuda compiler driver\n Copyright (c) 2005-2018 NVIDIA Corporation\n Built on Tue_Jun_12_23:07:04_CDT_2018\n Cuda compilation tools, release 9.2, V9.2.148\n </denchmark-code>\n \n GPU: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\n \n Exact command to reproduce:\n \n <denchmark-code>from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n \n def scope_1():\n     print(\"DS1 SCOPE =============\")\n     with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n         x = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\n                                , trainable=False, use_resource=True)             \n         print(\"graph: {}\".format(x.graph))\n         print(\"scope: {}\".format(tf.get_variable_scope().name))\n         print(\" name: {}\".format(x.name))\n         print(\"  var: {}\".format(str(x)))\n         current_scope = tf.get_variable_scope()       \n         assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\n     \n     def scope_2(inputs, label):        \n         print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n         print(\"DS1 SCOPE =============\")\n         #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n         with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n             y = tf.get_variable(\"x\", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32\n                                    , trainable=False, use_resource=True)         \n             print(\"graph: {}\".format(y.graph))\n             print(\"scope: {}\".format(tf.get_variable_scope().name))\n             print(\" name: {}\".format(y.name))\n             print(\"  var: {}\".format(str(y)))\n             print(\"=============\")\n             print(y)\n             print(inputs)\n             #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n             assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))\n             with tf.control_dependencies([assign_two]):\n                 with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\n                     return y.read_value(), label\n             #return x,label\n     \n     # test that original x is mutable\n     with tf.control_dependencies([assign_one]):\n         dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                     .map(scope_2)\n                     .batch(1)\n                     .repeat(1)        \n                     )\n     return dataset\n     \n                 \n with tf.variable_scope(\"scope_0\"):\n         dataset_fn = scope_1()\n \n with tf.variable_scope(\"iterator\"):\n     # Define iterator from_string_handle. In general it is useful to have\n     # this kind of iterator if one wants to switch between train and validation\n     # within the training loop.        \n     iterator_t = dataset_fn.make_initializable_iterator()\n     iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n     iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                 iterator_t.output_types,\n                                                 iterator_t.output_shapes)\n     \n     def get_next_item():\n         next_elem = iterator.get_next(name=\"next_element\")\n         x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n         return x, y    \n with tf.Session() as sess:\n \n     sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n     handle_t = sess.run(iterator_t.string_handle())\n     # Run data iterator initialisation\n     sess.run(iterator_t.initializer)\n     print(sess.graph.get_operations()) \n     while True:\n         try:\n             print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n         except tf.errors.OutOfRangeError:\n                         print(\"End of training dataset.\")\n                         break        \n     print()\n     print(\"global vars: {}\".format(tf.global_variables()))\n     print(\"local vars: {}\".format(tf.local_variables()))\n     print(tf.get_default_graph().get_name_scope())\n \n </denchmark-code>\n \n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n I am trying to create a function that would modify a Tensor within a pipeline of Dataset API.\n The scoping may seem weird, but that is minimal example that shows the problem that I created from my project. After adding  I started to get segmentation fault.\n A minimal example with  instead of  worked, see <denchmark-link:https://github.com/tensorflow/tensorflow/issues/22009>#22009</denchmark-link>\n \n I tried disabling GPU with config=tf.ConfigProto(device_count={'GPU': 0}) and CUDA_VISIBLE_DEVICES=\"\" but the result was the same.\n I will recompile TF overnight (from the current master) with --copt=-g and try to provide a stacktrace\n <denchmark-code>TF_BUILD_INFO = {container_type: \"gpu\", command: \"bazel build --config=opt --config=cuda --copt=-march=native --copt=-mfpmath=both --copt=-mtune=native --copt=-g --verbose_failures --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --jobs=8 --config=mkl --action_env=LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib tensorflow/tools/pip_package:build_pip_package\", source_HEAD: \"201be3d514d7239aa19496dba4dd0c85303b03f1\", source_remote_origin: \"https://github.com/tensorflow/tensorflow\", OS: \"Linux\", kernel: \"4.13.0-38-generic\", architecture: \"x86_64\", processor: \"Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz\", processor_count: \"8\", memory_total: \"32877820 kB\", swap_total: \"69444596 kB\", Bazel_version: \"Build label: 0.16.0\", Java_version: \"1.8.0_181\", Python_version: \"3.6.2\", gpp_version: \"g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\", swig_version: \"\", NVIDIA_driver_version: \"396.26\", CUDA_device_count: \"1\", CUDA_device_names: \"GeForce GTX 1080 Ti   (*PrimaryCard),\", CUDA_toolkit_version: \"V9.2.148\"}\n </denchmark-code>\n \n <denchmark-h:h3>Source code / logs</denchmark-h>\n \n <denchmark-code>DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x:0\n   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\n initial scope: \n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x_1:0\n   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n =============\n <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n Tensor(\"arg0:0\", shape=(), dtype=int32)\n 2018-09-02 20:52:39.456276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n 2018-09-02 20:52:39.456710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \n name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\n pciBusID: 0000:01:00.0\n totalMemory: 10.92GiB freeMemory: 10.23GiB\n 2018-09-02 20:52:39.456728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n 2018-09-02 20:52:39.665323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n 2018-09-02 20:52:39.665362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n 2018-09-02 20:52:39.665369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n 2018-09-02 20:52:39.665731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9887 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n 2018-09-02 20:52:39.756397: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n Segmentation fault (core dumped)\n </denchmark-code>\n \n log of run on CPU\n <denchmark-code>CUDA_VISIBLE_DEVICES=\"\" python3 bug.py \n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x:0\n   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\n initial scope: \n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x_1:0\n   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n =============\n <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n Tensor(\"arg0:0\", shape=(), dtype=int32)\n 2018-09-02 20:54:41.271567: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n 2018-09-02 20:54:41.271604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: 3bed2f328777\n 2018-09-02 20:54:41.271614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: 3bed2f328777\n 2018-09-02 20:54:41.271655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.26.0\n 2018-09-02 20:54:41.271686: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.26.0\n 2018-09-02 20:54:41.271694: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.26.0\n 2018-09-02 20:54:41.271962: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n Segmentation fault (core dumped)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mpekalski", "commentT": "2018-09-03T06:35:19Z", "comment_text": "\n \t\tThank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\n CUDA/cuDNN version\n GPU model and memory\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mpekalski", "commentT": "2018-09-03T07:44:02Z", "comment_text": "\n \t\tWhen executing line by line everything works fine until it starts the session.\n I've tried running it on different environment, on kaggle.com, and the it also fails.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "mpekalski", "commentT": "2018-09-03T08:14:44Z", "comment_text": "\n \t\tI tried it on one more environment. Where I have just installed TF from <denchmark-link:https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl>https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl</denchmark-link>\n \n I had to modify a bit lambda initializer in get_variable and specify the shape, but otherwise I got the same segmentation fault. This env does not have GPU, so it was run on a CPU.\n <denchmark-code># python3 tf_bug.py \n /opt/conda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n   from ._conv import register_converters as _register_converters\n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x:0\n   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\n initial scope: \n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x_1:0\n   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n =============\n <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n Tensor(\"arg0:0\", shape=(), dtype=int32)\n 2018-09-03 08:12:16.809170: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n Segmentation fault (core dumped)\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "mpekalski", "commentT": "2018-09-03T09:04:06Z", "comment_text": "\n \t\tI tried running the script in gdb but it says no stack. Any hints how to get it work?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "mpekalski", "commentT": "2018-09-04T08:33:16Z", "comment_text": "\n \t\tI've managed to get gdb working, looks like the issue is with inferring shape in scatter_nd_update\n <denchmark-code>0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()\n    from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\n </denchmark-code>\n \n Full log\n <denchmark-code># gdb python3\n GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1                                              Copyright (C) 2016 Free Software Foundation, Inc.                                         License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> py\n This is free software: you are free to change and redistribute it.\n There is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"gdb py\n and \"show warranty\" for details.\n This GDB was configured as \"x86_64-linux-gnu\".\n Type \"show configuration\" for configuration details.\n For bug reporting instructions, please see:\n <http://www.gnu.org/software/gdb/bugs/>.\n Find the GDB manual and other documentation resources online at:\n <http://www.gnu.org/software/gdb/documentation/>.\n For help, type \"help\".\n Type \"apropos word\" to search for commands related to \"word\"...\n Reading symbols from python3...done.\n (gdb) run bug.py\n Starting program: /opt/conda/bin/python3 bug.py\n [Thread debugging using libthread_db enabled]\n Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n [New Thread 0x7ffff31fa700 (LWP 606)]\n [New Thread 0x7ffff09f9700 (LWP 607)]\n [New Thread 0x7fffee1f8700 (LWP 608)]\n [New Thread 0x7fffeb9f7700 (LWP 609)]\n [New Thread 0x7fffe91f6700 (LWP 610)]\n [New Thread 0x7fffe69f5700 (LWP 611)]\n [New Thread 0x7fffe41f4700 (LWP 612)]\n [Thread 0x7fffe41f4700 (LWP 612) exited]\n [Thread 0x7fffe69f5700 (LWP 611) exited]\n [Thread 0x7fffe91f6700 (LWP 610) exited]\n [Thread 0x7fffeb9f7700 (LWP 609) exited]\n [Thread 0x7fffee1f8700 (LWP 608) exited]\n [Thread 0x7ffff09f9700 (LWP 607) exited]\n [Thread 0x7ffff31fa700 (LWP 606) exited]\n /opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n   return f(*args, **kwds)\n /opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n   return f(*args, **kwds)\n [New Thread 0x7fffe41f4700 (LWP 617)]\n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7ffff6789160>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x:0\n   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\n [New Thread 0x7fffe69f5700 (LWP 618)]\n initial scope:\n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7ffff6789160>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x_1:0\n   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n =============\n <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n Tensor(\"arg0:0\", shape=(), dtype=int32)\n [New Thread 0x7fffe91f6700 (LWP 619)]\n [New Thread 0x7fffeb9f7700 (LWP 620)]\n [New Thread 0x7fff871ff700 (LWP 621)]\n [New Thread 0x7fff869fe700 (LWP 622)]\n [New Thread 0x7fff861fd700 (LWP 623)]\n [New Thread 0x7fff859fc700 (LWP 624)]\n [New Thread 0x7fff851fb700 (LWP 625)]\n [New Thread 0x7fff849fa700 (LWP 626)]\n [New Thread 0x7fff5fdff700 (LWP 627)]\n [New Thread 0x7fff5f5fe700 (LWP 628)]\n [New Thread 0x7fff5edfd700 (LWP 629)]\n [New Thread 0x7fff5e5fc700 (LWP 630)]\n 2018-09-04 08:28:02.088839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n 2018-09-04 08:28:02.089267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\n name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\n pciBusID: 0000:01:00.0\n totalMemory: 10.92GiB freeMemory: 10.20GiB\n 2018-09-04 08:28:02.089287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\n 2018-09-04 08:28:02.089297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n 2018-09-04 08:28:02.089303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\n 2018-09-04 08:28:02.089307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\n 2018-09-04 08:28:02.089464: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n [New Thread 0x7fff5ddfb700 (LWP 631)]\n [New Thread 0x7fff5d5fa700 (LWP 632)]\n [New Thread 0x7fff5cdf9700 (LWP 633)]\n [Thread 0x7fff5cdf9700 (LWP 633) exited]\n [New Thread 0x7fff5cdf9700 (LWP 634)]\n [Thread 0x7fff5cdf9700 (LWP 634) exited]\n [New Thread 0x7fff5cdf9700 (LWP 635)]\n [Thread 0x7fff5cdf9700 (LWP 635) exited]\n [New Thread 0x7fff5cdf9700 (LWP 636)]\n [Thread 0x7fff5cdf9700 (LWP 636) exited]\n [New Thread 0x7fff5cdf9700 (LWP 637)]\n [Thread 0x7fff5cdf9700 (LWP 637) exited]\n [New Thread 0x7fff5cdf9700 (LWP 638)]\n [Thread 0x7fff5cdf9700 (LWP 638) exited]\n [New Thread 0x7fff5cdf9700 (LWP 639)]\n [Thread 0x7fff5cdf9700 (LWP 639) exited]\n [New Thread 0x7fff5cdf9700 (LWP 640)]\n [Thread 0x7fff5cdf9700 (LWP 640) exited]\n \n Thread 1 \"python3\" received signal SIGSEGV, Segmentation fault.\n 0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()\n    from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\n (gdb)\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "mpekalski", "commentT": "2018-09-04T09:10:55Z", "comment_text": "\n \t\tI've got the code running by explicitly specifying shape in tf.get_variable(), and then making sure that the corresponding tensors in assign ops had the same shape (so not assigning 1.0 but [1.0]). Still, I could not make it work with dynamic shapes, and segmentation fault should not take place but throw some error message.\n Here is the working code\n <denchmark-code>from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n \n def scope_1():\n     print(\"DS1 SCOPE =============\")\n     with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n         x = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\n                                , trainable=False, use_resource=True, shape=[1])             \n         print(\"graph: {}\".format(x.graph))\n         print(\"scope: {}\".format(tf.get_variable_scope().name))\n         print(\" name: {}\".format(x.name))\n         print(\"  var: {}\".format(str(x)))\n         current_scope = tf.get_variable_scope()       \n         assign_one = tf.assign(x, [1.0], name=\"x_is_one\")\n     \n     def scope_2(inputs, label):        \n         print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n         print(\"DS1 SCOPE =============\")\n         #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n         with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n             y = tf.get_variable(\"x\", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32\n                                    , trainable=False, use_resource=True, shape=[1])         \n             print(\"graph: {}\".format(y.graph))\n             print(\"scope: {}\".format(tf.get_variable_scope().name))\n             print(\" name: {}\".format(y.name))\n             print(\"  var: {}\".format(str(y)))\n             print(\"=============\")\n             print(y)\n             print(inputs)\n             #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n             assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), [1.0])))\n             with tf.control_dependencies([assign_two]):\n                 with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):\n                     return y.read_value(), label\n \n             #return x,label\n     \n     # test that original x is mutable\n     with tf.control_dependencies([assign_one]):\n         dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                     .map(scope_2)\n                     .batch(1)\n                     .repeat(1)        \n                     )\n     return dataset\n     \n                 \n with tf.variable_scope(\"scope_0\"):\n         dataset_fn = scope_1()\n \n with tf.variable_scope(\"iterator\"):\n     # Define iterator from_string_handle. In general it is useful to have\n     # this kind of iterator if one wants to switch between train and validation\n     # within the training loop.        \n     iterator_t = dataset_fn.make_initializable_iterator()\n     iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n     iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                 iterator_t.output_types,\n                                                 iterator_t.output_shapes)\n     \n     def get_next_item():\n         next_elem = iterator.get_next(name=\"next_element\")\n         x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n         return x, y    \n         \n with tf.Session() as sess:\n \n     sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n     handle_t = sess.run(iterator_t.string_handle())\n     # Run data iterator initialisation\n     sess.run(iterator_t.initializer)\n     print(sess.graph.get_operations()) \n     while True:\n         try:\n             print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n         except tf.errors.OutOfRangeError:\n                         print(\"End of training dataset.\")\n                         break        \n     print()\n     print(\"global vars: {}\".format(tf.global_variables()))\n     print(\"local vars: {}\".format(tf.local_variables()))\n     print(tf.get_default_graph().get_name_scope())\n </denchmark-code>\n \n and the output\n <denchmark-code>DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x:0\n   var: <tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>\n initial scope: \n DS1 SCOPE =============\n graph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>\n scope: scope_0/scope_1\n  name: scope_0/scope_1/x_1:0\n   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\n =============\n <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>\n Tensor(\"arg0:0\", shape=(), dtype=int32)\n [<tf.Operation 'Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0_1/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/scope_1/Const' type=Const>, <tf.Operation 'scope_0_1/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0_1/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/tensors/component_0' type=Const>, <tf.Operation 'scope_0_1/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/batch_size' type=Const>, <tf.Operation 'scope_0_1/count' type=Const>, <tf.Operation 'iterator/Iterator' type=Iterator>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDataset' type=BatchDataset>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandle' type=IteratorFromStringHandle>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\n (array([[0.22]], dtype=float32), array([-1], dtype=int32))\n (array([[0.22]], dtype=float32), array([-2], dtype=int32))\n (array([[0.22]], dtype=float32), array([-3], dtype=int32))\n (array([[0.22]], dtype=float32), array([-4], dtype=int32))\n (array([[0.22]], dtype=float32), array([-5], dtype=int32))\n End of training dataset.\n \n global vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>]\n local vars: []\n </denchmark-code>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "mpekalski", "commentT": "2018-09-04T09:14:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>\n  I have identified the reason for the segmentation fault. Please have a look at my comments above.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "mpekalski", "commentT": "2018-10-04T19:06:58Z", "comment_text": "\n \t\tNagging Assignee <denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>\n : It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "mpekalski", "commentT": "2018-10-11T00:46:16Z", "comment_text": "\n \t\tLooking at the documentation for scatter_nd_update (<denchmark-link:https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update>https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update</denchmark-link>\n ) seems like indexing into a zero rank tensor is sort of invalid? Adding Eugene for more info.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "mpekalski", "commentT": "2018-10-11T19:35:54Z", "comment_text": "\n \t\tThanks for reporting.  I found the cause of the problem and will send a fix.\n \t\t"}}}, "commit": {"commit_id": "85f4f6b7ced7afab7e77e65c2b21448cfbf2d6f2", "commit_author": "Eugene Brevdo", "commitT": "2018-10-12 13:15:27-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\framework\\common_shape_fns.cc", "file_new_name": "tensorflow\\core\\framework\\common_shape_fns.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1460,1461,1462,1463,1464,1521,1522", "deleted_lines": "1460,1517", "method_info": {"method_name": "tensorflow::shape_inference::ScatterNdUpdateShape", "method_params": "c", "method_startline": "1457", "method_endline": "1526"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\kernel_tests\\scatter_nd_ops_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\scatter_nd_ops_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "288,289,290,291,292,293,294,295", "deleted_lines": null, "method_info": {"method_name": "testResVarInvalidOutputShape", "method_params": "self", "method_startline": "288", "method_endline": "295"}}}}}}}