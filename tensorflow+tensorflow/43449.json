{"BR": {"BR_id": "43449", "BR_author": "woodyx218", "BRopenT": "2020-09-22T04:59:17Z", "BRcloseT": "2020-09-23T01:26:57Z", "BR_text": {"BRsummary": "tf.autodiff.ForwardAccumulator fails for Embedding layer", "BRdescription": "\n Please make sure that this is a bug. As per our\n GitHub Policy,\n we only address code/doc bugs, performance issues, feature requests and\n build/installation issues on GitHub. tag:bug_template\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n TensorFlow installed from (source or binary): pip\n TensorFlow version (use command below): 2.4.0-dev20200813\n Python version: 3.8.3\n Bazel version (if compiling from source):\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version: 10.1.168/7.6.5\n GPU model and memory:  GeForce GTX 1050/4GB\n \n You can collect some of this information using our environment capture\n <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>\n \n You can also obtain the TensorFlow version with:\n \n TF 1.0: python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\n TF 2.0: python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\n \n Describe the current behavior\n Calculating the Jacobian-vector product of an embedding layer produces\n AttributeError: 'IndexedSlices' object has no attribute '_as_tf_output'\n Describe the expected behavior\n No error, just like Dense, LSTM, convolutional layers. See a notebook link below that shows this error is ONLY related to Embedding layer.\n Standalone code to reproduce the issue\n Provide a reproducible test case that is the bare minimum necessary to generate\n the problem. If possible, please share a link to Colab/Jupyter/any notebook.\n <denchmark-code>import tensorflow as tf\n \n class RNN_Model(tf.keras.Model):\n     def __init__(self):\n         super(RNN_Model, self).__init__()\n         self.embed=tf.keras.layers.Embedding(5,1)\n         self.d2 = tf.keras.layers.Dense(2)\n         self(tf.constant([4,3,2])) # initialize\n     \n     @tf.function\n     def call(self, x):\n         x = self.embed(x)\n         return self.d2(x)\n      \n model=RNN_Model()\n \n v=[tf.ones(w.shape) for w in model.trainable_variables]\n with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:\n     loss = tf.reduce_sum(tf.constant([1,0])-model(tf.constant([[2,2,2], [1,1,1]]), training=True))\n acc.jvp(loss)\n </denchmark-code>\n \n See a complete example in <denchmark-link:https://drive.google.com/file/d/10Cb1nxcovmBSNE5zJOvRhyu-lHYhux15/view?usp=sharing>this notebook</denchmark-link>\n \n Other info / logs Include any logs or source code that would be helpful to\n diagnose the problem. If including tracebacks, please include the full\n traceback. Large logs and files should be attached.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "woodyx218", "commentT": "2020-09-22T11:17:46Z", "comment_text": "\n \t\tHave you tried to build the model  (model.build) after model creation model=RNN_Model()?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "woodyx218", "commentT": "2020-09-22T15:00:29Z", "comment_text": "\n \t\tYes I did. Still got the same error. It has nothing to do with building but with embedding layer. Please see my newly added example notebook.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "woodyx218", "commentT": "2020-09-22T15:20:32Z", "comment_text": "\n \t\tI don't think it is only the embedding layer. Can you try to remove  @tf.function from the call in the embedding case?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "woodyx218", "commentT": "2020-09-22T18:38:33Z", "comment_text": "\n \t\tIf I remove @tf.function from the call, there is some other error. Please take a look at the notebook hyper-link. It is only the embedding layer. Keeping @tf.function works fine for other layers.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "woodyx218", "commentT": "2020-09-22T18:54:01Z", "comment_text": "\n \t\tIf I remove @tf.function from the call is working:\n <denchmark-code>import tensorflow as tf\n from tensorflow.keras.layers import Dense, Embedding,Bidirectional,LSTM\n \n class RNN_Model(tf.keras.Model):\n     def __init__(self, dataset):\n         super(RNN_Model, self).__init__()\n         self.embed=Embedding(maxfeature,64)\n         self.blstm = Bidirectional(LSTM(64))\n         self.d1 = Dense(64, activation='relu')\n         self.d2 = Dense(2)\n                 \n     # Doing this to initialize model.trainable_variables\n         for text, labels in dataset:\n             self(text)\n             break\n \n     def call(self, x):\n         x = self.embed(x)\n         x = self.blstm(x)\n         x = self.d1(x)\n         x = self.d2(x)\n         return tf.nn.log_softmax(x)\n         \n ## Training Settings\n batch_size = 64\n maxfeature=100\n SEQ_LEN=32\n \n ## Load Problems\n (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=maxfeature)\n x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=SEQ_LEN)\n train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(batch_size)\n \n model=RNN_Model(train_ds)\n \n loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n \n # Compute JVP\n card = train_ds.cardinality()\n step=0\n for images, labels in train_ds:\n     step+=1\n     print(\"\\r%d/%d\" % (step , card), end=\"\")\n     v=[tf.ones(w.shape) for w in model.trainable_variables]\n     with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:\n         loss = loss_obj(labels, model(images, training=True))\n     acc.jvp(loss)\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "woodyx218", "commentT": "2020-09-22T18:57:58Z", "comment_text": "\n \t\t/cc <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>\n  if could be interested in this tracing case.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "woodyx218", "commentT": "2020-09-22T19:01:58Z", "comment_text": "\n \t\tI am afraid on my end, it shows the following error with your code:\n \n \n StagingError                              Traceback (most recent call last)\n  in \n 44     v=[tf.ones(w.shape) for w in model.trainable_variables]\n 45     with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:\n ---> 46         loss = loss_obj(labels, model(images, training=True))\n 47     acc.jvp(loss)\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in call(self, *args, **kwargs)\n 988\n 989         with ops.enable_auto_cast_variables(self._compute_dtype_object):\n --> 990           outputs = call_fn(inputs, *args, **kwargs)\n 991\n 992         if self._activity_regularizer:\n  in call(self, x)\n 17     def call(self, x):\n 18         x = self.embed(x)\n ---> 19         x = self.blstm(x)\n 20         x = self.d1(x)\n 21         x = self.d2(x)\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py in call(self, inputs, initial_state, constants, **kwargs)\n 528\n 529     if initial_state is None and constants is None:\n --> 530       return super(Bidirectional, self).call(inputs, **kwargs)\n 531\n 532     # Applies the same workaround as in RNN.__call__\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in call(self, *args, **kwargs)\n 988\n 989         with ops.enable_auto_cast_variables(self._compute_dtype_object):\n --> 990           outputs = call_fn(inputs, *args, **kwargs)\n 991\n 992         if self._activity_regularizer:\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py in call(self, inputs, training, mask, initial_state, constants)\n 641         forward_state, backward_state = None, None\n 642\n --> 643       y = self.forward_layer(forward_inputs,\n 644                              initial_state=forward_state, **kwargs)\n 645       y_rev = self.backward_layer(backward_inputs,\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py in call(self, inputs, initial_state, constants, **kwargs)\n 660\n 661     if initial_state is None and constants is None:\n --> 662       return super(RNN, self).call(inputs, **kwargs)\n 663\n 664     # If any of initial_state or constants are specified and are Keras\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in call(self, *args, **kwargs)\n 988\n 989         with ops.enable_auto_cast_variables(self._compute_dtype_object):\n --> 990           outputs = call_fn(inputs, *args, **kwargs)\n 991\n 992         if self._activity_regularizer:\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py in call(self, inputs, mask, training, initial_state)\n 1269           # GPU implementation when GPU is available.\n 1270           if can_use_gpu:\n -> 1271             last_output, outputs, new_h, new_c, runtime = gpu_lstm(\n 1272                 **gpu_lstm_kwargs)\n 1273           else:\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py in gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths)\n 1514       # Reverse axis 0 since the input is already convert to time major.\n 1515       inputs = array_ops.reverse(inputs, axis=[0])\n -> 1516     outputs, h, c, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n 1517         inputs, input_h=init_h, input_c=init_c, params=params, is_training=True,\n 1518         rnn_mode='lstm')\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\n 97       pass\n 98     try:\n ---> 99       return cudnn_rnn_eager_fallback(\n 100           input, input_h, input_c, params, rnn_mode=rnn_mode,\n 101           input_mode=input_mode, direction=direction, dropout=dropout,\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_cudnn_rnn_ops.py in cudnn_rnn_eager_fallback(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\n 180                              attrs=_attrs, ctx=ctx, name=name)\n 181   if _execute.must_record_gradient():\n --> 182     _execute.record_gradient(\n 183         \"CudnnRNN\", _inputs_flat, _attrs, _result)\n 184   _result = _CudnnRNNOutput._make(_result)\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py in _record_gradient(op_name, inputs, attrs, results)\n 173\n 174 def _record_gradient(op_name, inputs, attrs, results):\n --> 175   return pywrap_tfe.TFE_Py_RecordGradient(op_name, inputs, attrs, results,\n 176                                           ops.get_name_scope())\n 177\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\forwardprop.py in _jvp_dispatch(op_name, attr_tuple, inputs, outputs, tangents, use_batch)\n 211   # means we may trace a few more exact shapes before moving on to relaxation.\n 212   if _TRACE_COUNT.get(op_name, 0) < _TRACE_COUNT_LIMIT:\n --> 213     return _jvp_exact_shapes(op_name, attr_tuple, inputs, outputs, tangents,\n 214                              use_batch)\n 215   return _jvp_relaxed_shapes(op_name, attr_tuple, inputs, outputs, tangents,\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, *args, **kwargs)\n 2937     with self._lock:\n 2938       graph_function, flat_args, flat_kwargs = \n -> 2939           self._maybe_define_function(args, kwargs)\n 2940     return graph_function._filtered_call(flat_args, flat_kwargs)  # pylint: disable=protected-access\n 2941\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\n 3342\n 3343       self._function_cache.missed.add(call_context_key)\n -> 3344       graph_function = self._create_graph_function(args, kwargs)\n 3345       self._function_cache.primary[cache_key] = graph_function\n 3346\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n 3187     arg_names = base_arg_names + missing_arg_names\n 3188     graph_function = ConcreteFunction(\n -> 3189         func_graph_module.func_graph_from_py_func(\n 3190             self._name,\n 3191             self._python_function,\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n 985         _, original_func = tf_decorator.unwrap(python_func)\n 986\n --> 987       func_outputs = python_func(*func_args, **func_kwargs)\n 988\n 989       # invariant: func_outputs contains only Tensors, CompositeTensors,\n ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\n 972           except Exception as e:  # pylint:disable=broad-except\n 973             if hasattr(e, \"ag_error_metadata\"):\n --> 974               raise e.ag_error_metadata.to_exception(e)\n 975             else:\n 976               raise\n StagingError: in user code:\n C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\forwardprop.py:179 _jvp_helper_wrapper  *\n     return _jvp_helper(op_name, attr_tuple, inputs, outputs, tangents)\n C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\forwardprop.py:138 _jvp_helper  **\n     nontrivial_output_tangents = transpose_tape.gradient(\n C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py:1080 gradient\n     flat_grad = imperative_grad.imperative_grad(\n C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:71 imperative_grad\n     return pywrap_tfe.TFE_Py_TapeGradient(\n C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py:151 _gradient_function\n     grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\n C:\\Users\\Fight4Life\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\registry.py:98 lookup\n     raise LookupError(\n \n LookupError: gradient registry has no entry for: CudnnRNNBackprop\n \n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "woodyx218", "commentT": "2020-09-22T19:09:02Z", "comment_text": "\n \t\tI've just run your notebook on colab and it is running fine on CPU but we have the same error on the GPU runtime.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "woodyx218", "commentT": "2020-09-22T19:11:18Z", "comment_text": "\n \t\tThe GPU one I think that it is another issue like <denchmark-link:https://github.com/tensorflow/tensorflow/issues/37091#issuecomment-612205749>#37091 (comment)</denchmark-link>\n  /cc <denchmark-link:https://github.com/kaixih>@kaixih</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "woodyx218", "commentT": "2020-09-23T01:26:58Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449>No</denchmark-link>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "woodyx218", "commentT": "2020-09-23T03:08:15Z", "comment_text": "\n \t\tAlthough it works on CPU now, it is still desirable to work on GPU.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "woodyx218", "commentT": "2020-09-23T10:11:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alextp>@alextp</denchmark-link>\n  <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>\n  Are you still interested for the tracing part of the issue?\n <denchmark-code>@tf.function\n def call(self, x):\n </denchmark-code>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "woodyx218", "commentT": "2020-09-23T15:59:06Z", "comment_text": "\n \t\tLooks like the GPU issue has another bug (which is closed, but Scott would have more background there). It's just a missing RegisterGradient call for the gradient op.\n <denchmark-link:https://github.com/bhack>@bhack</denchmark-link>\n  what tracing issue? The fix has a test with/without tf.function.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "woodyx218", "commentT": "2020-09-23T16:13:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>\n  Yes I've retested the code example now and it is working also on GPU (at least on Colab).\n It was just a problem about timezones with tf-nightly builds to catch that commit.\n \t\t"}}}, "commit": {"commit_id": "79fce11a6cc8c3d9fd85e9a04b596fd4ea4d7b79", "commit_author": "Allen Lavoie", "commitT": "2020-09-22 18:26:01-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\python\\eager\\forwardprop_test.py", "file_new_name": "tensorflow\\python\\eager\\forwardprop_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "893,894,895,896,897,898", "deleted_lines": null, "method_info": {"method_name": "testIndexSlicesGrad", "method_params": "self", "method_startline": "893", "method_endline": "898"}}, "hunk_1": {"Ismethod": 1, "added_lines": "900,901,902,903,904,905,906,907,908,909", "deleted_lines": null, "method_info": {"method_name": "testIndexSlicesGradInFunction", "method_params": "self", "method_startline": "900", "method_endline": "909"}}, "hunk_2": {"Ismethod": 1, "added_lines": "902,903", "deleted_lines": null, "method_info": {"method_name": "testIndexSlicesGradInFunction.f", "method_params": "a", "method_startline": "902", "method_endline": "903"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\python\\eager\\function.py", "file_new_name": "tensorflow\\python\\eager\\function.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "912,913,914,915,916,917,918", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\python\\keras\\integration_test\\forwardprop_test.py", "file_new_name": "tensorflow\\python\\keras\\integration_test\\forwardprop_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "275,276", "deleted_lines": null, "method_info": {"method_name": "testEmbeddingLayerInFunction.call", "method_params": "self,x", "method_startline": "275", "method_endline": "276"}}, "hunk_1": {"Ismethod": 1, "added_lines": "269,270,271,272", "deleted_lines": null, "method_info": {"method_name": "testEmbeddingLayerInFunction.__init__", "method_params": "self", "method_startline": "269", "method_endline": "272"}}, "hunk_2": {"Ismethod": 1, "added_lines": "265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286", "deleted_lines": null, "method_info": {"method_name": "testEmbeddingLayerInFunction", "method_params": "self", "method_startline": "265", "method_endline": "286"}}}}}}}