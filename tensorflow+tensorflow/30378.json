{"BR": {"BR_id": "30378", "BR_author": "nguerinjr", "BRopenT": "2019-07-03T22:05:20Z", "BRcloseT": "2019-07-18T21:39:49Z", "BR_text": {"BRsummary": "Problems with keras model saving when there's a loss added with add_loss", "BRdescription": "\n System information\n System: windows 10, wsl with ubuntu 18 LTS\n Tensorflow Version: 2.0.0b1 in CPU mode (default, installed from pip)\n Python version: 3.6.8\n It also happens in real linux environments (actually, it's easy to simulate each these errors)\n Describe the current behavior\n I'm having many problems when saving/loading keras models with custom loss (added with add_loss). I'll describe each one of the scenarios below (I think they're all related)\n Code to reproduce the issue\n inp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))\n tensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)\n model = tf.keras.Model(inputs=inp, outputs=tensor)\n model.add_loss(tf.keras.losses.mean_absolute_error(tensor, tensor + 1))\n model.compile('adam')\n tf.keras.experimental.export_saved_model(model, 'model.tf')\n \n When not using a keras layer as loss, it produces a non-valid JSON:\n \n Traceback (most recent call last):\n File \"/home/nguerinjr/Documents/deep_coding_project/teste.py\", line 8, in \n tf.keras.experimental.export_saved_model(model, 'model.tf')\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 169, in export_saved_model\n _export_model_json(model, saved_model_path)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 177, in _export_model_json\n model_json = model.to_json()\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1449, in to_json\n model_config, default=serialization.get_json_type, **kwargs)\n File \"/usr/lib/python3.7/json/init.py\", line 238, in dumps\n *kw).encode(obj)\n File \"/usr/lib/python3.7/json/encoder.py\", line 199, in encode\n chunks = self.iterencode(o, _one_shot=True)\n File \"/usr/lib/python3.7/json/encoder.py\", line 257, in iterencode\n return _iterencode(o, 0)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/util/serialization.py\", line 69, in get_json_type\n raise TypeError('Not JSON Serializable:', obj)\n TypeError: ('Not JSON Serializable:', b'\\n\\x03add\\x12\\x03Add\\x1a\\x0fconv2d/Identity\\x1a\\x05add/y\\x07\\n\\x01T\\x12\\x020\\x01')\n Now, a code that uses keras layers\n Code to reproduce the issue\n inp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))\n tensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)\n model = tf.keras.Model(inputs=inp, outputs=tensor)\n lbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))\n model.add_loss(lbd([tensor, tensor + 1]))\n model.compile('adam')\n tf.keras.experimental.export_saved_model(model, 'model.tf')\n Traceback (most recent call last):\n File \"/home/nguerinjr/Documents/deep_coding_project/teste.py\", line 16, in \n tf.keras.experimental.export_saved_model(model, 'model.tf')\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 166, in export_saved_model\n input_signature)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 236, in _save_v1_format\n _export_mode(mode_keys.ModeKeys.TRAIN, has_saved_vars, **export_args)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 299, in _export_mode\n compile_clone=compile_clone)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py\", line 538, in clone_and_build_model\n clone = clone_model(model, input_tensors=input_tensors)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py\", line 326, in clone_model\n model, input_tensors=input_tensors, layer_fn=clone_function)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py\", line 202, in _clone_functional_model\n model._insert_layers(ancillary_layers, relevant_nodes=relevant_nodes)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1633, in _insert_layers\n for node in layer.inbound_nodes\n ValueError: min() arg is an empty sequence\n \n When using a keras layer, it loses their inbound_nodes (I've debugged it). It puts them apart from other layers, but loses information of the objects (it's not a question of passing or not custom_objects as param).\n \n Now, trying to use non-experimental saves/loads.\n Code to reproduce the issue\n inp = tf.keras.Input(batch_size=8, shape=(32, 32, 3))\n tensor = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3))(inp)\n model = tf.keras.Model(inputs=inp, outputs=tensor)\n lbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))\n model.add_loss(lbd([tensor, tensor + 1]))\n model.compile('adam')\n model.save('model.keras.tf', save_format='tf')\n tf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})\n \n A first annoying thing is that it's based on .ext. Even though it only saves in tf2 (put any extension), in load it verifies these extensions. It not a clear way of working in my opinion. Maybe some additional information on the files saved could make it not use the extensions.\n \n Traceback (most recent call last):\n File \"/home/nguerinjr/Documents/deep_coding_project/teste.py\", line 26, in \n tf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 141, in load_model\n return saved_model.load_from_saved_model_v2(filepath, compile)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 1225, in load_from_saved_model_v2\n model._training_config))  # pylint: disable=protected-access\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\n result = method(self, *args, **kwargs)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 337, in compile\n self._compile_weights_loss_and_weighted_metrics()\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\n result = method(self, *args, **kwargs)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1494, in _compile_weights_loss_and_weighted_metrics\n self.total_loss = self._prepare_total_loss(masks)\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1595, in _prepare_total_loss\n raise ValueError('The model cannot be compiled '\n ValueError: The model cannot be compiled because it has no loss to optimize.\n \n The second is related to the loading, is that it does not accept no loss to compile. Keras accepts it (you can see in the examples). It accepts because it accounts for the non-default losses added. This is another saving/loading bug.\n \n Both experimental and non-experimental functions seem to have some problems in saving / loading. I think if you simulate some custom scenarios with keras, you'll find a bunch of other errors. So, it's worth to take a whole look at them (specially considering that Keras is the default prototyping tool in tf2).\n In the current situation, i've not thought of a simple and easy way to save keras models with custom components (those which are not in the list of arguments of compile)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "nguerinjr", "commentT": "2019-07-04T00:01:18Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/nguerinjr>@nguerinjr</denchmark-link>\n ,\n Thanks for creating this issue. I've also struggled with this, but it's possible: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1ad6aae48d97937d7caed2627a1ff1f0889b2cc7>1ad6aae</denchmark-link>\n .\n The trick is here: \n \n \n tensorflow/tensorflow/python/keras/saving/hdf5_format_test.py\n \n \n          Line 821\n       in\n       1ad6aae\n \n \n \n \n \n \n  outputs = keras.layers.Lambda(lambda x: x[0])((y, custom_loss)) \n \n \n \n \n \n to connect the loss calculation with the rest of the network. It's not ideal, I know. There really should be an auxiliary outputs type argument in Model, or add_loss should retrace graph, but the workaround isn't terrible in this case.\n Hope this helps.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "nguerinjr", "commentT": "2019-07-04T11:47:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/nguerinjr>@nguerinjr</denchmark-link>\n  ,\n Can you please confirm if <denchmark-link:https://github.com/ppham27>@ppham27</denchmark-link>\n 's workaround is working for you.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "nguerinjr", "commentT": "2019-07-05T16:53:31Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/ppham27>@ppham27</denchmark-link>\n , <denchmark-link:https://github.com/rmothukuru>@rmothukuru</denchmark-link>\n  ,\n Yes, that's working in the scenario of the codes I've put here.\n I'll implement this workaround in my real code, where I have a bunch of custom_objects.\n As you've mentioned, <denchmark-link:https://github.com/ppham27>@ppham27</denchmark-link>\n , it's not ideal. Since there are those problems in the code, it's a good deal to make things work this way.\n Thanks!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "nguerinjr", "commentT": "2019-07-05T17:23:33Z", "comment_text": "\n \t\tGreat for what it's worth, I have a pending internal change that will trace the history when using add_loss and add_metric, so you don't need that line. Ideally, it will land at the end of next week, but I can't say for certain since I'm not actually on the TensorFlow team.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "nguerinjr", "commentT": "2019-07-18T21:39:50Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30378>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30378>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "a377701899b71b5f6bb0f157be763c283c7ff7e9", "commit_author": "Philip Pham", "commitT": "2019-07-18 14:37:33-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\python\\keras\\distribute\\distribute_strategy_test.py", "file_new_name": "tensorflow\\python\\keras\\distribute\\distribute_strategy_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850", "deleted_lines": null, "method_info": {"method_name": "test_fit_and_evaluate", "method_params": "self,distribution,model_fn,l1,l2", "method_startline": "1819", "method_endline": "1850"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772", "deleted_lines": null, "method_info": {"method_name": "_functional_with_add_loss_and_metric", "method_params": "input_shape,num_classes,l1,l2", "method_startline": "1749", "method_endline": "1772"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802", "deleted_lines": null, "method_info": {"method_name": "_sequential_with_add_loss_and_metric", "method_params": "input_shape,num_classes,l1,l2", "method_startline": "1775", "method_endline": "1802"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\keras\\engine\\base_layer.py", "file_new_name": "tensorflow\\python\\keras\\engine\\base_layer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1089", "deleted_lines": null, "method_info": {"method_name": "add_metric", "method_params": "self,value,aggregation,name", "method_startline": "1021", "method_endline": "1089"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1002", "deleted_lines": "1002,1003,1004,1005", "method_info": {"method_name": "add_loss", "method_params": "self,losses,inputs", "method_startline": "888", "method_endline": "1005"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\python\\keras\\engine\\network.py", "file_new_name": "tensorflow\\python\\keras\\engine\\network.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1661,1662,1663,1664,1665,1666,1667,1668", "deleted_lines": null, "method_info": {"method_name": "_graph_network_add_loss", "method_params": "self,symbolic_loss", "method_startline": "1661", "method_endline": "1668"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1670,1671,1672,1673,1674,1675", "deleted_lines": null, "method_info": {"method_name": "_graph_network_add_metric", "method_params": "self,value,aggregation,name", "method_startline": "1670", "method_endline": "1675"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885", "deleted_lines": null, "method_info": {"method_name": "_diff_layers", "method_params": "inputs,outputs,layers", "method_startline": "1871", "method_endline": "1885"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tensorflow\\python\\keras\\models.py", "file_new_name": "tensorflow\\python\\keras\\models.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "212,213,214,224,227,232", "deleted_lines": "140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,193,198,199,200,201", "method_info": {"method_name": "_clone_functional_model", "method_params": "model,input_tensors,layer_fn", "method_startline": "129", "method_endline": "233"}}, "hunk_1": {"Ismethod": 1, "added_lines": "58,59,60,61,62,63,64,65,66,67,68,69,70,71", "deleted_lines": null, "method_info": {"method_name": "_insert_ancillary_layers", "method_params": "model,ancillary_layers,metrics_names,new_nodes", "method_startline": "58", "method_endline": "71"}}, "hunk_2": {"Ismethod": 1, "added_lines": "236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262", "deleted_lines": "245,246,247,248,249,250,251,253,254,255,256,257,258,259,260,261,262", "method_info": {"method_name": "_remove_ancillary_layers", "method_params": "model,layer_map,layers", "method_startline": "236", "method_endline": "262"}}, "hunk_3": {"Ismethod": 1, "added_lines": "74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126", "deleted_lines": null, "method_info": {"method_name": "_make_new_nodes", "method_params": "nodes_by_depth,layer_fn,layer_map,tensor_map", "method_startline": "74", "method_endline": "126"}}, "hunk_4": {"Ismethod": 1, "added_lines": "301,302,306,307,308,309,310,311,312,313,314,315,316,317,319,320,321,322,323,325,332,333,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367", "deleted_lines": "271,277,278,279", "method_info": {"method_name": "_clone_sequential_model", "method_params": "model,input_tensors,layer_fn", "method_startline": "265", "method_endline": "367"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\keras\\saving\\hdf5_format_test.py", "file_new_name": "tensorflow\\python\\keras\\saving\\hdf5_format_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "818,820", "deleted_lines": "818,819,820,821", "method_info": {"method_name": "test_functional_model_with_custom_loss_and_metric", "method_params": "self", "method_startline": "811", "method_endline": "848"}}, "hunk_1": {"Ismethod": 1, "added_lines": "818,820", "deleted_lines": "818,819,820,821", "method_info": {"method_name": "test_functional_model_with_custom_loss_and_metric._make_model", "method_params": "", "method_startline": "815", "method_endline": "823"}}}}}}}