{"BR": {"BR_id": "33572", "BR_author": "dreamibor", "BRopenT": "2019-10-21T13:18:38Z", "BRcloseT": "2019-11-22T19:50:09Z", "BR_text": {"BRsummary": "[tflite] Support INT8 quantization for PACK with TFLITE_BUILTINS_INT8 OpsSet", "BRdescription": "\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\n TensorFlow installed from (source or binary): binary\n TensorFlow version (use command below): 1.14\n Python version: 3.6\n \n \n Similar to the UNPACK node issue in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/31902>#31902</denchmark-link>\n , the new TFLiteConverter post-training quantisation flow, as described in <denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations>https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations</denchmark-link>\n , does not support quantization of PACK/STACK operation when only integer operations are requested in the output model. When such conversion is attempted the following error is reported:\n \n RuntimeError: Quantization not yet supported for op: PACK\n \n Code to reproduce the issue\n For example, the script below:\n import tensorflow as tf\n import numpy as np\n \n def representative_dataset_gen():\n \tinput_1 = np.ones([1, 10],dtype=np.float32)\n \tinput_2 = np.ones([1, 10],dtype=np.float32)\n \tfor _ in range(10):\n \t\tyield [input_1, input_2]\n \n # tf Graph Input\n foo = tf.compat.v1.placeholder(\"float32\", [1, 10])\n bar = tf.compat.v1.placeholder(\"float32\", [1, 10])\n out_stacked = tf.stack([foo, bar], axis=0)\n \n with tf.compat.v1.Session() as sess:\n \ttf.io.write_graph(tf.compat.v1.get_default_graph(), '.','pack.pb', as_text=False)\n \n input_name = [\"Placeholder\", \"Placeholder_1\"]\n output_name = [\"stack\"]\n \n tflite_model_name = \"int8_pack.tflite\"\n converter = tf.lite.TFLiteConverter.from_frozen_graph(\"pack.pb\", input_name, output_name)\n converter.optimizations = [tf.lite.Optimize.DEFAULT]\n converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n converter.representative_dataset = representative_dataset_gen\n tflite_model = converter.convert()\n open(tflite_model_name, \"wb\").write(tflite_model)\n \n # Load TFLite model and allocate tensors.\n interpreter = tf.lite.Interpreter(tflite_model_name)\n interpreter.allocate_tensors()\n \n # Get input and output tensors.\n input_details = interpreter.get_input_details()\n output_details = interpreter.get_output_details()\n \n # Test model on random input data.\n input_shape = input_details[0]['shape']\n input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n interpreter.set_tensor(input_details[0]['index'], input_data)\n interpreter.invoke()`\n produces errors as follows:\n <denchmark-code>2019-10-21 14:02:33.682706: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n 2019-10-21 14:02:33.708278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\n 2019-10-21 14:02:33.708892: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x33f5a60 executing computations on platform Host. Devices:\n 2019-10-21 14:02:33.708924: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n 2019-10-21 14:02:33.717107: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n 2019-10-21 14:02:33.717228: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\n INFO: Initialized TensorFlow Lite runtime.\n Traceback (most recent call last):\n   File \"pack_example.py\", line 26, in <module>\n     tflite_model = converter.convert()\n   File \"/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 908, in convert\n     inference_output_type)\n   File \"/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 200, in _calibrate_quantize_model\n     inference_output_type, allow_float)\n   File \"/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\n     np.dtype(output_type.as_numpy_dtype()).num, allow_float)\n   File \"/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\n     return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\n RuntimeError: Quantization not yet supported for op: PACK\n </denchmark-code>\n \n Both kTfLiteUInt8 and kTfLiteInt8 version of the PACK operator is already implemented in TFLite (see <denchmark-link:https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/kernels/pack.cc>pack.cc</denchmark-link>\n ), so it should be straightforward to support PACK as well in the TFLite Converter.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dreamibor", "commentT": "2019-11-08T14:16:09Z", "comment_text": "\n \t\tAny updates?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dreamibor", "commentT": "2019-11-20T18:26:48Z", "comment_text": "\n \t\tHi, I have a CL in progress to fix this.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dreamibor", "commentT": "2019-11-22T19:50:10Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33572>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33572>No</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dreamibor", "commentT": "2020-07-31T09:04:38Z", "comment_text": "\n \t\tHey, I'm facing the same issue when converting  (see <denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md>model zoo</denchmark-link>\n ) to quantized tflite.\n Simple conversion to  tflite works just fine.\n With TF 2 I can't even convert the model to tflite proper.\n Using tensorflow_gpu version 1.5.3 on google Colab, Ubuntu 18.04.\n Any help would be much appreciated \ud83d\ude4f\n Conversion script looks like this:\n import tensorflow as tf\n import pathlib\n import os\n import cv2\n from PIL import Image\n import json\n import numpy as np\n \n frozen_model = './mobilenet_varroa_w_inputshape/frozen_inference_graph_00/tflite_graph.pb'\n # TF 1\n input_shapes = {'normalized_input_image_tensor':(1,896,896,3)}\n input_arrays = ['normalized_input_image_tensor']\n output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\n converter = tf.lite.TFLiteConverter.from_frozen_graph(frozen_model, input_arrays, output_arrays, input_shapes)\n \n converter.experimental_new_converter = False\n converter.optimizations = [tf.lite.Optimize.DEFAULT]\n \n def representative_dataset_gen():\n     directory_images = \"./rep_dataset/\"\n     for img in os.listdir(directory_images):\n       tmp_img = Image.open(f\"{directory_images}/{img}\")\n       tmp_img = np.array(tmp_img.getdata()).reshape((807, 807, 3)).astype(np.uint8)\n       tmp_img = cv2.resize(tmp_img, (896, 896), interpolation=cv2.INTER_AREA)\n       input_data = tmp_img.astype('float32')/255.0\n       input_data = tf.expand_dims(input_data, 0)\n       input_data = tf.convert_to_tensor(input_data, dtype=tf.float32)\n       yield [input_data]\n \n converter.representative_dataset = representative_dataset_gen\n converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n converter.allow_custom_ops = True\n converter.inference_input_type = tf.int8 \n converter.inference_output_type = tf.int8 \n tflite_quant_model = converter.convert() # crashes on this line\n open(\"mobilenet_test.tflite\", \"wb\").write(tflite_quant_model)\n \t\t"}}}, "commit": {"commit_id": "d8668d9e03b65c4a6d9ecb08e74b0b67798fbbab", "commit_author": "Suharsh Sivakumar", "commitT": "2019-11-22 11:49:22-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\lite\\tools\\optimize\\operator_property.cc", "file_new_name": "tensorflow\\lite\\tools\\optimize\\operator_property.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "289,290,291,292,293,294", "deleted_lines": null, "method_info": {"method_name": "tflite::optimize::operator_property::GetOperatorProperty", "method_params": "model,subgraph_index,op_index", "method_startline": "41", "method_endline": "397"}}}}}}}