{"BR": {"BR_id": "17246", "BR_author": "yaroslavvb", "BRopenT": "2018-02-24T23:39:54Z", "BRcloseT": "2019-03-07T00:10:47Z", "BR_text": {"BRsummary": "Fetching value of Variable unnecessarily slow", "BRdescription": "\n Doing sess.run(var) is about 5x slower than sess.run(var+1).\n python <denchmark-link:https://github.com/diux-dev/cluster/blob/26f8e01bd79e49fe2d1dac342dd90493f693b85c/yuxin_numpy/variable_fetch_bug_report.py>variable_fetch_bug_report.py</denchmark-link>\n \n 100MB variable\n fetch_cpu_variable  : 2.5 GB/sec, min: 40.74, median: 41.33, mean: 42.08\n fetch_cpu_variable_add: 12.6 GB/sec, min: 7.96, median: 8.54, mean: 8.71\n fetch_cpu_variable_concat: 14.0 GB/sec, min: 7.12, median: 8.14, mean: 8.28\n TensorFlow version info:\n version: 1.7.0-dev20180221\n : v1.6.0-rc1-337-gd100729\n <denchmark-link:https://github.com/tensorflow/tensorflow/commit/d100729>d100729</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "yaroslavvb", "commentT": "2018-02-26T19:54:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>\n  Any clue what's going on here?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "yaroslavvb", "commentT": "2018-02-26T20:31:16Z", "comment_text": "\n \t\tPresumably it's copying when it doesn't (?) need to. <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>\n  added the optimizations for the fetch path, so might know what could be going on here.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "yaroslavvb", "commentT": "2018-02-26T20:36:27Z", "comment_text": "\n \t\tDoes the problem also happen with resource variables? (tfe.Variable or tf.get_variable(..., use_resource=True)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "yaroslavvb", "commentT": "2018-02-26T22:10:03Z", "comment_text": "\n \t\tYes, same speed is with resource variables. I also get fast fetch speed if I turn off all optimizers and fetch var+0\n <denchmark-link:https://github.com/diux-dev/cluster/blob/37d069c20fae6aeac10a53e3f801d29aebc5d6b4/yuxin_numpy/tf_numpy_benchmark.py>tf_numpy_benchmark.py</denchmark-link>\n \n <denchmark-code>python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_add --size-mb=1024 --num-iters=31\n fetch_cpu_variable_add        :  21.0 GB/sec, min: 48.83, median: 56.08, mean: 55.69\n \n python tf_numpy_benchmark.py --benchmark=fetch_cpu_resource_variable --num-iters=11 --size-mb=1024\n fetch_cpu_variable            :   2.6 GB/sec, min: 401.36, median: 404.02, mean: 403.63\n </denchmark-code>\n \n I suspect that fetching variable triggers a single threaded memcpy. Meanwhile fetching \"var+0\" uses multiple cores, so it's essentially a multi-threaded memory copy\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "yaroslavvb", "commentT": "2018-02-26T22:58:33Z", "comment_text": "\n \t\tThe code which fetches tensors is <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/core/ndarray_tensor.cc#L331>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/core/ndarray_tensor.cc#L331</denchmark-link>\n  which calls <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L227>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L227</denchmark-link>\n  which triggers a copy if the refcount is not 1 (and it's never 1 for variables). It does the copy with normal memcpy.\n I remember people caring passionately about us not simply forwarding the memory when tensorflow still holds a reference to it as it can break some py_func use cases and some multithreaded use cases.\n So I guess we should use a faster memcpy?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "yaroslavvb", "commentT": "2018-02-27T00:58:35Z", "comment_text": "\n \t\tYes, faster memcpy would also resolve <denchmark-link:https://github.com/tensorflow/tensorflow/issues/17233>#17233</denchmark-link>\n  , until there are better tools to create 64-byte aligned numpy arrays.\n (PS: I wonder if this alignment requirement is moot in a first place. In PyTorch, I can inititialize tensors from unaligned numpy arrays, with memory reuse, and add them at 20 GB/second on CPU, there's no benefit in starting with aligned memory.)\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "yaroslavvb", "commentT": "2018-03-06T01:43:42Z", "comment_text": "\n \t\tHaving faster memcpy in TF also could be later worked into open-source distributed TensorFlow. Right now sending messages locally in Open-Source distributed TF is done at speed of single-threaded memcpy, so TF process can't take advantage of faster network cards (ie, AWS instances have 25 Gbps)\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "yaroslavvb", "commentT": "2018-03-20T17:48:18Z", "comment_text": "\n \t\tBTW, here's an example of multi-threaded memcpy with some performance numbers on the same system. The time for 100MB chunk goes from 40ms to 5ms on the same system\n <denchmark-link:https://github.com/diux-dev/cluster/blob/master/psbench/memcpy.cc>https://github.com/diux-dev/cluster/blob/master/psbench/memcpy.cc</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "yaroslavvb", "commentT": "2018-03-20T18:03:12Z", "comment_text": "\n \t\tOn dual XeonV4\n <denchmark-code>wget https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy.cc\n g++ -std=c++0x memcpy.cc -pthread -march=native -O6\n ./a.out 1000 32\n \n Stream copy 32 threads: 4.8 ms, 20.98 GB/sec\n Stream copy 32 threads: 4.9 ms, 20.53 GB/sec\n Stream copy 32 threads: 4.8 ms, 20.87 GB/sec\n Stream copy 32 threads: 4.9 ms, 20.36 GB/sec\n Stream copy 32 threads: 4.8 ms, 20.74 GB/sec\n Stream copy 32 threads: 4.7 ms, 21.39 GB/sec\n \n </denchmark-code>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "yaroslavvb", "commentT": "2018-03-20T18:14:53Z", "comment_text": "\n \t\tA single threaded call to the system memcpy should be able to hit the memory bandwidth of the system except in two cases:\n \n \n there are many small copies (not the case here)\n \n \n A single large copy will normally hit the memcpy path that avoids polluting the cache (the copies will be done using streaming memory instructions), this is known to decrease absolute performance but avoids cache pollution on multiprocessor machines.  This is hardcoded behavior of glibc.  The latest versions of glibc should expose this parameter as a tunable value (I put them there, see https://www.gnu.org/software/libc/manual/html_node/Hardware-Capability-Tunables.html#Hardware-Capability-Tunables).   Can you try making the non-temporal threshold larger than your copy size?\n \n \n I suspect what's happening is that when you shard the copy 32 times it drops below the hard-coded non temporal threshold and you're hitting the faster path (but blowing out the caches).\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "yaroslavvb", "commentT": "2018-03-20T19:54:30Z", "comment_text": "\n \t\tThat benchmark was done using a loop with calls to _mm256_stream_load_si256 and _mm256_stream_si256 rather than memcpy.\n I tried using <denchmark-link:https://github.com/diux-dev/cluster/blob/master/psbench/memcpy_classic.cc>regular memcpy</denchmark-link>\n  and the performance is very close, so I think stock memcpy is already using stream version.\n However, single threaded performance is quite far from multi-threaded performance. Here's an experiment on AWS c5.18xlarge instance (dual 18-core Skylake), it runs 6x faster than single-threaded memcpy, copying 100MB array in 2.7ms. Skylakes <denchmark-link:https://www.anandtech.com/show/11544/intel-skylake-ep-vs-amd-epyc-7000-cpu-battle-of-the-decade/12>supposedly have</denchmark-link>\n  200 GB/second memory bandwidth\n <denchmark-code>wget -N https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy_fast.cc\n g++ -std=c++0x memcpy_fast.cc -pthread -march=native -O6 -o memcpy_fast\n numactl --cpunodebind 0 --membind 0 ./memcpy_fast 100 16\n Stream copy 16 threads: 2.7 ms, 37.33 GB/sec\n Stream copy 16 threads: 2.7 ms, 37.41 GB/sec\n Stream copy 16 threads: 2.7 ms, 37.19 GB/sec\n Stream copy 16 threads: 2.7 ms, 37.27 GB/sec\n </denchmark-code>\n \n This <denchmark-link:http://web.archive.org/web/20131223174037/http://software.intel.com/en-us/articles/memcpy-performance/>article</denchmark-link>\n  talks limitations of default . Since it's precompiled, it doesn't use the latest instructions (ie, Skylakes have AVX512). Apparently  replaces  with machine optimized version\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "yaroslavvb", "commentT": "2018-03-20T20:05:22Z", "comment_text": "\n \t\tThat tunable didn't seem to make any difference for me, maybe glibc is too old (version 2.23)\n <denchmark-code>export GLIBC_TUNABLES=glibc.tune.x86_non_temporal_threshold=100000000000\n wget -N https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy_classic.cc\n g++ -std=c++0x memcpy_classic.cc -pthread -march=native -O6 -o memcpy_classic\n ./memcpy_classic 100 \n memcpy: 23.1 ms, 4.34 GB/sec\n memcpy: 23.1 ms, 4.34 GB/sec\n memcpy: 23.0 ms, 4.34 GB/sec\n memcpy: 23.0 ms, 4.34 GB/sec\n </denchmark-code>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "yaroslavvb", "commentT": "2018-03-20T20:40:47Z", "comment_text": "\n \t\tYes that's before it went in; 2.25 or 2.26 I believe.\n That article is nearly a decade old and out of date.  The memcpy maintainer in glibc works for Intel.  There are assembly routines for every specialized architecture variant including AVX 512 in the latest versions (but AVX2 is preferred because of the large down clocks associated with using AVX 512).\n I think your bandwidth calculation is off by a factor of 2, it only includes bytes read.\n That being said, it does seem like I was incorrect and multi-threading does provide a benefit, especially on skylake.  However, 2.5 GB/sec from single threaded system memcpy seems way too low (even lowly skylake should get 10 GB/sec), so I don't know that we completely understand what's going on here.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "yaroslavvb", "commentT": "2018-03-20T20:55:50Z", "comment_text": "\n \t\tI see.\n To summarize, currently tensorflow takes 40ms to fetch a 100MB variable on CPU, whereas copying 100MB on same machine is possible to do in 2.7ms.\n This delay is an issue when integrating TensorFlow with other systems. For instance resnet-50 backprop is 120ms, if you use something like Ray to synchronize parameter values between machines, this extra 40ms delay is significant\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "yaroslavvb", "commentT": "2018-03-31T21:00:21Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/tatianashp>@tatianashp</denchmark-link>\n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "yaroslavvb", "commentT": "2018-06-07T21:06:12Z", "comment_text": "\n \t\tI believe that <denchmark-link:https://github.com/tensorflow/tensorflow/commit/879fc3440495d9388754cb7d1878caf034d03d61>879fc34</denchmark-link>\n  should solve this issue (or at least make it slightly better). Though solution is CPU/platform/glibc dependent, on my CPU (Intel Broadwell with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:15MB) I see ~1.7x improvement.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "yaroslavvb", "commentT": "2018-06-12T17:12:38Z", "comment_text": "\n \t\tWeird, is memmove doing multi-threaded copy?\n BTW, here's the code that does 100MB copy in 2.7 ms on Skylake. Besides multi-threading, it uses stream instructions to turn off cache, cache just slows things down on large copies\n <denchmark-code>wget -N https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy_fast.cc\n g++ -std=c++0x memcpy_fast.cc -pthread -march=native -O6 -o memcpy_fast\n numactl --cpunodebind 0 --membind 0 ./memcpy_fast 100 16\n Stream copy 16 threads: 2.7 ms, 37.33 GB/sec\n Stream copy 16 threads: 2.7 ms, 37.41 GB/sec\n </denchmark-code>\n \n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "yaroslavvb", "commentT": "2018-06-12T18:04:37Z", "comment_text": "\n \t\tNope, they both are single threaded, but memmove is using sse3 instruction to copy memory, while memcpy is using sse2 (that's what I see in perf). This \"fix\" is more like an ugly workaround for the old glibc versions.\n There is a discussion in Eigen (<denchmark-link:https://bitbucket.org/eigen/eigen/pull-requests/292/adds-a-fast-memcpy-function-to-eigen/diff>https://bitbucket.org/eigen/eigen/pull-requests/292/adds-a-fast-memcpy-function-to-eigen/diff</denchmark-link>\n ), but that change was later rolled back because it's not  guaranteed to be faster.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "yaroslavvb", "commentT": "2018-06-28T17:30:55Z", "comment_text": "\n \t\tI'm closing this issue now since I think <denchmark-link:https://github.com/ezhulenev>@ezhulenev</denchmark-link>\n  's change fixed the problem. Please reopen if that's not the case.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "yaroslavvb", "commentT": "2018-06-28T17:41:46Z", "comment_text": "\n \t\tFrom the discussion in the eigen thread (by <denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>\n  ), looks like the above fix is only a workaround for suckiness in certain version of glibc (where memmove is even faster than memcpy).\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "yaroslavvb", "commentT": "2018-07-16T11:00:01Z", "comment_text": "\n \t\tThe change improves single threaded copy, but it's still much faster to fetch a+0 instead of a on Skylake, because the former does multi-threaded\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "yaroslavvb", "commentT": "2018-07-17T17:50:45Z", "comment_text": "\n \t\tBTW, I just did a benchmark with TF on DLAMI v11 (with MKL) on Skylake 18 core, and I'm seeing 10x improvement if I fetch \"var+0\" instead of var. Could try with nightly because of <denchmark-link:https://github.com/tensorflow/tensorflow/issues/20887>#20887</denchmark-link>\n \n <denchmark-code># fetching variable is slow from TF, because its single threaded\n python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable --num-iters=200\n fetch_cpu_variable            :   2.0 GB/sec, min: 50.75, median: 50.89, mean: 50.94\n \n # however, there's a trick, adding 0 to variable makes it multithreaded 10x faster\n python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_plus0 --num-iters=200\n fetch_cpu_variable            :  33.1 GB/sec, min:  3.02, median:  3.21, mean:  3.22\n \n </denchmark-code>\n \n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "yaroslavvb", "commentT": "2018-07-17T19:03:13Z", "comment_text": "\n \t\tStill present in tf_nightly (\"b'v1.9.0-rc2-572-geadcdf91aa'\")\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "yaroslavvb", "commentT": "2018-07-18T03:36:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>\n  Mkl has it's own kernel for Add, if it's easy for you to do the same test with default Tensorflow, I'd be very interested to know if it's any significant difference between MKL and Eigen for such simple kernel\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "yaroslavvb", "commentT": "2018-07-18T05:45:14Z", "comment_text": "\n \t\tDoing  I get about 8x speed-up instead of 10x by using +0 trick (tfnightly points to this commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/eadcdf91aa>eadcdf9</denchmark-link>\n )\n <denchmark-code># regular fetching\n python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable\n fetch_cpu_variable            :   2.0 GB/sec, min: 49.50, median: 49.62, mean: 49.68\n \n # stock tensorflow\n python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_plus0\n fetch_cpu_variable_plus0      :  16.2 GB/sec, min:  6.19, median:  6.94, mean:  6.94\n \n # DLAMI version\n python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_plus0\n fetch_cpu_variable_plus0      :  25.8 GB/sec, min:  3.87, median:  4.03, mean:  4.04\n </denchmark-code>\n \n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "yaroslavvb", "commentT": "2019-03-03T17:13:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>\n  Please let us know if this this still an issue with latest TF or can we close this issue? Thanks!\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "yaroslavvb", "commentT": "2019-03-07T00:10:46Z", "comment_text": "\n \t\tClosing due to lack of recent activity. Please open a new ticket when new information becomes available. Thanks!\n \t\t"}}}, "commit": {"commit_id": "879fc3440495d9388754cb7d1878caf034d03d61", "commit_author": "Eugene Zhulenev", "commitT": "2018-06-06 11:29:18-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\lib\\core\\ndarray_tensor.cc", "file_new_name": "tensorflow\\python\\lib\\core\\ndarray_tensor.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347", "deleted_lines": null, "method_info": {"method_name": "tensorflow::FastMemcpy", "method_params": "dst,src,size", "method_startline": "316", "method_endline": "347"}}, "hunk_1": {"Ismethod": 1, "added_lines": "399,400", "deleted_lines": "365,366", "method_info": {"method_name": "tensorflow::TF_TensorToPyArray", "method_params": "tensor,out_ndarray", "method_startline": "353", "method_endline": "407"}}}}}}}