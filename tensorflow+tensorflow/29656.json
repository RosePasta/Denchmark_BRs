{"BR": {"BR_id": "29656", "BR_author": "Sangwon91", "BRopenT": "2019-06-11T17:45:05Z", "BRcloseT": "2019-06-13T21:06:11Z", "BR_text": {"BRsummary": "Bug on `gather_nd` with gradient.", "BRdescription": "\n Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n TensorFlow installed from (source or binary): pip\n TensorFlow version (use command below): tf2-gpu-beta\n Python version: 3.6.8\n Bazel version (if compiling from source):\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version:\n GPU model and memory:\n \n You can collect some of this information using our environment capture\n <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>\n \n You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: \n Describe the current behavior\n A simple test code\n v = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)\n \n with tf.GradientTape() as tape:\n     l = tf.gather_nd(v, [[1, 1]])\n     l = tf.reduce_sum(l)\n     \n grads = tape.gradient(l, v)\n print(grads)\n gives following error message\n <denchmark-code>---------------------------------------------------------------------------\n LookupError                               Traceback (most recent call last)\n <ipython-input-12-28efd3aa3042> in <module>\n       5     l = tf.reduce_sum(l)\n       6 \n ----> 7 grads = tape.gradient(l, v)\n       8 print(grads)\n \n ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\n    1000         output_gradients=output_gradients,\n    1001         sources_raw=flat_sources_raw,\n -> 1002         unconnected_gradients=unconnected_gradients)\n    1003 \n    1004     if not self._persistent:\n \n ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\n      74       output_gradients,\n      75       sources_raw,\n ---> 76       compat.as_str(unconnected_gradients.value))\n \n ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\n     131   \"\"\"\n     132   mock_op = _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\n --> 133   grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\n     134   if grad_fn is None:\n     135     return [None] * num_inputs\n \n ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/registry.py in lookup(self, name)\n      95     else:\n      96       raise LookupError(\n ---> 97           \"%s registry has no entry for: %s\" % (self._name, name))\n \n LookupError: gradient registry has no entry for: ResourceGatherNd\n </denchmark-code>\n \n Describe the expected behavior\n the grads should be [[0, 0], [0, 1]] but error occurs.\n Code to reproduce the issue\n Provide a reproducible test case that is the bare minimum necessary to generate the problem.\n Other info / logs\n Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Sangwon91", "commentT": "2019-06-11T17:50:07Z", "comment_text": "\n \t\tI just found very weird behavior.\n If I replace v to v+0 like below,\n v = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)\n \n with tf.GradientTape() as tape:\n     l = tf.gather_nd(v+0, [[1, 1]])\n     l = tf.reduce_sum(l)\n     \n grads = tape.gradient(l, v)\n print(grads)\n it gives expected result...\n <denchmark-code>tf.Tensor(\n [[0. 0.]\n  [0. 1.]], shape=(2, 2), dtype=float32)\n </denchmark-code>\n \n It seems the direct access to variables with gather_nd ruines something unexpected...\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Sangwon91", "commentT": "2019-06-12T08:35:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Sangwon91>@Sangwon91</denchmark-link>\n  I could able to reproduce the reported issue with Tensorflow 2.0.0.beta0. Thanks!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Sangwon91", "commentT": "2019-06-13T21:06:12Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29656>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29656>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "a7ef0da19be94d5f189c8a3af960f1da77e58b41", "commit_author": "Alexandre Passos", "commitT": "2019-06-13 14:03:29-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\python\\kernel_tests\\BUILD", "file_new_name": "tensorflow\\python\\kernel_tests\\BUILD", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "837", "deleted_lines": "837"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\kernel_tests\\resource_variable_ops_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\resource_variable_ops_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "280,281,282,283,284,285,286,287,288,289,290", "deleted_lines": null, "method_info": {"method_name": "testGradientGatherNdIndexedSlices", "method_params": "self", "method_startline": "280", "method_endline": "290"}}, "hunk_1": {"Ismethod": 1, "added_lines": "267,268,269,270,271,272,273,274,275,276,277", "deleted_lines": null, "method_info": {"method_name": "testGradientGatherNd", "method_params": "self", "method_startline": "267", "method_endline": "277"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\ops\\array_grad.py", "file_new_name": "tensorflow\\python\\ops\\array_grad.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "567,568,569,570,571,572,573,574,575,576", "deleted_lines": null, "method_info": {"method_name": "_ResourceGatherNdGrad", "method_params": "op,grad", "method_startline": "567", "method_endline": "576"}}}}}}}