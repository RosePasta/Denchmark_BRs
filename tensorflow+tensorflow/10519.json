{"BR": {"BR_id": "10519", "BR_author": "ddtm", "BRopenT": "2017-06-08T03:26:09Z", "BRcloseT": "2017-06-22T00:34:51Z", "BR_text": {"BRsummary": "tf.contrib.data: tf-slim training pipeline gets stuck", "BRdescription": "\n <denchmark-h:h3>System information</denchmark-h>\n \n \n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\n Yes\n \n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n Linux leto28 3.16.0-4-amd64 1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux\n VERSION_ID=\"8\"\n VERSION=\"8 (jessie)\"\n \n \n TensorFlow installed from (source or binary):\n Binary\n \n \n TensorFlow version (use command below):\n tf.VERSION = 1.2.0-rc2\n tf.GIT_VERSION = v1.2.0-rc1-24-gce1d6ec\n tf.COMPILER_VERSION = v1.2.0-rc1-24-gce1d6ec\n \n \n Bazel version (if compiling from source):\n None\n \n \n CUDA/cuDNN version:\n 8.0/5.1\n \n \n GPU model and memory:\n TITAN X (Pascal), 12189MiB\n \n \n Exact command to reproduce:\n python ./mwe.py\n \n \n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n I recently ported my dataset handling to the new dataset API from . Now it seems that the  training pipeline stalls if I request just 1 or 2 CPUs for my job (it used to work just fine with the dataset API provided by ). I does work if I grab 4 CPUs. I tried to come up with a MWE (see below). The interesting thing is that it is not getting stuck if I remove one of the s or  at line 39. I suspect this issue is related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/10369>#10369</denchmark-link>\n .\n <denchmark-h:h3>Source code / logs</denchmark-h>\n \n import os\n import tensorflow as tf\n import tensorflow.contrib.data as tcd\n import tensorflow.contrib.slim as slim\n \n from tensorflow.contrib.data.python.ops.dataset_ops import _get_file_names\n \n DATASET_DIR = '/path_to_the_dataset'\n FILE_PATTERN = 'shapes_{}_*.tfrecord'\n IMAGE_SHAPE = [48, 48, 3]\n \n \n def _parse_function(example_proto):\n     features = {\n         \"image/encoded\": tf.FixedLenFeature(\n             (), tf.string, default_value=\"\"),\n         'image/annotation/color': tf.FixedLenFeature(\n             (), tf.int64, default_value=0),\n         'image/annotation/shape': tf.FixedLenFeature(\n             (), tf.int64, default_value=0),\n     }\n     parsed_features = tf.parse_single_example(example_proto, features)\n     image_decoded = tf.image.decode_image(parsed_features[\"image/encoded\"])\n     color = parsed_features['image/annotation/color']\n     shape = parsed_features['image/annotation/shape']\n \n     return image_decoded, color, shape\n \n \n def get_batch(batch_size=32, group_size=3, split_name='train'):\n     file_pattern = os.path.join(\n         DATASET_DIR, FILE_PATTERN.format(split_name))\n \n     file_names = _get_file_names(file_pattern, randomize_input=True)\n \n     dataset = tcd.TFRecordDataset(file_names)\n     dataset = dataset.map(_parse_function)\n \n     dataset = dataset.map(lambda image, color, shape: image)\n     dataset = dataset.shuffle(buffer_size=10000)\n     dataset = dataset.repeat().batch(group_size * batch_size)\n \n     iterator = dataset.make_one_shot_iterator()\n     images = iterator.get_next()\n \n     images = tf.split(images, group_size, axis=0)\n     images = [tf.reshape(x, [batch_size] + IMAGE_SHAPE) for x in images]\n \n     return images\n \n \n if __name__ == \"__main__\":\n     with tf.Graph().as_default():\n         x_1, x_2, x_3 = get_batch(batch_size=32,\n                                   group_size=3)\n \n         val = tf.reduce_sum(tf.add_n([x_1, x_2, x_3]))\n         val = tf.Print(val, [tf.constant(0)], \"I'm alive! \")\n \n         global_step = slim.get_or_create_global_step()\n         with tf.control_dependencies([val]):\n             update_global_step_op = tf.assign_add(global_step, 1)\n \n         train_op = update_global_step_op\n \n         tf.summary.scalar('Summary 1', val)\n         tf.summary.scalar('Summary 2', train_op)\n \n         logdir = 'mwe_logdir'\n         slim.learning.train(\n             train_op=train_op,\n             logdir=logdir,\n             number_of_steps=1000000)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ddtm", "commentT": "2017-06-08T05:00:43Z", "comment_text": "\n \t\tThanks for reporting this... it definitely looks like a bug. I think I've tracked it down to the \"OneShotIterator\" op, which internally blocks on this line while a function executes to build the dataset:\n \n \n \n tensorflow/tensorflow/core/kernels/iterator_ops.cc\n \n \n          Line 248\n       in\n       91cb809\n \n \n \n \n \n \n  n.WaitForNotification(); \n \n \n \n \n \n That will block one of the inter-op thread pool threads for the (typically short) execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool: when you have only 1 CPU, the system will deadlock as soon as you hit that line (because no more thread pool threads are available to run the function that will unblock it). When you have 2 CPUs, it can work, but slim.learning.train() uses tf.train.Supervisor, which asynchronously runs a background thread... that runs the same \"OneShotIterator\" op. To ensure that the op only initializes once, the initialization runs under a lock, acquired here:\n \n \n \n tensorflow/tensorflow/core/kernels/iterator_ops.cc\n \n \n          Line 194\n       in\n       91cb809\n \n \n \n \n \n \n  mutex_lock l(mu_); \n \n \n \n \n \n The problem is probably starting to become clear: two concurrent executions of the same \"OneShotIterator\" kernel will potentially block two inter-op thread pool threads, leading to deadlock in a 2-CPU system, because there are no more threads available to run the function that will unblock them.\n Anyway, mea culpa, and thanks again for finding the bug. I'll be working on a fix, although it might not make it into the final 1.2 release. In the mean time, there are a couple of workarounds:\n \n \n Increase the number of threads to more than 2 in the inter-op thread pool. You can do this by passing session_config=tf.ConfigProto(inter_op_parallelism_threads=3) to slim.learning.train().\n \n \n Use dataset.make_initializable_iterator() instead of dataset.make_one_shot_iterator(). This comes with the additional requirement that you have to run iterator.initializer, which is not completely trivial with slim.learning.train() because you don't have access to the tf.Session. One possibility is to pass local_init_op=tf.group(tf.local_variables_initializer(), tf.tables_initializer(), iterator.initializer) to slim.learning.train().\n \n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ddtm", "commentT": "2017-06-08T05:31:29Z", "comment_text": "\n \t\tWow, thanks for the swift response! The first workaround seems to be working perfectly.\n On a side note, I'm quite happy with the tf.contrib.data. My code has become way cleaner.\n \t\t"}}}, "commit": {"commit_id": "f5fcd1fdcf896f46aed03c7e61525b48b75d1acc", "commit_author": "Derek Murray", "commitT": "2017-06-14 14:09:31-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tensorflow\\contrib\\data\\python\\kernel_tests\\iterator_ops_test.py", "file_new_name": "tensorflow\\contrib\\data\\python\\kernel_tests\\iterator_ops_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "150,151,152,153,154", "deleted_lines": null, "method_info": {"method_name": "testOneShotIteratorNonBlocking.consumer_thread", "method_params": "", "method_startline": "150", "method_endline": "154"}}, "hunk_1": {"Ismethod": 1, "added_lines": "133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167", "deleted_lines": null, "method_info": {"method_name": "testOneShotIteratorNonBlocking", "method_params": "self", "method_startline": "133", "method_endline": "167"}}, "hunk_2": {"Ismethod": 1, "added_lines": "169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196", "deleted_lines": null, "method_info": {"method_name": "testOneShotIteratorInitializerFails", "method_params": "self", "method_startline": "169", "method_endline": "196"}}, "hunk_3": {"Ismethod": 1, "added_lines": "186,187,188", "deleted_lines": null, "method_info": {"method_name": "testOneShotIteratorInitializerFails.consumer_thread", "method_params": "", "method_startline": "186", "method_endline": "188"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "tensorflow\\core\\kernels\\iterator_ops.cc", "file_new_name": "tensorflow\\core\\kernels\\iterator_ops.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,222,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276", "deleted_lines": "193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,272,273,274,275", "method_info": {"method_name": "tensorflow::OneShotIteratorOp::Compute", "method_params": "ctx", "method_startline": "193", "method_endline": "276"}}, "hunk_1": {"Ismethod": 1, "added_lines": "226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246", "deleted_lines": "226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246", "method_info": {"method_name": "tensorflow::OneShotIteratorOp::Init", "method_params": "ctx,done", "method_startline": "226", "method_endline": "246"}}, "hunk_2": {"Ismethod": 1, "added_lines": "166,167,168,169,170,171,172,173,174,180,181,182", "deleted_lines": "171,172", "method_info": {"method_name": "tensorflow::OneShotIteratorOp::OneShotIteratorOp", "method_params": "ctx", "method_startline": "166", "method_endline": "185"}}, "hunk_3": {"Ismethod": 1, "added_lines": "206,207,208,209,210,211,212,213,214,215,216,217,218,222", "deleted_lines": "206,207,208,209,210,211,212,213,214,217,218,219,220,221,222,223", "method_info": {"method_name": "tensorflow::OneShotIteratorOp::ComputeAsync", "method_params": "ctx,done", "method_startline": "206", "method_endline": "223"}}, "hunk_4": {"Ismethod": 1, "added_lines": "321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337", "deleted_lines": null, "method_info": {"method_name": "tensorflow::OneShotIteratorOp::ProduceOutput", "method_params": "ctx,done", "method_startline": "321", "method_endline": "337"}}, "hunk_5": {"Ismethod": 1, "added_lines": "248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319", "deleted_lines": "248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,272,273,274,275,279,285", "method_info": {"method_name": "tensorflow::OneShotIteratorOp::TryInit", "method_params": "ctx,iterator,cinfo", "method_startline": "248", "method_endline": "319"}}}}}}}