{"BR": {"BR_id": "6405", "BR_author": "alvaroabascar", "BRopenT": "2020-11-18T12:46:51Z", "BRcloseT": "2020-11-23T09:01:00Z", "BR_text": {"BRsummary": "nlp.vocab.prune_vectors destroying most vectors due to lack of lexemes.", "BRdescription": "\n Since version 2.3 Lexemes are not loaded when loading a language with vectors, and since vocab.prune_vectors iterates over the vocab to retrieve all lexemes before pruning, this means that loading a language with vectors, and then calling vocab.prune_vectors(n) results in a model which retains only a few lexemes (those included by default with that spaCy language) .\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n Using the following code you can reproduce this. Observe that we call vocab.prune_vectors(n) with n=10000, but the final vectors has only 324 entries.\n import spacy\n \n nlp = spacy.load('en_core_web_md')\n print(nlp.vocab.vectors.shape)\n # prints (20000, 300)\n \n nlp.vocab.prune_vectors(10000)\n print(nlp.vocab.vectors.shape)\n # prints (324, 300)\n The following works. I found the solution in the <denchmark-link:https://spacy.io/usage/v2-3>2.3 release notes</denchmark-link>\n , in the section .\n import spacy\n \n nlp = spacy.load('en_core_web_md')\n print(nlp.vocab.vectors.shape)\n # prints (20000, 300)\n \n for orth in nlp.vocab.vectors:\n     nlp.vocab[orth]\n \n nlp.vocab.prune_vectors(10000)\n print(nlp.vocab.vectors.shape)\n # prints (324, 300)\n This makes me think that maybe that for loop (or something which loads all lexemes) should happen by default when calling prune_vectors. If needed I can prepare a MR.\n <denchmark-h:h2>Environment</denchmark-h>\n \n \n spaCy version: 2.3.2\n Platform: Linux-4.15.0-123-generic-x86_64-with-Ubuntu-18.04-bionic\n Python version: 3.7.5\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "alvaroabascar", "commentT": "2020-11-18T15:07:38Z", "comment_text": "\n \t\tThanks for the report, that does look like a bug!\n We normally only prune through spacy init-model, which does add all the lexemes first, so we missed this. I'll fix this for the upcoming v2.3.3 release.\n \t\t"}}}, "commit": {"commit_id": "a8c2dad466e5c5ec62532fea8156c744efa180eb", "commit_author": "Adriane Boyd", "commitT": "2020-11-23 10:00:59+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\vocab.pyx", "file_new_name": "spacy\\vocab.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "319,320,321", "deleted_lines": null}}}}}}