{"BR": {"BR_id": "2032", "BR_author": "honnibal", "BRopenT": "2018-02-25T01:56:48Z", "BRcloseT": "2018-04-03T12:03:45Z", "BR_text": {"BRsummary": "vocab.set_vector() has accidentally quadratic runtime", "BRdescription": "\n There had been reports that vocab.set_vector() was slow, which I figured \"yeah okay, adding vectors one by one is a bit inefficient. But you do it once and save out the model\".\n But I was just converting one of the recent FastText vectors, and it was going at 30 vectors per second --- so it would've taken like 30 hours!\n It turns out vectors.add() needs to find the next empty row, and it's using a set() object to keep track of the rows currently unset. I guess I figured there would only be a few, from deletions. However, if you first initialize the table with the shape you want, all the rows will be unset --- so to make n insertions, we iterate the list of n empty rows.\n The easiest fix is likely to just use a heap here instead of a set. However, the mechanism does feel pretty dumb overall. There should be a smarter way to do this.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "honnibal", "commentT": "2018-02-25T02:04:04Z", "comment_text": "\n \t\tIn the meantime, if you're affected by this bug, here's a mitigation. This script loads vectors in text format, and saves the nlp model to disk.\n Edit: If you're using spaCy v2.0.10, it's now much more convenient to use spacy init-model for this. It now supports .tgz and .zip files, making it very easy to convert FastText vectors\n '''\n Make an loadable directory from a FastText vectors file.\n \n https://fasttext.cc/docs/en/english-vectors.html\n '''\n from pathlib import Path\n import plac\n import spacy\n import numpy\n import tqdm\n from spacy.vocab import Vectors\n \n \n def fasttext2spacy(lang, in_loc, out_dir):\n     in_loc = Path(in_loc)\n     out_dir = Path(out_dir)\n     nlp = spacy.blank(lang)\n     with in_loc.open('r', encoding='utf8') as file_:\n         header = next(file_)\n         n_words, width = header.split()\n         n_words = int(n_words)\n         width = int(width)\n         data = numpy.zeros((n_words, width), dtype='f')\n         keys = []\n         for i, line in tqdm.tqdm(enumerate(file_), total=n_words):\n             word, vector = line.split(' ', 1)\n             data[i] = numpy.asarray(vector.split(), dtype='f')\n             _ = nlp.vocab[word]\n             keys.append(word)\n         nlp.vocab.vectors = Vectors(data=data, keys=keys)\n     nlp.to_disk(out_dir)\n \n if __name__ == '__main__':\n     plac.call(fasttext2spacy)\n Note that the resulting vocab won't have values for lex.prob or lex.cluster. If you need those, you can set them from a data file or another spaCy model.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "honnibal", "commentT": "2018-03-25T20:46:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n \n _unset is used only for 4 purposes\n \n Checking if an id belongs in it\n Removing an id from it\n Finding total count of _unset\n Finding minimum id which is in _unset\n \n For the first 3, we can use a simple boolean array. Total count can be stored as a separate variable which can be updated on every add and remove.\n For point 4, when key and row are not found, we have to end up doing a linear search.\n If the number of times when key and row are not found is very less, this would be a good approach.\n If not so, then heap sounds like the only way left.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "honnibal", "commentT": "2018-03-30T10:14:54Z", "comment_text": "\n \t\tI feel that we should be looking into AVL or RedBlack Trees too. heapq will have linear runtime for removing an index from a list. AVL or RedBlack Trees would give a logarithmic time for all operations which would still run pretty well in comparison to constant time operations in set.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "honnibal", "commentT": "2018-03-30T16:39:26Z", "comment_text": "\n \t\tThanks for this.\n I'm worried that it says that bintrees library is unmaintained. I'm also not really thrilled by that pure Python container library it suggests instead, as that will surely have higher memory use than a C-level solution.\n What if we used the C++ STL priority_queue and a boolean array? The boolean array can do your 1-3 above. We would keep all (but not only!) the available rows in the queue.We'd then have something like this:\n from libcpp.queue cimport priority_queue\n \n cdef struct UnsetC:\n     priority_queue[int] queue\n     char* status # I don't think bool works here? 8 bits doesnt seem a hardship.\n     int count\n \n cdef char is_unset(UnsetC unset, int i) nogil:\n     return unset.status[i]\n \n cdef void mark_as_set(UnsetC unset, int i) nogil:\n     unset.status[i] = 1\n     unset.count -= 1\n     # Don't iterate the priority queue. But if we do set next, pop it.\n     # Note that the priority queue stores negative i, to sort ascending.\n     if unset.top() == -i:\n         unset.pop()\n \n cdef int count_unset(UnsetC unset) nogil:\n     return unset.count\n \n cdef int next_available(UnsetC unset) nogil:\n     row = -unset.top()\n     while unset.status[row]:\n         unset.pop()\n         row = -unset.top()\n     return row\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "honnibal", "commentT": "2018-03-30T18:09:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n  Won't  be linear when  is not at the top? How would  handle  unset for any position?\n An additional point,  would run linear in the worst case if the last element of status[row] is False.\n What about completely removing the array and the priority_queue and go with c++ STL set? It would solve all the problems.\n <denchmark-h:h2>sorted set in cpp</denchmark-h>\n \n set[int] _unset\n <denchmark-h:h2>remove key - O(logn)</denchmark-h>\n \n _unset.erase(key)\n <denchmark-h:h2>add key - O(logn)</denchmark-h>\n \n _unset.insert(key)\n <denchmark-h:h2>count - O(1)</denchmark-h>\n \n _unset.size()\n <denchmark-h:h2>check if element exists - O(logn)</denchmark-h>\n \n _unset.count(key)\n <denchmark-h:h2>get minimum - O(1)</denchmark-h>\n \n if not _unset.empty()\n *_unset.rbegin()\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "honnibal", "commentT": "2018-03-30T18:32:58Z", "comment_text": "\n \t\t\n @honnibal Won't mark_as_set be linear when i is not at the top? How would mark_as_set handle unset for any position?\n \n No, because we don't require all entries in the priority queue to be available. If the top happens to be unavailable we go ahead and pop it, but otherwise we leave the queue alone. Then next time we do next_available, we check the status array to make sure we're not returning a value that's unavailable. At that point we know we're dealing with the top, so we can pop it without walking the priority queue.\n Essentially we remove from the queue lazily.\n STL set might be better though. I didn't know about this -- I'm used to thinking of Python's sets.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "honnibal", "commentT": "2018-05-07T17:52:59Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "1cdbb7c97c6bdc4e94e354546ba0306bf717ae50", "commit_author": "Suraj Rajan", "commitT": "2018-03-31 13:28:25+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": ".github\\CONTRIBUTOR_AGREEMENT.md", "file_new_name": ".github\\CONTRIBUTOR_AGREEMENT.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "94,101,104,105", "deleted_lines": "94,101,104,105"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "spacy\\tests\\vectors\\test_vectors.py", "file_new_name": "spacy\\tests\\vectors\\test_vectors.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "52,53,54,55,56,57,58,59,60,61,62", "deleted_lines": null, "method_info": {"method_name": "test_get_vector_resize", "method_params": "strings,data,resize_data", "method_startline": "52", "method_endline": "62"}}, "hunk_1": {"Ismethod": 1, "added_lines": "32,33", "deleted_lines": null, "method_info": {"method_name": "resize_data", "method_params": "", "method_startline": "32", "method_endline": "33"}}, "hunk_2": {"Ismethod": 1, "added_lines": "40,41,42,43,44", "deleted_lines": null, "method_info": {"method_name": "test_init_vectors_with_resize_shape", "method_params": "strings,resize_data", "method_startline": "40", "method_endline": "44"}}, "hunk_3": {"Ismethod": 1, "added_lines": "46,47,48,49,50", "deleted_lines": null, "method_info": {"method_name": "test_init_vectors_with_resize_data", "method_params": "data,resize_data", "method_startline": "46", "method_endline": "50"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\vectors.pyx", "file_new_name": "spacy\\vectors.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19,20,55,74,76,98,129,130,169,193,258,263,264,370,371", "deleted_lines": "53,72,74,96,127,128,167,191,256,261,262,368,369"}}}}}}