{"BR": {"BR_id": "5961", "BR_author": "TatsuyaShirakawa", "BRopenT": "2020-08-24T08:06:29Z", "BRcloseT": "2020-08-25T12:16:25Z", "BR_text": {"BRsummary": "cannot analyze ` \u0304 \u0304` with japanese models", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n When I tried the following very small script\n import spacy\n nlp = spacy.load('ja_core_news_sm')\n nlp(' \u0304 \u0304')\n I got the following error\n >>> nlp(' \u0304 \u0304')\n nlp(' \u0304 \u0304')\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.7/site-packages/spacy/lang/ja/__init__.py\", line 106, in get_dtokens_and_spaces\n     word_start = text[text_pos:].index(word)\n ValueError: substring not found\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\n   File \"/usr/local/lib/python3.7/site-packages/spacy/language.py\", line 441, in __call__\n     doc = self.make_doc(text)\n   File \"/usr/local/lib/python3.7/site-packages/spacy/lang/ja/__init__.py\", line 281, in make_doc\n     return self.tokenizer(text)\n   File \"/usr/local/lib/python3.7/site-packages/spacy/lang/ja/__init__.py\", line 145, in __call__\n     dtokens, spaces = get_dtokens_and_spaces(dtokens, text)\n   File \"/usr/local/lib/python3.7/site-packages/spacy/lang/ja/__init__.py\", line 108, in get_dtokens_and_spaces\n     raise ValueError(Errors.E194.format(text=text, words=words))\n ValueError: [E194] Unable to aligned mismatched text ' \u0304 \u0304' and words '[' ', '\u0304', ' \u0304']'.\n The minimal Dockerfile is here\n <denchmark-code>FROM python:3.7\n \n RUN pip install spacy\n RUN python -m spacy download ja_core_news_sm\n </denchmark-code>\n \n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n Operating System: Linux 04a7a76544e5 4.19.76-linuxkit #1 SMP Thu Oct 17 19:31:58 UTC 2019 x86_64 GNU/Linux\n Python Version Used: 3.7.7\n spaCy Version Used: 2.3.2\n Environment Information: Minimal Dockerfile is as bellow\n \n <denchmark-code>FROM python:3.7\n \n RUN pip install spacy\n RUN python -m spacy download ja_core_news_sm\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-24T09:52:45Z", "comment_text": "\n \t\tThanks for the report! This is definitely a bug.\n <denchmark-link:https://github.com/hiroshi-matsuda-rit>@hiroshi-matsuda-rit</denchmark-link>\n : I don't know whether you'd have time to look into this? I don't speak Japanese, so I'm not sure about the tokenization issues. It looks to me, from a first inspection, that the  function includes the space within the third token, but then  <denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/lang/ja/__init__.py#L124>skips over the space</denchmark-link>\n  which then ultimately results in an error because the third token can not be found anymore in the string. I feel like probably  should be fixed somehow?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-24T11:08:40Z", "comment_text": "\n \t\tThis behavior might be coming from SudachiPy.\n I'd like to research it soon.\n <denchmark-link:https://github.com/polm>@polm</denchmark-link>\n  Have you encountered this kind of problems?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-24T11:15:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sorami>@sorami</denchmark-link>\n  Could you help us?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-24T11:18:12Z", "comment_text": "\n \t\tLooks like it's a macron character? Wouldn't be used in normal Japanese, but might be used in romaji.\n <denchmark-link:https://www.fileformat.info/info/unicode/char/0304/index.htm>https://www.fileformat.info/info/unicode/char/0304/index.htm</denchmark-link>\n \n I suspect this has to do with how SudachiPy normalizes characters, this was a vaguely similar issue.\n <denchmark-link:https://github.com/WorksApplications/SudachiPy/issues/120>WorksApplications/SudachiPy#120</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-24T12:54:04Z", "comment_text": "\n \t\tThe sudachipy analysis didn't look obviously incorrect to me, either. I suspect the problem is that the third token returned by sudachipy starts with whitespace and that throws the alignment off like Sofie described. But I'm also not sure enough about how sudachipy should work to be sure where the bug is.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-25T08:53:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/adrianeboyd>@adrianeboyd</denchmark-link>\n  The third token of the output of SudachiPy for example sentence is starting with whitespace and it's unexpected behavior for current Japanese lang model.\n In such cases, we should divide a token into a whitespace and a remaining part.\n I'd make a quick fix for master branch.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-25T10:36:58Z", "comment_text": "\n \t\tAfter some workarounds, I decided to set space after field of each token by referring the surface of next token instead next char in text.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "TatsuyaShirakawa", "commentT": "2020-08-25T11:26:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sorami>@sorami</denchmark-link>\n  It seems SudachiPy has some inconsistency on dictionary_form and reading_form fields while analyzing the contexts including specific symbol chars after white space.\n <denchmark-link:https://github.com/svlandeg>@svlandeg</denchmark-link>\n  <denchmark-link:https://github.com/adrianeboyd>@adrianeboyd</denchmark-link>\n  I think we can release a bug-fix version even if SudachiPy is not fixed.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "TatsuyaShirakawa", "commentT": "2020-11-12T21:58:58Z", "comment_text": "\n \t\tHello, I realize this topic is closed but I recently ran into a similar problem when attempting to read text containing the character  \u0301. I was wondering if this is just malformed data on my part, or if the bugfix described in this issue should take care of it? And if the latter, is the bugfix already released? I didn't notice anything in 2.3.1. Thanks for your help!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "TatsuyaShirakawa", "commentT": "2020-11-13T01:58:25Z", "comment_text": "\n \t\tMy impression is that spaCy should not throw an exception on any text you throw at it. However, that means that it will process even garbage.\n It looks like you have a COMBINING ACUTE ACCENT floating by itself, which is not really going to be useful. You might be able to fix it by using Unicode NFKC normalization on your input text.\n \t\t"}}}, "commit": {"commit_id": "332803eda9e9999434d4da41e56d1689f353bbd8", "commit_author": "Hiroshi Matsuda", "commitT": "2020-08-25 14:16:24+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\lang\\ja\\__init__.py", "file_new_name": "spacy\\lang\\ja\\__init__.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101,122", "deleted_lines": "101,122", "method_info": {"method_name": "get_dtokens_and_spaces", "method_params": "dtokens,text,gap_tag", "method_startline": "82", "method_endline": "132"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tests\\tokenizer\\test_naughty_strings.py", "file_new_name": "spacy\\tests\\tokenizer\\test_naughty_strings.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "35", "deleted_lines": null}}}}}}