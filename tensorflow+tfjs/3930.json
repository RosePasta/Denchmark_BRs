{"BR": {"BR_id": "3930", "BR_author": "vladmandic", "BRopenT": "2020-09-15T12:54:14Z", "BRcloseT": "2020-09-30T17:29:51Z", "BR_text": {"BRsummary": "tfjs-node support for saved models does not recognize valid dtypes", "BRdescription": "\n Simply calling tfnode.node.getMetaGraphsFromSavedModel(path); on a model using uint8 results in error:\n <denchmark-code>(node:2420) UnhandledPromiseRejectionWarning: Error: Unsupported tensor DataType: DT_UINT8, try to modify the model in python to convert the datatype\n     at mapTFDtypeToJSDtype (/home/vlado/dev/test-tfjs/node_modules/@tensorflow/tfjs-node/dist/saved_model.js:465:19)\n </denchmark-code>\n \n However, support for  was added to  via <denchmark-link:https://github.com/tensorflow/tfjs/pull/2981>#2981</denchmark-link>\n  back in March.\n Those newly supported data types should be added throughout  codebase.\n Environment: Ubuntu 20.04 running NodeJS 14.9.0 with TFJS 2.3.0\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "vladmandic", "commentT": "2020-09-17T11:41:42Z", "comment_text": "\n \t\tMore details:\n There is an issue with  vs  mapping which causes failure on execution of a  in tfjs.\n Model in question is EfficientDet from TFHub: <denchmark-link:https://tfhub.dev/tensorflow/efficientdet/d0>https://tfhub.dev/tensorflow/efficientdet/d0</denchmark-link>\n \n I've found your an earlier fix <denchmark-link:https://github.com/tensorflow/tfjs/pull/2981>#2981</denchmark-link>\n  which patches  by mapping  to , but:\n a) Same is also needed in tfjs-node/saved_model:mapTFDtypeToJSDtype() (and possibly in other places)  :\n <denchmark-code>  Error: Unsupported tensor DataType: DT_UINT8, try to modify the model in python to convert the datatype\n   at mapTFDtypeToJSDtype (/home/vlado/dev/efficientdet/node_modules/@tensorflow/tfjs-node/dist/saved_model.js:471:19)\n </denchmark-code>\n \n b) During model execution, model expects to receive uint8, but receives int32 and fails with:\n <denchmark-code>  Error: Session fail to run with error: Expects arg[0] to be uint8 but int32 is provided\n   at NodeJSKernelBackend.runSavedModel (/home/vlado/dev/efficientdet/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:1592:43)  \n </denchmark-code>\n \n So I'm not sure that simply mapping uint8 to int32 is a fix?\n Referencing previous issue <denchmark-link:https://github.com/tensorflow/tfjs/issues/3374>#3374</denchmark-link>\n  closed without resolution.\n Gist with a test code is at <denchmark-link:https://gist.github.com/vladmandic/a7cf75109b7b48f8914a5b18da5c498f>https://gist.github.com/vladmandic/a7cf75109b7b48f8914a5b18da5c498f</denchmark-link>\n \n Links for direct download of a  are also included.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "vladmandic", "commentT": "2020-09-30T17:29:51Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/pyu10055>@pyu10055</denchmark-link>\n ,\n I've confirmed on several  models that  now works perfectly with  data type.\n Since <denchmark-link:https://github.com/tensorflow/tfjs/pull/3974>#3974</denchmark-link>\n  is already committed to master, I'm closing this issue.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "vladmandic", "commentT": "2020-09-30T17:29:52Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3930>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tfjs/issues/3930>No</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "vladmandic", "commentT": "2020-10-02T16:13:54Z", "comment_text": "\n \t\t\n More details:\n There is an issue with int32 vs uint8 mapping which causes failure on execution of a saved_model in tfjs.\n Model in question is EfficientDet from TFHub: https://tfhub.dev/tensorflow/efficientdet/d0\n I've found your an earlier fix #2981 which patches tfjs-converter by mapping uint8 to int32, but:\n a) Same is also needed in tfjs-node/saved_model:mapTFDtypeToJSDtype() (and possibly in other places) :\n   Error: Unsupported tensor DataType: DT_UINT8, try to modify the model in python to convert the datatype\n   at mapTFDtypeToJSDtype (/home/vlado/dev/efficientdet/node_modules/@tensorflow/tfjs-node/dist/saved_model.js:471:19)\n \n b) During model execution, model expects to receive uint8, but receives int32 and fails with:\n   Error: Session fail to run with error: Expects arg[0] to be uint8 but int32 is provided\n   at NodeJSKernelBackend.runSavedModel (/home/vlado/dev/efficientdet/node_modules/@tensorflow/tfjs-node/dist/nodejs_kernel_backend.js:1592:43)  \n \n So I'm not sure that simply mapping uint8 to int32 is a fix?\n Referencing previous issue #3374 closed without resolution.\n Gist with a test code is at https://gist.github.com/vladmandic/a7cf75109b7b48f8914a5b18da5c498f\n Links for direct download of a saved_model are also included.\n \n The same issue happens with DT_INT64 as well:\n <denchmark-code>2020-10-02 17:55:36.860435: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n 2020-10-02 17:55:36.883827: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x119b7b0a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n 2020-10-02 17:55:36.883865: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n (node:39361) UnhandledPromiseRejectionWarning: Error: Unsupported tensor DataType: DT_INT64, try to modify the model in python to convert the datatype\n </denchmark-code>\n \n after converting a GraphModel to SavedModel using tensorflowjs_converter.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "vladmandic", "commentT": "2020-10-02T16:17:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/loretoparisi>@loretoparisi</denchmark-link>\n  Yup, I've reported that under <denchmark-link:https://github.com/tensorflow/tfjs/issues/4004>#4004</denchmark-link>\n  and it was just fixed in <denchmark-link:https://github.com/tensorflow/tfjs/pull/4008>#4008</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "vladmandic", "commentT": "2020-10-05T10:40:06Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/vladmandic>@vladmandic</denchmark-link>\n  in the meantime  added a support while converting using  in latest version :\n <denchmark-code>$ tfjs_graph_converter --output_format tf_saved_model --compat_mode ./ ./saved/\n TensorFlow.js Graph Model Converter\n \n Graph model:    ./\n Output:         ./saved/\n Target format:  tf_saved_model\n \n Converting.... Done.\n Conversion took 1.667s\n </denchmark-code>\n \n The GraphModel now converted to SavedModel seems loading fine now:\n const tfjsnode = require('@tensorflow/tfjs-node');\n var loadSavedModel = function (path) {\n   return new Promise(function (resolve, reject) {\n     tfjsnode.node.loadSavedModel(this.path)\n       .then(res => {\n         console.log(\"loadSavedModel OK\");\n         resolve(res);\n       })\n       .catch(err => reject(err));\n   });\n }\n loadSavedModel('/Users/loretoparisi/webservice/toxicity_model/saved')\n   .catch(err => console.error(\"loadSavedModel\", err));\n This works fine:\n 2020-10-05 12:38:29.166043: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /Users/loretoparisi/webservice/toxicity_model/saved\n 2020-10-05 12:38:29.193234: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n 2020-10-05 12:38:29.252666: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n 2020-10-05 12:38:29.252729: I tensorflow/cc/saved_model/loader.cc:212] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /Users/loretoparisi/webservice/toxicity_model/saved/variables/variables.index\n 2020-10-05 12:38:29.252752: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 86709 microseconds.\n BUT, if I apply this to the TFJS model wrapper here:\n     ToxicityClassifier.prototype.loadModel = function () {\n         return __awaiter(this, void 0, void 0, function () {\n             return __generator(this, function (_a) {\n                 return [2, tfjsnode.node.loadSavedModel(path)];\n             });\n         });\n     };\n it will fail due to another error\n (node:43549) UnhandledPromiseRejectionWarning: Error: SavedModel outputs information is not available yet.\n     at TFSavedModel.get [as outputs] (/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/node_modules/@tensorflow/tfjs-node/dist/saved_model.js:265:19)\n     at ToxicityClassifier.<anonymous> (/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/toxicity-example/lib/toxicity/dist/index.js:101:35)\n ...\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "vladmandic", "commentT": "2020-10-07T14:07:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/loretoparisi>@loretoparisi</denchmark-link>\n  Do you still have the same problem? It works fine for me with int64 and int32 inputs with TFJS v2.5.0 and with int32 inputs in v2.3.0 and v2.4.0.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "vladmandic", "commentT": "2020-10-07T18:27:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/patlevin>@patlevin</denchmark-link>\n  thank you, let me check, they  have just released \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "vladmandic", "commentT": "2020-10-07T18:50:55Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/patlevin>@patlevin</denchmark-link>\n  <denchmark-link:https://github.com/vladmandic>@vladmandic</denchmark-link>\n  So the model correctly loads with tfjs , but, in this example at least, there is a specific error that is\n <denchmark-code>2020-10-07 20:49:15.662525: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: ./toxicity_model/saved\n 2020-10-07 20:49:15.702371: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n 2020-10-07 20:49:15.765097: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n 2020-10-07 20:49:15.765158: I tensorflow/cc/saved_model/loader.cc:212] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: ./toxicity_model/saved/variables/variables.index\n 2020-10-07 20:49:15.765180: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 102655 microseconds.\n (node:65757) UnhandledPromiseRejectionWarning: Error: SavedModel outputs information is not available yet.\n </denchmark-code>\n \n I have opened a specific issue here <denchmark-link:https://github.com/tensorflow/tfjs/issues/4035>#4035</denchmark-link>\n \n Thank you!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "vladmandic", "commentT": "2020-10-07T19:14:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/loretoparisi>@loretoparisi</denchmark-link>\n  Interesting. I used the following code and it worked just fine:\n const tf = require('@tensorflow/tfjs-node')\n \n async function run() {\n   const model = await tf.node.loadSavedModel('./models/toxicity_saved/')\n   // both indexArray and valueArray are obtained from two preprocessed test phrases that I used to verify\n   // model outputs\n   const indexArray = [\n     [0, 1], [0,2 ], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8],\n     [1, 0], [1, 1], [1, 2], [1, 3]\n   ]\n   const valueArray = [215, 13, 53, 4461, 2951, 519, 1129, 7, 78, 16, 123, 20, 6]\n   const indices = tf.tensor2d(indexArray, [indexArray.length, 2], 'int32')\n   const values = tf.tensor1d(valueArray, 'int32')\n   const modelInputs = {\n     Placeholder_1: indices,\n     Placeholder: values\n   }\n   const labels = model.predict(modelInputs)\n   indices.dispose()\n   values.dispose()\n   outputs = []\n   for (name in labels) {\n    const prediction = labels[name].dataSync()\n    const results = []\n    for (let input = 0; input < 2; ++input) {\n      const probs = prediction.slice(input * 2, input * 2 + 2)\n      let match = null\n      if (Math.max(probs[0], probs[1]) > 0.9) {\n        match = probs[0] > probs[1]\n      }\n      p= probs.toString() // just to print out the numbers\n      results.push({p, match})\n    }\n    outputs.push({label: name.split('/')[0], results})\n   }\n   for (x of outputs) {\n     console.log(x)\n   }\n }\n \n run()\n The model methods outputs() and inputs() aren't implemented yet for the SavmedModel-class, but in case you need them for some reason, outputs and inputs can be obtained using the getMetaGraphsFromSavedModel() and getSignatureDefEntryFromMetaGraphInfo() functions.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "vladmandic", "commentT": "2020-10-07T19:25:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/patlevin>@patlevin</denchmark-link>\n  thanks, I confirm that this way it works!\n <denchmark-code>ip-192-168-178-22:toxicity-example loretoparisi$ node test.js \n node-pre-gyp info This Node instance does not support builds for N-API version 6\n node-pre-gyp info This Node instance does not support builds for N-API version 6\n 2020-10-07 21:16:16.466776: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n 2020-10-07 21:16:16.494477: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x11a0dfb60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n 2020-10-07 21:16:16.494524: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n 2020-10-07 21:16:16.624539: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: ./toxicity_model/saved\n 2020-10-07 21:16:16.647951: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n 2020-10-07 21:16:16.704225: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n 2020-10-07 21:16:16.704289: I tensorflow/cc/saved_model/loader.cc:212] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: ./toxicity_model/saved/variables/variables.index\n 2020-10-07 21:16:16.704314: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 79774 microseconds.\n {\n   label: 'identity_attack',\n   results: [\n     { p: '0.9964505434036255,0.0035493909381330013', match: true },\n     { p: '0.9999773502349854,0.00002267475429107435', match: true }\n   ]\n }\n {\n   label: 'insult',\n   results: [\n     { p: '0.013952560722827911,0.9860473871231079', match: false },\n     { p: '0.9996521472930908,0.00034789409255608916', match: true }\n   ]\n }\n {\n   label: 'obscene',\n   results: [\n     { p: '0.997055172920227,0.002944822423160076', match: true },\n     { p: '0.9999693632125854,0.00003068893784075044', match: true }\n   ]\n }\n {\n   label: 'severe_toxicity',\n   results: [\n     { p: '0.9999983310699463,0.0000016291029396597878', match: true },\n     { p: '1,7.3735777483818765e-9', match: true }\n   ]\n }\n {\n   label: 'sexual_explicit',\n   results: [\n     { p: '0.9994053840637207,0.0005946253077127039', match: true },\n     { p: '0.9999841451644897,0.00001583264020155184', match: true }\n   ]\n }\n {\n   label: 'threat',\n   results: [\n     { p: '0.9994139671325684,0.000586066220421344', match: true },\n     { p: '0.9999756813049316,0.000024260229110950604', match: true }\n   ]\n }\n {\n   label: 'toxicity',\n   results: [\n     { p: '0.011850903742015362,0.988149106502533', match: false },\n     { p: '0.999394416809082,0.0006055471021682024', match: true }\n   ]\n }\n </denchmark-code>\n \n while, according to what you say, using the other way the error\n <denchmark-code>Error: SavedModel outputs information is not available yet.\n     at TFSavedModel.get [as outputs] (/Users/loretoparisi/Documents/Projects/AI/tensorflow-node-examples/node_modules/@tensorflow/tfjs-node/dist/saved_model.js:251:19)\n </denchmark-code>\n \n comes from within the saved_model class, in fact there we have a not implemented error!\n <denchmark-code>Object.defineProperty(TFSavedModel.prototype, \"outputs\", {\n         /**\n          * Return the array of output tensor info.\n          *\n          * @doc {heading: 'Models', subheading: 'SavedModel'}\n          */\n         get: function () {\n             throw new Error('SavedModel outputs information is not available yet.');\n         },\n         enumerable: true,\n         configurable: true\n     });\n </denchmark-code>\n \n hence the offending line in the toxicity example is the following I assume\n this.labels =\n                             model.outputs.map(function (d) { return d.name.split('/')[0]; });\n                         if (this.toxicityLabels.length === 0) {\n                             this.toxicityLabels = this.labels;\n                         }\n that must be changed in someway using the getMetaGraphsFromSavedModel method then.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "vladmandic", "commentT": "2020-10-07T19:30:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/loretoparisi>@loretoparisi</denchmark-link>\n  I'll create a pull-request that implements  and  on .\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "vladmandic", "commentT": "2020-10-07T19:39:37Z", "comment_text": "\n \t\t\n @loretoparisi I'll create a pull-request that implements outputs() and inputs() on SavedModel.\n \n super! In the meantime I have found the outputs something as you suggested\n const modelInfo = await tf.node.getMetaGraphsFromSavedModel('./toxicity_model/saved');\n console.dir(modelInfo[0].signatureDefs.serving_default.outputs, { depth: null, maxArrayLength: null });\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "vladmandic", "commentT": "2020-10-07T21:02:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/loretoparisi>@loretoparisi</denchmark-link>\n  btw, one advantage of working with  and  is that it shows actual signature names instead just an incrementing array (when model has multiple inputs and/or outputs) that you get from a .\n See <denchmark-link:https://github.com/tensorflow/tfjs/issues/3942>#3942</denchmark-link>\n  for details.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "vladmandic", "commentT": "2020-10-08T10:05:04Z", "comment_text": "\n \t\t\n @loretoparisi Interesting. I used the following code and it worked just fine:\n const tf = require('@tensorflow/tfjs-node')\n \n async function run() {\n   const model = await tf.node.loadSavedModel('./models/toxicity_saved/')\n   // both indexArray and valueArray are obtained from two preprocessed test phrases that I used to verify\n   // model outputs\n   const indexArray = [\n     [0, 1], [0,2 ], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8],\n     [1, 0], [1, 1], [1, 2], [1, 3]\n   ]\n   const valueArray = [215, 13, 53, 4461, 2951, 519, 1129, 7, 78, 16, 123, 20, 6]\n   const indices = tf.tensor2d(indexArray, [indexArray.length, 2], 'int32')\n   const values = tf.tensor1d(valueArray, 'int32')\n   const modelInputs = {\n     Placeholder_1: indices,\n     Placeholder: values\n   }\n   const labels = model.predict(modelInputs)\n   indices.dispose()\n   values.dispose()\n   outputs = []\n   for (name in labels) {\n    const prediction = labels[name].dataSync()\n    const results = []\n    for (let input = 0; input < 2; ++input) {\n      const probs = prediction.slice(input * 2, input * 2 + 2)\n      let match = null\n      if (Math.max(probs[0], probs[1]) > 0.9) {\n        match = probs[0] > probs[1]\n      }\n      p= probs.toString() // just to print out the numbers\n      results.push({p, match})\n    }\n    outputs.push({label: name.split('/')[0], results})\n   }\n   for (x of outputs) {\n     console.log(x)\n   }\n }\n \n run()\n The model methods outputs() and inputs() aren't implemented yet for the SavmedModel-class, but in case you need them for some reason, outputs and inputs can be obtained using the getMetaGraphsFromSavedModel() and getSignatureDefEntryFromMetaGraphInfo() functions.\n \n <denchmark-link:https://github.com/patlevin>@patlevin</denchmark-link>\n  thanks for the test script.\n I modified it a little bit to user the Universal Sentence Encoder:\n const use = require(\"@tensorflow-models/universal-sentence-encoder\");\n const model = await tf.node.loadSavedModel('./toxicity_model/saved');\n const tokenizer = await use.load();\n const sentences = ['you suck', 'hello how are you?'];\n var codes = await tokenizer.embed(sentences);\n console.log(codes);\n Now I have an array of Tensors like\n <denchmark-code>Tensor {\n   kept: false,\n   isDisposedInternal: false,\n   shape: [ 1, 512 ],\n   dtype: 'float32',\n   size: 512,\n   strides: [ 512 ],\n   dataId: {},\n   id: 837,\n   rankType: '2',\n   scopeId: 1289\n }\n </denchmark-code>\n \n and now I have to turn into indexes and values. I have tried this\n var encodings = await tokenizer.embed(sentences);\n var indicesArr = encodings.map(function (arr, i) { return arr.map(function (d, index) { return [i, index]; }); });\n var flattenedIndicesArr = [];\n for (i = 0; i < indicesArr.length; i++) {\n     flattenedIndicesArr =\n         flattenedIndicesArr.concat(indicesArr[i]);\n }\n var indices = tf.tensor2d(flattenedIndicesArr, [flattenedIndicesArr.length, 2], 'int32');\n var values = tf.tensor1d(tf.util.flatten(encodings), 'int32');\n But it seems that embed function of the Tokenizers is different from the encode function internally used that I cannot call external (it gives me an undefined if I try tokenizer.encode), so in this case I get an error:\n <denchmark-code>TypeError: encodings.map is not a function\n </denchmark-code>\n \n Thank you.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "vladmandic", "commentT": "2020-10-08T21:05:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/loretoparisi>@loretoparisi</denchmark-link>\n  The model expects an encoded word vector as input, while the Universal Sentence Encoder (USE) model returns embeddings.\n Basically, you'll want to use the loadTokenizer() function from the previous USE version, but that one requires TFJS 1.x...\n I have a working version locally, but it'd be better to fix the examples instead - see Issue model repo\n Unfortunately <denchmark-link:https://github.com/pyu10055>@pyu10055</denchmark-link>\n 's commit b02310745ceac6b8e4a475719c343da53e3cade2 on the USE-repo broke both the Toxicity  model and your use-case entirely...\n The real problem is that the examples are outdated and some changes broke TFJS 2.x compatibility (in the case of USE I fail to see the reasoning behind the change - might have been a mistake?).\n Meanwhile, I'll create a gist for you that contains all you need to get this working as a single-file solution. I'll get back to you in a bit.\n EDIT: I got confused here, since a similar issue was raised w.r.t. outdated tfjs-examples. The same applies to tfjs-models, though - basically some models are incompatible with TFJS 2.x due to package changes (not for technical reasons).\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "vladmandic", "commentT": "2020-10-09T08:47:56Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/patlevin>@patlevin</denchmark-link>\n  thank you! In fact I have just realized that the after recent updates models examples and core code diverged!\n \t\t"}}}, "commit": {"commit_id": "5e2d3f6151cc27659bfbe3e472d9f758e9853221", "commit_author": "Ping Yu", "commitT": "2020-09-27 07:42:43-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tfjs-core\\src\\base.ts", "file_new_name": "tfjs-core\\src\\base.ts", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "44", "deleted_lines": "44"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tfjs-core\\src\\model_types.ts", "file_new_name": "tfjs-core\\src\\model_types.ts", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "44,45,143,144,145,146,147,148,149,150,155", "deleted_lines": "145,146,147,148"}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "tfjs-node-gpu\\test_objects\\saved_model\\uint8_multiply\\saved_model.pb", "file_new_name": "tfjs-node-gpu\\test_objects\\saved_model\\uint8_multiply\\saved_model.pb"}, "file_3": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "tfjs-node-gpu\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.data-00000-of-00001", "file_new_name": "tfjs-node-gpu\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.data-00000-of-00001"}, "file_4": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "tfjs-node-gpu\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.index", "file_new_name": "tfjs-node-gpu\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.index"}, "file_5": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tfjs-node\\python\\unint8_model.py"}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tfjs-node\\src\\nodejs_kernel_backend.ts", "file_new_name": "tfjs-node\\src\\nodejs_kernel_backend.ts", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1922,1925,1926", "deleted_lines": "19,1907,1910"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 9, "file_old_name": "tfjs-node\\src\\saved_model.ts", "file_new_name": "tfjs-node\\src\\saved_model.ts", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "251,252,253,254", "deleted_lines": null, "method_info": {"method_name": "(anonymous)", "method_params": "", "method_startline": "251", "method_endline": "254"}}, "hunk_1": {"Ismethod": 1, "added_lines": "175,176,177,184", "deleted_lines": "179,180,181,182,183,184,185,186,187,188", "method_info": {"method_name": "getSignatureDefEntryFromMetaGraphInfo", "method_params": "string", "method_startline": "175", "method_endline": "188"}}, "hunk_2": {"Ismethod": 1, "added_lines": "391,392,410", "deleted_lines": "390,391,392,410", "method_info": {"method_name": "loadSavedModel", "method_params": "string,tags,signature", "method_startline": "383", "method_endline": "413"}}, "hunk_3": {"Ismethod": 1, "added_lines": "245,246,247,248,249,250,251,252,253,254,255,256", "deleted_lines": null, "method_info": {"method_name": "outputNodeNames", "method_params": "", "method_startline": "245", "method_endline": "256"}}, "hunk_4": {"Ismethod": 1, "added_lines": "175,176,177,184,198", "deleted_lines": "171,172,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195", "method_info": {"method_name": "getInputAndOutputNodeNameFromMetaGraphInfo", "method_params": "string", "method_startline": "171", "method_endline": "199"}}, "hunk_5": {"Ismethod": 1, "added_lines": null, "deleted_lines": "212,213", "method_info": {"method_name": "constructor", "method_params": "number,number,NodeJSKernelBackend", "method_startline": "210", "method_endline": "214"}}, "hunk_6": {"Ismethod": 1, "added_lines": "434", "deleted_lines": null, "method_info": {"method_name": "mapTFDtypeToJSDtype", "method_params": "string", "method_startline": "429", "method_endline": "447"}}, "hunk_7": {"Ismethod": 1, "added_lines": "201", "deleted_lines": null, "method_info": {"method_name": "constructor", "method_params": "number,number,SignatureDefEntry,NodeJSKernelBackend", "method_startline": "199", "method_endline": "202"}}, "hunk_8": {"Ismethod": 1, "added_lines": "92,93,128,129,130,131,148,149,150,151", "deleted_lines": "91,92,127,128,129,146,147,163,164", "method_info": {"method_name": "getMetaGraphsFromSavedModel", "method_params": "string", "method_startline": "85", "method_endline": "164"}}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tfjs-node\\src\\saved_model_test.ts", "file_new_name": "tfjs-node\\src\\saved_model_test.ts", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "190,192,193,194", "deleted_lines": "190,192,193,194", "method_info": {"method_name": "(anonymous)", "method_params": "", "method_startline": "187", "method_endline": "196"}}}}, "file_9": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "tfjs-node\\test_objects\\saved_model\\uint8_multiply\\saved_model.pb", "file_new_name": "tfjs-node\\test_objects\\saved_model\\uint8_multiply\\saved_model.pb"}, "file_10": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "tfjs-node\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.data-00000-of-00001", "file_new_name": "tfjs-node\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.data-00000-of-00001"}, "file_11": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "tfjs-node\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.index", "file_new_name": "tfjs-node\\test_objects\\saved_model\\uint8_multiply\\variables\\variables.index"}}}}