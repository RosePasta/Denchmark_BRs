{"BR": {"BR_id": "2411", "BR_author": "s-rog", "BRopenT": "2020-06-29T08:16:57Z", "BRcloseT": "2020-06-30T18:51:40Z", "BR_text": {"BRsummary": "0.8.2 calls backward on '_GeneratorContextManager'", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n 0.8.2 calls backward on '_GeneratorContextManager' and crashes training.\n 0.8.1 works correctly. my training_step returns {'loss':loss, 'log':{'learn_rate':self.lr}}\n <denchmark-code>Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n     fn(i, *args)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 538, in ddp_train\n     self.run_pretrain_routine(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1100, in run_pretrain_routine\n     self.train()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 370, in train\n     self.run_training_epoch()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch\n     batch_output = self.run_training_batch(batch, batch_idx)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 630, in run_training_batch\n     self.hiddens\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 804, in optimizer_closure\n     model_ref.backward(self, closure_loss, optimizer, opt_idx)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/hooks.py\", line 189, in backward\n     loss.backward()\n AttributeError: '_GeneratorContextManager' object has no attribute 'backward'\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n backward is called on the loss and training runs correctly\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "s-rog", "commentT": "2020-06-29T11:19:12Z", "comment_text": "\n \t\tdid you override optimizer step?\n could you try master? we just pushed a fix to a typo we had\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "s-rog", "commentT": "2020-06-29T12:47:22Z", "comment_text": "\n \t\tCan confirm this happens on 0.8.3\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "s-rog", "commentT": "2020-06-29T12:48:36Z", "comment_text": "\n \t\tok. Can you post a colab example that replicates this?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "s-rog", "commentT": "2020-06-30T00:40:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>\n  <denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n \n colab please\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "s-rog", "commentT": "2020-06-30T02:41:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  my optimizer step was untouched, I can't run more testing atm but I'll get to it as soon as I can\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "s-rog", "commentT": "2020-06-30T03:35:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  Hi I also encountered this, with normal Adam optimizer. I don't have a colab to replicate this atm but from what I saw earlier, this can be replicated with any setting as long as the Trainer is set to precision=16 when using Apex. Under this condition, the following lines from training_loop.py and hooks.py will run:\n if self.precision == 16 and not self.on_tpu closure_loss = model_ref.amp_scale_loss(closure_loss, optimizer, opt_idx) \n scaled_loss = amp.scale_loss(unscaled_loss, optimizer)\n will cause the closure_loss be a _GeneratorContextManager object. Which then cannot have a backward() method.\n It seems under the current design, pytorch lighting's scale_loss function can only be used as a context?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "s-rog", "commentT": "2020-06-30T07:03:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  Here's a colab example (my first time using colab so let me know if you have issues seeing it) <denchmark-link:https://colab.research.google.com/drive/1G08jVDpx-T-5HE2c89RLJdq4u67mM2-o?usp=sharing>https://colab.research.google.com/drive/1G08jVDpx-T-5HE2c89RLJdq4u67mM2-o?usp=sharing</denchmark-link>\n \n I suspect the issue lies with Apex AMP as suggested above by <denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "s-rog", "commentT": "2020-06-30T13:58:53Z", "comment_text": "\n \t\tummm. I think this is an apex issue. I can't replicate it with 16-bit native.\n <denchmark-link:https://user-images.githubusercontent.com/3640001/86135032-4c97ff80-bab8-11ea-942e-ffaae17aff07.png></denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "s-rog", "commentT": "2020-06-30T14:03:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>\n  min share a minimal example to reproduce?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "s-rog", "commentT": "2020-06-30T16:21:22Z", "comment_text": "\n \t\thi sorry for the delay: <denchmark-link:https://colab.research.google.com/drive/1rjaRRwgBTm4CKPfe9po_WSxnKqY4jDRv?usp=sharing>https://colab.research.google.com/drive/1rjaRRwgBTm4CKPfe9po_WSxnKqY4jDRv?usp=sharing</denchmark-link>\n \n I agree this is an apex issue, i.e. only occur when NATIVE_AMP_AVALAIBLE is false in the hooks.py\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "s-rog", "commentT": "2020-06-30T18:52:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>\n  , <denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>\n  ,<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n  this is fixed on master. Give it a try?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "s-rog", "commentT": "2020-06-30T20:25:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n   yes, the master version works for me now. Thanks!\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "s-rog", "commentT": "2020-07-01T00:35:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  can confirm as well! and sorry couldn't be more helpful earlier\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "s-rog", "commentT": "2020-07-01T03:47:06Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  thanks for the quick fix. I just upgraded but am now seeing a different error:\n <denchmark-code>GPU available: True, used: True\n TPU available: False, using: 0 TPU cores\n CUDA_VISIBLE_DEVICES: [0,1]\n Using APEX 16bit precision.\n initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n Loaded pretrained weights for efficientnet-b0\n /home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/train.py:140: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n   train_single_fold(args)\n Using APEX 16bit precision.\n initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n ----------------------------------------------------------------------------------------------------\n distributed_backend=ddp\n All DDP processes registered. Starting ddp with 2 processes\n ----------------------------------------------------------------------------------------------------\n Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n \n Defaults for this optimization level are:\n enabled                : True\n opt_level              : O1\n cast_model_type        : None\n patch_torch_functions  : True\n keep_batchnorm_fp32    : None\n master_weights         : None\n loss_scale             : dynamic\n Processing user overrides (additional kwargs that are not None)...\n After processing overrides, optimization options are:\n enabled                : True\n opt_level              : O1\n cast_model_type        : None\n patch_torch_functions  : True\n keep_batchnorm_fp32    : None\n master_weights         : None\n loss_scale             : dynamic\n \n   | Name      | Type             | Params\n -----------------------------------------------\n 0 | critereon | CrossEntropyLoss | 0     \n 1 | net       | EfficientNet     | 4 M   \n Validation sanity check:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                       | 1/2 [00:00<00:00,  1.01it/s]Traceback (most recent call last):\n   File \"/home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/train.py\", line 140, in <module>\n     train_single_fold(args)\n   File \"/home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/train.py\", line 64, in train_single_fold\n     trainer.fit(model)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 957, in fit\n     self.ddp_train(task, model)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 538, in ddp_train\n     self.run_pretrain_routine(model)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1141, in run_pretrain_routine\n     eval_results = self._evaluate(model,\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 346, in _evaluate\n     self.reduce_eval_ddp(eval_results)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 363, in reduce_eval_ddp\n     self.reduce_eval_ddp(v)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 365, in reduce_eval_ddp\n     dist.all_reduce(v, op=dist.reduce_op.SUM)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 898, in all_reduce\n     work = _default_pg.allreduce([tensor], opts)\n RuntimeError: Tensors must be CUDA and dense\n Traceback (most recent call last):\n   File \"train.py\", line 140, in <module>\n     train_single_fold(args)\n   File \"train.py\", line 64, in train_single_fold\n     trainer.fit(model)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 973, in fit\n     self.spawn_ddp_children(model)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 449, in spawn_ddp_children\n     self.ddp_train(local_rank, model, is_master=True)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 538, in ddp_train\n     self.run_pretrain_routine(model)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1141, in run_pretrain_routine\n     eval_results = self._evaluate(model,\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 346, in _evaluate\n     self.reduce_eval_ddp(eval_results)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 363, in reduce_eval_ddp\n     self.reduce_eval_ddp(v)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 365, in reduce_eval_ddp\n     dist.all_reduce(v, op=dist.reduce_op.SUM)\n   File \"/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 898, in all_reduce\n     work = _default_pg.allreduce([tensor], opts)\n RuntimeError: Tensors must be CUDA and dense\n </denchmark-code>\n \n I'm not manually assigning tensors to a device (i.e. PL should be assigning all tensors as CUDA tensors) and I am not using sparse tensors (at least not that I am aware of).\n EDIT: I found the issue. I guess metrics need to be CUDA tensors now. Thanks again :)\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "s-rog", "commentT": "2020-07-01T06:15:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>\n  mind send a new issue?\n \t\t"}}}, "commit": {"commit_id": "e8bb4165b76496089d24c74891f2167350e594be", "commit_author": "William Falcon", "commitT": "2020-06-30 14:51:39-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "805,806,807,808,809,813,814,815,816,817,818,819,820", "deleted_lines": null, "method_info": {"method_name": "optimizer_closure", "method_params": "self,split_batch,batch_idx,opt_idx,optimizer,hiddens", "method_startline": "759", "method_endline": "841"}}}}}}}