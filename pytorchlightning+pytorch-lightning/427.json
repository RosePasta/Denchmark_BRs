{"BR": {"BR_id": "427", "BR_author": "ssaru", "BRopenT": "2019-10-24T17:39:22Z", "BRcloseT": "2020-05-17T13:24:18Z", "BR_text": {"BRsummary": "save_weights_only parameter in ModelCheckpoint class look like doesn't work", "BRdescription": "\n <denchmark-h:h3>Common bugs:</denchmark-h>\n \n \n Tensorboard not showing in Jupyter-notebook see issue 79.\n PyTorch 1.1.0 vs 1.2.0 support see FAQ\n \n Describe the bug\n save_weights_only parameter in ModelCheckpoint class look like doesn't work\n document describe save_weight_only like that\n save_weights_only: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)).\n but save_weight_only parameter doesn't save model differently each different options\n To Reproduce\n Steps to reproduce the behavior:\n \n I used sample script in official document\n \n <denchmark-code>import os\n import torch\n from torch.nn import functional as F\n from torch.utils.data import DataLoader\n from torchvision.datasets import MNIST\n import torchvision.transforms as transforms\n \n import pytorch_lightning as pl\n \n class CoolSystem(pl.LightningModule):\n \n     def __init__(self):\n         super(CoolSystem, self).__init__()\n         # not the best model...\n         self.l1 = torch.nn.Linear(28 * 28, 10)\n \n     def forward(self, x):\n         return torch.relu(self.l1(x.view(x.size(0), -1)))\n \n     def training_step(self, batch, batch_nb):\n         # REQUIRED\n         x, y = batch\n         y_hat = self.forward(x)\n         loss = F.cross_entropy(y_hat, y)\n         tensorboard_logs = {'train_loss': loss}\n         return {'loss': loss, 'log': tensorboard_logs}\n \n     def validation_step(self, batch, batch_nb):\n         # OPTIONAL\n         x, y = batch\n         y_hat = self.forward(x)\n         return {'val_loss': F.cross_entropy(y_hat, y)}\n \n     def validation_end(self, outputs):\n         # OPTIONAL\n         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n         tensorboard_logs = {'val_loss': avg_loss}\n         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n \n     def configure_optimizers(self):\n         # REQUIRED\n         # can return multiple optimizers and learning_rate schedulers\n         # (LBFGS it is automatically supported, no need for closure function)\n         return torch.optim.Adam(self.parameters(), lr=0.02)\n \n     @pl.data_loader\n     def train_dataloader(self):\n         # REQUIRED\n         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n \n     @pl.data_loader\n     def val_dataloader(self):\n         # OPTIONAL\n         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n \n     @pl.data_loader\n     def test_dataloader(self):\n         # OPTIONAL\n         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n \n if __name__ == \"__main__\":\n     from pytorch_lightning import Trainer\n     from pytorch_lightning.callbacks import ModelCheckpoint\n \n     weight_path = os.path.join(os.getcwd(), 'checkpoint')\n     if not os.path.exists(weight_path):\n         os.mkdir(weight_path)\n \n     checkpoint_callback = ModelCheckpoint(\n         filepath=weight_path,\n         save_best_only=False,\n         verbose=True,\n         monitor='val_loss',\n         mode='min',\n         prefix='',\n         save_weights_only=False\n     )\n \n     gpus = torch.cuda.device_count()\n     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n     model = CoolSystem()\n     model.to(device)\n     trainer = Trainer(checkpoint_callback=checkpoint_callback,\n                       max_nb_epochs=1, train_percent_check=0.1)\n     trainer.fit(model)\n </denchmark-code>\n \n \n i just changed save_weights_only parameter & weight directory for saving. because if i don't model try restore weight\n \n for example i save model in \"checkpoint\" directory.\n after trained i move ckpt file other directory like called test\n so, test directory have two model file save_weight_only_True, save_weight_only_False\n \n \n i was checking what they have some different using torch.load\n (PATH). but not different...\n \n \n two file has same parameter like this\n save_weights_only_False\n \n \n <denchmark-code>{'epoch': 0, 'global_step': 187, 'checkpoint_callback_best': inf, 'optimizer_states': [{'state': {4830682784: {'step': 187, 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]])}, 4830682928: {'step': 187, 'exp_avg': tensor([ 2.8793e-12, -2.0038e-03,  1.6457e-03,  9.9219e-12, -1.4364e-02,\n          7.6137e-03,  9.4487e-12,  1.0773e-11,  9.9046e-03, -3.7999e-03]), 'exp_avg_sq': tensor([7.2368e-08, 7.3899e-05, 1.2118e-04, 8.5934e-07, 1.5810e-04, 9.1419e-05,\n         7.7932e-07, 1.0131e-06, 1.5449e-04, 2.0831e-04])}}, 'param_groups': [{'lr': 0.02, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [4830682784, 4830682928]}]}], 'lr_schedulers': [], 'state_dict': OrderedDict([('l1.weight', tensor([[ 0.0198, -0.0171, -0.0024,  ..., -0.0154,  0.0243, -0.0241],\n         [ 0.0349, -0.0040, -0.0051,  ...,  0.0159,  0.0295,  0.0129],\n         [ 0.0302, -0.0218, -0.0171,  ...,  0.0104,  0.0179, -0.0167],\n         ...,\n         [-0.0284,  0.0056,  0.0178,  ...,  0.0211, -0.0075,  0.0163],\n         [ 0.0145,  0.0204,  0.0121,  ..., -0.0013, -0.0103,  0.0043],\n         [ 0.0059, -0.0060,  0.0017,  ..., -0.0214,  0.0273, -0.0288]])), ('l1.bias', tensor([-0.1228,  0.3341, -0.0163, -0.1302,  0.1738,  0.3422, -0.1155, -0.1122,\n         -0.5156, -0.1902]))])}\n </denchmark-code>\n \n save_weights_only_True\n <denchmark-code>{'epoch': 0, 'global_step': 187, 'checkpoint_callback_best': inf, 'optimizer_states': [{'state': {4877819552: {'step': 187, 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]])}, 4877819696: {'step': 187, 'exp_avg': tensor([ 1.4936e-11, -1.6439e-03, -6.1600e-04, -4.5436e-03, -8.8385e-03,\n          4.9613e-12,  1.6526e-11,  4.4405e-03,  1.8937e-12, -6.5171e-04]), 'exp_avg_sq': tensor([1.9473e-06, 8.8430e-05, 1.4157e-04, 1.1038e-04, 1.3500e-04, 2.1486e-07,\n         2.3841e-06, 1.1693e-04, 3.1302e-08, 2.3050e-04])}}, 'param_groups': [{'lr': 0.02, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [4877819552, 4877819696]}]}], 'lr_schedulers': [], 'state_dict': OrderedDict([('l1.weight', tensor([[-0.0299,  0.0281, -0.0246,  ...,  0.0143,  0.0314,  0.0167],\n         [ 0.0066, -0.0160,  0.0200,  ..., -0.0266,  0.0097,  0.0138],\n         [ 0.0257,  0.0134, -0.0111,  ..., -0.0201,  0.0199, -0.0146],\n         ...,\n         [ 0.0260, -0.0082, -0.0049,  ...,  0.0277, -0.0070,  0.0275],\n         [ 0.0089, -0.0003, -0.0051,  ..., -0.0086,  0.0285, -0.0252],\n         [-0.0202,  0.0252,  0.0083,  ..., -0.0144, -0.0181,  0.0105]])), ('l1.bias', tensor([-0.1031,  0.2837,  0.0276, -0.1406,  0.2007, -0.1086, -0.1267,  0.2557,\n         -0.1239, -0.0374]))])}\n </denchmark-code>\n \n Expected behavior\n if save_weights_only: if True\n expected value like this\n <denchmark-code>{'state_dict': OrderedDict([('l1.weight', tensor([[-0.0299,  0.0281, -0.0246,  ...,  0.0143,  0.0314,  0.0167],\n         [ 0.0066, -0.0160,  0.0200,  ..., -0.0266,  0.0097,  0.0138],\n         [ 0.0257,  0.0134, -0.0111,  ..., -0.0201,  0.0199, -0.0146],\n         ...,\n         [ 0.0260, -0.0082, -0.0049,  ...,  0.0277, -0.0070,  0.0275],\n         [ 0.0089, -0.0003, -0.0051,  ..., -0.0086,  0.0285, -0.0252],\n         [-0.0202,  0.0252,  0.0083,  ..., -0.0144, -0.0181,  0.0105]])), ('l1.bias', tensor([-0.1031,  0.2837,  0.0276, -0.1406,  0.2007, -0.1086, -0.1267,  0.2557,\n         -0.1239, -0.0374]))])}\n </denchmark-code>\n \n Screenshots\n If applicable, add screenshots to help explain your problem.\n Desktop (please complete the following information):\n \n OS: MacBook Pro (15-inch, 2018); Mojave\n Browser: chrome\n Version:  pytorch-lightning==0.5.2.1, torch==1.3.0.post2, torchvision==0.4.1.post2, test-tube==0.7.3\n \n Additional context\n I try to find some reason, why save_weights_only parameter doesn't work\n i found <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer_io.py>TrainerIOMixin class</denchmark-link>\n  inside PyTorch lightning. and I feel save_weights_only parameter not was implemented in PyTorch lightning\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ssaru", "commentT": "2020-02-22T02:06:33Z", "comment_text": "\n \t\tThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ssaru", "commentT": "2020-03-01T10:10:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ssaru>@ssaru</denchmark-link>\n  could you check it with the latest version on master?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ssaru", "commentT": "2020-03-03T15:13:22Z", "comment_text": "\n \t\tAppreciate your reply.\n I will check it\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ssaru", "commentT": "2020-03-26T13:58:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ssaru>@ssaru</denchmark-link>\n  I assume that it was fixed, if not pls feel free to reopen... \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ssaru", "commentT": "2020-04-29T19:41:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  It's not fixed yet.\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/42d5cfc3b056b4c82a77a7cdcb8eafc63a812b67/pytorch_lightning/trainer/training_io.py#L291>dump_checkpoint</denchmark-link>\n  still returns everything regardless of the  parameter.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ssaru", "commentT": "2020-05-11T07:14:25Z", "comment_text": "\n \t\tI just ran into this bug myself. I can work on it, but probably not for another week or so. I took a quick look through the code and it doesn't seem too difficult to fix, but as I'm not familiar with the entire code base there might be some distant issue I haven't seen, but I think dump_checkpoint would need an argument and then it could save nothing but the state_dict; plus there'd have to be corresponding checks for the extra checkpoint keys added to restore and restore_training_state.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ssaru", "commentT": "2020-05-11T08:14:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rightaditya>@rightaditya</denchmark-link>\n  mind draft a PR and we can help to finish it fast... :]\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ssaru", "commentT": "2020-05-11T14:04:53Z", "comment_text": "\n \t\tI also just run into this and went ahead and created a draft PR. Saving only the weights is working. However, I haven't changed any logic regarding loading.\n \t\t"}}}, "commit": {"commit_id": "8c4c7b105e16fbe255e4715f54af2fa5d2a12fad", "commit_author": "Fabio Natanael Kepler", "commitT": "2020-05-17 09:24:17-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "74,75", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_new_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "142", "deleted_lines": "142", "method_info": {"method_name": "_save_model", "method_params": "self,filepath", "method_startline": "136", "method_endline": "144"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "pytorch_lightning\\trainer\\training_io.py", "file_new_name": "pytorch_lightning\\trainer\\training_io.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "309,315,316,317,318,319,320,321,323,324,325,326,328,330,331,332,333,335,337,338,339", "deleted_lines": "309,315,316,318,319,320,322,323,324,325,327,329,330,331,332,334,341,342,343,344", "method_info": {"method_name": "dump_checkpoint", "method_params": "self", "method_startline": "309", "method_endline": "366"}}, "hunk_1": {"Ismethod": 1, "added_lines": "395,396,397,398,399,400", "deleted_lines": null, "method_info": {"method_name": "restore_training_state", "method_params": "self,checkpoint", "method_startline": "388", "method_endline": "438"}}, "hunk_2": {"Ismethod": 1, "added_lines": "259,260", "deleted_lines": "259,260", "method_info": {"method_name": "save_checkpoint", "method_params": "self,filepath,bool", "method_startline": "259", "method_endline": "272"}}, "hunk_3": {"Ismethod": 1, "added_lines": "309,315,316,317,318,319,320,321,323,324,325,326,328,330,331,332,333,335,337,338,339", "deleted_lines": "309,315,316,318,319,320,322,323,324,325,327,329,330,331,332,334,341,342,343,344", "method_info": {"method_name": "dump_checkpoint", "method_params": "self,bool", "method_startline": "309", "method_endline": "367"}}, "hunk_4": {"Ismethod": 1, "added_lines": "259,260", "deleted_lines": "259,260", "method_info": {"method_name": "save_checkpoint", "method_params": "self,filepath", "method_startline": "259", "method_endline": "272"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\trainer\\test_trainer.py", "file_new_name": "tests\\trainer\\test_trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "273", "deleted_lines": "273", "method_info": {"method_name": "test_model_checkpoint_options.mock_save_function", "method_params": "filepath", "method_startline": "273", "method_endline": "274"}}, "hunk_1": {"Ismethod": 1, "added_lines": "273", "deleted_lines": "273", "method_info": {"method_name": "test_model_checkpoint_options.mock_save_function", "method_params": "filepath,args", "method_startline": "273", "method_endline": "274"}}, "hunk_2": {"Ismethod": 1, "added_lines": "299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334", "deleted_lines": null, "method_info": {"method_name": "test_model_checkpoint_only_weights", "method_params": "tmpdir", "method_startline": "299", "method_endline": "334"}}, "hunk_3": {"Ismethod": 1, "added_lines": "273", "deleted_lines": "273", "method_info": {"method_name": "test_model_checkpoint_options", "method_params": "tmpdir,save_top_k,file_prefix,expected_files", "method_startline": "270", "method_endline": "296"}}}}}}}