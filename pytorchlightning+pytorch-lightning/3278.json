{"BR": {"BR_id": "3278", "BR_author": "s-rog", "BRopenT": "2020-08-31T02:42:47Z", "BRcloseT": "2020-09-23T08:38:34Z", "BR_text": {"BRsummary": "on_fit_start not triggering (master)", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n on_fit_start is not being triggered on master as of <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/f46318ebfeb785a659c49091a6871584ccde3ee1>f46318e</denchmark-link>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Install master\n Run template with on_fit_start added (included below)\n (identical behavior for single gpu, ddp_spawn and ddp)\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>\"\"\"\n Runs a model on a single node across multiple gpus.\n \"\"\"\n import os\n from argparse import ArgumentParser\n \n from pl_examples.models.lightning_template import LightningTemplateModel\n from pytorch_lightning import Trainer, seed_everything\n \n seed_everything(234)\n \n class custom_template(LightningTemplateModel):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n     \n     def on_epoch_start(self):\n         print(\"on_epoch_start\")\n \n     def on_fit_start(self):\n         print(\"on_fit_start\")\n \n def main(args):\n     \"\"\" Main training routine specific for this project. \"\"\"\n     # ------------------------\n     # 1 INIT LIGHTNING MODEL\n     # ------------------------\n     model = custom_template(**vars(args))\n \n     # ------------------------\n     # 2 INIT TRAINER\n     # ------------------------\n     trainer = Trainer.from_argparse_args(args)\n \n     # ------------------------\n     # 3 START TRAINING\n     # ------------------------\n     trainer.fit(model)\n \n \n def run_cli():\n     # ------------------------\n     # TRAINING ARGUMENTS\n     # ------------------------\n     # these are project-wide arguments\n     root_dir = os.path.dirname(os.path.realpath(__file__))\n     parent_parser = ArgumentParser(add_help=False)\n \n     # each LightningModule defines arguments relevant to it\n     parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\n     parser = Trainer.add_argparse_args(parser)\n     parser.set_defaults(gpus=2, distributed_backend='ddp')\n     args = parser.parse_args()\n \n     # ---------------------\n     # RUN TRAINING\n     # ---------------------\n     main(args)\n \n \n if __name__ == '__main__':\n     run_cli()\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n \"on_fit_start\" should be printed along with \"on_epoch_start\" but only \"on_epoch_start\" is printed\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "s-rog", "commentT": "2020-08-31T20:26:06Z", "comment_text": "\n \t\tIt's a bug I think since at this point neither model nor accelerator_backend is assigned to the trainer.\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/trainer.py\n \n \n          Line 984\n       in\n       caf7893\n \n \n \n \n \n \n  self.call_hook('on_fit_start', model) \n \n \n \n \n \n Should be called after\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/trainer.py\n \n \n          Line 1008\n       in\n       caf7893\n \n \n \n \n \n \n  self.accelerator_backend.setup(model) \n \n \n \n \n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "s-rog", "commentT": "2020-09-02T15:46:11Z", "comment_text": "\n \t\tI'll move the hook and test it out, if it works I'll do a PR\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "s-rog", "commentT": "2020-09-03T03:56:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>\n  moving the hook can work but causes another peculiar issue\n <denchmark-code>  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n     result = fn(self, *args, **kwargs)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1029, in fit\n     self.call_hook('on_fit_start', model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1384, in call_hook\n     output = hook_fx(*args, **kwargs)\n TypeError: on_fit_start() takes 1 positional argument but 2 were given\n </denchmark-code>\n \n more specifically on_fit_start() gets called with 2 modules, still locating the source of this extra arg/module\n Edit:\n So call_hook has the following signature def call_hook(self, hook_name, *args, **kwargs) and within call_hook there is trainer_hook(*args, **kwargs) and hook_fx(*args, **kwargs) (in this case: on_fit_start) both are called with the same args but trainer_hook needs a model arg while hook_fx/on_fit_start can't take any args. Thoughts on how to move forward?\n Edit:\n bug still present as of <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/c64520e658806a87f282c74299b2ea4b7ea493ea>c64520e</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "s-rog", "commentT": "2020-09-03T19:51:02Z", "comment_text": "\n \t\tlet's just wait for refactors and keep this issue open. After exploring a bit more I saw a few more bugs \ud83d\ude05\n Although for now, the only place where the model is required is in TrainerCallbackHookMixin.on_fit_start for the callbacks but there also we can avoid the model parameter and just use self.get_model() just like in on_fit_end.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "s-rog", "commentT": "2020-09-21T06:20:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>\n  I implemented the following changes and now on_fit_start is working as expected:\n \n move hook\n change on_fit_start signature (like on_fit_end)\n change callback.on_fit_start args (like on_fit_end)\n \n \n After exploring a bit more I saw a few more bugs \ud83d\ude05\n \n What else did you find? I'll take a look before I submit a PR\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "s-rog", "commentT": "2020-09-22T18:52:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n  want to send the PR out?\n \t\t"}}}, "commit": {"commit_id": "49767e424f2a2ba2893d27ed0c757815e355c86e", "commit_author": "s-rog", "commitT": "2020-09-23 10:38:33+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\callback_hook.py", "file_new_name": "pytorch_lightning\\trainer\\callback_hook.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "49,52", "deleted_lines": "49,52", "method_info": {"method_name": "on_fit_start", "method_params": "self", "method_startline": "49", "method_endline": "52"}}, "hunk_1": {"Ismethod": 1, "added_lines": "49,52", "deleted_lines": "49,52", "method_info": {"method_name": "on_fit_start", "method_params": "self,model", "method_startline": "49", "method_endline": "52"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "298,299,300,301", "deleted_lines": "287,288,289"}}}}}}