{"BR": {"BR_id": "1306", "BR_author": "rzepinskip", "BRopenT": "2020-03-30T17:08:20Z", "BRcloseT": "2020-04-07T00:29:56Z", "BR_text": {"BRsummary": "RuntimeError: Unimplemented backend XLA on TPU", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n  raised for  line in  file when running MNIST on TPU. I think it was introduced in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/31b71483c47fa4aa688912b432726cdac0025a9b>31b7148</denchmark-link>\n .\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Go to MNIST on TPUs\n Run all\n Scroll down to trainer\n See error\n \n <denchmark-code>Traceback (most recent call last):\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n     fn(gindex, *args)\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n     fn(gindex, *args)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n     fn(gindex, *args)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n Exception in device=TPU:5: Unimplemented backend XLA\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n RuntimeError: Unimplemented backend XLA\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n RuntimeError: Unimplemented backend XLA\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n RuntimeError: Unimplemented backend XLA\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n RuntimeError: Unimplemented backend XLA\n Traceback (most recent call last):\n RuntimeError: Unimplemented backend XLA\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n RuntimeError: Unimplemented backend XLA\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\n     fn(gindex, *args)\n RuntimeError: Unimplemented backend XLA\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 499, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 920, in run_pretrain_routine\n     self.train()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 356, in train\n     self.run_training_epoch()\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 425, in run_training_epoch\n     output = self.run_training_batch(batch, batch_idx)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 582, in run_training_batch\n     self.batch_loss_value.append(loss)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py\", line 23, in append\n     if self.memory.type() != x.type():\n RuntimeError: Unimplemented backend XLA\n ---------------------------------------------------------------------------\n Exception                                 Traceback (most recent call last)\n <ipython-input-2-12f6e300d51d> in <module>()\n       6 # most basic trainer, uses good defaults\n       7 trainer = Trainer(num_tpu_cores=8)\n ----> 8 trainer.fit(model)\n \n 3 frames\n /usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py in join(self, timeout)\n     111                 raise Exception(\n     112                     \"process %d terminated with exit code %d\" %\n --> 113                     (error_index, exitcode)\n     114                 )\n     115 \n \n Exception: process 4 terminated with exit code 17\n </denchmark-code>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version (e.g., 1.0): 1.6\n OS (e.g., Linux): Linux\n How you installed PyTorch (conda, pip, source): pip\n Build command you used (if compiling from source): -\n Python version: 3.6\n CUDA/cuDNN version: -\n GPU models and configuration: -\n Any other relevant information: TPU backend\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "rzepinskip", "commentT": "2020-04-05T14:37:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n \n The issue is caused by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/f1e11d8b3874067016693c50ae253ec79eecda09/pytorch_lightning/trainer/supporters.py#L40>this line</denchmark-link>\n :\n <denchmark-code>        if self.memory.type() != x.type():\n             self.memory.type_as(x)\n </denchmark-code>\n \n For TPU x is a XLA tensor and x.type() results in Unimplemented backend XLA (for GPU type is torch.cuda.FloatTensor).\n Something like x = torch.Tensor([x]) before condition checking fixes the problem. Or we can just send one of the tensors to common device.\n Notebook for debugging on <denchmark-link:https://colab.research.google.com/drive/16Ug8IYPkqCu_NhK1FV7W-vszDsdgXfPD>Google Colab</denchmark-link>\n .\n \t\t"}}}, "commit": {"commit_id": "b8ff9bc1d242a18f5e7147f34d63f43fcdd0e50a", "commit_author": "Pawe\u0142 Rzepi\u0144ski", "commitT": "2020-04-06 20:29:55-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "24,73,74,81,86", "deleted_lines": "24,73,74,81"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\supporters.py", "file_new_name": "pytorch_lightning\\trainer\\supporters.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "39,40,41", "deleted_lines": "39,40,41", "method_info": {"method_name": "append", "method_params": "self,x", "method_startline": "38", "method_endline": "54"}}}}}}}