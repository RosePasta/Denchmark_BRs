{"BR": {"BR_id": "3335", "BR_author": "junwen-austin", "BRopenT": "2020-09-02T21:51:08Z", "BRcloseT": "2020-10-05T17:52:47Z", "BR_text": {"BRsummary": "Cannot replicate training results with seed_everything and deterministic flag = True with DDP", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n I noticed this when I was adding more metrics calculation to the LightningModule, for example, adding the confusion matrix at the end of validation/test epoch.  Before and after I added these functions (which do not appear to be dependent on any random seed), I noticed the training results are not the exactly the same.\n However, once I added these function and re-ran again, yes I got the same training results.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The training results should be identical even if some deterministic functions are added\n <denchmark-h:h3>Environment</denchmark-h>\n \n Please copy and paste the output from our\n <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>\n \n (or fill out the checklist below manually).\n You can get the script and run it with:\n <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py\n # For security purposes, please check the contents of collect_env_details.py before running it.\n python collect_env_details.py\n </denchmark-code>\n \n \n PyTorch Version (e.g., 1.0):  1.4\n OS (e.g., Linux):  Linux\n How you installed PyTorch (conda, pip, source): pip\n Build command you used (if compiling from source):\n Python version: 3.7\n CUDA/cuDNN version: 10.1\n GPU models and configuration: 4 GPUs DDP\n Any other relevant information:\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "junwen-austin", "commentT": "2020-09-04T04:12:37Z", "comment_text": "\n \t\tThanks for the report. We're also seeing something similar in our CI, unfortunately we don't have a lead yet.\n I did not exactly understand how you came to the conclusion that it is related to metrics. Did you try several runs with and without having metric? I don't think it is metrics, but rather just a ddp issue.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "junwen-austin", "commentT": "2020-09-04T16:20:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  Thanks for your follow-up. No, I do not think this issue has anything to do with the metrics that I was adding but more likely to DDP.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "junwen-austin", "commentT": "2020-09-20T21:37:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/junwen-austin>@junwen-austin</denchmark-link>\n  I may have fixed this problem with the linked PR. But to be sure it closes this issue, could you let me know the trainer flags you used? Did you use  (is also the default) together with\n ?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "junwen-austin", "commentT": "2020-09-20T21:46:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  Thanks for letting me know the progress. I used the ddp as the backend, not the ddp_spawn. Thanks\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "junwen-austin", "commentT": "2020-09-20T21:54:56Z", "comment_text": "\n \t\thmm, that's unfortunate because my fix only applies to ddp_spawn.\n for ddp, I ran several scripts (including my research) and always got deterministic behavior, so I am not sure what what the cause is in your case :(\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "junwen-austin", "commentT": "2020-09-20T22:25:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  it is rather a tricky one. I have two versions of the Lightning models v1 and v2. The only difference between them is that I added additional metric (confusion matrix in this case) in v2, and I noticed the training/validation/test results are slightly off, with both case having ddp as backend, same seed for seed_everything and deterministic flag = True.\n Since the additional code about the confusion matrix has nothing to do with randomization, I expect I should get the exactly the same results.\n S\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "junwen-austin", "commentT": "2020-09-20T23:41:29Z", "comment_text": "\n \t\tbut the confusion matrix is not used for training, right? It is just there for visualization?\n So, just to clarify to be 100% sure we're talking about the same\n \n you get the same training loss if you rerun v1 multiple times\n you get the same training loss if you rerun v2 multiple times\n the training loss between v1 and v2 are however different\n \n sorry if I misunderstood\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "junwen-austin", "commentT": "2020-09-21T00:56:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n \n \n Confusion matrix is used at the end of validation/test loop but not during the training loop\n \n \n you get the same training loss if you rerun v1 multiple times: Yes, also same validation/test loss\n \n \n you get the same training loss if you rerun v2 multiple times: Yes, also same validation/test loss\n \n \n the training loss between v1 and v2 are however different: Yes, also different validation/test loss\n \n \n Sorry about any miscommunication on my part. Thanks\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "junwen-austin", "commentT": "2020-09-22T20:41:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  able to repro? any clue?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "junwen-austin", "commentT": "2020-09-22T21:14:16Z", "comment_text": "\n \t\tUnfortunately not. I tried to reproduce by adding the confusion matrix to the validation epoch end as described, but this gives me same val and train loss as it is expected with setting the seed. <denchmark-link:https://github.com/junwen-austin>@junwen-austin</denchmark-link>\n  I think I need an example code from you or I don't know where to look for the problem.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "junwen-austin", "commentT": "2020-09-22T21:38:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  Thanks for looking into it. Sure, the pseudo code looks like:\n In v1, there is no confusion matrix at all or any calculation related to it.\n in v2, I added the following in the __init__ function:\n self.confusion_matrix = ConfusionMatrix()\n then at the end of validation/test epoch, I calculated confusion matrix based on sync'ed y and y_pred and then log each component of confusion matrix. Note: in both v1 and v2, y and y_pred are sync'ed before calculating any metric\n <denchmark-code>           def validation_epoch_end(self, result):\n                y = result['y']\n                y_pred = result['y_pred']\n                \n                if use_ddp:\n                    sync y\n                    sync y_pred\n             \n               # calculate other metrics\n                    ...\n              \n               # start the code only in v2:\n               cm = self.confusion_matrix(y_pred, y)\n               \n              # log each component of cm:\n              self.logger.add_scalar('TP', cm[1,1], global_step=self.global_step)\n              self.logger.add_scalar('FP', cm[0,1], global_step=self.global_step)\n              self.logger.add_scalar('FN', cm[1,0], global_step=self.global_step)\n              self.logger.add_scalar('TN', cm[0,0], global_step=self.global_step)\n              \n              # end the code only in v2\n \n \n \n </denchmark-code>\n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "junwen-austin", "commentT": "2020-09-23T09:11:35Z", "comment_text": "\n \t\tI had it almost like this, but without sync ddp. Can you share the whole runnable script? At this point I can only make guesses.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "junwen-austin", "commentT": "2020-09-23T14:13:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n Unfortunately I cannot share the entire script but I will provide more information in hoping of getting what you need.\n I have the following helper function with the Lightning Module:\n <denchmark-code>def calculate(self, batch):\n     data, y = batch\n     logit = self.model(data).     # y_hat is probability\n     loss = torch.nn.functional.CrossEntropy(logit, y)\n     y_hat = torch.nn.Softmax(dim=1)(logit)     \n     _, y_pred = torch.max(y_hat, dim=1)    # y_pred is the predicted label\n    \n    return loss, y, y_hat, y_pred\n \n def compute_metrics(self, y, y_hat, y_pred):\n       # compute metrics given y, y_hat, y_pred\n       acc = self.accuracy(y_pred, y)/self.trainer.world_size\n       f1 = self.f1(y_pred,y) / self.trainer.world_size\n       cm = self.confusion_matrix(y_pred, y)    # added in v2\n      \n       return acc, f1, cm\n  \n @staticmethod\n def log_metrics(result, kind, scalars=('loss','accuracy','f1'):\n         for scalar in scalars:\n                   result.log(scalar + '/' + kind, scalar, on_step=False, on_epoch=True, sync_dist=True, sync_dist_op='mean')\n \n \n def train_valid_test_step(self, batch, kind):\n           loss, y, y_hat, y_pred = self.calculate(batch)\n           if kind == 'train':\n                 result = pl.TrainResult(loss)\n           else:\n                 result = pl.EvalResult(checkpoint_on=loss)\n                 result.val_loss = loss\n \n           result.y =y\n           result.y_hat = y_hat\n           result.y_pred = y_pred\n  \n def sync_across_gpus(self, t):   # t is a tensor\n        \n         gather_t_tensor = [torch.ones_like(t) for _ in range(self.trainer.world_size)]\n         torch.distributed.all_gather(gather_t_tensor, t)\n         return torch.cat(gather_t_tensor)          \n \n def valid_test_epoch_end(self, result, kind):\n     loss = result.val_loss\n     y = result.y\n     y_hat = result.y_hat\n     y_pred = result.y_pred\n \n    if self.trainer.use_ddp:\n           loss = self.sync_across_gpus(loss)\n           y = self.sync_across_gpus(y)\n           y_pred = self.sync_across_gpus(y_pred)\n           y_hat = self.sync_across_gpus(y_hat)\n          \n    acc, f1, cm = self.compute_metrics(y, y_hat, y_pred)\n    result.loss = loss.mean()\n    result.accurancy = acc\n    result.f1 = f1\n    result.cm = cm # added in v2\n    \n    log_metrics(result, kind, scalars=('loss','accuracy','f1')\n \n    # added in v2\n    self.logger.add_scalar('TP', cm[1,1], global_step=self.global_step)\n    self.logger.add_scalar('FP', cm[0,1], global_step=self.global_step)\n    self.logger.add_scalar('FN', cm[1,0], global_step=self.global_step)\n    self.logger.add_scalar('TN', cm[0,0], global_step=self.global_step)\n \n \n def training_step(self, batch, batch_idx):\n      return self.train_valid_test_step(batch, 'train')\n \n def train_step_end(self, result):\n      loss = result.minimize\n      result.log('loss/train', loss, on_step=True, sync_dist=True, sync_dist_op='mean')\n      return result\n \n \n def validation_step(self, batch, batch_idx):\n      return self.train_valid_test_step(batch, 'valid')\n \n def validation_epoch_end(self, result):\n      \n      return self.valid_test_epoch_end(result,'valid')\n \n def test_step(self, batch, batch_idx):\n      return self.train_valid_test_step(batch, 'test')\n \n def test_epoch_end(self, result):\n      \n      return self.valid_test_epoch_end(result,'test')\n     \n </denchmark-code>\n \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "junwen-austin", "commentT": "2020-09-28T05:42:10Z", "comment_text": "\n \t\tstill no luck reproducing this. since you cannot share all the code, I had tried to combine what you show here with an existing example code, but no luck.\n maybe if you tried yourself to adapt our <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/basic_examples/mnist.py>mnist example</denchmark-link>\n  with your confusion matrix use case we get a reproducible script. I tried but I failed.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "junwen-austin", "commentT": "2020-09-28T16:58:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n Thanks I will do that and let you know.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "junwen-austin", "commentT": "2020-10-05T17:52:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  My apology I cannot replicate the issue seen from my work project. Let's close the issue for now and if I spot a similar one, I will have better documentation of it instead of trying to getting it from top of my head. Thanks.\n \t\t"}}}, "commit": {"commit_id": "a71d62d8409f4960a4b438b8d19c924d3636c73f", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-09-20 19:42:58-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "71", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\accelerators\\ddp_base_backend.py", "file_new_name": "pytorch_lightning\\accelerators\\ddp_base_backend.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101,102,103,104,105", "deleted_lines": null, "method_info": {"method_name": "ddp_train_tmp", "method_params": "self,process_idx,mp_queue,model,is_master,proc_offset", "method_startline": "89", "method_endline": "182"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\utilities\\seed.py", "file_new_name": "pytorch_lightning\\utilities\\seed.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "28,29,30,31,32,33,34,35,36,37,44,45,57", "deleted_lines": "28,29,36,37,38", "method_info": {"method_name": "seed_everything", "method_params": "None", "method_startline": "27", "method_endline": "62"}}}}}}}