{"BR": {"BR_id": "3693", "BR_author": "chrismaliszewski", "BRopenT": "2020-09-28T04:34:38Z", "BRcloseT": "2020-10-02T19:46:47Z", "BR_text": {"BRsummary": "Missing attribute \"training_step_output_for_epoch_end\"", "BRdescription": "\n I used the documentation way of stopping the training (<denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html#enable-early-stopping-using-callbacks-on-epoch-end>https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html#enable-early-stopping-using-callbacks-on-epoch-end</denchmark-link>\n ).\n If on_bath_start method returns -1 at the very beginning of an epoch, the titled AttributeError exception.\n The problem is in training_loop.py line 496 (batch_output.training_step_output_for_epoch_end).\n <denchmark-h:h4>Code sample</denchmark-h>\n \n Use the method and run your code:\n     def on_batch_start(self, batch):\n         return -1\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Check batch_output value if equals -1 before running trainin_loop.py line 495.\n The early stopping method achieved the same way the documentation specifies should not throw an exception but rather simply stop the training.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n \n GPU:\n available:         False\n version:           None\n \n \n Packages:\n \n numpy:             1.19.1\n pyTorch_debug:     False\n pyTorch_version:   1.6.0\n pytorch-lightning: 0.9.0\n tqdm:              4.49.0\n \n \n System:\n \n OS:                Windows\n architecture:\n \n 64bit\n WindowsPE\n \n \n processor:         Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\n python:            3.8.5\n version:           10.0.18362\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "chrismaliszewski", "commentT": "2020-09-28T04:35:19Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T04:24:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/chrismaliszewski>@chrismaliszewski</denchmark-link>\n  can you confirm this now stops the training epoch?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:01:54Z", "comment_text": "\n \t\tShould I update in conda command line, nothing has changed:\n <denchmark-code>Traceback (most recent call last):\n   File \"XXX\\__main__train.py\", line 54, in <module>\n     trainer.fit(model)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\states.py\", line 48, in wrapped_fn\n     result = fn(self, *args, **kwargs)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1084, in fit\n     results = self.accelerator_backend.train(model)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\accelerators\\cpu_backend.py\", line 39, in train\n     results = self.trainer.run_pretrain_routine(model)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1239, in run_pretrain_routine\n     self.train()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 394, in train\n     self.run_training_epoch()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 496, in run_training_epoch\n     batch_output.training_step_output_for_epoch_end,\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py\", line 144, in __getattr__\n     raise AttributeError(f'Missing attribute \"{key}\"')\n AttributeError: Missing attribute \"training_step_output_for_epoch_end\"\n Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]\n \n Process finished with exit code 1\n </denchmark-code>\n \n Or should I update directly from GitHub, i.e. using the method provided here: <denchmark-link:https://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github>https://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github</denchmark-link>\n ?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:06:01Z", "comment_text": "\n \t\tYes please, it's fixed on master, it hasn't been released yet. You can do the following in your conda or pip env\n <denchmark-code>pip install git+https://github.com/PyTorchLightning/pytorch-lightning.git@master --upgrade\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:27:42Z", "comment_text": "\n \t\tAfter the update you suggested I have MAJOR problems, even crashing errors.\n In regards to the issue I posted, I have the following error, no matter if I return -1 or anything else:\n <denchmark-code>File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 442, in fit\n     results = self.accelerator_backend.train()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\accelerators\\cpu_backend.py\", line 47, in train\n     results = self.train_or_test()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\accelerators\\base_backend.py\", line 43, in train_or_test\n     results = self.trainer.train()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 489, in train\n     self.train_loop.run_training_epoch()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 516, in run_training_epoch\n     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 617, in run_training_batch\n     response = self.trainer.call_hook('on_batch_start')\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 807, in call_hook\n     output = hook_fx(*args, **kwargs)\n TypeError: on_batch_start() missing 1 required positional argument: 'batch'\n Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]\n </denchmark-code>\n \n I haven't changed anything in the definition of my function and it looks as follows:\n <denchmark-code>    def on_batch_start(self, batch):\n         return -1\n         if self.get_early_stop(self.hparams['early_stop_path']):\n             return -1\n         else:\n             return batch\n </denchmark-code>\n \n where get_early_stop returns Boolean if the training should early stop at any given time.\n For the unknown reason, the args and kwargs in the line output = hook_fx(*args, **kwargs) are empty.\n If I remove method on_batch_start, the code follows further but crashes elsewhere, read the next comment.\n If you need further information, let me know and I try helping.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:39:45Z", "comment_text": "\n \t\tIn terms of other problems with the version.\n I have a crashing error or a warning is being displayed. Messages exclude each other in the potential resolving way.\n Error message 1.\n <denchmark-code> File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 442, in fit\n     results = self.accelerator_backend.train()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\accelerators\\cpu_backend.py\", line 47, in train\n     results = self.train_or_test()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\accelerators\\base_backend.py\", line 43, in train_or_test\n     results = self.trainer.train()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 489, in train\n     self.train_loop.run_training_epoch()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 539, in run_training_epoch\n     self.trainer.run_evaluation(test_mode=False)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 604, in run_evaluation\n     self.evaluation_loop.on_evaluation_epoch_end()\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\", line 298, in on_evaluation_epoch_end\n     self.trainer.call_hook('on_validation_epoch_end', *args, **kwargs)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 800, in call_hook\n     trainer_hook(*args, **kwargs)\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\callback_hook.py\", line 87, in on_validation_epoch_end\n     callback.on_validation_epoch_end(self, self.get_model())\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 152, in on_validation_epoch_end\n     if self._validate_condition_metric(trainer.logger_connector.callback_metrics):\n   File \"YYY\\anaconda3\\envs\\pt_cpu\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py\", line 116, in _validate_condition_metric\n     raise RuntimeError(error_msg)\n RuntimeError: Early stopping conditioned on metric `val_loss` which is not available. Either add `val_loss` to the return of `validation_epoch_end` or modify your `EarlyStopping` callback to use any of the following: ``\n </denchmark-code>\n \n Note the part Either add val_loss to the return of validation_epoch_end and that the error message is cut with nothing after the following:.\n Warning message 2.\n UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n after I remove return {'val_loss': loss} leaving just self.log(\"val_loss\", loss). So which one should I do? return or not return?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:50:12Z", "comment_text": "\n \t\tOkay. Regarding the first issue,  is deprecated in 0.9 and will be removed in 1.0.\n <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_batch_start>https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_batch_start</denchmark-link>\n \n Please use .\n <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_train_batch_start>https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_train_batch_start</denchmark-link>\n \n Also you probably don't want to stop training epoch just at the start of the training, I assume.\n     def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n         if self.get_early_stop(self.hparams['early_stop_path']):\n             return -1\n         else:\n             return batch\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:53:21Z", "comment_text": "\n \t\tThe method you provided works without any errors. Thank you for the advice.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:56:57Z", "comment_text": "\n \t\tRegarding the second issue, you can use  to use  in the early stop callback.\n For now, you can ignore the below warning, currently at fixing at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3812>#3812</denchmark-link>\n \n <denchmark-code>UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n </denchmark-code>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:58:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ydcjeff>@ydcjeff</denchmark-link>\n , I'll report you on that later. It's 9PM my time. Thank you.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "chrismaliszewski", "commentT": "2020-10-03T07:59:54Z", "comment_text": "\n \t\tOkay. Feel free to create an another issue, if something doesn't work with earlystopping\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "chrismaliszewski", "commentT": "2020-10-07T10:36:08Z", "comment_text": "\n \t\tForgot to report. The code you suggested works. Thank you again, <denchmark-link:https://github.com/ydcjeff>@ydcjeff</denchmark-link>\n .\n \t\t"}}}, "commit": {"commit_id": "9942f3ebdf14d0139b1b156dd56662b425f3c777", "commit_author": "Jeff Yang", "commitT": "2020-10-02 21:46:46+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "62,63", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\early_stopping.rst", "file_new_name": "docs\\source\\early_stopping.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13", "deleted_lines": "13"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "518,519,520,521", "deleted_lines": "530,531,532", "method_info": {"method_name": "run_training_epoch", "method_params": "self", "method_startline": "494", "method_endline": "594"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\models\\test_hooks.py", "file_new_name": "tests\\models\\test_hooks.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "119,120,121", "deleted_lines": null, "method_info": {"method_name": "test_on_train_batch_start_hook.on_train_batch_start", "method_params": "self,batch,batch_idx,dataloader_idx", "method_startline": "119", "method_endline": "121"}}, "hunk_1": {"Ismethod": 1, "added_lines": "117,118,119,120,121,122,123,124,125,126,127,128,129,130,131", "deleted_lines": null, "method_info": {"method_name": "test_on_train_batch_start_hook", "method_params": "max_epochs,batch_idx_", "method_startline": "117", "method_endline": "131"}}}}}}}