{"BR": {"BR_id": "2484", "BR_author": "wietsedv", "BRopenT": "2020-07-03T09:48:35Z", "BRcloseT": "2020-08-19T23:01:56Z", "BR_text": {"BRsummary": "Trainer.scale_batch_size requires model.batch_size instead of model.hparams.batch_size", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Trainer.scale_batch_size only works if a model has the batch_size property and does not work with model.hparams.batch_size even though all documentation points to the reverse.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n All of my hyperparameters are available as model.hparams like suggested in the documentation: (<denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/hyperparameters.html#lightningmodule-hyperparameters>hyperparameters, option 3</denchmark-link>\n .\n This means that my  is available as .\n This should be fully compatible with the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/training_tricks.html#auto-scaling-of-batch-size>documented example code</denchmark-link>\n  of  since that code also uses  instead of .\n However, when I put my model in Trainer.scale_batch_size, I get the following error:\n <denchmark-code>pytorch_lightning.utilities.exceptions.MisconfigurationException: Field batch_size not found in `model.hparams`\n </denchmark-code>\n \n <denchmark-h:h3>Example code</denchmark-h>\n \n <denchmark-code>class LitModel(pl.LightningModule):\n     def __init__(self, hparams):\n         super().__init__()\n         self.hparams = args\n \n model = LitModel(args)\n trainer = Trainer()\n trainer.scale_batch_size(model)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Either  should work with  or the error message, linked documentation examples and docstrings should all change (i.e. <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L139>here</denchmark-link>\n , <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L228>here</denchmark-link>\n  and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L233>here</denchmark-link>\n ).\n (I would prefer the second option. I think that it should work with both model.batch_size and model.hparams.batch_size.)\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n pytorch-lightning         0.8.4\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wietsedv", "commentT": "2020-07-03T09:49:38Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wietsedv", "commentT": "2020-07-03T10:53:40Z", "comment_text": "\n \t\tit seems like a nice first issue, <denchmark-link:https://github.com/wietsedv>@wietsedv</denchmark-link>\n  mind send a PR? \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wietsedv", "commentT": "2020-07-04T05:10:40Z", "comment_text": "\n \t\tAppears to be the same with the learning rate parameter.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wietsedv", "commentT": "2020-07-20T19:05:23Z", "comment_text": "\n \t\tA clean fix on the user side while waiting for the PR is to actually use self.hparams.batch_size and define self.batch_size as a property of your module:\n @property\n def batch_size(self):\n     return self.hparams.batch_size\n \n @batch_size.setter\n def batch_size(self, batch_size):\n     self.hparams.batch_size = batch_size\n That way you keep your hyper parameters together in case you want to dump them somewhere without having to add specific code.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "wietsedv", "commentT": "2020-07-24T15:29:35Z", "comment_text": "\n \t\tFrom <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1896>#1896</denchmark-link>\n  it seems that the problem is rather on the docs side than 's implementation.\n  is the correct location to look for the parameter, not .\n My above fix is thus also obsolete.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "wietsedv", "commentT": "2020-07-24T15:30:51Z", "comment_text": "\n \t\t\n From #1896 it seems that the problem is rather on the docs side than scale_batch_size()'s implementation.\n My above fix is thus also obsolete.\n \n I just tried your fix and it seemed to work :)\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "wietsedv", "commentT": "2020-07-24T15:37:53Z", "comment_text": "\n \t\tYes it does work, but from what they said in the PR I linked, hparams was just there as a temporary solution, and all hyper parameters are intended to be set as direct instance attributes in __init__.\n My fix is obsolete regarding the intended usage of PL.\n \t\t"}}}, "commit": {"commit_id": "7b917de94642f63eedaffde79fb973705d2288dd", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-08-19 19:01:55-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "151,152,155", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\training_tricks.py", "file_new_name": "pytorch_lightning\\trainer\\training_tricks.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27,30,162,163,164,165,166,167,168,169,170,276,278,286", "deleted_lines": "27,161,162,163,164,165,271,272,273,274,276,277,278,279,287"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tests\\trainer\\test_trainer_tricks.py", "file_new_name": "tests\\trainer\\test_trainer_tricks.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "240,241,242,243,244,245,246,247,248,249", "deleted_lines": null, "method_info": {"method_name": "test_auto_scale_batch_size_duplicate_attribute_warning", "method_params": "tmpdir", "method_startline": "240", "method_endline": "249"}}, "hunk_1": {"Ismethod": 1, "added_lines": "200,201,206,213,214,215,216,217,218", "deleted_lines": "200,201,203,206,208,209,210,211,212,213,214", "method_info": {"method_name": "test_trainer_arg", "method_params": "tmpdir,scale_arg", "method_startline": "200", "method_endline": "218"}}, "hunk_2": {"Ismethod": 1, "added_lines": "200,201,206", "deleted_lines": "200,201,203,206,208,209,210", "method_info": {"method_name": "test_auto_scale_batch_size_trainer_arg", "method_params": "tmpdir,scale_arg", "method_startline": "200", "method_endline": "210"}}, "hunk_3": {"Ismethod": 1, "added_lines": "214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237", "deleted_lines": "214", "method_info": {"method_name": "test_auto_scale_batch_size_set_model_attribute", "method_params": "tmpdir,use_hparams", "method_startline": "214", "method_endline": "237"}}, "hunk_4": {"Ismethod": 1, "added_lines": "223,224,225,226,227,228,229", "deleted_lines": null, "method_info": {"method_name": "test_auto_scale_batch_size_set_model_attribute.dataloader", "method_params": "self,args,kwargs", "method_startline": "223", "method_endline": "229"}}}}}}}