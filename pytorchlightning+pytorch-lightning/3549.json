{"BR": {"BR_id": "3549", "BR_author": "pbmstrk", "BRopenT": "2020-09-18T13:00:13Z", "BRcloseT": "2020-09-20T00:00:51Z", "BR_text": {"BRsummary": "Bug in validation_epoch_end", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n In the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html?highlight=validation_epoch_end#validation-epoch-end>documentation</denchmark-link>\n ,  is described as running at the end of a validation epoch and does not need to necessarily return anything.\n When running a slightly modified version of the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/new_project.html>example</denchmark-link>\n  in the docs,  seemingly runs once on a single batch of validation data and returns an error if nothing is returned.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n A MWE:\n <denchmark-code>import os\n import torch\n import torch.nn.functional as F\n from torchvision.datasets import MNIST\n from torchvision import transforms\n from torch.utils.data import DataLoader\n import pytorch_lightning as pl\n from torch.utils.data import random_split\n \n class LitModel(pl.LightningModule):\n \n     def __init__(self):\n         super().__init__()\n         self.layer_1 = torch.nn.Linear(28 * 28, 128)\n         self.layer_2 = torch.nn.Linear(128, 10)\n \n     def forward(self, x):\n         x = x.view(x.size(0), -1)\n         x = self.layer_1(x)\n         x = F.relu(x)\n         x = self.layer_2(x)\n         return x\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n         return optimizer\n \n     def training_step(self, batch, batch_idx):\n         x, y = batch\n         y_hat = self(x)\n         loss = F.cross_entropy(y_hat, y)\n \n         return {'loss': loss}\n \n     def validation_step(self, batch, batch_idx):\n         x, y = batch\n         y_hat = self(x)\n         loss = F.cross_entropy(y_hat, y)\n         \n         return {\"val_loss\": loss}\n \n     def validation_epoch_end(self, validation_step_outputs):\n         print(\"Length of outputs: {}\".format(len(validation_step_outputs)))\n         # a dummy return value to avoid error\n         return {\"val_loss\": 0}\n \n model = LitModel()\n \n dataset = MNIST(os.getcwd(), download=True, train=False, transform=transforms.ToTensor())\n train, val = random_split(dataset, [9000, 1000])\n \n train_loader = DataLoader(train)\n val_loader = DataLoader(val)\n \n # train\n trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)\n trainer.fit(model, train_loader, val_loader)\n </denchmark-code>\n \n <denchmark-h:h4>Output</denchmark-h>\n \n <denchmark-code>GPU available: True, used: True\n TPU available: False, using: 0 TPU cores\n CUDA_VISIBLE_DEVICES: [0]\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n   warnings.warn(*args, **kwargs)\n Length of outputs: 1\n Length of outputs: 1000\n Length of outputs: 1000\n Length of outputs: 1000\n Length of outputs: 1000\n Saving latest checkpoint..\n Length of outputs: 1000\n </denchmark-code>\n \n If no return value is given in validation_epoch_end, the following error occurs\n <denchmark-code>---------------------------------------------------------------------------\n AttributeError                            Traceback (most recent call last)\n <ipython-input-183-013dcacba267> in <module>()\n      10 # train\n      11 trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)\n ---> 12 trainer.fit(model, train_loader, val_loader)\n \n 7 frames\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)\n      46             if entering is not None:\n      47                 self.state = entering\n ---> 48             result = fn(self, *args, **kwargs)\n      49 \n      50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\n    1071             self.accelerator_backend = GPUBackend(self)\n    1072             model = self.accelerator_backend.setup(model)\n -> 1073             results = self.accelerator_backend.train(model)\n    1074 \n    1075         elif self.use_tpu:\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/gpu_backend.py in train(self, model)\n      49 \n      50     def train(self, model):\n ---> 51         results = self.trainer.run_pretrain_routine(model)\n      52         return results\n      53 \n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\n    1222 \n    1223         # run a few val batches before training starts\n -> 1224         self._run_sanity_check(ref_model, model)\n    1225 \n    1226         # clear cache before training\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model, model)\n    1255             num_loaders = len(self.val_dataloaders)\n    1256             max_batches = [self.num_sanity_val_steps] * num_loaders\n -> 1257             eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)\n    1258 \n    1259             # allow no returns from eval\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in _evaluate(self, model, dataloaders, max_batches, test_mode)\n     397 \n     398         # log callback metrics\n --> 399         self.__update_callback_metrics(eval_results, using_eval_result)\n     400 \n     401         # Write predictions to disk if they're available.\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in __update_callback_metrics(self, eval_results, using_eval_result)\n     429                         flat = {'val_loss': eval_result}\n     430                     else:\n --> 431                         flat = flatten_dict(eval_result)\n     432                     self.callback_metrics.update(flat)\n     433             else:\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/parsing.py in flatten_dict(source, result)\n     113         result = {}\n     114 \n --> 115     for k, v in source.items():\n     116         if isinstance(v, dict):\n     117             _ = flatten_dict(v, result)\n \n AttributeError: 'NoneType' object has no attribute 'items'\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The outputs in validation_epoch_end should contain results from all batches. Is a test validation batch running in the background, which could explain why the results of only one batch are included in the first loop.\n Return values in  were addressed in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2438>#2438</denchmark-link>\n  (with the same error) and from what I can tell included in the 0.8.4 release - perhaps I am missing something?\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch: 1.6.0\n pytorch-lightning: 0.9.0\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "pbmstrk", "commentT": "2020-09-18T13:01:10Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "pbmstrk", "commentT": "2020-09-18T15:17:40Z", "comment_text": "\n \t\tHi there! What you are seeing in the first output is caused by our \"validation sanity check\". We run a configurable number of validation steps before training starts to ensure the user doesn't run into any issues after a potentially timely series of training steps. This can be configured (or turned off) with pl.Trainer(num_sanity_val_steps=0).\n Regarding returning None from validation_epoch_end, it appears that functionality has recently been removed, I will look into whether that was intentional or not and update the code/docs accordingly.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "pbmstrk", "commentT": "2020-09-18T19:10:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/teddykoker>@teddykoker</denchmark-link>\n  That makes sense. Thanks for the pointer for configuring this!\n \t\t"}}}, "commit": {"commit_id": "9acee67c31c84dac74cc6169561a483d3b9c9f9d", "commit_author": "William Falcon", "commitT": "2020-09-19 20:00:50-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\connectors\\logger_connector.py", "file_new_name": "pytorch_lightning\\trainer\\connectors\\logger_connector.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "107,108,109", "deleted_lines": null, "method_info": {"method_name": "_log_on_evaluation_epoch_end_metrics", "method_params": "self,eval_results,using_eval_result", "method_startline": "106", "method_endline": "131"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\logging.py", "file_new_name": "pytorch_lightning\\trainer\\logging.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "76,77,78,79,140", "deleted_lines": "76,77,78,139", "method_info": {"method_name": "process_output", "method_params": "self,output,train", "method_startline": "55", "method_endline": "150"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\base\\model_valid_epoch_ends.py", "file_new_name": "tests\\base\\model_valid_epoch_ends.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "45,46,47", "deleted_lines": null, "method_info": {"method_name": "validation_epoch_end_return_none._mean", "method_params": "res,key", "method_startline": "45", "method_endline": "47"}}, "hunk_1": {"Ismethod": 1, "added_lines": "36,37,38,39,40,41,42,43,44,45,46,47,48,49", "deleted_lines": null, "method_info": {"method_name": "validation_epoch_end_return_none", "method_params": "self,outputs", "method_startline": "36", "method_endline": "49"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\trainer\\test_trainer_steps_result_return.py", "file_new_name": "tests\\trainer\\test_trainer_steps_result_return.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649", "deleted_lines": null, "method_info": {"method_name": "test_eval_loop_return_none", "method_params": "tmpdir", "method_startline": "628", "method_endline": "649"}}}}}}}