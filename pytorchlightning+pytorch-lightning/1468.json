{"BR": {"BR_id": "1468", "BR_author": "siddk", "BRopenT": "2020-04-12T22:32:36Z", "BRcloseT": "2020-04-19T11:03:41Z", "BR_text": {"BRsummary": "Mixing hparams and arguments in LightningModule.__init__() crashes load_from_checkpoint()", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Right now, if you initialize a Lightning Module with a mixture of a Namespace (hparams) as well as additional arguments (say to a Dataset), load_from_checkpoint can't recover.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Create a LightningModule as follows:\n class Model(pl.LightningModule):\n       def __init__(self, hparams, train_dataset, val_dataset):\n               self.hparams = hparams\n               self.train_dset, self.val_dset = train_dataset, val_dataset\n               ...\n Run training, then try to restore from checkpoint, via:\n nn = Model.restore_from_checkpoint(<PATH>, train_dataset=None, val_dataset=None)\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Ideally, you'd just be able to pass in the additional arguments (as above) and everything would work.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "siddk", "commentT": "2020-04-12T22:33:18Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "siddk", "commentT": "2020-04-13T09:08:22Z", "comment_text": "\n \t\tVery interesting issue. I think the whole restore_from_checkpoint could use a bit better documentation. I think you are trying to use checkpoints for something that was not the intention.\n I think what you want is run the model without training (this is why the train and val datasets are empty). In this case I would try falling back to plain torch solution, such as saving and reusing the state_dict:\n <denchmark-code>torch.save(nn.state_dict(), SAVE_PATH)\n loaded_model = nn(hparams)\n loaded_model.load_state_dict(torch.load(SAVE_PATH))\n loaded_model.eval()\n print(\"Model's state_dict:\")\n for param_tensor in loaded_model.state_dict():\n   print(param_tensor, \"\\t\", loaded_model.state_dict()[param_tensor].size())\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "siddk", "commentT": "2020-04-13T15:59:15Z", "comment_text": "\n \t\tThat makes sense for this usecase --> but say I want to pause/resume training later on (the intended usecase of restore from checkpoint). Here, I really want to be able to leverage the fact that Lightning remembers my hyperparameters, and I want to be able to just pass in the additional arguments (like the datasets I've constructed).\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "siddk", "commentT": "2020-04-13T16:04:49Z", "comment_text": "\n \t\tThis is the behavior in 0.7.3\n <denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html#lightningmodule-class>https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html#lightningmodule-class</denchmark-link>\n \n # or load passing whatever args the model takes to load\n MyLightningModule.load_from_checkpoint(\n     'path/to/checkpoint.ckpt',\n     learning_rate=0.1,\n     layers=2,\n     pretrained_model=some_model\n )\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "siddk", "commentT": "2020-04-13T16:32:15Z", "comment_text": "\n \t\tThis doesn't seem to be true if I explicitly pass a hparams argument... only if I break out each of the arguments and pass them to the init() method.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "siddk", "commentT": "2020-04-13T16:35:00Z", "comment_text": "\n \t\tshare a colab? this should be true on 0.7.3.\n For now you can do Trainer(PATH, **hparams)\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "siddk", "commentT": "2020-04-13T20:19:19Z", "comment_text": "\n \t\t<denchmark-link:https://colab.research.google.com/drive/1WmuZfOQxyi4nF_YvjFcJrcpVhNvuCXST>https://colab.research.google.com/drive/1WmuZfOQxyi4nF_YvjFcJrcpVhNvuCXST</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "siddk", "commentT": "2020-04-16T04:46:35Z", "comment_text": "\n \t\tI can reproduce this with that script locally as well on <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/e3001a092913514d65547d4e912382525cfedad2>e3001a0</denchmark-link>\n \n Gives me the following traceback\n <denchmark-code>Traceback (most recent call last):\n   File \"pl_loadfromcheckpoint.py\", line 40, in <module>\n     new_model = Model.load_from_checkpoint('save.ckpt', c='this is', d='a test')\n   File \"/home/henry/Coding/pytorch-lightning/pytorch_lightning/core/lightning.py\", line 1509, in load_from_checkpoint\n     model = cls._load_model_state(checkpoint, *args, **kwargs)\n   File \"/home/henry/Coding/pytorch-lightning/pytorch_lightning/core/lightning.py\", line 1541, in _load_model_state\n     model = cls(*model_args)\n TypeError: __init__() missing 2 required positional arguments: 'c' and 'd'\n </denchmark-code>\n \n It seems to be due to this if statement here which stops it taking into account anymore arguments if there's a hparam in the checkpoint\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1540>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1540</denchmark-link>\n \n EDIT: PRed <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1505>#1505</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "3c6f856f232ccd124ca90621cdda8094bae6e332", "commit_author": "Hengjian (Henry) Jia", "commitT": "2020-04-19 07:03:40-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "42,43", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\lightning.py", "file_new_name": "pytorch_lightning\\core\\lightning.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1548", "deleted_lines": "1547,1548,1549,1550", "method_info": {"method_name": "_load_model_state", "method_params": "cls,str,args,kwargs", "method_startline": "1522", "method_endline": "1554"}}}}}}}