{"BR": {"BR_id": "1538", "BR_author": "lezwon", "BRopenT": "2020-04-20T18:04:20Z", "BRcloseT": "2020-04-23T11:12:55Z", "BR_text": {"BRsummary": "`num_tpu_cores=8` does not work on kaggle", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When I try to train a model on Kaggle TPU's with num_tpu_cores set to 8, I receive an error Exception: process 2 terminated with exit code 1 . Would be great if this worked on kaggle.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Run this notebook:\n https://www.kaggle.com/lezwon/pytorch-on-tpu-with-pytorch-lightning\n \n <denchmark-code>---------------------------------------------------------------------------\n Exception                                 Traceback (most recent call last)\n <ipython-input-9-9251330963d1> in <module>\n       3 # most basic trainer, uses good defaults (1 TPU)\n       4 trainer = pl.Trainer(num_tpu_cores=8)\n ----> 5 trainer.fit(mnist_model)\n \n /opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, test_dataloaders)\n     714 \n     715             # train\n --> 716             xmp.spawn(self.tpu_train, args=(model,), nprocs=self.num_tpu_cores, start_method=start_method)\n     717 \n     718             # load weights if not interrupted\n \n /opt/conda/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method)\n     180         join=join,\n     181         daemon=daemon,\n --> 182         start_method=start_method)\n \n /opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)\n     156 \n     157     # Loop on join until it returns True or raises an exception.\n --> 158     while not context.join():\n     159         pass\n     160 \n \n /opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)\n     111                 raise Exception(\n     112                     \"process %d terminated with exit code %d\" %\n --> 113                     (error_index, exitcode)\n     114                 )\n     115 \n \n Exception: process 3 terminated with exit code 1\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n trainer = pl.Trainer(num_tpu_cores=8, precision=16) \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Run the model utilizing all 8 TPU cores.\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>cuda:\n \tGPU:\n \tavailable:           False\n \tversion:             None\n packages:\n \tnumpy:               1.18.2\n \tpyTorch_debug:       False\n \tpyTorch_version:     1.6.0a0+30e7055\n \tpytorch-lightning:   0.7.3\n \ttensorboard:         2.1.1\n \ttqdm:                4.42.0\n system:\n \tOS:                  Linux\n \tarchitecture:\n \t\t64bit\n \t\t\n \tprocessor:           \n \tpython:              3.6.6\n \tversion:             #1 SMP Sat Apr 4 00:12:45 PDT 2020\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lezwon", "commentT": "2020-04-20T22:54:06Z", "comment_text": "\n \t\tI think this is a kaggle problem?\n <denchmark-link:https://github.com/dlibenzi>@dlibenzi</denchmark-link>\n  any ideas?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lezwon", "commentT": "2020-04-20T22:58:07Z", "comment_text": "\n \t\tIt prolly needs this on top:\n <denchmark-code>!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lezwon", "commentT": "2020-04-20T23:34:16Z", "comment_text": "\n \t\tthose lines are already at the top:\n <denchmark-link:https://www.kaggle.com/pytorchlightning/pytorch-on-tpu-with-pytorch-lightning>https://www.kaggle.com/pytorchlightning/pytorch-on-tpu-with-pytorch-lightning</denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/3640001/79809189-eb2c1580-833d-11ea-80d2-4954e5ffca0d.png></denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lezwon", "commentT": "2020-04-21T00:55:23Z", "comment_text": "\n \t\tI bet the issue is here:\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/trainer.py\n \n \n          Line 762\n       in\n       bd16881\n \n \n \n \n \n \n  start_method = 'fork' if os.getenv('COLAB_GPU') else 'spawn' \n \n \n \n \n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lezwon", "commentT": "2020-04-21T01:03:08Z", "comment_text": "\n \t\tah... yes. good catch.\n know of something more general that we can check? i assume the only two options are kaggle and colab?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "lezwon", "commentT": "2020-04-21T01:04:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lezwon>@lezwon</denchmark-link>\n  want to find an environment variable we can check to know if on kaggle and submit a PR?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "lezwon", "commentT": "2020-04-21T01:06:17Z", "comment_text": "\n \t\tHonestly, pytorch does not like fork because of CUDA, but I would make that the default, with ability to change via some environment variable in cases someone have issues.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "lezwon", "commentT": "2020-04-21T01:11:15Z", "comment_text": "\n \t\ton GCP it would still be fork?\n when would it not be fork with TPUs?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "lezwon", "commentT": "2020-04-21T01:24:59Z", "comment_text": "\n \t\tFork is an issue with pytorch/CUDA mostly.\n But for safety, I would just add a Kaggle check as well in your code, and leave spawn as default.\n Fork also helps Colab and Kaggle because, being them low memory VMs, one can reduce the memory consumption by creating the model (on default pytorch/cpu) at global scope, and then doing to(xla_device) from within the xmp.spawn() target functions.\n This avoids creating pytorch/cpu models in each of the processes (one per core).\n You can see a few tricks to fit models on Colab here:\n <denchmark-link:https://colab.research.google.com/drive/1IvCxIg-Q_DlI7UNJuajpl4UZXNiW5jMg>https://colab.research.google.com/drive/1IvCxIg-Q_DlI7UNJuajpl4UZXNiW5jMg</denchmark-link>\n \n Like create model at global scope, and serialize the to(xla_device) calls to avoid all 8 processes rushing into allocation host memory at the same time.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "lezwon", "commentT": "2020-06-12T13:07:48Z", "comment_text": "\n \t\tI also have this issue. if I use GPU, the model is training normally, but when I try to TPU, this happens.\n EDIT: Having analyzed the issue is about the RAM crashing.\n \n I believe this has to do with XLA using up RAM. I constantly use up all my RAM, which causes the\n SIGKILL error. If you take a look at this: pytorch/xla#1280  --- reference Kaggle discussions\n \n <denchmark-code>INIT TPU local core: 0, global rank: 0\n INIT TPU local core: 4, global rank: 4\n INIT TPU local core: 6, global rank: 6\n INIT TPU local core: 3, global rank: 3\n INIT TPU local core: 7, global rank: 7\n INIT TPU local core: 5, global rank: 5\n INIT TPU local core: 2, global rank: 2\n INIT TPU local core: 1, global rank: 1\n \n  \n Validation sanity check:\n 0/? [00:00<?, ?it/s]\n Exception in device=TPU:6: Invalid argument: From /job:tpu_worker/replica:0/task:0:\n 2 root error(s) found.\n   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n \t [[XRTCompile_G3]]\n 0 successful operations.\n 0 derived errors ignored.\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\n     fn(gindex, *args)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 535, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1001, in run_pretrain_routine\n     False)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 256, in _evaluate\n     for batch_idx, batch in enumerate(dataloader):\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 31, in __next__\n     return self.next()\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 37, in next\n     xm.mark_step()\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 536, in mark_step\n     wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n RuntimeError: Invalid argument: From /job:tpu_worker/replica:0/task:0:\n 2 root error(s) found.\n   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n \t [[XRTCompile_G3]]\n 0 successful operations.\n 0 derived errors ignored.\n Exception in device=TPU:1: Invalid argument: From /job:tpu_worker/replica:0/task:0:\n 2 root error(s) found.\n   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n \t [[XRTCompile_G3]]\n 0 successful operations.\n 0 derived errors ignored.\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 231, in _start_fn\n     fn(gindex, *args)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py\", line 535, in tpu_train\n     self.run_pretrain_routine(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1001, in run_pretrain_routine\n     False)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 256, in _evaluate\n     for batch_idx, batch in enumerate(dataloader):\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 31, in __next__\n     return self.next()\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py\", line 37, in next\n     xm.mark_step()\n   File \"/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py\", line 536, in mark_step\n     wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))\n RuntimeError: Invalid argument: From /job:tpu_worker/replica:0/task:0:\n 2 root error(s) found.\n   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n \t [[{{node XRTCompile}}]]\n \t [[XRTCompile_G3]]\n 0 successful operations.\n 0 derived errors ignored.\n ---------------------------------------------------------------------------\n Exception                                 Traceback (most recent call last)\n <ipython-input-29-f6eba0e942ef> in <module>()\n       1 model = hatefull_memesCL()\n       2 if __name__ == '__main__':\n ----> 3     trainer.fit(model)\n \n 3 frames\n /usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py in join(self, timeout)\n     111                 raise Exception(\n     112                     \"process %d terminated with exit code %d\" %\n --> 113                     (error_index, exitcode)\n     114                 )\n     115 \n \n Exception: process 6 terminated with exit code 17 \n </denchmark-code>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "lezwon", "commentT": "2020-06-12T13:45:40Z", "comment_text": "\n \t\tHmm, this is something different:\n <denchmark-code>Invalid argument: Computation requires more parameters (732) than supported (limit 236).\n </denchmark-code>\n \n We have seen that a few time but I keep forgetting what the root cause was.\n It's a misconfiguration of the TPU service, but I do not remember how it can get in that state.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "lezwon", "commentT": "2020-06-12T13:47:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dlibenzi>@dlibenzi</denchmark-link>\n  it is interesting issue, i will let you know if i find the bug\n \t\t"}}}, "commit": {"commit_id": "831842972f7e2d25ae3a376d5584748c3054f899", "commit_author": "Lezwon Castelino", "commitT": "2020-04-23 07:12:54-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "29,30", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "758", "deleted_lines": "758"}}}}}}