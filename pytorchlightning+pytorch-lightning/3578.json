{"BR": {"BR_id": "3578", "BR_author": "carmocca", "BRopenT": "2020-09-21T00:54:24Z", "BRcloseT": "2020-09-25T12:18:07Z", "BR_text": {"BRsummary": "Incorrect \"Saving latest checkpoint\" warning", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n \"Saving latest checkpoint...\" warning appears regardless of whether a ModelCheckpoint exists or save_last is set to True\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 167 to 169\n       in\n       a71d62d\n \n \n \n \n \n \n  # Save latest checkpoint \n \n \n \n  rank_zero_warn('Saving latest checkpoint..') \n \n \n \n  self.check_checkpoint_callback(should_check_val=False, force_save=True) \n \n \n \n \n \n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 196 to 204\n       in\n       a71d62d\n \n \n \n \n \n \n  def check_checkpoint_callback(self, should_check_val, force_save=False): \n \n \n \n  model = self.trainer.get_model() \n \n \n \n  \n \n \n \n  # when no val loop is present or fast-dev-run still need to call checkpoints \n \n \n \n  # TODO bake this logic into the checkpoint callback \n \n \n \n  should_activate = not is_overridden('validation_step', model) and not should_check_val \n \n \n \n  if should_activate or force_save: \n \n \n \n  checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)] \n \n \n \n          [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks] \n \n \n \n \n \n This might confuse an user to think the last checkpoint got saved when it did not.\n <denchmark-h:h2>Proposed change:</denchmark-h>\n \n def check_checkpoint_callback(self, should_check_val, force_save=False):\n     model = self.trainer.get_model()\n \n     # when no val loop is present or fast-dev-run still need to call checkpoints\n     # TODO bake this logic into the checkpoint callback\n     should_activate = not is_overridden('validation_step', model) and not should_check_val\n     if should_activate or force_save:\n         checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)]\n         if any(c.save_last for c in checkpoint_callbacks):\n             rank_zero_warn('Saving latest checkpoint..')\n         [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks]\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "carmocca", "commentT": "2020-09-21T01:15:09Z", "comment_text": "\n \t\twhy not just remove the log line from training_loop and defer logging about saving the latest checkpoint to be within the checkpoint callback? that seems simpler to me\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "carmocca", "commentT": "2020-09-21T01:33:25Z", "comment_text": "\n \t\tBecause the logic to save last is inside of on_validation_end so it would appear after the first validation run\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "carmocca", "commentT": "2020-09-21T12:25:59Z", "comment_text": "\n \t\t\n save_last is set to True\n \n it was meant to save the checkpoint if someone interrupts the training.\n \n regardless of whether a ModelCheckpoint exists\n \n yea, should not log if no ModelCheckpoint is used.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "carmocca", "commentT": "2020-09-21T15:21:39Z", "comment_text": "\n \t\tThanks for the issue <denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>\n  . Mind sending a PR?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "carmocca", "commentT": "2020-09-21T17:35:42Z", "comment_text": "\n \t\tDone!\n \t\t"}}}, "commit": {"commit_id": "ed12e422a42472af1acb88f870dba3d43710b31d", "commit_author": "Carlos Mochol\u00ed", "commitT": "2020-09-25 14:18:06+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "195,197,199,200,201", "deleted_lines": "196,197,198,199,201,202", "method_info": {"method_name": "check_checkpoint_callback", "method_params": "self,should_save,is_last", "method_startline": "195", "method_endline": "202"}}, "hunk_1": {"Ismethod": 1, "added_lines": "670,671,673", "deleted_lines": "674,675", "method_info": {"method_name": "should_check_val_fx", "method_params": "self,batch_idx,is_last_batch", "method_startline": "667", "method_endline": "676"}}, "hunk_2": {"Ismethod": 1, "added_lines": "197,199,200,201", "deleted_lines": "196,197,198,199,201,202", "method_info": {"method_name": "check_checkpoint_callback", "method_params": "self,should_check_val,force_save", "method_startline": "196", "method_endline": "204"}}, "hunk_3": {"Ismethod": 1, "added_lines": "430,501,502", "deleted_lines": "505", "method_info": {"method_name": "run_training_epoch", "method_params": "self", "method_startline": "416", "method_endline": "505"}}, "hunk_4": {"Ismethod": 1, "added_lines": null, "deleted_lines": "39", "method_info": {"method_name": "__init__", "method_params": "self,trainer", "method_startline": "37", "method_endline": "44"}}, "hunk_5": {"Ismethod": 1, "added_lines": null, "deleted_lines": "227,228,229", "method_info": {"method_name": "on_train_epoch_start", "method_params": "self,epoch", "method_startline": "206", "method_endline": "236"}}, "hunk_6": {"Ismethod": 1, "added_lines": "167,168", "deleted_lines": "167,168,169", "method_info": {"method_name": "on_train_end", "method_params": "self", "method_startline": "161", "method_endline": "193"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\callbacks\\test_model_checkpoint.py", "file_new_name": "tests\\callbacks\\test_model_checkpoint.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "266,267,268,269,270,271,272,273,274,275,276,277", "deleted_lines": null, "method_info": {"method_name": "test_model_checkpoint_save_last_warning", "method_params": "tmpdir,caplog,max_epochs,should_validate,save_last", "method_startline": "266", "method_endline": "277"}}}}}}}