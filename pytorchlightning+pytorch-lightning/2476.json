{"BR": {"BR_id": "2476", "BR_author": "aeryen", "BRopenT": "2020-07-02T23:51:29Z", "BRcloseT": "2020-09-23T04:19:47Z", "BR_text": {"BRsummary": "Model and Input not on same GPU when training with native AMP and DP", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Training with distributed_backend='dp' and precision=16 result in the error that some input/output of the model is not on the same GPU.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-code>from pytorch_lightning import Trainer, seed_everything\n from pl_examples.models.lightning_template import LightningTemplateModel\n seed_everything(234)\n \n def main():\n     # model = LightningTemplateModel(**vars(args))\n     model = LightningTemplateModel()\n \n     trainer = Trainer(\n         gpus=2,\n         num_nodes=1,\n         distributed_backend='dp',\n         precision=16\n         )\n \n     trainer.fit(model)\n \n if __name__ == '__main__':\n     main()\n </denchmark-code>\n \n Run the above example code will result in following error:\n <denchmark-code>Traceback (most recent call last):\n   File \"./_debug_pytorch_lightning_16gpu.py\", line 28, in <module>\n     main()\n   File \"./_debug_pytorch_lightning_16gpu.py\", line 24, in main\n     trainer.fit(model)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 973, in fit\n     self.dp_train(model)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 267, in dp_train\n     self.run_pretrain_routine(model)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1139, in run_pretrain_routine\n     False)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 291, in _evaluate\n     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 441, in evaluation_forward\n     output = model(*args)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n     result = self.forward(*input, **kwargs)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 65, in forward\n     outputs = self.parallel_apply(replicas, inputs, kwargs)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 69, in parallel_apply\n     return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 209, in parallel_apply\n     raise output\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py\", line 172, in _worker\n     output = module.validation_step(*input, **kwargs)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pl_examples/models/lightning_template.py\", line 102, in validation_step\n     val_loss = F.cross_entropy(y_hat, y)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/functional.py\", line 2422, in cross_entropy\n     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n   File \"/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/functional.py\", line 2218, in nll_loss\n     ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n RuntimeError: Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /opt/conda/conda-bld/pytorch_1593673617738/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:31\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The model should train successfully on 2 GPUs with native PyTorch AMP.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n - GPU:\n - Tesla V100-SXM2-16GB\n - Tesla V100-SXM2-16GB\n - Tesla V100-SXM2-16GB\n - Tesla V100-SXM2-16GB\n - available:         True\n - version:           10.2\n Packages:\n - numpy:             1.18.5\n - pyTorch_debug:     False\n - pyTorch_version:   1.7.0.dev20200702\n - pytorch-lightning: 0.8.5-dev\n - tensorboard:       2.2.2\n - tqdm:              4.47.0\n System:\n - OS:                Linux\n - architecture:\n - 64bit\n -\n - processor:         x86_64\n - python:            3.7.7\n - version:           #30~18.04.1-Ubuntu SMP Mon Jun 22 15:48:21 UTC 2020\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n This only seems to be an issue with native AMP, not Apex AMP.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "aeryen", "commentT": "2020-07-02T23:52:26Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "aeryen", "commentT": "2020-07-03T00:22:17Z", "comment_text": "\n \t\tI just realized that without precision=16 the script would still fail with a different error\n TypeError: zip argument #1 must support iteration\n Am I doing something wrong? This seems like a separate bug.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "aeryen", "commentT": "2020-07-03T00:28:53Z", "comment_text": "\n \t\tI'm not using native amp yet but apex only works on ddp, and I suspect the native implementation might be as well.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "aeryen", "commentT": "2020-07-03T00:35:58Z", "comment_text": "\n \t\tI see, so it's not supported yet. Thanks for the information. Unfortunately, I have to stick with DP, because PyTorch does not allow gradient checkpointing with DDP. Let me see if I can get this to work.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "aeryen", "commentT": "2020-07-03T00:41:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n  regarding the issue that DP also doesn't work even without precision=16, can you kindly help me confirm that? should I open a separate issue?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "aeryen", "commentT": "2020-07-03T00:50:28Z", "comment_text": "\n \t\tI've never really used dp so I really can't tell you\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "aeryen", "commentT": "2020-07-05T02:22:31Z", "comment_text": "\n \t\tyes that's correct... for some reason, dp and apex do not mix well.\n Maybe <denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>\n  might know why when forward is called with dp the casting isn't automated?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "aeryen", "commentT": "2020-07-05T03:28:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  Hi, sorry, the issue is actually with native amp, not apex. Also, honestly, currently the DP is completely broken, not just when using precision=16. It does feel like people have mostly abandoned DP and moved to DDP in general?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "aeryen", "commentT": "2020-07-05T03:40:49Z", "comment_text": "\n \t\tyeah, dp should not really be used but we can\u2019t remove since some people\u2019s research depends on it.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "aeryen", "commentT": "2020-07-05T03:56:14Z", "comment_text": "\n \t\tSo here is my question (apologies if this is not a good place to discuss this): Isn't the PyTorch gradient checkpointing currently can only work with DP? My current conundrum, I'm trying to work with HuggingFace longformer, and without gradient checkpointing, I can only fit batch size 1 in a single GPU with 16g vram. So the best combo for me right now is Native AMP + DP + gradient checkpointing.\n I guess maybe the long term direction is either just move to TPU and not bother or wait to see if PyTorch can manage to get DDP to work with gradient checkpointing.\n edit: Sorry I'm slow so just realized there was an effort trying to make DDP work better with grad checkpoint, <denchmark-link:https://github.com/pytorch/pytorch/pull/24800>pytorch/pytorch#24800</denchmark-link>\n . Guess I'll stop bothering you guys and wait for that to work...\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "aeryen", "commentT": "2020-07-05T04:55:04Z", "comment_text": "\n \t\ttorch.cuda.amp should support DP, and torch.cuda.amp + DP is tested in pytorch CI, so the error is unexpected.  Do you have a minimal repro that uses raw Pytorch (not lighting)?  This would help us narrow down if the problem lies with DP, torch.cuda.amp, or how lightning combines them.  Also, can you post the complete backtrace?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "aeryen", "commentT": "2020-07-05T05:05:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>\n  My feeling is the issue is within lightning, I've done a rough test with raw Pytorch and haven't been able to reproduce the bug (but could be because I'm bad at this). I've added the trace to the top.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "aeryen", "commentT": "2020-08-03T19:48:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>\n  can you take a look?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "aeryen", "commentT": "2020-08-03T20:38:42Z", "comment_text": "\n \t\tI can look into this but probably not in the next few days, reping if I don't respond by then.  Did you ever get a repro with raw pytorch (not lightning)?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "aeryen", "commentT": "2020-08-03T21:53:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>\n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "aeryen", "commentT": "2020-08-03T21:59:35Z", "comment_text": "\n \t\tNo I still don't have repro for raw pytorch. So I still think it's within lighting.\n I tried to debug it, I even tried manually move the model to each GPU according to the location of the data within my outer forward(). The issue still occur. Looks to me the model's device jump to something else as soon as my sub modules' forward() are being called. I had the impression that threads are not using the model that belongs to them. I'm probably not explaining this as clear as I should...\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "aeryen", "commentT": "2020-08-23T16:30:34Z", "comment_text": "\n \t\tI had (maybe) the same issue with native AMP and DP backend after upgrading from 0.7.6 to 0.9.0.\n It turned out in my case the issue was in DataParallelBackend.setup. Native AMP wrapped the original model's forward function instead of DataParallel.forward.\n When code runs into <denchmark-link:https://github.com/pytorch/pytorch/blob/a97ca93c0e698b81599f7a0ca5cdbda947799431/torch/cuda/amp/autocast_mode.py#L135>this line</denchmark-link>\n , it can be seen that  is always the original model. Then the error occurs when the model and data mismatch.\n I changed the order of wrapping in DataParallelBackend.setup and the problem was solved for me.\n @@ -48,13 +48,13 @@ class DataParallelBackend(object):\n          # hack forward to do autocast for the user\n          self.model_autocast_original_forward = model.forward\n  \n +        # init torch data parallel\n +        model = self.__init_torch_data_parallel(model)\n +\n          # init half precision\n          if self.trainer.amp_backend:\n              model = self.__init_half_precision(model)\n  \n -        # init torch data parallel\n -        model = self.__init_torch_data_parallel(model)\n -\n          self.trainer.model = model\n  \n      def __init_torch_data_parallel(self, model):\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "aeryen", "commentT": "2020-08-23T16:40:27Z", "comment_text": "\n \t\twant to submit a PR?\n <denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>\n , does this make sense?\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "aeryen", "commentT": "2020-08-23T16:46:46Z", "comment_text": "\n \t\tSorry I\u2019m a bit hectic and not up for making a PR.\n \t\t"}}}, "commit": {"commit_id": "031274c25dedc92e383d2715e283a55a2b102d29", "commit_author": "William Falcon", "commitT": "2020-09-23 00:19:46-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pl_examples\\README.md", "file_new_name": "pl_examples\\README.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "2,3,7,8,10,11,12,14,16,17,18,19", "deleted_lines": "2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,35,36,37,38,39,40,41,42,44,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,66,67"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pl_examples\\__init__.py", "file_new_name": "pl_examples\\__init__.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pl_examples\\basic_examples\\README.md", "file_new_name": "pl_examples\\basic_examples\\README.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4,5,7,8,10,11,13,14,15,17,18,19,21,22,24,25,27,28,31,32,33,35,36,38,39,41,42,43,44,57,64", "deleted_lines": "4,6,7,9,10,11,12,13,15,16,17,20,21,23,24,26,27,28,31,32,33,34,35,36,38,39,54,61"}}}, "file_3": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "pl_examples\\basic_examples\\autoencoder.py"}, "file_4": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "pl_examples\\basic_examples\\cpu_template.py", "file_new_name": "None"}, "file_5": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "pl_examples\\basic_examples\\gpu_template.py", "file_new_name": "None"}, "file_6": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "pl_examples\\basic_examples\\image_classifier.py"}, "file_7": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "pl_examples\\basic_examples\\mnist.py"}, "file_8": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "pl_examples\\basic_examples\\multi_node_ddp2_demo.py", "file_new_name": "None"}, "file_9": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "pl_examples\\basic_examples\\multi_node_ddp_demo.py", "file_new_name": "None"}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pl_examples\\basic_examples\\submit_ddp2_job.sh", "file_new_name": "pl_examples\\basic_examples\\submit_ddp2_job.sh", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27", "deleted_lines": "27"}}}, "file_11": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pl_examples\\basic_examples\\submit_ddp_job.sh", "file_new_name": "pl_examples\\basic_examples\\submit_ddp_job.sh", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27", "deleted_lines": "27"}}}, "file_12": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pl_examples\\domain_templates\\semantic_segmentation.py", "file_new_name": "pl_examples\\domain_templates\\semantic_segmentation.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13", "deleted_lines": "13"}}}, "file_13": {"file_change_type": "RENAME", "file_Nmethod": 0, "file_old_name": "pl_examples\\models\\unet.py", "file_new_name": "pl_examples\\domain_templates\\unet.py"}, "file_14": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "pl_examples\\models\\lightning_template.py", "file_new_name": "None"}, "file_15": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "pl_examples\\test_examples.py", "file_new_name": "None"}, "file_16": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\accelerators\\dp_backend.py", "file_new_name": "pytorch_lightning\\accelerators\\dp_backend.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "124,125", "deleted_lines": null, "method_info": {"method_name": "training_step_end", "method_params": "self,output", "method_startline": "121", "method_endline": "126"}}, "hunk_1": {"Ismethod": 1, "added_lines": "131,132", "deleted_lines": null, "method_info": {"method_name": "validation_step_end", "method_params": "self,output", "method_startline": "128", "method_endline": "133"}}, "hunk_2": {"Ismethod": 1, "added_lines": "138,139", "deleted_lines": null, "method_info": {"method_name": "test_step_end", "method_params": "self,output", "method_startline": "135", "method_endline": "140"}}}}, "file_17": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "pytorch_lightning\\overrides\\data_parallel.py", "file_new_name": "pytorch_lightning\\overrides\\data_parallel.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "262,277,280,283,285,286,287,288", "deleted_lines": "246,249,253", "method_info": {"method_name": "parallel_apply", "method_params": "modules,inputs,kwargs_tup,devices", "method_startline": "231", "method_endline": "325"}}, "hunk_1": {"Ismethod": 1, "added_lines": "215,216,217,218,219,220,221,222,223,224,225,226,227,228", "deleted_lines": null, "method_info": {"method_name": "warn_missing_output", "method_params": "fx_called", "method_startline": "215", "method_endline": "228"}}, "hunk_2": {"Ismethod": 1, "added_lines": "334,335,336,337", "deleted_lines": null, "method_info": {"method_name": "auto_squeeze_dim_zeros", "method_params": "output", "method_startline": "328", "method_endline": "344"}}, "hunk_3": {"Ismethod": 1, "added_lines": "262,277,280,283,285,286,287,288", "deleted_lines": null, "method_info": {"method_name": "parallel_apply._worker", "method_params": "i,module,input,kwargs,device", "method_startline": "260", "method_endline": "296"}}, "hunk_4": {"Ismethod": 1, "added_lines": "164,165,177,180,183,207,208,209,210,211", "deleted_lines": null, "method_info": {"method_name": "forward", "method_params": "self,inputs,kwargs", "method_startline": "162", "method_endline": "212"}}}}, "file_18": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "pytorch_lightning\\utilities\\warning_utils.py"}, "file_19": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\base\\develop_utils.py", "file_new_name": "tests\\base\\develop_utils.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "6"}}}, "file_20": {"file_change_type": "RENAME", "file_Nmethod": 0, "file_old_name": "pl_examples\\models\\__init__.py", "file_new_name": "tests\\examples\\__init__.py"}, "file_21": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\examples\\test_examples.py"}}}}