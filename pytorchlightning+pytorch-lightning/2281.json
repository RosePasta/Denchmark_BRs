{"BR": {"BR_id": "2281", "BR_author": "Kshitij09", "BRopenT": "2020-06-19T20:22:20Z", "BRcloseT": "2020-06-20T11:38:48Z", "BR_text": {"BRsummary": "RuntimeError: OrderedDict mutated during iteration", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n I was getting RuntimeError: OrderedDict mutated during iteration.\n It seems like using the same LightningModule object with ModelSummary and Trainer causes this error.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n from pytorch_lightning.core.memory import ModelSummary\n model = CifarNet() # any pl module would work here\n ModelSummary(model,mode='full')\n trainer = Trainer(fast_dev_run=True,gpus=1)\n trainer.fit(model)\n Steps to reproduce the behavior:\n \n View model summary using ModelSummary class\n Call trainer.fit with same object.\n \n <denchmark-h:h3>Stacktrace</denchmark-h>\n \n RuntimeError                              Traceback (most recent call last)\n <ipython-input-20-8badc092c0ba> in <module>()\n       1 # Checking for errors\n       2 trainer = Trainer(fast_dev_run=True,gpus=1)\n ----> 3 trainer.fit(model)\n 11 frames\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\n     916 \n     917         elif self.single_gpu:\n --> 918             self.single_gpu_train(model)\n     919 \n     920         elif self.use_tpu:  # pragma: no-cover\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)\n     174             self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)\n     175 \n --> 176         self.run_pretrain_routine(model)\n     177 \n     178     def tpu_train(self, tpu_core_idx, model):\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\n    1091 \n    1092         # CORE TRAINING LOOP\n -> 1093         self.train()\n    1094 \n    1095     def test(\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in train(self)\n     373                 # RUN TNG EPOCH\n     374                 # -----------------\n --> 375                 self.run_training_epoch()\n     376 \n     377                 if self.max_steps and self.max_steps == self.global_step:\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)\n     456             # RUN TRAIN STEP\n     457             # ---------------\n --> 458             _outputs = self.run_training_batch(batch, batch_idx)\n     459             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs\n     460 \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)\n     632 \n     633                 # calculate loss\n --> 634                 loss, batch_output = optimizer_closure()\n     635 \n     636                 # check if loss or model weights are nan\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()\n     596                                                                     opt_idx, self.hiddens)\n     597                         else:\n --> 598                             output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)\n     599 \n     600                         # format and reduce outputs accordingly\n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)\n     771             batch = self.transfer_batch_to_gpu(batch, gpu_id)\n     772             args[0] = batch\n --> 773             output = self.model.training_step(*args)\n     774 \n     775         # TPU support\n <ipython-input-11-2482ebcf9d12> in training_step(self, batch, batch_idx)\n      55   def training_step(self,batch,batch_idx):\n      56     x, y = batch\n ---> 57     y_hat = self(x)\n      58 \n      59     return {'loss': F.cross_entropy(y_hat, y)}\n /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n     548             result = self._slow_forward(*input, **kwargs)\n     549         else:\n --> 550             result = self.forward(*input, **kwargs)\n     551         for hook in self._forward_hooks.values():\n     552             hook_result = hook(self, input, result)\n <ipython-input-11-2482ebcf9d12> in forward(self, x)\n      11 \n      12   def forward(self,x):\n ---> 13     return self.model(x)\n      14 \n      15   def prepare_data(self):\n /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n     549         else:\n     550             result = self.forward(*input, **kwargs)\n --> 551         for hook in self._forward_hooks.values():\n     552             hook_result = hook(self, input, result)\n     553             if hook_result is not None:\n RuntimeError: OrderedDict mutated during iteration\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n We should be able to use same object with both the classes\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>* CUDA:\n \t- GPU:\n \t\t- Tesla T4\n \t- available:         True\n \t- version:           10.1\n * Packages:\n \t- numpy:             1.18.5\n \t- pyTorch_debug:     False\n \t- pyTorch_version:   1.5.0+cu101\n \t- pytorch-lightning: 0.8.1\n \t- tensorboard:       2.2.2\n \t- tqdm:              4.41.1\n * System:\n \t- OS:                Linux\n \t- architecture:\n \t\t- 64bit\n \t\t- \n \t- processor:         x86_64\n \t- python:            3.6.9\n \t- version:           #1 SMP Wed Feb 19 05:26:34 PST 2020\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Kshitij09", "commentT": "2020-06-19T20:23:01Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Kshitij09", "commentT": "2020-06-19T21:31:22Z", "comment_text": "\n \t\t\n model = CifarNet() # any pl module would work here\n \n Could you paste the minimal code for CifarNet? I cannot reproduce it with PL examples, sorry.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Kshitij09", "commentT": "2020-06-19T22:59:56Z", "comment_text": "\n \t\tOkay ! I'm not sure which part is pertaining to this issue, so here is the link to my <denchmark-link:https://colab.research.google.com/drive/13ER3opHF3IacEfAyEWojuplBUHn2MBKU?usp=sharing>colab notebook</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Kshitij09", "commentT": "2020-06-20T08:48:54Z", "comment_text": "\n \t\tThanks, your notebook was very helpful. I fixed the bug here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2298>#2298</denchmark-link>\n \n You can verify that it works by installing from\n !pip install --upgrade git+https://github.com/awaelchli/pytorch-lightning@bugfix/summary_hook_handles timm wandb\n in the first cell of your notebook.\n \t\t"}}}, "commit": {"commit_id": "f972ab3a828eae1847a793da0b2c25c6074647a4", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-06-20 07:38:47-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "21,22", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "pytorch_lightning\\core\\memory.py", "file_new_name": "pytorch_lightning\\core\\memory.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "198,199", "deleted_lines": null, "method_info": {"method_name": "summarize", "method_params": "self", "method_startline": "194", "method_endline": "200"}}, "hunk_1": {"Ismethod": 1, "added_lines": "61,63,64,66,67,68,76", "deleted_lines": "69,74,78", "method_info": {"method_name": "_register_hook", "method_params": "self", "method_startline": "61", "method_endline": "78"}}, "hunk_2": {"Ismethod": 1, "added_lines": "93", "deleted_lines": null, "method_info": {"method_name": "out_size", "method_params": "self", "method_startline": "93", "method_endline": "94"}}, "hunk_3": {"Ismethod": 1, "added_lines": "76", "deleted_lines": "74", "method_info": {"method_name": "_register_hook.hook", "method_params": "module,inp,out", "method_startline": "71", "method_endline": "76"}}, "hunk_4": {"Ismethod": 1, "added_lines": "80,81,82,83,84,85,86", "deleted_lines": null, "method_info": {"method_name": "detach_hook", "method_params": "self", "method_startline": "80", "method_endline": "86"}}, "hunk_5": {"Ismethod": 1, "added_lines": "58,59", "deleted_lines": "59", "method_info": {"method_name": "__del__", "method_params": "self", "method_startline": "58", "method_endline": "59"}}, "hunk_6": {"Ismethod": 1, "added_lines": "89", "deleted_lines": null, "method_info": {"method_name": "in_size", "method_params": "self", "method_startline": "89", "method_endline": "90"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\core\\test_memory.py", "file_new_name": "tests\\core\\test_memory.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "96,97,98,99,100,101,102,103", "deleted_lines": null, "method_info": {"method_name": "test_hooks_removed_after_summarize", "method_params": "mode", "method_startline": "96", "method_endline": "103"}}}}}}}