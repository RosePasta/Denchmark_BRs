{"BR": {"BR_id": "4953", "BR_author": "rakhimovv", "BRopenT": "2020-12-02T20:09:05Z", "BRcloseT": "2020-12-07T19:31:56Z", "BR_text": {"BRsummary": "manual_optimization does not work with ddp", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Can't run ddp with manual optimization. Fails on the second batch with a error:\n RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the forwardfunction. Please make sure model parameters are not shared across multiple concurrent forward-backward passes2) Reused parameters in multiple reentrant backward passes. For example, if you use multiplecheckpoint functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases yet.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Change optimization to manual in basic gan bolt.\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Do not fail when n_gpus > 1\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n \n GPU:\n \n Tesla V100-SXM2-16GB\n Tesla V100-SXM2-16GB\n Tesla V100-SXM2-16GB\n Tesla V100-SXM2-16GB\n \n \n available:         True\n version:           10.2\n \n \n Packages:\n \n numpy:             1.19.4\n pyTorch_debug:     True\n pyTorch_version:   1.7.0\n pytorch-lightning: 1.0.8\n tqdm:              4.54.0\n \n \n System:\n \n OS:                Linux\n architecture:\n \n 64bit\n \n \n \n processor:         x86_64\n python:            3.7.9\n version:           #1 SMP Tue Sep 10 10:50:19 EDT 2019\n \n \n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n To have manual optimization working with GANs in multi-gpu regime is very useful applicaiton.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "rakhimovv", "commentT": "2020-12-02T20:09:55Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "rakhimovv", "commentT": "2020-12-02T21:36:43Z", "comment_text": "\n \t\tpossibly related <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4917>#4917</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "rakhimovv", "commentT": "2020-12-03T10:17:15Z", "comment_text": "\n \t\they <denchmark-link:https://github.com/rakhimovv>@rakhimovv</denchmark-link>\n ! We're seeing incorrect behaviour with DDP/manual optimization because backward DDP hooks are not being called correctly within the training step, thus meaning gradients are not being reduced per process.\n Our current short term solution is to assert to prevent users from using DDP/manual optimization by enforcing an assert. It will probably take some time for us to come up with a elegant fix for this!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "rakhimovv", "commentT": "2020-12-03T12:24:44Z", "comment_text": "\n \t\tHeu <denchmark-link:https://github.com/rakhimovv>@rakhimovv</denchmark-link>\n ,\n I noticed this bug too with pytorch 1.7.\n Can you try 1.6 ?\n Best,\n T.C\n \t\t"}}}, "commit": {"commit_id": "239347435029c0a02b305201ebbfa39d62746ca8", "commit_author": "chaton", "commitT": "2020-12-07 19:31:54+00:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "benchmarks\\test_sharded_parity.py", "file_new_name": "benchmarks\\test_sharded_parity.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "184,192,193,194", "deleted_lines": "185,193,194", "method_info": {"method_name": "training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "178", "method_endline": "196"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\accelerators\\accelerator.py", "file_new_name": "pytorch_lightning\\accelerators\\accelerator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "226,227,228,229,230,231,232,233", "deleted_lines": null, "method_info": {"method_name": "block_ddp_plugin_sync_behaviour", "method_params": "self", "method_startline": "226", "method_endline": "233"}}, "hunk_1": {"Ismethod": 1, "added_lines": "90,91,92,93,94,95,107,108,109,110", "deleted_lines": null, "method_info": {"method_name": "backward", "method_params": "self,closure_loss,optimizer,opt_idx,args,kwargs", "method_startline": "89", "method_endline": "111"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\overrides\\data_parallel.py", "file_new_name": "pytorch_lightning\\overrides\\data_parallel.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "218,219", "deleted_lines": null, "method_info": {"method_name": "reducer_reset_hooks", "method_params": "self", "method_startline": "218", "method_endline": "219"}}, "hunk_1": {"Ismethod": 1, "added_lines": "205,206", "deleted_lines": "208,209,210", "method_info": {"method_name": "reducer_prepare_for_backwards", "method_params": "self,output", "method_startline": "205", "method_endline": "216"}}, "hunk_2": {"Ismethod": 1, "added_lines": "164,198,199,200,201,202,203", "deleted_lines": null, "method_info": {"method_name": "forward", "method_params": "self,inputs,kwargs", "method_startline": "162", "method_endline": "203"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\plugins\\ddp_plugin.py", "file_new_name": "pytorch_lightning\\plugins\\ddp_plugin.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "146,147", "deleted_lines": null, "method_info": {"method_name": "on_before_manual_backward", "method_params": "self,LightningDistributedDataParallel,Any", "method_startline": "146", "method_endline": "147"}}, "hunk_1": {"Ismethod": 1, "added_lines": "149,150", "deleted_lines": null, "method_info": {"method_name": "on_after_manual_backward", "method_params": "self,LightningDistributedDataParallel", "method_startline": "149", "method_endline": "150"}}, "hunk_2": {"Ismethod": 1, "added_lines": "138,139,140,141,142,143,144", "deleted_lines": null, "method_info": {"method_name": "block_backward_sync", "method_params": "self,LightningDistributedDataParallel", "method_startline": "138", "method_endline": "144"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\plugins\\sharded_plugin.py", "file_new_name": "pytorch_lightning\\plugins\\sharded_plugin.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101,102", "deleted_lines": null, "method_info": {"method_name": "on_after_manual_backward", "method_params": "self", "method_startline": "101", "method_endline": "102"}}, "hunk_1": {"Ismethod": 1, "added_lines": "98,99", "deleted_lines": null, "method_info": {"method_name": "on_before_manual_backward", "method_params": "self,Any", "method_startline": "98", "method_endline": "99"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "682,683,685,686,687,688,689,690", "deleted_lines": "682,684,746,747", "method_info": {"method_name": "run_training_batch", "method_params": "self,batch,batch_idx,dataloader_idx", "method_startline": "640", "method_endline": "748"}}, "hunk_1": {"Ismethod": 1, "added_lines": "752,753,754,755,756,757,758,759,760,761,762,763,764,765,767", "deleted_lines": null, "method_info": {"method_name": "block_ddp_sync_behaviour", "method_params": "self", "method_startline": "751", "method_endline": "767"}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\special_tests.sh", "file_new_name": "tests\\special_tests.sh", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15,17", "deleted_lines": "14,15"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 13, "file_old_name": "tests\\trainer\\optimization\\test_manual_optimization.py", "file_new_name": "tests\\trainer\\optimization\\test_manual_optimization.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp", "method_params": "mock_sgd_step,mock_adam_step,tmpdir", "method_startline": "912", "method_endline": "1013"}}, "hunk_1": {"Ismethod": 1, "added_lines": "928,929,930", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.manual_sync_grad", "method_params": "self", "method_startline": "928", "method_endline": "930"}}, "hunk_2": {"Ismethod": 1, "added_lines": "932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "932", "method_endline": "977"}}, "hunk_3": {"Ismethod": 1, "added_lines": "964,965,966,967", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.dis_closure", "method_params": "", "method_startline": "964", "method_endline": "967"}}, "hunk_4": {"Ismethod": 1, "added_lines": "920,921,922", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.loss_ones", "method_params": "self,batch,prediction", "method_startline": "920", "method_endline": "922"}}, "hunk_5": {"Ismethod": 1, "added_lines": "924,925,926", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.loss_zeros", "method_params": "self,batch,prediction", "method_startline": "924", "method_endline": "926"}}, "hunk_6": {"Ismethod": 1, "added_lines": "943,944,945,946,947,948,949,950", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.compute_loss", "method_params": "", "method_startline": "943", "method_endline": "950"}}, "hunk_7": {"Ismethod": 1, "added_lines": "959,960,961,962", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.gen_closure", "method_params": "", "method_startline": "959", "method_endline": "962"}}, "hunk_8": {"Ismethod": 1, "added_lines": "983,984,985,986", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.configure_optimizers", "method_params": "self", "method_startline": "983", "method_endline": "986"}}, "hunk_9": {"Ismethod": 1, "added_lines": "866", "deleted_lines": "865", "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies", "method_params": "mock_sgd_step,mock_adam_step,tmpdir", "method_startline": "832", "method_endline": "905"}}, "hunk_10": {"Ismethod": 1, "added_lines": "979,980,981", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.training_epoch_end", "method_params": "self,outputs", "method_startline": "979", "method_endline": "981"}}, "hunk_11": {"Ismethod": 1, "added_lines": "952,953,954,955,956,957", "deleted_lines": null, "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.make_manual_backward", "method_params": "loss,opt,retain_graph", "method_startline": "952", "method_endline": "957"}}, "hunk_12": {"Ismethod": 1, "added_lines": "866", "deleted_lines": "865", "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies.training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "839", "method_endline": "873"}}}}}}}