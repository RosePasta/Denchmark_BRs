{"BR": {"BR_id": "241", "BR_author": "samhumeau", "BRopenT": "2019-09-21T16:05:45Z", "BRcloseT": "2019-10-02T15:10:41Z", "BR_text": {"BRsummary": "In Multi GPU DDP, pytorch-lightning creates several tfevents files", "BRdescription": "\n Describe the bug\n Right now pytorch-lightning seems to create several tfevent files in the multi-gpu ddp way:\n e.g. for 2 GPUs:\n <denchmark-code>-rw-rw-r--. 1 sam sam   40 Sep 19 08:11 events.out.tfevents.1568880714.google2-compute82.3156.0\n -rw-rw-r--. 1 sam sam 165K Sep 19 08:22 events.out.tfevents.1568880716.google2-compute82.3186.0\n -rw-rw-r--. 1 sam sam   40 Sep 19 08:11 events.out.tfevents.1568880718.google2-compute82.3199.0\n </denchmark-code>\n \n I suppose the first one is created by the main process and the next 2 are created by the 2 DDP processes (one per GPU). Unfortunately, the actual events are not logged in the last created one, and that confuses tensorboard, cf <denchmark-link:https://github.com/tensorflow/tensorboard/issues/1011>tensorflow/tensorboard#1011</denchmark-link>\n \n I have to restart tensorboard if I want to see the new data.\n A clear and concise description of what the bug is.\n To Reproduce\n Launch any training on multi GPU DDP.\n Expected behavior\n Only one tfevent file is created, from the master GPU.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "samhumeau", "commentT": "2019-09-21T16:12:20Z", "comment_text": "\n \t\tthanks for bringing this up. this has been reported a few times already. the problem is what you described.\n the solution is to init the logger from proc zero only. want to take a look at how we can approach this? <denchmark-link:https://github.com/neggert>@neggert</denchmark-link>\n  is working on an abstraction that will need this fix\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "samhumeau", "commentT": "2019-09-23T16:24:54Z", "comment_text": "\n \t\tAre you thinking we should make sure that the test tube experiment doesn't even get initialized unless we're on process 0? Right now I have it initialize, but never log, but that's easy enough to change.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "samhumeau", "commentT": "2019-09-23T16:28:12Z", "comment_text": "\n \t\tyeah, i think the best thing is to make sure it\u2019s only initialized once. This will save a ton of space in the experiment file as well\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "samhumeau", "commentT": "2019-09-27T19:19:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  Starting to take a look at this. This turns out to affect the MLFlow logger as well when doing multi-node DDP. I think what I'd like to do is make constructing the experiment / MLFlow run inside the logger lazy, so that it doesn't get created until a method that needs it is called.\n Once consequence of this is that users shouldn't call log_hyperparams themselves, since that will happen on multiple nodes in multi-node DDP. To make up for this, we should call it for them when they do training. I'm thinking we check to see if they've defined model.hparams, and if so, we can log for them. In code, it looks like adding this to __run_pretrain_routine:\n <denchmark-code>if hasattr(ref_model, \"hparams\"):\n     self.logger.log_hyperparams(ref_model.hparams)\n </denchmark-code>\n \n Thoughts? I guess we should document somewhere that we're expecting a hparams attribute, although I think most of the examples follow that convention already.\n (Side note: <denchmark-link:https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#save-a-snapshot-of-all-hyperparameters>the docs claim this is already done automatically</denchmark-link>\n , but I don't see it in the code anywhere.)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "samhumeau", "commentT": "2019-09-27T19:59:04Z", "comment_text": "\n \t\tMakes sense, i think the hparams makes the most sense. I wonder if there's a way to automatically do it even if users don't define hparams. Maybe argparse has some sort of global state we can inspect? or look at vars in the current frame? I'd love to remove the need for users to remember to have to use hparams.\n I'm converned about people who don't use argparse and/or init their models  using actual args.\n Case 1:\n MyObj(lr=0.1, ..., arg_2=0.3)\n Case 2:\n MyObj(hparams)\n Case 3:\n MyObj(hparams, lr=0.1, ...)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "samhumeau", "commentT": "2019-09-28T00:11:19Z", "comment_text": "\n \t\tYeah, I'd definitely welcome other ideas that would cover those cases. Maybe ask users to define hparams as a property if they're not doing case 2, but they still want lightning to log their parameters for them?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "samhumeau", "commentT": "2019-10-21T16:08:44Z", "comment_text": "\n \t\tin 0.5.1.3 multiple tfevents files are still being created with ddp (I'm not logging any hparams if that matters), did that PR fix it? or is there more work to be done?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "samhumeau", "commentT": "2019-10-21T16:11:46Z", "comment_text": "\n \t\tAre you interacting with the logger manually at all before training starts? Are you doing single-node or multi-node DDP?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "samhumeau", "commentT": "2019-10-22T02:56:08Z", "comment_text": "\n \t\tsingle node, and the only time I manually call logger is in optimizer step (self.logger.log_metrics) otherwise I only return log entries in training step and validation end\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "samhumeau", "commentT": "2019-11-05T08:48:06Z", "comment_text": "\n \t\tI removed that call and I'm still getting multiple tfevents, no other calls to logging besides metrics returned by train and val steps. Currently using the experimental --reload_multifile=true in tensorboard to get around the issue.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "samhumeau", "commentT": "2020-05-11T08:40:12Z", "comment_text": "\n \t\tis this problem solved?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "samhumeau", "commentT": "2020-07-02T07:43:50Z", "comment_text": "\n \t\tI see same problem, why was this closed?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "samhumeau", "commentT": "2020-07-02T08:19:33Z", "comment_text": "\n \t\tmultiple tfevents files are still created but tensorboard updates made this a non-issue for me with everything displaying and updating correctly\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "samhumeau", "commentT": "2020-07-02T08:37:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n \n Thank you for your answer.\n What version of tensorboard are you using?\n I'm using tensorboard-2.2.1, but when I set logdir to a folder that contains multiple tfevents, I get the following error:\n \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "samhumeau", "commentT": "2020-07-02T08:44:30Z", "comment_text": "\n \t\tI'm also on 2.2.1 but I'm using  within the <denchmark-link:https://ngc.nvidia.com/catalog/containers/nvidia:pytorch>ngc pytorch container</denchmark-link>\n  so I don't manually setup logdir\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "samhumeau", "commentT": "2020-07-02T08:51:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n \n Oh, It (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/241#issuecomment-549722983>#241 (comment)</denchmark-link>\n ) meant adding --reload_multifile= true to the tensorboard line!\n I solved my problem. Thank you very much.\n I would like to ask you another question.\n Is there no error if you don't specify a Trainer logger using pl_loggers.TensorBoardLogger()?\n Unless I specify the logger of Trainer() separately, the error that tensorboard path already exists will occur.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "samhumeau", "commentT": "2020-07-02T08:58:30Z", "comment_text": "\n \t\tI don't remember what I did back then to get logging working... but currently I use TestTubeLogger and call it in trainer as logger=TestTubeLogger(\".\", \"lightning_logs\")\n this logs losses/metrics and hparams in self.hparams correctly (this method logs hparams under TEXT in tensorboard)\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "samhumeau", "commentT": "2020-07-02T10:05:57Z", "comment_text": "\n \t\tare you guys on 0.8.4?\n <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "samhumeau", "commentT": "2020-07-04T23:49:10Z", "comment_text": "\n \t\tmultiple tfevent files does not mean they come from differnt gpus, it's just a tensorboard thing.\n 0.8.4 logs only on rank 0.\n previously we had the problem that other ranks would log as well, this would lead to multiple directories (version0, version1, ...) but this is fixed now.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "samhumeau", "commentT": "2020-07-04T23:50:49Z", "comment_text": "\n \t\tif you manually log things, then do this:\n if self.trainer.is_global_zero:\n     # your custom non-Lightning logging\n \t\t"}}}, "commit": {"commit_id": "614cb3c03bd0894238b3197f3b7f904656f284f4", "commit_author": "Nic Eggert", "commitT": "2019-10-02 11:10:40-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\Trainer\\Logging.md", "file_new_name": "docs\\Trainer\\Logging.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "122,125,126,127,128,129,130,131,132,133,134,135,136", "deleted_lines": "122,124,125,127"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\logging\\mlflow_logger.py", "file_new_name": "pytorch_lightning\\logging\\mlflow_logger.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "19,20,21,23,26,28,29,32,33", "deleted_lines": "20,22,23,26", "method_info": {"method_name": "run_id", "method_params": "self", "method_startline": "19", "method_endline": "33"}}, "hunk_1": {"Ismethod": 1, "added_lines": "15,16", "deleted_lines": "12", "method_info": {"method_name": "__init__", "method_params": "self,experiment_name,tracking_uri", "method_startline": "12", "method_endline": "16"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "pytorch_lightning\\logging\\test_tube_logger.py", "file_new_name": "pytorch_lightning\\logging\\test_tube_logger.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "71,72,73,74", "deleted_lines": null, "method_info": {"method_name": "version", "method_params": "self", "method_startline": "70", "method_endline": "74"}}, "hunk_1": {"Ismethod": 1, "added_lines": "22,23,24,25,26,27,28,29,30,31,33", "deleted_lines": null, "method_info": {"method_name": "experiment", "method_params": "self", "method_startline": "22", "method_endline": "33"}}, "hunk_2": {"Ismethod": 1, "added_lines": "57,58,59,60", "deleted_lines": "60", "method_info": {"method_name": "rank", "method_params": "self", "method_startline": "56", "method_endline": "60"}}, "hunk_3": {"Ismethod": 1, "added_lines": "64,65,66,67", "deleted_lines": "64,65", "method_info": {"method_name": "rank", "method_params": "self,value", "method_startline": "63", "method_endline": "67"}}, "hunk_4": {"Ismethod": 1, "added_lines": "82", "deleted_lines": null, "method_info": {"method_name": "__getstate__", "method_params": "self", "method_startline": "80", "method_endline": "83"}}, "hunk_5": {"Ismethod": 1, "added_lines": "86,87", "deleted_lines": null, "method_info": {"method_name": "__setstate__", "method_params": "self,state", "method_startline": "85", "method_endline": "88"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "893,894,895", "deleted_lines": "893,894", "method_info": {"method_name": "__run_pretrain_routine", "method_params": "self,model", "method_startline": "855", "method_endline": "926"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\test_logging.py", "file_new_name": "tests\\test_logging.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "22,23", "method_info": {"method_name": "test_testtube_logger", "method_params": "", "method_startline": "13", "method_endline": "35"}}}}}}}