{"BR": {"BR_id": "1510", "BR_author": "alexeykarnachev", "BRopenT": "2020-04-16T16:25:24Z", "BRcloseT": "2020-04-19T20:41:55Z", "BR_text": {"BRsummary": "Memory (CPU and GPU) leaks during the 1st epoch", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Hello.\n This memory leak occurs during the first epoch. If one has a large epoch time (I had > 10 days), the OOM error will come. It's interesting, that in precision=16 mode, it leaks out on the GPU and the CPU both. If we switch amp optimization off (precision=32), the leak goes only on the CPU.\n Also, I checked the number of tensors, which are tracked by the garbage collector. And it appeared to be linearly increasing during the first epoch, and then (on the 2nd epoch starts), it falls to the initial value and begins increasing again.\n Let me provide the plots:\n <denchmark-h:hr></denchmark-h>\n \n Experiment 1: amp_level='O2', precision=16\n <denchmark-link:https://user-images.githubusercontent.com/7495098/79478408-0cc28f80-8014-11ea-861b-1b9443de5351.png></denchmark-link>\n \n \n <denchmark-link:https://user-images.githubusercontent.com/7495098/79478523-324f9900-8014-11ea-8e05-c9ef20b1c4d8.png></denchmark-link>\n \n \n <denchmark-link:https://user-images.githubusercontent.com/7495098/79478667-5ad79300-8014-11ea-8727-545230c81649.png></denchmark-link>\n \n \n <denchmark-h:hr></denchmark-h>\n \n Experiment 2: amp_level=None, precision=None\n <denchmark-link:https://user-images.githubusercontent.com/7495098/79478906-a4c07900-8014-11ea-80f6-9284bf86eafe.png></denchmark-link>\n \n \n <denchmark-link:https://user-images.githubusercontent.com/7495098/79478952-b4d85880-8014-11ea-849c-f38c3e1a88ce.png></denchmark-link>\n \n \n <denchmark-link:https://user-images.githubusercontent.com/7495098/79478999-c588ce80-8014-11ea-8755-4d2a2d7451d5.png></denchmark-link>\n \n \n <denchmark-h:hr></denchmark-h>\n \n As you can see, both cases have a CPU leak. The \"amp\"-case also has a GPU leak.\n Also, it's clear, that such leaky behavior stops when the 2nd epoch starts.\n On these plots, the 2nd epoch starts on the 2nd \"saw claw\" of the \"Num-of-tensors\" plot.\n Also, there is another observation: the speed of tensors number increasing is 1001. And this is my forward pass method:\n     def training_step(self, batch, batch_idx):\n         losses = self.forward(batch)\n         num_of_tensors = get_num_of_tensors()\n         log = {'Num-of-tensors': num_of_tensors, 'Cpu-mem-usg': get_cpu_mem()}\n \n         for i, loss in enumerate(losses):\n             log[f'loss{i}'] = loss\n \n         print(num_of_tensors)\n         return {'loss': losses[0], 'log': log}\n Here I return exactly 1001 tensor: one for loss and 1000 for log.\n In my real experiments I had only 3 tensors. It took ~2-3 days to get OOM. But in the current example (see To Reproduce) it will crash much faster.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Execute Code sample (this script has no arguments, so change needed values manually in script).\n Go to the tensorboard to check plots.\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-link:https://gist.github.com/alexeykarnachev/47de06b93a717ab0664eded42ed2826a>https://gist.github.com/alexeykarnachev/47de06b93a717ab0664eded42ed2826a</denchmark-link>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The number of tensors, GPU and CPU memory does not increase during the training.\n <denchmark-h:h3>Environment</denchmark-h>\n \n PyTorch version: 1.4.0\n OS: Ubuntu 16.04.6 LTS\n Python version: 3.7\n Versions of relevant libraries:\n [pip] numpy==1.18.1\n [pip] pytorch-lightning==0.7.3\n [pip] torch==1.4.0\n [pip] torchvision==0.5.0\n <denchmark-h:h3>Additional context</denchmark-h>\n \n Sorry for so messy flow of the information, but I don't know, how to structure it more clearly.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "alexeykarnachev", "commentT": "2020-04-16T16:35:20Z", "comment_text": "\n \t\tby leak you mean tensors build up during epoch 1? but after that the memory stays constant? ie: there is no more \"leak\" for epochs >= 2?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "alexeykarnachev", "commentT": "2020-04-16T16:41:11Z", "comment_text": "\n \t\tYes, the memory stays constant after 1st epoch ends (although, the number of tensors begins increasing again)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "alexeykarnachev", "commentT": "2020-04-16T20:06:55Z", "comment_text": "\n \t\tThe whole output of a training step is stored.\n In your code with every training step, there are new tensors created.\n With z log[f'loss{i}'] = loss.item()there is no leak.\n I think there is a mistake in optimizer_closure() in the training loop which, returns whole batch output dict. It should be enough to return only callback_metrics instead of the whole batch output.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "alexeykarnachev", "commentT": "2020-04-16T20:47:48Z", "comment_text": "\n \t\tYes, I agreed, that with .item() there is no leak because all tensors \"disappear in place\" (I did not check it, but I believe that it so). But, I suppose, that .item() will slow my code.\n On the other hand, .item() is performed anyway by the Trainer itself (before logging), so maybe it's not a big deal to call .item() beforehand. At least as a hotfix solution\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "alexeykarnachev", "commentT": "2020-04-16T22:47:18Z", "comment_text": "\n \t\tOh, no sorry, just checked: it will be a leak even if we perform log[f'loss{i}'] = loss.item()\n Because we still have 'loss': losses[0] part (the actual loss tensor, which needs to be minimized).\n So, it will be a leak with speed 1 tensor per step. It's very slow, but the OOM will occur anyway in 6-9 days\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T00:33:33Z", "comment_text": "\n \t\tcan you submit a PR? i thought we took care of all the metrics.\n we should also use detach instead of item no? to not slow code down\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T06:20:31Z", "comment_text": "\n \t\tWe take care of it in process_output() but then in optimizer_closure() we return original output_dict again.\n We pass then a list of original outputs to the training_epoch_end().\n I think w should not do that bc loss, log and progress_bar is handling by us in a proper way so we should return to training_epoch_end only other keys from output_dict and let a user manage it.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T08:41:19Z", "comment_text": "\n \t\tWhat about fp32-mode? There is no leak on the GPU in such a case. What could be the reason?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T11:36:23Z", "comment_text": "\n \t\t@AratorField , do you mean this?\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 427 to 428\n       in\n       9b31272\n \n \n \n \n \n \n  # bookkeeping \n \n \n \n  outputs = [] \n \n \n \n \n \n Here is a list that stores all train step outputs during the epoch.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T11:47:21Z", "comment_text": "\n \t\tbut we detach everything.\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n          Line 448\n       in\n       9b31272\n \n \n \n \n \n \n  outputs.append(_recursive_detach(batch_output)) \n \n \n \n \n \n how could it leak?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T11:55:28Z", "comment_text": "\n \t\tYes, but they (tensors) are still on the GPU after detach. So, in case of long epochs or huge outputs from the training step, the GPU memory will blow after some time.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T12:39:52Z", "comment_text": "\n \t\tWe can create something like _recursive_item() or remove keys loss, log, progress_bar from batch_output before appending to outputs.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T13:45:17Z", "comment_text": "\n \t\tIs it in general a good practice to store values during the epoch? The size of such a bookkeeping list is undetermined in the general case. I mean, that one could have almost an infinite epoch and sooner or later he'll be faced with OOM (GPU or CPU, it does not matter).\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T13:59:01Z", "comment_text": "\n \t\tthe thing is that .item() slows things down.\n so we want to detach but not .item().\n The tradeoff is that we plug the memory leak but slow things down.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T14:03:27Z", "comment_text": "\n \t\tThere is no reason to store loss, log and progress_bar for the whole epoch.\n Any other key in output_dict could be valuable and has to be stored i.e. for metrics calculating.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T14:11:32Z", "comment_text": "\n \t\tMaybe it's possible to introduce a flag, which shows, should we store tensors in this list during an epoch or not.\n Or, maybe you can advise me some hot-fix, that I can apply locally. Because now, I can not train even 1 epoch :)\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T14:16:33Z", "comment_text": "\n \t\tI even have no training_epoch_end method. Maybe, we can check if this method is not determined by the user, we can skip batch results bookkeeping?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T14:28:42Z", "comment_text": "\n \t\t\n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n          Line 611\n       in\n       8544b33\n \n \n \n \n \n \n  return closure_loss, output_dict \n \n \n \n \n \n change it to return closure_loss , callback_metrics\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "alexeykarnachev", "commentT": "2020-04-17T14:34:48Z", "comment_text": "\n \t\tThank you, I'll patch it locally for now.\n \t\t"}}}, "commit": {"commit_id": "ae2e14e3ed45e23dbe2868017b630fa7be9e5604", "commit_author": "William Falcon", "commitT": "2020-04-19 16:41:54-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pl_examples\\basic_examples\\cpu_template.py", "file_new_name": "pl_examples\\basic_examples\\cpu_template.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "31", "deleted_lines": "31", "method_info": {"method_name": "main", "method_params": "hparams", "method_startline": "18", "method_endline": "36"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "pytorch_lightning\\callbacks\\early_stopping.py", "file_new_name": "pytorch_lightning\\callbacks\\early_stopping.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "73,74,75,76,77,78,79,80", "deleted_lines": "72", "method_info": {"method_name": "check_metrics", "method_params": "self,logs", "method_startline": "72", "method_endline": "86"}}, "hunk_1": {"Ismethod": 1, "added_lines": "75,76,77,78,79,80", "deleted_lines": "92", "method_info": {"method_name": "_validate_condition_metric", "method_params": "self,logs", "method_startline": "75", "method_endline": "94"}}, "hunk_2": {"Ismethod": 1, "added_lines": "100", "deleted_lines": "97", "method_info": {"method_name": "on_train_start", "method_params": "self,trainer,pl_module", "method_startline": "96", "method_endline": "100"}}, "hunk_3": {"Ismethod": 1, "added_lines": null, "deleted_lines": "46", "method_info": {"method_name": "__init__", "method_params": "self,str,float,int,bool,str,bool", "method_startline": "46", "method_endline": "47"}}, "hunk_4": {"Ismethod": 1, "added_lines": "49", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,str,float,int,bool,str,bool", "method_startline": "49", "method_endline": "50"}}, "hunk_5": {"Ismethod": 1, "added_lines": "105,109,110,111", "deleted_lines": null, "method_info": {"method_name": "on_epoch_end", "method_params": "self,trainer,pl_module", "method_startline": "102", "method_endline": "122"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_new_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "213,214,215", "deleted_lines": "206", "method_info": {"method_name": "on_validation_end", "method_params": "self,trainer,pl_module", "method_startline": "186", "method_endline": "224"}}, "hunk_1": {"Ismethod": 1, "added_lines": "142,143,144,145", "deleted_lines": null, "method_info": {"method_name": "check_monitor_top_k", "method_params": "self,current", "method_startline": "138", "method_endline": "146"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_new_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "416,417", "deleted_lines": null, "method_info": {"method_name": "run_evaluation", "method_params": "self,bool", "method_startline": "326", "method_endline": "417"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\logging.py", "file_new_name": "pytorch_lightning\\trainer\\logging.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "177,178,179", "deleted_lines": "176,177,178,179", "method_info": {"method_name": "process_output", "method_params": "self,output,train", "method_startline": "98", "method_endline": "181"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "894,896", "deleted_lines": "895", "method_info": {"method_name": "run_pretrain_routine", "method_params": "self,LightningModule", "method_startline": "806", "method_endline": "909"}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "800,801", "deleted_lines": "803", "method_info": {"method_name": "call_checkpoint_callback", "method_params": "self", "method_startline": "800", "method_endline": "803"}}, "hunk_1": {"Ismethod": 1, "added_lines": "799,800,801", "deleted_lines": null, "method_info": {"method_name": "call_early_stop_callback", "method_params": "self", "method_startline": "799", "method_endline": "801"}}, "hunk_2": {"Ismethod": 1, "added_lines": "448,449,450,451,452,470,471,472,476,477,507,516,519", "deleted_lines": "447,448,482,483,484,485,486,487,488,489,490,491,508,509,510,519,522,523,524", "method_info": {"method_name": "run_training_epoch", "method_params": "self", "method_startline": "405", "method_endline": "527"}}, "hunk_3": {"Ismethod": 1, "added_lines": "606", "deleted_lines": "611", "method_info": {"method_name": "run_training_batch", "method_params": "self,batch,batch_idx", "method_startline": "529", "method_endline": "664"}}, "hunk_4": {"Ismethod": 1, "added_lines": null, "deleted_lines": "819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842", "method_info": {"method_name": "_recursive_detach", "method_params": "in_dict", "method_startline": "819", "method_endline": "842"}}, "hunk_5": {"Ismethod": 1, "added_lines": "606", "deleted_lines": null, "method_info": {"method_name": "run_training_batch.optimizer_closure", "method_params": "", "method_startline": "573", "method_endline": "606"}}}}, "file_7": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "pytorch_lightning\\utilities\\memory_utils.py"}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\base\\utils.py", "file_new_name": "tests\\base\\utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "26", "deleted_lines": "26", "method_info": {"method_name": "assert_speed_parity", "method_params": "pl_times,pt_times,num_epochs", "method_startline": "23", "method_endline": "33"}}}}}}}