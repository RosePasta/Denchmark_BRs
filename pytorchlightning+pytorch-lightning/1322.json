{"BR": {"BR_id": "1322", "BR_author": "VitorGuizilini-TRI", "BRopenT": "2020-03-31T17:35:57Z", "BRcloseT": "2020-04-05T15:07:17Z", "BR_text": {"BRsummary": "Training loop temporarily hangs after every 4 steps", "BRdescription": "\n I am porting some of my code to pytorch lightning, and everything seems to work fine. However, for some reason after every 4 training steps I see some temporary hanging (~1 second), which is severely slowing down my overall training time. Am I missing some obvious configuration?  This is my Trainer configuration:\n <denchmark-code>    trainer = pl.Trainer(\n         gpus=8\n         num_nodes=1,\n         distributed_backend='ddp',\n         checkpoint_callback=False,\n         max_epochs=50,\n         max_steps=None,\n         progress_bar_refresh_rate=1,\n         check_val_every_n_epoch=1,\n         val_check_interval=1.0,\n         gradient_clip_val=0.0,\n         log_save_interval=0,\n         num_sanity_val_steps=0,\n         amp_level='O0',\n     )\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "VitorGuizilini-TRI", "commentT": "2020-03-31T17:36:45Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "VitorGuizilini-TRI", "commentT": "2020-04-04T12:34:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "VitorGuizilini-TRI", "commentT": "2020-04-04T12:39:03Z", "comment_text": "\n \t\tThanks for the issue! Would it be possible to post the code that reproduces this error? I've only seen this sort of behaviour before when the number of data loading workers is low - are you working with large data here (e.g. big images)?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "VitorGuizilini-TRI", "commentT": "2020-04-04T16:24:16Z", "comment_text": "\n \t\tI increased the number of workers and it works perfectly now, thank you very much! You can close this issue.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "VitorGuizilini-TRI", "commentT": "2020-04-04T16:27:14Z", "comment_text": "\n \t\tshould we throw a warning when users use few workers?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "VitorGuizilini-TRI", "commentT": "2020-04-04T16:34:04Z", "comment_text": "\n \t\tIf possible, sure! Seems like an obvious solution now, but it could save a couple of hours for other people. :)\n \t\t"}}}, "commit": {"commit_id": "b18accc64ccd24095c11fdbd64cc924456134592", "commit_author": "Ethan Harris", "commitT": "2020-04-05 11:07:16-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "29", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\data_loading.py", "file_new_name": "pytorch_lightning\\trainer\\data_loading.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "77,78,79,80,81", "deleted_lines": null, "method_info": {"method_name": "_worker_check", "method_params": "self,DataLoader,str", "method_startline": "77", "method_endline": "81"}}, "hunk_1": {"Ismethod": 1, "added_lines": "122,128", "deleted_lines": null, "method_info": {"method_name": "reset_train_dataloader", "method_params": "self,LightningModule", "method_startline": "114", "method_endline": "162"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\trainer\\test_dataloaders.py", "file_new_name": "tests\\trainer\\test_dataloaders.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527", "deleted_lines": null, "method_info": {"method_name": "test_warning_with_few_workers", "method_params": "tmpdir", "method_startline": "489", "method_endline": "527"}}}}}}}