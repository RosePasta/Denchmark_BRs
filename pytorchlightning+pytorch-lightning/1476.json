{"BR": {"BR_id": "1476", "BR_author": "rmrao", "BRopenT": "2020-04-13T17:57:32Z", "BRcloseT": "2020-04-20T12:03:53Z", "BR_text": {"BRsummary": "Learning rate scheduler should step after each optimizer step", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n I'm not sure that this is a bug or if it is a deliberate design decision, but right now the learning rate schedule gets updated at every \"step\" which actually corresponds to every forward pass. I think a more standard implementation would have the learning rate scheduler \"step\" interval correspond to being updated every backwards pass. This has caused me a lot of problems with instability as I did not realize that using standard learning rate warmups of say 16000 steps would actually only warm up for 1000 steps if I set accumulate_grad_batches=16.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "rmrao", "commentT": "2020-04-13T18:00:27Z", "comment_text": "\n \t\tgood point. it should be every backward pass as you mention.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "rmrao", "commentT": "2020-04-13T18:00:37Z", "comment_text": "\n \t\tmind submitting  PR?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "rmrao", "commentT": "2020-04-13T18:06:12Z", "comment_text": "\n \t\tSure, will do.\n \t\t"}}}, "commit": {"commit_id": "0203938af8f69a19b7e0264f18e03d543d86e0e9", "commit_author": "Roshan Rao", "commitT": "2020-04-20 08:03:52-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27,28,34,39,41", "deleted_lines": "29,33,38,40,55"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "457,458,459,460,461", "deleted_lines": "457,458", "method_info": {"method_name": "run_training_epoch", "method_params": "self", "method_startline": "405", "method_endline": "530"}}}}}}}