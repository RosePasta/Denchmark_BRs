{"BR": {"BR_id": "1893", "BR_author": "robert-wagner", "BRopenT": "2019-02-10T03:01:10Z", "BRcloseT": "2019-10-28T09:24:08Z", "BR_text": {"BRsummary": "Moving a module to cuda throws errors", "BRdescription": "\n <denchmark-code>import numpy as np                                                                                                                                         \n import torch                                                                                                                                               \n import torch.utils.data as data                                                                                                                            \n import torch.nn as nn                                                                                                                                      \n import torch.nn.functional as F                                                                                                                            \n import torch.optim as optim                                                                                                                                \n from torchvision import datasets, transforms                                                                                                               \n                                                                                                                                                            \n import torch                                                                                                                                               \n from torch import nn                                                                                                                                       \n from torch import optim                                                                                                                                    \n from torchvision.datasets.mnist import MNIST                                                                                                               \n import pdb                                                                                                                                                 \n                                                                                                                                                            \n import syft as sy                                                                                                                                          \n class Net(nn.Module):                                                                                                                                      \n     def __init__(self):                                                                                                                                    \n         super(Net, self).__init__()                                                                                                                        \n         self.conv1 = nn.Conv2d(1, 20, 5, 1)                                                                                                                \n         self.conv2 = nn.Conv2d(20, 50, 5, 1)                                                                                                               \n         self.fc1 = nn.Linear(4*4*50, 500)                                                                                                                  \n         self.fc2 = nn.Linear(500, 10)                                                                                                                      \n                                                                                                                                                            \n     def forward(self, x):                                                                                                                                  \n         x = F.relu(self.conv1(x))                                                                                                                          \n         x = F.max_pool2d(x, 2, 2)                                                                                                                          \n         x = F.relu(self.conv2(x))                                                                                                                          \n         x = F.max_pool2d(x, 2, 2)                                                                                                                          \n         x = x.view(-1, 4*4*50)                                                                                                                             \n         x = F.relu(self.fc1(x))                                                                                                                            \n         x = self.fc2(x)                                                                                                                                    \n         return F.log_softmax(x, dim=1)                                                                                                                     \n                                                                                                                                                            \n                                                                                                                                                            \n hook = sy.TorchHook(torch)                                                                                                                                 \n device = torch.device(\"cuda\")                                                                                                                              \n model = Net().to(device)                                                                                                                                   \n print(model) ```\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "robert-wagner", "commentT": "2019-02-11T10:07:03Z", "comment_text": "\n \t\tCan you add the stacktrace?\n One good-first-issue would be in the case you have only cpu, and try to call .to(device), to fix the error which is only due to us not serializing devices. This could be easily handled.\n The part with the gpu and cuda is not part of the good-first-issue.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "robert-wagner", "commentT": "2019-02-12T15:24:00Z", "comment_text": "\n \t\t<denchmark-code>Traceback (most recent call last):\n   File \"test.py\", line 37, in <module>\n     model = Net().to(device)                                                                                                                                   \n   File \"/network/home/maloneyj/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 381, in to\n     return self._apply(convert)\n   File \"/network/home/maloneyj/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 187, in _apply\n     module._apply(fn)\n   File \"/network/home/maloneyj/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 193, in _apply\n     param.data = fn(param.data)\n   File \"/network/home/maloneyj/PySyft/syft/frameworks/torch/hook.py\", line 339, in data\n     self.native_param_data.set_(new_data)  # .wrap()\n RuntimeError: Expected object of backend CPU but got backend CUDA for argument #2 'source'\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "robert-wagner", "commentT": "2019-03-03T19:47:03Z", "comment_text": "\n \t\tin data(): new_data is cuda whereas native_param_data is cpu\n fix should be moving new_data to cpu while setting native_param_data\n Please correct me if I'm wrong\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "robert-wagner", "commentT": "2019-04-01T19:41:31Z", "comment_text": "\n \t\tFYI, for the code snippet above I'm able to fix the error adding this line after importing torch:\n torch.set_default_tensor_type(torch.cuda.FloatTensor)\n Not sure if this is ideal for every case...\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "robert-wagner", "commentT": "2019-04-01T21:47:59Z", "comment_text": "\n \t\t\n FYI, for the code snippet above I'm able to fix the error adding this line after importing torch:\n torch.set_default_tensor_type(torch.cuda.FloatTensor)\n Not sure if this is ideal for every case...\n \n <denchmark-link:https://github.com/mari-linhares>@mari-linhares</denchmark-link>\n  nice finding. This is changing default tensor type and should be considered as a work around for now. What we are truely looking for is to to data transfer over cuda\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "robert-wagner", "commentT": "2019-04-02T19:56:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bhushan23>@bhushan23</denchmark-link>\n  do you know why changing the tensor to FloatTensor works? I'm not using my work computer right now, but I assume since there's no error that the data is transferred over to cuda. I'll check when I get home.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "robert-wagner", "commentT": "2019-04-02T20:17:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mari-linhares>@mari-linhares</denchmark-link>\n \n try changing  with  you will be able to reproduce original error.\n set_default_tensor_type creates tensors of specified type and I think, setting to cuda float tensor is leading to ensuring hook is also using cuda.floattensor and hence no error as both new_data and native_param_data are on cuda\n Ref: <denchmark-link:https://pytorch.org/docs/stable/torch.html#torch.set_default_tensor_type>https://pytorch.org/docs/stable/torch.html#torch.set_default_tensor_type</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "robert-wagner", "commentT": "2019-05-14T22:16:28Z", "comment_text": "\n \t\tThis issue is breaking <denchmark-link:examples/tutorials/Part%208%20-%20Federated%20Learning%20on%20MNIST%20using%20a%20CNN.ipynb>tutorial 8</denchmark-link>\n  at the moment when used with CUDA.\n [Python 3.7, PySyft from master branch]\n I've tried to apply <denchmark-link:https://github.com/mari-linhares>@mari-linhares</denchmark-link>\n  's <denchmark-link:https://github.com/OpenMined/PySyft/issues/1893#issuecomment-478717757>workaround</denchmark-link>\n  to no success so far:\n \n when set after distributing the dataset, the instruction crashes with IndexError: list index out of range\n \n <denchmark-code>---------------------------------------------------------------------------\n KeyError                                  Traceback (most recent call last)\n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in hook_function_args(attr, args, kwargs, return_args_type)\n     135         # TODO rename registry or use another one than for methods\n --> 136         hook_args = hook_method_args_functions[attr]\n     137         get_tensor_type_function = get_tensor_type_functions[attr]\n \n KeyError: 'torch.set_default_tensor_type'\n \n During handling of the above exception, another exception occurred:\n \n IndexError                                Traceback (most recent call last)\n <ipython-input-6-fb50db14b868> in <module>\n       3 if device.type == \"cuda\":\n       4   os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n ----> 5   torch.set_default_tensor_type(torch.cuda.FloatTensor)\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/hook/hook.py in overloaded_func(*args, **kwargs)\n     691             cmd_name = f\"{attr.__module__}.{attr.__name__}\"\n     692             command = (cmd_name, None, args, kwargs)\n --> 693             response = TorchTensor.handle_func_command(command)\n     694             return response\n     695 \n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/tensors/interpreters/native.py in handle_func_command(cls, command)\n     191             # Note that we return also args_type which helps handling case 3 in the docstring\n     192             new_args, new_kwargs, new_type, args_type = syft.frameworks.torch.hook_args.hook_function_args(\n --> 193                 cmd, args, kwargs, return_args_type=True\n     194             )\n     195             # This handles case 3: it redirects the command to the appropriate class depending\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in hook_function_args(attr, args, kwargs, return_args_type)\n     141     except (IndexError, KeyError, AssertionError):  # Update the function in case of an error\n     142         args_hook_function, get_tensor_type_function = build_hook_args_function(\n --> 143             args, return_tuple=True\n     144         )\n     145         # Store the utility functions in registries\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in build_hook_args_function(args, return_tuple)\n     171     # Build a function with this rule to efficiently the child type of the\n     172     # tensor found in the args\n --> 173     get_tensor_type_function = build_get_tensor_type(rule)\n     174     return args_hook_function, get_tensor_type_function\n     175 \n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/hook/hook_args.py in build_get_tensor_type(rules, layer)\n     392 \n     393     if first_layer:\n --> 394         return lambdas[0]\n     395     else:\n     396         return lambdas\n \n IndexError: list index out of range\n </denchmark-code>\n \n \n when set right after import torch, data distribution fails with RuntimeError: expected type torch.FloatTensor but got torch.cuda.FloatTensor\n \n <denchmark-code>RuntimeError                              Traceback (most recent call last)\n <ipython-input-8-4085cd6569bc> in <module>\n       5                        transforms.Normalize((0.1307,), (0.3081,))\n       6                    ]))\n ----> 7     .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n       8     batch_size=args.batch_size, shuffle=True, **kwargs)\n       9 \n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/federated/dataset.py in dataset_federate(dataset, workers)\n      89     datasets = []\n      90     data_loader = torch.utils.data.DataLoader(dataset, batch_size=data_size)\n ---> 91     for dataset_idx, (data, targets) in enumerate(data_loader):\n      92         worker = workers[dataset_idx % len(workers)]\n      93         logger.debug(\"Sending data to worker %s\", worker.id)\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\n     613         if self.num_workers == 0:  # same-process loading\n     614             indices = next(self.sample_iter)  # may raise StopIteration\n --> 615             batch = self.collate_fn([self.dataset[i] for i in indices])\n     616             if self.pin_memory:\n     617                 batch = pin_memory_batch(batch)\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py in <listcomp>(.0)\n     613         if self.num_workers == 0:  # same-process loading\n     614             indices = next(self.sample_iter)  # may raise StopIteration\n --> 615             batch = self.collate_fn([self.dataset[i] for i in indices])\n     616             if self.pin_memory:\n     617                 batch = pin_memory_batch(batch)\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/torchvision/datasets/mnist.py in __getitem__(self, index)\n      93 \n      94         if self.transform is not None:\n ---> 95             img = self.transform(img)\n      96 \n      97         if self.target_transform is not None:\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/torchvision/transforms/transforms.py in __call__(self, img)\n      58     def __call__(self, img):\n      59         for t in self.transforms:\n ---> 60             img = t(img)\n      61         return img\n      62 \n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/torchvision/transforms/transforms.py in __call__(self, tensor)\n     161             Tensor: Normalized Tensor image.\n     162         \"\"\"\n --> 163         return F.normalize(tensor, self.mean, self.std, self.inplace)\n     164 \n     165     def __repr__(self):\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/torchvision/transforms/functional.py in normalize(tensor, mean, std, inplace)\n     206     mean = torch.tensor(mean, dtype=torch.float32)\n     207     std = torch.tensor(std, dtype=torch.float32)\n --> 208     tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n     209     return tensor\n     210 \n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/hook/hook.py in overloaded_native_method(self, *args, **kwargs)\n     637                 except BaseException as e:\n     638                     # we can make some errors more descriptive with this method\n --> 639                     raise route_method_exception(e, self, args, kwargs)\n     640 \n     641             else:  # means that there is a wrapper to remove\n \n /home/xxx/PySyft/venv/lib/python3.7/site-packages/syft-0.1.14a1-py3.7.egg/syft/frameworks/torch/hook/hook.py in overloaded_native_method(self, *args, **kwargs)\n     631                 try:\n     632                     if isinstance(args, tuple):\n --> 633                         response = method(*args, **kwargs)\n     634                     else:\n     635                         response = method(args, **kwargs)\n \n RuntimeError: expected type torch.FloatTensor but got torch.cuda.FloatTensor\n </denchmark-code>\n \n This might also break other tutorials when using GPUs, have not tried yet.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "robert-wagner", "commentT": "2019-05-26T11:19:05Z", "comment_text": "\n \t\tDisclaimer: I needed this running (asap) to train a large-ish dataset on GPU, so it's more like a hacky workaround than an actual solution, but I'll be happy if it helps anyone with the same problem. Or can be used to build up a proper fix.\n So I was experiencing the same problem as <denchmark-link:https://github.com/jopasserat>@jopasserat</denchmark-link>\n  . I think the problem was that the function that handles function commands in hooks.py needed to convert a  to a  , but this exception was not contemplated. So basically I am catching the  that happens in those cases.\n Apart from that I'm using . I tried to convert the non-cuda tensors to cuda but for some reason in the wrapped tensors  method doesn't seem to work for me. So unless I set it to default I get the problem <denchmark-link:https://github.com/bhushan23>@bhushan23</denchmark-link>\n  was mentioning. Plus, I need to set the  (I think the method has been overwritten so it does not run anymore) and  (as that will only work with dense CPU tensors).\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "robert-wagner", "commentT": "2019-08-01T09:39:47Z", "comment_text": "\n \t\t\n FYI, for the code snippet above I'm able to fix the error adding this line after importing torch:\n torch.set_default_tensor_type(torch.cuda.FloatTensor)\n Not sure if this is ideal for every case...\n \n I am so glad to see this, it  really solved my problem of worrying all afternoon\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "robert-wagner", "commentT": "2019-09-02T15:44:48Z", "comment_text": "\n \t\tI found a workaround that worked for my needs. The trick is to hook pysyft after you move your model to cuda.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "robert-wagner", "commentT": "2019-09-03T03:42:32Z", "comment_text": "\n \t\t\n I found a workaround that worked for my needs. The trick is to hook pysyft after you move your model to cuda.\n \n what do you mean\uff1f Hook the worker\uff1fBut if you hook after these, how can you make federated dataset? For example,in the tutorial example>advanced>CIFAR10. Can you post your code\n \t\t"}}}, "commit": {"commit_id": "3d5ae6e186d0bad6f52af651aab891ff50c3ac11", "commit_author": "yeggasd", "commitT": "2019-10-28 10:24:07+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "examples\\tutorials\\Part 06 - Federated Learning on MNIST using a CNN.ipynb", "file_new_name": "examples\\tutorials\\Part 06 - Federated Learning on MNIST using a CNN.ipynb", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "44,45,476", "deleted_lines": "474"}}}}}}