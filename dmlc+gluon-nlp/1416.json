{"BR": {"BR_id": "1416", "BR_author": "preeyank5", "BRopenT": "2020-10-29T20:58:56Z", "BRcloseT": "2020-10-31T00:33:15Z", "BR_text": {"BRsummary": "Issue in formatting the text wikipedia files", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n I tried to run the code block that formats the downloaded the wikipedia text files. Link - <denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/datasets/pretrain_corpus/README.md#wikipedia>https://github.com/dmlc/gluon-nlp/blob/master/scripts/datasets/pretrain_corpus/README.md#wikipedia</denchmark-link>\n  , but got an error. Even though the WikiExtractor.py file is present in the mentioned directory.\n <denchmark-h:h3>Error Message</denchmark-h>\n \n ImportError: Cannot import WikiExtractor! You can download the \"WikiExtractor.py\" in <denchmark-link:https://github.com/attardi/wikiextractor>https://github.com/attardi/wikiextractor</denchmark-link>\n  to /home/ec2-user/gluon-nlp/gluon-nlp/scripts/datasets/pretrain_corpus/gluon-nlp/scripts/datasets/pretrain_corpus\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n (If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)\n <denchmark-h:h3>Steps to reproduce</denchmark-h>\n \n <denchmark-h:h3>Create mxnet2.0 python 3.6 environment</denchmark-h>\n \n conda create -n mxnet2_p36 python=3.6\n source activate mxnet2_p36\n <denchmark-h:h3>Check Cuda version</denchmark-h>\n \n nvcc --version\n <denchmark-h:h3>Install mxnet-cu100 2.0</denchmark-h>\n \n python3 -m pip install -U --pre \"mxnet-cu100>=2.0.0b20200926\" -f <denchmark-link:https://dist.mxnet.io/python>https://dist.mxnet.io/python</denchmark-link>\n \n <denchmark-h:h3>Git clone from gluon nlp</denchmark-h>\n \n git clone -b master <denchmark-link:https://github.com/dmlc/gluon-nlp.git>https://github.com/dmlc/gluon-nlp.git</denchmark-link>\n \n <denchmark-h:h3>cd to gluon-nlp</denchmark-h>\n \n cd gluon-nlp/\n <denchmark-h:h3>Install gluon-nlp</denchmark-h>\n \n python3 -m pip install -U -e .\"[extras]\"\n <denchmark-h:h3>Check nlp_data</denchmark-h>\n \n nlp_data help\n <denchmark-h:h3>Check nlp_process</denchmark-h>\n \n nlp_process help\n <denchmark-h:h1>Download Hindi Wikipedia corpus</denchmark-h>\n \n python3 prepare_wikipedia.py --mode download --lang hi --date latest -o ./\n <denchmark-h:h1>Properly format the text files</denchmark-h>\n \n python3 prepare_wikipedia.py --mode format -i [path-to-wiki.xml.bz2] -o ./\n Trying on Sagemaker ml.t2.medium instance.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "preeyank5", "commentT": "2020-10-29T21:05:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ZheyuYe>@ZheyuYe</denchmark-link>\n  Would you have time to take a look?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "preeyank5", "commentT": "2020-10-31T00:33:15Z", "comment_text": "\n \t\tAs confirmed by <denchmark-link:https://github.com/preeyank5>@preeyank5</denchmark-link>\n  , this has been fixed by <denchmark-link:https://github.com/dmlc/gluon-nlp/pull/1417>#1417</denchmark-link>\n  so I've closed the issue.\n \t\t"}}}, "commit": {"commit_id": "e82e0a7c907d19b5642f5425495ae9f5cb57b743", "commit_author": "Xingjian Shi", "commitT": "2020-10-29 21:31:23-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "scripts\\datasets\\general_nlp_benchmark\\prepare_glue.py", "file_new_name": "scripts\\datasets\\general_nlp_benchmark\\prepare_glue.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "237,242", "deleted_lines": "237", "method_info": {"method_name": "read_rte", "method_params": "dir_path", "method_startline": "233", "method_endline": "243"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "scripts\\datasets\\pretrain_corpus\\README.md", "file_new_name": "scripts\\datasets\\pretrain_corpus\\README.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "30,31,33,34,35,38,39,40,41,42,43,44,45,47,48,49,51", "deleted_lines": "31,32,35"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "scripts\\datasets\\pretrain_corpus\\prepare_wikipedia.py", "file_new_name": "scripts\\datasets\\pretrain_corpus\\prepare_wikipedia.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "171,172,210,211", "deleted_lines": "171,180,183", "method_info": {"method_name": "format_wikicorpus", "method_params": "input,output,bytes,num_process,num_out_files", "method_startline": "171", "method_endline": "213"}}, "hunk_1": {"Ismethod": 1, "added_lines": "135,136", "deleted_lines": null, "method_info": {"method_name": "get_parser", "method_params": "", "method_startline": "106", "method_endline": "137"}}, "hunk_2": {"Ismethod": 1, "added_lines": "210,211,214,215", "deleted_lines": "221,224", "method_info": {"method_name": "main", "method_params": "args", "method_startline": "205", "method_endline": "231"}}, "hunk_3": {"Ismethod": 1, "added_lines": null, "deleted_lines": "67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83", "method_info": {"method_name": "try_import_wikiextractor", "method_params": "", "method_startline": "67", "method_endline": "83"}}, "hunk_4": {"Ismethod": 1, "added_lines": "156,165,166,168,170,171,172", "deleted_lines": "171,180,183", "method_info": {"method_name": "format_wikicorpus", "method_params": "input,output,bytes,num_process,num_out_files,quiet", "method_startline": "156", "method_endline": "202"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "setup.py", "file_new_name": "setup.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "88", "deleted_lines": null}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\gluonnlp\\utils\\lazy_imports.py", "file_new_name": "src\\gluonnlp\\utils\\lazy_imports.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "172,173,174,175,176,177,178,179,180,181", "deleted_lines": null, "method_info": {"method_name": "try_import_wikiextractor", "method_params": "", "method_startline": "172", "method_endline": "181"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\README.md", "file_new_name": "tests\\README.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "42,43", "deleted_lines": "42,43"}}}, "file_6": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\data_cli\\test_glue.py"}, "file_7": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\data_cli\\test_wikipedia.py"}}}}