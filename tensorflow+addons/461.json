{"BR": {"BR_id": "461", "BR_author": "guillaumekln", "BRopenT": "2019-08-29T16:57:43Z", "BRcloseT": "2019-09-10T01:02:54Z", "BR_text": {"BRsummary": "Setting a custom attention_layer fails with AttentionMechanism without memory", "BRdescription": "\n System information\n \n Have I written custom code: Yes\n OS Platform and Distribution: Ubuntu 16.04\n TensorFlow installed from: binary\n TensorFlow version: 2.0.0rc0\n TensorFlow Addons installed from: PyPi\n TensorFlow Addons version: 0.5.0.dev20190829\n Python version and type: 3.6\n \n Describe the bug\n When creating an AttentionMechanism without a memory and then creating an AttentionWrapper with a custom attention_layer, an error is raised.\n Describe the expected behavior\n No error should be raised.\n Code to reproduce the issue\n import tensorflow as tf\n import tensorflow_addons as tfa\n \n units = 32\n attention_mechanism = tfa.seq2seq.LuongAttention(units)\n cell = tf.keras.layers.LSTMCell(units)\n attention_layer = tf.keras.layers.Dense(\n     units, use_bias=False, activation=tf.math.tanh)\n attention_wrapper = tfa.seq2seq.AttentionWrapper(\n     cell, attention_mechanism, attention_layer=attention_layer)\n Other info / logs\n <denchmark-code>  File \"/lib/python3.6/site-packages/tensorflow_addons/seq2seq/attention_wrapper.py\", line 1698, in <genexpr>\n     ])[-1]) for layer, mechanism in zip(\n AttributeError: 'NoneType' object has no attribute 'shape'\n </denchmark-code>\n \n The code tries to compute the attention layer output shape and for that, it uses attention_mechanism.values which is None at the time the AttentionWrapper constructor is called.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "guillaumekln", "commentT": "2019-08-30T06:03:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>\n  Qianli, could you take a look? Thanks.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "guillaumekln", "commentT": "2019-08-30T12:24:16Z", "comment_text": "\n \t\tThis one is a bit tricky. A possible fix is to lazily compute . However, in the usage introduced by <denchmark-link:https://github.com/tensorflow/addons/pull/354>#354</denchmark-link>\n , it will not work until  is called. We could just raise an exception in that case saying that the user should call this method first.\n \t\t"}}}, "commit": {"commit_id": "6633c43afb6bdd66b43933a2221847b4d273bc87", "commit_author": "Guillaume Klein", "commitT": "2019-09-09 21:02:53-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "file_new_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1812", "deleted_lines": null, "method_info": {"method_name": "state_size", "method_params": "self", "method_startline": "1802", "method_endline": "1819"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1745", "deleted_lines": null, "method_info": {"method_name": "_batch_size_checks", "method_params": "self,batch_size,error_message", "method_startline": "1744", "method_endline": "1752"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1735,1736,1737,1738,1739,1740,1741,1742", "deleted_lines": null, "method_info": {"method_name": "_attention_mechanisms_checks", "method_params": "self", "method_startline": "1735", "method_endline": "1742"}}, "hunk_3": {"Ismethod": 1, "added_lines": "1871", "deleted_lines": "1844", "method_info": {"method_name": "get_initial_state", "method_params": "self,inputs,batch_size,dtype", "method_startline": "1821", "method_endline": "1884"}}, "hunk_4": {"Ismethod": 1, "added_lines": "1797", "deleted_lines": null, "method_info": {"method_name": "output_size", "method_params": "self", "method_startline": "1795", "method_endline": "1799"}}, "hunk_5": {"Ismethod": 1, "added_lines": "1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772", "deleted_lines": "1770", "method_info": {"method_name": "_get_attention_layer_size", "method_params": "self", "method_startline": "1754", "method_endline": "1772"}}, "hunk_6": {"Ismethod": 1, "added_lines": "142,143,144,145,146", "deleted_lines": null, "method_info": {"method_name": "memory_initialized", "method_params": "self", "method_startline": "142", "method_endline": "146"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\seq2seq\\attention_wrapper_test.py", "file_new_name": "tensorflow_addons\\seq2seq\\attention_wrapper_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271", "deleted_lines": null, "method_info": {"method_name": "testCustomAttentionLayer", "method_params": "self", "method_startline": "251", "method_endline": "271"}}}}}}}