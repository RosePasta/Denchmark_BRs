{"BR": {"BR_id": "1920", "BR_author": "shkarupa-alex", "BRopenT": "2020-06-09T12:29:36Z", "BRcloseT": "2020-06-16T23:03:51Z", "BR_text": {"BRsummary": "LookAhead + RAdam + mixed_fp16 = apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'", "BRdescription": "\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\n TensorFlow version and how it was installed (source or binary): source\n TensorFlow-Addons version and how it was installed (source or binary): source\n Python version: 3.8.2\n Is GPU used? (yes/no): yes\n \n Describe the bug\n When i set global mixed precision policy tf.keras.mixed_precision.experimental.set_policy('mixed_float16') meta-optimizer LookAhead + RAdam raises error\n Epoch 1/5\n ---------------------------------------------------------------------------\n TypeError                                 Traceback (most recent call last)\n <ipython-input-19-841302a83165> in <module>\n ----> 1 model.fit(\n       2     train_ds,\n       3     epochs=1 if lr_finder else 5,\n       4     callbacks=callbacks,\n       5     steps_per_epoch=findlr_steps if lr_finder else None,\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\n      64   def _method_wrapper(self, *args, **kwargs):\n      65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\n ---> 66       return method(self, *args, **kwargs)\n      67 \n      68     # Running inside `run_distribute_coordinator` already.\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n     846                 batch_size=batch_size):\n     847               callbacks.on_train_batch_begin(step)\n --> 848               tmp_logs = train_function(iterator)\n     849               # Catch OutOfRangeError for Datasets of unknown size.\n     850               # This blocks until the batch has finished executing.\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n     578         xla_context.Exit()\n     579     else:\n --> 580       result = self._call(*args, **kwds)\n     581 \n     582     if tracing_count == self._get_tracing_count():\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n     625       # This is the first call of __call__, so we have to initialize.\n     626       initializers = []\n --> 627       self._initialize(args, kwds, add_initializers_to=initializers)\n     628     finally:\n     629       # At this point we know that the initialization is complete (or less\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\n     503     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\n     504     self._concrete_stateful_fn = (\n --> 505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n     506             *args, **kwds))\n     507 \n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n    2444       args, kwargs = None, None\n    2445     with self._lock:\n -> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n    2447     return graph_function\n    2448 \n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n    2775 \n    2776       self._function_cache.missed.add(call_context_key)\n -> 2777       graph_function = self._create_graph_function(args, kwargs)\n    2778       self._function_cache.primary[cache_key] = graph_function\n    2779       return graph_function, args, kwargs\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n    2655     arg_names = base_arg_names + missing_arg_names\n    2656     graph_function = ConcreteFunction(\n -> 2657         func_graph_module.func_graph_from_py_func(\n    2658             self._name,\n    2659             self._python_function,\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n     979         _, original_func = tf_decorator.unwrap(python_func)\n     980 \n --> 981       func_outputs = python_func(*func_args, **func_kwargs)\n     982 \n     983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\n     439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n     440         # the function a weak reference to itself to avoid a reference cycle.\n --> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n     442     weak_wrapped_fn = weakref.ref(wrapped_fn)\n     443 \n \n ~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n     966           except Exception as e:  # pylint:disable=broad-except\n     967             if hasattr(e, \"ag_error_metadata\"):\n --> 968               raise e.ag_error_metadata.to_exception(e)\n     969             else:\n     970               raise\n \n TypeError: in user code:\n \n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n         outputs = self.distribute_strategy.run(\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n         return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n         return self._call_for_each_replica(fn, args, kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n         return fn(*args, **kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:540 train_step  **\n         _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1810 _minimize\n         optimizer.apply_gradients(\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:245 apply_gradients\n         return distribution_strategy_context.get_replica_context().merge_call(\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call\n         return self._merge_call(merge_fn, args, kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2427 _merge_call\n         return merge_fn(self._strategy, *args, **kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:269 _apply_gradients_cross_replica  **\n         maybe_apply_op = smart_cond.smart_cond(should_apply_grads,\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py:58 smart_cond\n         return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:507 new_func\n         return func(*args, **kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:1177 cond\n         return cond_v2.cond_v2(pred, true_fn, false_fn, name)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/ops/cond_v2.py:78 cond_v2\n         true_graph = func_graph_module.func_graph_from_py_func(\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func\n         func_outputs = python_func(*func_args, **func_kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:261 apply_fn\n         return distribution.extended.call_for_each_replica(\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n         return self._call_for_each_replica(fn, args, kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n         return fn(*args, **kwargs)\n     /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:279 _apply_gradients\n         return self._optimizer.apply_gradients(\n \n     TypeError: apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'\n Code to reproduce the issue\n tf.keras.mixed_precision.experimental.set_policy('mixed_float16')\n # model = ...\n \n optimizer = Lookahead(RectifiedAdam(0.003))\n model.compile(\n     optimizer=optimizer,\n     loss='binary_crossentropy',\n     metrics=['accuracy'],\n     run_eagerly=False,\n )\n \n # model.fit(...)\n Other info / logs\n Addons built from master branch today\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "shkarupa-alex", "commentT": "2020-06-10T20:40:56Z", "comment_text": "\n \t\tusing ranger on tpu in kaggle hangs up (timeout after about 20 minutes)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "shkarupa-alex", "commentT": "2020-06-11T11:27:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/shkarupa-alex>@shkarupa-alex</denchmark-link>\n  Do you have a very minimal dummy  to reproduce this?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "shkarupa-alex", "commentT": "2020-06-12T09:38:11Z", "comment_text": "\n \t\t\n @shkarupa-alex Do you have a very minimal dummy model to reproduce this?\n \n Here it is <denchmark-link:https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing>https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "shkarupa-alex", "commentT": "2020-06-12T09:53:53Z", "comment_text": "\n \t\tCan you enable the access to the colab?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "shkarupa-alex", "commentT": "2020-06-12T10:03:43Z", "comment_text": "\n \t\tSorry. Access granted\n <denchmark-link:https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing>https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "shkarupa-alex", "commentT": "2020-06-12T10:56:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/shkarupa-alex>@shkarupa-alex</denchmark-link>\n  Can you try <denchmark-link:https://github.com/tensorflow/addons/pull/1924>#1924</denchmark-link>\n ?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "shkarupa-alex", "commentT": "2020-06-12T11:53:52Z", "comment_text": "\n \t\tNo error after patch applied\n \t\t"}}}, "commit": {"commit_id": "bd5bbfc66e750cba94360236e7f85481662ba4de", "commit_author": "bhack", "commitT": "2020-06-16 19:03:49-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\optimizers\\lookahead.py", "file_new_name": "tensorflow_addons\\optimizers\\lookahead.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "99,103", "deleted_lines": "99,103", "method_info": {"method_name": "apply_gradients", "method_params": "self,grads_and_vars,name", "method_startline": "99", "method_endline": "103"}}, "hunk_1": {"Ismethod": 1, "added_lines": "99,103", "deleted_lines": "99,103", "method_info": {"method_name": "apply_gradients", "method_params": "self,grads_and_vars,name,kwargs", "method_startline": "99", "method_endline": "103"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\optimizers\\tests\\lookahead_test.py", "file_new_name": "tensorflow_addons\\optimizers\\tests\\lookahead_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146", "deleted_lines": null, "method_info": {"method_name": "test_fit_simple_linear_model_mixed_precision", "method_params": "", "method_startline": "123", "method_endline": "146"}}}}}}}