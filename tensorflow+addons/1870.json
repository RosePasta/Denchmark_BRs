{"BR": {"BR_id": "1870", "BR_author": "avnx", "BRopenT": "2020-05-21T19:11:52Z", "BRcloseT": "2020-05-24T05:31:26Z", "BR_text": {"BRsummary": "Error in types description of tfa.seq2seq.AttentionWrapper", "BRdescription": "\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LT\n TensorFlow version and how it was installed (source or binary): 2.2.0\n TensorFlow-Addons version and how it was installed (source or binary): 0.10.0\n Python version: 3.6.9\n Is GPU used? (yes/no): yes\n \n As I noticed you added type description to classes.\n But I found that at least in my case there is a conflict of types in description with real mission of arguments.\n According to docs attention_mechanism and attention_layer_size in tfa.seq2seq.AttentionWrapper could be lists (for MultiHead attention case) or single instances. But in types you specified that attention_mechanism and attention_layer_size must be tf.keras.layers.Layer and  Optional[FloatTensorLike] respectively.\n <denchmark-link:https://github.com/tensorflow/addons/blob/v0.9.1/tensorflow_addons/seq2seq/attention_wrapper.py#L1621>https://github.com/tensorflow/addons/blob/v0.9.1/tensorflow_addons/seq2seq/attention_wrapper.py#L1621</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "avnx", "commentT": "2020-05-22T07:58:36Z", "comment_text": "\n \t\tThanks for reporting. The description is correct but not the types annotation. The PR referenced above should fix this.\n \t\t"}}}, "commit": {"commit_id": "64ade2170c404c7c6962bbe5e84634fee2095b29", "commit_author": "Guillaume Klein", "commitT": "2020-05-23 22:31:25-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "file_new_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1622,1623,1629,1630", "deleted_lines": "1621,1622,1628", "method_info": {"method_name": "__init__", "method_params": "self,Layer,Layer,None,bool,None,bool,None,None,None,None,kwargs", "method_startline": "1618", "method_endline": "1630"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1622,1623,1629,1630,1631", "deleted_lines": "1621,1622,1628", "method_info": {"method_name": "__init__", "method_params": "self,Layer,Layer,Number,None,bool,None,bool,None,None,Layer,None,None,kwargs", "method_startline": "1619", "method_endline": "1633"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\seq2seq\\tests\\attention_wrapper_test.py", "file_new_name": "tensorflow_addons\\seq2seq\\tests\\attention_wrapper_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "937,938,939,940,941,942,943,944,945,946", "deleted_lines": null, "method_info": {"method_name": "test_attention_wrapper_with_multiple_attention_mechanisms", "method_params": "", "method_startline": "937", "method_endline": "946"}}}}}}}