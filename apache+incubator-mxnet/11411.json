{"BR": {"BR_id": "11411", "BR_author": "safrooze", "BRopenT": "2018-06-27T01:31:27Z", "BRcloseT": "2018-08-21T15:43:02Z", "BR_text": {"BRsummary": "nd.softmax() doesn't support grad_req='add'", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n  does not support adding gradients specified by setting  to . This is obvious in <denchmark-link:https://github.com/apache/incubator-mxnet/blob/a0a52b38a9046a3ede20c900ff64154642e8e2da/src/operator/nn/softmax-inl.h#L267>this line of code</denchmark-link>\n .\n However,  which is deprecated does support  equal to . This issue came up in <denchmark-link:https://discuss.mxnet.io/t/aggregate-gradients-manually-over-n-batches/504/14?u=safrooze>this discuss questions</denchmark-link>\n .\n Package used: Python\n <denchmark-h:h2>Error Message:</denchmark-h>\n \n <denchmark-code>mxnet.base.MXNetError: [18:24:22] src/operator/nn/./softmax-inl.h:267: Check failed: req[0] != kAddTo (3 vs. 3)\n </denchmark-code>\n \n <denchmark-h:h2>Minimum reproducible example</denchmark-h>\n \n import mxnet as mx\n from mxnet import gluon, nd, autograd\n \n print(mx.__version__)\n \n grad_req = 'add'\n \n x = (nd.random.uniform(shape=(2, 10)) - 0.5) * 10\n x_g = nd.zeros_like(x)\n \n autograd.mark_variables(x, x_g, grad_req)\n \n with autograd.record():\n     # Change nd.softmax to nd.SoftmaxActivation and the code doesn't throw an error\n     y = nd.softmax(x)\n y.backward()\n print(x_g.asnumpy())\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "safrooze", "commentT": "2018-06-27T15:54:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/safrooze>@safrooze</denchmark-link>\n  Thank you for submitting the issue! We are labeling it so MXNet community members can help resolve it.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "safrooze", "commentT": "2018-07-12T21:31:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/safrooze>@safrooze</denchmark-link>\n  Taking a look at this now.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "safrooze", "commentT": "2018-07-20T16:41:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/safrooze>@safrooze</denchmark-link>\n  <denchmark-link:https://github.com/apache/incubator-mxnet/pull/11836>#11836</denchmark-link>\n  should fix this issue, I've tried your reproduction script and here's the output from it:\n <denchmark-code>1.3.0\n [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n     0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n     0.00000000e+00   0.00000000e+00]\n  [ -3.20495186e-09  -2.34470221e-10  -3.99168448e-10  -9.83819623e-11\n    -3.74679523e-08  -8.85198338e-12  -7.68905792e-08  -7.67132469e-11\n    -2.32276004e-10  -5.95953564e-10]]\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "safrooze", "commentT": "2018-08-20T22:46:19Z", "comment_text": "\n \t\tThat's great, thanks <denchmark-link:https://github.com/haojin2>@haojin2</denchmark-link>\n !\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "safrooze", "commentT": "2018-08-21T15:43:24Z", "comment_text": "\n \t\tResolving as  <denchmark-link:https://github.com/apache/incubator-mxnet/pull/11836>#11836</denchmark-link>\n  is merged.\n \t\t"}}}, "commit": {"commit_id": "d7b39f4c055a46c1b819b0dff4ae468c1983e016", "commit_author": "Hao Jin", "commitT": "2018-08-20 22:38:08-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\contrib\\ctc_loss-inl.h", "file_new_name": "src\\operator\\contrib\\ctc_loss-inl.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "429", "deleted_lines": "429", "method_info": {"method_name": "mxnet::op::CTCLossOp::cudnn_forward", "method_params": "ctx,s,data,costs,grad,data_lengths,label_lengths,packed_labels,max_seq_len,batch_size,alphabet_size,req_grad", "method_startline": "354", "method_endline": "433"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "src\\operator\\nn\\softmax-inl.h", "file_new_name": "src\\operator\\nn\\softmax-inl.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "137,140,141,145,146", "deleted_lines": "139,143", "method_info": {"method_name": "mxnet::op::mxnet_op::SoftmaxGrad", "method_params": "s,out,ograd,igrad,shape,axis,temperature", "method_startline": "115", "method_endline": "150"}}, "hunk_1": {"Ismethod": 1, "added_lines": "249", "deleted_lines": "244", "method_info": {"method_name": "mxnet::op::mxnet_op::SoftmaxGrad", "method_params": "s,out,ograd,igrad,shape,axis,temperature", "method_startline": "238", "method_endline": "253"}}, "hunk_2": {"Ismethod": 1, "added_lines": "312,313,314,315,316,317,318,319,320,321,322", "deleted_lines": "301,308,309,310,311,312,313,314,315,316,317,318", "method_info": {"method_name": "mxnet::op::SoftmaxGradCompute", "method_params": "attrs,ctx,inputs,req,outputs", "method_startline": "299", "method_endline": "324"}}, "hunk_3": {"Ismethod": 1, "added_lines": "228,230,231,232", "deleted_lines": "226,227,232", "method_info": {"method_name": "mxnet::op::mxnet_op::softmax_gradient_kernel", "method_params": "out,ograd,igrad,M,axis,sshape,stride,temperature", "method_startline": "209", "method_endline": "234"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\python\\unittest\\test_operator.py", "file_new_name": "tests\\python\\unittest\\test_operator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535", "deleted_lines": "4525,4526,4527,4528,4529,4530,4531", "method_info": {"method_name": "test_new_softmax", "method_params": "", "method_startline": "4523", "method_endline": "4535"}}}}}}}