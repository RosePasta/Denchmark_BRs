{"BR": {"BR_id": "9624", "BR_author": "solin319", "BRopenT": "2018-01-30T01:36:43Z", "BRcloseT": "2020-09-26T18:48:45Z", "BR_text": {"BRsummary": "problem when set fix_gamma=True in batchnorm", "BRdescription": "\n If fix_gamma is true, then set gamma to 1 and its gradient to 0.\n But the value of gamma will be changed during parameters update. So the gamma saved in param file was not 1. These will bring a problem in convert MXNet parameters to other deep-learning platforms.\n This problem was caused by we set a default weight-decay in SGD optimizer.\n We must define variable gamma with wd_mult=0 to fix gamma=1 during training.\n Can MXNet set wd of gamma to 0 automatically when fix_gamma=1?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "solin319", "commentT": "2018-02-01T20:40:21Z", "comment_text": "\n \t\tThis is a legacy design defect. When fix_gamma is true there shouldn't be a gamma parameter.\n In the future this could be solved by creating new operator batch_norm and deprecating BatchNorm\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "solin319", "commentT": "2018-02-27T20:19:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sandeep-krishnamurthy>@sandeep-krishnamurthy</denchmark-link>\n  : Tag: Bug\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "solin319", "commentT": "2018-06-14T08:04:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/solin319>@solin319</denchmark-link>\n  how did you check the value of gamma? I suspect I am facing the same issue and hence, want to verify. But unfortunately, I couldn't find gamma in either arg_params or aux_params.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "solin319", "commentT": "2019-03-14T00:52:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mxnet-label-bot>@mxnet-label-bot</denchmark-link>\n  add [Operator]\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "solin319", "commentT": "2019-04-15T20:24:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/solin319>@solin319</denchmark-link>\n  I guess, the fix is already merged and closed.\n <denchmark-link:https://github.com/anirudh2290>@anirudh2290</denchmark-link>\n  Can we close this one.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "solin319", "commentT": "2019-04-15T21:08:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Vikas89>@Vikas89</denchmark-link>\n  the fix is only for coreml converter. The operator hasnt been fixed yet.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "solin319", "commentT": "2020-09-26T08:55:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/solin319>@solin319</denchmark-link>\n  Does it means that the parameters would be updated even its grad_req is set to \"null\" if the wd_mult is not set to zero?\n I think this behavior is unexpected.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "solin319", "commentT": "2020-09-26T18:48:45Z", "comment_text": "\n \t\tI think <denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  fixed this issue in <denchmark-link:https://github.com/apache/incubator-mxnet/pull/18500>#18500</denchmark-link>\n . Now the batchnorm op would respond to grad_req=null correctly.\n \t\t"}}}, "commit": {"commit_id": "0f7d33d7470a444cf9b3f6644bdc41a22f7afe85", "commit_author": "Zhao HG", "commitT": "2019-01-16 15:59:43-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tools\\coreml\\converter\\_layers.py", "file_new_name": "tools\\coreml\\converter\\_layers.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "475,476,477,481,482,490,491", "deleted_lines": "475,476", "method_info": {"method_name": "convert_batchnorm", "method_params": "net,node,module,builder", "method_startline": "453", "method_endline": "501"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tools\\coreml\\test\\test_mxnet_converter.py", "file_new_name": "tools\\coreml\\test\\test_mxnet_converter.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973", "deleted_lines": null, "method_info": {"method_name": "test_batch_norm_with_fix_gamma", "method_params": "self", "method_startline": "941", "method_endline": "973"}}}}}}}