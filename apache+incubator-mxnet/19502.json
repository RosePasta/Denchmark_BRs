{"BR": {"BR_id": "19502", "BR_author": "kpuatamazon", "BRopenT": "2020-11-09T14:36:43Z", "BRcloseT": "2020-11-10T15:38:26Z", "BR_text": {"BRsummary": "CI slowdown on CentOS CPU related to OpenMP and intgemm", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n intgemm is a 3rd-party library written by me and included as a submodule. Unrelated continuous integration tests were going slower afterwards on CentOS 7 CPU.  This has been discussed in comments on <denchmark-link:https://github.com/apache/incubator-mxnet/commit/13936020d4cc3ccb2d4192adccaa282cef509193#commitcomment-43224930>1393602#commitcomment-43224930</denchmark-link>\n  .\n After losing several hairs, the issue appears to be OpenMP support in intgemm's CMakeLists.txt:\n <denchmark-link:https://github.com/kpu/intgemm/blob/8f28282c3bd854922da638024d2659be52e892e9/CMakeLists.txt#L47-L56>https://github.com/kpu/intgemm/blob/8f28282c3bd854922da638024d2659be52e892e9/CMakeLists.txt#L47-L56</denchmark-link>\n \n I think this is causing MXNet to use the slow CentOS OpenMP instead of the bundled support.\n <denchmark-h:h2>Question</denchmark-h>\n \n What's the best practice for a standalone library that has its own OpenMP support to not step on MXNet's internal support?\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n Started with c5.18xlarge with the latest AL2 machine learning image.\n To build:\n <denchmark-code># Always delete the build directory.  This is sneaky and appears to survive.  \n rm -rf build; ci/build.py --docker-registry mxnetci --platform centos7_cpu --docker-build-retries 3 --shm-size 500m /work/runtime_functions.sh build_centos7_cpu\n </denchmark-code>\n \n To run:\n <denchmark-code>#Running\n docker run --cap-add SYS_PTRACE --rm --shm-size=500m -v $HOME/incubator-mxnet:/work/mxnet -v $HOME/incubator-mxnet/build:/work/build -v $HOME/.ccache:/work/ccache -u 1001:1001 -e CCACHE_MAXSIZE=500G -e CCACHE_TEMPDIR=/tmp/ccache -e CCACHE_DIR=/work/ccache -e CCACHE_LOGFILE=/tmp/ccache.log -ti mxnetci/build.centos7_cpu:latest bash\n CI_CUDA_COMPUTE_CAPABILITIES='-gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_70,code=sm_70'\n CI_CMAKE_CUDA_ARCH='5.2 7.0'\n set +x\n source /opt/rh/rh-python36/enable\n export PATH=/opt/rh/rh-python36/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n PATH=/opt/rh/rh-python36/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n export LD_LIBRARY_PATH=/opt/rh/rh-python36/root/usr/lib64\n LD_LIBRARY_PATH=/opt/rh/rh-python36/root/usr/lib64\n export MANPATH=/opt/rh/rh-python36/root/usr/share/man:\n MANPATH=/opt/rh/rh-python36/root/usr/share/man:\n export PKG_CONFIG_PATH=/opt/rh/rh-python36/root/usr/lib64/pkgconfig\n PKG_CONFIG_PATH=/opt/rh/rh-python36/root/usr/lib64/pkgconfig\n export XDG_DATA_DIRS=/opt/rh/rh-python36/root/usr/share:/usr/local/share:/usr/share\n XDG_DATA_DIRS=/opt/rh/rh-python36/root/usr/share:/usr/local/share:/usr/share\n cd /work/mxnet\n nproc\n expr 72 / 4\n OMP_NUM_THREADS=18\n python -m pytest --verbose tests/python/unittest/test_gluon.py::test_slice_pooling2d_slice_pooling2d\n </denchmark-code>\n \n Repeat the above steps for master and again with these lines commented out in 3rdparty/intgemm/CMakeLists.txt\n <denchmark-code>#option(USE_OPENMP \"Use OpenMP\" OFF)\n #if (USE_OPENMP)\n #  message(STATUS \"Compiling with OpenMP\")\n #  find_package(OpenMP)\n #  if (NOT ${OpenMP_CXX_FOUND})\n #    message(SEND_ERROR \"OpenMP requested but C++ support not found\")\n #  endif()\n #  add_compile_options(${OpenMP_CXX_FLAGS})\n #  target_link_libraries(intgemm PUBLIC OpenMP::OpenMP_CXX)\n #endif()\n </denchmark-code>\n \n The master version takes about 548.22s on a c5.18xlarge, while the commented version takes about 58.87s.\n cc <denchmark-link:https://github.com/mseth10>@mseth10</denchmark-link>\n  <denchmark-link:https://github.com/access2rohit>@access2rohit</denchmark-link>\n  <denchmark-link:https://github.com/leezu>@leezu</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kpuatamazon", "commentT": "2020-11-09T16:37:48Z", "comment_text": "\n \t\t\n I think this is causing MXNet to use the slow CentOS OpenMP instead of the bundled support.\n \n Another hypothesis is that the slowdown results from an interaction of the llvm openmp linked by mxnet + the gcc 7 openmp (?) provided by devtoolset 7 on centos and used by intgemm. Let's try if the issue persists when only linking one openmp implementation.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kpuatamazon", "commentT": "2020-11-09T18:00:02Z", "comment_text": "\n \t\tThe Centos 7 OpenMP performance is terrible.\n To test <denchmark-link:https://github.com/leezu>@leezu</denchmark-link>\n  's hypothesis, I created a branch <denchmark-link:https://github.com/kpuatamazon/incubator-mxnet/tree/removeopenmp>https://github.com/kpuatamazon/incubator-mxnet/tree/removeopenmp</denchmark-link>\n  without the 3rdparty OpenMP.  This means there should only be the OS-/compiler-provided OpenMP.\n The same test above took 542.95s with intgemm's CMakeLists.txt as is, and 539.99s with my OpenMP commented out like below.  No real difference here.\n <denchmark-code>#option(USE_OPENMP \"Use OpenMP\" OFF)\n #if (USE_OPENMP)\n #  message(STATUS \"Compiling with OpenMP\")\n #  find_package(OpenMP)\n #  if (NOT ${OpenMP_CXX_FOUND})\n #    message(SEND_ERROR \"OpenMP requested but C++ support not found\")\n #  endif()\n #  add_compile_options(${OpenMP_CXX_FLAGS})\n #  target_link_libraries(intgemm PUBLIC OpenMP::OpenMP_CXX)\n #endif()\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kpuatamazon", "commentT": "2020-11-09T18:04:58Z", "comment_text": "\n \t\tThank you for testing this <denchmark-link:https://github.com/kpuatamazon>@kpuatamazon</denchmark-link>\n ! If the slowdown is solely due to the CentOS 7 OpenMP, it implies that we should also enable 3rdparty/openmp in the static builds distributed by others on pypi etc. It was previously disabled for consistency with the old Makefile staticbuild.\n \n \n \n incubator-mxnet/CMakeLists.txt\n \n \n          Line 412\n       in\n       564c6d3\n \n \n \n \n \n \n  AND NOT CMAKE_BUILD_TYPE STREQUAL \"Distribution\" \n \n \n \n \n \n Further, if the slowdown is due to the CentOS 7 OpenMP and unrelated to intgemm, I would expect the \"slow test\" issue to be present in the Jenkins CD pipeline both before and after intgemm was merged.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "kpuatamazon", "commentT": "2020-11-09T18:22:22Z", "comment_text": "\n \t\tOn the CD test pipeline (which uses the system provided openmp), I observe on Aug 30 2020:\n <denchmark-code>[2020-08-30T20:36:17.326Z] 20.44s call     tests/python/unittest/test_gluon.py::test_slice_pooling2d_slice_pooling2d\n </denchmark-code>\n \n <denchmark-link:https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1633/nodes/250/steps/288/log/?start=0>https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1633/nodes/250/steps/288/log/?start=0</denchmark-link>\n \n and on Sep 1 (after intgemm was merged)\n <denchmark-code>[2020-09-01T20:22:31.173Z] 19.14s call     tests/python/unittest/test_gluon.py::test_slice_pooling2d_slice_pooling2d\n </denchmark-code>\n \n <denchmark-link:https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1643/nodes/253/steps/288/log/?start=0>https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1643/nodes/253/steps/288/log/?start=0</denchmark-link>\n \n I checked the <denchmark-link:https://jenkins.mxnet-ci.amazon-ml.com/blue/rest/organizations/jenkins/pipelines/restricted-mxnet-cd/pipelines/mxnet-cd-release-job/runs/1633/nodes/66/steps/173/log/?start=0>respective build logs</denchmark-link>\n , and they do not contain the string . They only list\n <denchmark-code>[2020-08-30T19:58:43.183Z] -- Found OpenMP_C: -fopenmp (found version \"4.5\") \n [2020-08-30T19:58:43.183Z] -- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n [2020-08-30T19:58:43.183Z] -- Found OpenMP: TRUE (found version \"4.5\")  \n </denchmark-code>\n \n meaning that the CentOS 7 system openmp is used.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "kpuatamazon", "commentT": "2020-11-09T19:02:16Z", "comment_text": "\n \t\tI can reproduce the difference in perforamce between CD CentOS setup and CI CentOS setup locally.\n The test runs in ~20 seconds in the CD setup and takes much longer on the CI setup.\n Both use the system provided openmp.\n For CD setup, I use\n <denchmark-code>rm -rf lib build; python ci/build.py --cache-intermediate --platform centos7_cpu /work/runtime_functions.sh build_static_libmxnet cpu\n </denchmark-code>\n \n and for CI setup I use\n <denchmark-code>rm -rf lib build; ci/build.py --docker-registry mxnetci --platform centos7_cpu --docker-build-retries 3 --shm-size 500m /work/runtime_functions.sh build_centos7_cpu\n </denchmark-code>\n \n In both cases I have modified the cmake file to remove intgemm:\n diff --git a/CMakeLists.txt b/CMakeLists.txt\n index 07075d752..8bfd03f53 100644\n --- a/CMakeLists.txt\n +++ b/CMakeLists.txt\n @@ -295,12 +295,6 @@ if(USE_MKLDNN)\n    set_target_properties(dnnl PROPERTIES CXX_CLANG_TIDY \"\")  # don't lint 3rdparty dependency\n  endif()\n \n -if(USE_INTGEMM)\n -  message(STATUS \"Using intgemm\")\n -  add_subdirectory(3rdparty/intgemm EXCLUDE_FROM_ALL)\n -  add_definitions(-DMXNET_USE_INTGEMM=1)\n -endif()\n -\n  # Allow Cuda compiles outside of src tree to find things in 'src' and 'include'\n  include_directories(${CMAKE_CURRENT_SOURCE_DIR}/include)\n  include_directories(${CMAKE_CURRENT_SOURCE_DIR}/src)\n @@ -509,10 +503,8 @@ endif()\n  FILE(GLOB_RECURSE SOURCE \"src/*.cc\" \"src/*.h\" \"include/*.h\")\n  FILE(GLOB_RECURSE CUDA \"src/*.cu\" \"src/*.cuh\")\n \n -if(NOT USE_INTGEMM)\n    FILE(GLOB_RECURSE INTGEMM_OPERATOR_SOURCE \"src/operator/contrib/intgemm/*.cc\" \"src/operator/contrib/intgemm/*.h\")\n    list(REMOVE_ITEM SOURCE ${INTGEMM_OPERATOR_SOURCE})\n -endif()\n \n  # add nnvm to source\n  FILE(GLOB_RECURSE NNVMSOURCE\n @@ -840,9 +832,6 @@ if(USE_MKLDNN)\n        ${CMAKE_BINARY_DIR}/3rdparty/mkldnn/include/dnnl_version.h  ${CMAKE_SOURCE_DIR}/include/mkldnn/)\n  endif()\n \n -if(USE_INTGEMM)\n -  target_link_libraries(mxnet PRIVATE intgemm)\n -endif()\n \n  function(BuildTVMOP)\n    # scope the variables in BuildTVM.cmake to avoid conflict\n and deleted the 3rdparty/openmp folder.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "kpuatamazon", "commentT": "2020-11-09T22:28:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/kpuatamazon>@kpuatamazon</denchmark-link>\n  removing the test coverage instrumentations from the centos build brings the test back to .\n I suspect that the test coverage instrumentation would never be part of llvm OpenMP (3rdparty/openmp) due to\n \n \n \n incubator-mxnet/CMakeLists.txt\n \n \n          Line 401\n       in\n       551a8d3\n \n \n \n \n \n \n  set(CMAKE_BUILD_TYPE Release) \n \n \n \n \n \n However, as intgemm pulls in the system openmp the coverage instrumentations starts slowing down the tests.\n I'm not yet sure why the same doesn't apply to the ubuntu cpu tests, which also include intgemm and enable test coverage instrumentations.\n \t\t"}}}, "commit": {"commit_id": "d46dc96f349092be8eebb6293ec8f512c11c0418", "commit_author": "Leonard Lausen", "commitT": "2020-11-10 07:38:25-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "ci\\docker\\runtime_functions.sh", "file_new_name": "ci\\docker\\runtime_functions.sh", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "266"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "ci\\jenkins\\Jenkins_steps.groovy", "file_new_name": "ci\\jenkins\\Jenkins_steps.groovy", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "283,869", "deleted_lines": "283,869"}}}}}}