{"BR": {"BR_id": "14301", "BR_author": "ashokei", "BRopenT": "2019-03-02T07:34:24Z", "BRcloseT": "2019-03-05T08:14:59Z", "BR_text": {"BRsummary": "SoftmaxOutput crashes with normalization \"valid\"", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n <denchmark-h:h2>Environment info (Required)</denchmark-h>\n \n ubuntu 16.04 default build\n and, run below script.\n <denchmark-code>import numpy as np                                                \n import mxnet as mx                                                \n xpu = mx.cpu()                                                    \n x = mx.sym.Variable('x')                                          \n label = mx.sym.Variable('label')                                  \n x_nd = mx.nd.array([[1, 6, 4, 2],[1, 6, 4, 2]], ctx=xpu)          \n grad_x = mx.nd.zeros((2,4), ctx=xpu)                              \n label_nd = mx.nd.array([1,1], ctx=xpu)                            \n                                                                   \n sym = mx.sym.SoftmaxOutput(data=x, label=label, ignore_label=0,   \n                            use_ignore=True, normalization=\"valid\")\n ex = sym.bind(ctx=xpu, args={'x': x_nd, 'label': label_nd},       \n               args_grad={'x': grad_x})                            \n                                                                   \n ex.forward(is_train=True)                                         \n softmax_out = ex.outputs[0].asnumpy()                             \n ex.backward(is_train=True)                                        \n \n </denchmark-code>\n \n MXNet commit hash:\n <denchmark-link:https://github.com/apache/incubator-mxnet/commit/fb4f9d55382538fe688638b741830d84ae0d783e>fb4f9d5</denchmark-link>\n \n Build config:\n make\n <denchmark-h:h2>Error Message:</denchmark-h>\n \n <denchmark-code>terminated by signal SIGSEGV (Address boundary error)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ashokei", "commentT": "2019-03-02T07:34:26Z", "comment_text": "\n \t\tHey, this is the MXNet Label Bot.\n Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.\n Here are my recommended labels: Bug\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ashokei", "commentT": "2019-03-02T08:51:09Z", "comment_text": "\n \t\tThanks for your report!\n reproduce the bug in MXNet <denchmark-link:https://github.com/apache/incubator-mxnet/commit/fb4f9d55382538fe688638b741830d84ae0d783e>fb4f9d5</denchmark-link>\n \n It is strange that the address of ctx.requested[softmaxout_enum::kTempSpace] is 0 in src/operator/softmax_output-inl.h   .\n ctx.requested.size() is 0 in Backward.\n It is a bug that BackwardResource is not called when ex.backward(is_train=true) is called.\n I do not know why SoftmaxOutput is not a legacy operator.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ashokei", "commentT": "2019-03-04T00:04:44Z", "comment_text": "\n \t\tI too recently saw an issue with Softmax that generated a segfault.  This behavior began with the Softmax operator changes introduced by <denchmark-link:https://github.com/apache/incubator-mxnet/pull/13699>#13699</denchmark-link>\n  and occurs when the framework is compiled with USE_MKLDNN=0.  The failing test is with sockeye:\n <denchmark-code>test/integration/test_constraints_int.py::test_constraints[--encoder rnn --decoder rnn --num-layers 1 --rnn-cell-type lstm --rnn-num-hidden 8 --num-embed 4  --rnn-attention-type mlp --rnn-attention-num-hidden 8 --loss cross-entropy --optimized-metric perplexity --max-updates 2 --checkpoint-frequency 2 --optimizer adam --initial-learning-rate 0.01 --batch-type sentence  --decode-and-evaluate 0-2-10] ./test.sh: line 3:    62 Segmentation fault      python setup.py test\n ++ RV=139\n </denchmark-code>\n \n Perhaps you could verify that your fix corrects this behavior?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ashokei", "commentT": "2019-03-04T06:15:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/DickJC123>@DickJC123</denchmark-link>\n \n Hi! I test sockeye in my laptop with MXNet(master) with \n All tests pass except for test/unit/test_inference.py::test_topk_func\n <denchmark-code>test/unit/test_inference.py::test_topk_func[1-5-200] FAILED              [ 60%]\n test/unit/test_inference.py::test_topk_func[5-5-200] FAILED              [ 60%]\n test/unit/test_inference.py::test_topk_func[1-1-200] PASSED              [ 60%]\n test/unit/test_inference.py::test_topk_func[5-1-200] PASSED              [ 60%]\n test/unit/test_inference.py::test_topk_func[10-10-100] FAILED            [ 60%]\n </denchmark-code>\n \n In sockeye, there is no any Softmax with normalization valid, so it couldn't trigger the bug in this issue.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ashokei", "commentT": "2019-03-04T21:09:58Z", "comment_text": "\n \t\tI can also confirm this issue, it happens only when normalization-\"valid\" and while executing the Executor.backward function call. For instance this sample code works fine -\n <denchmark-code>import mxnet as mx                                                                                                                                            \n import numpy as np\n \n xpu = mx.cpu()\n x_nd = mx.nd.array([[1, 6, 4, 2],[1, 6, 4, 2]], ctx=xpu)    \n grad_x = mx.nd.zeros((2,4), ctx=xpu)    \n label_nd = mx.nd.array([1,1], ctx=xpu)\n \n x_nd.attach_grad()\n \n with mx.autograd.record():\n     y = mx.nd.SoftmaxOutput(data=x_nd, label=label_nd, ignore_label=0, use_ignore=True) #, normalization=\"valid\")\n \n y.backward()\n print(x_nd.grad)\n </denchmark-code>\n \n So the bug is with the gradient calculation of softmax output when normalization=\"valid\"\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ashokei", "commentT": "2019-03-04T21:48:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  Sockeye can use 'valid' normalization in its SoftmaxOutput operator use, see <denchmark-link:https://github.com/awslabs/sockeye/blob/2f44099cd4f488bd8d348d74e9ae85095f72501e/sockeye/loss.py#L112>here</denchmark-link>\n .\n The failure you are observing for  is related to <denchmark-link:https://github.com/apache/incubator-mxnet/issues/13862>#13862</denchmark-link>\n , which is an open problem.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ashokei", "commentT": "2019-03-04T23:02:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/fhieber>@fhieber</denchmark-link>\n  Sorry that I overlooked it.\n <denchmark-link:https://github.com/anirudhacharya>@anirudhacharya</denchmark-link>\n  <denchmark-link:https://github.com/apache/incubator-mxnet/pull/14302>#14302</denchmark-link>\n  will address the problem.\n \t\t"}}}, "commit": {"commit_id": "5065f13dfee6153df573339067954f22257ef9a6", "commit_author": "JackieWu", "commitT": "2019-03-05 16:14:58+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\softmax_output-inl.h", "file_new_name": "src\\operator\\softmax_output-inl.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "429,430,431,432", "method_info": {"method_name": "mxnet::op::SoftmaxOutputProp::BackwardResource", "method_params": "in_shape", "method_startline": "429", "method_endline": "432"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\operator\\softmax_output.cc", "file_new_name": "src\\operator\\softmax_output.cc", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "265,266,267", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\python\\unittest\\test_operator.py", "file_new_name": "tests\\python\\unittest\\test_operator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "6501,6502,6503,6504,6505,6506,6507,6508,6509,6510,6511,6512,6513,6514,6515,6516,6517,6518,6519,6520,6521,6522,6523,6524,6525,6526,6527,6528,6529,6530,6531,6532,6533,6534,6535,6536,6537,6538,6539,6540,6541,6542,6543,6544,6545,6546,6547,6548,6549,6550,6551,6552,6553,6554,6555,6556,6557,6558,6559", "deleted_lines": null, "method_info": {"method_name": "test_softmax_output_normalization._softmaxoutput_normalization", "method_params": "multi_output,use_ignore,normalization", "method_startline": "6501", "method_endline": "6559"}}, "hunk_1": {"Ismethod": 1, "added_lines": "6500,6501,6502,6503,6504,6505,6506,6507,6508,6509,6510,6511,6512,6513,6514,6515,6516,6517,6518,6519,6520,6521,6522,6523,6524,6525,6526,6527,6528,6529,6530,6531,6532,6533,6534,6535,6536,6537,6538,6539,6540,6541,6542,6543,6544,6545,6546,6547,6548,6549,6550,6551,6552,6553,6554,6555,6556,6557,6558,6559,6560,6561,6562,6563,6564,6565", "deleted_lines": null, "method_info": {"method_name": "test_softmax_output_normalization", "method_params": "", "method_startline": "6500", "method_endline": "6565"}}}}}}}