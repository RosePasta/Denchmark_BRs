{"BR": {"BR_id": "8863", "BR_author": "shuokay", "BRopenT": "2017-11-29T07:47:29Z", "BRcloseT": "2017-12-11T02:52:35Z", "BR_text": {"BRsummary": "[gluon] segmentation error when getting the shape of output ndarray of transpose conv", "BRdescription": "\n <denchmark-h:h2>Environment info (Required)</denchmark-h>\n \n <denchmark-code>----------Python Info----------\n ('Version      :', '2.7.12')\n ('Compiler     :', 'GCC 5.4.0 20160609')\n ('Build        :', ('default', 'Nov 19 2016 06:48:10'))\n ('Arch         :', ('64bit', 'ELF'))\n ------------Pip Info-----------\n ('Version      :', '9.0.1')\n ('Directory    :', '/usr/local/lib/python2.7/dist-packages/pip')\n ----------MXNet Info-----------\n ('Version      :', '0.12.1')\n ('Directory    :', '/home/luban/incubator-mxnet/python/mxnet')\n Traceback (most recent call last):\n   File \"diagnose.py\", line 171, in <module>\n     check_mxnet()\n   File \"diagnose.py\", line 113, in check_mxnet\n     except FileNotFoundError:\n NameError: global name 'FileNotFoundError' is not defined\n </denchmark-code>\n \n Package used (Python/R/Scala/Julia): Python2.7.12\n MXNet commit hash: <denchmark-link:https://github.com/apache/incubator-mxnet/commit/91ffd691bf865833a51ca793eda125b7f9befc18>91ffd69</denchmark-link>\n \n Build config:\n <denchmark-code>#-------------------------------------------------------------------------------\n #  Template configuration for compiling mxnet\n #\n #  If you want to change the configuration, please use the following\n #  steps. Assume you are on the root directory of mxnet. First copy the this\n #  file so that any local changes will be ignored by git\n #\n #  $ cp make/config.mk .\n #\n #  Next modify the according entries, and then compile by\n #\n #  $ make\n #\n #  or build in parallel with 8 threads\n #\n #  $ make -j8\n #-------------------------------------------------------------------------------\n \n #---------------------\n # choice of compiler\n #--------------------\n \n export CC = gcc\n export CXX = g++\n export NVCC = nvcc\n \n # whether compile with options for MXNet developer\n DEV = 0\n \n # whether compile with debug\n DEBUG = 0\n \n # whether compile with profiler\n USE_PROFILER =\n \n # whether to turn on signal handler (e.g. segfault logger)\n USE_SIGNAL_HANDLER =\n \n # the additional link flags you want to add\n ADD_LDFLAGS = -L/usr/local/nvidia/lib64\n \n # the additional compile flags you want to add\n ADD_CFLAGS =\n \n #---------------------------------------------\n # matrix computation libraries for CPU/GPU\n #---------------------------------------------\n \n # whether use CUDA during compile\n USE_CUDA = 1\n \n # add the path to CUDA library to link and compile flag\n # if you have already add them to environment variable, leave it as NONE\n # USE_CUDA_PATH = /usr/local/cuda\n USE_CUDA_PATH = /usr/local/cuda\n \n # whether use CuDNN R3 library\n USE_CUDNN = 1\n \n #whether to use NCCL library\n USE_NCCL = 0\n #add the path to NCCL library\n USE_NCCL_PATH = NONE\n \n # whether use opencv during compilation\n # you can disable it, however, you will not able to use\n # imbin iterator\n USE_OPENCV = 1\n \n #whether use libjpeg-turbo for image decode without OpenCV wrapper\n USE_LIBJPEG_TURBO = 0\n #add the path to libjpeg-turbo library\n USE_LIBJPEG_TURBO_PATH = NONE\n \n # use openmp for parallelization\n USE_OPENMP = 1\n \n # MKL ML Library for Intel CPU/Xeon Phi\n # Please refer to MKL_README.md for details\n \n # MKL ML Library folder, need to be root for /usr/local\n # Change to User Home directory for standard user\n # For USE_BLAS!=mkl only\n MKLML_ROOT=/usr/local\n \n # whether use MKL2017 library\n USE_MKL2017 = 0\n \n # whether use MKL2017 experimental feature for high performance\n # Prerequisite USE_MKL2017=1\n USE_MKL2017_EXPERIMENTAL = 0\n \n # whether use NNPACK library\n USE_NNPACK = 0\n \n # choose the version of blas you want to use\n # can be: mkl, blas, atlas, openblas\n # in default use atlas for linux while apple for osx\n UNAME_S := $(shell uname -s)\n ifeq ($(UNAME_S), Darwin)\n USE_BLAS = apple\n else\n USE_BLAS = openblas\n endif\n \n # whether use lapack during compilation\n # only effective when compiled with blas versions openblas/apple/atlas/mkl\n USE_LAPACK = 1\n \n # path to lapack library in case of a non-standard installation\n USE_LAPACK_PATH =\n \n # by default, disable lapack when using MKL\n # switch on when there is a full installation of MKL available (not just MKL2017/MKL_ML)\n ifeq ($(USE_BLAS), mkl)\n USE_LAPACK = 0\n endif\n \n # add path to intel library, you may need it for MKL, if you did not add the path\n # to environment variable\n USE_INTEL_PATH = NONE\n \n # If use MKL only for BLAS, choose static link automatically to allow python wrapper\n ifeq ($(USE_MKL2017), 0)\n ifeq ($(USE_BLAS), mkl)\n USE_STATIC_MKL = 1\n endif\n else\n USE_STATIC_MKL = NONE\n endif\n \n #----------------------------\n # Settings for power and arm arch\n #----------------------------\n ARCH := $(shell uname -a)\n ifneq (,$(filter $(ARCH), armv6l armv7l powerpc64le ppc64le aarch64))\n \tUSE_SSE=0\n else\n \tUSE_SSE=1\n endif\n \n #----------------------------\n # distributed computing\n #----------------------------\n \n # whether or not to enable multi-machine supporting\n USE_DIST_KVSTORE = 0\n \n # whether or not allow to read and write HDFS directly. If yes, then hadoop is\n # required\n USE_HDFS = 0\n \n # path to libjvm.so. required if USE_HDFS=1\n LIBJVM=$(JAVA_HOME)/jre/lib/amd64/server\n \n # whether or not allow to read and write AWS S3 directly. If yes, then\n # libcurl4-openssl-dev is required, it can be installed on Ubuntu by\n # sudo apt-get install -y libcurl4-openssl-dev\n USE_S3 = 0\n \n #----------------------------\n # performance settings\n #----------------------------\n # Use operator tuning\n USE_OPERATOR_TUNING = 1\n \n # Use gperftools if found\n USE_GPERFTOOLS = 1\n \n # Use JEMalloc if found, and not using gperftools\n USE_JEMALLOC = 1\n \n #----------------------------\n # additional operators\n #----------------------------\n \n # path to folders containing projects specific operators that you don't want to put in src/operators\n EXTRA_OPERATORS =\n \n #----------------------------\n # other features\n #----------------------------\n \n # Create C++ interface package\n USE_CPP_PACKAGE = 0\n \n #----------------------------\n # plugins\n #----------------------------\n \n # whether to use caffe integration. This requires installing caffe.\n # You also need to add CAFFE_PATH/build/lib to your LD_LIBRARY_PATH\n # CAFFE_PATH = $(HOME)/caffe\n # MXNET_PLUGINS += plugin/caffe/caffe.mk\n \n # whether to use torch integration. This requires installing torch.\n # You also need to add TORCH_PATH/install/lib to your LD_LIBRARY_PATH\n # TORCH_PATH = $(HOME)/torch\n # MXNET_PLUGINS += plugin/torch/torch.mk\n \n # WARPCTC_PATH = $(HOME)/warp-ctc\n # MXNET_PLUGINS += plugin/warpctc/warpctc.mk\n \n # whether to use sframe integration. This requires build sframe\n # git@github.com:dato-code/SFrame.git\n # SFRAME_PATH = $(HOME)/SFrame\n # MXNET_PLUGINS += plugin/sframe/plugin.mk\n </denchmark-code>\n \n <denchmark-h:h2>Error Message:</denchmark-h>\n \n [1]    17341 segmentation fault (core dumped)  python resnet-fcn.py\n <denchmark-h:h2>Minimum reproducible example</denchmark-h>\n \n import mxnet as mx\n from mxnet import gluon\n from mxnet import nd\n ctx = mx.gpu(1)\n net4 = gluon.nn.Sequential()\n conv_t4 = gluon.nn.Conv2DTranspose(1024, kernel_size=(4, 4), strides=(2, 2), padding=(1, 1))\n conv_t4.initialize(init=mx.init.Xavier(), ctx=ctx)\n x4 = nd.uniform(low=0, high=255, shape=(2, 1024, 14, 14), ctx=ctx)\n net4.add(conv_t4)\n print (net4(x4)).shape\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "shuokay", "commentT": "2017-11-29T18:42:43Z", "comment_text": "\n \t\tIt's a bug in the sampling op. The following code would crash at commit <denchmark-link:https://github.com/apache/incubator-mxnet/commit/3226bce2c912bd0289be171c7025a6651e025407>3226bce</denchmark-link>\n :\n  import mxnet as mx\n  from mxnet import nd\n  ctx = mx.cpu(0)\n  x4 = nd.uniform(low=0, high=255, shape=(2, 1024, 14, 14), ctx=ctx)\n  print(x4.shape)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "shuokay", "commentT": "2017-11-29T22:00:58Z", "comment_text": "\n \t\tCore dump at sample_op.h:266. Maybe related to this PR: <denchmark-link:https://github.com/apache/incubator-mxnet/pull/8179>#8179</denchmark-link>\n \n Note: NaiveEngine can run through though.\n <denchmark-link:https://github.com/asmushetzel>@asmushetzel</denchmark-link>\n \n <denchmark-code>(gdb) bt\n #0  0x00000000010e40c0 in ?? ()\n #1  0x00007fffe78cd9bc in mxnet::op::Scalar2Array<mshadow::cpu, float>::~Scalar2Array (this=0x7fffb58d5e80, __in_chrg=<optimized out>) at src/operator/random/./sample_op.h:266\n #2  0x00007fffe78c4df7 in mxnet::op::SampleMaster<mshadow::cpu, mxnet::op::UniformSampler<mshadow::cpu> >::op (attrs=..., ctx=..., req=@0x1d739d0: mxnet::kWriteTo, \n     outputs=0x7fffb58d66e0) at src/operator/random/./sample_op.h:295\n #3  0x00007fffe78ba92c in mxnet::op::Sample_<mshadow::cpu, mxnet::op::UniformSampler<mshadow::cpu> > (attrs=..., ctx=..., inputs=std::vector of length 0, capacity 0, \n     req=std::vector of length 1, capacity 1 = {...}, outputs=std::vector of length 1, capacity 1 = {...}) at src/operator/random/./sample_op.h:454\n #4  0x00007fffe77b3700 in std::_Function_handler<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&), void (*)(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::_M_invoke(std::_Any_data const&, nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) (__functor=..., __args#0=..., \n     __args#1=..., __args#2=std::vector of length 0, capacity 0, __args#3=std::vector of length 1, capacity 1 = {...}, __args#4=std::vector of length 1, capacity 1 = {...})\n     at /usr/include/c++/5/functional:1871\n #5  0x00007fffe994ead2 in std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)>::operator()(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&) const (this=0x1d31858, __args#0=..., __args#1=..., __args#2=std::vector of length 0, capacity 0, __args#3=std::vector of length 1, capacity 1 = {...}, \n     __args#4=std::vector of length 1, capacity 1 = {...}) at /usr/include/c++/5/functional:2267\n #6  0x00007fffe9948ce0 in mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const (__closure=0x1d317d0, rctx=...) at src/imperative/./imperative_utils.h:360\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "shuokay", "commentT": "2017-12-01T17:29:48Z", "comment_text": "\n \t\tlooking into it\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "shuokay", "commentT": "2017-12-10T12:35:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/shuokay>@shuokay</denchmark-link>\n  PR8935 which avoids this special allocation has been merged. Can you do a recheck on your side whether things work ok now?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "shuokay", "commentT": "2017-12-11T02:52:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/asmushetzel>@asmushetzel</denchmark-link>\n \n No segmentation fault and gloun exit normally. Thanks for your work.\n \t\t"}}}, "commit": {"commit_id": "01b7ae3a4db14a47f426882b0fbf2e32192a3954", "commit_author": "moin", "commitT": "2017-12-08 16:55:36-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 15, "file_old_name": "src\\operator\\random\\sample_op.h", "file_new_name": "src\\operator\\random\\sample_op.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "259,260,261,262,263,264", "deleted_lines": "259,260,261,262,263,264", "method_info": {"method_name": "mxnet::op::Scalar2Array::Scalar2Array", "method_params": "scalar,ctx", "method_startline": "259", "method_endline": "264"}}, "hunk_1": {"Ismethod": 1, "added_lines": "324,325,326,327,331", "deleted_lines": "319", "method_info": {"method_name": "mxnet::op::SampleMaster<xpu,GammaSampler<xpu>>::op", "method_params": "attrs,ctx,req,outputs", "method_startline": "316", "method_endline": "333"}}, "hunk_2": {"Ismethod": 1, "added_lines": "302,303,304,305,309", "deleted_lines": "295,296,300", "method_info": {"method_name": "mxnet::op::SampleMaster<xpu,NormalSampler<xpu>>::op", "method_params": "attrs,ctx,req,outputs", "method_startline": "295", "method_endline": "311"}}, "hunk_3": {"Ismethod": 1, "added_lines": "345,346,347,351", "deleted_lines": "339,353", "method_info": {"method_name": "mxnet::op::SampleMaster<xpu,ExponentialSampler<xpu>>::op", "method_params": "attrs,ctx,req,outputs", "method_startline": "338", "method_endline": "353"}}, "hunk_4": {"Ismethod": 1, "added_lines": "268", "deleted_lines": "268", "method_info": {"method_name": "mxnet::op::Scalar2Array::Ref", "method_params": "", "method_startline": "268", "method_endline": "268"}}, "hunk_5": {"Ismethod": 1, "added_lines": "365,366,367,371", "deleted_lines": "358,372,373", "method_info": {"method_name": "mxnet::op::SampleMaster<xpu,PoissonSampler<xpu>>::op", "method_params": "attrs,ctx,req,outputs", "method_startline": "358", "method_endline": "373"}}, "hunk_6": {"Ismethod": 1, "added_lines": "409,410,411,412,416", "deleted_lines": "414,415", "method_info": {"method_name": "mxnet::op::SampleMaster<xpu,GeneralizedNegativeBinomialSampler<xpu>>::op", "method_params": "attrs,ctx,req,outputs", "method_startline": "399", "method_endline": "418"}}, "hunk_7": {"Ismethod": 1, "added_lines": "265", "deleted_lines": "265,266,267", "method_info": {"method_name": "mxnet::op::Scalar2Array::~Scalar2Array", "method_params": "", "method_startline": "265", "method_endline": "267"}}, "hunk_8": {"Ismethod": 1, "added_lines": null, "deleted_lines": "252", "method_info": {"method_name": "mxnet::op::AllocContext<cpu>", "method_params": "", "method_startline": "252", "method_endline": "252"}}, "hunk_9": {"Ismethod": 1, "added_lines": "281,282,283,284,288", "deleted_lines": "274,277,278,279,280", "method_info": {"method_name": "mxnet::op::SampleMaster<xpu,UniformSampler<xpu>>::op", "method_params": "attrs,ctx,req,outputs", "method_startline": "274", "method_endline": "290"}}, "hunk_10": {"Ismethod": 1, "added_lines": "248,249,250,251,254,255,256,257,258,259,260,261,262,263,264,265", "deleted_lines": "248,249,250,251,252,253,254,255,257,258,259,260,261,262,263,264,265,266", "method_info": {"method_name": "mxnet::op::GetSamplingTempData", "method_params": "N,p1,p2,ctx,seeds,parm1,parm2", "method_startline": "248", "method_endline": "266"}}, "hunk_11": {"Ismethod": 1, "added_lines": "386,387,388,392", "deleted_lines": "392,393", "method_info": {"method_name": "mxnet::op::SampleMaster<xpu,NegativeBinomialSampler<xpu>>::op", "method_params": "attrs,ctx,req,outputs", "method_startline": "378", "method_endline": "394"}}, "hunk_12": {"Ismethod": 1, "added_lines": "254", "deleted_lines": "254", "method_info": {"method_name": "mxnet::op::AllocContext<gpu>", "method_params": "", "method_startline": "254", "method_endline": "254"}}, "hunk_13": {"Ismethod": 1, "added_lines": null, "deleted_lines": "269", "method_info": {"method_name": "mxnet::op::Scalar2Array::GetTensor", "method_params": "", "method_startline": "269", "method_endline": "269"}}, "hunk_14": {"Ismethod": 1, "added_lines": "281", "deleted_lines": "274,277,278,279,280", "method_info": {"method_name": "mxnet::op::GetSeeds", "method_params": "N,ctx", "method_startline": "274", "method_endline": "281"}}}}}}}