{"BR": {"BR_id": "9363", "BR_author": "eric-haibin-lin", "BRopenT": "2018-01-09T23:05:47Z", "BRcloseT": "2018-03-12T19:33:39Z", "BR_text": {"BRsummary": "Incorrect weight_decay implementation in AdaGrad", "BRdescription": "\n In Adagrad, weight_decay should be applied to grad before clipping the gradient/updating the state/weight.\n <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L856-L858>https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L856-L858</denchmark-link>\n \n Also adadelta: <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L982>https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer.py#L982</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "eric-haibin-lin", "commentT": "2018-01-17T15:01:12Z", "comment_text": "\n \t\twd term is also not merged into grad in SGD. Should we fix them? Will it cause some problems for backward compatibility?\n cc <denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "eric-haibin-lin", "commentT": "2018-01-17T15:45:22Z", "comment_text": "\n \t\tFor SGD, not merging in the wd term will linearly scale the \u201creal\u201d wd and the impact is smaller compared to Adagrad. However, the change may break lots of our current examples and I\u2019m not sure whether to revise it or not. We need to investigate how the other packages implement the WD term.\n \n Get Outlook for iOS<<denchmark-link:https://aka.ms/o0ukef>https://aka.ms/o0ukef</denchmark-link>\n >\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n ________________________________\n From: Ziyue Huang <notifications@github.com>\n Sent: Wednesday, January 17, 2018 7:01:29 AM\n To: apache/incubator-mxnet\n Cc: Xingjian SHI; Mention\n Subject: Re: [apache/incubator-mxnet] Incorrect weight_decay implementation in AdaGrad (#9363)\n \n \n wd term is also not merged into grad in SGD. Should we fix them? Will it cause some problems for backward compatibility?\n cc @sxjscience<https://github.com/sxjscience>\n \n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub<#9363 (comment)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AE8D7tu5nAPXmIkEMh35dR1rS6yrQ5odks5tLgtJgaJpZM4RYksA>.\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "eric-haibin-lin", "commentT": "2018-03-12T19:33:39Z", "comment_text": "\n \t\tClosed by the PR. Is the AdaDelta solved?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "eric-haibin-lin", "commentT": "2018-03-15T03:45:36Z", "comment_text": "\n \t\tNo, the change was reverted in the PR. My PR only contains a sparse op with Wd == 0\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "eric-haibin-lin", "commentT": "2018-03-15T04:17:26Z", "comment_text": "\n \t\tOK, we need to reopen it then.\n \n Get Outlook for iOS<<denchmark-link:https://aka.ms/o0ukef>https://aka.ms/o0ukef</denchmark-link>\n >\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n ________________________________\n From: Haibin Lin <notifications@github.com>\n Sent: Wednesday, March 14, 2018 8:45:48 PM\n To: apache/incubator-mxnet\n Cc: Xingjian SHI; State change\n Subject: Re: [apache/incubator-mxnet] Incorrect weight_decay implementation in AdaGrad (#9363)\n \n \n No, the change was reverted in the PR. My PR only contains a sparse op with Wd == 0\n \n \u2014\n You are receiving this because you modified the open/close state.\n Reply to this email directly, view it on GitHub<#9363 (comment)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AE8D7h68o3ArEYcJ96deH8NaLvVaOhW2ks5teePrgaJpZM4RYksA>.\n \n \t\t"}}}, "commit": {"commit_id": "fc9e70bf2d349ad4c6cb65ff3f0958e23a7410bf", "commit_author": "Haibin Lin", "commitT": "2018-03-03 14:12:23+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\api\\python\\ndarray\\sparse.md", "file_new_name": "docs\\api\\python\\ndarray\\sparse.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "487", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\mxnet\\optimizer.py", "file_new_name": "python\\mxnet\\optimizer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1098,1102,1103,1104,1105,1106,1108,1109,1110", "deleted_lines": "1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1111", "method_info": {"method_name": "update", "method_params": "self,index,weight,grad,state", "method_startline": "1091", "method_endline": "1113"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "src\\operator\\optimizer_op-inl.h", "file_new_name": "src\\operator\\optimizer_op-inl.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::AdagradUpdateEx", "method_params": "attrs,ctx,inputs,req,outputs", "method_startline": "1618", "method_endline": "1632"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::AdagradStorageType", "method_params": "attrs,dev_mask,dispatch_mode,in_attrs,out_attrs", "method_startline": "1512", "method_endline": "1529"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::AdagradUpdateDnsRspDnsImpl", "method_params": "param,ctx,weight,grad,state,req,out", "method_startline": "1559", "method_endline": "1591"}}, "hunk_3": {"Ismethod": 1, "added_lines": "1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::AdagradParam::DMLC_DECLARE_PARAMETER", "method_params": "AdagradParam", "method_startline": "1492", "method_endline": "1509"}}, "hunk_4": {"Ismethod": 1, "added_lines": "1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::AdagradUpdateRspRspRspImpl", "method_params": "param,ctx,weight,grad,state,req,out", "method_startline": "1594", "method_endline": "1615"}}, "hunk_5": {"Ismethod": 1, "added_lines": "1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::AdagradDnsRspDnsKernel::Map", "method_params": "i,row_length,out_data,state_data,weight_data,grad_idx,grad_data,clip_gradient,epsilon,lr,rescale_grad", "method_startline": "1534", "method_endline": "1555"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\operator\\optimizer_op.cc", "file_new_name": "src\\operator\\optimizer_op.cc", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "41,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570", "deleted_lines": null}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\operator\\optimizer_op.cu", "file_new_name": "src\\operator\\optimizer_op.cu", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "203,204,205", "deleted_lines": null}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\python\\unittest\\test_optimizer.py", "file_new_name": "tests\\python\\unittest\\test_optimizer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031", "deleted_lines": null, "method_info": {"method_name": "test_adagrad", "method_params": "", "method_startline": "1009", "method_endline": "1031"}}, "hunk_1": {"Ismethod": 1, "added_lines": "993,994", "deleted_lines": null, "method_info": {"method_name": "create_state", "method_params": "self,index,weight", "method_startline": "993", "method_endline": "994"}}, "hunk_2": {"Ismethod": 1, "added_lines": "989,990,991", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,eps,kwargs", "method_startline": "989", "method_endline": "991"}}, "hunk_3": {"Ismethod": 1, "added_lines": "996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007", "deleted_lines": null, "method_info": {"method_name": "update", "method_params": "self,index,weight,grad,state", "method_startline": "996", "method_endline": "1007"}}}}}}}