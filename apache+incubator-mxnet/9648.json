{"BR": {"BR_id": "9648", "BR_author": "zhanghang1989", "BRopenT": "2018-01-31T19:26:01Z", "BRcloseT": "2018-08-06T18:20:07Z", "BR_text": {"BRsummary": "BatchNorm Evaluation Mode Backward Fails with cudnn Enabled", "BRdescription": "\n When installing MXNet with cudnn enabled USE_CUDNN=1. The backward of BatchNorm operator fails\n <denchmark-h:h2>Reproducing the error</denchmark-h>\n \n <denchmark-code>import mxnet as mx\n import mxnet.ndarray as F\n from mxnet import autograd\n \n B,C,H,W = 4,3,2,2\n x = mx.nd.random.poisson(1,shape=(B,C,H,W)).as_in_context(mx.gpu(0))\n gamma = mx.nd.random.normal(shape=(C)).as_in_context(mx.gpu(0))\n beta = mx.nd.random.normal(shape=(C)).as_in_context(mx.gpu(0))\n mean = mx.nd.random.normal(shape=(C)).as_in_context(mx.gpu(0))\n std = mx.nd.random.normal(shape=(C)).as_in_context(mx.gpu(0))\n x.attach_grad()\n \n with autograd.record(False):\n     y = F.BatchNorm(x, gamma, beta, mean, std.square(), fix_gamma=False)     \n     loss=y.square().sum()\n loss.backward(train_mode=False)\n </denchmark-code>\n \n got the error:\n terminate called after throwing an instance of 'dmlc::Error'\n   what():  [19:23:23] src/engine/./threaded_engine.h:359: [19:23:23] src/operator/nn/./cudnn/cudnn_batch_norm-inl.h:193: Check failed: ctx.is_train && !param_.use_global_stats use global statistics is not yet supported in CuDNNBatchNorm\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "zhanghang1989", "commentT": "2018-01-31T19:30:15Z", "comment_text": "\n \t\tBackward in evaluation mode should be handled differently. Current code is here:\n <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cu#L645-L656>https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cu#L645-L656</denchmark-link>\n \n PyTorch had a related issue <denchmark-link:https://github.com/pytorch/pytorch/issues/4284>pytorch/pytorch#4284</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "zhanghang1989", "commentT": "2018-02-06T23:22:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/eric-haibin-lin>@eric-haibin-lin</denchmark-link>\n  I think this should be labeled as \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "zhanghang1989", "commentT": "2018-04-09T16:08:25Z", "comment_text": "\n \t\tWith what frequency can you reproduce?  I haven't been able to reproduce.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "zhanghang1989", "commentT": "2018-04-14T06:55:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/zhanghang1989>@zhanghang1989</denchmark-link>\n  related pr merged. can this be closed ?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "zhanghang1989", "commentT": "2018-05-21T23:43:49Z", "comment_text": "\n \t\tReopen this due to CUDNN is handled wrongly <denchmark-link:https://github.com/piiswrong>@piiswrong</denchmark-link>\n \n Related PR <denchmark-link:https://github.com/apache/incubator-mxnet/pull/10470>#10470</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "zhanghang1989", "commentT": "2019-08-19T10:13:46Z", "comment_text": "\n \t\tConfig:\n ubuntu 18.04\n python 3.6.4\n cuda_10.1.243_418.87.00_linux.run\n nvcc: NVIDIA (R) Cuda compiler driver\n Copyright (c) 2005-2017 NVIDIA Corporation\n Built on Fri_Nov__3_21:07:56_CDT_2017\n Cuda compilation tools, release 9.1, V9.1.85\n torch==1.1.0\n Code to be run:\n <denchmark-code>import torch\n import torch.nn as nn\n import torch.nn.functional as F\n import math\n import numpy as np\n \n def _upsample(x):\n     h, w = x.shape[2:]\n     return F.upsample_bilinear(x, size=(h * 2, w * 2))\n \n \n def upsample_conv(x, conv):\n     return conv(_upsample(x))\n \n class genBlock(nn.Module):\n     def __init__(self, in_channels, out_channels,\n                  activation=F.relu, hidden_channels=None, ksize=3, pad=1, upsample=False, n_classes=0):\n         super(genBlock, self).__init__()\n         self.activation = activation\n         self.upsample = upsample\n         self.learnable_sc = in_channels != out_channels or upsample\n         hidden_channels = out_channels if hidden_channels is None else hidden_channels\n         self.n_classes = n_classes\n         self.c1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=ksize, padding=pad)\n         #nn.init.xavier_uniform_(self.c1.weight.data, math.sqrt(2))\n         self.c2 = nn.Conv2d(hidden_channels, out_channels, kernel_size=ksize, padding=pad)\n         #nn.init.xavier_uniform_(self.c2.weight.data, math.sqrt(2))\n         self.b1 = nn.BatchNorm2d(in_channels)\n         self.b2 = nn.BatchNorm2d(hidden_channels)\n         if self.learnable_sc:\n             self.c_sc = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, padding=pad)\n     def residual(self, x):\n         h = x\n         h = self.b1(h)\n         h = self.activation(h)\n         h = upsample_conv(h, self.c1) if self.upsample else self.c1(h)\n         h = self.b2(h)\n         h = self.activation(h)\n         h = self.c2(h)\n         return h\n \n     def shortcut(self, x):\n         if self.learnable_sc:\n             x = upsample_conv(x, self.c_sc) if self.upsample else self.c_sc(x)\n             return x\n         else:\n             return x\n \n     def forward(self, input):\n         return self.residual(input) + self.shortcut(input)\n if __name__== \"__main__\":\n \n     noise = torch.randn(1,256, 4, 4).cuda()\n     g = genBlock(256, 256, activation=F.relu, upsample=True).cuda()\n     #g.apply(weights_init)\n     out = g(noise)\n     print(out.shape)\n </denchmark-code>\n \n Traceback (most recent call last):\n File \"test3.py\", line 56, in \n out = g(noise)\n File \"/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in call\n result = self.forward(*input, **kwargs)\n File \"test3.py\", line 50, in forward\n return self.residual(input) + self.shortcut(input)\n File \"test3.py\", line 34, in residual\n h = self.b1(h)\n File \"/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 493, in call\n result = self.forward(*input, **kwargs)\n File \"/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\", line 83, in forward\n exponential_average_factor, self.eps)\n File \"/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/functional.py\", line 1697, in batch_norm\n training, momentum, eps, torch.backends.cudnn.enabled\n RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n \t\t"}}}, "commit": {"commit_id": "649a0a16b91e4c81ec911ecc3be0439d9ebc2a52", "commit_author": "Chris Olivier", "commitT": "2018-04-09 14:43:53-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\nn\\cudnn\\cudnn_batch_norm-inl.h", "file_new_name": "src\\operator\\nn\\cudnn\\cudnn_batch_norm-inl.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "184,185,229,230,267,268", "deleted_lines": "168,169,229,230,267,268", "method_info": {"method_name": "mxnet::op::CuDNNBatchNormOp::Backward", "method_params": "ctx,inputs,req,outputs", "method_startline": "160", "method_endline": "272"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\python\\gpu\\test_operator_gpu.py", "file_new_name": "tests\\python\\gpu\\test_operator_gpu.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835", "deleted_lines": "1821", "method_info": {"method_name": "test_batchnorm_backwards_notrain", "method_params": "", "method_startline": "1820", "method_endline": "1835"}}}}}}}