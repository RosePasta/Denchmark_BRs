{"BR": {"BR_id": "15759", "BR_author": "sxjscience", "BRopenT": "2019-08-06T00:16:21Z", "BRcloseT": "2019-09-05T18:33:47Z", "BR_text": {"BRsummary": "[Optimizer][Bug] Gradient is mutated in the Adam optimizer", "BRdescription": "\n In the implementation of Adam: grad, mean and var are all mutated (See <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op-inl.h#L1307-L1315>https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op-inl.h#L1307-L1315</denchmark-link>\n ). However, the  flag is only set to {2, 3}, which should be {1, 2, 3}. (See <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op.cc#L699>https://github.com/apache/incubator-mxnet/blob/master/src/operator/optimizer_op.cc#L699</denchmark-link>\n )\n To reproduce the bug, you may check the value of the gradient before/after  <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1226-L1227>https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/optimizer/optimizer.py#L1226-L1227</denchmark-link>\n  .\n We can add the following into optimizer.py\n grad1 = grad.asnumpy()\n adam_update(weight, grad, mean, var, out=weight, ...)\n grad2 = grad.asnumpy()\n import numpy as np\n np.testing.assert_allclose(grad1, grad2)\n Create an adam optimizer with wd=1E-3 and train any model. You will find that the gradient has been mutated.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sxjscience", "commentT": "2019-08-06T00:16:23Z", "comment_text": "\n \t\tHey, this is the MXNet Label Bot.\n Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.\n Here are my recommended labels: Bug\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sxjscience", "commentT": "2019-08-07T14:30:46Z", "comment_text": "\n \t\tOn looking at other optimizer's updater,\n These are the cases where original grad is being updated.\n Will update the PR accordingly.\n RMSPropAlexUpdate\n \n \n \n incubator-mxnet/src/operator/optimizer_op-inl.h\n \n \n         Lines 1600 to 1619\n       in\n       7186123\n \n \n \n \n \n \n  inline void RMSPropAlexUpdate(const nnvm::NodeAttrs &attrs, \n \n \n \n  const OpContext &ctx, \n \n \n \n  const std::vector<TBlob> &inputs, \n \n \n \n  const std::vector<OpReqType> &req, \n \n \n \n  const std::vector<TBlob> &outputs) { \n \n \n \n  using namespace mshadow; \n \n \n \n  using namespace mshadow::expr; \n \n \n \n  using namespace mshadow_op; \n \n \n \n  const RMSPropAlexParam &param = nnvm::get<RMSPropAlexParam>(attrs.parsed); \n \n \n \n    Stream<xpu> *s = ctx.get_stream<xpu>(); \n \n \n \n  MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { \n \n \n \n      Tensor<xpu, 2, DType> weight = inputs[0].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> grad = inputs[1].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> state_n = inputs[2].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> state_g = inputs[3].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> delta = inputs[4].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> out = outputs[0].FlatTo2D<xpu, DType>(s); \n \n \n \n  \n \n \n \n      grad = scalar<DType>(param.rescale_grad) * grad + \n \n \n \n             scalar<DType>(param.wd) * weight; \n \n \n \n \n \n RMSPropUpdate\n \n \n \n incubator-mxnet/src/operator/optimizer_op-inl.h\n \n \n         Lines 1691 to 1708\n       in\n       7186123\n \n \n \n \n \n \n  template <typename xpu> \n \n \n \n  inline void RMSPropUpdate(const nnvm::NodeAttrs &attrs, const OpContext &ctx, \n \n \n \n  const std::vector<TBlob> &inputs, \n \n \n \n  const std::vector<OpReqType> &req, \n \n \n \n  const std::vector<TBlob> &outputs) { \n \n \n \n  using namespace mshadow; \n \n \n \n  using namespace mshadow::expr; \n \n \n \n  using namespace mshadow_op; \n \n \n \n  const RMSPropParam &param = nnvm::get<RMSPropParam>(attrs.parsed); \n \n \n \n    Stream<xpu> *s = ctx.get_stream<xpu>(); \n \n \n \n  MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { \n \n \n \n      Tensor<xpu, 2, DType> weight = inputs[0].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> grad = inputs[1].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> state_n = inputs[2].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> out = outputs[0].FlatTo2D<xpu, DType>(s); \n \n \n \n  \n \n \n \n      grad = scalar<DType>(param.rescale_grad) * grad + \n \n \n \n             scalar<DType>(param.wd) * weight; \n \n \n \n \n \n FtrlUpdate\n \n \n \n incubator-mxnet/src/operator/optimizer_op-inl.h\n \n \n         Lines 1784 to 1803\n       in\n       7186123\n \n \n \n \n \n \n  template<typename xpu> \n \n \n \n  inline void FtrlUpdate(const nnvm::NodeAttrs& attrs, \n \n \n \n  const OpContext &ctx, \n \n \n \n  const std::vector<TBlob> &inputs, \n \n \n \n  const std::vector<OpReqType> &req, \n \n \n \n  const std::vector<TBlob> &outputs) { \n \n \n \n  using namespace mshadow; \n \n \n \n  using namespace mshadow::expr; \n \n \n \n  using namespace mshadow_op; \n \n \n \n  const FtrlParam& param = nnvm::get<FtrlParam>(attrs.parsed); \n \n \n \n    Stream<xpu>* s = ctx.get_stream<xpu>(); \n \n \n \n  MSHADOW_REAL_TYPE_SWITCH(inputs[0].type_flag_, DType, { \n \n \n \n      Tensor<xpu, 2, DType> weight = inputs[0].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> grad = inputs[1].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> z = inputs[2].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> n = inputs[3].FlatTo2D<xpu, DType>(s); \n \n \n \n      Tensor<xpu, 2, DType> out = outputs[0].FlatTo2D<xpu, DType>(s); \n \n \n \n  \n \n \n \n      grad = scalar<DType>(param.rescale_grad) * grad; \n \n \n \n  \n \n \n \n \n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sxjscience", "commentT": "2019-09-05T18:34:58Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/kshitij12345>@kshitij12345</denchmark-link>\n  !\n \t\t"}}}, "commit": {"commit_id": "d60be31df6b05385cd8adc4b8b26b34a33e7693c", "commit_author": "kshitij12345", "commitT": "2019-09-05 11:33:43-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 8, "file_old_name": "src\\operator\\optimizer_op-inl.h", "file_new_name": "src\\operator\\optimizer_op-inl.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317", "deleted_lines": "1302,1303,1304,1314,1315,1316,1317", "method_info": {"method_name": "mxnet::op::AdamUpdateKernel::Map", "method_params": "i,out_data,mean_data,var_data,weight_data,grad_data,clip_gradient,rescale_grad,beta1,beta2,lr,wd,epsilon,req", "method_startline": "1298", "method_endline": "1317"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643", "deleted_lines": "1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643", "method_info": {"method_name": "mxnet::op::RMSPropAlexUpdateKernel::Map", "method_params": "i,out_data,state_n_data,state_g_data,delta_data,weight_data,grad_data,clip_gradient,rescale_grad,gamma1,gamma2,lr,wd,clip_weights,epsilon,req", "method_startline": "1613", "method_endline": "1643"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1652,1656,1657,1658,1659,1660,1661,1663,1664,1665,1666,1667,1668", "deleted_lines": "1647,1648,1649", "method_info": {"method_name": "mxnet::op::RMSPropAlexUpdate", "method_params": "attrs,ctx,inputs,req,outputs", "method_startline": "1647", "method_endline": "1670"}}, "hunk_3": {"Ismethod": 1, "added_lines": "1824,1825,1835,1836,1837,1838,1839", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::FtrlUpdate", "method_params": "attrs,ctx,inputs,req,outputs", "method_startline": "1819", "method_endline": "1841"}}, "hunk_4": {"Ismethod": 1, "added_lines": "1326,1336,1337,1338,1339,1340,1341", "deleted_lines": "1321,1322,1323,1324,1325,1326,1327,1328,1329", "method_info": {"method_name": "mxnet::op::AdamUpdate", "method_params": "attrs,ctx,inputs,req,outputs", "method_startline": "1321", "method_endline": "1343"}}, "hunk_5": {"Ismethod": 1, "added_lines": "1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815", "deleted_lines": "1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815", "method_info": {"method_name": "mxnet::op::FtrlUpdateKernel::Map", "method_params": "i,out_data,n_data,z_data,weight_data,grad_data,clip_gradient,rescale_grad,beta,lamda1,lr,wd,req", "method_startline": "1793", "method_endline": "1815"}}, "hunk_6": {"Ismethod": 1, "added_lines": "1742,1746,1747,1748,1749,1751,1752,1753,1754,1755", "deleted_lines": "1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748", "method_info": {"method_name": "mxnet::op::RMSPropUpdate", "method_params": "attrs,ctx,inputs,req,outputs", "method_startline": "1738", "method_endline": "1757"}}, "hunk_7": {"Ismethod": 1, "added_lines": "1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734", "deleted_lines": "1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734", "method_info": {"method_name": "mxnet::op::RMSPropUpdateKernel::Map", "method_params": "i,out_data,state_n_data,weight_data,grad_data,clip_gradient,rescale_grad,gamma1,lr,wd,clip_weights,epsilon,req", "method_startline": "1712", "method_endline": "1734"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\python\\unittest\\test_ndarray.py", "file_new_name": "tests\\python\\unittest\\test_ndarray.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952", "deleted_lines": null, "method_info": {"method_name": "test_update_ops_mutation", "method_params": "", "method_startline": "1891", "method_endline": "1952"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924", "deleted_lines": null, "method_info": {"method_name": "test_update_ops_mutation.test_op", "method_params": "op,num_inputs,mutated_inputs,kwargs", "method_startline": "1899", "method_endline": "1924"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1892,1893,1894", "deleted_lines": null, "method_info": {"method_name": "test_update_ops_mutation.assert_mutate", "method_params": "x,y,op", "method_startline": "1892", "method_endline": "1894"}}, "hunk_3": {"Ismethod": 1, "added_lines": "1896,1897", "deleted_lines": null, "method_info": {"method_name": "test_update_ops_mutation.assert_unchanged", "method_params": "x,y,op", "method_startline": "1896", "method_endline": "1897"}}}}}}}