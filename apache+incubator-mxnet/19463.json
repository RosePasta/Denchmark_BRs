{"BR": {"BR_id": "19463", "BR_author": "sxjscience", "BRopenT": "2020-11-02T07:18:05Z", "BRcloseT": "2020-11-19T23:58:33Z", "BR_text": {"BRsummary": "[Bug][AMP][2.0] AMP issue of the concatenate operator", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n Reproducible example:\n import mxnet as mx\n from mxnet.gluon import nn\n from mxnet import amp\n mx.npx.set_np()\n amp.init()\n \n \n class Foo(nn.HybridBlock):\n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         self.dense0 = nn.Dense(16, in_units=8)\n \n     def forward(self, x):\n         return mx.np.concatenate([self.dense0(x), x], axis=-1)\n \n foo = Foo()\n foo.initialize(ctx=mx.gpu())\n \n data = mx.np.random.normal(0, 1, (32, 8), ctx=mx.gpu())\n out = foo(data)\n print(out.dtype)\n Output:\n <denchmark-code>---------------------------------------------------------------------------\n MXNetError                                Traceback (most recent call last)\n <ipython-input-9-07e1c93ce642> in <module>\n      18 \n      19 data = mx.np.random.normal(0, 1, (32, 8), ctx=mx.gpu())\n ---> 20 out = foo(data)\n      21 print(out.dtype)\n \n ~/.local/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, x, *args)\n    1419             if not self._active:\n    1420                 # Normal imperative computation of forward()\n -> 1421                 return super().__call__(x, *args)\n    1422 \n    1423             if dc.is_deferred_compute():\n \n ~/.local/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)\n     709             hook(self, args)\n     710 \n --> 711         out = self.forward(*args)\n     712 \n     713         for hook in self._forward_hooks.values():\n \n <ipython-input-9-07e1c93ce642> in forward(self, x)\n      12 \n      13     def forward(self, x):\n ---> 14         return mx.np.concatenate([self.dense0(x), x], axis=-1)\n      15 \n      16 foo = Foo()\n \n ~/.local/lib/python3.6/site-packages/mxnet/numpy/multiarray.py in concatenate(seq, axis, out)\n    6520     array([1., 2., 3., 4., 5., 6.])\n    6521     \"\"\"\n -> 6522     return _mx_nd_np.concatenate(seq, axis=axis, out=out)\n    6523 \n    6524 \n \n ~/.local/lib/python3.6/site-packages/mxnet/ndarray/numpy/_op.py in concatenate(seq, axis, out)\n    4522            [3., 4., 6.]])\n    4523     \"\"\"\n -> 4524     return _api_internal.concatenate(*seq, axis, out)\n    4525 \n    4526 \n \n ~/.local/lib/python3.6/site-packages/mxnet/_ffi/_ctypes/function.py in __call__(self, *args)\n     113                 self.handle, values, tcodes, ctypes.c_int(num_args),\n     114                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:\n --> 115             raise get_last_ffi_error()\n     116         _ = temp_args\n     117         _ = args\n \n MXNetError: MXNetError: Type inconsistent, Provided = float32, inferred type = float16\n </denchmark-code>\n \n <denchmark-link:https://github.com/mk-61>@mk-61</denchmark-link>\n  Would you take a look on this? I'm not super familiar with the amp code so it may take me more time to resolve this issue.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sxjscience", "commentT": "2020-11-03T23:49:13Z", "comment_text": "\n \t\tRemoved the  because <denchmark-link:https://github.com/mk-61>@mk-61</denchmark-link>\n  is able to reproduce the issue per offline discussion.\n \t\t"}}}, "commit": {"commit_id": "66488662ea6d33b8aeb64b632ea45194181f794a", "commit_author": "mk-61", "commitT": "2020-11-19 15:58:32-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "ci\\docker\\runtime_functions.sh", "file_new_name": "ci\\docker\\runtime_functions.sh", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "783,785,837,842,857,862,874,879,905,910", "deleted_lines": "783,836,855,871,901"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\mxnet\\amp\\amp.py", "file_new_name": "python\\mxnet\\amp\\amp.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "83", "deleted_lines": "83", "method_info": {"method_name": "_get_nd_fun_to_wrap", "method_params": "name,module,submodule_dict", "method_startline": "67", "method_endline": "83"}}, "hunk_1": {"Ismethod": 1, "added_lines": "90,92,93,94,95,96,97,98,99,100,101,102,103", "deleted_lines": "90,92,93,94,95,96", "method_info": {"method_name": "_get_np_fun_to_wrap", "method_params": "name,ns_prefix", "method_startline": "85", "method_endline": "103"}}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\python\\gpu\\test_amp_init.py"}}}}