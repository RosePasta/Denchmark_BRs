{"BR": {"BR_id": "4253", "BR_author": "ArsenLuca", "BRopenT": "2016-12-16T02:08:41Z", "BRcloseT": "2018-04-25T15:42:02Z", "BR_text": {"BRsummary": "training-accuracy nan", "BRdescription": "\n When I was training using my own dataset, the training accuracy was shown to be nan when in epoch 8N-1(e.g. epoch 7, 15,23,31....). This makes me feel puzzled....\n Also, in the example <denchmark-link:https://github.com/dmlc/mxnet-notebooks/blob/master/python/how_to/finetune.ipynb>https://github.com/dmlc/mxnet-notebooks/blob/master/python/how_to/finetune.ipynb</denchmark-link>\n , the training accuracy falls to be nan in epoch 6. I was wondering how did this happen?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ArsenLuca", "commentT": "2016-12-16T07:16:37Z", "comment_text": "\n \t\tbecause when called get() for a metric, the count is 0, which is often due to reset() is called. it  doesn't not affect model, but sometime annoying.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ArsenLuca", "commentT": "2016-12-16T08:23:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mli>@mli</denchmark-link>\n  many thanks\uff0cit's really annoying...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ArsenLuca", "commentT": "2017-08-28T09:45:47Z", "comment_text": "\n \t\tsame problem. really annoying\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ArsenLuca", "commentT": "2017-09-26T09:14:50Z", "comment_text": "\n \t\tsame problem. Does anyone know how to fix it?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ArsenLuca", "commentT": "2017-10-12T08:57:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/apache/incubator-mxnet/issues/5528>#5528</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ArsenLuca", "commentT": "2018-04-06T08:53:44Z", "comment_text": "\n \t\tThe \"Epoch[x] Train-accuracy=xxx\" line printed at the end of every epoch gives the impression that the accuracy metric is for the entire epoch. In reality, it is NOT.\n For example if an epoch consists of 101 batches and we were printing metrics every 10 batch because callback was created that way, what will be printed as 'Train-accuracy=xxx' at the end of an epoch is actually just the accuracy from a single batch (101'st batch). Printing this as 'Epoch[x] Train-accuracy=xxx' is very misleading.\n Ideally we should remove <denchmark-link:https://github.com/apache/incubator-mxnet/blob/62bf9ec16886434d5e4eac279db19cb18c5c9c45/python/mxnet/module/base_module.py#L535>this</denchmark-link>\n  misleading print statement. But then, I'm sure there is a lot of existing scripts out there that look for this statement. We can't remove this without breaking those scripts. Since this will be a breaking change, let's do it in a major version change.\n For now, to avoid the nan error, we can avoid resetting the metrics when processing callback for the last batch. It will be reset at the beginning of the next epoch anyway.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ArsenLuca", "commentT": "2018-04-06T08:55:03Z", "comment_text": "\n \t\tThis should fix it:\n <denchmark-code>diff --git a/python/mxnet/module/base_module.py b/python/mxnet/module/base_module.py\n index c03f8e7..10b3e75 100644\n --- a/python/mxnet/module/base_module.py\n +++ b/python/mxnet/module/base_module.py\n @@ -22,6 +22,7 @@\n  import time\n  import logging\n  import warnings\n +from copy import deepcopy\n  \n  from .. import metric\n  from .. import ndarray\n @@ -523,8 +524,9 @@ class BaseModule(object):\n                      monitor.toc_print()\n  \n                  if batch_end_callback is not None:\n +                    arg_eval_metric = eval_metric if not end_of_batch else deepcopy(eval_metric)\n                      batch_end_params = BatchEndParam(epoch=epoch, nbatch=nbatch,\n -                                                     eval_metric=eval_metric,\n +                                                     eval_metric=arg_eval_metric,\n                                                       locals=locals())\n                      for callback in _as_list(batch_end_callback):\n                          callback(batch_end_params)\n \n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ArsenLuca", "commentT": "2018-04-14T06:04:16Z", "comment_text": "\n \t\tIndu - why is this issue still open if PR 10437 fixed the issue?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ArsenLuca", "commentT": "2018-04-25T15:42:02Z", "comment_text": "\n \t\tFixed by <denchmark-link:https://github.com/apache/incubator-mxnet/pull/10437>#10437</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "6abcdbb3333979d2aa8d7c3c7253762e921ba478", "commit_author": "Indu Bharathi", "commitT": "2018-04-09 09:56:54-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\mxnet\\module\\base_module.py", "file_new_name": "python\\mxnet\\module\\base_module.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "525,526,527,537", "deleted_lines": "534"}}}}}}