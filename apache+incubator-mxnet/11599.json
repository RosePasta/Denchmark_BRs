{"BR": {"BR_id": "11599", "BR_author": "junrushao1994", "BRopenT": "2018-07-07T06:52:45Z", "BRcloseT": "2018-08-16T00:07:19Z", "BR_text": {"BRsummary": "Autograd fails when using `take` operator repeatedly", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n Here provides a strange example that a backward pass on the accumulated sum of a 2-d NDArray sc, may potentially cause autograd to fail, depending on the shape of sc.\n <denchmark-h:h2>Minimum reproducible example</denchmark-h>\n \n TL;DR. This code calculates x = sum(sc[i, j] for each i, j), and then invokes a backward pass x.backward(). The error message says that it could not calculate the gradient w.r.t. some variables, but I didn't require gradient for non-differentiable variables like i or j.\n import mxnet as mx\n \n def _array(shape):\n     \"\"\"Create an NDArray with random entries and the given shape\n     \"\"\"\n     return mx.nd.random.uniform(-1.0, 1.0, shape=shape, dtype=\"float32\")\n \n def index(sc, i, j):\n     \"\"\"Equivalent to sc[i, j] in numpy\n     \"\"\"\n     return sc.take(i).squeeze(axis=0)  \\\n              .take(j).squeeze(axis=0)\n \n # tweaking `sc` in different shapes, the code sometimes fails, sometimes not\n row_len, col_len = 2, 8\n \n # the scanned variable\n sc = _array((row_len, col_len))\n sc.attach_grad()  # we only require the gradient w.r.t. `sc`\n \n # `i`, `j` are loop variables which don't require grad\n i = mx.nd.array([0], dtype=\"int64\")\n j = mx.nd.array([0], dtype=\"int64\")\n \n with mx.autograd.record(train_mode=True):\n     xs = []\n     for _ in range(row_len):\n         x_i = []\n         for _ in range(col_len):\n             x_ij = index(sc, i, j)\n             x_i.append(x_ij)\n             j = j + 1\n         i = i + 1\n         j = j - col_len  # reset j\n         xs.append(mx.nd.stack(*x_i))\n     x = mx.nd.stack(*xs)\n     x = x.sum()\n \n x.backward()\n print(sc.grad.asnumpy())   # the expected result should be all `1`s\n <denchmark-h:h2>Error Message</denchmark-h>\n \n <denchmark-code>>> python2 test.py\n /home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n   from ._conv import register_converters as _register_converters\n Traceback (most recent call last):\n   File \"test.py\", line 33, in <module>\n     print(sc.grad.asnumpy())\n   File \"/home/ubuntu/Projects/mxnet/python/mxnet/ndarray/ndarray.py\", line 1910, in asnumpy\n     ctypes.c_size_t(data.size)))\n   File \"/home/ubuntu/Projects/mxnet/python/mxnet/base.py\", line 210, in check_call\n     raise MXNetError(py_str(_LIB.MXGetLastError()))\n mxnet.base.MXNetError: [06:43:28] src/operator/tensor/./indexing_op.h:829: Check failed: req[take_::kIdx] == kNullOp (1 vs. 0) take layer doesn't support gradient into index\n \n Stack trace returned 10 entries:\n [bt] (0) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f740cb6231b]\n [bt] (1) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f740cb62b58]\n [bt] (2) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::op::TakeOpBackward<mshadow::cpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0x220) [0x7f740e46c710]\n [bt] (3) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x291) [0x7f740f26a7b1]\n [bt] (4) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x68) [0x7f740f7a7538]\n [bt] (5) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x47) [0x7f740f7a7517]\n [bt] (6) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x47) [0x7f740f7a7517]\n [bt] (7) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x47) [0x7f740f7a7517]\n [bt] (8) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x47) [0x7f740f7a7517]\n [bt] (9) /home/ubuntu/Projects/mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x47) [0x7f740f7a7517]\n </denchmark-code>\n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n \n Save the code as test.py, and run python2 test.py\n \n <denchmark-h:h2>What have you tried to solve it?</denchmark-h>\n \n I have no idea where it comes from. That by tweaking row_len and col_len, it behaves differently seems weird to me. I only observed that\n \n When row_len = 1, it never fails\n When row_len = 2, and col_len < 8, it doesn't fails; when col_len >= 8, it fails\n When row_len = 3, and col_len < 7, it doesn't fails; when col_len >= 7, it fails\n When row_len = 4, and col_len < 7, it doesn't fails; when col_len >= 7, it fails\n \n <denchmark-h:h2>Environment info</denchmark-h>\n \n Not relevant though.\n <denchmark-code>----------Python Info----------\n ('Version      :', '2.7.15')\n ('Compiler     :', 'GCC 7.2.0')\n ('Build        :', ('default', 'May  1 2018 23:32:55'))\n ('Arch         :', ('64bit', ''))\n ------------Pip Info-----------\n ('Version      :', '10.0.1')\n ('Directory    :', '/home/ubuntu/anaconda2/lib/python2.7/site-packages/pip')\n ----------MXNet Info-----------\n /home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n   from ._conv import register_converters as _register_converters\n ('Version      :', '1.3.0')\n ('Directory    :', '/home/ubuntu/Projects/junru-mxnet/python/mxnet')\n Hashtag not found. Not installed from pre-built package.\n ----------System Info----------\n ('Platform     :', 'Linux-4.4.0-1062-aws-x86_64-with-debian-stretch-sid')\n ('system       :', 'Linux')\n ('node         :', 'ip-172-31-42-30')\n ('release      :', '4.4.0-1062-aws')\n ('version      :', '#71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018')\n ----------Hardware Info----------\n ('machine      :', 'x86_64')\n ('processor    :', 'x86_64')\n Architecture:          x86_64\n CPU op-mode(s):        32-bit, 64-bit\n Byte Order:            Little Endian\n CPU(s):                72\n On-line CPU(s) list:   0-71\n Thread(s) per core:    2\n Core(s) per socket:    18\n Socket(s):             2\n NUMA node(s):          2\n Vendor ID:             GenuineIntel\n CPU family:            6\n Model:                 85\n Model name:            Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz\n Stepping:              3\n CPU MHz:               3000.000\n BogoMIPS:              6000.00\n Hypervisor vendor:     KVM\n Virtualization type:   full\n L1d cache:             32K\n L1i cache:             32K\n L2 cache:              1024K\n L3 cache:              25344K\n NUMA node0 CPU(s):     0-17,36-53\n NUMA node1 CPU(s):     18-35,54-71\n Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single kaiser fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f rdseed adx smap clflushopt clwb avx512cd xsaveopt xsavec xgetbv1 ida arat\n ----------Network Test----------\n Setting timeout: 10\n Timing for MXNet: https://github.com/apache/incubator-mxnet, DNS: 0.0019 sec, LOAD: 0.5934 sec.\n Timing for PYPI: https://pypi.python.org/pypi/pip, DNS: 0.0070 sec, LOAD: 0.1024 sec.\n Timing for FashionMNIST: https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz, DNS: 0.0099 sec, LOAD: 0.5567 sec.\n Timing for Conda: https://repo.continuum.io/pkgs/free/, DNS: 0.0035 sec, LOAD: 0.0305 sec.\n Timing for Gluon Tutorial(en): http://gluon.mxnet.io, DNS: 0.0994 sec, LOAD: 0.5750 sec.\n Timing for Gluon Tutorial(cn): https://zh.gluon.ai, DNS: 0.1169 sec, LOAD: 0.5358 sec.\n </denchmark-code>\n \n <denchmark-h:h2>Build info</denchmark-h>\n \n Compiler: gcc\n MXNet commit hash: <denchmark-link:https://github.com/apache/incubator-mxnet/commit/dd954b45a35d5eb9e8cdb6c6e19dd872a0a3d84d>dd954b4</denchmark-link>\n  (current HEAD)\n Build config: default\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "junrushao1994", "commentT": "2018-07-07T06:54:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/haojin2>@haojin2</denchmark-link>\n  <denchmark-link:https://github.com/zheng-da>@zheng-da</denchmark-link>\n  Could you help take a look at this? Did I misunderstand anything about the  operator? Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "junrushao1994", "commentT": "2018-07-09T23:09:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sandeep-krishnamurthy>@sandeep-krishnamurthy</denchmark-link>\n  Could you please label this issue: ,\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "junrushao1994", "commentT": "2018-07-09T23:11:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/junrushao1994>@junrushao1994</denchmark-link>\n  will take a look later today\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "junrushao1994", "commentT": "2018-07-11T00:43:44Z", "comment_text": "\n \t\tSo the problem is that the req type of the indices is not null, maybe this is due to the usage, I'll take a deeper dive into your issue later.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "junrushao1994", "commentT": "2018-07-31T18:55:52Z", "comment_text": "\n \t\tHey any updates?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "junrushao1994", "commentT": "2018-08-02T00:26:05Z", "comment_text": "\n \t\tFix in <denchmark-link:https://github.com/apache/incubator-mxnet/pull/11983>#11983</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "junrushao1994", "commentT": "2018-08-02T00:37:43Z", "comment_text": "\n \t\tThank you <denchmark-link:https://github.com/haojin2>@haojin2</denchmark-link>\n ! You rock!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "junrushao1994", "commentT": "2018-08-02T00:40:17Z", "comment_text": "\n \t\tWill close the issue once <denchmark-link:https://github.com/apache/incubator-mxnet/pull/11983>#11983</denchmark-link>\n  is merged\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "junrushao1994", "commentT": "2018-08-16T00:07:19Z", "comment_text": "\n \t\tClosed since <denchmark-link:https://github.com/apache/incubator-mxnet/pull/11983>#11983</denchmark-link>\n  is merged. Thank you <denchmark-link:https://github.com/haojin2>@haojin2</denchmark-link>\n !\n \t\t"}}}, "commit": {"commit_id": "0455a112c9aaa681b3e3e194975eb46d8ac4fcc3", "commit_author": "Hao Jin", "commitT": "2018-08-15 15:31:14-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\tensor\\indexing_op.h", "file_new_name": "src\\operator\\tensor\\indexing_op.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1037,1038,1055,1056,1057,1058,1059", "deleted_lines": "1037,1038", "method_info": {"method_name": "mxnet::op::TakeOpBackward", "method_params": "attrs,ctx,inputs,req,outputs", "method_startline": "1028", "method_endline": "1094"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\python\\unittest\\test_operator.py", "file_new_name": "tests\\python\\unittest\\test_operator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3852", "deleted_lines": null, "method_info": {"method_name": "test_take", "method_params": "", "method_startline": "3762", "method_endline": "3852"}}, "hunk_1": {"Ismethod": 1, "added_lines": "3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838", "deleted_lines": null, "method_info": {"method_name": "test_take.check_autograd_req", "method_params": "", "method_startline": "3815", "method_endline": "3838"}}}}}}}