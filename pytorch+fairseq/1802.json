{"BR": {"BR_id": "1802", "BR_author": "avi-jit", "BRopenT": "2020-03-08T21:16:49Z", "BRcloseT": "2020-03-09T13:02:41Z", "BR_text": {"BRsummary": "AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'", "BRdescription": "\n <denchmark-h:h4>AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'</denchmark-h>\n \n I'm trying to finetune roberta by registering a new classification head, as explained here: <denchmark-link:github.com/pytorch/fairseq/blob/master/examples/roberta/README.custom_classification.md>github.com/pytorch/fairseq/blob/master/examples/roberta/README.custom_classification.md</denchmark-link>\n . I've tokenized and preprocessed my data as described but the training phase is where I'm stuck.\n <denchmark-h:h4>Code & Output</denchmark-h>\n \n Here's the execution script and arguments:\n <denchmark-code>WARMUP_UPDATES=479\n TOTAL_NUM_UPDATES=7812\n LR=1e-05\n HEAD_NAME=head_name\n NUM_CLASSES=1014\n MAX_SENTENCES=2\n ROBERTA_PATH=roberta.large/model.pt \n CUDA_VISIBLE_DEVICES=0\n DATA_DIR=../data/\n MAX_TOKENS=4\n \n python train.py $DATA_DIR \\\n --restore-file $ROBERTA_PATH --max-positions 512 --max-sentences $MAX_SENTENCES \\\n --max-tokens $MAX_TOKENS --task sentence_prediction --reset-optimizer \\\n --reset-dataloader --reset-meters --required-batch-size-multiple 1 --init-token 0 \\\n --separator-token 2 --arch roberta_large --criterion sentence_prediction \\\n --classification-head-name $HEAD_NAME --num-classes $NUM_CLASSES --dropout 0.1 \\\n --attention-dropout 0.1 --weight-decay 0.1 --optimizer adam \\\n --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 --clip-norm 0.0 \\\n --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES \\\n --warmup-updates $WARMUP_UPDATES --fp16 --fp16-init-scale 4 \\\n --threshold-loss-scale 1 --fp16-scale-window 128 --max-epoch 10 \\\n --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n --truncate-sequence --find-unused-parameters --update-freq 4 \n </denchmark-code>\n \n And here's the error trace:\n <denchmark-code>2020-03-08 13:40:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, add_prev_output_tokens=False, all_gather_list_size=16384, arch='roberta_large', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe=None, broadcast_buffers=False, bucket_cap_mb=25, classification_head_name='head_name', clip_norm=0.0, cpu=False, criterion='sentence_prediction', curriculum=0, data='../data/', dataset_impl=None, ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=24, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=128, init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=100, lr=[1e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_positions=512, max_sentences=2, max_sentences_valid=2, max_tokens=4, max_tokens_valid=4, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_shuffle=False, num_classes=1014, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, regression_target=False, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='roberta.large/model.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, separator_token=2, skip_invalid_size_inputs_valid_test=False, task='sentence_prediction', tensorboard_logdir='', threshold_loss_scale=1.0, tokenizer=None, total_num_update=7812, train_subset='train', truncate_sequence=True, update_freq=[4], use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=479, weight_decay=0.1)\n 2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n 2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 1009 types\n 2020-03-08 13:40:04 | INFO | fairseq.data.data_utils | loaded 40151 examples from: ../data/input0/valid\n 2020-03-08 13:40:04 | INFO | fairseq.data.data_utils | loaded 40151 examples from: ../data/label/valid\n 2020-03-08 13:40:04 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 40151\n 2020-03-08 13:40:11 | INFO | fairseq_cli.train | RobertaModel \n </denchmark-code>\n \n ... I'm removing the model specs (layer wise) since it's too long ...\n <denchmark-code>2020-03-08 13:40:11 | INFO | fairseq_cli.train | model roberta_large, criterion SentencePredictionCriterion\n 2020-03-08 13:40:11 | INFO | fairseq_cli.train | num. model params: 357499983 (num. trained: 357499983)\n 2020-03-08 13:40:11 | INFO | fairseq_cli.train | training on 1 GPUs\n 2020-03-08 13:40:11 | INFO | fairseq_cli.train | max tokens per GPU = 4 and max sentences per GPU = 2\n 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.dense.weight\n 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.dense.bias\n 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.out_proj.weight\n 2020-03-08 13:40:11 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.head_name.out_proj.bias\n 2020-03-08 13:40:11 | INFO | fairseq.trainer | loaded checkpoint roberta.large/model.pt (epoch 1 @ 0 updates)\n 2020-03-08 13:40:11 | INFO | fairseq.trainer | loading train data for epoch 1\n 2020-03-08 13:40:11 | INFO | fairseq.data.data_utils | loaded 160600 examples from: ../data/input0/train\n 2020-03-08 13:40:11 | INFO | fairseq.data.data_utils | loaded 160600 examples from: ../data/label/train\n 2020-03-08 13:40:11 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 160600\n 2020-03-08 13:40:11 | WARNING | fairseq.data.data_utils | 160580 samples have invalid sizes and will be skipped, max_positions=4, first few sample ids=[141335, 12811, 122058, 104432, 1925, 141500, 65517, 143950, 59283, 155828]\n Traceback (most recent call last):\n   File \"train.py\", line 11, in <module>\n     cli_main()\n   File \"/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py\", line 322, in cli_main\n     main(args)\n   File \"/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py\", line 100, in main\n     train(args, trainer, task, epoch_itr)\n   File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/contextlib.py\", line 74, in inner\n     return func(*args, **kwds)\n   File \"/nas/home/thawani/MCS/fairseq/fairseq_cli/train.py\", line 177, in train\n     log_output = trainer.train_step(samples)\n   File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/contextlib.py\", line 74, in inner\n     return func(*args, **kwds)\n   File \"/nas/home/thawani/MCS/fairseq/fairseq/trainer.py\", line 319, in train_step\n     ignore_grad=is_dummy_batch,\n   File \"/nas/home/thawani/MCS/fairseq/fairseq/tasks/fairseq_task.py\", line 337, in train_step\n     loss, sample_size, logging_output = criterion(model, sample)\n   File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/nas/home/thawani/MCS/fairseq/fairseq/criterions/sentence_prediction.py\", line 40, in forward\n     and self.args.classification_head_name in model.classification_heads\n   File \"/nas/home/thawani/anaconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 576, in __getattr__\n     type(self).__name__, name))\n AttributeError: 'SentencePredictionCriterion' object has no attribute 'args'\n \n </denchmark-code>\n \n <denchmark-h:h4>What have you tried?</denchmark-h>\n \n \n creating new environment and trying again from scratch: same issue.\n tracing the error through the code: I can't make heads or tails out of it.\n \n <denchmark-h:h4>What's your environment?</denchmark-h>\n \n \n fairseq Version: 0.9.0\n PyTorch Version: 1.4.0\n OS: CentOS Linux release 7.7.1908\n How you installed fairseq: source\n Build command used: pip install --editable .\n Python version: 3.7.6\n CUDA/cuDNN version: 10.2\n GPU models and configuration: NVIDIA TU102\n Any other relevant information: I'm using a conda enviornment\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "avi-jit", "commentT": "2020-03-08T22:42:05Z", "comment_text": "\n \t\tThanks, this is a bug introduced in our recent refactoring of criterions to not depend on args: <denchmark-link:https://github.com/pytorch/fairseq/commit/46b773a393c423f653887c382e4d55e69627454d>46b773a</denchmark-link>\n . I'll submit a fix\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "avi-jit", "commentT": "2020-03-08T22:47:00Z", "comment_text": "\n \t\tLooks like a quick fix - <denchmark-link:https://github.com/myleott>@myleott</denchmark-link>\n  want me to take it?\n \t\t"}}}, "commit": {"commit_id": "9e1fb47035e87df5d1b7fd86dc152b666dca9217", "commit_author": "Elijah Rippeth", "commitT": "2020-03-09 06:02:31-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "fairseq\\criterions\\sentence_prediction.py", "file_new_name": "fairseq\\criterions\\sentence_prediction.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "41,47,52,73", "deleted_lines": "40,46,51,72", "method_info": {"method_name": "forward", "method_params": "self,model,sample,reduce", "method_startline": "31", "method_endline": "77"}}, "hunk_1": {"Ismethod": 1, "added_lines": "18", "deleted_lines": "18", "method_info": {"method_name": "__init__", "method_params": "self,task,classification_head_name", "method_startline": "18", "method_endline": "20"}}, "hunk_2": {"Ismethod": 1, "added_lines": "18,21", "deleted_lines": "18", "method_info": {"method_name": "__init__", "method_params": "self,task,classification_head_name,regression_target", "method_startline": "18", "method_endline": "21"}}}}}}}