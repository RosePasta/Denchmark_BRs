{"BR": {"BR_id": "1826", "BR_author": "freewym", "BRopenT": "2020-03-12T05:04:42Z", "BRcloseT": "2020-03-21T18:15:44Z", "BR_text": {"BRsummary": "CUDA error when training with multiple GPUs and criterion's logging_outputs_can_be_summed() is False", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Currently almost all subclass of FairseqCriterion 's logging_outputs_can_be_summed() returns True, and it trains OK with multiple GPUs. But if I change, for example, CrossEntropyCriterion's  logging_outputs_can_be_summed () to return False and train with 2 GPUs , it gives error:\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\n raise Exception(msg)\n Exception:\n -- Process 1 terminated with the following error:\n Traceback (most recent call last):\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n fn(i, *args)\n File \"/home/ywang/fairseq6/train.py\", line 286, in distributed_main\n main(args, init_distributed=True)\n File \"/home/ywang/fairseq6/train.py\", line 96, in main\n train(args, trainer, task, epoch_itr)\n File \"/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\n return func(*args, **kwds)\n File \"/home/ywang/fairseq6/train.py\", line 176, in train\n log_output = trainer.train_step(samples)\n File \"/export/b02/ywang/anaconda3/lib/python3.7/contextlib.py\", line 74, in inner\n return func(*args, **kwds)\n File \"/home/ywang/fairseq6/fairseq/trainer.py\", line 347, in train_step\n logging_outputs, sample_size, ooms, ignore=is_dummy_batch,\n File \"/home/ywang/fairseq6/fairseq/trainer.py\", line 616, in _aggregate_logging_outputs\n logging_outputs, *extra_stats_to_sum, ignore=ignore\n File \"/home/ywang/fairseq6/fairseq/trainer.py\", line 634, in _all_gather_list_sync\n max_size=getattr(self.args, 'all_gather_list_size', 16384),\n File \"/home/ywang/fairseq6/fairseq/distributed_utils.py\", line 176, in all_gather_list\n result.append(pickle.loads(bytes(out_buffer[header_size:header_size + enc_size].tolist())))\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/storage.py\", line 134, in _load_from_bytes\n return torch.load(io.BytesIO(b))\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 529, in load\n return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 702, in _legacy_load\n result = unpickler.load()\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 665, in persistent_load\n deserialized_objects[root_key] = restore_location(obj, location)\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 156, in default_restore_location\n result = fn(storage, location)\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/serialization.py\", line 136, in _cuda_deserialize\n return storage_type(obj.size())\n File \"/export/b02/ywang/anaconda3/lib/python3.7/site-packages/torch/cuda/init.py\", line 480, in _lazy_new\n return super(_CudaBase, cls).new(cls, *args, **kwargs)\n RuntimeError: CUDA error: all CUDA-capable devices are busy or unavailable\n The reason why I want it to return False is, I have some stats in logging_output which is a tensor not scalar, to be aggregated in reduce_matrices().\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior (always include the command you ran):\n \n Just have CrossEntropyCriterion.logging_outputs_can_be_summed() to return False and train with multi-GPUs\n See error\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n \n fairseq Version (e.g., 1.0 or master): master\n PyTorch Version (e.g., 1.0): 1.4\n OS (e.g., Linux): Linux\n How you installed fairseq (pip, source): source\n Build command you used (if compiling from source):\n Python version: 3.7\n CUDA/cuDNN version: 10.2\n GPU models and configuration:  GeForce GTX 1080\n Any other relevant information:\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "freewym", "commentT": "2020-03-20T21:29:40Z", "comment_text": "\n \t\tMerging the above fix, please reopen if this is still an issue after this is merged!\n \t\t"}}}, "commit": {"commit_id": "aed8cf4c38db088c2f640fa941a7d2c462d72b16", "commit_author": "Myle Ott", "commitT": "2020-03-21 11:16:20-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\benchmark\\dummy_lm.py", "file_new_name": "fairseq\\benchmark\\dummy_lm.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "46", "deleted_lines": "41,42", "method_info": {"method_name": "setup_task", "method_params": "cls,args,kwargs", "method_startline": "41", "method_endline": "47"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\benchmark\\dummy_masked_lm.py", "file_new_name": "fairseq\\benchmark\\dummy_masked_lm.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "57", "deleted_lines": "52,53", "method_info": {"method_name": "setup_task", "method_params": "cls,args,kwargs", "method_startline": "52", "method_endline": "58"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\distributed_utils.py", "file_new_name": "fairseq\\distributed_utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "155", "deleted_lines": null, "method_info": {"method_name": "all_gather_list", "method_params": "data,group,max_size", "method_startline": "131", "method_endline": "188"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "fairseq\\utils.py", "file_new_name": "fairseq\\utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "71,72", "deleted_lines": null, "method_info": {"method_name": "move_to_cpu._move_to_cpu", "method_params": "tensor", "method_startline": "71", "method_endline": "72"}}, "hunk_1": {"Ismethod": 1, "added_lines": "70,71,72,73,74", "deleted_lines": null, "method_info": {"method_name": "move_to_cpu", "method_params": "sample", "method_startline": "70", "method_endline": "74"}}}}}}}