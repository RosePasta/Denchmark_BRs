{"BR": {"BR_id": "1866", "BR_author": "Guangxuan-Xiao", "BRopenT": "2020-03-19T04:54:19Z", "BRcloseT": "2020-03-21T18:11:25Z", "BR_text": {"BRsummary": "Can't finetune roberta on wsc with the given bash script.", "BRdescription": "\n <denchmark-h:h2>\u2753 Questions and Help</denchmark-h>\n \n <denchmark-h:h3>Before asking:</denchmark-h>\n \n \n search the issues.\n search the docs.\n \n <denchmark-h:h4>What is your question?</denchmark-h>\n \n When I tried to reproducing roberta wsc and winogrande fintuing result with the bash script given in the README, it just couldn't work. It always shows that python is unable to infer criterion arguments and it requires me to implement WSCCriterion.build_criterion, but I don't know how.\n <denchmark-h:h4>Code</denchmark-h>\n \n CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train WSC/ \\\n     --restore-file $ROBERTA_PATH \\\n     --reset-optimizer --reset-dataloader --reset-meters \\\n     --no-epoch-checkpoints --no-last-checkpoints --no-save-optimizer-state \\\n     --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n     --valid-subset val \\\n     --fp16 --ddp-backend no_c10d \\\n     --user-dir ${FAIRSEQ_USER_DIR} \\\n     --task wsc --criterion wsc --wsc-cross-entropy \\\n     --arch roberta_base --bpe gpt2 --max-positions 512 \\\n     --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n     --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-06 \\\n     --lr-scheduler polynomial_decay --lr $LR \\\n     --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_NUM_UPDATES \\\n     --max-sentences $MAX_SENTENCES \\\n     --max-update $TOTAL_NUM_UPDATES \\\n     --log-format simple --log-interval 100 \\\n     --seed $SEED\n <denchmark-h:h4>What have you tried?</denchmark-h>\n \n To search in this repository desperately and find nothing.\n <denchmark-h:h4>What's your environment?</denchmark-h>\n \n \n fairseq Version (e.g., 1.0 or master):0.9.0\n PyTorch Version: 1.4.0\n OS (e.g., Linux): Ubuntu\n How you installed fairseq (pip, source): pip\n Build command you used (if compiling from source):\n Python version: 3.7.4\n CUDA/cuDNN version: 10.1\n GPU models and configuration: RTX 2080ti\n Any other relevant information:\n The full trackback:\n \n <denchmark-code>2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11739\n 2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11739\n 2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11739\n 2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 1\n 2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 2\n 2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:11739\n 2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 3\n 2020-03-19 04:44:55 | INFO | fairseq.distributed_utils | initialized host ubuntu as rank 0\n | dictionary: 50265 types\n | dictionary: 50265 types\n 2020-03-19 04:45:03 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, all_gather_list_size=16384, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bpe='gpt2', broadcast_buffers=False, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='wsc', curriculum=0, data='WSC/', dataset_impl=None, ddp_backend='no_c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11739', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gpt2_encoder_json='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', gpt2_vocab_bpe='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe', init_token=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, log_format='simple', log_interval=100, lr=[2e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_tokens=None, max_tokens_valid=None, max_update=2000, maximize_best_checkpoint_metric=True, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_epoch_checkpoints=True, no_last_checkpoints=True, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/xgx/Commonsense/test/model/roberta.base/model.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, save_predictions=None, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=False, task='wsc', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, total_num_update=2000, train_subset='train', update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir='/home/xgx/fairseq/examples/roberta/wsc', valid_subset='val', validate_interval=1, warmup_updates=250, weight_decay=0.01, wsc_cross_entropy=True, wsc_margin_alpha=1.0, wsc_margin_beta=0.0)\n | dictionary: 50265 types\n | dictionary: 50265 types\n Traceback (most recent call last):\n   File \"/home/xgx/miniconda3/bin/fairseq-train\", line 11, in <module>\n     load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\n   File \"/home/xgx/fairseq/fairseq_cli/train.py\", line 317, in cli_main\n     nprocs=args.distributed_world_size,\n   File \"/home/xgx/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n     while not spawn_context.join():\n   File \"/home/xgx/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\n     raise Exception(msg)\n Exception: \n \n -- Process 1 terminated with the following error:\n Traceback (most recent call last):\n   File \"/home/xgx/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n     fn(i, *args)\n   File \"/home/xgx/fairseq/fairseq_cli/train.py\", line 286, in distributed_main\n     main(args, init_distributed=True)\n   File \"/home/xgx/fairseq/fairseq_cli/train.py\", line 63, in main\n     criterion = task.build_criterion(args)\n   File \"/home/xgx/fairseq/fairseq/tasks/fairseq_task.py\", line 226, in build_criterion\n     return criterions.build_criterion(args, self)\n   File \"/home/xgx/fairseq/fairseq/registry.py\", line 41, in build_x\n     return builder(args, *extra_args, **extra_kwargs)\n   File \"/home/xgx/fairseq/fairseq/criterions/fairseq_criterion.py\", line 56, in build_criterion\n     '{}.build_criterion'.format(cls.__name__)\n NotImplementedError: Unable to infer Criterion arguments, please implement WSCCriterion.build_criterion\n </denchmark-code>\n \n \t"}, "comments": {}}, "commit": {"commit_id": "e28d15b33aa4434b04d8dd16427b802d6035ea22", "commit_author": "Myle Ott", "commitT": "2020-03-21 11:11:16-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "examples\\roberta\\wsc\\wsc_criterion.py", "file_new_name": "examples\\roberta\\wsc\\wsc_criterion.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13,17", "deleted_lines": "13,17"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\roberta\\wsc\\wsc_task.py", "file_new_name": "examples\\roberta\\wsc\\wsc_task.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "98", "deleted_lines": "98", "method_info": {"method_name": "binarize_with_mask", "method_params": "self,txt,prefix,suffix,leading_space,trailing_space", "method_startline": "93", "method_endline": "102"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "examples\\speech_recognition\\criterions\\ASG_loss.py", "file_new_name": "examples\\speech_recognition\\criterions\\ASG_loss.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13,18", "deleted_lines": "13,18"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "examples\\speech_recognition\\criterions\\CTC_loss.py", "file_new_name": "examples\\speech_recognition\\criterions\\CTC_loss.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15,78", "deleted_lines": "15,78"}}}}}}