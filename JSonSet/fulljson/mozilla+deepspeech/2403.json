{"BR": {"BR_id": "2403", "BR_author": "carlfm01", "BRopenT": "2019-10-07T08:58:32Z", "BRcloseT": "2019-10-19T09:06:25Z", "BR_text": {"BRsummary": "Investigate memory leak", "BRdescription": "\n \n Have I written custom code (as opposed to running examples on an unmodified clone of the repository): No\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise\n TensorFlow version of the Nuget: v1.13.1-13-g174b4760eb\n DeepSpeech version of the Nuget:v0.6.0-alpha.1-0-g90e7980\n Python version: NA\n Bazel version (if compiling from source): NA\n GCC/Compiler version (if compiling from source): NA\n CUDA/cuDNN version: NA\n GPU model and memory: NA\n Exact command to reproduce: NA. Using C# client\n Nuget version: :0.6.0-alpha.1\n \n Description:\n Memory not being released on model destroy when using the language model.\n The issue is coming from 0.6.0-alpha.1, 0.6.0-alpha.0 works just fine.\n <denchmark-link:https://github.com/lissyx>@lissyx</denchmark-link>\n  the LAZY config change is not the source of the issue, lazy change: <denchmark-link:https://github.com/mozilla/DeepSpeech/pull/2385>#2385</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "carlfm01", "commentT": "2019-10-07T08:59:31Z", "comment_text": "\n \t\t\n The issue is coming from 0.6.0-alpha.1, 0.6.0-alpha.0 works just fine.\n \n Thanks for that. Do you reproduce on Linux as well?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "carlfm01", "commentT": "2019-10-07T09:00:31Z", "comment_text": "\n \t\t\n Thanks for that. Do you reproduce on Linux as well?\n \n Not yet, I was testing the Nugets first\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "carlfm01", "commentT": "2019-10-07T09:01:28Z", "comment_text": "\n \t\tThe only big changes in that window are indeed related to the LM: <denchmark-link:https://github.com/mozilla/DeepSpeech/commit/31afc6811f1cacc7eba1943e27931de3d7b8a8dc>31afc68</denchmark-link>\n  cc <denchmark-link:https://github.com/reuben>@reuben</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "carlfm01", "commentT": "2019-10-07T09:08:03Z", "comment_text": "\n \t\tIn your tests are you specifying an empty trie path and relying on the Scorer to create it dynamically? In other words, is Scorer::fill_dictionary being called?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "carlfm01", "commentT": "2019-10-07T09:13:44Z", "comment_text": "\n \t\t\n In your tests are you specifying an empty trie path and relying on the Scorer to create it dynamicall\n \n No, I'm using a trie file path, let me see without it.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "carlfm01", "commentT": "2019-10-07T10:10:36Z", "comment_text": "\n \t\tWithout the trie, it keeps in the range from 2GB to 4GB every run, without a noticeable difference in resource usage from the first one and the last one.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "carlfm01", "commentT": "2019-10-07T15:09:32Z", "comment_text": "\n \t\tSo far I have a hard time reproducing the leak. <denchmark-link:https://github.com/carlfm01>@carlfm01</denchmark-link>\n  do you have specific code to share for reproducing ? amounts of memory leaked ?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "carlfm01", "commentT": "2019-10-07T16:10:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/carlfm01>@carlfm01</denchmark-link>\n  Your previous message mentionned growing from 200MB to 700MB over 20 iterations, that would mean we are loosing 25MB per run.\n So far, I can only account for ~2MB at best:\n <denchmark-code>==22384== LEAK SUMMARY:\n ==22384==    definitely lost: 24 bytes in 1 blocks\n ==22384==    indirectly lost: 0 bytes in 0 blocks\n ==22384==      possibly lost: 332,124 bytes in 1,521 blocks\n ==22384==    still reachable: 2,655,432 bytes in 33,076 blocks\n ==22384==                       of which reachable via heuristic:\n ==22384==                         newarray           : 52,576 bytes in 196 blocks\n ==22384==         suppressed: 0 bytes in 0 blocks\n </denchmark-code>\n \n And most of it is from TensorFlow itself, and this might be false-positive from valgrind.\n The only items that would connect to language model / decoder would account only for a few bytes (10 occurrences, between 24 bytes and 32 bytes, so at worst it's < 320 bytes per run). Obivously, very far away from what you experience. I do fear this might be Windows-specific. Or at least, not reproductible on Linux / under valgrind.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "carlfm01", "commentT": "2019-10-07T20:42:10Z", "comment_text": "\n \t\t\n do you have specific code to share for reproducing?\n \n Just the console example using a for over the same file::<denchmark-link:https://gist.github.com/carlfm01/fd69a8ca2784837dabf9375d35258953#file-test-cs-L59>https://gist.github.com/carlfm01/fd69a8ca2784837dabf9375d35258953#file-test-cs-L59</denchmark-link>\n \n To see the memory usage I'm using the VS profiler(poor details of the unmanaged side)\n Now I want to compile the console client for Linux to perform the same test, or that was exactly what you did?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "carlfm01", "commentT": "2019-10-08T01:45:07Z", "comment_text": "\n \t\tFinally compiled and now testing:\n Versions:\n <denchmark-code>TensorFlow: v1.14.0-16-g3b4ce374f5\n DeepSpeech: v0.6.0-alpha.8-16-gfb611ef\n </denchmark-code>\n \n Valgrind report for 1 run:\n <denchmark-code>==32502== LEAK SUMMARY:\n ==32502==    definitely lost: 24 bytes in 1 blocks\n ==32502==    indirectly lost: 214 bytes in 5 blocks\n ==32502==      possibly lost: 331,620 bytes in 1,500 blocks\n ==32502==    still reachable: 2,115,668 bytes in 39,762 blocks\n ==32502==                       of which reachable via heuristic:\n ==32502==                         stdstring          : 465,999 bytes in 11,883 blocks\n ==32502==                         newarray           : 42,880 bytes in 194 blocks\n ==32502==         suppressed: 0 bytes in 0 blocks\n </denchmark-code>\n \n 20 runs:\n <denchmark-code>==45309==\n ==45309== LEAK SUMMARY:\n ==45309==    definitely lost: 7,520 bytes in 42 blocks\n ==45309==    indirectly lost: 7,959,270 bytes in 102,263 blocks\n ==45309==      possibly lost: 2,973,857 bytes in 34,726 blocks\n ==45309==    still reachable: 2,154,783 bytes in 40,201 blocks\n ==45309==                       of which reachable via heuristic:\n ==45309==                         stdstring          : 938,728 bytes in 17,033 blocks\n ==45309==                         newarray           : 208,224 bytes in 988 blocks\n </denchmark-code>\n \n \n I do fear this might be Windows-specific.\n \n Given the results looks you are correct.\n Now looking :<denchmark-link:https://github.com/kkm000/openfst/issues/8>kkm000/openfst#8</denchmark-link>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "carlfm01", "commentT": "2019-10-08T06:41:51Z", "comment_text": "\n \t\t\n Now looking :kkm000/openfst#8\n \n So it would mean ConstFst is not really doing mmap() on windows, and thus we leak from there?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "carlfm01", "commentT": "2019-10-08T16:46:57Z", "comment_text": "\n \t\t\n not really doing mmap() on windows\n \n Instead is using a custom implementation using a buffer: <denchmark-link:https://github.com/kkm000/openfst/blob/989affd3043b6357e6047a395565c3e0d979c01f/src/lib/mapped-file.cc#L48>https://github.com/kkm000/openfst/blob/989affd3043b6357e6047a395565c3e0d979c01f/src/lib/mapped-file.cc#L48</denchmark-link>\n \n I'll compile and debug that file, I also want to test a few things with <denchmark-link:https://code.google.com/archive/p/mman-win32/>https://code.google.com/archive/p/mman-win32/</denchmark-link>\n  and see if we can get rid of the buffer implementation.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "carlfm01", "commentT": "2019-10-09T06:12:44Z", "comment_text": "\n \t\tThe destructor of  MappedFile is never called:\n \n \n \n DeepSpeech/native_client/ctcdecode/third_party/openfst-1.6.9-win/src/lib/mapped-file.cc\n \n \n          Line 26\n       in\n       031479d\n \n \n \n \n \n \n  MappedFile::~MappedFile() { \n \n \n \n \n \n then never executes:\n \n \n \n DeepSpeech/native_client/ctcdecode/third_party/openfst-1.6.9-win/src/lib/mapped-file.cc\n \n \n          Line 38\n       in\n       031479d\n \n \n \n \n \n \n  operator delete(static_cast<char *>(region_.data) - region_.offset); \n \n \n \n \n \n Before I found that the deconstructor is not called, I tried to replicate with the python client on windows and turns out that the python client does no contains , why <denchmark-link:https://github.com/lissyx>@lissyx</denchmark-link>\n  ? I'm missing something?\n Windows Python version : \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "carlfm01", "commentT": "2019-10-09T06:58:04Z", "comment_text": "\n \t\t\n turns out that the python client does no contains freeModel\n \n It is handled by the wrapper: \n \n \n DeepSpeech/native_client/python/__init__.py\n \n \n         Lines 42 to 45\n       in\n       031479d\n \n \n \n \n \n \n  def __del__(self): \n \n \n \n  if self._impl: \n \n \n \n  deepspeech.impl.FreeModel(self._impl) \n \n \n \n  self._impl = None \n \n \n \n \n \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "carlfm01", "commentT": "2019-10-09T07:10:17Z", "comment_text": "\n \t\t\n The deconstructor of MappedFile is never called\n \n Just to make sure, this is not only when using the Python code, this is always ?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "carlfm01", "commentT": "2019-10-09T07:30:10Z", "comment_text": "\n \t\t\n Just to make sure, this is not only when using the Python code, this is always ?\n \n Yes always, with both, python client and the c# client.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "carlfm01", "commentT": "2019-10-09T07:31:46Z", "comment_text": "\n \t\t\n c# client.\n \n Can you replicate with the C++ basic client ? Just to see if the .Net bindings could have a play in the equation.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "carlfm01", "commentT": "2019-10-09T08:26:23Z", "comment_text": "\n \t\tMappedFile as much as I can read in the windows part is all std::unique_ptr<> scoped, is it possible we are missing something at a upper level?\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "carlfm01", "commentT": "2019-10-09T18:20:53Z", "comment_text": "\n \t\t\n Can you replicate with the C++ basic client ?\n \n yes I'll test the basic C++ client, allow me some time to complete my builds and switch back to r1.14\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "carlfm01", "commentT": "2019-10-10T09:21:13Z", "comment_text": "\n \t\t\n yes I'll test the basic C++ client,\n \n Dealing with make: \\bin\\amd64\\cl.exe: Command not found, I'll read the cluster examples again and try carefully.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "carlfm01", "commentT": "2019-10-14T15:43:29Z", "comment_text": "\n \t\t\n \n yes I'll test the basic C++ client,\n \n Dealing with make: \\bin\\amd64\\cl.exe: Command not found, I'll read the cluster examples again and try carefully.\n \n Have you been able to sort this out?\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "carlfm01", "commentT": "2019-10-15T03:26:36Z", "comment_text": "\n \t\tHello <denchmark-link:https://github.com/lissyx>@lissyx</denchmark-link>\n , unfortunately not, last week was a busy week working on TTS. I tried with short time windows but did not get any luck.\n I'm back to it :)\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "carlfm01", "commentT": "2019-10-16T01:12:11Z", "comment_text": "\n \t\tJust realized I wasted my time, Bazel is not detecting changes inside header files :), manually removed the fst.obj under _objs and now I see the execution path printed (With this I was trying to see where is mapped-file allocated but not released).\n Bazel version: 0.24.1\n At this point I don't know which changes were applied for the tests, testing again... :/\n Now about the C++ basic client:\n \n Can you replicate with the C++ basic client ?\n \n Yes, is eating the same amount of memory as the .Net client.\n \n make: \\bin\\amd64\\cl.exe: Command not found\n \n solved this by replacing :\n \n \n \n DeepSpeech/native_client/definitions.mk\n \n \n          Line 39\n       in\n       5fa6d23\n \n \n \n \n \n \n  TOOLCHAIN := '$(VCINSTALLDIR)\\bin\\amd64\\' \n \n \n \n \n \n with my full path to the cl.exe of my VS and then running vcvars64 before the make command\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "carlfm01", "commentT": "2019-10-16T13:43:12Z", "comment_text": "\n \t\t\n with my full path to the cl.exe of my VS and then running vcvars64 before the make command\n \n Right, reminds me of things I had to do on TaskCluster. I assumed that on developper systems, this should be already dealt with.\n \n \n Can you replicate with the C++ basic client ?\n \n Yes, is eating the same amount of memory as the .Net client.\n \n Good, at least confirms it's not coming from the bindings. Do you think you can investigate why the destructor is not called ?\n We have some code that triggers some lost but still reachable memory under valgrind on linux, and it deals with what calls this, so I'm wondering if this is not the root cause indeed, and we are just more lucky / going through another path on linux to free ?\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "carlfm01", "commentT": "2019-10-17T07:29:40Z", "comment_text": "\n \t\t\n Do you think you can investigate why the destructor is not called ?\n \n Yes, I'm already trying to spot the issue, but due to my newbie eyes for C++ I'm not making any significant progress.\n The only thing that seems wrong apart from the destructor of MappedFile  never called is the destructor of ConstFstImpl only called for the first run, then never again. I want to see if this happens also on Linux.\n \n scoped, is it possible we are missing something at a upper level?\n \n Upper ConstFstImpl is ConstFst which is used for PathTrie as FstType I sort of feel the issue is coming from PathTrie and the usage of share_ptr:\n \n \n \n DeepSpeech/native_client/ctcdecode/path_trie.cpp\n \n \n          Line 83\n       in\n       336daa1\n \n \n \n \n \n \n  new_path->matcher_ = matcher_; \n \n \n \n \n \n \u00bfWhat do you think?\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "carlfm01", "commentT": "2019-10-17T16:18:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/carlfm01>@carlfm01</denchmark-link>\n  Long-shot, but doing so does not seems to trigger issues here:\n <denchmark-code>diff --git a/native_client/ctcdecode/path_trie.cpp b/native_client/ctcdecode/path_trie.cpp\n index 51f75ff..dee792d 100644\n --- a/native_client/ctcdecode/path_trie.cpp\n +++ b/native_client/ctcdecode/path_trie.cpp\n @@ -33,6 +33,8 @@ PathTrie::~PathTrie() {\n    for (auto child : children_) {\n      delete child.second;\n    }\n +\n +  matcher_ = nullptr;\n  }\n  \n  PathTrie* PathTrie::get_path_trie(int new_char, int new_timestep, float cur_log_prob_c, bool reset) {\n </denchmark-code>\n \n This should make sure that any PathTrie destruction frees the matching allocation of matcher_.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "carlfm01", "commentT": "2019-10-17T17:26:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/carlfm01>@carlfm01</denchmark-link>\n  I'm having more doubts (and <denchmark-link:https://github.com/reuben>@reuben</denchmark-link>\n  shares this as well) against  there, which is  in  and that we  in . This  call triggers a  behind.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "carlfm01", "commentT": "2019-10-18T10:15:21Z", "comment_text": "\n \t\ti'm keeping that open until <denchmark-link:https://github.com/carlfm01>@carlfm01</denchmark-link>\n  can confirm this is fixed\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "carlfm01", "commentT": "2019-10-19T04:03:02Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/lissyx>@lissyx</denchmark-link>\n \n Using :\n TensorFlow: v1.14.0-16-g3b4ce374f5\n DeepSpeech: v0.6.0-alpha.10-2-g469ddd2\n First run 53MB, last run 45MB.(.Net client)\n I can confirm that issue is fixed for the .Net client.\n Testing with Python and C++ client, looks like the C++ client is not releasing completely.\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "carlfm01", "commentT": "2019-10-19T09:06:22Z", "comment_text": "\n \t\tI'm going to close it then, we can still fix the C++ client but if the leak in the lib is fixed it's the most important :)\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "carlfm01", "commentT": "2019-11-18T09:56:43Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "ef3f8004ce7a1125f5b84aaf18e821e0ed2b1391", "commit_author": "Alexandre Lissy", "commitT": "2019-10-18 10:15:59+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "native_client\\ctcdecode\\ctc_beam_search_decoder.cpp", "file_new_name": "native_client\\ctcdecode\\ctc_beam_search_decoder.cpp", "file_complexity": {"file_NLOC": "207", "file_CCN": "38", "file_NToken": "1434"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "40,41", "deleted_lines": "40", "method_info": {"method_name": "DecoderState::init", "method_params": "alphabet,beam_size,cutoff_prob,cutoff_top_n,ext_scorer", "method_startline": "17", "method_endline": "48", "method_complexity": {"method_NLOC": "25", "method_CCN": "3", "method_NToken": "172", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "native_client\\ctcdecode\\path_trie.cpp", "file_new_name": "native_client\\ctcdecode\\path_trie.cpp", "file_complexity": {"file_NLOC": "170", "file_CCN": "38", "file_NToken": "1105"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "168,170", "deleted_lines": "168,170", "method_info": {"method_name": "PathTrie::set_dictionary", "method_params": "dictionary", "method_startline": "168", "method_endline": "172", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "28", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "168,170", "deleted_lines": "168,170", "method_info": {"method_name": "PathTrie::set_dictionary", "method_params": "dictionary", "method_startline": "168", "method_endline": "172", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "32", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "native_client\\ctcdecode\\path_trie.h", "file_new_name": "native_client\\ctcdecode\\path_trie.h", "file_complexity": {"file_NLOC": "44", "file_CCN": "1", "file_NToken": "288"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "42,75", "deleted_lines": "42,75"}}}}}}