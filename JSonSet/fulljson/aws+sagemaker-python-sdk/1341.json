{"BR": {"BR_id": "1341", "BR_author": "cfregly", "BRopenT": "2020-03-09T23:05:11Z", "BRcloseT": "2020-03-11T17:56:16Z", "BR_text": {"BRsummary": "`KeyError: 'ProcessingOutputConfig'` when calling Calling `ProcessingJob.from_processing_name()` for a ProcessingJob defined without a ProcessingOutput (ie. Spark Processor)", "BRdescription": "\n After running this ScriptProcessor:\n <denchmark-code>processor.run(code='preprocess-spark.py',\n               arguments=['s3_input_data', balanced_train_data_input,\n                          's3_output_data', balanced_train_data_tfidf_output,\n               ],    \n               logs=True,\n               wait=False\n )\n </denchmark-code>\n \n and running this code:\n <denchmark-code>preprocessing_job_description = processor.jobs[-1].describe()\n processing_job_name = preprocessing_job_description['ProcessingJobName']\n \n running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n </denchmark-code>\n \n I'm seeing this error\n <denchmark-code>---------------------------------------------------------------------------\n KeyError                                  Traceback (most recent call last)\n <ipython-input-55-5b16753426ad> in <module>()\n       1 running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n ----> 2                                                                             sagemaker_session=sagemaker_session)\n       3 running_processor.describe()\n \n ~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/processing.py in from_processing_name(cls, sagemaker_session, processing_job_name)\n     668             outputs=[\n     669                 ProcessingOutput(\n --> 670                     source=job_desc[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n     671                         \"LocalPath\"\n     672                     ],\n \n KeyError: 'ProcessingOutputConfig'\n </denchmark-code>\n \n Note:  I'm not using  because I am using a Spark Processor and writing directly to S3.  (See <denchmark-link:https://github.com/aws/amazon-sagemaker-examples/issues/994>aws/amazon-sagemaker-examples#994</denchmark-link>\n  for more info on why I'm doing this.)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "cfregly", "commentT": "2020-03-09T23:17:39Z", "comment_text": "\n \t\tA workaround is to add a dummy ProcessingOutput as follows:\n <denchmark-code>processor.run(code='preprocess-spark.py',\n               arguments=['s3_input_data', balanced_train_data_input,\n                          's3_output_data', balanced_train_data_tfidf_output,\n               ],\n               # We need this dummy output to allow us to call \n               #    ProcessingJob.from_processing_name() later \n               #    to describe the job and poll for Completed status\n               outputs=[\n                        ProcessingOutput(s3_upload_mode='EndOfJob',\n                                         output_name='dummy-output',\n                                         source='/opt/ml/processing/output')\n               ],            \n               logs=True,\n               wait=False\n )\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "cfregly", "commentT": "2020-03-10T19:12:11Z", "comment_text": "\n \t\tOh no!\n Thanks for reaching out!\n This definitely looks like a bug on our part. I went ahead and created a PR to fix this issue, as well as add a test for it to ensure we don't lose this functionality in the future.\n Thanks for letting us know!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "cfregly", "commentT": "2020-03-11T17:56:16Z", "comment_text": "\n \t\tA fix for this was released to PyPI a few minutes ago =)\n Let me know if there's anything else we can do to help!\n \t\t"}}}, "commit": {"commit_id": "13fc68c1fbc73ba9791863d5976e529133a7786e", "commit_author": "Karim Nakad", "commitT": "2020-03-10 14:32:42-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\sagemaker\\processing.py", "file_new_name": "src\\sagemaker\\processing.py", "file_complexity": {"file_NLOC": "424", "file_CCN": "60", "file_NToken": "2237"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "651,652,653,666,667,668,669,670,671,672,674,675,676,678,679,680,681,682,683,684,685,686,687,688,689,690", "deleted_lines": "651,652,653,654,667,668,670,671,672,673,674,675,676,678,679", "method_info": {"method_name": "from_processing_name", "method_params": "cls,sagemaker_session,processing_job_name", "method_startline": "635", "method_endline": "691", "method_complexity": {"method_NLOC": "40", "method_CCN": "7", "method_NToken": "231", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\integ\\test_processing.py", "file_new_name": "tests\\integ\\test_processing.py", "file_complexity": {"file_NLOC": "515", "file_CCN": "12", "file_NToken": "3081"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23,24,25,26,27,28,29,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514", "deleted_lines": "23"}}}}}}