{"BR": {"BR_id": "216", "BR_author": "reminisce", "BRopenT": "2018-01-26T05:34:11Z", "BRcloseT": "2018-02-03T10:57:48Z", "BR_text": {"BRsummary": "Cannot display more than one histogram at the same time", "BRdescription": "\n I was trying VisualDL in an <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/example/gluon/mnist.py>MXNet tutorial</denchmark-link>\n  to track the gradients of  for each epoch. However, only the histogram of the second gradient can be displayed.\n The changes I made to the tutorial are adding writer creation before the epoch loop:\n     param_grad_writers = {}\n     for name in param_names[:2]:\n         print(name)\n         with logger.mode(name):\n             param_grad_writers[name] = logger.histogram(name, num_buckets)\n and calling add_record at the end of each epoch loop:\n <denchmark-code>        grads = [i.grad() for i in net.collect_params().values()]\n         assert len(grads) == len(param_names)\n         for i, name in enumerate(param_names[:2]):\n             print('%d %s' % (i, name))\n             print(grads[i].shape)\n             assert name in param_grad_writers\n             assert np.prod(grads[i].shape) >= num_buckets\n             param_grad_writers[name].add_record(epoch, list(grads[i].asnumpy().flatten()))\n </denchmark-code>\n \n Here is the complete script I ran.\n from __future__ import print_function\n \n import random\n from visualdl import LogWriter\n import argparse\n import logging\n logging.basicConfig(level=logging.DEBUG)\n \n import numpy as np\n import mxnet as mx\n from mxnet import gluon, autograd\n from mxnet.gluon import nn\n \n # Parse CLI arguments\n \n parser = argparse.ArgumentParser(description='MXNet Gluon MNIST Example')\n parser.add_argument('--batch-size', type=int, default=100,\n                     help='batch size for training and testing (default: 100)')\n parser.add_argument('--epochs', type=int, default=10,\n                     help='number of epochs to train (default: 10)')\n parser.add_argument('--lr', type=float, default=0.1,\n                     help='learning rate (default: 0.1)')\n parser.add_argument('--momentum', type=float, default=0.9,\n                     help='SGD momentum (default: 0.9)')\n parser.add_argument('--cuda', action='store_true', default=False,\n                     help='Train on GPU with CUDA')\n parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n                     help='how many batches to wait before logging training status')\n opt = parser.parse_args()\n \n \n \n logdir = \"./tmp\"\n logger = LogWriter(logdir, sync_cycle=10)\n \n # mark the components with 'train' label.\n with logger.mode(\"train\"):\n     train_acc = logger.scalar(\"scalars/accuracy\")\n \n with logger.mode('valid'):\n     valid_acc = logger.scalar(\"scalars/accuracy\")\n \n \n # define network\n \n net = nn.Sequential()\n with net.name_scope():\n     net.add(nn.Dense(128, activation='relu'))\n     net.add(nn.Dense(64, activation='relu'))\n     net.add(nn.Dense(10))\n \n \n # data\n \n def transformer(data, label):\n     data = data.reshape((-1,)).astype(np.float32)/255\n     return data, label\n \n train_data = gluon.data.DataLoader(\n     gluon.data.vision.MNIST('./data', train=True, transform=transformer),\n     batch_size=opt.batch_size, shuffle=True, last_batch='discard')\n \n val_data = gluon.data.DataLoader(\n     gluon.data.vision.MNIST('./data', train=False, transform=transformer),\n     batch_size=opt.batch_size, shuffle=False)\n \n # train\n \n def test(ctx):\n     metric = mx.metric.Accuracy()\n     for data, label in val_data:\n         data = data.as_in_context(ctx)\n         label = label.as_in_context(ctx)\n         output = net(data)\n         metric.update([label], [output])\n \n     return metric.get()\n \n \n def train(epochs, ctx):\n     # Collect all parameters from net and its children, then initialize them.\n     net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n \n     # Trainer is for updating parameters with gradient.\n     trainer = gluon.Trainer(net.collect_params(), 'sgd',\n                             {'learning_rate': opt.lr, 'momentum': opt.momentum})\n     metric = mx.metric.Accuracy()\n     loss = gluon.loss.SoftmaxCrossEntropyLoss()\n \n     params = net.collect_params()\n     param_names = params.keys()\n     print(param_names)\n \n     num_buckets = 100\n     param_grad_writers = {}\n     for name in param_names[:2]:\n         print(name)\n         with logger.mode(name):\n             param_grad_writers[name] = logger.histogram(name, num_buckets)\n \n     for epoch in range(epochs):\n         # reset data iterator and metric at begining of epoch.\n         metric.reset()\n         for i, (data, label) in enumerate(train_data):\n             # Copy data to ctx if necessary\n             data = data.as_in_context(ctx)\n             label = label.as_in_context(ctx)\n             # Start recording computation graph with record() section.\n             # Recorded graphs can then be differentiated with backward.\n             with autograd.record():\n                 output = net(data)\n                 L = loss(output, label)\n             L.backward()\n \n             # take a gradient step with batch_size equal to data.shape[0]\n             trainer.step(data.shape[0])\n             # update metric at last.\n             metric.update([label], [output])\n \n             if i % opt.log_interval == 0 and i > 0:\n                 name, acc = metric.get()\n                 print('[Epoch %d Batch %d] Training: %s=%f'%(epoch, i, name, acc))\n \n         grads = [i.grad() for i in net.collect_params().values()]\n         assert len(grads) == len(param_names)\n         for i, name in enumerate(param_names[:2]):\n             print('%d %s' % (i, name))\n             print(grads[i].shape)\n             assert name in param_grad_writers\n             assert np.prod(grads[i].shape) >= num_buckets\n             param_grad_writers[name].add_record(epoch, list(grads[i].asnumpy().flatten()))\n \n         name, acc = metric.get()\n         print('[Epoch %d] Training: %s=%f'%(epoch, name, acc))\n \n         name, val_acc = test(ctx)\n         print('[Epoch %d] Validation: %s=%f'%(epoch, name, val_acc))\n \n         train_acc.add_record(epoch, acc)\n         valid_acc.add_record(epoch, val_acc)\n \n     net.save_params('mnist.params')\n \n \n if __name__ == '__main__':\n     if opt.cuda:\n         ctx = mx.gpu(0)\n     else:\n         ctx = mx.cpu()\n     train(opt.epochs, ctx)\n Here is the screen shot:\n <denchmark-link:https://user-images.githubusercontent.com/4978794/35426375-5aa83ede-0217-11e8-90fe-ed9cdf3020b0.png></denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "reminisce", "commentT": "2018-01-30T05:08:48Z", "comment_text": "\n \t\tThere are two possibilities:\n \n the second histogram does not have any record, so nothing to draw\n there is an error in processing the records by the LogReader\n \n scalar has a bug in reading records, and that may also affect the histogram, it is fixed in <denchmark-link:https://github.com/PaddlePaddle/VisualDL/pull/225>#225</denchmark-link>\n .\n If the second situation, try to pull the latest code and install from source.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "reminisce", "commentT": "2018-02-02T02:01:55Z", "comment_text": "\n \t\tThe issue still persists. I create two histograms, one with mode \"train\" and the other with mode \"test\".\n The later one shows up properly, but the other can't render properly.\n <denchmark-link:https://user-images.githubusercontent.com/1093725/35713019-fbd814fc-0779-11e8-9146-ccb648e47855.png></denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "f58b45e699cdaacadfd52508a13e91f6ed04cf5e", "commit_author": "Jeff Wang", "commitT": "2018-02-03 18:57:47+08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "frontend\\src\\histogram\\Histogram.san", "file_new_name": "frontend\\src\\histogram\\Histogram.san", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "65", "deleted_lines": "65"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "frontend\\src\\scalars\\ui\\chart.san", "file_new_name": "frontend\\src\\scalars\\ui\\chart.san", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "132,145", "deleted_lines": null}}}}}}