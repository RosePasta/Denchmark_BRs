{"BR": {"BR_id": "608", "BR_author": "dhgokul", "BRopenT": "2018-05-29T14:12:29Z", "BRcloseT": "2018-07-27T04:43:40Z", "BR_text": {"BRsummary": "Turicreate - Training process not proceed after 91 iteration , it loading for more than an hour @ GPU", "BRdescription": "\n I have trained around 3.5 k images for single category in GPU version, it took around 10 mins to create Sframe data, after that training process log started, it reached 91 iteration and keep on loading not proceed to next step - PFA\n <denchmark-link:https://user-images.githubusercontent.com/5689804/40663991-889a52ac-6377-11e8-8931-aa73fd5c69d1.png></denchmark-link>\n \n Total dataset size (3500 images)  - 91 MB, Maxi image size - 80 kb\n Machine type : AWS deep learning\n Any help appreciated !!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dhgokul", "commentT": "2018-05-29T14:14:14Z", "comment_text": "\n \t\tIf your dataset is licensed permissively, can you share the dataset with us if possible? Its mostly likely something to do with a single data point in your dataset and we'd like to be able to isolate it. We can try and reproduce it. Is most unusual that this happens.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dhgokul", "commentT": "2018-05-29T14:27:45Z", "comment_text": "\n \t\tThanks for your reply srikris, I like to share dataset privately , Can you share emailid or provide option to share\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dhgokul", "commentT": "2018-05-29T14:30:57Z", "comment_text": "\n \t\tCan you file a report to the Apple Bug reporter (<denchmark-link:https://bugreport.apple.com/>https://bugreport.apple.com/</denchmark-link>\n ). Its easier for us to share information in there that is private so we can diagnose better.\n \t\t"}, "comments_3": {"comment_id": 5, "comment_author": "dhgokul", "commentT": "2018-05-29T16:12:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/srikris>@srikris</denchmark-link>\n  Did you get chance to review dataset? Thanks for your response !\n \t\t"}, "comments_4": {"comment_id": 6, "comment_author": "dhgokul", "commentT": "2018-06-06T10:56:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/srikris>@srikris</denchmark-link>\n   Any updates on this issue ?\n \t\t"}, "comments_5": {"comment_id": 7, "comment_author": "dhgokul", "commentT": "2018-06-06T13:37:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dhgokul>@dhgokul</denchmark-link>\n  sorry for the slow response and thank you for your patience!\n We're a bit slower to response this week as we're all at WWDC, supporting Turicreate's developer community.\n \t\t"}, "comments_6": {"comment_id": 8, "comment_author": "dhgokul", "commentT": "2018-06-07T16:20:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dhgokul>@dhgokul</denchmark-link>\n  Sorry. I can't seem to get access to your dataset. Can you re-share it? It may have expired. Sorry!\n \t\t"}, "comments_7": {"comment_id": 9, "comment_author": "dhgokul", "commentT": "2018-06-08T07:04:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/srikris>@srikris</denchmark-link>\n  I have report this issue in Apple Bug reporter . Please check\n <denchmark-link:https://bugreport.apple.com/web/?problemID=40926208>https://bugreport.apple.com/web/?problemID=40926208</denchmark-link>\n \n Summary :\n Area:\n CoreML\n Summary: Turicreate training is not proceed forward after few iterations(say 91 iteration) @ GPU\n Steps to Reproduce: Run turicreate python script in AWS deep learning machine, and it got hanged not proceed forward , But it running in background for an hour\n Expected Results: It will create coreML model at the completion of turicreate training process.\n Actual Results: Turicreate training is not proceed forward after few iterations(say 91 iteration)\n Version/Build: Turi version 4.3.2 (stable version)\n cuda 9.0\n mxnet 1.1.0\n Configuration: AWS deep learning machine (Instance type : g2.2xlarge)\n \t\t"}, "comments_8": {"comment_id": 10, "comment_author": "dhgokul", "commentT": "2018-06-08T15:57:59Z", "comment_text": "\n \t\tI think around iteration 100 is when the first dataset shuffle would happen given your dataset size, so I think this is what is taking long, causing a freeze.\n To unblock you, you may want to try turning this off using an undocumented setting:\n tc.object_detector.create(..., _advanced_parameters={'shuffle': False})\n Of course, this issue is on us and we will work on finding a better solution. Shuffling is generally good for training, so disabling it could lead to a slightly less accurate model.\n \t\t"}, "comments_9": {"comment_id": 11, "comment_author": "dhgokul", "commentT": "2018-06-08T16:51:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n  Thanks will check and let you know . Is there's any specific maximum image dataset for turicreate. How many max images can use for training\n \t\t"}, "comments_10": {"comment_id": 12, "comment_author": "dhgokul", "commentT": "2018-06-08T16:53:58Z", "comment_text": "\n \t\tThere is no hard maximum and we have used much larger datasets on laptops, so this is somewhat mysterious to me. How much RAM does your AWS instance have?\n \t\t"}, "comments_11": {"comment_id": 13, "comment_author": "dhgokul", "commentT": "2018-06-08T17:23:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n  Machine configuration : AWS g2.2xlarge, 15.0 GiB, 8 vCPUs, 15gb ram\n \t\t"}, "comments_12": {"comment_id": 14, "comment_author": "dhgokul", "commentT": "2018-06-11T14:57:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n  Thanks a lot, it work's really well !\n After disable shuffle option in turicreate API ,mAP(mean average precision) value is less when compared to previous model (smaller dataset)\n Please suggest any changes required in turicreate API\n \t\t"}, "comments_13": {"comment_id": 15, "comment_author": "dhgokul", "commentT": "2018-06-11T16:18:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dhgokul>@dhgokul</denchmark-link>\n  Thanks so much for trying this and reporting back. This is really useful information for us.\n By default we shuffle the data at the regular intervals for a reason, so turning it off it can unfortunately have negative consequences on evaluation metrics. This is particularly true if your data is sorted in some way already, such as \"all cars first, then all bicycles, etc.\".\n We will work on reducing the memory overhead of sorting, so that you don't have to make this model compromise simply to get it to train without error. Again, apologies for these issues!\n \t\t"}, "comments_14": {"comment_id": 16, "comment_author": "dhgokul", "commentT": "2018-06-12T05:48:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n  I am using single category for turi training. If we require shuffle option for that ?\n \t\t"}, "comments_15": {"comment_id": 17, "comment_author": "dhgokul", "commentT": "2018-06-12T14:06:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n  After disable shuffle option , Object detection looks not accurate , it detect all objects\n Dataset Size(1900) with shuffle = true -> Object detection works @ 75 %\n Dataset Size(3755) with shuffle  = false -> It detect all objects , when moving camera\n \t\t"}, "comments_16": {"comment_id": 18, "comment_author": "dhgokul", "commentT": "2018-06-12T14:12:09Z", "comment_text": "\n \t\tIf we disable shuffle option as false, it will created coreML model, but it is not accurate and detect all the objects when compared to model created with shuffle option as true.\n If we enable shuffle option, still we get same memory error mentioned below\n [14:02:01] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\n 2018-06-12 14:02:12  Training    1/1000  Loss  4.299\n 2018-06-12 14:02:22  Training   10/1000  Loss  4.283\n 2018-06-12 14:02:32  Training   19/1000  Loss  4.101\n 2018-06-12 14:02:43  Training   28/1000  Loss  3.865\n 2018-06-12 14:02:53  Training   37/1000  Loss  3.529\n 2018-06-12 14:03:03  Training   46/1000  Loss  3.121\n 2018-06-12 14:03:14  Training   54/1000  Loss  3.017\n 2018-06-12 14:03:25  Training   63/1000  Loss  2.951\n 2018-06-12 14:03:35  Training   72/1000  Loss  2.947\n 2018-06-12 14:03:48  Training   81/1000  Loss  2.941\n 2018-06-12 14:03:58  Training   91/1000  Loss  2.940\n Traceback (most recent call last):\n File \"turi.py\", line 73, in \n model = tc.object_detector.create(train_data, feature='image', annotations='annotations', max_iterations=1000)\n File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/object_detector.py\", line 336, in create\n for batch in loader:\n File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py\", line 123, in next\n return self._next()\n File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py\", line 232, in _next\n self.sframe = self.sframe.sort(_TMP_COL_RANDOM_ORDER)\n File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/data_structures/sframe.py\", line 5421, in sort\n return SFrame(_proxy=self.proxy.sort(sort_column_names, sort_column_orders))\n File \"/home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/context.py\", line 49, in exit\n raise exc_type(exc_value)\n MemoryError: std::bad_alloc\n \t\t"}, "comments_17": {"comment_id": 19, "comment_author": "dhgokul", "commentT": "2018-06-19T06:23:56Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n  Any update on this issue ?\n \t\t"}, "comments_18": {"comment_id": 20, "comment_author": "dhgokul", "commentT": "2018-06-25T22:00:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dhgokul>@dhgokul</denchmark-link>\n  I tried reproducing this on a VM with limited memory and so far I have been unable to. I will mark this as \"need repro\" and let some colleagues know so that they can give it a try.\n \t\t"}, "comments_19": {"comment_id": 21, "comment_author": "dhgokul", "commentT": "2018-06-27T12:59:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n  Still i am not able to train model in GPU machine. PFB link for dataset\n Dataset size is small when compared to previous dataset - 534 images\n <denchmark-link:https://bugreport.apple.com/web/?problemID=41519407>https://bugreport.apple.com/web/?problemID=41519407</denchmark-link>\n \n GPU log : Process keep on increase at initial training process (0  to 90 + percentage) ,after that it not processed\n <denchmark-link:https://user-images.githubusercontent.com/5689804/41975123-77d4e762-7a37-11e8-87ba-95d1b558a094.png></denchmark-link>\n \n CPU log : Process keep on loading\n <denchmark-link:https://user-images.githubusercontent.com/5689804/41975124-78072f06-7a37-11e8-9350-e40484c22b5a.png></denchmark-link>\n \n Training log : It keep on loading at 1st iteration\n <denchmark-link:https://user-images.githubusercontent.com/5689804/41975125-7838dbc8-7a37-11e8-86f5-58f91eedbe33.png></denchmark-link>\n \n \t\t"}, "comments_20": {"comment_id": 22, "comment_author": "dhgokul", "commentT": "2018-06-27T13:00:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n   Please check and let me know how to fix this issue, i tried same dataset in CPU machine, got memory error , but my dataset is only 584 images\n Traceback (most recent call last):\n File \"turi.py\", line 70, in \n model = tc.object_detector.create(train_data, feature='image', annotations='annotations', max_iterations=10)\n File \"/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/object_detector.py\", line 336, in create\n for batch in loader:\n File \"/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py\", line 123, in next\n return self._next()\n File \"/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/toolkits/object_detector/_sframe_loader.py\", line 145, in _next\n row = self.sframe[self.cur_sample]\n File \"/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/data_structures/sframe.py\", line 3540, in getitem\n val_list = list(SFrame(_proxy = self.proxy.copy_range(lb, 1, ub)))\n File \"/home/aximsoft/venv/local/lib/python2.7/site-packages/turicreate/data_structures/sframe.py\", line 3669, in generator\n self.proxy.begin_iterator()\n File \"turicreate/cython/cy_sframe.pyx\", line 231, in turicreate.cython.cy_sframe.UnitySFrameProxy.begin_iterator\n File \"turicreate/cython/cy_sframe.pyx\", line 232, in turicreate.cython.cy_sframe.UnitySFrameProxy.begin_iterator\n MemoryError: std::bad_alloc\n \t\t"}, "comments_21": {"comment_id": 23, "comment_author": "dhgokul", "commentT": "2018-06-27T13:51:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gustavla>@gustavla</denchmark-link>\n   I got error mentioned below @ GPU machine\n [13:48:20] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\n Segmentation fault: 11\n Stack trace returned 9 entries:\n [bt] (0) /home/ubuntu/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2a9e78) [0x7f79213dae78]\n [bt] (1) /home/ubuntu/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2909e8e) [0x7f7923a3ae8e]\n [bt] (2) /lib/x86_64-linux-gnu/libc.so.6(+0x354b0) [0x7f79693194b0]\n [bt] (3) /lib/x86_64-linux-gnu/libc.so.6(+0x15fe73) [0x7f7969443e73]\n [bt] (4) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0xe86455) [0x7f7948018455]\n [bt] (5) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(turi::lambda::lambda_master::bulk_eval(unsigned long, turi::sframe_rows const&, std::vector<turi::flexible_type, std::allocatorturi::flexible_type >&, bool, int)+0x18d) [0x7f7948018dfd]\n [bt] (6) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0x7d15a9) [0x7f79479635a9]\n [bt] (7) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0x7fdf80) [0x7f794798ff80]\n [bt] (8) /home/ubuntu/venv/local/lib/python2.7/site-packages/turicreate/cython/../libunity_shared.so(+0x1a1331f) [0x7f7948ba531f]\n \t\t"}, "comments_22": {"comment_id": 24, "comment_author": "dhgokul", "commentT": "2019-06-23T20:00:56Z", "comment_text": "\n \t\tCan I safely stop a training process at any time without losing my model? I've been iterating for a few days now and wish to stop as I think my loss values are sufficiently low now (went from over 80 to 0.106.\n \t\t"}}}, "commit": {"commit_id": "1f310d21aa3523e9dbdb18aba50ab78161736939", "commit_author": "Shantanu Chhabra", "commitT": "2018-07-26 21:43:37-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\unity\\python\\turicreate\\toolkits\\object_detector\\_sframe_loader.py", "file_new_name": "src\\unity\\python\\turicreate\\toolkits\\object_detector\\_sframe_loader.py", "file_complexity": {"file_NLOC": "314", "file_CCN": "65", "file_NToken": "2278"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "21,22,23", "method_info": {"method_name": "_convert_image_to_raw", "method_params": "image", "method_startline": "21", "method_endline": "23", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "28", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "80", "deleted_lines": "88", "method_info": {"method_name": "_next", "method_params": "self", "method_startline": "67", "method_endline": "92", "method_complexity": {"method_NLOC": "19", "method_CCN": "8", "method_NToken": "146", "method_nesting_level": "1"}}}}}}}}