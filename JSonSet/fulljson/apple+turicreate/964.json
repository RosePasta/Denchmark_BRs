{"BR": {"BR_id": "964", "BR_author": "tetreault", "BRopenT": "2018-08-12T20:14:16Z", "BRcloseT": "2018-08-20T18:49:23Z", "BR_text": {"BRsummary": "Activity Classifier random_split_by_session should support datasets with fewer than 100 sessions", "BRdescription": "\n Hello, making this new thread since i erroneously discussed this in an unrelated thread (<denchmark-link:https://github.com/apple/turicreate/issues/813>#813</denchmark-link>\n )\n First post:\n Total machine learning noob here but came across Turi because I have a task that requires somewhat reliably going from standing-to-kneeling posture (and ideally vice versa). I was following along with the documentation, and made sure my preprocessed CSV mirrors the format used in the table in the \"advanced usage\" section.\n I'm running the following python script to generate a model:\n <denchmark-code>import turicreate as tc\n dataSFrame = tc.SFrame('motion_data_all.csv')\n \n # Train/test split by recording sessions\n train, test = tc.activity_classifier.util.random_split_by_session(dataSFrame, session_id='exp_id', fraction=0.8)\n \n # Create an activity classifier\n motion_model = tc.activity_classifier.create(train, session_id='exp_id', target='motion', prediction_window=50)\n \n metrics = motion_model.evaluate(test)\n print(metrics['accuracy'])\n \n # Save the model for later use in Turi Create\n motion_model.save('motion_model.model')\n \n # Export for use in Core ML\n motion_model.export_coreml('MotionActivityClassifier.mlmodel')\n \n # try out the model \n model_test_data = dataSFrame[(dataSFrame['motion'] == 'sit') & (dataSFrame['exp_id'] == 1)][500:1000]\n motion_model.predict(model_test_data, output_frequency='per_window')\n </denchmark-code>\n \n My training data is extremely small, as far as ML training data goes. Approx 3.5k rows of standing/sitting/kneeling, each accurately labeled. In the doc you use the HAPT data and then process it into a format with exp_id and the activity label. My data set mirrors that end result.\n When I run the following line in spyder:\n <denchmark-code>metrics = motion_model.evaluate(test)\n print(metrics['accuracy'])\n </denchmark-code>\n \n I get the following error: ToolkitError: Size of prediction probability vector(3) != number of classes(1).\n Just to be clear I'm not completely messing up from the start, I've included some screenshots from the file I use with the following: dataSFrame = tc.SFrame('motion_data_all.csv').\n <denchmark-link:https://user-images.githubusercontent.com/5934354/43996590-278bee60-9d94-11e8-8e00-3c38a1cb9838.png></denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/5934354/43996592-2de67262-9d94-11e8-9188-acac9ca507fa.png></denchmark-link>\n \n When I run the line:  I get the following output:\n <denchmark-link:https://user-images.githubusercontent.com/5934354/43996600-6aac9082-9d94-11e8-83f3-2ea9ce9f13e6.png></denchmark-link>\n \n Every project is unique so I don't expect anyone to debug this for me but any advice pointing me on what to read to learn how to resolve/understand that following error (ToolkitError: Size of prediction probability vector(3) != number of classes(1).) would be great.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tetreault", "commentT": "2018-08-12T20:14:54Z", "comment_text": "\n \t\tSecond post:\n Hey guys! Sorry for hijacking this thread my mistake :) I understand if this should be in a new thread.\n For starters, I'm definitely using an older version, in my console I see turicreate==4.3.2 which is just what pip installed.\n Overall my data i'm loading in to the SFrame has 4 sessions.\n The lines you asked me to run above result in the following:\n <denchmark-code>len(dataSFrame['exp_id'].unique()) # outputs 4\n len(train['motion'].unique()) # outputs 3\n len(test['motion'].unique()) # outputs 1\n </denchmark-code>\n \n So therein lies the problem I suppose, if I get the error ToolkitError: Size of prediction probability vector(3) != number of classes(1). and i'm seeing 3 of the sessions are in train, but only 1 in test?\n Thanks again for your time and response, as I mentioned I'm treading in very new water and am unfamiliar with ML as a whole but slowly trying to work on it!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tetreault", "commentT": "2018-08-13T08:23:54Z", "comment_text": "\n \t\tCopy of my previous reply in the other thread:\n First - please verify that you use the latest turicreate version, 5.0b3. There was a bug recently fixed in random_split_by_session(). You can do it by either running pip freeze | grep turicreate in shell, or print tc.__version__ in python.\n In addition, can you please provide the output of the following prints?\n <denchmark-code>len(dataSFrame['exp_id'].unique())\n len(train['motion'].unique())\n len(test['motion'].unique())\n </denchmark-code>\n \n It seems that your train set only contains two sessions? Is that correct?\n The train set should contain examples from all supported activities. During training a mapping is created that maps all supported activities to integers, and this mapping is used by the prediction/evaluation functions. It is also advisable that the test set contain a mixture of all activities (though not mandatory).\n I'm surprised you even got a valid train-test split. The random_split_by_session() function is not intended for very small datasets - from the very reason that it is not likely to randomly include all activities if the data is too small. It should have errored out and refuse to do the split below a certain number of sessions (currently hard coded to 100).\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tetreault", "commentT": "2018-08-13T08:30:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tetreault>@tetreault</denchmark-link>\n  - it seems that you dataset is too small. Please try to collect more data.\n It seems that you have collected a single session from each activity (motion) - you should collect a larger number of activities - taken at different times (maybe even from different users).\n Note that a session may include a person doing a variety of activities. You don't have to record data of someone only sitting, then only standing etc. It's ok if you do - but then try to have at least 4-5 different sessions of each activity.\n You could try to artificially break down the data you already have to shorter sessions - but then you would probably encounter the original problem mentioned in <denchmark-link:https://github.com/apple/turicreate/issues/813>#813</denchmark-link>\n  .\n I'm closing the issue for now. Feel free to re-open if you have more questions or encounter further problems.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "tetreault", "commentT": "2018-08-13T14:42:42Z", "comment_text": "\n \t\tOkay <denchmark-link:https://github.com/igiloh>@igiloh</denchmark-link>\n  I doubled back, grabbed all the HAPT data. I ran the steps in the <denchmark-link:https://apple.github.io/turicreate/docs/userguide/activity_classifier/data-preparation.html>data prep page</denchmark-link>\n . Everything went smoothly.\n So now I'm using the code specified for creating the model (see <denchmark-link:https://apple.github.io/turicreate/docs/userguide/activity_classifier/>here</denchmark-link>\n ). This data set now is the full HAPT data set, so much much larger than my 3.5k rows I put together myself.\n When I run the following line: train, test = tc.activity_classifier.util.random_split_by_session(data, session_id='exp_id', fraction=0.8)\n I now receive the following warning: The dataset has less than the minimum of 100 sessions required for train-validation split. Continuing without validation set This results in an empty test set, so I cannot run metrics = model.evaluate(test).\n Is this the problem you mentioned with random_split_by_session in your prior comment? I wanted to also confirm that I made sure to update turicreate and am indeed using v5.0b3.\n Thanks again for your time and any advice!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "tetreault", "commentT": "2018-08-13T15:45:51Z", "comment_text": "\n \t\tinterestingly enough, downgrading turicreate from v5.0b3 to v4.3.2 makes random_split_by_session work correctly.\n Running the following line in turicreate v4.3.2:\n train, test = tc.activity_classifier.util.random_split_by_session(data, session_id='exp_id', fraction=0.8)\n Now properly gives me a training set with 580,082 rows, and a test set with 168,381 rows.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "tetreault", "commentT": "2018-08-14T15:10:25Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/tetreault>@tetreault</denchmark-link>\n ,\n Actually - it's the other way around \ud83d\ude04\n We've added a safeguard in v.50b3 that prevents from creating an automatic validation test if the dataset is too small. The reason, as I have mentioned above, is to try to prevent the case that both train and test sets contain only part of the labels (motions).\n I admit this decision is done rather crudely - we simply don't allow splitting if there's less than a certain amount of sessions (currently hard coded to 100).\n In addition - the safeguard was added mainly to prevent the activity_classifier.create() function from creating an auto validation set. In no way it should block a user who is explicitly calling the random_split_by_session() API.\n I'm changing this issue to a feature request, so we will add:\n \n As a first step - an enforce flag to random_split_by_session(), allowing the user to split his sessions on any dataset when using this API.\n We should probably improve the decision mechanism for whether or not to create a validation set during create(). It should check the labels distribution of the train and validation sets (or some other method) - rather then a hard limit on the number of sessions.\n \n <denchmark-link:https://github.com/nickjong>@nickjong</denchmark-link>\n  - mind if I assign this FR to you? Feel free to re-assign it to whoever relevant.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "tetreault", "commentT": "2018-08-14T15:18:07Z", "comment_text": "\n \t\tI have also created <denchmark-link:https://github.com/apple/turicreate/issues/969>#969</denchmark-link>\n  - the error you got when you called  was a bit ambiguous. We can probably detect these cases right away and give a more informative error.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "tetreault", "commentT": "2018-08-14T16:16:46Z", "comment_text": "\n \t\tYou guys are awesome and on top of this stuff! So far my experience with turi and its maintainers is super positive. Thanks for the hard work guys!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "tetreault", "commentT": "2018-08-16T18:25:16Z", "comment_text": "\n \t\tLabeling as a bug since the current experience seems wrong....\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "tetreault", "commentT": "2018-08-17T16:39:45Z", "comment_text": "\n \t\tWe should definitely do step 1 of what Ilai suggested. Step 2 is really a separate matter, since this case is less about the validation_set='auto' and more about allowing users to invoke our validation-set creation code to produce test sets. <denchmark-link:https://github.com/apple/turicreate/issues/969>#969</denchmark-link>\n  covers a separate case: the fact that we currently throw an exception if the test set doesn't happen to span all of the labels present in the training data.\n With regard to step 2, note that most of our supervised toolkits currently just use the heuristic of using a validation set when the number of instances is above 100. I think the main question here is what is an \"instance\". A session feels like it's much bigger than an instance, since it may comprise several prediction windows, each of which has a separate (if not independent label). Given that our implementation already splits each session into segments of 20 prediction windows, we could in theory do the auto split if we have at least 100 of these 20-window segments. But that's nontrivial to compute accurately, and is probably best implemented on the C++ side as part of the chunking code. (Chunk first, then split.)\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "tetreault", "commentT": "2018-08-19T10:59:39Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/nickjong>@nickjong</denchmark-link>\n  - I agree, the train-validation split probably should move to the C++ part. This will also save the need to calculate the number of sessions using SFrame operations - which is quite slow.\n It does seem though that 100 sessions is a too high threshold for most datasets. It seems that most users collect much fewer data. Even the HAPT dataset (which we consider small, but may very well be a typical one) only has 42 sessions.\n For the time being - maybe we can add another threshold for num_sessions above which we do the split but verify that both sets include all possible labels. I.e.:\n if num_sessions < 30:\n      #don't do split\n if num_sessions > 30 and num_sessions <100:\n      train, valid = split()\n      if len(train[target].unique())) != len(valid[target].unique())):\n            #give some warning\\error\n if num_sessions > 100:\n       train, valid = split()\n      # just split. Verifying will probably be quite slow\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "tetreault", "commentT": "2018-08-20T19:00:20Z", "comment_text": "\n \t\tLet's use <denchmark-link:https://github.com/apple/turicreate/issues/991>#991</denchmark-link>\n  to track a fix for 6.0. I'm worried that we don't have time to adequately design and test a fix for 5.0, which we're releasing in just a couple of days. For 5.0, I'm pinning my hopes on <denchmark-link:https://github.com/apple/turicreate/issues/969>#969</denchmark-link>\n : it shouldn't really be problematic if the validation set and training data span different numbers of labels.\n \t\t"}}}, "commit": {"commit_id": "ba6b3cdb7537a9b6f99716fed9f0bc092b28cca0", "commit_author": "Nick Jong", "commitT": "2018-08-20 11:49:22-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "src\\unity\\python\\turicreate\\test\\test_activity_classifier.py", "file_new_name": "src\\unity\\python\\turicreate\\test\\test_activity_classifier.py", "file_complexity": {"file_NLOC": "300", "file_CCN": "49", "file_NToken": "2650"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "129", "deleted_lines": "129", "method_info": {"method_name": "_create_auto_validation_set", "method_params": "self", "method_startline": "129", "method_endline": "136", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "55", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "179", "deleted_lines": null, "method_info": {"method_name": "test_create_auto_validation_set_small", "method_params": "self", "method_startline": "175", "method_endline": "179", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "39", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "138,139,140,141,142,143,144,145,146,147,148,149,155", "deleted_lines": "138,144,150,151,152,153", "method_info": {"method_name": "_test_random_split_by_session", "method_params": "self,num_sessions,is_small", "method_startline": "138", "method_endline": "166", "method_complexity": {"method_NLOC": "21", "method_CCN": "3", "method_NToken": "221", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "146,147,148,149,155", "deleted_lines": "150,151,152,153,172,173", "method_info": {"method_name": "test_random_split_by_session", "method_params": "self", "method_startline": "146", "method_endline": "173", "method_complexity": {"method_NLOC": "21", "method_CCN": "1", "method_NToken": "219", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "179", "deleted_lines": "181", "method_info": {"method_name": "test_create_auto_validation_set_typical", "method_params": "self", "method_startline": "175", "method_endline": "181", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "47", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "129,138,139,140,141,142,143,144", "deleted_lines": "129,138,144", "method_info": {"method_name": "_create_auto_validation_set", "method_params": "self,is_small", "method_startline": "129", "method_endline": "144", "method_complexity": {"method_NLOC": "14", "method_CCN": "2", "method_NToken": "133", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": null, "deleted_lines": "196", "method_info": {"method_name": "test_create_auto_validation_set_string_session_id", "method_params": "self", "method_startline": "183", "method_endline": "196", "method_complexity": {"method_NLOC": "11", "method_CCN": "2", "method_NToken": "115", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\unity\\python\\turicreate\\toolkits\\activity_classifier\\_activity_classifier.py", "file_new_name": "src\\unity\\python\\turicreate\\toolkits\\activity_classifier\\_activity_classifier.py", "file_complexity": {"file_NLOC": "449", "file_CCN": "46", "file_NToken": "3720"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "31,169,170,171,172,173,174,175,176,177,178", "deleted_lines": "168"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\unity\\python\\turicreate\\toolkits\\activity_classifier\\util.py", "file_new_name": "src\\unity\\python\\turicreate\\toolkits\\activity_classifier\\util.py", "file_complexity": {"file_NLOC": "36", "file_CCN": "5", "file_NToken": "262"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "76,77,78,79,80", "method_info": {"method_name": "random_split_by_session", "method_params": "dataset,session_id,fraction,seed", "method_startline": "21", "method_endline": "109", "method_complexity": {"method_NLOC": "26", "method_CCN": "5", "method_NToken": "206", "method_nesting_level": "0"}}}}}}}}