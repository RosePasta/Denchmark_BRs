{"BR": {"BR_id": "540", "BR_author": "OleStauning", "BRopenT": "2017-02-01T22:04:05Z", "BRcloseT": "2017-03-10T20:42:45Z", "BR_text": {"BRsummary": "SIGSEGV when adding dropout layer", "BRdescription": "\n Code:\n <denchmark-code>    network<sequential> net;\n     adam opt;\n \n     net << fc<relu>(9*2,40)\n         << fc<relu>(40,10)\n         << dropout(10,0.5)\n         << fc<sigmoid>(10,2);\n \n     int epoch=0;\n     int epochs = 50;\n     int batch = 50;\n     net.fit<mse>(opt, xtrain, ytrain, batch, epochs,\n     [](){},\n     [&](){\n         auto loss = net.get_loss<mse>(xtest, ytest);\n         std::cout << \"Loss=\" << loss << std::endl;\n     });\n </denchmark-code>\n \n Error when using valgrind:\n <denchmark-code>==7176== Invalid read of size 1\n ==7176==    at 0x115C35: tiny_dnn::dropout_layer::back_propagation(std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*> > const&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*> > const&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*> >&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >*> >&) (dropout_layer.h:86)\n ==7176==    by 0x112683: tiny_dnn::layer::backward() (layer.h:515)\n ==7176==    by 0x11486E: tiny_dnn::sequential::backward(std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&) (nodes.h:285)\n ==7176==    by 0x13032C: void tiny_dnn::network<tiny_dnn::sequential>::bprop<tiny_dnn::mse>(std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&) (network.h:946)\n ==7176==    by 0x12D112: void tiny_dnn::network<tiny_dnn::sequential>::train_onebatch<tiny_dnn::mse, tiny_dnn::adam>(tiny_dnn::adam&, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, int, int, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*) (network.h:845)\n ==7176==    by 0x128B7C: void tiny_dnn::network<tiny_dnn::sequential>::train_once<tiny_dnn::mse, tiny_dnn::adam>(tiny_dnn::adam&, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, int, int, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*) (network.h:818)\n ==7176==    by 0x10E407: bool tiny_dnn::network<tiny_dnn::sequential>::fit<tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}>(tiny_dnn::adam&, std::vector<tiny_dnn::adam<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::aligned_allocator> >, std::allocator<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const&, std::vector<tiny_dnn::adam<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::aligned_allocator> >, std::allocator<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, std::vector<tiny_dnn::adam<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::aligned_allocator> >, std::allocator<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> > > >) (network.h:784)\n ==7176==    by 0x10E11B: bool tiny_dnn::network<tiny_dnn::sequential>::fit<tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}, std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, tiny_dnn::aligned_allocator>(tiny_dnn::adam&, {lambda()#2}<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::adam> > const&, {lambda()#2}<tiny_dnn::aligned_allocator, std::allocator<{lambda()#2}<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::adam> > const> > const&, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, {lambda()#2}<tiny_dnn::aligned_allocator, std::allocator<{lambda()#2}<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::adam> > const> >) (network.h:335)\n .....................\n ==4475== Process terminating with default action of signal 11 (SIGSEGV)\n ==4475==  Access not within mapped region at address 0x10\n ==4475==    at 0x12113A: std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() (shared_ptr_base.h:150)\n ==4475==    by 0x11AFCE: std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count() (shared_ptr_base.h:662)\n ==4475==    by 0x114D4F: std::__shared_ptr<tiny_dnn::edge, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr() (shared_ptr_base.h:928)\n ==4475==    by 0x114D6B: std::shared_ptr<tiny_dnn::edge>::~shared_ptr() (shared_ptr.h:93)\n ==4475==    by 0x115E54: tiny_dnn::layer::backward() (layer.h:511)\n ==4475==    by 0x118078: tiny_dnn::sequential::backward(std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&) (nodes.h:285)\n ==4475==    by 0x136CC6: void tiny_dnn::network<tiny_dnn::sequential>::bprop<tiny_dnn::mse>(std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&, std::vector<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, std::allocator<std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > > > const&) (network.h:946)\n ==4475==    by 0x13309E: void tiny_dnn::network<tiny_dnn::sequential>::train_onebatch<tiny_dnn::mse, tiny_dnn::adam>(tiny_dnn::adam&, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, int, int, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*) (network.h:845)\n ==4475==    by 0x12E01C: void tiny_dnn::network<tiny_dnn::sequential>::train_once<tiny_dnn::mse, tiny_dnn::adam>(tiny_dnn::adam&, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*, int, int, std::vector<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const*) (network.h:818)\n ==4475==    by 0x110E71: bool tiny_dnn::network<tiny_dnn::sequential>::fit<tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}>(tiny_dnn::adam&, std::vector<tiny_dnn::adam<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::aligned_allocator> >, std::allocator<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> > > > const&, std::vector<tiny_dnn::adam<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::aligned_allocator> >, std::allocator<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> > > >, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, std::vector<tiny_dnn::adam<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::aligned_allocator> >, std::allocator<tiny_dnn::adam<float, tiny_dnn::aligned_allocator<float, 64ul> > > >) (network.h:784)\n ==4475==    by 0x110B85: bool tiny_dnn::network<tiny_dnn::sequential>::fit<tiny_dnn::mse, tiny_dnn::adam, main::{lambda()#1}, main::{lambda()#2}, std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, tiny_dnn::aligned_allocator>(tiny_dnn::adam&, {lambda()#2}<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::adam> > const&, {lambda()#2}<tiny_dnn::aligned_allocator, std::allocator<{lambda()#2}<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::adam> > const> > const&, unsigned long, int, main::{lambda()#1}, main::{lambda()#2}, bool, int, {lambda()#2}<tiny_dnn::aligned_allocator, std::allocator<{lambda()#2}<std::vector<float, tiny_dnn::aligned_allocator<float, 64ul> >, std::allocator<tiny_dnn::adam> > const> >) (network.h:335)\n </denchmark-code>\n \n The code works fine without the dropout layer. Am I doing something wrong?\n Best regards\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "OleStauning", "commentT": "2017-02-01T23:13:36Z", "comment_text": "\n \t\tI saw a crash with dropout, but with a different model of course. Also works fine if I comment out dropout. This is on windows\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "OleStauning", "commentT": "2017-02-24T18:26:07Z", "comment_text": "\n \t\tMy guess is out_type of dropout_layer is {vector_type::data} and in_type of fully_connected_layer is {vector_type::data, vector_type::weight, vector_type::bias} or {vector_type::data, vector_type::weight}, so they can't be connected.\n But if it's really so, network class should report an error about inappropriate layer configuration.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "OleStauning", "commentT": "2017-02-24T18:59:03Z", "comment_text": "\n \t\tIt was my misunderstanding.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "OleStauning", "commentT": "2017-02-24T19:31:03Z", "comment_text": "\n \t\tIn dropout_layer::back_propagation method, in_grad is updated.\n for (serial_size_t sample = 0;\n       sample < static_cast<serial_size_t>(prev_delta.size()); ++sample) {\n   for (serial_size_t i = 0;\n         i < static_cast<serial_size_t>(curr_delta.size()); i++) {\n     prev_delta[sample][i] = mask_[sample][i] * curr_delta[sample][i];\n   }\n }\n During the process, mask_ member is accesed. A problem is found by adding below assert statement.\n   assert(mask_[sample].size() == curr_delta.size());\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "OleStauning", "commentT": "2017-02-25T01:57:43Z", "comment_text": "\n \t\tPosted a fix to the problem.\n <denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/543#issuecomment-282450220>#543 (comment)</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "OleStauning", "commentT": "2017-03-02T15:03:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/OleStauning>@OleStauning</denchmark-link>\n  can you please check with current master?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "OleStauning", "commentT": "2017-03-10T20:42:45Z", "comment_text": "\n \t\tYes - this have fixed the problem. Thanks.\n \t\t"}}}, "commit": {"commit_id": "9c4ae800b4f034693aee20d45fac817c81a9e3d6", "commit_author": "Evgeniy Zheltonozhskiy", "commitT": "2017-03-02 16:55:20+02:00", "commit_complexity": {"commit_NLOC": "0.30597014925373134", "commit_CCN": "0.6865671641791045", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "test\\test.cpp", "file_new_name": "test\\test.cpp", "file_complexity": {"file_NLOC": "32", "file_CCN": "1", "file_NToken": "88"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "17", "deleted_lines": "16,17"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "test\\test_dropout_layer.h", "file_new_name": "test\\test_dropout_layer.h", "file_complexity": {"file_NLOC": "68", "file_CCN": "6", "file_NToken": "800"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::TEST", "method_params": "dropout,full_net_batch", "method_startline": "82", "method_endline": "108", "method_complexity": {"method_NLOC": "18", "method_CCN": "2", "method_NToken": "269", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::TEST", "method_params": "dropout,full_net", "method_startline": "53", "method_endline": "80", "method_complexity": {"method_NLOC": "18", "method_CCN": "2", "method_NToken": "269", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\test_fully_connected_layer.h", "file_new_name": "test\\test_fully_connected_layer.h", "file_complexity": {"file_NLOC": "137", "file_CCN": "15", "file_NToken": "1599"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::TEST", "method_params": "fully_connected,train_different_batches", "method_startline": "53", "method_endline": "94", "method_complexity": {"method_NLOC": "30", "method_CCN": "3", "method_NToken": "363", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "test\\test_network.h", "file_new_name": "test\\test_network.h", "file_complexity": {"file_NLOC": "452", "file_CCN": "60", "file_NToken": "4862"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "631", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::TEST", "method_params": "network,read_write", "method_startline": "591", "method_endline": "644", "method_complexity": {"method_NLOC": "44", "method_CCN": "2", "method_NToken": "390", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::TEST", "method_params": "network,train_predict_different_batches", "method_startline": "257", "method_endline": "310", "method_complexity": {"method_NLOC": "42", "method_CCN": "7", "method_NToken": "484", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "test\\testhelper.h", "file_new_name": "test\\testhelper.h", "file_complexity": {"file_NLOC": "173", "file_CCN": "29", "file_NToken": "1326"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "118,120,121,122,127", "deleted_lines": "100", "method_info": {"method_name": "tiny_dnn::serialization_test", "method_params": "src,dst", "method_startline": "99", "method_endline": "128", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "176", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96", "deleted_lines": "83,93,95", "method_info": {"method_name": "tiny_dnn::network_serialization_test", "method_params": "src,dst", "method_startline": "73", "method_endline": "96", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "148", "method_nesting_level": "1"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tiny_dnn\\layers\\batch_normalization_layer.h", "file_new_name": "tiny_dnn\\layers\\batch_normalization_layer.h", "file_complexity": {"file_NLOC": "166", "file_CCN": "42", "file_NToken": "1326"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "177,178,179,180,181", "deleted_lines": "177,178,183,184", "method_info": {"method_name": "tiny_dnn::batch_normalization_layer::save", "method_params": "os,precision", "method_startline": "177", "method_endline": "184", "method_complexity": {"method_NLOC": "8", "method_CCN": "3", "method_NToken": "66", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "186,187", "deleted_lines": "183,184", "method_info": {"method_name": "tiny_dnn::batch_normalization_layer::load", "method_params": "is", "method_startline": "183", "method_endline": "187", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "44", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "186,187,188,189", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::batch_normalization_layer::load", "method_params": "is,precision", "method_startline": "186", "method_endline": "192", "method_complexity": {"method_NLOC": "7", "method_CCN": "3", "method_NToken": "61", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "177,178,179,180,181", "deleted_lines": "177,178", "method_info": {"method_name": "tiny_dnn::batch_normalization_layer::save", "method_params": "os", "method_startline": "177", "method_endline": "181", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "49", "method_nesting_level": "2"}}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tiny_dnn\\layers\\dropout_layer.h", "file_new_name": "tiny_dnn\\layers\\dropout_layer.h", "file_complexity": {"file_NLOC": "94", "file_CCN": "23", "file_NToken": "791"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "82,83,84,85,86", "deleted_lines": "82,83,84,85", "method_info": {"method_name": "tiny_dnn::dropout_layer::back_propagation", "method_params": "in_data,out_data,out_grad,in_grad", "method_startline": "72", "method_endline": "90", "method_complexity": {"method_NLOC": "15", "method_CCN": "3", "method_NToken": "150", "method_nesting_level": "2"}}}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tiny_dnn\\layers\\layer.h", "file_new_name": "tiny_dnn\\layers\\layer.h", "file_complexity": {"file_NLOC": "512", "file_CCN": "133", "file_NToken": "4178"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "334,335,336,337,338,339,340,341,342", "deleted_lines": "334,335,336,337", "method_info": {"method_name": "tiny_dnn::layer::save", "method_params": "os", "method_startline": "334", "method_endline": "342", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "44", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "350", "deleted_lines": "344", "method_info": {"method_name": "tiny_dnn::layer::load", "method_params": "is", "method_startline": "344", "method_endline": "350", "method_complexity": {"method_NLOC": "7", "method_CCN": "3", "method_NToken": "45", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "350,351,352,353,354", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::layer::load", "method_params": "is,precision", "method_startline": "350", "method_endline": "360", "method_complexity": {"method_NLOC": "11", "method_CCN": "3", "method_NToken": "70", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "334,335,336,337,338,339,340,341,342,343", "deleted_lines": "334,335,336,337,344", "method_info": {"method_name": "tiny_dnn::layer::save", "method_params": "os,precision", "method_startline": "334", "method_endline": "348", "method_complexity": {"method_NLOC": "10", "method_CCN": "3", "method_NToken": "69", "method_nesting_level": "2"}}}}}}}}