{"BR": {"BR_id": "7684", "BR_author": "simon-mo", "BRopenT": "2020-03-21T07:17:08Z", "BRcloseT": "2020-09-24T19:42:52Z", "BR_text": {"BRsummary": "Memory monitor errors in asyncio mode", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n In asyncio, the callstack cannot be retrieved via inspect.currentframe because it will error\n <denchmark-code>Exception ignored in: 'ray._raylet.get_py_stack'\n Traceback (most recent call last):\n   File \"/Users/simonmo/miniconda3/lib/python3.6/inspect.py\", line 1497, in currentframe\n     return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n ValueError: call stack is not deep enough\n </denchmark-code>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n Doc example <denchmark-link:https://ray.readthedocs.io/en/latest/async_api.html#asyncio-concurrency-for-actors>https://ray.readthedocs.io/en/latest/async_api.html#asyncio-concurrency-for-actors</denchmark-link>\n \n Run this in IPython\n import ray\n import asyncio\n ray.init()\n \n @ray.remote\n class AsyncActor:\n     # multiple invocation of this method can be running in\n     # the event loop at the same time\n     async def run_concurrent(self):\n         print(\"started\")\n         await asyncio.sleep(2) # concurrent workload here\n         print(\"finished\")\n \n actor = AsyncActor.remote()\n \n # regular ray.get\n ray.get([actor.run_concurrent.remote() for _ in range(4)])\n \n # async ray.get\n await actor.run_concurrent.remote()\n Observe\n <denchmark-code>2020-03-21 00:14:42,634\tINFO resource_spec.py:212 -- Starting Ray with 23.39 GiB memory available for workers and up to 11.71 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n 2020-03-21 00:14:42,987\tINFO services.py:1123 -- View the Ray dashboard at localhost:8265\n (pid=88430) started\n (pid=88430) started\n (pid=88430) started\n (pid=88430) started\n (pid=88430) finished\n (pid=88430) finished\n (pid=88430) finished\n (pid=88430) finished\n (pid=88430) started\n ---------------------------------------------------------------------------\n ValueError                                Traceback (most recent call last)\n ~/miniconda3/lib/python3.6/inspect.py in currentframe()\n    1495 def currentframe():\n    1496     \"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\n -> 1497     return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n    1498\n    1499 def stack(context=1):\n \n ValueError: call stack is not deep enough\n Exception ignored in: 'ray._raylet.get_py_stack'\n Traceback (most recent call last):\n   File \"/Users/simonmo/miniconda3/lib/python3.6/inspect.py\", line 1497, in currentframe\n     return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n ValueError: call stack is not deep enough\n </denchmark-code>\n \n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "simon-mo", "commentT": "2020-03-21T07:22:49Z", "comment_text": "\n \t\tSeems to be a cython bug <denchmark-link:https://github.com/cython/cython/issues/2735>cython/cython#2735</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "simon-mo", "commentT": "2020-03-21T07:45:36Z", "comment_text": "\n \t\tIt's known limitation <denchmark-link:https://cython.readthedocs.io/en/latest/src/userguide/limitations.html#stack-frames>https://cython.readthedocs.io/en/latest/src/userguide/limitations.html#stack-frames</denchmark-link>\n , cython code doesn't generate stack frames so when we get_py_stack is called with cython code it failes.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "simon-mo", "commentT": "2020-03-21T08:09:03Z", "comment_text": "\n \t\tget_py_stack seems to be called during during asyncio object id integration in cython, in particular the following three functions where we invoked raw ObjectID constructor:\n \n \n \n ray/python/ray/_raylet.pyx\n \n \n         Lines 529 to 541\n       in\n       a7a5d17\n \n \n \n \n \n \n  cdef void async_plasma_callback(CObjectID object_id, \n \n \n \n                                  int64_t data_size, \n \n \n \n                                  int64_t metadata_size) with gil: \n \n \n \n      core_worker = ray.worker.global_worker.core_worker \n \n \n \n      event_handler = core_worker.get_plasma_event_handler() \n \n \n \n  if event_handler is not None: \n \n \n \n          obj_id = ObjectID(object_id.Binary()) \n \n \n \n  if data_size > 0 and obj_id: \n \n \n \n  # This must be asynchronous to allow objects to avoid blocking \n \n \n \n  # the IO thread. \n \n \n \n              event_handler._loop.call_soon_threadsafe( \n \n \n \n                  event_handler._complete_future, obj_id) \n \n \n \n  \n \n \n \n \n \n \n \n \n ray/python/ray/_raylet.pyx\n \n \n         Lines 1168 to 1188\n       in\n       a7a5d17\n \n \n \n \n \n \n  cdef void async_set_result_callback(shared_ptr[CRayObject] obj, \n \n \n \n                                      CObjectID object_id, \n \n \n \n  void *future) with gil: \n \n \n \n      cdef: \n \n \n \n          c_vector[shared_ptr[CRayObject]] objects_to_deserialize \n \n \n \n  \n \n \n \n      py_future = <object>(future) \n \n \n \n      loop = py_future._loop \n \n \n \n  \n \n \n \n  # Object is retrieved from in memory store. \n \n \n \n  # Here we go through the code path used to deserialize objects. \n \n \n \n      objects_to_deserialize.push_back(obj) \n \n \n \n      data_metadata_pairs = RayObjectsToDataMetadataPairs( \n \n \n \n          objects_to_deserialize) \n \n \n \n      ids_to_deserialize = [ObjectID(object_id.Binary())] \n \n \n \n      objects = ray.worker.global_worker.deserialize_objects( \n \n \n \n          data_metadata_pairs, ids_to_deserialize) \n \n \n \n      loop.call_soon_threadsafe(lambda: py_future.set_result( \n \n \n \n          AsyncGetResponse( \n \n \n \n  plasma_fallback_id=None, result=objects[0]))) \n \n \n \n  \n \n \n \n \n \n \n \n \n ray/python/ray/_raylet.pyx\n \n \n         Lines 1189 to 1197\n       in\n       a7a5d17\n \n \n \n \n \n \n  cdef void async_retry_with_plasma_callback(shared_ptr[CRayObject] obj, \n \n \n \n                                             CObjectID object_id, \n \n \n \n  void *future) with gil: \n \n \n \n      py_future = <object>(future) \n \n \n \n      loop = py_future._loop \n \n \n \n      loop.call_soon_threadsafe(lambda: py_future.set_result( \n \n \n \n                  AsyncGetResponse( \n \n \n \n  plasma_fallback_id=ObjectID(object_id.Binary()), \n \n \n \n  result=None))) \n \n \n \n \n \n (there also exists similar invocation in _raylet.pyx for serializing actor handles, and a helper method)\n <denchmark-link:https://github.com/stephanie-wang>@stephanie-wang</denchmark-link>\n  does  construction here invoke ref counting mechanism? If so, what part of the asyncio-object id integration should be changed.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "simon-mo", "commentT": "2020-03-21T08:09:13Z", "comment_text": "\n \t\talso cc <denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "89d959fd6ac206a1a3b5a6cb151d19290df6a235", "commit_author": "Simon Mo", "commitT": "2020-03-21 15:16:12-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\_raylet.pyx", "file_new_name": "python\\ray\\_raylet.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "572,573,574,575,576,577", "deleted_lines": "572"}}}}}}