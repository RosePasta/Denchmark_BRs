{"BR": {"BR_id": "12967", "BR_author": "kbc8894", "BRopenT": "2020-12-18T06:47:19Z", "BRcloseT": "2021-01-20T18:16:53Z", "BR_text": {"BRsummary": "[k8s][autoscaler] file_mounts option apply to head and worker node differently.", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n python 3.8.5\n ray 1.0.1\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a short code snippet (less than 50 lines if possible) that can be copy-pasted to reproduce the issue. The snippet should have no external library dependencies (i.e., use fake or mock data / environments):\n When I run ray up cluster.yaml -y, /example folder is mounted in head node.\n However, /example/example folder is mounted in worker node.\n <denchmark-code># An unique identifier for the head node and workers of this cluster.\n cluster_name: default\n \n # The minimum number of workers nodes to launch in addition to the head\n # node. This number should be >= 0.\n min_workers: 0\n \n # The maximum number of workers nodes to launch in addition to the head\n # node. This takes precedence over min_workers.\n max_workers: 2\n \n # The autoscaler will scale up the cluster faster with higher upscaling speed.\n # E.g., if the task requires adding more nodes then autoscaler will gradually\n # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.\n # This number should be > 0.\n upscaling_speed: 1.0\n \n # If a node is idle for this many minutes, it will be removed.\n idle_timeout_minutes: 5\n \n # Kubernetes resources that need to be configured for the autoscaler to be\n # able to manage the Ray cluster. If any of the provided resources don't\n # exist, the autoscaler will attempt to create them. If this fails, you may\n # not have the required permissions and will have to request them to be\n # created by your cluster administrator.\n provider:\n     type: kubernetes\n \n     # Exposing external IP addresses for ray pods isn't currently supported.\n     use_internal_ips: true\n \n     # Namespace to use for all resources created.\n     namespace: ray\n \n     # ServiceAccount created by the autoscaler for the head node pod that it\n     # runs in. If this field isn't provided, the head pod config below must\n     # contain a user-created service account with the proper permissions.\n     autoscaler_service_account:\n         apiVersion: v1\n         kind: ServiceAccount\n         metadata:\n             name: autoscaler\n \n     # Role created by the autoscaler for the head node pod that it runs in.\n     # If this field isn't provided, the role referenced in\n     # autoscaler_role_binding must exist and have at least these permissions.\n     autoscaler_role:\n         kind: Role\n         apiVersion: rbac.authorization.k8s.io/v1\n         metadata:\n             name: autoscaler\n         rules:\n         - apiGroups: [\"\"]\n           resources: [\"pods\", \"pods/status\", \"pods/exec\"]\n           verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\", \"patch\"]\n \n     # RoleBinding created by the autoscaler for the head node pod that it runs\n     # in. If this field isn't provided, the head pod config below must contain\n     # a user-created service account with the proper permissions.\n     autoscaler_role_binding:\n         apiVersion: rbac.authorization.k8s.io/v1\n         kind: RoleBinding\n         metadata:\n             name: autoscaler\n         subjects:\n         - kind: ServiceAccount\n           name: autoscaler\n         roleRef:\n             kind: Role\n             name: autoscaler\n             apiGroup: rbac.authorization.k8s.io\n \n     services:\n       # Service that maps to the head node of the Ray cluster.\n       - apiVersion: v1\n         kind: Service\n         metadata:\n             # NOTE: If you're running multiple Ray clusters with services\n             # on one Kubernetes cluster, they must have unique service\n             # names.\n             name: ray-head\n         spec:\n             # This selector must match the head node pod's selector below.\n             selector:\n                 component: ray-head\n             ports:\n                 - protocol: TCP\n                   port: 8000\n                   targetPort: 8000\n \n       # Service that maps to the worker nodes of the Ray cluster.\n       - apiVersion: v1\n         kind: Service\n         metadata:\n             # NOTE: If you're running multiple Ray clusters with services\n             # on one Kubernetes cluster, they must have unique service\n             # names.\n             name: ray-workers\n         spec:\n             # This selector must match the worker node pods' selector below.\n             selector:\n                 component: ray-worker\n             ports:\n                 - protocol: TCP\n                   port: 8000\n                   targetPort: 8000\n \n # Kubernetes pod config for the head node pod.\n head_node:\n     apiVersion: v1\n     kind: Pod\n     metadata:\n         # Automatically generates a name for the pod with this prefix.\n         generateName: ray-head-\n \n         # Must match the head node service selector above if a head node\n         # service is required.\n         labels:\n             component: ray-head\n     spec:\n         # Change this if you altered the autoscaler_service_account above\n         # or want to provide your own.\n         serviceAccountName: autoscaler\n \n         # Restarting the head node automatically is not currently supported.\n         # If the head node goes down, `ray up` must be run again.\n         restartPolicy: Never\n \n         # This volume allocates shared memory for Ray to use for its plasma\n         # object store. If you do not provide this, Ray will fall back to\n         # /tmp which cause slowdowns if is not a shared memory volume.\n         volumes:\n         - name: dshm\n           emptyDir:\n               medium: Memory\n \n         containers:\n         - name: ray-node\n           imagePullPolicy: Always\n           # You are free (and encouraged) to use your own container image,\n           # but it should have the following installed:\n           #   - rsync (used for `ray rsync` commands and file mounts)\n           #   - screen (used for `ray attach`)\n           #   - kubectl (used by the autoscaler to manage worker pods)\n           image: rayproject/ray:nightly\n           # Do not change this command - it keeps the pod alive until it is\n           # explicitly killed.\n           command: [\"/bin/bash\", \"-c\", \"--\"]\n           args: [\"trap : TERM INT; sleep infinity & wait;\"]\n           ports:\n               - containerPort: 6379 # Redis port.\n               - containerPort: 6380 # Redis port.\n               - containerPort: 6381 # Redis port.\n               - containerPort: 12345 # Ray internal communication.\n               - containerPort: 12346 # Ray internal communication.\n \n           # This volume allocates shared memory for Ray to use for its plasma\n           # object store. If you do not provide this, Ray will fall back to\n           # /tmp which cause slowdowns if is not a shared memory volume.\n           volumeMounts:\n               - mountPath: /dev/shm\n                 name: dshm\n           resources:\n               requests:\n                   cpu: 1000m\n                   memory: 512Mi\n               limits:\n                   # The maximum memory that this pod is allowed to use. The\n                   # limit will be detected by ray and split to use 10% for\n                   # redis, 30% for the shared memory object store, and the\n                   # rest for application memory. If this limit is not set and\n                   # the object store size is not set manually, ray will\n                   # allocate a very large object store in each pod that may\n                   # cause problems for other pods.\n                   memory: 2Gi\n           env:\n               # This is used in the head_start_ray_commands below so that\n               # Ray can spawn the correct number of processes. Omitting this\n               # may lead to degraded performance.\n               - name: MY_CPU_REQUEST\n                 valueFrom:\n                     resourceFieldRef:\n                         resource: requests.cpu\n \n # Kubernetes pod config for worker node pods.\n worker_nodes:\n     apiVersion: v1\n     kind: Pod\n     metadata:\n         # Automatically generates a name for the pod with this prefix.\n         generateName: ray-worker-\n \n         # Must match the worker node service selector above if a worker node\n         # service is required.\n         labels:\n             component: ray-worker\n     spec:\n         serviceAccountName: default\n \n         # Worker nodes will be managed automatically by the head node, so\n         # do not change the restart policy.\n         restartPolicy: Never\n \n         # This volume allocates shared memory for Ray to use for its plasma\n         # object store. If you do not provide this, Ray will fall back to\n         # /tmp which cause slowdowns if is not a shared memory volume.\n         volumes:\n         - name: dshm\n           emptyDir:\n               medium: Memory\n \n         containers:\n         - name: ray-node\n           imagePullPolicy: Always\n           # You are free (and encouraged) to use your own container image,\n           # but it should have the following installed:\n           #   - rsync (used for `ray rsync` commands and file mounts)\n           image: rayproject/ray:nightly\n           # Do not change this command - it keeps the pod alive until it is\n           # explicitly killed.\n           command: [\"/bin/bash\", \"-c\", \"--\"]\n           args: [\"trap : TERM INT; sleep infinity & wait;\"]\n           ports:\n               - containerPort: 12345 # Ray internal communication.\n               - containerPort: 12346 # Ray internal communication.\n \n           # This volume allocates shared memory for Ray to use for its plasma\n           # object store. If you do not provide this, Ray will fall back to\n           # /tmp which cause slowdowns if is not a shared memory volume.\n           volumeMounts:\n               - mountPath: /dev/shm\n                 name: dshm\n           resources:\n               requests:\n                   cpu: 1000m\n                   memory: 512Mi\n               limits:\n                   # This memory limit will be detected by ray and split into\n                   # 30% for plasma, and 70% for workers.\n                   memory: 2Gi\n           env:\n               # This is used in the head_start_ray_commands below so that\n               # Ray can spawn the correct number of processes. Omitting this\n               # may lead to degraded performance.\n               - name: MY_CPU_REQUEST\n                 valueFrom:\n                     resourceFieldRef:\n                         resource: requests.cpu\n \n # Files or directories to copy to the head and worker nodes. The format is a\n # dictionary from REMOTE_PATH: LOCAL_PATH, e.g.\n file_mounts: {\n       \"/example\": \"/path/to/example\"\n #    \"/path1/on/remote/machine\": \"/path1/on/local/machine\",\n #    \"/path2/on/remote/machine\": \"/path2/on/local/machine\",\n }\n \n # Files or directories to copy from the head node to the worker nodes. The format is a\n # list of paths. The same path on the head node will be copied to the worker node.\n # This behavior is a subset of the file_mounts behavior. In the vast majority of cases\n # you should just use file_mounts. Only use this if you know what you're doing!\n cluster_synced_files: []\n \n # Whether changes to directories in file_mounts or cluster_synced_files in the head node\n # should sync to the worker node continuously\n file_mounts_sync_continuously: False\n \n # Patterns for files to exclude when running rsync up or rsync down.\n # This is not supported on kubernetes.\n # rsync_exclude: []\n \n # Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for\n # in the source directory and recursively through all subdirectories. For example, if .gitignore is provided\n # as a value, the behavior will match git's behavior for finding and using .gitignore files.\n # This is not supported on kubernetes.\n # rsync_filter: []\n \n # List of commands that will be run before `setup_commands`. If docker is\n # enabled, these commands will run outside the container and before docker\n # is setup.\n initialization_commands: []\n \n # List of shell commands to run to set up nodes.\n setup_commands: []\n \n # Custom commands that will be run on the head node after common setup.\n head_setup_commands: []\n \n # Custom commands that will be run on worker nodes after common setup.\n worker_setup_commands: []\n \n # Command to start ray on the head node. You don't need to change this.\n # Note webui-host is set to 0.0.0.0 so that kubernetes can port forward.\n head_start_ray_commands:\n     - ray stop\n     - ulimit -n 65536; ray start --head --num-cpus=$MY_CPU_REQUEST --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host 0.0.0.0\n \n # Command to start ray on worker nodes. You don't need to change this.\n worker_start_ray_commands:\n     - ray stop\n     - ulimit -n 65536; ray start --num-cpus=$MY_CPU_REQUEST --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n </denchmark-code>\n \n If the code snippet cannot be run by itself, the issue will be closed with \"needs-repro-script\".\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kbc8894", "commentT": "2020-12-18T08:50:49Z", "comment_text": "\n \t\tI think this is similar to <denchmark-link:https://github.com/ray-project/ray/issues/12909>#12909</denchmark-link>\n  -- can you install  via  on your container?\n cc @Gekho457 this is because kubectl cp doesn't support directories properly.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kbc8894", "commentT": "2020-12-18T09:21:59Z", "comment_text": "\n \t\t\n I think this is similar to #12909 -- can you install rsync via apt-get install rsync on your container?\n cc @Gekho457 this is because kubectl cp doesn't support directories properly.\n \n <denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  I use <denchmark-link:https://hub.docker.com/r/rayproject/ray-ml>rayproject/ray-ml</denchmark-link>\n  image.\n I think <denchmark-link:https://github.com/ray-project/ray/blob/master/docker/base-deps/Dockerfile>base-dep</denchmark-link>\n  image already installed rsync.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kbc8894", "commentT": "2020-12-18T16:44:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/kbc8894>@kbc8894</denchmark-link>\n \n Could you try using a newer version of Ray, for example  or the latest master?\n I think the problem should have been fixed by this PR: <denchmark-link:https://github.com/ray-project/ray/pull/12356>#12356</denchmark-link>\n .\n Probably what you're experiencing is that rsync is being used in one place and (as a result of the bug fixed in that PR) kubectl cp is being used in another place.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "kbc8894", "commentT": "2020-12-18T17:12:25Z", "comment_text": "\n \t\t\n @richardliaw I use rayproject/ray-ml image.\n \n Ah, if you're currently running ray up out of rayproject/ray-ml, one approach is to switch to a more recent image: rayproject/ray-ml:nightly.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "kbc8894", "commentT": "2020-12-19T05:40:23Z", "comment_text": "\n \t\t\n @kbc8894\n Could you try using a newer version of Ray, for example ray1.1.0 or the latest master?\n I think the problem should have been fixed by this PR: #12356.\n Probably what you're experiencing is that rsync is being used in one place and (as a result of the bug fixed in that PR) kubectl cp is being used in another place.\n \n <denchmark-link:https://github.com/DmitriGekhtman>@DmitriGekhtman</denchmark-link>\n \n rayproject/ray-ml:1.1.0 is not working\n error message is\n <denchmark-code>2020-12-19 14:09:05,413 INFO command_runner.py:165 -- NodeUpdater: ray-head-bc-7724m: Running kubectl -n ray exec -it ray-head-bc-7724m -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (mkdir -p /example)'\n mkdir: cannot create directory \u2018/example\u2019: Permission denied\n </denchmark-code>\n \n rayproject/ray-ml:latest is also not working\n <denchmark-code>(base) root@ray-worker-bc-hpd9n:/example# pwd\n /example\n (base) root@ray-worker-bc-hpd9n:/example# cd example/\n (base) root@ray-worker-bc-hpd9n:/example/example# pwd\n /example/example\n </denchmark-code>\n \n rayproject/ray-ml:nightly is not working\n error message is\n <denchmark-code>2020-12-19 14:39:06,092 INFO command_runner.py:165 -- NodeUpdater: ray-head-bc-xsrxv: Running kubectl -n ray exec -it ray-head-bc-xsrxv -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (mkdir -p /example)'\n mkdir: cannot create directory \u2018/example\u2019: Permission denie\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "kbc8894", "commentT": "2020-12-19T06:36:22Z", "comment_text": "\n \t\tI see -- that's happening because, as of this PR <denchmark-link:https://github.com/ray-project/ray/issues/10786>#10786</denchmark-link>\n , the Ray images have a non-root user.\n For now, the way to get around this is to mount into a subdirectory of the remote container's home folder:\n \"~/example\"\n Is that sufficient for your use case?\n We'll update the examples to\n (a) suggest mounting into a subdirectory of the container's home\n (b) explain that the ray images use a non-root user\n \t\t"}}}, "commit": {"commit_id": "4832b3906611a92b027747c74bec6051dfb3fe72", "commit_author": "Dmitri Gekhtman", "commitT": "2020-12-19 16:09:24-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\kubernetes\\defaults.yaml", "file_new_name": "python\\ray\\autoscaler\\kubernetes\\defaults.yaml", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "253,254,256,257", "deleted_lines": "253,254"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\kubernetes\\example-full.yaml", "file_new_name": "python\\ray\\autoscaler\\kubernetes\\example-full.yaml", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "253,254,256,257", "deleted_lines": "253,254"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\kubernetes\\example-ingress.yaml", "file_new_name": "python\\ray\\autoscaler\\kubernetes\\example-ingress.yaml", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "289,290,292,293", "deleted_lines": "289,290"}}}}}}