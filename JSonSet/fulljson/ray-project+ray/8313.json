{"BR": {"BR_id": "8313", "BR_author": "yutaizhou", "BRopenT": "2020-05-04T19:18:13Z", "BRcloseT": "2020-08-11T19:10:29Z", "BR_text": {"BRsummary": "Ray cannot identify non-base 10 GPU numbering on SLURM", "BRdescription": "\n Hey all,\n I am using Ray on SLURM equipped with 2 GPUs per node. I get the following error when I submit a job that acquired GPUs:\n <denchmark-code>                                                                                                                                                                   \n   1   /state/partition1/user/yutai/raytmp\n   2   Traceback (most recent call last):\n   3   File \"/home/gridsan/yutai/.conda/envs/football2/bin/ray\", line 5, in <module>\n   4     from ray.scripts.scripts import main\n   5   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/__init__.py\", line 65, in <module>\n   6     from ray.worker import (\n   7   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py\", line 488, in <module>\n   8     global_worker = Worker()\n   9   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py\", line 118, in __init__\n  10     self.original_gpu_ids = ray.utils.get_cuda_visible_devices()\n  11   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py\", line 288, in get_cuda_visible_devices\n  12     return [int(i) for i in gpu_ids_str.split(\",\")]\n  13   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py\", line 288, in <listcomp>\n  14     return [int(i) for i in gpu_ids_str.split(\",\")]\n  15 srun: error: d-10-14-2: task 0: Exited with exit code 1\n  16 ValueError: invalid literal for int() with base 10: 'GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb'\n  17 adding workers\n  18 GPUS: GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb,GPU-3de6a983-6da7-e1cf-ffc6-a285c71ff5d3\n  19 Traceback (most recent call last):\n  20   File \"ray_test.py\", line 5, in <module>\n  21     import ray\n  22   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/__init__.py\", line 65, in <module>\n  23     from ray.worker import (\n  24   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py\", line 488, in <module>\n  25     global_worker = Worker()\n  26   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/worker.py\", line 118, in __init__\n  27     self.original_gpu_ids = ray.utils.get_cuda_visible_devices()\n  28   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py\", line 288, in get_cuda_visible_devices\n  29     return [int(i) for i in gpu_ids_str.split(\",\")]\n  30   File \"/home/gridsan/yutai/.conda/envs/football2/lib/python3.6/site-packages/ray/utils.py\", line 288, in <listcomp>\n  31     return [int(i) for i in gpu_ids_str.split(\",\")]\n  32 ValueError: invalid literal for int() with base 10: 'GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb'\n </denchmark-code>\n \n echo \"GPUS: $CUDA_VISIBLE_DEVICES\" gets me the following:\n GPUS: GPU-77e3a4cd-22fb-5d31-b8a6-409df6133eeb,GPU-3de6a983-6da7-e1cf-ffc6-a285c71ff5d3\n I am using Ray 0.8.4, and Python 3.6.10, on Ubuntu.\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Here are my sbatch shell script and python script. Note there are small deviations from this SLURM example in the docs as our SLURM system team initially got the example to work and I am simply using their modifications. The conda environment shouldn't be the bottleneck here.\n <denchmark-code>#!/bin/bash\n \n #SBATCH -o %j.log\n #SBATCH --job-name=ray_test\n #SBATCH -n 4\n #SBATCH -N 2\n #SBATCH --gres=gpu:volta:2\n \n export LC_ALL=C.UTF-8\n export LANG=C.UTF-8\n \n ((worker_num=$SLURM_NNODES-1)) # Must be one less that the total number of nodes\n echo $worker_num\n \n # Set up environment\n eval \"$(conda shell.bash hook)\"\n conda activate football2\n # or\n # conda activate /home/gridsan/groups/ERGO_GRF/football_env/football\n \n nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names\n nodes_array=( $nodes )\n \n node1=${nodes_array[0]}\n \n # Set IP address/port of head node\n ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address) # Making address\n port='6379'\n ip_head=$ip_prefix':'$port\n export ip_head # Exporting for latter access by trainer.py\n \n # Temporary directory for logging\n tmpdir='/state/partition1/user/'$USER'/raytmp'\n echo $tmpdir\n mkdircmd='mkdir -p '$tmpdir\n \n # Start the head\n srun --nodes=1 --ntasks=1 -w $node1 $mkdircmd\n srun --nodes=1 --ntasks=1 -w $node1 ray start --temp-dir=$tmpdir --block --head --redis-port=$port &\n sleep 5\n \n # Start workers\n echo \"adding workers\"\n for ((  i=1; i<=$worker_num; i++ ))\n do\n     node2=${nodes_array[$i]}\n     srun --nodes=1 --ntasks=1 -w $node2 mkdir -p $tmpdir\n     srun --nodes=1 --ntasks=1 -w $node2 ray start --temp-dir=$tmpdir --block --address=$ip_head & # Starting the workers\n done\n echo \"GPUS: $CUDA_VISIBLE_DEVICES\"\n python ray_test.py $SLURM_NTASKS # Pass the total number of allocated CPUs\n </denchmark-code>\n \n <denchmark-code># trainer.py\n import os\n import sys\n import time\n import ray\n import torch \n \n \n ray.init(address=os.environ[\"ip_head\"])\n \n @ray.remote\n def f():\n   time.sleep(1)\n \n # The following takes one second (assuming that ray was able to access all of the allocated nodes).\n start = time.time()\n num_cpus = int(sys.argv[1])\n ray.get([f.remote() for _ in range(num_cpus)])\n end = time.time()\n print(end - start)\n \n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "yutaizhou", "commentT": "2020-05-04T19:54:30Z", "comment_text": "\n \t\tThanks for opening this issue <denchmark-link:https://github.com/yutaizhou>@yutaizhou</denchmark-link>\n  ! Is it possible to map the string back to the original device index (integers)?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "yutaizhou", "commentT": "2020-05-05T00:35:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  thanks for checking in! I think I could, since every node in our HPC system only has 2 GPUs anyway. I could just map the 2 GPUs as 0 and 1? Not sure if that would work across the entire system though. I will try it\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "yutaizhou", "commentT": "2020-05-07T17:12:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  So I have tried remapping the GPU IDs but didn't get very far..\n \n After talking to the HPC team, here is my understanding of the status and some direct quotes from them:\n \n \n \"We\u2019re using NVIDIA\u2019s UUID API to set the device names to the UUID, rather than the default. The problem with the default naming scheme (0,1,etc) is that it is not consistent. What\u2019s listed as GPU 0 might change even within a job, which you can imagine would cause major problems if you have two people on a node, each allocated one GPU.\" -MITSC\n \n \n In a shared environment like a HPC system, UUID seems to be the best (only?) way to to make sure people only have access to the GPUs that is allocated to them. Apparently TF and PyTorch don't have a problem with that. The HPC team essentially recommends Ray to accept non-default GPU naming scheme.\n \n \n What do you think?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "yutaizhou", "commentT": "2020-05-07T20:06:06Z", "comment_text": "\n \t\tOK got it. Let me see if we can prio this. cc <denchmark-link:https://github.com/anabranch>@anabranch</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "yutaizhou", "commentT": "2020-05-08T15:04:20Z", "comment_text": "\n \t\tGreat, thank you Richard!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "yutaizhou", "commentT": "2020-07-06T22:36:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yutaizhou>@yutaizhou</denchmark-link>\n  Did you find a fix? I'm having the same issue on PBS.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "yutaizhou", "commentT": "2020-07-09T13:47:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Graeme22>@Graeme22</denchmark-link>\n  yes but it is very hacky, and I still believe this should be addressed upstream in Ray.\n When you do srun to spin up nodes, add an extra unset.sh file:\n <denchmark-code>srun --nodes=1 --ntasks=1 -w $node1 unset.sh && ray start --temp-dir=$tmpdir --block --head --redis-port=$port &\n </denchmark-code>\n \n unset.sh\n <denchmark-code>unset CUDA_VISIBLE_DEVICES\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "yutaizhou", "commentT": "2020-07-10T21:23:20Z", "comment_text": "\n \t\tFor anyone having this issue, my quick fix was the following: Change line 288 of utils.py to this:\n <denchmark-code>return gpu_ids_str.split(\",\")\n </denchmark-code>\n \n The string based IDs still work as expected, and avoid casting error when they're not numerical.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "yutaizhou", "commentT": "2020-08-11T19:10:29Z", "comment_text": "\n \t\tThis should be fixed in <denchmark-link:https://github.com/ray-project/ray/pull/9744>#9744</denchmark-link>\n ! Please ping (or open a new issue if not!)\n \t\t"}}}, "commit": {"commit_id": "32cd94b7505bf163db83d86f06b86fec698361b1", "commit_author": "yncxcw", "commitT": "2020-08-11 12:09:46-07:00", "commit_complexity": {"commit_NLOC": "0.08333333333333333", "commit_CCN": "0.08333333333333333", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\actors.rst", "file_new_name": "doc\\source\\actors.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "75,76", "deleted_lines": "75,76"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\using-ray-with-gpus.rst", "file_new_name": "doc\\source\\using-ray-with-gpus.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "36,37", "deleted_lines": "36,37"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\_raylet.pyx", "file_new_name": "python\\ray\\_raylet.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "356", "deleted_lines": "356"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\ray\\tests\\test_actor_resources.py", "file_new_name": "python\\ray\\tests\\test_actor_resources.py", "file_complexity": {"file_NLOC": "456", "file_CCN": "126", "file_NToken": "3541"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101", "deleted_lines": "101", "method_info": {"method_name": "test_actor_gpus.get_location_and_ids", "method_params": "self", "method_startline": "100", "method_endline": "103", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "37", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "98,101", "deleted_lines": "98,101", "method_info": {"method_name": "test_actor_gpus", "method_params": "ray_start_cluster", "method_startline": "86", "method_endline": "122", "method_complexity": {"method_NLOC": "25", "method_CCN": "7", "method_NToken": "194", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "98", "deleted_lines": "98", "method_info": {"method_name": "test_actor_gpus.__init__", "method_params": "self", "method_startline": "97", "method_endline": "98", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "2"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "python\\ray\\tests\\test_advanced_2.py", "file_new_name": "python\\ray\\tests\\test_advanced_2.py", "file_complexity": {"file_NLOC": "542", "file_CCN": "148", "file_NToken": "4442"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "693", "deleted_lines": null, "method_info": {"method_name": "test_local_mode_gpus.f", "method_params": "", "method_startline": "689", "method_endline": "693", "method_complexity": {"method_NLOC": "5", "method_CCN": "2", "method_NToken": "30", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "671,672", "deleted_lines": null, "method_info": {"method_name": "test_specific_gpus.g", "method_params": "", "method_startline": "668", "method_endline": "672", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "38", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "665", "deleted_lines": null, "method_info": {"method_name": "test_specific_gpus.f", "method_params": "", "method_startline": "662", "method_endline": "665", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "28", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652", "deleted_lines": "646,652", "method_info": {"method_name": "test_gpu_ids_as_str", "method_params": "save_gpu_ids_shutdown_only,as_str", "method_startline": "637", "method_endline": "652", "method_complexity": {"method_NLOC": "8", "method_CCN": "3", "method_NToken": "67", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "644,645,646,647,648,649,650", "deleted_lines": "646", "method_info": {"method_name": "test_gpu_ids_as_str.get_gpu_ids", "method_params": "as_str", "method_startline": "644", "method_endline": "650", "method_complexity": {"method_NLOC": "7", "method_CCN": "3", "method_NToken": "37", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "693", "deleted_lines": null, "method_info": {"method_name": "test_local_mode_gpus", "method_params": "save_gpu_ids_shutdown_only", "method_startline": "678", "method_endline": "695", "method_complexity": {"method_NLOC": "10", "method_CCN": "3", "method_NToken": "87", "method_nesting_level": "0"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "665,671,672", "deleted_lines": "674", "method_info": {"method_name": "test_specific_gpus", "method_params": "save_gpu_ids_shutdown_only", "method_startline": "655", "method_endline": "675", "method_complexity": {"method_NLOC": "11", "method_CCN": "4", "method_NToken": "104", "method_nesting_level": "0"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\utils.py", "file_new_name": "python\\ray\\utils.py", "file_complexity": {"file_NLOC": "401", "file_CCN": "106", "file_NToken": "2504"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "274,275,276,289,290", "deleted_lines": "274,275,276,289", "method_info": {"method_name": "get_cuda_visible_devices", "method_params": "", "method_startline": "270", "method_endline": "290", "method_complexity": {"method_NLOC": "9", "method_CCN": "4", "method_NToken": "50", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "300", "deleted_lines": "299", "method_info": {"method_name": "set_cuda_visible_devices", "method_params": "gpu_ids", "method_startline": "296", "method_endline": "308", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "39", "method_nesting_level": "0"}}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\worker.py", "file_new_name": "python\\ray\\worker.py", "file_complexity": {"file_NLOC": "999", "file_CCN": "160", "file_NToken": "5322"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "376,384,385,386,387", "deleted_lines": "376,403", "method_info": {"method_name": "get_gpu_ids", "method_params": "", "method_startline": "376", "method_endline": "405", "method_complexity": {"method_NLOC": "13", "method_CCN": "5", "method_NToken": "83", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "376,384,385,386,387,407,408,409,410,411,412,413,414,415,416,417", "deleted_lines": "376,403", "method_info": {"method_name": "get_gpu_ids", "method_params": "as_str", "method_startline": "376", "method_endline": "419", "method_complexity": {"method_NLOC": "22", "method_CCN": "8", "method_NToken": "126", "method_nesting_level": "0"}}}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\evaluation\\rollout_worker.py", "file_new_name": "rllib\\evaluation\\rollout_worker.py", "file_complexity": {"file_NLOC": "976", "file_CCN": "93", "file_NToken": "5019"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "416", "deleted_lines": "416"}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\policy\\torch_policy.py", "file_new_name": "rllib\\policy\\torch_policy.py", "file_complexity": {"file_NLOC": "483", "file_CCN": "43", "file_NToken": "3112"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "101", "deleted_lines": "101"}}}}}}