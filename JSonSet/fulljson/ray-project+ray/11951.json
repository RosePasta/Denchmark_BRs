{"BR": {"BR_id": "11951", "BR_author": "krfricke", "BRopenT": "2020-11-11T19:31:25Z", "BRcloseT": "2020-11-13T01:22:43Z", "BR_text": {"BRsummary": "[autoscaler] restarting ray with `ray up` switches object store location from /dev/shm to /tmp", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Restarting the ray runtime on a cluster that uses docker with shm moves the plasma object store location to /tmp.\n After starting the cluster the first time (scroll to end):\n <denchmark-code># ps aux | grep plasma_directory\n root        8435  1.3  0.0 979289612 67788 ?     Sl   11:24   0:03 /root/anaconda3/lib/python3.7/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/raylet --store_socket_name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/plasma_store --object_manager_port=8076 --min_worker_port=10000 --max_worker_port=10999 --node_manager_port=63984 --node_ip_address=172.31.16.137 --redis_address=172.31.16.137 --redis_port=6379 --num_initial_workers=64 --maximum_startup_concurrency=64 --static_resource_list=node:172.31.16.137,1.0,CPU,64,memory,455,object_store_memory,6580 --config_list=plasma_store_as_thread,True --python_worker_command=/root/anaconda3/bin/python /root/anaconda3/lib/python3.7/site-packages/ray/workers/default_worker.py --node-ip-address=172.31.16.137 --node-manager-port=63984 --object-store-name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/plasma_store --raylet-name=/tmp/ray/session_2020-11-11_11-24-02_969644_8401/sockets/raylet --redis-address=172.31.16.137:6379 --config-list=plasma_store_as_thread,True --temp-dir=/tmp/ray --metrics-agent-port=41883 --redis-password=5241590000000000 --java_worker_command= --cpp_worker_command= --redis_password=5241590000000000 --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2020-11-11_11-24-02_969644_8401 --metrics-agent-port=41883 --metrics_export_port=63079 --object_store_memory=500000000000 --plasma_directory=/dev/shm --head_node\n </denchmark-code>\n \n <denchmark-code>ray up -y issue.yaml\n </denchmark-code>\n \n After restarting (scroll to end):\n <denchmark-code>root       13477  3.1  0.0 491008224 67388 ?     Sl   11:29   0:00 /root/anaconda3/lib/python3.7/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/raylet --store_socket_name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/plasma_store --object_manager_port=8076 --min_worker_port=10000 --max_worker_port=10999 --node_manager_port=53349 --node_ip_address=172.31.16.137 --redis_address=172.31.16.137 --redis_port=6379 --num_initial_workers=64 --maximum_startup_concurrency=64 --static_resource_list=node:172.31.16.137,1.0,CPU,64,memory,455,object_store_memory,6580 --config_list=plasma_store_as_thread,True --python_worker_command=/root/anaconda3/bin/python /root/anaconda3/lib/python3.7/site-packages/ray/workers/default_worker.py --node-ip-address=172.31.16.137 --node-manager-port=53349 --object-store-name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/plasma_store --raylet-name=/tmp/ray/session_2020-11-11_11-29-30_982983_13439/sockets/raylet --redis-address=172.31.16.137:6379 --config-list=plasma_store_as_thread,True --temp-dir=/tmp/ray --metrics-agent-port=60879 --redis-password=5241590000000000 --java_worker_command= --cpp_worker_command= --redis_password=5241590000000000 --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2020-11-11_11-29-30_982983_13439 --metrics-agent-port=60879 --metrics_export_port=50207 --object_store_memory=500000000000 --plasma_directory=/tmp --head_node\n root       14852  0.0  0.0   5192  2412 pts/10   S+   11:29   0:00 grep --color=auto plasma_directory\n </denchmark-code>\n \n issue.yaml\n <denchmark-code>cluster_name: stale_ref_issue\n min_workers: 0\n max_workers: 0\n initial_workers: 0\n autoscaling_mode: default\n docker:\n     image: 'anyscale/ray-ml:latest'\n     container_name: ray_container\n     pull_before_run: false\n     run_options:\n         - --privileged\n         - --shm-size=510000000000\n target_utilization_fraction: 0.8\n idle_timeout_minutes: 5\n provider:\n     type: aws\n     region: us-west-2\n     availability_zone: us-west-2a\n     # cache_stopped_nodes: false\n auth:\n     ssh_user: ubuntu\n head_node:\n     InstanceType: r5.16xlarge\n     ImageId: ami-05ac7a76b4c679a79\n \n worker_nodes:\n     InstanceType: m5.xlarge\n     ImageId: ami-05ac7a76b4c679a79\n     InstanceMarketOptions:\n         MarketType: spot\n file_mounts: {\n     \"/root/xgboost-benchmark\": \"/Users/kai/coding/anyscale/projects/xgboost-benchmark\",\n }\n cluster_synced_files:\n     - /tmp/ray_tmp_mount/~/xgboost-benchmark\n file_mounts_sync_continuously: true\n initialization_commands: []\n setup_commands: []\n head_setup_commands: \n     - apt install -y lsof\n worker_setup_commands: []\n head_start_ray_commands:\n     - ray stop\n     - \"ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --object-store-memory=500000000000 --resources='{\\\"actor_cpus\\\": 0}'\"\n worker_start_ray_commands:\n     - ray stop\n     - \"ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 --resources='{\\\"actor_cpus\\\": 4}'\"\n metadata:\n     anyscale:\n         working_dir: ~/xgboost-benchmark\n </denchmark-code>\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "krfricke", "commentT": "2020-11-11T20:13:07Z", "comment_text": "\n \t\tWhat is the output near the actual  ray start ?\n Is there a message like:\n <denchmark-code>2020-11-11 20:12:52,890 WARNING services.py:1625 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "krfricke", "commentT": "2020-11-11T21:36:19Z", "comment_text": "\n \t\tYep, there is:\n <denchmark-code>2020-11-11 11:29:31,384\tINFO services.py:1166 -- View the Ray dashboard at http://localhost:8265\n 2020-11-11 11:29:31,386\tWARNING services.py:1625 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 477999988736 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "krfricke", "commentT": "2020-11-11T21:38:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/krfricke>@krfricke</denchmark-link>\n  Was the cluster up when you added the ? You might need to  and  again if that is the case.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "krfricke", "commentT": "2020-11-11T22:06:42Z", "comment_text": "\n \t\tYes it was. I think this is actually related to <denchmark-link:https://github.com/ray-project/ray/issues/11950>#11950</denchmark-link>\n :  is still occupied by 30 GB of stale file handles, which are cleared only after ray is restarted.\n Is the size of /dev/shm checked after ?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "krfricke", "commentT": "2020-11-11T22:34:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/krfricke>@krfricke</denchmark-link>\n  Oh shoot! I meant  from your laptop to tear the cluster down.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "krfricke", "commentT": "2020-11-12T09:25:31Z", "comment_text": "\n \t\tYes, that works. However I was under the impression that running ray up on a running cluster is a viable way to just restart the ray runtime (and run setup commands etc), in fact I was using this quite a lot to save time on node startups.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "krfricke", "commentT": "2020-11-12T16:16:48Z", "comment_text": "\n \t\tray up will restart the ray runtime, but to change docker configuration (i.e. add --shm-size), you need to restart the docker runtime :/\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "krfricke", "commentT": "2020-11-12T16:22:30Z", "comment_text": "\n \t\tSo the problem here is that I didn't change anything. I.e. --shm-size was set all the time, but only lead to using /dev/shm on the first start, not the second. Though it seems this is just due to /dev/shm still  being occupied by the object spilled from the last session.\n \t\t"}}}, "commit": {"commit_id": "3b56a1a5220fe0c3b522cff43dbb6ef7e33bfd99", "commit_author": "Ian Rodney", "commitT": "2020-11-12 17:22:42-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.96", "commit_Nprams": "0.96"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\installation.rst", "file_new_name": "doc\\source\\installation.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "281,282", "deleted_lines": "281"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\autoscaler\\_private\\command_runner.py", "file_new_name": "python\\ray\\autoscaler\\_private\\command_runner.py", "file_complexity": {"file_NLOC": "626", "file_CCN": "104", "file_NToken": "3882"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "722,723", "deleted_lines": "719,720", "method_info": {"method_name": "run_init", "method_params": "self,as_head,file_mounts", "method_startline": "684", "method_endline": "766", "method_complexity": {"method_NLOC": "71", "method_CCN": "13", "method_NToken": "448", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806", "deleted_lines": null, "method_info": {"method_name": "_auto_configure_shm", "method_params": "self", "method_startline": "787", "method_endline": "806", "method_complexity": {"method_NLOC": "19", "method_CCN": "5", "method_NToken": "114", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\_private\\constants.py", "file_new_name": "python\\ray\\autoscaler\\_private\\constants.py", "file_complexity": {"file_NLOC": "20", "file_CCN": "2", "file_NToken": "109"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4,5", "deleted_lines": "4"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\ray-schema.json", "file_new_name": "python\\ray\\autoscaler\\ray-schema.json", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "245,246,247,248,249", "deleted_lines": null}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\ray_constants.py", "file_new_name": "python\\ray\\ray_constants.py", "file_complexity": {"file_NLOC": "116", "file_CCN": "12", "file_NToken": "492"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27,28", "deleted_lines": null}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\resource_spec.py", "file_new_name": "python\\ray\\resource_spec.py", "file_complexity": {"file_NLOC": "228", "file_CCN": "51", "file_NToken": "1125"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "182,183,184", "deleted_lines": "182", "method_info": {"method_name": "resolve", "method_params": "self,is_head,node_ip_address", "method_startline": "128", "method_endline": "229", "method_complexity": {"method_NLOC": "77", "method_CCN": "18", "method_NToken": "424", "method_nesting_level": "1"}}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\tests\\test_autoscaler.py", "file_new_name": "python\\ray\\tests\\test_autoscaler.py", "file_complexity": {"file_NLOC": "1360", "file_CCN": "138", "file_NToken": "8629"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548", "deleted_lines": null, "method_info": {"method_name": "testAutodetectResources", "method_params": "self", "method_startline": "1519", "method_endline": "1548", "method_complexity": {"method_NLOC": "29", "method_CCN": "1", "method_NToken": "152", "method_nesting_level": "1"}}}}}}}}