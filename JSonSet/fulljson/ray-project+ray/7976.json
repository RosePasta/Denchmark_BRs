{"BR": {"BR_id": "7976", "BR_author": "annaluo676", "BRopenT": "2020-04-11T05:21:34Z", "BRcloseT": "2020-04-16T23:06:59Z", "BR_text": {"BRsummary": "[core][rllib] Decreasing CPU utilization with Ray 0.8.4", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version: 0.8.4\n TensorFlow version: 2.1.0\n CPU utilization kept decreasing during an ARS job training.  See below for the metric on a ec2 p2.x16large instance.\n <denchmark-link:https://user-images.githubusercontent.com/45078924/79036017-b919f700-7b78-11ea-99d0-b08859d55a5c.png></denchmark-link>\n \n > ray memory showed 31 workers pinned in memory, which was expected. There are 124 local references, mostly at ray/rllib/utils/filter_manager.py:<listcomp>:25, ray/rllib/agents/ars/ars.py:<listcomp>:193 and ray/rllib/agents/ars/ars.py:<listcomp>:314. Seems that discounted reference counting didn't cause objects to explode.\n Besides, Ray dashboard showed almost all Worker instead of Worker.do_rollout().\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Script to reproduce:\n <denchmark-code>import ray\n import os\n from ray.rllib.agents.trainer import with_common_config\n from ray.rllib.agents.ars.ars import ARSTrainer\n \n gpus = [str(i) for i in list(range(12,16))]\n cpus = 32\n os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(gpus)\n \n ray.init(\n     num_cpus=cpus,\n     num_gpus=len(gpus),\n     object_store_memory=int(1.5e11))\n \n ars_config = with_common_config({\n     \"noise_stdev\": 0.02,\n     \"num_rollouts\": 50,\n     \"rollouts_used\": 25,\n     \"num_workers\": 31,\n     \"sgd_stepsize\": 0.01,\n     \"noise_size\": 250000000,\n     \"eval_prob\": 0.5,\n     \"model\":{\n         \"fcnet_hiddens\": []  # a linear policy\n         }\n     })\n \n agent = ARSTrainer(env=\"CartPole-v0\", \n                    config=ars_config)\n \n n_iter = int(1e7)\n for i in range(n_iter):    \n     result = agent.train()\n </denchmark-code>\n \n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "annaluo676", "commentT": "2020-04-11T05:53:16Z", "comment_text": "\n \t\t0.8.3 also exhibits this lazy worker syndrome; Ray 0.8.2 doesn't.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "annaluo676", "commentT": "2020-04-11T06:39:19Z", "comment_text": "\n \t\tThanks anna!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "annaluo676", "commentT": "2020-04-15T06:10:56Z", "comment_text": "\n \t\tI can also observe from RLlib's internal metrics, time per iteration gradually increases and CPU load decreases.\n <denchmark-link:https://user-images.githubusercontent.com/14922/79304002-285b5800-7ea5-11ea-9f44-e54fcb7c0b4c.png></denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "annaluo676", "commentT": "2020-04-15T20:23:03Z", "comment_text": "\n \t\tperf top shows\n <denchmark-code>Overhead  Shared Object                                      Symbol\n   10.96%  _raylet.so                                         [.] ray::RayObject::IsInPlasmaError\n    5.12%  _raylet.so                                         [.] ray::CoreWorkerMemoryStore::GetMemoryStoreStatisticalData\n    4.85%  perf                                               [.] __symbols__insert\n    3.17%  python3.6                                          [.] _PyEval_EvalFrameDefault\n    2.86%  libstdc++.so.6.0.26                                [.] std::string::_M_copy\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "annaluo676", "commentT": "2020-04-15T20:24:11Z", "comment_text": "\n \t\tThis is pointing to a leak in the number of objects in the memory store.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "annaluo676", "commentT": "2020-04-15T22:57:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/annaluo676>@annaluo676</denchmark-link>\n  just tested and this patch appears to resolve the slowdown problems.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "annaluo676", "commentT": "2020-04-16T00:16:57Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n . According to <denchmark-link:https://ray.readthedocs.io/en/latest/development.html#compilation>https://ray.readthedocs.io/en/latest/development.html#compilation</denchmark-link>\n  I will need to recompile the c++ code changes.  emitted the following error:\n <denchmark-code>INFO: Writing tracer profile to '/home/ubuntu/.cache/bazel/_bazel_ubuntu/4694ec92a350130755ebd31627e14086/command.profile.gz'\n ERROR: Skipping '//:ray_pkg': no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)\n WARNING: Target pattern parsing failed.\n ERROR: no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)\n </denchmark-code>\n \n The error seems to be caused by the symlinks for Rllib. Is this an issue?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "annaluo676", "commentT": "2020-04-16T00:18:45Z", "comment_text": "\n \t\tI think the right way is pip install -e . In the python directory, not\n calling that script.\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Wed, Apr 15, 2020, 5:17 PM Anna Luo ***@***.***> wrote:\n  Thanks @ericl <https://github.com/ericl>. According to\n  https://ray.readthedocs.io/en/latest/development.html#compilation I will\n  need to recompile the c++ code changes. bash build.sh emitted the\n  following error:\n \n  INFO: Writing tracer profile to '/home/ubuntu/.cache/bazel/_bazel_ubuntu/4694ec92a350130755ebd31627e14086/command.profile.gz'\n  ERROR: Skipping '//:ray_pkg': no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)\n  WARNING: Target pattern parsing failed.\n  ERROR: no such package '': error globbing [python/ray/*.py, python/ray/autoscaler/*.py, python/ray/autoscaler/aws/example-full.yaml, python/ray/autoscaler/azure/example-full.yaml, python/ray/autoscaler/gcp/example-full.yaml, python/ray/autoscaler/local/example-full.yaml, python/ray/cloudpickle/*.py, python/ray/core/__init__.py, python/ray/core/generated/__init__.py, python/ray/core/generated/ray/__init__.py, python/ray/core/generated/ray/protocol/__init__.py, python/ray/dashboard/*.py, python/ray/dashboard/metrics_exporter/*.py, python/ray/experimental/*.py, python/ray/util/*.py, python/ray/internal/*.py, python/ray/projects/*.py, python/ray/projects/schema.json, python/ray/projects/templates/cluster_template.yaml, python/ray/projects/templates/project_template.yaml, python/ray/projects/templates/requirements.txt, python/ray/workers/default_worker.py]: /efs/annaluo/ray/python/ray/autoscaler (Too many levels of symbolic links)\n \n  The error seems to be caused by the symlinks for Rllib. Is this an issue?\n \n  \u2014\n  You are receiving this because you were mentioned.\n  Reply to this email directly, view it on GitHub\n  <#7976 (comment)>,\n  or unsubscribe\n  <https://github.com/notifications/unsubscribe-auth/AAADUSVW6CZU7WZD3NAXOLLRMZFAPANCNFSM4MF3TSCQ>\n  .\n \n \n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "annaluo676", "commentT": "2020-04-17T19:23:03Z", "comment_text": "\n \t\tUnfortunately I was not able to recompile the whole package.  Since the the changes were merged, I can test with the latest wheel. Thanks Eric for looking into this!\n \t\t"}}}, "commit": {"commit_id": "55ce2bba1099df23fda959f88e049085eaf9c0dd", "commit_author": "Eric Liang", "commitT": "2020-04-16 13:16:40-07:00", "commit_complexity": {"commit_NLOC": "0.6666666666666666", "commit_CCN": "1.0", "commit_Nprams": "0.6666666666666666"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\core_worker\\core_worker.cc", "file_new_name": "src\\ray\\core_worker\\core_worker.cc", "file_complexity": {"file_NLOC": "1495", "file_CCN": "255", "file_NToken": "11767"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1811", "deleted_lines": null, "method_info": {"method_name": "ray::CoreWorker::HandleGetCoreWorkerStats", "method_params": "request,reply,send_reply_callback", "method_startline": "1782", "method_endline": "1821", "method_complexity": {"method_NLOC": "35", "method_CCN": "4", "method_NToken": "302", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\core_worker\\store_provider\\memory_store\\memory_store.cc", "file_new_name": "src\\ray\\core_worker\\store_provider\\memory_store\\memory_store.cc", "file_complexity": {"file_NLOC": "360", "file_CCN": "87", "file_NToken": "2552"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "458,459,460", "deleted_lines": "458", "method_info": {"method_name": "ray::CoreWorkerMemoryStore::GetMemoryStoreStatisticalData", "method_params": "", "method_startline": "454", "method_endline": "466", "method_complexity": {"method_NLOC": "13", "method_CCN": "3", "method_NToken": "72", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\ray\\core_worker\\store_provider\\memory_store\\memory_store.h", "file_new_name": "src\\ray\\core_worker\\store_provider\\memory_store\\memory_store.h", "file_complexity": {"file_NLOC": "67", "file_CCN": "2", "file_NToken": "557"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "16", "deleted_lines": null}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\ray\\protobuf\\common.proto", "file_new_name": "src\\ray\\protobuf\\common.proto", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "272,273,275,277,279,281,283,285", "deleted_lines": "273,275,277,279,281,283"}}}}}}