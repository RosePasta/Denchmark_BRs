{"BR": {"BR_id": "12255", "BR_author": "PidgeyBE", "BRopenT": "2020-11-23T08:12:44Z", "BRcloseT": "2020-11-24T17:13:16Z", "BR_text": {"BRsummary": "[autoscaler/k8s] Autoscaler does not recover from interruption of k8s API", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n \n ray==1.0.0\n kubernetes autoscaling\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n In for example Google Cloud, it is not uncommon for the kubernetes master to be rebooted (e.g. when node-pools are added/removed, master is upgraded, etc..).\n However, in this case, the k8s API used by ray autoscale is temporarily unavailable and the autoscale monitor crashes, without recovering. The head node is kept, to keep log files, but since the monitor stopped, autoscaling is not possible anymore.\n In a production environment this is unacceptable behavior. Either the monitor should restart itself after failure (preferred) or the head node should also kill itself to keep the autoscaling cluster state consistent. In the latter case, the software using the ray cluster could restart the ray cluster itself.\n Issue can be reproduced by e.g. killing a kubernetes master node.\n Example logs:\n <denchmark-link:https://github.com/ray-project/ray/files/5581604/monitor.out.txt>monitor.out.txt</denchmark-link>\n \n <denchmark-link:https://github.com/ray-project/ray/files/5581606/monitor.errr.txt>monitor.errr.txt</denchmark-link>\n \n In this case the issue occured in . The same could happen when starting a new worker pod though, I'm not sure what will happen then...\n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "PidgeyBE", "commentT": "2020-11-23T23:41:47Z", "comment_text": "\n \t\t^ PR above looks good.\n Also <denchmark-link:https://github.com/PidgeyBE>@PidgeyBE</denchmark-link>\n  I think you can set AUTOSCALER_MAX_NUM_FAILURES=99999 to achieve a similar effect:\n \n \n \t\t"}}}, "commit": {"commit_id": "e66ddab190455850f824650e028ac1f861a6ef92", "commit_author": "Gekho457", "commitT": "2020-11-24 11:13:15-06:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\autoscaler\\_private\\autoscaler.py", "file_new_name": "python\\ray\\autoscaler\\_private\\autoscaler.py", "file_complexity": {"file_NLOC": "509", "file_CCN": "89", "file_NToken": "3110"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "134,135,136,137,138,139,140", "deleted_lines": "133", "method_info": {"method_name": "update", "method_params": "self", "method_startline": "124", "method_endline": "144", "method_complexity": {"method_NLOC": "19", "method_CCN": "6", "method_NToken": "99", "method_nesting_level": "1"}}}}}}}}