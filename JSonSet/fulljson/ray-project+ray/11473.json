{"BR": {"BR_id": "11473", "BR_author": "mvindiola1", "BRopenT": "2020-10-19T19:16:29Z", "BRcloseT": "2020-10-28T21:23:07Z", "BR_text": {"BRsummary": "[RLLIB] remote_worker environments not being closed", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version 1.0.0\n Python 3.7.7\n Tensorflow 2.2\n I have wrapped a custom environment in an rllib MultiAgentEnv. This environment requires that I post an http request when the environment is closed in order to shut down remote resources. If this request takes too long the remote resources are never freed.\n I have attached a sample script adapted from ray/rllib/examples/centralized_critic_2.py.\n I also adapted TwoStepGame to add a close method and introduce a 5 second sleep to simulate a slow closing environment. In this example there are 2 workers and 4 envs_per_worker\n I have attached two example: 1. Where the sleep is 0 and all of the environments are closed correctly and 2. Where the sleep is 5 seconds and only the environments in the local_worker are closed and none of the remote_worker envs are closed.\n The source of the problem comes from here:\n \n \n \n ray/rllib/evaluation/worker_set.py\n \n \n         Lines 145 to 150\n       in\n       acbd12e\n \n \n \n \n \n \n  def stop(self) -> None: \n \n \n \n  \"\"\"Stop all rollout workers.\"\"\" \n \n \n \n  self.local_worker().stop() \n \n \n \n  for w in self.remote_workers(): \n \n \n \n  w.stop.remote() \n \n \n \n  w.__ray_terminate__.remote() \n \n \n \n \n \n I have fixed this locally like this:\n     def stop(self) -> None:\n         \"\"\"Stop all rollout workers.\"\"\"\n         self.local_worker().stop()\n         fhandles = [w.stop.remote() for w in self.remote_workers()]\n         ray.get(fhandles)\n         for w in self.remote_workers():\n             w.__ray_terminate__.remote()\n In this example with time.sleep(0) you will see that all the envs are closed and all of the counter decrement calls complete.\n <denchmark-link:https://github.com/ray-project/ray/files/5404171/remote_worker_env_close_sleep0.txt>remote_worker_env_close_sleep0.txt</denchmark-link>\n \n In this example with time.sleep(5) you will see that only the first 4 envs are closed (on the local_worker) and after tune has finished there are 8 calls to counter.decrement that do not occur.\n <denchmark-link:https://github.com/ray-project/ray/files/5404172/remote_worker_env_close_sleep20.txt>remote_worker_env_close_sleep20.txt</denchmark-link>\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n <denchmark-code>from gym.spaces import Dict, Discrete\n import argparse\n import ray\n import time\n \n from ray import tune\n from ray.rllib.examples.models.centralized_critic_models import YetAnotherCentralizedCriticModel\n from ray.rllib.examples.env.two_step_game import TwoStepGame\n from ray.rllib.examples.centralized_critic_2 import FillInActions,central_critic_observer\n from ray.rllib.models import ModelCatalog\n \n \n parser = argparse.ArgumentParser()\n parser.add_argument(\"--torch\", action=\"store_true\")\n parser.add_argument(\"--as-test\", action=\"store_true\")\n parser.add_argument(\"--stop-iters\", type=int, default=1)\n parser.add_argument(\"--stop-timesteps\", type=int, default=1)\n parser.add_argument(\"--stop-reward\", type=float, default=7.99)\n \n \n if __name__ == \"__main__\":\n     args = parser.parse_args()\n \n     ray.init()\n \n     @ray.remote\n     class Counter(object):\n         def __init__(self):\n             self.value = 0\n \n         def increment(self):\n             self.value += 1\n             return self.value\n \n         def decrement(self):\n             self.value -= 1\n             return self.value\n \n         def get_counter(self):\n             return self.value\n \n     counter = Counter.remote()\n \n     class TwoStepGameSlow(TwoStepGame):\n \n         def __init__(self, env_config):\n             num = ray.get(counter.increment.remote())\n             print(\"Creating Game: \", num)\n             return super().__init__(env_config)\n \n         def close(self):\n             num = ray.get(counter.decrement.remote())\n             print(\"Closing Game, Remaining: \", num)\n             time.sleep(5)\n \n     ModelCatalog.register_custom_model(\n         \"cc_model\",  YetAnotherCentralizedCriticModel)\n \n     action_space = Discrete(2)\n     observer_space = Dict({\n         \"own_obs\": Discrete(6),\n         # These two fields are filled in by the CentralCriticObserver, and are\n         # not used for inference, only for training.\n         \"opponent_obs\": Discrete(6),\n         \"opponent_action\": Discrete(2),\n     })\n \n     config = {\n         \"env\": TwoStepGameSlow,\n         \"batch_mode\": \"complete_episodes\",\n         \"callbacks\": FillInActions,\n         \"num_workers\": 2,\n         \"num_envs_per_worker\": 4,\n         \"multiagent\": {\n             \"policies\": {\n                 \"pol1\": (None, observer_space, action_space, {}),\n                 \"pol2\": (None, observer_space, action_space, {}),\n             },\n             \"policy_mapping_fn\": lambda x: \"pol1\" if x == 0 else \"pol2\",\n             \"observation_fn\": central_critic_observer,\n         },\n         \"model\": {\n             \"custom_model\": \"cc_model\",\n         },\n     }\n \n     stop = {\n         \"training_iteration\": args.stop_iters,\n     }\n \n     results = tune.run(\"PPO\", config=config, stop=stop, verbose=0)\n     print(\"TUNE DONE\")\n     print(\"Unclosed Games: \", ray.get(counter.get_counter.remote()))\n     ray.shutdown()\n </denchmark-code>\n \n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {}}, "commit": {"commit_id": "9e68b77796c1478973f5236adfc09f3399721659", "commit_author": "mvindiola1", "commitT": "2020-10-28 14:23:06-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\evaluation\\worker_set.py", "file_new_name": "rllib\\evaluation\\worker_set.py", "file_complexity": {"file_NLOC": "297", "file_CCN": "26", "file_NToken": "1839"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "147,148,149,150,151,152,153,154,155", "deleted_lines": "147,148,149,150", "method_info": {"method_name": "stop", "method_params": "self", "method_startline": "145", "method_endline": "155", "method_complexity": {"method_NLOC": "11", "method_CCN": "5", "method_NToken": "71", "method_nesting_level": "1"}}}}}}}}