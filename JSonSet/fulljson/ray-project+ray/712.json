{"BR": {"BR_id": "712", "BR_author": "robertnishihara", "BRopenT": "2017-07-05T22:13:06Z", "BRcloseT": "2017-07-18T06:26:40Z", "BR_text": {"BRsummary": "Test failure in jenkins tests \"psutil.NoSuchProcess process no longer exists\".", "BRdescription": "\n I sometimes see errors like the following in the jenkins tests.\n Traceback (most recent call last):\n   File \"/ray/test/jenkins_tests/multi_node_tests/remove_driver_test.py\", line 264, in <module>\n     cleanup_driver(redis_address, driver_index)\n   File \"/ray/test/jenkins_tests/multi_node_tests/remove_driver_test.py\", line 225, in cleanup_driver\n     wait_for_pid_to_exit(pid)\n   File \"/opt/conda/lib/python2.7/site-packages/ray-0.1.2-py2.7-linux-x86_64.egg/ray/test/test_utils.py\", line 130, in wait_for_pid_to_exit\n     if not _pid_alive(pid):\n   File \"/opt/conda/lib/python2.7/site-packages/ray-0.1.2-py2.7-linux-x86_64.egg/ray/test/test_utils.py\", line 121, in _pid_alive\n     if psutil.Process(pid).status() == psutil.STATUS_ZOMBIE:\n   File \"/opt/conda/lib/python2.7/site-packages/psutil/__init__.py\", line 627, in status\n     return self._proc.status()\n   File \"/opt/conda/lib/python2.7/site-packages/psutil/_pslinux.py\", line 968, in wrapper\n     raise NoSuchProcess(self.pid, self._name)\n psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=22)\n The error can be seen in the following log file <denchmark-link:https://amplab.cs.berkeley.edu/jenkins/job/Ray-PRB/1164/console>https://amplab.cs.berkeley.edu/jenkins/job/Ray-PRB/1164/console</denchmark-link>\n .\n The following error from <denchmark-link:https://amplab.cs.berkeley.edu/jenkins/job/Ray-PRB/1185/console>https://amplab.cs.berkeley.edu/jenkins/job/Ray-PRB/1185/console</denchmark-link>\n  is possibly related.\n Traceback (most recent call last):\n   File \"/opt/conda/lib/python2.7/site-packages/ray-0.1.2-py2.7-linux-x86_64.egg/ray/actor.py\", line 82, in fetch_and_register_actor\n     unpickled_class = pickle.loads(pickled_class)\n   File \"/opt/conda/lib/python2.7/pickle.py\", line 1388, in loads\n     return Unpickler(file).load()\n   File \"/opt/conda/lib/python2.7/pickle.py\", line 864, in load\n     dispatch[key](self)\n   File \"/opt/conda/lib/python2.7/pickle.py\", line 1139, in load_reduce\n     value = func(*args)\n   File \"/opt/conda/lib/python2.7/site-packages/cloudpickle/cloudpickle.py\", line 840, in subimport\n     __import__(name)\n   File \"/opt/conda/lib/python2.7/site-packages/gym/__init__.py\", line 48, in <module>\n     sanity_check_dependencies()\n   File \"/opt/conda/lib/python2.7/site-packages/gym/__init__.py\", line 17, in sanity_check_dependencies\n     import requests\n   File \"/opt/conda/lib/python2.7/site-packages/requests/__init__.py\", line 63, in <module>\n     from . import utils\n   File \"/opt/conda/lib/python2.7/site-packages/requests/utils.py\", line 42, in <module>\n     if platform.system() == 'Windows':\n   File \"/opt/conda/lib/python2.7/platform.py\", line 1263, in system\n     return uname()[0]\n   File \"/opt/conda/lib/python2.7/platform.py\", line 1230, in uname\n     processor = _syscmd_uname('-p','')\n   File \"/opt/conda/lib/python2.7/platform.py\", line 965, in _syscmd_uname\n     rc = f.close()\n IOError: [Errno 10] No child processes\n The issue may be related to the fact that workers are created by forking the local scheduler, and the local scheduler changes the  handler. Signal handlers shouldn't survive a call to , but <denchmark-link:https://stackoverflow.com/questions/2333637/is-it-possible-to-signal-handler-to-survive-after-exec>https://stackoverflow.com/questions/2333637/is-it-possible-to-signal-handler-to-survive-after-exec</denchmark-link>\n  makes it seem like  may be an exception.\n The connection between the error message and the  handler is suggested by <denchmark-link:https://github.com/tornadoweb/tornado/issues/1160>tornadoweb/tornado#1160</denchmark-link>\n .\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "robertnishihara", "commentT": "2017-07-18T06:26:40Z", "comment_text": "\n \t\tAfter experimenting with the issue in <denchmark-link:https://github.com/ray-project/ray/issues/745>#745</denchmark-link>\n  (which is fixed by <denchmark-link:https://github.com/ray-project/ray/pull/713>#713</denchmark-link>\n ), I think this <denchmark-link:https://github.com/ray-project/ray/pull/713>#713</denchmark-link>\n  really does address this issue, so I'm closing the issue.\n \t\t"}}}, "commit": {"commit_id": "6c45657280f40c36826e456ecfff358519ed81dd", "commit_author": "Robert Nishihara", "commitT": "2017-07-07 14:50:37+00:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\local_scheduler\\local_scheduler.cc", "file_new_name": "src\\local_scheduler\\local_scheduler.cc", "file_complexity": {"file_NLOC": "955", "file_CCN": "165", "file_NToken": "6096"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "223,224,225", "deleted_lines": null, "method_info": {"method_name": "start_worker", "method_params": "state,actor_id", "method_startline": "209", "method_endline": "246", "method_complexity": {"method_NLOC": "31", "method_CCN": "5", "method_NToken": "226", "method_nesting_level": "0"}}}}}}}}