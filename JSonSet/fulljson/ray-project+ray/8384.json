{"BR": {"BR_id": "8384", "BR_author": "ManuelZierl", "BRopenT": "2020-05-09T10:45:52Z", "BRcloseT": "2020-07-17T10:14:35Z", "BR_text": {"BRsummary": "[rllib] QMIX doesn't learn anything", "BRdescription": "\n I've alread posted this question on stackoverflow but since i didn't got an answer there i will repost it here (<denchmark-link:https://stackoverflow.com/questions/61523164/ray-rllib-qmix-doesnt-learn-anything>https://stackoverflow.com/questions/61523164/ray-rllib-qmix-doesnt-learn-anything</denchmark-link>\n )\n I wanted to try out the QMIX implementation of Ray/Rllib library but there must be something wrong of how I'm using it because it doesn't seem to learn anything. Since I'm new to Ray/Rllib I started with the \"TwoStepGame\" example the libary provides as an example on there github repo (<denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/examples/twostep_game.py>https://github.com/ray-project/ray/blob/master/rllib/examples/twostep_game.py</denchmark-link>\n ), trying to understand how to use it. Since for the start this example was a little bit to complex for me I adjusted it to make a example that is as simple as possible. Problem: Qmix doesn't seem to learn, means the resulting reward pretty much matches the expected value of a random policy.\n Let me explain the idea of my very simple experiment. We have 2 agents. Every agent can make 3 actions (Discrete(3)). If he makes the action 0 he gets a reward of 0.5 if not 0. So this should be a very simple task, since the best policy is just taking action 0.\n Here is my implementation:\n <denchmark-code>from gym.spaces import Tuple, MultiDiscrete, Dict, Discrete\n import numpy as np\n \n import ray\n from ray import tune\n from ray.tune import register_env, grid_search\n from ray.rllib.env.multi_agent_env import MultiAgentEnv\n from ray.rllib.agents.qmix.qmix_policy import ENV_STATE\n \n \n class TwoStepGame(MultiAgentEnv):\n     action_space = Discrete(3)\n \n     def __init__(self, env_config):\n         self.counter = 0\n \n     def reset(self):\n         return {0: {'obs': np.array([0]), 'state': np.array([0])},\n                 1: {'obs': np.array([0]), 'state': np.array([0])}}\n \n     def step(self, action_dict):\n         self.counter += 1\n         move1 = action_dict[0]\n         move2 = action_dict[1]\n         reward_1 = 0\n         reward_2 = 0\n         if move1 == 0:\n             reward_1 = 0.5\n         if move2 == 0:\n             reward_2 = 0.5\n \n         obs = {0: {'obs': np.array([0]), 'state': np.array([0])},\n                1: {'obs': np.array([0]), 'state': np.array([0])}}\n         done = False\n         if self.counter > 100:\n             self.counter = 0\n             done = True\n \n         return obs, {0: reward_1, 1: reward_2}, {\"__all__\": done}, {}\n \n \n if __name__ == \"__main__\":\n \n     grouping = {\"group_1\": [0, 1]}\n \n     obs_space = Tuple([\n         Dict({\n             \"obs\": MultiDiscrete([2]),\n             ENV_STATE: MultiDiscrete([3])\n         }),\n         Dict({\n             \"obs\": MultiDiscrete([2]),\n             ENV_STATE: MultiDiscrete([3])\n         }),\n     ])\n \n     act_space = Tuple([\n         TwoStepGame.action_space,\n         TwoStepGame.action_space,\n     ])\n \n     register_env(\"grouped_twostep\",\n         lambda config: TwoStepGame(config).with_agent_groups(\n             grouping, obs_space=obs_space, act_space=act_space))\n \n     config = {\n         \"mixer\": grid_search([\"qmix\"]),\n         \"env_config\": {\n             \"separate_state_space\": True,\n             \"one_hot_state_encoding\": True\n         },\n     }\n \n     ray.init(num_cpus=1)\n     tune.run(\n         \"QMIX\",\n         stop={\n             \"timesteps_total\": 100000,\n         },\n         config=dict(config, **{\n             \"env\": \"grouped_twostep\",\n         }),\n     )\n </denchmark-code>\n \n and here is the result of the output when I run it for 100 000 timesteps\n <denchmark-code>+----------------------------+------------+-------+---------+--------+------------------+--------+----------+\n | Trial name                 | status     | loc   | mixer   |   iter |   total time (s) |     ts |   reward |\n |----------------------------+------------+-------+---------+--------+------------------+--------+----------|\n | QMIX_grouped_twostep_00000 | TERMINATED |       | qmix    |    100 |          276.796 | 101000 |   33.505 |\n +----------------------------+------------+-------+---------+--------+------------------+--------+----------+\n \n \n \n Process finished with exit code 0\n </denchmark-code>\n \n As you can see the policy seems to be random since the expected value is 1/3 and the resulting reward is 33.505 (because I reset the enviroment every 100 timesteps).\n My Question: What do i not understand? There must be something wrong with my configuration or maybe my understanding of how rllib works. But since the best policy is very very simpel (just always take action 0) it seems to me like this algorithm cannot learn.\n \n \n \n software\n version\n \n \n \n \n ray\n 0.8.4\n \n \n python\n 3.6.9\n \n \n tensorflow\n 1.14.0\n \n \n OS\n Ubuntu (running in a VM on a Windows OS) Release 18.04\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ManuelZierl", "commentT": "2020-06-02T10:03:26Z", "comment_text": "\n \t\tOn StarCraft,  QMix from RLLib also seems to learn random policy even train many episodes, it still learns a random policy.  <denchmark-link:https://github.com/oxwhirl/smac/issues/42>oxwhirl/smac#42</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ManuelZierl", "commentT": "2020-07-16T18:48:59Z", "comment_text": "\n \t\tTaking a look at this now ...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ManuelZierl", "commentT": "2020-07-16T19:00:33Z", "comment_text": "\n \t\tThe exploration's epsilon is never reduced, leading to always acting randomly (epsilon is always 1.0).\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ManuelZierl", "commentT": "2020-07-16T20:38:48Z", "comment_text": "\n \t\tThis PR fixes the issue. QMIX is now also using our exploration API (it wasn't before, which is why it broke with the introduction of that API in 0.8.4). It defaults to  (same as DQN).\n Also, a test case has been added to confirm simple learning capabilities on the above TwoStepGame.\n <denchmark-link:https://github.com/ray-project/ray/pull/9527>#9527</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ManuelZierl", "commentT": "2020-07-16T20:39:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ManuelZierl>@ManuelZierl</denchmark-link>\n  Thanks for filing this! The fix should be merged tomorrow or over the WE.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ManuelZierl", "commentT": "2020-07-16T20:40:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/GoingMyWay>@GoingMyWay</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ManuelZierl", "commentT": "2020-07-16T21:47:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n  Great, I will try it to see if it can work on different SC scenarios.\n \t\t"}}}, "commit": {"commit_id": "78dfed268391fb5ea927a5094d9c171b9da8c052", "commit_author": "Sven Mika", "commitT": "2020-07-17 12:14:34+02:00", "commit_complexity": {"commit_NLOC": "0.03571428571428571", "commit_CCN": "0.14285714285714285", "commit_Nprams": "0.8571428571428571"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\rllib-algorithms.rst", "file_new_name": "doc\\source\\rllib-algorithms.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "479,481,499", "deleted_lines": "479,481,499"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\rllib-examples.rst", "file_new_name": "doc\\source\\rllib-examples.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "83", "deleted_lines": "83"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\BUILD", "file_new_name": "rllib\\BUILD", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1946,1947,1950,1955,1956,1959,1964,1965,1968,1973,1974,1977,1978", "deleted_lines": "1946,1947,1950,1955,1956,1959,1964,1965,1968,1973,1974,1977,1978"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\dqn\\dqn.py", "file_new_name": "rllib\\agents\\dqn\\dqn.py", "file_complexity": {"file_NLOC": "223", "file_CCN": "32", "file_NToken": "1312"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "43", "deleted_lines": "43"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\dqn\\dqn_torch_policy.py", "file_new_name": "rllib\\agents\\dqn\\dqn_torch_policy.py", "file_complexity": {"file_NLOC": "208", "file_CCN": "19", "file_NToken": "1438"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "172,173,174,190,191,192,193", "deleted_lines": "172,185,189", "method_info": {"method_name": "build_q_losses", "method_params": "policy,model,_,train_batch", "method_startline": "151", "method_endline": "202", "method_complexity": {"method_NLOC": "42", "method_CCN": "2", "method_NToken": "295", "method_nesting_level": "0"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\qmix\\qmix.py", "file_new_name": "rllib\\agents\\qmix\\qmix.py", "file_complexity": {"file_NLOC": "94", "file_CCN": "7", "file_NToken": "537"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136", "deleted_lines": null, "method_info": {"method_name": "validate_config", "method_params": "config", "method_startline": "113", "method_endline": "136", "method_complexity": {"method_NLOC": "24", "method_CCN": "6", "method_NToken": "130", "method_nesting_level": "0"}}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\qmix\\qmix_policy.py", "file_new_name": "rllib\\agents\\qmix\\qmix_policy.py", "file_complexity": {"file_NLOC": "434", "file_CCN": "64", "file_NToken": "3435"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "261", "deleted_lines": null, "method_info": {"method_name": "compute_actions", "method_params": "self,obs_batch,state_batches,prev_action_batch,prev_reward_batch,info_batch,episodes,explore,timestep,kwargs", "method_startline": "253", "method_endline": "262", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "38", "method_nesting_level": "1"}}}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\qmix\\tests\\test_qmix.py", "file_new_name": "rllib\\agents\\qmix\\tests\\test_qmix.py", "file_complexity": {"file_NLOC": "77", "file_CCN": "6", "file_NToken": "462"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "56", "deleted_lines": "56"}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\contrib\\maddpg\\maddpg.py", "file_new_name": "rllib\\contrib\\maddpg\\maddpg.py", "file_complexity": {"file_NLOC": "88", "file_CCN": "11", "file_NToken": "540"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8,9", "deleted_lines": "8,9"}}}, "file_9": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\examples\\centralized_critic.py", "file_new_name": "rllib\\examples\\centralized_critic.py", "file_complexity": {"file_NLOC": "203", "file_CCN": "10", "file_NToken": "1282"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "6,7,8,9,10", "deleted_lines": "6,7,8,9"}}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\examples\\models\\parametric_actions_model.py", "file_new_name": "rllib\\examples\\models\\parametric_actions_model.py", "file_complexity": {"file_NLOC": "74", "file_CCN": "6", "file_NToken": "502"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101,105", "deleted_lines": "102,106", "method_info": {"method_name": "forward", "method_params": "self,input_dict,state,seq_lens", "method_startline": "84", "method_endline": "106", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "103", "method_nesting_level": "1"}}}}}, "file_11": {"file_change_type": "RENAME", "file_Nmethod": 0, "file_old_name": "rllib\\examples\\twostep_game.py", "file_new_name": "rllib\\examples\\two_step_game.py", "file_complexity": {"file_NLOC": "107", "file_CCN": "0", "file_NToken": "601"}}, "file_12": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\utils\\exploration\\epsilon_greedy.py", "file_new_name": "rllib\\utils\\exploration\\epsilon_greedy.py", "file_complexity": {"file_NLOC": "115", "file_CCN": "10", "file_NToken": "781"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "142", "deleted_lines": "143", "method_info": {"method_name": "_get_torch_exploration_action", "method_params": "self,q_values,explore,timestep", "method_startline": "121", "method_endline": "156", "method_complexity": {"method_NLOC": "19", "method_CCN": "2", "method_NToken": "154", "method_nesting_level": "1"}}}}}}}}