{"BR": {"BR_id": "6751", "BR_author": "kfstorm", "BRopenT": "2020-01-08T15:17:13Z", "BRcloseT": "2020-01-13T17:38:01Z", "BR_text": {"BRsummary": "`__ray_kill__` does not work as expected", "BRdescription": "\n Related PR: <denchmark-link:https://github.com/ray-project/ray/pull/6523>#6523</denchmark-link>\n \n <denchmark-h:h2>Analysis</denchmark-h>\n \n In the test case test_kill, the specified timeout value of ray.wait is too small, so it's very likely that the actor is not created yet when reached the timeout. So the remote call to hang is still pending to be pushed and the assertion assert len(ready) == 0 can be passed.\n \n \n \n ray/python/ray/tests/test_actor.py\n \n \n         Lines 1423 to 1429\n       in\n       0a5d010\n \n \n \n \n \n \n  actor = Actor.remote() \n \n \n \n  result = actor.hang.remote() \n \n \n \n  ready, _ = ray.wait([result], timeout=0.1) \n \n \n \n  assert len(ready) == 0 \n \n \n \n  actor.__ray_kill__() \n \n \n \n  with pytest.raises(ray.exceptions.RayActorError): \n \n \n \n  ray.get(result) \n \n \n \n \n \n Then the __ray_kill__ operation is initiated, the kill operation is first cached at local. After the actor is alive, the kill operation is pushed to the remote actor prior to the previous remote task, because in CoreWorkerDirectActorTaskSubmitter::SendPendingTasks, the KillActor messages are always sent before any actor tasks.\n \n \n \n ray/src/ray/core_worker/transport/direct_actor_transport.cc\n \n \n         Lines 125 to 144\n       in\n       0a5d010\n \n \n \n \n \n \n  // Check if there is a pending force kill. If there is, send it and disconnect the \n \n \n \n  // client. \n \n \n \n  if (pending_force_kills_.find(actor_id) != pending_force_kills_.end()) { \n \n \n \n    rpc::KillActorRequest request; \n \n \n \n    request.set_intended_actor_id(actor_id.Binary()); \n \n \n \n  RAY_CHECK_OK(client->KillActor(request, nullptr)); \n \n \n \n    pending_force_kills_.erase(actor_id); \n \n \n \n  } \n \n \n \n  \n \n \n \n  // Submit all pending requests. \n \n \n \n  auto &requests = pending_requests_[actor_id]; \n \n \n \n  auto head = requests.begin(); \n \n \n \n  while (head != requests.end() && head->first == next_send_position_[actor_id]) { \n \n \n \n  auto request = std::move(head->second); \n \n \n \n    head = requests.erase(head); \n \n \n \n  \n \n \n \n  auto num_returns = request->task_spec().num_returns(); \n \n \n \n  auto task_id = TaskID::FromBinary(request->task_spec().task_id()); \n \n \n \n  PushActorTask(*client, std::move(request), actor_id, task_id, num_returns); \n \n \n \n  } \n \n \n \n \n \n At the remote side, the worker receives the KillActor message first and just invokes the CoreWorker::Shutdown, and then both the io_service_ and the task_execution_service_ are stopped. The hang function never get the chance to run.\n \n \n \n ray/src/ray/core_worker/core_worker.cc\n \n \n         Lines 1156 to 1157\n       in\n       0a5d010\n \n \n \n \n \n \n  RAY_LOG(INFO) << \"Got KillActor, shutting down...\"; \n \n \n \n  Shutdown(); \n \n \n \n \n \n \n \n \n ray/src/ray/core_worker/core_worker.cc\n \n \n         Lines 259 to 264\n       in\n       0a5d010\n \n \n \n \n \n \n  void CoreWorker::Shutdown() { \n \n \n \n    io_service_.stop(); \n \n \n \n  if (worker_type_ == WorkerType::WORKER) { \n \n \n \n      task_execution_service_.stop(); \n \n \n \n    } \n \n \n \n  } \n \n \n \n \n \n Eventually, the test case will pass but it can't verify that the __ray_kill__ works.\n <denchmark-h:h2>Test case update</denchmark-h>\n \n If we change the timeout value to 5, the test case will fail with below message. That's because ray.get(ray.ObjectID.from_random()) actually throws an UnreconstructableError, not Never returns as the comment says.\n >       assert len(ready) == 0\n E       assert 1 == 0\n E         -1\n E         +0\n \n python/ray/tests/test_actor.py:1426: AssertionError\n Then we change the implementation of hang to an infinite loop with time.sleep(1), now the test case hangs forever as I expected, but the worker still won't quit. I think it's because the hang function is still running and task_execution_service_.stop() won't abort the execution of hang.\n Final diff:\n diff --git a/python/ray/tests/test_actor.py b/python/ray/tests/test_actor.py\n index 9c43c65fc..98956b297 100644\n --- a/python/ray/tests/test_actor.py\n +++ b/python/ray/tests/test_actor.py\n @@ -1418,11 +1418,14 @@ def test_kill(ray_start_regular):\n      class Actor:\n          def hang(self):\n              # Never returns.\n -            ray.get(ray.ObjectID.from_random())\n +            while True:\n +                time.sleep(1)\n  \n      actor = Actor.remote()\n      result = actor.hang.remote()\n -    ready, _ = ray.wait([result], timeout=0.1)\n +    ready, _ = ray.wait([result], timeout=5)\n +    if len(ready) > 0:\n +        print(ray.get(result))\n      assert len(ready) == 0\n      actor.__ray_kill__()\n      with pytest.raises(ray.exceptions.RayActorError):\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kfstorm", "commentT": "2020-01-08T17:25:59Z", "comment_text": "\n \t\tProbably kill should have an option to immediately quit the actor instead of doing clean shutdown?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kfstorm", "commentT": "2020-01-08T23:01:31Z", "comment_text": "\n \t\tThanks for reporting this <denchmark-link:https://github.com/kfstorm>@kfstorm</denchmark-link>\n . Seems like the simplest solution is to just called  in the kill handler, though that would mean we no longer perform normal cleanup. I think that's acceptable given that this is a \"last resort\" force kill though. Thoughts?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kfstorm", "commentT": "2020-01-09T00:46:07Z", "comment_text": "\n \t\tSince there\u2019s no graceful way to abort the executing task, I totally agree that we can just exit the process.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "kfstorm", "commentT": "2020-01-10T01:01:10Z", "comment_text": "\n \t\tPlease see this PR: <denchmark-link:https://github.com/ray-project/ray/pull/6760>#6760</denchmark-link>\n \n By the way, thanks for the detailed bug report!\n \t\t"}}}, "commit": {"commit_id": "a950e95c7d38df32311da90b20208e2d35b7aa08", "commit_author": "Edward Oakes", "commitT": "2020-01-13 11:37:59-06:00", "commit_complexity": {"commit_NLOC": "0.25", "commit_CCN": "1.0", "commit_Nprams": "0.25"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\tests\\test_actor.py", "file_new_name": "python\\ray\\tests\\test_actor.py", "file_complexity": {"file_NLOC": "995", "file_CCN": "262", "file_NToken": "7533"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1416,1417,1421", "deleted_lines": "1416,1417,1421", "method_info": {"method_name": "test_kill", "method_params": "ray_start_regular", "method_startline": "1412", "method_endline": "1425", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "78", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "1416,1417", "deleted_lines": "1416,1417", "method_info": {"method_name": "test_kill.hang", "method_params": "self", "method_startline": "1415", "method_endline": "1417", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "14", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\core_worker\\core_worker.cc", "file_new_name": "src\\ray\\core_worker\\core_worker.cc", "file_complexity": {"file_NLOC": "983", "file_CCN": "164", "file_NToken": "7900"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1156,1157,1158,1159,1160", "deleted_lines": "1156,1157", "method_info": {"method_name": "ray::CoreWorker::HandleKillActor", "method_params": "request,reply,send_reply_callback", "method_startline": "1142", "method_endline": "1161", "method_complexity": {"method_NLOC": "20", "method_CCN": "3", "method_NToken": "133", "method_nesting_level": "1"}}}}}}}}