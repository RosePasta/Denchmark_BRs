{"BR": {"BR_id": "7073", "BR_author": "matter-funds", "BRopenT": "2020-02-06T16:49:01Z", "BRcloseT": "2020-11-25T22:09:54Z", "BR_text": {"BRsummary": "Using max_calls=1 with high memory usage functions causes OOM issues", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Invoking ray.remote with max_calls=1 and functions which require large amounts of memory causes workers to crash with OOM issues. Using max_calls=None causes no issues.\n Ray throws the following error:\n <denchmark-code>020-02-06 16:40:08,621  ERROR worker.py:1003 -- Possible unhandled error from worker: ray::IDLE (pid=4978, ip=172.31.38.182)\n   File \"python/ray/_raylet.pyx\", line 631, in ray._raylet.execute_task\n   File \"/home/ubuntu/project/env/lib/python3.6/site-packages/ray/memory_monitor.py\", line 126, in raise_if_low_memory\n     self.error_threshold))\n ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node ip-172-31-38-182 is used (3.56 / 3.62 GB). The top 10 memory consumers are:\n \n PID     MEM     COMMAND\n 4977    0.33GiB ray::__main__.foo()\n 4125    0.13GiB ray::IDLE\n 4062    0.13GiB ray::IDLE\n 4335    0.13GiB ray::IDLE\n 4315    0.13GiB ray::IDLE\n 4229    0.13GiB ray::IDLE\n 4206    0.13GiB ray::IDLE\n 4175    0.13GiB ray::IDLE\n 4280    0.13GiB ray::IDLE\n 4436    0.13GiB ray::IDLE\n \n In addition, up to 0.0 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`. Note that Ray assumes all system memory is available for use by workers. If your system has other applications running, you should manually set these memory limits to a lower value.\n </denchmark-code>\n \n It seems that all those ray::IDLE processes are hanging around eating up memory. The worker machine sometimes recovers, but sometimes freezes due to lack of memory and eventually crashes.\n Ray version and other system information (Python version, TensorFlow version, OS):\n ray version 0.8.1\n Python 3.6\n Running ray cluster on AWS. Head node is r5.large (with 16G RAM), worker nodes are c5.large (with 4G RAM).\n Ubuntu 18.04 LTS on head, worker and driver nodes.\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Reproducing the issue involves setting up a ray cluster. The relevant setting from my .yaml are:\n <denchmark-code>cluster_name: ray_test\n min_workers: 2\n max_workers: 2\n head_node:\n     InstanceType: r5.large\n     ImageId: ami-0ac3869e129c60af6 # Ubuntu 18.04 LTS\n     BlockDeviceMappings:\n         - DeviceName: /dev/sda1\n           Ebs:\n               VolumeSize: 100\n worker_nodes:\n     InstanceType: c5.large\n     ImageId: ami-0ac3869e129c60af6 # Ubuntu 18.04 LTS\n </denchmark-code>\n \n The python script looks as follows:\n ray_test.py\n <denchmark-code>import ray\n import time\n \n ray.init('auto')\n \n def foo():\n   #This functions does nothing, but takes up around 400M of RAM\n   a = [1] * 30000000\n \n jobs = []\n for _ in range(4000):\n   jobs.append(ray.remote(max_calls=1)(foo).remote())\n \n objids = jobs\n while True:\n   objids_ready, objids_pending = ray.wait(objids, len(objids), 0)\n   time.sleep(0.5)\n   if 0==len(objids_pending):\n     break\n </denchmark-code>\n \n The steps to repro are as follows:\n \n Launch the ray cluster with ray up autoscaler/ray_test.yaml\n Launch local ray node with 0 resources with ray start --address=$INTERNAL_HEAD_IP:6379 --redis-password='5241590000000000' --num-cpus=0 --num-gpus=0. I don't think this is mandatory, but I like to have my driver not on the head.\n Run python ray_test.py.\n \n At this point, ray will start complaining, first with:\n <denchmark-code>{CPU: 2.000000}, {memory: 2.197266 GiB}, {object_store_memory: 0.634766 GiB}. In total there are 1 pending tasks and 0 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n </denchmark-code>\n \n And eventually with the message I've pasted at the top of this post, the one talking about ray::IDLE.\n Interestingly, if I remove the max_calls=1 argument in ray_test.py, I can no longer reproduce the issue.\n I speculate the issue has to do with how ray spawns a new process for each function run, but there's no way for me to be sure.\n Running with memory=500*1024*1024 delays the crash, but doesn't prevent it.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "matter-funds", "commentT": "2020-07-08T01:14:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/matter-funds>@matter-funds</denchmark-link>\n  we found and fixed a bug based on your script, but we're not sure it's the bug you originally detected. Can you  still reproduce this bug on the latest master (preferably on one machine?)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "matter-funds", "commentT": "2020-07-08T01:18:11Z", "comment_text": "\n \t\tJust to add on... one possible issue is the use of ray.remote(foo) in a loop... which is re-registering the function many times. If you lift it to @ray.remote once then it seems to run stably.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "matter-funds", "commentT": "2020-11-11T21:24:46Z", "comment_text": "\n \t\tHi, I'm a bot from the Ray team :)\n To help human contributors to focus on more relevant issues, I will automatically add the stale label to issues that have had no activity for more than 4 months.\n If there is no further activity in the 14 days, the issue will be closed!\n \n If you'd like to keep the issue open, just leave any comment, and the stale label will be removed!\n If you'd like to get more attention to the issue, please tag one of Ray's contributors.\n \n You can always ask for help on our <denchmark-link:https://discuss.ray.io/>discussion forum</denchmark-link>\n  or <denchmark-link:https://github.com/ray-project/ray#getting-involved>Ray's public slack channel</denchmark-link>\n .\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "matter-funds", "commentT": "2020-11-25T22:09:51Z", "comment_text": "\n \t\tHi again! The issue will be closed because there has been no more activity in the 14 days since the last message.\n Please feel free to reopen or open a new issue if you'd still like it to be addressed.\n Again, you can always ask for help on our <denchmark-link:https://discuss.ray.io>discussion forum</denchmark-link>\n  or <denchmark-link:https://github.com/ray-project/ray#getting-involved>Ray's public slack channel</denchmark-link>\n .\n Thanks again for opening the issue!\n \t\t"}}}, "commit": {"commit_id": "21af0ceb0c916bdafc1e8f82d13cf7c80b6d2b28", "commit_author": "Alex Wu", "commitT": "2020-07-28 13:51:34-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\import_thread.py", "file_new_name": "python\\ray\\import_thread.py", "file_complexity": {"file_NLOC": "130", "file_CCN": "28", "file_NToken": "714"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "139,140,141,142,143,144,145", "deleted_lines": "139,140,141", "method_info": {"method_name": "_process_key", "method_params": "self,key", "method_startline": "110", "method_endline": "157", "method_complexity": {"method_NLOC": "30", "method_CCN": "8", "method_NToken": "165", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\remote_function.py", "file_new_name": "python\\ray\\remote_function.py", "file_complexity": {"file_NLOC": "165", "file_CCN": "15", "file_NToken": "772"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "44", "deleted_lines": "44"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\tests\\test_advanced.py", "file_new_name": "python\\ray\\tests\\test_advanced.py", "file_complexity": {"file_NLOC": "361", "file_CCN": "90", "file_NToken": "2628"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "205,206", "deleted_lines": "205", "method_info": {"method_name": "test_profiling_api", "method_params": "ray_start_2_cpus", "method_startline": "176", "method_endline": "221", "method_complexity": {"method_NLOC": "34", "method_CCN": "6", "method_NToken": "147", "method_nesting_level": "0"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ray\\core_worker\\core_worker.cc", "file_new_name": "src\\ray\\core_worker\\core_worker.cc", "file_complexity": {"file_NLOC": "1671", "file_CCN": "289", "file_NToken": "13222"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "152,153,154,155", "method_info": {"method_name": "ray::CoreWorkerProcess::GetCoreWorker", "method_params": "", "method_startline": "149", "method_endline": "163", "method_complexity": {"method_NLOC": "11", "method_CCN": "2", "method_NToken": "62", "method_nesting_level": "1"}}}}}}}}