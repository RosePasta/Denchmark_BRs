{"BR": {"BR_id": "4693", "BR_author": "jacooba", "BRopenT": "2019-04-24T15:35:43Z", "BRcloseT": "2019-05-08T21:07:30Z", "BR_text": {"BRsummary": "Qmix Bug with Truncated Episodes or when max_seq_len is set", "BRdescription": "\n <denchmark-h:h3>System information</denchmark-h>\n \n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04\n Ray installed from (source or binary): source\n Ray version:  0.6.2\n Python version:  3.5.5\n Exact command to reproduce: (running on custom env)\n \n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n Qmix is set up to add padded q-values of -9999999 to the end of every sequence. These q-values are not being correctly masked out. They are ignored in terminal states, due to line 108 in qmix_policy_graph.py, however they are being used in non-terminal states that happen to be at the end of a sequence. This is causing astronomical TD-errors on my custom env with 1 agent.\n The issue is also a bit more complicated than just masking them out, as the code tries to do: There does need to be a next state (observation) to complete the bootstrapped return. If it is simply masked out, then certain states will never receive a back propagated loss.\n I would suggest also passing in next_states to the loss function (new_obs in samples dictionary I believe). If terminal states are not recorded with a next observation, then one can be added as padding, since it will be 0-ed out by (1 - terminated) in line 108 anyway.\n This issue can be verified by truncating episodes and noting that reward + self.gamma*-9999999 is contained in the variable masked_td_error.\n <denchmark-h:h3>Source code / logs</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jacooba", "commentT": "2019-04-25T00:31:18Z", "comment_text": "\n \t\tTo clarify, this only happens if batch mode is set to truncate episodes?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jacooba", "commentT": "2019-04-25T09:22:14Z", "comment_text": "\n \t\tI assume it would also be an issue with complete episodes but with a max_seq_len less than the length of the episode. It seems to be an issue if there is a non-terminal state at the end of a sequence.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jacooba", "commentT": "2019-04-25T09:41:29Z", "comment_text": "\n \t\tI have attached code that I believe reproduces the same bug. The commands are:\n python3 twostep_game_no_bug.py --run QMIX\n python3 twostep_game_bug.py --run QMIX\n the variable masked_td_error seems to still contain the dummy constant in the bug version.\n <denchmark-link:https://github.com/ray-project/ray/files/3116258/Qmix.bug.zip>Qmix bug.zip</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jacooba", "commentT": "2019-04-25T20:58:31Z", "comment_text": "\n \t\tHm I see. The proposed fix makes sense to me, and it also seems ok to potentially ignore the last state when truncating. Do you want to make a patch?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jacooba", "commentT": "2019-04-26T08:46:20Z", "comment_text": "\n \t\tSure. I can't do it this second, but should have some time next week.\n \t\t"}}}, "commit": {"commit_id": "28496c8b50a576dc853e74574bc12ed2b4d9a56d", "commit_author": "Jacob Beck", "commitT": "2019-05-08 14:07:29-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\ray\\rllib\\agents\\qmix\\qmix_policy_graph.py", "file_new_name": "python\\ray\\rllib\\agents\\qmix\\qmix_policy_graph.py", "file_complexity": {"file_NLOC": "334", "file_CCN": "44", "file_NToken": "2793"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "49,50,54,55,56,57,59,61,74,76,85,86,88,91,92,97,98,99,100,106,107,108,109,114,115", "deleted_lines": "49,53,54,55,56,71,73,82,84,85,86,87,90,95,96,106,107", "method_info": {"method_name": "forward", "method_params": "self,rewards,actions,terminated,mask,obs,action_mask", "method_startline": "49", "method_endline": "122", "method_complexity": {"method_NLOC": "37", "method_CCN": "7", "method_NToken": "419", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "250,251,255,256,261,262,263,268,269,275,276,280,281,282,286,287,288,291,295,296", "deleted_lines": "250,251,256,257,258,259,260,266,267,274,277,278,279,283", "method_info": {"method_name": "learn_on_batch", "method_params": "self,samples", "method_startline": "247", "method_endline": "315", "method_complexity": {"method_NLOC": "54", "method_CCN": "3", "method_NToken": "532", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "49,50", "deleted_lines": "49", "method_info": {"method_name": "forward", "method_params": "self,rewards,actions,terminated,mask,obs,next_obs,action_mask,next_action_mask", "method_startline": "49", "method_endline": "50", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "21", "method_nesting_level": "1"}}}}}}}}