{"BR": {"BR_id": "9071", "BR_author": "msloma144", "BRopenT": "2020-06-21T17:25:08Z", "BRcloseT": "2021-01-19T13:21:27Z", "BR_text": {"BRsummary": "[rllib] State shapes incorrect using custom model (TorchModelV2, TFModelV2) (PPO)", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n It seems that the states being passed to TorchModelV2 and TFModelV2 are incorrect, as the shapes don't seem to match up. Please see the stack traces below. Note that I am using PPO. Also, I do not want to use the RecurrentNetwork as I need more control than that provides.\n Ray version and other system information (Python version, TensorFlow version, OS):\n Python 3.8.3, ray: 0.9.0.dev0, torch: 1.5.1, tensorflow: 2.2.0, WSL Ubuntu 18.04.4 LTS\n <denchmark-h:h3>Keras/Tensorflow Error</denchmark-h>\n \n <denchmark-code>2020-06-21 13:09:32,571\tERROR trial_runner.py:524 -- Trial PPO_TestingGym_f28cf_00000: Error processing event.\n Traceback (most recent call last):\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 472, in _process_trial\n     result = self.trial_executor.fetch_result(trial)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 430, in fetch_result\n     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/worker.py\", line 1478, in get\n     raise value.as_instanceof_cause()\n ray.exceptions.RayTaskError(InvalidArgumentError): ray::PPO.train() (pid=20426, ip=192.168.2.105)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1349, in _run_fn\n     return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1441, in _call_tf_sessionrun\n     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [120,64] vs. [6,64]\n \t [[{{node default_policy_1/tower_1/model_1/lstm/while/lstm_cell/add_6}}]]\n \n During handling of the above exception, another exception occurred:\n \n ray::PPO.train() (pid=20426, ip=192.168.2.105)\n   File \"python/ray/_raylet.pyx\", line 443, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 446, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 447, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 401, in ray._raylet.execute_task.function_executor\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 520, in train\n     raise e\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 506, in train\n     result = Trainable.train(self)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trainable.py\", line 317, in train\n     result = self._train()\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 137, in _train\n     return self._train_exec_impl()\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 175, in _train_exec_impl\n     res = next(self.train_exec_impl)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 731, in __next__\n     return next(self.built_iterator)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 814, in apply_filter\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 814, in apply_filter\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 752, in apply_foreach\n     result = fn(item)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py\", line 204, in __call__\n     batch_fetches = optimizer.optimize(\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/multi_gpu_impl.py\", line 257, in optimize\n     return sess.run(fetches, feed_dict=feed_dict)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 957, in run\n     result = self._run(None, fetches, feed_dict, options_ptr,\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1180, in _run\n     results = self._do_run(handle, final_targets, final_fetches,\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1358, in _do_run\n     return self._do_call(_run_fn, feeds, fetches, targets, options,\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\n     raise type(e)(node_def, op, message)\n tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [120,64] vs. [6,64]\n \t [[node default_policy_1/tower_1/model_1/lstm/while/lstm_cell/add_6 (defined at mnt/c/Users/user/Desktop/RLlib_Issue/rnn_model.py:81) ]]\n </denchmark-code>\n \n <denchmark-h:h3>Pytorch Error</denchmark-h>\n \n <denchmark-code>2020-06-21 13:14:26,130\tERROR trial_runner.py:524 -- Trial PPO_TestingGym_a4dcc_00000: Error processing event.\n Traceback (most recent call last):\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 472, in _process_trial\n     result = self.trial_executor.fetch_result(trial)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 430, in fetch_result\n     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/worker.py\", line 1478, in get\n     raise value.as_instanceof_cause()\n ray.exceptions.RayTaskError(RuntimeError): ray::PPO.train() (pid=21085, ip=192.168.2.105)\n   File \"python/ray/_raylet.pyx\", line 447, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 401, in ray._raylet.execute_task.function_executor\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 520, in train\n     raise e\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 506, in train\n     result = Trainable.train(self)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/tune/trainable.py\", line 317, in train\n     result = self._train()\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 137, in _train\n     return self._train_exec_impl()\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\", line 175, in _train_exec_impl\n     res = next(self.train_exec_impl)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 731, in __next__\n     return next(self.built_iterator)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 814, in apply_filter\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 814, in apply_filter\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n     for item in it:\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/util/iter.py\", line 752, in apply_foreach\n     result = fn(item)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py\", line 62, in __call__\n     info = do_minibatch_sgd(\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/utils/sgd.py\", line 114, in do_minibatch_sgd\n     batch_fetches = (local_worker.learn_on_batch(\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 737, in learn_on_batch\n     info_out[pid] = policy.learn_on_batch(batch)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\", line 242, in learn_on_batch\n     self._loss(self, self.model, self.dist_class, train_batch))\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 113, in ppo_surrogate_loss\n     logits, state = model.from_batch(train_batch)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/models/modelv2.py\", line 224, in from_batch\n     return self.__call__(input_dict, states, train_batch.get(\"seq_lens\"))\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/ray/rllib/models/modelv2.py\", line 181, in __call__\n     res = self.forward(restored, state or [], seq_lens)\n   File \"/mnt/c/Users/user/Desktop/RLlib_Issue/rnn_model.py\", line 166, in forward\n     self._features, [h, c] = self.lstm(x, [torch.unsqueeze(state[0], 0),\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 567, in forward\n     self.check_forward_args(input, hx, batch_sizes)\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 522, in check_forward_args\n     self.check_hidden_size(hidden[0], expected_hidden_size,\n   File \"/home/user/anaconda3/envs/RLlibTesting/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 187, in check_hidden_size\n     raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n RuntimeError: Expected hidden[0] size (1, 140, 256), got (1, 7, 256)\n </denchmark-code>\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n If we cannot run your script, we cannot fix your issue.\n \n [ X ] I have verified my script runs in a clean environment and reproduces the issue. (created new conda environment and installed all packages from scratch using pip)\n [ X ] I have verified the issue also occurs with the latest wheels.\n \n <denchmark-h:h4>rllib_ppo_agent.py</denchmark-h>\n \n from testing_gym import TestingGym\n from ray.rllib.models import ModelCatalog\n from ray.tune.registry import register_env\n from rnn_model import TorchRNNModel, RNNModel\n from ray import tune\n \n timesteps = 5\n \n \n def env_creator(env_config):\n     env = TestingGym()\n     return env  # return an env instance\n \n \n if __name__ == \"__main__\":\n     register_env(\"TestingGym\", env_creator)\n     # also have had issues with TF models\n     ModelCatalog.register_custom_model(\"torch_model\", TorchRNNModel)\n     ModelCatalog.register_custom_model(\"keras_model\",  RNNModel)\n \n     tune.run(\n         \"A2C\",\n         stop={\"episodes_total\": 500},\n         checkpoint_at_end=True,\n         checkpoint_freq=100,\n         config={\n             \"env\": \"TestingGym\",\n             \"num_workers\": 14,\n             \"env_config\": {},\n             \"lr\": 0.000001,\n             \"framework\": \"torch\",\n             \"model\": {\n                 \"custom_model_config\":\n                     {\n                         \"timesteps\": timesteps\n                     },\n                 \"fcnet_hiddens\": [256, 256, 256, 256],\n                 \"custom_model\": \"torch_model\",\n             }\n         },\n         local_dir=\"./results\", )\n <denchmark-h:h4>rnn_model.py</denchmark-h>\n \n import numpy as np\n \n from ray.rllib.models.modelv2 import ModelV2\n from ray.rllib.models.preprocessors import get_preprocessor\n from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n from ray.rllib.utils.annotations import override\n from ray.rllib.utils.framework import try_import_tf, try_import_torch\n from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n \n tf = try_import_tf()\n torch, nn = try_import_torch()\n \n \n class RNNModel(TFModelV2):\n     \"\"\"Example of using the Keras functional API to define a RNN model.\"\"\"\n \n     def __init__(self,\n                  obs_space,\n                  action_space,\n                  num_outputs,\n                  model_config,\n                  name,\n                  hiddens_size=256,\n                  cell_size=64,\n                  timesteps=5):\n         super(RNNModel, self).__init__(obs_space, action_space, num_outputs,\n                                        model_config, name)\n         self.obs_space = obs_space\n         self.cell_size = cell_size\n         self.timesteps = timesteps\n \n         print(f\"OBS SPACE: {obs_space.shape}\")\n         # Define input layers\n         input_layer = tf.keras.layers.Input(\n             shape=(timesteps, int(obs_space.shape[0]/self.timesteps)), name=\"inputs\")\n \n         state_in_h = tf.keras.layers.Input(shape=(cell_size, ), name=\"h\")\n         state_in_c = tf.keras.layers.Input(shape=(cell_size, ), name=\"c\")\n         #seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\", dtype=tf.int32)\n \n         # Preprocess observation with a hidden layer and send to LSTM cell\n         dense1 = tf.keras.layers.Dense(\n             hiddens_size, activation=tf.nn.sigmoid, name=\"dense1\")(input_layer)\n         lstm_out, state_h, state_c = tf.keras.layers.LSTM(\n             cell_size,\n             #return_sequences=True,\n             return_state=True, name=\"lstm\")(\n                 inputs=dense1,\n                 #mask=tf.sequence_mask(seq_in),\n                 initial_state=[state_in_h, state_in_c])\n         #flats = tf.keras.layers.Flatten()(lstm_out)\n         # Postprocess LSTM output with another hidden layer and compute values\n \n         _ = lstm_out\n         for units in model_config[\"fcnet_hiddens\"]:\n             _ = tf.keras.layers.Dense(\n                 units,\n                 activation=tf.keras.activations.sigmoid)(_)\n \n         logits = tf.keras.layers.Dense(\n             self.num_outputs,\n             activation=tf.keras.activations.linear,\n             name=\"logits\")(_)\n         values = tf.keras.layers.Dense(\n             1, activation=None, name=\"values\")(_)\n \n         # Create the RNN model\n         self.rnn_model = tf.keras.Model(\n             inputs=[input_layer, state_in_h, state_in_c],\n             outputs=[logits, values, state_h, state_c])\n         self.register_variables(self.rnn_model.variables)\n         self.rnn_model.summary()\n \n     @override(TFModelV2)\n     def forward(self, inputs, state, seq_lens):\n         print(\"forward\")\n         print(f\"INPUTS: {state}\")\n         inputs = inputs['obs']\n         inputs = tf.reshape(tensor=inputs, shape=[-1, self.timesteps, int(self.obs_space.shape[0]/self.timesteps)])\n \n         model_out, self._value_out, h, c = self.rnn_model([inputs,] + state)\n         return model_out, [h, c]\n \n     @override(ModelV2)\n     def get_initial_state(self):\n         return [\n             np.zeros(self.cell_size, np.float32),\n             np.zeros(self.cell_size, np.float32),\n         ]\n \n     @override(ModelV2)\n     def value_function(self):\n         return tf.reshape(self._value_out, [-1])\n \n \n class TorchRNNModel(TorchModelV2, nn.Module):\n     def __init__(self,\n                  obs_space,\n                  action_space,\n                  num_outputs,\n                  model_config,\n                  name,\n                  fc_size=64,\n                  lstm_state_size=256,\n                  num_symbols=5,\n                  timesteps=5):\n         super().__init__(obs_space, action_space, num_outputs, model_config,\n                          name)\n         nn.Module.__init__(self)\n         self.timesteps = timesteps\n         self.num_symbols = num_symbols\n \n         self.obs_size = get_preprocessor(obs_space)(obs_space).size\n         print(f\"RNN Obs Size: {self.obs_size}\")\n         self.obs_size = int(self.obs_size/self.timesteps)\n         self.fc_size = fc_size\n         self.lstm_state_size = lstm_state_size\n \n         # Build the Module from fc + LSTM + 2xfc (action + value outs).\n         self.fc1 = nn.Linear(self.obs_size, self.fc_size)\n         self.lstm = nn.LSTM(self.fc_size, self.lstm_state_size, batch_first=True)\n         self.action_branch = nn.Linear(self.lstm_state_size, num_outputs)\n         self.value_branch = nn.Linear(self.lstm_state_size, 1)\n         # Holds the current \"base\" output (before logits layer).\n         self._features = None\n \n     @override(ModelV2)\n     def get_initial_state(self):\n         # Place hidden states on same device as model.\n         h = [\n             self.fc1.weight.new(1, self.lstm_state_size).zero_().squeeze(0),\n             self.fc1.weight.new(1, self.lstm_state_size).zero_().squeeze(0)\n         ]\n         print(f\"Inital State: {h[0].shape},  {h[1].shape}\")\n         return h\n \n     @override(ModelV2)\n     def value_function(self):\n         assert self._features is not None, \"must call forward() first\"\n         return torch.reshape(self.value_branch(self._features), [-1])\n \n     @override(ModelV2)\n     def forward(self, inputs, state, seq_lens):\n         \"\"\"\n         Feeds `inputs` (B x T x ..) through the Gru Unit.\n \n         Returns the resulting outputs as a sequence (B x T x ...).\n         Values are stored in self._cur_value in simple (B) shape (where B\n         contains both the B and T dims!).\n \n         Returns:\n             NN Outputs (B x T x ...) as sequence.\n             The state batches as a List of two items (c- and h-states).\n         \"\"\"\n         print(\"forward\")\n         #print(f\"INPUTS: {state}\")\n         inputs = inputs['obs']\n         # if not isinstance(inputs, tuple):\n         inputs = torch.reshape(input=inputs, shape=(-1, self.timesteps, int(self.obs_size)))\n         print(f\"inputs shape: {inputs.shape}\")\n         print(f\"state sizes: h {torch.unsqueeze(state[0], 0).shape}, c {torch.unsqueeze(state[1], 0).shape}\")\n         # embedding_input = inputs[:, :, :self.num_symbols]\n         # inputs = inputs[:, :, self.num_symbols:]\n \n         x = nn.functional.relu(self.fc1(inputs))\n         self._features, [h, c] = self.lstm(x, [torch.unsqueeze(state[0], 0),\n                                                torch.unsqueeze(state[1], 0)])\n         print(f\"state size after: h {h.shape}, c {c.shape}\")\n         print(f\"LSTM shape: {self._features.shape}\")\n         self._features = self._features[:, -1, :]\n         print(f\"LSTM shape After: {self._features.shape}\")\n         action_out = self.action_branch(self._features)\n         print(f\"action shape: {action_out.shape}\")\n \n         return action_out, [torch.squeeze(h, 0), torch.squeeze(c, 0)]\n <denchmark-h:h4>testing_gym.py</denchmark-h>\n \n import gym\n from gym import error, spaces, utils\n from gym.utils import seeding\n import numpy as np\n import sys\n \n \n class TestingGym(gym.Env):\n     metadata = {'render.modes': ['human']}\n \n     def __init__(self, timesteps=5):\n         self.timesteps = timesteps\n \n         super(TestingGym, self).__init__()\n \n         self.reward_range = (-sys.float_info.max-1, sys.float_info.max)\n \n         self.action_space = spaces.Box(low=np.array([0, 0]), high=np.array([4, 1]), dtype=np.float16)\n \n         self.done_counter = 0\n         self.obs_length = 15\n         self.observation_space = spaces.Box(low=-sys.float_info.max-1, high=sys.float_info.max, shape=(self.timesteps * self.obs_length,), dtype=np.float32)\n \n     def _initial_observation(self):\n         curr_obs = np.random.random((self.timesteps, self.obs_length))\n         curr_obs = curr_obs.reshape((self.timesteps * self.obs_length,))\n         print(f\"Obs Length: {curr_obs.shape}\")\n         return curr_obs\n \n     def step(self, action):\n         self.done_counter += 1\n \n         curr_obs = np.random.random((self.timesteps, self.obs_length))\n         curr_obs = curr_obs.reshape((self.timesteps * self.obs_length,))\n \n         if self.done_counter > 1000:\n             done = True\n         else:\n             done = False\n \n         print(f\"Obs Length: {curr_obs.shape}\")\n         return curr_obs, 1, done, {}\n \n     def reset(self):\n         self.done_counter = 0\n \n         return self._initial_observation()\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "msloma144", "commentT": "2020-12-26T02:42:22Z", "comment_text": "\n \t\tThank you for sharing this, as I am facing the same challenge in recent days using TorchRNNModel with TorchModelV2.  I believe it happens on my experiment after workers finish collecting experiences and batch shape changes.\n Ray version and other system information (Python version, TensorFlow version, OS):\n Python 3.7.6, ray: 1.1.0, torch: 1.7.0a0+b7147fe, WSL Ubuntu 18.04.4 LTS\n RuntimeError: Expected hidden[0] size (1, 1, 128), got (1, 32, 128)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "msloma144", "commentT": "2020-12-30T00:23:57Z", "comment_text": "\n \t\tIs there a workaround to get the correct shape the first time around?  It seems very connected to this thread #<denchmark-link:https://github.com/ray-project/ray/issues/12509>#12509</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "msloma144", "commentT": "2020-12-30T01:51:17Z", "comment_text": "\n \t\tStrange, I'm getting a different error on your torch script, related to the states list being empty.\n The reason is that A3C uses a buggy ValueFunction mixin, which assumes that no RNN is being used. I'll fix this now and re-try. ...\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "msloma144", "commentT": "2020-12-30T02:08:52Z", "comment_text": "\n \t\tThanks for looking at this Sven.  My results are the same when using A3C/A2C.  If we change it over to PPO then we get the state shape error.\n <denchmark-code>state sizes: h torch.Size([1, 32, 256]), c torch.Size([1, 32, 256])\n state size after: h torch.Size([1, 32, 256]), c torch.Size([1, 32, 256])\n LSTM shape: torch.Size([32, 5, 256])\n LSTM shape After: torch.Size([32, 256])\n action shape: torch.Size([32, 4])\n * forward pass #1\n inputs shape: torch.Size([1, 5, 15])\n state sizes: h torch.Size([1, 1, 256]), c torch.Size([1, 1, 256])\n state size after: h torch.Size([1, 1, 256]), c torch.Size([1, 1, 256])\n LSTM shape: torch.Size([1, 5, 256])\n LSTM shape After: torch.Size([1, 256])\n action shape: torch.Size([1, 4])\n * forward pass #2\n inputs shape: torch.Size([32, 5, 15])\n state sizes: h torch.Size([1, 4, 256]), c torch.Size([1, 4, 256])\n </denchmark-code>\n \n RuntimeError: Expected hidden[0] size (1, 32, 256), got [1, 4, 256]\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "msloma144", "commentT": "2020-12-30T03:56:53Z", "comment_text": "\n \t\tOk, getting the same error now after having fixed these legacy A2C issues. ....\n \t\t"}}}, "commit": {"commit_id": "2e3655e8a9b1e37fa6e29f11db02a6b53cfb5928", "commit_author": "Sven Mika", "commitT": "2021-01-19 14:22:36+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "rllib\\agents\\a3c\\a3c_tf_policy.py", "file_new_name": "rllib\\agents\\a3c\\a3c_tf_policy.py", "file_complexity": {"file_NLOC": "87", "file_CCN": "9", "file_NToken": "645"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "99", "deleted_lines": null, "method_info": {"method_name": "setup_mixins", "method_params": "policy,obs_space,action_space,config", "method_startline": "98", "method_endline": "100", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "39", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "17,18,19,20", "deleted_lines": null, "method_info": {"method_name": "postprocess_advantages", "method_params": "policy,sample_batch,other_agent_batches,episode", "method_startline": "17", "method_endline": "20", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": null, "deleted_lines": "75,76,77,78,79,80,81,82,83", "method_info": {"method_name": "__init__.value", "method_params": "ob,prev_action,prev_reward,state", "method_startline": "75", "method_endline": "83", "method_complexity": {"method_NLOC": "9", "method_CCN": "2", "method_NToken": "106", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": null, "deleted_lines": "73,74,75,76,77,78,79,80,81,82,83,84,85", "method_info": {"method_name": "__init__", "method_params": "self", "method_startline": "73", "method_endline": "85", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "21", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "rllib\\agents\\a3c\\a3c_torch_policy.py", "file_new_name": "rllib\\agents\\a3c\\a3c_torch_policy.py", "file_complexity": {"file_NLOC": "77", "file_CCN": "6", "file_NToken": "503"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "82,83,84", "method_info": {"method_name": "_value", "method_params": "self,obs", "method_startline": "82", "method_endline": "84", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "51", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "18,19,20,21", "deleted_lines": null, "method_info": {"method_name": "add_advantages", "method_params": "policy,sample_batch,other_agent_batches,episode", "method_startline": "18", "method_endline": "21", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "70,71,72", "deleted_lines": "70,71,72", "method_info": {"method_name": "setup_mixins", "method_params": "Policy,Space,Space,TrainerConfigDict", "method_startline": "70", "method_endline": "72", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "70,71,72,73,74", "deleted_lines": "60,61,62,63,64,65,66,67,68,69,70,71,72,73,74", "method_info": {"method_name": "apply_grad_clipping", "method_params": "policy,optimizer,loss", "method_startline": "60", "method_endline": "74", "method_complexity": {"method_NLOC": "13", "method_CCN": "5", "method_NToken": "99", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\cql\\cql_torch_policy.py", "file_new_name": "rllib\\agents\\cql\\cql_torch_policy.py", "file_complexity": {"file_NLOC": "242", "file_CCN": "8", "file_NToken": "1960"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "24,25", "deleted_lines": "11,25"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\ddpg\\ddpg_torch_policy.py", "file_new_name": "rllib\\agents\\ddpg\\ddpg_torch_policy.py", "file_complexity": {"file_NLOC": "194", "file_CCN": "29", "file_NToken": "1489"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "12", "deleted_lines": "4,13"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\dqn\\dqn_tf_policy.py", "file_new_name": "rllib\\agents\\dqn\\dqn_tf_policy.py", "file_complexity": {"file_NLOC": "347", "file_CCN": "21", "file_NToken": "2470"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "304,305,306,307,308", "deleted_lines": "304,305,306,307,308,309,310,311,312,313,314"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\dqn\\dqn_torch_policy.py", "file_new_name": "rllib\\agents\\dqn\\dqn_torch_policy.py", "file_complexity": {"file_NLOC": "320", "file_CCN": "13", "file_NToken": "2180"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,23", "deleted_lines": "7,23,24,25"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\dreamer\\dreamer_torch_policy.py", "file_new_name": "rllib\\agents\\dreamer\\dreamer_torch_policy.py", "file_complexity": {"file_NLOC": "174", "file_CCN": "14", "file_NToken": "1553"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8", "deleted_lines": "4"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\impala\\impala.py", "file_new_name": "rllib\\agents\\impala\\impala.py", "file_complexity": {"file_NLOC": "191", "file_CCN": "24", "file_NToken": "1219"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "162", "deleted_lines": null, "method_info": {"method_name": "get_policy_class", "method_params": "config", "method_startline": "148", "method_endline": "163", "method_complexity": {"method_NLOC": "16", "method_CCN": "4", "method_NToken": "77", "method_nesting_level": "0"}}}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\impala\\vtrace_torch_policy.py", "file_new_name": "rllib\\agents\\impala\\vtrace_torch_policy.py", "file_complexity": {"file_NLOC": "183", "file_CCN": "18", "file_NToken": "1354"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13,14", "deleted_lines": "6,14,15"}}}, "file_9": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\maml\\maml_tf_policy.py", "file_new_name": "rllib\\agents\\maml\\maml_tf_policy.py", "file_complexity": {"file_NLOC": "365", "file_CCN": "36", "file_NToken": "2487"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4,5,6,7,425", "deleted_lines": "4,5,6,7,425"}}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\maml\\maml_torch_policy.py", "file_new_name": "rllib\\agents\\maml\\maml_torch_policy.py", "file_complexity": {"file_NLOC": "306", "file_CCN": "23", "file_NToken": "2081"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4,5,8,12,358", "deleted_lines": "4,7,8,11,358"}}}, "file_11": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\mbmpo\\mbmpo_torch_policy.py", "file_new_name": "rllib\\agents\\mbmpo\\mbmpo_torch_policy.py", "file_complexity": {"file_NLOC": "75", "file_CCN": "1", "file_NToken": "412"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8,10,17,88", "deleted_lines": "6,9,10,88"}}}, "file_12": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\ppo\\appo_tf_policy.py", "file_new_name": "rllib\\agents\\ppo\\appo_tf_policy.py", "file_complexity": {"file_NLOC": "344", "file_CCN": "16", "file_NToken": "2014"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "17,18,341,342", "deleted_lines": "16,18,341,342"}}}, "file_13": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\ppo\\appo_torch_policy.py", "file_new_name": "rllib\\agents\\ppo\\appo_torch_policy.py", "file_complexity": {"file_NLOC": "244", "file_CCN": "12", "file_NToken": "1724"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "29,30", "deleted_lines": "13,30,31"}}}, "file_14": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\ppo\\ppo_tf_policy.py", "file_new_name": "rllib\\agents\\ppo\\ppo_tf_policy.py", "file_complexity": {"file_NLOC": "277", "file_CCN": "19", "file_NToken": "1520"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "330,331,332,333,334", "deleted_lines": null, "method_info": {"method_name": "postprocess_ppo_gae", "method_params": "Policy,SampleBatch,AgentID,None,None", "method_startline": "330", "method_endline": "334", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "36", "method_nesting_level": "0"}}}}}, "file_15": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\ppo\\ppo_torch_policy.py", "file_new_name": "rllib\\agents\\ppo\\ppo_torch_policy.py", "file_complexity": {"file_NLOC": "232", "file_CCN": "16", "file_NToken": "1226"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "10,11,12,21,22,281", "deleted_lines": "10,11,12,13,22,23,282"}}}, "file_16": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\agents\\ppo\\tests\\test_ppo.py", "file_new_name": "rllib\\agents\\ppo\\tests\\test_ppo.py", "file_complexity": {"file_NLOC": "322", "file_CCN": "53", "file_NToken": "2634"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "256,257,258", "deleted_lines": "258,259,260,261,262", "method_info": {"method_name": "test_ppo_loss_function", "method_params": "self", "method_startline": "226", "method_endline": "321", "method_complexity": {"method_NLOC": "81", "method_CCN": "18", "method_NToken": "587", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "216,217", "deleted_lines": "215,216,217,218,219", "method_info": {"method_name": "test_ppo_free_log_std", "method_params": "self", "method_startline": "177", "method_endline": "224", "method_complexity": {"method_NLOC": "33", "method_CCN": "8", "method_NToken": "234", "method_nesting_level": "1"}}}}}, "file_17": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\sac\\sac.py", "file_new_name": "rllib\\agents\\sac\\sac.py", "file_complexity": {"file_NLOC": "105", "file_CCN": "6", "file_NToken": "417"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "76,106", "deleted_lines": "76,77,107,108,109"}}}, "file_18": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\sac\\sac_torch_policy.py", "file_new_name": "rllib\\agents\\sac\\sac_torch_policy.py", "file_complexity": {"file_NLOC": "400", "file_CCN": "15", "file_NToken": "2191"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "25", "deleted_lines": "12,26"}}}, "file_19": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\contrib\\maddpg\\maddpg_policy.py", "file_new_name": "rllib\\contrib\\maddpg\\maddpg_policy.py", "file_complexity": {"file_NLOC": "303", "file_CCN": "36", "file_NToken": "2170"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "268,269,270,271,272", "deleted_lines": "268,269,270,271,272,273", "method_info": {"method_name": "gradients", "method_params": "self,optimizer,loss", "method_startline": "267", "method_endline": "273", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "68", "method_nesting_level": "1"}}}}}, "file_20": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\evaluation\\postprocessing.py", "file_new_name": "rllib\\evaluation\\postprocessing.py", "file_complexity": {"file_NLOC": "116", "file_CCN": "3", "file_NToken": "623"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "84,85,86,87,88", "deleted_lines": null, "method_info": {"method_name": "compute_gae_for_sample_batch", "method_params": "Policy,SampleBatch,AgentID,None,None", "method_startline": "84", "method_endline": "88", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "36", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "149,150,151,152,153,154,155,156,157,158,159,160,161", "deleted_lines": null, "method_info": {"method_name": "discount_cumsum", "method_params": "ndarray,float", "method_startline": "149", "method_endline": "161", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "53", "method_nesting_level": "0"}}}}}, "file_21": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\tuned_examples\\sac\\mspacman-sac.yaml", "file_new_name": "rllib\\tuned_examples\\sac\\mspacman-sac.yaml", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "17,18,20,21", "deleted_lines": "17,18,20,21"}}}, "file_22": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\utils\\tf_ops.py", "file_new_name": "rllib\\utils\\tf_ops.py", "file_complexity": {"file_NLOC": "129", "file_CCN": "42", "file_NToken": "1058"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "95,105,106", "deleted_lines": "95,105,106", "method_info": {"method_name": "minimize_and_clip", "method_params": "optimizer,objective,var_list,clip_val", "method_startline": "89", "method_endline": "106", "method_complexity": {"method_NLOC": "11", "method_CCN": "6", "method_NToken": "110", "method_nesting_level": "0"}}}}}, "file_23": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\utils\\torch_ops.py", "file_new_name": "rllib\\utils\\torch_ops.py", "file_complexity": {"file_NLOC": "109", "file_CCN": "42", "file_NToken": "1052"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38", "deleted_lines": null, "method_info": {"method_name": "apply_grad_clipping", "method_params": "policy,optimizer,loss", "method_startline": "17", "method_endline": "38", "method_complexity": {"method_NLOC": "13", "method_CCN": "5", "method_NToken": "100", "method_nesting_level": "0"}}}}}}}}