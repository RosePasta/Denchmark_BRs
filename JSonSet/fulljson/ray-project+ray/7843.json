{"BR": {"BR_id": "7843", "BR_author": "pengzhenghao", "BRopenT": "2020-04-01T03:14:32Z", "BRcloseT": "2020-04-03T17:44:59Z", "BR_text": {"BRsummary": "[rllib] PPO policy return all zeros for action_logp", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n PPO policy return all zeros when computing the log probability of actions.\n ray=0.8.2 (latest)\n tensorflow=2.1.0\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n import ray\n import numpy as np\n from ray.rllib.agents.ppo import PPOTrainer\n \n ray.init()\n \n ppo_agent = PPOTrainer(env=\"BipedalWalker-v2\")\n \n ppo_agent.get_policy().get_session().run(\n     ppo_agent.get_policy()._action_logp,\n     feed_dict={\n         ppo_agent.get_policy()._input_dict[\"obs\"]: np.random.random((500, 24))\n     }\n )\n Result in\n <denchmark-code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0.], dtype=float32)\n </denchmark-code>\n \n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "pengzhenghao", "commentT": "2020-04-01T03:36:18Z", "comment_text": "\n \t\tThe issue still happen in Ray=0.8.3\n import ray\n import numpy as np\n from ray.rllib.agents.ppo import PPOTrainer\n \n print(ray.__version__)\n \n ray.init(ignore_reinit_error=True)\n \n ppo_agent = PPOTrainer(env=\"BipedalWalker-v2\")\n \n ppo_agent.get_policy().get_session().run(\n     ppo_agent.get_policy()._sampled_action_logp,\n     feed_dict={\n         ppo_agent.get_policy()._input_dict[\"obs\"]: np.random.random((500, 24))\n     }\n )\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "pengzhenghao", "commentT": "2020-04-01T04:45:26Z", "comment_text": "\n \t\tRay 0.8.1 works well! Something wrong with the updates introduced by 0.8.2\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "pengzhenghao", "commentT": "2020-04-01T05:38:11Z", "comment_text": "\n \t\tI did a bit of digging and it seems related to the is_exploring flag somehow being False for PPO in multi-GPU mode (works fine with simple_optimizer: True). <denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n \n I think this is since the explore placeholder isn't passed to copies in dynamic_tf_policy.copy().\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "pengzhenghao", "commentT": "2020-04-01T06:11:53Z", "comment_text": "\n \t\tYeah, that's possible. I'll take a look.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "pengzhenghao", "commentT": "2020-04-01T06:32:07Z", "comment_text": "\n \t\tYeah, it's not copied along. Also default placeholder is True for TFPolicy, but False for DynamicTFPolicy (I made them both True now).\n We are still not copying the is_training placeholder either? Shouldn't we do that as well?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "pengzhenghao", "commentT": "2020-04-01T06:33:27Z", "comment_text": "\n \t\tI'm assuming the copy()-method is used for multi-GPU tower generation?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "pengzhenghao", "commentT": "2020-04-01T06:37:09Z", "comment_text": "\n \t\tAlso, you should probably rather do:\n <denchmark-code>a, state_out, extras = trainer.get_policy().compute_actions([your_obs])\n \n # Then:\n action_probs = extras[\"action_prob\"]\n action_logp = extras[\"action_logp\"]\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "pengzhenghao", "commentT": "2020-04-01T07:44:08Z", "comment_text": "\n \t\tYep, copy is only used to generate towers.\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Tue, Mar 31, 2020, 11:37 PM Sven Mika ***@***.***> wrote:\n  Also, you should probably rather do:\n \n  a, state_out, extras = trainer.get_policy().compute_actions([your_obs])\n \n  # Then:\n  action_probs = extras[\"action_prob\"]\n  action_logp = extras[\"action_logp\"]\n \n  \u2014\n  You are receiving this because you commented.\n  Reply to this email directly, view it on GitHub\n  <#7843 (comment)>,\n  or unsubscribe\n  <https://github.com/notifications/unsubscribe-auth/AAADUSWBDXCPIHZOF2WQW7LRKLOKFANCNFSM4LYK5SMQ>\n  .\n \n \n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "pengzhenghao", "commentT": "2020-04-01T10:12:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pengzhenghao>@pengzhenghao</denchmark-link>\n  Could you try this PR and see whether it fixes your issue?\n <denchmark-link:https://github.com/ray-project/ray/pull/7846>#7846</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "pengzhenghao", "commentT": "2020-04-02T01:32:07Z", "comment_text": "\n \t\t<denchmark-link:https://user-images.githubusercontent.com/22206995/78201694-ca9e3780-74c4-11ea-950c-209674ae29a5.png></denchmark-link>\n \n Is this related the issue and PR?\n \t\t"}}}, "commit": {"commit_id": "bb6c67523171f807c631461b97c00475b21300ef", "commit_author": "Sven Mika", "commitT": "2020-04-03 10:44:58-07:00", "commit_complexity": {"commit_NLOC": "0.04", "commit_CCN": "0.96", "commit_Nprams": "0.96"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\BUILD", "file_new_name": "rllib\\BUILD", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "113", "deleted_lines": "113"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\ppo\\ppo.py", "file_new_name": "rllib\\agents\\ppo\\ppo.py", "file_complexity": {"file_NLOC": "132", "file_CCN": "27", "file_NToken": "752"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "98,99", "deleted_lines": "95", "method_info": {"method_name": "choose_policy_optimizer", "method_params": "workers,config", "method_startline": "80", "method_endline": "99", "method_complexity": {"method_NLOC": "19", "method_CCN": "2", "method_NToken": "112", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\ppo\\tests\\test_ppo.py", "file_new_name": "rllib\\agents\\ppo\\tests\\test_ppo.py", "file_complexity": {"file_NLOC": "184", "file_CCN": "22", "file_NToken": "1688"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74", "deleted_lines": null, "method_info": {"method_name": "test_ppo_fake_multi_gpu_learning", "method_params": "self", "method_startline": "49", "method_endline": "74", "method_complexity": {"method_NLOC": "22", "method_CCN": "3", "method_NToken": "142", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\optimizers\\multi_gpu_optimizer.py", "file_new_name": "rllib\\optimizers\\multi_gpu_optimizer.py", "file_complexity": {"file_NLOC": "187", "file_CCN": "23", "file_NToken": "1276"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "49", "method_info": {"method_name": "__init__", "method_params": "self,workers,sgd_batch_size,num_sgd_iter,rollout_fragment_length,num_envs_per_worker,train_batch_size,num_gpus,standardize_fields,shuffle_sequences", "method_startline": "40", "method_endline": "49", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "40", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "50,51", "deleted_lines": "49", "method_info": {"method_name": "__init__", "method_params": "self,workers,sgd_batch_size,num_sgd_iter,rollout_fragment_length,num_envs_per_worker,train_batch_size,num_gpus,standardize_fields,shuffle_sequences,_fake_gpus", "method_startline": "41", "method_endline": "51", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\policy\\dynamic_tf_policy.py", "file_new_name": "rllib\\policy\\dynamic_tf_policy.py", "file_complexity": {"file_NLOC": "314", "file_CCN": "36", "file_NToken": "2047"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "285,286,287", "deleted_lines": "284,285", "method_info": {"method_name": "copy", "method_params": "self,existing_inputs", "method_startline": "262", "method_endline": "304", "method_complexity": {"method_NLOC": "36", "method_CCN": "10", "method_NToken": "327", "method_nesting_level": "1"}}}}}, "file_5": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "rllib\\tuned_examples\\regression_tests\\cartpole-ppo-tf-multi-gpu.yaml", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}}}}