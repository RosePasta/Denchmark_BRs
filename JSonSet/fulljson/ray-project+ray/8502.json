{"BR": {"BR_id": "8502", "BR_author": "edoakes", "BRopenT": "2020-05-19T15:37:00Z", "BRcloseT": "2020-12-08T17:36:54Z", "BR_text": {"BRsummary": "'ray memory' fails if there are many objects in scope", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Helping a user debug OOM errors and asked them to run ray memory. ray memory crashed with the following output:\n <denchmark-code>2020-05-19 02:13:32,283\tINFO scripts.py:976 -- Connecting to Ray instance at 172.31.6.12:34940.\n 2020-05-19 02:13:32,284\tWARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n (pid=5906) E0519 02:13:32.383447  5906 plasma_store_provider.cc:108] Failed to put object d47fe8ca624da001ffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.\n (pid=5906) Waiting 1000ms for space to free up...\n (pid=5906) 2020-05-19 02:13:32,594\tINFO (unknown file):0 -- gc.collect() freed 10 refs in 0.11551751299975876 seconds\n (pid=5771) E0519 02:13:32.686894  5771 plasma_store_provider.cc:118] Failed to put object 72e67d09154b35b1ffffffff010000c801000000 after 6 attempts. Plasma store status:\n (pid=5771) num clients with quota: 0\n (pid=5771) quota map size: 0\n (pid=5771) pinned quota map size: 0\n (pid=5771) allocated bytes: 19130609999\n (pid=5771) allocation limit: 19130641612\n (pid=5771) pinned bytes: 19130609999\n (pid=5771) (global lru) capacity: 19130641612\n (pid=5771) (global lru) used: 0%\n (pid=5771) (global lru) num objects: 0\n (pid=5771) (global lru) num evictions: 0\n (pid=5771) (global lru) bytes evicted: 0\n (pid=5771) ---\n (pid=5771) --- Tip: Use the `ray memory` command to list active objects in the cluster.\n (pid=5771) ---\n (pid=5771) E0519 02:13:32.880080  5771 plasma_store_provider.cc:108] Failed to put object 1f5c36abed661dbeffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.\n (pid=5771) Waiting 1000ms for space to free up...\n (pid=5769) E0519 02:13:32.882894  5769 plasma_store_provider.cc:108] Failed to put object cb31822e7f0e3c70ffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.\n (pid=5769) Waiting 2000ms for space to free up...\n (pid=5771) 2020-05-19 02:13:33,215\tINFO (unknown file):0 -- gc.collect() freed 10 refs in 0.23763301200006026 seconds\n (pid=5906) E0519 02:13:33.383901  5906 plasma_store_provider.cc:108] Failed to put object d47fe8ca624da001ffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.\n (pid=5906) Waiting 2000ms for space to free up...\n Traceback (most recent call last):\n   File \"/home/ubuntu/src/seeweed/ml/bin/ray\", line 8, in <module>\n     sys.exit(main())\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/ray/scripts/scripts.py\", line 1028, in main\n     return cli()\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py\", line 829, in __call__\n     return self.main(*args, **kwargs)\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py\", line 782, in main\n     rv = self.invoke(ctx)\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py\", line 1259, in invoke\n     return _process_result(sub_ctx.command.invoke(sub_ctx))\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py\", line 1066, in invoke\n     return ctx.invoke(self.callback, **ctx.params)\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/click/core.py\", line 610, in invoke\n     return callback(*args, **kwargs)\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/ray/scripts/scripts.py\", line 978, in memory\n     print(ray.internal.internal_api.memory_summary())\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/ray/internal/internal_api.py\", line 28, in memory_summary\n     node_manager_pb2.FormatGlobalMemoryInfoRequest(), timeout=30.0)\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/grpc/_channel.py\", line 826, in __call__\n     return _end_unary_response_blocking(state, call, False, None)\n   File \"/home/ubuntu/src/seeweed/ml/lib/python3.7/site-packages/grpc/_channel.py\", line 729, in _end_unary_response_blocking\n     raise _InactiveRpcError(state)\n grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n \tstatus = StatusCode.RESOURCE_EXHAUSTED\n \tdetails = \"Received message larger than max (28892999 vs. 4194304)\"\n \tdebug_error_string = \"{\"created\":\"@1589854413.712252174\",\"description\":\"Received message larger than max (28892999 vs. 4194304)\",\"file\":\"src/core/ext/filters/message_size/message_size_filter.cc\",\"file_line\":188,\"grpc_status\":8}\"\n >\n (pid=5771) E0519 02:13:33.880635  5771 plasma_store_provider.cc:108] Failed to put object 1f5c36abed661dbeffffffff010000c801000000 in object store because it is full. Object size is 196886 bytes.\n (pid=5771) Waiting 2000ms for space to free up...\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "edoakes", "commentT": "2020-05-19T15:53:18Z", "comment_text": "\n \t\tThis is probably related to the fact that the dashboard doesn't work well when it is running in a big cluster.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "edoakes", "commentT": "2020-05-19T20:22:54Z", "comment_text": "\n \t\tThis is hitting the internal gRPC message size limit. In C++ land we configure it to 100MB by default, but it looks like the python client does not have this setting set.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "edoakes", "commentT": "2020-05-22T18:27:12Z", "comment_text": "\n \t\tHi\n I have exactly the same error:\n <denchmark-code>INFO (unknown file):0 -- gc.collect() freed 10 refs in 0.11551751299975876 seconds \n INFO (unknown file):0 -- gc.collect() freed 10 refs in 0.11551751299975876 seconds\n ... etc\n </denchmark-code>\n \n Interestingly (maybe it might help you investigate):it happens only when number of workers is superior to number of CPUs. When number of workers is <= number of CPUs, it works fine\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "edoakes", "commentT": "2020-05-25T05:34:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n  I will set this P1 because it looks pretty important for anyone who uses big clutsers. Let's find the assignee in the next planning.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "edoakes", "commentT": "2020-08-28T00:30:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pitoupitou>@pitoupitou</denchmark-link>\n  Hi, are these gc.collect() messages normal behavior? I'm getting a lot of them, although my job is not erroring out.\n \t\t"}}}, "commit": {"commit_id": "2a9079aef99fcefe8eeddadd98da583970d39fa1", "commit_author": "Keqiu Hu", "commitT": "2020-12-08 09:36:53-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\includes\\ray_config.pxd", "file_new_name": "python\\ray\\includes\\ray_config.pxd", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "71,72", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\includes\\ray_config.pxi", "file_new_name": "python\\ray\\includes\\ray_config.pxi", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "122,123,124,125", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\internal\\internal_api.py", "file_new_name": "python\\ray\\internal\\internal_api.py", "file_complexity": {"file_NLOC": "44", "file_CCN": "8", "file_NToken": "279"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "27,28,29,30,31,32,33", "deleted_lines": "25", "method_info": {"method_name": "memory_summary", "method_params": "", "method_startline": "16", "method_endline": "37", "method_complexity": {"method_NLOC": "18", "method_CCN": "1", "method_NToken": "112", "method_nesting_level": "0"}}}}}}}}