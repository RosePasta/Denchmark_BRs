{"BR": {"BR_id": "10653", "BR_author": "mvindiola1", "BRopenT": "2020-09-08T19:13:53Z", "BRcloseT": "2020-09-21T03:01:52Z", "BR_text": {"BRsummary": "[rllib] DDPG TWIN_Q LOSS ERROR", "BRdescription": "\n The TD3 loss according to OpenAI Spinning up is:\n <denchmark-link:https://camo.githubusercontent.com/9c2dd76c2a0c8b8453802102917774b8c27a661cd14c0d7566fb4ec96da0bed5/68747470733a2f2f7370696e6e696e6775702e6f70656e61692e636f6d2f656e2f6c61746573742f5f696d616765732f6d6174682f376435633138663439613234326363336565633535346637313766653466336266633131396261622e737667></denchmark-link>\n \n <denchmark-link:https://camo.githubusercontent.com/9223fea845a0dc6073fe1d17a13c68b8ccb94271f1e2026403656274af557355/68747470733a2f2f7370696e6e696e6775702e6f70656e61692e636f6d2f656e2f6c61746573742f5f696d616765732f6d6174682f636437333732366138613338343561646534363761656435373731343931326638363866366233362e737667></denchmark-link>\n \n They implement this as you would expect:\n <denchmark-code># MSE loss against Bellman backup\n loss_q1 = ((q1 - backup)**2).mean()\n loss_q2 = ((q2 - backup)**2).mean()\n loss_q = loss_q1 + loss_q2\n </denchmark-code>\n \n The rllib implementation is off and I ran into training issues when using it.\n Tensorflow:\n \n \n \n ray/rllib/agents/ddpg/ddpg_tf_policy.py\n \n \n         Lines 174 to 184\n       in\n       39c598b\n \n \n \n \n \n \n  if twin_q: \n \n \n \n  td_error = q_t_selected - q_t_selected_target \n \n \n \n  twin_td_error = twin_q_t_selected - q_t_selected_target \n \n \n \n  td_error = td_error + twin_td_error \n \n \n \n  if use_huber: \n \n \n \n  errors = huber_loss(td_error, huber_threshold) + \\ \n \n \n \n  huber_loss(twin_td_error, huber_threshold) \n \n \n \n  else: \n \n \n \n  errors = 0.5 * tf.math.square(td_error) + \\ \n \n \n \n  0.5 * tf.math.square(twin_td_error) \n \n \n \n  else: \n \n \n \n \n \n Pytorch:\n \n \n \n ray/rllib/agents/ddpg/ddpg_torch_policy.py\n \n \n         Lines 124 to 134\n       in\n       5851e89\n \n \n \n \n \n \n  if twin_q: \n \n \n \n  td_error = q_t_selected - q_t_selected_target \n \n \n \n  twin_td_error = twin_q_t_selected - q_t_selected_target \n \n \n \n  td_error = td_error + twin_td_error \n \n \n \n  if use_huber: \n \n \n \n  errors = huber_loss(td_error, huber_threshold) \\ \n \n \n \n  + huber_loss(twin_td_error, huber_threshold) \n \n \n \n  else: \n \n \n \n  errors = 0.5 * \\ \n \n \n \n              (torch.pow(td_error, 2.0) + torch.pow(twin_td_error, 2.0)) \n \n \n \n  else: \n \n \n \n \n \n The error is on line 177 and 127 respectively where the td_error and twin_td_error are added together and then squared a couple lines later. Removing that line would bring the implementation in line with the spinningup reference implementation. This worked for me to improve training on my problem. They also do not multiply by the constant (0.5) but I would not expect that to make much of a difference either way.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mvindiola1", "commentT": "2020-09-11T19:16:29Z", "comment_text": "\n \t\tInteresting, do you want to push a fix. Also cc <denchmark-link:https://github.com/michaelzhiluo>@michaelzhiluo</denchmark-link>\n  if you could take a look here.\n \t\t"}}}, "commit": {"commit_id": "2b893d1bb5be8f8db87f732bb73c3f7cab425395", "commit_author": "mvindiola1", "commitT": "2020-09-20 20:01:51-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\ddpg\\ddpg_tf_policy.py", "file_new_name": "rllib\\agents\\ddpg\\ddpg_tf_policy.py", "file_complexity": {"file_NLOC": "330", "file_CCN": "49", "file_NToken": "2430"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "178", "method_info": {"method_name": "ddpg_actor_critic_loss", "method_params": "policy,model,_,train_batch", "method_startline": "99", "method_endline": "234", "method_complexity": {"method_NLOC": "102", "method_CCN": "15", "method_NToken": "762", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\ddpg\\ddpg_torch_policy.py", "file_new_name": "rllib\\agents\\ddpg\\ddpg_torch_policy.py", "file_complexity": {"file_NLOC": "194", "file_CCN": "29", "file_NToken": "1495"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "127", "method_info": {"method_name": "ddpg_actor_critic_loss", "method_params": "policy,model,_,train_batch", "method_startline": "31", "method_endline": "169", "method_complexity": {"method_NLOC": "98", "method_CCN": "14", "method_NToken": "775", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\ddpg\\tests\\test_ddpg.py", "file_new_name": "rllib\\agents\\ddpg\\tests\\test_ddpg.py", "file_complexity": {"file_NLOC": "413", "file_CCN": "59", "file_NToken": "2920"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "498"}}}}}}