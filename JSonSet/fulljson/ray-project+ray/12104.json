{"BR": {"BR_id": "12104", "BR_author": "richardliaw", "BRopenT": "2020-11-18T09:03:20Z", "BRcloseT": "2020-12-29T02:56:29Z", "BR_text": {"BRsummary": "[autoscaler] [docker] ray up on stopped node fails?", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n I ran ray up on a cluster that I had previously stopped.\n The ray up was unable to finish properly due to a tmp call.\n cc <denchmark-link:https://github.com/ijrsvt>@ijrsvt</denchmark-link>\n \n <denchmark-code>(base) \u279c  aws git:(docker-fix) \u2717 ray up example-full.yaml -y\n Cluster: default\n \n Loaded cached provider configuration\n If you experience issues with the cloud provider, try re-running the command with --no-config-cache.\n AWS config\n   IAM Profile: ray-autoscaler-v1 [default]\n   EC2 Key pair (head & workers): ray-autoscaler_2_us-west-2 [default]\n   VPC Subnets (head & workers): subnet-d26e76b6, subnet-503c0e26 [default]\n   EC2 Security groups (head & workers): sg-0e2ad6debfc8e0814 [default]\n   EC2 AMI (head & workers): ami-0a2363a9cff180a64\n \n No head node found. Launching a new cluster. Confirm [y/N]: y [automatic, due to --yes]\n \n Acquiring an up-to-date head node\n   Reusing nodes i-0c90dd53359e25223. To disable reuse, set `cache_stopped_nodes: False` under `provider` in the cluster configuration.\n   Stopping instances to reuse\n   Launched a new head node\n   Fetching the new head node\n \n <1/1> Setting up head node\n   Prepared bootstrap config\n   New status: waiting-for-ssh\n   [1/6] Waiting for SSH to become available\n     Running `uptime` as a test.\n     Waiting for IP\n       Not yet available, retrying in 10 seconds\n       Received: 35.165.135.138\n ssh: connect to host 35.165.135.138 port 22: Connection refused\n     SSH still not available SSH command failed., retrying in 5 seconds.\n Warning: Permanently added '35.165.135.138' (ECDSA) to the list of known hosts.\n  09:00:19 up 0 min,  1 user,  load average: 0.35, 0.08, 0.03\n Shared connection to 35.165.135.138 closed.\n     Success.\n Shared connection to 35.165.135.138 closed.\n latest-gpu: Pulling from rayproject/ray\n Digest: sha256:9e330168fbeface86427d29b4a1a996bdefca42fff38ff155a29c8d1d1020b74\n Status: Image is up to date for rayproject/ray:latest-gpu\n docker.io/rayproject/ray:latest-gpu\n Shared connection to 35.165.135.138 closed.\n Shared connection to 35.165.135.138 closed.\n Shared connection to 35.165.135.138 closed.\n Shared connection to 35.165.135.138 closed.\n NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n \n Shared connection to 35.165.135.138 closed.\n 2020-11-18 01:00:24,949\tWARNING command_runner.py:780 -- Nvidia Container Runtime is present, but no GPUs found.\n Shared connection to 35.165.135.138 closed.\n 9191520da0ddb9d6259795e5a758a298934bdab25ee153e60ddcd350318ed789\n Shared connection to 35.165.135.138 closed.\n Shared connection to 35.165.135.138 closed.\n lstat /tmp/ray_tmp_mount: no such file or directory\n Shared connection to 35.165.135.138 closed.\n   New status: update-failed\n   !!!\n   SSH command failed.\n   !!!\n \n   Failed to setup head node.\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "richardliaw", "commentT": "2020-11-18T09:21:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  can you rerun with something like \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "richardliaw", "commentT": "2020-11-18T11:25:25Z", "comment_text": "\n \t\tI get this error too <denchmark-link:https://github.com/ijrsvt>@ijrsvt</denchmark-link>\n  .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "richardliaw", "commentT": "2020-11-20T23:58:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  Follow up question, did you happen to edit your YAML between shutting down and restarting?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "richardliaw", "commentT": "2020-11-21T00:11:39Z", "comment_text": "\n \t\thmmm maybe i modified the filemounts\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "richardliaw", "commentT": "2020-11-23T18:37:18Z", "comment_text": "\n \t\tSame thing is happening for me. I did modify the setup_commands on the yaml between shutting down and restarting.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "richardliaw", "commentT": "2020-12-07T19:09:03Z", "comment_text": "\n \t\t+1\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "richardliaw", "commentT": "2020-12-14T19:30:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AmeerHajAli>@AmeerHajAli</denchmark-link>\n  <denchmark-link:https://github.com/ijrsvt>@ijrsvt</denchmark-link>\n  this sounds like a potential P0 issue, should we raise the priority?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "richardliaw", "commentT": "2020-12-14T19:41:45Z", "comment_text": "\n \t\tI'm having a related issue with DockerSyncer on sync_down:\n \n invalid output path: directory \"/tmp/ray_tmp_mount/...\n \n Interestingly earlier in the logs I see the directory being created:\n \n VINFO command_runner.py:474 -- Running `^[[1mmkdir -p /tmp/ray_tmp_mount/...\n \n Unfortunately I cannot create an issue as my code is proprietary and I likely won't have time to build a minimal example, but the issues are related at least with respect to the directory in question. I'm looking over the framework code now.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "richardliaw", "commentT": "2020-12-14T19:53:56Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AmeerHajAli>@AmeerHajAli</denchmark-link>\n  can you please take a look at this ASAP? Not sure if it should be release blocking.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "richardliaw", "commentT": "2020-12-15T12:22:16Z", "comment_text": "\n \t\tI think this is a release blocker.\n Basically, it is straightforward to reproduce:\n \n ray up\n ray down\n ray up -> fails because it can't locate /tmp/ray_tmp_mount\n @ijrsvt , I think the command runner is looking for the /tmp/ray_tmp_mount and fails because it assumes it is there in the cached node. But it seems like AWS cleans the /tmp directory on terminated nodes.\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "richardliaw", "commentT": "2020-12-15T23:19:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AmeerHajAli>@AmeerHajAli</denchmark-link>\n  This is not a new regression and I think that rushing this fix as a cherry pick is more risky than waiting for this fix to go through nightlies.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "richardliaw", "commentT": "2020-12-18T18:25:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ijrsvt>@ijrsvt</denchmark-link>\n  , I think it is very common for someone to run  after a . I understand it is not a regression. But I am not sure what is the workaround here? The user will have to change the cluster name or work very hard to make it ray up again. If this is not a good reason for blocking a release then I am fine with it, but at least we should have some documentation somewhere telling the user what to do when he faces this issue.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "richardliaw", "commentT": "2020-12-18T20:19:11Z", "comment_text": "\n \t\tHey, just to help clarify, I think there's two conversations happening -- a conversation around deadline and conversation around importance.\n <denchmark-link:https://github.com/AmeerHajAli>@AmeerHajAli</denchmark-link>\n  I think Ian and me and you absolutely agree that it is important to provide a workaround.\n I think Ian was just pushing back on the \"prioritization/deadline\" for this task. In terms of prioritization, I think it's ok to do it within the time frame of this sprint or even the sprint after (since everyone is going on holidays anyways).\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "richardliaw", "commentT": "2020-12-18T20:55:29Z", "comment_text": "\n \t\tI definitely agree that this is important, I think a workaround should be to set the following in provider:\n <denchmark-code>cache_stopped_nodes: False\n </denchmark-code>\n \n \t\t"}}}, "commit": {"commit_id": "7ad56826dbab8b718de47fbf2752ba0c81cbd843", "commit_author": "Ian Rodney", "commitT": "2020-12-28 18:56:28-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\ray\\autoscaler\\_private\\command_runner.py", "file_new_name": "python\\ray\\autoscaler\\_private\\command_runner.py", "file_complexity": {"file_NLOC": "682", "file_CCN": "109", "file_NToken": "4172"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,785,786,787,788,789,790,791,792,793,794,795", "deleted_lines": "721,743,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791", "method_info": {"method_name": "run_init", "method_params": "self,as_head,file_mounts", "method_startline": "721", "method_endline": "803", "method_complexity": {"method_NLOC": "71", "method_CCN": "13", "method_NToken": "448", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "721,722", "deleted_lines": "721", "method_info": {"method_name": "_check_if_container_restart_is_needed", "method_params": "self,str,str", "method_startline": "721", "method_endline": "722", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "20", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "763,785,786,787,788,789,790,791,792,793,794,795,815,820,821,822,823,824,833", "deleted_lines": "763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791", "method_info": {"method_name": "run_init", "method_params": "self,as_head,file_mounts,sync_run_yet", "method_startline": "763", "method_endline": "833", "method_complexity": {"method_NLOC": "55", "method_CCN": "12", "method_NToken": "363", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\autoscaler\\_private\\updater.py", "file_new_name": "python\\ray\\autoscaler\\_private\\updater.py", "file_complexity": {"file_NLOC": "381", "file_CCN": "54", "file_NToken": "2172"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "295,296,297,298,299,300,378,379", "deleted_lines": "295,296,374", "method_info": {"method_name": "do_update", "method_params": "self", "method_startline": "279", "method_endline": "452", "method_complexity": {"method_NLOC": "140", "method_CCN": "21", "method_NToken": "788", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\autoscaler\\command_runner.py", "file_new_name": "python\\ray\\autoscaler\\command_runner.py", "file_complexity": {"file_NLOC": "83", "file_CCN": "5", "file_NToken": "204"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "78,79", "deleted_lines": "78", "method_info": {"method_name": "run_init", "method_params": "self,bool,str,bool", "method_startline": "78", "method_endline": "79", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "78,79,85", "deleted_lines": "78", "method_info": {"method_name": "run_init", "method_params": "self,bool,str", "method_startline": "78", "method_endline": "85", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "24", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 9, "file_old_name": "python\\ray\\tests\\test_autoscaler.py", "file_new_name": "python\\ray\\tests\\test_autoscaler.py", "file_complexity": {"file_NLOC": "1694", "file_CCN": "173", "file_NToken": "10843"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1577,1580,1637,1638,1639,1640,1641,1642,1643", "deleted_lines": null, "method_info": {"method_name": "testSetupCommandsWithStoppedNodeCachingNoDocker", "method_params": "self", "method_startline": "1577", "method_endline": "1643", "method_complexity": {"method_NLOC": "59", "method_CCN": "2", "method_NToken": "393", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "1778,1779,1808,1809", "deleted_lines": null, "method_info": {"method_name": "testContinuousFileMounts", "method_params": "self", "method_startline": "1766", "method_endline": "1823", "method_complexity": {"method_NLOC": "52", "method_CCN": "7", "method_NToken": "350", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497", "deleted_lines": null, "method_info": {"method_name": "testGetOrCreateHeadNodeFromStopped", "method_params": "self", "method_startline": "433", "method_endline": "497", "method_complexity": {"method_NLOC": "56", "method_CCN": "14", "method_NToken": "369", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595", "deleted_lines": null, "method_info": {"method_name": "testDockerFileMountsRemoved", "method_params": "self", "method_startline": "548", "method_endline": "595", "method_complexity": {"method_NLOC": "45", "method_CCN": "1", "method_NToken": "259", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "400,402", "deleted_lines": "399,401", "method_info": {"method_name": "testGetOrCreateHeadNode", "method_params": "self", "method_startline": "395", "method_endline": "430", "method_complexity": {"method_NLOC": "34", "method_CCN": "1", "method_NToken": "200", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "1720,1721", "deleted_lines": null, "method_info": {"method_name": "testMultiNodeReuse", "method_params": "self", "method_startline": "1718", "method_endline": "1763", "method_complexity": {"method_NLOC": "42", "method_CCN": "3", "method_NToken": "290", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546", "deleted_lines": null, "method_info": {"method_name": "testDockerFileMountsAdded", "method_params": "self", "method_startline": "500", "method_endline": "546", "method_complexity": {"method_NLOC": "45", "method_CCN": "1", "method_NToken": "262", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1411,1439,1454", "method_info": {"method_name": "testSetupCommandsWithStoppedNodeCaching", "method_params": "self", "method_startline": "1411", "method_endline": "1479", "method_complexity": {"method_NLOC": "61", "method_CCN": "2", "method_NToken": "412", "method_nesting_level": "1"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708", "deleted_lines": null, "method_info": {"method_name": "testSetupCommandsWithStoppedNodeCachingDocker", "method_params": "self", "method_startline": "1645", "method_endline": "1716", "method_complexity": {"method_NLOC": "61", "method_CCN": "2", "method_NToken": "412", "method_nesting_level": "1"}}}}}}}}