{"BR": {"BR_id": "12881", "BR_author": "AmeerHajAli", "BRopenT": "2020-12-15T16:55:43Z", "BRcloseT": "2020-12-18T09:32:13Z", "BR_text": {"BRsummary": "[K8s] Make sure autoscaler detects GPUs", "BRdescription": "\n The cluster launcher is not properly detection GPUs in Kubernetes.\n CC <denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  , Can you please provide reproduction script here?\n Please feel free to change the priority here.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "AmeerHajAli", "commentT": "2020-12-15T16:59:21Z", "comment_text": "\n \t\tHere you go:\n <denchmark-code># An unique identifier for the head node and workers of this cluster.\n cluster_name: default\n \n # The minimum number of workers nodes to launch in addition to the head\n # node. This number should be >= 0.\n min_workers: 3\n \n # The maximum number of workers nodes to launch in addition to the head\n # node. This takes precedence over min_workers.\n max_workers: 3\n \n # The autoscaler will scale up the cluster faster with higher upscaling speed.\n # E.g., if the task requires adding more nodes then autoscaler will gradually\n # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.\n # This number should be > 0.\n upscaling_speed: 1.0\n \n # If a node is idle for this many minutes, it will be removed.\n idle_timeout_minutes: 5\n \n # Kubernetes resources that need to be configured for the autoscaler to be\n # able to manage the Ray cluster. If any of the provided resources don't\n # exist, the autoscaler will attempt to create them. If this fails, you may\n # not have the required permissions and will have to request them to be\n # created by your cluster administrator.\n provider:\n     type: kubernetes\n \n     # Exposing external IP addresses for ray pods isn't currently supported.\n     use_internal_ips: true\n \n     # Namespace to use for all resources created.\n     namespace: ray\n \n     # ServiceAccount created by the autoscaler for the head node pod that it\n     # runs in. If this field isn't provided, the head pod config below must\n     # contain a user-created service account with the proper permissions.\n     autoscaler_service_account:\n         apiVersion: v1\n         kind: ServiceAccount\n         metadata:\n             name: autoscaler\n \n     # Role created by the autoscaler for the head node pod that it runs in.\n     # If this field isn't provided, the role referenced in\n     # autoscaler_role_binding must exist and have at least these permissions.\n     autoscaler_role:\n         kind: Role\n         apiVersion: rbac.authorization.k8s.io/v1\n         metadata:\n             name: autoscaler\n         rules:\n         - apiGroups: [\"\"]\n           resources: [\"pods\", \"pods/status\", \"pods/exec\"]\n           verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\", \"patch\"]\n \n     # RoleBinding created by the autoscaler for the head node pod that it runs\n     # in. If this field isn't provided, the head pod config below must contain\n     # a user-created service account with the proper permissions.\n     autoscaler_role_binding:\n         apiVersion: rbac.authorization.k8s.io/v1\n         kind: RoleBinding\n         metadata:\n             name: autoscaler\n         subjects:\n         - kind: ServiceAccount\n           name: autoscaler\n         roleRef:\n             kind: Role\n             name: autoscaler\n             apiGroup: rbac.authorization.k8s.io\n \n     services:\n       # Service that maps to the head node of the Ray cluster.\n       - apiVersion: v1\n         kind: Service\n         metadata:\n             # NOTE: If you're running multiple Ray clusters with services\n             # on one Kubernetes cluster, they must have unique service\n             # names.\n             name: ray-head\n         spec:\n             # This selector must match the head node pod's selector below.\n             selector:\n                 component: ray-head\n             ports:\n                 - protocol: TCP\n                   port: 8000\n                   targetPort: 8000\n \n       # Service that maps to the worker nodes of the Ray cluster.\n       - apiVersion: v1\n         kind: Service\n         metadata:\n             # NOTE: If you're running multiple Ray clusters with services\n             # on one Kubernetes cluster, they must have unique service\n             # names.\n             name: ray-workers\n         spec:\n             # This selector must match the worker node pods' selector below.\n             selector:\n                 component: ray-worker\n             ports:\n                 - protocol: TCP\n                   port: 8000\n                   targetPort: 8000\n \n # Kubernetes pod config for the head node pod.\n head_node:\n     apiVersion: v1\n     kind: Pod\n     metadata:\n         # Automatically generates a name for the pod with this prefix.\n         generateName: ray-head-\n \n         # Must match the head node service selector above if a head node\n         # service is required.\n         labels:\n             component: ray-head\n     spec:\n         # Change this if you altered the autoscaler_service_account above\n         # or want to provide your own.\n         serviceAccountName: autoscaler\n         tolerations:\n            - effect: NoSchedule\n              key: nvidia.com/gpu\n              operator: Exists\n         nodeSelector:\n           cloud.google.com/gke-nodepool: pool-1\n \n \n         # Restarting the head node automatically is not currently supported.\n         # If the head node goes down, `ray up` must be run again.\n         restartPolicy: Never\n \n         # This volume allocates shared memory for Ray to use for its plasma\n         # object store. If you do not provide this, Ray will fall back to\n         # /tmp which cause slowdowns if is not a shared memory volume.\n         volumes:\n         - name: dshm\n           emptyDir:\n               medium: Memory\n \n         containers:\n         - name: ray-node\n           imagePullPolicy: Always\n           # You are free (and encouraged) to use your own container image,\n           # but it should have the following installed:\n           #   - rsync (used for `ray rsync` commands and file mounts)\n           #   - screen (used for `ray attach`)\n           #   - kubectl (used by the autoscaler to manage worker pods)\n           image: rayproject/ray-ml:nightly-gpu\n           # Do not change this command - it keeps the pod alive until it is\n           # explicitly killed.\n           command: [\"/bin/bash\", \"-c\", \"--\"]\n           args: [\"trap : TERM INT; sleep infinity & wait;\"]\n           ports:\n               - containerPort: 6379 # Redis port.\n               - containerPort: 6380 # Redis port.\n               - containerPort: 6381 # Redis port.\n               - containerPort: 12345 # Ray internal communication.\n               - containerPort: 12346 # Ray internal communication.\n \n           # This volume allocates shared memory for Ray to use for its plasma\n           # object store. If you do not provide this, Ray will fall back to\n           # /tmp which cause slowdowns if is not a shared memory volume.\n           volumeMounts:\n               - mountPath: /dev/shm\n                 name: dshm\n           resources:\n               requests:\n                   cpu: 1000m\n                   memory: 512Mi\n                   # nvidia.com/gpu: 1\n               limits:\n                   # The maximum memory that this pod is allowed to use. The\n                   # limit will be detected by ray and split to use 10% for\n                   # redis, 30% for the shared memory object store, and the\n                   # rest for application memory. If this limit is not set and\n                   # the object store size is not set manually, ray will\n                   # allocate a very large object store in each pod that may\n                   # cause problems for other pods.\n                   memory: 2Gi\n                   nvidia.com/gpu: 1\n           env:\n               # This is used in the head_start_ray_commands below so that\n               # Ray can spawn the correct number of processes. Omitting this\n               # may lead to degraded performance.\n               - name: MY_CPU_REQUEST\n                 valueFrom:\n                     resourceFieldRef:\n                         resource: requests.cpu\n \n # Kubernetes pod config for worker node pods.\n worker_nodes:\n     apiVersion: v1\n     kind: Pod\n     metadata:\n         # Automatically generates a name for the pod with this prefix.\n         generateName: ray-worker-\n \n         # Must match the worker node service selector above if a worker node\n         # service is required.\n         labels:\n             component: ray-worker\n     spec:\n         serviceAccountName: default\n         tolerations:\n            - effect: NoSchedule\n              key: nvidia.com/gpu\n              operator: Exists\n         nodeSelector:\n           cloud.google.com/gke-nodepool: pool-1\n \n         # Worker nodes will be managed automatically by the head node, so\n         # do not change the restart policy.\n         restartPolicy: Never\n \n         # This volume allocates shared memory for Ray to use for its plasma\n         # object store. If you do not provide this, Ray will fall back to\n         # /tmp which cause slowdowns if is not a shared memory volume.\n         volumes:\n         - name: dshm\n           emptyDir:\n               medium: Memory\n \n         containers:\n         - name: ray-node\n           imagePullPolicy: Always\n           # You are free (and encouraged) to use your own container image,\n           # but it should have the following installed:\n           #   - rsync (used for `ray rsync` commands and file mounts)\n           image: rayproject/ray-ml:nightly-gpu\n           # Do not change this command - it keeps the pod alive until it is\n           # explicitly killed.\n           command: [\"/bin/bash\", \"-c\", \"--\"]\n           args: [\"trap : TERM INT; sleep infinity & wait;\"]\n           ports:\n               - containerPort: 12345 # Ray internal communication.\n               - containerPort: 12346 # Ray internal communication.\n \n           # This volume allocates shared memory for Ray to use for its plasma\n           # object store. If you do not provide this, Ray will fall back to\n           # /tmp which cause slowdowns if is not a shared memory volume.\n           volumeMounts:\n               - mountPath: /dev/shm\n                 name: dshm\n           resources:\n               requests:\n                   cpu: 1000m\n                   memory: 512Mi\n                   # nvidia.com/gpu: 1\n               limits:\n                   # This memory limit will be detected by ray and split into\n                   # 30% for plasma, and 70% for workers.\n                   memory: 2Gi\n                   nvidia.com/gpu: 1\n           env:\n               # This is used in the head_start_ray_commands below so that\n               # Ray can spawn the correct number of processes. Omitting this\n               # may lead to degraded performance.\n               - name: MY_CPU_REQUEST\n                 valueFrom:\n                     resourceFieldRef:\n                         resource: requests.cpu\n \n # Files or directories to copy to the head and worker nodes. The format is a\n # dictionary from REMOTE_PATH: LOCAL_PATH, e.g.\n file_mounts: {\n #    \"/path1/on/remote/machine\": \"/path1/on/local/machine\",\n #    \"/path2/on/remote/machine\": \"/path2/on/local/machine\",\n }\n \n # Files or directories to copy from the head node to the worker nodes. The format is a\n # list of paths. The same path on the head node will be copied to the worker node.\n # This behavior is a subset of the file_mounts behavior. In the vast majority of cases\n # you should just use file_mounts. Only use this if you know what you're doing!\n cluster_synced_files: []\n \n # Whether changes to directories in file_mounts or cluster_synced_files in the head node\n # should sync to the worker node continuously\n file_mounts_sync_continuously: False\n \n # Patterns for files to exclude when running rsync up or rsync down.\n # This is not supported on kubernetes.\n # rsync_exclude: []\n \n # Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for\n # in the source directory and recursively through all subdirectories. For example, if .gitignore is provided\n # as a value, the behavior will match git's behavior for finding and using .gitignore files.\n # This is not supported on kubernetes.\n # rsync_filter: []\n \n # List of commands that will be run before `setup_commands`. If docker is\n # enabled, these commands will run outside the container and before docker\n # is setup.\n initialization_commands: []\n \n # List of shell commands to run to set up nodes.\n setup_commands: []\n    # - pip install\n \n # Custom commands that will be run on the head node after common setup.\n head_setup_commands: []\n \n # Custom commands that will be run on worker nodes after common setup.\n worker_setup_commands: []\n \n # Command to start ray on the head node. You don't need to change this.\n # Note webui-host is set to 0.0.0.0 so that kubernetes can port forward.\n head_start_ray_commands:\n     - ray stop\n     - ulimit -n 65536; RAY_OVERRIDE_RESOURCES='{\"CPU\":1,\"GPU\":1}' ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host 0.0.0.0\n \n # Command to start ray on worker nodes. You don't need to change this.\n worker_start_ray_commands:\n     - ray stop\n     - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n \n </denchmark-code>\n \n You'll want to start a GPU cluster (on say Google Kubernetes Engine).\n \t\t"}}}, "commit": {"commit_id": "bff50cfc37380140d53debfb140e262a38668db0", "commit_author": "Gekho457", "commitT": "2020-12-18 01:32:12-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\_private\\commands.py", "file_new_name": "python\\ray\\autoscaler\\_private\\commands.py", "file_complexity": {"file_NLOC": "896", "file_CCN": "45", "file_NToken": "5011"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "283,284,285,286", "deleted_lines": "283,284,285"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "python\\ray\\autoscaler\\_private\\kubernetes\\config.py", "file_new_name": "python\\ray\\autoscaler\\_private\\kubernetes\\config.py", "file_complexity": {"file_NLOC": "212", "file_CCN": "56", "file_NToken": "1528"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,134,135,136,137,138,139,140", "deleted_lines": null, "method_info": {"method_name": "_get_resource", "method_params": "container_resources,resource_name,field_name", "method_startline": "111", "method_endline": "140", "method_complexity": {"method_NLOC": "12", "method_CCN": "6", "method_NToken": "89", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "146", "deleted_lines": null, "method_info": {"method_name": "_parse_resource", "method_params": "resource", "method_startline": "143", "method_endline": "149", "method_complexity": {"method_NLOC": "6", "method_CCN": "2", "method_NToken": "44", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "63,64,65,66,67,68,69", "deleted_lines": null, "method_info": {"method_name": "fillout_resources_kubernetes", "method_params": "config", "method_startline": "62", "method_endline": "84", "method_complexity": {"method_NLOC": "16", "method_CCN": "4", "method_NToken": "112", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "106,107", "deleted_lines": "103,104,105,106", "method_info": {"method_name": "get_resource", "method_params": "container_resources,resource_name", "method_startline": "100", "method_endline": "108", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "53", "method_nesting_level": "0"}}}}}}}}