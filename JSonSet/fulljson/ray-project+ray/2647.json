{"BR": {"BR_id": "2647", "BR_author": "whikwon", "BRopenT": "2018-08-14T04:10:52Z", "BRcloseT": "2018-08-17T01:04:32Z", "BR_text": {"BRsummary": "[rllib] Ape-X multiple VMs training issue", "BRdescription": "\n <denchmark-h:h3>System information</denchmark-h>\n \n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\n Ray installed from (source or binary): source\n Ray version: 0.5.0\n Python version: 3.6.3\n Exact command to reproduce:\n \n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n I made a cluster using 3 Azure VMs and ran the basic code of Ape-X DDPG.\n As soon as I execute the code, All CPUs start and stop immediately with no errors.\n Actually, when I ran the same code in one VM, it worked.\n <denchmark-h:h3>Source code / logs</denchmark-h>\n \n import ray\n import ray.rllib.agents.ddpg as ddpg\n \n ray.init(<head-ip:port>)\n agent = ddpg.ApexDDPGAgent(config={'num_workers': 145,}, env='Pendulum-v0')\n \n for i in range(1000):\n     result = agent.train()\n     print('result: {}'.format(result))\n \n     if i % 100 == 0:\n         checkpoint = agent.save()\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "whikwon", "commentT": "2018-08-14T04:25:37Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/whikwon>@whikwon</denchmark-link>\n ,\n Can you verify that the ray cluster has actually been created? You can do this by posting the output of ray.global_state.client_table().\n Also, can you clarify by what you mean when the CPUs start and stop?\n This would help debug the issue. Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "whikwon", "commentT": "2018-08-14T04:42:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  Thanks for the quick response.\n I think cluster has been created. (picture below)\n <denchmark-link:https://user-images.githubusercontent.com/30768596/44072013-68fe7248-9fc7-11e8-977e-6273fe661a40.png></denchmark-link>\n \n I'm monitoring cpu usage using  top in console. when i execute the code above, all cpu go to 100% in a second and being dead.\n Anyway, all cpu don't work at all but code i executed doesn't generate any error.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "whikwon", "commentT": "2018-08-14T04:50:41Z", "comment_text": "\n \t\tNo problem - does the code simply return? Or does it hang?\n It's possible that it's starting and waiting for actors (which may take some time initially).\n Also, it would be good to set OMP_NUM_THREADS as an environment variable, or else you could hit some OS thread limit.\n One place to check for errors is to tail /tmp/raylogs/worker*.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "whikwon", "commentT": "2018-08-14T05:01:27Z", "comment_text": "\n \t\tJust hang. I thought it would take a while but status doesn't change.\n <denchmark-link:https://user-images.githubusercontent.com/30768596/44072525-4c76660a-9fca-11e8-9e70-bb0af21dd9f2.png></denchmark-link>\n \n where should i have to give the OMP_NUM_THREADS?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "whikwon", "commentT": "2018-08-14T05:05:24Z", "comment_text": "\n \t\tPerhaps run something like `export OMP_NUM_THREADS=2` on each machine\n before initializing Ray (or after ray stop and initializing Ray again).\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Mon, Aug 13, 2018 at 10:01 PM Whi Kwon ***@***.***> wrote:\n  Just hang. I thought it would take a while but status doesn't change.\n \n  [image: image]\n  <https://user-images.githubusercontent.com/30768596/44072525-4c76660a-9fca-11e8-9e70-bb0af21dd9f2.png>\n \n  where should i have to give the OMP_NUM_THREADS?\n \n  \u2014\n  You are receiving this because you were mentioned.\n  Reply to this email directly, view it on GitHub\n  <#2647 (comment)>,\n  or mute the thread\n  <https://github.com/notifications/unsubscribe-auth/AEUc5R2Uxy8s-6KkbBKO3Q_CNMNf7xhxks5uQlmpgaJpZM4V7wzd>\n  .\n \n \n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "whikwon", "commentT": "2018-08-14T05:12:16Z", "comment_text": "\n \t\tThanks. I tried but it doesn't work. OMG\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "whikwon", "commentT": "2018-08-14T19:59:39Z", "comment_text": "\n \t\tSorry to hear. What are the contents of the worker logs? tail /tmp/raylogs/worker*\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "whikwon", "commentT": "2018-08-14T21:56:35Z", "comment_text": "\n \t\tIt is possible you are running into this issue: <denchmark-link:https://github.com/ray-project/ray/issues/2541>#2541</denchmark-link>\n \n One work around is to avoid launching that many workers, so if you have N virtual CPUs, use N/2 workers to avoid completely filling up the cluster. I usually find no performance benefit from using more than N/2 workers anyways, since model infererence is very floating point intensive and doesn't benefit from hyperthreading.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "whikwon", "commentT": "2018-08-15T05:12:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  Nothing special, tensorflow and gym warning.. something like that.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "whikwon", "commentT": "2018-08-15T05:13:55Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n  i had no problem running N virtual CPUs in one Azure Virtual Machine.\n Now, I'm using M virtual and tried M*N/2 workers but same situation happens. (No warning, No CPU usage. Just hanging.)\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "whikwon", "commentT": "2018-08-15T07:01:15Z", "comment_text": "\n \t\tDo you get the hang with a very small number of workers, similar or less than that for a single machine?\n Trying to figure out if this is a multi node issue.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "whikwon", "commentT": "2018-08-15T07:17:08Z", "comment_text": "\n \t\tOh btw, I would also switch to running the agent with Tune (see rllib training api doc for usage). That usually takes care of subtle resource allocation issues, and will print out the current usage as well.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "whikwon", "commentT": "2018-08-15T20:41:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ray-project/ray/pull/2661>#2661</denchmark-link>\n \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "whikwon", "commentT": "2018-08-17T01:04:32Z", "comment_text": "\n \t\tShould be fixed -- feel free to reopen if it happens still.\n \t\t"}}}, "commit": {"commit_id": "6670880f03bc6856342b6304cbf72fe4a8d78e4c", "commit_author": "Eric Liang", "commitT": "2018-08-16 18:03:50-07:00", "commit_complexity": {"commit_NLOC": "0.8421052631578947", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\rllib\\agents\\dqn\\dqn.py", "file_new_name": "python\\ray\\rllib\\agents\\dqn\\dqn.py", "file_complexity": {"file_NLOC": "174", "file_CCN": "26", "file_NToken": "1184"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "140,141,142,143,144,145,146,147,148,149,150,151,152,153,157,158,159,160", "deleted_lines": "140,141,142,143,144", "method_info": {"method_name": "_init", "method_params": "self", "method_startline": "122", "method_endline": "163", "method_complexity": {"method_NLOC": "27", "method_CCN": "6", "method_NToken": "205", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "141,142,143,144,145,146,147", "deleted_lines": "141,142,143,144", "method_info": {"method_name": "_init.create_remote_evaluators", "method_params": "", "method_startline": "141", "method_endline": "147", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\rllib\\optimizers\\async_replay_optimizer.py", "file_new_name": "python\\ray\\rllib\\optimizers\\async_replay_optimizer.py", "file_complexity": {"file_NLOC": "247", "file_CCN": "31", "file_NToken": "1618"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "215", "deleted_lines": null, "method_info": {"method_name": "step", "method_params": "self", "method_startline": "214", "method_endline": "227", "method_complexity": {"method_NLOC": "14", "method_CCN": "3", "method_NToken": "108", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "205,206", "deleted_lines": null, "method_info": {"method_name": "set_evaluators", "method_params": "self,remote_evaluators", "method_startline": "205", "method_endline": "212", "method_complexity": {"method_NLOC": "8", "method_CCN": "3", "method_NToken": "68", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "test\\jenkins_tests\\run_multi_node_tests.sh", "file_new_name": "test\\jenkins_tests\\run_multi_node_tests.sh", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "274,275,276,277,278,279,280,281,282,283,284,285,286,287,319", "deleted_lines": "117,118,119,120,121,122,123,124,125,126,127,128,129,130,319"}}}}}}