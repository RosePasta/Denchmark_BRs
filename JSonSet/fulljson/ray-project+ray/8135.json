{"BR": {"BR_id": "8135", "BR_author": "yskim5892", "BRopenT": "2020-04-22T19:18:16Z", "BRcloseT": "2020-05-04T09:25:13Z", "BR_text": {"BRsummary": "[RLlib] Model with continuous action space outputs inf", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n \n Python 3.7.5\n Ray : from commit d66d126\n OS : Linux (Ubuntu 18.04)\n \n I've made a custom environment which implements MultiAgentEnv, custom trainer similar to DDPGTrainer, and a custom optimizer.\n In this setting, high level agent's action space is Box(24), which is same as the observation space.\n When I try to train with these, high level agent just outputs action full of infs(after random-action timesteps for exploration). I've added print lines for debugging, so you can easily check it.\n I think this problem is not related with my custom replay buffer or custom optimizer, because the policy is just simply outputting action full of infs(as you can check from prints).  It might be related with the multiagent pipeline, because this problem didn't happen when I ran single DDPGTrainer on a simple environment with continuous action space.\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n If we cannot run your script, we cannot fix your issue.\n <denchmark-code>import ray\n from ray import tune\n from ray.tune import function\n from ray.rllib.env import MultiAgentEnv\n \n from ray.rllib.evaluation.metrics import get_learner_stats\n from ray.rllib.optimizers.policy_optimizer import PolicyOptimizer\n from ray.rllib.optimizers.replay_buffer import ReplayBuffer\n from ray.rllib.utils.annotations import override\n from ray.rllib.utils.timer import TimerStat\n from ray.rllib.utils.memory import ray_get_and_free\n \n from ray.rllib.agents.ddpg.ddpg import DDPGTrainer, DEFAULT_CONFIG\n from ray.rllib.agents.ddpg.ddpg_tf_policy import DDPGTFPolicy\n from ray.rllib.policy.sample_batch import SampleBatch, MultiAgentBatch\n from ray.rllib.utils.compression import pack_if_needed, unpack_if_needed\n \n import numpy as np\n import gym\n import random\n import collections\n \n class MultiStepReplayBuffer(ReplayBuffer):\n     def __init__(self, size, sub_replay_buffer):\n         ReplayBuffer.__init__(self, size)\n         # storage: List of (obs, action, reward, next_obs, done, start_idx, steps)\n         self.sub = sub_replay_buffer\n         self._next_subidx = 0\n         \n     def add(self, obs, action, reward, next_obs, done, steps):\n         assert steps <= self._maxsize\n         assert (self._next_subidx + steps) % self._maxsize == self.sub._next_idx\n         data = (obs, action, reward, next_obs, done, self._next_subidx, steps)\n         self._num_added += 1\n         \n         a = self._next_subidx\n         b = self._next_subidx + steps\n \n         overlapped = True\n         while len(self._storage) >= 1 and overlapped:\n             overlapped = False\n             c = self._storage[0][5]\n             d = c + self._storage[0][6]\n             if b < self._maxsize and c < b and a < d:\n                 overlapped = True\n             elif b >= self._maxsize:\n                 b = b % self._maxsize\n                 if c < b or a < d:\n                     overlapped = True\n             if overlapped:\n                 self._eviction_started = True\n                 self._storage.pop(0)\n     \n         self._storage.append(data)\n         self._next_subidx = b\n \n     def _encode_sample(self, idxes):\n         obses, actions, rewards, next_obses, dones, l_obs_seqs, l_action_seqs, l_reward_seqs = [], [], [], [], [], [], [], []\n         for i in idxes:\n             obs, action, reward, next_obs, done, subidx, step = self._storage[i]\n             obses.append(np.array(unpack_if_needed(obs), copy=False))\n             actions.append(np.array(action, copy=False))\n             rewards.append(reward)\n             next_obses.append(np.array(unpack_if_needed(next_obs), copy=False))\n             dones.append(done)\n             l_obs_seq, l_action_seq, l_reward_seq = [], [], []\n             for j in range(subidx, subidx + step):\n                 j = j % self._maxsize\n                 l_obs, l_action, l_reward, _, _ = self.sub._storage[j]\n                 l_obs_seq.append(np.array(unpack_if_needed(l_obs), copy=False))\n                 l_action_seq.append(np.array(l_action, copy=False))\n                 l_reward_seq.append(l_reward)\n             l_obs_seqs.append(l_obs_seq)\n             l_action_seqs.append(l_action_seq)\n             l_reward_seqs.append(l_reward_seq)\n             self._hit_count[i] += 1\n         return np.array(obses), np.array(actions), np.array(rewards), np.array(next_obses), np.array(dones), np.array(l_obs_seqs), np.array(l_action_seqs), np.array(l_reward_seqs)\n \n \n class HIRO_Optimizer(PolicyOptimizer):\n     def __init__(self, workers, learning_starts=1000, buffer_size=10000, train_batch_size=32, before_learn_on_batch=None):\n         PolicyOptimizer.__init__(self, workers)\n         self.replay_starts = learning_starts\n         self.max_buffer_size = buffer_size\n         self.train_batch_size = train_batch_size\n         self.before_learn_on_batch = before_learn_on_batch\n \n         assert self.max_buffer_size >= self.replay_starts\n \n         def new_buffer():\n             return ReplayBuffer(buffer_size)\n \n         self.replay_buffers = {'l' : ReplayBuffer(buffer_size)}\n         self.replay_buffers['h'] = MultiStepReplayBuffer(buffer_size, self.replay_buffers['l'])\n         \n         self.buffer_size = 0\n         self.l_steps = 0\n \n         self.sample_timer = TimerStat()\n         self.replay_timer = TimerStat()\n         self.grad_timer = TimerStat()\n         \n         self.learner_stats = {}\n \n     @override(PolicyOptimizer)\n     def step(self):\n         with self.sample_timer:\n             batch = self.workers.local_worker().sample()\n \n             for policy_id, s in batch.policy_batches.items():\n                 for row in s.rows():\n                     if policy_id == 'h':\n                         self.replay_buffers[policy_id].add(\n                             pack_if_needed(row['obs']),\n                             row['actions'],\n                             row['rewards'],\n                             pack_if_needed(row['new_obs']),\n                             row['dones'],\n                             self.l_steps)\n                         self.l_steps = 0\n                     if policy_id == 'l':\n                         self.replay_buffers[policy_id].add(\n                             pack_if_needed(row['obs']),\n                             row['actions'],\n                             row['rewards'],\n                             pack_if_needed(row['new_obs']),\n                             row['dones'],\n                             weight=None)\n                         self.l_steps += 1\n         \n         if self.num_steps_sampled >= self.replay_starts:\n             self._optimize()\n         \n         self.num_steps_sampled += batch.count\n         print(self.num_steps_sampled)\n \n     @override(PolicyOptimizer)\n     def stats(self):\n         return dict(\n             PolicyOptimizer.stats(self), **{\n                 'sample_time_ms': round(1000 * self.sample_timer.mean, 3),\n                 'replay_time_ms': round(1000 * self.replay_timer.mean, 3),\n                 'grad_time_ms': round(1000 * self.grad_timer.mean, 3),\n                 'opt_peak_throughput': round(self.grad_timer.mean_throughput, 3),\n                 'opt_samples': round(self.grad_timer.mean_units_processed, 3),\n                 'learner': self.learner_stats,\n             })\n \n     def _optimize(self):\n         samples = self._replay()\n \n         with self.grad_timer:\n             if self.before_learn_on_batch:\n                 samples = self.before_learn_on_batch(samples, self.workers.local_worker().policy_map, self.train_batch_size)\n             info_dict = self.workers.local_worker().learn_on_batch(samples)\n             for policy_id, info in info_dict.items():\n                 self.learner_stats[policy_id] = get_learner_stats(info)\n             self.grad_timer.push_units_processed(samples.count)\n \n         self.num_steps_trained += samples.count\n \n     def _replay(self):\n         samples = {}\n         idxes = None\n         with self.replay_timer:\n             for policy_id, replay_buffer in self.replay_buffers.items():\n                 idxes = replay_buffer.sample_idxes(self.train_batch_size)\n \n                 if policy_id == 'l':\n                     (obses, actions, rewards, next_obses, dones) = replay_buffer.sample_with_idxes(idxes)\n                     weights, batch_indexes = np.ones_like(rewards), -np.ones_like(rewards)\n \n                     samples[policy_id] = SampleBatch({\n                         'obs': obses,\n                         'actions': actions,\n                         'rewards': rewards,\n                         'new_obs': next_obses,\n                         'dones': dones,\n                         'weights': weights,\n                         'batch_indexes': batch_indexes\n                     })\n \n                 elif policy_id == 'h':\n                     (obses, actions, rewards, next_obses, dones, l_obs_seqs, l_action_seqs, l_reward_seqs) = replay_buffer.sample_with_idxes(idxes)\n                     weights, batch_indexes = np.ones_like(rewards), -np.ones_like(rewards)\n                     samples[policy_id] = SampleBatch({\n                         'obs': obses,\n                         'actions': actions,\n                         'rewards': rewards,\n                         'new_obs': next_obses,\n                         'dones': dones,\n                         'weights': weights,\n                         'batch_indexes': batch_indexes,\n                         'l_obs_seqs': l_obs_seqs,\n                         'l_action_seqs': l_action_seqs,\n                         'l_reward_seqs': l_reward_seqs\n                     })\n         return MultiAgentBatch(samples, self.train_batch_size)\n \n \n def make_policy_optimizer(workers, config):\n     return HIRO_Optimizer(    \n         workers,\n         learning_starts=config['learning_starts'],\n         buffer_size=config['buffer_size'],\n         train_batch_size=config['train_batch_size'])\n \n HIROTrainer = DDPGTrainer.with_updates(\n     name=\"HIRO\",\n     default_config=DEFAULT_CONFIG,\n     make_policy_optimizer=make_policy_optimizer\n )\n \n \n class HIRO_env_wrapper(MultiAgentEnv):\n     def __init__(self, config):\n         try:\n             self.env = config['flat_env']\n             self.observation_space = self.env.observation_space\n             self.action_space = self.env.action_space\n         except AttributeError:\n             self.env = gym.make(config['flat_env'])\n             self.observation_space = self.env.observation_space\n             self.action_space = self.env.action_space\n         self.config = config\n \n     def reset(self):\n         self.state = self.env.reset()\n         self.goal = self.state\n         self.l_steps = 0\n \n         return {'h': self.state, 'l': np.concatenate((self.state, self.goal))}\n \n     def step(self, action_dict):\n         obs, rew, done, info = {}, {}, {'__all__' : False}, {'l' : {}}\n \n         if 'h' in action_dict:\n             action = action_dict['h']\n             self.goal = action\n             self.h_reward = 0\n             self.l_reward = 0\n             self.l_steps = 0\n \n         if 'l' in action_dict:\n             action = action_dict['l']\n             self.l_steps += 1\n             next_state, ext_reward, f_done, _ = self.env.step(action)\n \n             self.h_reward += ext_reward\n             self.goal = self.state + self.goal - next_state\n             self.l_reward = -(np.dot(self.goal, self.goal)) ** 0.5\n \n             self.state = next_state\n \n             if f_done:\n                 done = {'l' : True, 'h' : True, '__all__': True}\n                 obs['h'] = self.state\n                 rew['h'] = self.h_reward\n                 info['h'] = {}\n \n             elif self.l_steps >= self.config['max_sub_policy_steps']:\n                 done = {'l' : True, 'h' : False, '__all__': False}\n                 obs['h'] = self.state\n                 rew['h'] = self.h_reward\n                 info['h'] = {}\n \n         obs['l'] = np.concatenate((self.state, self.goal))\n         rew['l'] = self.l_reward\n         print(rew, done)\n         print(action_dict)\n         return obs, rew, done, info\n \n from ray.rllib.agents.ddpg.ddpg import DDPGTrainer, DEFAULT_CONFIG\n trainer = HIROTrainer\n config = DEFAULT_CONFIG\n \n config['env_config'] = {'flat_env': 'BipedalWalker-v3', 'max_sub_policy_steps' : 5, 'num_goal_candidates' : 4}\n env = HIRO_env_wrapper(config['env_config'])\n config['env'] = HIRO_env_wrapper\n obs_space = env.observation_space\n action_space = env.action_space\n \n def policy_mapping(agent_id):\n     return agent_id\n l_obs_space = gym.spaces.Box(low=obs_space.low[0], high=obs_space.high[0], shape=(obs_space.shape[0]*2,))\n \n config['multiagent'] = {\n     'policies': {\n         'h': (None, obs_space, obs_space, {}),\n         'l': (None, l_obs_space, action_space, {}),\n     },\n     'policies_to_train': ['h', 'l'],\n     'policy_mapping_fn': policy_mapping,\n }\n \n ray.init(num_cpus=18, num_gpus=3)\n config['timesteps_per_iteration'] = 5\n config['train_batch_size'] = 8\n config['num_gpus'] = 3\n config['lr'] = tune.sample_from(lambda spec : np.random.choice(np.array([1e-5, 3e-5, 1e-4, 3e-4, 1e-3], dtype=np.float32)))\n \n tune.run(trainer, config=config)\n \n </denchmark-code>\n \n \n [v] I have verified my script runs in a clean environment and reproduces the issue.\n [v] I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "yskim5892", "commentT": "2020-04-23T03:49:19Z", "comment_text": "\n \t\tI've found some simpler(or minimal) setup that produces this problem. I think you'd better check the issue with this setting.\n Code :\n <denchmark-code>import ray\n from ray import tune\n from ray.rllib.env import MultiAgentEnv\n import numpy as np\n import gym\n \n class temp_env(MultiAgentEnv):\n     def __init__(self, config):\n         self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=[4])\n         self.action_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=[4])\n         self.count = 0\n         self.config = config\n \n     def reset(self):\n         self.state = np.random.normal(size=(4))\n         return {'a': self.state}\n \n     def step(self, action_dict):\n         action = action_dict['a']\n         rew = -np.linalg.norm(self.state - action)\n         self.state = np.random.normal(size=(4))\n         self.count += 1\n         print(self.count, rew, action)\n \n         return {'a': self.state}, {'a': rew}, {'__all__': rew >= -0.1}, {'a' : {}}\n \n def policy_mapping(agent_id):\n     return agent_id\n \n from ray.rllib.agents.ddpg.ddpg import DDPGTrainer, DEFAULT_CONFIG\n trainer = DDPGTrainer\n config = DEFAULT_CONFIG\n \n env = temp_env(config['env_config'])\n config['env'] = temp_env\n obs_space = env.observation_space\n action_space = env.action_space\n \n ray.init(num_cpus=18, num_gpus=3)\n config['num_gpus'] = 3\n config['lr'] = tune.sample_from(lambda spec : np.random.choice(np.array([1e-5, 3e-5, 1e-4, 3e-4, 1e-3], dtype=np.float32)))\n \n config['multiagent'] = {\n     'policies': {\n         'a': (None, obs_space, action_space, {}),\n     },\n     'policies_to_train': ['a'],\n     'policy_mapping_fn': policy_mapping,\n }\n config['prioritized_replay'] = False\n \n tune.run(trainer, config=config)\n </denchmark-code>\n \n Output:\n <denchmark-code>(pid=9477) 1998 -0.8631670757585075 [ 0.13583446  0.9148087  -0.3667308   0.49348983]                                                             [172/31315]\n (pid=9477) 1999 -4.473830981654697 [-0.8914048  1.2773074 -1.2671033 -1.0367289]\n Result for DDPG_temp_env_00000:\n   custom_metrics: {}\n   date: 2020-04-23_12-44-30\n   done: false\n   episode_len_mean: .nan\n   episode_reward_max: .nan\n   episode_reward_mean: .nan\n   episode_reward_min: .nan\n   episodes_this_iter: 0\n   episodes_total: 0\n   experiment_id: 3b03eb6a59fa47c99c170634e38be968\n   experiment_tag: '0'\n   hostname: silver\n   info:\n     exploration_infos:\n     - cur_scale: 1.0\n     grad_time_ms: 5.322\n     learner:\n       a:\n         max_q: -0.643890380859375\n         mean_q: -2.6590957641601562\n         min_q: -5.414635181427002\n         model: {}\n     num_steps_sampled: 2000\n     num_steps_trained: 128000\n     num_target_updates: 2000\n     opt_peak_throughput: 48106.713\n     opt_samples: 256.0\n     replay_time_ms: 12.831\n     sample_time_ms: 5.855\n     update_time_ms: 0.005\n   iterations_since_restore: 2\n   node_ip: 147.46.219.155\n   num_healthy_workers: 0\n   off_policy_estimator: {}\n   optimizer_steps_this_iter: 1000\n   perf:\n     cpu_util_percent: 48.71363636363637\n     ram_util_percent: 63.24545454545456\n   pid: 9477\n   policy_reward_max: {}\n   policy_reward_mean: {}\n   policy_reward_min: {}\n   sampler_perf: {}\n   time_since_restore: 22.34330129623413\n   time_this_iter_s: 15.495360374450684\n   time_total_s: 22.34330129623413\n   timestamp: 1587613470\n   timesteps_since_restore: 2000\n   timesteps_this_iter: 1000\n   timesteps_total: 2000\n   training_iteration: 2\n   trial_id: '00000'\n \n (pid=9477) 2000 -3.373277687817646 [-1.4948887 -0.0500416 -1.3712397 -2.068592 ]\n == Status ==\n Memory usage on this node: 79.6/125.6 GiB\n Using FIFO scheduling algorithm.\n Resources requested: 1/18 CPUs, 3/3 GPUs, 0.0/30.08 GiB heap, 0.0/10.35 GiB objects\n Result logdir: /home/silver/ray_results/DDPG\n Number of trials: 1 (1 RUNNING)\n +---------------------+----------+---------------------+--------+------------------+------+----------+\n | Trial name          | status   | loc                 |   iter |   total time (s) |   ts |   reward |\n |---------------------+----------+---------------------+--------+------------------+------+----------|\n | DDPG_temp_env_00000 | RUNNING  | 147.46.219.155:9477 |      2 |          22.3433 | 2000 |      nan |\n +---------------------+----------+---------------------+--------+------------------+------+----------+\n \n \n (pid=9477) 2001 -inf [inf inf inf inf]\n (pid=9477) 2002 -inf [inf inf inf inf]\n (pid=9477) 2003 -inf [inf inf inf inf]\n \n \n </denchmark-code>\n \n As you can see, outputting inf starts exactly at 2001 step.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "yskim5892", "commentT": "2020-04-23T05:31:44Z", "comment_text": "\n \t\tI've just found out that this problem happens even without Multiagent setting. Just simple trainer on simple environment with continuous actions space outputs actions full of infs.\n The same issue happens with A3CTrainer, but seems to not happen with PPOTrainer, ImpalaTrainer.\n Code:\n <denchmark-code>import ray\n from ray import tune\n import numpy as np\n import gym\n \n class temp_env(gym.Env):\n     def __init__(self, config):\n         self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=[4])\n         self.action_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=[4])\n         self.count = 0\n         self.config = config\n \n     def reset(self):\n         self.state = np.random.normal(size=(4))\n         return self.state\n \n     def step(self, action):\n         rew = -np.linalg.norm(self.state - action)\n         self.state = np.random.normal(size=(4))\n         self.count += 1\n         print(self.count, rew, action)\n \n         return self.state, rew, rew >= -0.1, {}\n \n \n from ray.rllib.agents.ddpg.ddpg import DDPGTrainer, DEFAULT_CONFIG\n trainer = DDPGTrainer\n config = DEFAULT_CONFIG\n \n env = temp_env(config['env_config'])\n config['env'] = temp_env\n \n ray.init(num_cpus=18, num_gpus=3)\n config['num_gpus'] = 3\n \n tune.run(trainer, config=config)\n </denchmark-code>\n \n Output:\n <denchmark-code>(pid=8860) 1998 -2.8897517974019196 [ 0.37765503  1.3648146  -0.25582075  1.2834811 ]\n (pid=8860) 1999 -2.6102417085274663 [ 1.3649862  -1.2796965   0.13636492 -0.56519246]\n (pid=8860) 2000 -2.416133637871286 [ 0.38399577  0.18302272 -0.21990576  0.6936583 ]\n Result for DDPG_temp_env_00000:\n   custom_metrics: {}\n   date: 2020-04-23_14-30-11\n   done: false\n   episode_len_mean: .nan\n   episode_reward_max: .nan\n   episode_reward_mean: .nan\n   episode_reward_min: .nan\n   episodes_this_iter: 0\n   episodes_total: 0\n   experiment_id: f336e1a754b94bc88f39f98d15e57af9\n   experiment_tag: '0'\n   hostname: silver\n   info:\n     exploration_infos:\n     - cur_scale: 1.0\n     grad_time_ms: 30.428\n     learner:\n       default_policy:\n         max_q: -0.5143887400627136\n         mean_q: -2.735276699066162\n         min_q: -5.7676682472229\n         model: {}\n     num_steps_sampled: 2000\n     num_steps_trained: 128000\n     num_target_updates: 2000\n     opt_peak_throughput: 8413.355\n     opt_samples: 256.0\n     replay_time_ms: 18.703\n     sample_time_ms: 5.05\n     update_time_ms: 0.004\n   iterations_since_restore: 2\n   node_ip: 147.46.219.155\n   num_healthy_workers: 0\n   off_policy_estimator: {}\n   optimizer_steps_this_iter: 1000\n   perf:\n     cpu_util_percent: 46.84146341463415\n     ram_util_percent: 63.99024390243902\n   pid: 8860\n   policy_reward_max: {}\n   policy_reward_mean: {}\n   policy_reward_min: {}\n   sampler_perf: {}\n   time_since_restore: 35.39928650856018\n   time_this_iter_s: 28.73469829559326\n   time_total_s: 35.39928650856018\n   timestamp: 1587619811\n   timesteps_since_restore: 2000\n   timesteps_this_iter: 1000\n   timesteps_total: 2000\n   training_iteration: 2\n   trial_id: '00000'\n \n == Status ==\n Memory usage on this node: 80.5/125.6 GiB\n Using FIFO scheduling algorithm.\n Resources requested: 1/18 CPUs, 3/3 GPUs, 0.0/29.74 GiB heap, 0.0/10.25 GiB objects\n Result logdir: /home/silver/ray_results/DDPG\n Number of trials: 1 (1 RUNNING)\n +---------------------+----------+---------------------+--------+------------------+------+----------+\n | Trial name          | status   | loc                 |   iter |   total time (s) |   ts |   reward |\n |---------------------+----------+---------------------+--------+------------------+------+----------|\n | DDPG_temp_env_00000 | RUNNING  | 147.46.219.155:8860 |      2 |          35.3993 | 2000 |      nan |\n +---------------------+----------+---------------------+--------+------------------+------+----------+\n \n \n (pid=8860) 2001 -inf [inf inf inf inf]\n \n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "yskim5892", "commentT": "2020-04-23T12:21:08Z", "comment_text": "\n \t\tPerhaps this issue is related to <denchmark-link:https://github.com/ray-project/ray/issues/7923>#7923</denchmark-link>\n ?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "yskim5892", "commentT": "2020-04-23T19:42:19Z", "comment_text": "\n \t\tIs this because your action space is bounded between -inf and +inf? I noticed a NaN error for continuous action spaces using SAC (even when I bound my action space to say 0,1) but didn't get the same error using ddpg or td3. I'm also interested in why this error is occurring as I've been pulling my hair out thinking my environment was broken when it might be something else related to rllib.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "yskim5892", "commentT": "2020-04-23T19:48:04Z", "comment_text": "\n \t\tFor me, the error also occurred in a bounded action space. I tried stable-baselines, and there it worked, so I'm quite confident that this is a bug in RLlib (that being said, I'd still choose RlLib over stable-baselines whenever possible).\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "yskim5892", "commentT": "2020-04-23T19:51:55Z", "comment_text": "\n \t\tAny idea as to when this might occur or how to avoid it?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "yskim5892", "commentT": "2020-05-03T20:47:21Z", "comment_text": "\n \t\tJust reproduced it. Happens in tf and torch at roughly the same time. The loss is already NaN at the first optimizer step. Taking a closer look. <denchmark-link:https://github.com/janblumenkamp>@janblumenkamp</denchmark-link>\n  Yeah, these issues could be all related. We'll see. ...\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "yskim5892", "commentT": "2020-05-03T20:57:26Z", "comment_text": "\n \t\tGot it. It happens in our Lambda layer in the DDPG model. In there, we \"scale\" the actions to be within the bounds, which causes this issue due to the bounds being [-inf inf]:\n <denchmark-code>                sigmoid_out = nn.Sigmoid()(2.0 * x)\n                 # Rescale to actual env policy scale\n                 # (shape of sigmoid_out is [batch_size, dim_actions],\n                 # so we reshape to get same dims)\n                 action_range = (action_space.high - action_space.low)[None]  # <- HERE\n                 low_action = action_space.low[None]\n                 actions = torch.from_numpy(action_range) * sigmoid_out + \\\n                     torch.from_numpy(low_action)\n                 return actions\n </denchmark-code>\n \n Will fix this. ...\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "yskim5892", "commentT": "2020-05-03T20:57:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/janblumenkamp>@janblumenkamp</denchmark-link>\n  <denchmark-link:https://github.com/yskim5892>@yskim5892</denchmark-link>\n  <denchmark-link:https://github.com/regproj>@regproj</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "yskim5892", "commentT": "2020-05-03T20:58:20Z", "comment_text": "\n \t\tThanks for filing this issue <denchmark-link:https://github.com/yskim5892>@yskim5892</denchmark-link>\n  !\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "yskim5892", "commentT": "2020-05-04T08:21:39Z", "comment_text": "\n \t\tIt's also our OrnsteinUhlenbeck Noise exploration module.\n In there, we scale the noise according to the action-space range (in this case: inf). Noise is inf, action becomes inf.\n Question is: How large should noise be in case of an [-inf, inf] action space? 1000? 100? 1? 10? :)\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "yskim5892", "commentT": "2020-05-04T08:22:24Z", "comment_text": "\n \t\tSo there are two similar bugs here: Lambda squashing layer and the noise. Will create a PR today.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "yskim5892", "commentT": "2020-05-04T09:25:12Z", "comment_text": "\n \t\tHere is the PR: <denchmark-link:https://github.com/ray-project/ray/pull/8302>#8302</denchmark-link>\n \n Please let me know, if this doesn't work somehow. I tested both tf and torch versions of DDPG and none of them was showing the problem of inf actions/rewards anymore.\n Closing this issue. Feel free to re-open if the problem still occurs on your end.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "yskim5892", "commentT": "2020-10-01T12:22:22Z", "comment_text": "\n \t\tHi,\n I'm having this issue too with a custom environment in multi agent settings. Both my observation and action spaces are continuous and bounded:\n obs_space = Box(low=0, high=100, shape=(1,))\n act_space = Box(low=np.array([0] * 2), high=np.array([1] * 2), shape=(2,))\n The problem occurs nearly always at the beginning with one over two samples when running a Population Based Training with 2 samples, but it seem it does not occur when running a trainer without PBT.\n The issue seems related to this one. I can give access to github repository if needed.\n Best regards\n Jean\n \t\t"}}}, "commit": {"commit_id": "a00144f746bb2ba4da5b1ddaf4954d6f5320c953", "commit_author": "Sven Mika", "commitT": "2020-05-04 22:27:30+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.5", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\agents\\ddpg\\ddpg_tf_model.py", "file_new_name": "rllib\\agents\\ddpg\\ddpg_tf_model.py", "file_complexity": {"file_NLOC": "118", "file_CCN": "12", "file_NToken": "734"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "83,84,85", "deleted_lines": "82,83,85", "method_info": {"method_name": "lambda_", "method_params": "x", "method_startline": "80", "method_endline": "85", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "48", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\agents\\ddpg\\ddpg_torch_model.py", "file_new_name": "rllib\\agents\\ddpg\\ddpg_torch_model.py", "file_complexity": {"file_NLOC": "127", "file_CCN": "13", "file_NToken": "716"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "91,93,94", "deleted_lines": "91,92,93,94", "method_info": {"method_name": "forward", "method_params": "self_,x", "method_startline": "91", "method_endline": "94", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "34", "method_nesting_level": "3"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "89,91,93,94", "deleted_lines": "85,87,88,89,90,91,92,93,94", "method_info": {"method_name": "forward", "method_params": "self,x", "method_startline": "85", "method_endline": "94", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "63", "method_nesting_level": "3"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\agents\\ddpg\\ddpg_torch_policy.py", "file_new_name": "rllib\\agents\\ddpg\\ddpg_torch_policy.py", "file_complexity": {"file_NLOC": "188", "file_CCN": "31", "file_NToken": "1471"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "163,164,165,169,170,172,173,174", "deleted_lines": "163,167,169,170", "method_info": {"method_name": "make_ddpg_optimizers", "method_params": "policy,config", "method_startline": "162", "method_endline": "175", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "82", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "83,84,158", "deleted_lines": "130,157,158", "method_info": {"method_name": "ddpg_actor_critic_loss", "method_params": "policy,model,_,train_batch", "method_startline": "30", "method_endline": "159", "method_complexity": {"method_NLOC": "90", "method_CCN": "14", "method_NToken": "739", "method_nesting_level": "0"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\policy\\torch_policy.py", "file_new_name": "rllib\\policy\\torch_policy.py", "file_complexity": {"file_NLOC": "325", "file_CCN": "52", "file_NToken": "2165"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "235", "deleted_lines": "243", "method_info": {"method_name": "learn_on_batch", "method_params": "self,postprocessed_batch", "method_startline": "223", "method_endline": "275", "method_complexity": {"method_NLOC": "39", "method_CCN": "12", "method_NToken": "331", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\utils\\exploration\\ornstein_uhlenbeck_noise.py", "file_new_name": "rllib\\utils\\exploration\\ornstein_uhlenbeck_noise.py", "file_complexity": {"file_NLOC": "119", "file_CCN": "9", "file_NToken": "845"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "99,100,101,102", "deleted_lines": "99,100", "method_info": {"method_name": "_get_tf_exploration_action_op", "method_params": "self,action_dist,explore,timestep", "method_startline": "85", "method_endline": "132", "method_complexity": {"method_NLOC": "36", "method_CCN": "4", "method_NToken": "333", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "161,162,163", "deleted_lines": null, "method_info": {"method_name": "_get_torch_exploration_action", "method_params": "self,action_dist,explore,timestep", "method_startline": "135", "method_endline": "177", "method_complexity": {"method_NLOC": "32", "method_CCN": "4", "method_NToken": "274", "method_nesting_level": "1"}}}}}}}}