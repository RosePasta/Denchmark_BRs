{"BR": {"BR_id": "7293", "BR_author": "falsemail", "BRopenT": "2020-02-24T13:37:04Z", "BRcloseT": "2020-03-23T19:40:23Z", "BR_text": {"BRsummary": "[rllib]custom metrics can be displayed on TensorBoard when override modelv2 's metrics method?", "BRdescription": "\n I override metrics method of modelv2.py, return a dict of my custom metrics.\n but the metrics can be displayed on tensorboard.\n the algorithm I used is ppo\n <denchmark-code>def metrics(self):\n     \"\"\"Override to return custom metrics from your model.\n \n     The stats will be reported as part of the learner stats, i.e.,\n         info:\n             learner:\n                 model:\n                     key1: metric1\n                     key2: metric2\n \n     Returns:\n         Dict of string keys to scalar tensors.\n     \"\"\"\n     return {}\n </denchmark-code>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "falsemail", "commentT": "2020-02-24T20:56:43Z", "comment_text": "\n \t\tCan you please provide a reproduction script (see the \"bug\" issue template)?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "falsemail", "commentT": "2020-02-25T08:25:34Z", "comment_text": "\n \t\t\n Can you please provide a reproduction script (see the \"bug\" issue template)?\n \n thanks for replay,here is my script below:\n class HiRange(gym.Env):\n \"\"\"Example of a custom env in which you have to walk down a corridor.\n You can configure the length of the corridor via the env config.\"\"\"\n <denchmark-code>def __init__(self, config):\n     # ctype class created\n     self.action_space = Box(\n         -1.0, 1.0, shape=(3,), dtype=np.float32)\n     self.observation_space = Box(\n         -1000.0, 1000.0, shape=(3,), dtype=np.float32)\n \n     self.last_reward = 0\n     self.x = 0\n     self.y = 0\n     self.z = 0\n     self.distance = 0\n \n def reset(self):\n     self.last_reward = 0\n     self.x = 0\n     self.y = 0\n     self.z = 0\n     self.distance = 0\n     self.target_pos = [1000,1000,1000]\n     self.distance = np.sqrt(3*1000**2)\n     self.distance_zero = np.sqrt(3*1000**2)\n     self.pos=np.array([0,0,0])\n     return np.array([0,0,0])\n \n def step(self, actions):\n     self.pos = self.pos + actions\n     x = self.target_pos[0] - self.pos[0]\n     y = self.target_pos[1] - self.pos[1]\n     z = self.target_pos[2] - self.pos[2]\n \n     distance = x * x + y * y + z * z\n     distance = np.sqrt(distance)\n     if abs(x) <= 10 and abs(y) <= 10 and abs(z) <= 10:\n         reward = (self.distance_zero - distance) / (self.distance_zero + 1e-10)\n         done = True\n     elif x > 500 or y > 500 or z > 500: # Out of range\n         reward = (self.distance_zero - distance) / (self.distance_zero + 1e-10) if not np.isnan(x * y * z) else -9\n         done = True\n     else:\n         reward = (self.distance - distance) / (self.distance_zero + 1e-10)\n         done = False\n     self.distance = distance\n     return self.pos, reward, done, {}\n </denchmark-code>\n \n class CustomModel(RecurrentTFModelV2):\n \"\"\"Example of using the Keras functional API to define a RNN model.\"\"\"\n <denchmark-code>def __init__(self,\n              obs_space,\n              action_space,\n              num_outputs,\n              model_config,\n              name,\n              hiddens_size=256,\n              cell_size=64):\n     super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n                                       model_config, name)\n     self.cell_size = cell_size\n \n     # Define input layers\n     input_layer = tf.keras.layers.Input(\n         shape=(None, obs_space.shape[0]), name=\"inputs\")\n     state_in_h = tf.keras.layers.Input(shape=(cell_size,), name=\"h\")\n     state_in_c = tf.keras.layers.Input(shape=(cell_size,), name=\"c\")\n     seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\")\n \n     # Preprocess observation with a hidden layer and send to LSTM cell\n     dense1 = tf.keras.layers.Dense(\n         hiddens_size, activation=tf.nn.relu, name=\"dense1\")(input_layer)\n     lstm_out, state_h, state_c = tf.keras.layers.LSTM(\n         cell_size, return_sequences=True, return_state=True, name=\"lstm\")(\n         inputs=dense1,\n         mask=tf.sequence_mask(seq_in),\n         initial_state=[state_in_h, state_in_c])\n \n     # Postprocess LSTM output with another hidden layer and compute values\n     logits = tf.keras.layers.Dense(\n         self.num_outputs,\n         activation=tf.keras.activations.linear,\n         name=\"logits\")(lstm_out)\n     values = tf.keras.layers.Dense(\n         256, activation=tf.nn.relu, name=\"values-1\")(input_layer)\n     batch_norm_v1 = tf.keras.layers.BatchNormalization()\n     values = batch_norm_v1(values)\n \n     values = tf.keras.layers.Dense(\n         256, activation=tf.nn.relu, name=\"values-2\")(values)\n \n     batch_norm_v2 = tf.keras.layers.BatchNormalization()\n     values = batch_norm_v2(values)\n     self.update_ops_hi = batch_norm_v1.updates + batch_norm_v2.updates\n \n     values = tf.keras.layers.Dense(\n         1, activation=None, name=\"values-3\")(values)\n     # Create the RNN model\n     self.rnn_model = tf.keras.Model(\n         inputs=[input_layer, seq_in, state_in_h, state_in_c],\n         outputs=[logits, values, state_h, state_c])\n     self.register_variables(self.rnn_model.variables)\n     self.rnn_model.summary()\n \n @override(TFModelV2)\n def update_ops(self):\n     return self.update_ops_hi\n \n @override(RecurrentTFModelV2)\n def forward_rnn(self, inputs, state, seq_lens):\n     model_out, self._value_out, h, c = self.rnn_model([inputs, seq_lens] +\n                                                       state)\n     return model_out, [h, c]\n \n @override(ModelV2)\n def get_initial_state(self):\n     return [\n         np.zeros(self.cell_size, np.float32),\n         np.zeros(self.cell_size, np.float32),\n     ]\n @override(ModelV2)\n def metrics(self):\n     return {\"value_output\": self._value_out}\n \n @override(ModelV2)\n def value_function(self):\n     return tf.reshape(self._value_out, [-1])\n </denchmark-code>\n \n if name == \"main\":\n # Can also register the env creator function explicitly with:\n # register_env(\"corridor\", lambda config: SimpleCorridor(config))\n ray.init(webui_host='127.0.0.1')\n <denchmark-code>def explore(config):\n     # ensure we collect enough timesteps to do sgd\n     if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n         config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n     # ensure we run at least one sgd iter\n     if config[\"num_sgd_iter\"] < 1:\n         config[\"num_sgd_iter\"] = 1\n     return config\n \n pbt = PopulationBasedTraining(\n     time_attr=\"time_total_s\",\n     metric=\"episode_reward_mean\",\n     mode=\"max\",\n     perturbation_interval=1200,\n     resample_probability=0.25,\n     # Specifies the mutations of these hyperparams\n     hyperparam_mutations={\n         \"lambda\": lambda: random.uniform(0.9, 1.0),\n         \"clip_param\": lambda: random.uniform(0.02, 0.5),\n         \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 1e-6],\n         \"num_sgd_iter\": lambda: random.randint(1, 30),\n         \"sgd_minibatch_size\": lambda: random.randint(1024, 16384 * 2),\n         \"train_batch_size\": lambda: random.randint(20000, 200000),\n         # \"sgd_minibatch_size\": lambda: random.randint(102, 163 * 2),\n         # \"train_batch_size\": lambda: random.randint(2000, 3000),\n         \"entropy_coeff\": [0.0, 0.0001, 1e-6, 1e-8, 1e-9, 6e-10],\n         # \"env_config\": lambda: {\n         #     \"class_level\": phase,\n         # },\n     },\n     custom_explore_fn=explore)\n \n ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n tune.run(\n     \"PPO\",\n     name='hiworld',\n     checkpoint_freq=20,\n     num_samples=2,\n     scheduler=pbt,\n     config={\n         \"env\": HiRange,  # or \"corridor\" if registered above\n         \"model\": {\n             \"custom_model\": \"my_model\",\n         },\n         'num_workers': 3,\n         \"lambda\": 0.95,\n         \"clip_param\": 0.2,\n         \"entropy_coeff\": 6e-10,\n         \"lr\": 1e-5,\n         \"num_sgd_iter\": sample_from(\n             lambda spec: random.choice([10, 20, 30])),\n         \"sgd_minibatch_size\": sample_from(\n             lambda spec: random.choice([1024, 2048, 4096])),\n         \"train_batch_size\": sample_from(\n             lambda spec: random.choice([100000, 150000])),\n         \"env_config\": {},\n     },\n \n )\n </denchmark-code>\n \n and the result is :\n Result for PPO_HiRange_9edaf3bc:\n custom_metrics: {}\n date: 2020-02-25_16-19-24\n done: false\n episode_len_mean: 1.0\n episode_reward_max: 0.0009999999999999204\n episode_reward_mean: 2.7741471189582446e-05\n episode_reward_min: -0.0010000000000000516\n episodes_this_iter: 100000\n episodes_total: 300000\n experiment_id: 7868003cfc6a4ea29255993ecec37173\n experiment_tag: 0_num_sgd_iter=30,sgd_minibatch_size=4096,train_batch_size=100000\n hostname: iZbp15xx9gqtg1muz0y8pwZ\n info:\n grad_time_ms: 39355.053\n learner:\n default_policy:\n cur_kl_coeff: 0.05000000074505806\n cur_lr: 9.999999747378752e-06\n entropy: 4.191382884979248\n entropy_coeff: 5.999999941330714e-10\n kl: 0.0014330720296129584\n policy_loss: -0.042286962270736694\n total_loss: -0.04221513494849205\n vf_explained_var: -1.4901161193847656e-08\n vf_loss: 1.7163733900815714e-07\n load_time_ms: 1765.43\n num_steps_sampled: 300000\n num_steps_trained: 294912\n sample_time_ms: 81602.604\n update_time_ms: 820.159\n iterations_since_restore: 3\n node_ip: 172.16.162.61\n num_healthy_workers: 3\n off_policy_estimator: {}\n perf:\n cpu_util_percent: 85.96818181818182\n ram_util_percent: 42.67897727272727\n pid: 1656\n policy_reward_max: {}\n policy_reward_mean: {}\n policy_reward_min: {}\n sampler_perf:\n mean_env_wait_ms: 0.0669762693249851\n mean_inference_ms: 1.4597577955574508\n mean_processing_ms: 0.8858957409465554\n time_since_restore: 375.51816630363464\n time_this_iter_s: 123.7411642074585\n time_total_s: 375.51816630363464\n timestamp: 1582618764\n timesteps_since_restore: 300000\n timesteps_this_iter: 100000\n timesteps_total: 300000\n training_iteration: 3\n trial_id: 9edaf3bc\n as you saw, the custom metric wasn't in info, and can not be displayed in tensorboard~\n \t\t"}}}, "commit": {"commit_id": "9a590ac6a55c398a4cd7eb52d7f5f4c982d4a5be", "commit_author": "Eric Liang", "commitT": "2020-03-23 12:40:22-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\examples\\custom_keras_model.py", "file_new_name": "rllib\\examples\\custom_keras_model.py", "file_complexity": {"file_NLOC": "108", "file_CCN": "9", "file_NToken": "806"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "57,58", "deleted_lines": null, "method_info": {"method_name": "metrics", "method_params": "self", "method_startline": "57", "method_endline": "58", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "18", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "106,107,108,109,110", "deleted_lines": "106,107,108,109", "method_info": {"method_name": "check_has_custom_metric", "method_params": "result", "method_startline": "106", "method_endline": "110", "method_complexity": {"method_NLOC": "5", "method_CCN": "2", "method_NToken": "40", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "rllib\\utils\\sgd.py", "file_new_name": "rllib\\utils\\sgd.py", "file_complexity": {"file_NLOC": "61", "file_CCN": "12", "file_NToken": "433"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "18,19,30,31", "deleted_lines": null, "method_info": {"method_name": "averaged", "method_params": "kv", "method_startline": "15", "method_endline": "32", "method_complexity": {"method_NLOC": "8", "method_CCN": "4", "method_NToken": "65", "method_nesting_level": "0"}}}}}}}}