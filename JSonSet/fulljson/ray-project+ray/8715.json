{"BR": {"BR_id": "8715", "BR_author": "stephanie-wang", "BRopenT": "2020-06-01T23:13:07Z", "BRcloseT": "2020-06-02T23:06:37Z", "BR_text": {"BRsummary": "[autoscaler] Cluster becomes unusable when job exits.", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version and other system information (Python version, TensorFlow version, OS): 0.9dev\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n If we cannot run your script, we cannot fix your issue.\n I ran with the default AWS autoscaler config (python/ray/autoscaler/aws/example-full.yaml), modified the minimum to 2 workers. Steps to reproduce:\n \n SSH into head node.\n Run a Ray script from bash (while True: ray.get(foo.remote()) works)\n Cancel Ray script with CTRL+C.\n \n Then, in /tmp/ray/session_latest/logs/monitor.err, I get this:\n <denchmark-code>2020-06-01 23:06:52,035 INFO autoscaler.py:619 -- StandardAutoscaler: 2/2 target nodes (0 pending) (2 updating) (bringup=True)\n 2020-06-01 23:06:52,035 INFO autoscaler.py:620 -- LoadMetrics: MostDelayedHeartbeats={'172.30.0.103': 0.1708686351776123}, NodeIdleSeconds=Min=0 Mean=0 Max=0, NumNodesConnected=1, NumNodesUsed=0.02, ResourceUsage=1.0/64.0 CPU, 0.0 GiB/178.57 GiB memory, 0.0/1.0 node:172.30.0.103, 0.0 GiB/55.73 GiB object_store_memory, TimeSinceLastHeartbeat=Min=0 Mean=0 Max=0\n 2020-06-01 23:06:56,727 INFO updater.py:257 -- NodeUpdater: i-0fe7dc6a0f3fd63be: Running uptime on 172.30.1.111...\n 2020-06-01 23:06:56,733 INFO updater.py:257 -- NodeUpdater: i-0a74d5ee1dadf1b74: Running uptime on 172.30.1.30...\n Warning: Permanently added '172.30.1.111' (ECDSA) to the list of known hosts.^M\n Warning: Permanently added '172.30.1.30' (ECDSA) to the list of known hosts.^M\n Error in monitor loop\n Traceback (most recent call last):\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py\", line 277, in run\n     self._run()\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py\", line 235, in _run\n     self.process_messages()\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py\", line 187, in process_messages\n     message_handler(channel, data)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/monitor.py\", line 125, in xray_job_notification_handler\n     job_data = gcs_entries.entries[0]\n IndexError: list index (0) out of range\n 2020-06-01 23:06:56,765 ERROR autoscaler.py:645 -- StandardAutoscaler: kill_workers triggered\n 2020-06-01 23:06:56,828 INFO node_provider.py:353 -- AWSNodeProvider: terminating nodes ['i-0fe7dc6a0f3fd63be', 'i-0a74d5ee1dadf1b74'] (spot nodes cannot be stopped, only terminated)\n 2020-06-01 23:06:56,997 ERROR autoscaler.py:650 -- StandardAutoscaler: terminated 2 node(s)\n 2020-06-01 23:06:57,123 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...\n Monitor: Cleanup exception. Trying again...\n mux_client_request_session: read from master failed: Broken pipe^M\n Failed to connect to new control master^M\n mux_client_request_session: read from master failed: Broken pipe^M\n Failed to connect to new control master^M\n 2020-06-01 23:06:59,270 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...\n Monitor: Cleanup exception. Trying again...\n 2020-06-01 23:07:01,385 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...\n Monitor: Cleanup exception. Trying again...\n 2020-06-01 23:07:02,231 INFO updater.py:257 -- NodeUpdater: i-0a74d5ee1dadf1b74: Running uptime on 172.30.1.30...\n ssh: connect to host 172.30.1.30 port 22: Connection refused^M\n 2020-06-01 23:07:02,243 INFO updater.py:257 -- NodeUpdater: i-0fe7dc6a0f3fd63be: Running uptime on 172.30.1.111...\n ssh: connect to host 172.30.1.111 port 22: Connection refused^M\n 2020-06-01 23:07:03,513 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...\n Monitor: Cleanup exception. Trying again...\n 2020-06-01 23:07:05,622 INFO commands.py:93 -- teardown_cluster: Keeping 2 nodes...\n Monitor: Cleanup exception. Trying again...\n </denchmark-code>\n \n Other times I get the same Python exception and teardown messages, but not the infinitely repeating messages about \"Cleanup exception\". However, the messages just stop completely and no new worker nodes are started. I didn't confirm, but this might be if I wait for the worker nodes to come up completely before starting the job.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "stephanie-wang", "commentT": "2020-06-01T23:13:32Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n , might be a P0?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "stephanie-wang", "commentT": "2020-06-01T23:22:53Z", "comment_text": "\n \t\tActually, this just happened again without a keyboard interrupt, I think because my driver exited.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "stephanie-wang", "commentT": "2020-06-01T23:28:20Z", "comment_text": "\n \t\tReproduced the same error with this short script, head node with 0 CPUs, and 2 worker nodes with lots of CPUs:\n <denchmark-code>import ray\n import time\n \n ray.init(address=\"auto\")\n \n \n @ray.remote\n def foo():\n     return\n \n \n for _ in range(100):\n     ray.get(foo.remote())\n     time.sleep(0.1)\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "stephanie-wang", "commentT": "2020-06-02T01:22:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/stephanie-wang>@stephanie-wang</denchmark-link>\n  We should find an assignee for this task as it is a release blocker. I will post it in the slack channel!!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "stephanie-wang", "commentT": "2020-06-02T06:22:08Z", "comment_text": "\n \t\tAfter this PR is merged, <denchmark-link:https://github.com/ray-project/ray/pull/8401/files>https://github.com/ray-project/ray/pull/8401/files</denchmark-link>\n , we stopped using AsyncAppend, but we started using Put. This makes this part of code incompatible. <denchmark-link:https://github.com/ffbin>@ffbin</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "stephanie-wang", "commentT": "2020-06-02T06:26:08Z", "comment_text": "\n \t\tI think we should be careful when we start using the store_client next time because it seems like it will change some storing mechanism. => Seems like autoscaler tests are more like \"unit tests\".\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "stephanie-wang", "commentT": "2020-06-02T11:35:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rkooo567>@rkooo567</denchmark-link>\n  Thanks! dashboard.py is also need change, i'll add comments to your PR.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "stephanie-wang", "commentT": "2020-06-02T16:16:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ffbin>@ffbin</denchmark-link>\n  Thanks for finding that out!\n \t\t"}}}, "commit": {"commit_id": "7c4399110045158daa280be2f6545adc7e03ffa5", "commit_author": "SangBin Cho", "commitT": "2020-06-02 16:06:36-07:00", "commit_complexity": {"commit_NLOC": "0.10526315789473684", "commit_CCN": "0.9473684210526315", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\dashboard\\dashboard.py", "file_new_name": "python\\ray\\dashboard\\dashboard.py", "file_complexity": {"file_NLOC": "1038", "file_CCN": "189", "file_NToken": "7243"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "847,848", "deleted_lines": "847", "method_info": {"method_name": "run", "method_params": "self", "method_startline": "788", "method_endline": "870", "method_complexity": {"method_NLOC": "76", "method_CCN": "8", "method_NToken": "608", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\monitor.py", "file_new_name": "python\\ray\\monitor.py", "file_complexity": {"file_NLOC": "224", "file_CCN": "40", "file_NToken": "1180"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "124,125", "deleted_lines": "124,125", "method_info": {"method_name": "xray_job_notification_handler", "method_params": "self,unused_channel,data", "method_startline": "117", "method_endline": "131", "method_complexity": {"method_NLOC": "9", "method_CCN": "2", "method_NToken": "64", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\state.py", "file_new_name": "python\\ray\\state.py", "file_complexity": {"file_NLOC": "546", "file_CCN": "102", "file_NToken": "3223"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "219", "deleted_lines": "219", "method_info": {"method_name": "actor_table", "method_params": "self,actor_id", "method_startline": "205", "method_endline": "235", "method_complexity": {"method_NLOC": "20", "method_CCN": "4", "method_NToken": "125", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\tests\\test_global_state.py", "file_new_name": "python\\ray\\tests\\test_global_state.py", "file_complexity": {"file_NLOC": "98", "file_CCN": "15", "file_NToken": "646"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128", "deleted_lines": null, "method_info": {"method_name": "test_global_state_actor_entry", "method_params": "ray_start_regular", "method_startline": "109", "method_endline": "128", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "148", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "112,113", "deleted_lines": null, "method_info": {"method_name": "test_global_state_actor_entry.ready", "method_params": "self", "method_startline": "112", "method_endline": "113", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "6", "method_nesting_level": "2"}}}}}}}}