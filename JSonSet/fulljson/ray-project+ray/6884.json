{"BR": {"BR_id": "6884", "BR_author": "houcharlie", "BRopenT": "2020-01-21T23:22:53Z", "BRcloseT": "2020-01-24T18:29:36Z", "BR_text": {"BRsummary": "[RLlib] Using LSTM model raises ValueError (Tuple obs_space, similar to #3367)", "BRdescription": "\n <denchmark-h:h3>System Information</denchmark-h>\n \n \n Fedora 7.7 (Maipo) server\n Ray from pip\n Ray version 0.8.0\n Python version 3.6.8\n \n <denchmark-h:h3>Problem</denchmark-h>\n \n I have a custom environment with a Tuple observation space, using the default model in rllib that is trained using PPO.  Everything works when I don't turn on the LSTM option.  However, when I do turn it on, I get the following stack trace:\n File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 90, in __init__ Trainer.__init__(self, config, env, logger_creator) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 398, in __init__ Trainable.__init__(self, config, logger_creator) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/tune/trainable.py\", line 96, in __init__ self._setup(copy.deepcopy(self.config)) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 523, in _setup self._init(self.config, self.env_creator) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 109, in _init self.config[\"num_workers\"]) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 568, in _make_workers logdir=self.logdir) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\", line 64, in __init__ RolloutWorker, env_creator, policy, 0, self._local_config) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\", line 220, in _make_worker _fake_sampler=config.get(\"_fake_sampler\", False)) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 350, in __init__ self._build_policy_map(policy_dict, policy_config) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 766, in _build_policy_map policy_map[name] = cls(obs_space, act_space, merged_conf) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py\", line 143, in __init__ obs_include_prev_action_reward=obs_include_prev_action_reward) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 198, in __init__ before_loss_init(self, obs_space, action_space, config) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/policy/tf_policy_template.py\", line 127, in before_loss_init_wrapper self._extra_action_fetches = extra_action_fetches_fn(self) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/agents/ppo/ppo_policy.py\", line 170, in vf_preds_and_logits_fetches SampleBatch.VF_PREDS: policy.model.value_function(), File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/models/tf/modelv1_compat.py\", line 148, in value_function seq_lens=None) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/models/catalog.py\", line 481, in get_model seq_lens) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/models/catalog.py\", line 521, in _get_model num_outputs, options) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/models/model.py\", line 57, in __init__ input_dict[\"obs\"], obs_space) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/models/model.py\", line 230, in restore_original_dimensions return _unpack_obs(obs, obs_space.original_space, tensorlib=tensorlib) File \"/afs/ece.cmu.edu/usr/charlieh/.local/lib/python3.6/site-packages/ray/rllib/models/model.py\", line 260, in _unpack_obs prep.shape[0], obs.shape)) ValueError: Expected flattened obs shape of [None, 143], got (?, 256)\n My state space is\n obs_space = spaces.Tuple(spaces.Discrete(35),spaces.Discrete(35),spaces.Discrete(35), spaces.Discrete(3), spaces.Discrete(35))\n and if I run\n prep = get_preprocessor(spy_state_space)(spy_state_space) print(prep)\n I get\n \n (143,)\n \n As expected.  I believe that this is similar to issue <denchmark-link:https://github.com/ray-project/ray/issues/3367>#3367</denchmark-link>\n  (except this time the observation space is a Tuple rather than a Dict.  Am I doing something wrong or is this a bug?  If requested, I can also try posting my code.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "houcharlie", "commentT": "2020-01-22T04:49:27Z", "comment_text": "\n \t\tCc <denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "houcharlie", "commentT": "2020-01-22T09:19:28Z", "comment_text": "\n \t\tWill try to reproduce. ...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "houcharlie", "commentT": "2020-01-22T12:13:43Z", "comment_text": "\n \t\tMy minimal example works fine. Could you check, to see what I might do differently?\n <denchmark-code>import gym\n from gym.spaces import Tuple, Discrete\n import numpy as np\n \n from ray.rllib.agents.ppo import PPOTrainer\n from ray.rllib.utils import try_import_tf\n \n tf = try_import_tf()\n \n \n class RandomEnv(gym.Env):\n     \"\"\"\n     A randomly acting environment that can be instantiated with arbitrary\n     action and observation spaces.\n     \"\"\"\n     def __init__(self, config):\n         # Action space.\n         self.action_space = config[\"action_space\"]\n         # Observation space from which to sample.\n         self.observation_space = config[\"observation_space\"]\n         # Reward space from which to sample.\n         self.reward_space = config.get(\n             \"reward_space\",\n             gym.spaces.Box(low=-1.0, high=1.0, shape=(), dtype=np.float32)\n         )\n         # Chance that an episode ends at any step.\n         self.p_done = config.get(\"p_done\", 0.1)\n \n     def reset(self):\n         return self.observation_space.sample()\n \n     def step(self, action):\n         return self.observation_space.sample(), float(self.reward_space.sample()), \\\n             bool(np.random.choice(\n                 [True, False], p=[self.p_done, 1.0 - self.p_done]\n             )), {}\n \n \n if __name__ == \"__main__\":\n     trainer = PPOTrainer(\n         config={\n             \"model\": {\n                 \"use_lstm\": True,\n             },\n             \"vf_share_layers\": True,\n             \"num_workers\": 0,  # no parallelism\n             \"env_config\": {\n                 \"action_space\": Discrete(2),\n                 # Test a simple Tuple observation space.\n                 \"observation_space\": Tuple([Discrete(2), Discrete(2)])\n             }\n         },\n         env=RandomEnv,\n     )\n     for _ in range(2):\n         results = trainer.train()\n         print(results)\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "houcharlie", "commentT": "2020-01-22T12:17:57Z", "comment_text": "\n \t\tAh, got it! When I set vf_share_layers to False (in my example above), I get the same error as you do. Ok, this seems to be a bug.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "houcharlie", "commentT": "2020-01-22T12:29:06Z", "comment_text": "\n \t\tSo RLlib sets use_lstm automatically to False if you do vf_share_layers == False (separate policy and vf networks) and outputs a warning: 2020-01-22 13:26:40,266\tWARNING modelv1_compat.py:131 -- It is not recommended to use a LSTM model with vf_share_layers=False (consider setting it to True). If you want to not share layers, you can implement a custom LSTM model that overrides the value_function() method.\n I'll fix the error message (it shouldn't be thrown at all), but in the meantime, could you set your config to vf_share_layers=True and model->use_lstm=True? Then it should work.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "houcharlie", "commentT": "2020-01-22T16:11:48Z", "comment_text": "\n \t\tI have a fix (and PR submitted) in <denchmark-link:https://github.com/sven1977/ray/tree/test_ppo_lstm_issue_with_tuple_spaces>https://github.com/sven1977/ray/tree/test_ppo_lstm_issue_with_tuple_spaces</denchmark-link>\n \n Feel free to try and let me know.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "houcharlie", "commentT": "2020-01-23T00:57:47Z", "comment_text": "\n \t\tSetting vf_share_layers to True worked!  Is there a way to try the fix through pip or do I have to clone this repo?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "houcharlie", "commentT": "2020-01-23T08:30:31Z", "comment_text": "\n \t\tHold on. Sorry, I made a mistake in hardcoding a numpy array as input into the forward pass through the (non-shared) vf-NN (thinking pytorch or eager :/ ). I'm taking a closer look:\n <denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n  So my understanding right now is: If the user wants to use a separate vf-NN with the LSTM option True, our current code indicates that in that case, we force shared=True (the warning is not very clear about that, I think).\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "houcharlie", "commentT": "2020-01-23T08:34:02Z", "comment_text": "\n \t\tThe original warning was misleading in making the user think, that the networks are still separate (policy w/ LSTM and vf w/o LSTM), which they were not, unless one defines a custom Model and overrides value_function().\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "houcharlie", "commentT": "2020-01-23T10:55:23Z", "comment_text": "\n \t\tOk, can you try again?\n <denchmark-link:https://github.com/sven1977/ray/tree/test_ppo_lstm_issue_with_tuple_spaces>https://github.com/sven1977/ray/tree/test_ppo_lstm_issue_with_tuple_spaces</denchmark-link>\n \n Just waiting for the respective PR to get merged into ray's master, hopefully later today.\n \t\t"}}}, "commit": {"commit_id": "446cbdf2e0e3bcd0476adb9931af4cdacc00225d", "commit_author": "Sven Mika", "commitT": "2020-01-24 10:29:35-08:00", "commit_complexity": {"commit_NLOC": "0.75", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "ci\\jenkins_tests\\run_rllib_tests.sh", "file_new_name": "ci\\jenkins_tests\\run_rllib_tests.sh", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "490,491,492", "deleted_lines": null}}}, "file_1": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "rllib\\examples\\random_env.py", "file_complexity": {"file_NLOC": "54", "file_CCN": "3", "file_NToken": "278"}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\models\\tf\\modelv1_compat.py", "file_new_name": "rllib\\models\\tf\\modelv1_compat.py", "file_complexity": {"file_NLOC": "126", "file_CCN": "22", "file_NToken": "804"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "127,128,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,149", "deleted_lines": "129,130,131,132,133,134,135,136,139", "method_info": {"method_name": "make_v1_wrapper.value_function", "method_params": "self", "method_startline": "112", "method_endline": "155", "method_complexity": {"method_NLOC": "35", "method_CCN": "4", "method_NToken": "186", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "127,128,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,149", "deleted_lines": "129,130,131,132,133,134,135,136,139", "method_info": {"method_name": "make_v1_wrapper", "method_params": "legacy_model_cls", "method_startline": "17", "method_endline": "161", "method_complexity": {"method_NLOC": "41", "method_CCN": "4", "method_NToken": "205", "method_nesting_level": "0"}}}}}}}}