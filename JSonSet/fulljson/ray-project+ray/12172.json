{"BR": {"BR_id": "12172", "BR_author": "tuxa", "BRopenT": "2020-11-19T19:49:22Z", "BRcloseT": "2020-11-23T22:29:42Z", "BR_text": {"BRsummary": "[tune] PBT checkpoints are not synced to head in GCP using Docker", "BRdescription": "\n <denchmark-h:h3>System</denchmark-h>\n \n Using the instance image and docker image from the example config:\n Image: projects/deeplearning-platform-release/global/images/family/tf-1-13-cpu\n Docker Image:  rayproject/ray:latest-gpu\n Ubuntu 18.04\n ray 1.0.1\n python 3.7.7\n Following dependencies are also installed:\n torch==1.7.0\n pytorch-lightning==1.0.6\n torchvision==0.8.1\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n I am trying to deploy the tune-pytorch-lightning with population based training example on the GCP (dockerized). The checkpoints are stored in /root/ray_results/tune_mnist_pbt, but after the perturbation interval, new folders get created directly in  /root/ray_results (e.g. /root/ray_results/2020-11-19_11-26-16i3yoquuf )  to use the weights of the highest performing experiment. The problem is that this folders dont get synced from the workers to the head when doing distributed training on GCP, whereas everything in /root/ray_results/tune_mnist_pbt/ gets synced:\n <denchmark-code>2020-11-19 19:42:27,243 VINFO updater.py:460 -- `rsync`ed /root/ray_results/tune_mnist_pbt/DEFAULT_1d092_00006_6_layer_1_size=128,layer_2_size=64_2020-11-19_19-41-58/ (remote) to /root/ray_results/tune_mn\n ist_pbt/DEFAULT_1d092_00006_6_layer_1_size=128,layer_2_size=64_2020-11-19_19-41-58/ (local)                                                                                                                 \n 2020-11-19 19:42:27,247 ERROR trial_runner.py:868 -- Trial DEFAULT_1d092_00006: Error handling checkpoint /root/ray_results/2020-11-19_19-42-24w11y5gdl/checkpoint_1/                                       \n Traceback (most recent call last):\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 864, in _process_trial_save\n     trial.on_checkpoint(trial.saving_to)\n   File \"/root/anaconda3/lib/python3.7/site-packages/ray/tune/trial.py\", line 498, in on_checkpoint\n     self, checkpoint.value))\n ray.tune.error.TuneError: Trial DEFAULT_1d092_00006: Checkpoint path /root/ray_results/2020-11-19_19-42-24w11y5gdl/checkpoint_1/ not found after successful sync down.\n 2020-11-19 19:42:27,250 WARNING util.py:140 -- The `process_trial_save` operation took 1.05049467086792 seconds to complete, which may be a performance bottleneck.\n </denchmark-code>\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please use ray_cluster.yaml and tune_plt.py to reproduce the issue:\n <denchmark-code># spawns 1 head and 1 worker\n ray up ray_cluster.yaml -y\n \n # submits ray_cluster.yaml. after the first epoch of the worker node the error can be seen in the terminal\n ray submit ray_cluster.yaml tune_pl.py  --start\n </denchmark-code>\n \n ray_cluster.yaml\n (set project_id)\n <denchmark-code>cluster_name: default\n min_workers: 1\n max_workers: 1\n initial_workers: 1\n autoscaling_mode: default\n \n docker:\n     image: \"rayproject/ray:latest-gpu\"\n     container_name: \"ray_container\"\n \n     pull_before_run: True\n     run_options: []\n \n target_utilization_fraction: 0.8\n idle_timeout_minutes: 10\n \n provider:\n     type: gcp\n     region: europe-west4\n     availability_zone: europe-west4-b\n     project_id: null\n \n auth:\n     ssh_user: ubuntu\n \n head_node:\n     machineType: n1-standard-2\n     disks:\n       - boot: true\n         autoDelete: true\n         type: PERSISTENT\n         initializeParams:\n           diskSizeGb: 50\n           sourceImage: projects/deeplearning-platform-release/global/images/family/tf-1-13-cpu\n     scheduling:\n       - onHostMaintenance: TERMINATE\n \n worker_nodes:\n     machineType: n1-standard-2\n     disks:\n       - boot: true\n         autoDelete: true\n         type: PERSISTENT\n         initializeParams:\n           diskSizeGb: 50\n           sourceImage: projects/deeplearning-platform-release/global/images/family/tf-1-13-cpu\n     scheduling:\n       - onHostMaintenance: TERMINATE\n \n cluster_synced_files: []\n file_mounts_sync_continuously: True\n initialization_commands: []\n \n rsync_exclude:\n     - \"**/.git\"\n     - \"**/.git/**\"\n \n \n rsync_filter:\n     - \".gitignore\"\n \n setup_commands:\n   - pip install torch==1.7.0\n   - pip install pytorch-lightning==1.0.6\n   - pip install torchvision==0.8.1\n \n head_setup_commands:\n   - pip install google-api-python-client==1.7.8\n \n worker_setup_commands: []\n \n head_start_ray_commands:\n     - ray stop\n     - >-\n       ulimit -n 65536;\n       ray start\n       --head\n       --port=6379\n       --object-manager-port=8076\n       --autoscaling-config=~/ray_bootstrap_config.yaml\n \n worker_start_ray_commands:\n     - ray stop\n     - >-\n       ulimit -n 65536;\n       ray start\n       --address=$RAY_HEAD_IP:6379\n       --object-manager-port=8076\n \n </denchmark-code>\n \n tune_pl.py\n <denchmark-code>import torch\n import pytorch_lightning as pl\n from torch.utils.data import DataLoader, random_split\n from torch.nn import functional as F\n from torchvision.datasets import MNIST\n from torchvision import transforms\n import os\n \n import shutil\n from functools import partial\n from tempfile import mkdtemp\n from pytorch_lightning.loggers import TensorBoardLogger\n from pytorch_lightning.utilities.cloud_io import load as pl_load\n from ray import tune\n import ray\n from ray.tune.integration.docker import DockerSyncer\n from ray.tune import CLIReporter\n from ray.tune.schedulers import PopulationBasedTraining\n from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n \n class LightningMNISTClassifier(pl.LightningModule):\n     \"\"\"\n     This has been adapted from\n     https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09\n     \"\"\"\n \n     def __init__(self, config, data_dir=None):\n         super(LightningMNISTClassifier, self).__init__()\n \n         self.data_dir = data_dir or os.getcwd()\n \n         self.layer_1_size = config[\"layer_1_size\"]\n         self.layer_2_size = config[\"layer_2_size\"]\n         self.lr = config[\"lr\"]\n         self.batch_size = config[\"batch_size\"]\n \n         # mnist images are (1, 28, 28) (channels, width, height)\n         self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n         self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n         self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n \n     def forward(self, x):\n         batch_size, channels, width, height = x.size()\n         x = x.view(batch_size, -1)\n \n         x = self.layer_1(x)\n         x = torch.relu(x)\n \n         x = self.layer_2(x)\n         x = torch.relu(x)\n \n         x = self.layer_3(x)\n         x = torch.log_softmax(x, dim=1)\n \n         return x\n \n     def cross_entropy_loss(self, logits, labels):\n         return F.nll_loss(logits, labels)\n \n     def accuracy(self, logits, labels):\n         _, predicted = torch.max(logits.data, 1)\n         correct = (predicted == labels).sum().item()\n         accuracy = correct / len(labels)\n         return torch.tensor(accuracy)\n \n     def training_step(self, train_batch, batch_idx):\n         x, y = train_batch\n         logits = self.forward(x)\n         loss = self.cross_entropy_loss(logits, y)\n         accuracy = self.accuracy(logits, y)\n \n         self.log(\"ptl/train_loss\", loss)\n         self.log(\"ptl/train_accuracy\", accuracy)\n         return loss\n \n     def validation_step(self, val_batch, batch_idx):\n         x, y = val_batch\n         logits = self.forward(x)\n         loss = self.cross_entropy_loss(logits, y)\n         accuracy = self.accuracy(logits, y)\n         return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n \n     def validation_epoch_end(self, outputs):\n         avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n         avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n         self.log(\"ptl/val_loss\", avg_loss)\n         self.log(\"ptl/val_accuracy\", avg_acc)\n \n \n     @staticmethod\n     def download_data(data_dir):\n         transform = transforms.Compose([\n             transforms.ToTensor(),\n             transforms.Normalize((0.1307, ), (0.3081, ))\n         ])\n         return MNIST(data_dir, train=True, download=True, transform=transform)\n \n     def prepare_data(self):\n         mnist_train = self.download_data(self.data_dir)\n \n         self.mnist_train, self.mnist_val = random_split(\n             mnist_train, [55000, 5000])\n \n     def train_dataloader(self):\n         return DataLoader(self.mnist_train, batch_size=int(self.batch_size))\n \n     def val_dataloader(self):\n         return DataLoader(self.mnist_val, batch_size=int(self.batch_size))\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n         return optimizer\n \n def train_mnist_tune(config, data_dir=None, num_epochs=10, num_gpus=0):\n     model = LightningMNISTClassifier(config, data_dir)\n     trainer = pl.Trainer(\n         max_epochs=num_epochs,\n         gpus=num_gpus,\n         logger=TensorBoardLogger(\n             save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n         progress_bar_refresh_rate=0,\n         callbacks=[\n             TuneReportCallback(\n                 {\n                     \"loss\": \"ptl/val_loss\",\n                     \"mean_accuracy\": \"ptl/val_accuracy\"\n                 },\n                 on=\"validation_end\")\n         ])\n     trainer.fit(model)\n \n \n def train_mnist_tune_checkpoint(config,\n                                 checkpoint_dir=None,\n                                 data_dir=None,\n                                 num_epochs=10):\n \n     trainer = pl.Trainer(\n         max_epochs=num_epochs,\n         gpus=0,\n         logger=TensorBoardLogger(\n             save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n         progress_bar_refresh_rate=0,\n         callbacks=[\n             TuneReportCheckpointCallback(\n                 metrics={\n                     \"loss\": \"ptl/val_loss\",\n                     \"mean_accuracy\": \"ptl/val_accuracy\"\n                 },\n                 filename=\"checkpoint\",\n                 on=\"validation_end\")\n         ])\n     if checkpoint_dir:\n         ckpt = pl_load(\n             os.path.join(checkpoint_dir, \"checkpoint\"),\n             map_location=lambda storage, loc: storage)\n         model = LightningMNISTClassifier._load_model_state(ckpt, config=config)\n         trainer.current_epoch = ckpt[\"epoch\"]\n     else:\n         model = LightningMNISTClassifier(config=config, data_dir=data_dir)\n \n     trainer.fit(model)\n \n \n def tune_mnist_pbt(num_samples=64, num_epochs=3):\n \n     data_dir = mkdtemp(prefix=\"mnist_data_\")\n \n     LightningMNISTClassifier.download_data(data_dir)\n \n     ray.init(address='auto')\n \n     config = {\n         \"layer_1_size\": tune.choice([32, 64, 128]),\n         \"layer_2_size\": tune.choice([64, 128, 256]),\n         \"lr\": 1e-3,\n         \"batch_size\": 64,\n     }\n \n     scheduler = PopulationBasedTraining(\n         time_attr=\"training_iteration\",\n         metric=\"loss\",\n         mode=\"min\",\n         perturbation_interval=1, # setting this to 1 so that the sync issue happens immediately after the first epoch\n         hyperparam_mutations={\n             \"lr\": tune.loguniform(1e-4, 1e-1),\n             \"batch_size\": [32, 64, 128]\n         })\n \n     reporter = CLIReporter(\n         parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n         metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n \n     sync_config = tune.SyncConfig(\n         sync_to_driver=DockerSyncer\n     )\n \n     tune.run(\n         partial(\n             train_mnist_tune_checkpoint,\n             data_dir=data_dir,\n             num_epochs=num_epochs),\n         resources_per_trial={\n             \"cpu\": 1,\n             \"gpu\": 0\n         },\n         config=config,\n         sync_config=sync_config,\n         num_samples=num_samples,\n         scheduler=scheduler,\n         progress_reporter=reporter,\n         fail_fast=True,\n         queue_trials=True,\n         reuse_actors=True,\n         name=\"tune_mnist_pbt\")\n \n     shutil.rmtree(data_dir)\n \n if __name__ == '__main__':\n     tune_mnist_pbt()\n </denchmark-code>\n \n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tuxa", "commentT": "2020-11-19T20:03:56Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/krfricke>@krfricke</denchmark-link>\n  it seems like the wrong checkpoint is being synced?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tuxa", "commentT": "2020-11-19T20:06:44Z", "comment_text": "\n \t\tYep, this looks odd. Thanks for raising this issue <denchmark-link:https://github.com/tuxa>@tuxa</denchmark-link>\n , I'll look into this.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tuxa", "commentT": "2020-11-21T00:53:14Z", "comment_text": "\n \t\tOK I'm able to repro this and found one of the key issues. It seems like on the trainable, we're not using a logger-creator and so the logdir that the trainable is using is different from expected:\n <denchmark-code>(pid=raylet, ip=172.31.63.45) 2020-11-21 00:33:00,087\tINFO trainable.py:329 -- Restored on 172.31.63.45 from checkpoint: /root/ray_results/2020-11-21_00-32-59b631ud_k/tmpqd39pvc1restore_from_object/checkpoint\n </denchmark-code>\n \n while the actual checkpoint should be:\n <denchmark-code>2020-11-21 00:33:02,820\tDEBUG syncer.py:281 -- Syncing from ('172.31.63.45', '/root/ray_results/pbt_test/PBTBenchmarkExample_15f0c_00004_4_2020-11-21_00-32-50') to /root/ray_results/pbt_test/PBTBenchmarkExample_15f0c_00004_4_2020-11-21_00-32-50/\n </denchmark-code>\n \n (with the PBTBenchmark prefix)\n \t\t"}}}, "commit": {"commit_id": "e59fe65d3dee1803de2da5ab200059e992de7f9b", "commit_author": "Richard Liaw", "commitT": "2020-11-23 14:29:41-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "doc\\source\\tune\\user-guide.rst", "file_new_name": "doc\\source\\tune\\user-guide.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "689", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\ray\\autoscaler\\_private\\cli_logger.py", "file_new_name": "python\\ray\\autoscaler\\_private\\cli_logger.py", "file_complexity": {"file_NLOC": "419", "file_CCN": "73", "file_NToken": "2493"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "352", "deleted_lines": null, "method_info": {"method_name": "_set_verbosity", "method_params": "self,x", "method_startline": "350", "method_endline": "352", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "286", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self", "method_startline": "282", "method_endline": "296", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "54", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "344,345,346", "deleted_lines": "343", "method_info": {"method_name": "verbosity", "method_params": "self", "method_startline": "343", "method_endline": "348", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "26", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\autoscaler\\_private\\commands.py", "file_new_name": "python\\ray\\autoscaler\\_private\\commands.py", "file_complexity": {"file_NLOC": "892", "file_CCN": "45", "file_NToken": "4963"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "975,976,977", "deleted_lines": "975,976", "method_info": {"method_name": "rsync_to_node", "method_params": "node_id,is_head_node", "method_startline": "947", "method_endline": "980", "method_complexity": {"method_NLOC": "32", "method_CCN": "5", "method_NToken": "166", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\autoscaler\\sdk.py", "file_new_name": "python\\ray\\autoscaler\\sdk.py", "file_complexity": {"file_NLOC": "216", "file_CCN": "15", "file_NToken": "816"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "210,211,212", "deleted_lines": null, "method_info": {"method_name": "configure_logging", "method_params": "None,None,None", "method_startline": "210", "method_endline": "212", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "30", "method_nesting_level": "0"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\tune\\examples\\pbt_example.py", "file_new_name": "python\\ray\\tune\\examples\\pbt_example.py", "file_complexity": {"file_NLOC": "104", "file_CCN": "8", "file_NToken": "504"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "88,89,90,91,93,94,95,96", "deleted_lines": "89"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\tune\\integration\\docker.py", "file_new_name": "python\\ray\\tune\\integration\\docker.py", "file_complexity": {"file_NLOC": "89", "file_CCN": "8", "file_NToken": "400"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5,8,17,18,19,42,43,44", "deleted_lines": "5"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\tune\\ray_trial_executor.py", "file_new_name": "python\\ray\\tune\\ray_trial_executor.py", "file_complexity": {"file_NLOC": "538", "file_CCN": "129", "file_NToken": "3456"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "401,402", "deleted_lines": "401,402"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\tune\\trainable.py", "file_new_name": "python\\ray\\tune\\trainable.py", "file_complexity": {"file_NLOC": "374", "file_CCN": "79", "file_NToken": "2279"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "394,395,396,397,398,399", "deleted_lines": "394", "method_info": {"method_name": "reset", "method_params": "self,new_config,logger_creator", "method_startline": "384", "method_endline": "422", "method_complexity": {"method_NLOC": "26", "method_CCN": "3", "method_NToken": "155", "method_nesting_level": "1"}}}}}}}}