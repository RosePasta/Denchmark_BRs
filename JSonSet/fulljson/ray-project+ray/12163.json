{"BR": {"BR_id": "12163", "BR_author": "Manuscrit", "BRopenT": "2020-11-19T15:28:24Z", "BRcloseT": "2020-12-01T09:45:18Z", "BR_text": {"BRsummary": "[rllib] LR not updated by LearningRateSchedule in TorchPolicy", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n In a  (I did not test with ) with  mixin, the lr value of the optimizers lr_group are not updated (LR stays constant). The  attribute has the right value but the optimizers are being updated to use it.\n This may be related/similar to <denchmark-link:https://github.com/ray-project/ray/pull/6101>#6101</denchmark-link>\n  and <denchmark-link:https://github.com/ray-project/ray/issues/10423>#10423</denchmark-link>\n .\n The  method of the Policy is not overridden when using the  mixin.\n Tested with DQN and PPO (PyTorch only).\n Ray version and other system information (Python version, TensorFlow version, OS):\n ray version 1.0.0 and nightly\n python 3.8.5\n torch 1.7.0\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n import argparse\n \n import ray\n from ray import tune\n from ray.rllib.examples.env.parametric_actions_cartpole import \\\n     ParametricActionsCartPole\n from ray.rllib.examples.models.parametric_actions_model import \\\n     ParametricActionsModel, TorchParametricActionsModel\n from ray.rllib.models import ModelCatalog\n from ray.rllib.utils.test_utils import check_learning_achieved\n from ray.tune.registry import register_env\n from ray.rllib.agents.callbacks import DefaultCallbacks\n \n parser = argparse.ArgumentParser()\n parser.add_argument(\"--run\", type=str, default=\"PPO\")\n parser.add_argument(\"--torch\", action=\"store_true\")\n parser.add_argument(\"--as-test\", action=\"store_true\")\n parser.add_argument(\"--stop-iters\", type=int, default=200)\n parser.add_argument(\"--stop-reward\", type=float, default=150.0)\n parser.add_argument(\"--stop-timesteps\", type=int, default=100000)\n \n \n class MyCallbacks(DefaultCallbacks):\n \n     def on_train_result(self, *, trainer, result: dict, **kwargs):\n         def print_true_lr(policy, policy_id):\n             print(\"policy.cur_lr\", policy.cur_lr)\n             for j, opt in enumerate(policy._optimizers):\n                 print(policy_id, j, [p[\"lr\"] for p in opt.param_groups])\n                 for p in opt.param_groups:\n                     assert p[\"lr\"] == policy.cur_lr, f'should have p[\"lr\"] == policy.cur_lr but got p[\"lr\"] ' \\\n                                                      f'= {p[\"lr\"]} and policy.cur_lr {policy.cur_lr}'\n         trainer.workers.foreach_policy(print_true_lr)\n \n \n if __name__ == \"__main__\":\n     args = parser.parse_args()\n     ray.init()\n \n     register_env(\"pa_cartpole\", lambda _: ParametricActionsCartPole(10))\n     ModelCatalog.register_custom_model(\n         \"pa_model\", TorchParametricActionsModel\n         if args.torch else ParametricActionsModel)\n \n     if args.run == \"DQN\":\n         cfg = {\n             # TODO(ekl) we need to set these to prevent the masked values\n             # from being further processed in DistributionalQModel, which\n             # would mess up the masking. It is possible to support these if we\n             # defined a custom DistributionalQModel that is aware of masking.\n             \"hiddens\": [],\n             \"dueling\": False,\n         }\n     else:\n         cfg = {}\n \n     config = dict({\n         \"env\": \"pa_cartpole\",\n         \"model\": {\n             \"custom_model\": \"pa_model\",\n         },\n         \"num_workers\": 0,\n         \"framework\": \"torch\" if args.torch else \"tf\",\n \n         \"callbacks\": MyCallbacks,\n         # === Optimization ===\n         # Learning rate for adam optimizer\n         \"lr\": 1e-3,\n         # Learning rate schedule\n         \"lr_schedule\": [(0, 1e-3), (20000, 3e-5 / 1e9)],\n \n     }, **cfg)\n \n     stop = {\n         \"training_iteration\": args.stop_iters,\n         \"timesteps_total\": args.stop_timesteps,\n         \"episode_reward_mean\": args.stop_reward,\n     }\n \n     results = tune.run(args.run, stop=stop, config=config, verbose=1)\n \n     if args.as_test:\n         check_learning_achieved(results, args.stop_reward)\n \n     ray.shutdown()\n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Manuscrit", "commentT": "2020-11-25T09:46:34Z", "comment_text": "\n \t\tThanks for filing this issue <denchmark-link:https://github.com/Manuscrit>@Manuscrit</denchmark-link>\n \n PR: <denchmark-link:https://github.com/ray-project/ray/pull/12396>#12396</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "6475297bd3774fa1bdad5233164d2aa65e2c86e5", "commit_author": "Sven Mika", "commitT": "2020-11-26 13:14:11+01:00", "commit_complexity": {"commit_NLOC": "0.9375", "commit_CCN": "1.0", "commit_Nprams": "0.8125"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "rllib\\agents\\ppo\\tests\\test_ppo.py", "file_new_name": "rllib\\agents\\ppo\\tests\\test_ppo.py", "file_complexity": {"file_NLOC": "325", "file_CCN": "53", "file_NToken": "2670"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,73", "deleted_lines": "48", "method_info": {"method_name": "test_ppo_compilation", "method_params": "self", "method_startline": "48", "method_endline": "74", "method_complexity": {"method_NLOC": "24", "method_CCN": "5", "method_NToken": "170", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "59,60,61", "deleted_lines": null, "method_info": {"method_name": "on_train_result", "method_params": "self,trainer,dict,kwargs", "method_startline": "59", "method_endline": "61", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "39", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "48,49,50,51,52,53,54,55,56,57", "deleted_lines": "48", "method_info": {"method_name": "_check_lr_tf", "method_params": "policy,policy_id", "method_startline": "48", "method_endline": "57", "method_complexity": {"method_NLOC": "10", "method_CCN": "2", "method_NToken": "68", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "73,76,77,78", "deleted_lines": null, "method_info": {"method_name": "test_ppo_compilation_and_lr_schedule", "method_params": "self", "method_startline": "73", "method_endline": "102", "method_complexity": {"method_NLOC": "25", "method_CCN": "5", "method_NToken": "176", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "42,43,44,45", "deleted_lines": null, "method_info": {"method_name": "_check_lr_torch", "method_params": "policy,policy_id", "method_startline": "42", "method_endline": "45", "method_complexity": {"method_NLOC": "4", "method_CCN": "3", "method_NToken": "37", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\policy\\torch_policy.py", "file_new_name": "rllib\\policy\\torch_policy.py", "file_complexity": {"file_NLOC": "478", "file_CCN": "41", "file_NToken": "3051"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "619,623", "method_info": {"method_name": "optimizer", "method_params": "self", "method_startline": "619", "method_endline": "623", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "31", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "615", "deleted_lines": "615,617,618,619", "method_info": {"method_name": "on_global_var_update", "method_params": "self,global_vars", "method_startline": "614", "method_endline": "619", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "52", "method_nesting_level": "1"}}}}}}}}