{"BR": {"BR_id": "2705", "BR_author": "fritzo", "BRopenT": "2020-12-03T13:27:00Z", "BRcloseT": "2020-12-04T04:37:33Z", "BR_text": {"BRsummary": "PyroModule.requires_grad_(False).__call__() issues unwanted warning", "BRdescription": "\n After training a PyroModule model, I'd like to detach it and use it elsewhere in a \"frozen\" state. One obvious way to do so would be to call my_module.requires_grad_(False) to detach all parameters. Doing so results in the desired behavior, but has the side effect of triggering the following warning (many many times):\n <denchmark-code>/Users/fobermey/github/pyro-ppl/pyro/pyro/primitives.py:361: UserWarning: linear.bias was not registered in the param store because requires_grad=False\n   \" requires_grad=False\")\n </denchmark-code>\n \n It is unclear to me how to resolve this. Should we provide a detach_() method for modules that converts all PyroParams to Tensors? Should we simply remove that warning if it occurs in a PyroModule? Should we be calling some other method like my_module.train(False)?\n \t"}, "comments": {}}, "commit": {"commit_id": "a707d96c36490fbe65d1a22bb36d82ae957b1158", "commit_author": "Fritz Obermeyer", "commitT": "2020-12-03 22:37:33-06:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pyro\\primitives.py", "file_new_name": "pyro\\primitives.py", "file_complexity": {"file_NLOC": "268", "file_CCN": "35", "file_NToken": "992"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "359,360,361,362", "deleted_lines": "359,360,361", "method_info": {"method_name": "module", "method_params": "name,nn_module,update_module_params", "method_startline": "324", "method_endline": "380", "method_complexity": {"method_NLOC": "33", "method_CCN": "12", "method_NToken": "212", "method_nesting_level": "0"}}}}}}}}