{"BR": {"BR_id": "1778", "BR_author": "cfperez", "BRopenT": "2019-02-28T21:47:14Z", "BRcloseT": "2019-03-06T19:23:40Z", "BR_text": {"BRsummary": "[discussion] Pyro sets all Parameters to requires_grad=True", "BRdescription": "\n <denchmark-h:h3>Issue Description</denchmark-h>\n \n pyro.module() set requires_grad=True fo all nn.Parameters in a module, even if they were created with requires_grad=False.\n Using register_buffer() solves this problem, but requires changing code that otherwise works in PyTorch.\n It is hard to see how this would not be considered unexpected behavior. The question is how important is it, and if it is worth changing the behavior (and risk breaking other code?) or simply logging a warning + adding documentation.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n OSX, Python 3.6\n PyTorch 1.0\n Pyro 0.3.1\n \n <denchmark-h:h3>Code Snippet</denchmark-h>\n \n import torch\n import pyro\n \n class Module(torch.nn.Module):\n     def __init__(self):\n         super().__init__()\n         self.with_grad = torch.nn.Parameter(torch.zeros(1))\n         self.no_grad = torch.nn.Parameter(torch.zeros(1), requires_grad=False)\n \n m = Module()\n assert m.with_grad.requires_grad\n assert not m.no_grad.requires_grad\n \n pyro_m = pyro.module('module', m)\n assert pyro_m.with_grad.requires_grad\n assert not pyro_m.no_grad.requires_grad\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "cfperez", "commentT": "2019-02-28T23:07:52Z", "comment_text": "\n \t\tThanks for the report <denchmark-link:https://github.com/cfperez>@cfperez</denchmark-link>\n  . Can you clarify your expected/desired behavior? When a PyTorch param has  does the parameter simply not get updated during training? If that is the case, I guess we could ignore such params in  and possible emit a warning recommending use of  (so as to maintain plausible backwards compatibility with previous versions of Pyro).\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "cfperez", "commentT": "2019-03-01T02:06:33Z", "comment_text": "\n \t\t\n does the parameter simply not get updated during training\n \n yes, from <denchmark-link:https://pytorch.org/docs/stable/notes/autograd.html#excluding-subgraphs>the pytorch docs</denchmark-link>\n \n \n we could ignore such params in pyro.module and possible emit a warning recommending use of register_buffer()\n \n i can make this fix; i think this might have been a later pytorch feature we didnt account for - i seem to remember that when module was implemented in pyro (~pytorch 0.2), Parameters were defined to be Variables with requires_grad = True.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "cfperez", "commentT": "2019-03-01T03:41:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jpchen>@jpchen</denchmark-link>\n  Great. Just to ensure we're in agreement: I believe the correct behavior is to  calling  inside  whenever the param has . Specifically, I believe changes should be limited to , and there should be no changes required to . Let me know if you disagree. Otherwise, thanks for implementing this!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "cfperez", "commentT": "2019-03-01T08:56:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cfperez>@cfperez</denchmark-link>\n  what is your use case of non-grad parameters of a nn?  if you have fixed weights or non-learnable params can't you just not wrap it in ?\n \t\t"}}}, "commit": {"commit_id": "3a4b64a6f9d520677d3dd3f73707b7b4cfba4c51", "commit_author": "JP", "commitT": "2019-03-01 15:38:51-08:00", "commit_complexity": {"commit_NLOC": "0.7777777777777778", "commit_CCN": "0.7777777777777778", "commit_Nprams": "0.7777777777777778"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pyro\\primitives.py", "file_new_name": "pyro\\primitives.py", "file_complexity": {"file_NLOC": "237", "file_CCN": "25", "file_NToken": "775"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "248,249,250,251,252,253,254,255,256,257,258", "deleted_lines": "248,249,250,251,252,253,254", "method_info": {"method_name": "module", "method_params": "name,nn_module,update_module_params", "method_startline": "221", "method_endline": "276", "method_complexity": {"method_NLOC": "32", "method_CCN": "11", "method_NToken": "213", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "setup.py", "file_new_name": "setup.py", "file_complexity": {"file_NLOC": "105", "file_CCN": "0", "file_NToken": "485"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "63", "deleted_lines": "63"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\params\\test_module.py", "file_new_name": "tests\\params\\test_module.py", "file_complexity": {"file_NLOC": "73", "file_CCN": "15", "file_NToken": "612"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "72,73", "deleted_lines": null, "method_info": {"method_name": "test_param_no_grad.forward", "method_params": "self,s", "method_startline": "72", "method_endline": "73", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "8", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "67,68,69,70", "deleted_lines": null, "method_info": {"method_name": "test_param_no_grad.__init__", "method_params": "self", "method_startline": "67", "method_endline": "70", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "45", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "65,66,67,68,69,70,71,72,73,74,75,76,77,78", "deleted_lines": null, "method_info": {"method_name": "test_param_no_grad", "method_params": "nn_module", "method_startline": "65", "method_endline": "78", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "62", "method_nesting_level": "0"}}}}}}}}