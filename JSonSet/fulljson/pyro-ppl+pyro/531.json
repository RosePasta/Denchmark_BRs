{"BR": {"BR_id": "531", "BR_author": "fritzo", "BRopenT": "2017-11-08T04:10:24Z", "BRcloseT": "2017-11-08T23:39:37Z", "BR_text": {"BRsummary": "site[\"scale\"] is double counted at non-reparameterized sites", "BRdescription": "\n <denchmark-h:h2>The Problem</denchmark-h>\n \n Consider the surrogate elbo at a nonreparameterized site (as <denchmark-link:https://github.com/uber/pyro/blob/ece8ccc167695f81aa7ec1fed74a70c8fc15254b/pyro/infer/trace_elbo.py#L150>computed in</denchmark-link>\n  )\n log_p = model_site[\"log_pdf\"]\n log_q = guide_site[\"log_pdf\"]\n surrogate_elbo = log_p + (log_p - log_q).detach() * log_q\n Now suppose this site occurs in an  where we're subsampling 1% of the data, so that a  sets . Then  <denchmark-link:https://github.com/uber/pyro/blob/ece8ccc167695f81aa7ec1fed74a70c8fc15254b/pyro/poutine/trace.py#L71>will scale</denchmark-link>\n  both  and  by 100. Therefore the 's first term  will be scaled by 100, but the second term  will be scaled by 100 * 100.\n <denchmark-h:h2>Scope</denchmark-h>\n \n This should only affect models in one of the following scenarios:\n \n The model has both global and local sites, and the local sites are non-reparameterized and subsampled.\n The model has local sites that are non-reparameterized and subsampled and includes model parameters at that site (not only guide parameters).\n \n <denchmark-h:h2>Possible Solutions</denchmark-h>\n \n \n Handle site[\"scale\"] separately, outside of Trace.log_pdf(). This scale is already wired through Trace_ELBO and TraceGraph_ELBO as weight.\n Pros: Avoids being too clever. Plays well with scale that varies across a batch (as in BranchPoutine).\n Cons: May require special treatment of site[\"scale\"] in many places outside of Trace_ELBO and TraceGraph_ELBO.\n Keep the existing logic of Trace.log_pdf(), and remove a factor of site[\"scale\"] from the (log_p - log_q).detach() * log_q term.\n Pros: Minimally invasive.\n Cons: Seems like a hack. Does not play well with scales that vary across a batch.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "fritzo", "commentT": "2017-11-08T08:17:53Z", "comment_text": "\n \t\t\n but the second term (log_p - log_q).detach() * log_q will be scaled by 100 * 100.\n \n Is this really a bug?\n log_p_scaled is a good estimate of log_p_true, similarly for log_q. If you want to compute an estimate of (log_p_true - log_q_true) * log_q_true then (log_p_scaled - log_q_scaled) * log_q_scaled gives you that.\n If you get lucky and log_p_scaled = log_p_true (likewise for q) then you'll compute the exact thing. This wouldn't be the case if you pulled out an extra scaling factor as proposed here?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "fritzo", "commentT": "2017-11-08T16:48:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/null-a>@null-a</denchmark-link>\n  Yes I think you're right that this works as intended in . Thanks for explaining!\n I'm still a little confused about , which for an entirely local model <denchmark-link:https://github.com/uber/pyro/blob/ece8ccc167695f81aa7ec1fed74a70c8fc15254b/pyro/infer/tracegraph_elbo.py#L276>computes</denchmark-link>\n   independently for each datum in a minibatch. I would expect the  values to be scaled by , whereas in practice it is the  and  that are scaled, so that  is scaled . Do you think it's possible that  is correct, but  is incorrect?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "fritzo", "commentT": "2017-11-08T17:24:22Z", "comment_text": "\n \t\ti'm afraid <denchmark-link:https://github.com/fritzo>@fritzo</denchmark-link>\n  is right. there's an implicit rao-blackwellization that happens when you subsample which makes it a bit confusing. we'll fix it.\n \t\t"}}}, "commit": {"commit_id": "b94f06a2257bf362208c28048544deb297aa5e24", "commit_author": "Fritz Obermeyer", "commitT": "2017-11-08 15:39:36-08:00", "commit_complexity": {"commit_NLOC": "0.3611111111111111", "commit_CCN": "0.9722222222222222", "commit_Nprams": "0.9722222222222222"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pyro\\infer\\trace_elbo.py", "file_new_name": "pyro\\infer\\trace_elbo.py", "file_complexity": {"file_NLOC": "116", "file_CCN": "34", "file_NToken": "956"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "138,139,140,141,142,144,145,147,151,152", "deleted_lines": "138,139,140,141,142,144,146,150,151", "method_info": {"method_name": "loss_and_grads", "method_params": "self,model,guide,args,kwargs", "method_startline": "123", "method_endline": "180", "method_complexity": {"method_NLOC": "40", "method_CCN": "14", "method_NToken": "307", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pyro\\infer\\tracegraph_elbo.py", "file_new_name": "pyro\\infer\\tracegraph_elbo.py", "file_complexity": {"file_NLOC": "204", "file_CCN": "57", "file_NToken": "1517"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "269,270,271,276,278", "deleted_lines": "273,274,276,277", "method_info": {"method_name": "loss_and_grads", "method_params": "self,model,guide,args,kwargs", "method_startline": "91", "method_endline": "298", "method_complexity": {"method_NLOC": "137", "method_CCN": "46", "method_NToken": "1079", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\infer\\test_gradient.py", "file_complexity": {"file_NLOC": "78", "file_CCN": "11", "file_NToken": "935"}}, "file_3": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "tests\\infer\\test_gradient_step.py", "file_new_name": "None", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}}}}