{"BR": {"BR_id": "1730", "BR_author": "fehiepsi", "BRopenT": "2019-02-01T17:45:21Z", "BRcloseT": "2019-07-16T01:12:00Z", "BR_text": {"BRsummary": "MCMC does not work well with multi-chains in CPU due to Memory Error", "BRdescription": "\n The following script is copied exactly from Bayesian regression tutorial (except num_samples=3000 and num_chains=2). While running it, I get MemoryError. I have tried to debug this memory error by changing various settings (ulimit, shared memory segment) in my system but no hope. I get this issue in two systems which I have so I guess this will not happen for only me. If so, then this is a serious problem which can come from how we use torch.multiprocessing in Pyro.\n <denchmark-code>import numpy as np\n import pandas as pd\n import torch\n \n import pyro\n import pyro.distributions as dist\n from pyro.infer.mcmc import MCMC, NUTS\n \n pyro.enable_validation(True)\n pyro.set_rng_seed(1)\n DATA_URL = \"https://d2fefpcigoriu7.cloudfront.net/datasets/rugged_data.csv\"\n rugged_data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n \n #torch.multiprocessing.set_sharing_strategy(\"file_system\")\n \n \n def model(is_cont_africa, ruggedness, log_gdp):\n     a = pyro.sample(\"a\", dist.Normal(8., 1000.))\n     b_a = pyro.sample(\"bA\", dist.Normal(0., 1.))\n     b_r = pyro.sample(\"bR\", dist.Normal(0., 1.))\n     b_ar = pyro.sample(\"bAR\", dist.Normal(0., 1.))\n     sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n     mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness\n     with pyro.iarange(\"data\", len(ruggedness)):\n         pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=log_gdp)\n \n         \n df = rugged_data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n df = df[np.isfinite(df.rgdppc_2000)]\n df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n train = torch.tensor(df.values, dtype=torch.float)\n is_cont_africa, ruggedness, log_gdp = train[:, 0], train[:, 1], train[:, 2]\n \n nuts_kernel = NUTS(model, adapt_step_size=True)\n hmc_posterior = MCMC(nuts_kernel, num_samples=4000, warmup_steps=1000, num_chains=4).run(is_cont_africa, ruggedness, log_gdp)\n </denchmark-code>\n \n Here is its <denchmark-link:https://gist.github.com/fehiepsi/03a28b713e90aa4b13cd644b68b27782>backtrack</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "fehiepsi", "commentT": "2019-02-01T18:54:02Z", "comment_text": "\n \t\tCould you also mention the pytorch version and OS? Does the memory keep increasing leading to the crash?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "fehiepsi", "commentT": "2019-02-01T19:38:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/neerajprad>@neerajprad</denchmark-link>\n  The following system is tested:\n \n Python: python 3.6, 3.7\n OS: ubuntu 16.04, 18.04\n PyTorch: 0.4.1 (with old version of pyro), 1.0, nightly, pytorch-cpu, pytorch\n \n Yes, memory keeps increasing, but I think it is a normal behaviour with MCMC? Shared memory increases to about 400MB, then the error happens. Did you have the above problem?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "fehiepsi", "commentT": "2019-02-01T20:02:19Z", "comment_text": "\n \t\tI don't see this on mac, but I can replicate it on my linux system. The shared memory shouldn't really increase because the queue is getting cleared in the main process.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "fehiepsi", "commentT": "2019-02-01T20:44:45Z", "comment_text": "\n \t\t\n I don't see this on mac, but I can replicate it on my linux system. The shared memory shouldn't really increase because the queue is getting cleared in the main process.\n \n Interesting! Good to know that it is a system issue. I have tried so many ways to detect and resolve that error. Your information is the best one I have. Thanks <denchmark-link:https://github.com/neerajprad>@neerajprad</denchmark-link>\n  !\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "fehiepsi", "commentT": "2019-02-01T22:44:10Z", "comment_text": "\n \t\tPossibly related <denchmark-link:https://github.com/pytorch/pytorch/issues/13246>pytorch/pytorch#13246</denchmark-link>\n .\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "fehiepsi", "commentT": "2019-02-01T22:46:55Z", "comment_text": "\n \t\tI think we are hitting some system level limitations on shared memory. If you change the _ParallelSampler to yield None, args[1].. instead of the trace, you will find that no error is thrown. I think what is happening is that even though we read from the queue, the objects remain in shared memory and after a certain point we hit some limitation due to which the OS ends up killing the process.\n One thing would be to find out what limitation are we hitting into and increase the limit in . But I think we shouldn't do that. I think this issue will resolve once <denchmark-link:https://github.com/pyro-ppl/pyro/issues/1725>#1725</denchmark-link>\n  is resolved, since we won't be keeping the objects in shared memory but reducing it to some other representation alongside. This will ensure that our shared memory resource consumption remains fixed during the entire sampling.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "fehiepsi", "commentT": "2019-02-01T23:10:24Z", "comment_text": "\n \t\tI have tried to increase various system configurations but no hope. Hope that this will be resolved when <denchmark-link:https://github.com/pyro-ppl/pyro/issues/1725>#1725</denchmark-link>\n  is resolved.\n Btw, I found an interesting thing during the way: using file_system (in torch.multiprocessing.set_sharing_strategy) speeds up inference a lot (1.5x - 3x) over file_descriptor strategy (the default one). :)\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "fehiepsi", "commentT": "2019-02-02T07:22:03Z", "comment_text": "\n \t\t\n using file_system (in torch.multiprocessing.set_sharing_strategy) speeds up inference a lot (1.5x - 3x) over file_descriptor strategy (the default one). :)\n \n Do you see this speed-up on this example too? I didn't see anything noticeable.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "fehiepsi", "commentT": "2019-02-02T14:35:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/neerajprad>@neerajprad</denchmark-link>\n  , for the above example (with ,  to avoid Memory error,  to 20 to reduce its effect on timing):\n \n num_chains=2: 2s vs 5s. (averaging for all chains: 180its/s vs 90its/s)\n num_chains=4: 3s vs 10s. (160its/s vs 50its/s)\n num_chains=6: 5s vs 15s. (100its/s vs 33its/s)\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "fehiepsi", "commentT": "2019-07-09T18:26:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/fehiepsi>@fehiepsi</denchmark-link>\n  - This seems to work fine now. Could you check this again on your system and close this issue if it is resolved?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "fehiepsi", "commentT": "2019-07-09T20:54:14Z", "comment_text": "\n \t\tYup, it works nicely now. :)\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "fehiepsi", "commentT": "2019-07-13T13:11:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/neerajprad>@neerajprad</denchmark-link>\n  I still got this problem with higher number of samples and .\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "fehiepsi", "commentT": "2019-07-14T03:54:39Z", "comment_text": "\n \t\tDoes your proposed solution from the <denchmark-link:https://forum.pyro.ai/t/error-when-sampling-begins-with-multiple-chains/1138/7>forum</denchmark-link>\n  address this issue?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "fehiepsi", "commentT": "2019-07-14T03:57:58Z", "comment_text": "\n \t\tYes, using the new api allows me getting higher num_samples, but the root problem still remains there. The solution in forum fixes this issue.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "fehiepsi", "commentT": "2019-07-14T16:02:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/neerajprad>@neerajprad</denchmark-link>\n  I made some benchmarks to see if there are regressions using clone operator.\n <denchmark-code>import torch\n torch.set_default_tensor_type(torch.cuda.FloatTensor)\n \n params = {}\n for i in range(100):\n     params['x{}'.format(i)] = torch.randn(100)\n \n %%time\n for n in range(10000):\n     cloned_params = {}\n     for k, v in params.items():\n         cloned_params[k] = v.clone()\n </denchmark-code>\n \n It took 8.9s to generate 10000 samples (each sample has 100 sites and each site is a 100-D tensor). So the overhead is somehow large (but small comparing to sampling) when there are many sites are involved.\n If checkpoint solution is not elegant (at least not elegant to me because how to specify checkpoint value depending on system/platform), we may tell MCMC only generate flatten values. The overhead will be small\n <denchmark-code>params_flat = torch.cat(list(params.values()))\n \n %%time\n for n in range(10000):\n     cloned_params = params_flat.clone()\n </denchmark-code>\n \n took 100ms. After done sampling, we'll just need to populate stacked flatten samples to stacked (dict type) samples, which should be cheap. I'll make a PR to address this and benchmark carefully.\n \t\t"}}}, "commit": {"commit_id": "5976a16fa0eebb7674c374b972481d8568c9ffb2", "commit_author": "Du Phan", "commitT": "2019-05-07 11:16:34-07:00", "commit_complexity": {"commit_NLOC": "0.85", "commit_CCN": "0.8333333333333334", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 20, "file_old_name": "pyro\\infer\\mcmc\\hmc.py", "file_new_name": "pyro\\infer\\mcmc\\hmc.py", "file_complexity": {"file_NLOC": "267", "file_CCN": "35", "file_NToken": "1428"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "164", "deleted_lines": "156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171", "method_info": {"method_name": "_potential_energy", "method_params": "self,z", "method_startline": "156", "method_endline": "171", "method_complexity": {"method_NLOC": "13", "method_CCN": "5", "method_NToken": "112", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "367,368,369,370,371", "method_info": {"method_name": "_initialize_step_size", "method_params": "self", "method_startline": "367", "method_endline": "372", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "44", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "164,185", "deleted_lines": "156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189", "method_info": {"method_name": "_find_reasonable_step_size", "method_params": "self", "method_startline": "152", "method_endline": "189", "method_complexity": {"method_NLOC": "26", "method_CCN": "5", "method_NToken": "216", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "140", "deleted_lines": "139,140", "method_info": {"method_name": "_energy", "method_params": "self,z,r", "method_startline": "139", "method_endline": "140", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "23", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": null, "deleted_lines": "326,327", "method_info": {"method_name": "initial_params", "method_params": "self,params", "method_startline": "324", "method_endline": "327", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "25", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "239,240,241,242,243,244,245,246,247,248,249,250,251,252", "deleted_lines": "252", "method_info": {"method_name": "_initialize_adapter", "method_params": "self", "method_startline": "239", "method_endline": "253", "method_complexity": {"method_NLOC": "14", "method_CCN": "4", "method_NToken": "113", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "217", "deleted_lines": null, "method_info": {"method_name": "initial_params", "method_params": "self", "method_startline": "216", "method_endline": "217", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "185", "deleted_lines": "178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193", "method_info": {"method_name": "_potential_energy_jit.compiled", "method_params": "zi", "method_startline": "178", "method_endline": "193", "method_complexity": {"method_NLOC": "14", "method_CCN": "5", "method_NToken": "126", "method_nesting_level": "2"}}}, "hunk_8": {"Ismethod": 1, "added_lines": null, "deleted_lines": "329,330,331,332,333,334,335,336,337,338,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365", "method_info": {"method_name": "_initialize_sampler", "method_params": "self", "method_startline": "329", "method_endline": "365", "method_complexity": {"method_NLOC": "35", "method_CCN": "10", "method_NToken": "293", "method_nesting_level": "1"}}}, "hunk_9": {"Ismethod": 1, "added_lines": "257,258,259,260,261,262", "deleted_lines": "258,259,260,261,262", "method_info": {"method_name": "setup", "method_params": "self,warmup_steps,args,kwargs", "method_startline": "255", "method_endline": "262", "method_complexity": {"method_NLOC": "8", "method_CCN": "3", "method_NToken": "66", "method_nesting_level": "1"}}}, "hunk_10": {"Ismethod": 1, "added_lines": null, "deleted_lines": "146,147", "method_info": {"method_name": "_compute_trace_log_prob", "method_params": "self,model_trace", "method_startline": "146", "method_endline": "147", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "16", "method_nesting_level": "1"}}}, "hunk_11": {"Ismethod": 1, "added_lines": null, "deleted_lines": "132,133,134,135,136,137,138", "method_info": {"method_name": "_get_trace", "method_params": "self,z", "method_startline": "132", "method_endline": "138", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "69", "method_nesting_level": "1"}}}, "hunk_12": {"Ismethod": 1, "added_lines": "288", "deleted_lines": "275,281,282,283,302,303,304,305,306,307,308,309,310,311,312,313,314", "method_info": {"method_name": "sample", "method_params": "self,params", "method_startline": "275", "method_endline": "314", "method_complexity": {"method_NLOC": "30", "method_CCN": "5", "method_NToken": "259", "method_nesting_level": "1"}}}, "hunk_13": {"Ismethod": 1, "added_lines": "196,197,198", "deleted_lines": "191,192,193,194,195,196,197,198,200", "method_info": {"method_name": "_sample_r", "method_params": "self,name", "method_startline": "191", "method_endline": "201", "method_complexity": {"method_NLOC": "11", "method_CCN": "2", "method_NToken": "90", "method_nesting_level": "1"}}}, "hunk_14": {"Ismethod": 1, "added_lines": "92", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,model,potential_fn,step_size,trajectory_length,num_steps,adapt_step_size,adapt_mass_matrix,full_mass,transforms,max_plate_nesting,jit_compile,jit_options,ignore_jit_warnings,target_accept_prob", "method_startline": "90", "method_endline": "104", "method_complexity": {"method_NLOC": "15", "method_CCN": "1", "method_NToken": "61", "method_nesting_level": "1"}}}, "hunk_15": {"Ismethod": 1, "added_lines": "217", "deleted_lines": "205,206,207,208,209,212,213", "method_info": {"method_name": "_reset", "method_params": "self", "method_startline": "202", "method_endline": "217", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "82", "method_nesting_level": "1"}}}, "hunk_16": {"Ismethod": 1, "added_lines": "185,196,197", "deleted_lines": "173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197", "method_info": {"method_name": "_potential_energy_jit", "method_params": "self,z", "method_startline": "173", "method_endline": "197", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "84", "method_nesting_level": "1"}}}, "hunk_17": {"Ismethod": 1, "added_lines": "223,224,225,226,227,228,229,230,231,232,233,234,235,236", "deleted_lines": "231", "method_info": {"method_name": "_initialize_model_properties", "method_params": "self,model_args,model_kwargs", "method_startline": "223", "method_endline": "237", "method_complexity": {"method_NLOC": "15", "method_CCN": "1", "method_NToken": "78", "method_nesting_level": "1"}}}, "hunk_18": {"Ismethod": 1, "added_lines": "258,259,260,261,262", "deleted_lines": "258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274", "method_info": {"method_name": "_guess_max_plate_nesting", "method_params": "self", "method_startline": "258", "method_endline": "274", "method_complexity": {"method_NLOC": "11", "method_CCN": "7", "method_NToken": "93", "method_nesting_level": "1"}}}, "hunk_19": {"Ismethod": 1, "added_lines": null, "deleted_lines": "141,142,143,144", "method_info": {"method_name": "_iter_latent_nodes", "method_params": "trace", "method_startline": "141", "method_endline": "144", "method_complexity": {"method_NLOC": "4", "method_CCN": "4", "method_NToken": "56", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pyro\\infer\\mcmc\\mcmc.py", "file_new_name": "pyro\\infer\\mcmc\\mcmc.py", "file_complexity": {"file_NLOC": "261", "file_CCN": "52", "file_NToken": "1856"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "193,194", "deleted_lines": null, "method_info": {"method_name": "_trace_wrap", "method_params": "self,z,args,kwargs", "method_startline": "192", "method_endline": "202", "method_complexity": {"method_NLOC": "11", "method_CCN": "4", "method_NToken": "113", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "85,86,87,88,89,90,91,92,93", "method_info": {"method_name": "run", "method_params": "self", "method_startline": "75", "method_endline": "101", "method_complexity": {"method_NLOC": "16", "method_CCN": "6", "method_NToken": "170", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pyro\\infer\\mcmc\\nuts.py", "file_new_name": "pyro\\infer\\mcmc\\nuts.py", "file_complexity": {"file_NLOC": "280", "file_CCN": "40", "file_NToken": "1543"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "114", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,model,potential_fn,step_size,adapt_step_size,adapt_mass_matrix,full_mass,use_multinomial_sampling,transforms,max_plate_nesting,jit_compile,jit_options,ignore_jit_warnings,target_accept_prob,max_tree_depth", "method_startline": "112", "method_endline": "126", "method_complexity": {"method_NLOC": "15", "method_CCN": "1", "method_NToken": "61", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "170", "deleted_lines": null, "method_info": {"method_name": "_build_basetree", "method_params": "self,z,r,z_grads,log_slice,direction,energy_current", "method_startline": "167", "method_endline": "190", "method_complexity": {"method_NLOC": "17", "method_CCN": "6", "method_NToken": "211", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "pyro\\infer\\mcmc\\util.py", "file_new_name": "pyro\\infer\\mcmc\\util.py", "file_complexity": {"file_NLOC": "265", "file_CCN": "60", "file_NToken": "1733"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "263,264", "deleted_lines": null, "method_info": {"method_name": "initialize_model", "method_params": "model,model_args", "method_startline": "263", "method_endline": "264", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "35", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "334,335,336", "deleted_lines": null, "method_info": {"method_name": "_pe_jit", "method_params": "zi", "method_startline": "334", "method_endline": "336", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "22", "method_nesting_level": "3"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "237,238,239,240,241,242,243,244,245", "deleted_lines": null, "method_info": {"method_name": "_pe_maker.potential_energy", "method_params": "params", "method_startline": "237", "method_endline": "245", "method_complexity": {"method_NLOC": "9", "method_CCN": "3", "method_NToken": "102", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233", "deleted_lines": null, "method_info": {"method_name": "_guess_max_plate_nesting", "method_params": "model,args,kwargs", "method_startline": "217", "method_endline": "233", "method_complexity": {"method_NLOC": "11", "method_CCN": "7", "method_NToken": "91", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "250,251", "deleted_lines": null, "method_info": {"method_name": "_get_init_params", "method_params": "model,model_args,model_kwargs,transforms,potential_fn,prototype_params,max_tries_initial_params", "method_startline": "250", "method_endline": "251", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "0"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "236,237,238,239,240,241,242,243,244,245,246,247", "deleted_lines": null, "method_info": {"method_name": "_pe_maker", "method_params": "model,model_args,model_kwargs,trace_prob_evaluator,transforms", "method_startline": "236", "method_endline": "247", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "0"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "340,341,342", "deleted_lines": null, "method_info": {"method_name": "potential_fn", "method_params": "params", "method_startline": "340", "method_endline": "342", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "27", "method_nesting_level": "3"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tests\\infer\\mcmc\\test_hmc.py", "file_new_name": "tests\\infer\\mcmc\\test_hmc.py", "file_complexity": {"file_NLOC": "234", "file_CCN": "22", "file_NToken": "2497"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "267,268,269,270,271,272,273,274,275,276,277,278,279,280", "deleted_lines": "266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287", "method_info": {"method_name": "test_initial_params", "method_params": "monkeypatch", "method_startline": "266", "method_endline": "287", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "117", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "271,272", "deleted_lines": "271,272", "method_info": {"method_name": "test_unnormalized_normal.potential_fn", "method_params": "params", "method_startline": "271", "method_endline": "272", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "274,275,276,277,278", "deleted_lines": "274,275,276,277,278", "method_info": {"method_name": "test_initial_params.model", "method_params": "data", "method_startline": "274", "method_endline": "278", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "68", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "268,269,270,271,272,273,274,275,276,277,278,279,280", "deleted_lines": "268,269,270,271,272,273,274,275,276,277,278,279,280", "method_info": {"method_name": "test_unnormalized_normal", "method_params": "jit", "method_startline": "268", "method_endline": "280", "method_complexity": {"method_NLOC": "10", "method_CCN": "2", "method_NToken": "132", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "280", "deleted_lines": "280,281", "method_info": {"method_name": "test_initial_params.tr_log_prob", "method_params": "trace,values", "method_startline": "280", "method_endline": "281", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "1"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\infer\\mcmc\\test_nuts.py", "file_new_name": "tests\\infer\\mcmc\\test_nuts.py", "file_complexity": {"file_NLOC": "311", "file_CCN": "36", "file_NToken": "3751"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "176,177", "deleted_lines": "176", "method_info": {"method_name": "test_beta_bernoulli", "method_params": "step_size,adapt_step_size,adapt_mass_matrix,full_mass", "method_startline": "166", "method_endline": "180", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "124", "method_nesting_level": "0"}}}}}}}}