{"BR": {"BR_id": "807", "BR_author": "martaranzini", "BRopenT": "2020-07-23T09:00:32Z", "BRcloseT": "2020-10-28T19:38:34Z", "BR_text": {"BRsummary": "Training instability with Dice Loss/Tversky Loss", "BRdescription": "\n I am training a 2D UNet to segment fetal MR images using MONAI and I have been observing some instability in the training when using MONAI Dice loss formulation. After some iteration, the loss jumps up and the network stops learning, as the gradients drop to zero. Here is an example (orange is loss on training set computed over 2D slices, blue is loss on validation computed over 3D volume).\n <denchmark-link:https://user-images.githubusercontent.com/28106739/88265546-9b0a7980-ccc5-11ea-9158-a716dafd32c7.png></denchmark-link>\n \n After investigating several aspects (using the same deterministic seed), I've narrowed down the issue to the presence of the smooth term in both the numerator and denominator of the Dice Loss:\n f = 1.0 - (2.0 * intersection + smooth) / (denominator + smooth)\n When using the formulation:\n \n without the smooth term in the numerator, the training was stable and no longer showed unexpected behaviour:\n <denchmark-link:https://user-images.githubusercontent.com/28106739/88266174-a27e5280-ccc6-11ea-8b5b-ec14c008a92f.png></denchmark-link>\n \n [Note: this experiment was trained for much longer to make sure the jump would not appear later in the training]\n The same pattern was observed also for the Tversky Loss, so it could be worth investigating the stability of the losses to identify the best default option.\n Software version\n MONAI version: 0.1.0+84.ga683c4e.dirty\n Python version: 3.7.4 (default, Jul  9 2019, 03:52:42)  [GCC 5.4.0 20160609]\n Numpy version: 1.18.2\n Pytorch version: 1.4.0\n Ignite version: 0.3.0\n Training information\n Using MONAI PersistentCache\n 2D UNet (as default in MONAI)\n Adam optimiser, LR = 1e-3, no LR decay\n Batch size: 10\n Other tests\n The following aspects were investigated but did not solve the instability issue:\n \n Gradient clipping\n Different optimisers (SGD, SGD + Momentum)\n Transforming the binary segmentations to a two-channel approach ([background segmentation, foreground segmentation])\n Choosing smooth = 1.0 as default here (https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/training/loss_functions/dice_loss.py). However, this made the behaviour even more severe and the jump would happen sooner in the training.\n \n The following losses were also investigated\n \n Binary Cross Entropy --> stable\n Dice Loss + Binary Cross Entropy --> unstable\n Dice Loss (no smooth at numerator) + Binary Cross Entropy --> stable\n Tversky Loss --> Unstable\n Tversky Loss (no smooth at numerator) --> stable\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "martaranzini", "commentT": "2020-07-23T10:23:03Z", "comment_text": "\n \t\tthanks for this nice report, will look into this (cc <denchmark-link:https://github.com/FabianIsensee>@FabianIsensee</denchmark-link>\n  <denchmark-link:https://github.com/ericspod>@ericspod</denchmark-link>\n  <denchmark-link:https://github.com/holgerroth>@holgerroth</denchmark-link>\n )\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "martaranzini", "commentT": "2020-07-23T14:04:50Z", "comment_text": "\n \t\tInteresting observation. I must admit that I have never investigated this in depth in nnU-Net. I should also note that while the loss function defaults to , nnU-Net actually uses  (see here <denchmark-link:https://github.com/MIC-DKFZ/nnUNet/blob/9524dc33425627d1e4b21336a5202b1bcc8157e5/nnunet/training/network_training/nnUNetTrainer.py#L112>https://github.com/MIC-DKFZ/nnUNet/blob/9524dc33425627d1e4b21336a5202b1bcc8157e5/nnunet/training/network_training/nnUNetTrainer.py#L112</denchmark-link>\n ).\n Have you tried training the 2D configuration of nnU-Net with this dataset? It would be quite interesting in knowing whether the instability happens there as well\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "martaranzini", "commentT": "2020-07-23T14:12:41Z", "comment_text": "\n \t\tNo, I actually haven't tried and trained nnUnet with my data, but I can look into it in the next days and feedback whether I observe the same behaviour :) (<denchmark-link:https://github.com/LucasFidon>@LucasFidon</denchmark-link>\n  may find this conversation useful as well)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "martaranzini", "commentT": "2020-07-23T14:41:01Z", "comment_text": "\n \t\tWhen you do, please beware that there is a memory leak on Ubuntu 18 and above with Turing cards. Unfortunately there is nothing I can do about it because this is related to mixed precision training with apex/amp (and also occurs with pytorch autocast). I already opened an issue about this and I hope someone will look into this. If you observe this on your system as well, please let me know. You can get rid of it by training in fp32 with the -fp32 option (you will need more GPU memory).\n Let me know if you need any assistance getting 2D data to run. This is not straightforward right now. You basically need to create dummy 3D niftis that have shape 1 in the first axis.\n Best,\n Fabian\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "martaranzini", "commentT": "2020-08-04T21:41:41Z", "comment_text": "\n \t\tI'm trying to replicate this issue with some synthetic data, looks like using  instead of  is much more stable. <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n  are you using the softmax=True option, and could you please try sigmoid=True if possible?\n i guess softmax is causing some over/underflow I'll look further into this\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "martaranzini", "commentT": "2020-08-04T22:05:49Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n \n I was actually using  already in the experiments I reported in the issue. When using a single-channel output, I put  and . Conversely, in the two-channel test I did, I used the softmax instead of the sigmoid. In both cases I would observe the reported behaviour.\n We are also working on training the nnU-net on the same data to see if it shows the same behaviour. We are going to report back about it as soon as possible.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "martaranzini", "commentT": "2020-08-04T22:54:10Z", "comment_text": "\n \t\tok, thanks for the information, I haven't tested a single-channel case yet, I'll do that as well. for now two-channel + sigmoid seems to be more stable compared with two-channel + softmax. looks like it's independent of the choice of network afaik\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "martaranzini", "commentT": "2020-08-05T09:48:15Z", "comment_text": "\n \t\tOk, I see, thanks! I can try and test two-channel + sigmoid instead of softmax on my data as well and see if it gets more stable. I will let you know.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "martaranzini", "commentT": "2020-09-21T12:30:03Z", "comment_text": "\n \t\tHi all,\n First, apologies for the delay in getting back to you about this issue \u2013 we have been running a few experiments to put together a MONAI and nnU-Net comparison and this required quite some time. We hope you will find our results interesting and informative.\n <denchmark-link:https://github.com/LucasFidon>@LucasFidon</denchmark-link>\n  kindly ran a few experiments with nnU-Net and this allowed us to identify some implementation differences. Both MONAI and nnU-Net use the Dice formulation:\n <denchmark-link:https://user-images.githubusercontent.com/28106739/93765463-d4e7f580-fc0c-11ea-9f38-052521ac6538.gif></denchmark-link>\n \n However, as a default MONAI computes the Dice per element in the batch and then the loss is averaged across the batch (we will refer to this simply as \u201cDice\u201d). We noticed that in nnU-Net the Dice is instead computed directly as a single value across the whole batch (i.e. not per image). The average is computed only across the channels, not across the batch elements. We refer to this approach as \u201cBatch Dice\u201d.\n We experimented with both formulations in both frameworks, and also tested for Dice loss and Dice + Cross entropy loss at training, as well as for the use of a single- or 2-channel approach.\n The performance of the different trained models on the validation set is reported below (note: they are separated in groups A-E for our own clinical interpretation, but the group separation is not particularly relevant for this issue). The sampling strategy at training is also reported.\n <denchmark-link:https://user-images.githubusercontent.com/28106739/93765510-e92bf280-fc0c-11ea-852f-011322d256f8.png></denchmark-link>\n \n A few specifications:\n \n MONAI \u2013 Dice no smooth at numerator used the formulation:\n \n nnU-Net \u2013 Batch Dice + Xent, 2-channel, ensemble indicates ensemble performance from 5-fold cross validation at training\n NeuroImage indicates a published two-step approach on our dataset, and it is reported just for reference.\n \n We gather two main observations from these experiments:\n \n Training instability: On our dataset, nnU-Net did not present the training instability observed with the default MONAI implementation of Dice. This is also confirmed when known implementation differences were ruled out (nnU-Net \u2013 Dice, 2-channel, uniform sampling). Also, nnU-Net generally provides better performance. @FabianIsensee, are there any other implementation differences that could justify our results?\n Dice vs Batch Dice: In both frameworks, the Batch Dice implementation clearly outperforms the \u201cnormal\u201d Dice computation. This could be an interesting feature to be added in MONAI \u2013 happy to open another issue/PR about this.\n \n <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n : I did retrain also the two-channel with sigmoid instead of softmax. With respect to the single-channel, the gradients do not drop to zero (with either sigmoid and softmax), but I still observe some instability in the loss which I cannot fully explain:\n <denchmark-link:https://user-images.githubusercontent.com/28106739/93765815-65bed100-fc0d-11ea-8cda-3d3cc774736c.png></denchmark-link>\n \n Looking forward to hearing your comments, and happy to run more experiments to investigate this further!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "martaranzini", "commentT": "2020-09-21T13:07:26Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n  , thank you so much for the detailed report! It's a very interesting read and I am happy to see that nnU-Net did not disappoint.\n Just so that I think about the results in the correct context (<denchmark-link:https://github.com/LucasFidon>@LucasFidon</denchmark-link>\n  ) : Did you provide separate 2D slices as training examples? Or did you just run the 2D nnU-net configuration on the 3D images? My guess is the latter, but it would be important to be sure.\n The batch dice (this is what I call it as well) is implemented on purpose in nnU-Net, but it is not always active. nnU-Net sets that automatically:\n \n if the patch size does not cover the entire image nnU-Net uses batch dice. Since small patches do not represent the true class distribution anyways we might as well benefit from the increased stability of batch dice. Batch dice is used for all 2D U-Nets or 3d_fullres U-Nets if no 3d_lowres is present\n nnU-Net uses 'sample dice' (that is what I call the regular Dice) for all cases where the patch size covers almost entire training cases, so that would be the 3d_lowres U-Net and the 3d_fullres U-Net if no lowres is present.\n \n Note that there are cases where sample Dice is better than batch dice, so replacing it categorically is not a good idea.\n I don't know about the MONAI implementation, but using the sample Dice in 2D segmentation tasks in nnU-Net is a bad idea. That is because nnU-Net never optimizes the background class with the Dice (just the CE). What this translates to is a loss function that will not yield any useful gradients on slices where no foreground voxels are present - essentially ignoring these slices in optimization. If you want to run sample Dice with nnU-Net, make sure to set do_bg=True in the SoftDiceLoss :-) (this problem does not exist when pairing the Dice with CE because the CE term saves it ;-) )\n In my experiments, using uniform sampling gives pretty much the same results as oversampling foreground. Right now we see uniform sampling in nnU-Net only for a scenario which is also unstable in non-uniform sampling. It would be very interesting to see the nnU-Net performance for uniform sampling with the default loss function as well to ensure that oversampling it not skewing the results.\n I am not quite sure I understand what you mean by 'ensemble indicates ensemble performance from 5-fold cross validation at training'. Is that simply the Dice score averaged over the five folds? Or do you have a heldout validation set where you applied the ensemble to?\n Since your segmentation problem seems to be 3D - what was your motivation to go with a 2D U-Net? I believe you could get better results by running the 3D U-Net. If you have compute resources to spare you could just run the 3d_fullres config. If your dataset is public I can also do that for you :-)\n Let me know what you think!\n Best,\n Fabian\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "martaranzini", "commentT": "2020-09-21T13:52:35Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/FabianIsensee>@FabianIsensee</denchmark-link>\n , thank you for the quick reply!\n Regarding 3D vs 2D, in fetal MRI the 3D MRI are motion-corrupted stacks of 2D slices. In addition the in-plane resolution is typically several times higher than the resolution across planes (please see [1] for more details).\n In [1] it has already been shown empirically that 2D CNNs were superior to 3D CNNs for the task of fetal brain extraction. As a result, I have trained nnU-Net only in '2d' mode. For training, the 2D slices were used as input (with an extra first dimension of size 1 as you suggested above).\n Regarding the validation dataset used in the figure of <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n , it is a heldout validation set.\n The nnU-Net ensemble is the ensemble of the five 2D U-Nets trained for the five folds created by nnUNet.\n The dataset is not publicly available... But I will train nnU-Net for the default configuration (batch Dice + CE) except I will deactivate the oversampling :)\n Do you think that it is worth trying do_bg=True also with batch Dice + CE or does do_bg=False always work better in your experience?\n Thanks,\n Lucas\n [1] <denchmark-link:https://www.sciencedirect.com/science/article/pii/S1053811919309152>Ebner, Michael, et al. \"An automated framework for localization, segmentation and super-resolution reconstruction of fetal brain MRI.\" NeuroImage 206 (2020): 116324.</denchmark-link>\n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "martaranzini", "commentT": "2020-09-21T13:54:39Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/FabianIsensee>@FabianIsensee</denchmark-link>\n  ,\n Thanks for your comments, very interesting, especially in the analysis of when batch Dice is more suitable than sample Dice. I totally agree that it should not replace it categorically, but it would be useful to add an option in MONAI for the user to choose one over the other.\n I will leave to <denchmark-link:https://github.com/LucasFidon>@LucasFidon</denchmark-link>\n  the answers to the nnU-Net experiments as he actually did all the hard part on that :)\n I can however reply to the details about our segmentation problem. Our task is brain segmentation of fetal MR images for subsequent super-resolution-reconstruction. Our data is 3D but because of heavy (inter-slice) motion artifacts (and heavy anisotropic resolution), we need to approach it as a 2D problem, where we segment each slice independently. Only at inference we perform a 3D assessment, by computing the Dice for the whole 3D image - and this 3D assessment is reported for the validation set in the figure above. We did run some tests with 3D training but it was clearly underperforming the 2D approach.\n In addition, the FOV of our images are quite large and we do have a lot of background-only slices - and we want to be able to correctly predict them as empty. So the comment you raised about the background is spot-on and we will look into greater detail about it. I know for sure we did include the background in MONAI (in the two-channel approaches), but we will check the single-channel and nnU-Net better for this.\n Re: the single-channel, I think that removing the smooth term at the numerator in the Dice will have the same effect of ignoring the background slices. On the other hand, we have this instability in the Dice loss and in the Tversky loss in the MONAI framework when keeping the smooth term at the numerator that we cannot explain.\n Many thanks!\n Marta\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "martaranzini", "commentT": "2020-09-21T13:55:24Z", "comment_text": "\n \t\tOups, I think we posted about at the same time, thanks for answering the nnU-Net questions, <denchmark-link:https://github.com/LucasFidon>@LucasFidon</denchmark-link>\n !\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "martaranzini", "commentT": "2020-09-21T14:07:11Z", "comment_text": "\n \t\tHi Lucas,\n thanks for your response. In that case it would certainly have been easier to simply plug the 3D volumes into nnU-net and let it handle the slicing itself (it's gonna choose the in plane axis automatically and this way you will also get whole 3D volumes as output). I presume you have generated a manual split file to ensure that the data is split properly during cross-validation? This is very important!\n I don't think that evaluating do_bg=True with batch dice + CE is going to give you a substantially different result. I think it is more important to use it in combination with sample Dice (without CE). CE covers up a lot of the potential shortcomings of the Dice loss, which is why we use it in nnU-Net (nnU-Net is supposed to be as robust as possible).\n nnU-Net handles anisotropic 3D data very well. Motion artifacts can certainly cause a problem, but I would really encourage you to also try the 3D data and see what happens. You might be surprised. Generally speaking you are of course correct, though. On very anisotropic datasets (for example ACDC cine MRI) the benefits of 3D over 2D are less than they could be.\n <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n  Your response appeared right while I was writing, so let me address your points in this post as well: As long as you are using batch dice OR an additional CE term (essentially in all the cases where nnU-Net did not fail) you do not need to worry about empty slices (as long as they are also presented during training). That becomes only relevant when sample Dice is used without an additional CE term and without .\n Regarding the smooth term in the numerator: Yes I think of it the same way. If there is no smooth term then the numerator and thus the entire loss is just 0 for empty slices. Note that this is a little bit more complicated though:\n \n if you are using softmax and you are also optimizing the background class, this should not be a problem because by optimizing the background you are still affecting the output neurons of foreground (coupled via softmax)\n if you are using softmax and not optimizing the background class then this will be a problem\n if you are using sigmoid, this will be a problem (no coupling of outputs, so this is always a problem irrespective of whether you optimize the bg or not)\n \n Best,\n Fabian\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "martaranzini", "commentT": "2020-09-21T14:30:51Z", "comment_text": "\n \t\tThanks for the analysis <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n \n from these results for now we could:\n \n  add a BatchDice as a separate loss function impl.\n  add a numerator smooth option to all the relevant losses and defaults to 0.0\n ((2.0 * intersection + eps_1) / (denominator + eps_2); eps_1 defaults to 0.0)\n \n Also, not sure whether matching the overall performance is a goal of this ticket, but the gap might come from the fact that the MONAI vanilla UNet doesn't use any type of multi-scale losses or deep supervision -- both the NeuroImage paper and (I think) nnUnet uses a multi-scale loss.\n would you help evaluate the recent dynunet (which includes a multi-scale head) <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n ? <denchmark-link:https://github.com/Project-MONAI/tutorials/blob/master/modules/dynunet_tutorial.ipynb>https://github.com/Project-MONAI/tutorials/blob/master/modules/dynunet_tutorial.ipynb</denchmark-link>\n \n cc <denchmark-link:https://github.com/mmarcinkiewicz>@mmarcinkiewicz</denchmark-link>\n  <denchmark-link:https://github.com/pribalta>@pribalta</denchmark-link>\n  <denchmark-link:https://github.com/Nic-Ma>@Nic-Ma</denchmark-link>\n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "martaranzini", "commentT": "2020-09-21T14:36:18Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/FabianIsensee>@FabianIsensee</denchmark-link>\n .\n For the preprocessing of the data, following nnU-Net filenames convention, I have separated the 2D slices of  and put them in /path-to-nnU-Net-raw-fetal-data/imagesTr, and I have copied the 3D images of  directly to /path-to-nnU-Net-raw-fetal-data/imagesVal (so nnU-Net gives me 3D output when I run inference for those images).\n I have used the automatic split of nnU-Net.\n If I understand well, this implies that Marta's training set = nnU-Net training set + nnU-Net validation set. But all the images from Marta's training set are still used for training by nnU-Net in the ensemble.\n I will give a try to nnU-Net in 3d_fullres mode.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "martaranzini", "commentT": "2020-09-21T14:58:52Z", "comment_text": "\n \t\tHi Lucas,\n if you have split your 3d images into a series of 2d images and you have not accounted for this in a manually designed split file your entire set of cross-validation experiments is not valid. This is because nnU-Net per default expects to be able to treat the training cases independently. If you provide pseudo-2D slices, nnU-Net will just split those for its cross-validation - how would it know to do it differently? They are separate training example for all nnU-Net knows.\n nnU-Net will always use all the cases located in the imagesTr folder for its cross validation. nnU-Net will not touch images located in other folder (for example imagesVal).\n So unless I misread something, all the cross-validation results you reported above are overly optimistic. Unless you are not reporting cross-validation splits at all (you could also have trained nnU-Net with fold all and then used a single model to predict the validation cases)?\n My suggestion would be to give nnU-net the 3D volumes volumes instead and let it do the 2D slicing itself. Then it will also be able to create proper splits. You can still use the 2D configuration of course. This will make everything so much easier. And this will allow you to also use the 3d unet on the data without having to have a separate copy.\n I suggested using dummy 2D slices only because I was under the impression that you have actual 2D data (where each 2D image is a completely different training case).\n <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n  deep supervision only has a very small effect and I would be surprised if you could even see that on this dataset\n Best,\n Fabian\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "martaranzini", "commentT": "2020-09-21T15:01:50Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n , the two modifications you mentioned are exactly the changes I added to my \"custom\" Dice class in MONAI.\n Also I think the batch dice could be easily integrated in the DiceLoss class. In my case, for the batch dice I simply added a flag in the () of the DiceLoss and modify the forward() method with:\n if self.batch_version:\n             # reducing only spatial dimensions and batch (not channels)\n             reduce_axis = [0] + list(range(2, len(input.shape)))\n else:\n             # reducing only spatial dimensions (not batch nor channels)\n             reduce_axis = list(range(2, len(input.shape)))\n instead of \n \n \n MONAI/monai/losses/dice.py\n \n \n          Line 131\n       in\n       0aee00b\n \n \n \n \n \n \n  reduce_axis = list(range(2, len(input.shape))) \n \n \n \n \n \n And sure, I am happy to test the dynunet on our dataset and report back.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "martaranzini", "commentT": "2020-09-21T15:02:15Z", "comment_text": "\n \t\t\n Thanks @wyli, the two modifications you mentioned are exactly the changes I added to my \"custom\" Dice class in MONAI.\n Also I think the batch dice could be easily integrated in the DiceLoss class. In my case, for the batch dice I simply added a flag in the init() of the DiceLoss and modify the forward() method with:\n if self.batch_version:\n             # reducing only spatial dimensions and batch (not channels)\n             reduce_axis = [0] + list(range(2, len(input.shape)))\n else:\n             # reducing only spatial dimensions (not batch nor channels)\n             reduce_axis = list(range(2, len(input.shape)))\n instead of\n \n \n \n MONAI/monai/losses/dice.py\n \n \n          Line 131\n       in\n       0aee00b\n \n \n \n \n \n \n  reduce_axis = list(range(2, len(input.shape))) \n \n \n \n \n \n And sure, I am happy to test the dynunet on our dataset and report back.\n \n thank you!\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "martaranzini", "commentT": "2020-09-21T15:14:01Z", "comment_text": "\n \t\tHi Fabian,\n No, the Dice score results reported by Marta are for the cases in imagesVal (that nnU-Net does not touch during training), so none of the nnU-Net models has seen any of those cases during training!\n I understand that Marta's training cases are split across the different splits, but it is not a problem here because the results reported are not the cross-validation results :)\n Sorry if it was not clear (there was a typo in my previous message)\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "martaranzini", "commentT": "2020-09-21T15:23:36Z", "comment_text": "\n \t\tHi Lucas,\n In that case I owe you my apologies :-) I must have misread that. It would still have been cleaner to split the files properly - like this the ensemble it not really worth much, but I don't think that this matters.\n If you are willing to run them, the following experiments would be interesting (this essentially repeats what we have discussed before):\n \n run default nnU-Net without oversampling to confirm that oversampling has no effect on the result (i expect the outcome to be the same)\n out of curiosity: run the 3d_fullres configuration and see what happens\n \n <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n  since the splits are OK, there must be something else going on. And yeah - that's going to be very hard to figure out. In my experience it really is the little things that create big differences. Something obvious like the network architecture has much less impact on the model performance than one would think ^^ Also I don't think that the exact loss formulation is at fault. If you want to start searching: using the same learning rate schedule as well as the same number of iterations as nnU-Net would be a good start. After that standardized losses: use CE only for both frameworks (nnunet simply uses pytorch's CE). Data augmentation. Inference strategy. The list is long\n Best,\n Fabian\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "martaranzini", "commentT": "2020-09-21T16:01:50Z", "comment_text": "\n \t\t\n @wyli since the splits are OK, there must be something else going on. And yeah - that's going to be very hard to figure out. In my experience it really is the little things that create big differences. Something obvious like the network architecture has much less impact on the model performance than one would think ^^ Also I don't think that the exact loss formulation is at fault. If you want to start searching: using the same learning rate schedule as well as the same number of iterations as nnU-Net would be a good start. After that standardized losses: use CE only for both frameworks (nnunet simply uses pytorch's CE). Data augmentation. Inference strategy. The list is long\n \n Sure, it's probably hard if I go it alone :D as MONAI is a consortium effort, I'm sure as a team we can identify the issue and close the gaps quickly. Thanks for your inputs here!\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "martaranzini", "commentT": "2020-09-21T17:02:54Z", "comment_text": "\n \t\tTrue that! If you need my input on something I am always happy to help out.\n Best,\n Fabian\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "martaranzini", "commentT": "2020-10-05T10:23:24Z", "comment_text": "\n \t\tFor the record, this might be interesting to look at:\n Nordstr\u00f6m M., Bao H., L\u00f6fman F., Hult H., Maki A., Sugiyama M. (2020) Calibrated Surrogate Maximization of Dice. In: Martel A.L. et al. (eds) Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2020. MICCAI 2020. Lecture Notes in Computer Science, vol 12264. Springer, Cham. <denchmark-link:https://doi.org/10.1007/978-3-030-59719-1_27>https://doi.org/10.1007/978-3-030-59719-1_27</denchmark-link>\n \n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "martaranzini", "commentT": "2020-10-14T08:31:03Z", "comment_text": "\n \t\tIs there somewhere an implementation for this loss? I would like to give it a try but the paper is a bit too mathy for me (They even manage to make the soft dice loss look complicated ;-) ) and I don't have the time to go through all of that right now.\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "martaranzini", "commentT": "2020-10-28T11:03:07Z", "comment_text": "\n \t\tHi all,\n <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n : I finally have the updates on the DynUnet.\n First, I would like to point out that together with <denchmark-link:https://github.com/LucasFidon>@LucasFidon</denchmark-link>\n  we identified a huge source of discrepancy in patch size and batch size between the manually selected values in MONAI and the automatically determined ones in nnU-Net.\n In MONAI I was using a way smaller patch size (roughly a factor of 5), which explains the very large gap of performance.\n I did rerun the experiments with the \u201cstandard\u201d MONAI UNet, but using the same patch and batch size as determined by nnU-Net. These results are reported in red (first boxplot of each group) in the figure below.\n For the DynUnet, wrt the MONAI tutorial I only modified the spacing transform to apply it only in the x-y plane, but no change of spacing along z (as our data is heavily affected by out-of-plane motion artefacts). All the training has been performed in 2D.\n Orange and light orange boxplots are the results with Dice + Xent and Batch Dice + Xent as losses respectively.\n Here are the results on our validation sets (not seen at training):\n <denchmark-link:https://user-images.githubusercontent.com/28106739/97426487-713ba100-190b-11eb-87d6-f3b81c6afe53.png></denchmark-link>\n \n Overall, we managed to reduce the gap substantially compared to our previous results with very minor modifications of existing tools in MONAI. However, using the optimal hyperparameters as determined by nnU-Net played a big role in this.\n Note: for dynUnet, both Dice and Batch Dice I kept the original MONAI formulation of Dice:\n <denchmark-link:https://user-images.githubusercontent.com/28106739/97426320-30438c80-190b-11eb-90fd-3dafe627f410.png></denchmark-link>\n \n with smooth=1e-5. In this case, it did not show the previously observed instability at training.\n However, with the \"standard\" UNet, this formulation would still show the instability, despite the optimised patch and batch size. For that experiment, the smooth term at numerator was set to 0.\n Hope this helps, and please let me know if I can help further :)\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "martaranzini", "commentT": "2020-10-28T11:13:43Z", "comment_text": "\n \t\tthanks <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n , for the loss function, I'll create a PR for flexible options of  and the \"batch version\"\n perhaps adds a new Dice+Xent loss as well. would be great to have your review for the PR :)\n FYI <denchmark-link:https://github.com/mmarcinkiewicz>@mmarcinkiewicz</denchmark-link>\n  <denchmark-link:https://github.com/pribalta>@pribalta</denchmark-link>\n  <denchmark-link:https://github.com/Nic-Ma>@Nic-Ma</denchmark-link>\n  <denchmark-link:https://github.com/yiheng-wang-nv>@yiheng-wang-nv</denchmark-link>\n \n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "martaranzini", "commentT": "2020-10-29T06:37:46Z", "comment_text": "\n \t\tThank you for the update <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n  . Looks like the differences have been resolved :-)\n <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n  I would also volunteer for reviewing the PR. I've got a lot of experience with batch Dice and Dice+Ce from developing nnU-Net\n Edit: Ah I am too late :-D\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "martaranzini", "commentT": "2021-01-15T13:08:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/FabianIsensee>@FabianIsensee</denchmark-link>\n  , <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n  <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n  ,\n Wonderful work. Could you kindly suggest what set of  heuristic rules/steps of nnU-Net  should I follow when using monai's dynUnet?\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "martaranzini", "commentT": "2021-01-20T08:28:07Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/saruarlive>@saruarlive</denchmark-link>\n ,\n Apologies for the delay in getting back to you. For an optimal outcome with the dynUnet, I used the patch size estimated by the heuristic rules in nnU-Net. If I recall correctly, the spacing is also another parameter that is automatically optimised in nnU-Net but not in dynUnet, so you may need to decide for that too.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "martaranzini", "commentT": "2021-01-20T08:43:54Z", "comment_text": "\n \t\tSince dynUNet does not offer all the functionality nnU-Net does I would highly recommend also running nnU-Net to se whether there is a difference in segmentation performance\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "martaranzini", "commentT": "2021-01-20T12:51:51Z", "comment_text": "\n \t\tDear <denchmark-link:https://github.com/FabianIsensee>@FabianIsensee</denchmark-link>\n , <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n ,\n Thanks for your reply.\n Could you mention specifically what are those functionalities differ with the dyUnet?  Is the network design the same for both DyUnet and nnUnet (the network design looks the same to me after reading your BRATS 20 paper)?\n If so, then I can use the same set of augmentations, a learning rate scheduler from nnU-Net,\n Is the pixel spacing fixed for a specific dataset during training,  for example for brast 2020 data, it is (1.0, 1.0, 1.0), right.\n The reason I am asking because I will adapt from both.\n I create a class (compatible with monai framework followed from fabian's nnU-Net), not sure how fast it is,\n <denchmark-link:https://github.com/martaranzini>@martaranzini</denchmark-link>\n  provide some inputs\n <denchmark-code>from batchgenerators.augmentations.utils import (\n     create_zero_centered_coordinate_mesh, \n     elastic_deform_coordinates,\n     interpolate_img,\n )\n \n from batchgenerators.augmentations.crop_and_pad_augmentations import (\n     random_crop as random_crop_aug,\n     center_crop as center_crop_aug,\n )\n \n class RandElasticDeformd(Randomizable, MapTransform):\n \n     \"\"\"\n     Dictionary-based version :py:class: Saruar created transforms.RandAdjustBrightnessd`.\n \n     See `numpy.random.normal` for additional details.\n     https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html\n \n     Args:\n         keys: Keys to pick data for transformation.\n         prob: Probability of Elastic deformation.\n         alpha and sigma: parameters.\n         \n     \"\"\"\n \n     def __init__(\n         self,\n         keys: KeysCollection,\n         label_key: str = 'label',\n         patch_size: Union[Sequence[int], int] = None,\n         alpha:Union[Tuple[float, float], float] = (0., 900.),\n         sigma:Union[Tuple[float, float], float] = (9., 13.),\n         prob: float = 0.1,\n         patch_center_dist_from_border:  Union[Sequence[int], int] = None, \n         random_crop:bool = False,\n         border_mode : Union[Sequence[str], str] = (\"nearest\",\"constant\"),\n         border_cval : Union[Sequence[int], int] = (0, 0),\n         order_interpolation : Union[Sequence[int], int] = (3, 0),\n         \n         #dtype: np.dtype = np.float32,\n     ) -> None:\n         super().__init__(keys)\n         self.label_key = label_key\n         self.alpha = alpha\n         self.sigma = sigma\n         self.prob = prob\n         self.patch_size = ensure_tuple_rep(patch_size,3)\n         self.patch_center_dist_from_border = ensure_tuple_rep(patch_center_dist_from_border,3)\n         self.border_mode = ensure_tuple_rep(border_mode, len(self.keys))\n         self.border_cval = ensure_tuple_rep(border_cval, len(self.keys))\n         self.order_interpolation = ensure_tuple_rep(order_interpolation, len(self.keys))\n         self.random_crop = random_crop\n         \n         \n         \n \n     def randomize(self, data: Optional[Any] = None) -> None:\n         self._do_transform = self.R.random_sample() < self.prob\n \n \n     def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Dict[Hashable, np.ndarray]:\n         self.randomize()\n         d = dict(data)\n          \n         if not self._do_transform:\n             return d\n         for key in self.keys:\n             keyorder = 0\n             self.patch_size = (d[key].shape[1], d[key].shape[2], d[key].shape[3]) if self.patch_size == (None, None, None) else self.patch_size\n             self.patch_center_dist_from_border = (d[key].shape[1]//2, d[key].shape[2]//2, d[key].shape[3]//2)  \\\n             if self.patch_center_dist_from_border == (None, None, None) else self.patch_center_dist_from_border\n             \n             coords = create_zero_centered_coordinate_mesh(self.patch_size)\n             aa = np.random.uniform(self.alpha[0], self.alpha[1])\n             ss = np.random.uniform(self.sigma[0], self.sigma[1])\n             coords = elastic_deform_coordinates(coords, aa, ss)\n            \n             \n             for pi in range(len(self.patch_size)):\n                 if self.random_crop:\n                     ctr = np.random.uniform(self.patch_center_dist_from_border[pi],\n                                             d[key].shape[pi + 1] - self.patch_center_dist_from_border[pi])\n                 else:\n                     \n                     ctr = int(np.round(d[key].shape[pi + 1] / 2.))\n                 \n                 coords[pi] += ctr\n             \n             for c in range(d[key].shape[0]):\n                 \n                 if key==self.label_key:\n                     \n                     d[key][c] = interpolate_img(d[key][c], coords, self.order_interpolation[keyorder],\n                                                 self.border_mode[keyorder], cval=self.border_cval[keyorder], is_seg=True)\n                 else:\n                     d[key][c] = interpolate_img(d[key][c], coords, self.order_interpolation[keyorder],\n                                                 self.border_mode[keyorder], cval=self.border_cval[keyorder])\n \n                                               \n         return d\n </denchmark-code>\n \n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "martaranzini", "commentT": "2021-01-20T13:27:51Z", "comment_text": "\n \t\tHi,\n there are many implementation details that are different. The architecture is somewhat not important. So if you do anything - please also run nnU-Net and verify that you are getting the same segmentation performance in dynUNet.\n I am not familiar with the implementation of dynUNet. You will need to find the differences yourself ;-) That is only necessary though if you find that the performance between the two differs\n Best,\n Fabian\n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "martaranzini", "commentT": "2021-01-20T15:45:37Z", "comment_text": "\n \t\tHi,\n Perhaps <denchmark-link:https://github.com/wyli>@wyli</denchmark-link>\n  could help point out the implementation differences between dynUnet and nnU-Net?\n Regarding the pixel spacing, in MONAI it all depends on what type of transforms you use/need. In my case, I did use Spacingd - which brings all input images to the same pixel spacing - and I chose the pixel spacing according some needs in my application. If you don't apply any transform to change the spacing, then the data will be processed without considering the mm-space information, only the voxel-space.\n I think that nnU-Net instead determines the optimal spacing from the training set and all images are then resampled to that spacing (<denchmark-link:https://github.com/FabianIsensee>@FabianIsensee</denchmark-link>\n  please correct me if I am wrong).\n Also, the batch size as well is another parameter that gets optimised internally by nnU-Net but needs to be manually set in dynUnet. So in our experiments we took the batch size to be the same as the one used by nnU-Net.\n I hope this helps.\n \t\t"}}}, "commit": {"commit_id": "62cf423fb6196492cec90518febd18406fbeb303", "commit_author": "Wenqi Li", "commitT": "2020-10-28 19:38:33+00:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 8, "file_old_name": "monai\\losses\\dice.py", "file_new_name": "monai\\losses\\dice.py", "file_complexity": {"file_NLOC": "449", "file_CCN": "41", "file_NToken": "2303"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "100,143,144,145,146,161", "deleted_lines": "146,170,171,172", "method_info": {"method_name": "forward", "method_params": "self,Tensor,Tensor", "method_startline": "100", "method_endline": "172", "method_complexity": {"method_NLOC": "56", "method_CCN": "15", "method_NToken": "366", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "398,399,400,401,402", "deleted_lines": "397,402", "method_info": {"method_name": "__init__", "method_params": "self,ndarray,LossReduction,MEAN,float,float", "method_startline": "397", "method_endline": "402", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "46", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "50,51,52", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,bool,bool,bool,bool,None,bool,bool,LossReduction,MEAN,float,float,bool", "method_startline": "40", "method_endline": "52", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "84", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": null, "deleted_lines": "372", "method_info": {"method_name": "__init__", "method_params": "self,ndarray,LossReduction,MEAN", "method_startline": "371", "method_endline": "372", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "233,234,235", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,bool,bool,bool,bool,None,Weight,SQUARE,LossReduction,MEAN,float,float,bool", "method_startline": "224", "method_endline": "235", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "85", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "185,210", "deleted_lines": "198", "method_info": {"method_name": "forward", "method_params": "self,Tensor,Tensor,None", "method_startline": "185", "method_endline": "210", "method_complexity": {"method_NLOC": "22", "method_CCN": "4", "method_NToken": "157", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "96,97,98,100,143,144,145,146", "deleted_lines": "88,93,146", "method_info": {"method_name": "forward", "method_params": "self,Tensor,Tensor,float", "method_startline": "88", "method_endline": "157", "method_complexity": {"method_NLOC": "55", "method_CCN": "14", "method_NToken": "358", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": null, "deleted_lines": "170,171", "method_info": {"method_name": "forward", "method_params": "self,Tensor,Tensor,float,None", "method_startline": "170", "method_endline": "171", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "34", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "monai\\losses\\tversky.py", "file_new_name": "monai\\losses\\tversky.py", "file_complexity": {"file_NLOC": "130", "file_CCN": "14", "file_NToken": "590"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "96,144,145,146,151,152", "deleted_lines": "137,138,139", "method_info": {"method_name": "forward", "method_params": "self,Tensor,Tensor", "method_startline": "96", "method_endline": "162", "method_complexity": {"method_NLOC": "54", "method_CCN": "13", "method_NToken": "339", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "45,46,47", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,bool,bool,bool,bool,None,float,float,LossReduction,MEAN,float,float,bool", "method_startline": "35", "method_endline": "47", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "88", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "92,93,94,96,144,145,146", "deleted_lines": "84,89,137,138,139", "method_info": {"method_name": "forward", "method_params": "self,Tensor,Tensor,float", "method_startline": "84", "method_endline": "149", "method_complexity": {"method_NLOC": "53", "method_CCN": "12", "method_NToken": "331", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\test_dice_loss.py", "file_new_name": "tests\\test_dice_loss.py", "file_complexity": {"file_NLOC": "179", "file_CCN": "4", "file_NToken": "2423"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,30,38,46,54,55,56,57,58,59,60,61,69,77,78,79,80,81,82,83,84,92,116,124,125,126,127,128,129,130,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160", "deleted_lines": "22,26,31,35,40,44,49,53,58,62,67,71,76,80,85,89,98,107,112,116,121,125"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\test_generalized_dice_loss.py", "file_new_name": "tests\\test_generalized_dice_loss.py", "file_complexity": {"file_NLOC": "162", "file_CCN": "4", "file_NToken": "2014"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,30,38,46,54,62,63,64,65,66,67,68,69,77,78,79,80,81,82,83,84,92,100,108,109,110,111,112,113,114,115,123,131,132,133,134,135,136,137", "deleted_lines": "22,26,31,35,40,44,49,53,58,62,67,71,76,80,85,89,94,98,103,107,112,116,121,125"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\test_masked_dice_loss.py", "file_new_name": "tests\\test_masked_dice_loss.py", "file_complexity": {"file_NLOC": "137", "file_CCN": "4", "file_NToken": "1825"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,31,40,49,58,59,60,61,62,63,64,65,73,81,82,83,84,85,86,87,88,96,104,112", "deleted_lines": "22,27,32,37,42,47,52,57,62,66,71,75,80,84,89,93,98,102,107,111"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\test_seg_loss_integration.py", "file_new_name": "tests\\test_seg_loss_integration.py", "file_complexity": {"file_NLOC": "88", "file_CCN": "9", "file_NToken": "1028"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23,24,25,35", "deleted_lines": "23"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\test_tversky_loss.py", "file_new_name": "tests\\test_tversky_loss.py", "file_complexity": {"file_NLOC": "168", "file_CCN": "3", "file_NToken": "2307"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,30,38,46,47,48,49,50,51,52,53,54,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,85,93,94,95,96,97,98,99,100,108,116,124,125,126,127,128,129,130,131,132,140,141,142,143,144,145,146", "deleted_lines": "22,26,31,35,40,44,49,53,58,62,67,71,76,80,85,89,94,98,103,107,112,116"}}}}}}