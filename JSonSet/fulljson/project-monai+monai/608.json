{"BR": {"BR_id": "608", "BR_author": "petteriTeikari", "BRopenT": "2020-06-23T14:05:34Z", "BRcloseT": "2020-06-29T15:32:34Z", "BR_text": {"BRsummary": "Checkpoint not saved even though loss improving (Pytorch Lighting example)", "BRdescription": "\n \n I was trying to implement the PyTorching example:\n <denchmark-link:https://github.com/Project-MONAI/MONAI/blob/master/examples/notebooks/spleen_segmentation_3d_lightning.ipynb>https://github.com/Project-MONAI/MONAI/blob/master/examples/notebooks/spleen_segmentation_3d_lightning.ipynb</denchmark-link>\n  (non-notebook code attached) and noticed that the checkpoints were not saved\n \n As in screencap  was improving, but no checkpoint was saved to disk? Is the problem similar to here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/511>PyTorchLightning/pytorch-lightning#511</denchmark-link>\n \n I replaced\n return {'log': tensorboard_logs}\n from def validation_epoch_end(self, outputs): to\n return {'mean_val_dice': torch.tensor(mean_val_dice), 'log': tensorboard_logs}\n And the checkpoints were saved now, but the {val_loss:.2f}-{val_dice:.2f} were not updated in checkpoint filename?\n \n <denchmark-link:https://user-images.githubusercontent.com/1060514/85412790-e2d0a080-b561-11ea-91e5-3dfaf857918a.png></denchmark-link>\n \n Environment (please complete the following information):\n \n OS: Ubuntu 18.04, see screencap for monai.config.print_config()\n \n Additional context\n Non-notebook code\n import os\n import sys\n import glob\n import numpy as np\n import torch\n from torch.utils.data import DataLoader\n import matplotlib.pyplot as plt\n import monai\n from monai.transforms import \\\n     Compose, LoadNiftid, AddChanneld, ScaleIntensityRanged, RandCropByPosNegLabeld, \\\n     CropForegroundd, RandAffined, Spacingd, Orientationd, ToTensord\n from monai.data import list_data_collate\n from monai.inferers import sliding_window_inference\n from monai.networks.layers import Norm\n from monai.metrics import compute_meandice\n from monai.utils import set_determinism\n from pytorch_lightning import LightningModule, Trainer, loggers\n from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n \n monai.config.print_config()\n \n class Net(LightningModule):\n     def __init__(self):\n         super().__init__()\n         self._model = monai.networks.nets.UNet(dimensions=3, in_channels=1, out_channels=2,\n                                                channels=(16, 32, 64, 128, 256), strides=(2, 2, 2, 2),\n                                                num_res_units=2, norm=Norm.BATCH)\n         self.loss_function = monai.losses.DiceLoss(to_onehot_y=True, softmax=True)\n         self.best_val_dice = 0\n         self.best_val_epoch = 0\n \n     def forward(self, x):\n         return self._model(x)\n \n     def prepare_data(self):\n         # set up the correct data path\n         # 1.6 GB dataset from https://drive.google.com/drive/folders/1HqEgzS8BV2c7xYNrZdEAnrHk7osJJ--2\n         # http://medicaldecathlon.com/\n         # Medical Segmentation Decathlon: Generalisable 3D Semantic Segmentation\n         data_root = '/home/petteri/Task09_Spleen'\n         train_images = sorted(glob.glob(os.path.join(data_root, 'imagesTr', '*.nii.gz')))\n         train_labels = sorted(glob.glob(os.path.join(data_root, 'labelsTr', '*.nii.gz')))\n         data_dicts = [{'image': image_name, 'label': label_name}\n                       for image_name, label_name in zip(train_images, train_labels)]\n         train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n \n         # set deterministic training for reproducibility\n         set_determinism(seed=0)\n \n         # define the data transforms\n         train_transforms = Compose([\n             LoadNiftid(keys=['image', 'label']),\n             AddChanneld(keys=['image', 'label']),\n             Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), interp_order=('bilinear', 'nearest')),\n             Orientationd(keys=['image', 'label'], axcodes='RAS'),\n             ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n             CropForegroundd(keys=['image', 'label'], source_key='image'),\n             # randomly crop out patch samples from big image based on pos / neg ratio\n             # the image centers of negative samples must be in valid image area\n             RandCropByPosNegLabeld(keys=['image', 'label'], label_key='label', size=(96, 96, 96), pos=1,\n                                    neg=1, num_samples=4, image_key='image', image_threshold=0),\n             # user can also add other random transforms\n             # RandAffined(keys=['image', 'label'], mode=('bilinear', 'nearest'), prob=1.0, spatial_size=(96, 96, 96),\n             #             rotate_range=(0, 0, np.pi/15), scale_range=(0.1, 0.1, 0.1)),\n             ToTensord(keys=['image', 'label'])\n         ])\n         val_transforms = Compose([\n             LoadNiftid(keys=['image', 'label']),\n             AddChanneld(keys=['image', 'label']),\n             Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), interp_order=('bilinear', 'nearest')),\n             Orientationd(keys=['image', 'label'], axcodes='RAS'),\n             ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n             CropForegroundd(keys=['image', 'label'], source_key='image'),\n             ToTensord(keys=['image', 'label'])\n         ])\n \n         # we use cached datasets - these are 10x faster than regular datasets\n         self.train_ds = monai.data.CacheDataset(\n             data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4\n         )\n         self.val_ds = monai.data.CacheDataset(\n             data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4\n         )\n         # self.train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n         # self.val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n \n     def train_dataloader(self):\n         train_loader = DataLoader(self.train_ds, batch_size=2, shuffle=True,\n                                   num_workers=4, collate_fn=list_data_collate)\n         return train_loader\n \n     def val_dataloader(self):\n         val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n         return val_loader\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n         return optimizer\n \n     def training_step(self, batch, batch_idx):\n         images, labels = batch['image'], batch['label']\n         output = self.forward(images)\n         loss = self.loss_function(output, labels)\n         tensorboard_logs = {'train_loss': loss.item()}\n         return {'loss': loss, 'log': tensorboard_logs}\n \n     def validation_step(self, batch, batch_idx):\n         images, labels = batch['image'], batch['label']\n         roi_size = (160, 160, 160)\n         sw_batch_size = 4\n         outputs = sliding_window_inference(images, roi_size, sw_batch_size, self.forward)\n         loss = self.loss_function(outputs, labels)\n         value = compute_meandice(y_pred=outputs, y=labels, include_background=False,\n                                  to_onehot_y=True, mutually_exclusive=True)\n         return {'val_loss': loss, 'val_dice': value}\n \n     def validation_epoch_end(self, outputs):\n         val_dice = 0\n         num_items = 0\n         for output in outputs:\n             val_dice += output['val_dice'].sum().item()\n             num_items += len(output['val_dice'])\n         mean_val_dice = val_dice / num_items\n         tensorboard_logs = {'val_dice': mean_val_dice}\n         if mean_val_dice > self.best_val_dice:\n             self.best_val_dice = mean_val_dice\n             self.best_val_epoch = self.current_epoch\n             print('Validation loss improved, a new checkpoint _should be saved_ (Petteri)')\n         print('current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}'.format(\n             self.current_epoch, mean_val_dice, self.best_val_dice, self.best_val_epoch))\n         return {'log': tensorboard_logs}\n \n ## Run the training\n # initialise the LightningModule\n net = Net()\n \n # set up loggers and checkpoints\n tb_logger = loggers.TensorBoardLogger(save_dir='logs')\n checkpoint_callback = ModelCheckpoint(filepath='logs/{epoch}-{val_loss:.2f}-{val_dice:.2f}')\n \n # initialise Lightning's trainer.\n trainer = Trainer(gpus=[0],\n                   max_epochs=600,\n                   logger=tb_logger,\n                   checkpoint_callback=checkpoint_callback,\n                   show_progress_bar=True,\n                   num_sanity_val_steps=1\n                   )\n # train\n trainer.fit(net)\n \n print('train completed, best_metric: {:.4f} at epoch {}'.format(net.best_val_dice, net.best_val_epoch))\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "petteriTeikari", "commentT": "2020-06-23T16:08:15Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/marksgraham>@marksgraham</denchmark-link>\n  ,\n Could you please help take a look at this issue? As you are an expert on PyTorch Lightning.\n Thanks in advance.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "petteriTeikari", "commentT": "2020-06-24T14:29:55Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/petteriTeikari>@petteriTeikari</denchmark-link>\n \n Can you try updating validation_epoch_end to the following:\n     def validation_epoch_end(self, outputs):\n         val_dice = 0\n         val_loss = 0\n         num_items = 0\n         for output in outputs:\n             val_dice += output['val_dice'].sum().item()\n             num_items += len(output['val_dice'])\n             val_loss += output['val_loss'].sum().item()\n         mean_val_dice = torch.tensor(val_dice / num_items)\n         mean_val_loss = torch.tensor(val_loss / num_items)\n         tensorboard_logs = {'val_dice': mean_val_dice, 'val_loss': mean_val_loss}\n         if mean_val_dice > self.best_val_dice:\n             self.best_val_dice = mean_val_dice\n             self.best_val_epoch = self.current_epoch\n             print('Validation loss improved, a new checkpoint _should be saved_ (Petteri)')\n         print('current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}'.format(\n             self.current_epoch, mean_val_dice, self.best_val_dice, self.best_val_epoch))\n         return {'mean_val_dice': mean_val_dice, 'log': tensorboard_logs}\n This calculates the average val loss and adds it to the logs, so the checkpointer can monitor the value. Let me know if it works.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "petteriTeikari", "commentT": "2020-06-24T20:26:16Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/marksgraham>@marksgraham</denchmark-link>\n \n Yeah I was a bit unclear initially, and that indeed worked so that my checkpoint was saved, but it seemed a quick fix as then {val_loss:.2f}-{val_dice:.2f} were not passed to the output filename. So maybe the tutorial needs to be updated to handle both val_loss tracking and proper ckpt filename formatting\n And that I of course added that  monitor='mean_val_dice'to checkboint callback\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "petteriTeikari", "commentT": "2020-06-24T21:48:50Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/petteriTeikari>@petteriTeikari</denchmark-link>\n \n Did you try replacing your function with the one I posted above? This worked for me - it saved the checkpoints along with the desired val_loss + val_dice filenames. You don't need to add monitor='mean_val_dice' if you update the function as I described.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "petteriTeikari", "commentT": "2020-06-25T02:29:05Z", "comment_text": "\n \t\tThat indeeds does what it is supposed to do <denchmark-link:https://github.com/marksgraham>@marksgraham</denchmark-link>\n  , thanks :)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "petteriTeikari", "commentT": "2020-06-25T05:26:05Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/marksgraham>@marksgraham</denchmark-link>\n  , thanks very much for your quick help!\n Do we need to update the existing Lightning example?\n Thanks.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "petteriTeikari", "commentT": "2020-06-29T08:41:48Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/Nic-Ma>@Nic-Ma</denchmark-link>\n ,\n I think we should. I can go a PR with the update.\n Mark\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "petteriTeikari", "commentT": "2020-06-29T08:57:00Z", "comment_text": "\n \t\t\n Hi @Nic-Ma,\n I think we should. I can go a PR with the update.\n Mark\n \n Sounds good, we will release MONAI v0.2 soon, if you can submit a quick PR, that would be great.\n thanks in advance!\n \t\t"}}}, "commit": {"commit_id": "bd15d22cc1c962405d80b6c486a9d751479c3ee9", "commit_author": "Mark Graham", "commitT": "2020-06-29 16:32:33+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "examples\\notebooks\\spleen_segmentation_3d_lightning.ipynb", "file_new_name": "examples\\notebooks\\spleen_segmentation_3d_lightning.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "205,208,210,211,212", "deleted_lines": "205,206,210,211"}}}}}}