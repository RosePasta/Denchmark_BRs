{"BR": {"BR_id": "102", "BR_author": "MajesticKhan", "BRopenT": "2019-05-04T19:38:48Z", "BRcloseT": "2019-05-14T05:10:15Z", "BR_text": {"BRsummary": "Deserializing Architecture json file", "BRdescription": "\n NOTE: I am running Adanet on windows 10 using python 3.5.2\n Has anybody ran into this error: TypeError: the JSON object must be str, not 'bytes'?\n I am currently debugging the code and this is what I have so far:\n gfile.read() outputs  bytes instead of string. This causes an error when _Architecture.deserialize() uses json.loads() which requires a string object.\n \n \n \n adanet/adanet/core/estimator.py\n \n \n         Lines 1145 to 1146\n       in\n       cb9c718\n \n \n \n \n \n \n  with tf.io.gfile.GFile(filename, \"rb\") as gfile: \n \n \n \n  return _Architecture.deserialize(gfile.read()) \n \n \n \n \n \n \n \n \n adanet/adanet/core/architecture.py\n \n \n         Lines 103 to 119\n       in\n       cb9c718\n \n \n \n \n \n \n  def deserialize(serialized_architecture): \n \n \n \n  \"\"\"Deserializes a serialized architecture. \n \n \n \n   \n \n \n \n      Args: \n \n \n \n        serialized_architecture: String representation of an `_Architecture` \n \n \n \n          obtained by calling `serialize`. \n \n \n \n   \n \n \n \n      Returns: \n \n \n \n        A deserialized `_Architecture` instance. \n \n \n \n      \"\"\" \n \n \n \n  \n \n \n \n  ensemble_arch = json.loads(serialized_architecture) \n \n \n \n  architecture = _Architecture(ensemble_arch[\"ensemble_candidate_name\"]) \n \n \n \n  for subnet in ensemble_arch[\"subnetworks\"]: \n \n \n \n  architecture.add_subnetwork(subnet[\"iteration_number\"], \n \n \n \n  subnet[\"builder_name\"]) \n \n \n \n  return architecture \n \n \n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "MajesticKhan", "commentT": "2019-05-04T21:22:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/MajesticKhan>@MajesticKhan</denchmark-link>\n : We only currently test on Linux machine with CI. If you find a fix, feel free to send a PR and I'll review it.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "MajesticKhan", "commentT": "2019-05-04T22:59:06Z", "comment_text": "\n \t\tGot it.\n Let me dig a little deeper into the issue and start testing out other examples to check if the issue is on my side, if not I will create a PR with the fix. I'll keep you updated through gitter\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "MajesticKhan", "commentT": "2019-05-07T11:22:33Z", "comment_text": "\n \t\tI'm trying to train on using cloud ML engine and I'm getting the same error, I'm not sure it is windows related\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "MajesticKhan", "commentT": "2019-05-07T14:02:00Z", "comment_text": "\n \t\tOkay, the serializing issue is a simple fix, gfile.read().decode(). However, there is another issue when temp_model_dir folder is still being used by an initial model preventing files from being deleted. I hacked my way around this by creating mutiple temp_model_dir folders and not deleting them. After that, it was smooth sailing.\n Currently, I'm creating a simple script to test both on windows and linux. After that, I'll figure out how to deal with the folder issue and submit a pr with the fixes.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "MajesticKhan", "commentT": "2019-05-08T05:27:26Z", "comment_text": "\n \t\tSorry accidentally clicked on closed.\n <denchmark-link:https://github.com/WillianPaiva>@WillianPaiva</denchmark-link>\n  What version of numpy are you using? When I reinstalled Adanet, it reverted back to the previous numpy to 1.15 which prevented me from running a simple script that trains a dnn model. Going back to numpy 1.16.3 allowed me to run the script again\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "MajesticKhan", "commentT": "2019-05-08T15:52:20Z", "comment_text": "\n \t\t\n When I reinstalled Adanet, it reverted back to the previous numpy to 1.15\n \n <denchmark-link:https://github.com/MajesticKhan>@MajesticKhan</denchmark-link>\n : I think our dependency requirements were hard-coded to be too strict, so I'll create a commit to loosen those requirements. Did upgrading to numpy 1.16.3 fix any of the other issues you were experiencing?\n <denchmark-link:https://github.com/WillianPaiva>@WillianPaiva</denchmark-link>\n : Are you seeing this exact error on Cloud MLE? Are you running as a single process or  distributed training?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "MajesticKhan", "commentT": "2019-05-09T04:34:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cweill>@cweill</denchmark-link>\n , I reinstalled Adanet and it defaulted to installing numpy 1.15.4 which still caused errors. I would consider strictly upgrading to match the numpy version that Tensorflow 1.13 is using which is numpy-1.16.3\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "MajesticKhan", "commentT": "2019-05-09T05:38:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cweill>@cweill</denchmark-link>\n  This is the script that I will be using to test both on Linux and Windows 10. I copied it from my repository for easier viewing. I will move on to implementing my fixes to Adanet.\n #----------------------------------------------------------------Import Libraries\n import tensorflow as tf\n import pandas as pd\n import adanet\n tf.logging.set_verbosity(\"INFO\")\n #----------------------------------------------------------------\n \n \n #----------------------------------------------------------------State meta data\n Features  = ['SepalLength', 'SepalWidth',\n              'PetalLength', 'PetalWidth']\n TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n TEST_URL  = \"http://download.tensorflow.org/data/iris_test.csv\"\n #----------------------------------------------------------------\n \n \n #----------------------------------------------------------------Create feature numeric columns\n feature_columns = []\n for key in Features:\n     feature_columns.append(tf.feature_column.numeric_column(key=key))\n #----------------------------------------------------------------\n \n \n #----------------------------------------------------------------Define dataset input function\n def input_fn(file_path, batch_size, epoch, y_name='Species'):\n \n     temp     = pd.read_csv(file_path, names = Features + [y_name], header = 0)\n     labels   = temp.pop(y_name)\n     features = temp\n \n     dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n \n     # Shuffle, repeat, and batch the examples.\n     dataset = dataset.shuffle(1000).repeat(epoch).batch(batch_size)\n \n     # Return the dataset.\n     return dataset\n #----------------------------------------------------------------\n \n \n #----------------------------------------------------------------Create the estimator\n config = tf.estimator.RunConfig(\n   model_dir              = \"Path to Directory\",\n )\n \n # Define the model head for computing loss and evaluation metrics.\n head = tf.contrib.estimator.multi_class_head(n_classes=3)\n \n # Learn to ensemble linear and neural network models.\n estimator = adanet.AutoEnsembleEstimator(\n     head = head,\n     candidate_pool = {\n         \"linear\":\n             tf.estimator.LinearEstimator(\n                 head=head,\n                 feature_columns=feature_columns),\n         \"dnn\":\n             tf.estimator.DNNEstimator(\n                 head=head,\n                 feature_columns=feature_columns,\n                 hidden_units=[10,10])},\n     max_iteration_steps=50)\n #----------------------------------------------------------------\n \n \n #----------------------------------------------------------------Execute\n # Train the Model.\n estimator.train(\n     input_fn = lambda: input_fn(TRAIN_URL,10, epoch = None),\n     max_steps = 100)\n \n # Evaluate the model.\n eval_result = estimator.evaluate(\n     input_fn = lambda: input_fn(TEST_URL,10, epoch = 1))\n #----------------------------------------------------------------\n \n \n #----------------------------------------------------------------Sources\n # https://github.com/tensorflow/models/tree/master/samples/core/get_started\n #----------------------------------------------------------------\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "MajesticKhan", "commentT": "2019-05-11T01:00:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/MajesticKhan>@MajesticKhan</denchmark-link>\n  That script looks good to me. Keep us posted. And feel free to send a PR with a fix for reading the architecture JSON file.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "MajesticKhan", "commentT": "2019-05-11T03:53:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cweill>@cweill</denchmark-link>\n  Working on that pr now...\n Two things:\n 1.) I confirmed that the json issue is affected on linux as well...need to test it on Linux\n 2.) The numpy version should be 1.16.3, I get the error below when I installed Adanet on a VM from GCE\n ImportError: Something is wrong with the numpy installation. While importing we detected an older version of numpy in ['/home/usr/.local/lib/python3.5/site-packages/numpy']. One method of fixing this is to repeatedly uninstall numpy until none is found, then reinstall this version.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "MajesticKhan", "commentT": "2019-05-11T04:32:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cweill>@cweill</denchmark-link>\n  Submitted the PR...there is an additional issue which is related to the temp_dir_folder, however I will open up a separate issue on that and close this one once the changes are confirmed for deserializing the architecture file.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "MajesticKhan", "commentT": "2019-06-05T14:59:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/MajesticKhan>@MajesticKhan</denchmark-link>\n  <denchmark-link:https://github.com/cweill>@cweill</denchmark-link>\n \n Note: I was running the following tutorial on Ubuntu 16.04, with Tensorflow 1.13, Python 3.5 and numpy 1.16.4.\n <denchmark-link:url>https://github.com/tensorflow/adanet/blob/master/adanet/examples/tutorials/customizing_adanet.ipynb</denchmark-link>\n \n I met the same problem (see the attached file for detailed error log)\n The specific code snippet I tried to run is as follows,\n <denchmark-code>#@test {\"skip\": true}\n #@title Parameters\n LEARNING_RATE = 0.003  #@param {type:\"number\"}\n TRAIN_STEPS = 5000  #@param {type:\"integer\"}\n BATCH_SIZE = 64  #@param {type:\"integer\"}\n ADANET_ITERATIONS = 2  #@param {type:\"integer\"}\n \n estimator = adanet.Estimator(\n     head=head,\n     subnetwork_generator=simple_dnn.Generator(\n         feature_columns=feature_columns,\n         optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE),\n         seed=RANDOM_SEED),\n     max_iteration_steps=TRAIN_STEPS // ADANET_ITERATIONS,\n     evaluator=adanet.Evaluator(\n         input_fn=input_fn(\"train\", training=False, batch_size=BATCH_SIZE),\n         steps=None),\n     config=make_config(\"simple_dnn\"))\n \n results, _ = tf.estimator.train_and_evaluate(\n     estimator,\n     train_spec=tf.estimator.TrainSpec(\n         input_fn=input_fn(\"train\", training=True, batch_size=BATCH_SIZE),\n         max_steps=TRAIN_STEPS),\n     eval_spec=tf.estimator.EvalSpec(\n         input_fn=input_fn(\"test\", training=False, batch_size=BATCH_SIZE),\n         steps=None,\n         start_delay_secs=1,\n         throttle_secs=1,  \n     ))\n print(\"Accuracy:\", results[\"accuracy\"])\n print(\"Loss:\", results[\"average_loss\"])\n </denchmark-code>\n \n It seems to me that updating the numpy version to 1.16.4 cannot work well. I wonder do you have any idea how to deal with this problem?\n Detailed Error Log: <denchmark-link:https://github.com/tensorflow/adanet/files/3257720/error.log>error.log</denchmark-link>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "MajesticKhan", "commentT": "2019-06-05T16:29:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LakeCarrot>@LakeCarrot</denchmark-link>\n  ,\n Running the latest version of numpy is not the issue. The error that you are seeing is related to ensemble_arch = json.loads(serialized_architecture), please see the first post for further details.\n Essentially, the serialized_architecture is in byte format when it should be in string format. However, this implies that you are running python 3.5 or lower as reading the documentation for python 3.6+ shows that json.loads() can take in bytes as inputs.\n See the links below:\n -<denchmark-link:https://docs.python.org/3.5/library/json.html>json.loads()</denchmark-link>\n  on python 3.5\n -<denchmark-link:https://docs.python.org/3.6/library/json.html>json.loads()</denchmark-link>\n  on python 3.6\n Credit goes to <denchmark-link:https://stackoverflow.com/questions/42683478/typeerror-the-json-object-must-be-str-not-bytes/42683509>Ikar Pohorsk\u00fd</denchmark-link>\n  for spotting the <denchmark-link:https://docs.python.org/3/whatsnew/3.6.html#json>updates to json.loads()</denchmark-link>\n  in python 3.6\n I believe <denchmark-link:https://github.com/cweill>@cweill</denchmark-link>\n  will release the latest version with the included update that fixes the serialization issue.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "MajesticKhan", "commentT": "2019-06-05T18:25:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/MajesticKhan>@MajesticKhan</denchmark-link>\n  ,\n Thanks a lot for your replay. Yeah. after updating python from 3.5 to 3.6, problem solved. The problem is because of json.loads() function.\n \t\t"}}}, "commit": {"commit_id": "931d6833a9489839ec54089c83ffd2aec6e20e33", "commit_author": "Charles Weill", "commitT": "2019-05-08 15:19:27-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "adanet\\pip_package\\setup.py", "file_new_name": "adanet\\pip_package\\setup.py", "file_complexity": {"file_NLOC": "54", "file_CCN": "0", "file_NToken": "155"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "29,30,31,32,33,34,35,36", "deleted_lines": "29,30,31,32,33,34"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "requirements.txt", "file_new_name": "requirements.txt", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1,2,3,4,5,6,7,8", "deleted_lines": "1,2,3,4,5,6,7,8"}}}}}}