{"BR": {"BR_id": "21", "BR_author": "xiao1228", "BRopenT": "2018-10-02T14:48:40Z", "BRcloseT": "2019-03-21T13:32:19Z", "BR_text": {"BRsummary": "Multi-GPU Training", "BRdescription": "\n Hi,\n Have you tried to run training on multiple gpus?\n I am getting the below error when I try to do that.thank you\n <denchmark-code>Traceback (most recent call last):\n   File \"train.py\", line 194, in <module>\n     main(opt)\n   File \"train.py\", line 128, in main\n     loss = model(imgs, targets, requestPrecision=True)\n   File \"/opt/anaconda/envs/pytorch_p35/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 477, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/opt/anaconda/envs/pytorch_p35/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 123, in forward\n     outputs = self.parallel_apply(replicas, inputs, kwargs)\n   File \"/opt/anaconda/envs/pytorch_p35/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 133, in parallel_apply\n     return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n   File \"/opt/anaconda/envs/pytorch_p35/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 77, in parallel_apply\n     raise output\n   File \"/opt/anaconda/envs/pytorch_p35/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 53, in _worker\n     output = module(*input, **kwargs)\n   File \"/opt/anaconda/envs/pytorch_p35/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 477, in __call__\n     result = self.forward(*input, **kwargs)\n TypeError: forward() missing 1 required positional argument: 'x'\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xiao1228", "commentT": "2018-10-02T20:13:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/xiao1228>@xiao1228</denchmark-link>\n  The requirements clearly state Python 3.6. I'd advise you to follow them.\n Multi-GPU training is still a work in progress. If you could help debug this after upgrading your Python that would be great!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xiao1228", "commentT": "2018-10-03T10:15:53Z", "comment_text": "\n \t\thi I am getting the same error after upgrade to Python 3.6. I will work on that if I can fix it will update you\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "xiao1228", "commentT": "2018-10-10T16:23:18Z", "comment_text": "\n \t\tGetting the error below by trying to move all the variables to cuda\n \n utils/utils.py\", line 293, in build_targets\n TP[b, i] = (pconf  > 0.5) & (iou_pred  > 0.5)  & (pcls == tc)\n RuntimeError: Assertion `THCTensor_(checkGPU)(state, 3, self_, src1, src2)' failed.  at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:688\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "xiao1228", "commentT": "2018-10-31T13:05:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/xiao1228>@xiao1228</denchmark-link>\n  Have you solved your 1st problem in this issue? I wanna turn the code into multi-gpu and met it, too. I'm still confused. Thank you.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "xiao1228", "commentT": "2018-11-02T14:52:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/xiao1228>@xiao1228</denchmark-link>\n  <denchmark-link:https://github.com/zhaoyang10>@zhaoyang10</denchmark-link>\n  the code does not support multi-GPU yet unfortunately. I only have a single-GPU machine so I have not been able to debug this issue. If you come up with a solution please advise me, or submit a pull request. Many thanks!!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "xiao1228", "commentT": "2018-11-29T11:02:00Z", "comment_text": "\n \t\tI've changed the code to raise an error when multi-GPU operation is attempted, until this is resolved.\n \n \n \n yolov3/train.py\n \n \n         Lines 60 to 63\n       in\n       af0033c\n \n \n \n \n \n \n  if torch.cuda.device_count() > 1: \n \n \n \n  raise Exception('Multi-GPU not currently supported: https://github.com/ultralytics/yolov3/issues/21') \n \n \n \n  # print('Using ', torch.cuda.device_count(), ' GPUs') \n \n \n \n  # model = nn.DataParallel(model) \n \n \n \n \n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "xiao1228", "commentT": "2019-03-07T19:27:51Z", "comment_text": "\n \t\tI've added multi-GPU training support to the pipeline.\n See here: <denchmark-link:https://github.com/ultralytics/yolov3/pull/121>#121</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "xiao1228", "commentT": "2019-03-08T03:36:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alexpolichroniadis>@alexpolichroniadis</denchmark-link>\n    , I run your code ,may not work on 4 x 1080ti.....\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "xiao1228", "commentT": "2019-03-08T03:56:19Z", "comment_text": "\n \t\tTested on 8x1080tis.\n What's the trace?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "xiao1228", "commentT": "2019-03-08T07:45:09Z", "comment_text": "\n \t\tEpoch       Batch        xy        wh      conf       cls     total  nTargets      time\n C:\\Users\\NJ\\Anaconda3\\lib\\site-packages\\torch\\nn\\parallel_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n warnings.warn('Was asked to gather along dimension 0, but all '\n C:\\Users\\NJ\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:24: UserWarning: PyTorch is not compiled with NCCL support\n warnings.warn('PyTorch is not compiled with NCCL support')\n <denchmark-code>    0/99      0/3643      1.13      5.46       555      13.3       575   1.6e+03      26.3\n     0/99      1/3643      1.65       8.2       833        20       863   3.2e+03      3.28\n     0/99      2/3643      2.19      11.2  1.11e+03      26.7  1.15e+03   4.8e+03      1.77\n     0/99      3/3643      2.72      14.1  1.39e+03      33.3  1.44e+03   6.4e+03      1.77\n     0/99      4/3643      3.25      17.1  1.67e+03        40  1.73e+03     8e+03      1.82\n     0/99      5/3643      3.78      20.1  1.94e+03      46.6  2.01e+03   9.6e+03      1.64\n     0/99      6/3643      4.31      23.1  2.22e+03      53.3   2.3e+03  1.12e+04      1.66\n     0/99      7/3643      4.83      26.1   2.5e+03      59.9  2.59e+03  1.28e+04      1.73\n     0/99      8/3643      5.35      29.1  2.78e+03      66.6  2.88e+03  1.44e+04      1.81\n     0/99      9/3643      5.87      32.1  3.05e+03      73.2  3.16e+03   1.6e+04      1.62\n     0/99     10/3643       6.4      35.1  3.33e+03      79.9  3.45e+03  1.76e+04      1.74\n     0/99     11/3643      6.92      38.1  3.61e+03      86.6  3.74e+03  1.92e+04      1.65\n     0/99     12/3643      7.45      41.1  3.89e+03      93.2  4.03e+03  2.08e+04      1.73\n ....\n ....\n     0/99    333/3643       176       830  5.25e+04   2.2e+03  5.57e+04  5.34e+05      1.88\n     0/99    334/3643       177       831  5.25e+04  2.21e+03  5.57e+04  5.36e+05       1.7\n     0/99    335/3643       177       833  5.25e+04  2.21e+03  5.58e+04  5.38e+05      1.79\n     0/99    336/3643       178       835  5.26e+04  2.22e+03  5.58e+04  5.39e+05       1.8\n     0/99    337/3643       178       836  5.26e+04  2.23e+03  5.59e+04  5.41e+05      1.68\n     0/99    338/3643       179       838  5.27e+04  2.23e+03  5.59e+04  5.42e+05      1.85\n     0/99    339/3643       179       840  5.27e+04  2.24e+03   5.6e+04  5.44e+05      1.91\n     0/99    340/3643       180       841  5.27e+04  2.24e+03   5.6e+04  5.46e+05      1.84\n     0/99    341/3643       180       843  5.28e+04  2.25e+03   5.6e+04  5.47e+05      1.83\n     0/99    342/3643       181       845  5.28e+04  2.26e+03  5.61e+04  5.49e+05       1.7\n     0/99    343/3643       181       846  5.29e+04  2.26e+03  5.61e+04   5.5e+05      1.95\n     0/99    344/3643       182       848  5.29e+04  2.27e+03  5.62e+04  5.52e+05      1.89\n </denchmark-code>\n \n loss is growing...\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "xiao1228", "commentT": "2019-03-08T07:48:04Z", "comment_text": "\n \t\tLooks like it's working. You are training and batches are being pushed through your model. Lack of NCCL support is a problem with your installation of Pytorch, not the code of this repo. The UserWarning can be ignored.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "xiao1228", "commentT": "2019-03-08T07:49:42Z", "comment_text": "\n \t\tAlso what is your batch size, specified when running train.py (--batch-size) ?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "xiao1228", "commentT": "2019-03-08T08:13:01Z", "comment_text": "\n \t\t<denchmark-code>parser.add_argument('--epochs', type=int, default=100, help='number of epochs')\n parser.add_argument('--batch-size', type=int, default=32, help='size of each image batch')\n parser.add_argument('--accumulated-batches', type=int, default=1, help='number of batches before optimizer step')\n parser.add_argument('--cfg', type=str, default='cfg/yolov3.cfg', help='cfg file path')\n parser.add_argument('--data-cfg', type=str, default='cfg/coco.data', help='coco.data file path')\n parser.add_argument('--multi-scale', action='store_true', help='random image sizes per batch 320 - 608')\n parser.add_argument('--img-size', type=int, default=32 * 13, help='pixels')\n parser.add_argument('--resume', action='store_true', help='resume training flag')\n parser.add_argument('--num-workers', type=int, default=0, help='number of workers for dataloader')\n parser.add_argument('--var', type=float, default=0, help='test variable')\n </denchmark-code>\n \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "xiao1228", "commentT": "2019-03-08T08:15:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alexpolichroniadis>@alexpolichroniadis</denchmark-link>\n    thank you for your reply :)\n I use the default params but the modify batch size\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "xiao1228", "commentT": "2019-03-08T08:26:24Z", "comment_text": "\n \t\t\n @alexpolichroniadis thank you for your reply :)\n I use the default params but the modify batch size\n \n Looking at your output, it looks like you pulled an earlier commit from that PR, based on the exploding loss report I'm seeing. I fixed that today in the latest commit. Are you sure you are working off the latest commit of that PR?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "xiao1228", "commentT": "2019-03-08T08:32:11Z", "comment_text": "\n \t\tToday,I just download from                  <denchmark-link:https://github.com/alexpolichroniadis/yolov3>https://github.com/alexpolichroniadis/yolov3</denchmark-link>\n \n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "xiao1228", "commentT": "2019-03-08T08:33:42Z", "comment_text": "\n \t\tMaybe,  use :<denchmark-link:https://github.com/alexpolichroniadis/yolov3/tree/multigpu>https://github.com/alexpolichroniadis/yolov3/tree/multigpu</denchmark-link>\n     will be better and I will try\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "xiao1228", "commentT": "2019-03-08T08:36:51Z", "comment_text": "\n \t\t\n Maybe, use :https://github.com/alexpolichroniadis/yolov3/tree/multigpu will be better and I will try\n \n Yes, that is the correct branch to work off. Master still has an older version.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "xiao1228", "commentT": "2019-03-08T08:54:08Z", "comment_text": "\n \t\tThis code runs like this\uff1a\n 219                     module.104.batch_norm_104.bias      True          256                [256]            0            0\n 220                         module.105.conv_105.weight      True        65280     [255, 256, 1, 1]     0.000114       0.0362\n 221                           module.105.conv_105.bias      True          255                [255]     -0.00154        0.036\n Model Summary: 222 layers, 6.19491e+07 parameters, 6.19491e+07 gradients\n Epoch       Batch        xy        wh      conf       cls     total  nTargets      time\n and then it keep this state up to now.It looks like this program may not running\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "xiao1228", "commentT": "2019-03-08T09:20:14Z", "comment_text": "\n \t\t\n This code runs like this\uff1a\n 219 module.104.batch_norm_104.bias True 256 [256] 0 0\n 220 module.105.conv_105.weight True 65280 [255, 256, 1, 1] 0.000114 0.0362\n 221 module.105.conv_105.bias True 255 [255] -0.00154 0.036\n Model Summary: 222 layers, 6.19491e+07 parameters, 6.19491e+07 gradients\n Epoch Batch xy wh conf cls total nTargets time\n and then it keep this state up to now.It looks like this program may not running\n \n I noticed that you are running this code on Windows. Keep in mind that pytorch's DataParallel might not be operational on Windows machines due to lack of NCCL support, see <denchmark-link:https://github.com/pytorch/pytorch/issues/12277>here</denchmark-link>\n . My testing was on an Ubuntu machine.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "xiao1228", "commentT": "2019-03-08T09:35:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alexpolichroniadis>@alexpolichroniadis</denchmark-link>\n  ,Thanks for your help,this code work well now  :)\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "xiao1228", "commentT": "2019-03-08T17:23:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alexpolichroniadis>@alexpolichroniadis</denchmark-link>\n  I get an error when I run the following on a GCP PyTorch instance with 2 GPUs. I noticed you changed coco.data from the darknet default, so I updated this to point back to the default, and this fixed the error.\n sudo rm -rf yolov3 && git clone -b multigpu --depth 1 https://github.com/alexpolichroniadis/yolov3\n cd yolov3 && python3 train.py\n \n Namespace(accumulated_batches=1, batch_size=16, cfg='cfg/yolov3.cfg', data_cfg='cfg/coco.data', epochs=100, img_size=416, multi_scale=Fa\n lse, num_workers=0, resume=False, var=0)\n Using CUDA. Available devices: \n 0 - Tesla P100-PCIE-16GB - 16280MB\n 1 - Tesla P100-PCIE-16GB - 16280MB\n Traceback (most recent call last):\n   File \"train.py\", line 234, in <module>\n     var=opt.var,\n   File \"train.py\", line 46, in train\n     train_loader = ImageLabelDataset(train_path, batch_size, img_size, multi_scale=multi_scale, augment=True)\n   File \"/home/ultralytics/yolov3/utils/datasets.py\", line 105, in __init__\n     for x in self.img_files]\n AttributeError: 'ImageLabelDataset' object has no attribute 'img_files'\n Now I see a seperate problem though, there doesn't appear to be any speedup. Single P100 takes about 0.6s, the same as 2 P100s here:\n <denchmark-code>   Epoch       Batch        xy        wh      conf       cls     total  nTargets      time\n /opt/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but a\n ll input tensors were scalars; will instead unsqueeze and return a vector.\n   warnings.warn('Was asked to gather along dimension 0, but all '\n     0/99      0/7327      0.51      2.76       277      6.66       287       121         7\n     0/99      1/7327      0.52       2.7       278      6.65       287        99     0.712\n     0/99      2/7327     0.539      2.83       278      6.65       288       143     0.631\n     0/99      3/7327     0.543      2.83       278      6.64       288       123     0.608\n ...\n </denchmark-code>\n \n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "xiao1228", "commentT": "2019-03-08T17:47:11Z", "comment_text": "\n \t\tWhats your batch size? If your batch can fit perfectly on one GPU, then (in most cases) you are better off using a single GPU. The benefits of a multi GPU setup is cranking up the batch size and having more images be processed in the same amount of time. Try setting your --batch-size to 128 (or something outside what a single GPU can handle) for example and re-testing.\n Since batching happens on the CPU, there are also cases where the CPU becomes the bottleneck then (the GPU waits for the batch to be created). This becomes more apparent with big batch sizes. In all there is balance that needs to be found and is not directly apparent.\n One other thing: With nn.Dataparallel, there is preliminary loading of the GPUs with a copy of the model each. This happens on the first batch and is reflected in the higher time reported when processing the first batch.\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "xiao1228", "commentT": "2019-03-08T18:04:32Z", "comment_text": "\n \t\tAn example:\n On my setup, for a batch size of 128, processing time per batch is 1 sec.\n For a batch size of 256, it is 1.6sec.\n (all cases with dataparallel on).\n On a single 1080Ti a batch size of 128 is not doable.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "xiao1228", "commentT": "2019-03-16T18:35:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/door5719>@door5719</denchmark-link>\n  <denchmark-link:https://github.com/alexpolichroniadis>@alexpolichroniadis</denchmark-link>\n  thanks for the info. We started on our own multi_gpu branch (<denchmark-link:https://github.com/ultralytics/yolov3/tree/multi_gpu>https://github.com/ultralytics/yolov3/tree/multi_gpu</denchmark-link>\n ), with a secondary goal of trying out a different loss approach, selecting a single anchor from the 9 available for each target. The new loss  produced significantly worse results, so it appears the current method of selecting one anchor from each yolo layer is correct. In the process we did get multi_gpu operational, though not with the speedups expected. We did not attempt to use a multithreaded PyTorch dataloader, nor PIL in place of OpenCV, as we found both of these slower in our single-GPU profiling last year.\n We don't have multiple gpu machines on premise so we tested this with GCP Deep Learning VMs. We used  (max that 1 P100 can handle) times the number of GPUs. All other training setting were defaults. We selected the fastest batch out of the first 30 for timing purposes. Results are below for our branch  and the <denchmark-link:https://github.com/ultralytics/yolov3/pull/121>#121</denchmark-link>\n  PR. In both cases the speedups were very poor. It's possible the IO ops were constrained by GCP due to the limited SSD size, we will try again with a larger SSD but we wanted to get these results out here for feedback. If anyone has another repo or PR we can compare against please let us know!\n <denchmark-link:https://cloud.google.com/deep-learning-vm/>https://cloud.google.com/deep-learning-vm/</denchmark-link>\n \n  n1-highmem-4 (4 vCPUs, 26 GB memory)\n  Intel Skylake\n  1-4 x NVIDIA Tesla P100\n  500 GB SSD\n \n \n \n GPUs\n batch_size\n yolov3/tree/multi_gpu\n yolov3/pull/121\n \n \n \n \n (P100)\n (images)\n (s/batch)\n (s/batch)\n \n \n 1\n 26\n 0.91s\n 1.05s\n \n \n 2\n 52\n 1.60s\n 1.76s\n \n \n 4\n 104\n 2.26s\n 2.81s\n \n \n \n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "xiao1228", "commentT": "2019-03-17T05:46:10Z", "comment_text": "\n \t\tI think torch.nn.parallel.DistributedDataParallel is better than nn.DataParallel. The usage of DataParallel should be  bottleneck.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "xiao1228", "commentT": "2019-03-18T03:46:01Z", "comment_text": "\n \t\tBecause the box2 is torch.FloatTensor, the anchor_vec is on cpu. while the box1 is on GPU.\n so, just use .cuda() to transform the data  into torch.cuda.FloatTensor()\n `        box2 = anchor_vec.cuda().unsqueeze(1)\n <denchmark-code>    inter_area = torch.min(box1, box2).prod(2)`\n </denchmark-code>\n \n but, when you fix this, the below will also come out some bug.\n `        txy[b, a, gj, gi] = gxy - gxy.floor()\n <denchmark-code>    # Width and height\n     twh[b, a, gj, gi] = torch.log(gwh/ anchor_vec[a]) `\n </denchmark-code>\n \n you need to transform the data type to GPU or Cuda according to the error info.\n However, the main reason for multi-GPU training lies in\n         for i, (imgs, targets, _, _) in enumerate(dataloader):\n where the imgs is a tensor, but the targets are lists. When parallel the imgs.to(device). The imgs are divided into batch_size/GPU_nums. But the targets cannot targets.to(device)(since it is a list), and the targets are the same num as the batch_size, cannot distribute into every GPUs.\n             if nM > 0: lxy = k * MSELoss(xy[mask], txy[mask]) lwh = k * MSELoss(wh[mask], twh[mask])\n the xy, txy, wh, twh is not the same dims as the batch_size.\n the xy, wh is batch_size/GPU_nums.\n but the txy, twh is the targets_nums( batch_size). There will occur some error.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "xiao1228", "commentT": "2019-03-18T10:58:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/longxianlei>@longxianlei</denchmark-link>\n  we just PRd our under-development multi_gpu branch into the master branch, so multi-GPU functionality now works. Many of the items you raised above should be resolved. Can you try the latest commit and see if it works for you? See <denchmark-link:https://github.com/ultralytics/yolov3/pull/135>#135</denchmark-link>\n  for more info.\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "xiao1228", "commentT": "2019-03-18T13:30:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  keep in mind that batch sizes should be integer multiples of the number of available GPUs. For a batch size of 26 on 4 GPUs, you are essentially pushing 26//4 = 6 images on all GPUs and the two remaining ones are pushed on the last GPU. This is unbalanced as each GPU processes batch sizes of 6/6/6/8.\n The ideal batch size to test here would be 4*6=24. And multiples of 24 thereafter. Also it is true that the actual bottleneck might be IO at this point.\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "xiao1228", "commentT": "2019-03-19T14:47:18Z", "comment_text": "\n \t\tUpdated times with batch_size=24, and comparison to existing study.\n <denchmark-link:https://cloud.google.com/deep-learning-vm/>https://cloud.google.com/deep-learning-vm/</denchmark-link>\n \n  n1-highmem-4 (4 vCPUs, 26 GB memory)\n  Intel Skylake\n  1-4 x NVIDIA Tesla P100\n  100 GB SSD\n \n \n \n GPUs\n batch_size\n 613ce1b\n COCO epoch\n \n \n \n \n (P100)\n (images)\n (s/batch)\n (min/epoch)\n \n \n 1\n 24\n 0.84s\n 70min\n \n \n 2\n 48\n 1.27s\n 53min\n \n \n 4\n 96\n 2.11s\n 44min\n \n \n \n Comparison results from <denchmark-link:https://github.com/ilkarman/DeepLearningFrameworks>https://github.com/ilkarman/DeepLearningFrameworks</denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/26833433/54614486-2ab26f00-4a65-11e9-8f93-8c1ad533f04d.png></denchmark-link>\n \n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "xiao1228", "commentT": "2019-03-21T13:23:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alexpolichroniadis>@alexpolichroniadis</denchmark-link>\n , <denchmark-link:https://github.com/longxianlei>@longxianlei</denchmark-link>\n , <denchmark-link:https://github.com/LightToYang>@LightToYang</denchmark-link>\n  Great news! Lack of multithreading in the dataloader was slowing down multi-GPU significantly (<denchmark-link:https://github.com/ultralytics/yolov3/issues/141>#141</denchmark-link>\n ). I reimplented support for DataLoader multithreading, and speeds have improved greatly (more than double in some cases). The new test results are below for the latest commit.\n <denchmark-link:https://cloud.google.com/deep-learning-vm/>https://cloud.google.com/deep-learning-vm/</denchmark-link>\n \n  n1-standard-8 (8 vCPUs, 30 GB memory)\n  Intel Skylake\n  1-4 x NVIDIA Tesla P100\n  100 GB SSD\n \n \n \n GPUs\n batch_size\n speed\n COCO epoch\n \n \n \n \n (P100)\n (images)\n (s/batch)\n (min/epoch)\n \n \n 1\n 16\n 0.39s\n 48min\n \n \n 2\n 32\n 0.48s\n 29min\n \n \n 4\n 64\n 0.65s\n 20min\n \n \n \n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "xiao1228", "commentT": "2019-03-21T14:24:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  I never noticed that the default for the dataloder's num_workers set to 0 because I set it manually all the time, whoops. \n Good results indeed. In line with what I was getting.\n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "xiao1228", "commentT": "2020-08-13T02:06:08Z", "comment_text": "\n \t\tHad the same issue. But I've used this repo on multi-GPU before and it's worked well. Somebody had posted saying the batch-size in the last iteration might be lesser than the batch-size given during training so I removed a few images to make the validation set images a multiple of 8, as I'd given 8 as my batch-size during training and it solved the issue.\n \t\t"}}}, "commit": {"commit_id": "45fac6bff10a7c71e9a3d18475ff5ef7246aad14", "commit_author": "Glenn Jocher", "commitT": "2019-03-17 23:45:39+02:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "models.py", "file_new_name": "models.py", "file_complexity": {"file_NLOC": "224", "file_CCN": "60", "file_NToken": "2425"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "182,201,208,209", "deleted_lines": "182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206", "method_info": {"method_name": "forward", "method_params": "self,x,var", "method_startline": "182", "method_endline": "209", "method_complexity": {"method_NLOC": "26", "method_CCN": "11", "method_NToken": "241", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "126,128,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,182,201,208,209", "deleted_lines": "122,129,130,131,132,133,135,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206", "method_info": {"method_name": "forward", "method_params": "self,p,img_size,targets,var", "method_startline": "122", "method_endline": "211", "method_complexity": {"method_NLOC": "54", "method_CCN": "10", "method_NToken": "791", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "217,223,226,227,228", "deleted_lines": "228", "method_info": {"method_name": "create_grids", "method_params": "self,img_size,nG,device", "method_startline": "217", "method_endline": "228", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "147", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "119,126,128,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160", "deleted_lines": "119,120,122,129,130,131,132,133,135,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165", "method_info": {"method_name": "forward", "method_params": "self,p,img_size,var", "method_startline": "119", "method_endline": "165", "method_complexity": {"method_NLOC": "28", "method_CCN": "5", "method_NToken": "442", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "228", "deleted_lines": "228,229,230,249,250,251,252,253,254,258,259,260,264,265", "method_info": {"method_name": "forward", "method_params": "self,x,targets,var", "method_startline": "228", "method_endline": "265", "method_complexity": {"method_NLOC": "34", "method_CCN": "14", "method_NToken": "315", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": null, "deleted_lines": "273,279,282,283", "method_info": {"method_name": "create_grids", "method_params": "self,img_size,nG", "method_startline": "273", "method_endline": "283", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "113", "method_nesting_level": "0"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "105,109,110,111,112,113,114,116,117", "deleted_lines": "107,109,112,114,115,116,117", "method_info": {"method_name": "__init__", "method_params": "self,anchors,nC,img_size,yolo_layer,cfg", "method_startline": "101", "method_endline": "117", "method_complexity": {"method_NLOC": "12", "method_CCN": "3", "method_NToken": "113", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "test.py", "file_new_name": "test.py", "file_complexity": {"file_NLOC": "131", "file_CCN": "1", "file_NToken": "1192"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "20,21", "deleted_lines": "20", "method_info": {"method_name": "test", "method_params": "cfg,data_cfg,weights,batch_size,img_size,iou_thres,conf_thres,nms_thres,save_json,model", "method_startline": "11", "method_endline": "21", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "41", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "20", "deleted_lines": "20", "method_info": {"method_name": "test", "method_params": "cfg,data_cfg,weights,batch_size,img_size,iou_thres,conf_thres,nms_thres,save_json", "method_startline": "11", "method_endline": "20", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "37", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "train.py", "file_new_name": "train.py", "file_complexity": {"file_NLOC": "140", "file_CCN": "1", "file_NToken": "1189"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "20", "method_info": {"method_name": "train", "method_params": "cfg,data_cfg,img_size,resume,epochs,batch_size,accumulated_batches,multi_scale,freeze_backbone,var", "method_startline": "10", "method_endline": "20", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "38", "method_nesting_level": "0"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "utils\\datasets.py", "file_new_name": "utils\\datasets.py", "file_complexity": {"file_NLOC": "223", "file_CCN": "41", "file_NToken": "2827"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "209,210,211,212,222", "deleted_lines": "210", "method_info": {"method_name": "__next__", "method_params": "self", "method_startline": "116", "method_endline": "223", "method_complexity": {"method_NLOC": "78", "method_CCN": "19", "method_NToken": "976", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "utils\\gcp.sh", "file_new_name": "utils\\gcp.sh", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7,19,20,21,22,23,24,25,26", "deleted_lines": null}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "utils\\torch_utils.py", "file_new_name": "utils\\torch_utils.py", "file_complexity": {"file_NLOC": "17", "file_CCN": "7", "file_NToken": "137"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "19,21,22,23", "deleted_lines": "20,21,22", "method_info": {"method_name": "select_device", "method_params": "force_cpu", "method_startline": "10", "method_endline": "26", "method_complexity": {"method_NLOC": "12", "method_CCN": "6", "method_NToken": "104", "method_nesting_level": "0"}}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 17, "file_old_name": "utils\\utils.py", "file_new_name": "utils\\utils.py", "file_complexity": {"file_NLOC": "275", "file_CCN": "75", "file_NToken": "3828"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "30", "deleted_lines": "29,30", "method_info": {"method_name": "load_classes", "method_params": "path", "method_startline": "29", "method_endline": "33", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "35", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "234,235,236,238,239,240,242,243,245,246,247,248", "deleted_lines": "234,235,236,237,238,239,240,241,242,243,244,245,246,247,248", "method_info": {"method_name": "wh_iou", "method_params": "box1,box2", "method_startline": "234", "method_endline": "248", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "80", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283", "deleted_lines": "251,252,253,254,255,256,257,259,260,261,262,263,264,266,267,269,270,271,273,274,275,276,278,279,280,281,282", "method_info": {"method_name": "compute_loss", "method_params": "p,targets", "method_startline": "251", "method_endline": "283", "method_complexity": {"method_NLOC": "22", "method_CCN": "5", "method_NToken": "316", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "36,37,45", "deleted_lines": "36,44", "method_info": {"method_name": "model_info", "method_params": "model", "method_startline": "36", "method_endline": "45", "method_complexity": {"method_NLOC": "9", "method_CCN": "5", "method_NToken": "146", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "433,434,435", "deleted_lines": null, "method_info": {"method_name": "get_yolo_layers", "method_params": "model", "method_startline": "433", "method_endline": "435", "method_complexity": {"method_NLOC": "3", "method_CCN": "4", "method_NToken": "36", "method_nesting_level": "0"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "466", "deleted_lines": "468,469", "method_info": {"method_name": "coco_only_people", "method_params": "path", "method_startline": "465", "method_endline": "471", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "76", "method_nesting_level": "0"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "477,480,487", "deleted_lines": "477,479,482,489", "method_info": {"method_name": "plot_results", "method_params": "", "method_startline": "477", "method_endline": "492", "method_complexity": {"method_NLOC": "13", "method_CCN": "4", "method_NToken": "154", "method_nesting_level": "0"}}}, "hunk_7": {"Ismethod": 1, "added_lines": null, "deleted_lines": "449", "method_info": {"method_name": "strip_optimizer_from_checkpoint", "method_params": "filename", "method_startline": "447", "method_endline": "452", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "41", "method_nesting_level": "0"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,314,317,318,320,321,323,324,325,326,327,328", "deleted_lines": "286,287,288,289,291,294,297,298,300,301,302,304,317,318,319,320,321,322,323,324,325,326,327,328", "method_info": {"method_name": "build_targets", "method_params": "model,targets,pred", "method_startline": "286", "method_endline": "328", "method_complexity": {"method_NLOC": "24", "method_CCN": "4", "method_NToken": "297", "method_nesting_level": "0"}}}, "hunk_9": {"Ismethod": 1, "added_lines": "70,71", "deleted_lines": null, "method_info": {"method_name": "plot_one_box", "method_params": "x,img,color,label,line_thickness", "method_startline": "70", "method_endline": "81", "method_complexity": {"method_NLOC": "11", "method_CCN": "5", "method_NToken": "239", "method_nesting_level": "0"}}}, "hunk_10": {"Ismethod": 1, "added_lines": "354,376,391,392,393,395,399,407,414", "deleted_lines": "331,332,333,334,335,336,337,338,339,340,341,342,343,344,379,380,381,396,397,398,400,404,412,419", "method_info": {"method_name": "non_max_suppression", "method_params": "prediction,conf_thres,nms_thres", "method_startline": "331", "method_endline": "430", "method_complexity": {"method_NLOC": "48", "method_CCN": "16", "method_NToken": "561", "method_nesting_level": "0"}}}, "hunk_11": {"Ismethod": 1, "added_lines": "181", "deleted_lines": "179", "method_info": {"method_name": "compute_ap", "method_params": "recall,precision", "method_startline": "179", "method_endline": "204", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "139", "method_nesting_level": "0"}}}, "hunk_12": {"Ismethod": 1, "added_lines": "208,210,211,213,217,224,225,227,228,229,231", "deleted_lines": "208,209,210,212,216,217,223,224,225,226,227,229,230,231", "method_info": {"method_name": "bbox_iou", "method_params": "box1,box2,x1y1x2y2", "method_startline": "207", "method_endline": "231", "method_complexity": {"method_NLOC": "15", "method_CCN": "2", "method_NToken": "275", "method_nesting_level": "0"}}}, "hunk_13": {"Ismethod": 1, "added_lines": "474,476,477,480,487", "deleted_lines": "477,479,482,489", "method_info": {"method_name": "plot_results", "method_params": "start", "method_startline": "474", "method_endline": "490", "method_complexity": {"method_NLOC": "13", "method_CCN": "4", "method_NToken": "159", "method_nesting_level": "0"}}}, "hunk_14": {"Ismethod": 1, "added_lines": "238,239,240,242,243,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304", "deleted_lines": "237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,259,260,261,262,263,264,266,267,269,270,271,273,274,275,276,278,279,280,281,282,284,285,286,287,288,289,291,294,297,298,300,301,302,304", "method_info": {"method_name": "build_targets", "method_params": "target,anchor_vec,nA,nC,nG", "method_startline": "237", "method_endline": "304", "method_complexity": {"method_NLOC": "38", "method_CCN": "7", "method_NToken": "481", "method_nesting_level": "0"}}}, "hunk_15": {"Ismethod": 1, "added_lines": "127", "deleted_lines": "125", "method_info": {"method_name": "ap_per_class", "method_params": "tp,conf,pred_cls,target_cls", "method_startline": "125", "method_endline": "176", "method_complexity": {"method_NLOC": "25", "method_CCN": "6", "method_NToken": "305", "method_nesting_level": "0"}}}, "hunk_16": {"Ismethod": 1, "added_lines": "455", "deleted_lines": "456,457", "method_info": {"method_name": "coco_class_count", "method_params": "path", "method_startline": "454", "method_endline": "462", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "98", "method_nesting_level": "0"}}}}}}}}