{"BR": {"BR_id": "80", "BR_author": "ET-Chan", "BRopenT": "2018-03-08T17:35:47Z", "BRcloseT": "2018-03-09T11:00:19Z", "BR_text": {"BRsummary": "gpu_allow_growth is not working", "BRdescription": "\n Calling the tutorial code with additional argument gpu_allow_growth\n <denchmark-code>python -m bin.main train_and_eval --gpu_allow_growth --model config/models/nmt_small.py --config config/opennmt-defaults.yml config/data/toy-ende.yml\n </denchmark-code>\n \n does not prevent GPU memory being fully allocated. After some digging, it can be found that a call in <denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/utils/parallel.py#L27>https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/utils/parallel.py#L27</denchmark-link>\n  is causing this.\n It seems that this issue is already brought up on a <denchmark-link:https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow>stackoverflow question</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ET-Chan", "commentT": "2018-03-09T09:53:27Z", "comment_text": "\n \t\tInteresting. Thanks for highlighting the culprit!\n An easy fix would be to not call device_lib.list_local_devices() when num_devices is 1, thus making --gpu_allow_growth only work for single GPU training. I would assume that when training on multiple GPUs, people do not execute other tasks on them.\n What do you think?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ET-Chan", "commentT": "2018-03-09T10:37:33Z", "comment_text": "\n \t\tI am not entirely sure if this is a reasonable assumption. We, however, can enforce allow_growth by a trick mentioned in\n <denchmark-link:https://github.com/tensorflow/tensorflow/issues/8021>tensorflow/tensorflow#8021</denchmark-link>\n \n Specifically, create a session before the first list_local_devices call, as some gpu options, e.g. allow_growth, only have effect when it is used in the 1st session created in the process.\n I've tested in by just adding\n <denchmark-code>gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333, allow_growth=True)\n sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n </denchmark-code>\n \n in <denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/blob/master/bin/main.py#L79>https://github.com/OpenNMT/OpenNMT-tf/blob/master/bin/main.py#L79</denchmark-link>\n  and it works.\n (For details why this works, read <denchmark-link:https://github.com/tensorflow/tensorflow/issues/8136>tensorflow/tensorflow#8136</denchmark-link>\n )\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ET-Chan", "commentT": "2018-03-09T11:01:01Z", "comment_text": "\n \t\tThanks for the details. I applied your recommendation in the commit above.\n \t\t"}}}, "commit": {"commit_id": "eadbb039cb7c28d9d091921fc226b0d0b2b5b079", "commit_author": "Guillaume Klein", "commitT": "2018-03-09 11:58:54+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "opennmt\\runner.py", "file_new_name": "opennmt\\runner.py", "file_complexity": {"file_NLOC": "150", "file_CCN": "23", "file_NToken": "1275"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "48,49,50,51", "deleted_lines": null}}}}}}