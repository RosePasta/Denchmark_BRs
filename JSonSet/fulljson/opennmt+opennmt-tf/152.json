{"BR": {"BR_id": "152", "BR_author": "IdiosyncraticDragon", "BRopenT": "2018-06-15T03:17:41Z", "BRcloseT": "2018-06-15T07:25:02Z", "BR_text": {"BRsummary": "Error when using pre-trained word embedding features", "BRdescription": "\n System configuration:\n Tensorflow : 1.8.0\n OpenNMT-tf: 1.5.0\n I use pre-trained word embedding for training Transformer, and it can only last for one epoch and will be failed when it is constructing the model for the next epoch. Here is my code, the \"embedding_file_key\" and \"embedding_file_with_header\" caused the error:\n <denchmark-code>\n class Transformer(onmt.models.Transformer):\n   \"\"\"Defines a Transformer model as decribed in https://arxiv.org/abs/1706.03762.\"\"\"\n   def __init__(self):\n     super(Transformer, self).__init__(\n         source_inputter=onmt.inputters.WordEmbedder(\n             vocabulary_file_key=\"source_words_vocabulary\",\n             embedding_file_key=\"source_words_embeddings\",\n             embedding_file_with_header=False,\n             embedding_size=512),\n         target_inputter=onmt.inputters.WordEmbedder(\n             vocabulary_file_key=\"target_words_vocabulary\",\n             embedding_file_key=\"target_words_embeddings\",\n             embedding_file_with_header=False,\n             embedding_size=512),\n         num_layers=6,\n         num_units=512,\n         num_heads=8,\n         ffn_inner_dim=2048,\n         dropout=0.1,\n         attention_dropout=0.1,\n         relu_dropout=0.1)\n \n model = Transformer()\n \n runner = Runner(\n       model,\n       config,\n       seed=None,\n       num_devices=1,\n       gpu_allow_growth=True)\n \n runner.train_and_evaluate()\n </denchmark-code>\n \n My config file is almost the same with <denchmark-link:https://github.com/OpenNMT/OpenNMT-tf/blob/master/scripts/wmt/config/wmt_ende.yml>OpenNMT-tf/scripts/wmt/config/wmt_ende.yml</denchmark-link>\n  except adding \"source_words_embeddings\" and \"target_words_embeddings\" :\n <denchmark-code># The directory where models and summaries will be saved. It is created if it does not exist.\n #model_dir: /home/lgy/deepModels/tf_models/wmt_ende_transformer_4gpu_lr2_ws8000_dur2_0.998\n model_dir: /home/lgy/deepModels/tf_models\n \n data:\n   train_features_file: /home/lgy/data/wmt/wmt14-de-en/amax/train.en\n   train_labels_file: /home/lgy/data/wmt/wmt14-de-en/amax/train.de\n   eval_features_file: /home/lgy/data/wmt/wmt14-de-en/amax/valid.en\n   eval_labels_file: /home/lgy/data/wmt/wmt14-de-en/amax/valid.de\n   source_words_vocabulary: /home/lgy/data/wmt/wmt14-de-en/amax/wmtende.vocab\n   target_words_vocabulary: /home/lgy/data/wmt/wmt14-de-en/amax/wmtende.vocab\n   source_words_embeddings: /home/lgy/deepModels/tf_models/wmt_ende_transformer_4gpu_lr2_ws8000_dur2_0.998/compositional_encode_M32K16.txt\n   target_words_embeddings: /home/lgy/deepModels/tf_models/wmt_ende_transformer_4gpu_lr2_ws8000_dur2_0.998/compositional_decode_M32K16.txt\n </denchmark-code>\n \n I can run the first epoch successfully, but failed when the model is constructing for the next epoch. It should be noted that if I delete the the \"embedding_file_key\" and \"embedding_file_with_header\" in the code, it ran successfully without error. Here is the error message:\n <denchmark-code>INFO:tensorflow:Saving checkpoints for 16221 into /home/lgy/deepModels/tf_models/wmt_ende_transformer_1gpu_lr2_ws8000_dur2_0.998_compositionalencoding_M64K16/model.ckpt.\n INFO:tensorflow:Loss for final step: 3.9958026.\n INFO:tensorflow:Calling model_fn.\n INFO:tensorflow:Done calling model_fn.\n INFO:tensorflow:Starting evaluation at 2018-06-14-15:33:54\n INFO:tensorflow:Graph was finalized.\n 2018-06-14 23:33:54.898526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\n 2018-06-14 23:33:54.898579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n 2018-06-14 23:33:54.898588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0\n 2018-06-14 23:33:54.898594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N\n 2018-06-14 23:33:54.898894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15127 MB memory) -> physical GPU (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 6.0)\n INFO:tensorflow:Restoring parameters from /home/lgy/deepModels/tf_models/wmt_ende_transformer_1gpu_lr2_ws8000_dur2_0.998_compositionalencoding_M64K16/model.ckpt-16221\n INFO:tensorflow:Running local_init_op.\n 2018-06-14 23:33:55.571416: I tensorflow/core/kernels/lookup_util.cc:373] Table trying to initialize from file /home/lgy/data/wmt/wmt14-de-en/amax/wmtende.vocab is already initialized.\n INFO:tensorflow:Done running local_init_op.\n INFO:tensorflow:Evaluation predictions saved to /home/lgy/deepModels/tf_models/wmt_ende_transformer_1gpu_lr2_ws8000_dur2_0.998_compositionalencoding_M64K16/eval/predictions.txt.16221\n INFO:tensorflow:BLEU evaluation score: 15.020000\n INFO:tensorflow:Finished evaluation at 2018-06-14-15:39:44\n INFO:tensorflow:Saving dict for global step 16221: global_step = 16221, loss = 3.0185592\n INFO:tensorflow:Calling model_fn.\n Traceback (most recent call last):\n   File \"tmp_train_transformer.py\", line 74, in <module>\n     runner.train_and_evaluate()\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/runner.py\", line 148, in train_and_evaluate\n     tf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 439, in train_and_evaluate\n     executor.run()\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 518, in run\n     self.run_local()\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 657, in run_local\n     eval_result = evaluator.evaluate_and_export()\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 858, in evaluate_and_export\n     self._export_eval_result(eval_result, is_the_final_export)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/training.py\", line 889, in _export_eval_result\n     is_the_final_export=is_the_final_export)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/exporter.py\", line 232, in export\n     is_the_final_export)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/exporter.py\", line 123, in export\n     strip_default_attrs=self._strip_default_attrs)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 613, in export_savedmodel\n     config=self.config)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 831, in _call_model_fn\n     model_fn_results = self._model_fn(features=features, **kwargs)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/models/model.py\", line 128, in _model_fn\n     _, predictions = self._build(features, labels, params, mode, config=config)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/models/sequence_to_sequence.py\", line 185, in _build\n     return_alignment_history=True))\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/decoders/self_attention_decoder.py\", line 315, in dynamic_decode_and_search\n     eos_id=end_token)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/utils/beam_search.py\", line 557, in beam_search\n     back_prop=False)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3224, in while_loop\n     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2956, in BuildLoop\n     pred, body, original_loop_vars, loop_vars, shape_invariants)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2893, in _BuildLoop\n     body_result = body(*packed_vars_for_body)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/utils/beam_search.py\", line 485, in inner_loop\n     i, alive_seq, alive_log_probs, states)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/utils/beam_search.py\", line 379, in grow_topk\n     flat_logits, flat_states = symbols_to_logits_fn(flat_ids, i, flat_states)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/decoders/self_attention_decoder.py\", line 101, in _impl\n     inputs = embedding_fn(ids[:, -1:])\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/models/sequence_to_sequence.py\", line 103, in _target_embedding_fn\n     return self.target_inputter.transform(ids, mode=mode)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/inputters/text_inputter.py\", line 390, in transform\n     trainable=self.trainable)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n     use_resource=use_resource, constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 751, in _get_single_variable\n     \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\n ValueError: Variable transformer/decoder/w_embs does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\n </denchmark-code>\n \n This message starts from the end of the first epoch including evaluating BLEU on the validation set and ends at the ValueError. According to the error message,  I edited line 103 in \u201c/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.5.0-py2.7.egg/opennmt/models/sequence_to_sequence.py\u201d from\n <denchmark-code>def _scoped_target_embedding_fn(self, mode, scope):\n     def _target_embedding_fn(ids):\n       try:\n         with tf.variable_scope(scope):\n           return self.target_inputter.transform(ids, mode=mode)\n       except ValueError:\n         with tf.variable_scope(scope, reuse=True):\n           return self.target_inputter.transform(ids, mode=mode) # line 103\n     return _target_embedding_fn\n </denchmark-code>\n \n to\n <denchmark-code>def _scoped_target_embedding_fn(self, mode, scope):\n     def _target_embedding_fn(ids):\n       try:\n         with tf.variable_scope(scope):\n           return self.target_inputter.transform(ids, mode=mode)\n       except ValueError:\n         with tf.variable_scope(scope, reuse=tf.AUTO_REUSE): # I edit here\n           return self.target_inputter.transform(ids, mode=mode)\n     return _target_embedding_fn\n </denchmark-code>\n \n Then the running failed again at the same place but with a different error message (The first epoch is also successful and it fails at the beginning of the second epoch). The message is\n <denchmark-code>  File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.1.0-py2.7.egg/opennmt/models/sequence_to_sequence.py\", line 105, in _target_embedding_fn\n     return self.target_inputter.transform(ids, mode=mode)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/OpenNMT_tf-1.1.0-py2.7.egg/opennmt/inputters/text_inputter.py\", line 390, in transform\n     trainable=self.trainable)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 439, in get_variable\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\n     use_resource=use_resource, constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 800, in _get_single_variable\n     use_resource=use_resource)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2157, in variable\n     use_resource=use_resource)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2147, in <lambda>\n     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 2130, in default_variable_creator\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\n     constraint=constraint)\n   File \"/home/lgy/test/nmt_transformer_opennmt_tf-workspace/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n     \"initializer.\" % name)\n ValueError: Initializer for variable transformer/decoder/w_embs_1/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.\n </denchmark-code>\n \n I think this is a bug, but I am not sure whether it is the bug of OpenNMT-tf or tensorflow, and I found a issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/14729>tensorflow/tensorflow#14729</denchmark-link>\n  talks about a similar error in github of tensorflow.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "IdiosyncraticDragon", "commentT": "2018-06-15T07:26:02Z", "comment_text": "\n \t\tThanks for reporting! I was able to quickly reproduce it. The commit above should fix this.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "IdiosyncraticDragon", "commentT": "2018-06-15T11:48:53Z", "comment_text": "\n \t\tGreat! I tried and the bug has really been fixed. Thanks for the efficient work!\n \t\t"}}}, "commit": {"commit_id": "faacf80a715ee1140ea5bb628d4967289a3ab745", "commit_author": "Guillaume Klein", "commitT": "2018-06-15 09:25:01+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19,20", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "opennmt\\inputters\\text_inputter.py", "file_new_name": "opennmt\\inputters\\text_inputter.py", "file_complexity": {"file_NLOC": "434", "file_CCN": "51", "file_NToken": "2650"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "379,380,384", "deleted_lines": "379,380,382", "method_info": {"method_name": "transform", "method_params": "self,inputs,mode", "method_startline": "365", "method_endline": "399", "method_complexity": {"method_NLOC": "29", "method_CCN": "3", "method_NToken": "187", "method_nesting_level": "1"}}}}}}}}