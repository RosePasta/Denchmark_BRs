{"BR": {"BR_id": "1015", "BR_author": "carter54", "BRopenT": "2019-11-20T02:31:59Z", "BRcloseT": "2019-11-20T08:20:28Z", "BR_text": {"BRsummary": "prev_len in gpt.py", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n the gpt code in\n \n \n \n gluon-nlp/scripts/text_generation/model/gpt.py\n \n \n          Line 258\n       in\n       5e11334\n \n \n \n \n \n \n  prev_len = states[0].shape[1] \n \n \n \n \n \n seems to have a mistake.\n I might be wrong, but I think the code prev_len = states[0].shape[1] means to return the length of previous key/value matrix (or the previous input tokens) . However it returns the number of multi-head (12 for the 124M gpt2 model).\n <denchmark-h:h3>Error Message</denchmark-h>\n \n NA\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n run scripts/text_generation/sequence_sampling.py script and print the output of prev_len = states[0].shape[1] at line 258 in scripts/text_generation/model/gpt.py.\n <denchmark-h:h2>What have you tried to solve it?</denchmark-h>\n \n \n change to prev_len = states[0].shape[2]\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "carter54", "commentT": "2019-11-20T04:20:07Z", "comment_text": "\n \t\tThen the positional embedding is calculated for the wrong positions\n \n \n \n gluon-nlp/scripts/text_generation/model/gpt.py\n \n \n         Lines 262 to 265\n       in\n       5e11334\n \n \n \n \n \n \n  data_pos = mx.nd.arange(prev_len, prev_len + seq_len, ctx=data.context, dtype=np.float32) \n \n \n \n  data_pos = mx.nd.broadcast_axes(mx.nd.expand_dims(data_pos, axis=0), \n \n \n \n  axis=0, size=batch_size) \n \n \n \n  out = self._embed(data) + self._pos_embed(data_pos) \n \n \n \n \n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "carter54", "commentT": "2019-11-20T04:33:44Z", "comment_text": "\n \t\tAdded a fix to <denchmark-link:https://github.com/dmlc/gluon-nlp/pull/1010>#1010</denchmark-link>\n \n <denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n  please help to review.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "carter54", "commentT": "2019-11-20T04:47:40Z", "comment_text": "\n \t\tYes, it's a bug and it should be states[0].shape[2]\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "carter54", "commentT": "2019-11-20T04:47:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/carter54>@carter54</denchmark-link>\n  Really appreciate for pointing out this!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "carter54", "commentT": "2019-11-20T07:42:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n  happy to help! May I ask a question not related to this bug. Is there any method to accelerate the inference speed of gpt2 model? Can HybridBlock help? or quantization possibly?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "carter54", "commentT": "2019-11-20T07:45:08Z", "comment_text": "\n \t\tHybridization will definitely help. Quantization also helps but I\u2019m not sure how to do that in MXNet. Another solution is to dump the Json file and use TVM for inference.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "carter54", "commentT": "2019-11-20T08:17:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n  Thanks for your reply~ I read the TVM tutorial, most of the examples are within image processing area. Did you try TVM on gpt2 model before? Is there any examples and what range of the acceleration ratio can be expected? Thank you~\n \t\t"}}}, "commit": {"commit_id": "ebfc920026a2a8a5bbc1cc531eeba75e4e21bff7", "commit_author": "Leonard Lausen", "commitT": "2019-11-20 16:20:27+08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.5454545454545454"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytest.ini", "file_new_name": "pytest.ini", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15,16,17,18", "deleted_lines": "15"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "scripts\\tests\\test_scripts.py", "file_new_name": "scripts\\tests\\test_scripts.py", "file_complexity": {"file_NLOC": "289", "file_CCN": "32", "file_NToken": "2147"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "144,145,146,147,148,149,150", "deleted_lines": "144,145", "method_info": {"method_name": "test_sampling", "method_params": "method,lmmodel", "method_startline": "144", "method_endline": "159", "method_complexity": {"method_NLOC": "16", "method_CCN": "5", "method_NToken": "108", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "144,145,146,147,148,149,150", "deleted_lines": "144,145", "method_info": {"method_name": "test_sampling", "method_params": "method", "method_startline": "144", "method_endline": "154", "method_complexity": {"method_NLOC": "11", "method_CCN": "3", "method_NToken": "92", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "scripts\\text_generation\\model\\gpt.py", "file_new_name": "scripts\\text_generation\\model\\gpt.py", "file_complexity": {"file_NLOC": "367", "file_CCN": "18", "file_NToken": "2158"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "94,95,96,97,98,99,100,101,104,105,106,107,108,109,113,116,124,125,126,128,129,131,132,136", "deleted_lines": "91,92,93,97,100,101,102,103,104,105,109,112,120,121,122,124,125,127,128,132,133", "method_info": {"method_name": "forward", "method_params": "self,data,states", "method_startline": "91", "method_endline": "136", "method_complexity": {"method_NLOC": "38", "method_CCN": "4", "method_NToken": "587", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "90,94,95,96,97,98,99,100,101,104,105,106,107,108,109,113,116,124,125,126,128,129,131,132,136,137", "deleted_lines": "91,92,93,97,100,101,102,103,104,105,109,112,120,121,122,124,125,127,128,132,133", "method_info": {"method_name": "hybrid_forward", "method_params": "self,F,data,states", "method_startline": "90", "method_endline": "140", "method_complexity": {"method_NLOC": "39", "method_CCN": "4", "method_NToken": "636", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "scripts\\text_generation\\sequence_sampling.py", "file_new_name": "scripts\\text_generation\\sequence_sampling.py", "file_complexity": {"file_NLOC": "141", "file_CCN": "17", "file_NToken": "1208"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "173,174,175", "deleted_lines": null, "method_info": {"method_name": "generate", "method_params": "", "method_startline": "144", "method_endline": "186", "method_complexity": {"method_NLOC": "39", "method_CCN": "6", "method_NToken": "360", "method_nesting_level": "0"}}}}}}}}