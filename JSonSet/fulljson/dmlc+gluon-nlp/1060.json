{"BR": {"BR_id": "1060", "BR_author": "liuzh91", "BRopenT": "2019-12-18T06:53:18Z", "BRcloseT": "2020-01-03T08:19:03Z", "BR_text": {"BRsummary": "Cannot apply parameter sharing on WeightDropParameter", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n Parameter sharing of WeightDropParameter is broken. I apply weight sharing between AWDRNN and train.AWDRNN as followed:\n model = nlp.model.train.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,\n                                args.tied, args.dropout, args.weight_dropout,\n                                args.dropout_h, args.dropout_i, args.dropout_e)\n model_eval = nlp.model.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,\n                               args.tied, args.dropout, args.weight_dropout,\n                               args.dropout_h, args.dropout_i, args.dropout_e,\n                               params=model.collect_params())\n \n model.initialize(mx.init.Xavier(), ctx=context)\n \n model.hybridize(static_alloc=True)\n \n print(model)\n \n def check_initialized(net):\n     params = net.collect_params()\n     for param in params:\n         try:\n             params[param].list_ctx()\n         except RuntimeError:\n             return False\n     return True\n \n print(check_initialized(model))\n print(check_initialized(model_eval))\n <denchmark-h:h3>Log Message</denchmark-h>\n \n After I ran the above code, I got the following message:\n True\n False\n It appeared that model_eval is not properly initialized. After an inspection, we found it is the WeightDropParameter that not initialized.\n <denchmark-code>(Pdb) model_eval.collect_params()[\"awdrnn0_hybridsequential0_embedding0_weight\"].data()\n *** RuntimeError: Parameter 'awdrnn0_hybridsequential0_embedding0_weight' has not been initialized. Note that you should initialize parameters and create Trainer with Block.collect_params() instead of Block.params because the later does not include Parameters of nested child Blocks\n (Pdb) model_eval.collect_params()[\"awdrnn0_hybridsequential0_embedding0_weight\"]\n WeightDropParameter awdrnn0_hybridsequential0_embedding0_weight (shape=(33278, 400), dtype=float32, rate=0.1, mode=training)\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n My environment specs:\n <denchmark-code>curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python\n \n ---------Python Info----------\n Version      : 3.6.6\n Compiler     : GCC 7.2.0\n Build        : ('default', 'Jun 28 2018 17:14:51')\n Arch         : ('64bit', '')\n ------------Pip Info-----------\n Version      : 19.2.3\n Directory    : /home/ubuntu/anaconda3/lib/python3.6/site-packages/pip\n ----------MXNet Info-----------\n Version      : 1.6.0\n Directory    : /home/ubuntu/anaconda3/lib/python3.6/site-packages/mxnet\n Num GPUs     : 1\n Hashtag not found. Not installed from pre-built package.\n ----------System Info----------\n Platform     : Linux-4.15.0-1056-aws-x86_64-with-debian-buster-sid\n system       : Linux\n node         : ip-172-31-23-26\n release      : 4.15.0-1056-aws\n version      : #58-Ubuntu SMP Tue Nov 26 15:14:34 UTC 2019\n ----------Hardware Info----------\n machine      : x86_64\n processor    : x86_64\n Architecture:        x86_64\n CPU op-mode(s):      32-bit, 64-bit\n Byte Order:          Little Endian\n CPU(s):              8\n On-line CPU(s) list: 0-7\n Thread(s) per core:  2\n Core(s) per socket:  4\n Socket(s):           1\n NUMA node(s):        1\n Vendor ID:           GenuineIntel\n CPU family:          6\n Model:               79\n Model name:          Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n Stepping:            1\n CPU MHz:             2704.026\n CPU max MHz:         3000.0000\n CPU min MHz:         1200.0000\n BogoMIPS:            4600.12\n Hypervisor vendor:   Xen\n Virtualization type: full\n L1d cache:           32K\n L1i cache:           32K\n L2 cache:            256K\n L3 cache:            46080K\n NUMA node0 CPU(s):   0-7\n Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt\n ----------Network Test----------\n Setting timeout: 10\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "liuzh91", "commentT": "2019-12-18T06:56:42Z", "comment_text": "\n \t\tOne workaround to the above problem is to initialize the model parameters before applying weight sharing:\n <denchmark-code>model = nlp.model.train.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,\n                                args.tied, args.dropout, args.weight_dropout,\n                                args.dropout_h, args.dropout_i, args.dropout_e)\n model.initialize(mx.init.Xavier(), ctx=context)\n model_eval = nlp.model.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,\n                               args.tied, args.dropout, args.weight_dropout,\n                               args.dropout_h, args.dropout_i, args.dropout_e,\n                               params=model.collect_params())\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "liuzh91", "commentT": "2019-12-18T06:57:38Z", "comment_text": "\n \t\tI suggest we drop the WeightDropParameter. Instead users can manually call Dropout in forward.\n Understanding WeightDropParameter requires intricate familiarity with the way Gluon handles Parameters and Blocks, and most users will have a hard time to understand it. However, our goal is to have easy to understand code. Thus in it's current implementation, WeightDropParameter doesn't align with our goals.\n Further, as pointed out by <denchmark-link:https://github.com/liuzh91>@liuzh91</denchmark-link>\n , the implementation is currently broken.\n What do others think?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "liuzh91", "commentT": "2019-12-20T07:06:00Z", "comment_text": "\n \t\tThe above workaround does not work if we have args.tied turned on. In this case, after I ran the code check_initialized(model_eval), I got the following error message:\n <denchmark-code>*** RuntimeError: Parameter 'awdrnn0_hybridsequential0_embedding0_bias' has not been initialized\n </denchmark-code>\n \n It is exactly the output layer of the model_evalthat not been initialized. model_eval contains the following parameters:\n <denchmark-code>model_eval.collect_params()\n awdrnn0_ (\n   WeightDropParameter awdrnn0_hybridsequential0_embedding0_weight (shape=(33278, 400), dtype=float32, rate=0.1, mode=training)\n   Parameter awdrnn0_hybridsequential1_lstm0_l0_i2h_weight (shape=(4600, 400), dtype=float32)\n   WeightDropParameter awdrnn0_hybridsequential1_lstm0_l0_h2h_weight (shape=(4600, 1150), dtype=float32, rate=0.5, mode=training)\n   Parameter awdrnn0_hybridsequential1_lstm0_l0_i2h_bias (shape=(4600,), dtype=float32)\n   Parameter awdrnn0_hybridsequential1_lstm0_l0_h2h_bias (shape=(4600,), dtype=float32)\n   Parameter awdrnn0_hybridsequential1_lstm1_l0_i2h_weight (shape=(4600, 1150), dtype=float32)\n   WeightDropParameter awdrnn0_hybridsequential1_lstm1_l0_h2h_weight (shape=(4600, 1150), dtype=float32, rate=0.5, mode=training)\n   Parameter awdrnn0_hybridsequential1_lstm1_l0_i2h_bias (shape=(4600,), dtype=float32)\n   Parameter awdrnn0_hybridsequential1_lstm1_l0_h2h_bias (shape=(4600,), dtype=float32)\n   Parameter awdrnn0_hybridsequential1_lstm2_l0_i2h_weight (shape=(1600, 1150), dtype=float32)\n   WeightDropParameter awdrnn0_hybridsequential1_lstm2_l0_h2h_weight (shape=(1600, 400), dtype=float32, rate=0.5, mode=training)\n   Parameter awdrnn0_hybridsequential1_lstm2_l0_i2h_bias (shape=(1600,), dtype=float32)\n   Parameter awdrnn0_hybridsequential1_lstm2_l0_h2h_bias (shape=(1600,), dtype=float32)\n   Parameter awdrnn0_hybridsequential0_embedding0_bias (shape=(33278,), dtype=float32)\n )\n </denchmark-code>\n \n WeightDropParameter awdrnn0_hybridsequential0_embedding0_weight and  Parameter awdrnn0_hybridsequential0_embedding0_bias have their weights tied up.  WeightDropParameter awdrnn0_hybridsequential0_embedding0_weight is properly initialized whereas Parameter awdrnn0_hybridsequential0_embedding0_bias is not initialized. I guess it is WeightDropParameter that causes the initialization error.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "liuzh91", "commentT": "2019-12-24T10:36:37Z", "comment_text": "\n \t\tIf I applied weight dropout during the forward pass:\n embedding_param = self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight']\n embedding_param.set_data(mx.nd.Dropout(embedding_param.data(), 0.3))\n I got the following error msg:\n <denchmark-code>*** mxnet.base.MXNetError: [10:20:43] src/imperative/imperative.cc:203: Check failed: AGInfo::IsNone(*output): Assigning to NDArrays that are already in a computational graph will cause undefined behavior when evaluating gradients. Please call backward first to clear the graph or do this out side of a record section. Also note that you cannot use inplace operations like +=, *=, relu(x, out=x), y[idx]=x, etc inside a record section.\n </denchmark-code>\n \n Note that my code is wrapped in the autograd.record().  The error suggests I apply the weight dropout outside the record section, but it is impractical in this case. Is there any workaround for this error?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "liuzh91", "commentT": "2019-12-24T16:23:19Z", "comment_text": "\n \t\tYes, this will not work indeed.\n You need to do something like\n def forward(self, ...):\n     ....\n     embedding_param = self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight'].data(CTX)\n     embedding_param = mx.nd.Dropout(embedding_param, 0.3)\n     ....\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "liuzh91", "commentT": "2019-12-25T04:03:09Z", "comment_text": "\n \t\t\n Yes, this will not work indeed.\n You need to do something like\n def forward(self, ...):\n     ....\n     embedding_param = self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight'].data(CTX)\n     embedding_param = mx.nd.Dropout(embedding_param, 0.3)\n     ....\n \n It should use slice assign embedding_param[:] instead of embedding_param.  Otherwise, the weight self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight'] does not change at all. Concretely, I implement the following code:\n     embedding_param = self.collect_params(['awdrnn0_hybridsequential0_embedding0_weight'].data(CTX)\n     embedding_param[:] = mx.nd.Dropout(embedding_param, 0.3)\n But if I do it in this way, I still get the same MXNetError:\n <denchmark-code>*** mxnet.base.MXNetError: [03:55:51] src/imperative/imperative.cc:203: Check failed: AGInfo::IsNone(*output): Assigning to NDArrays that are already in a computational graph will cause undefined behavior when evaluating gradients. Please call backward first to clear the graph or do this out side of a record section. Also note that you cannot use inplace operations like +=, *=, relu(x, out=x), y[idx]=x, etc inside a record section.\n </denchmark-code>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "liuzh91", "commentT": "2019-12-25T06:47:35Z", "comment_text": "\n \t\t\n I suggest we drop the WeightDropParameter. Instead users can manually call Dropout in forward.\n Understanding WeightDropParameter requires intricate familiarity with the way Gluon handles Parameters and Blocks, and most users will have a hard time to understand it. However, our goal is to have easy to understand code. Thus in it's current implementation, WeightDropParameter doesn't align with our goals.\n Further, as pointed out by @liuzh91, the implementation is currently broken.\n What do others think?\n \n It seems we cannot manually assign weights during the record session. I found a bunch of similar  assignment errors from the internet, e.g., <denchmark-link:https://discuss.mxnet.io/t/autograd-record-error-about-assigning-to-ndarray/322>https://discuss.mxnet.io/t/autograd-record-error-about-assigning-to-ndarray/322</denchmark-link>\n .  <denchmark-link:https://github.com/apache/incubator-mxnet/issues/9989>apache/incubator-mxnet#9989</denchmark-link>\n \n Alternatively, I have tried weight assignment outside the record session suggested by the error msg. However, mx.nd.Dropout deferred the evaluation until entering the with autograd.record() scope.\n Perhaps we still need to switch back to the apply_weight_drop() function and fix the shared parameters problem there. In apply_weight_drop(), it technically replaces every param matching local_param_regex with a new dropped_param even the param is an instance of WeightDropParameter. If my understanding is correct, assume that the param is already an instance of WeightDropParameter by weight sharing, we do not need to create a new WeightDropParameter dropped_param and replace every occurrence of the param. The following assignment is therefore unnecessary:\n \n \n \n gluon-nlp/src/gluonnlp/model/utils.py\n \n \n          Line 92\n       in\n       434187e\n \n \n \n \n \n \n  dropped_param = WeightDropParameter(param, rate, weight_dropout_mode, axes) \n \n \n \n \n \n It can be replaced by\n dropped_param = param\n [UPDATE] If I add a check before creating a new dropped_param in L92:\n if isinstance(param, WeightDropParameter):\n     continue\n The initialization works if I don't have args.tied turned on. If args.tied turned on, the tied weight parameters  Parameter awdrnn0_hybridsequential0_embedding0_bias still suffer the initialization problem. The bug is triggered by the following code:\n     with output.name_scope():\n             if self._tie_weights:\n                 output.add(nn.Dense(self._vocab_size, flatten=False,\n                                     params=self.embedding[0].params))\n             else:\n                 output.add(nn.Dense(self._vocab_size, flatten=False))\n [UPDATE]2:\n After some investigation, I found the following debugging info:\n <denchmark-code>(Pdb) model_eval.decoder[0]._params._shared.list_ctx()\n [cpu(0)]\n (Pdb) model_eval.decoder[0]._params.list_ctx()\n *** RuntimeError: Parameter 'awdrnn0_hybridsequential0_embedding0_bias' has not been initialized\n </denchmark-code>\n \n I believe params should always have the same context with the params._shared. There is something messed up here.\n [UPDATE]3:\n I am confused with the weight tied here. The two tied layers has different initialized values:\n <denchmark-code>(Pdb) model.collect_params()['awdrnn0_hybridsequential0_embedding0_weight']._data\n [\n [[ 0.0097627   0.01856892  0.04303787 ... -0.08871634 -0.01311667\n   -0.00243246]\n  [-0.03764082  0.07620091  0.03926869 ...  0.00636984 -0.06295353\n    0.06907154]\n  [-0.0197481   0.00725491  0.08585829 ... -0.09896208  0.09590539\n    0.09404436]\n  ...\n  [-0.05431841  0.02288689  0.05555419 ...  0.05949707  0.00848633\n   -0.0083269 ]\n  [ 0.04172196  0.00805981 -0.03484908 ... -0.08370983  0.04527131\n    0.05345441]\n  [ 0.07405332  0.05066857  0.02247771 ... -0.00969069  0.03976088\n    0.00947104]]\n <NDArray 33278x400 @cpu(0)>]\n (Pdb) model.collect_params()['awdrnn0_hybridsequential0_embedding0_bias']._data\n [\n [0. 0. 0. ... 0. 0. 0.]\n <NDArray 33278 @cpu(0)>]\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "liuzh91", "commentT": "2019-12-26T11:46:01Z", "comment_text": "\n \t\tWhy do you want to assign the weight? You can access the non-weight-dropped weight and apply dropout to the sliced part that you are interested. It's not necessary nor possible to modify the actual parameter via assignment in the forward pass.\n The code I posted above, which doesn't do any in-place modification of the weight is correct. You just need to keep the variable around so that you use the same dropped out version at any location in the forward pass.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "liuzh91", "commentT": "2019-12-26T14:41:10Z", "comment_text": "\n \t\t\n Why do you want to assign the weight? You can access the non-weight-dropped weight and apply dropout to the sliced part that you are interested. It's not necessary nor possible to modify the actual parameter via assignment in the forward pass.\n The code I posted above, which doesn't do any in-place modification of the weight is correct. You just need to keep the variable around so that you use the same dropped out version at any location in the forward pass.\n \n The key problem is that how to do gradient backpropogation if the weight is not updated? If some weights are dropped in the forward pass, they should never be updated during current iteration. I really doubt whether the gradient computation will still work correctly with your hack.\n Also, keeping network parameters in a temporary variable is also not memory efficient.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "liuzh91", "commentT": "2019-12-27T02:29:42Z", "comment_text": "\n \t\tI think we can add the weight drop flag to the Dense/RNN/Embedding layers. In that case, we can apply dropout inside hybrid_forward:\n <denchmark-link:https://github.com/apache/incubator-mxnet/blob/2ad3ce408b08456bff0b92a82aa3c484adde29f2/python/mxnet/gluon/nn/basic_layers.py#L222-L228>https://github.com/apache/incubator-mxnet/blob/2ad3ce408b08456bff0b92a82aa3c484adde29f2/python/mxnet/gluon/nn/basic_layers.py#L222-L228</denchmark-link>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "liuzh91", "commentT": "2019-12-27T03:54:12Z", "comment_text": "\n \t\tI finally figured out there is a mxnet weight sharing bug. When weight sharing is used with tied weight, something unexpected will occur. For this piece of code:\n model = nlp.model.train.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,\n                                args.tied, args.dropout, args.weight_dropout,\n                                args.dropout_h, args.dropout_i, args.dropout_e)\n model_eval = nlp.model.AWDRNN(args.model, len(vocab), args.emsize, args.nhid, args.nlayers,\n                               args.tied, args.dropout, args.weight_dropout,\n                               args.dropout_h, args.dropout_i, args.dropout_e,\n                               params=model.collect_params())\n \n model.initialize(mx.init.Xavier(), ctx=context)\n \n model.hybridize(static_alloc=True)\n Even I comment out all the apply_weight_drop in the code, the initialization error still occurred if args.tied is turned on. I print out some pdb info here:\n <denchmark-code>(Pdb) model_eval.decoder[0]._params\n awdrnn0_hybridsequential0_embedding0_ (\n   Parameter awdrnn0_hybridsequential0_embedding0_weight (shape=(33278, 400), dtype=float32)\n   Parameter awdrnn0_hybridsequential0_embedding0_bias (shape=(33278,), dtype=float32)\n )\n (Pdb) model_eval.decoder[0]._params.list_ctx()\n *** RuntimeError: Parameter 'awdrnn0_hybridsequential0_embedding0_bias' has not been initialized\n (Pdb) model_eval.decoder[0]._params._shared\n awdrnn0_hybridsequential0_embedding0_ (\n   Parameter awdrnn0_hybridsequential0_embedding0_weight (shape=(33278, 400), dtype=float32)\n )\n (Pdb) model_eval.decoder[0]._params._shared.list_ctx()\n [cpu(0)]\n </denchmark-code>\n \n The conclusion is: no matter whether we use weight drop or not, weight tied and weight sharing cannot be used simultaneously. Otherwise, there will be an initialization error. I will submit a separate issue for this in MXNET.\n For the current weight drop bug, I think my fix is more simple and elegant. I will submit my PR very soon.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "liuzh91", "commentT": "2019-12-27T04:10:58Z", "comment_text": "\n \t\t\n I think we can add the weight drop flag to the Dense/RNN/Embedding layers. In that case, we can apply dropout inside hybrid_forward:\n https://github.com/apache/incubator-mxnet/blob/2ad3ce408b08456bff0b92a82aa3c484adde29f2/python/mxnet/gluon/nn/basic_layers.py#L222-L228\n \n It is similar to <denchmark-link:https://github.com/leezu>@leezu</denchmark-link>\n  's proposal.  My point is that using weight drop in / will lead to some unexpected gradient computation error. So I  suggest we stick to the existing implementation of .\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "liuzh91", "commentT": "2019-12-27T04:15:39Z", "comment_text": "\n \t\tLet me try that. It should be fine (otherwise we will need to fix it in the MXNet side). The autograd error you\u2019ve met is due to the fact that the AG engine in MXNet does not support assignment operations.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "liuzh91", "commentT": "2019-12-27T12:47:25Z", "comment_text": "\n \t\t\n The key problem is that how to do gradient backpropogation if the weight is not updated? If some weights are dropped in the forward pass, they should never be updated during current iteration. I really doubt whether the gradient computation will still work correctly with your hack.\n \n It's the same as Dropout. The gradient will be 0 for the dropped out parts. The proposal above is correct.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "liuzh91", "commentT": "2019-12-27T13:35:54Z", "comment_text": "\n \t\t\n It's the same as Dropout. The gradient will be 0 for the dropped out parts. The proposal above is correct.\n \n I will check if gradient computation works correctly or not.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "liuzh91", "commentT": "2019-12-27T16:25:42Z", "comment_text": "\n \t\tOk, thank you. For your convenience, let me give a more complete code regarding above example:\n If you had before\n def forward(self, x):\n     ....\n     embeddings = self.embedding(x)  # self.embedding is a gluon.nn.Embedding\n     ....\n then for weight dropout you need to\n def forward(self, x):\n     ....\n     embedding_param = self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight'].data(CTX)\n     dropped_embedding = mx.nd.Dropout(embedding_param, 0.3)\n     word_embedding = mx.nd.Embedding(x, dropped_embedding)\n     ....\n Further, you can optimize the computation by only performing dropout on the part of the weight that you are interested in:\n def forward(self, x):\n     ....\n     embedding_param = self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight'].data(CTX)\n     word_embedding = mx.nd.Embedding(x, embedding_param)\n     dropped_embedding = mx.nd.Dropout(word_embedding, 0.3)\n     ....\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "liuzh91", "commentT": "2019-12-30T06:55:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/leezu>@leezu</denchmark-link>\n   Thanks for the example. I have a few concerns about the your code.\n def forward(self, x):\n     ....\n     embedding_param = self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight'].data(CTX)\n     word_embedding = mx.nd.Embedding(x, embedding_param)\n     dropped_embedding = mx.nd.Dropout(word_embedding, 0.3)\n     ....\n I believe this piece of code is to apply dropout on the word embedding instead of embedding weights.\n original embedding architecture looks like:\n embedding = nn.HybridSequential()\n with embedding.name_scope():\n     embedding_block = nn.Embedding(self._vocab_size, self._embed_size,\n                                    weight_initializer=init.Uniform(0.1))\n     if self._drop_e:\n         apply_weight_drop(embedding_block, 'weight', self._drop_e, axes=(1,))\n     embedding.add(embedding_block)\n     if self._drop_i:\n         embedding.add(nn.Dropout(self._drop_i, axes=(0,)))\n return embedding\n So I think your implementation should be like\n def forward(self, x):\n     ....\n     embedding_param = self.collect_params()['awdrnn0_hybridsequential0_embedding0_weight'].data(CTX)\n     if self._drop_e:\n         embedding_param = mx.nd.Dropout(embedding_param, self._drop_e)\n     word_embedding = mx.nd.Embedding(x, embedding_param)\n     if self._drop_i:\n         word_embedding = mx.nd.Dropout(word_embedding, self._drop_i)\n     ....\n Each time if there is a dropout operation, do you have to reimplement the block structure in the forward function? It looks really impractical. what about weight drop on rnn cells? \n \n \n gluon-nlp/src/gluonnlp/model/utils.py\n \n \n          Line 240\n       in\n       434187e\n \n \n \n \n \n \n  apply_weight_drop(rnn_cell, 'h2h_weight', rate=weight_dropout) \n \n \n \n \n \n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "liuzh91", "commentT": "2019-12-30T09:51:01Z", "comment_text": "\n \t\tYes, above example for applying dropout sparsely was not correct. You would need the np.unique with return_inverse=True to make sure the same dropout mask is applied to repeated words.\n But in fact, this kind of optimization is not crucial (it is currently not done either).\n You're right about rnn cells. One way to avoid re-implementing the rnn cell is to add weight_drop support to the cells upstream in mxnet as <denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n  suggested (<denchmark-link:https://github.com/dmlc/gluon-nlp/issues/1060#issuecomment-569169629>#1060 (comment)</denchmark-link>\n ).\n So for now it may be simplest to keep apply_weight_drop around for the RNN use-case? When redesigning parameter handling in MXNet 2 we can revisit the design of apply_weight_drop and simplify the weight drop API / implementation.\n \t\t"}}}, "commit": {"commit_id": "757396630ca404415c039c65d092a4f5aa0470d1", "commit_author": "liuzh91", "commitT": "2019-12-31 09:40:21+00:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\gluonnlp\\model\\utils.py", "file_new_name": "src\\gluonnlp\\model\\utils.py", "file_complexity": {"file_NLOC": "207", "file_CCN": "30", "file_NToken": "1073"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "90,91", "deleted_lines": null}}}}}}