{"BR": {"BR_id": "1102", "BR_author": "liuzh91", "BRopenT": "2020-01-08T07:03:27Z", "BRcloseT": "2020-01-10T02:46:03Z", "BR_text": {"BRsummary": "Log average loss metric of training GNMT", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n Current log_avg_loss in the train_gnmt.py script is incorrect.\n with mx.autograd.record():\n         out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n         loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean()\n         loss = loss * (tgt_seq.shape[1] - 1) / (tgt_valid_length - 1).mean()\n         loss.backward()\n step_loss = loss.asscalar()\n log_avg_loss += step_loss\n log_avg_gnorm += gnorm\n log_avg_loss sums up averaged losses from different batches. It is divided by args.log_interval to compute the loss during the interval. It is inconsistent with the loss metric used in train_transformer script. In the latter script, losses and valid lengths during the interval are summed up separately. The average loss is computed from interval_loss/interval_valid_length.\n Although these two computations are similar, I think it will be better to unify them in training scripts.\n <denchmark-h:h2>Log Message</denchmark-h>\n \n I print out the log message here:\n <denchmark-code>2020-01-08 07:00:24,259 - root - [Epoch 0 Batch 100/1043] loss=6.2948, ppl=541.7368, gnorm=0.7030, throughput=36.26K wps, wc=606.57K\n 2020-01-08 07:00:37,441 - root - [Epoch 0 Batch 200/1043] loss=5.7232, ppl=305.8789, gnorm=0.3394, throughput=44.32K wps, wc=584.18K\n 2020-01-08 07:00:50,411 - root - [Epoch 0 Batch 300/1043] loss=5.2613, ppl=192.7247, gnorm=0.3442, throughput=45.71K wps, wc=592.78K\n 2020-01-08 07:01:03,364 - root - [Epoch 0 Batch 400/1043] loss=4.9213, ppl=137.1854, gnorm=0.3193, throughput=44.10K wps, wc=571.26K\n 2020-01-08 07:01:16,174 - root - [Epoch 0 Batch 500/1043] loss=4.6531, ppl=104.9109, gnorm=0.3304, throughput=43.29K wps, wc=554.48K\n 2020-01-08 07:01:28,367 - root - [Epoch 0 Batch 600/1043] loss=4.4185, ppl=82.9729, gnorm=0.3167, throughput=44.80K wps, wc=546.26K\n 2020-01-08 07:01:41,019 - root - [Epoch 0 Batch 700/1043] loss=4.3236, ppl=75.4571, gnorm=0.3128, throughput=44.82K wps, wc=566.96K\n 2020-01-08 07:01:53,287 - root - [Epoch 0 Batch 800/1043] loss=4.2010, ppl=66.7544, gnorm=0.3120, throughput=44.95K wps, wc=551.40K\n 2020-01-08 07:02:06,260 - root - [Epoch 0 Batch 900/1043] loss=4.0903, ppl=59.7581, gnorm=0.3177, throughput=44.53K wps, wc=577.61K\n 2020-01-08 07:02:19,849 - root - [Epoch 0 Batch 1000/1043] loss=4.0345, ppl=56.5169, gnorm=0.3055, throughput=43.28K wps, wc=588.16K\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "liuzh91", "commentT": "2020-01-08T21:27:01Z", "comment_text": "\n \t\tThey are different because train_transformer uses multi-gpu training. Where is interval_valid_length? I think there is no such variable.\n \t\t"}}}, "commit": {"commit_id": "a528e747f5f6bbbfa9f5cb5f8142da462a738746", "commit_author": "liuzh91", "commitT": "2020-01-08 21:27:12-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "scripts\\machine_translation\\train_gnmt.py", "file_new_name": "scripts\\machine_translation\\train_gnmt.py", "file_complexity": {"file_NLOC": "229", "file_CCN": "14", "file_NToken": "2138"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "202,203,217,218,219,220,227,228,236,237,241,242", "deleted_lines": "202,216,223,224,232,233,237", "method_info": {"method_name": "train", "method_params": "", "method_startline": "193", "method_endline": "281", "method_complexity": {"method_NLOC": "85", "method_CCN": "8", "method_NToken": "821", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "173,176", "deleted_lines": "173,176", "method_info": {"method_name": "evaluate", "method_params": "data_loader", "method_startline": "147", "method_endline": "190", "method_complexity": {"method_NLOC": "29", "method_CCN": "6", "method_NToken": "305", "method_nesting_level": "0"}}}}}}}}