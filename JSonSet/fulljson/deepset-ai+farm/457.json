{"BR": {"BR_id": "457", "BR_author": "ftesser", "BRopenT": "2020-07-13T20:54:29Z", "BRcloseT": "2020-09-14T16:29:26Z", "BR_text": {"BRsummary": "Data Handler dataset creation fails if features are not computed for some baskets", "BRdescription": "\n Describe the bug\n method farm.data_handler.processor.Processor._create_dataset fails if features are not computed for some basket.\n The specific case of the error message below is the training of SQuAD question answering, the context of the issue is the following:\n \n the input SQuAD data contains some \"errors\" (some misalignment between answer text and selected text in the context);\n this misalignment cause the exclusion of the features computation the _featurize_samples method (see the try-except):  \n \n \n FARM/farm/data_handler/processor.py\n \n \n          Line 295\n       in\n       b8b59c4\n \n \n \n \n \n \n  def _featurize_samples(self): \n \n \n \n \n \n finally in _create_dataset when the basket has no feature  features_flat.extend \n \n \n FARM/farm/data_handler/processor.py\n \n \n          Line 308\n       in\n       b8b59c4\n \n \n \n \n \n \n  features_flat.extend(sample.features) \n \n \n \n \n \n gives the error message reported below.\n \n Error message\n <denchmark-code>multiprocessing.pool.RemoteTraceback: \n \"\"\"\n Traceback (most recent call last):\n   File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n     result = (True, func(*args, **kwds))\n   File \"/home/fabio/src/git_repositories/FARM/farm/data_handler/data_silo.py\", line 124, in _dataset_from_chunk\n     dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n   File \"/home/fabio/src/git_repositories/FARM/farm/data_handler/processor.py\", line 1144, in dataset_from_dicts\n     dataset, tensor_names = self._create_dataset(keep_baskets=False)\n   File \"/home/fabio/src/git_repositories/FARM/farm/data_handler/processor.py\", line 308, in _create_dataset\n     features_flat.extend(sample.features)\n TypeError: 'NoneType' object is not iterable\n \"\"\"\n \n The above exception was the direct cause of the following exception:\n \n Traceback (most recent call last):\n   File \"/home/fabio/src/git_repositories/DocumentFeaturesIdentificator/bin/farm_qa\", line 10, in <module>\n     farm_qa.main()\n   File \"/home/fabio/src/git_repositories/DocumentFeaturesIdentificator/bin/../documentfeaturesidentificator/farm_utils/farm_qa.py\", line 211, in main\n     options.func(options)\n   File \"/home/fabio/src/git_repositories/DocumentFeaturesIdentificator/bin/../documentfeaturesidentificator/farm_utils/farm_qa.py\", line 107, in train_general_purpose\n     train_mock(options, base_lm_model, out_model_basename, train_filename, dev_filename, test_filename)\n   File \"/home/fabio/src/git_repositories/DocumentFeaturesIdentificator/bin/../documentfeaturesidentificator/farm_utils/farm_qa.py\", line 146, in train_mock\n     question_answering(options_mock)\n   File \"/home/fabio/src/git_repositories/DocumentFeaturesIdentificator/bin/../documentfeaturesidentificator/farm_utils/question_answering.py\", line 79, in question_answering\n     data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False)\n   File \"/home/fabio/src/git_repositories/FARM/farm/data_handler/data_silo.py\", line 105, in __init__\n     self._load_data()\n   File \"/home/fabio/src/git_repositories/FARM/farm/data_handler/data_silo.py\", line 207, in _load_data\n     self.data[\"train\"], self.tensor_names = self._get_dataset(train_file)\n   File \"/home/fabio/src/git_repositories/FARM/farm/data_handler/data_silo.py\", line 176, in _get_dataset\n     for dataset, tensor_names in results:\n   File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 865, in next\n     raise value\n TypeError: 'NoneType' object is not iterable\n </denchmark-code>\n \n Expected behavior\n Logging the errors (perhaps with more information) and automatically exclude the basket with errors from the dataset.\n Additional context\n Add any other context about the problem here, like type of downstream task, part of  etc..\n To Reproduce\n Manually corrupt squad data test (e.g.: add a character in an answer in train-v2.0.json) and then run examples/question_answering.py\n System:\n \n OS: Linux\n GPU/CPU: CPU\n FARM version: 0.4.6\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ftesser", "commentT": "2020-07-13T21:01:59Z", "comment_text": "\n \t\tIn order to automatically remove the baskets without features from the dataset I have two proposals: but I am not sure which one is better (or if they are OK with the design of the data pipeline):\n \n remove the baskets without features directly on _featurize_samples;\n remove the baskets without features on _create_dataset.\n \n If you think one of these two approaches  can be ok I can make a PR.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ftesser", "commentT": "2020-07-14T09:17:44Z", "comment_text": "\n \t\tThank you for pointing this out and working on this! Yes I think removing baskets without features is the right move here. I would just opt for 2. since preprocessing could fail at _init_samples_in_baskets() or _featurize_samples(). Removing baskets at _create_dataset() would be able to deal with both cases\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ftesser", "commentT": "2020-07-14T09:39:17Z", "comment_text": "\n \t\tIssue seems related to <denchmark-link:https://github.com/deepset-ai/FARM/issues/454>#454</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ftesser", "commentT": "2020-07-14T13:39:23Z", "comment_text": "\n \t\tAh OK, if this is related to this other issue probably I have not the necessary global overview of the project to fix both (perhaps fixing <denchmark-link:https://github.com/deepset-ai/FARM/issues/454>#454</denchmark-link>\n  will result in the fix of this issue as well?)\n Anyway, if you think should be helpful, I can make a PR for my original solution 2.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ftesser", "commentT": "2020-07-14T13:53:53Z", "comment_text": "\n \t\tLets move forward step by step.\n I think the issue you raised is more localized and easier to tackle. So we should proceed with your PR first.\n Maybe it's even the other way around and <denchmark-link:https://github.com/deepset-ai/FARM/issues/454>#454</denchmark-link>\n  is solved by your PR ;)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ftesser", "commentT": "2020-07-14T14:35:01Z", "comment_text": "\n \t\tOk <denchmark-link:https://github.com/Timoeller>@Timoeller</denchmark-link>\n  :)\n <denchmark-link:https://github.com/brandenchan>@brandenchan</denchmark-link>\n  this last PR is removing baskets at _create_dataset().\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ftesser", "commentT": "2020-07-14T16:14:13Z", "comment_text": "\n \t\tHi all, with this last PR the train of the \"misaligned train squad file\" ends successfully.\n However if I try to predict a \"misaligned test squad file\" with the model:\n model = QAInferencer.load(save_dir, task_type='question_answering')\n result = model.inference_from_file(file=test_filename)\n the misaligned basket are removed also in case (in the inference case this could be avoided as we just use the context and the questions but this is acceptable as I suppose the same squad data processor is used for train and inference):\n <denchmark-code>07/14/2020 17:48:14 - ERROR - farm.data_handler.processor -   Basket id: id_internal: 1999-0, id_external: 572881704b864d1900164a50\n 07/14/2020 17:48:14 - ERROR - farm.data_handler.processor -   Error message: Answer using start/end indices is 'Quattro' while gold label text is 'quattro'\n 07/14/2020 17:48:14 - WARNING - farm.data_handler.processor -   Removing the following baskets because of errors in computing features:\n 07/14/2020 17:48:14 - WARNING - farm.data_handler.processor -   Basket id: id_internal: 1999-0, id_external: 572881704b864d1900164a50\n </denchmark-code>\n \n but then I obtained the following error:\n <denchmark-code>Traceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/infer.py\", line 645, in inference_from_file\n     multiprocessing_chunksize=multiprocessing_chunksize, streaming=streaming)\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/infer.py\", line 345, in inference_from_file\n     streaming=streaming,\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/infer.py\", line 637, in inference_from_dicts\n     multiprocessing_chunksize=multiprocessing_chunksize, streaming=streaming)\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/infer.py\", line 418, in inference_from_dicts\n     return list(predictions)\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/infer.py\", line 489, in _inference_with_multiprocessing\n     dataset, tensor_names, baskets\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/infer.py\", line 600, in _get_predictions_and_aggregate\n     baskets=baskets)\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/modeling/adaptive_model.py\", line 104, in formatted_preds\n     preds = head.formatted_preds(logits=logits_for_head, **kwargs)\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/modeling/prediction_head.py\", line 1218, in formatted_preds\n     preds_d = self.aggregate_preds(preds, passage_start_t, ids, seq_2_start_t)\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/modeling/prediction_head.py\", line 1310, in aggregate_preds\n     pred_d = self.pred_to_doc_idxs(preds[sample_idx], curr_passage_start_t)\n   File \"/home/test/.local/lib/python3.7/site-packages/farm/modeling/prediction_head.py\", line 1447, in pred_to_doc_idxs\n     assert start >= 0\n AssertionError\n </denchmark-code>\n \n Perhaps some misalignment between baskets (some baskets has been removed) in the aggregations phase?\n PS: if I try to predict a \"correct squad file\" with the same model I do not obtain the error.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ftesser", "commentT": "2020-07-20T09:31:14Z", "comment_text": "\n \t\tHmmm this is something to do with the calculation of document indexes. Is there any chance you can send us the input data that you're working with so that we can try to replicate this on our side?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ftesser", "commentT": "2020-07-22T11:07:03Z", "comment_text": "\n \t\tHello <denchmark-link:https://github.com/brandenchan>@brandenchan</denchmark-link>\n , I have followed your suggestion for this last PR.\n I tested the new code with my \"bugged data\" and now, I do not obtain any errors in both train and inference.\n So probably there was some misalignment between baskets.\n Regarding the data, if you like to investigate more, I can send you some examples that failed before this PR.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "ftesser", "commentT": "2020-07-22T12:30:26Z", "comment_text": "\n \t\tOk great! I think that <denchmark-link:https://github.com/deepset-ai/FARM/pull/471>#471</denchmark-link>\n  is certainly an improvement on the error handling front. I also think misalignment between baskets is a likely reason why you were getting the index error so I don't think its necessary to investigate that right now. Thanks for your work on this!\n \t\t"}}}, "commit": {"commit_id": "65932d94628ae5e05fc119225da62e2dd0e79566", "commit_author": "ftesser", "commitT": "2020-07-22 14:31:28+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "farm\\data_handler\\processor.py", "file_new_name": "farm\\data_handler\\processor.py", "file_complexity": {"file_NLOC": "1209", "file_CCN": "198", "file_NToken": "6966"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "305,306,307,308,309,310,311,312,313,314,315,316,317,318,319", "deleted_lines": "307,308", "method_info": {"method_name": "_check_sample_features", "method_params": "basket", "method_startline": "305", "method_endline": "319", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "26", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "324,326,327,328,329,330,331,332,333,334,335,336,337,338", "deleted_lines": null, "method_info": {"method_name": "_create_dataset", "method_params": "self,keep_baskets", "method_startline": "322", "method_endline": "343", "method_complexity": {"method_NLOC": "18", "method_CCN": "7", "method_NToken": "113", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "302", "deleted_lines": null, "method_info": {"method_name": "_featurize_samples", "method_params": "self", "method_startline": "295", "method_endline": "303", "method_complexity": {"method_NLOC": "9", "method_CCN": "4", "method_NToken": "59", "method_nesting_level": "1"}}}}}}}}