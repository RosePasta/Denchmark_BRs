{"BR": {"BR_id": "519", "BR_author": "PhilipMay", "BRopenT": "2020-09-04T16:43:36Z", "BRcloseT": "2020-10-21T15:15:04Z", "BR_text": {"BRsummary": "Getting different predictions on different runs with same ELECTRA model.", "BRdescription": "\n I trained an Electra Model on text classification. 3 Classes.\n Saved it with\n <denchmark-code>    save_dir = Path(\"saved_models/bert-german-doc-tutorial\")\n     model.save(save_dir)\n     processor.save(save_dir)\n </denchmark-code>\n \n Now I load Data I want to predict on and getting different results:\n <denchmark-code>1st run:\n labal_n          5055\n label_i           855\n label_e          8\n \n 2nd run\n labal_n          4609\n label_e          990\n label_i           319\n \n 3rd run\n label_i        3355\n labal_n       2510\n label_e       53\n </denchmark-code>\n \n How on earth can that be? Even if the model would be a total crap the prediction should be deterministic - right?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "PhilipMay", "commentT": "2020-09-04T16:55:13Z", "comment_text": "\n \t\tHow do you load it? Its Friday evening, this might also affect the preds or at least the code ;)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "PhilipMay", "commentT": "2020-09-04T18:47:33Z", "comment_text": "\n \t\tWell - I did use torch 1.6.0. Might that be the reason?\n And transformers 3.1.0\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "PhilipMay", "commentT": "2020-09-04T19:12:29Z", "comment_text": "\n \t\tIs it possible that there is a bug in transformers that does not let me load saved delectra models from local disk?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "PhilipMay", "commentT": "2020-09-05T08:00:01Z", "comment_text": "\n \t\t\n How do you load it?\n \n <denchmark-code>    model = Inferencer.load(model_path,\n                         num_processes=4,\n                         batch_size=batch_size,\n                         gpu=True,\n                         )\n     result = model.inference_from_dicts(dicts=unlabeled_text_dict, multiprocessing_chunksize=4000)\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "PhilipMay", "commentT": "2020-09-05T08:03:33Z", "comment_text": "\n \t\tI tested it with a normal BERT model.\n There I get 100% stable results.\n Only with electra I get unstable things...\n Loading / saving electra models with FARM / transformers has a bug is my current hypothesis.\n Maybe some layers are not saved / loaded and so are random initialized...\n Do you have any idea how and where I could start a debugging?\n This might also effect your \"future release plans\" if you know what I mean... <denchmark-link:https://github.com/Timoeller>@Timoeller</denchmark-link>\n  <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "PhilipMay", "commentT": "2020-09-05T08:06:40Z", "comment_text": "\n \t\tMhh, I believe the culprit might be hidden somewhere else : )\n \n save_dir = Path(\"saved_models/bert-german-doc-tutorial\")\n \n in FARM we still have string based model loading, so if you name your model \"bert\" it will be treated as such. Could that be the problem?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "PhilipMay", "commentT": "2020-09-05T08:15:28Z", "comment_text": "\n \t\t\n in FARM we still have string based model loading, so if you name your model \"bert\" it will be treated as such. Could that be the problem?\n \n It is \"./models/german-nlp-group-electra-base-german-uncased\"\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "PhilipMay", "commentT": "2020-09-05T08:44:35Z", "comment_text": "\n \t\tHow about posting the code you actually used so we can find potential bugs? :p\n As of debugging, I would step into the debugger when loading the model and see if the weights are assigned correctly and are the same across multiple restarts.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "PhilipMay", "commentT": "2020-09-05T10:55:47Z", "comment_text": "\n \t\t\n How about posting the code you actually used so we can find potential bugs? :p\n \n Yes - I will try to build an example with an open dataset.\n <denchmark-link:https://github.com/Timoeller>@Timoeller</denchmark-link>\n  I am doing EarlyStopping which saves the model and loads the best after stopping to evaluate against test.\n That seems to work. Maybe there is a difference in loading it while training and loading it for Inference?\n I will check that tonight...\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "PhilipMay", "commentT": "2020-09-05T22:56:48Z", "comment_text": "\n \t\tVery strange, because the Inferencer constructor calls set_all_seeds \ud83e\udd14\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "PhilipMay", "commentT": "2020-09-06T04:36:21Z", "comment_text": "\n \t\tI always thought that the Trainer (callback) saving and loading for test evaluation is working correctly and that there is only a bug in the Interferer. But that is not the case. See here <denchmark-link:https://github.com/deepset-ai/FARM/issues/522>#522</denchmark-link>\n .\n When training, the model is not loaded from disk when doing test set evaluation but the old is used.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "PhilipMay", "commentT": "2020-09-06T04:45:02Z", "comment_text": "\n \t\t\n Very strange, because the Inferencer constructor calls set_all_seeds thinking\n \n <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n  do you think this might reset important parts of the neutwork?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "PhilipMay", "commentT": "2020-09-07T07:33:00Z", "comment_text": "\n \t\tI doule checked this. With \"normal\" BERT models there is no problem. Seems to be electra specific.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "PhilipMay", "commentT": "2020-09-07T08:57:43Z", "comment_text": "\n \t\tSo early stopping is unrelated to Inferencer loading, lets not try to mix bugs : )\n Can you create a minimal script reproducing the ELECTRA Inferencer error?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "PhilipMay", "commentT": "2020-09-07T10:01:39Z", "comment_text": "\n \t\tExample code to reproduce: <denchmark-link:https://gist.github.com/PhilipMay/bd250cba591b3252b8da2f3d31ee5b64>https://gist.github.com/PhilipMay/bd250cba591b3252b8da2f3d31ee5b64</denchmark-link>\n \n I am using the master branch from FARM, transformers 3.1.0 and torch 1.6.0.\n First run it with lang_model = \"bert-base-german-dbmdz-uncased\". After that run with lang_model = \"german-nlp-group/electra-base-german-uncased\". Compare the results as I did below:\n <denchmark-h:h2>Output for the bert model</denchmark-h>\n \n lang_model = \"bert-base-german-dbmdz-uncased\":\n <denchmark-code>result from early stopping (on dev set) 0.7936361061278443\n result from test set (with best loaded trial) 0.7892217833682655\n Please compare result from early stopping (on dev set) and result from test set (with best loaded trial)...\n </denchmark-code>\n \n Best early stopping result on dev set is 0.79 while the test set result is 0.78. This is the expected result.\n ... note that the trainer stored the model after each evaluation (when it was better) and then loads and evaluates it vs. test.\n Test set evaluation is a little bit lower but in the same range. Everything is ok...\n <denchmark-h:h2>Output for the bert model</denchmark-h>\n \n lang_model = \"german-nlp-group/electra-base-german-uncased\":\n <denchmark-code>result from early stopping (on dev set) 0.7837095866567335\n result from test set (with best loaded trial) 0.47395779141955396\n </denchmark-code>\n \n Best early stopping result on dev set is 0.78 while the test set result is 0.47.\n The test evaluation is much lower which is not normal.\n F1 Macro of 0.47 should be close to the naive baseline (random prediction) on this dataset.\n My guess is that there is a bug in saving or loading electra models.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "PhilipMay", "commentT": "2020-09-07T10:35:57Z", "comment_text": "\n \t\tAre you sure of this code:\n <denchmark-link:https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265>https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265</denchmark-link>\n \n <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n  you wrote it 4 months ago. Could you check that?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "PhilipMay", "commentT": "2020-09-07T12:17:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/PhilipMay>@PhilipMay</denchmark-link>\n  I thought we were talking about different inferencer results.\n How about we break this down. Load the inferencer once, make preds1, load it another time, make preds2. If preds1 != preds2 we should fix something.\n Including crossval and early stopping adds too much complexity on this issue. Is that correct or am I missing something?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "PhilipMay", "commentT": "2020-09-07T14:09:25Z", "comment_text": "\n \t\tOk <denchmark-link:https://github.com/Timoeller>@Timoeller</denchmark-link>\n  - sorry for the bad example. You are right.\n Here is a more streamlined example without crossvalidaton: <denchmark-link:https://gist.github.com/PhilipMay/2e42eeb7174cf0a122036a26ab38ceba#file-electra_load_test-py>https://gist.github.com/PhilipMay/2e42eeb7174cf0a122036a26ab38ceba#file-electra_load_test-py</denchmark-link>\n \n I load a model, train it on germeval18 wait for automatic test set evaluation and write down the f1 macro.\n Then the model is saved and I press return to continue.\n Then model is loaded to inferencer and evaluated on test set again. Then f1 macro is calculated and printed.\n For lang_model = \"bert-base-german-dbmdz-uncased\"\n Test F1 macro on training: 0.7699\n Test F1 macro on inference: 0.7686296419156032\n For lang_model = \"german-nlp-group/electra-base-german-uncased\"\n Test F1 macro on training: 0.7628\n Test F1 macro on inference: 0.4487975932065501\n The difference in case of electra shows the bug. The numbers should be that same!\n <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n  could you maybe have a look on this? You were the nice guy who added electra support to FARM.\n Please make sure to install the current transformers version 3.1.0 to run this. Otherwise the tokenizer will drop umlauts.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "PhilipMay", "commentT": "2020-09-07T15:16:53Z", "comment_text": "\n \t\tThe issue comes from the class weights, those are not properly loaded and not specifying task_type=\"classification\" in the inferencer.\n And I also observe the label mapping is inverted, but only sometimes. Unsure why this happens...\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "PhilipMay", "commentT": "2020-09-07T18:44:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Timoeller>@Timoeller</denchmark-link>\n  do you know more details?\n \n Are you sure that the problem is coming from loading and not from saving?\n Is it the language model that is not correctly loaded or is it the head?\n Is the bug located in HF or Farm?\n \n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "PhilipMay", "commentT": "2020-09-08T07:55:27Z", "comment_text": "\n \t\t\n Very strange, because the Inferencer constructor calls set_all_seeds thinking\n \n I removed that line. Bug still persists.\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "PhilipMay", "commentT": "2020-09-08T08:49:50Z", "comment_text": "\n \t\tOk, I did some further testing as well because there are apparently weird things happening.\n There seems to be a difference when running the script\n \n with model training + inferencing in one go vs\n model training, saving, stopping the script. Running a new script with inference on the saved model only\n \n Could you please verify this <denchmark-link:https://github.com/PhilipMay>@PhilipMay</denchmark-link>\n \n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "PhilipMay", "commentT": "2020-09-08T09:19:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Timoeller>@Timoeller</denchmark-link>\n  I can approve this. Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does  and we have the .\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "PhilipMay", "commentT": "2020-09-08T09:21:32Z", "comment_text": "\n \t\tAll GPUs are currently occupied for EACL, but I'll try to run your script soon :)\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "PhilipMay", "commentT": "2020-09-08T09:22:42Z", "comment_text": "\n \t\t\n All GPUs are currently occupied for EACL, but I'll try to run your script soon :)\n \n <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n  that would be awsome!\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "PhilipMay", "commentT": "2020-09-08T09:54:26Z", "comment_text": "\n \t\t\n Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does not happen and we have the same F1 macro on test set on both runs.\n \n Ok, so now we have a hook on the mystery. It obviously must be that variables do not get GCed and that affects loading or predictions somehow... I rather believe this is a pytorch GC issue on the computation graph and some variables being not deleted.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "PhilipMay", "commentT": "2020-09-08T10:00:42Z", "comment_text": "\n \t\t\n \n Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does not happen and we have the same F1 macro on test set on both runs.\n \n Ok, so now we have a hook on the mystery. It obviously must be that variables do not get GCed and that affects loading or predictions somehow... I rather believe this is a pytorch GC issue on the computation graph and some variables being not deleted.\n \n The strange thing is: Why does it happen with Electra but not with BERT? The most important difference I see is here:\n <denchmark-link:https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265>https://github.com/deepset-ai/FARM/blob/master/farm/modeling/language_model.py#L1255-L1265</denchmark-link>\n \n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "PhilipMay", "commentT": "2020-09-09T06:46:18Z", "comment_text": "\n \t\t\n \n Running training, final test set evaluation and saving in one script and then loading and inference in other script the bug does not happen and we have the same F1 macro on test set on both runs.\n \n Ok, so now we have a hook on the mystery. It obviously must be that variables do not get GCed and that affects loading or predictions somehow... I rather believe this is a pytorch GC issue on the computation graph and some variables being not deleted.\n \n Yes and no...\n For this example this is right. But I have a case where I use early stopping. The electra model that is saved by early stopping is even not working correctly when loaded and evaluated in an other script. But maybe that is a 2nd bug...\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "PhilipMay", "commentT": "2020-09-09T10:36:34Z", "comment_text": "\n \t\tI would also separate these cases for simplicity.\n How do you want to proceed with the pytorch GC issue?\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "PhilipMay", "commentT": "2020-09-09T10:40:27Z", "comment_text": "\n \t\t\n How do you want to proceed with the pytorch GC issue?\n \n Well - to be honest: I hope <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n  checks and fixes it.\n I made some experiments with things I thought would be the reason but did not come to a solution. I maybe check the recent torch issues in github. But since it works with BERT but not with Electra I still think it might be a bug in FARM.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "PhilipMay", "commentT": "2020-09-09T20:00:49Z", "comment_text": "\n \t\tOn it...\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "PhilipMay", "commentT": "2020-09-09T21:10:57Z", "comment_text": "\n \t\tIt's very strange, because the root cause is the language_model = LanguageModel.load(lang_model) call. And it's working for DistilBERT (that also uses the SequenceSummary pooling stuff)...\n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "PhilipMay", "commentT": "2020-09-10T10:10:39Z", "comment_text": "\n \t\tYes. I tested it with the german Distilbert. And it was all ok. No Bug.\n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "PhilipMay", "commentT": "2020-09-12T16:03:41Z", "comment_text": "\n \t\tAre there any new insights in this topic?\n <denchmark-link:https://github.com/stefan-it>@stefan-it</denchmark-link>\n  would it help to have a look at transformers and how they added the head on top of electra?\n <denchmark-link:https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_electra.py#L424>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_electra.py#L424</denchmark-link>\n \n \t\t"}, "comments_34": {"comment_id": 35, "comment_author": "PhilipMay", "commentT": "2020-10-02T16:45:15Z", "comment_text": "\n \t\tHi, just wanted to bunp this. Any idea how to continue with this?\n Thanks\n Philip\n \t\t"}, "comments_35": {"comment_id": 36, "comment_author": "PhilipMay", "commentT": "2020-10-04T11:11:55Z", "comment_text": "\n \t\tSince we (stefan-it and deepset) will be releasing our joint German Electra model soon we will then also look into this issue.\n We will most likely be able to include checking this issue in our next sprint.\n \t\t"}, "comments_36": {"comment_id": 37, "comment_author": "PhilipMay", "commentT": "2020-10-21T08:22:18Z", "comment_text": "\n \t\tWe are boiling down this strange model re loading behaviour. Our next steps will be to determine when the error happens. Then we will deep dive into why:\n <denchmark-code>\u2022 Producing bug\n     \u25e6 Single script training and loading again\n     \u25e6 Single script model loading twice without training\n     \u25e6 CPU vs CUDA\n     \u25e6 Inferencer vs Evaluator (Evaluator also needs to be loaded from disk as in our Trainner.train fct during early stopping)\n \u2022 Not producing bug (sanity check)\n     \u25e6 Separate scripts training + saving + loading\n \u2022 What is going wrong in weight initialization\n     \u25e6 Is LM initialized wrong\n     \u25e6 Or just PH\n     \u25e6 Or both?\n </denchmark-code>\n \n \t\t"}, "comments_37": {"comment_id": 38, "comment_author": "PhilipMay", "commentT": "2020-10-21T12:06:29Z", "comment_text": "\n \t\tGreat <denchmark-link:https://github.com/Timoeller>@Timoeller</denchmark-link>\n  ! Many thanks.\n \t\t"}}}, "commit": {"commit_id": "dac388aa00eb63ad12452f97e40f51a218110b90", "commit_author": "bogdankostic", "commitT": "2020-10-21 17:15:03+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "farm\\modeling\\language_model.py", "file_new_name": "farm\\modeling\\language_model.py", "file_complexity": {"file_NLOC": "917", "file_CCN": "144", "file_NToken": "5719"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1278", "deleted_lines": null, "method_info": {"method_name": "load", "method_params": "cls,pretrained_model_name_or_path,language,kwargs", "method_startline": "1238", "method_endline": "1281", "method_complexity": {"method_NLOC": "23", "method_CCN": "3", "method_NToken": "181", "method_nesting_level": "1"}}}}}}}}