{"BR": {"BR_id": "553", "BR_author": "Ganeshpadmanaban", "BRopenT": "2020-09-23T08:09:58Z", "BRcloseT": "2020-10-04T11:05:00Z", "BR_text": {"BRsummary": "Converting FARM classification model to hugging face model", "BRdescription": "\n Describe the bug\n I'm not able to save a trained farm-classification model to hugging face format since I want to move to the other framework for more specific operations.\n Error message\n <denchmark-code>---------------------------------------------------------------------------\n RuntimeError                              Traceback (most recent call last)\n <ipython-input-83-b2a730b6ac24> in <module>\n ----> 1 convert_to_transformers()\n \n <ipython-input-82-8ab35f02f804> in convert_to_transformers()\n      12 \n      13     # convert to transformers\n ---> 14     transformer_model = model.convert_to_transformers()\n      15 \n      16     # save it (note: transformers use str instead of Path objects)\n \n     555             setattr(transformers_model, transformers_model.base_model_prefix, self.language_model.model)\n     556             transformers_model.classifier.load_state_dict(\n --> 557                 self.prediction_heads[0].feed_forward.feed_forward[0].state_dict())\n     558         elif self.prediction_heads[0].model_type == \"token_classification\":\n     559             # add more info to config\n \n    1043         if len(error_msgs) > 0:\n    1044             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n -> 1045                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n    1046         return _IncompatibleKeys(missing_keys, unexpected_keys)\n    1047 \n \n RuntimeError: Error(s) in loading state_dict for Linear:\n \tsize mismatch for weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 4096]).\n </denchmark-code>\n \n To Reproduce\n \n Load language model \"distilbert-base-german-cased\"\n Add prediction head (TextClassificationHead)\n Save model\n Convert to huggingface using https://github.com/deepset-ai/FARM/blob/master/examples/conversion_huggingface_models.py\n \n System:\n \n OS: Ubuntu 18.04\n GPU/CPU: i7\n FARM version: 0.4.8\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Ganeshpadmanaban", "commentT": "2020-09-29T09:41:29Z", "comment_text": "\n \t\tSince you are doing classification did you also try the conversion_huggingface_models_classification.py script directly beneath the example you linked?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Ganeshpadmanaban", "commentT": "2020-10-01T09:00:01Z", "comment_text": "\n \t\t\n Since you are doing classification did you also try the conversion_huggingface_models_classification.py script directly beneath the example you linked?\n \n Oh yeah, i tried conversion_huggingface_models_classification.py and the output is from that actually. Possibly architecture for distrilbert is not compatible either. I'll investigate further this weekend and update.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Ganeshpadmanaban", "commentT": "2020-10-01T09:26:09Z", "comment_text": "\n \t\tOk, strange. Can you give more details to what model you are converting? Can you post the actual code you used for converting?\n Judging from the error message there is a language model size mismatch: 768 is the embedding dimension of base models, 4096 is the embedding dimension of some larger models.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Ganeshpadmanaban", "commentT": "2020-10-01T09:53:34Z", "comment_text": "\n \t\t<denchmark-code>import os\n import logging\n from pathlib import Path\n import pandas as pd\n \n from farm.data_handler.data_silo import DataSilo\n from farm.data_handler.processor import Processor\n from farm.data_handler.processor import TextClassificationProcessor\n from farm.modeling.optimization import initialize_optimizer\n from farm.infer import Inferencer\n from farm.modeling.adaptive_model import AdaptiveModel\n from farm.modeling.language_model import LanguageModel\n from farm.modeling.prediction_head import TextClassificationHead\n from farm.modeling.tokenization import Tokenizer\n from farm.train import Trainer\n from farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n \n from transformers.pipelines import pipeline\n \n def create_data():\n \n     # create data\n     if not os.path.exists('dataset'):\n         os.makedirs('dataset')\n \n     dataset = pd.DataFrame(columns=[\"text\", \"label\"])\n     dataset[\"text\"] = [\"happy\", \"bad\", \"awesome!\", \"okay\", \"nothing\", \"whatever\", \"great\", \"good\", \"fine\", \"goody\", \"hallo!\", \"mhh\"]\n     dataset[\"label\"] = [\"POS\", \"NEG\", \"POS\", \"NEG\", \"NEG\", \"NEG\", \"POS\", \"POS\", \"POS\", \"POS\", \"POS\", \"NEG\"]\n \n     dataset.to_csv(\"dataset/train.tsv\", index=False, sep=\"\\t\")\n \n     # create data\n     dataset = pd.DataFrame(columns=[\"text\", \"label\"])\n     dataset[\"text\"] = [\"whatever\", \"great\", \"good\", \"fine\"]\n     dataset[\"label\"] = [\"NEG\", \"POS\", \"POS\", \"POS\"]\n \n \n     dataset.to_csv(\"dataset/test.tsv\", index=False, sep=\"\\t\")\n \n def create_model():\n \n     ##########################\n     ########## Settings\n     ##########################\n     set_all_seeds(seed=42)\n     n_epochs = 1\n     batch_size = 4\n     evaluate_every = 100\n     lang_model = \"distilbert-base-german-cased\"\n     do_lower_case = False\n     use_amp = None\n \n     device, n_gpu = initialize_device_settings(use_cuda=True, use_amp=use_amp)\n \n     # 1.Create a tokenizer\n     tokenizer = Tokenizer.load(pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case)\n \n     # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n     # Here we load GermEval 2018 Data automaticaly if it is not available.\n     # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n \n     label_list = [\"POS\", \"NEG\"]\n     metric = \"f1_macro\"\n \n     processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                             max_seq_len=32,\n                                             data_dir=Path(\"dataset/\"),\n                                             label_list=label_list,\n                                             metric=metric,\n                                             label_column_name=\"label\"\n                                             )\n \n     # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a\n     #    few descriptive statistics of our datasets\n     data_silo = DataSilo(\n         processor=processor,\n         batch_size=batch_size)\n \n     # 4. Create an AdaptiveModel\n     # a) which consists of a pretrained language model as a basis\n     language_model = LanguageModel.load(lang_model)\n     # b) and a prediction head on top that is suited for our task => Text classification\n     prediction_head = TextClassificationHead(\n         class_weights=data_silo.calculate_class_weights(task_name=\"text_classification\"),\n         num_labels=len(label_list))\n \n     model = AdaptiveModel(\n         language_model=language_model,\n         prediction_heads=[prediction_head],\n         embeds_dropout_prob=0.1,\n         lm_output_types=[\"per_sequence\"],\n         device=device)\n \n     # 5. Create an optimizer\n     model, optimizer, lr_schedule = initialize_optimizer(\n         model=model,\n         learning_rate=3e-5,\n         device=device,\n         n_batches=len(data_silo.loaders[\"train\"]),\n         n_epochs=n_epochs,\n         use_amp=use_amp)\n \n     # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n     trainer = Trainer(\n         model=model,\n         optimizer=optimizer,\n         data_silo=data_silo,\n         epochs=n_epochs,\n         n_gpu=n_gpu,\n         lr_schedule=lr_schedule,\n         evaluate_every=evaluate_every,\n         device=device)\n \n     # 7. Let it grow\n     trainer.train()\n \n     # 8. Hooray! You have a model. Store it:\n     save_dir = Path(\"saved_models/test-model\")\n     model.save(save_dir)\n     processor.save(save_dir)\n \n # ##############################################\n # ###  From FARM -> Transformers\n # ##############################################\n def convert_to_transformers():\n     farm_input_dir = Path(\"saved_models/test-model\")\n     transformers_output_dir = \"saved_models/test-model-out\"\n     #\n     # # # load from FARM format\n     model = AdaptiveModel.load(farm_input_dir, device=\"cpu\")\n     processor = Processor.load_from_dir(farm_input_dir)\n     model.connect_heads_with_processor(processor.tasks)\n \n     # convert to transformers\n     transformer_model = model.convert_to_transformers()\n \n     # save it (note: transformers use str instead of Path objects)\n     Path(transformers_output_dir).mkdir(parents=True, exist_ok=True)\n     transformer_model.save_pretrained(transformers_output_dir)\n     processor.tokenizer.save_pretrained(transformers_output_dir)\n \n \n if __name__ == \"__main__\":\n \tcreate_data()\n \tcreate_model()\n \tconvert_to_transformers()\n </denchmark-code>\n \n Here you go :)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Ganeshpadmanaban", "commentT": "2020-10-04T11:07:02Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/Ganeshpadmanaban>@Ganeshpadmanaban</denchmark-link>\n  for the detailed code. I could reproduce your issue.\n I found and fixed the bug on FARM side. When you look at the PR you will see that the wrong config type was loaded.\n I tested your code again and it works now. Could you verify it on your side as well, please?\n \t\t"}}}, "commit": {"commit_id": "c106bca84f36a44ef890e43a1e3da6171e4a8bd8", "commit_author": "Timo Moeller", "commitT": "2020-10-04 13:04:59+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "farm\\modeling\\language_model.py", "file_new_name": "farm\\modeling\\language_model.py", "file_complexity": {"file_NLOC": "808", "file_CCN": "128", "file_NToken": "5024"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "767", "deleted_lines": "767", "method_info": {"method_name": "load", "method_params": "cls,pretrained_model_name_or_path,language,kwargs", "method_startline": "745", "method_endline": "786", "method_complexity": {"method_NLOC": "22", "method_CCN": "3", "method_NToken": "176", "method_nesting_level": "1"}}}}}}}}