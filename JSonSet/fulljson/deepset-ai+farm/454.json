{"BR": {"BR_id": "454", "BR_author": "Timoeller", "BRopenT": "2020-07-10T08:22:01Z", "BRcloseT": "2020-10-28T17:02:03Z", "BR_text": {"BRsummary": "QA inference breaks on large dataset", "BRdescription": "\n Describe the bug\n QA inference failing on very large dataset\n Error message\n Inferencing Samples: 0 Batches [00:00, ? Batches/s]Traceback (most recent call last):\n File \"question_answering_inference.py\", line 89, in \n question_answering()\n File \"question_answering_inference.py\", line 34, in question_answering\n result = model.inference_from_file(file=filename, return_json=False, multiprocessing_chunksize=1)\n File \"/home/ubuntu/pycharm/timosyncNEW/farm/infer.py\", line 645, in inference_from_file\n multiprocessing_chunksize=multiprocessing_chunksize, streaming=streaming)\n File \"/home/ubuntu/pycharm/timosyncNEW/farm/infer.py\", line 345, in inference_from_file\n streaming=streaming,\n File \"/home/ubuntu/pycharm/timosyncNEW/farm/infer.py\", line 637, in inference_from_dicts\n multiprocessing_chunksize=multiprocessing_chunksize, streaming=streaming)\n File \"/home/ubuntu/pycharm/timosyncNEW/farm/infer.py\", line 418, in inference_from_dicts\n return list(predictions)\n File \"/home/ubuntu/pycharm/timosyncNEW/farm/infer.py\", line 489, in _inference_with_multiprocessing\n dataset, tensor_names, baskets\n File \"/home/ubuntu/pycharm/timosyncNEW/farm/infer.py\", line 576, in _get_predictions_and_aggregate\n for i, batch in enumerate(tqdm(data_loader, desc=f\"Inferencing Samples\", unit=\" Batches\", disable=self.disable_tqdm)):\n File \"/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/tqdm/_tqdm.py\", line 1060, in iter\n for obj in iterable:\n File \"/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 345, in next\n data = self._next_data()\n File \"/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 384, in _next_data\n index = self._next_index()  # may raise StopIteration\n File \"/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 339, in _next_index\n return next(self._sampler_iter)  # may raise StopIteration\n File \"/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/sampler.py\", line 200, in iter\n for idx in self.sampler:\n File \"/home/ubuntu/miniconda3/envs/py37/lib/python3.7/site-packages/torch/utils/data/sampler.py\", line 62, in iter\n return iter(range(len(self.data_source)))\n TypeError: object of type 'NoneType' has no len()\n Additional context\n I suspect this has to do with one whole chunk in multiprocessing not being transformed correctly. We should remove empty datasets before sending them to _get_predictions_and_aggregate.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Timoeller", "commentT": "2020-09-11T10:34:05Z", "comment_text": "\n \t\tThis issue has been automatically marked as stale because it has not had recent activity. It will be closed in 14 days if no further activity occurs.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Timoeller", "commentT": "2020-09-24T15:49:25Z", "comment_text": "\n \t\tI have the same problem running inference_from_file on Kubeflow with FARM 0.4.7, but I can't reproduce the same error on Colab\n predictions_json = model.inference_from_file(file=json_eval)\n File \"/usr/local/lib/python3.7/site-packages/farm/infer.py\", line 379, in inference_from_file\n streaming=streaming,\n File \"/usr/local/lib/python3.7/site-packages/farm/infer.py\", line 456, in inference_from_dicts\n return list(predictions)\n File \"/usr/local/lib/python3.7/site-packages/farm/infer.py\", line 531, in _inference_with_multiprocessing\n dataset, tensor_names, baskets\n File \"/usr/local/lib/python3.7/site-packages/farm/infer.py\", line 610, in _get_predictions_and_aggregate\n for i, batch in enumerate(tqdm(data_loader, desc=f\"Inferencing Samples\", unit=\" Batches\", disable=self.disable_tqdm)):\n File \"/usr/local/lib/python3.7/site-packages/tqdm/std.py\", line 1133, in iter\n for obj in iterable:\n File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 363, in next\n data = self._next_data()\n File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 402, in _next_data\n index = self._next_index()  # may raise StopIteration\n File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 357, in _next_index\n return next(self._sampler_iter)  # may raise StopIteration\n File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/sampler.py\", line 208, in iter\n for idx in self.sampler:\n File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/sampler.py\", line 62, in iter\n return iter(range(len(self.data_source)))\n TypeError: object of type 'NoneType' has no len()\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Timoeller", "commentT": "2020-09-24T18:35:31Z", "comment_text": "\n \t\tStrange that this error pops up, I thought this happens in very special cases only.\n How many cpu cores are on your Kubeflow instance?\n When working in colab, can you tell if many QA pairs are dismissed and not loaded for modelling?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Timoeller", "commentT": "2020-09-25T08:46:03Z", "comment_text": "\n \t\t\n Strange that this error pops up, I thought this happens in very special cases only.\n How many cpu cores are on your Kubeflow instance?\n When working in colab, can you tell if many QA pairs are dismissed and not loaded for modelling?\n \n The Kubeflow node has 7 parallel workers.  It seems there are no qa pairs dismissed In  Colab.\n I realized that I can fix the problem in Kubeflow removing empty qas ( \"qas\": []) in squad dataset. But I can't figure out why this is not happening in Colab. Both experiments are running without GPU\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Timoeller", "commentT": "2020-09-29T09:29:28Z", "comment_text": "\n \t\tThanks for the additional infos. The devil is surprisingly in the details of our framework : )\n When you have 7 workers the data is chunked into smaller parts than when you have only 2 (as in colab). I suspect one of these chunks contained only empty qas lists and therefor a whole chunk does not contain any QA pairs. Do you have a lot of empty lists? And how did you create the annotations?\n \t\t"}}}, "commit": {"commit_id": "c21466c692d7d5d026cda9a6470d8713ca8ac558", "commit_author": "Timo Moeller", "commitT": "2020-10-28 18:02:03+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "farm\\infer.py", "file_new_name": "farm\\infer.py", "file_complexity": {"file_NLOC": "357", "file_CCN": "46", "file_NToken": "2268"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "508,509,510,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526", "deleted_lines": "508,509,510,511,512,514,515,516,517,518,519,520,521,522"}}}}}}