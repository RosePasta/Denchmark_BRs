{"BR": {"BR_id": "1306", "BR_author": "xylophone21", "BRopenT": "2020-12-05T09:16:39Z", "BRcloseT": "2020-12-24T03:06:41Z", "BR_text": {"BRsummary": "--enable-microbatch got pickle error", "BRdescription": "\n Describe the bug\n Run demo with --enable-microbatch\n serve PyTorchFashionClassifier:latest --enable-microbatch\n To Reproduce\n serve PyTorchFashionClassifier:latest --enable-microbatch\n Expected behavior\n run with microbatch\n Screenshots/Logs\n [2020-12-05 17:18:06,106] DEBUG - Creating local YataiService instance\n [2020-12-05 17:18:06,401] DEBUG - Upgrading tables to the latest revision\n [2020-12-05 17:18:06,427] INFO - Getting latest version PyTorchFashionClassifier:20201204180721_6A5E98\n [2020-12-05 17:18:06,427] INFO - Starting BentoML API server in development mode..\n [2020-12-05 17:18:07,866] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fc91aac7100>, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))\n [2020-12-05 17:18:09,371] DEBUG - Using BentoML default docker base image 'bentoml/model-server:0.9.2-py38'\n [2020-12-05 17:18:09,640] WARNING - BentoML by default does not include spacy and torchvision package when using PytorchModelArtifact. To make sure BentoML bundle those packages if they are required for your model, either import those packages in BentoService definition file or manually add them via @env(pip_packages=['torchvision']) when defining a BentoService\n [2020-12-05 17:18:09,642] WARNING - pip package requirement torch already exist\n [2020-12-05 17:18:10,654] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fc91cf88fd0>, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))\n [2020-12-05 17:18:10,726] INFO - Micro batch enabled for API predict\n [2020-12-05 17:18:10,726] INFO - Your system nofile limit is 10240, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n [2020-12-05 17:18:11,765] DEBUG - HTTPSConnectionPool(host='api.amplitude.com', port=443): Max retries exceeded with url: /httpapi (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fc91e8d87f0>, 'Connection to api.amplitude.com timed out. (connect timeout=1)'))\n Traceback (most recent call last):\n File \"/Users/lihui/Code/dubhe/bentomlTest/server.py\", line 7, in \n bentoml.cli.cli()\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py\", line 829, in call\n return self.main(*args, **kwargs)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py\", line 782, in main\n rv = self.invoke(ctx)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py\", line 1259, in invoke\n return _process_result(sub_ctx.command.invoke(sub_ctx))\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py\", line 1066, in invoke\n return ctx.invoke(self.callback, **ctx.params)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\n return callback(*args, **kwargs)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py\", line 138, in wrapper\n return func(*args, **kwargs)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py\", line 115, in wrapper\n return_value = func(*args, **kwargs)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/click_utils.py\", line 99, in wrapper\n return func(*args, **kwargs)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/cli/bento_service.py\", line 243, in serve\n start_dev_server(saved_bundle_path, port, enable_microbatch, run_with_ngrok)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/server/init.py\", line 76, in start_dev_server\n marshal_server.async_start(port=port)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/site-packages/bentoml/marshal/marshal.py\", line 298, in async_start\n marshal_proc.start()\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/process.py\", line 121, in start\n self._popen = self._Popen(self)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/context.py\", line 224, in _Popen\n return _default_context.get_context().Process._Popen(process_obj)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/context.py\", line 284, in _Popen\n return Popen(process_obj)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 32, in init\n super().init(process_obj)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_fork.py\", line 19, in init\n self._launch(process_obj)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/popen_spawn_posix.py\", line 47, in _launch\n reduction.dump(process_obj, fp)\n File \"/Users/lihui/opt/miniconda3/envs/dubhe/lib/python3.8/multiprocessing/reduction.py\", line 60, in dump\n ForkingPickler(file, protocol).dump(obj)\n AttributeError: Can't pickle local object 'metrics_patch.._MarshalService'\n Process finished with exit code 1\n Environment:\n \n OS: MacOS 10.15.7\n Python Version Python 3.8\n \n <denchmark-code>{\n  \"date\": \"2020-10-15T09:08:27-0700\",\n  \"dirty\": false,\n  \"error\": null,\n  \"full-revisionid\": \"25c319d8161629d695eaecf6418e50fea6535d6e\",\n  \"version\": \"0.9.2\"\n }\n </denchmark-code>\n \n Additional context\n No\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xylophone21", "commentT": "2020-12-08T08:16:14Z", "comment_text": "\n \t\tMaybe related: <denchmark-link:https://github.com/bentoml/BentoML/pull/995>#995</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xylophone21", "commentT": "2020-12-08T09:09:42Z", "comment_text": "\n \t\tI reproduced this on macOS with Python3.8. I'll draft a new PR like <denchmark-link:https://github.com/bentoml/BentoML/pull/995>#995</denchmark-link>\n . Before the new release, you can use\n <denchmark-code>bentoml serve-gunicorn PyTorchFashionClassifier:latest --enable-microbatch --workers 1\n </denchmark-code>\n \n or Python 3.7 instead.\n \t\t"}}}, "commit": {"commit_id": "1d885119ae8e560b4fbd60f0de80f08383ab0a05", "commit_author": "bojiang", "commitT": "2020-12-23 19:06:40-08:00", "commit_complexity": {"commit_NLOC": "0.5833333333333334", "commit_CCN": "1.0", "commit_Nprams": "0.08333333333333333"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "bentoml\\marshal\\dispatcher.py", "file_new_name": "bentoml\\marshal\\dispatcher.py", "file_complexity": {"file_NLOC": "175", "file_CCN": "47", "file_NToken": "1147"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "134", "deleted_lines": null, "method_info": {"method_name": "__call__", "method_params": "self,callback", "method_startline": "131", "method_endline": "146", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "24", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "bentoml\\marshal\\marshal.py", "file_new_name": "bentoml\\marshal\\marshal.py", "file_complexity": {"file_NLOC": "273", "file_CCN": "34", "file_NToken": "1629"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "40,41,42,43,44,45", "deleted_lines": null, "method_info": {"method_name": "metrics_patch.__init__", "method_params": "self,args,kwargs", "method_startline": "39", "method_endline": "83", "method_complexity": {"method_NLOC": "42", "method_CCN": "3", "method_NToken": "227", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "193", "deleted_lines": "187", "method_info": {"method_name": "add_batch_handler", "method_params": "self,api_name,max_latency,max_batch_size", "method_startline": "178", "method_endline": "194", "method_complexity": {"method_NLOC": "9", "method_CCN": "2", "method_NToken": "67", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "40,41,42,43,44,45", "deleted_lines": null, "method_info": {"method_name": "metrics_patch", "method_params": "cls", "method_startline": "37", "method_endline": "117", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "21", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "bentoml\\server\\__init__.py", "file_new_name": "bentoml\\server\\__init__.py", "file_complexity": {"file_NLOC": "134", "file_CCN": "7", "file_NToken": "598"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101,102,103,104,105,106", "deleted_lines": null, "method_info": {"method_name": "start_dev_batching_server", "method_params": "str,int,int,int,int", "method_startline": "101", "method_endline": "106", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "22", "method_nesting_level": "0"}}}}}}}}