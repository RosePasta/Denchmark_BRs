{"BR": {"BR_id": "2594", "BR_author": "baldwint", "BRopenT": "2020-07-22T00:03:43Z", "BRcloseT": "2020-07-31T07:37:18Z", "BR_text": {"BRsummary": "RayContext fails when Spark configuration object returns None", "BRdescription": "\n I am trying to use the RayContext in a hosted Spark environment (Databricks) using the SparkContext that is provided to me rather than instantiating one myself.\n The code ray_ctx = RayContext(sc=sc) fails with the error\n     223         # Ray Manager is only needed for Spark cluster mode to monitor ray processes.\n     224         else:\n --> 225             self.num_ray_nodes = int(self.sc.getConf().get(\"spark.executor.instances\"))\n     226             self.ray_node_cpu_cores = int(self.sc.getConf().get(\"spark.executor.cores\"))\n     227             self.python_loc = os.environ['PYSPARK_PYTHON']\n \n TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\n It seems the RayContext initializer is not handling the possibility that some spark contexts can return None for these configuration values.\n I am using analytics-zoo 0.8.1 from PyPI on the Databricks 6.6 ML runtime.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "baldwint", "commentT": "2020-07-22T06:49:32Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/baldwint>@baldwint</denchmark-link>\n \n Thanks for pointing out this. I just noticed this issue recently.\n Are you running on a yarn cluster or other clusters on Databricks? Seems for a yarn cluster, \"spark.executor.instances\" should be there but for standalone clusters, such property is probably missing.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "baldwint", "commentT": "2020-07-22T18:11:02Z", "comment_text": "\n \t\tI created and launched a cluster using the Azure Databricks user interface, using the standard mode (not high concurrency), with fixed number of nodes (no autoscaling). I am not sure whether this product uses YARN internally or not. I assumed it did, but after searching the internet I now think it may be running Spark in the standalone mode.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "baldwint", "commentT": "2020-07-23T06:00:20Z", "comment_text": "\n \t\t\n I created and launched a cluster using the Azure Databricks user interface, using the standard mode (not high concurrency), with fixed number of nodes (no autoscaling). I am not sure whether this product uses YARN internally or not. I assumed it did, but after searching the internet I now think it may be running Spark in the standalone mode.\n \n Thanks so much for the information. Then can you check if spark.executor.cores and spark.cores.max are in the conf? Seens spark.executor.instances is only for yarn and for standalone clusters, we can calculate the number using floor(total cores/executor cores).\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "baldwint", "commentT": "2020-07-23T19:23:47Z", "comment_text": "\n \t\tUnfortunately spark.executor.cores and spark.cores.max are also missing from the conf.\n It seems that in standalone mode Spark has poor support for determining these values from within Python. After reading <denchmark-link:https://kb.databricks.com/clusters/calculate-number-of-cores.html>this documentation page</denchmark-link>\n  I was able to determine the correct number of executors using\n def get_num_executors(sc):\n     return len(sc._jsc.sc().statusTracker().getExecutorInfos()) - 1\n Where 1 is subtracted because we do not want to count the driver node. It's hard to believe that this is the intended way to do it, but I have found versions of this on various Stack Overflow answers.\n Executor cores is more difficult. The method described in the above article does not work because in my cluster the driver node is not the same type as the workers (it has a different number of cores). There is an environment variable, $SPARK_WORKER_CORES, with cores per executor, but this is not set on the driver node (only on workers). I had to do a UDF in order to see this value from the driver node:\n import os\n import pyspark.sql.functions as F\n \n def get_executor_cores(spark):\n     df = spark.createDataFrame([[i,] for i in range(100)], schema=[\"dummy\"]).withColumn(\n         \"swc\", F.udf(lambda: os.popen(\"echo $SPARK_WORKER_CORES\").read())()\n     )\n     return int(df.select(F.max(\"swc\")).collect()[0][0])\n Both of these solutions are too hacky to be a part of any library code, in my opinion, but I have not found a better way.\n Once I know these two values, I am able to avoid the error by manually setting the configuration values that RayContext expects. On a cluster with 2 worker nodes and 4 cores per worker node:\n sc._conf.set('spark.executor.instances', '2')\n sc._conf.set('spark.executor.cores', '4')\n And then RayContext can pick these values up. However I would prefer to pass these directly to RayContext somehow, rather than modifying the spark config.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "baldwint", "commentT": "2020-07-27T17:54:29Z", "comment_text": "\n \t\tI think the broader issue here is support for non-YARN Spark clusters, which might not be an intended use case for the project at all (in which case no action is needed here and this issue can be closed).\n Even if that's true, I wanted to say a little bit more about the steps I took to get the RayContext working on a cluster in the standalone mode, in case others find this issue while trying to do the same. In my case (using Databricks runtime 6.6 ML and analytics-zoo 0.8.1), there are a total of five configuration options that I had to set:\n from zoo.ray import RayContext\n from zoo.common.nncontext import init_nncontext\n \n sc._conf.set('spark.executor.instances', str(NUMBER_OF_EXECUTORS))\n sc._conf.set('spark.executor.cores', str(CORES_PER_EXECUTOR))\n sc._conf.set('spark.cores.max', str(NUMBER_OF_EXECUTORS * CORES_PER_EXECUTOR))\n sc._conf.set('spark.shuffle.blockTransferService', 'nio')\n sc._conf.set('spark.scheduler.minRegisteredResourcesRatio', '1.0')\n \n init_nncontext()\n ray_ctx = RayContext(sc=sc)\n Where NUMBER_OF_EXECUTORS and CORES_PER_EXECUTOR are determined manually. This works, provided that the two JARs mentioned in the BIGDL_JARS environment variable are properly loaded (which I had to do manually using Databricks' UI).\n Finally, I had to monkey patch faulthandler.enable with a no-op to resolve an incompatibility with Databricks' ConsoleBuffer object that it uses for stdout/stderr streams, but this is a runtime-specific fix that is not related to Analytics Zoo.\n After all that, everything seems to work.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "baldwint", "commentT": "2020-07-28T02:04:39Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/baldwint>@baldwint</denchmark-link>\n \n So sorry that I missed your reply last week. We are planning to support non-yarn clusters and thanks so much for your detailed information!\n Yeah, the jars and some configurations are required by the BigDL. For RayOnSpark, as you mentioned, I can modify our implementation so that users can directly pass the configurations if they wish in case the values are not detectable. I would raise a PR for this very soon. Thanks so much for your help and support!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "baldwint", "commentT": "2020-07-28T02:07:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jason-dai>@jason-dai</denchmark-link>\n  For the other configuration handling, shall we add the experience shared by Thomas to FAQ or somewhere else?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "baldwint", "commentT": "2020-07-28T12:17:11Z", "comment_text": "\n \t\t\n @jason-dai For the other configuration handling, shall we add the experience shared by Thomas to FAQ or somewhere else?\n \n We should at least update <denchmark-link:https://analytics-zoo.github.io/master/#ProgrammingGuide/AnalyticsZoo-on-Databricks/>https://analytics-zoo.github.io/master/#ProgrammingGuide/AnalyticsZoo-on-Databricks/</denchmark-link>\n \n Will these be fixed by init_spark_local?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "baldwint", "commentT": "2020-07-29T01:55:49Z", "comment_text": "\n \t\t\n \n @jason-dai For the other configuration handling, shall we add the experience shared by Thomas to FAQ or somewhere else?\n \n We should at least update https://analytics-zoo.github.io/master/#ProgrammingGuide/AnalyticsZoo-on-Databricks/\n Will these be fixed by init_spark_local?\n \n If the BigDL required configurations could not be obtained, I think there's the same problem with init_spark_on_local?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "baldwint", "commentT": "2020-07-29T10:16:48Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/baldwint>@baldwint</denchmark-link>\n \n When you launch the cluster on Azure Databricks, isn't there a place for you to add a conf file to start Spark?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "baldwint", "commentT": "2020-07-30T17:22:15Z", "comment_text": "\n \t\tYes, there is, and I had forgotten about this interface when I posted my messages above since I don't use it very often. Entering the config variables in the Databricks UI as described in your documentation page is the correct way to set them, rather than using sc._conf.set as I described above. I have since switched to doing that.\n I'm afraid I'm guilty of not having read your <denchmark-link:https://analytics-zoo.github.io/master/#ProgrammingGuide/AnalyticsZoo-on-Databricks/>documentation page</denchmark-link>\n  before posting this issue (somehow didn't see it). Indeed, the five values I mentioned earlier are not sufficient for RayOnSpark, and the other values from the docs are needed to avoid other problems. In particular, setting the memory size configs has helped me avoid hitting physical memory limits. My complete configuration now reads\n <denchmark-code>spark.serializer org.apache.spark.serializer.JavaSerializer\n spark.shuffle.blockTransferService nio\n spark.databricks.delta.preview.enabled true\n spark.executor.cores 8\n spark.executor.memory 17g\n spark.speculation false\n spark.executor.instances 6\n spark.driver.memory 16g\n spark.scheduler.minRegisteredResourcesRatio 1.0\n spark.cores.max 48\n </denchmark-code>\n \n which is the same as in the documentation, but with cores/instances/memory values set for a cluster of Standard_DS4_v2 nodes with 6 workers.\n I think your documentation page is in good shape, I just didn't understand that it was my responsibility to set these configuration values in the first place (my mistake, sorry). Seeing TypeError made me think it was a code issue.\n Thanks for your attentive support.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "baldwint", "commentT": "2020-07-31T01:38:36Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/baldwint>@baldwint</denchmark-link>\n \n Thanks for your information!\n Actually only num executors and executor cores are required for RayOnSpark. Other configurations are required for starting Spark, Analytics Zoo or BigDL, you need them even you don't use RayOnSpark.\n I have proposed a PR to allow users specify num executors and executor cores for RayOnSpark in case the values can't be detected in SparkConf.\n Feel free to tell me if you encounter further problems!\n \t\t"}}}, "commit": {"commit_id": "87ac6606046b05bbc6dee80a7a2789f9f61f80a4", "commit_author": "Kai Huang", "commitT": "2020-07-31 15:37:17+08:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pyzoo\\test\\zoo\\ray\\test_ray_on_local.py", "file_new_name": "pyzoo\\test\\zoo\\ray\\test_ray_on_local.py", "file_complexity": {"file_NLOC": "22", "file_CCN": "4", "file_NToken": "148"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "34,35", "deleted_lines": "34,35", "method_info": {"method_name": "test_local", "method_params": "self", "method_startline": "27", "method_endline": "41", "method_complexity": {"method_NLOC": "12", "method_CCN": "3", "method_NToken": "98", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pyzoo\\zoo\\ray\\raycontext.py", "file_new_name": "pyzoo\\zoo\\ray\\raycontext.py", "file_complexity": {"file_NLOC": "332", "file_CCN": "48", "file_NToken": "2094"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "358", "method_info": {"method_name": "_start_restricted_worker", "method_params": "self,num_cores,node_ip_address", "method_startline": "357", "method_endline": "371", "method_complexity": {"method_NLOC": "14", "method_CCN": "2", "method_NToken": "99", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "198", "method_info": {"method_name": "__init__", "method_params": "self,sc,redis_port,password,object_store_memory,verbose,env,extra_params", "method_startline": "197", "method_endline": "198", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "31", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "199,200", "deleted_lines": "198", "method_info": {"method_name": "__init__", "method_params": "self,sc,redis_port,password,object_store_memory,verbose,env,extra_params,num_ray_nodes,ray_node_cpu_cores", "method_startline": "198", "method_endline": "200", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "39", "method_nesting_level": "1"}}}}}}}}