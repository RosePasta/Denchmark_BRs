{"BR": {"BR_id": "11104", "BR_author": "sakvaua", "BRopenT": "2018-09-07T08:36:58Z", "BRcloseT": "2018-09-28T18:59:59Z", "BR_text": {"BRsummary": "Cannot save optimizer weights due to h5 error \"object header message is too large\"", "BRdescription": "\n When trying to save my model I get the runtime error below. There was a similar issue when model layers names were too long and it can be solved by giving layers shorter names. This time the error pops up when saving optimizer weights. getattr(model.optimizer,'weights') shows\n <denchmark-code>[<tf.Variable 'Adam/iterations:0' shape=() dtype=int64_ref>,\n  <tf.Variable 'training/Adam/Variable:0' shape=(3, 3, 1, 64) dtype=float32_ref>,\n  <tf.Variable 'training/Adam/Variable_1:0' shape=(64,) dtype=float32_ref>,\n  <tf.Variable 'training/Adam/Variable_2:0' shape=(64,) dtype=float32_ref>,\n ...]\n \n </denchmark-code>\n \n and if I convert it to numpy array its length is above the 64k limits which gives h5 runtime. I can save the model if I use save_model(....,include_optimizer=False) but I need the optimizer state. Is there any way I can reduce the length of \"training/Adam/Variable:0\"... names so as to fit them into 64k hdf5 table limit. Thanks.\n <denchmark-code>---------------------------------------------------------------------------\n RuntimeError                              Traceback (most recent call last)\n <ipython-input-130-d231b4a5a40c> in <module>()\n ----> 1 model.save('model')\n \n C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py in save(self, filepath, overwrite, include_optimizer)\n    1083             raise NotImplementedError\n    1084         from ..models import save_model\n -> 1085         save_model(self, filepath, overwrite, include_optimizer)\n    1086 \n    1087     def save_weights(self, filepath, overwrite=True):\n \n C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py in save_model(model, filepath, overwrite, include_optimizer)\n     173                     #print('Weight names',weight_names,len(weight_names),np.asarray(weight_names).nbytes)\n     174                     optimizer_weights_group.attrs[\n --> 175                         'weight_names'] = weight_names\n     176                     for name, val in zip(weight_names, weight_values):\n     177                         param_dset = optimizer_weights_group.create_dataset(\n \n h5py\\_objects.pyx in h5py._objects.with_phil.wrapper()\n \n h5py\\_objects.pyx in h5py._objects.with_phil.wrapper()\n \n C:\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\attrs.py in __setitem__(self, name, value)\n      93         use the methods create() and modify().\n      94         \"\"\"\n ---> 95         self.create(name, data=value, dtype=base.guess_dtype(value))\n      96 \n      97     @with_phil\n \n C:\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\attrs.py in create(self, name, data, shape, dtype)\n     186 \n     187             try:\n --> 188                 attr = h5a.create(self._id, self._e(tempname), htype, space)\n     189             except:\n     190                 raise\n \n h5py\\_objects.pyx in h5py._objects.with_phil.wrapper()\n \n h5py\\_objects.pyx in h5py._objects.with_phil.wrapper()\n \n h5py\\h5a.pyx in h5py.h5a.create()\n \n RuntimeError: Unable to create attribute (object header message is too large)\n </denchmark-code>\n \n Please make sure that the boxes below are checked before you submit your issue. If your issue is an implementation question, please ask your question on <denchmark-link:http://stackoverflow.com/questions/tagged/keras>StackOverflow</denchmark-link>\n  or <denchmark-link:https://keras-slack-autojoin.herokuapp.com/>join the Keras Slack channel</denchmark-link>\n  and ask there instead of filing a GitHub issue.\n Thank you!\n \n \n  Check that you are up-to-date with the master branch of Keras. You can update with:\n pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps\n \n \n  If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.\n \n \n  If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:\n pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps\n \n \n  Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).\n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sakvaua", "commentT": "2018-09-08T19:05:04Z", "comment_text": "\n \t\tDo I understand this correctly that you're working with a model that has a quite large amount of weights? I'm guessing more than 2200 or so. Unfortunately it doesn't look like the names of these variables can be controlled, excepted for the name of the optimizer. So if you exceed those 64k by just a small amount you could perhaps save 3 bytes per variable by doing the following, but it really won't save you much:\n class A(Adam):\n     pass\n While you wait for this to be fixed properly, you could try to just do save_model(..., include_optimizer=False) as you described, then re-open the h5py file and use your own custom optimizer saving code that doesn't have this problem. I regularly save additional data into h5py and it works great.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sakvaua", "commentT": "2018-09-09T19:32:25Z", "comment_text": "\n \t\tCould you provide a minimal script to help us narrow down the possible bug? Thank you.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sakvaua", "commentT": "2018-09-10T08:38:39Z", "comment_text": "\n \t\tHere is a sample code that reproduces the error. It takes quite a bit of time though. 10 mins to save. Not sure why.\n import numpy as np\n from keras.models import Model, Input\n from keras.layers import Conv2D, Concatenate, GlobalAveragePooling2D\n from keras.optimizers import Adam\n \n inp = Input(shape=(10, 10, 1))\n \n layersC = []\n for i in range(10):\n     layers = []\n     for j in range(40):\n         x = Conv2D(1, (1, 1))(inp)\n         layers.append(x)\n     layersC.append(Concatenate()(layers))\n \n out = Concatenate()(layersC)\n out = Conv2D(1, (1, 1))(out)\n out = GlobalAveragePooling2D()(out)\n m = Model(inputs=inp, outputs=out)\n m.compile(optimizer=Adam(1e-4), loss='mse')\n m.summary()\n x = np.array(np.random.normal(size=(100, 10, 10, 1), loc=0, scale=1))\n y = np.array(np.random.normal(size=(100, 1), loc=0, scale=1))\n m.fit(x=x, y=y)\n \n symbolic_weights = getattr(m.optimizer, 'weights')\n if symbolic_weights:\n     weight_names = []\n     for i, w in enumerate(symbolic_weights):\n         if hasattr(w, 'name') and w.name:\n             name = str(w.name)\n         else:\n             name = 'param_' + str(i)\n         weight_names.append(name.encode('utf8'))\n print(np.array(weight_names).nbytes)\n \n m.save('model')\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "sakvaua", "commentT": "2018-09-10T15:58:28Z", "comment_text": "\n \t\tI could indeed reproduce the issue with this script. Thanks for the detailed report.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "sakvaua", "commentT": "2018-09-10T16:01:06Z", "comment_text": "\n \t\tLinked to issue <denchmark-link:https://github.com/keras-team/keras/issues/6766>#6766</denchmark-link>\n  PR welcome.\n \t\t"}}}, "commit": {"commit_id": "b9ee83cc227ac0719a0de937ae65392473fe007f", "commit_author": "\u00c1lvaro Peris", "commitT": "2018-09-28 11:43:07-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "keras\\engine\\saving.py", "file_new_name": "keras\\engine\\saving.py", "file_complexity": {"file_NLOC": "686", "file_CCN": "145", "file_NToken": "4773"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "37", "deleted_lines": "37", "method_info": {"method_name": "_serialize_model", "method_params": "model,f,include_optimizer", "method_startline": "27", "method_endline": "171", "method_complexity": {"method_NLOC": "93", "method_CCN": "23", "method_NToken": "639", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "keras\\utils\\io_utils.py", "file_new_name": "keras\\utils\\io_utils.py", "file_complexity": {"file_NLOC": "270", "file_CCN": "82", "file_NToken": "1648"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "222", "deleted_lines": "222", "method_info": {"method_name": "__setitem__", "method_params": "self,attr,val", "method_startline": "198", "method_endline": "256", "method_complexity": {"method_NLOC": "45", "method_CCN": "19", "method_NToken": "353", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\keras\\utils\\io_utils_test.py", "file_new_name": "tests\\keras\\utils\\io_utils_test.py", "file_complexity": {"file_NLOC": "135", "file_CCN": "8", "file_NToken": "1180"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "186", "deleted_lines": "186", "method_info": {"method_name": "test_h5dict_groups", "method_params": "", "method_startline": "168", "method_endline": "209", "method_complexity": {"method_NLOC": "28", "method_CCN": "2", "method_NToken": "203", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "153", "deleted_lines": "153", "method_info": {"method_name": "test_h5dict_attrs", "method_params": "", "method_startline": "137", "method_endline": "165", "method_complexity": {"method_NLOC": "16", "method_CCN": "2", "method_NToken": "139", "method_nesting_level": "0"}}}}}}}}