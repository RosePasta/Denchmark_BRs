{"BR": {"BR_id": "305", "BR_author": "ellehoej", "BRopenT": "2018-10-29T13:45:38Z", "BRcloseT": "2018-11-09T19:49:21Z", "BR_text": {"BRsummary": "DeepExplainer doesn't work with keras concatenate layers", "BRdescription": "\n Hi,\n Thanks for some great work.\n I'm getting an error (see below) when using DeepExplainer (SHAP 0.24, TF 1.8, Keras 2.2.4) on a keras model that uses \"concatenate\" through the Keras functional API.\n The model is simply:\n <denchmark-code>x_input = Input(shape=(days_input, n_features_input))\n \n x_d = Flatten()(x_input)\n \n x_d1 = Dense(256, use_bias=False)(x_d)\n x_d1 = BatchNormalization()(x_d1)\n x_dl = PReLU()(x_d1)\n x_d1 = Dropout(0.5)(x_d1)\n \n x_d3 = Dense(128, use_bias=False)(x_d1)\n x_d3 = BatchNormalization()(x_d3)\n x_d3 = PReLU()(x_d3)\n x_d3 = Dropout(0.5)(x_d3)\n \n x_d3 = concatenate([x_d1, x_d3])\n \n dense = Dense(1,activation='sigmoid')\n \n x = dense(x_d3)\n \n model = Model(inputs=x_input, outputs=x)\n model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n </denchmark-code>\n \n Generating the DeepExplainer works:\n <denchmark-code>e = shap.DeepExplainer(model, train_X[:20])\n </denchmark-code>\n \n But when generating shap_values:\n <denchmark-code>shap_values = e.shap_values(test_X)\n </denchmark-code>\n \n I get the following error:\n <denchmark-code>InvalidArgumentError                      Traceback (most recent call last)\n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in get_attr(self, name)\n    2326         with c_api_util.tf_buffer() as buf:\n -> 2327           c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n    2328           data = c_api.TF_GetBuffer(buf)\n \n InvalidArgumentError: Operation 'concatenate_1/concat' has no attr named '_XlaCompile'.\n \n During handling of the above exception, another exception occurred:\n \n ValueError                                Traceback (most recent call last)\n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\n     379     try:\n --> 380       xla_compile = op.get_attr(\"_XlaCompile\")\n     381       xla_separate_compiled_gradients = op.get_attr(\n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in get_attr(self, name)\n    2330         # Convert to ValueError for backwards compatibility.\n -> 2331         raise ValueError(str(e))\n    2332       x = attr_value_pb2.AttrValue()\n \n ValueError: Operation 'concatenate_1/concat' has no attr named '_XlaCompile'.\n \n During handling of the above exception, another exception occurred:\n \n AssertionError                            Traceback (most recent call last)\n <ipython-input-23-9f62380f1037> in <module>()\n ----> 1 shap_values = e.shap_values(test_X)\n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\shap\\explainers\\deep.py in shap_values(self, X, ranked_outputs, output_rank_order)\n     290                 # run attribution computation graph\n     291                 feature_ind = model_output_ranks[j,i]\n --> 292                 sample_phis = self.run(self.phi_symbolic(feature_ind), self.model_inputs, joint_input)\n     293 \n     294                 # assign the attributions to the right part of the output arrays\n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\shap\\explainers\\deep.py in phi_symbolic(self, i)\n     202             try:\n     203                 out = self.model_output[:,i] if self.multi_output else self.model_output\n --> 204                 self.phi_symbolics[i] = tf.gradients(out, self.model_inputs)\n     205 \n     206             finally:\n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\n     492   with ops.get_default_graph()._lock:  # pylint: disable=protected-access\n     493     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n --> 494                             gate_gradients, aggregation_method, stop_gradients)\n     495 \n     496 \n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\n     634                 # functions.\n     635                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n --> 636                                          lambda: grad_fn(op, *out_grads))\n     637               else:\n     638                 # For function call ops, we add a 'SymbolicGradient'\n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\n     383       xla_scope = op.get_attr(\"_XlaScope\").decode()\n     384     except ValueError:\n --> 385       return grad_fn()  # Exit early\n     386 \n     387   if not xla_compile:\n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py in <lambda>()\n     634                 # functions.\n     635                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n --> 636                                          lambda: grad_fn(op, *out_grads))\n     637               else:\n     638                 # For function call ops, we add a 'SymbolicGradient'\n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\shap\\explainers\\deep.py in custom_grad(self, op, *grads)\n     315         \"\"\" Passes a gradient op creation request to the correct handler.\n     316         \"\"\"\n --> 317         return op_handlers[op.type](self, op, *grads)\n     318 \n     319 \n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\shap\\explainers\\deep.py in handler(explainer, op, *grads)\n     515 def linearity_1d(input_ind):\n     516     def handler(explainer, op, *grads):\n --> 517         return linearity_1d_handler(input_ind, explainer, op, *grads)\n     518     return handler\n     519 \n \n C:\\Anaconda\\envs\\keras_updated\\lib\\site-packages\\shap\\explainers\\deep.py in linearity_1d_handler(input_ind, explainer, op, *grads)\n     522     for i in range(len(op.inputs)):\n     523         if i != input_ind:\n --> 524             assert not explainer._variable_inputs(op)[i], str(i) + \"th input to \" + op.name + \" cannot vary!\"\n     525 \n     526     return explainer.orig_grads[op.type](op, *grads)\n \n AssertionError: 1th input to concatenate_1/concat cannot vary!\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ellehoej", "commentT": "2018-11-02T23:42:17Z", "comment_text": "\n \t\tThe 1th input is the second (axis) argument so it looks like you have an axis that depends on your inputs...is that supposed to be true? Typically people don't choose the axis they concat with based on the model inputs.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ellehoej", "commentT": "2018-11-05T09:25:14Z", "comment_text": "\n \t\tI think you misread the notation around  <denchmark-link:https://github.com/slundberg>@slundberg</denchmark-link>\n  .\n I get the same error using the following network:\n <denchmark-code># Define inputs\n input_features = Input(shape=(n_features_input,))\n input_embeddings = Input(shape=(1,))\n \n # Define embeddings\n x_embeddings = Embedding(input_dim=n_stations, output_dim=embedding_size)(input_embeddings)\n x_embeddings = Reshape((embedding_size, ))(x_embeddings)\n \n # Concatenate\n x = Concatenate(axis=1)([input_features, x_embeddings])\n \n # Layers\n x = Dense(512, kernel_initializer='normal', activation='relu')(x)\n x = Dense(256, kernel_initializer='normal', activation='relu')(x)\n output = Dense(1, activation='linear')(x)\n \n # Compile model\n model = Model(\n     inputs=[input_features, input_stations, input_vis_codes], \n     outputs=output)\n model.compile(\n     loss='mean_squared_error', \n     optimizer=keras.optimizers.Adam(lr=learning_rate),\n     metrics=['mse'])\n </denchmark-code>\n \n The final error line (same as <denchmark-link:https://github.com/ellehoej>@ellehoej</denchmark-link>\n  ):\n <denchmark-code>AssertionError: 1th input to concatenate_10/concat cannot vary!\n </denchmark-code>\n \n Is the Concatenate layer just not supported in shap?\n I am using\n \n Tensorflow 1.8 backend\n Keras 2.2.4\n Python 3.6\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ellehoej", "commentT": "2018-11-08T18:31:16Z", "comment_text": "\n \t\tThere must be an issue with the assertion then, causing Concatenate to\n break. I will look into it as soon as I can, it could be that the assertion\n is checking the wrong input.\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Mon, Nov 5, 2018 at 1:25 AM Lasse Regin Nielsen ***@***.***> wrote:\n  I think you misread the notation @slundberg <https://github.com/slundberg>\n  .\n \n  # Define inputs\n  input_features = Input(shape=(n_features_input,))\n  input_embeddings = Input(shape=(1,))\n \n  # Define embeddings\n  x_embeddings = Embedding(input_dim=n_stations, output_dim=embedding_size)(input_embeddings)\n  x_embeddings = Reshape((embedding_size, ))(x_embeddings)\n \n  # Concatenate\n  x = Concatenate(axis=1)([input_features, x_embeddings])\n \n  # Layers\n  x = Dense(512, kernel_initializer='normal', activation='relu')(x)\n  x = Dense(256, kernel_initializer='normal', activation='relu')(x)\n  output = Dense(1, activation='linear')(x)\n \n  # Compile model\n  model = Model(\n      inputs=[input_features, input_stations, input_vis_codes],\n      outputs=output)\n  model.compile(\n      loss='mean_squared_error',\n      optimizer=keras.optimizers.Adam(lr=learning_rate),\n      metrics=['mse'])\n \n  and I get the same error as @ellehoej <https://github.com/ellehoej>\n \n  AssertionError: 1th input to concatenate_10/concat cannot vary!\n \n  Is the Concatenate layer just not supported in shap?\n \n  \u2014\n  You are receiving this because you were mentioned.\n \n \n  Reply to this email directly, view it on GitHub\n  <#305 (comment)>, or mute\n  the thread\n  <https://github.com/notifications/unsubscribe-auth/ADkTxW8ACQmBfb2X8txlMw19ZeaQf1M7ks5usAP8gaJpZM4X_NKx>\n  .\n \n \n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ellehoej", "commentT": "2018-11-08T18:32:24Z", "comment_text": "\n \t\tThank you very much <denchmark-link:https://github.com/slundberg>@slundberg</denchmark-link>\n  ! Let me know if you need anything from me.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ellehoej", "commentT": "2018-11-09T17:06:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LasseRegin>@LasseRegin</denchmark-link>\n  do you happen to already have a complete notebook that demonstrates the problem? If not I can build one.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ellehoej", "commentT": "2018-11-09T17:30:30Z", "comment_text": "\n \t\tI created a new (simplified) Notebook demonstrating the problem. It can be found here:\n <denchmark-link:https://gist.github.com/LasseRegin/a3de5fea9e6c3a499df52973574ed405>https://gist.github.com/LasseRegin/a3de5fea9e6c3a499df52973574ed405</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ellehoej", "commentT": "2018-11-09T17:31:46Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/slundberg>@slundberg</denchmark-link>\n ,\n Unfortunately we are not able to share the data,  but here's another example:\n <denchmark-code>#Define input from training set.\n n_features_input = train_X_features.shape[1]\n x_features = Input(shape=(n_features_input,), name ='Features_input')\n \n \n x_1 = Dense(256, use_bias=False)(x_features)\n x_1 = BatchNormalization()(x_1)\n x_l = PReLU()(x_1)\n x_1 = Dropout(0.5)(x_1)\n \n \n x_2 = Dense(256, use_bias=False)(x_1)\n x_2 = BatchNormalization()(x_2)\n x_2 = PReLU()(x_2)\n x_2 = Dropout(0.5)(x_2)\n \n x_3 = concatenate([x_1, x_2])\n \n dense = Dense(1,activation='sigmoid')\n x = dense(x_3)\n \n model = Model(inputs=x_features, outputs=x)\n \n adam = keras.optimizers.Adam(lr=3e-4)\n model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse'])\n model.summary()\n </denchmark-code>\n \n The model summary is then:\n <denchmark-code>__________________________________________________________________________________________________\n Layer (type)                    Output Shape         Param #     Connected to                     \n ==================================================================================================\n Features_input (InputLayer)     (None, 64)           0                                            \n __________________________________________________________________________________________________\n dense_1 (Dense)                 (None, 256)          16384       Features_input[0][0]             \n __________________________________________________________________________________________________\n batch_normalization_1 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n __________________________________________________________________________________________________\n dropout_1 (Dropout)             (None, 256)          0           batch_normalization_1[0][0]      \n __________________________________________________________________________________________________\n dense_2 (Dense)                 (None, 256)          65536       dropout_1[0][0]                  \n __________________________________________________________________________________________________\n batch_normalization_2 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n __________________________________________________________________________________________________\n p_re_lu_2 (PReLU)               (None, 256)          256         batch_normalization_2[0][0]      \n __________________________________________________________________________________________________\n dropout_2 (Dropout)             (None, 256)          0           p_re_lu_2[0][0]                  \n __________________________________________________________________________________________________\n concatenate_1 (Concatenate)     (None, 512)          0           dropout_1[0][0]                  \n                                                                  dropout_2[0][0]                  \n __________________________________________________________________________________________________\n dense_3 (Dense)                 (None, 1)            513         concatenate_1[0][0]              \n ==================================================================================================\n Total params: 84,737\n Trainable params: 83,713\n Non-trainable params: 1,024\n </denchmark-code>\n \n <denchmark-code>background = SHAP_train_X_scaled[::100]\n explainer = shap.DeepExplainer(model, background)\n X_shap = SHAP_train_X_scaled[::1000].values\n shap_values = explainer.shap_values(X_shap)\n </denchmark-code>\n \n Gives us the error.\n Best regards\n Mads\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ellehoej", "commentT": "2018-11-09T17:33:19Z", "comment_text": "\n \t\tExcellent, <denchmark-link:https://github.com/LasseRegin>@LasseRegin</denchmark-link>\n !\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ellehoej", "commentT": "2018-11-09T19:50:07Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/LasseRegin>@LasseRegin</denchmark-link>\n  for the setup! I found and fixed the problem.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "ellehoej", "commentT": "2018-11-09T21:17:21Z", "comment_text": "\n \t\tThat is great <denchmark-link:https://github.com/slundberg>@slundberg</denchmark-link>\n  ! Thank you very much!\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "ellehoej", "commentT": "2018-11-12T07:47:24Z", "comment_text": "\n \t\tThanks for the quick fix and for a great package in general <denchmark-link:https://github.com/slundberg>@slundberg</denchmark-link>\n  , it works in our end as well now.\n \t\t"}}}, "commit": {"commit_id": "890d85b49228c061a0d46ad76e0bbaa6b506d015", "commit_author": "Scott Lundberg", "commitT": "2018-11-09 11:49:13-08:00", "commit_complexity": {"commit_NLOC": "0.9090909090909091", "commit_CCN": "0.9090909090909091", "commit_Nprams": "0.2727272727272727"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "shap\\__init__.py", "file_new_name": "shap\\__init__.py", "file_complexity": {"file_NLOC": "14", "file_CCN": "0", "file_NToken": "92"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "3", "deleted_lines": "3"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "shap\\explainers\\deep\\__init__.py", "file_new_name": "shap\\explainers\\deep\\__init__.py", "file_complexity": {"file_NLOC": "35", "file_CCN": "7", "file_NToken": "162"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "82", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,model,data,session,learning_phase_flags", "method_startline": "18", "method_endline": "82", "method_complexity": {"method_NLOC": "19", "method_CCN": "6", "method_NToken": "106", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "shap\\explainers\\deep\\deep_tf.py", "file_new_name": "shap\\explainers\\deep\\deep_tf.py", "file_complexity": {"file_NLOC": "401", "file_CCN": "152", "file_NToken": "4094"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "484,485,486,487", "deleted_lines": null, "method_info": {"method_name": "linearity_with_excluded", "method_params": "input_inds", "method_startline": "484", "method_endline": "487", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "489,490,491,492,493,494", "deleted_lines": null, "method_info": {"method_name": "linearity_with_excluded_handler", "method_params": "input_inds,explainer,op,grads", "method_startline": "489", "method_endline": "494", "method_complexity": {"method_NLOC": "5", "method_CCN": "4", "method_NToken": "80", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "485,486", "deleted_lines": null, "method_info": {"method_name": "linearity_with_excluded.handler", "method_params": "explainer,op,grads", "method_startline": "485", "method_endline": "486", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "22", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "478", "deleted_lines": "478", "method_info": {"method_name": "linearity_1d_handler", "method_params": "input_ind,explainer,op,grads", "method_startline": "477", "method_endline": "482", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "69", "method_nesting_level": "0"}}}}}}}}