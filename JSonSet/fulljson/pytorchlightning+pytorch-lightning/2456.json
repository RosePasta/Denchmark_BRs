{"BR": {"BR_id": "2456", "BR_author": "griff4692", "BRopenT": "2020-07-01T21:45:01Z", "BRcloseT": "2020-07-02T11:04:19Z", "BR_text": {"BRsummary": "multi-gpu training triggers CUDA out of memory error", "BRdescription": "\n Hi -\n I am running into issues when going from single to multi-gpu training.  Specifically, if I switch the line\n pl.Trainer(gpus=1, precision=16, distributed_backend='ddp')\n to\n pl.Trainer(gpus=4, precision=16, distributed_backend='ddp')\n I get the dreaded CUDA out of memory error.  Is there any reason why the parallelism causes the GPU to receive more data?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "griff4692", "commentT": "2020-07-01T21:46:18Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "griff4692", "commentT": "2020-07-02T06:34:56Z", "comment_text": "\n \t\tHi, what are your outputs of the validation_step? If there are any large tensors, it's likely they get synced back to root GPU by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2434>#2434</denchmark-link>\n  . We're working on that.\n cc <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n   ^^\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "griff4692", "commentT": "2020-07-02T12:27:05Z", "comment_text": "\n \t\tHi - I actually haven't implemented the validation step yet.  this just occurs on the training side\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "griff4692", "commentT": "2020-07-02T13:04:39Z", "comment_text": "\n \t\twhat is your gpu consumption on a single gpu (used/available)?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "griff4692", "commentT": "2020-07-02T13:34:42Z", "comment_text": "\n \t\tOn single gpu, I am using 5/11 GB.  The problem seems to be that when I switch over to multiple GPUs, there is an explosion of processes created on the first GPU.  Any ideas what could be causing this?\n <denchmark-link:https://user-images.githubusercontent.com/12277915/86365204-32405c00-bc47-11ea-9211-0e6ccfb57f8d.png></denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "griff4692", "commentT": "2020-07-02T13:39:04Z", "comment_text": "\n \t\tFixed it!.  I was calling .to('cuda') on my input tensors in my Dataset __get__item function which caused all the data to be uploaded to the first GPU.  Removed that and solved the problem.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "griff4692", "commentT": "2020-07-02T13:45:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>\n  does that mean we should add back the all reduce for val?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "griff4692", "commentT": "2020-07-02T15:17:56Z", "comment_text": "\n \t\tNo, there were other issues with that as well :D Let's just keep it out for now.\n \t\t"}}}, "commit": {"commit_id": "afdfba1dc6061c5e1ee6eaf215500d6a56e95482", "commit_author": "William Falcon", "commitT": "2020-07-02 07:04:18-04:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_new_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_complexity": {"file_NLOC": "336", "file_CCN": "39", "file_NToken": "1239"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "357,358,359,360,361,362,363,364,365,366,367,368", "method_info": {"method_name": "reduce_eval_ddp", "method_params": "self,eval_results", "method_startline": "357", "method_endline": "368", "method_complexity": {"method_NLOC": "10", "method_CCN": "6", "method_NToken": "82", "method_nesting_level": "1"}}}}}}}}