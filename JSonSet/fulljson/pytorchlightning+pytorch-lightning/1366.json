{"BR": {"BR_id": "1366", "BR_author": "belskikh", "BRopenT": "2020-04-03T16:01:47Z", "BRcloseT": "2020-04-24T21:21:01Z", "BR_text": {"BRsummary": "ModelCheckpoint tries to remove already removed checkpoint in DDP mode", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When training in DDP mode with ModelCheckpoint callback, the train process fails, when ModelCheckpoint callback tries to remove previous checkpoint. I assume that it was already deleted by another process.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n Run training with \"ddp\" backend and ModelCheckpoint callback with save_top_k={some_number}\n <denchmark-code>  File \"/home/myuser/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap                                                                                                 \n     fn(i, *args)                                                                                \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 342, in ddp_train\n     self.run_pretrain_routine(model)                                                                                                        \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 830, in run_pretrain_routine\n     self.train()                                                                                                                                                                                                   \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 343, in train                                                      \n     self.run_training_epoch()                            \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch                                                                       \n     self.call_checkpoint_callback()                                                                                                                                                   \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 737, in call_checkpoint_callback\n     self.checkpoint_callback.on_validation_end(self, self.get_model())                                                                                                                                             \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 204, in on_validation_end                                      \n     self._do_check_save(filepath, current, epoch)    \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 221, in _do_check_save                                                                      \n     self._del_model(delpath)                                                                                                                                                           \n   File \"/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 121, in _del_model\n     os.remove(filepath)                                                                                                                                                                                            \n FileNotFoundError: [Errno 2] No such file or directory: {PREVIOUS_CHECKPOINT_NAME}\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n I expect that ModelCheckpoint callbacks from different DDP processes will not concurrent with each other in saving/deleteng files.\n I fixed by rewriting _del_model method of ModelCheckpoint callback:\n <denchmark-code>class DDPModelCheckpoint(ModelCheckpoint):\n     def _del_model(self, filepath):\n         try:\n             os.remove(filepath)\n         except Exception:\n             pass\n </denchmark-code>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version: 1.4\n OS: Ubuntu 18.04\n How you installed PyTorch - conda\n Python version: 3.7\n CUDA/cuDNN version: 10.2\n GPU models and configuration: 2x2080Ti\n pytorch-lightning version: 0.7.1\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "belskikh", "commentT": "2020-04-03T16:02:40Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "belskikh", "commentT": "2020-04-04T08:16:38Z", "comment_text": "\n \t\tYour suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply\n <denchmark-code>if os.path.isfile(filepath):\n     os.remove(filepath)\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "belskikh", "commentT": "2020-04-04T08:18:29Z", "comment_text": "\n \t\tIs there also an issue with saving? Does it save/overwrite the file in multiple processes?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "belskikh", "commentT": "2020-04-06T07:34:57Z", "comment_text": "\n \t\tI encountered that one too. From my perspective, model updates should be happening within main worker only (master worker). However, I guess each workers lightning created is trying to delete their own checkpoints. However, slave worker never created one (and it shouldn't be). I solved the problem in a similar way with <denchmark-link:https://github.com/belskikh>@belskikh</denchmark-link>\n  's workaround but it did not feel right and downgraded to 0.6.0.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "belskikh", "commentT": "2020-04-06T09:47:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  of course it is not the best (it may be the worse, actually) solution\n It is just a fast workaround, waiting for solid fix\n I agree with logic, when only one ModelCheckpoint callback should save/delete weights, because all weights are the same on all nodes at the end of the training step.\n It can be done somehow like this:\n <denchmark-code>class ModelCheckpoint(..., main_worker_rank: int = 0):\n ....\n     def _del_model(self, filepath):\n         if self.main_worker_rank == dist.get_rank():\n              # do delete\n </denchmark-code>\n \n And the same for the saving code.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "belskikh", "commentT": "2020-04-06T10:00:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  should Lightning do checkpoints only on rank 0? It could be a problem if writing to a shared filesystem between nodes. AFAIK the loggers already do that by only logging in process 0.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "belskikh", "commentT": "2020-04-06T11:20:09Z", "comment_text": "\n \t\tI do not think, that it should do it only on specific rank, I think user should have ability to specify rank (node), where checkpoints will be saved\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "belskikh", "commentT": "2020-04-07T19:23:25Z", "comment_text": "\n \t\t\n Your suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply\n if os.path.isfile(filepath):\n     os.remove(filepath)\n \n \n this does not work in async, I have observed many times that the if pass for both but then when you try really delete it, it is missing for one of them...\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "belskikh", "commentT": "2020-04-07T19:28:48Z", "comment_text": "\n \t\tshould checkpointing be done in only one process then, like loggers?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "belskikh", "commentT": "2020-04-07T20:11:15Z", "comment_text": "\n \t\tyup. checkpoint should only happen from world_rank = 0 gpu 0\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "belskikh", "commentT": "2020-04-07T21:01:26Z", "comment_text": "\n \t\twell, ModelCeckpoint doesn't have a rank...\n \t\t"}}}, "commit": {"commit_id": "58a467dd68b157fdba8824a437dbaf698ad88569", "commit_author": "Jirka Borovec", "commitT": "2020-04-24 17:21:00-04:00", "commit_complexity": {"commit_NLOC": "0.7586206896551724", "commit_CCN": "0.20689655172413793", "commit_Nprams": "0.1724137931034483"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "25,26,27,28,40,45,46,84,85,102", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_new_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_complexity": {"file_NLOC": "204", "file_CCN": "33", "file_NToken": "1016"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "133,134", "deleted_lines": "132", "method_info": {"method_name": "_del_model", "method_params": "self,filepath", "method_startline": "132", "method_endline": "134", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "23", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\__init__.py", "file_new_name": "pytorch_lightning\\loggers\\__init__.py", "file_complexity": {"file_NLOC": "126", "file_CCN": "0", "file_NToken": "175"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "33,34,84", "deleted_lines": "33,83"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "pytorch_lightning\\loggers\\base.py", "file_new_name": "pytorch_lightning\\loggers\\base.py", "file_complexity": {"file_NLOC": "257", "file_CCN": "55", "file_NToken": "1338"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "255,256,257", "method_info": {"method_name": "rank", "method_params": "self", "method_startline": "255", "method_endline": "257", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "12", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "13,14,15,16,17,18,19,20,21,22,23,24,25", "method_info": {"method_name": "rank_zero_only", "method_params": "Callable", "method_startline": "13", "method_endline": "25", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": null, "deleted_lines": "21,22,23", "method_info": {"method_name": "rank_zero_only.wrapped_fn", "method_params": "self,args,kwargs", "method_startline": "21", "method_endline": "23", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "28", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": null, "deleted_lines": "260,261,262", "method_info": {"method_name": "rank", "method_params": "self,int", "method_startline": "260", "method_endline": "262", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\comet.py", "file_new_name": "pytorch_lightning\\loggers\\comet.py", "file_complexity": {"file_NLOC": "190", "file_CCN": "12", "file_NToken": "692"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27,29", "deleted_lines": "27"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\mlflow.py", "file_new_name": "pytorch_lightning\\loggers\\mlflow.py", "file_complexity": {"file_NLOC": "115", "file_CCN": "14", "file_NToken": "487"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "18,19", "deleted_lines": "18"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\neptune.py", "file_new_name": "pytorch_lightning\\loggers\\neptune.py", "file_complexity": {"file_NLOC": "341", "file_CCN": "20", "file_NToken": "898"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "21,22", "deleted_lines": "21"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\tensorboard.py", "file_new_name": "pytorch_lightning\\loggers\\tensorboard.py", "file_complexity": {"file_NLOC": "154", "file_CCN": "27", "file_NToken": "816"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "17,18", "deleted_lines": "16"}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\loggers\\test_tube.py", "file_new_name": "pytorch_lightning\\loggers\\test_tube.py", "file_complexity": {"file_NLOC": "138", "file_CCN": "15", "file_NToken": "582"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "138,139", "method_info": {"method_name": "rank", "method_params": "self", "method_startline": "138", "method_endline": "139", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "11", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "142,143,144,145", "method_info": {"method_name": "rank", "method_params": "self,int", "method_startline": "142", "method_endline": "145", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "31", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "96", "deleted_lines": "95", "method_info": {"method_name": "experiment", "method_params": "self", "method_startline": "75", "method_endline": "98", "method_complexity": {"method_NLOC": "23", "method_CCN": "2", "method_NToken": "74", "method_nesting_level": "1"}}}}}, "file_9": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\trains.py", "file_new_name": "pytorch_lightning\\loggers\\trains.py", "file_complexity": {"file_NLOC": "348", "file_CCN": "43", "file_NToken": "1232"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,23", "deleted_lines": "22"}}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\wandb.py", "file_new_name": "pytorch_lightning\\loggers\\wandb.py", "file_complexity": {"file_NLOC": "116", "file_CCN": "15", "file_NToken": "550"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "18,19", "deleted_lines": "18"}}}, "file_11": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_complexity": {"file_NLOC": "322", "file_CCN": "69", "file_NToken": "1249"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "328,329", "deleted_lines": "325,326,328,329,330", "method_info": {"method_name": "ddp_train", "method_params": "self,process_idx,model", "method_startline": "298", "method_endline": "373", "method_complexity": {"method_NLOC": "36", "method_CCN": "13", "method_NToken": "294", "method_nesting_level": "1"}}}}}, "file_12": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\distrib_parts.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_parts.py", "file_complexity": {"file_NLOC": "597", "file_CCN": "87", "file_NToken": "1752"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "509", "deleted_lines": "509", "method_info": {"method_name": "tpu_train", "method_params": "self,tpu_core_idx,model", "method_startline": "494", "method_endline": "525", "method_complexity": {"method_NLOC": "16", "method_CCN": "4", "method_NToken": "134", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "612", "deleted_lines": "612,613,614,615,616", "method_info": {"method_name": "horovod_train", "method_params": "self,model", "method_startline": "567", "method_endline": "619", "method_complexity": {"method_NLOC": "27", "method_CCN": "10", "method_NToken": "243", "method_nesting_level": "1"}}}}}, "file_13": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\logging.py", "file_new_name": "pytorch_lightning\\trainer\\logging.py", "file_complexity": {"file_NLOC": "117", "file_CCN": "46", "file_NToken": "775"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "36,44", "method_info": {"method_name": "configure_logger", "method_params": "self,logger", "method_startline": "28", "method_endline": "44", "method_complexity": {"method_NLOC": "16", "method_CCN": "4", "method_NToken": "83", "method_nesting_level": "1"}}}}}, "file_14": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\utilities\\__init__.py", "file_new_name": "pytorch_lightning\\utilities\\__init__.py", "file_complexity": {"file_NLOC": "2", "file_CCN": "0", "file_NToken": "11"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "3", "deleted_lines": "3"}}}, "file_15": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "pytorch_lightning\\utilities\\distributed.py", "file_complexity": {"file_NLOC": "15", "file_CCN": "4", "file_NToken": "87"}}, "file_16": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\utilities\\warnings.py", "file_new_name": "None", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_17": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\loggers\\test_base.py", "file_new_name": "tests\\loggers\\test_base.py", "file_complexity": {"file_NLOC": "136", "file_CCN": "22", "file_NToken": "858"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8,9", "deleted_lines": "8"}}}}}}