{"BR": {"BR_id": "1751", "BR_author": "marcopodda", "BRopenT": "2020-05-07T06:12:51Z", "BRcloseT": "2020-05-26T23:06:06Z", "BR_text": {"BRsummary": "Early Stopping behavior", "BRdescription": "\n Hi there,\n thanks for the great library (I am using 0.7.5.). I am not following the bug report template as I'm not sure this is indeed a bug, or simply I cannot understand how early stopping is implemented. My code looks as follows:\n <denchmark-code>    early_stop_callback = EarlyStopping(\n         monitor='val_acc',\n         min_delta=0.0,\n         patience=80,\n         verbose=True,\n         mode=self.mode\n     )\n \n     trainer = Trainer(\n         early_stop_callback=early_stop_callback,\n         auto_select_gpus=True,\n         max_epochs=200,\n         terminate_on_nan=True,\n         show_progress_bar=True,\n         fast_dev_run=False,\n         gpus=1\n     )\n </denchmark-code>\n \n As I understand it, the model should perform early stopping after AT LEAST 80 epochs have passed without improvement on the validation accuracy. However, in my case, early stopping happened at epoch 75. Is this how it should be?\n As I said, I am not sure this is actually a bug or a choice (perhaps early stopping is implemented at the batch level?). If it is indeed a bug, I will work a reproducible example. Thank you!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "marcopodda", "commentT": "2020-05-07T06:13:44Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "marcopodda", "commentT": "2020-05-07T08:07:26Z", "comment_text": "\n \t\tI would expect that it should iterate for at least 80 epochs, too. So to me, it looks like a bug or some kind of unexpected behavior. Would be great to figure it out!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "marcopodda", "commentT": "2020-05-07T09:54:37Z", "comment_text": "\n \t\tOk then, I'll work out some notebook to see if I can reproduce.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "marcopodda", "commentT": "2020-05-07T14:16:58Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/mateuszpieniak>@mateuszpieniak</denchmark-link>\n \n Here is a working example. As you can see, it stops at epoch 41 even though patience is set to 80.\n <denchmark-link:https://github.com/marcopodda/pl-es-example/blob/master/ES%20example.ipynb>https://github.com/marcopodda/pl-es-example/blob/master/ES%20example.ipynb</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "marcopodda", "commentT": "2020-05-07T16:00:54Z", "comment_text": "\n \t\tIt is definitely a bug. I discovered that EarlyStopping.on_epoch_end is executed twice within one epoch, meaning that patience=160 should solve your issue temporarily.\n In the file training_loop.py:\n First call:\n <denchmark-code>            if self.fast_dev_run or should_check_val:\n                 self.run_evaluation(test_mode=self.testing)\n                 self.call_checkpoint_callback()\n                 self.call_early_stop_callback()\n </denchmark-code>\n \n Second call:\n <denchmark-code>                # TODO wrap this logic into the callback\n                 if self.enable_early_stop:\n                     if (met_min_epochs and met_min_steps) or self.fast_dev_run:\n                         should_stop = self.early_stop_callback.on_epoch_end(self, self.get_model())\n                         # stop training\n                         stop = should_stop and met_min_epochs\n                         if stop:\n                             self.run_training_teardown()\n                             return\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "marcopodda", "commentT": "2020-05-08T09:45:12Z", "comment_text": "\n \t\tI upgraded to the bleeding edge version yesterday and can confirm that this started happening to me too. I didn't have an issue before I upgraded (I think I was on 0.7.3 before?)\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "marcopodda", "commentT": "2020-05-10T23:57:11Z", "comment_text": "\n \t\tYep we ran into this as well. It is called once in trainer and once in the on epoch end callback.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "marcopodda", "commentT": "2020-05-11T20:45:55Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>\n  <denchmark-link:https://github.com/ricpruss>@ricpruss</denchmark-link>\n  mind send a fix, PR?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "marcopodda", "commentT": "2020-05-11T21:44:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  Well, I would love to make my first PL's PR if that's okay? \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "marcopodda", "commentT": "2020-05-11T21:48:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mateuszpieniak>@mateuszpieniak</denchmark-link>\n  sure go ahead! \n \t\t"}}}, "commit": {"commit_id": "3af4994d5a84bc80738b50983b4b42c3eb946433", "commit_author": "Mateusz Pieniak", "commitT": "2020-05-26 19:06:06-04:00", "commit_complexity": {"commit_NLOC": "0.4", "commit_CCN": "0.4", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "41,42", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "536", "file_CCN": "127", "file_NToken": "2636"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "794,795,796", "method_info": {"method_name": "call_early_stop_callback", "method_params": "self", "method_startline": "794", "method_endline": "796", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "24", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "455,501", "method_info": {"method_name": "run_training_epoch", "method_params": "self", "method_startline": "380", "method_endline": "509", "method_complexity": {"method_NLOC": "68", "method_CCN": "35", "method_NToken": "562", "method_nesting_level": "1"}}}}}}}}