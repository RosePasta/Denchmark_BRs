{"BR": {"BR_id": "2315", "BR_author": "elias-ramzi", "BRopenT": "2020-06-22T13:25:26Z", "BRcloseT": "2020-06-26T13:33:36Z", "BR_text": {"BRsummary": "Bug in average_precision Metric", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Hi everyone, I encountered a bug when using the average_precision metric (pytorch_lightning.metrics.functional.classification). It yields incorrect results (negative ones).\n There seems to be a missing parenthesis in the code here :\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/functional/classification.py#L847>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/functional/classification.py#L847</denchmark-link>\n \n It works when corrected as :\n return -torch.sum((recall[1:] - recall[:-1]) * precision[:-1])\n In order to reproduce negative results :\n <denchmark-code>import torch\n import pytorch_lightning.metrics.functional.classification as M\n \n torch.manual_seed(23)\n truth = (torch.rand(100) > .6)\n pred = torch.rand(100)\n \n M.average_precision(pred, truth)\n </denchmark-code>\n \n I did not find an issue on this topic yet. If needed I can submit a PR.\n Thanks \u263a\ufe0f\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "elias-ramzi", "commentT": "2020-06-22T13:26:06Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "elias-ramzi", "commentT": "2020-06-22T14:58:38Z", "comment_text": "\n \t\tI'd like to fix it\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "elias-ramzi", "commentT": "2020-06-22T18:22:07Z", "comment_text": "\n \t\tHi, I have already created a branch for the PR !\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "elias-ramzi", "commentT": "2020-06-22T20:10:01Z", "comment_text": "\n \t\tThanks for the issue and the fix! <denchmark-link:https://github.com/InCogNiTo124>@InCogNiTo124</denchmark-link>\n  Just saying- there are many other open issues if you want to take a stab at :)\n \t\t"}}}, "commit": {"commit_id": "92f122e0df7e233f3a8b7873c7294155afbbf852", "commit_author": "elias-ramzi", "commitT": "2020-06-23 13:21:00+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23,24", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\metrics\\functional\\classification.py", "file_new_name": "pytorch_lightning\\metrics\\functional\\classification.py", "file_complexity": {"file_NLOC": "786", "file_CCN": "33", "file_NToken": "3087"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "847", "deleted_lines": "847"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\metrics\\functional\\test_classification.py", "file_new_name": "tests\\metrics\\functional\\test_classification.py", "file_complexity": {"file_NLOC": "301", "file_CCN": "22", "file_NToken": "4583"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "345,352,353,354,355,356", "deleted_lines": "345,348,350,351,353,356", "method_info": {"method_name": "test_average_precision_constant_values", "method_params": "", "method_startline": "345", "method_endline": "356", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "47", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "356,357", "deleted_lines": "356", "method_info": {"method_name": "test_average_precision", "method_params": "scores,target,expected_score", "method_startline": "356", "method_endline": "357", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "18", "method_nesting_level": "0"}}}}}}}}