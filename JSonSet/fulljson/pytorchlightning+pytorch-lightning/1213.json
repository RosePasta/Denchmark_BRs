{"BR": {"BR_id": "1213", "BR_author": "Ir1d", "BRopenT": "2020-03-23T11:26:40Z", "BRcloseT": "2020-07-10T01:27:31Z", "BR_text": {"BRsummary": "Testing in dp mode uses only one of the GPUs", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n Run a test without training\n <denchmark-link:https://user-images.githubusercontent.com/10709657/77311956-1706ac00-6d3c-11ea-89fe-3cf2156babe2.png></denchmark-link>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n Modified from the conference-seed repo\n trainer = Trainer(\n             gpus=\"-1\",\n             distributed_backend='dp',\n         )\n trainer.test(model)\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n \n pl version: 0.6.0\n PyTorch Version (e.g., 1.0): 1.2\n OS (e.g., Linux): Ubuntu\n How you installed PyTorch (conda, pip, source): pip\n Build command you used (if compiling from source):\n Python version: 3.6\n CUDA/cuDNN version: 10.1\n GPU models and configuration:\n Any other relevant information:\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Ir1d", "commentT": "2020-03-23T11:29:51Z", "comment_text": "\n \t\tummm yeah, that's a bug. it should run via dp. <denchmark-link:https://github.com/Ir1d>@Ir1d</denchmark-link>\n  want to submit a PR?\n <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>\n  any one else experience this?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Ir1d", "commentT": "2020-03-23T11:31:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  I tried wrapping the model in  like the training loop did, but it tells me that LightningDataParallel object doen't have a . How do I debug this?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Ir1d", "commentT": "2020-03-23T11:40:28Z", "comment_text": "\n \t\tyou wouldn\u2019t wrap it yourself ever haha.\n the trainer does the wrapping for you.\n the trainer needs to be modified to run the test on the correct method when done this way\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Ir1d", "commentT": "2020-03-23T11:46:52Z", "comment_text": "\n \t\tI was trying to wrap it in evaluate in pytorch_lightning/trainer/evaluation_loop.py . Do you have any idea where to wrap this func?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Ir1d", "commentT": "2020-03-23T11:48:20Z", "comment_text": "\n \t\tAnyway, we've find one possible workround here:\n After defining a torch model, and before sending it into PL model, wrap it with nn.dataparallel.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Ir1d", "commentT": "2020-03-23T12:09:23Z", "comment_text": "\n \t\tevaluate is private... you're not meant to call it directly.\n call .test()\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "Ir1d", "commentT": "2020-03-23T12:15:31Z", "comment_text": "\n \t\tlightning does the wrapping by itself...\n the fact that this doesn't work, is a bug.\n model = MyLightningModule.load_from_checkpoint(...)\n trainer = Trainer(\n             gpus=\"-1\",\n             distributed_backend='dp',\n         )\n trainer.test(model)\n The bug needs to be addressed correctly.\n It's weird because we have tests for this... double check that this is really not working for you.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "Ir1d", "commentT": "2020-03-23T12:38:02Z", "comment_text": "\n \t\t\n evaluate is private... you're not meant to call it directly.\n call .test()\n \n so let's rename it starting with _ to be clear that it is private from it name\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "Ir1d", "commentT": "2020-03-23T12:41:39Z", "comment_text": "\n \t\tI was calling .test and its not working\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "Ir1d", "commentT": "2020-03-27T12:30:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/neggert>@neggert</denchmark-link>\n  may you have look at this multi GPU issue?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "Ir1d", "commentT": "2020-06-08T12:41:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/neggert>@neggert</denchmark-link>\n  ping :)\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "Ir1d", "commentT": "2020-06-26T13:46:56Z", "comment_text": "\n \t\tlooking at this with next sprint\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "Ir1d", "commentT": "2020-07-10T01:27:31Z", "comment_text": "\n \t\tfixed! (0.8.5)\n \t\t"}}}, "commit": {"commit_id": "c869dd8b8f6301f3726df84535a3da4e9acf04ec", "commit_author": "Jirka Borovec", "commitT": "2020-03-30 12:14:27-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23", "deleted_lines": "23"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_new_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_complexity": {"file_NLOC": "318", "file_CCN": "57", "file_NToken": "1203"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "220", "deleted_lines": "220", "method_info": {"method_name": "_evaluate", "method_params": "self,LightningModule,dataloaders,int,bool", "method_startline": "220", "method_endline": "320", "method_complexity": {"method_NLOC": "56", "method_CCN": "19", "method_NToken": "409", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "368", "deleted_lines": "368", "method_info": {"method_name": "run_evaluation", "method_params": "self,bool", "method_startline": "322", "method_endline": "404", "method_complexity": {"method_NLOC": "50", "method_CCN": "17", "method_NToken": "335", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "220", "deleted_lines": "220", "method_info": {"method_name": "evaluate", "method_params": "self,LightningModule,dataloaders,int,bool", "method_startline": "220", "method_endline": "320", "method_complexity": {"method_NLOC": "56", "method_CCN": "19", "method_NToken": "409", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "746", "file_CCN": "70", "file_NToken": "3197"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "896,897,898,899", "deleted_lines": "896,897,898,899", "method_info": {"method_name": "run_pretrain_routine", "method_params": "self,LightningModule", "method_startline": "817", "method_endline": "920", "method_complexity": {"method_NLOC": "54", "method_CCN": "19", "method_NToken": "410", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\test_deprecated.py", "file_new_name": "tests\\test_deprecated.py", "file_complexity": {"file_NLOC": "73", "file_CCN": "12", "file_NToken": "564"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "98,109", "deleted_lines": "98,109", "method_info": {"method_name": "test_tbd_remove_in_v1_0_0_model_hooks", "method_params": "", "method_startline": "87", "method_endline": "110", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "151", "method_nesting_level": "0"}}}}}}}}