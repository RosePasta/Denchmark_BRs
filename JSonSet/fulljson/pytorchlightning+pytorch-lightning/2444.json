{"BR": {"BR_id": "2444", "BR_author": "NumesSanguis", "BRopenT": "2020-07-01T08:42:24Z", "BRcloseT": "2020-08-28T07:07:44Z", "BR_text": {"BRsummary": "self.hparam silently removes params that are not serializable", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Following the approach found under <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html#lightningmodule-hyperparameters>hyperparameters in the docs</denchmark-link>\n , step 3, I passed a dict with parameters to my .\n In the  printing  shows all contents I passed.\n However, in the function , some hparams are gone.\n This might be related to that not all param values are YAML serializable, and therefore automatically removed? Because the 2 removed params are \"criterion\": torch.nn.BCELoss() and \"optimizer\": partial(optim.Adam, lr=0.001).\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Run the following script:\n \n from functools import partial\n import torch\n import torch.optim as optim\n from torch.utils.data import Dataset\n import pytorch_lightning as pl\n from pytorch_lightning import Trainer\n \n # partial to give all params, except the data\n hparams = {\n     \"criterion\": torch.nn.BCELoss(),  # F.cross_entropy(),  # loss function\n     \"optimizer\": partial(optim.Adam, lr=0.001),  # (lr=0.001),\n     # \"learning_rate\": 0.001,\n     \"filters\": 64,\n     \"layers\": 2\n }\n \n class EmptyDataset(Dataset):\n     def __init__(self, transform=None):\n         pass\n     \n     def __len__(self):\n         return 32\n     \n     def __getitem__(self, idx):\n         return {\"input\": np.array([1]), \"output\": \"nothing\"}\n \n class LitLake(pl.LightningModule):\n     def __init__(self, hparams: dict, transforms: dict = None):\n         super().__init__()\n         \n         self.hparams = hparams\n         print(\"self.hparams\\n\", self.hparams)\n         \n     def forward(self, x):\n         pass\n     \n     def training_step(self, batch, batch_idx):\n         \"\"\"\n         Lightning calls this inside the training loop with the data from the training dataloader\n         passed in as `batch`.\n         \"\"\"\n         # forward pass\n         x, y = batch\n         y_hat = self(x)\n         loss = self.hparams[\"criterion\"](y_hat, y)\n         tensorboard_logs = {'train_loss': loss}\n         return {'loss': loss, 'log': tensorboard_logs}\n         \n     def configure_optimizers(self):\n         print(\"self.hparams\\n\", self.hparams)\n         optimizer = self.hparams[\"optimizer\"](self.parameters())\n         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n         return [optimizer], [scheduler]\n     \n     def train_dataloader(self):\n          return DataLoader(EmptyDataset(), batch_size=4, num_workers=1)\n \n model = LitLake(hparams=hparams)\n # most basic trainer, uses good defaults\n trainer = Trainer()  # gpus=1, num_nodes=1\n trainer.fit(model)  # KeyError: 'optimizer'\n \n See error\n \n Script output (CLICK ME)\n \n self.hparams\n  \"criterion\": BCELoss()\n \"filters\":   64\n \"layers\":    2\n \"optimizer\": functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001)\n GPU available: True, used: False\n TPU available: False, using: 0 TPU cores\n self.hparams\n  \"filters\": 64\n \"layers\":  2\n Traceback (most recent call last):\n   File \"lightning_hparams_bug.py\", line 61, in <module>\n     trainer.fit(model)  # KeyError: 'optimizer'\n   File \"/home/*user*/anaconda3/envs/onseilake/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 965, in fit\n     self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n   File \"/home/*user*/anaconda3/envs/onseilake/lib/python3.7/site-packages/pytorch_lightning/trainer/optimizers.py\", line 18, in init_optimizers\n     optim_conf = model.configure_optimizers()\n   File \"lightning_hparams_bug.py\", line 51, in configure_optimizers\n     optimizer = self.hparams[\"optimizer\"](self.parameters())\n KeyError: 'optimizer'\n \n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Either:\n \n self.hparams keeping the non-serializable parameters (will give problems with loading?)\n Throw an error explaining why those param values are not acceptable and how to approach it, instead of silently removing them.\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version (e.g., 1.0): 1.5.0\n OS (e.g., Linux): Ubuntu 18.04\n How you installed PyTorch (conda, pip, source): conda\n Build command you used (if compiling from source): -\n Python version: 3.7.6\n CUDA/cuDNN version: 10.2\n GPU models and configuration: 1x GeForce GTX 1080 Ti\n Any other relevant information: -\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n Bug reproduced by <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "NumesSanguis", "commentT": "2020-07-01T19:13:22Z", "comment_text": "\n \t\tHappens to me too. I'm also waiting for this fix, since i really want to use the new feature to put anything in hparams.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "NumesSanguis", "commentT": "2020-07-02T16:33:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dscarmo>@dscarmo</denchmark-link>\n  you can take it over and send a PR :] or I ll check it tomorrow...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "NumesSanguis", "commentT": "2020-08-04T19:29:05Z", "comment_text": "\n \t\tNeed to add warning about this.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "NumesSanguis", "commentT": "2020-08-18T23:14:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  this should be fixed with PR, right?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "NumesSanguis", "commentT": "2020-08-18T23:19:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>\n  yes the linked PR solves this.\n \t\t"}}}, "commit": {"commit_id": "d5254ff9dfb67fba388de224a320f3a562561a80", "commit_author": "monney", "commitT": "2020-08-28 09:07:43+02:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.6"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\utilities\\__init__.py", "file_new_name": "pytorch_lightning\\utilities\\__init__.py", "file_complexity": {"file_NLOC": "20", "file_CCN": "0", "file_NToken": "127"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9", "deleted_lines": "9"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\utilities\\parsing.py", "file_new_name": "pytorch_lightning\\utilities\\parsing.py", "file_complexity": {"file_NLOC": "141", "file_CCN": "41", "file_NToken": "792"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "56,58,60,61,62,63,64,65,66", "deleted_lines": "55,56,57,58,59,60,61", "method_info": {"method_name": "clean_namespace", "method_params": "hparams", "method_startline": "55", "method_endline": "66", "method_complexity": {"method_NLOC": "8", "method_CCN": "5", "method_NToken": "60", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "45,46,47,48,49,50,51,52", "deleted_lines": "46,47,48,49,50,51,52", "method_info": {"method_name": "is_picklable", "method_params": "object", "method_startline": "45", "method_endline": "52", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "27", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\models\\test_hparams.py", "file_new_name": "tests\\models\\test_hparams.py", "file_complexity": {"file_NLOC": "318", "file_CCN": "55", "file_NToken": "2584"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "425,426,427,428,429,430", "deleted_lines": null, "method_info": {"method_name": "test_hparams_pickle_warning", "method_params": "tmpdir", "method_startline": "425", "method_endline": "430", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "47", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "285", "deleted_lines": "285", "method_info": {"method_name": "test_collect_init_arguments", "method_params": "tmpdir,cls", "method_startline": "251", "method_endline": "293", "method_complexity": {"method_NLOC": "29", "method_CCN": "7", "method_NToken": "272", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "419,420,421,422", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,foo,pickle_me", "method_startline": "419", "method_endline": "422", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "1"}}}}}}}}