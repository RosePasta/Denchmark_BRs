{"BR": {"BR_id": "850", "BR_author": "hjalmarlucius", "BRopenT": "2020-02-15T01:26:14Z", "BRcloseT": "2020-02-22T01:27:20Z", "BR_text": {"BRsummary": "Epoch end checkpoint restarts previous epoch", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n If restarting the training and reloading the model, the epoch that the checkpoint had just completed is restarted rather than beginning the next.\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n When a checkpoint upon epoch end is saved, restarting it should resume its state and start the next epoch.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hjalmarlucius", "commentT": "2020-02-16T11:25:27Z", "comment_text": "\n \t\t\n \n \n pytorch-lightning/pytorch_lightning/trainer/training_io.py\n \n \n          Line 389\n       in\n       edd4a87\n \n \n \n \n \n \n  self.current_epoch = checkpoint['epoch'] \n \n \n \n \n \n This seems as simple as replacing the line above with self.current_epoch = checkpoint['epoch'] + 1 since the checkpointers save at the end of validation and the main loop runs from the current epoch.\n We should probably also increase the global step by 1 since this happens after saving the checkpoint.\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_io.py\n \n \n          Line 388\n       in\n       edd4a87\n \n \n \n \n \n \n  self.global_step = checkpoint['global_step'] \n \n \n \n \n \n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n          Line 411\n       in\n       edd4a87\n \n \n \n \n \n \n  self.run_evaluation(test=self.testing) \n \n \n \n \n \n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n          Line 430\n       in\n       edd4a87\n \n \n \n \n \n \n  self.global_step += 1 \n \n \n \n \n \n I'll add a test in whilst I do it.\n <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  Any other thoughts if I put a PR in with this? The test should presumably go in ?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hjalmarlucius", "commentT": "2020-02-16T15:47:36Z", "comment_text": "\n \t\tThat works when saving at epoch end but there's many cases of saving during an epoch as well (e.g. for a very large dataset). Both the epoch number and the global step are technically correct upon saving mid or end epoch but when resuming, the loop starts at the beginning. The best solution is to make the resume reliably restart precisely where in the loop it left off. I'm not that familiar with this code but guess one should then also save the batch_idx.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hjalmarlucius", "commentT": "2020-02-16T17:03:20Z", "comment_text": "\n \t\tI can think of at least one way to do it, although it's not ideal:\n \n Use the global step as stored and when resuming, load this into a (hidden?) trainer variable which is checked every training batch.\n Skip each batch until the batch number is above this saved variable, at which point we can set it to None or something.\n Run batches as normal\n \n Two Problems I see:\n \n When the data set is shuffled there would be no guarantee of seeing only new samples after the reload without somehow 'resuming' the dataloaders.\n Any calls that are usually made during the batch wouldn't happen. For example, the tqdm update calls would need to be faked so it didn't appear to end the epoch early in the progress bars.\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hjalmarlucius", "commentT": "2020-02-17T13:11:56Z", "comment_text": "\n \t\tCurrently I've put in a PR (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/866>#866</denchmark-link>\n )  that deals with the off by one when loading a checkpoint from epoch end. I've added a warning when loading a mid-epoch checkpoint that says resuming training is not reliable and to consider loading an end of epoch checkpoint.\n If it's preferable I can also rerun the previous epoch when we detect mid-epoch checkpoints, but this technically means you run for more epochs than you would expect/report, so I'm not sure if this is a good idea.\n I'd suggest a new issue and discussion on how to resume mid epoch checkpoints, since we have no way of ensure data set states are preserved, and close this issue with the PR I have up.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "hjalmarlucius", "commentT": "2020-02-18T17:20:24Z", "comment_text": "\n \t\tGreat, a good compromise for now\n \t\t"}}}, "commit": {"commit_id": "6e7dc9c2363779a12e8122ee1e2d470a0b0f013e", "commit_author": "Matt Painter", "commitT": "2020-02-21 20:27:19-05:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.8571428571428571", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\trainer\\training_io.py", "file_new_name": "pytorch_lightning\\trainer\\training_io.py", "file_complexity": {"file_NLOC": "314", "file_CCN": "72", "file_NToken": "1558"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "311,312", "deleted_lines": "310,311", "method_info": {"method_name": "dump_checkpoint", "method_params": "self", "method_startline": "309", "method_endline": "350", "method_complexity": {"method_NLOC": "29", "method_CCN": "8", "method_NToken": "202", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "392,393,394,395,396,397,398,399,400,401,402", "deleted_lines": null, "method_info": {"method_name": "restore_training_state", "method_params": "self,checkpoint", "method_startline": "375", "method_endline": "419", "method_complexity": {"method_NLOC": "27", "method_CCN": "14", "method_NToken": "232", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "128,129", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self", "method_startline": "112", "method_endline": "129", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "80", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\test_restore_models.py", "file_new_name": "tests\\test_restore_models.py", "file_complexity": {"file_NLOC": "218", "file_CCN": "19", "file_NToken": "1394"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "285,286", "deleted_lines": "285", "method_info": {"method_name": "test_cpu_restore_training", "method_params": "tmpdir", "method_startline": "262", "method_endline": "321", "method_complexity": {"method_NLOC": "32", "method_CCN": "1", "method_NToken": "189", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "217,218", "deleted_lines": "217,218", "method_info": {"method_name": "test_dp_resume", "method_params": "tmpdir", "method_startline": "184", "method_endline": "259", "method_complexity": {"method_NLOC": "35", "method_CCN": "2", "method_NToken": "214", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\test_trainer.py", "file_new_name": "tests\\test_trainer.py", "file_complexity": {"file_NLOC": "471", "file_CCN": "43", "file_NToken": "3090"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440", "deleted_lines": null, "method_info": {"method_name": "test_resume_from_checkpoint_epoch_restored.new_model", "method_params": "", "method_startline": "424", "method_endline": "440", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "50", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "433,434", "deleted_lines": null, "method_info": {"method_name": "test_resume_from_checkpoint_epoch_restored.test_resume_from_checkpoint_epoch_restored.new_model.increment_batch", "method_params": "self,_", "method_startline": "433", "method_endline": "434", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "12", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481", "deleted_lines": null, "method_info": {"method_name": "test_resume_from_checkpoint_epoch_restored", "method_params": "tmpdir", "method_startline": "416", "method_endline": "481", "method_complexity": {"method_NLOC": "33", "method_CCN": "2", "method_NToken": "200", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "430,431", "deleted_lines": null, "method_info": {"method_name": "test_resume_from_checkpoint_epoch_restored.test_resume_from_checkpoint_epoch_restored.new_model.increment_epoch", "method_params": "self", "method_startline": "430", "method_endline": "431", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "10", "method_nesting_level": "2"}}}}}}}}