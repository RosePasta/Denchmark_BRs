{"BR": {"BR_id": "3974", "BR_author": "nrupatunga", "BRopenT": "2020-10-08T04:31:02Z", "BRcloseT": "2020-10-08T14:20:56Z", "BR_text": {"BRsummary": "[Bug]: Late update of Trainer `current_epoch` property for `LightningDataModule`", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Late update of Trainer current_epoch property for LightningDataModule object.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n The below code reproduces the issue:\n Please check for the print logs for the current_epoch number in train_dataloader.\n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>import torch\n import torch.nn as nn\n import torch.nn.functional as F\n from torchvision import transforms\n from torchvision.datasets import MNIST\n from torch.utils.data import random_split, DataLoader\n \n import pytorch_lightning as pl\n \n \n class LitModel(pl.LightningModule):\n \n     def __init__(self, channels, width, height, num_classes, hidden_size=64, learning_rate=2e-4):\n \n         super().__init__()\n \n         # We take in input dimensions as parameters and use those to dynamically build model.\n         self.channels = channels\n         self.width = width\n         self.height = height\n         self.num_classes = num_classes\n         self.hidden_size = hidden_size\n         self.learning_rate = learning_rate\n \n         self.model = nn.Sequential(\n             nn.Flatten(),\n             nn.Linear(channels * width * height, hidden_size),\n             nn.ReLU(),\n             nn.Dropout(0.1),\n             nn.Linear(hidden_size, hidden_size),\n             nn.ReLU(),\n             nn.Dropout(0.1),\n             nn.Linear(hidden_size, num_classes))\n \n     def forward(self, x):\n         x = self.model(x)\n         return F.log_softmax(x, dim=1)\n \n     def training_step(self, batch, batch_idx):\n         x, y = batch\n         logits = self(x)\n         loss = F.nll_loss(logits, y)\n         return loss\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n         return optimizer\n \n \n class MNISTDataModule(pl.LightningDataModule):\n \n     def __init__(self, data_dir: str = './'):\n         super().__init__()\n         self.data_dir = data_dir\n         self.transform = transforms.Compose([\n             transforms.ToTensor(),\n             transforms.Normalize((0.1307,), (0.3081,))\n         ])\n \n         # self.dims is returned when you call dm.size()\n         # Setting default dims here because we know them.\n         # Could optionally be assigned dynamically in dm.setup()\n         self.dims = (1, 28, 28)\n         self.num_classes = 10\n \n     def prepare_data(self):\n         # download\n         MNIST(self.data_dir, train=True, download=True)\n         MNIST(self.data_dir, train=False, download=True)\n \n     def setup(self, stage=None):\n \n         # Assign train/val datasets for use in dataloaders\n         if stage == 'fit' or stage is None:\n             mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n             self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n \n         # Assign test dataset for use in dataloader(s)\n         if stage == 'test' or stage is None:\n             self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n \n     def train_dataloader(self):\n         print('\\n----------------------------')\n         print(f'Current epoch: {self.trainer.current_epoch}')\n         print('----------------------------')\n         if self.trainer.current_epoch > 2:\n             return DataLoader(self.mnist_train, batch_size=32)\n         else:\n             return DataLoader(self.mnist_train, batch_size=32)\n \n \n # Init DataModule\n dm = MNISTDataModule()\n # Init model from datamodule's attributes\n model = LitModel(*dm.size(), dm.num_classes)\n # Init trainer\n trainer = pl.Trainer(max_epochs=5, progress_bar_refresh_rate=20, gpus=1, reload_dataloaders_every_epoch=True)\n # Pass the datamodule as arg to trainer.fit to override model hooks :)\n trainer.fit(model, dm)\n </denchmark-code>\n \n <denchmark-h:h4>logs</denchmark-h>\n \n Note: current_epoch is 0 two time, which is indeed due to the late update of this property\n <denchmark-code>----------------------------\n Current epoch: 0\n ----------------------------\n /home/nthere/2020/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n   warnings.warn(*args, **kwargs)\n Epoch 0:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 1700/1719 [00:10<00:00, 157.72it/s, loss=0.284, v_num=16]\n ----------------------------\n Current epoch: 0\n ----------------------------\n Epoch 1:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 1700/1719 [00:10<00:00, 164.23it/s, loss=0.200, v_num=16]\n ----------------------------\n Current epoch: 1\n ----------------------------\n Epoch 2:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 1700/1719 [00:10<00:00, 164.02it/s, loss=0.168, v_num=16]\n ----------------------------\n Current epoch: 2\n ----------------------------\n Epoch 3:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                | 180/1719 [00:01<00:09, 163.17it/s, loss=0.185, v_num=16]\n ^C/home/nthere/2020/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n   warnings.warn(*args, **kwargs)\n Epoch 3:  10%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                                                                | 180/1719 [00:01<00:10, 150.43it/s, loss=0.185, v_num=16]\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n current_epoch should be updated and reflect the right epoch number for the LightningDataModule\n <denchmark-h:h3>Environment</denchmark-h>\n \n Please copy and paste the output from our\n <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>\n \n (or fill out the checklist below manually).\n You can get the script and run it with:\n <denchmark-code>* CUDA:\n         - GPU:\n                 - GeForce GTX 960\n         - available:         True\n         - version:           10.1\n * Packages:\n         - numpy:             1.18.5\n         - pyTorch_debug:     False\n         - pyTorch_version:   1.5.0+cu101\n         - pytorch-lightning: 0.10.0\n         - tqdm:              4.47.0\n * System:\n         - OS:                Linux\n         - architecture:\n                 - 64bit\n                 - ELF\n         - processor:         x86_64\n         - python:            3.8.3\n         - version:           #113~16.04.1-Ubuntu SMP Fri Jul 10 04:37:08 UTC 2020\n \n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "nrupatunga", "commentT": "2020-10-08T04:31:48Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "nrupatunga", "commentT": "2020-10-08T05:08:41Z", "comment_text": "\n \t\tgreat catch <denchmark-link:https://github.com/nrupatunga>@nrupatunga</denchmark-link>\n  !\n \n this bug happens specifically when using the flag reload_dataloaders_every_epoch\n this is because reload_dataloaders_every_epoch  happens before train_epoch_start\n trainer.current_epoch is updated in train_epoch_start, so the epoch state when used inside of train_dataloader is stale\n \n We could:\n \n set trainer.current_epoch directly in the trainer at the beginning of the loop\n Move reload_dataloaders_every_epoch into train_epoch_start\n Or both?\n \n What do you think <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  ?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "nrupatunga", "commentT": "2020-10-08T05:11:31Z", "comment_text": "\n \t\tSuggested changes in this PR\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3975>#3975</denchmark-link>\n \n Please review let me know if there is a better way, Thank you\n \t\t"}}}, "commit": {"commit_id": "fcfa5874923000a4b391c88b6488e065aee4d671", "commit_author": "Nrupatunga", "commitT": "2020-10-08 10:20:55-04:00", "commit_complexity": {"commit_NLOC": "0.03125", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "584", "file_CCN": "59", "file_NToken": "2872"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "456,457,458,459", "method_info": {"method_name": "train", "method_params": "self", "method_startline": "438", "method_endline": "503", "method_complexity": {"method_NLOC": "37", "method_CCN": "11", "method_NToken": "218", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "479", "file_CCN": "147", "file_NToken": "3754"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "209,210,211,212,215,216,217,218", "deleted_lines": "217,218,219", "method_info": {"method_name": "on_train_epoch_start", "method_params": "self,epoch", "method_startline": "208", "method_endline": "239", "method_complexity": {"method_NLOC": "17", "method_CCN": "3", "method_NToken": "120", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tests\\core\\test_datamodules.py", "file_new_name": "tests\\core\\test_datamodules.py", "file_complexity": {"file_NLOC": "300", "file_CCN": "31", "file_NToken": "1900"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444", "deleted_lines": null, "method_info": {"method_name": "test_dm_reload_dataloaders_every_epoch", "method_params": "tmpdir", "method_startline": "424", "method_endline": "444", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "78", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "406,407", "deleted_lines": null, "method_info": {"method_name": "prepare_data", "method_params": "self", "method_startline": "406", "method_endline": "407", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "409,410,411,412,413,414,415", "deleted_lines": null, "method_info": {"method_name": "setup", "method_params": "self,None", "method_startline": "409", "method_endline": "415", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "69", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "401,402,403,404", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,str", "method_startline": "401", "method_endline": "404", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "417,418,419,420,421", "deleted_lines": null, "method_info": {"method_name": "train_dataloader", "method_params": "self", "method_startline": "417", "method_endline": "421", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "39", "method_nesting_level": "1"}}}}}}}}