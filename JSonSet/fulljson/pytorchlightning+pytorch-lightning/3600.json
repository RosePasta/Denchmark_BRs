{"BR": {"BR_id": "3600", "BR_author": "josh-gleason", "BRopenT": "2020-09-22T01:50:07Z", "BRcloseT": "2020-09-22T14:19:30Z", "BR_text": {"BRsummary": "Infinite hang when running `Trainer.test` after `Trainer.fit` with DDP", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n If I run Trainer.test after running Trainer.fit with distributed_backend='ddp' then the system hangs.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n Run the following script\n # main.py\n import os\n from argparse import ArgumentParser\n from pl_examples.models.lightning_template import LightningTemplateModel\n from pytorch_lightning import Trainer, seed_everything\n \n seed_everything(234)\n \n \n def main(args):\n     model = LightningTemplateModel(**vars(args))\n     trainer = Trainer.from_argparse_args(args)\n     trainer.fit(model)     # if this is commented out then test will complete, otherwise it hangs\n     trainer.test(model)\n \n \n def run_cli():\n     root_dir = os.path.dirname(os.path.realpath(__file__))\n     parent_parser = ArgumentParser(add_help=False)\n     parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\n     parser = Trainer.add_argparse_args(parser)\n     parser.set_defaults(gpus=2)\n     args = parser.parse_args()\n \n     main(args)\n \n \n if __name__ == '__main__':\n     run_cli()\n with command line arguments (assuming >= 2 GPUs)\n python main.py --gpus 2 --hidden_dim 500 --max_epochs 1 --distributed_backend ddp\n Running this script causes the program to hang during test phase.\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n I would expect Trainer.test to complete rather than hanging.\n <denchmark-h:h3>Environment</denchmark-h>\n \n Output of collect_env_details.py:\n <denchmark-code>* CUDA:\n         - GPU:\n                 - GeForce RTX 2080 Ti\n                 - GeForce RTX 2080 Ti\n         - available:         True\n         - version:           10.2\n * Packages:\n         - numpy:             1.19.1\n         - pyTorch_debug:     False\n         - pyTorch_version:   1.6.0\n         - pytorch-lightning: 0.9.1rc3\n         - tqdm:              4.49.0\n * System:\n         - OS:                Linux\n         - architecture:\n                 - 64bit\n                 - ELF\n         - processor:         x86_64\n         - python:            3.7.5\n         - version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020\n </denchmark-code>\n \n \n PyTorch Version: 1.6.0\n OS: Ubuntu 20.04\n How you installed PyTorch: pip\n Build command you used (if compiling from source):\n Python version: 3.7.5\n CUDA/cuDNN version: 7.6.5\n GPU models and configuration: GeForce RTX 2080 Ti (x2)\n \n List of all installed packages (output of pip freeze):\n <denchmark-code>absl-py==0.10.0\n cachetools==4.1.1\n certifi==2020.6.20\n chardet==3.0.4\n decorator==4.4.2\n fsspec==0.8.2\n future==0.18.2\n google-auth==1.21.2\n google-auth-oauthlib==0.4.1\n grpcio==1.32.0\n idna==2.10\n importlib-metadata==1.7.0\n Markdown==3.2.2\n networkx==2.5\n numpy==1.19.1\n oauthlib==3.1.0\n packaging==20.4\n Pillow==7.2.0\n pkg-resources==0.0.0\n protobuf==3.13.0\n pyasn1==0.4.8\n pyasn1-modules==0.2.8\n pyparsing==2.4.7\n pytorch-lightning==0.9.1rc3\n PyYAML==5.3.1\n requests==2.24.0\n requests-oauthlib==1.3.0\n rsa==4.6\n six==1.15.0\n tensorboard==2.2.0\n tensorboard-plugin-wit==1.7.0\n torch==1.6.0\n torchvision==0.7.0\n tqdm==4.49.0\n urllib3==1.25.10\n Werkzeug==1.0.1\n zipp==3.1.0\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n If I comment out trainer.fit then everything works as expected.\n I was able to pause the execution during hang while running in PyCharm. The following are the stack frames for the main thread, which is the only thread I could get to pause.\n <denchmark-code>select, selectors.py:418\n wait, connection.py:920\n _poll, connection.py:414\n poll, connection.py:257\n get, queues.py:104\n _worker_loop, worker.py:167\n run, process.py:99\n _bootstrap, process.py:297\n _launch, popen_fork.py:74\n __init__, popen_fork.py:20\n _Popen, context.py:277\n _Popen, context.py:223\n start, process.py:112\n __init__, dataloader.py:737\n __iter__, dataloader.py:291\n run_evaluation, trainer.py:437\n run_test, trainer.py:489\n train_or_test, base_backend.py:34\n ddp_train, ddp_backend.py:243\n train, ddp_backend.py:138\n fit, trainer.py:324\n wrapped_fn, states.py:48\n __test_given_model, trainer.py:627\n test, trainer.py:564\n wrapped_fn, states.py:48\n main, main.py:13\n run_cli, main.py:24\n <module>, main.py:28\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "josh-gleason", "commentT": "2020-09-22T01:50:47Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "josh-gleason", "commentT": "2020-09-22T10:51:42Z", "comment_text": "\n \t\tHi, in this ddp mode you can call trainer.fit / test only once.\n <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel>https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel</denchmark-link>\n \n \n There are cases in which it is not possible to use DDP. Examples are:\n \n Jupyter Notebook, Google COLAB, Kaggle, etc.\n You have a nested script without a root package\n Your script needs to invoke .fit or .test multiple times\n \n \n you need to switch to ddp_spawn or launch your .test in a separate script.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "josh-gleason", "commentT": "2020-09-22T18:42:48Z", "comment_text": "\n \t\tInteresting. I read this but to me it seemed to indicate you could call both fit and test as long as neither were called multiple times. Perhaps the documentation could be updated to make this more clear.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "josh-gleason", "commentT": "2020-09-22T19:47:58Z", "comment_text": "\n \t\tyeah granted, this may be a bit ambiguous.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "josh-gleason", "commentT": "2020-09-26T11:00:16Z", "comment_text": "\n \t\tHi\n I encountered that DDP couldn't work for me when running my python script as a module (python -m).\n Does it match the second limitation case? (\"You have a nested script without a root package\").\n I couldn't really understand this one, and I am not sure why DDP cannot work with modules.\n When I am running the same script not as module, everything works fine.\n I tested it also on a really simple MNIST example.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "josh-gleason", "commentT": "2020-09-26T11:06:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AvivWn>@AvivWn</denchmark-link>\n  Because when you run the ddp script, it will call itself under the hood again n-1 times where n is the number of gpus you selected. So, example, if we did this:\n python train.py --gpus 4 --something-else --distributed_backend=ddp \n this process will launch on gpu 0 and then call\n <denchmark-code>python train.py --gpus \"1,\"  --something-else --distributed_backend=ddp\n python train.py --gpus \"2,\"  --something-else --distributed_backend=ddp\n python train.py --gpus \"3,\"  --something-else --distributed_backend=ddp\n </denchmark-code>\n \n (simplified example)\n so if we wanted to support the module way of launching the script, we would have to strip -m ... from the command and append it to the subprocess calls.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "josh-gleason", "commentT": "2020-09-26T11:17:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n   Sure\n And is it impossible to programmatically add the \"-m\" only if it specified? It should be just a single if, right?\n It is frustrating to know that a module cannot be run as DDP, after I have already built a full working module project.\n Transforming it into a script won't be easy...\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "josh-gleason", "commentT": "2020-09-28T02:19:33Z", "comment_text": "\n \t\tNo, I would not assume it is impossible. If the -m option can be stripped from arglist, it should not be too difficult.\n Very sorry that it causes trouble for you but it looks like they just didn't think about this use case when the ddp backend was added.\n Let's open a feature request for this.\n \t\t"}}}, "commit": {"commit_id": "d2a3d6aa8e8b69e6f373243bd25165a0963d7a53", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-09-22 16:57:01-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\multi_gpu.rst", "file_new_name": "docs\\source\\multi_gpu.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "295,299", "deleted_lines": "295,299"}}}}}}