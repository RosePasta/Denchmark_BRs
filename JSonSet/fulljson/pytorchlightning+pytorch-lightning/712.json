{"BR": {"BR_id": "712", "BR_author": "colehurwitz", "BRopenT": "2020-01-19T20:18:08Z", "BRcloseT": "2020-01-21T13:09:28Z", "BR_text": {"BRsummary": "Trainer is setting parameters with requires_grad=False to requires_grad=True (bug)", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When training a model that has some parameters where requires_grad=False,  the Trainer  is actually setting requires_grad=True for these parameters and changing them. The bug appears to originate in the TrainerTrainLoopMixin code.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-h:h4>Steps to reproduce the behavior:</denchmark-h>\n \n \n Create a model with some parameters which have requires_grad=False\n Fit the model using the Trainer\n Check to see if the parameters which were set with `requires_grad=False' have changed.\n \n <denchmark-h:h4>Code sample (to reproduce the bug)</denchmark-h>\n \n <denchmark-code>import torch\n import numpy as np\n import os\n from torch.nn import functional as F\n from torch.utils.data import DataLoader\n import pytorch_lightning as pl\n \n # Make toy dataset\n features = torch.from_numpy(np.asarray([[0],[0],[0],[1],[1],[1]])).float()\n targets = torch.from_numpy(np.asarray([0,0,0,1,1,1]))\n train = torch.utils.data.TensorDataset(features, targets)\n train_loader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n \n \n #Define lightning model\n class CoolSystem(pl.LightningModule):\n \n     def __init__(self):\n         super(CoolSystem, self).__init__()\n         self.l1 = torch.nn.Linear(1, 10)\n         self.l2 = torch.nn.Linear(10, 2)\n         for param in self.l2.parameters():\n             param.requires_grad = False\n         self.loss_func = torch.nn.CrossEntropyLoss()\n    \n     def forward(self, x):\n         return self.l2(torch.relu(self.l1(x)))\n \n     def training_step(self, batch, batch_idx):\n         x, y = batch\n         y_hat = self.forward(x)\n         loss = self.loss_func(y_hat, y)\n         tensorboard_logs = {'train_loss': loss}\n         return {'loss': loss, 'log': tensorboard_logs}\n \n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters(), lr=0.02)\n \n     @pl.data_loader\n     def train_dataloader(self):\n         return train_loader\n \n # Run the lightning model (check parameter before and after training)\n \n coolsystem = CoolSystem()\n print(list(coolsystem.parameters())[3])\n trainer = pl.Trainer(min_epochs=10, max_epochs=10, logger=False)    \n trainer.fit(coolsystem)\n list(coolsystem.parameters())[3]\n \n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n <denchmark-h:h4>Expected</denchmark-h>\n \n The parameters with requires_grad == False should not change during training.\n <denchmark-h:h4>Actual</denchmark-h>\n \n The printed out parameter before training has requires_grad == False, but after training with the Trainer, the parameter now has requires_grad == True and has changed values.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version 1.3.1\n Linux\n PyTorch installed with pip\n Python 3.7.1\n pytorch-lightning 0.6.0\n \n <denchmark-h:h3>Where I think the issue is!</denchmark-h>\n \n Here is the code snippet from training_loop.py that I think is causing the issue:\n <denchmark-code>class TrainerTrainLoopMixin(ABC):\n             .\n             .\n             .\n     def run_training_batch(self, batch, batch_idx):\n             .\n             .\n             .\n             # call training_step once per optimizer\n             for opt_idx, optimizer in enumerate(self.optimizers):\n                 # make sure only the gradients of the current optimizer's paramaters are calculated\n                 # in the training step to prevent dangling gradients in multiple-optimizer setup.\n                 for param in self.get_model().parameters():\n                     param.requires_grad = False\n                 for group in optimizer.param_groups:\n                     for param in group['params']:\n                         param.requires_grad = True\n </denchmark-code>\n \n As you can see, the params in the model are all set to  param.requires_grad = True during each training batch!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "colehurwitz", "commentT": "2020-01-21T03:57:13Z", "comment_text": "\n \t\tThis behavior was introduced in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/603>#603</denchmark-link>\n . Looks like we may want to revisit some of the changes that PR made. In the meantime, you can work around the problem by not passing parameters with  to your optimizer.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "colehurwitz", "commentT": "2020-01-21T03:58:36Z", "comment_text": "\n \t\t@aybberk <denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>\n  ^\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:02:35Z", "comment_text": "\n \t\tI just disabled that loop for now since I am not working with GANs. thanks for responding and good luck fixing it!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:16:59Z", "comment_text": "\n \t\t@aybberk <denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>\n  yeah. we need to revert this change or make a change to address\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:23:57Z", "comment_text": "\n \t\tI think <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/603#discussion_r358626166>#603 (comment)</denchmark-link>\n  would also fix that when not working with GANs.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:35:55Z", "comment_text": "\n \t\tUnless I am misunderstanding, the solution would be to only do this if there is more than one optimizer. However, normal networks can have more than one optimizer as well and not want this behavior right?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:42:56Z", "comment_text": "\n \t\tYou are probably correct, but I can't think of a such niche situation right now.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:47:12Z", "comment_text": "\n \t\tIf you want different learning rates for some parameters than you would use different optimizers, right? For example, if you are making a VAE with a deep encoder and a parametric model for the decoder than you may want different optimizers for the neural network and the learned decoder params. Maybe it is niche, but I have used it before.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:48:21Z", "comment_text": "\n \t\ti suggest we do this only if 2 optimizes are present. PR? <denchmark-link:https://github.com/colehurwitz>@colehurwitz</denchmark-link>\n  or @aybberk\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:49:59Z", "comment_text": "\n \t\t\n i suggest we do this only if 2 optimizes are present. PR? @colehurwitz or @aybberk\n \n PR is ready with this suggestion.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:53:54Z", "comment_text": "\n \t\tOk, this fixes my immediate issue at least. Thanks!\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "colehurwitz", "commentT": "2020-01-21T12:54:02Z", "comment_text": "\n \t\t\n If you want different learning rates for some parameters than you would use different optimizers, right? For example, if you are making a VAE with a deep encoder and a parametric model for the decoder than you may want different optimizers for the neural network and the learned decoder params. Maybe it is niche, but I have used it before.\n \n You are right, but it would still be implemented in Lightning with iteration between optimizers and this change would not affect the gradients in that setup. Also, I need to mention I think it would not be the best way to implement the situation in Lightning since it automatically iterates between optimizers for same training loop.\n edit: You mentioned you used that setup, did you do it in Lightning or something else?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "colehurwitz", "commentT": "2020-01-21T13:02:23Z", "comment_text": "\n \t\tI did this in regular Pytorch for a recent paper. You bring up some good points though, I did not freeze any parameters so that example may not apply. I do think that if this is specifically a GAN issue though, maybe there is a GAN specific solution? Maybe not though.\n Anyways, I appreciate the quick response and the fix should be appropriate for my current work!\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "colehurwitz", "commentT": "2020-01-21T13:04:17Z", "comment_text": "\n \t\tThe most structured way to fix it would be, as <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n   mentioned, to save states before regular freeze/unfreeze operation and gradient calculation and load it after gradients are computed. I can work on it but I'm not really sure if it messes up with the distributed training settings since I do not know anything about ddp internals.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "colehurwitz", "commentT": "2020-01-21T13:08:51Z", "comment_text": "\n \t\tWould it be possible to only freeze and unfreeze variables that have requires_grad=True or is that too expensive to search for every loop?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "colehurwitz", "commentT": "2020-01-21T13:08:52Z", "comment_text": "\n \t\tit shouldn't mess up the internals @aybberk. and I would do it how <denchmark-link:https://github.com/colehurwitz>@colehurwitz</denchmark-link>\n  mentioned\n \t\t"}}}, "commit": {"commit_id": "a2b20b46bca5101627ed392aec17611ac0e97133", "commit_author": "Ayberk Ayd\u0131n", "commitT": "2020-01-21 08:09:27-05:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "423", "file_CCN": "85", "file_NToken": "1760"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "463,464,465,466,467,468", "deleted_lines": "463,464,465,466,467", "method_info": {"method_name": "run_training_batch", "method_params": "self,batch,batch_idx", "method_startline": "429", "method_endline": "553", "method_complexity": {"method_NLOC": "51", "method_CCN": "20", "method_NToken": "408", "method_nesting_level": "1"}}}}}}}}