{"BR": {"BR_id": "1566", "BR_author": "karlinjf", "BRopenT": "2020-04-22T21:32:23Z", "BRcloseT": "2020-04-23T18:28:21Z", "BR_text": {"BRsummary": "Batch being moved to gpu repeatedly with multiple optimizers and single gpu training", "BRdescription": "\n If you have multiple optimizers, then transfer_batch_to_gpu winds up getting called once per opt_idx, and the batch is copied each time via copy.copy(batch) in training_forward. Why copy the batch when there is only a single gpu? By removing the copy.copy() my GAN model moves from 8.53it/s to 9.25it/s. Pretty significant speedup.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "karlinjf", "commentT": "2020-04-22T22:15:11Z", "comment_text": "\n \t\tamazing find! mind submitting a PR?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "karlinjf", "commentT": "2020-04-22T22:44:28Z", "comment_text": "\n \t\tSure, will do.\n \t\t"}}}, "commit": {"commit_id": "41b6cbb3ca8a3a43e091d7f0de4d0184a8870d19", "commit_author": "karlinjf", "commitT": "2020-04-23 14:28:20-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "561", "file_CCN": "131", "file_NToken": "2792"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "757,758,759,760,761", "deleted_lines": "757", "method_info": {"method_name": "training_forward", "method_params": "self,batch,batch_idx,opt_idx,hiddens", "method_startline": "715", "method_endline": "792", "method_complexity": {"method_NLOC": "43", "method_CCN": "14", "method_NToken": "322", "method_nesting_level": "1"}}}}}}}}