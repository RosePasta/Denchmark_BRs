{"BR": {"BR_id": "4304", "BR_author": "jopo666", "BRopenT": "2020-10-22T12:12:49Z", "BRcloseT": "2020-11-25T19:44:06Z", "BR_text": {"BRsummary": "TensorBoardLogger not working as expected with accumulate_grad_batches&gt;1", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When logging inside training step to TensorBoard and using accumulate_grad_batches > 1 inside pl.Trainer() the behavior is not as expected.\n With  everything looks good.\n <denchmark-link:https://user-images.githubusercontent.com/49716607/96869625-65b62900-1478-11eb-85fd-c3c5d219c65e.png></denchmark-link>\n \n With accumulate_grad_batches = 8 the values are reported on the same step.\n <denchmark-link:https://user-images.githubusercontent.com/49716607/96869632-66e75600-1478-11eb-981b-399af7dd173c.png></denchmark-link>\n \n <denchmark-h:h3>To Reproduce (sorry for not using colab)</denchmark-h>\n \n <denchmark-code>import os\n \n import torch\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n \n import pytorch_lightning as pl\n from pytorch_lightning.loggers import TensorBoardLogger\n from torchvision.datasets.mnist import MNIST\n from torchvision import transforms\n \n class LitClassifier(pl.LightningModule):\n     def __init__(self, hidden_dim=128, learning_rate=1e-3):\n         super().__init__()\n         self.save_hyperparameters()\n \n         self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\n         self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\n \n     def forward(self, x):\n         x = x.view(x.size(0), -1)\n         x = torch.relu(self.l1(x))\n         x = torch.relu(self.l2(x))\n         return x\n \n     def training_step(self, batch, batch_idx):\n         x, y = batch\n         y_hat = self(x)\n         loss = F.cross_entropy(y_hat, y)\n         self.log(\"train_loss\",loss)\n         return loss\n \n     def validation_step(self, batch, batch_idx):\n         x, y = batch\n         y_hat = self(x)\n         loss = F.cross_entropy(y_hat, y)\n \n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n \n def run_test(accumulate_grad_batches,batch_size,num_workers):\n     dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n     mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n     train_loader = DataLoader(mnist_train,batch_size)\n     val_loader = DataLoader(mnist_val,batch_size)\n \n     model = LitClassifier()\n \n     trainer = pl.Trainer(\n         logger=TensorBoardLogger(os.getcwd()',name='bug'),\n         accumulate_grad_batches=accumulate_grad_batches,\n         max_epochs=2\n         )\n     trainer.fit(model, train_loader, val_loader)\n \n run_test(1,32)\n run_test(8,32)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Take a mean of the values logged at same step?\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version: 1.6\n OS: Linux\n How you installed PyTorch: conda\n Python version: 3.8\n Tensorboard: 2.3.0\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jopo666", "commentT": "2020-10-22T12:13:31Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jopo666", "commentT": "2020-10-23T13:46:38Z", "comment_text": "\n \t\tTo add something to the issue: when using DataParallel () it might cause even worse results, i.e in W&B it looks like this:\n <denchmark-link:https://user-images.githubusercontent.com/6958772/97011285-c2801500-1546-11eb-8362-37f2e5e1814e.png></denchmark-link>\n \n I'm also logging from training_step.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jopo666", "commentT": "2020-10-23T21:20:19Z", "comment_text": "\n \t\tThe first one looks like an issue with the limitations of  since  is updated after every accumulated step, and since x-axis is the  so for every  it's logging 8 values(num accumulated batches). I tried this on Wandb the it's looking good there.\n <denchmark-link:https://user-images.githubusercontent.com/30778939/97055069-a199dd00-15a3-11eb-9872-b9c4ebeb6348.png></denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jopo666", "commentT": "2020-10-24T14:01:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>\n  have you checked that with or without DataParallel?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jopo666", "commentT": "2020-10-24T14:11:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/marrrcin>@marrrcin</denchmark-link>\n  don't have GPUs , can't check it.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jopo666", "commentT": "2020-10-24T18:13:55Z", "comment_text": "\n \t\tSo what I've found out is the following:\n \n Logging with on_step=False and on_epoch=True in __step_end yields fine charts on the epoch level (for any kind of step).\n Logging with on_step=True and on_epoch=False in training_step_end results in messed up charts that are dependant on the accumulate_grad_batches parameter of the Trainer as in the picture:\n \n Each red box contains number of points equal to accumulate_grad_batches.\n \n It seems to me like the problem is that for the same Trainer's global_step, there are accumulate_grad_batches-calls to both train_step and backward and when you log from there, the charts are invalid. IMHO when accumulate_grad_batches is > 1 then there should be some callback / on_* event just before the optimizer step happens that will have access to the losses calculated during the accumulation step.\n My current workaround to this is to store every training loss's value during the same  and only call  when global step increments - then the chart looks normal:\n <denchmark-link:https://user-images.githubusercontent.com/6958772/97088721-27189e00-1633-11eb-8e25-337ec66d7175.png></denchmark-link>\n \n It's a nasty workaround, I hope <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  you will be able to fix it in more robust manner.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jopo666", "commentT": "2020-10-28T14:45:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  any update on this?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "jopo666", "commentT": "2020-11-10T09:37:21Z", "comment_text": "\n \t\tPing <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "jopo666", "commentT": "2020-11-17T18:19:57Z", "comment_text": "\n \t\tdefining the behavior only for accumulate_grad_batches,\n avoid making general logging changes\n \t\t"}}}, "commit": {"commit_id": "204a0a2d03ce7dfb014e347f1d34a49ef5e86902", "commit_author": "chaton", "commitT": "2020-11-25 19:44:05+00:00", "commit_complexity": {"commit_NLOC": "0.3181818181818182", "commit_CCN": "0.9318181818181818", "commit_Nprams": "0.09090909090909091"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "103,104", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py", "file_new_name": "pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py", "file_complexity": {"file_NLOC": "393", "file_CCN": "110", "file_NToken": "2805"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "627", "deleted_lines": "622", "method_info": {"method_name": "log_train_step_metrics", "method_params": "self,batch_output", "method_startline": "620", "method_endline": "628", "method_complexity": {"method_NLOC": "7", "method_CCN": "5", "method_NToken": "67", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "98,99", "deleted_lines": "98", "method_info": {"method_name": "on_trainer_init", "method_params": "self,logger,int,int,bool", "method_startline": "98", "method_endline": "99", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "184,193,194,212,213,214,215,216", "deleted_lines": "184,210,211", "method_info": {"method_name": "log_metrics", "method_params": "self,metrics,grad_norm_dic,step", "method_startline": "184", "method_endline": "221", "method_complexity": {"method_NLOC": "17", "method_CCN": "8", "method_NToken": "158", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "184,193,194,212,213,214,215,216", "deleted_lines": "184,210,211", "method_info": {"method_name": "log_metrics", "method_params": "self,metrics,grad_norm_dic,step,log_train_step_metrics", "method_startline": "184", "method_endline": "226", "method_complexity": {"method_NLOC": "20", "method_CCN": "9", "method_NToken": "174", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "652", "file_CCN": "70", "file_NToken": "3104"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "24,25,26,27,32,34,38,39,40,41,43,44,45,46,47,48,49,51,55,57,59,61,65,388", "deleted_lines": "28,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,388"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\loggers\\test_all.py", "file_new_name": "tests\\loggers\\test_all.py", "file_complexity": {"file_NLOC": "261", "file_CCN": "29", "file_NToken": "1874"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "127,135", "deleted_lines": "127,135", "method_info": {"method_name": "_test_loggers_fit_test", "method_params": "tmpdir,logger_class", "method_startline": "81", "method_endline": "139", "method_complexity": {"method_NLOC": "45", "method_CCN": "7", "method_NToken": "284", "method_nesting_level": "0"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\loggers\\test_tensorboard.py", "file_new_name": "tests\\loggers\\test_tensorboard.py", "file_complexity": {"file_NLOC": "181", "file_CCN": "22", "file_NToken": "1366"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "231,232,233,234,235", "deleted_lines": null, "method_info": {"method_name": "test_tensorboard_with_accummulated_gradients.validation_step", "method_params": "self,batch,batch_idx", "method_startline": "231", "method_endline": "235", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "45", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "219,220,221,222,223,224,225,226,227,228,229", "deleted_lines": null, "method_info": {"method_name": "test_tensorboard_with_accummulated_gradients.training_step", "method_params": "self,batch,batch_idx", "method_startline": "219", "method_endline": "229", "method_complexity": {"method_NLOC": "9", "method_CCN": "2", "method_NToken": "87", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264", "deleted_lines": null, "method_info": {"method_name": "test_tensorboard_with_accummulated_gradients", "method_params": "mock_log_metrics,expected,tmpdir", "method_startline": "211", "method_endline": "264", "method_complexity": {"method_NLOC": "27", "method_CCN": "5", "method_NToken": "166", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "237,238,239,240", "deleted_lines": null, "method_info": {"method_name": "test_tensorboard_with_accummulated_gradients.configure_optimizers", "method_params": "self", "method_startline": "237", "method_endline": "240", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "50", "method_nesting_level": "2"}}}}}}}}