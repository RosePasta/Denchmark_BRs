{"BR": {"BR_id": "1507", "BR_author": "fellnerse", "BRopenT": "2020-04-16T08:01:01Z", "BRcloseT": "2020-04-23T21:32:37Z", "BR_text": {"BRsummary": "After update from 0.5.x to 0.7.3 merge_dicts #1278 sometimes breaks training", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n After I updated from a quite old lightning version to the newest one, I sometimes get a TypeError from merge_dicts. I guess it's related to this MR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1278>#1278</denchmark-link>\n  . This Type error is deterministic, meaning it always occurs at the same global step during training. It somehow seems to be related to val_check_interval as well. For some data changing this value leads to no Error. But for other datasets this does not work. Also this only happens during training step, I suspect the training step after validating.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n I have no Idea.\n <denchmark-code>File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 363, in train\n     self.run_training_epoch()\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 470, in run_training_epoch\n     self.log_metrics(batch_step_metrics, grad_norm_dic)\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py\", line 74, in log_metrics\n     self.logger.agg_and_log_metrics(scalar_metrics, step=step)\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py\", line 128, in agg_and_log_metrics\n     agg_step, metrics_to_log = self._aggregate_metrics(metrics=metrics, step=step)\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py\", line 101, in _aggregate_metrics\n     agg_step, agg_mets = self._finalize_agg_metrics()\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py\", line 116, in _finalize_agg_metrics\n     agg_mets = merge_dicts(self._metrics_to_agg, self._agg_key_funcs, self._agg_default_func)\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py\", line 347, in merge_dicts\n     agg_val = fn([v for v in [d_in.get(k) for d_in in dicts] if v is not None])\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 3118, in mean\n     out=out, **kwargs)\n   File \"/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py\", line 75, in _mean\n     ret = umr_sum(arr, axis, dtype, out, keepdims)\n TypeError: unsupported operand type(s) for +: 'dict' and 'dict'\n </denchmark-code>\n \n Sometimes its also 'dict' and 'int'\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n At least should not break training, but maybe a more verbose message what is wrong. Its quite hard for me to debug, as the structure of the logs I'm returning to lightning does not change.\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>cuda:\n         GPU:\n                 GeForce RTX 2080 Ti\n                 GeForce RTX 2080 Ti\n                 GeForce RTX 2080 Ti\n                 GeForce RTX 2080 Ti\n                 GeForce RTX 2080 Ti\n                 GeForce RTX 2080 Ti\n                 GeForce RTX 2080 Ti\n                 GeForce RTX 2080 Ti\n         available:           True\n         version:             10.1.243\n packages:\n         numpy:               1.16.4\n         pyTorch_debug:       False\n         pyTorch_version:     1.3.0\n         pytorch-lightning:   0.7.3\n         tensorboard:         2.2.0\n         tqdm:                4.45.0\n system:\n         OS:                  Linux\n         architecture:\n                 64bit\n                 ELF\n         processor:           x86_64\n         python:              3.7.7\n         version:             #97~16.04.1-Ubuntu SMP Wed Apr 1 03:03:31 UTC 2020\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n Also for some reason some runs have an issue with multiprocessing, but it does not break the training:\n <denchmark-code>Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:00<00:00,  8.76it/s]\n   File \"/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n     finalizer()\n   File \"/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n     res = self._callback(*self._args, **self._kwargs)\n   File \"/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/multiprocessing/util.py\", line 110, in _remove_temp_dir\n     rmtree(tempdir)\n   File \"/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/shutil.py\", line 498, in rmtree\n     onerror(os.rmdir, path, sys.exc_info())\n   File \"/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/shutil.py\", line 496, in rmtree\n     os.rmdir(path)\n OSError: [Errno 39] Directory not empty: '/tmp/pymp-jcqai2xr'\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "fellnerse", "commentT": "2020-04-16T15:45:31Z", "comment_text": "\n \t\tDid you passed any 'agg_key_funcs' to the logger class? If I understand the code correctly, by default np.mean is used to aggregate the dict values returned during training. Maybe numpy tries in the mean function to add (+ func) values which can't be summed up?\n Can you maybe post the code snippets where you return the metrics to log in the lightning module and the initialization of the logger if you use one? If you don't use a logger, you can disable it by passing logger=False to the trainer (don't know if your previous version had logger on by default).\n Hope I can help :)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "fellnerse", "commentT": "2020-04-16T16:43:42Z", "comment_text": "\n \t\tThanks for the quick reply!\n No I'm not using any 'agg_key_funcs' that I know of.\n \n If I understand the code correctly, by default np.mean is used to aggregate the dict values returned during training.\n \n This only happens when there is a step in time where two times stuff is logged, right? So my guess is that at some point that is the case that two logs have to be \"unified\"  but this fails, because I'm using \"dict in dicts\". I need this tho, because I want to have i.e. loss train and val in the same graph.\n I'm using the TestTubeLogger:\n    logger = TestTubeLogger(save_dir=log_dir, name=name, description=description)\n and just pass this to the Trainer.\n The metric logging to lightning is a bit scattered:\n \n train_step in model:\n \n <denchmark-code>       x, target = batch\n        pred = self.forward(x)\n        loss = self.loss(pred, target)\n        lightning_log = {\"loss\": loss}\n \n        with torch.no_grad():\n            train_acc = self.calculate_accuracy(pred, target)\n            tensorboard_log = {\"loss\": loss, \"acc\": train_acc}\n \n        return tensorboard_log, lightning_log\n </denchmark-code>\n \n \n this is passed to a function that lets me add train and val to same graph:\n \n <denchmark-code>    def _construct_lightning_log(\n         self,\n         tensorboard_log: dict,\n         lightning_log: dict = None,\n         suffix: str = \"train\",\n         prefix: str = \"metrics\",\n     ):\n         lightning_log = lightning_log or {}\n         fixed_log = {}\n \n         for metric, value in tensorboard_log.items():\n             if isinstance(value, dict):\n                 fixed_log[f\"{prefix}/{metric}\"] = value\n             else:\n                 fixed_log[f\"{prefix}/{metric}\"] = {suffix: value}\n         return {\"log\": fixed_log, **lightning_log}\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "fellnerse", "commentT": "2020-04-16T20:39:29Z", "comment_text": "\n \t\tDo you pass it after training_step or training_epoch_end? I think lightning collects your logs and tries to aggregate it to one value. I can't test it now. Maybe tomorrow.\n But when I quickly type this into python interpreter:\n <denchmark-code>>>> d={}\n >>> np.mean([d,d])\n Traceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\n   File \"<__array_function__ internals>\", line 5, in mean\n   File \"/usr/lib/python3.8/site-packages/numpy/core/fromnumeric.py\", line 3334, in mean\n     return _methods._mean(a, axis=axis, dtype=dtype,\n   File \"/usr/lib/python3.8/site-packages/numpy/core/_methods.py\", line 151, in _mean\n     ret = umr_sum(arr, axis, dtype, out, keepdims)\n TypeError: unsupported operand type(s) for +: 'dict' and 'dict'\n </denchmark-code>\n \n Seems like getting your error.\n Maybe print what you exactly return and when it crashes. When I have time tomorrow, I will also make some tests.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "fellnerse", "commentT": "2020-04-17T11:01:44Z", "comment_text": "\n \t\tAfter training_step. I not have a training_epoch_end or training_end method defined.\n \n I think lightning collects your logs and tries to aggregate it to one value.\n \n Yes I think so as well.\n Ok I return something like this:\n {'metrics/aud_std': {'test': tensor(1.6337, device='cuda:0')}, 'metrics/class_loss_diff': {'test': tensor(nan)}, 'metrics/class_loss_val': {'0': tensor(nan), '1': tensor(91.5485)}, 'metrics/loss': {'test': tensor(45.7742, device='cuda:0')}, 'metrics/vid_std': {'test': tensor(1.6506, device='cuda:0')}}\n What do you mean by when it crashes exactly? I think when it crashes it's always the train step after an validation step (keep in mind I'm validation several times during one epoch). If I change the val_check_interval the error either disappears or happens at a different batch number.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "fellnerse", "commentT": "2020-04-17T19:48:25Z", "comment_text": "\n \t\tHello.\n I think the problem is in your metrics type. Metrics must have the Dict[str, float] type. But in your case, the metrics is a nested dict. So, that's why values are failed to be aggregated.\n Is it possible for you to flatten the dictionary?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "fellnerse", "commentT": "2020-04-20T06:56:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>\n  Hey! Ah yes that's what I thought. Do you know why the metrics dict is enforced to be of this type? In 0.5.x this was not an issue as far as I know.\n I mean, yes I can flatten it but I want to have i.e. val/loss and train/loss in the same graph. It's basically this: <denchmark-link:https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars>https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars</denchmark-link>\n \n I know that here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1144#issuecomment-599089378>#1144 (comment)</denchmark-link>\n  It was said that this should not be done, but for me this is essential.\n Is there a way that I can overwrite the merge_dicts function? If so how would I do that?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "fellnerse", "commentT": "2020-04-20T12:12:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/fellnerse>@fellnerse</denchmark-link>\n  Okay, I got your point, let's ask Borda's advice)\n <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n , what do you think? Is it possible to combine nested metrics dictionaries with metrics aggregation logic? At first sight, it doesn't look like a big problem. Maybe you can see any side effects of tracking aggregated metrics with nested dictionaries? If no, I can try to fix this issue\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "fellnerse", "commentT": "2020-04-20T14:37:49Z", "comment_text": "\n \t\tI ques it can be used, just need to care about the depth and the aggregation will be a bit complicated...\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "fellnerse", "commentT": "2020-04-24T15:26:45Z", "comment_text": "\n \t\tCool, thanks for implementing this so fast!\n \t\t"}}}, "commit": {"commit_id": "edb8d7a23cac91d607ab97c0adcbb815780936ac", "commit_author": "Alexey Karnachev", "commitT": "2020-04-23 17:32:36-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "53,54", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\loggers\\base.py", "file_new_name": "pytorch_lightning\\loggers\\base.py", "file_complexity": {"file_NLOC": "275", "file_CCN": "62", "file_NToken": "1448"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "283,351,352,353,355,357,358,359,360,361,363,367,368,369,370,371,372,373", "deleted_lines": "350,351,352,354,356,358,362,363,364"}}}}}}