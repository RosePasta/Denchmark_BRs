{"BR": {"BR_id": "3945", "BR_author": "ananthsub", "BRopenT": "2020-10-07T16:53:58Z", "BRcloseT": "2020-10-07T17:46:28Z", "BR_text": {"BRsummary": "Unexpected signature for validation_step", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n <denchmark-code>TypeError: validation_step() takes 3 positional arguments but 4 were given\n </denchmark-code>\n \n Full stacktrace: <denchmark-link:https://gist.github.com/ananthsub/cbe20db7f5bf22fdcbe77c0e0f8c3e49>https://gist.github.com/ananthsub/cbe20db7f5bf22fdcbe77c0e0f8c3e49</denchmark-link>\n \n This test is passing for me on 0.9.1rc4, but not on master\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-code>from typing import Optional\n import unittest\n import torch\n from pytorch_lightning import LightningModule\n from torch.utils.data.dataset import Dataset\n class RandomDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n     def __getitem__(self, index):\n         return self.data[index]\n     def __len__(self):\n         return self.len\n class TestModule(LightningModule):\n     def __init__(self, epoch_min_loss_override: Optional[int] = None):\n         \"\"\"LightningModule for testing purposes\n         Args:\n             epoch_min_loss_override (int, optional): Pass in an epoch that will be set to the minimum\n                 validation loss for testing purposes (zero based). If None this is ignored. Defaults to None.\n         \"\"\"\n         super().__init__()\n         self.layer = torch.nn.Linear(32, 2)\n         self.epoch_min_loss_override = epoch_min_loss_override\n     def forward(self, x):\n         return self.layer(x)\n     def loss(self, batch, prediction):\n         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\n         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\n     def training_step(self, batch, batch_idx):\n         output = self.forward(batch)\n         loss = self.loss(batch, output)\n         return {\"output\": output, \"loss\": loss, \"checkpoint_on\": loss}\n     def validation_step(self, batch, batch_idx):\n         output = self.forward(batch)\n         loss = self.loss(batch, output)\n         return {\"output\": output, \"loss\": loss, \"checkpoint_on\": loss}\n     def test_step(self, batch, batch_idx):\n         output = self.forward(batch)\n         loss = self.loss(batch, output)\n         return {\"output\": output, \"loss\": loss}\n     def training_epoch_end(self, outputs) -> None:\n         avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n         self.log(\"avg_loss\", avg_loss)\n     def validation_epoch_end(self, outputs) -> None:\n         avg_val_loss = torch.stack(\n             [torch.randn(1, requires_grad=True) for _ in outputs]\n         ).mean()\n         # For testing purposes allow a nominated epoch to have a low loss\n         if self.current_epoch == self.epoch_min_loss_override:\n             avg_val_loss -= 1e10\n         self.log(\"val_loss\", avg_val_loss)\n         self.log(\"checkpoint_on\", avg_val_loss)\n     def test_epoch_end(self, outputs) -> None:\n         avg_loss = torch.stack(\n             [torch.randn(1, requires_grad=True) for _ in outputs]\n         ).mean()\n         self.log(\"val_loss\", avg_loss)\n     def configure_optimizers(self):\n         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\n         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n         return [optimizer], [lr_scheduler]\n     def train_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64))\n     def val_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64))\n     def test_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64))\n class TestPL(unittest.TestCase):\n     def test_strict_model_load(self):\n         model = TestModule()\n         model.layer = torch.nn.Linear(32, 4)\n         checkpoint = ModelCheckpoint(\n             save_top_k=1,\n             monitor=\"val_loss\",\n         )\n         trainer = Trainer(\n             checkpoint_callback=checkpoint,\n             logger=logger,\n             overfit_batches=0.20,\n             max_epochs=1,\n         )\n         result = trainer.fit(model)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The test doesn't crash, and trainer.fit() works\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ananthsub", "commentT": "2020-10-07T17:15:15Z", "comment_text": "\n \t\tfixed!\n \t\t"}}}, "commit": {"commit_id": "6044cf900317ec9542fb1745976c9a96cc70b396", "commit_author": "William Falcon", "commitT": "2020-10-07 13:46:27-04:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\data_loading.py", "file_new_name": "pytorch_lightning\\trainer\\data_loading.py", "file_complexity": {"file_NLOC": "265", "file_CCN": "53", "file_NToken": "1646"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "238,239,240", "deleted_lines": "245,246,247"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\trainer\\flags\\test_overfit_batches.py", "file_new_name": "tests\\trainer\\flags\\test_overfit_batches.py", "file_complexity": {"file_NLOC": "35", "file_CCN": "5", "file_NToken": "231"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "40,41,42,43,44,45,46,47,48,49,50,51,52,53,54", "deleted_lines": null, "method_info": {"method_name": "test_overfit_basic", "method_params": "tmpdir,overfit", "method_startline": "40", "method_endline": "54", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "40", "method_nesting_level": "0"}}}}}}}}