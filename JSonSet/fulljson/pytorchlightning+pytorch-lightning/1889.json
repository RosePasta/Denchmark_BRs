{"BR": {"BR_id": "1889", "BR_author": "HansBambel", "BRopenT": "2020-05-19T10:28:37Z", "BRcloseT": "2020-05-19T17:16:27Z", "BR_text": {"BRsummary": "trainer.scale_batch_size() throws exception due to LRScheduler", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n I tried finding the biggest possible batch_size for my training, but PL raises a MisconfigurationException saying that my LRScheduler (ReduceLROnPlateau) is conditioned on a metric that is only available after validation_epoch_end. The available metrics are: loss, val_loss.\n I assume the LRScheduler requires a metric from the training loop for this to work? Why is this neccessary?\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Have a model with a metric that only exists in validation_epoch_end\n Have a LRScheduler which monitors that metric\n Use trainer.scale_batch_size\n See error\n \n <denchmark-code>File \"C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 779, in update_learning_rates\n     raise MisconfigurationException(\n pytorch_lightning.utilities.exceptions.MisconfigurationException: ReduceLROnPlateau conditioned on metric meanIoU which is not available. Available metrics are: loss,train_loss. Condition can be set using `monitor` key in lr scheduler dict\n \n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>trainer = pl.Trainer(gpus=hparams.gpus)\n new_batch_size = trainer.scale_batch_size(net, mode='binsearch', init_val=8)\n </denchmark-code>\n \n and in my model:\n <denchmark-code>    def configure_optimizers(self):\n         opt = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n         scheduler = {\n          'scheduler': optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=5),\n          'monitor': 'meanIoU',  # Default: val_loss\n         }\n         return [opt], [scheduler]\n \n     def validation_epoch_end(self, outputs):\n         avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n         iou_class, mean_iou = self.iou_metric.value()\n         mean_iou = torch.tensor(mean_iou)\n         self.iou_metric.reset()\n         logs = {\"val_loss\": avg_loss, \"meanIoU\": mean_iou}\n         return {\"meanIoU\": mean_iou, \"log\": logs,\n                 \"progress_bar\": {\"val_loss\": avg_loss, \"meanIoU\": mean_iou}}\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n No Exception and the maximum batch_size for my model.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n \n GPU:\n \n GeForce RTX 2070 SUPER\n \n \n available:         True\n version:           10.1\n \n \n Packages:\n \n numpy:             1.18.1\n pyTorch_debug:     False\n pyTorch_version:   1.4.0\n pytorch-lightning: 0.7.6\n tensorboard:       2.1.0\n tqdm:              4.45.0\n \n \n System:\n \n OS:                Windows\n architecture:\n \n 64bit\n WindowsPE\n \n \n processor:         AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\n python:            3.8.2\n version:           10.0.18362\n \n \n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "HansBambel", "commentT": "2020-05-19T11:52:22Z", "comment_text": "\n \t\tJust to be sure, this error only happens when you run the .scale_batch_size(...) method and not when you run .fit(...)?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "HansBambel", "commentT": "2020-05-19T13:05:33Z", "comment_text": "\n \t\t\n Just to be sure, this error only happens when you run the .scale_batch_size(...) method and not when you run .fit(...)?\n \n Correct\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "HansBambel", "commentT": "2020-05-19T13:14:53Z", "comment_text": "\n \t\tThe problem here is that learning rates are updated right after the training_epoch has returned, even when we have reached max_steps and should just run the training_teardown\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 348 to 355\n       in\n       ac76dfc\n \n \n \n \n \n \n  self.run_training_epoch() \n \n \n \n  \n \n \n \n  # update LR schedulers \n \n \n \n  self.update_learning_rates(interval='epoch') \n \n \n \n  \n \n \n \n  if self.max_steps and self.max_steps == self.global_step: \n \n \n \n  self.run_training_teardown() \n \n \n \n  return \n \n \n \n \n \n We could probably just switch order of the two statements and this will be solved.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "HansBambel", "commentT": "2020-05-19T13:18:44Z", "comment_text": "\n \t\tGreat that you found the reason for that behavior already!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "HansBambel", "commentT": "2020-05-19T13:19:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/HansBambel>@HansBambel</denchmark-link>\n  can you locally try if this works for you, and if it does, would you be up for doing a PR?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "HansBambel", "commentT": "2020-05-19T13:46:52Z", "comment_text": "\n \t\t\n @HansBambel can you locally try if this works for you, and if it does, would you be up for doing a PR?\n \n I'll try it out!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "HansBambel", "commentT": "2020-05-19T15:10:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>\n  it worked! I created a PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1891>#1891</denchmark-link>\n  for it.\n \t\t"}}}, "commit": {"commit_id": "3459a546672303204a4ae6efcc2613a90f003903", "commit_author": "Kevin Trebing", "commitT": "2020-05-19 13:16:26-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "540", "file_CCN": "128", "file_NToken": "2661"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "354,355,356", "deleted_lines": "350,351,352", "method_info": {"method_name": "train", "method_params": "self", "method_startline": "302", "method_endline": "377", "method_complexity": {"method_NLOC": "43", "method_CCN": "19", "method_NToken": "301", "method_nesting_level": "1"}}}}}}}}