{"BR": {"BR_id": "4276", "BR_author": "neergaard", "BRopenT": "2020-10-21T08:16:15Z", "BRcloseT": "2020-10-26T11:57:04Z", "BR_text": {"BRsummary": "WandbLogger fails in 1.0.2 due to non-JSON serializable object", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n After updating to PL 1.0.2, the WandbLogger fails with the following TypeError:\n <denchmark-code>Traceback (most recent call last):\n   File \"wandblogger_issue.py\", line 12, in <module>\n     wandb_logger.log_hyperparams(vars(args))\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 35, in wrapped_fn\n     return fn(*args, **kwargs)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py\", line 138, in log_hyperparams\n     self.experiment.config.update(params, allow_val_change=True)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/sdk/wandb_config.py\", line 87, in update\n     self._callback(data=self._as_dict())\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 587, in _config_callback\n     self._backend.interface.publish_config(data)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/interface/interface.py\", line 496, in publish_config\n     cfg = self._make_config(config_dict)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/interface/interface.py\", line 232, in _make_config\n     update.value_json = json_dumps_safer(json_friendly(v)[0])\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/util.py\", line 524, in json_dumps_safer\n     return json.dumps(obj, cls=WandBJSONEncoder, **kwargs)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/__init__.py\", line 238, in dumps\n     **kw).encode(obj)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/encoder.py\", line 199, in encode\n     chunks = self.iterencode(o, _one_shot=True)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/encoder.py\", line 257, in iterencode\n     return _iterencode(o, 0)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/util.py\", line 480, in default\n     return json.JSONEncoder.default(self, obj)\n   File \"/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/encoder.py\", line 179, in default\n     raise TypeError(f'Object of type {o.__class__.__name__} '\n TypeError: Object of type function is not JSON serializable\n </denchmark-code>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Run the following code snippet to reproduce:\n <denchmark-code>from argparse import ArgumentParser\n from pprint import pprint\n \n from pytorch_lightning import Trainer\n from pytorch_lightning.loggers import WandbLogger\n \n \n if __name__ == \"__main__\":\n \n     parser = ArgumentParser()\n     parser = Trainer.add_argparse_args(parent_parser=parser)\n     args = parser.parse_args()\n     pprint(vars(args))\n     wandb_logger = WandbLogger()\n     wandb_logger.log_hyperparams(vars(args))\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Hyperparams are logged as usual without any TypeError.\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>* CUDA:\n         - GPU:\n         - available:         False\n         - version:           10.2\n * Packages:\n         - numpy:             1.19.1\n         - pyTorch_debug:     False\n         - pyTorch_version:   1.6.0\n         - pytorch-lightning: 1.0.2\n         - tensorboard:       2.3.0\n         - tqdm:              4.50.2\n * System:\n         - OS:                Linux\n         - architecture:\n                 - 64bit\n                 - \n         - processor:         x86_64\n         - python:            3.7.9\n         - version:           #1 SMP Mon Jul 29 17:46:05 UTC 2019\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n Pretty printing the arguments gives the following clue about the error:\n <denchmark-code>{'accelerator': None,\n  'accumulate_grad_batches': 1,\n  'amp_backend': 'native',\n  'amp_level': 'O2',\n  'auto_lr_find': False,\n  'auto_scale_batch_size': False,\n  'auto_select_gpus': False,\n  'automatic_optimization': True,\n  'benchmark': False,\n  'check_val_every_n_epoch': 1,\n  'checkpoint_callback': True,\n  'default_root_dir': None,\n  'deterministic': False,\n  'distributed_backend': None,\n  'fast_dev_run': False,\n  'flush_logs_every_n_steps': 100,\n  'gpus': <function _gpus_arg_default at 0x7f26b7788f80>,\n  'gradient_clip_val': 0,\n  'limit_test_batches': 1.0,\n  'limit_train_batches': 1.0,\n  'limit_val_batches': 1.0,\n  'log_every_n_steps': 50,\n  'log_gpu_memory': None,\n  'logger': True,\n  'max_epochs': 1000,\n  'max_steps': None,\n  'min_epochs': 1,\n  'min_steps': None,\n  'num_nodes': 1,\n  'num_processes': 1,\n  'num_sanity_val_steps': 2,\n  'overfit_batches': 0.0,\n  'precision': 32,\n  'prepare_data_per_node': True,\n  'process_position': 0,\n  'profiler': None,\n  'progress_bar_refresh_rate': 1,\n  'reload_dataloaders_every_epoch': False,\n  'replace_sampler_ddp': True,\n  'resume_from_checkpoint': None,\n  'sync_batchnorm': False,\n  'terminate_on_nan': False,\n  'tpu_cores': <function _gpus_arg_default at 0x7f26b7788f80>,\n  'track_grad_norm': -1,\n  'truncated_bptt_steps': None,\n  'val_check_interval': 1.0,\n  'weights_save_path': None,\n  'weights_summary': 'top'}\n </denchmark-code>\n \n I assume the issue comes from the gpus and tpu_cores values, which are function calls, when not explicitly supplied as arguments.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "neergaard", "commentT": "2020-10-21T12:19:55Z", "comment_text": "\n \t\tTrainer.add_argparse_args adds some functions to the args Namespace which are not JSON serializable, so an error is thrown when WandbLogger tries to save the hyperparameters of the run. I temporarily got around the issue by removing functions before calling save_hyperparameters, but this definitely needs a fix.\n <denchmark-code>class MyModel(LightningModule):\n     def __init__(self, hparams, *args, **kwargs):\n         super().__init__()\n         self.save_hyperparameters({k:v for (k,v) in vars(hparams).items() if not callable(v)})\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "neergaard", "commentT": "2020-10-22T07:45:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ursulean>@ursulean</denchmark-link>\n  As a temporary solution, this works great thanks!\n But definitely, it needs a proper fix.\n I am not sure what the changes between 0.9.0 and 1.0 are since both of them sets the default arg to  (see here for <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/b40de5464a953ff5866a255f4670d318bd8fd65a/pytorch_lightning/trainer/trainer.py#L770>0.9.0</denchmark-link>\n  and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/5c153c224442c8315a2b8ddc0b64a24dc6798aa3/pytorch_lightning/utilities/argparse_utils.py#L188>1.0.0</denchmark-link>\n ), but I might have overlooked something obvious.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "neergaard", "commentT": "2020-10-23T07:56:50Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>\n ,\n Would you mind creating a test to reproduce this bug (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py</denchmark-link>\n ).\n Best regards,\n T.C\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "neergaard", "commentT": "2020-10-23T08:13:37Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  sure, do you want me to just do it in a Colab notebook or do you want a gist with the script you linked to?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "neergaard", "commentT": "2020-10-23T08:20:37Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>\n , I would prefer the gist :) Easier to integrated in our tests :)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "neergaard", "commentT": "2020-10-23T08:21:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  For know here's a gist with the test (<denchmark-link:https://gist.github.com/neergaard/ed0620ab9405b79d420b99db3e43605a>https://gist.github.com/neergaard/ed0620ab9405b79d420b99db3e43605a</denchmark-link>\n ). I've basically inserted the code snippet I supplied in my orig post without deleting anything of the bug report code, but it should run and return the TypeError still.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "neergaard", "commentT": "2020-10-23T08:40:15Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>\n ,\n Without the provided arguments, I can't use the gist :)\n <denchmark-link:https://user-images.githubusercontent.com/12861981/96976415-bcbd0c00-1513-11eb-8f34-24673a6851fc.png></denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "neergaard", "commentT": "2020-10-23T08:42:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  That's why I asked if you preferred a gist or a colab notebook, as the script as such does not function in a notebook, but the gist works using the command line.\n I don't know how to get the default arguments from the Trainer in a notebook?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "neergaard", "commentT": "2020-10-23T08:46:01Z", "comment_text": "\n \t\tHey, you can provide the command line as a string :)\n <denchmark-code>opt = \"--name_1 arg_1 .... --name_n arg_n\".split(\" \")\n parser = ArgumentParser()\n parser = Trainer.add_argparse_args(parent_parser=parser)\n args = parser.parse_args(opt)\n </denchmark-code>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "neergaard", "commentT": "2020-10-23T08:51:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  D'oh! Simple solution works great, thanks!\n I've updated the gist, can you try it now?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "neergaard", "commentT": "2020-10-23T08:54:02Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>\n  ,\n I will also update bug_report to add this trick :) Thanks for asking about it :)\n Best regards\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "neergaard", "commentT": "2020-10-23T08:59:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  thanks for helping out!\n I can add that I investigated the issue more, and it doesn't seem to be a problem in WandB version 0.10.8, but it is an issue in version 0.10.7.\n edit: what I mean is, using wandb==0.10.8 does not result in a TypeError, but I still think the _gpus_arg_default default value should be handled properly in Pytorch Lightning.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "neergaard", "commentT": "2020-10-23T09:02:16Z", "comment_text": "\n \t\tI will look into this afternoon or tomorrow. Feel free to investigate and submit a PR if you find the bug :)\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "neergaard", "commentT": "2020-10-23T11:06:11Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>\n ,\n Feel free to have a look at the PR.\n Best,\n T.C\n \t\t"}}}, "commit": {"commit_id": "f07ee33db679a4b4bdcb4a2a221aa5cbb05d7b34", "commit_author": "chaton", "commitT": "2020-10-26 11:57:03+00:00", "commit_complexity": {"commit_NLOC": "0.627906976744186", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": ".gitignore", "file_new_name": ".gitignore", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "141", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "14,17,20,23,26,29,31,32,35,36,37,38,41,44,47,51", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\loggers\\base.py", "file_new_name": "pytorch_lightning\\loggers\\base.py", "file_complexity": {"file_NLOC": "351", "file_CCN": "81", "file_NToken": "1758"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "182,183,184,185,186,187,188,189,190,191,192", "deleted_lines": null, "method_info": {"method_name": "_sanitize_callable_params._sanitize_callable", "method_params": "val", "method_startline": "182", "method_endline": "192", "method_complexity": {"method_NLOC": "10", "method_CCN": "4", "method_NToken": "43", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194", "deleted_lines": null, "method_info": {"method_name": "_sanitize_callable_params", "method_params": "str", "method_startline": "172", "method_endline": "194", "method_complexity": {"method_NLOC": "12", "method_CCN": "2", "method_NToken": "41", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\wandb.py", "file_new_name": "pytorch_lightning\\loggers\\wandb.py", "file_complexity": {"file_NLOC": "127", "file_CCN": "18", "file_NToken": "625"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "138", "deleted_lines": null, "method_info": {"method_name": "log_hyperparams", "method_params": "self,str", "method_startline": "135", "method_endline": "139", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "59", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\loggers\\test_wandb.py", "file_new_name": "tests\\loggers\\test_wandb.py", "file_complexity": {"file_NLOC": "86", "file_CCN": "8", "file_NToken": "707"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "130,131", "deleted_lines": null, "method_info": {"method_name": "test_wandb_sanitize_callable_params.wrapper_something", "method_params": "", "method_startline": "130", "method_endline": "131", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "6", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140", "deleted_lines": null, "method_info": {"method_name": "test_wandb_sanitize_callable_params", "method_params": "tmpdir", "method_startline": "116", "method_endline": "140", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "107", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "126,127", "deleted_lines": null, "method_info": {"method_name": "test_wandb_sanitize_callable_params.return_something", "method_params": "", "method_startline": "126", "method_endline": "127", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "6", "method_nesting_level": "1"}}}}}}}}