{"BR": {"BR_id": "1119", "BR_author": "xingzhaolee", "BRopenT": "2020-03-11T13:02:06Z", "BRcloseT": "2020-03-12T14:50:01Z", "BR_text": {"BRsummary": "Checkpoint fails in single node multi-GPU mode using  DDP", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Checkpoint fails in single node multi-GPU mode using  DDP.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n python pl_examples/basic_examples/gpu_template.py --distributed_backend ddp --gpus 2\n Epoch 2: : 700it [00:28, 42.69it/s, l/home/xz/anaconda3/envs/x/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown                                                                                                                                                                                                                 \n   len(cache))\n Traceback (most recent call last):\n   File \"gpu_template.py\", line 79, in <module>\n     main(hyperparams)\n   File \"gpu_template.py\", line 40, in main\n     trainer.fit(model)\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 590, in fit\n     mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n     while not spawn_context.join():\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\n     raise Exception(msg)\n Exception: \n \n -- Process 1 terminated with the following error:\n Traceback (most recent call last):\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n     fn(i, *args)\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 342, in ddp_train\n     self.run_pretrain_routine(model)\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 830, in run_pretrain_routine\n     self.train()\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 343, in train\n     self.run_training_epoch()\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch\n     self.call_checkpoint_callback()\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 737, in call_checkpoint_callback\n     self.checkpoint_callback.on_validation_end(self, self.get_model())\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 204, in on_validation_end\n     self._do_check_save(filepath, current, epoch)\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 221, in _do_check_save\n     self._del_model(delpath)\n   File \"/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 121, in _del_model\n     os.remove(filepath)\n FileNotFoundError: [Errno 2] No such file or directory: '/home/xz/pytorch-lightning/pl_examples/basic_examples/lightning_logs/version_1/checkpoints/epoch=0.ckpt'\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xingzhaolee", "commentT": "2020-03-11T13:43:26Z", "comment_text": "\n \t\tyeah we shall run all examples in CI too\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xingzhaolee", "commentT": "2020-03-11T20:39:10Z", "comment_text": "\n \t\tI believe this happens with multiple gpus as well. And it only seems to happen if ModelCheckpoint(save_top_k) is set greater than 1. Still converting models to 0.7.1 but wanted to share this ...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "xingzhaolee", "commentT": "2020-03-12T02:19:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  fixed. part of the code that caused the bug was removed a few commits back.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "xingzhaolee", "commentT": "2020-03-12T03:28:12Z", "comment_text": "\n \t\tfix is in master?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "xingzhaolee", "commentT": "2020-03-12T03:32:04Z", "comment_text": "\n \t\tfix for DDP checkpoint is in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1125>#1125</denchmark-link>\n , still waiting for it to be reviewed and merged.\n \n I believe this happens with multiple gpus as well. And it only seems to happen if ModelChckepoint(save_top_k) is set greater than 1. Still converting models to 0.7.1 but wanted to share this ...\n \n as for this issue, on my side it seems to work fine. can you double check?\n \t\t"}}}, "commit": {"commit_id": "b4d4e489bf413ebf3288d29c5905d2292ce18d58", "commit_author": "Xing Zhao LEE", "commitT": "2020-03-12 15:50:00+01:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_new_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_complexity": {"file_NLOC": "187", "file_CCN": "27", "file_NToken": "918"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "178,179,180,181", "deleted_lines": null, "method_info": {"method_name": "on_validation_end", "method_params": "self,trainer,pl_module", "method_startline": "177", "method_endline": "218", "method_complexity": {"method_NLOC": "33", "method_CCN": "10", "method_NToken": "187", "method_nesting_level": "1"}}}}}}}}