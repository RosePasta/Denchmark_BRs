{"BR": {"BR_id": "4275", "BR_author": "tchaton", "BRopenT": "2020-10-21T07:54:54Z", "BRcloseT": "2020-10-21T14:06:43Z", "BR_text": {"BRsummary": "[HOT-BUG] Checkpoint in callbacks list fails", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n <denchmark-h:h2>Please reproduce using the BoringModel and post here</denchmark-h>\n \n The ModelCheckpoint is not properly setup when provided through the list of callbacks.\n <denchmark-code>def test_checkpoint_within_callbacks_list(tmpdir):\n     \"\"\"\n     This test validates that the checkpoint can be called when provided to callacks list\n     \"\"\"\n \n     os.environ['PL_DEV_DEBUG'] = '1'\n \n     checkpoint_callback = ModelCheckpoint(monitor='val_loss', filepath=osp.join(tmpdir, \"{epoch:02d}\"))\n \n     class ExtendedBoringModel(BoringModel):\n \n         def validation_step(self, batch, batch_idx):\n             output = self.layer(batch)\n             loss = self.loss(batch, output)\n             return {\"val_loss\": loss}\n \n     model = ExtendedBoringModel()\n     model.validation_step_end = None\n     model.validation_epoch_end = None\n     trainer = pl.Trainer(max_epochs=1, \n                          limit_train_batches=2, \n                          limit_val_batches=2, \n                          limit_test_batches=2, \n                          callbacks=[checkpoint_callback])\n \n     trainer.fit(model)\n     assert os.listdir(tmpdir) == ['epoch=00.ckpt]']\n </denchmark-code>\n \n <denchmark-code>\n tests/checkpointing/test_model_checkpoint.py:590: \n _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n pytorch_lightning/trainer/trainer.py:439: in fit\n     results = self.accelerator_backend.train()\n pytorch_lightning/accelerators/cpu_accelerator.py:48: in train\n     results = self.train_or_test()\n pytorch_lightning/accelerators/accelerator.py:66: in train_or_test\n     results = self.trainer.train()\n pytorch_lightning/trainer/trainer.py:482: in train\n     self.train_loop.run_training_epoch()\n pytorch_lightning/trainer/training_loop.py:569: in run_training_epoch\n     self.trainer.run_evaluation(test_mode=False)\n pytorch_lightning/trainer/trainer.py:609: in run_evaluation\n     self.evaluation_loop.on_evaluation_end()\n pytorch_lightning/trainer/evaluation_loop.py:109: in on_evaluation_end\n     self.trainer.call_hook('on_validation_end', *args, **kwargs)\n pytorch_lightning/trainer/trainer.py:822: in call_hook\n     trainer_hook(*args, **kwargs)\n pytorch_lightning/trainer/callback_hook.py:177: in on_validation_end\n     callback.on_validation_end(self, self.get_model())\n pytorch_lightning/callbacks/model_checkpoint.py:167: in on_validation_end\n     self.save_checkpoint(trainer, pl_module)\n pytorch_lightning/callbacks/model_checkpoint.py:213: in save_checkpoint\n     self._save_top_k_checkpoints(monitor_candidates, trainer, pl_module, epoch, filepath)\n pytorch_lightning/callbacks/model_checkpoint.py:494: in _save_top_k_checkpoints\n     self._update_best_and_save(filepath, current, epoch, trainer, pl_module)\n pytorch_lightning/callbacks/model_checkpoint.py:543: in _update_best_and_save\n     self._save_model(filepath, trainer, pl_module)\n _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n \n self = <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x13a775b80>\n filepath = '/private/var/folders/q9/1sc07dt91w9581rl1vt6vd4w0000gn/T/pytest-of-thomas/pytest-1058/test_checkpoint_within_callbac0/epoch=00.ckpt'\n trainer = <pytorch_lightning.trainer.trainer.Trainer object at 0x13a7756a0>, pl_module = ExtendedBoringModel(\n   (layer): Linear(in_features=32, out_features=2, bias=True)\n )\n \n     def _save_model(self, filepath: str, trainer, pl_module):\n         # in debugging, track when we save checkpoints\n         trainer.dev_debugger.track_checkpointing_history(filepath)\n     \n         # make paths\n         if trainer.is_global_zero:\n             self._fs.makedirs(os.path.dirname(filepath), exist_ok=True)\n     \n         # delegate the saving to the trainer\n         if self.save_function is not None:\n             self.save_function(filepath, self.save_weights_only)\n         else:\n >           raise ValueError(\".save_function() not set\")\n E           ValueError: .save_function() not set\n \n pytorch_lightning/callbacks/model_checkpoint.py:297: ValueError\n ----------------------------------------------------------------------------------------- Captured log call ------------------------------------------------------------------------------------------\n INFO     lightning:distributed.py:49 GPU available: False, used: False\n INFO     lightning:distributed.py:49 TPU available: False, using: 0 TPU cores\n INFO     lightning:lightning.py:1290 \n   | Name  | Type   | Params\n ---------------------------------\n 0 | layer | Linear | 66\n ========================================================================================== warnings summary ==========================================================================================\n .venv/lib/python3.8/site-packages/comet_ml/monkey_patching.py:19\n   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/comet_ml/monkey_patching.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n     import imp\n \n .venv/lib/python3.8/site-packages/pandas/compat/__init__.py:120\n   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n     warnings.warn(msg)\n \n .venv/lib/python3.8/site-packages/wandb/util.py:35\n   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/wandb/util.py:35: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n     from collections import namedtuple, Mapping, Sequence\n \n .venv/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55\n   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n     assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\n \n tests/checkpointing/test_model_checkpoint.py::test_checkpoint_within_callbacks_list\n   /Users/thomas/Documents/projects/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n     warnings.warn(*args, **kwargs)\n \n tests/checkpointing/test_model_checkpoint.py::test_checkpoint_within_callbacks_list\n   /Users/thomas/Documents/projects/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n     warnings.warn(*args, **kwargs)\n \n -- Docs: https://docs.pytest.org/en/stable/warnings.html\n ====================================================================================== short test summary info =======================================================================================\n FAILED tests/checkpointing/test_model_checkpoint.py::test_checkpoint_within_callbacks_list - ValueError: .save_function() not set\n </denchmark-code>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n Please copy and paste the output from our\n <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>\n \n (or fill out the checklist below manually).\n You can get the script and run it with:\n <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py\n # For security purposes, please check the contents of collect_env_details.py before running it.\n python collect_env_details.py\n </denchmark-code>\n \n \n PyTorch Version (e.g., 1.0):\n OS (e.g., Linux):\n How you installed PyTorch (conda, pip, source):\n Build command you used (if compiling from source):\n Python version:\n CUDA/cuDNN version:\n GPU models and configuration:\n Any other relevant information:\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tchaton", "commentT": "2020-10-21T08:05:18Z", "comment_text": "\n \t\tisn't passing ModelCheckpoint in callbacks flag WIP?\n here is the PR: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3990>#3990</denchmark-link>\n \n <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tchaton", "commentT": "2020-10-21T12:23:10Z", "comment_text": "\n \t\tYes exactly. ModelCheckpoint (currently) is special, meaning there can only be one and you have to pass it to Trainer(checkpoint_callback=), passing to callbacks argument is not allowed.\n In my PR I try to remove this limitation partially, and it requires several stages of changes and multiple PRs to get this done.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tchaton", "commentT": "2020-10-21T12:42:35Z", "comment_text": "\n \t\tthe solution here is to move the save function inside the callback. no need to have these separated anymore\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "tchaton", "commentT": "2020-10-21T13:03:32Z", "comment_text": "\n \t\tThat's exactly what I'm doing here:\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3990>#3990</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "8a20d6af51d13adec37593c1356ce08ef380e828", "commit_author": "William Falcon", "commitT": "2020-10-21 10:06:42-04:00", "commit_complexity": {"commit_NLOC": "0.15", "commit_CCN": "1.0", "commit_Nprams": "0.75"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_new_name": "pytorch_lightning\\callbacks\\model_checkpoint.py", "file_complexity": {"file_NLOC": "426", "file_CCN": "86", "file_NToken": "2244"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "162", "deleted_lines": null, "method_info": {"method_name": "on_pretrain_routine_start", "method_params": "self,trainer,pl_module", "method_startline": "157", "method_endline": "162", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "25", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\connectors\\callback_connector.py", "file_new_name": "pytorch_lightning\\trainer\\connectors\\callback_connector.py", "file_complexity": {"file_NLOC": "52", "file_CCN": "11", "file_NToken": "277"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "64,65", "method_info": {"method_name": "init_default_checkpoint_callback", "method_params": "self,checkpoint_callback", "method_startline": "59", "method_endline": "67", "method_complexity": {"method_NLOC": "8", "method_CCN": "4", "method_NToken": "42", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\checkpointing\\test_model_checkpoint.py", "file_new_name": "tests\\checkpointing\\test_model_checkpoint.py", "file_complexity": {"file_NLOC": "394", "file_CCN": "47", "file_NToken": "3003"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "529,530,531,532", "deleted_lines": null, "method_info": {"method_name": "test_checkpoint_within_callbacks_list.validation_step", "method_params": "self,batch,batch_idx", "method_startline": "529", "method_endline": "532", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "33", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546", "deleted_lines": null, "method_info": {"method_name": "test_checkpoint_within_callbacks_list", "method_params": "tmpdir", "method_startline": "518", "method_endline": "546", "method_complexity": {"method_NLOC": "17", "method_CCN": "1", "method_NToken": "101", "method_nesting_level": "0"}}}}}}}}