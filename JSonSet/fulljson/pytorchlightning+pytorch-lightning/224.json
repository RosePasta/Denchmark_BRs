{"BR": {"BR_id": "224", "BR_author": "ananyahjha93", "BRopenT": "2019-09-15T19:54:42Z", "BRcloseT": "2019-09-16T16:35:30Z", "BR_text": {"BRsummary": "set_epoch for DistributedSampler", "BRdescription": "\n Describe the bug\n PyTorch example suggests the use set_epoch function for DistributedSampler class before each epoch start. I could not find this function call in lightning's trainer module.\n <denchmark-link:https://github.com/pytorch/examples/blob/master/imagenet/main.py>https://github.com/pytorch/examples/blob/master/imagenet/main.py</denchmark-link>\n \n Line 232-234\n As can be seen from the DistributedSampler class code (<denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py>https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py</denchmark-link>\n ), the set_epoch function is required to set the seed for each  function call.\n Can you confirm if this function has been called on DistributedSampler (for training dataset) at some point in lightning's trainer module?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ananyahjha93", "commentT": "2019-09-15T22:28:13Z", "comment_text": "\n \t\tit's not called which means it is equivalent to shuffle=False (ie: shuffled once but not again every epoch). We can add it though to enable shuffling\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ananyahjha93", "commentT": "2019-09-15T22:28:21Z", "comment_text": "\n \t\tdo you want to submit that PR?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ananyahjha93", "commentT": "2019-09-16T07:21:25Z", "comment_text": "\n \t\tAdded a PR for this, do you want me to update any docs in my commit related to this issue? Also, I have successfully tested the library with this fix included in my experiments.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ananyahjha93", "commentT": "2020-11-02T19:26:27Z", "comment_text": "\n \t\t\n it's not called which means it is equivalent to shuffle=False (ie: shuffled once but not again every epoch). We can add it though to enable shuffling\n \n A short note on this for future readers:\n No setting epochs in DistributedSampler does not equivalent to shuffle=False (for both dataloader and distributed sampler).\n \t\t"}}}, "commit": {"commit_id": "c0f3b6b035f955fc371dec412d3816712f3fc1dd", "commit_author": "Ananya Harsh Jha", "commitT": "2019-09-16 10:21:00-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "777", "file_CCN": "237", "file_NToken": "4924"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "932,933,934,935", "deleted_lines": null, "method_info": {"method_name": "__train", "method_params": "self", "method_startline": "929", "method_endline": "970", "method_complexity": {"method_NLOC": "23", "method_CCN": "10", "method_NToken": "158", "method_nesting_level": "1"}}}}}}}}