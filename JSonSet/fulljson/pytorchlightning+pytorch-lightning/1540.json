{"BR": {"BR_id": "1540", "BR_author": "kevinc13", "BRopenT": "2020-04-20T22:25:22Z", "BRcloseT": "2020-04-21T18:29:16Z", "BR_text": {"BRsummary": "DDP on GPUs invalid ordinal", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n On latest version (master), training with DDP backend on GPUs (let's say 6,7) results in a CUDA error \"Invalid device ordinal.\"\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Run any Lightning Module with DDP backend on more than 1 GPU with GPU indexes that do not start from 0\n See error\n \n <denchmark-code>INFO:lightning:GPU available: True, used: True\n INFO:lightning:VISIBLE GPUS: 6,7\n WARNING:lightning:SLURM_NODEID or NODE_RANK environment variable is not defined. Set as 0.\n WARNING:lightning:MASTER_ADDR environment variable is not defined. Set as localhost\n WARNING:lightning:SLURM_NODEID or NODE_RANK environment variable is not defined. Set as 0.\n WARNING:lightning:MASTER_ADDR environment variable is not defined. Set as localhost\n THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal\n THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal\n Traceback (most recent call last):\n   File \"bin/run.py\", line 110, in <module>\n     run(config, args.mode)\n   File \"bin/run.py\", line 86, in run\n     trainer.fit(system)\n   File \"/home/kevin/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 744, in fit\n     mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\n   File \"/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n     while not spawn_context.join():\n   File \"/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\n     raise Exception(msg)\n Exception:\n \n -- Process 1 terminated with the following error:\n Traceback (most recent call last):\n   File \"/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n     fn(i, *args)\n   File \"/home/kevin/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 332, in ddp_train\n     torch.cuda.set_device(self.root_gpu)\n   File \"/home/kevin/miniconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\", line 292, in set_device\n     torch._C._cuda_setDevice(device)\n RuntimeError: cuda runtime error (101) : invalid device ordinal at /pytorch/torch/csrc/cuda/Module.cpp:59\n \n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n I expect this error to not occur.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n - GPU:\n - TITAN RTX\n - TITAN RTX\n - TITAN RTX\n - TITAN RTX\n - TITAN RTX\n - TITAN RTX\n - TITAN RTX\n - TITAN RTX\n - available:         True\n - version:           10.1\n Packages:\n - numpy:             1.17.4\n - pyTorch_debug:     False\n - pyTorch_version:   1.4.0\n - pytorch-lightning: 0.7.4rc1\n - tensorboard:       2.0.0\n - tqdm:              4.45.0\n System:\n - OS:                Linux\n - architecture:\n - 64bit\n -\n - processor:         x86_64\n - python:            3.7.4\n - version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n My best guess is that due to the new refactoring for ddp cpu backend, setting the CUDA device to root_gpu instead of using the provided gpu_idx parameter in ddp_train() is causing the error. Specifically, for 2 gpu, it's using index 6 instead of index 0 and index 7 instead of index 1 since we're setting CUDA_VISIBLE_DEVICES=6,7.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kevinc13", "commentT": "2020-04-20T22:25:56Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kevinc13", "commentT": "2020-04-20T22:51:53Z", "comment_text": "\n \t\tgood catch. mind submitting a PR?\n \t\t"}}}, "commit": {"commit_id": "bafdeca42f746aac59b4f0c1103264d7bff556db", "commit_author": "Kevin Chen", "commitT": "2020-04-21 14:29:15-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_complexity": {"file_NLOC": "294", "file_CCN": "60", "file_NToken": "1122"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "330", "deleted_lines": "330", "method_info": {"method_name": "ddp_train", "method_params": "self,process_idx,model", "method_startline": "280", "method_endline": "359", "method_complexity": {"method_NLOC": "39", "method_CCN": "12", "method_NToken": "299", "method_nesting_level": "1"}}}}}}}}