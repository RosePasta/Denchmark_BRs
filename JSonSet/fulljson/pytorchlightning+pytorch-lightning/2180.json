{"BR": {"BR_id": "2180", "BR_author": "Nilanshrajput", "BRopenT": "2020-06-14T09:57:52Z", "BRcloseT": "2020-06-17T14:52:59Z", "BR_text": {"BRsummary": "Global Gradient calculation is turned off during validation step.", "BRdescription": "\n If an error occurs during the validation step, the tradition calculation is turned off for the runtime, you have to either specifically enable it or restart runtime!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Nilanshrajput", "commentT": "2020-06-15T17:28:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Nilanshrajput>@Nilanshrajput</denchmark-link>\n  is it a question?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Nilanshrajput", "commentT": "2020-06-15T17:31:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n , no it's a bug!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Nilanshrajput", "commentT": "2020-06-15T19:45:09Z", "comment_text": "\n \t\tcould you please describe more what is happening or give an example?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Nilanshrajput", "commentT": "2020-06-15T20:09:08Z", "comment_text": "\n \t\tI used the  MNIST example for demonstrating the error. <denchmark-link:https://colab.research.google.com/drive/1LWlUurpIu_pD4fP_cUgwhF5f0nS6fvb4?usp=sharing>https://colab.research.google.com/drive/1LWlUurpIu_pD4fP_cUgwhF5f0nS6fvb4?usp=sharing</denchmark-link>\n \n Here in validation step I added  which will cause the error during execution, to remove the error you can comment that line, but this error will pop-up  . This error is generated because gradient calculation was turned off during the validation step globally, and due to error while executing and never reverted.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Nilanshrajput", "commentT": "2020-06-15T20:24:30Z", "comment_text": "\n \t\t# disable gradients to save memory torch.set_grad_enabled(False) \n This line in trainer.evaluation_loop.py sets global grad calculation False, this is reverted at the end of evaluation loop but that's is never executed if there is an error raised in between, and the model remain in eval mode with gradient calculation disabled.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Nilanshrajput", "commentT": "2020-06-15T20:30:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  This could be resolved in two ways i think:\n \n use local context for disabling grad  with torch.no_grad():. but here also if error occurs the model will remain in eval mode, also for a long piece of code it might not look good.\n At the start of training_loop add following lines model.train() torch.set_grad_enabled(True). This is better method I think.\n I can add a PR if you agree.\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "Nilanshrajput", "commentT": "2020-06-15T20:58:38Z", "comment_text": "\n \t\t\n \n At the start of training_loop add following lines model.train() torch.set_grad_enabled(True). This is better method I think.\n I can add a PR if you agree.\n \n \n Yeah, sending a PR would be great...\n cc: <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "Nilanshrajput", "commentT": "2020-06-16T11:50:42Z", "comment_text": "\n \t\tIf an error is raised, the program should crash no?\n In what scenario do you want to keep training with an error in your validation loop?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "Nilanshrajput", "commentT": "2020-06-16T11:57:49Z", "comment_text": "\n \t\tNo program should crash, but when you are working in notebooks, after resolving the error, the grad calculation does not take place. as in validation step it was turned off and never turned back on.\n Can you try the collab link i provided, after resolving error you have start the runtime.\n \t\t"}}}, "commit": {"commit_id": "25c7465591371f8fe4b4244ccc996706f4136cea", "commit_author": "Nilansh Rajput", "commitT": "2020-06-17 10:52:58-04:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "551", "file_CCN": "129", "file_NToken": "2708"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "322,323,324,325,326,327", "deleted_lines": null, "method_info": {"method_name": "train", "method_params": "self", "method_startline": "309", "method_endline": "411", "method_complexity": {"method_NLOC": "49", "method_CCN": "21", "method_NToken": "341", "method_nesting_level": "1"}}}}}}}}