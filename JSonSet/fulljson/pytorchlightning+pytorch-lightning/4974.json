{"BR": {"BR_id": "4974", "BR_author": "yikuanli", "BRopenT": "2020-12-04T15:02:39Z", "BRcloseT": "2020-12-11T13:51:46Z", "BR_text": {"BRsummary": "AttributeError: 'LightningOptimizer' object has no attribute 'state'", "BRdescription": "\n I am using pytorch lightning with Adam optmiser to train a BYOL model, the model and pipeline works fine while training, when I stop and resume from checkpoint, it raise this error. I didn't do anything just resume from previous checkpoint, have no idea why there is an error like this, I upgrade my bolts and pytorch lightning to the mast (I think its the latest version)\n <denchmark-link:https://user-images.githubusercontent.com/40010984/101178978-8b227f00-3641-11eb-8d72-a3b3ca8c1173.png></denchmark-link>\n \n my opmizer is like this, and Adam is the original Adam provided by pytorch\n <denchmark-link:https://user-images.githubusercontent.com/40010984/101179109-be650e00-3641-11eb-951c-0717e217b532.png></denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "yikuanli", "commentT": "2020-12-04T16:04:57Z", "comment_text": "\n \t\tThanks for the report, it would be really really useful (and get this fixed faster) if you could reproduce this via the bug report model:\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py</denchmark-link>\n \n cc <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "yikuanli", "commentT": "2020-12-04T16:14:19Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>\n ,\n Thanks a lot for testing the latest version of Lightning. I am going to work on this asap.\n If you could reproduce the bug with BoringModel, it will help a lot.\n While waiting for fix, you can use Trainer(enable_pl_optimizer=False).\n Best regards,\n T.C\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "yikuanli", "commentT": "2020-12-04T16:27:52Z", "comment_text": "\n \t\tThanks for the comments, I used BoringModel to test, and seems it doesn't have this problem, I am wondering if the wrapper and LWCA LR is the problem.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "yikuanli", "commentT": "2020-12-04T16:38:46Z", "comment_text": "\n \t\t<denchmark-code>import os\n import torch\n from torch.utils.data import Dataset\n from pytorch_lightning import Trainer, LightningModule\n from pytorch_lightning.callbacks import ModelCheckpoint\n import pytorch_lightning as pl\n from pl_bolts.optimizers.lars_scheduling import LARSWrapper\n from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n \n class CheckpointEveryNSteps(pl.Callback):\n     \"\"\"\n     Save a checkpoint every N steps, instead of Lightning's default that checkpoints\n     based on validation loss.\n     \"\"\"\n \n     def __init__(\n         self,\n         save_step_frequency,\n         prefix=\"latest-Checkpoint\",\n         use_modelcheckpoint_filename=False,\n     ):\n         \"\"\"\n         Args:\n             save_step_frequency: how often to save in steps\n             prefix: add a prefix to the name, only used if\n                 use_modelcheckpoint_filename=False\n             use_modelcheckpoint_filename: just use the ModelCheckpoint callback's\n                 default filename, don't use ours.\n         \"\"\"\n         self.save_step_frequency = save_step_frequency\n         self.prefix = prefix\n         self.use_modelcheckpoint_filename = use_modelcheckpoint_filename\n \n     def on_batch_end(self, trainer: pl.Trainer, _):\n         \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n         global_step = trainer.global_step\n         if global_step % self.save_step_frequency == 0:\n             if self.use_modelcheckpoint_filename:\n                 filename = trainer.checkpoint_callback.filename\n             else:\n                 filename = \"{}.ckpt\".format(self.prefix)\n             ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n             trainer.save_checkpoint(ckpt_path)\n \n \n class RandomDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         return self.data[index]\n \n     def __len__(self):\n         return self.len\n \n \n class BoringModel(LightningModule):\n \n     def __init__(self):\n         \"\"\"\n         Testing PL Module\n         Use as follows:\n         - subclass\n         - modify the behavior for what you want\n         class TestModel(BaseTestModel):\n             def training_step(...):\n                 # do your own thing\n         or:\n         model = BaseTestModel()\n         model.training_epoch_end = None\n         \"\"\"\n         super().__init__()\n         self.layer = torch.nn.Linear(32, 2)\n \n     def forward(self, x):\n         return self.layer(x)\n \n     def loss(self, batch, prediction):\n         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\n \n     def step(self, x):\n         x = self.layer(x)\n         out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\n         return out\n \n     def training_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         return {\"loss\": loss}\n \n     def training_step_end(self, training_step_outputs):\n         return training_step_outputs\n \n     def training_epoch_end(self, outputs) -> None:\n         torch.stack([x[\"loss\"] for x in outputs]).mean()\n \n     def validation_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         return {\"x\": loss}\n \n     def validation_epoch_end(self, outputs) -> None:\n         torch.stack([x['x'] for x in outputs]).mean()\n \n     def test_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         return {\"y\": loss}\n \n     def test_epoch_end(self, outputs) -> None:\n         torch.stack([x[\"y\"] for x in outputs]).mean()\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.Adam(self.parameters(), lr=0.1)\n \n         optimizer = LARSWrapper(optimizer)\n         scheduler = LinearWarmupCosineAnnealingLR(\n             optimizer,\n             warmup_epochs= 1, \n             max_epochs= 20\n         )\n         return [optimizer], [scheduler]\n \n \n def run_test():\n \n     class TestModel(BoringModel):\n \n         def on_train_epoch_start(self) -> None:\n             print('override any method to prove your bug')\n \n \n     train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=1)\n     val_data = torch.utils.data.DataLoader(RandomDataset(32, 64),batch_size=1)\n     test_data = torch.utils.data.DataLoader(RandomDataset(32, 64),batch_size=1)\n \n \n     checkpoint_callback = ModelCheckpoint(monitor='loss', mode= 'min', filepath='./checkpoint')\n     model = TestModel()\n     trainer = Trainer(\n         default_root_dir=os.getcwd(),\n         resume_from_checkpoint='./latest-Checkpoint.ckpt',\n         max_epochs=10,\n         weights_summary=None,\n         accelerator= 'ddp',\n         log_every_n_steps=1,\n         gpus=1,\n         checkpoint_callback= checkpoint_callback,\n         callbacks=[CheckpointEveryNSteps(1)]\n     )\n     trainer.fit(model, train_data, val_data)\n \n \n if __name__ == '__main__':\n     run_test()\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "yikuanli", "commentT": "2020-12-04T16:39:22Z", "comment_text": "\n \t\tI reproduce the error, it is because of the ddp, if I train with ddp, this error occurs\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "yikuanli", "commentT": "2020-12-04T16:57:47Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>\n ,\n Let me prioritise this :)\n Best,\n T.C\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "yikuanli", "commentT": "2020-12-04T20:08:57Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>\n ,\n Would you mind trying this branch: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4981>#4981</denchmark-link>\n .\n I wasn't able to reproduce your bug, but another one. When resolved, it seems to train fine.\n I would be interested to see what you get your bug. Please, have a look at my test if I missed something.\n Best regards,\n T.C\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "yikuanli", "commentT": "2020-12-07T10:13:46Z", "comment_text": "\n \t\tHi, <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n , I am sorry, need to ask how to pull this branch you mentioned in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4981>#4981</denchmark-link>\n . I am not very familiar with those management in GitHub,\n BTW, the way I got that problem is firstly run the boring model train with for example 5 epoch with ddp, and not resume from check point.\n after ran it, I got a checkpoint and resume from that checkpoint, but change to 10 epoch for example, because check point already save the first 5, and I need to increase the number to let it load the model and kept training.\n thanks for helping out, plz let me know how to test that branch ,and I will do it.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "yikuanli", "commentT": "2020-12-07T11:54:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>\n   LARSWrapper should not be passed to a Scheduler, since it is not an optimizer object. We have updated out SimCLR and SwAV code accordingly. I will update the BYOL with this soon.\n To see how LR schedule is set for LARS, refer to:\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/self_supervised/swav/swav_module.py#L330>https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/self_supervised/swav/swav_module.py#L330</denchmark-link>\n \n <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>\n  LARSWrapper's step method takes in a closure and the class itself sets the param_group and state property. The reason we would prefer to keep LARSWrapper as a wrapper class for an Optimizer, instead of an Optimizer itself, is because any Optimizer passed to this wrapper can get the layer-wise LR scaling property of LARS.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "yikuanli", "commentT": "2020-12-09T11:39:50Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>\n ,\n Not sure to understand why Lars can't be an optimizer. I found several implementations making this cleaner: <denchmark-link:https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11>https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11</denchmark-link>\n \n I will look into this deeper.\n Best,\n T.C\n \t\t"}}}, "commit": {"commit_id": "7755572b4f37b811b83f6a933329b01af4735e66", "commit_author": "chaton", "commitT": "2020-12-11 14:51:45+01:00", "commit_complexity": {"commit_NLOC": "0.5", "commit_CCN": "0.9038461538461539", "commit_Nprams": "0.8461538461538461"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\core\\optimizer.py", "file_new_name": "pytorch_lightning\\core\\optimizer.py", "file_complexity": {"file_NLOC": "131", "file_CCN": "36", "file_NToken": "893"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "76", "method_info": {"method_name": "_on_trainer_init", "method_params": "self,trainer", "method_startline": "74", "method_endline": "80", "method_complexity": {"method_NLOC": "7", "method_CCN": "3", "method_NToken": "49", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "114,115,116,117,118", "deleted_lines": "114", "method_info": {"method_name": "__optimizer_step", "method_params": "self,args,None,str,kwargs", "method_startline": "100", "method_endline": "138", "method_complexity": {"method_NLOC": "32", "method_CCN": "7", "method_NToken": "213", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\utilities\\__init__.py", "file_new_name": "pytorch_lightning\\utilities\\__init__.py", "file_complexity": {"file_NLOC": "96", "file_CCN": "11", "file_NToken": "460"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "57", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "requirements\\extra.txt", "file_new_name": "requirements\\extra.txt", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "10", "deleted_lines": "10"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\core\\test_lightning_module.py", "file_new_name": "tests\\core\\test_lightning_module.py", "file_complexity": {"file_NLOC": "69", "file_CCN": "13", "file_NToken": "438"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "58,59,60,61", "deleted_lines": null, "method_info": {"method_name": "test_automatic_optimization_num_calls.training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "58", "method_endline": "61", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "35", "method_nesting_level": "3"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "58,59,60,61,62", "deleted_lines": null, "method_info": {"method_name": "test_automatic_optimization_num_calls", "method_params": "enable_pl_optimizer,tmpdir", "method_startline": "49", "method_endline": "105", "method_complexity": {"method_NLOC": "36", "method_CCN": "7", "method_NToken": "193", "method_nesting_level": "0"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 13, "file_old_name": "tests\\core\\test_lightning_optimizer.py", "file_new_name": "tests\\core\\test_lightning_optimizer.py", "file_complexity": {"file_NLOC": "334", "file_CCN": "62", "file_NToken": "2565"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "154,160", "deleted_lines": "152,158", "method_info": {"method_name": "test_lightning_optimizer_manual_optimization_and_accumulated_gradients.training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "146", "method_endline": "160", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "79", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "200,201", "deleted_lines": "198,199,200", "method_info": {"method_name": "test_state", "method_params": "tmpdir", "method_startline": "192", "method_endline": "207", "method_complexity": {"method_NLOC": "16", "method_CCN": "3", "method_NToken": "126", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "227,228", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface.state", "method_params": "self", "method_startline": "227", "method_endline": "228", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "11", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "238,239,240", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface.step", "method_params": "self", "method_startline": "238", "method_endline": "240", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "2"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "101,107", "deleted_lines": "99,105", "method_info": {"method_name": "test_lightning_optimizer_manual_optimization.training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "93", "method_endline": "107", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "79", "method_nesting_level": "2"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "244,245,246,247", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface.configure_optimizers", "method_params": "self", "method_startline": "244", "method_endline": "247", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "35", "method_nesting_level": "2"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "231,232", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface.param_groups", "method_params": "self", "method_startline": "231", "method_endline": "232", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "11", "method_nesting_level": "2"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "101,107", "deleted_lines": "99,105", "method_info": {"method_name": "test_lightning_optimizer_manual_optimization", "method_params": "mock_sgd_step,mock_adam_step,tmpdir", "method_startline": "87", "method_endline": "135", "method_complexity": {"method_NLOC": "20", "method_CCN": "1", "method_NToken": "96", "method_nesting_level": "0"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface", "method_params": "tmpdir", "method_startline": "210", "method_endline": "256", "method_complexity": {"method_NLOC": "22", "method_CCN": "1", "method_NToken": "73", "method_nesting_level": "0"}}}, "hunk_9": {"Ismethod": 1, "added_lines": "235,236", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface.param_groups", "method_params": "self,value", "method_startline": "235", "method_endline": "236", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "14", "method_nesting_level": "2"}}}, "hunk_10": {"Ismethod": 1, "added_lines": "212,213,214,215,216,217,218,219,220", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface.__init__", "method_params": "self,optimizer", "method_startline": "212", "method_endline": "220", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "75", "method_nesting_level": "2"}}}, "hunk_11": {"Ismethod": 1, "added_lines": "154,160", "deleted_lines": "152,158", "method_info": {"method_name": "test_lightning_optimizer_manual_optimization_and_accumulated_gradients", "method_params": "mock_sgd_step,mock_adam_step,tmpdir", "method_startline": "140", "method_endline": "189", "method_complexity": {"method_NLOC": "21", "method_CCN": "1", "method_NToken": "100", "method_nesting_level": "0"}}}, "hunk_12": {"Ismethod": 1, "added_lines": "223,224", "deleted_lines": null, "method_info": {"method_name": "test_lightning_optimizer_with_wrong_optimizer_interface.__class__", "method_params": "self", "method_startline": "223", "method_endline": "224", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "7", "method_nesting_level": "2"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\trainer\\optimization\\test_manual_optimization.py", "file_new_name": "tests\\trainer\\optimization\\test_manual_optimization.py", "file_complexity": {"file_NLOC": "728", "file_CCN": "120", "file_NToken": "5450"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "905,936,938", "deleted_lines": "905,936,938,939", "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies", "method_params": "mock_sgd_step,mock_adam_step,tmpdir", "method_startline": "864", "method_endline": "939", "method_complexity": {"method_NLOC": "26", "method_CCN": "3", "method_NToken": "134", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "905", "deleted_lines": "905", "method_info": {"method_name": "test_step_with_optimizer_closure_with_different_frequencies.training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "871", "method_endline": "905", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "67", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "828", "deleted_lines": "828", "method_info": {"method_name": "test_step_with_optimizer_closure_and_extra_arguments.training_step", "method_params": "self,batch,batch_idx", "method_startline": "813", "method_endline": "828", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "48", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "828,858", "deleted_lines": "828,858", "method_info": {"method_name": "test_step_with_optimizer_closure_and_extra_arguments", "method_params": "step_mock,tmpdir", "method_startline": "806", "method_endline": "859", "method_complexity": {"method_NLOC": "24", "method_CCN": "2", "method_NToken": "109", "method_nesting_level": "0"}}}}}}}}