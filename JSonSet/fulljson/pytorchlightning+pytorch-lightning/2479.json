{"BR": {"BR_id": "2479", "BR_author": "jgbos", "BRopenT": "2020-07-03T00:34:47Z", "BRcloseT": "2020-10-05T01:27:52Z", "BR_text": {"BRsummary": "init_slurm_connection causing hostname errors", "BRdescription": "\n <denchmark-h:h1>Problem</denchmark-h>\n \n Can you update this function to support checking if MASTER_ADDR and MASTER_PORT are already in os.environ?  Running into some weird errors where this code adds host to MASTER_ADDR and crashes the code.\n \n \n \n pytorch-lightning/pytorch_lightning/core/lightning.py\n \n \n          Line 903\n       in\n       0697dd3\n \n \n \n \n \n \n  def _init_slurm_connection(self) -> None: \n \n \n \n \n \n <denchmark-h:h1>Solution</denchmark-h>\n \n Do you prefer a pull request?  Here's the check I put it\n def _init_slurm_connection(self) -> None:\n         \"\"\"\n         Sets up environment variables necessary for pytorch distributed communications\n         based on slurm environment.\n         \"\"\"\n \n         if 'MASTER_PORT' not in os.environ:\n             # use slurm job id for the port number\n             # guarantees unique ports across jobs from same grid search\n             try:\n                 # use the last 4 numbers in the job id as the id\n                 default_port = os.environ['SLURM_JOB_ID']\n                 default_port = default_port[-4:]\n \n                 # all ports should be in the 10k+ range\n                 default_port = int(default_port) + 15000\n \n             except Exception:\n                 default_port = 12910\n \n             # if user gave a port number, use that one instead\n             try:\n                 default_port = os.environ['MASTER_PORT']\n             except Exception:\n                 os.environ['MASTER_PORT'] = str(default_port)\n \n         # figure out the root node addr\n         if 'MASTER_ADDR' not in os.environ:\n             try:\n                 root_node = os.environ['SLURM_NODELIST'].split(' ')[0]\n             except Exception:\n                 root_node = '127.0.0.1'\n \n             root_node = self.trainer.resolve_root_node_address(root_node)\n             os.environ['MASTER_ADDR'] = root_node\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jgbos", "commentT": "2020-07-13T15:05:21Z", "comment_text": "\n \t\tOk, back on this and tracked down the bug.  The problem is in this line:\n   root_node = os.environ['SLURM_NODELIST'].split(' ')[0]\n On my system, SLURM_NODELIST is a comma delineated list, so split(',') instead of split(' ').\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jgbos", "commentT": "2020-07-13T15:10:02Z", "comment_text": "\n \t\tAdditionally, our system will use this for two systems:\n  SLURM_NODE_LIST=d-10-11-[1-2]\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jgbos", "commentT": "2020-08-04T20:52:07Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  is this fixed?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jgbos", "commentT": "2020-08-26T14:02:47Z", "comment_text": "\n \t\tfyi, this is not fixed in general.  I think the problem is fixed for the example above\n SLURM_NODE_LIST=d-10-11-[1-2]\n but not the case for\n SLURM_NODE_LIST=d-10-11-1,d-11-12-1\n My current hack for PL to run on slurm is to set MASTER_ADDR in the sbatch script and then do the following:\n     # fix slurm node list\n     if 'MASTER_ADDR' in os.environ and 'SLURM_NODELIST' in os.environ:\n         slurm_node_list = os.environ['SLURM_NODELIST'].split(',')\n         master_addr = os.environ['MASTER_ADDR']\n         os.environ['SLURM_NODELIST'] = ' '.join([master_addr] + slurm_node_list)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jgbos", "commentT": "2020-09-22T21:27:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jgbos>@jgbos</denchmark-link>\n  mind sending a PR for this fix?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jgbos", "commentT": "2020-09-23T03:28:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>\n  I currently do this before I run lightning so I  feel my solution is just a  and that there's probably a better solution.  It currently requires the user to set  first.  If I find some time I'll take a look at the code, maybe come upon a real solution once I see how lightning works here.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jgbos", "commentT": "2020-10-05T01:27:52Z", "comment_text": "\n \t\tthis is fixed now! no longer needed to hack anything. You can pass your own ClusterEnvironment and do whatever logic you need.\n <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#cluster-environment>https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#cluster-environment</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "c6df63a58817b6414f8a3ae28edd9e6552be3914", "commit_author": "William Falcon", "commitT": "2020-10-04 21:30:33-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\__init__.py", "file_new_name": "pytorch_lightning\\trainer\\__init__.py", "file_complexity": {"file_NLOC": "1125", "file_CCN": "0", "file_NToken": "24"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "361,369,382,383", "deleted_lines": "368"}}}}}}