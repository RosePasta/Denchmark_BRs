{"BR": {"BR_id": "2688", "BR_author": "thschaaf", "BRopenT": "2020-07-24T19:28:26Z", "BRcloseT": "2020-07-31T11:53:09Z", "BR_text": {"BRsummary": "Training on GPU failed with Torchtext when using include_lengths=True in torchtext.data.Field", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n The issues raises in pytorch_lightning/utilities/apply_func.py which assumes that the attributes of a Batch from trochtext are Tensors, however if torchtext.data.Field is configured to include a length Tensor (include_lengths=True) the field is a tuple.\n A bugfix is prepared and a PR can be submitted soon.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Use Torchtext Field with include_lengths=True on a GPU machine and fit model.\n Training works on CPU but fails on GPU with: TypeError: cannot unpack non-iterable NoneType object\n \n <denchmark-h:h3>Full Error Message</denchmark-h>\n \n <denchmark-code>Traceback (most recent call last):\n  File \"debug_torchtext.py\", line 105, in <module>\n   trainer.fit(model)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1003, in fit\n   results = self.single_gpu_train(model)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 186, in single_gpu_train\n   results = self.run_pretrain_routine(model)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1213, in run_pretrain_routine\n   self.train()\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 370, in train\n   self.run_training_epoch()\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 452, in run_training_epoch\n   batch_output = self.run_training_batch(batch, batch_idx)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 632, in run_training_batch\n   self.hiddens\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 776, in optimizer_closure\n   hiddens)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 944, in training_forward\n   batch = self.transfer_batch_to_gpu(batch, gpu_id)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 159, in transfer_batch_to_gpu\n   return self.__transfer_batch_to_device(batch, device)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\", line 164, in __transfer_batch_to_device\n   return model.transfer_batch_to_device(batch, device)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py\", line 242, in transfer_batch_to_device\n   return move_data_to_device(batch, device)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py\", line 128, in move_data_to_device\n   return apply_to_collection(batch, dtype=(TransferableDataType, Batch), function=batch_to)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py\", line 35, in apply_to_collection\n   return function(data, *args, **kwargs)\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py\", line 103, in batch_to\n   device_field = getattr(data, field).to(device, non_blocking=True)\n AttributeError: 'tuple' object has no attribute 'to'\n Exception ignored in: <function tqdm.__del__ at 0x7fcb5e0b2680>\n Traceback (most recent call last):\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py\", line 1086, in __del__\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py\", line 1293, in close\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py\", line 1471, in display\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py\", line 1089, in __repr__\n  File \"/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py\", line 1433, in format_dict\n TypeError: cannot unpack non-iterable NoneType object\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>import torch\n from torch import nn, Tensor\n import pytorch_lightning as pl\n from pytorch_lightning import Trainer, seed_everything\n from torchtext import data\n seed_everything(1234)\n def get_debug_data_loader():\n     text_field = data.Field(sequential=True, pad_first=False,\n                             init_token=\"<s>\", eos_token=\"</s>\", include_lengths=True)\n     example1 = data.example.Example.fromdict({\"text\": \"a b c a c\"}, {\"text\": (\"text\", text_field)})\n     example2 = data.example.Example.fromdict({\"text\": \"b c a a\"}, {\"text\": (\"text\", text_field)})\n     example3 = data.example.Example.fromdict({\"text\": \"c b a\"}, {\"text\": (\"text\", text_field)})\n     dataset = data.Dataset([example1, example2, example3], {\"text\": text_field})\n     text_field.build_vocab(dataset)\n     iterator = data.Iterator(dataset, batch_size=3,\n                              sort_key=None, device=None, batch_size_fn=None,\n                              train=True, repeat=False, shuffle=None, sort=None, sort_within_batch=None)\n     return iterator, text_field\n class DebugModel(pl.LightningModule):\n     def __init__(self):\n         super(DebugModel, self).__init__()\n         # setup data loader\n         self.debug_data_loader, self.text_field = get_debug_data_loader()\n         self.learning_rate = 0.001\n         self.hid_dim = 4\n         pad_idx = self.text_field.vocab.stoi['<pad>']\n         self.criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n         self.INPUT_DIM = len(self.text_field.vocab)\n         self.ENC_EMB_DIM = 4  # keep it small for debugging\n         self.embedding = nn.Embedding(self.INPUT_DIM, self.ENC_EMB_DIM)\n         self.rnn = nn.GRU(self.ENC_EMB_DIM, self.hid_dim, 1, bidirectional=False)\n         self.out = nn.Linear(self.hid_dim, self.embedding.num_embeddings)\n         self.OUTPUT_DIM = len(self.text_field.vocab)\n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n     def forward(self, input_seq, length):\n         embedded: Tensor = self.embedding(input_seq)\n         packed_embedded: Tensor = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=False,\n                                                                           enforce_sorted=False)\n         packed_outputs, hidden = self.rnn(packed_embedded)  # [sent len, batch size, emb dim]\n         outputs, length = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs)\n         # outputs -> [sent len, batch size, hid dim * n directions]\n         # hidden -> [n layers * n directions, batch size, hid dim]\n         output = outputs.squeeze(0)\n         prediction = self.out(output)\n         return prediction\n     @staticmethod\n     def _parse_batch(batch):\n         source = batch.text[0]\n         source_length = batch.text[1]\n         return source, source_length\n     def training_step(self, batch, batch_nb):\n         x = self._parse_batch(batch)\n         target, target_length = x\n         output = self.forward(target, target_length)\n         loss = self.criterion(output[:-1].view(-1, output.shape[2]), target[1:].view(-1))\n         prefix = 'train'\n         tensorboard_logs = {f'{prefix}_loss': loss.item()}\n         result = {'loss': loss, 'log': tensorboard_logs}\n         return result\n     def train_dataloader(self):\n         return self.debug_data_loader\n model = DebugModel()\n cuda_device_cnt = torch.cuda.device_count()\n if cuda_device_cnt > 0:\n     use_num_cuda_devices = 1\n else:\n     use_num_cuda_devices = None\n trainer = Trainer(fast_dev_run=False, max_steps=None,\n                   gradient_clip_val=10,\n                   weights_summary='full', gpus=use_num_cuda_devices,\n                   show_progress_bar=True)\n trainer.fit(model)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Should not raise an error :-)\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code> CUDA:\n     - GPU:\n         - TITAN X (Pascal)\n     - available:     True\n     - version:      10.2\n * Packages:\n     - numpy:       1.17.3\n     - pyTorch_debug:   False\n     - pyTorch_version:  1.5.1\n     - pytorch-lightning: 0.8.5\n     - tensorboard:    2.2.2\n     - tqdm:       4.47.0\n * System:\n     - OS:        Linux\n     - architecture:\n         - 64bit\n         - \n     - processor:     x86_64\n     - python:      3.7.4\n     - version:      #1 SMP Tue Mar 17 23:49:17 UTC 2020\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {}}, "commit": {"commit_id": "a6719f09f0a383034f4285d65cba880208a03ae4", "commit_author": "Thomas Schaaf", "commitT": "2020-07-31 07:53:08-04:00", "commit_complexity": {"commit_NLOC": "0.3793103448275862", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "50,51", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\utilities\\apply_func.py", "file_new_name": "pytorch_lightning\\utilities\\apply_func.py", "file_complexity": {"file_NLOC": "72", "file_CCN": "17", "file_NToken": "369"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "104,107,108", "deleted_lines": "102,103,106,107", "method_info": {"method_name": "move_data_to_device.batch_to", "method_params": "data", "method_startline": "97", "method_endline": "108", "method_complexity": {"method_NLOC": "8", "method_CCN": "4", "method_NToken": "62", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "96,104,107,108", "deleted_lines": "102,103,106,107", "method_info": {"method_name": "move_data_to_device", "method_params": "Any,device", "method_startline": "79", "method_endline": "110", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "33", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\utilities\\test_apply_func_torchtext.py", "file_complexity": {"file_NLOC": "39", "file_CCN": "4", "file_NToken": "376"}}}}}