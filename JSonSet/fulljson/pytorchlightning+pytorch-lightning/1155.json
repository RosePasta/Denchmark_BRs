{"BR": {"BR_id": "1155", "BR_author": "qmeeus", "BRopenT": "2020-03-15T13:43:17Z", "BRcloseT": "2020-05-03T23:15:57Z", "BR_text": {"BRsummary": "No validation checks when overfit_pct is set", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When setting the overfit_pct to any value between 0 and 1 (exclusive) in trainer, the validation checks are disabled.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n I have worked on a minimal example to reproduce the bug:\n import pytorch_lightning as pl\n import torch\n \n class Dataset(torch.utils.data.Dataset):\n \n     def __init__(self, input_dim, output_dim):\n         super(Dataset, self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n \n     def __getitem__(self, idx):\n         X = torch.rand(1, self.input_dim)\n         y = torch.randint(0, self.output_dim, (1,))\n         return X, y\n \n     def __len__(self):\n         return 1000\n \n class Model(pl.LightningModule):\n \n     def __init__(self, input_dim, output_dim):\n         super(Model, self).__init__()\n         self.layer = torch.nn.Linear(input_dim, output_dim)\n         self.dataset = Dataset(input_dim, output_dim)\n \n     def forward(self, x, y):\n         yhat = torch.softmax(self.layer(x), -1)\n         return F.nll_loss(logits, y)\n \n     def train_dataloader(self):\n         return torch.utils.data.DataLoader(self.dataset, batch_size=64)\n \n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters(), lr=1e-3)\n \n     def training_step(self, batch, batch_idx):\n         loss = self.forward(*batch)\n         return {'loss': loss, 'log': {'loss': loss}}\n \n     def validation_step(self, batch, batch_idx):\n         loss = self.forward(*batch)\n         return {'val_loss': loss, 'log': {'val_loss': loss}}\n \n \n if __name__ == '__main__':\n     model = Model(100, 10)\n     trainer = pl.Trainer(overfit_pct=.01)\n     trainer.fit(model)\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Validation checks occur normally\n <denchmark-h:h3>Environment</denchmark-h>\n \n PyTorch version: 1.4.0\n Is debug build: No\n CUDA used to build PyTorch: 10.1\n \n OS: Manjaro Linux\n GCC version: (GCC) 8.3.0\n CMake version: Could not collect\n \n Python version: 3.7\n Is CUDA available: No\n CUDA runtime version: 10.2.89\n GPU models and configuration: Could not collect\n Nvidia driver version: Could not collect\n cuDNN version: /usr/lib/libcudnn.so.7.6.5\n \n Versions of relevant libraries:\n [pip] numpy==1.18.1\n [pip] pytorch-lightning==0.7.1\n [pip] torch==1.4.0\n [pip] torchvision==0.5.0\n [conda] mkl                       2020.0                      166  \n [conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\n [conda] pytorch-lightning         0.7.1                    pypi_0    pypi\n [conda] torchvision               0.5.0                py37_cu101    pytorch\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "qmeeus", "commentT": "2020-03-15T13:43:56Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "qmeeus", "commentT": "2020-03-18T21:49:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>\n  <denchmark-link:https://github.com/hadim>@hadim</denchmark-link>\n  <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  mind check?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "qmeeus", "commentT": "2020-03-21T04:01:07Z", "comment_text": "\n \t\t, but I had to fix <denchmark-link:https://github.com/qmeeus>@qmeeus</denchmark-link>\n 's code sample to make it visible.\n The sanity validation checks run, but the validation at the end of the epoch doesn't.\n When setting , validation checks work as expected.\n Here is the fixed minimal code sample:\n <denchmark-code>import pytorch_lightning as pl\n import torch\n import torch.nn.functional as F\n \n \n class Dataset(torch.utils.data.Dataset):\n \n     def __init__(self, input_dim, output_dim):\n         super(Dataset, self).__init__()\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n \n     def __getitem__(self, idx):\n         X = torch.rand(self.input_dim)\n         y = torch.randint(0, self.output_dim, (1,))\n         return X, y\n \n     def __len__(self):\n         return 1000\n \n \n class Model(pl.LightningModule):\n \n     def __init__(self, input_dim, output_dim):\n         super(Model, self).__init__()\n         self.layer = torch.nn.Linear(input_dim, output_dim)\n         self.dataset = Dataset(input_dim, output_dim)\n \n     def forward(self, x, y):\n         logits = torch.softmax(self.layer(x), -1)\n         return F.nll_loss(logits, y.flatten(0))\n \n     def train_dataloader(self):\n         return torch.utils.data.DataLoader(self.dataset, batch_size=64)\n \n     def val_dataloader(self):\n         return torch.utils.data.DataLoader(self.dataset, batch_size=64)\n \n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters(), lr=1e-3)\n \n     def training_step(self, batch, batch_idx):\n         loss = self.forward(*batch)\n         return {'loss': loss, 'log': {'loss': loss}}\n \n     def validation_step(self, batch, batch_idx):\n         loss = self.forward(*batch)\n         print('see that validation runs only in sanity check')\n         return {'val_loss': loss, 'log': {'val_loss': loss}}\n \n     def validation_end(self, outputs):\n         loss = torch.stack([output['val_loss'] for output in outputs]).mean()\n         return {'val_loss': loss, 'log': {'val_loss': loss}}\n \n \n if __name__ == '__main__':\n     model = Model(100, 10)\n     trainer = pl.Trainer(overfit_pct=0.1, max_epochs=10)\n     trainer.fit(model)\n </denchmark-code>\n \n For the record, <denchmark-link:https://github.com/qmeeus>@qmeeus</denchmark-link>\n  your code had these issues:\n \n No val_dataloader defined\n Wrong shapes returned in dataloader\n Wrong shape for nll_loss labels\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "qmeeus", "commentT": "2020-03-21T04:14:27Z", "comment_text": "\n \t\tActually overfit_pct argument is not documented in the Trainer class. We should fix that and say that setting overfit_pct is the same as setting train_percent_check, val_percent_check and test_percent_check.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "qmeeus", "commentT": "2020-03-21T06:26:11Z", "comment_text": "\n \t\tFalse alarm! Turns out it is simply because you chose a too small value for overfit_pct.\n Your dataset has size 1000, and dataloader has batch_size 64.\n 1000 / 64 ~= 15 batches\n When you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch.\n <denchmark-link:https://github.com/qmeeus>@qmeeus</denchmark-link>\n  Please let me know if it isn't clear. I think the behaviour of is correct.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "qmeeus", "commentT": "2020-03-21T06:30:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  Should we make it so that does not round to 0 batches?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "qmeeus", "commentT": "2020-03-21T10:42:11Z", "comment_text": "\n \t\t\n False alarm! Turns out it is simply because you chose a too small value for overfit_pct.\n Your dataset has size 1000, and dataloader has batch_size 64.\n 1000 / 64 ~= 15 batches\n When you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch.\n @qmeeus Please let me know if it isn't clear. I think the behaviour of overfit_pct is correct.\n \n Awesome, thanks !\n \t\t"}}}, "commit": {"commit_id": "d735055e6fb6225ad11c566e3711888c0cb4a21e", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-03-24 14:49:11-04:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\__init__.py", "file_new_name": "pytorch_lightning\\trainer\\__init__.py", "file_complexity": {"file_NLOC": "900", "file_CCN": "0", "file_NToken": "14"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "552,553,563,564,565,566,567,568,569,570,571,572,573,574,575", "deleted_lines": "552,553"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "677", "file_CCN": "58", "file_NToken": "2949"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "166,167", "deleted_lines": null}}}}}}