{"BR": {"BR_id": "3778", "BR_author": "carmocca", "BRopenT": "2020-10-01T22:03:07Z", "BRcloseT": "2020-10-04T21:10:26Z", "BR_text": {"BRsummary": "training_step log requires that tbptt_reduce_fx is also set", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n training_step log requires that tbptt_reduce_fx is also set.\n <denchmark-h:h4>Code sample</denchmark-h>\n \n def training_step(self, batch, batch_idx):\n     ...\n     self.log(\"train_loss\", loss, on_step=False, on_epoch=True, sync_dist=True)\n     self.log(\n         \"foo\",\n         torch.tensor(self.current_epoch),\n         on_step=False,\n         on_epoch=True,\n         reduce_fx=max,\n         #tbptt_reduce_fx=max, ERROR when commented\n     )\n     return loss\n \n def validation_step(self, batch, batch_idx):\n     # No issues here\n     self.log(\n         \"bar\",\n         torch.tensor(self.global_step),\n         on_step=False,\n         on_epoch=True,\n         reduce_fx=max,\n     )\n Error:\n <denchmark-code>time_outputs = [{'tr_loss': tensor(6.2107), 'foo': tensor(0), 'minimize': tensor(6.2107)}]\n \n     @classmethod\n     def reduce_across_time(cls, time_outputs):\n         # auto-reduce across time for tbptt\n         meta = time_outputs[0]['meta']\n     \n         # in 1.0 the results have 'extra'. Once we deprecate 0.10.0 we may not need this\n         if 'extra' in time_outputs[0]:\n             [x.pop('extra', None) for x in time_outputs]\n     \n         result = cls()\n         result = recursive_gather(time_outputs, result)\n         recursive_stack(result)\n     \n         for k, value in result.items():\n             if k in ['meta', 'extra']:\n                 continue\n     \n             # pick the reduce fx\n             if k in ['checkpoint_on', 'early_stop_on', 'minimize']:\n                 tbptt_reduce_fx = torch.mean\n             else:\n                 tbptt_reduce_fx = meta[k]['tbptt_reduce_fx']\n >           result[k] = tbptt_reduce_fx(value)\n E           RuntimeError: Can only calculate the mean of floating types. Got Long instead.\n \n ../../venv/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:423: RuntimeError\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n That tbptt_reduce_fx is not required\n <denchmark-h:h3>Environment</denchmark-h>\n \n Master @ commit <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/8be002ccc7c2e8371ab426ea07c953f72747269e>8be002c</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "carmocca", "commentT": "2020-10-02T13:01:25Z", "comment_text": "\n \t\tgood catch. yeah, that's a bug.\n want to submit a PR?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "carmocca", "commentT": "2020-10-02T14:31:12Z", "comment_text": "\n \t\tI am not familiar with the tbptt logic, so maybe its better someone else does it.\n I'll create a PR with the failing test so somebody can work on it.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "carmocca", "commentT": "2020-12-22T23:48:14Z", "comment_text": "\n \t\tWhat the hell is this tbptt, this looks crazy specific, why is this necessary? Why not keep things simple?\n \t\t"}}}, "commit": {"commit_id": "89cc12311f5eaa7860d66bce9bfe3d93255f35b6", "commit_author": "Carlos Mochol\u00ed", "commitT": "2020-10-04 17:10:25-04:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\step_result.py", "file_new_name": "pytorch_lightning\\core\\step_result.py", "file_complexity": {"file_NLOC": "534", "file_CCN": "137", "file_NToken": "3429"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "423", "deleted_lines": "423", "method_info": {"method_name": "reduce_across_time", "method_params": "cls,time_outputs", "method_startline": "402", "method_endline": "426", "method_complexity": {"method_NLOC": "17", "method_CCN": "6", "method_NToken": "124", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\trainer\\logging\\test_train_loop_logging_1_0.py", "file_new_name": "tests\\trainer\\logging\\test_train_loop_logging_1_0.py", "file_complexity": {"file_NLOC": "195", "file_CCN": "14", "file_NToken": "1352"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277", "deleted_lines": null, "method_info": {"method_name": "test__training_step__log_max_reduce_fx", "method_params": "tmpdir,batches,fx,result", "method_startline": "249", "method_endline": "277", "method_complexity": {"method_NLOC": "15", "method_CCN": "1", "method_NToken": "76", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "259,260,261,262,263", "deleted_lines": null, "method_info": {"method_name": "test__training_step__log_max_reduce_fx.validation_step", "method_params": "self,batch,batch_idx", "method_startline": "259", "method_endline": "263", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "62", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "254,255,256,257", "deleted_lines": null, "method_info": {"method_name": "test__training_step__log_max_reduce_fx.training_step", "method_params": "self,batch,batch_idx", "method_startline": "254", "method_endline": "257", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "51", "method_nesting_level": "2"}}}}}}}}