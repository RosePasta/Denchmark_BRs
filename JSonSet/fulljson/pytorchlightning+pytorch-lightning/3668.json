{"BR": {"BR_id": "3668", "BR_author": "gerardsn", "BRopenT": "2020-09-26T15:38:17Z", "BRcloseT": "2020-10-06T01:30:42Z", "BR_text": {"BRsummary": "incorrect batch_sizes when Dataloader returns a dict with multiple tensors.", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Tracked batch sizes in result object are incorrect when a Dataloader returns a dict with multiple tensors.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Create data loader that returns a dict, e.g. batch = {'batchA': tensor_A, 'batchB': tensor_B}.\n Both entires have batch size N with N != 2.\n For this example a batch size of 2 will be logged since len(batch) == 2.\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py\n \n \n         Lines 147 to 150\n       in\n       05e5f03\n \n \n \n \n \n \n  # track batch size for weighted average \n \n \n \n  is_result_obj = isinstance(output, Result) \n \n \n \n  if is_result_obj: \n \n \n \n  output.track_batch_size(len(batch)) \n \n \n \n \n \n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 304 to 306\n       in\n       05e5f03\n \n \n \n \n \n \n  # track batch size for weighted average \n \n \n \n  if is_result_obj: \n \n \n \n  training_step_output.track_batch_size(len(split_batch)) \n \n \n \n \n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Log correct batch size.\n I'm not sure what can be defined as the 'correct' batch size when there are multiple tensors, but I expect that each tensor in the dict has the same batch_size. So, maybe something like:\n if is_result_obj:\n     if isinstance(batch, dict):\n         batch = batch[list(batch.keys())[0]]\n     result_obj.track_batch_size(len(batch))\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "gerardsn", "commentT": "2020-10-01T20:33:02Z", "comment_text": "\n \t\tI think doing just len(batch) is still wrong since here if the batch is a tuple or some kind of custom batch datatype then len(batch) will be wrong. Considering the basic mnist example too it will give 2 only which is wrong.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "gerardsn", "commentT": "2020-10-02T03:17:19Z", "comment_text": "\n \t\tThis should probably catch most things. Might be a bit much though.\n It returns 1 if it fails to determine the batch size to prevent issues with weighted averaging in reduce_on_epoch_end.\n if is_result_obj:\n     result_obj.track_batch_size(unpack_batchsize(batch))\n \n # maybe add as staticmethod to ResultObj?\n def unpack_batchsize(sample):\n     \"\"\" \n     Recursively unpack sample to find a torch.Tensor.\n     returns len(tensor) when found, or 1 when it hits an empty or non iterable.\n     \"\"\"\n     if isinstance(sample, torch.Tensor):\n         sample = len(sample)\n     elif isinstance(sample, dict):\n         sample = next(iter(sample.values()), 1)\n     elif isinstance(sample, Iterable):\n         sample = next(iter(sample), 1)\n     else:\n         sample = 1  \n \n     if isinstance(sample, int):\n         return sample\n     return unpack_batchsize(sample)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "gerardsn", "commentT": "2020-10-02T03:59:08Z", "comment_text": "\n \t\tI suggest adding a function to the LightningModule batch_len_fx which defaults to len if it is not overriden. Anything could be a batch and lightning shouldn't have the responsability of supporting any batch type.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "gerardsn", "commentT": "2020-10-02T06:04:56Z", "comment_text": "\n \t\tExactly what I had in mind <denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>\n . Or maybe simple ask to put  in  itself if ??\n .log('some_metric', metric_value, on_epoch=True, batch_size=batch_size)\n .log('some_metric', metric_value, on_epoch=False)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "gerardsn", "commentT": "2020-10-02T06:07:11Z", "comment_text": "\n \t\tLightning currently defaults to weighted_mean for reduction on epoch end by substituting the reduction method if it is torch.mean:\n \n \n \n pytorch-lightning/pytorch_lightning/core/step_result.py\n \n \n         Lines 389 to 390\n       in\n       ebc1b23\n \n \n \n \n \n \n  if fx == torch.mean: \n \n \n \n  reduced_val = weighted_mean(result[k], batch_sizes) \n \n \n \n \n \n If this is the desired behaviour, I think Lightning should at least attempt getting a reasonable estimate for the batch size. In most use cases the dataloader will return multiple tensors, resulting in an incorrect batch estimate if  is the default. (e.g. any supervised method has at least (X, y) in its batch, producing  as mentioned by <denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>\n )\n This could still be done using batch_len_fx though. On first call, if the method is not overriden, replace the batch_len_fx with a reasonable estimate based on the type of batch. (e.g. len of [tensor, first value in Iterable])\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "gerardsn", "commentT": "2020-10-02T06:18:16Z", "comment_text": "\n \t\t\n Exactly what I had in mind @carmocca. Or maybe simple ask to put batch_size in .log itself if on_epoch=True??\n .log('some_metric', metric_value, on_epoch=True, batch_size=batch_size)\n .log('some_metric', metric_value, on_epoch=False)\n \n this should work too. Probably default to 1 if not provided since len is likely to be wrong.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "gerardsn", "commentT": "2020-10-02T20:54:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gerardsn>@gerardsn</denchmark-link>\n  I have a problem exactly with this  function.\n I'm working with the latest Lightning version from master.\n \n \n \n pytorch-lightning/pytorch_lightning/core/step_result.py\n \n \n          Line 369\n       in\n       ebc1b23\n \n \n \n \n \n \n  def reduce_on_epoch_end(cls, outputs): \n \n \n \n \n \n It gets outputs = [{'checkpoint_on': tensor(28.3303, device='cuda:0'), 'val_loss': tensor(28.3303, device='cuda:0'), 'val_precision1': 0.12652068126520682}].\n Because I have only one batch per epoch in validation.\n Lightning tries to reduce on epoch end.\n It feeds\n result = tensor([27.8364], device='cuda:0'), weights=tensor([2]), into the weighted_mean function and I get an error here:\n \n \n \n pytorch-lightning/pytorch_lightning/core/step_result.py\n \n \n          Line 897\n       in\n       ebc1b23\n \n \n \n \n \n \n  weights = weights.to(result.device)[:result.size(0)] \n \n \n \n \n \n AttributeError: 'list' object has no attribute 'device'\n I think it's related to this issue. It would be nice to not reduce anything if it's just one batch per epoch.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "gerardsn", "commentT": "2020-10-02T22:15:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/fogside>@fogside</denchmark-link>\n  in your example result is a tensor so  should not though an error.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "gerardsn", "commentT": "2020-10-02T22:17:58Z", "comment_text": "\n \t\t\n @fogside in your example result is a tensor so result.device should not though an error.\n \n But it's a list with a tensor inside.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "gerardsn", "commentT": "2020-10-02T22:21:30Z", "comment_text": "\n \t\t\n result = tensor([27.8364], device='cuda:0'), weights=tensor([2])\n \n you refering to this right?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "gerardsn", "commentT": "2020-10-02T22:29:07Z", "comment_text": "\n \t\t\n \n result = tensor([27.8364], device='cuda:0'), weights=tensor([2])\n \n you refering to this right?\n \n Sorry, I just realized, that I was mistaken.\n Actually it calls this method twice for some reason.\n I added prints at the beginning of weighted_mean function and at the reduce_on_epoch_end (I also changed the number of batches in this example)\n <denchmark-code>Result[k] tensor([23.6331, 26.0617, 24.0941, 25.3255], device='cuda:0')\n result:  tensor([23.6331, 26.0617, 24.0941, 25.3255], device='cuda:0')\n weights:  tensor([2, 2, 2, 2])\n Result[k] [0.14285714285714285, 0.06451612903225806, 0.056179775280898875, 0.13793103448275862]\n result:  [0.14285714285714285, 0.06451612903225806, 0.056179775280898875, 0.13793103448275862]\n weights:  tensor([2, 2, 2, 2])\n </denchmark-code>\n \n And on the second time it gives me the error.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "gerardsn", "commentT": "2020-10-02T22:30:45Z", "comment_text": "\n \t\tare you logging non-tensor values? maybe doing .item() somewhere in the logs? if not, can you put .log statements here??\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "gerardsn", "commentT": "2020-10-02T22:33:17Z", "comment_text": "\n \t\t\n are you logging non-tensor values?\n \n Yes, I was calculating precision in numpy.. Isn't it possible to log non-tensor values?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "gerardsn", "commentT": "2020-10-02T22:34:43Z", "comment_text": "\n \t\tno.. also to calculate precision or anyother metric you can try pl.metrics package which computes these metrics on the current device itself.\n or you can just do torch.tensor(numpy_value) in .log\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "gerardsn", "commentT": "2020-10-02T22:42:38Z", "comment_text": "\n \t\t\n no.. also to calculate precision or another metric you can try pl.metrics package which does all of these on the current device itself.\n \n I see. Thank you!\n Actually I was trying to work with <denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning>pytorch-metric-learning</denchmark-link>\n  and used some function for topk precision estimation from there. But it looks quite tough to merge these 2 frameworks. I see now that topk precision should be calculated in pytorch. Another thing is that I need to have as big batch as possible to get a good topk estimation (it's even better to have the whole val set), that's why I found it hard to make this estimations in the . Maybe I should look into some Callbacks?\n But it's not related to this issue.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "gerardsn", "commentT": "2020-10-02T22:51:57Z", "comment_text": "\n \t\talready working on topk accuracy. Maybe will add topk precision and recall in pl.metrics as well. Can you point me to the implementation of topk precision in pytorch-metric-learning package. It would be helpful. Thanks :)\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "gerardsn", "commentT": "2020-10-03T08:05:17Z", "comment_text": "\n \t\t\n already working on topk accuracy. Maybe will add topk precision and recall in pl.metrics as well. Can you point me to the implementation of topk precision in pytorch-metric-learning package. It would be helpful. Thanks :)\n \n It's great!\n Sure. I used the class AccuracyCalculator like this\n <denchmark-code>accuracy_calculator = AccuracyCalculator(include=(\"mean_average_precision_at_r\"),  k=5)\n accuracies = self.accuracy_calculator.get_accuracy(embeddings,\n                                                            embeddings,\n                                                            labels,\n                                                            labels,\n                                                            True)\n \n </denchmark-code>\n \n Implementation:\n <denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/blob/10bed5ee8719a543827aa32ea658603c2fcb0130/src/pytorch_metric_learning/utils/accuracy_calculator.py#L45>https://github.com/KevinMusgrave/pytorch-metric-learning/blob/10bed5ee8719a543827aa32ea658603c2fcb0130/src/pytorch_metric_learning/utils/accuracy_calculator.py#L45</denchmark-link>\n \n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "gerardsn", "commentT": "2020-10-03T14:32:57Z", "comment_text": "\n \t\tSo I guess 2 things should be fixed:\n \n Track correct batch_size\n Allow non-tensor numeric values in .log(...)\n \n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "gerardsn", "commentT": "2020-10-05T13:43:15Z", "comment_text": "\n \t\t\n @gerardsn I have a problem exactly with this weighted_mean function.\n I'm working with the latest Lightning version from master.\n \n \n \n pytorch-lightning/pytorch_lightning/core/step_result.py\n \n \n          Line 369\n       in\n       ebc1b23\n \n \n \n \n \n \n  def reduce_on_epoch_end(cls, outputs): \n \n \n \n \n \n It gets outputs = [{'checkpoint_on': tensor(28.3303, device='cuda:0'), 'val_loss': tensor(28.3303, device='cuda:0'), 'val_precision1': 0.12652068126520682}].\n Because I have only one batch per epoch in validation.\n Lightning tries to reduce on epoch end.\n It feeds\n result = tensor([27.8364], device='cuda:0'), weights=tensor([2]), into the weighted_mean function and I get an error here:\n \n \n \n pytorch-lightning/pytorch_lightning/core/step_result.py\n \n \n          Line 897\n       in\n       ebc1b23\n \n \n \n \n \n \n  weights = weights.to(result.device)[:result.size(0)] \n \n \n \n \n \n AttributeError: 'list' object has no attribute 'device'\n I think it's related to this issue. It would be nice to not reduce anything if it's just one batch per epoch.\n \n this is fixed on master\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "gerardsn", "commentT": "2020-10-05T13:44:31Z", "comment_text": "\n \t\tok, making changes to this today.\n What do we want as the default behavior? doesn't the custom reduce function solve the problem of custom batches etc?\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "gerardsn", "commentT": "2020-10-05T14:43:52Z", "comment_text": "\n \t\tThe batches are not tracked correctly.\n \t\t"}}}, "commit": {"commit_id": "b34c7add23553f10f6f0d7caf4177c67ee213f3a", "commit_author": "William Falcon", "commitT": "2020-10-05 21:30:41-04:00", "commit_complexity": {"commit_NLOC": "0.7524752475247525", "commit_CCN": "0.9405940594059405", "commit_Nprams": "0.7722772277227723"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 8, "file_old_name": "pytorch_lightning\\core\\step_result.py", "file_new_name": "pytorch_lightning\\core\\step_result.py", "file_complexity": {"file_NLOC": "583", "file_CCN": "153", "file_NToken": "3797"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341", "deleted_lines": null, "method_info": {"method_name": "unpack_batch_size", "method_params": "self,sample", "method_startline": "326", "method_endline": "341", "method_complexity": {"method_NLOC": "12", "method_CCN": "4", "method_NToken": "91", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "442,443,444,445,446,447", "deleted_lines": "423", "method_info": {"method_name": "reduce_across_time", "method_params": "cls,time_outputs", "method_startline": "421", "method_endline": "450", "method_complexity": {"method_NLOC": "20", "method_CCN": "7", "method_NToken": "140", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972", "deleted_lines": null, "method_info": {"method_name": "_process_dataloader_aggregated_steps", "method_params": "result,weights", "method_startline": "948", "method_endline": "972", "method_complexity": {"method_NLOC": "15", "method_CCN": "5", "method_NToken": "125", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "513,514,515,516,517,518,519,520,521,522", "deleted_lines": null, "method_info": {"method_name": "_recursive_fx_apply", "method_params": "dict,fx", "method_startline": "513", "method_endline": "522", "method_complexity": {"method_NLOC": "9", "method_CCN": "4", "method_NToken": "70", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "221,222", "deleted_lines": "220", "method_info": {"method_name": "track_batch_size", "method_params": "self,batch_size", "method_startline": "220", "method_endline": "222", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "25", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "935,936,937,938,939,940,941,942,943,944", "deleted_lines": null, "method_info": {"method_name": "weighted_mean", "method_params": "result,weights", "method_startline": "934", "method_endline": "945", "method_complexity": {"method_NLOC": "10", "method_CCN": "3", "method_NToken": "99", "method_nesting_level": "0"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "221,222", "deleted_lines": null, "method_info": {"method_name": "track_batch_size", "method_params": "self,batch", "method_startline": "221", "method_endline": "224", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "33", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "493,494,495,496,497,498,500", "deleted_lines": null, "method_info": {"method_name": "recursive_gather", "method_params": "None", "method_startline": "486", "method_endline": "502", "method_complexity": {"method_NLOC": "14", "method_CCN": "6", "method_NToken": "111", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\tensorboard.py", "file_new_name": "pytorch_lightning\\loggers\\tensorboard.py", "file_complexity": {"file_NLOC": "205", "file_CCN": "34", "file_NToken": "1072"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "183,184,185,186,187,188,189,190,191", "deleted_lines": "182", "method_info": {"method_name": "log_metrics", "method_params": "self,str,None", "method_startline": "177", "method_endline": "191", "method_complexity": {"method_NLOC": "13", "method_CCN": "5", "method_NToken": "111", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_new_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_complexity": {"file_NLOC": "236", "file_CCN": "85", "file_NToken": "1762"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "323", "deleted_lines": "323", "method_info": {"method_name": "log_evaluation_step_metrics", "method_params": "self,batch,batch_idx", "method_startline": "318", "method_endline": "326", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "46", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "168", "deleted_lines": "168", "method_info": {"method_name": "evaluation_step", "method_params": "self,test_mode,batch,batch_idx,dataloader_idx", "method_startline": "155", "method_endline": "175", "method_complexity": {"method_NLOC": "13", "method_CCN": "5", "method_NToken": "96", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\base\\boring_model.py", "file_new_name": "tests\\base\\boring_model.py", "file_complexity": {"file_NLOC": "63", "file_CCN": "24", "file_NToken": "577"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "11,12,13,14", "deleted_lines": null, "method_info": {"method_name": "__getitem__", "method_params": "self,index", "method_startline": "11", "method_endline": "14", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "30", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "16,17", "deleted_lines": null, "method_info": {"method_name": "__len__", "method_params": "self", "method_startline": "16", "method_endline": "17", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "7,8,9", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,size,length", "method_startline": "7", "method_endline": "9", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "26", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tests\\trainer\\logging\\test_train_loop_logging_1_0.py", "file_new_name": "tests\\trainer\\logging\\test_train_loop_logging_1_0.py", "file_complexity": {"file_NLOC": "286", "file_CCN": "27", "file_NToken": "2010"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "378,379", "deleted_lines": null, "method_info": {"method_name": "test_different_batch_types_for_sizing.val_dataloader", "method_params": "self", "method_startline": "378", "method_endline": "379", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "25", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "375,376", "deleted_lines": null, "method_info": {"method_name": "test_different_batch_types_for_sizing.train_dataloader", "method_params": "self", "method_startline": "375", "method_endline": "376", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "25", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "360,361,362,363,364,365", "deleted_lines": null, "method_info": {"method_name": "test_different_batch_types_for_sizing.training_step", "method_params": "self,batch,batch_idx", "method_startline": "360", "method_endline": "365", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "61", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398", "deleted_lines": null, "method_info": {"method_name": "test_different_batch_types_for_sizing", "method_params": "tmpdir", "method_startline": "357", "method_endline": "398", "method_complexity": {"method_NLOC": "22", "method_CCN": "1", "method_NToken": "84", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "367,368,369,370,371,372,373", "deleted_lines": null, "method_info": {"method_name": "test_different_batch_types_for_sizing.validation_step", "method_params": "self,batch,batch_idx", "method_startline": "367", "method_endline": "373", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "75", "method_nesting_level": "2"}}}}}}}}