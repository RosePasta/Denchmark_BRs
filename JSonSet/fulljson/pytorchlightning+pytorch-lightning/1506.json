{"BR": {"BR_id": "1506", "BR_author": "s-rog", "BRopenT": "2020-04-16T05:13:36Z", "BRcloseT": "2020-04-19T20:58:59Z", "BR_text": {"BRsummary": "0.7.3 breaks reusable dataloaders in DDP", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n 0.7.3 breaks reusable dataloaders in DDP\n <denchmark-code>Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n     fn(i, *args)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 345, in ddp_train\n     self.run_pretrain_routine(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 864, in run_pretrain_routine\n     self.train()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 296, in train\n     self.reset_train_dataloader(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py\", line 128, in reset_train_dataloader\n     self.train_dataloader = self.auto_add_sampler(self.train_dataloader, train=True)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py\", line 112, in auto_add_sampler\n     dataloader = type(dataloader)(**dl_args)\n   File \"../main/dataset.py\", line 15, in __init__\n     super().__init__(*args, **kwargs)\n TypeError: __init__() got an unexpected keyword argument 'iterator'\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>class _RepeatSampler(object):\n     def __init__(self, sampler):\n         self.sampler = sampler\n \n     def __iter__(self):\n         while True:\n             yield from iter(self.sampler)\n \n class FastDataLoader(torch.utils.data.dataloader.DataLoader):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\n         self.iterator = super().__iter__()\n \n     def __len__(self):\n         return len(self.batch_sampler.sampler)\n \n     def __iter__(self):\n         for i in range(len(self)):\n             yield next(self.iterator)\n </denchmark-code>\n \n replace Dataloader with FastDataLoader in lightning\n (this snippet is from <denchmark-link:https://github.com/pytorch/pytorch/issues/15849>pytorch/pytorch#15849</denchmark-link>\n )\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Dataloaders initialize correctly and are reused between train/val/epochs (works as expected in 0.7.1)\n <denchmark-h:h3>Probable Cause</denchmark-h>\n \n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1425>#1425</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "s-rog", "commentT": "2020-04-16T12:22:08Z", "comment_text": "\n \t\tummm yeah. we should change the dataloader swap with swapping a dataloader init from the class or not swipe the dataloder at all but set the correct sampler.\n <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>\n  any ideas?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "s-rog", "commentT": "2020-04-17T07:43:44Z", "comment_text": "\n \t\tThis is a mixture of  <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1425>#1425</denchmark-link>\n  and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1346>#1346</denchmark-link>\n \n And I don't think we can prevent this when we want to set correct samplers also in subclasses of DataLoader. We use all public attributes for reinitialization.\n The probably easiest fix for you, would be to change self.iterator to self._iterator to avoid passing this argument in reinit.\n If we just change the sampler, this might yield unexpected behaviour.\n \t\t"}}}, "commit": {"commit_id": "c71bd73acb5a89bb2a8ff44beab37fd2ceba352b", "commit_author": "Justus Schock", "commitT": "2020-04-19 16:58:57-04:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "11", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\data_loading.py", "file_new_name": "pytorch_lightning\\trainer\\data_loading.py", "file_complexity": {"file_NLOC": "204", "file_CCN": "38", "file_NToken": "1076"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "92,93", "deleted_lines": "91,92,93,94", "method_info": {"method_name": "auto_add_sampler", "method_params": "self,DataLoader,bool", "method_startline": "87", "method_endline": "113", "method_complexity": {"method_NLOC": "20", "method_CCN": "10", "method_NToken": "146", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "812", "file_CCN": "86", "file_NToken": "3344"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "130", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,float,int,int,int,str,None,bool,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,str,int,None,None,None,bool,bool,bool,False,bool,default_save_path,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,bool,kwargs", "method_startline": "85", "method_endline": "140", "method_complexity": {"method_NLOC": "56", "method_CCN": "1", "method_NToken": "410", "method_nesting_level": "1"}}}}}}}}