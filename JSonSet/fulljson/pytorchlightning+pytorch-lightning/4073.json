{"BR": {"BR_id": "4073", "BR_author": "willprice", "BRopenT": "2020-10-11T11:59:57Z", "BRcloseT": "2020-12-04T18:10:08Z", "BR_text": {"BRsummary": "Data Parallel bug (return outputs not being moved to same device)", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Under backend='dp' doesn't handle reduction of the loss across multiple GPUs correctly. This is present in v0.10--v1.0.0rc4\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n import torch\n import pytorch_lightning as ptl\n from pytorch_lightning import LightningModule\n from torch.utils.data import Dataset\n \n \n class RandomDictDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         a = self.data[index]\n         b = a + 2\n         return {\"a\": a, \"b\": b}\n \n     def __len__(self):\n         return self.len\n \n \n class RandomDictStringDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         return {\"id\": str(index), \"x\": self.data[index]}\n \n     def __len__(self):\n         return self.len\n \n \n class RandomDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         return self.data[index]\n \n     def __len__(self):\n         return self.len\n \n \n class BoringModel(LightningModule):\n     def __init__(self):\n         \"\"\"\n         Testing PL Module\n         Use as follows:\n         - subclass\n         - modify the behavior for what you want\n         class TestModel(BaseTestModel):\n             def training_step(...):\n                 # do your own thing\n         or:\n         model = BaseTestModel()\n         model.training_epoch_end = None\n         \"\"\"\n         super().__init__()\n         self.layer = torch.nn.Linear(32, 2)\n \n     def forward(self, x):\n         return self.layer(x)\n \n     def loss(self, batch, prediction):\n         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\n         return torch.nn.functional.cross_entropy(\n             prediction,\n             torch.ones(len(prediction), dtype=torch.long, device=prediction.device),\n         )\n \n     def training_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log(\"loss\", loss)\n         return loss\n \n     def validation_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log(\"loss\", loss)\n         return loss\n \n     def test_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         return loss\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\n         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n         return [optimizer], [lr_scheduler]\n \n     def train_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)\n \n     def val_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)\n \n     def test_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)\n \n \n def main():\n     model = BoringModel()\n     trainer = ptl.Trainer(\n         distributed_backend=\"dp\",\n         gpus=4,\n     )\n     trainer.fit(model)\n \n \n if __name__ == \"__main__\":\n     main()\n Produces the following\n <denchmark-code>GPU available: True, used: True\n TPU available: False, using: 0 TPU cores\n LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n \n | Name  | Type   | Params\n ---------------------------------\n 0 | layer | Linear | 66\n /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n Validation sanity check: 0it [00:00, ?it/s]/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n warnings.warn('Was asked to gather along dimension 0, but all '\n /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n Epoch 1:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                                                      | 4/8 [00:00<00:00, 184.41it/s, loss=0.497, v_num=53]\n File \"dp_bug.py\", line 118, in <module>\n main()\n File \"dp_bug.py\", line 114, in main\n trainer.fit(model)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 440, in fit\n results = self.accelerator_backend.train()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/dp_accelerator.py\", line 97, in train\n results = self.train_or_test()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 53, in train_or_test\n results = self.trainer.train()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 483, in train\n self.train_loop.run_training_epoch()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 557, in run_training_epoch\n self.trainer.run_evaluation(test_mode=False)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in run_evaluation\n eval_loop_results = self.evaluation_loop.log_epoch_metrics(deprecated_eval_results, epoch_logs, test_mode)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 210, in log_epoch_metrics\n eval_loop_results = self.trainer.logger_connector.on_evaluation_epoch_end(\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 113, in on_evaluation_epoch_end\n self._log_on_evaluation_epoch_end_metrics(epoch_logs)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 178, in _log_on_evaluation_epoch_end_metrics\n reduced_epoch_metrics = dl_metrics[0].__class__.reduce_on_epoch_end(dl_metrics)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py\", line 433, in reduce_on_epoch_end\n recursive_stack(result)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py\", line 552, in recursive_stack\n result[k] = collate_tensors(v)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py\", line 574, in collate_tensors\n return torch.stack(items)\n RuntimeError: All input tensors must be on the same device. Received cuda:3 and cuda:1\n Exception ignored in: <function tqdm.__del__ at 0x7fcf54050a60>\n Traceback (most recent call last):\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1087, in __del__\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1294, in close\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1472, in display\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1090, in __repr__\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1434, in format_dict\n TypeError: cannot unpack non-iterable NoneType object\n </denchmark-code>\n \n Specifically note the line saying\n <denchmark-code>RuntimeError: All input tensors must be on the same device. Received cuda:3 and cuda:1\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version (e.g., 1.0): 1.6.0\n OS (e.g., Linux): Ubuntu 18.04\n How you installed PyTorch (conda, pip, source): conda\n Build command you used (if compiling from source): N/A\n Python version: 3.8.5\n CUDA/cuDNN version: 11.0\n GPU models and configuration: 8 GPU (RTX 2080Ti)\n Any other relevant information:\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n This works on v0.9.0:\n import torch\n import pytorch_lightning as ptl\n from pytorch_lightning import LightningModule\n from torch.utils.data import Dataset\n \n \n class RandomDictDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         a = self.data[index]\n         b = a + 2\n         return {\"a\": a, \"b\": b}\n \n     def __len__(self):\n         return self.len\n \n \n class RandomDictStringDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         return {\"id\": str(index), \"x\": self.data[index]}\n \n     def __len__(self):\n         return self.len\n \n \n class RandomDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         return self.data[index]\n \n     def __len__(self):\n         return self.len\n \n \n class BoringModel(LightningModule):\n     def __init__(self):\n         \"\"\"\n         Testing PL Module\n         Use as follows:\n         - subclass\n         - modify the behavior for what you want\n         class TestModel(BaseTestModel):\n             def training_step(...):\n                 # do your own thing\n         or:\n         model = BaseTestModel()\n         model.training_epoch_end = None\n         \"\"\"\n         super().__init__()\n         self.layer = torch.nn.Linear(32, 2)\n \n     def forward(self, x):\n         return self.layer(x)\n \n     def loss(self, batch, prediction):\n         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\n         return torch.nn.functional.cross_entropy(\n             prediction,\n             torch.ones(len(prediction), dtype=torch.long, device=prediction.device),\n         )\n \n     def training_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         return {\"loss\": loss}\n \n     def validation_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         return {\"val_loss\": loss}\n \n     def test_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         return {\"test_loss\": loss}\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\n         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n         return [optimizer], [lr_scheduler]\n \n     def train_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)\n \n     def val_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)\n \n     def test_dataloader(self):\n         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)\n \n \n def main():\n     model = BoringModel()\n     trainer = ptl.Trainer(\n         distributed_backend=\"dp\",\n         gpus=4,\n         # log_every_n_steps=5,\n         # flush_logs_every_n_steps=20,\n         # benchmark=True,\n         # gradient_clip_val=20,\n     )\n     trainer.fit(model)\n \n \n if __name__ == \"__main__\":\n     main()\n but causes this error under v1.0.0rc4\n <denchmark-code>GPU available: True, used: True\n TPU available: False, using: 0 TPU cores\n LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n \n | Name  | Type   | Params\n ---------------------------------\n 0 | layer | Linear | 66    \n /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n warnings.warn(*args, **kwargs)\n Epoch 0:   0%|                                                                                                                                                                                                          | 0/8 [00:00<?, ?it/s]Traceback (most recent call last):\n File \"dp_bug.py\", line 116, in <module>\n main()\n File \"dp_bug.py\", line 112, in main\n trainer.fit(model)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 440, in fit\n results = self.accelerator_backend.train()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/dp_accelerator.py\", line 97, in train\n results = self.train_or_test()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 53, in train_or_test\n results = self.trainer.train()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 483, in train\n self.train_loop.run_training_epoch()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 529, in run_training_epoch\n batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 661, in run_training_batch\n opt_closure_result = self.training_step_and_backward(\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 753, in training_step_and_backward\n self.backward(result, optimizer, opt_idx)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 767, in backward\n result.closure_loss = self.trainer.accelerator_backend.backward(\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 83, in backward\n model.backward(closure_loss, optimizer, opt_idx)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1077, in backward\n loss.backward()\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/tensor.py\", line 185, in backward\n torch.autograd.backward(self, gradient, retain_graph, create_graph)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 121, in backward\n grad_tensors = _make_grads(tensors, grad_tensors)\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 47, in _make_grads\n raise RuntimeError(\"grad can be implicitly created only for scalar outputs\")\n RuntimeError: grad can be implicitly created only for scalar outputs\n Exception ignored in: <function tqdm.__del__ at 0x7fed7b0c1a60>\n Traceback (most recent call last):\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1087, in __del__\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1294, in close\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1472, in display\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1090, in __repr__\n File \"/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py\", line 1434, in format_dict\n TypeError: cannot unpack non-iterable NoneType object\n \n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "willprice", "commentT": "2020-10-12T11:08:58Z", "comment_text": "\n \t\timport torch\n \n # this is what is happening in will's code:\n \n prediction = torch.rand(8, 2, requires_grad=True)\n \n # device 0 computes:\n x = torch.nn.functional.cross_entropy(\n     prediction,\n     torch.ones(len(prediction), dtype=torch.long, device=prediction.device),\n )\n \n # devices 1 computes:\n y = torch.nn.functional.cross_entropy(\n     prediction,\n     torch.ones(len(prediction), dtype=torch.long, device=prediction.device),\n )\n \n # dp backend calls backward on stacked tensor\n l = torch.stack((x, y))\n l.backward()  # backward on a non-scalar\n Here is the pytorch code that shows the problem. Gives the same error as reported by <denchmark-link:https://github.com/willprice>@willprice</denchmark-link>\n \n <denchmark-code>  File \"/home/adrian/repositories/imagenet-optical-flow/asdf.py\", line 19, in <module>\n     l.backward()\n   File \"/home/adrian/bin/anaconda3/envs/lightning-0.7.1/lib/python3.7/site-packages/torch/tensor.py\", line 185, in backward\n     torch.autograd.backward(self, gradient, retain_graph, create_graph)\n   File \"/home/adrian/bin/anaconda3/envs/lightning-0.7.1/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 121, in backward\n     grad_tensors = _make_grads(tensors, grad_tensors)\n   File \"/home/adrian/bin/anaconda3/envs/lightning-0.7.1/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 47, in _make_grads\n     raise RuntimeError(\"grad can be implicitly created only for scalar outputs\")\n RuntimeError: grad can be implicitly created only for scalar outputs\n </denchmark-code>\n \n Conclusion: Somewhere in the dp backend the losses get stacked and backward is called on a non-scalar tensor.\n Have limited time rn, so dropping this info here for now to pick it up later\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "willprice", "commentT": "2020-10-14T09:20:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/willprice>@willprice</denchmark-link>\n  can you check the fix fro <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4138>#4138</denchmark-link>\n  ? For me this worked on the reproduction script.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "willprice", "commentT": "2020-11-12T15:37:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/willprice>@willprice</denchmark-link>\n  friendly ping :)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "willprice", "commentT": "2020-11-12T17:32:45Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>\n , I can confirm that this is fixed for me on the reproduction script and my own codebase. Although I did run into <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2350>this issue</denchmark-link>\n  when testing out <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4138>#4138</denchmark-link>\n  on my codebase (I have  set on my ).\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "willprice", "commentT": "2020-12-03T10:44:11Z", "comment_text": "\n \t\tI think I rediscovered this bug in our examples:\n \n I will try to help with the PR <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>\n  has still open so we can also finish <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4764>#4764</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "f23f5e56480d1a4784fc7829d014590fe4ca1454", "commit_author": "Justus Schock", "commitT": "2020-12-04 19:10:07+01:00", "commit_complexity": {"commit_NLOC": "0.5892857142857143", "commit_CCN": "0.5178571428571429", "commit_Nprams": "0.75"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "106", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\core\\step_result.py", "file_new_name": "pytorch_lightning\\core\\step_result.py", "file_complexity": {"file_NLOC": "677", "file_CCN": "197", "file_NToken": "4611"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "403,404,407", "deleted_lines": "403,404,407", "method_info": {"method_name": "to", "method_params": "self,args,kwargs", "method_startline": "403", "method_endline": "407", "method_complexity": {"method_NLOC": "4", "method_CCN": "3", "method_NToken": "50", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "409,410,411", "deleted_lines": null, "method_info": {"method_name": "cpu", "method_params": "self", "method_startline": "409", "method_endline": "411", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py", "file_new_name": "pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py", "file_complexity": {"file_NLOC": "425", "file_CCN": "87", "file_NToken": "2616"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "407,408", "deleted_lines": null, "method_info": {"method_name": "cache_result", "method_params": "self", "method_startline": "372", "method_endline": "418", "method_complexity": {"method_NLOC": "30", "method_CCN": "6", "method_NToken": "176", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py", "file_new_name": "pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py", "file_complexity": {"file_NLOC": "392", "file_CCN": "110", "file_NToken": "2804"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "391,392,393,394", "deleted_lines": "391,392,393,394,395", "method_info": {"method_name": "log_train_epoch_end_metrics", "method_params": "self,epoch_output,checkpoint_accumulator,early_stopping_accumulator,num_optimizers", "method_startline": "391", "method_endline": "395", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "13", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "292", "deleted_lines": "292,293,294,295", "method_info": {"method_name": "_track_callback_metrics", "method_params": "self,eval_results,using_eval_result", "method_startline": "291", "method_endline": "331", "method_complexity": {"method_NLOC": "34", "method_CCN": "14", "method_NToken": "283", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "388,389,390,391,392,393", "deleted_lines": "391,392,393", "method_info": {"method_name": "log_train_epoch_end_metrics", "method_params": "self,epoch_output,checkpoint_accumulator,early_stopping_accumulator,num_optimizers", "method_startline": "388", "method_endline": "393", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "12", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 11, "file_old_name": "tests\\trainer\\logging\\test_logger_connector.py", "file_new_name": "tests\\trainer\\logging\\test_logger_connector.py", "file_complexity": {"file_NLOC": "324", "file_CCN": "54", "file_NToken": "2168"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "422,423,424,425", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.test_step", "method_params": "self,args,kwargs", "method_startline": "422", "method_endline": "425", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "48", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "405,406,407", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.training_step_end", "method_params": "self,training_step_outputs", "method_startline": "405", "method_endline": "407", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "418,419,420", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.validation_epoch_end", "method_params": "self,outputs", "method_startline": "418", "method_endline": "420", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "33", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "434,435", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.val_dataloader", "method_params": "self", "method_startline": "434", "method_endline": "435", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "2"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "437,438", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.test_dataloader", "method_params": "self", "method_startline": "437", "method_endline": "438", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "2"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "427,428,429", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.test_epoch_end", "method_params": "self,outputs", "method_startline": "427", "method_endline": "429", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "33", "method_nesting_level": "2"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp", "method_params": "tmpdir", "method_startline": "394", "method_endline": "450", "method_complexity": {"method_NLOC": "24", "method_CCN": "1", "method_NToken": "91", "method_nesting_level": "0"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "409,410,411", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.training_epoch_end", "method_params": "self,outputs", "method_startline": "409", "method_endline": "411", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "36", "method_nesting_level": "2"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "431,432", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.train_dataloader", "method_params": "self", "method_startline": "431", "method_endline": "432", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "2"}}}, "hunk_9": {"Ismethod": 1, "added_lines": "413,414,415,416", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.validation_step", "method_params": "self,args,kwargs", "method_startline": "413", "method_endline": "416", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "48", "method_nesting_level": "2"}}}, "hunk_10": {"Ismethod": 1, "added_lines": "400,401,402,403", "deleted_lines": null, "method_info": {"method_name": "test_epoch_results_cache_dp.training_step", "method_params": "self,args,kwargs", "method_startline": "400", "method_endline": "403", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "46", "method_nesting_level": "2"}}}}}}}}