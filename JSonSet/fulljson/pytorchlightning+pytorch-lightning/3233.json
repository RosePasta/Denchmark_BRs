{"BR": {"BR_id": "3233", "BR_author": "carmocca", "BRopenT": "2020-08-27T21:56:50Z", "BRcloseT": "2020-09-03T20:07:50Z", "BR_text": {"BRsummary": "auto_scale_batch_size not working with datamodule", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n The Trainer expects the LightningModule to have self.batch_size (see scale_batch_size() in training_tricks.py). However, if one is using the new LightningDataModule, that should be the class with self.batch_size defined.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n assert hasattr(lightning_data_module, \"batch_size\")\n trainer = Trainer(auto_scale_batch_size=True)\n trainer.fit(lightning_module, datamodule=lightning_data_module)\n pytorch_lightning.utilities.exceptions.MisconfigurationException: Field batch_size not found in both `model` and `model.hparams`\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n auto_scale_batch_size should work using LightningDataModule\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>* Packages:\n \t- numpy:             1.18.5\n \t- pyTorch_debug:     False\n \t- pyTorch_version:   1.6.0\n \t- pytorch-lightning: 0.9.1rc1\n \t- tensorboard:       2.2.0\n \t- tqdm:              4.48.2\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "carmocca", "commentT": "2020-08-27T23:26:56Z", "comment_text": "\n \t\tI knew this bug report will eventually come :) same will happen for auto_lr_find.\n We need to generalize our lightning_hasattr and getattr helper functions to include the datamodule. In total, we have\n <denchmark-code>model.attribute_name\n model.hparams.attribute_name\n model.dm.attribute_name\n </denchmark-code>\n \n all of these should be considered by both lr_find and auto_scale_batch_size\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "carmocca", "commentT": "2020-08-28T14:56:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n   So currently there is no solution to this issue?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "carmocca", "commentT": "2020-08-28T15:00:09Z", "comment_text": "\n \t\tno, you need to set it manually, sorry.\n But I'll try to make a PR this weekend if not today. Should be a relatively easy fix unless I missed something.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "carmocca", "commentT": "2020-08-28T15:22:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  I completely agree that it should be a simple fix, since all the attribute getting/setting is handled by our own  ect functions.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "carmocca", "commentT": "2020-09-03T20:47:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>\n  As you may already know, we fixed this. Just a note in case you didn't see it, you now need to call .tune() instead of .fit():\n trainer.tune(lightning_module, datamodule=lightning_data_module)\n This is to better distinguish the training from the tuning step. However, it may be subject to change since there are some refactors happening right now.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "carmocca", "commentT": "2020-09-04T07:38:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n In which version  will be officially introduced?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "carmocca", "commentT": "2020-09-04T21:01:02Z", "comment_text": "\n \t\tIf it stays, in v1.0\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "carmocca", "commentT": "2020-09-05T02:01:15Z", "comment_text": "\n \t\tI am not familiar with the tuning interface. Few questions:\n Right now, I can use auto_scale_batch_size by passing the option to the trainer and calling fit. This does the auto scale procedure and then starts training\n If I understand correctly, when tune is released the following will be the necessary:\n <denchmark-code>should_auto_scale_bs = # comes from the user\n trainer = Trainer(auto_scale_batch_size=should_auto_scale_bs)\n \n if should_auto_scale_bs:\n     trainer.tune(...)\n trainer.fit(...)\n </denchmark-code>\n \n Im assuming tune doesnt run fit automatically when it is finished.\n What would happen if the code ran trainer.tune(...) outside of the if and auto_scale_batch_size was False?\n Also, shouldn't auto_scale_batch_size be a parameter of tune instead of Trainer? (Maybe it is, I just don't know where is the tune discussion).\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "carmocca", "commentT": "2020-09-05T03:33:33Z", "comment_text": "\n \t\tI do not know how to answer these questions, <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  added the tune method so it would be best to ask him how he sees it being used in the future.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "carmocca", "commentT": "2020-09-05T09:34:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>\n  I think the process in the future will be:\n <denchmark-code>trainer = Trainer(auto_scale_batch_size = should_auto_scale_bs)\n trainer.tune(model) # will do nothing if should_auto_scale_bs=False\n trainer.fit(model)\n </denchmark-code>\n \n it should still be easy for the user to use these features. The moving of the tuning from fit to tune is to disentangle the hyperparameter tuning from the actual optimization of the network. This will make it easier for us in the future to implement more tuning algorithms.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "carmocca", "commentT": "2020-10-23T21:03:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  , I have the same issue happening with me, my code is:\n <denchmark-code> # Reading and intilaizing the Trainer\n     trainer_config = config.pop('trainer_config')\n     trainer = Trainer(\n         **trainer_config,\n         callbacks=callbacks,\n         logger=loggers,\n         checkpoint_callback=checkpoint_callback,\n         auto_scale_batch_size=(dataset_config['batch_size'] is None)\n     )\n \n     # optimizing batch size if batch_size is none\n     if dataset_config['batch_size'] is None:\n         print(\"Batch Size is None attempting to tune batch size\")\n         # tuner = Tuner(trainer)\n         # optimal_batch_size = tuner.scale_batch_size(\n         #     model, mode='power',\n         #     batch_arg_name='batch_size',\n         #     datamodule=dataset)\n         optimal_batch_size = trainer.tune(model, datamodule=dataset)\n         print(f\"Found best batch size to be: {optimal_batch_size}\")\n         dataset.batch_size = optimal_batch_size\n </denchmark-code>\n \n I tracked the issue and I think there is a problem in logic from <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L186>parsing.py Line186</denchmark-link>\n  to <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L193>parsing.py Line193</denchmark-link>\n \n The problem is that I have a hparams attribute in my model (I don't know where that came from),  but it doesn't contain a batch size attribute, the batch_size is an attribute that is contained in the datamodule, if <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L192>this if condition</denchmark-link>\n  is executed then I think there would not be a problem\n \t\t"}}}, "commit": {"commit_id": "48c22c8bad9a47141c7160d92f2edc9e2e4ad159", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-09-03 22:07:49+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.75"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "38,39", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "1072", "file_CCN": "138", "file_NToken": "4640"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "954,955,956,957,958,959,960", "deleted_lines": "954"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "pytorch_lightning\\trainer\\training_tricks.py", "file_new_name": "pytorch_lightning\\trainer\\training_tricks.py", "file_complexity": {"file_NLOC": "252", "file_CCN": "41", "file_NToken": "1512"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "137", "deleted_lines": "137", "method_info": {"method_name": "scale_batch_size", "method_params": "self,LightningModule,str,int,int,int,str", "method_startline": "131", "method_endline": "137", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "39", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "294,302", "deleted_lines": "301", "method_info": {"method_name": "_run_power_scaling", "method_params": "trainer,model,new_size,batch_arg_name,max_trials,fit_kwargs", "method_startline": "294", "method_endline": "314", "method_complexity": {"method_NLOC": "15", "method_CCN": "4", "method_NToken": "100", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "317,328", "deleted_lines": "327", "method_info": {"method_name": "_run_binsearch_scaling", "method_params": "trainer,model,new_size,batch_arg_name,max_trials,fit_kwargs", "method_startline": "317", "method_endline": "353", "method_complexity": {"method_NLOC": "30", "method_CCN": "8", "method_NToken": "164", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "137,138", "deleted_lines": "137", "method_info": {"method_name": "scale_batch_size", "method_params": "self,LightningModule,str,int,int,int,str,fit_kwargs", "method_startline": "131", "method_endline": "138", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "42", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "294,302", "deleted_lines": "293,301", "method_info": {"method_name": "_run_power_scaling", "method_params": "trainer,model,new_size,batch_arg_name,max_trials", "method_startline": "293", "method_endline": "313", "method_complexity": {"method_NLOC": "15", "method_CCN": "4", "method_NToken": "94", "method_nesting_level": "0"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "317,328", "deleted_lines": "316,327", "method_info": {"method_name": "_run_binsearch_scaling", "method_params": "trainer,model,new_size,batch_arg_name,max_trials", "method_startline": "316", "method_endline": "352", "method_complexity": {"method_NLOC": "30", "method_CCN": "8", "method_NToken": "158", "method_nesting_level": "0"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\utilities\\parsing.py", "file_new_name": "pytorch_lightning\\utilities\\parsing.py", "file_complexity": {"file_NLOC": "173", "file_CCN": "53", "file_NToken": "950"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "201,202,203,204,214,215,216,217,219,220", "deleted_lines": "208,209,215", "method_info": {"method_name": "lightning_getattr", "method_params": "model,attribute", "method_startline": "200", "method_endline": "221", "method_complexity": {"method_NLOC": "15", "method_CCN": "7", "method_NToken": "110", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "226,227,228,229,230,231,232,233,234,238,245,246,247,248", "deleted_lines": "225,226,227", "method_info": {"method_name": "lightning_setattr", "method_params": "model,attribute,value", "method_startline": "224", "method_endline": "248", "method_complexity": {"method_NLOC": "14", "method_CCN": "8", "method_NToken": "117", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "178,179,180,181,191,192,193", "deleted_lines": "178,179,196,197", "method_info": {"method_name": "lightning_hasattr", "method_params": "model,attribute", "method_startline": "177", "method_endline": "197", "method_complexity": {"method_NLOC": "14", "method_CCN": "7", "method_NToken": "100", "method_nesting_level": "0"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\trainer\\test_trainer_tricks.py", "file_new_name": "tests\\trainer\\test_trainer_tricks.py", "file_complexity": {"file_NLOC": "200", "file_CCN": "19", "file_NToken": "1587"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "232,233,234,237,240,241,244,245,246", "deleted_lines": "235", "method_info": {"method_name": "test_auto_scale_batch_size_set_model_attribute", "method_params": "tmpdir,use_hparams", "method_startline": "215", "method_endline": "246", "method_complexity": {"method_NLOC": "18", "method_CCN": "3", "method_NToken": "138", "method_nesting_level": "0"}}}}}}}}