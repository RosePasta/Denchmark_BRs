{"BR": {"BR_id": "3597", "BR_author": "sshleifer", "BRopenT": "2020-09-22T00:26:46Z", "BRcloseT": "2020-10-07T11:43:18Z", "BR_text": {"BRsummary": "distributed training: ModelCheckpoint is receiving bad data", "BRdescription": "\n You can reproduce in 4 minutes on 0.9.0.\n I tried master and got an unrelated wandb error and gave up trying to reproduce there.\n you must be on a machine with multiple gpus\n git clone git@github.com:huggingface/transformers.git\n cd transformers\n pip install -e .\n pip install -e .[examples]  # installs pytorch-lightning==0.8.5\n git checkout pl-checkpoint-bug\n cd examples/seq2seq\n wget https://s3.amazonaws.com/datasets.huggingface.co/translation/wmt_en_ro.tar.gz\n tar -xzvf wmt_en_ro.tar.gz\n \n export MAX_LEN=128\n export m=sshleifer/student_marian_en_ro_6_3\n \n python finetune.py \\\n   --learning_rate=3e-4 \\\n   --do_train \\\n   --do_predict \\\n   --fp16 \\\n   --val_check_interval 0.25 \\\n   --data_dir wmt_en_ro \\\n   --max_source_length $MAX_LEN --max_target_length $MAX_LEN --val_max_target_length $MAX_LEN --test_max_target_length $MAX_LEN \\\n   --freeze_encoder --freeze_embeds \\\n   --train_batch_size=64 --eval_batch_size=64 \\\n   --tokenizer_name $m --model_name_or_path $m \\\n   --warmup_steps 500 --sortish_sampler --logger_name wandb \\\n   --fp16_opt_level=O1 --task translation --num_sanity_val_steps=0 \\\n   --model_name_or_path $m --gpus 8 --num_train_epochs=1 \\\n   --data_dir wmt_mar_pl --output_dir dmar_pl_only_v3 --save_top_k=10\n <denchmark-h:h3>Results</denchmark-h>\n \n ls dmar_pl_only_v3/*.ckpt\n <denchmark-code>-rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:58 dmar_pl_only_v3/val_avg_bleu=23.3951-step_count=5.ckpt\n -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:57 dmar_pl_only_v3/val_avg_bleu=23.2619-step_count=4.ckpt\n -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:56 dmar_pl_only_v3/val_avg_bleu=22.6724-step_count=3.ckpt\n -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:56 dmar_pl_only_v3/val_avg_bleu=22.2664-step_count=2.ckpt\n -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:55 dmar_pl_only_v3/val_avg_bleu=23.2263-step_count=1.ckpt\n </denchmark-code>\n \n There are 5 checkpoints which much lower scores. PL thinks the best checkpoint is from step 5, but\n <denchmark-code>cat dmar_pl_only_v3/metrics.json | grep bleu\n </denchmark-code>\n \n <denchmark-code>            \"val_avg_bleu\": 26.4513,\n             \"val_avg_bleu\": 25.5289,\n             \"val_avg_bleu\": 25.6942,\n             \"val_avg_bleu\": 26.2227,\n             \"val_avg_bleu\": 25.8546,\n </denchmark-code>\n \n (the best checkpoint is step 1)\n When I evaluate offline on the best checkpoint without truncation, I get val_bleu =  27+, which makes me nearly certain that the numbers in  (which I create and save in <denchmark-link:https://github.com/huggingface/transformers/blob/master/examples/seq2seq/finetune.py#L209>finetune.py</denchmark-link>\n  are correct and the numbers in the saved paths are incorrect.)\n Is this a known issue with a workaround? How can I fix? Should be high priority because suboptimal checkpoint saving is a huge productivity drain.\n <denchmark-h:h3>Additional Notes:</denchmark-h>\n \n \n The numbers logged to wandb are also the low/wrong ones.\n on 1 or 2 GPU the numbers are identical!\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sshleifer", "commentT": "2020-09-22T09:44:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>\n  verifying this but you might be right, even I felt in one of my previous training runs that the epoch with minimum validation wasn't saved.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sshleifer", "commentT": "2020-09-22T16:50:39Z", "comment_text": "\n \t\tWhere in the PL code does trainer.callback_metrics gather data from all nodes?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sshleifer", "commentT": "2020-09-22T16:52:00Z", "comment_text": "\n \t\tCC: <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>\n  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "sshleifer", "commentT": "2020-09-22T19:51:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>\n  if you use our metrics they have built-in ddp reduction (each metric separately)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "sshleifer", "commentT": "2020-09-22T20:51:11Z", "comment_text": "\n \t\tDo you have a working example of how that would work for ROUGE/BLEU?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "sshleifer", "commentT": "2020-09-22T20:52:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>\n  looked at my logs, my confusion was because I was looking at online fine tuning loss values for pre-training, checking this for BLEU. BLEU needs to be verified for DDP sync right now, this is something we are already working on.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "sshleifer", "commentT": "2020-09-22T21:12:39Z", "comment_text": "\n \t\tOK, so in the current state of the code, is trainer.callback_metrics always from rank 0, from the last rank to finish, or from the first rank to finish?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "sshleifer", "commentT": "2020-09-23T06:07:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>\n  <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>\n  just wanted to clarify that this has nothing to do with the metric package, since this is calculating blue score using another package.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "sshleifer", "commentT": "2020-10-05T03:30:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>\n  mind trying master? we added a lot of tests to model checkpoint and cleaned up the logging API.\n 1.0 will be released in a few days with these changes.\n 0.10.0rc1 is now available.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "sshleifer", "commentT": "2020-10-07T09:11:59Z", "comment_text": "\n \t\tLet's see how it works with the new Metric API made by <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>\n  and <denchmark-link:https://github.com/teddykoker>@teddykoker</denchmark-link>\n \n <denchmark-link:https://github.com/SeanNaren>@SeanNaren</denchmark-link>\n  it may be solved, but it would be nice to add a test for this case to prevent it in future\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "sshleifer", "commentT": "2020-10-07T09:43:53Z", "comment_text": "\n \t\tI can confirm on pytorch-lightning <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/9c415d2c71ddc44ca3344ba1136c0dd546fc5ef6>master</denchmark-link>\n  and transformers <denchmark-link:https://github.com/huggingface/transformers/commit/8fa0c956b34123d1f1406ae96d74c484976d0e3f>master</denchmark-link>\n  metrics/ckpts are in sync. Modified the cmd slightly by disabling wandb + fixed data_dir path.\n <denchmark-code>python finetune.py \\\n   --learning_rate=3e-4 \\\n   --do_train \\\n   --fp16 \\\n   --val_check_interval 0.25 \\\n   --data_dir wmt_en_ro \\\n   --max_source_length $MAX_LEN --max_target_length $MAX_LEN --val_max_target_length $MAX_LEN --test_max_target_length $MAX_LEN \\\n   --freeze_encoder --freeze_embeds \\\n   --train_batch_size=64 --eval_batch_size=64 \\\n   --tokenizer_name $m --model_name_or_path $m \\\n   --warmup_steps 500 --sortish_sampler  \\\n   --fp16_opt_level=O1 --task translation --num_sanity_val_steps=0 \\\n   --model_name_or_path $m --gpus 8 --num_train_epochs=1 \\\n   --data_dir wmt_mar_pl --output_dir dmar_pl_only_v3 --save_top_k=10\n \n </denchmark-code>\n \n <denchmark-code>~/transformers/examples/seq2seq$ ls dmar_pl_only_v3/*.ckpt\n 'dmar_pl_only_v3/val_avg_bleu=21.3473-step_count=1.ckpt'\n 'dmar_pl_only_v3/val_avg_bleu=21.5114-step_count=2.ckpt'\n 'dmar_pl_only_v3/val_avg_bleu=23.1029-step_count=3.ckpt'\n 'dmar_pl_only_v3/val_avg_bleu=23.2499-step_count=4.ckpt'\n </denchmark-code>\n \n <denchmark-code>cat dmar_pl_only_v3/metrics.json | grep bleu\n             \"val_avg_bleu\": 21.3473,\n             \"val_avg_bleu\": 21.51145,\n             \"val_avg_bleu\": 23.10295,\n             \"val_avg_bleu\": 23.24995,\n </denchmark-code>\n \n When using --do_predict there is an unrelated issue in Transformers which needs to be fixed in finetune.py. I think main gets run again:\n <denchmark-code>Traceback (most recent call last):\n   File \"/home/jovyan/transformers/examples/seq2seq/finetune.py\", line 440, in <module>\n     main(args)\n   File \"/home/jovyan/transformers/examples/seq2seq/finetune.py\", line 376, in main\n     raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n ValueError: Output directory (dmar_pl_only_v3) already exists and is not empty.\n </denchmark-code>\n \n As <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  said I'll get a test in place to ensure the file path metrics are correct!\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "sshleifer", "commentT": "2020-10-07T14:51:39Z", "comment_text": "\n \t\tOn pl=master in my multigpu unittest, getting\n trainer.test()\n \n ERR:   File \"/home/shleifer/miniconda3/envs/nb/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 773, in __test_using_best_weights\n ERR:     'ckpt_path is \"best\", but ModelCheckpoint is not configured to save the best model.'\n ERR: pytorch_lightning.utilities.exceptions.MisconfigurationException: ckpt_path is \"best\", but ModelCheckpoint is not configured to save the best model.`\n There is 1 checkpoint in the save directory at the correct time.\n Does ModelCheckpoint need to be configured differently?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "sshleifer", "commentT": "2020-10-07T14:53:51Z", "comment_text": "\n \t\thave you set a monitor key?\n only with this will it be able to track best\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "sshleifer", "commentT": "2020-10-07T15:16:11Z", "comment_text": "\n \t\tyes, we removed the magical \u201cval_loss\u201d key for 1.0.\n by default without a checkpoint CB we save the last always.\n to change that you have to now pass in a callback with the monitor keyword.\n it makes it less magical now and more transparent. also opens the door to multiple checkpoint callbacks soon\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "sshleifer", "commentT": "2020-10-07T15:17:59Z", "comment_text": "\n \t\tYes I have a checkpoint callback with a monitor\n     checkpoint_callback = ModelCheckpoint(\n         filepath=os.path.join(output_dir, exp),\n         monitor=f\"val_{metric}\",\n         mode=\"min\" if \"loss\" in metric else \"max\",\n         save_top_k=save_top_k,\n         period=0,  # Interval (number of val checks) between checkpoints.\n \n     )\n It correctly saves checkpoints but trainer.test doesn't recognize it for some reason.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "sshleifer", "commentT": "2020-10-07T15:53:03Z", "comment_text": "\n \t\tdo you run ddp? how do I reproduce it?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "sshleifer", "commentT": "2020-10-07T15:59:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>\n  can you reproduce using the BoringModel?\n if you submit a failing test for this using BoringModel we can fix in a few mins\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "sshleifer", "commentT": "2020-10-07T19:08:55Z", "comment_text": "\n \t\tTried and gave up after 30 mins.\n Had to guess install instructions deal (after scanning README) with failing horovod install. (I blindly ran pip install -r requirements/devel.txt) Then I tried to add to test_ddp and test_trainer but my test was failing for other reasons than the bug I was trying to isolate. (I'm not whining just telling you cause I would want you to tell me if the roles were reversed.)\n Is there a test that uses:\n DDP, BoringModel, and ModelCheckpoint and calls trainer.test at the end?\n If that exists and passes, I won't be able to replicate my issue.\n if it doesn't exist but passes: you increased the likelihood that my issue is my fault and improved your test coverage.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "sshleifer", "commentT": "2020-10-07T19:36:52Z", "comment_text": "\n \t\tyup! this test\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/backends/test_ddp.py#L44-L57>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/backends/test_ddp.py#L44-L57</denchmark-link>\n \n \n EvalModelTemplate which is a bit more involved than BoringModel\n DDP\n modelcheckpoint\n calls both fit and test at the end\n \n And... to be extra safe ;)  it even makes sure it gets 90+ acc\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "sshleifer", "commentT": "2020-10-19T13:56:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>\n  I can confirm this works using the below boring model test:\n import os\n \n import torch\n from torch.utils.data import Dataset\n \n from pytorch_lightning import Trainer, LightningModule\n from pytorch_lightning.callbacks import ModelCheckpoint\n \n \n class RandomDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         return self.data[index]\n \n     def __len__(self):\n         return self.len\n \n \n class BoringModel(LightningModule):\n \n     def __init__(self):\n         \"\"\"\n         Testing PL Module\n \n         Use as follows:\n         - subclass\n         - modify the behavior for what you want\n \n         class TestModel(BaseTestModel):\n             def training_step(...):\n                 # do your own thing\n \n         or:\n \n         model = BaseTestModel()\n         model.training_epoch_end = None\n \n         \"\"\"\n         super().__init__()\n         self.layer = torch.nn.Linear(32, 2)\n \n     def forward(self, x):\n         return self.layer(x)\n \n     def loss(self, batch, prediction):\n         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\n         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\n \n     def step(self, x):\n         x = self.layer(x)\n         out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\n         return out\n \n     def training_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log('loss', loss)\n         return {\"loss\": loss}\n \n     def training_step_end(self, training_step_outputs):\n         return training_step_outputs\n \n     def validation_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log('x', loss)\n \n     def test_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log('y', loss)\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.AdamW(self.layer.parameters(), lr=0.1)\n         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n         return [optimizer], [lr_scheduler]\n \n \n def run_test():\n     # fake data\n     train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\n     val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\n \n     # model\n     model = BoringModel()\n     trainer = Trainer(\n         default_root_dir=os.getcwd(),\n         limit_train_batches=4,\n         limit_val_batches=2,\n         max_epochs=1,\n         accelerator='ddp',\n         gpus=2,\n         checkpoint_callback=ModelCheckpoint(\n             monitor='x',\n             mode='min',\n             save_top_k=1\n         )\n     )\n     trainer.fit(model, train_data, val_data)\n     trainer.test()\n \n \n if __name__ == '__main__':\n     run_test()\n I think the error message is a bit ambiguous, but the issue might be not calling self.log to report the metrics we'd like to save on.\n \t\t"}}}, "commit": {"commit_id": "2aebf65241ab054df9256cc33d37236651691a48", "commit_author": "Sean Naren", "commitT": "2020-10-07 07:43:17-04:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\checkpointing\\test_model_checkpoint.py", "file_new_name": "tests\\checkpointing\\test_model_checkpoint.py", "file_complexity": {"file_NLOC": "380", "file_CCN": "45", "file_NToken": "2891"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43", "deleted_lines": null, "method_info": {"method_name": "test_model_checkpoint_correct_score", "method_params": "tmpdir,save_top_k", "method_startline": "23", "method_endline": "43", "method_complexity": {"method_NLOC": "14", "method_CCN": "3", "method_NToken": "129", "method_nesting_level": "0"}}}}}}}}