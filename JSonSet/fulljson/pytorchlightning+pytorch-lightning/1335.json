{"BR": {"BR_id": "1335", "BR_author": "areshytko", "BRopenT": "2020-04-01T23:31:03Z", "BRcloseT": "2020-04-06T14:17:17Z", "BR_text": {"BRsummary": "Trainer DDP should invoke load_spawn_weights() only in proc_rank == 0", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Trainer DDP load_spawn_weights should happen only in proc_rank == 0 since only in this process (node) save_spawn_weights actually saves checkpoint\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n setup two-node cluster.\n set SLURM_NODEID on each node: '0' on node 0 and '1' on node 1.\n run the script python app.py on each node.\n see stdout on the node 1:\n \n <denchmark-code>Traceback (most recent call last):\n   File \"app.py\", line 166, in <module>\n     main_()  # pylint: disable=no-value-for-parameter\n   File \"app.py\", line 162, in main_\n     trainer.fit(model)\n   File \"/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 593, in fit\n     self.load_spawn_weights(model)\n   File \"/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 368, in load_spawn_weights\n     loaded_model = original_model.__class__.load_from_checkpoint(path)\n   File \"/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\", line 1353, in load_from_checkpoint\n     checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n   File \"/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/torch/serialization.py\", line 525, in load\n     with _open_file_like(f, 'rb') as opened_file:\n   File \"/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/torch/serialization.py\", line 212, in _open_file_like\n     return _open_file(name_or_buffer, mode)\n   File \"/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/torch/serialization.py\", line 193, in __init__\n     super(_open_file, self).__init__(open(name, mode))\n FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/pytorch-lightning-intro-guide/__temp_weight_ddp_end.ckpt'\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n app.py:\n <denchmark-code>import pathlib\n \n import pytorch_lightning as pl\n import torch\n from torch.nn import functional as F\n from torch.optim import Adam\n from torch.utils.data import DataLoader, random_split\n from torchvision import datasets, transforms\n \n \n class LitMNIST(pl.LightningModule):\n     def __init__(self):\n         super().__init__()\n         self.layer_1 = torch.nn.Linear(28 * 28, 128)\n         self.layer_2 = torch.nn.Linear(128, 256)\n         self.layer_3 = torch.nn.Linear(256, 10)\n \n         self.train_dataset = None\n         self.val_dataset = None\n         self.test_dataset = None\n \n     def forward(self, x):\n         batch_size, channels, width, height = x.size()\n         x = x.view(batch_size, -1)\n         x = self.layer_1(x)\n         x = F.relu(x)\n         x = self.layer_2(x)\n         x = F.relu(x)\n         x = self.layer_3(x)\n         x = F.log_softmax(x, dim=1)\n         return x\n \n     def prepare_data(self):\n         # transform\n         transform = transforms.Compose(\n             [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n \n         # download\n         data_dir = pathlib.Path.home() / 'data'\n         mnist_train = datasets.MNIST(data_dir, train=True,\n                                      download=True, transform=transform)\n         mnist_test = datasets.MNIST(data_dir, train=False,\n                                     download=True, transform=transform)\n \n         # train/val split\n         mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\n \n         # assign to use in dataloaders\n         self.train_dataset = mnist_train\n         self.val_dataset = mnist_val\n         self.test_dataset = mnist_test\n \n     def train_dataloader(self):\n         return DataLoader(self.train_dataset, batch_size=64)\n \n     def val_dataloader(self):\n         return DataLoader(self.val_dataset, batch_size=64)\n \n     def test_dataloader(self):\n         return DataLoader(self.test_dataset, batch_size=64)\n \n     def configure_optimizers(self):\n         return Adam(self.parameters(), lr=1e-3)\n \n     def training_step(self, batch, batch_idx):\n         x, y = batch\n         logits = self(x)\n         loss = F.nll_loss(logits, y)\n \n         # add logging\n         logs = {'loss': loss}\n         return {'loss': loss, 'log': logs}\n \n     def validation_step(self, batch, batch_idx):\n         x, y = batch\n         logits = self(x)\n         loss = F.nll_loss(logits, y)\n         return {'val_loss': loss}\n \n     def test_step(self, batch, batch_idx):\n         x, y = batch\n         logits = self(x)\n         loss = F.nll_loss(logits, y)\n         return {'val_loss': loss}\n \n     def test_epoch_end(self, outputs):\n         avg_loss = torch.stack(  # pylint: disable=no-member\n             [x['val_loss'] for x in outputs]).mean()\n         tensorboard_logs = {'val_loss': avg_loss}\n         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n \n     def init_ddp_connection(self, proc_rank: int, world_size: int) -> None:\n         torch.distributed.init_process_group(\n             'nccl', rank=proc_rank, world_size=world_size)\n \n def main():\n     model = LitMNIST()\n \n     gpus = 1\n     num_nodes = 2\n \n     trainer = pl.Trainer(gpus=gpus,\n                          num_nodes=num_nodes,\n                          distributed_backend='ddp',\n                          max_epochs=3)\n     trainer.fit(model)\n \n \n if __name__ == '__main__':\n     main()\n \n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n All workers on all nodes should finish without errors.\n <denchmark-h:h3>Environment</denchmark-h>\n \n On each node:\n <denchmark-code>cuda:\n \tGPU:\n \t\tTesla K80\n \t\tTesla K80\n \t\tTesla K80\n \t\tTesla K80\n \t\tTesla K80\n \t\tTesla K80\n \t\tTesla K80\n \t\tTesla K80\n \tavailable:           True\n \tversion:             10.1\n packages:\n \tnumpy:               1.16.6\n \tpyTorch_debug:       False\n \tpyTorch_version:     1.4.0\n \tpytorch-lightning:   0.7.1\n \ttensorboard:         2.2.0\n \ttqdm:                4.44.1\n system:\n \tOS:                  Linux\n \tarchitecture:\n \t\t64bit\n \n \tprocessor:           x86_64\n \tpython:              3.7.7\n \tversion:             #113-Ubuntu SMP Wed Jan 29 14:54:54 UTC 2020\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {}}, "commit": {"commit_id": "9754c5da55059dd89cf0a4fd582fe5df9449bbe5", "commit_author": "areshytko", "commitT": "2020-04-06 10:17:16-04:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_complexity": {"file_NLOC": "272", "file_CCN": "47", "file_NToken": "979"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "367,369,370,371,372,373,374,375,376,377,378", "deleted_lines": "366,367,368,370,371,373,374", "method_info": {"method_name": "load_spawn_weights", "method_params": "self,original_model", "method_startline": "359", "method_endline": "380", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "60", "method_nesting_level": "1"}}}}}}}}