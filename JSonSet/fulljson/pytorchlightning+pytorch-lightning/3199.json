{"BR": {"BR_id": "3199", "BR_author": "Lucas-Steinmann", "BRopenT": "2020-08-26T17:53:25Z", "BRcloseT": "2020-09-18T21:08:06Z", "BR_text": {"BRsummary": "Early Stopping + result dictionary + no validation not working.", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n The case where the user does not use validation and returns a dictionary (instead of a TrainResult) during training does not work in combination with early stopping.\n The test case which should check this is here:\n \n \n \n pytorch-lightning/tests/callbacks/test_early_stopping.py\n \n \n         Lines 136 to 159\n       in\n       bd35c86\n \n \n \n \n \n \n  def test_early_stopping_no_val_step(tmpdir): \n \n \n \n  \"\"\"Test that early stopping callback falls back to training metrics when no validation defined.\"\"\" \n \n \n \n  \n \n \n \n  class CurrentModel(EvalModelTemplate): \n \n \n \n  def training_step(self, *args, **kwargs): \n \n \n \n  output = super().training_step(*args, **kwargs) \n \n \n \n  output.update({'my_train_metric': output['loss']})  # could be anything else \n \n \n \n  return output \n \n \n \n  \n \n \n \n  model = CurrentModel() \n \n \n \n  model.validation_step = None \n \n \n \n  model.val_dataloader = None \n \n \n \n  \n \n \n \n  stopping = EarlyStopping(monitor='my_train_metric', min_delta=0.1) \n \n \n \n  trainer = Trainer( \n \n \n \n  default_root_dir=tmpdir, \n \n \n \n  early_stop_callback=stopping, \n \n \n \n  overfit_batches=0.20, \n \n \n \n  max_epochs=2, \n \n \n \n      ) \n \n \n \n  result = trainer.fit(model) \n \n \n \n  \n \n \n \n  assert result == 1, 'training failed to complete' \n \n \n \n  assert trainer.current_epoch < trainer.max_epochs \n \n \n \n \n \n The check in the last line is wrong. It should actually compare:\n     assert trainer.current_epoch < trainer.max_epochs - 1\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Fix the test case\n Run tests.\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n I guess using the test case is simpler and easier.\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n That is an interesting question indeed. Possibilities are:\n \n Test case should pass with correct comparison\n The docs and  @williamFalcon in #3193 (comment) suggest that only 'loss' should work.\n \n So before fixing this issue, it should be settled what the expected behavior is.\n If you tell me, I'm happy to help.\n I could also include it in the pull request where I already tried to bring the docs in line with the test cases.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n \n GPU:\n \n GeForce GTX 1080 Ti\n \n \n available:         True\n version:           10.1\n \n \n Packages:\n \n numpy:             1.19.1\n pyTorch_debug:     False\n pyTorch_version:   1.6.0\n pytorch-lightning: 0.9.1dev\n tensorboard:       2.2.0\n tqdm:              4.48.2\n \n \n System:\n \n OS:                Linux\n architecture:\n \n 64bit\n ELF\n \n \n processor:         x86_64\n python:            3.8.3\n version:           #113~16.04.1-Ubuntu SMP Fri Jul 10 04:37:08 UTC 2020\n \n \n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Lucas-Steinmann", "commentT": "2020-08-26T19:00:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Lucas-Steinmann>@Lucas-Steinmann</denchmark-link>\n  mind sending a PR for this?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Lucas-Steinmann", "commentT": "2020-08-26T19:41:11Z", "comment_text": "\n \t\tI'd love to, but what is the intended behavior?\n Possibility 1, 2, or doesn't it matter?\n Maybe we should wait on <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  answer, since he already expressed his opinion.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Lucas-Steinmann", "commentT": "2020-08-27T10:31:29Z", "comment_text": "\n \t\tThere is a third possibility, which is suggested by the code which would have to be changed:\n \n \n \n pytorch-lightning/pytorch_lightning/callbacks/early_stopping.py\n \n \n          Line 169\n       in\n       4d98419\n \n \n \n \n \n \n  # early stopping can also work in the train loop when there is no val loop and when using structured results \n \n \n \n \n \n This comment says, that the current behavior is correct. (assuming \"structured result\" means Result object)\n So either code, docs or test is correct and the other two have to be adjusted. :)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Lucas-Steinmann", "commentT": "2020-08-27T14:10:02Z", "comment_text": "\n \t\tif you're not using the trainresult then the key 'loss' needs to be present\n All of these are equivalent\n return loss\n \n return {'loss': loss}\n \n return TrainResult(minimize=loss)\n \n return TrainResult(loss)\n But the recommended way is:\n <denchmark-code>return TrainResult(loss)\n </denchmark-code>\n \n We're moving away from plain dictionaries (TrainResult is just a dict also, but adds type checking)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Lucas-Steinmann", "commentT": "2020-08-27T14:18:33Z", "comment_text": "\n \t\tBut return {'loss': loss} doesn't work with early stopping.\n Also the test, which suggests that any key should work still contradicts you.\n Furthermore the test is still wrong because the last line does not actually check whether early stopping was triggered (off-by-one error)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Lucas-Steinmann", "commentT": "2020-08-28T16:11:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Lucas-Steinmann>@Lucas-Steinmann</denchmark-link>\n  reopening this then!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "Lucas-Steinmann", "commentT": "2020-09-01T13:24:41Z", "comment_text": "\n \t\tJust to be clear. The next step is for you (the project owners) to decide what the correct behaviour is:\n \n The docs (only dicts with 'loss' key should work with early stopping): Then we have to fix the code and the tests.\n The code (early stopping should only work with TrainResult): Then we have to remove the test and fix the docs\n The test (early stopping should work with dicts and any key): Then we have to fix the code and the docs.\n \n I would love to help but I think it is not for me to decide what is correct.\n If you pick one, I'll implement it.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "Lucas-Steinmann", "commentT": "2020-09-01T13:33:04Z", "comment_text": "\n \t\tThe correct behavior is neither of the above...\n \n if using dicts, it should work with whatever you put in the monitor argument of the EarlyStopping or ModelCheckpoint.\n \n EarlyStopping(monitor='jiraffe')\n \n # must have this key\n return {'jiraffe': x}\n \n If using results it should work with whatever is in the early_stop_on argument:\n \n acc = ...\n loss = ...\n f1 = ...\n \n # pick whatever you want to early stop on\n key_i_care_about = f1 or loss or acc\n result = pl.TrainResult(early_stop_on=key_i_care_about)\n If you ALSO pass the key in the EvalResult then the EvalResult takes precedence...\n def training_step(...):\n    # early stop on has NO effect because you ALSO have one in validation_step\n     result = TrainResult(early_stop_on=acc)\n \n def validation_step(...)\n     my_val_loss = MSE(y, y_hat)\n     result = EvalResult(early_stop_on=my_val_loss)\n Finally, I'm pretty sure this is how it works today given that I personally implemented these and wrote a ton of tests...\n So, my question is:\n \n does it not work like I described? then we need to fix the bugs\n if it does work, then we need to clean up the docs to explain this better.\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "Lucas-Steinmann", "commentT": "2020-09-01T13:44:16Z", "comment_text": "\n \t\tOk. 1. currently does not work (atleast a few days ago, when I wrote the issue, see my minimal example here: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3193#discussion_r478517184>#3193 (comment)</denchmark-link>\n )\n There is a test, which exactly checks 1.  but has an off-by-one error and would actually fail if the error is fixed (see my first comment on this issue).\n If I understand the docs correctly, they currently say it should only work with 'loss' key. Maybe it was meant to say the default behaviour is that it only works with 'loss' key: \n \n \n pytorch-lightning/docs/source/results.rst\n \n \n          Line 109\n       in\n       3910ad0\n \n \n \n \n \n \n   # early stop + checkpoint can only use the `loss` when done manually via dictionaries \n \n \n \n \n \n I think the bug in the code is located here, where it only checks for the early_stop_on key:\n \n \n \n pytorch-lightning/pytorch_lightning/callbacks/early_stopping.py\n \n \n         Lines 170 to 173\n       in\n       3910ad0\n \n \n \n \n \n \n  train_es_key = 'early_stop_on' \n \n \n \n  if trainer.callback_metrics.get(train_es_key, None) is not None: \n \n \n \n  self.monitor = train_es_key \n \n \n \n  should_check_early_stop = True \n \n \n \n \n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "Lucas-Steinmann", "commentT": "2020-09-04T15:27:20Z", "comment_text": "\n \t\tTo prevent misunderstanding:\n <denchmark-code>EarlyStopping(monitor='jiraffe')\n \n # must have this key\n return {'jiraffe': x}\n </denchmark-code>\n \n the return {'jiraffe': x} is in training_step()?\n I've implemented a pull request. <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3347>#3347</denchmark-link>\n .\n It first corrects the test, which then fails. Then I fixed the issue.\n Afterwards another bug was unveiled, which I fixed in the subsequent commits.\n Then I also updated the docs.\n \t\t"}}}, "commit": {"commit_id": "197acd535fee5e79dafeeff14cc742095c77bd70", "commit_author": "Lucas Steinmann", "commitT": "2020-09-18 23:08:04+02:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.8421052631578947"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\results.rst", "file_new_name": "docs\\source\\results.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "104,105,106,107,108,109,110,111,112,113"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\callbacks\\early_stopping.py", "file_new_name": "pytorch_lightning\\callbacks\\early_stopping.py", "file_complexity": {"file_NLOC": "173", "file_CCN": "34", "file_NToken": "876"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "163,164,165", "deleted_lines": "160,164", "method_info": {"method_name": "on_validation_epoch_end", "method_params": "self,trainer,pl_module", "method_startline": "149", "method_endline": "165", "method_complexity": {"method_NLOC": "11", "method_CCN": "5", "method_NToken": "74", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "169,172,174,179,180,181", "deleted_lines": "167", "method_info": {"method_name": "on_train_epoch_end", "method_params": "self,trainer,pl_module", "method_startline": "167", "method_endline": "184", "method_complexity": {"method_NLOC": "12", "method_CCN": "5", "method_NToken": "79", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "199,200,201,202", "deleted_lines": null, "method_info": {"method_name": "_run_early_stopping_check", "method_params": "self,trainer,pl_module", "method_startline": "198", "method_endline": "232", "method_complexity": {"method_NLOC": "21", "method_CCN": "7", "method_NToken": "159", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\callbacks\\test_early_stopping.py", "file_new_name": "tests\\callbacks\\test_early_stopping.py", "file_complexity": {"file_NLOC": "137", "file_CCN": "15", "file_NToken": "928"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191", "deleted_lines": null, "method_info": {"method_name": "test_early_stopping_functionality_arbitrary_key", "method_params": "tmpdir", "method_startline": "173", "method_endline": "191", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "61", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "140,145,150", "deleted_lines": "140,145,150", "method_info": {"method_name": "test_early_stopping_no_val_step", "method_params": "tmpdir", "method_startline": "127", "method_endline": "150", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "94", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "177,178,179,180", "deleted_lines": null, "method_info": {"method_name": "test_early_stopping_functionality_arbitrary_key.validation_epoch_end", "method_params": "self,outputs", "method_startline": "177", "method_endline": "180", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "45", "method_nesting_level": "2"}}}}}}}}