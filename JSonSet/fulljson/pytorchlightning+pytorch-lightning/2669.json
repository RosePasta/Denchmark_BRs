{"BR": {"BR_id": "2669", "BR_author": "ananyahjha93", "BRopenT": "2020-07-22T09:29:05Z", "BRcloseT": "2020-07-24T08:26:06Z", "BR_text": {"BRsummary": "--gpus flag with add_argparse_args bug", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When using\n <denchmark-code>parser = Trainer.add_argparse_args(parser)\n args = parser.parse_args()\n \n trainer = Trainer.from_argparse_args(args)\n </denchmark-code>\n \n if the user does not provide the --gpus flag, the code allocates some memory on the GPU and reports\n GPU available: True, used: True\n but does not use this GPU. This issue has been discovered in bolts:\n \n PyTorchLightning/pytorch-lightning-bolts#35\n PyTorchLightning/pytorch-lightning-bolts#124\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n Use the following method to create the trainer object\n <denchmark-code>parser = Trainer.add_argparse_args(parser)\n args = parser.parse_args()\n \n trainer = Trainer.from_argparse_args(args)\n </denchmark-code>\n \n and don't pass the --gpus flag.\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n If --gpus flag is not provided in the script call, then Lightning should report\n GPU available: True, used: False\n with a warning, and not allocate any memory on the GPU. This way, the user can set the --gpus flag if they have missed out on it.\n \t"}, "comments": {}}, "commit": {"commit_id": "6780214b27e6ebace9cf38b6f5701224204e28ad", "commit_author": "Ananya Harsh Jha", "commitT": "2020-07-24 08:26:05+00:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_complexity": {"file_NLOC": "473", "file_CCN": "113", "file_NToken": "2328"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "313,314", "deleted_lines": null, "method_info": {"method_name": "set_distributed_mode", "method_params": "self,distributed_backend", "method_startline": "247", "method_endline": "314", "method_complexity": {"method_NLOC": "58", "method_CCN": "28", "method_NToken": "336", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "1143", "file_CCN": "137", "file_NToken": "4848"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "533,534", "deleted_lines": "466,467"}}}}}}