{"BR": {"BR_id": "2330", "BR_author": "stllfe", "BRopenT": "2020-06-23T14:44:48Z", "BRcloseT": "2020-06-25T13:21:43Z", "BR_text": {"BRsummary": "`use_amp` and multiple optimizers bug", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Faced this issue when tried to use mixed precision with my two-head model which has two pairs of optimizer & scheduler. Without use_amp everything works fine.\n With it enabled, I get:\n <denchmark-code>TypeError: 'CosineAnnealingLR' object is not subscriptable\n </denchmark-code>\n \n My investigation has ended up here:\n     def reinit_scheduler_properties(self, optimizers: list, schedulers: list):\n         # Reinitialize optimizer.step properties added by schedulers\n         for scheduler in schedulers:\n             for optimizer in optimizers:\n                 scheduler = scheduler['scheduler']  # <===== this place\n                 # ...\n                 if scheduler.optimizer == optimizer:\n                     # ...\n Obviously, next optimizer will get scheduler as an actual non-dict object as it was reassigned on the first iteration...\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Build any LightningModule, which configure_optimizers() method outputs lists of two optimizers and two schedulers. In my case it's something like:\n \n def configure_optimizers(self):\n     opt_1 = Adam(params=self.head_1.params(), ...)\n     opt_2 = Adam(params=self.head_2.params(), ...)\n     \n     sch_1 = CosineAnnealingLR(optimizer=opt_1, ...)\n     sch_2 = CosineAnnealingLR(optimizer=opt_2, ...)\n     return [opt_1, opt_2], [sch_1, sch_2]\n \n Build a Trainer object with  use_amp=True\n Call trainer.fit(model)\n See error\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>* CUDA:\n         - GPU:\n                 - GeForce GTX 1080 Ti\n         - available:         True\n         - version:           10.2\n * Packages:\n         - numpy:             1.18.5\n         - pyTorch_debug:     False\n         - pyTorch_version:   1.5.0\n         - pytorch-lightning: 0.8.1\n         - tensorboard:       1.15.0\n         - tqdm:              4.45.0\n * System:\n         - OS:                Linux\n         - architecture:\n                 - 64bit\n                 -\n         - processor:         x86_64\n         - python:            3.7.5\n         - version:           #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "stllfe", "commentT": "2020-06-23T14:45:28Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "stllfe", "commentT": "2020-06-24T10:09:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  this seems easy to fix (move the  part to the outer loop), and we could also fix issue <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2078>#2078</denchmark-link>\n  while fixing this function\n \t\t"}}}, "commit": {"commit_id": "c275e1fc91df4d351799b633e9df08e010094bfe", "commit_author": "William Falcon", "commitT": "2020-06-25 09:21:41-04:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\optimizers.py", "file_new_name": "pytorch_lightning\\trainer\\optimizers.py", "file_complexity": {"file_NLOC": "117", "file_CCN": "23", "file_NToken": "714"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "114,115,121,122,123,124,126,127,128,129,130,131,132", "deleted_lines": "115,120,122", "method_info": {"method_name": "reinit_scheduler_properties", "method_params": "self,list,list", "method_startline": "111", "method_endline": "132", "method_complexity": {"method_NLOC": "17", "method_CCN": "8", "method_NToken": "111", "method_nesting_level": "1"}}}}}}}}