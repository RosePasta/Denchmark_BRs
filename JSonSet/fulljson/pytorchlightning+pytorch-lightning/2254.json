{"BR": {"BR_id": "2254", "BR_author": "s-rog", "BRopenT": "2020-06-19T02:37:22Z", "BRcloseT": "2020-07-10T01:20:18Z", "BR_text": {"BRsummary": "Single node DDP: \"Default process group is not initialized\"", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Unable to start single node ddp training on 0.8.0\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n was going to run the gpu_template but... #2235\n both methods of running the template result in the same error\n <denchmark-code>$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp_spawn\n $ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp\n </denchmark-code>\n \n <denchmark-code>GPU available: True, used: True\n TPU available: False, using: 0 TPU cores\n CUDA_VISIBLE_DEVICES: [0,1,2,3]\n Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n     \"__main__\", mod_spec)\n   File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n     exec(code, run_globals)\n   File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\n     main(hyperparams)\n   File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\n     trainer.fit(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 860, in fit\n     self.barrier('fit_prepare_data')\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1261, in barrier\n     torch_distrib.barrier()\n   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 1484, in barrier\n     _check_default_pg()\n   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py\", line 187, in _check_default_pg\n     \"Default process group is not initialized\"\n AssertionError: Default process group is not initialized\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "s-rog", "commentT": "2020-06-19T02:47:50Z", "comment_text": "\n \t\tcan you post code to reproduce? just a minimal example that breaks\n BTW, the GPU template is fixed...\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "s-rog", "commentT": "2020-06-19T02:50:00Z", "comment_text": "\n \t\tdone, let me post my env as well\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "s-rog", "commentT": "2020-06-19T02:50:36Z", "comment_text": "\n \t\tok wait... i think i see it. one sec\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "s-rog", "commentT": "2020-06-19T04:50:07Z", "comment_text": "\n \t\tI just tested the merged changes with both ddp and ddp_spawn again got this:\n <denchmark-code>Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\n     main(hyperparams)\n   File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\n     trainer.fit(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 891, in fit\n Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n     \"__main__\", mod_spec)\n   File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n     self.ddp_train(task, model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\n     exec(code, run_globals)\n   File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 80, in <module>\n     main(hyperparams)\n   File \"/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py\", line 41, in main\n     trainer.fit(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 907, in fit\n     self.setup()\n TypeError: setup() missing 1 required positional argument: 'stage'\n     self.spawn_ddp_children(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 441, in spawn_ddp_children\n     self.ddp_train(local_rank, model, is_master=True)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 479, in ddp_train\n     self.setup()\n TypeError: setup() missing 1 required positional argument: 'stage'\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "s-rog", "commentT": "2020-06-19T05:14:30Z", "comment_text": "\n \t\ttry again. that was a typo\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "s-rog", "commentT": "2020-06-19T05:47:52Z", "comment_text": "\n \t\tcheers, works now!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "s-rog", "commentT": "2020-06-23T05:35:19Z", "comment_text": "\n \t\tStill having the Default process group is not initialized issue when using trainer.test\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "s-rog", "commentT": "2020-06-23T06:30:56Z", "comment_text": "\n \t\t\n Still having the Default process group is not initialized issue when using trainer.test\n \n I still have this bug as well. One temporary solution is creating a new single GPU trainer to do the test.\n Like\n <denchmark-code>trainer = Trainer(gpus=1, deterministic=True, logger=logger)\n trainer.model = model\n trainer.test()\n </denchmark-code>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "s-rog", "commentT": "2020-06-23T19:57:28Z", "comment_text": "\n \t\tRight, I know it works on single gpu. I have a large test set and ideally want faster inference using multiple gpus.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "s-rog", "commentT": "2020-07-02T15:11:23Z", "comment_text": "\n \t\tCan we re-open this issue? I am still having the Default process group is not initialized issue when I hit trainer.test() with ddp (with any number of gpus, even 1). I'm using the latest release from yesterday.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "s-rog", "commentT": "2020-07-02T15:33:13Z", "comment_text": "\n \t\t+1, doesn't look like the issue is resolved yet.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "s-rog", "commentT": "2020-07-04T05:32:04Z", "comment_text": "\n \t\thaving the same problem..... I also tried to downgrade pl to an older version, like 0.7.5, and try to using the older version to do the inference. But, the model trained and saved using the 0.8.x seems to not directly be compatible with older version.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "s-rog", "commentT": "2020-07-09T12:11:00Z", "comment_text": "\n \t\tversion: 0.8.4  train with ddp,  Got \"Default process group is not initialized\" when run trainer.test()\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "s-rog", "commentT": "2020-07-09T12:18:32Z", "comment_text": "\n \t\tcould you try master? this is fixed there\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "s-rog", "commentT": "2020-07-09T19:06:49Z", "comment_text": "\n \t\tJust tried it, it works fine now! Thank you!\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "s-rog", "commentT": "2020-08-17T19:13:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  Trying 0.8.5\n Trained with ddp, and testing with ddp, but got the following error message:\n <denchmark-code>AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.\n </denchmark-code>\n \n Any idea?\n Thanks!\n \t\t"}}}, "commit": {"commit_id": "57d5f6e74a3bcd8f5c73211ba3a4e2480fcc1114", "commit_author": "William Falcon", "commitT": "2020-06-19 00:42:20-04:00", "commit_complexity": {"commit_NLOC": "0.29411764705882354", "commit_CCN": "0.0", "commit_Nprams": "0.5882352941176471"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\trainer.rst", "file_new_name": "docs\\source\\trainer.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "12", "deleted_lines": "12"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\hooks.py", "file_new_name": "pytorch_lightning\\core\\hooks.py", "file_complexity": {"file_NLOC": "169", "file_CCN": "20", "file_NToken": "303"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33", "deleted_lines": "33", "method_info": {"method_name": "teardown", "method_params": "self,str", "method_startline": "28", "method_endline": "34", "method_complexity": {"method_NLOC": "1", "method_CCN": "1", "method_NToken": "10", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_complexity": {"file_NLOC": "434", "file_CCN": "98", "file_NToken": "2040"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "478,479,480,481,482", "deleted_lines": null, "method_info": {"method_name": "ddp_train", "method_params": "self,process_idx,model,is_master,proc_offset", "method_startline": "443", "method_endline": "530", "method_complexity": {"method_NLOC": "45", "method_CCN": "15", "method_NToken": "373", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "pytorch_lightning\\trainer\\distrib_parts.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_parts.py", "file_complexity": {"file_NLOC": "327", "file_CCN": "88", "file_NToken": "1953"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "179,180,181,182,183", "deleted_lines": null, "method_info": {"method_name": "tpu_train", "method_params": "self,tpu_core_idx,model", "method_startline": "178", "method_endline": "215", "method_complexity": {"method_NLOC": "20", "method_CCN": "7", "method_NToken": "179", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "263,264,265,266,267", "deleted_lines": null, "method_info": {"method_name": "horovod_train", "method_params": "self,model", "method_startline": "262", "method_endline": "322", "method_complexity": {"method_NLOC": "33", "method_CCN": "12", "method_NToken": "289", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "218,219,220,221", "deleted_lines": null, "method_info": {"method_name": "dp_train", "method_params": "self,model", "method_startline": "217", "method_endline": "260", "method_complexity": {"method_NLOC": "25", "method_CCN": "9", "method_NToken": "203", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "158,159,160,161,162", "deleted_lines": null, "method_info": {"method_name": "single_gpu_train", "method_params": "self,model", "method_startline": "157", "method_endline": "176", "method_complexity": {"method_NLOC": "11", "method_CCN": "4", "method_NToken": "108", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "file_complexity": {"file_NLOC": "1006", "file_CCN": "103", "file_NToken": "4304"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "894,899,906,929,930,931,948,949,950,951,952", "deleted_lines": "860,861,862,863,864,865,900,905,912"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "557", "file_CCN": "131", "file_NToken": "2728"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "262", "deleted_lines": "261", "method_info": {"method_name": "is_function_implemented", "method_params": "self,args", "method_startline": "261", "method_endline": "262", "method_complexity": {"method_NLOC": "1", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "705,706,707,708,709", "deleted_lines": null, "method_info": {"method_name": "run_training_teardown", "method_params": "self", "method_startline": "702", "method_endline": "723", "method_complexity": {"method_NLOC": "13", "method_CCN": "7", "method_NToken": "92", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "262", "deleted_lines": null, "method_info": {"method_name": "is_function_implemented", "method_params": "self,args,kwargs", "method_startline": "262", "method_endline": "263", "method_complexity": {"method_NLOC": "1", "method_CCN": "1", "method_NToken": "12", "method_nesting_level": "1"}}}}}}}}