{"BR": {"BR_id": "3393", "BR_author": "patrickorlando", "BRopenT": "2020-09-08T06:28:36Z", "BRcloseT": "2020-09-09T09:38:27Z", "BR_text": {"BRsummary": "MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\n I have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n \n Start an MLFlow server locally\n \n <denchmark-code>mlflow ui\n </denchmark-code>\n \n \n Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\n Uncomment out the tracking_uri to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>import torch\n from torch.utils.data import TensorDataset, DataLoader\n import pytorch_lightning as pl\n \n class MyModel(pl.LightningModule):\n     def __init__(self):\n         super().__init__()\n         self.num_examples = 5000\n         self.num_valid = 1000\n         self.batch_size = 64\n         self.lr = 1e-3\n         self.wd = 1e-2\n         self.num_features = 2\n         self.linear = torch.nn.Linear(self.num_features, 1)\n         self.loss_func = torch.nn.MSELoss()\n         self.X = torch.rand(self.num_examples, self.num_features)\n         self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\n         \n     def forward(self, x):\n         return self.linear(x)\n \n     def train_dataloader(self): \n         ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\n         dl = DataLoader(ds, batch_size=self.batch_size)\n         return dl\n \n     def val_dataloader(self): \n         ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\n         dl = DataLoader(ds, batch_size=self.batch_size)\n         return dl\n \n     def configure_optimizers(self):\n         return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n \n     def training_step(self, batch, batch_idx):\n         x, y = batch\n         yhat = self(x)\n         loss = self.loss_func(yhat, y)\n         result = pl.TrainResult(minimize=loss)\n         result.log('train_loss', loss, on_epoch=True, on_step=False)\n         return result\n \n     def validation_step(self, batch, batch_idx):\n         x, y = batch\n         yhat = self(x)\n         loss = self.loss_func(yhat, y)\n         result = pl.EvalResult(early_stop_on=loss)\n         result.log('val_loss', loss, on_epoch=True, on_step=False)\n         return result\n \n if __name__ == '__main__':\n     from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\n     mlf_logger = MLFlowLogger(\n         experiment_name=f\"MyModel\",\n         # tracking_uri=\"http://localhost:5000\"\n     )\n     trainer = pl.Trainer(\n         min_epochs=5,\n         max_epochs=50,\n         early_stop_callback=True,\n         logger=mlf_logger\n     )\n     model = MyModel()\n     trainer.fit(model)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n When using the TrainResult and EvalResult, or manually handling metric logging using the training_epoch_end and validation_epoch_end callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop.\n This would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>* CUDA:\n \t- GPU:\n \t- available:         False\n \t- version:           None\n * Packages:\n \t- numpy:             1.18.2\n \t- pyTorch_debug:     False\n \t- pyTorch_version:   1.6.0+cpu\n \t- pytorch-lightning: 0.9.0\n \t- tensorboard:       2.2.0\n \t- tqdm:              4.48.2\n * System:\n \t- OS:                Linux\n \t- architecture:\n \t\t- 64bit\n \t\t-\n \t- processor:         x86_64\n \t- python:            3.7.9\n \t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n We host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed.\n It appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\n <denchmark-h:h3>Solution</denchmark-h>\n \n I've done a bit of debugging in the codebase and have been able to isolate the cause in two places\n \n \n \n pytorch-lightning/pytorch_lightning/loggers/mlflow.py\n \n \n         Lines 125 to 129\n       in\n       d438ad8\n \n \n \n \n \n \n  @property \n \n \n \n  def run_id(self): \n \n \n \n  # create the experiment if it does not exist to get the run id \n \n \n \n  _ = self.experiment \n \n \n \n  return self._run_id \n \n \n \n \n \n Here self.experiment is called regardless of whether self._run_id exists. If we add an if not self._run_id here we avoid calling self._mlflow_client.get_experiment_by_name(self._experiment_name) on each step.\n However we still call it each time we log metrics to MFlow, because of the property self.experiment.\n \n \n \n pytorch-lightning/pytorch_lightning/loggers/mlflow.py\n \n \n         Lines 100 to 112\n       in\n       d438ad8\n \n \n \n \n \n \n  @property \n \n \n \n  @rank_zero_experiment \n \n \n \n  def experiment(self) -> MlflowClient: \n \n \n \n  r\"\"\" \n \n \n \n          Actual MLflow object. To use MLflow features in your \n \n \n \n          :class:`~pytorch_lightning.core.lightning.LightningModule` do the following. \n \n \n \n   \n \n \n \n          Example:: \n \n \n \n   \n \n \n \n              self.logger.experiment.some_mlflow_function() \n \n \n \n   \n \n \n \n          \"\"\" \n \n \n \n  expt = self._mlflow_client.get_experiment_by_name(self._experiment_name) \n \n \n \n \n \n Here if we store expt within the logger and only call self._mlflow_client.get_experiment_by_name when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\n I'd be happy to raise a PR for this fix.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "patrickorlando", "commentT": "2020-09-08T06:29:17Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "patrickorlando", "commentT": "2020-09-08T08:11:43Z", "comment_text": "\n \t\thave you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.\n I mean, your network is a single linear layer, you probably run through epochs super fast.\n I am not yet convinced it is a bug, but I'll try your example code\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "patrickorlando", "commentT": "2020-09-08T08:18:48Z", "comment_text": "\n \t\they <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n , Thanks for replying!\n The model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.\n I've done some debugging and added a solution section to the issue. It appears to be in in the experiment property of the MLFlowLogger. Each time .experiment is accessed, self._mlflow_client.get_experiment_by_name(self._experiment_name) is called, which communicates with the MLFlow server.\n It seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "patrickorlando", "commentT": "2020-09-08T08:21:54Z", "comment_text": "\n \t\toh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "patrickorlando", "commentT": "2020-09-08T08:24:26Z", "comment_text": "\n \t\tyeah sure, I'll link it here shortly.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "patrickorlando", "commentT": "2020-09-08T08:38:24Z", "comment_text": "\n \t\tDid you encounter this <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3392>#3392</denchmark-link>\n  problem as well?\n \t\t"}}}, "commit": {"commit_id": "656c1af0df0cd0a8102a69c9c5045e86dc2b6b3a", "commit_author": "Patrick Orlando", "commitT": "2020-09-09 11:38:26+02:00", "commit_complexity": {"commit_NLOC": "0.8888888888888888", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "46,47", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\mlflow.py", "file_new_name": "pytorch_lightning\\loggers\\mlflow.py", "file_complexity": {"file_NLOC": "143", "file_CCN": "19", "file_NToken": "597"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "112,113,114,115,116,117,118,119,120", "deleted_lines": "112,113,114,115,116,117,118,119,120", "method_info": {"method_name": "experiment", "method_params": "self", "method_startline": "102", "method_endline": "123", "method_complexity": {"method_NLOC": "21", "method_CCN": "4", "method_NToken": "106", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\loggers\\test_mlflow.py", "file_new_name": "tests\\loggers\\test_mlflow.py", "file_complexity": {"file_NLOC": "40", "file_CCN": "4", "file_NToken": "375"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "47,48,49,50,51,52,53,54", "deleted_lines": null, "method_info": {"method_name": "test_mlflow_experiment_id_retrieved_once", "method_params": "tmpdir", "method_startline": "47", "method_endline": "54", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "61", "method_nesting_level": "0"}}}}}}}}