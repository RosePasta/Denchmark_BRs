{"BR": {"BR_id": "44278", "BR_author": "jackd", "BRopenT": "2020-10-24T03:59:35Z", "BRcloseT": "2020-12-11T18:07:59Z", "BR_text": {"BRsummary": "Unexpected `snapshot` behaviour with `flat_map` in tf-nightly", "BRdescription": "\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\n TensorFlow installed from (source or binary): binary (pip)\n TensorFlow version (use command below): '2.4.0-dev20201023'\n Python version: 3.7.7\n \n Describe the current behavior\n A dataset formed by flat_mapping multiple snapshotted datasets which have each been iterated over individually (thus producing files on disk) results in a dataset which seemingly does not use those files on disk. This is different to the behaviour of cache in tf-2.3 and tf-nightly and snapshot in tf-2.3\n Describe the expected behavior\n snapshot to work equivalently in 2.3 as tf-nightly, and similarly to cache.\n \n Colab <denchmark-link:https://colab.research.google.com/drive/13mXDdNKNg04MEDvFTwmQq0tAw8L0MY02?usp=sharing>here</denchmark-link>\n .\n import os\n from tempfile import TemporaryDirectory\n \n import numpy as np\n import tensorflow as tf\n \n \n def as_numpy(ds: tf.data.Dataset):\n     return np.array([x.numpy() for x in ds])\n \n \n def get_data(\n     num_repeats=2,\n     snap=False,\n     preprocess_early=False,\n     preprocess_late=False,\n     del_rng=False,\n ):\n     \"\"\"\n     Get numpy results from a data pipeline.\n \n     The pipeline looks like:\n         1. range\n         2. add stateful random noise\n         3. create `num_repeats` `cache`d or `snapshot`ted versions\n         4. `flat_map` if num_repeats > 1\n \n     Args:\n         num_repeats: number of duplicates created in step 3 above.\n         snap: use `snapshot` (otherwise use `cache`)\n         preprocess_early: if True, we iterate over individually cached / snapshotted\n             datasets prior to flat-mapping.\n         preprocess_late: if True, we iterate over the `flat_map`ped dataset\n         del_rng: if True, we delete the rng responsible for generating random noise in\n             step 2. This will cause an error if this map function is called again,\n             rather than using cached / snapshotted files on disk.\n \n     Returns:\n         Two iterations of the repeated dataset.\n     \"\"\"\n     rng = tf.random.Generator.from_seed(0)\n     dataset = tf.data.Dataset.range(10).map(\n         lambda x: tf.cast(x, tf.float32) + rng.uniform(())\n     )\n     with TemporaryDirectory() as tmp_dir:\n         paths = [os.path.join(tmp_dir, f\"repeat-{i}\") for i in range(num_repeats)]\n         if snap:\n             datasets = [\n                 dataset.apply(tf.data.experimental.snapshot(path)) for path in paths\n             ]\n         else:\n             datasets = [dataset.cache(path) for path in paths]\n         if preprocess_early:\n             # iterate over datasets individually to force saving to file\n             for ds in datasets:\n                 as_numpy(ds)\n         if num_repeats == 1:\n             (dataset,) = datasets\n         else:\n             dataset = tf.data.Dataset.from_tensor_slices(datasets).flat_map(lambda x: x)\n         if preprocess_late:\n             # iterate over concatenated dataset to force saving to file\n             as_numpy(dataset)\n         if del_rng:\n             # this will cause an error is the original mapped dataset is called\n             del rng\n         return as_numpy(dataset), as_numpy(dataset)\n \n \n class SnapshotTest(tf.test.TestCase):\n     def test_consistent(self):\n         base0, base1 = get_data()\n         np.testing.assert_equal(base0, base1)\n \n     def test_reproducible(self):\n         base0, _ = get_data()\n         s0, s1 = get_data()\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n     def test_snapshot(self):\n         base0, _ = get_data()\n         s0, s1 = get_data(snap=True)\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n     def test_preprocess_late(self):\n         base0, _ = get_data()\n         s0, s1 = get_data(snap=True, preprocess_late=True)\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n     def test_preprocess_late_del_rng(self):\n         base0, _ = get_data()\n         s0, s1 = get_data(snap=True, preprocess_late=True, del_rng=True)\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n     def test_preprocess_early(self):\n         base0, _ = get_data()\n         s0, s1 = get_data(snap=True, preprocess_early=True)\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n     def test_preprocess_early_del_rng(self):\n         base0, _ = get_data()\n         s0, s1 = get_data(snap=True, preprocess_early=True, del_rng=True)\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n     def test_preprocess_no_repeats(self):\n         # preprocess_early is equivalent to preprocess_late here\n         base0, _ = get_data(num_repeats=1)\n         s0, s1 = get_data(snap=True, preprocess_early=True, num_repeats=1)\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n     def test_preprocess_del_rng_no_repeats(self):\n         # preprocess_early is equivalent to preprocess_late here\n         base0, _ = get_data(num_repeats=1)\n         s0, s1 = get_data(snap=True, preprocess_early=True, num_repeats=1, del_rng=True)\n         np.testing.assert_equal(s0, s1)\n         np.testing.assert_equal(s0, base0)\n \n \n if __name__ == \"__main__\":\n     tf.test.main()\n Other info / logs\n Failed test output:\n <denchmark-code>======================================================================\n ERROR: test_preprocess_early_del_rng (__main__.SnapshotTest)\n SnapshotTest.test_preprocess_early_del_rng\n ----------------------------------------------------------------------\n Traceback (most recent call last):\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\n     yield\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\n     output_shapes=self._flat_output_shapes)\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\n     _ops.raise_from_not_ok_status(e, name)\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\n     six.raise_from(core._status_to_exception(e.code, message), None)\n   File \"<string>\", line 3, in raise_from\n tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_AnonymousVar6/N10tensorflow3VarE does not exist.\n \t [[{{node stateful_uniform/StatefulUniform}}]] [Op:IteratorGetNext]\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"foob.py\", line 107, in test_preprocess_early_del_rng\n     s0, s1 = get_data(snap=True, preprocess_early=True, del_rng=True)\n   File \"foob.py\", line 67, in get_data\n     return as_numpy(dataset), as_numpy(dataset)\n   File \"foob.py\", line 9, in as_numpy\n     return np.array([x.numpy() for x in ds])\n   File \"foob.py\", line 9, in <listcomp>\n     return np.array([x.numpy() for x in ds])\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 747, in __next__\n     return self._next_internal()\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\n     return structure.from_compatible_tensor_list(self._element_spec, ret)\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/contextlib.py\", line 130, in __exit__\n     self.gen.throw(type, value, traceback)\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\n     executor_new.wait()\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\", line 69, in wait\n     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_AnonymousVar6/N10tensorflow3VarE does not exist.\n \t [[{{node stateful_uniform/StatefulUniform}}]]\n \n ======================================================================\n FAIL: test_preprocess_early (__main__.SnapshotTest)\n SnapshotTest.test_preprocess_early\n ----------------------------------------------------------------------\n Traceback (most recent call last):\n   File \"foob.py\", line 103, in test_preprocess_early\n     np.testing.assert_equal(s0, base0)\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py\", line 342, in assert_equal\n     return assert_array_equal(actual, desired, err_msg, verbose)\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py\", line 931, in assert_array_equal\n     verbose=verbose, header='Arrays are not equal')\n   File \"/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\n     raise AssertionError(msg)\n AssertionError: \n Arrays are not equal\n \n Mismatched elements: 20 / 20 (100%)\n Max absolute difference: 0.90819454\n Max relative difference: 1.9366292\n  x: array([0.91562 , 1.45509 , 2.253555, 3.829305, 4.681193, 5.65526 ,\n        6.401854, 7.514806, 8.184864, 9.174181, 0.130606, 1.063369,\n        2.513922, 3.190604, 4.433053, 5.044663, 6.653943, 7.007094,\n        8.878403, 9.046815], dtype=float32)\n  y: array([0.311793, 1.18098 , 2.761353, 3.138052, 4.027518, 5.460741,\n        6.235661, 7.175892, 8.786037, 9.549028, 0.860469, 1.631952,\n        2.669349, 3.255722, 4.884421, 5.066545, 6.267429, 7.34992 ,\n        8.16538 , 9.955009], dtype=float32)\n \n ----------------------------------------------------------------------\n Ran 10 tests in 0.849s\n \n FAILED (failures=1, errors=1, skipped=1)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jackd", "commentT": "2020-10-27T12:24:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>\n \n I am able to replicate the issue reported,please find the <denchmark-link:https://colab.research.google.com/gist/Saduf2019/919b0216f01d1b8d41444ee0f24b47f3/untitled453.ipynb>gist here</denchmark-link>\n .\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jackd", "commentT": "2020-10-30T20:14:21Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/jackd>@jackd</denchmark-link>\n , thank you for the thorough testing and reproduction details! I dug into the issue and found a couple behaviors that combined to cause this issue:\n \n Datasets passed through flat_map functions are not optimized before iterating over them.\n This commit changed SnapshotDataset so that when the dataset is optimized, the dataset hash changes. This happens because the compression type gets changed/resolved from AUTO to SNAPPY, and compression type is included in the snapshot hash.\n \n I'm preparing a fix that will address both issues by always computing snapshot hashes based on the structure of the dataset before optimization.\n To work around the issue in the short term, you could change the way you implement preprocess_early to use flat_map, like so:\n if preprocess_early:\n     # iterate over datasets individually to force saving to file\n     for ds in datasets:\n         as_numpy(tf.data.Dataset.from_tensors(ds).flat_map(lambda x: x))\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jackd", "commentT": "2020-12-11T18:07:58Z", "comment_text": "\n \t\tThis should be fixed now, right?\n Please reopen if that is not the case.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jackd", "commentT": "2020-12-11T18:08:01Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44278>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44278>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "cbc7f31b7d6aac81158be084201d3b3e8e346907", "commit_author": "Andrew Audibert", "commitT": "2020-11-03 11:59:47-08:00", "commit_complexity": {"commit_NLOC": "0.6", "commit_CCN": "1.0", "commit_Nprams": "0.7"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tensorflow\\core\\kernels\\data\\experimental\\snapshot_dataset_op.cc", "file_new_name": "tensorflow\\core\\kernels\\data\\experimental\\snapshot_dataset_op.cc", "file_complexity": {"file_NLOC": "1945", "file_CCN": "228", "file_NToken": "13615"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "377,378,379,380,381,382,408,409", "deleted_lines": null, "method_info": {"method_name": "tensorflow::data::experimental::SnapshotDatasetV2Op::Dataset::AsGraphDefInternal", "method_params": "ctx,b,output", "method_startline": "350", "method_endline": "415", "method_complexity": {"method_NLOC": "50", "method_CCN": "1", "method_NToken": "358", "method_nesting_level": "3"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "838,839,840,841", "deleted_lines": "843,844,845,846,847", "method_info": {"method_name": "tensorflow::data::experimental::SnapshotDatasetV2Op::SnapshotDatasetV2Op", "method_params": "ctx", "method_startline": "822", "method_endline": "847", "method_complexity": {"method_NLOC": "22", "method_CCN": "3", "method_NToken": "212", "method_nesting_level": "3"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "307", "deleted_lines": "307,308", "method_info": {"method_name": "tensorflow::data::experimental::SnapshotDatasetV2Op::Dataset::Dataset", "method_params": "ctx,input,hash,path,compression,reader_prefix,writer_prefix,reader_func,shard_func", "method_startline": "297", "method_endline": "313", "method_complexity": {"method_NLOC": "17", "method_CCN": "1", "method_NToken": "128", "method_nesting_level": "3"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,885,886", "deleted_lines": "849,850,851,852,853,854,855,856,868,869", "method_info": {"method_name": "tensorflow::data::experimental::SnapshotDatasetV2Op::MakeDataset", "method_params": "ctx,input,output", "method_startline": "849", "method_endline": "887", "method_complexity": {"method_NLOC": "34", "method_CCN": "3", "method_NToken": "245", "method_nesting_level": "3"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\core\\kernels\\data\\experimental\\snapshot_dataset_op.h", "file_new_name": "tensorflow\\core\\kernels\\data\\experimental\\snapshot_dataset_op.h", "file_complexity": {"file_NLOC": "61", "file_CCN": "0", "file_NToken": "298"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "49,50,80,81", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\core\\ops\\experimental_dataset_ops.cc", "file_new_name": "tensorflow\\core\\ops\\experimental_dataset_ops.cc", "file_complexity": {"file_NLOC": "1017", "file_CCN": "0", "file_NToken": "6618"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "971,972", "deleted_lines": null}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\data\\experimental\\kernel_tests\\snapshot_test.py", "file_new_name": "tensorflow\\python\\data\\experimental\\kernel_tests\\snapshot_test.py", "file_complexity": {"file_NLOC": "775", "file_CCN": "88", "file_NToken": "6142"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "360,361,362,363,364,365,366,367,368,369,370", "deleted_lines": null, "method_info": {"method_name": "testReadUsingFlatMap", "method_params": "self", "method_startline": "360", "method_endline": "370", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "100", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "373,374,375,376,377,378,379,380,381,382,383,384,385,386", "deleted_lines": null, "method_info": {"method_name": "testReadOptimizableUsingFlatMap", "method_params": "self", "method_startline": "373", "method_endline": "386", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "120", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\tools\\api\\golden\\v1\\tensorflow.raw_ops.pbtxt", "file_new_name": "tensorflow\\tools\\api\\golden\\v1\\tensorflow.raw_ops.pbtxt", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4125", "deleted_lines": "4125"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\tools\\api\\golden\\v2\\tensorflow.raw_ops.pbtxt", "file_new_name": "tensorflow\\tools\\api\\golden\\v2\\tensorflow.raw_ops.pbtxt", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4125", "deleted_lines": "4125"}}}}}}