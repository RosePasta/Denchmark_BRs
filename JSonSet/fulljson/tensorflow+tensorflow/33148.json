{"BR": {"BR_id": "33148", "BR_author": "mimxrt", "BRopenT": "2019-10-08T15:25:27Z", "BRcloseT": "2020-07-14T16:40:54Z", "BR_text": {"BRsummary": "Masking LSTM: OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM", "BRdescription": "\n <denchmark-h:h3>System information</denchmark-h>\n \n \n Have I written custom code: Yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS\n TensorFlow installed from (source or binary): Binary, pip\n TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\n Python version: Python 3.7.3\n CUDA/cuDNN version: CUDA=10.0, CUDNN=7.6.2.24-1\n GPU model and memory: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n \n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n It seems there is an issue with the CuDNN LSTM implementation when using a tf.keras.layers.Masking layer.\n <denchmark-code>batch_size = 256\n num_tsteps = 144\n num_features = 130\n num_units = 88\n model = tf.keras.Sequential([\n     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\n     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\n     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),\n     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\n     tf.keras.layers.Activation('sigmoid'),\n ])\n </denchmark-code>\n \n Similar to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33069>#33069</denchmark-link>\n  I receive this error during training and I have strictly right-padded data (I am doing trimming and right-padding manually). However, in contrast to this issue, I confirmed that I do not have any inputs containing only zeroes via the following snippet:\n <denchmark-code>for i, e in enumerate(ds_train):\n     res = []\n     f, l = [x.numpy() for x in e]\n     for j in range(f.shape[0]):\n         if not (f[j] == 0.0).all():\n             res.append(1)\n         else:\n             res.append(0)\n     fin = [res[0]]\n     for e in res[1:]:\n         if e != fin[-1]:\n             fin.append(e)\n     print(\"i {}: {}\".format(i, fin))\n \n # Result:\n i 0: [1, 0]\n i 1: [1, 0]\n i 2: [1, 0]\n i 3: [1, 0]\n i 4: [1]\n i 5: [1, 0]\n ...\n </denchmark-code>\n \n If I remove the Masking-layer, the error does not occur. I confirmed this by running a complete epoch (2324 batches), however, the training is probably pretty pointless when including the padded data.\n Is there any other pitfall that I am missing that could cause this issue?\n <denchmark-h:h3>Source code / logs</denchmark-h>\n \n Python output:\n <denchmark-code>Epoch 1/1000\n WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \n \n \n CancelledErrorTraceback (most recent call last)\n <ipython-input-7-1c503c2dd55c> in <module>\n ----> 1 m.fit(train=True)\n \n /ws/tf/vol_local/_model_lstm.py in fit(self, train, verbose)\n     315             ]\n     316             self.model.fit(ds_train, epochs=num_epochs, verbose=verbose, shuffle=False,\n --> 317                                 validation_data=ds_val, validation_steps=None, callbacks=cbs)\n     318             #self.model.save(sess_hdf5_path)\n     319             self.model.save_weights(self.sess_h5_path.as_posix())\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\n     726         max_queue_size=max_queue_size,\n     727         workers=workers,\n --> 728         use_multiprocessing=use_multiprocessing)\n     729 \n     730   def evaluate(self,\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\n     322                 mode=ModeKeys.TRAIN,\n     323                 training_context=training_context,\n --> 324                 total_epochs=epochs)\n     325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\n     326 \n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\n     121         step=step, mode=mode, size=current_batch_size) as batch_logs:\n     122       try:\n --> 123         batch_outs = execution_function(iterator)\n     124       except (StopIteration, errors.OutOfRangeError):\n     125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)\n      84     # `numpy` translates Tensors to values in Eager mode.\n      85     return nest.map_structure(_non_none_constant_value,\n ---> 86                               distributed_function(input_fn))\n      87 \n      88   return execution_function\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\n     455 \n     456     tracing_count = self._get_tracing_count()\n --> 457     result = self._call(*args, **kwds)\n     458     if tracing_count == self._get_tracing_count():\n     459       self._call_counter.called_without_tracing()\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\n     518         # Lifting succeeded, so variables are initialized and we can run the\n     519         # stateless function.\n --> 520         return self._stateless_fn(*args, **kwds)\n     521     else:\n     522       canon_args, canon_kwds = \\\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\n    1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\n    1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n -> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n    1824 \n    1825   @property\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\n    1139          if isinstance(t, (ops.Tensor,\n    1140                            resource_variable_ops.BaseResourceVariable))),\n -> 1141         self.captured_inputs)\n    1142 \n    1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\n    1222     if executing_eagerly:\n    1223       flat_outputs = forward_function.call(\n -> 1224           ctx, args, cancellation_manager=cancellation_manager)\n    1225     else:\n    1226       gradient_name = self._delayed_rewrite_functions.register()\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\n     509               inputs=args,\n     510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\n --> 511               ctx=ctx)\n     512         else:\n     513           outputs = execute.execute_with_cancellation(\n \n /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n      65     else:\n      66       message = e.message\n ---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\n      68   except TypeError as e:\n      69     keras_symbolic_tensors = [\n \n /ws/miniconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\n \n CancelledError:  [_Derived_]RecvAsync is cancelled.\n \t [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]\n \t [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]] [Op:__inference_distributed_function_172102]\n \n Function call stack:\n distributed_function\n </denchmark-code>\n \n Command line log:\n <denchmark-code>2019-10-08 14:38:27.367875: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_169668_171093' and '__inference___backward_cudnn_lstm_with_fallback_169668_171093_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_172102' both implement 'lstm_dce676f4-acdd-4bb5-88d9-e8dd57573aba' but their signatures do not match.\n 2019-10-08 14:38:27.536666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n 2019-10-08 14:38:39.982582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n 2019-10-08 14:38:41.215567: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM\n in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\n 2019-10-08 14:38:41.215616: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\n in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\n \t [[{{node cond_64/then/_0/CudnnRNNV3}}]]\n 2019-10-08 14:38:41.215638: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\n \t [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]\n \t [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]]\n 2019-10-08 14:38:41.215693: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\n \t [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mimxrt", "commentT": "2019-10-09T09:28:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>\n  ,\n Thanks for reporting the issue, can you please provide simple and standalone code to reproduce the issue?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mimxrt", "commentT": "2019-10-09T14:46:37Z", "comment_text": "\n \t\tI am trying to create a standalone example, however, I am facing some other errors that prevent me from finishing it. I created the following example which can produce the error using one .tfrecord file of my dataset. In the example, I generate a .tfrecord file from random data (for reproducibility) but TensorFlow fails to parse the file afterwards (actually parsing works but I still get the error):\n <denchmark-code>import numpy as np\n import tensorflow as tf\n \n gpus = tf.config.experimental.list_physical_devices('GPU')\n for gpu in gpus:\n     tf.config.experimental.set_memory_growth(gpu, True)\n assert tf.executing_eagerly()\n \n batch_size = 256\n num_tsteps = 144\n num_features = 130\n num_units = 88\n \n #n_files = 3320\n n_files = 10\n num_epochs = 1000\n \n seq_len_max_trunc = batch_size * num_tsteps\n flen = 3728\n \n ### Create TFRecord\n \n X = np.random.rand(flen + 1, num_features)\n n_label0 = int((flen + 1) * 0.2)\n Y = np.concatenate((\n     np.zeros((n_label0, 1)), # label 0\n     np.ones((flen - n_label0 + 1, 1)), # label 1\n ), axis=0)\n ds_out = tf.data.Dataset.from_tensor_slices((X, Y))\n ds_ser = ds_out.map(lambda *x: \n    tf.reshape(tf.py_function(lambda *v: \n        tf.train.Example(features=tf.train.Features(feature={\n            \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),\n            \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),\n        })).SerializeToString(), x, tf.string\n    ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE\n )\n for i, e in enumerate(ds_ser):\n     ex = tf.train.Example.FromString(e.numpy())\n     print(\"{}: {}\".format(i, ex).replace(\" \", \"\").replace(\"\\n\", \" \")[:100])\n writer = tf.data.experimental.TFRecordWriter(\"temp.tfrecord\")\n writer.write(ds_ser)\n \n ### Read TFRecord and train\n \n files = [\"temp.tfrecord\"] * n_files\n #files = [\"data/myfile.tfrecord\"] * n_files\n \n model = tf.keras.Sequential([\n     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\n     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\n     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),\n     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\n     tf.keras.layers.Activation('sigmoid'),\n ])\n model.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n \n \n def _prep_ds_file(file):\n     _ds = tf.data.TFRecordDataset(file)\n     for i, e in enumerate(_ds):\n         ex = tf.train.Example.FromString(e.numpy())\n         print(\"{}: {}\".format(i, ex).replace(\" \", \"\").replace(\"\\n\", \" \")[:100])\n     print(\"\\n\\n\\n\\n\\n\\n\\n\")\n     _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {\n         \"features\": tf.io.FixedLenFeature([132], tf.float32),\n         \"label\": tf.io.FixedLenFeature([1], tf.float32),\n     }), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n \n     _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[\"features\"][2:], v[\"label\"])))\n \n     _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)\n     _ds = _ds.take(_trunc)\n \n     _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps\n     if _c_pad >= 0:\n         assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc\n         _ds_pad = tf.data.Dataset.from_tensors((\n             tf.constant(0.0, shape=[num_features,]),\n             tf.constant(0.0, shape=[1,])))\n         _ds_pad = _ds_pad.repeat(_c_pad)\n         _ds = _ds.concatenate(_ds_pad) # pad to correct size\n \n     _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)\n     _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))\n \n     _ds = _ds.batch(batch_size, drop_remainder=True)\n     \n     return _ds\n \n \n ds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)\n fs_train = ds_fs.take(int(n_files * 0.7))\n fs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))\n \n ds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]\n for f in fs_train.skip(1):\n     ds_train = ds_train.concatenate(_prep_ds_file(f))\n ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n \n ds_val = [_prep_ds_file(f) for f in fs_val.take(1)][0]\n for f in fs_val.skip(1):\n     ds_val = ds_val.concatenate(_prep_ds_file(f))\n ds_val = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n \n cbs = [\n     tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n ]\n model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,\n           validation_data=ds_val, validation_steps=None, callbacks=cbs)\n </denchmark-code>\n \n For me this code produces the following error:\n <denchmark-code>InvalidArgumentError: 2 root error(s) found.\n   (0) Invalid argument:  {{function_node __inference_Dataset_map_<lambda>_32068}} Key: features.  Can't parse serialized Example.\n \t [[{{node ParseSingleExample/ParseSingleExample}}]]\n \t [[IteratorGetNext]]\n \t [[IteratorGetNext/_2]]\n   (1) Invalid argument:  {{function_node __inference_Dataset_map_<lambda>_32068}} Key: features.  Can't parse serialized Example.\n \t [[{{node ParseSingleExample/ParseSingleExample}}]]\n \t [[IteratorGetNext]]\n 0 successful operations.\n 0 derived errors ignored. [Op:__inference_distributed_function_64891]\n \n Function call stack:\n distributed_function -> distributed_function\n </denchmark-code>\n \n This is weird because it does iterate through the full file and prints the values on the screen (see the dataset iteration both when generating and when parsing the .tfrecord file in _prep_ds_file()). I can see that elements 0 to 3728 are printed. When switching to the file myfile.tfrecord (commented out) I can also see the elements until 3728 (same amount) and finally receive the error mentioned in the issue above.\n Do you have any idea what causes this error? If not I could provile the myfile.tfrecord file but of course that would not be ideal for reproducibility.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "mimxrt", "commentT": "2019-10-10T05:49:11Z", "comment_text": "\n \t\tIssue replicating with TF-2.0, kindly find the <denchmark-link:https://colab.sandbox.google.com/gist/oanush/11dcf7d43fb3a2ba2c1d97e22ff145d2/33148.ipynb>gist</denchmark-link>\n  of colab.Thanks!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "mimxrt", "commentT": "2019-10-10T09:00:05Z", "comment_text": "\n \t\tI found my mistake and updated the code accordingly. Please update the gist to represent the corrected and less verbose version below. The error happens in TF-2.0 and does not happen in TF-1.0 (1.14.0). Please also try to removing the Masking layer to confirm the issue only exists with masking.\n <denchmark-code>import numpy as np\n import tensorflow as tf\n \n # NOTE: Comment the block below for testing with TF-1.0\n gpus = tf.config.experimental.list_physical_devices('GPU')\n for gpu in gpus:\n     tf.config.experimental.set_memory_growth(gpu, True)\n \n # NOTE: Uncomment the block below for testing with TF-1.0\n # tf.compat.v1.enable_eager_execution()\n # config = tf.compat.v1.ConfigProto()\n # config.gpu_options.allow_growth = True\n # sess = tf.compat.v1.Session(config=config)\n # tf.compat.v1.keras.backend.set_session(sess)\n \n assert tf.executing_eagerly()\n \n batch_size = 256\n num_tsteps = 144\n num_features = 130\n num_units = 88\n \n #n_files = 3320\n n_files = 10\n num_epochs = 1000\n \n seq_len_max_trunc = batch_size * num_tsteps\n flen = 3728\n \n ### Create TFRecord\n \n X = np.random.rand(flen + 1, num_features + 2)\n n_label0 = int((flen + 1) * 0.2)\n Y = np.concatenate((\n     np.zeros((n_label0, 1)), # label 0\n     np.ones((flen - n_label0 + 1, 1)), # label 1\n ), axis=0)\n ds_out = tf.data.Dataset.from_tensor_slices((X, Y))\n ds_ser = ds_out.map(lambda *x: \n    tf.reshape(tf.py_function(lambda *v: \n        tf.train.Example(features=tf.train.Features(feature={\n            \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),\n            \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),\n        })).SerializeToString(), x, tf.string\n    ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE\n )\n writer = tf.data.experimental.TFRecordWriter(\"temp.tfrecord\")\n writer.write(ds_ser)\n \n ### Read TFRecord and train\n \n files = [\"temp.tfrecord\"] * n_files\n \n model = tf.keras.Sequential([\n     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\n     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\n     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),\n     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\n     tf.keras.layers.Activation('sigmoid'),\n ])\n model.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n \n \n def _prep_ds_file(file):\n     _ds = tf.data.TFRecordDataset(file)\n     _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {\n         \"features\": tf.io.FixedLenFeature([132], tf.float32),\n         \"label\": tf.io.FixedLenFeature([1], tf.float32),\n     }), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n         \n     _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[\"features\"][2:], v[\"label\"])))\n \n     _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)\n     _ds = _ds.take(_trunc)\n \n     _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps\n     if _c_pad >= 0:\n         assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc\n         _ds_pad = tf.data.Dataset.from_tensors((\n             tf.constant(0.0, shape=[num_features,]),\n             tf.constant(0.0, shape=[1,])))\n         _ds_pad = _ds_pad.repeat(_c_pad)\n         _ds = _ds.concatenate(_ds_pad) # pad to correct size\n \n     _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)\n     _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))\n \n     _ds = _ds.batch(batch_size, drop_remainder=True)\n     \n     return _ds\n \n \n ds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)\n fs_train = ds_fs.take(int(n_files * 0.7))\n fs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))\n \n ds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]\n for f in fs_train.skip(1):\n     ds_train = ds_train.concatenate(_prep_ds_file(f))\n ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n \n cbs = [\n     tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n ]\n model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,\n           validation_data=None, validation_steps=None, callbacks=cbs)\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "mimxrt", "commentT": "2019-10-10T16:57:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>\n  I could reproduce the issue with . However, if I use only cpu then there is no error (with and without masking) as shown in the original post. I think root-cause may be related to . <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/3a768611de0a28330d047f12e120f931/untitled553.ipynb>Here</denchmark-link>\n  is the gist for your reference. Thanks!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "mimxrt", "commentT": "2019-10-11T07:17:23Z", "comment_text": "\n \t\tThank you for your reply. I'm not sure what you are saying though: did you try removing tf.config.experimental.set_memory_growth(gpu, True) at all? I tried and for me the same error happens. For me, the only difference is that TF occupies all available GPU memory.\n Could you please try again if the error also happens without memory growth? If so, I guess the issue could be in the GPU LSTM implementation (cuDNN)?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "mimxrt", "commentT": "2019-10-11T20:14:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>\n  I mentioned two things.\n \n I could reproduce the issue with tf-nightly-gpu\n Just tried to point some root-cause. I ran your code as it is in a cpu and I don't see any error. so I guess root-cause may be related to tf.config.experimental.set_memory_growth(gpu, True). I may be wrong.\n \n Thanks!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "mimxrt", "commentT": "2019-10-14T07:12:44Z", "comment_text": "\n \t\tI see, thank you for the clarification. So in summary we can say that this issue does not happen on the CPU and that it is related to the GPU implementation. As I tried running it without tf.config.experimental.set_memory_growth(gpu, True) and the error still occurs, it should probably be save to rule that out as the cause/issue.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "mimxrt", "commentT": "2019-11-12T01:34:44Z", "comment_text": "\n \t\tsame problem here. did you find any solution?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "mimxrt", "commentT": "2019-11-12T07:33:03Z", "comment_text": "\n \t\tNo sorry, still waiting for any help myself.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "mimxrt", "commentT": "2019-11-12T08:12:30Z", "comment_text": "\n \t\tI changed my model from:\n         self.model = Sequential([\n             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1],mask_zeros=True),\n             LSTM(self.lstm_out),\n             Dense(len(self.item_map)-1),\n         ])\n to:\n         self.model = Sequential([\n             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1]),\n             Masking(mask_value=0),\n             LSTM(self.lstm_out),\n             Dense(len(self.item_map)-1),\n         ])\n And solved my isssue\n I know <denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>\n 's code has the same model and I dont know why it works for me,\n but im adding this for anyone else comes here with the issue and maybe it can help with debugging\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "mimxrt", "commentT": "2019-11-12T09:35:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ynsgnr>@ynsgnr</denchmark-link>\n  if you are running on CPU then it works properly, the problem is when you run it on GPU. By the way, you do not use the Timedistirbuted layer in your code since this problem shows up when you use Timedistributed with LSTM.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "mimxrt", "commentT": "2019-11-12T10:16:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>\n , I tried your suggestion but it still produces the same error. I am new to GitHub , how to tell this problem to someone from TensorFlow? do they see our conversation ?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "mimxrt", "commentT": "2019-11-12T10:20:14Z", "comment_text": "\n \t\tIn my experience and example the issue stems from the Masking + LSTM combination, not from the  layer. Please try the example in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342>#33148 (comment)</denchmark-link>\n  without it, i.e.\n <denchmark-code>model = tf.keras.Sequential([\n     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),\n     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),\n     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=False, stateful=False),\n     tf.keras.layers.Dense(1),\n     tf.keras.layers.Activation('sigmoid'),\n ])\n </denchmark-code>\n \n (change return_sequences to False and remove TimeDistributed).\n You can also try to remove the Masking layer in the example above and the error will go away.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "mimxrt", "commentT": "2019-11-12T11:40:52Z", "comment_text": "\n \t\tbut I have this model:\n        model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=True,mask_zero=True) model.add(embeding_layer) model.add(layers.LSTM(50)) model.add(layers.Dropout(0.5)) model.add(layers.Dense(3,activation='softmax')) opt=tf.keras.optimizers.RMSprop(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy']) self.model=model\n which works fine and with no error. As you can see I used masking and LSTM at the same time. so in my experience the problem stem from TimeDistributed and masking.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "mimxrt", "commentT": "2019-11-12T11:51:23Z", "comment_text": "\n \t\tInteresting. Can you provide a complete example like mine so I can try your code as well?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "mimxrt", "commentT": "2019-11-12T12:01:51Z", "comment_text": "\n \t\tthe above example is a simple classifier, you can make a random dataset and feed the model with it, so you can see that it will work with no problem.\n this is where the model does not work and produce an error while training :\n        model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=True) model.add(TimeDistributed(embeding_layer)) model.add(TimeDistributed(tf.keras.layers.LSTM(50))) model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50))) # model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(100))) model.add(layers.Dense(3,activation='softmax')) opt=tf.keras.optimizers.Adam(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n in this example, if I set mask_zero=true at the embedding layer then, it crushes at the begging or sometimes at the end of the epoc when evaluating the validation loss. and this is the error message :\n  C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\Scripts\\python.exe C:/Users/jalil/PycharmProjects/untitled1/main_file.py\n 2019-11-14 14:11:36.983144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\n 2019-11-14 14:11:45.638679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\n 2019-11-14 14:11:46.216495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\n name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\n pciBusID: 0000:01:00.0\n 2019-11-14 14:11:46.216676: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\n 2019-11-14 14:11:46.217282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n 2019-11-14 14:11:50.885396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n 2019-11-14 14:11:51.214275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\n name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\n pciBusID: 0000:01:00.0\n 2019-11-14 14:11:51.214484: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\n 2019-11-14 14:11:51.218182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n 2019-11-14 14:11:51.905201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n 2019-11-14 14:11:51.905307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] 0\n 2019-11-14 14:11:51.905366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N\n 2019-11-14 14:11:51.906228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)\n WARNING:tensorflow:From C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n Instructions for updating:\n Use tf.where in 2.0, which has the same broadcast rule as np.where\n Train on 35000 samples, validate on 6447 samples\n Epoch 1/1000\n 2019-11-14 14:12:33.178251: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_671418_672877' and '__inference___backward_cudnn_lstm_with_fallback_671418_672877_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_675292' both implement 'lstm_81cdaa4a-fa6f-4675-abbb-02fb4cd0189b' but their signatures do not match.\n 2019-11-14 14:12:33.544669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\n 2019-11-14 14:12:34.397677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\n 32/35000 [..............................] - ETA: 2:03:422019-11-14 14:12:35.151804: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM\n in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\n 2019-11-14 14:12:35.152137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM\n in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\n [[{{node cond_64/then/_0/CudnnRNNV3}}]]\n 2019-11-14 14:12:35.152541: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} CUDNN_STATUS_BAD_PARAM\n in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'\n [[{{node cond_64/then/_0/CudnnRNNV3}}]]\n [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]]\n 32/35000 [..............................] - ETA: 2:25:41Traceback (most recent call last):\n File \"C:/Users/jalil/PycharmProjects/untitled1/main_file.py\", line 102, in\n main_model_instance.train_model(train_batch_data,train_batch_labels,test_batch_data,test_batch_labels)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\main_model.py\", line 103, in train_model\n history = self.model.fit(x=np.array(train_batch_data),y=np.array(train_batch_labels),validation_data=(np.array(test_batch_data),np.array(test_batch_labels)),epochs=1000,callbacks=[tensorboard_callback])\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 734, in fit\n use_multiprocessing=use_multiprocessing)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\n total_epochs=epochs)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\n batch_outs = execution_function(iterator)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\n distributed_function(input_fn))\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 439, in call\n return self._stateless_fn(*args, *kwds)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1822, in call\n return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\n self.captured_inputs)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\n ctx, args, cancellation_manager=cancellation_manager)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\n ctx=ctx)\n File \"C:\\Users\\jalil\\PycharmProjects\\untitled1\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\n six.raise_from(core._status_to_exception(e.code, message), None)\n File \"\", line 3, in raise_from\n tensorflow.python.framework.errors_impl.UnknownError: [Derived] CUDNN_STATUS_BAD_PARAM\n in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void)&padding_fill)'\n [[{{node cond_64/then/_0/CudnnRNNV3}}]]\n [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_675292]\n Function call stack:\n distributed_function -> distributed_function -> distributed_function\n `\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "mimxrt", "commentT": "2020-05-02T18:58:32Z", "comment_text": "\n \t\timport tensorflow as tf\n tf.compat.v1.disable_eager_execution()\n Dissable eager execution and everything is running fine without the fused rnn kernel. Thx for the help guys :)\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "mimxrt", "commentT": "2020-06-01T12:39:43Z", "comment_text": "\n \t\tI have a work around that seems to work:  force TF to use the non CuDNN implementation by selecting a sigmoid activation instead of TANH\n layers.LSTM(...,activation='sigmoid')\n Outputs\n WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n This forces TF to use a generic GPU kernel in place of CuDNN.  It's slower but a slower implementation is a lot faster than not working at all ;p\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "mimxrt", "commentT": "2020-06-09T07:27:49Z", "comment_text": "\n \t\tIf you really want that as a workaround, you can find the requirements for the cuDNN implementation <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM?hl=en#used-in-the-notebooks_1>here</denchmark-link>\n .\n \n The requirements to use the cuDNN implementation are:\n \n activation == tanh\n recurrent_activation == sigmoid\n recurrent_dropout == 0\n unroll is False\n use_bias is True\n Inputs are not masked or strictly right padded.\n \n \n I guess changing any one of these will result in the CPU non-cuDNN implementation being used.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "mimxrt", "commentT": "2020-06-12T19:32:36Z", "comment_text": "\n \t\tThat's right mimxrt.  I guess the interesting point here is that the inputs to my LSTM are always masked, but I have to force the non-cudnn implementation using the activation function.  Might be a clue for someone who can fix this.\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "mimxrt", "commentT": "2020-06-26T15:47:49Z", "comment_text": "\n \t\tI'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "mimxrt", "commentT": "2020-07-03T10:55:45Z", "comment_text": "\n \t\tFacing this same issue and reported here: <denchmark-link:https://github.com/tensorflow/tensorflow/issues/40982>#40982</denchmark-link>\n .\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "mimxrt", "commentT": "2020-07-06T16:47:51Z", "comment_text": "\n \t\t\n I'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.\n \n There is a private field you can set to disable the cudnn kernel once the layer is created.\n <denchmark-code>layer = tf.keras.layers.LSTM(4)\n layer. _could_use_gpu_kernel = False\n </denchmark-code>\n \n This will focus the lstm layer to use the non-cudnn implementation, even landed on GPU.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "mimxrt", "commentT": "2020-07-06T16:50:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/queirozfcom>@queirozfcom</denchmark-link>\n  what would you suggest to use between this option and disabling eager execution as when eager execution is disabled it does work?\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "mimxrt", "commentT": "2020-07-06T16:57:32Z", "comment_text": "\n \t\tDisable eager will have larger side effects since it change the runtime to execute in graph/session context. We don't recommend user to fallback to graph unless they really need some feature in graph/session.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "mimxrt", "commentT": "2020-07-06T17:13:03Z", "comment_text": "\n \t\tThanks :)\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "mimxrt", "commentT": "2020-07-10T18:40:02Z", "comment_text": "\n \t\tI am still facing this issue using TF 2.2.0. I also found the same workaround of forcing the LSTM to not use the cuDNN implementation to work, however it is nearly prohibitively slow. I found the generic GPU implementation took ~30 times longer to train per epoch than the cuDNN version. I hope this can be fixed soon.\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "mimxrt", "commentT": "2020-07-14T16:40:54Z", "comment_text": "\n \t\tI have disable the cudnn code path if there is fully masked data in the batch. It will have some performance dip since it fallback to generic kernel, but won't error out.\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "mimxrt", "commentT": "2020-07-14T16:40:56Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148>No</denchmark-link>\n \n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "mimxrt", "commentT": "2020-07-22T00:57:42Z", "comment_text": "\n \t\tfacing this issue using TF 2.2.0 while evaluating\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "mimxrt", "commentT": "2020-07-22T02:36:50Z", "comment_text": "\n \t\tPlease try with the latest nightly version.\n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "mimxrt", "commentT": "2020-08-07T06:42:40Z", "comment_text": "\n \t\t\n @qlzh727 Not yet. We might need to wait for the next major release for the fix.\n \n cudnn 8 is here, is there any info about this issue?\n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "mimxrt", "commentT": "2020-09-25T09:44:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/geetachavan1>@geetachavan1</denchmark-link>\n  I am using `tensorflow==2.3.0 but for this network:\n def call(self, inputs, training=None, memory_states=None, **kwargs):\n \n     x = self.embedding(inputs, training=training)\n \n     if memory_states is None:\n         memory_states = self.get_initial_state(shape_list(x)[0])\n \n     next_memory_states = []\n \n     for i, (lstm, norm) in enumerate(self.lstm_stack):\n         x = norm(x, training=training)\n         outputs = lstm(x, training=training, initial_state=memory_states[i])\n         x, state = outputs[0], outputs[1:]\n         next_memory_states.append(state)\n \n     return x, next_memory_states\n where we have:\n self.embedding = layers.Embedding(vocab_size, embedding_size, mask_zero=True)\n self.lstm_stack = list()\n for _ in range(num_layers):\n     lstm = layers.LSTM(\n         units=lstm_units,\n         return_sequences=True,\n         return_state=True,\n         dropout=dropout\n     )\n     norm = layers.LayerNormalization()\n     self.lstm_stack.append((lstm, norm))\n I am getting\n <denchmark-code>Traceback (most recent call last):\n   File \"/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py\", line 153, in <module>\n     main()\n   File \"/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py\", line 85, in main\n     y_greedy = model.greedy_decode(encoder_inputs)[0]\n   File \"/home/sfalk/tmp/speech-v2/asr/model/transducer.py\", line 351, in greedy_decode\n     init_predict_network_state = PredictNetworkState(*self.predict_network(init_yseq))\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\n     outputs = call_fn(inputs, *args, **kwargs)\n   File \"/home/sfalk/tmp/speech-v2/asr/model/transducer.py\", line 118, in call\n     outputs = lstm(x, training=training, initial_state=memory_states[i])\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 720, in __call__\n     return super(RNN, self).__call__(inputs, **kwargs)\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\n     outputs = call_fn(inputs, *args, **kwargs)\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\", line 1176, in call\n     last_output, outputs, new_h, new_c, runtime = gpu_lstm(\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\", line 1401, in gpu_lstm\n     outputs, h, c, _, _ = gen_cudnn_rnn_ops.cudnn_rnnv3(\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 1914, in cudnn_rnnv3\n     return cudnn_rnnv3_eager_fallback(\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 2011, in cudnn_rnnv3_eager_fallback\n     _result = _execute.execute(b\"CudnnRNNV3\", 5, inputs=_inputs_flat,\n   File \"/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM\n in tensorflow/stream_executor/cuda/cuda_dnn.cc(1521): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)' [Op:CudnnRNNV3]\n </denchmark-code>\n \n Shouldn't this be fixed?\n \t\t"}, "comments_34": {"comment_id": 35, "comment_author": "mimxrt", "commentT": "2020-10-15T20:28:54Z", "comment_text": "\n \t\tSome update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342>#33148 (comment)</denchmark-link>\n .\n \t\t"}, "comments_35": {"comment_id": 36, "comment_author": "mimxrt", "commentT": "2020-11-28T18:48:49Z", "comment_text": "\n \t\tWhen will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.\n \t\t"}, "comments_36": {"comment_id": 37, "comment_author": "mimxrt", "commentT": "2020-11-30T06:33:20Z", "comment_text": "\n \t\t\n When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.\n \n cudnn 8.0.5 is already out <denchmark-link:https://docs.nvidia.com/deeplearning/cudnn/archives/index.html>https://docs.nvidia.com/deeplearning/cudnn/archives/index.html</denchmark-link>\n \n \t\t"}, "comments_37": {"comment_id": 38, "comment_author": "mimxrt", "commentT": "2020-12-07T11:02:11Z", "comment_text": "\n \t\t\n Some update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by #33148 (comment).\n \n <denchmark-link:https://github.com/kaixih>@kaixih</denchmark-link>\n  where did you read it?\n I'd like to give it a try.\n \t\t"}}}, "commit": {"commit_id": "4d582a660b4e84fb283eba598127ae40fdd8d1ed", "commit_author": "Scott Zhu", "commitT": "2020-07-13 21:27:11-07:00", "commit_complexity": {"commit_NLOC": "0.16363636363636364", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\keras\\layers\\gru_v2_test.py", "file_new_name": "tensorflow\\python\\keras\\layers\\gru_v2_test.py", "file_complexity": {"file_NLOC": "636", "file_CCN": "46", "file_NToken": "5284"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641", "deleted_lines": null, "method_info": {"method_name": "test_with_fully_masked_inputs", "method_params": "self", "method_startline": "615", "method_endline": "641", "method_complexity": {"method_NLOC": "23", "method_CCN": "1", "method_NToken": "137", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\keras\\layers\\lstm_v2_test.py", "file_new_name": "tensorflow\\python\\keras\\layers\\lstm_v2_test.py", "file_complexity": {"file_NLOC": "898", "file_CCN": "72", "file_NToken": "7214"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842", "deleted_lines": null, "method_info": {"method_name": "test_with_fully_masked_inputs", "method_params": "self", "method_startline": "816", "method_endline": "842", "method_complexity": {"method_NLOC": "23", "method_CCN": "1", "method_NToken": "137", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 11, "file_old_name": "tensorflow\\python\\keras\\layers\\recurrent_v2.py", "file_new_name": "tensorflow\\python\\keras\\layers\\recurrent_v2.py", "file_complexity": {"file_NLOC": "1174", "file_CCN": "69", "file_NToken": "4974"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1172", "deleted_lines": "1172", "method_info": {"method_name": "call", "method_params": "self,inputs,mask,training,initial_state", "method_startline": "1100", "method_endline": "1204", "method_complexity": {"method_NLOC": "79", "method_CCN": "18", "method_NToken": "519", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "738", "deleted_lines": "738", "method_info": {"method_name": "cudnn_gru_fn", "method_params": "", "method_startline": "738", "method_endline": "748", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "1564,1581,1593,1594", "deleted_lines": "1564,1581,1582,1583,1588,1589", "method_info": {"method_name": "is_sequence_right_padded", "method_params": "mask,time_major", "method_startline": "1564", "method_endline": "1594", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "77", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603", "deleted_lines": null, "method_info": {"method_name": "has_fully_masked_sequence", "method_params": "mask", "method_startline": "1593", "method_endline": "1603", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "26", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "750", "deleted_lines": "750", "method_info": {"method_name": "input_not_right_padded", "method_params": "", "method_startline": "750", "method_endline": "761", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "47", "method_nesting_level": "2"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "738", "deleted_lines": "738", "method_info": {"method_name": "input_right_padded", "method_params": "", "method_startline": "738", "method_endline": "748", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "2"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "1509", "deleted_lines": "1509", "method_info": {"method_name": "cudnn_lstm_fn", "method_params": "", "method_startline": "1509", "method_endline": "1520", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "47", "method_nesting_level": "2"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "1564,1581", "deleted_lines": "1564,1581,1582,1583,1588,1589", "method_info": {"method_name": "is_sequence_right_padded", "method_params": "mask", "method_startline": "1564", "method_endline": "1590", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "64", "method_nesting_level": "0"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "1606,1607,1608,1609,1610,1611,1612", "deleted_lines": null, "method_info": {"method_name": "is_cudnn_supported_inputs", "method_params": "mask,time_major", "method_startline": "1606", "method_endline": "1612", "method_complexity": {"method_NLOC": "6", "method_CCN": "2", "method_NToken": "38", "method_nesting_level": "0"}}}, "hunk_9": {"Ismethod": 1, "added_lines": "750", "deleted_lines": "750", "method_info": {"method_name": "standard_gru_fn", "method_params": "", "method_startline": "750", "method_endline": "761", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "47", "method_nesting_level": "2"}}}, "hunk_10": {"Ismethod": 1, "added_lines": "1522", "deleted_lines": "1522", "method_info": {"method_name": "stardard_lstm_fn", "method_params": "", "method_startline": "1522", "method_endline": "1534", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "51", "method_nesting_level": "2"}}}}}}}}