{"BR": {"BR_id": "33340", "BR_author": "off99555", "BRopenT": "2019-10-14T15:13:52Z", "BRcloseT": "2020-01-15T05:51:39Z", "BR_text": {"BRsummary": "Significant prediction slowdown after model.compile()", "BRdescription": "\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n TensorFlow installed from (source or binary): pip install tensorflow\n TensorFlow version: 2.0.0\n Python version: 3.7\n CUDA/cuDNN version: CUDA=10.0, cuDNN=7.6.4\n GPU model and memory: GTX 1060 6GB\n \n Describe the current behavior\n The prediction speed is slowed down a lot after model.compile() call.\n Describe the expected behavior\n Speed should not be affected. Predict function is used by users assuming that it will work fast because we use it all the time in production. It should not cause surprise to users.\n \n <denchmark-link:https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true>https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true</denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/15215732/66762282-e3dc0900-eecf-11e9-8d93-82c8bcc5325b.png></denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "off99555", "commentT": "2019-10-14T16:30:26Z", "comment_text": "\n \t\t<denchmark-link:https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58378941#58378941>Relevant SO</denchmark-link>\n , and another minimal reproducible example:\n from tensorflow.keras.layers import Input, Dense\n from tensorflow.keras.models import Model\n import numpy as np\n from time import time\n \n def timeit(func, arg, iterations):\n     t0 = time()\n     for _ in range(iterations):\n         func(arg)\n     print(\"%.4f sec\" % (time() - t0))\n \n ipt   = Input(shape=(4,))\n x     = Dense(2, activation='relu')(ipt)\n out   = Dense(1, activation='sigmoid')(x)\n model = Model(ipt, out)\n \n X = np.random.randn(32,4)\n \n timeit(model.predict, X, 1000)\n model.compile('adam', loss='binary_crossentropy')\n timeit(model.predict, X, 1000)\n model._make_train_function()  # build optimizer\n timeit(model.predict, X, 1000)\n Outputs:\n 0.9891 sec\n 29.785 sec\n 29.521 sec\n That's a 30-fold slowdown. Worse yet, building the optimizer does not elicit any further slowdowns - so \"graph size\" may not be the main explanation here.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "off99555", "commentT": "2019-10-15T00:01:01Z", "comment_text": "\n \t\t<denchmark-link:https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58385156#58385156>Solved</denchmark-link>\n .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "off99555", "commentT": "2019-10-15T07:49:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>\n  ,\n Can you confirm if the issue is resolved?Thanks!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "off99555", "commentT": "2019-10-15T11:57:39Z", "comment_text": "\n \t\tIt does not seem to me that it is resolved. It's more like we know how the issue occurs but we don't have a solution, just workaround. I need to compare timing between compiled and non-compiled version and see which is faster. But I don't think a user will be aware of this in general. So we should make a better solution.\n In this case, should I close the issue or keep it open?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "off99555", "commentT": "2019-10-15T13:46:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>\n  I'd agree to request a documentation improvement from TensorFlow to notify users of this, but I doubt any code-level changes will be implemented to address this as it'd require revamping a massive portion of TF graph. It's up to the user to be aware of functionality differences and adjust accordingly - but admittedly, while this isn't the only issue where a workaround is required, other cases are at least documented.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "off99555", "commentT": "2019-10-18T21:10:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>\n  I cannot reproduce the issue. When I ran it in colab, computation time is little more but not as much as you reported. I have checked some other models also. Please check the <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/d91713318125c34768c5f8841e60b928/untitled582.ipynb>gist here</denchmark-link>\n .\n <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>\n  I could reproduce your issue. The reason is that when the model is very small, it takes more time to predict after compilation. I have checked other similar small models and I noticed it. Thanks!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "off99555", "commentT": "2019-10-18T21:25:32Z", "comment_text": "\n \t\t\n The reason is that when the model is very small, it takes more time to predict after compilation\n \n <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>\n  That is the problem, yes, but not its explanation - I've done the latter <denchmark-link:https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58385156#58385156>here</denchmark-link>\n . Further, it's not the model size, but model size  data size.\n As noted by <denchmark-link:https://github.com/off99555>@off99555</denchmark-link>\n  , the solution only works for those aware of the problem and its workaround - this issue, however, isn't documented or mentioned in a docstring. To remedy, some form of mention should be made in documentation or  docstring as a disclaimer.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "off99555", "commentT": "2019-12-28T19:11:44Z", "comment_text": "\n \t\tAs OverLordGoldDragon mentioned, you can disable the experimental_run_tf_function flag for a Keras model and force it to the old v1 execution path (I think). It's a bit nicer than disabling eager execution globally.\n The good news is that you can do this by simply passing the param to the compile() function, so you don't need to make any private calls. Like so: model.compile(loss=loss, optimizer=optimizer, experimental_run_tf_function=False)\n This restores the performance for me (I experienced more than 10x slowdown). Let me second everybody in that I'm surprised this is not a big issue - the parameter is on the public interface of a major release, but it's not even mentioned in the docstring.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "off99555", "commentT": "2019-12-28T19:51:41Z", "comment_text": "\n \t\tNoone assigned; if it doesn't cause a compile error, doesn't mean it isn't a worthwhile problem. Has anyone made a docstring PR on this yet? If not, I could\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "off99555", "commentT": "2019-12-28T23:12:17Z", "comment_text": "\n \t\tHi. Let me try to address some of the questions here and see if that helps.\n \n Has anyone made a docstring PR on this yet?\n \n experimental_run_tf_function is an implementation detail, and that flag is mostly there as a debug during the transition. We don't plan to document it because it will be removed at some point in the future and the True behavior will be the only behavior.\n Now I expect that you may be surprised (or aghast) that it's going to be always on given the discussion in this thread. What experimental_run_tf_function does is funnel all calls to fit, evaluate, and predict through a central adapter which creates a Dataset and performs a variety of checks and input validation. This is generally desirable because it makes everything more robust, but there is some overhead to spinning up this machinery which is not amortized by small models with little data.\n Code to profile the step:\n <denchmark-code>import cProfile\n import pstats\n \n profiler = cProfile.Profile()\n profiler.enable()\n for _ in range(5):\n   model.predict(x)\n profiler.disable()\n \n stats = pstats.Stats(profiler)\n stats.sort_stats(\"cumtime\").print_stats(20)\n </denchmark-code>\n \n In this case most of the extra time is spent creating the dataset; there is machinery in there which makes sense, but it's surplus to requirements for the degenerate case of a single batch. (<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n  in case you want to look into the init time, but it's not obvious that it's unreasonable given the pipeline.) Really, this is not what  is for. That endpoint is for predictions on lots of data where the batching and aggregation machinery in that endpoint is necessary. For single batch prediction there is , which doesn't invoke all of that machinery and just directly calls into the model function. I tested it, and it is identical in v1 and v2. (And faster than even v1 )\n However this seems to be a common pitfall; I see lots of issues around single batch . (<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>\n  increment your counter...) From a documentation standpoint, I think the most valuable contribution would be to document  in the  docstring, and probably also warn in  when the batch cardinality is one. <denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>\n  <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>\n   Can you remind me to bring this up at the next triage? And <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>\n  if you want to take a crack at a PR that would be great; feel free to tag me and I'll try to provide some assistance.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "off99555", "commentT": "2019-12-28T23:12:52Z", "comment_text": "\n \t\tOh, and <denchmark-link:https://github.com/goldiegadde>@goldiegadde</denchmark-link>\n  for tracking 2.2 performance, of course.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "off99555", "commentT": "2019-12-29T00:47:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/robieta>@robieta</denchmark-link>\n  Solid response, thanks - I'll look into the PR sometime. As for , so long as a faster alternative remains (i.e. ) and is noted in a docstring, it'd make an excellent resolution.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "off99555", "commentT": "2019-12-29T14:17:29Z", "comment_text": "\n \t\tThanks for the clarification, and for the quick help! model.predict_on_batch speeds things up, but is still significantly slower on the v2 path. Here is the cumsum using your profile snippet (using tf2.0.0, calling predict 100 times instead of 5) for a small DQN model on a small batch of data:\n \n \n \n experimental_run_tf_function\n predict()\n predict_on_batch()\n \n \n \n \n False / v1\n 0.209s\n 0.078s\n \n \n True / v2 (default)\n 3.720s\n 0.246s\n \n \n \n So, anyone with a single batch should switch to predict_on_batch, but an even faster option exists (v1 predict_on_batch), which is going to be deprecated if I understood correctly.\n Is this use case truly so exotic that we simply should not use the Keras API for it? I understand that we should of course look into batching, but for anyone who just writes quick prototypes it would be nice to have a fast light-weight way of evaluating things. Anyway, warning single-batch users of predict() will surely help most users, so that sounds great.\n Also, here's a detail that may be important to people who migrate their code: When running in v2 mode, predict_on_batch will not return a numpy array (contrary to the docstring), but an EagerTensor instead. The caller may want to wrap the result -- as in np.array(model.predict_on_batch(...)) -- to guarantee the same behavior for both v1 and v2. predict however returns a numpy array in both cases.  If you like, I can make a PR for the docstring.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "off99555", "commentT": "2020-01-03T14:47:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/robieta>@robieta</denchmark-link>\n  Actually I'll pass on making a PR; currently rather occupied - may try later if noone has done it\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "off99555", "commentT": "2020-01-13T11:48:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>\n  <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>\n   I tried to reproduce this with tf nightly but it doesn't seem to occur anymore. Or equivalently even without compile it becomes pretty slow as well.\n Now this is more of a tensorflow 1.15 much faster than tensorflow v2 -- can you confirm?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "off99555", "commentT": "2020-01-14T11:00:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>\n  Can confirm for nightly. The v1 path fallback has been removed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/c73c99ca3e0bacf2bca313f270bb3eae28869530#diff-de9b96ac2d81503324cbbbe21732031f>c73c99c#diff-de9b96ac2d81503324cbbbe21732031f</denchmark-link>\n  , so everything now executes on v2, regardless of model.compile()\n It's still possible to get the old speed back behavior by using tf.compat.v1.disable_eager_execution(). In any case, for most users moving to predict_on_batch() should be recommended, and also makes the slowdown less drastic (see numbers above) so the issue is less extreme.\n That said, sorry for taking so long on the PR for warning single-batch predict() users.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "off99555", "commentT": "2020-01-15T02:44:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ttbrunner>@ttbrunner</denchmark-link>\n  Ah yeah that was the commit, thanks.\n Ok so we follow the adapter pattern for convert numpy and dataframes to dataset first, and has a single path for execution. Apparently the speed down is mainly two things: 1) the construction of dataset. 2) creating tf.function for predict. (Check TensorLikeDataAdapter under /python/keras/engine/data_adapter.py if you're interested)\n <denchmark-link:https://github.com/off99555>@off99555</denchmark-link>\n  <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>\n  <denchmark-link:https://github.com/ttbrunner>@ttbrunner</denchmark-link>\n  So here's what I would recommend going forward:\n \n \n you can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no \"conversion to dataset\" part, and also it's directly calling a cached tf.function. However be aware that if you have batch norm layers or any other layers that behaves differently between training and inference, make sure to call it with model(x, training=False)\n \n \n I will make a docstring to recommend users to use model call and explain predict is for large dataset.\n \n \n SG?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "off99555", "commentT": "2020-01-15T03:02:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>\n  I haven't benchmarked any of it, but if it is as you say - sounds like an excellent resolution indeed. Do let me know when it's done so I update my SO answer -- thanks for following through with this to the end.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "off99555", "commentT": "2020-01-15T05:51:39Z", "comment_text": "\n \t\tI have updated the doc, also tested the performance for model(x) in nightly. Closing it for now. Thanks all for reporting and collaborative work!\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "off99555", "commentT": "2020-01-15T06:04:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>\n  Great. To clarify, is the speedup for TF 2.1+ only, or also 2.0? (if latter, is 2.1 even faster?)\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "off99555", "commentT": "2020-01-15T06:05:21Z", "comment_text": "\n \t\t\n @tanzhenyu Great. To clarify, is the speedup for TF 2.1+ only, or also 2.0? (if latter, is 2.1 even faster?)\n \n I believe this should be universal to 2.x versions\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "off99555", "commentT": "2020-01-15T09:06:29Z", "comment_text": "\n \t\tThanks for the docstring update, also for the explanation. I'm always interested!\n Can confirm that model(x) has the same runtime as predict_on_batch(x), i.e. the v2 path is still slightly slower. It's OK for my use case though, so thanks again.\n Another note for users: it's possible to specify model.run_eagerly = False before compiling. With this and the model(x) call, I am getting almost the same performance as in v1, without globally disabling eager execution.\n P.S.: Sorry for the many edits of this post.\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "off99555", "commentT": "2020-01-16T05:54:40Z", "comment_text": "\n \t\t\n Thanks for the docstring update, also for the explanation. I'm always interested!\n Can confirm that model(x) has the same runtime as predict_on_batch(x), i.e. the v2 path is still slightly slower. It's OK for my use case though, so thanks again.\n Another note for users: it's possible to specify model.run_eagerly = False before compiling. With this and the model(x) call, I am getting almost the same performance as in v1, without globally disabling eager execution.\n P.S.: Sorry for the many edits of this post.\n \n You can also compile(..., run_eagerly=False)\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "off99555", "commentT": "2020-01-16T06:12:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>\n  Side-ish question: what does  do in TF2, which runs in eager by default? From inspecting a fair chunk of the source code, it seems to change only a few execution paths for certain less-usual dataset types. Likewise, any difference with  vs. disabling eager?\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "off99555", "commentT": "2020-03-27T21:50:32Z", "comment_text": "\n \t\t\n @ttbrunner Ah yeah that was the commit, thanks.\n Ok so we follow the adapter pattern for convert numpy and dataframes to dataset first, and has a single path for execution. Apparently the speed down is mainly two things: 1) the construction of dataset. 2) creating tf.function for predict. (Check TensorLikeDataAdapter under /python/keras/engine/data_adapter.py if you're interested)\n @off99555 @OverLordGoldDragon @ttbrunner So here's what I would recommend going forward:\n \n you can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no \"conversion to dataset\" part, and also it's directly calling a cached tf.function. However be aware that if you have batch norm layers or any other layers that behaves differently between training and inference, make sure to call it with model(x, training=False)\n I will make a docstring to recommend users to use model call and explain predict is for large dataset.\n \n SG?\n \n <denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>\n  <denchmark-link:https://github.com/robieta>@robieta</denchmark-link>\n  Could you please clarify what the difference between model.predict_on_batch(x) and model(x) is?\n The call stack and execution times (as captured by statistical profiler pyinstrument) are extremely different.\n Here are the details for model.predict_on_batch(x):\n <denchmark-code>0.017 <module>  code1.py:1\n \u2514\u2500 0.017 predict_on_batch  tensorflow_core/python/keras/engine/training.py:1220\n    \u2514\u2500 0.017 predict_on_batch  tensorflow_core/python/keras/engine/training_v2_utils.py:514\n       \u251c\u2500 0.005 _standardize_user_data  tensorflow_core/python/keras/engine/training.py:2247\n       \u2502  \u251c\u2500 0.000 run_eagerly  tensorflow_core/python/keras/engine/training.py:508\n       \u2502  \u2502  \u2514\u2500 0.000 wrapped  tensorflow_core/python/training/tracking/layer_utils.py:126\n       \u2502  \u2514\u2500 0.005 _standardize_tensors  tensorflow_core/python/keras/engine/training.py:2385\n       \u2502     \u251c\u2500 0.001 standardize_input_data  tensorflow_core/python/keras/engine/training_utils.py:460\n       \u2502     \u2502  \u2514\u2500 0.000 <listcomp>  tensorflow_core/python/keras/engine/training_utils.py:526\n       \u2502     \u2502     \u2514\u2500 0.000 standardize_single_array  tensorflow_core/python/keras/engine/training_utils.py:439\n       \u2502     \u251c\u2500 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471\n       \u2502     \u2502  \u2514\u2500 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\n       \u2502     \u2502     \u251c\u2500 0.000 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\n       \u2502     \u2502     \u2502  \u2514\u2500 0.000 _yield_value  tensorflow_core/python/util/nest.py:174\n       \u2502     \u2502     \u2502     \u2514\u2500 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179\n       \u2502     \u2502     \u2514\u2500 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119\n       \u2502     \u251c\u2500 0.002 map_structure  tensorflow_core/python/util/nest.py:507\n       \u2502     \u2502  \u251c\u2500 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568\n       \u2502     \u2502  \u2502  \u2514\u2500 0.001 _type_spec_from_value  tensorflow_core/python/keras/engine/training.py:2431\n       \u2502     \u2502  \u2502     \u2514\u2500 0.000 __init__  tensorflow_core/python/framework/tensor_spec.py:39\n       \u2502     \u2502  \u2514\u2500 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471\n       \u2502     \u2502     \u2514\u2500 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\n       \u2502     \u2502        \u251c\u2500 0.001 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\n       \u2502     \u2502        \u2502  \u2514\u2500 0.000 _yield_value  tensorflow_core/python/util/nest.py:174\n       \u2502     \u2502        \u2502     \u2514\u2500 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179\n       \u2502     \u2502        \u2514\u2500 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119\n       \u2502     \u2502           \u2514\u2500 0.000 [self]  \n       \u2502     \u2514\u2500 0.000 wrapped  tensorflow_core/python/training/tracking/layer_utils.py:126\n       \u251c\u2500 0.001 cast_to_model_input_dtypes  tensorflow_core/python/keras/engine/training_utils.py:1363\n       \u2502  \u2514\u2500 0.001 map_structure  tensorflow_core/python/util/nest.py:507\n       \u2502     \u251c\u2500 0.000 pack_sequence_as  tensorflow_core/python/util/nest.py:471\n       \u2502     \u2502  \u2514\u2500 0.000 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\n       \u2502     \u2502     \u2514\u2500 0.000 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\n       \u2502     \u251c\u2500 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568\n       \u2502     \u2502  \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\n       \u2502     \u2502     \u2514\u2500 0.001 cast  tensorflow_core/python/ops/math_ops.py:649\n       \u2502     \u2502        \u2514\u2500 0.000 cast  tensorflow_core/python/ops/gen_math_ops.py:1944\n       \u2502     \u2514\u2500 0.000 pack_sequence_as  tensorflow_core/python/util/nest.py:471\n       \u251c\u2500 0.000 _get_or_make_on_batch_function  tensorflow_core/python/keras/engine/training_v2_utils.py:103\n       \u2514\u2500 0.010 __call__  tensorflow_core/python/eager/def_function.py:551\n          \u2514\u2500 0.010 _call  tensorflow_core/python/eager/def_function.py:590\n             \u2514\u2500 0.010 __call__  tensorflow_core/python/eager/function.py:2359\n                \u251c\u2500 0.000 _maybe_define_function  tensorflow_core/python/eager/function.py:2637\n                \u2502  \u2514\u2500 0.000 _cache_key  tensorflow_core/python/eager/function.py:2496\n                \u2514\u2500 0.010 _filtered_call  tensorflow_core/python/eager/function.py:1593\n                   \u2514\u2500 0.009 _call_flat  tensorflow_core/python/eager/function.py:1613\n                      \u251c\u2500 0.008 call  tensorflow_core/python/eager/function.py:505\n                      \u2502  \u2514\u2500 0.008 quick_execute  tensorflow_core/python/eager/execute.py:33\n                      \u2502     \u2514\u2500 0.008 [self]  \n                      \u2514\u2500 0.001 _build_call_outputs  tensorflow_core/python/eager/function.py:1909\n                         \u2514\u2500 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471\n                            \u2514\u2500 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431\n                               \u251c\u2500 0.001 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396\n                               \u2502  \u2514\u2500 0.000 _yield_value  tensorflow_core/python/util/nest.py:174\n                               \u2502     \u2514\u2500 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179\n                               \u2514\u2500 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119\n </denchmark-code>\n \n And here are the details for the much slower model(x):\n <denchmark-code>0.041 <module>  code2.py:1\n \u2514\u2500 0.041 __call__  tensorflow_core/python/keras/engine/base_layer.py:628\n    \u2514\u2500 0.041 call  tensorflow_core/python/keras/engine/network.py:693\n       \u2514\u2500 0.041 _run_internal_graph  tensorflow_core/python/keras/engine/network.py:791\n          \u2514\u2500 0.041 __call__  tensorflow_core/python/keras/layers/recurrent.py:637\n             \u2514\u2500 0.041 __call__  tensorflow_core/python/keras/engine/base_layer.py:628\n                \u251c\u2500 0.001 helper  contextlib.py:237\n                \u2502  \u2514\u2500 0.001 __init__  contextlib.py:81\n                \u251c\u2500 0.039 call  tensorflow_core/python/keras/layers/recurrent.py:2689\n                \u2502  \u2514\u2500 0.039 call  tensorflow_core/python/keras/layers/recurrent.py:699\n                \u2502     \u251c\u2500 0.001 _process_inputs  tensorflow_core/python/keras/layers/recurrent.py:804\n                \u2502     \u2502  \u2514\u2500 0.001 get_initial_state  tensorflow_core/python/keras/layers/recurrent.py:613\n                \u2502     \u2502     \u2514\u2500 0.001 get_initial_state  tensorflow_core/python/keras/layers/recurrent.py:2455\n                \u2502     \u2502        \u2514\u2500 0.001 _generate_zero_filled_state_for_cell  tensorflow_core/python/keras/layers/recurrent.py:2891\n                \u2502     \u2502           \u2514\u2500 0.001 _generate_zero_filled_state  tensorflow_core/python/keras/layers/recurrent.py:2898\n                \u2502     \u2502              \u2514\u2500 0.001 map_structure  tensorflow_core/python/util/nest.py:507\n                \u2502     \u2502                 \u2514\u2500 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568\n                \u2502     \u2502                    \u2514\u2500 0.001 create_zeros  tensorflow_core/python/keras/layers/recurrent.py:2905\n                \u2502     \u2502                       \u2514\u2500 0.001 zeros  tensorflow_core/python/ops/array_ops.py:2399\n                \u2502     \u2502                          \u2514\u2500 0.001 fill  tensorflow_core/python/ops/array_ops.py:198\n                \u2502     \u2502                             \u2514\u2500 0.001 fill  tensorflow_core/python/ops/gen_array_ops.py:3193\n                \u2502     \u2514\u2500 0.038 rnn  tensorflow_core/python/keras/backend.py:3808\n                \u2502        \u251c\u2500 0.001 <listcomp>  tensorflow_core/python/keras/backend.py:4023\n                \u2502        \u2502  \u2514\u2500 0.001 _slice_helper  tensorflow_core/python/ops/array_ops.py:759\n                \u2502        \u2502     \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\n                \u2502        \u251c\u2500 0.001 [self]  \n                \u2502        \u2514\u2500 0.036 while_loop  tensorflow_core/python/ops/control_flow_ops.py:2482\n                \u2502           \u251c\u2500 0.018 _step  tensorflow_core/python/keras/backend.py:4096\n                \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502           \u2502  \u251c\u2500 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765\n                \u2502           \u2502  \u2502  \u2514\u2500 0.003 call  tensorflow_core/python/keras/layers/recurrent.py:2355\n                \u2502           \u2502  \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\n                \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396\n                \u2502           \u2502  \u2502     \u251c\u2500 0.001 bias_add  tensorflow_core/python/keras/backend.py:5508\n                \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 bias_add  tensorflow_core/python/ops/nn_ops.py:2699\n                \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\n                \u2502           \u2502  \u2502        \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\n                \u2502           \u2502  \u2502           \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\n                \u2502           \u2502  \u2502              \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\n                \u2502           \u2502  \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4132\n                \u2502           \u2502  \u2502  \u2514\u2500 0.001 wrapped  tensorflow_core/python/util/tf_should_use.py:234\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 write  tensorflow_core/python/ops/tensor_array_ops.py:1139\n                \u2502           \u2502  \u2502        \u2514\u2500 0.001 write  tensorflow_core/python/ops/tensor_array_ops.py:829\n                \u2502           \u2502  \u2502           \u2514\u2500 0.001 _write  tensorflow_core/python/ops/tensor_array_ops.py:779\n                \u2502           \u2502  \u2502              \u2514\u2500 0.001 numpy  tensorflow_core/python/framework/ops.py:918\n                \u2502           \u2502  \u2502                 \u2514\u2500 0.001 _numpy  tensorflow_core/python/framework/ops.py:905\n                \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502           \u2502  \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889\n                \u2502           \u2502  \u2502        \u2514\u2500 0.001 select_v2  tensorflow_core/python/ops/gen_math_ops.py:8662\n                \u2502           \u2502  \u251c\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\n                \u2502           \u2502  \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\n                \u2502           \u2502  \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\n                \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189\n                \u2502           \u2502  \u2502     \u2502        \u2514\u2500 0.001 __eq__  tensorflow_core/python/framework/dtypes.py:261\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\n                \u2502           \u2502  \u2502        \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\n                \u2502           \u2502  \u2502           \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\n                \u2502           \u2502  \u2502              \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\n                \u2502           \u2502  \u251c\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502           \u2502  \u2502  \u2514\u2500 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396\n                \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502           \u2502  \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\n                \u2502           \u2502  \u2502        \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\n                \u2502           \u2502  \u2502           \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\n                \u2502           \u2502  \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\n                \u2502           \u2502  \u2502                 \u2514\u2500 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263\n                \u2502           \u2502  \u2502                    \u2514\u2500 0.001 _autopacking_conversion_function  tensorflow_core/python/ops/array_ops.py:1355\n                \u2502           \u2502  \u2502                       \u2514\u2500 0.001 _should_not_autopack  tensorflow_core/python/ops/array_ops.py:1345\n                \u2502           \u2502  \u2502                          \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/ops/array_ops.py:1351\n                \u2502           \u2502  \u251c\u2500 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765\n                \u2502           \u2502  \u2502  \u251c\u2500 0.001 [self]  \n                \u2502           \u2502  \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\n                \u2502           \u2502  \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\n                \u2502           \u2502  \u2502     \u2502  \u2514\u2500 0.001 sigmoid  tensorflow_core/python/keras/activations.py:246\n                \u2502           \u2502  \u2502     \u2502     \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/math_ops.py:3134\n                \u2502           \u2502  \u2502     \u2502        \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/gen_math_ops.py:8720\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\n                \u2502           \u2502  \u2502        \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\n                \u2502           \u2502  \u2502           \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\n                \u2502           \u2502  \u2502              \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\n                \u2502           \u2502  \u251c\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502           \u2502  \u2502  \u2514\u2500 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 add_v2  tensorflow_core/python/ops/gen_math_ops.py:451\n                \u2502           \u2502  \u251c\u2500 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502           \u2502  \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\n                \u2502           \u2502  \u2502     \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\n                \u2502           \u2502  \u2502        \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\n                \u2502           \u2502  \u2502           \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\n                \u2502           \u2502  \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\n                \u2502           \u2502  \u2502                 \u2514\u2500 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263\n                \u2502           \u2502  \u2514\u2500 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765\n                \u2502           \u2502     \u251c\u2500 0.001 [self]  \n                \u2502           \u2502     \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\n                \u2502           \u2502        \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\n                \u2502           \u2502        \u2502  \u2514\u2500 0.001 sigmoid  tensorflow_core/python/keras/activations.py:246\n                \u2502           \u2502        \u2502     \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/math_ops.py:3134\n                \u2502           \u2502        \u2502        \u2514\u2500 0.001 sigmoid  tensorflow_core/python/ops/gen_math_ops.py:8720\n                \u2502           \u2502        \u2514\u2500 0.001 dot  tensorflow_core/python/keras/backend.py:1614\n                \u2502           \u2502           \u2514\u2500 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177\n                \u2502           \u2502              \u2514\u2500 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617\n                \u2502           \u2502                 \u2514\u2500 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576\n                \u2502           \u251c\u2500 0.001 [self]  \n                \u2502           \u251c\u2500 0.004 _step  tensorflow_core/python/keras/backend.py:4096\n                \u2502           \u2502  \u251c\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502           \u2502  \u2502  \u2514\u2500 0.002 <genexpr>  tensorflow_core/python/keras/backend.py:4067\n                \u2502           \u2502  \u2502     \u2514\u2500 0.002 _expand_mask  tensorflow_core/python/keras/backend.py:3913\n                \u2502           \u2502  \u2502        \u2514\u2500 0.002 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\n                \u2502           \u2502  \u2502           \u2514\u2500 0.002 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\n                \u2502           \u2502  \u2502              \u251c\u2500 0.001 quick_execute  tensorflow_core/python/eager/execute.py:33\n                \u2502           \u2502  \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\n                \u2502           \u2502  \u2502                 \u2514\u2500 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263\n                \u2502           \u2502  \u2502                    \u2514\u2500 0.001 get  tensorflow_core/python/framework/tensor_conversion_registry.py:114\n                \u2502           \u2502  \u2514\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\n                \u2502           \u2502     \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\n                \u2502           \u2502        \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\n                \u2502           \u2502        \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502           \u2502        \u2502     \u2514\u2500 0.001 _mul_dispatch  tensorflow_core/python/ops/math_ops.py:1197\n                \u2502           \u2502        \u2502        \u2514\u2500 0.001 mul  tensorflow_core/python/ops/gen_math_ops.py:6093\n                \u2502           \u2502        \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502           \u251c\u2500 0.001 assert_same_structure  tensorflow_core/python/util/nest.py:293\n                \u2502           \u2514\u2500 0.012 _step  tensorflow_core/python/keras/backend.py:4096\n                \u2502              \u251c\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502              \u2502  \u2514\u2500 0.002 <genexpr>  tensorflow_core/python/keras/backend.py:4067\n                \u2502              \u2502     \u2514\u2500 0.002 _expand_mask  tensorflow_core/python/keras/backend.py:3913\n                \u2502              \u2502        \u2514\u2500 0.002 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\n                \u2502              \u2502           \u2514\u2500 0.002 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\n                \u2502              \u251c\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\n                \u2502              \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\n                \u2502              \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\n                \u2502              \u2502     \u2502  \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502              \u2502     \u2502     \u2514\u2500 0.001 _mul_dispatch  tensorflow_core/python/ops/math_ops.py:1197\n                \u2502              \u2502     \u2502        \u2514\u2500 0.001 mul  tensorflow_core/python/ops/gen_math_ops.py:6093\n                \u2502              \u2502     \u2514\u2500 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899\n                \u2502              \u2502        \u2514\u2500 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189\n                \u2502              \u2502           \u2514\u2500 0.001 add_v2  tensorflow_core/python/ops/gen_math_ops.py:451\n                \u2502              \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4108\n                \u2502              \u2502  \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:1127\n                \u2502              \u2502     \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:746\n                \u2502              \u251c\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502              \u2502  \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070\n                \u2502              \u2502  \u2502  \u2514\u2500 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889\n                \u2502              \u2502  \u2502     \u2514\u2500 0.001 select_v2  tensorflow_core/python/ops/gen_math_ops.py:8662\n                \u2502              \u2502  \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\n                \u2502              \u2502     \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\n                \u2502              \u2502        \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\n                \u2502              \u2502           \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\n                \u2502              \u2502              \u2514\u2500 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236\n                \u2502              \u2502                 \u2514\u2500 0.001 <listcomp>  tensorflow_core/python/eager/execute.py:271\n                \u2502              \u251c\u2500 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765\n                \u2502              \u2502  \u2514\u2500 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355\n                \u2502              \u2502     \u251c\u2500 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346\n                \u2502              \u2502     \u2502  \u2514\u2500 0.001 tanh  tensorflow_core/python/keras/activations.py:224\n                \u2502              \u2502     \u2502     \u2514\u2500 0.001 tanh  tensorflow_core/python/ops/gen_math_ops.py:10284\n                \u2502              \u2502     \u2514\u2500 0.001 bias_add  tensorflow_core/python/keras/backend.py:5508\n                \u2502              \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4108\n                \u2502              \u2502  \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:1127\n                \u2502              \u2502     \u2514\u2500 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:746\n                \u2502              \u2514\u2500 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065\n                \u2502                 \u251c\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070\n                \u2502                 \u2502  \u2514\u2500 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889\n                \u2502                 \u2514\u2500 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067\n                \u2502                    \u2514\u2500 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913\n                \u2502                       \u2514\u2500 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344\n                \u2502                          \u2514\u2500 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431\n                \u2514\u2500 0.001 _set_mask_metadata  tensorflow_core/python/keras/engine/base_layer.py:1918\n                   \u2514\u2500 0.001 flatten  tensorflow_core/python/util/nest.py:242\n </denchmark-code>\n \n Out of the two suggestions you guys provided in this thread, the one that ended up being put in the docstring is the slowest one (even slower than model.predict(x)) for my simple LSTM encoder being run on Tensorflow 2.1.0 on with GPU support, and other users like me might be confused.\n Also, can you please clarify why calling model(x) runs the model step by step even though run_eagerly is set to False and, as per tanzhenyu's comment, it should instead call a cached tf.function?\n Thanks!\n \t\t"}}}, "commit": {"commit_id": "42f469be0f3e8c36624f0b01c571e7ed15f75faf", "commit_author": "Zhenyu Tan", "commitT": "2020-01-14 20:01:47-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\python\\keras\\engine\\training.py", "file_new_name": "tensorflow\\python\\keras\\engine\\training.py", "file_complexity": {"file_NLOC": "1575", "file_CCN": "339", "file_NToken": "9039"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "837,838,839,840,841,842", "deleted_lines": "837"}}}}}}