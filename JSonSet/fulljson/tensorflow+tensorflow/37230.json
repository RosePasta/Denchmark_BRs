{"BR": {"BR_id": "37230", "BR_author": "GeorgeLiang3", "BRopenT": "2020-03-02T12:43:36Z", "BRcloseT": "2020-03-30T23:27:38Z", "BR_text": {"BRsummary": "tf.function second derivative error", "BRdescription": "\n Please make sure that this is a bug. As per our\n GitHub Policy,\n we only address code/doc bugs, performance issues, feature requests and\n build/installation issues on GitHub. tag:bug_template\n System information\n \n \n Have I written custom code (as opposed to using a stock\n example script provided in TensorFlow): yes\n \n \n OS Platform and Distribution (e.g.,\n Linux Ubuntu 16.04): macOS Mojave 10.14.6\n \n \n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\n the issue happens on mobile device: N/A\n \n \n TensorFlow installed from (source or\n binary): - TensorFlow version (use command below):binary  v2.1.0-rc2-17\n \n \n Python version:  - Bazel\n version (if compiling from source): Python 3.7.1\n \n \n GCC/Compiler version (if compiling from\n source):\n \n \n CUDA/cuDNN version: - GPU model and memory:\n \n \n Describe the current behavior\n I have a custom function need to use slicing in a for loop. The first-order derivative is working properly but the second-order derivative gives the error. The error only occurs when the function is decorated with tf.function. Below is a simplified code to reproduce the error.\n Describe the expected behavior\n Standalone code to reproduce the issue\n Provide a reproducible test case that is the bare minimum necessary to generate\n the problem. If possible, please share a link to Colab/Jupyter/any notebook.\n <denchmark-code>import tensorflow as tf\n x =tf.random.uniform([9],minval = -50,maxval = -30,seed = 1,dtype = tf.float32)\n \n @tf.function\n def A(x):\n     return x[1]\n \n @tf.function\n def g(x):\n \n     Z_sum = tf.constant([0.],dtype = tf.float32)\n \n     for i in tf.range(x.shape[0]):\n         Z_sum = tf.add(Z_sum, A(x))\n \n     return Z_sum\n \n with tf.GradientTape() as t:\n     t.watch(x)\n     with tf.GradientTape() as tt:\n         tt.watch(x)\n         loss = g(x)\n     jac = tt.gradient(loss,x)\n hess = t.gradient(jac,x)\n </denchmark-code>\n \n Other info / logs Include any logs or source code that would be helpful to\n diagnose the problem. If including tracebacks, please include the full\n traceback. Large logs and files should be attached.\n <denchmark-h:hr></denchmark-h>\n \n InvalidArgumentError                      Traceback (most recent call last)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n 2325       with c_api_util.tf_buffer() as buf:\n -> 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n 2327         data = c_api.TF_GetBuffer(buf)\n InvalidArgumentError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.\n During handling of the above exception, another exception occurred:\n ValueError                                Traceback (most recent call last)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n 330     try:\n --> 331       xla_compile = op.get_attr(\"_XlaCompile\")\n 332       xla_separate_compiled_gradients = op.get_attr(\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n 2329       # Convert to ValueError for backwards compatibility.\n -> 2330       raise ValueError(str(e))\n 2331     x = attr_value_pb2.AttrValue()\n ValueError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.\n During handling of the above exception, another exception occurred:\n InvalidArgumentError                      Traceback (most recent call last)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n 2325       with c_api_util.tf_buffer() as buf:\n -> 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n 2327         data = c_api.TF_GetBuffer(buf)\n InvalidArgumentError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.\n During handling of the above exception, another exception occurred:\n ValueError                                Traceback (most recent call last)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n 330     try:\n --> 331       xla_compile = op.get_attr(\"_XlaCompile\")\n 332       xla_separate_compiled_gradients = op.get_attr(\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n 2329       # Convert to ValueError for backwards compatibility.\n -> 2330       raise ValueError(str(e))\n 2331     x = attr_value_pb2.AttrValue()\n ValueError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.\n During handling of the above exception, another exception occurred:\n ValueError                                Traceback (most recent call last)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\n 467               as_ref=input_arg.is_ref,\n --> 468               preferred_dtype=default_dtype)\n 469         except TypeError as err:\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\n 1313     if ret is None:\n -> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n 1315\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\n 316   _ = as_ref\n --> 317   return constant(v, dtype=dtype, name=name)\n 318\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)\n 257   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n --> 258                         allow_broadcast=True)\n 259\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\n 295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\n --> 296           allow_broadcast=allow_broadcast))\n 297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\n 438     if values is None:\n --> 439       raise ValueError(\"None values not supported.\")\n 440     # if dtype is provided, forces numpy array to be the type\n ValueError: None values not supported.\n During handling of the above exception, another exception occurred:\n ValueError                                Traceback (most recent call last)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\n 481             observed = ops.convert_to_tensor(\n --> 482                 values, as_ref=input_arg.is_ref).dtype.name\n 483           except ValueError as err:\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\n 1313     if ret is None:\n -> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n 1315\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\n 316   _ = as_ref\n --> 317   return constant(v, dtype=dtype, name=name)\n 318\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)\n 257   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n --> 258                         allow_broadcast=True)\n 259\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\n 295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\n --> 296           allow_broadcast=allow_broadcast))\n 297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\n 438     if values is None:\n --> 439       raise ValueError(\"None values not supported.\")\n 440     # if dtype is provided, forces numpy array to be the type\n ValueError: None values not supported.\n During handling of the above exception, another exception occurred:\n ValueError                                Traceback (most recent call last)\n  in \n 21         tt.watch(x)\n 22         loss = g(x)\n ---> 23     jac = tt.gradient(loss,x)\n 24 hess = t.gradient(jac,x)\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\n 1027         output_gradients=output_gradients,\n 1028         sources_raw=flat_sources_raw,\n -> 1029         unconnected_gradients=unconnected_gradients)\n 1030\n 1031     if not self._persistent:\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\n 75       output_gradients,\n 76       sources_raw,\n ---> 77       compat.as_str(unconnected_gradients.value))\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backward_function_wrapper(*args)\n 1254           break\n 1255       return backward._call_flat(  # pylint: disable=protected-access\n -> 1256           processed_args, remapped_captures)\n 1257\n 1258     return _backward_function_wrapper, recorded_outputs\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\n 1695         possible_gradient_type,\n 1696         executing_eagerly)\n -> 1697     forward_function, args_with_tangents = forward_backward.forward()\n 1698     if executing_eagerly:\n 1699       flat_outputs = forward_function.call(\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self)\n 1421     \"\"\"Builds or retrieves a forward function for this call.\"\"\"\n 1422     forward_function = self._functions.forward(\n -> 1423         self._inference_args, self._input_tangents)\n 1424     return forward_function, self._inference_args + self._input_tangents\n 1425\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self, inference_args, input_tangents)\n 1183       (self._forward, self._forward_graph, self._backward,\n 1184        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (\n -> 1185            self._forward_and_backward_functions(inference_args, input_tangents))\n 1186     return self._forward\n 1187\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)\n 1329     outputs = self._func_graph.outputs[:self._num_inference_outputs]\n 1330     return self._build_functions_for_outputs(\n -> 1331         outputs, inference_args, input_tangents)\n 1332\n 1333\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)\n 888             self._func_graph.inputs,\n 889             grad_ys=gradients_wrt_outputs,\n --> 890             src_graph=self._func_graph)\n 891\n 892       captures_from_forward = [\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\n 667                 # functions.\n 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n --> 669                                          lambda: grad_fn(op, *out_grads))\n 670               else:\n 671                 # For function call ops, we add a 'SymbolicGradient'\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n 334       xla_scope = op.get_attr(\"_XlaScope\").decode()\n 335     except ValueError:\n --> 336       return grad_fn()  # Exit early\n 337\n 338   if not xla_compile:\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in ()\n 667                 # functions.\n 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n --> 669                                          lambda: grad_fn(op, *out_grads))\n 670               else:\n 671                 # For function call ops, we add a 'SymbolicGradient'\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _WhileGrad(op, *grads)\n 357   body_grad_graph, args = _create_grad_func(\n 358       ys, xs, non_none_grads, cond_graph, body_graph,\n --> 359       util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)\n 360\n 361   if body_grad_graph.while_op_needs_rewrite:\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _create_grad_func(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)\n 599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\n 600                                          maximum_iterations, while_op,\n --> 601                                          body_graph_inputs, body_graph_outputs))\n 602\n 603   # Update the list of outputs with tensors corresponding to the captured\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n 976                                           converted_func)\n 977\n --> 978       func_outputs = python_func(*func_args, **func_kwargs)\n 979\n 980       # invariant: func_outputs contains only Tensors, CompositeTensors,\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in (*args)\n 595   grad_func_graph = func_graph_module.func_graph_from_py_func(\n 596       name,\n --> 597       lambda *args: _grad_fn(ys, xs, args, body_graph),\n 598       args, {},\n 599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _grad_fn(ys, xs, args, func_graph)\n 655   grad_outs = gradients_util._GradientsHelper(\n 656       ys, xs, grad_ys=grad_ys, src_graph=func_graph,\n --> 657       unconnected_gradients=\"zero\")\n 658\n 659   # TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\n 667                 # functions.\n 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n --> 669                                          lambda: grad_fn(op, *out_grads))\n 670               else:\n 671                 # For function call ops, we add a 'SymbolicGradient'\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n 334       xla_scope = op.get_attr(\"_XlaScope\").decode()\n 335     except ValueError:\n --> 336       return grad_fn()  # Exit early\n 337\n 338   if not xla_compile:\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in ()\n 667                 # functions.\n 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n --> 669                                          lambda: grad_fn(op, *out_grads))\n 670               else:\n 671                 # For function call ops, we add a 'SymbolicGradient'\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/list_ops.py in _PopBackGrad(op, dlist, delement)\n 187         element_shape=gen_list_ops.tensor_list_element_shape(\n 188             op.outputs[0], shape_type=dtypes.int32))\n --> 189   return gen_list_ops.tensor_list_push_back(dlist, delement), None\n 190\n 191\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_list_ops.py in tensor_list_push_back(input_handle, tensor, name)\n 761   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n 762         \"TensorListPushBack\", input_handle=input_handle, tensor=tensor,\n --> 763                               name=name)\n 764   _result = _outputs[:]\n 765   if _execute.must_record_gradient():\n ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\n 484             raise ValueError(\n 485                 \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\n --> 486                 (input_name, err))\n 487           prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n 488                     (input_name, op_type_name, observed))\n ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "GeorgeLiang3", "commentT": "2020-03-04T09:14:20Z", "comment_text": "\n \t\tI think this is related to issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/15219>#15219</denchmark-link>\n , is there any solution for it now?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "GeorgeLiang3", "commentT": "2020-03-04T10:20:01Z", "comment_text": "\n \t\tCould able to reproduce the issue with Tf 2.1.\n Please find the gist <denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/28674efbe2941bb129d5622c13892277/untitled416.ipynb>here</denchmark-link>\n . Thanks!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "GeorgeLiang3", "commentT": "2020-03-05T03:50:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/GeorgeLiang3>@GeorgeLiang3</denchmark-link>\n  I don't run into any error when I am using tf-nightly. Please find my gist <denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/f7bcc6f630e4864f0c269256f9d3df96/untitled37.ipynb>here</denchmark-link>\n . Thanks!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "GeorgeLiang3", "commentT": "2020-03-05T10:41:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>\n   Thanks for your reply! But if I add one more 'if' or 'for' statement in the function, the both give the same error. Please find my gist <denchmark-link:https://colab.research.google.com/drive/1p0bb2ePkc6ect0Q26RVB_BByOUFtppeq>here</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "GeorgeLiang3", "commentT": "2020-03-10T12:27:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>\n  Any solution to this? Thanks in advance\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "GeorgeLiang3", "commentT": "2020-03-30T23:27:40Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "cf09044d9e7b232080f95f0f910a6803904df1de", "commit_author": "Saurabh Saxena", "commitT": "2020-03-30 16:22:09-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.8888888888888888"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\eager\\pywrap_gradient_exclusions.cc", "file_new_name": "tensorflow\\python\\eager\\pywrap_gradient_exclusions.cc", "file_complexity": {"file_NLOC": "851", "file_CCN": "6", "file_NToken": "4309"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "413", "deleted_lines": "413,836", "method_info": {"method_name": "OpGradientUnusedOutputIndices", "method_params": "op_name", "method_startline": "411", "method_endline": "881", "method_complexity": {"method_NLOC": "470", "method_CCN": "2", "method_NToken": "2059", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\kernel_tests\\list_ops_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\list_ops_test.py", "file_complexity": {"file_NLOC": "1426", "file_CCN": "151", "file_NToken": "14533"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686", "deleted_lines": null, "method_info": {"method_name": "testPopBackGrad", "method_params": "self", "method_startline": "1668", "method_endline": "1686", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "85", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "1672,1673,1674,1675,1676", "deleted_lines": null, "method_info": {"method_name": "testPopBackGrad.g", "method_params": "x", "method_startline": "1672", "method_endline": "1676", "method_complexity": {"method_NLOC": "5", "method_CCN": "2", "method_NToken": "33", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\ops\\list_ops.py", "file_new_name": "tensorflow\\python\\ops\\list_ops.py", "file_complexity": {"file_NLOC": "262", "file_CCN": "42", "file_NToken": "1746"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "189,190", "deleted_lines": null, "method_info": {"method_name": "_PopBackGrad", "method_params": "op,dlist,delement", "method_startline": "183", "method_endline": "191", "method_complexity": {"method_NLOC": "9", "method_CCN": "3", "method_NToken": "73", "method_nesting_level": "0"}}}}}}}}