{"BR": {"BR_id": "15034", "BR_author": "khanrc", "BRopenT": "2017-12-01T10:24:45Z", "BRcloseT": "2018-01-31T23:02:26Z", "BR_text": {"BRsummary": "Optimize graph & graph transform tools do not support NCHW", "BRdescription": "\n I tried optimizing graph using both <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md>Graph transform tool</denchmark-link>\n  and <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py>Optimize graph for inference</denchmark-link>\n . Both cases produced the same error because the fused batchnorm used not NCHW, but NHWC. I've got the error like this:\n <denchmark-code>InvalidArgumentError (see above for traceback): Must provide as many biases as the channel dimension of the input tensor: [256] vs. 19 in [1,256,19,19]\n \t [[Node: prefix/convblock/BatchNorm/FusedBatchNorm = BiasAdd[T=DT_FLOAT, data_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefix/convblock/Conv2D, prefix/convblock/Conv2D_bn_offset)]\n </denchmark-code>\n \n Although NCHW is faster than NHWC in GPU environment, why the tools do not support NCHW?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "khanrc", "commentT": "2017-12-01T17:47:50Z", "comment_text": "\n \t\tCould you provide a reproducible test case of what exactly you tried to do? Generally speaking, I think a lot of the tooling after training requires NHWC, as that was the only format when those were written. If you could provide a reproducible test case, we could work to improve it. <denchmark-link:https://github.com/petewarden>@petewarden</denchmark-link>\n , do you have any other comments?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "khanrc", "commentT": "2017-12-04T03:04:48Z", "comment_text": "\n \t\tI tried:\n <denchmark-code>python tensorflow/python/tools/optimize_for_inference.py \\\n --input ./ckpt/frozen_model.pb \\\n --output ./ckpt/optimized_model.pb \\\n --frozen_graph true \\\n --input_names Placeholder \\\n --output_names policy_head/softmax,value_head/value/Tanh\n </denchmark-code>\n \n and\n <denchmark-code>tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n --in_graph='./ckpt/frozen_model.pb' \\\n --out_graph='./ckpt/transformed_model.pb' \\\n --inputs='Placeholder' \\\n --outputs='policy_head/softmax,value_head/value/Tanh' \\\n --transforms='\n fold_constants(ignore_errors=true)\n fold_batch_norms\n fold_old_batch_norms\n fuse_pad_and_conv\n fuse_resize_and_conv\n fuse_resize_pad_and_conv\n '\n </denchmark-code>\n \n In both cases, the error occurred in fused batchnorm. The frozen model worked well, but the optimized model and transformed model emitted error.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "khanrc", "commentT": "2017-12-22T07:34:33Z", "comment_text": "\n \t\tIt has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "khanrc", "commentT": "2018-01-05T19:08:07Z", "comment_text": "\n \t\tNagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "khanrc", "commentT": "2018-01-24T13:17:05Z", "comment_text": "\n \t\tNagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.\n \t\t"}}}, "commit": {"commit_id": "6afe900f543e0005ce69b3152330f1b7b16cb286", "commit_author": "yegord", "commitT": "2018-01-31 15:02:25-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.8333333333333334", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\tools\\optimize_for_inference_lib.py", "file_new_name": "tensorflow\\python\\tools\\optimize_for_inference_lib.py", "file_complexity": {"file_NLOC": "324", "file_CCN": "55", "file_NToken": "2055"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "352", "deleted_lines": null, "method_info": {"method_name": "fold_batch_norms", "method_params": "input_graph_def", "method_startline": "201", "method_endline": "365", "method_complexity": {"method_NLOC": "133", "method_CCN": "19", "method_NToken": "1015", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\tools\\optimize_for_inference_test.py", "file_new_name": "tensorflow\\python\\tools\\optimize_for_inference_test.py", "file_complexity": {"file_NLOC": "264", "file_CCN": "18", "file_NToken": "2645"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222", "deleted_lines": "176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217", "method_info": {"method_name": "testFoldFusedBatchNorms", "method_params": "self", "method_startline": "175", "method_endline": "222", "method_complexity": {"method_NLOC": "45", "method_CCN": "4", "method_NToken": "472", "method_nesting_level": "1"}}}}}}}}