{"BR": {"BR_id": "7404", "BR_author": "malmaud", "BRopenT": "2017-02-10T03:14:10Z", "BRcloseT": "2018-03-29T18:11:56Z", "BR_text": {"BRsummary": "No attribute 'outer_context' when calculating gradient from imported graph", "BRdescription": "\n It seems when you import a graph with a \"while\" loop, you can't calculate gradients as you could on the original graph. e.g.\n import tensorflow as tf\n i=tf.constant(0, name=\"input\")\n out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=\"output\")\n graph_def = tf.get_default_graph().as_graph_def()\n \n g = tf.Graph()\n with g.as_default():\n     tf.import_graph_def(graph_def)\n s = tf.Session(graph=g)\n i_imported = g.get_tensor_by_name(\"import/input:0\")\n out_imported = g.get_tensor_by_name(\"import/output/Exit:0\")\n tf.gradients(out_imported, i_imported)\n <denchmark-code>---------------------------------------------------------------------------\n AttributeError                            Traceback (most recent call last)\n <ipython-input-12-e7e2b78684d3> in <module>()\n ----> 1 tf.gradients(out_imported, i_imported)\n \n /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\n     439     pending_count, loop_state = _PendingCount(ops.get_default_graph(), to_ops,\n     440                                               from_ops,\n --> 441                                               colocate_gradients_with_ops)\n     442 \n     443     # Iterate over the collected ops.\n \n \n /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in _PendingCount(graph, to_ops, from_ops, colocate_gradients_with_ops)\n     184   # 'loop_state' is None if there are no while loops.\n     185   loop_state = control_flow_ops.MaybeCreateControlFlowState(\n --> 186       between_op_list, between_ops, colocate_gradients_with_ops)\n     187 \n     188   # Initialize pending count for between ops.\n \n /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in MaybeCreateControlFlowState(between_op_list, between_ops, colocate_gradients_with_ops)\n    1293           loop_state.AddWhileContext(op, between_op_list, between_ops)\n    1294       else:\n -> 1295         loop_state.AddWhileContext(op, between_op_list, between_ops)\n    1296   return loop_state\n    1297 \n \n /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in AddWhileContext(self, op, between_op_list, between_ops)\n    1102     if grad_state is None:\n    1103       # This is a new while loop so create a grad state for it.\n -> 1104       outer_forward_ctxt = forward_ctxt.outer_context\n    1105       if outer_forward_ctxt:\n    1106         outer_forward_ctxt = outer_forward_ctxt.GetWhileContext()\n \n AttributeError: 'NoneType' object has no attribute 'outer_context'\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "malmaud", "commentT": "2017-02-10T16:43:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n  Any idea what would be happening here?  Do while loop gradients depend on unserialized information?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "malmaud", "commentT": "2017-02-11T07:22:35Z", "comment_text": "\n \t\tYes, though I thought Yuan and Sherry had added the appropriate serialization to the metagraph.  Is this on master branch of tf?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "malmaud", "commentT": "2017-02-11T12:46:43Z", "comment_text": "\n \t\tYa, this is master.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "malmaud", "commentT": "2017-02-11T17:14:36Z", "comment_text": "\n \t\tOK, I see if I save and restore the metagraph (via tf.train.export_meta_graph and tf.train.import_meta_graph, then I can take the gradient in the restored session. So it's just a problem if you try serializing and importing the GraphDef itself.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "malmaud", "commentT": "2017-02-13T15:50:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n  Is there an easy way to produce a nicer error message here, so that users don't have to guess the graph vs. metagraph issue?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "malmaud", "commentT": "2017-02-13T17:00:39Z", "comment_text": "\n \t\tYes; I believe that the forward_ctxt object here:\n \n forward_ctxt.outer_context\n \n should have a <denchmark-link:https://github.com/Property>@Property</denchmark-link>\n  outer_context; and if no such internal object is\n set, it can raise a ValueError/InternalError that maybe you're trying to\n call gradients on a while loop without properly serializing your graph via\n MetaGraphDef.\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Mon, Feb 13, 2017 at 7:51 AM, Geoffrey Irving ***@***.***> wrote:\n  @ebrevdo <https://github.com/ebrevdo> Is there an easy way to produce a\n  nicer error message here, so that users don't have to guess the graph vs.\n  metagraph issue?\n \n  \u2014\n  You are receiving this because you were mentioned.\n  Reply to this email directly, view it on GitHub\n  <#7404 (comment)>,\n  or mute the thread\n  <https://github.com/notifications/unsubscribe-auth/ABtimxlSpTQAQlztIVyjpp6fmXDWkZ60ks5rcHvvgaJpZM4L89DK>\n  .\n \n \n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "malmaud", "commentT": "2017-11-10T16:58:13Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n  , is there any update on this issue? Thanks!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "malmaud", "commentT": "2017-11-14T06:10:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n , any update on this issue or any workaround? Thanks!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "malmaud", "commentT": "2017-12-20T19:27:54Z", "comment_text": "\n \t\tIt has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "malmaud", "commentT": "2018-01-04T19:18:55Z", "comment_text": "\n \t\tIt has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "malmaud", "commentT": "2018-01-24T13:18:44Z", "comment_text": "\n \t\tNagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "malmaud", "commentT": "2018-02-08T00:52:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n  should we mark this contributions welcome?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "malmaud", "commentT": "2018-02-08T02:26:46Z", "comment_text": "\n \t\tYes\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "malmaud", "commentT": "2018-03-28T15:29:35Z", "comment_text": "\n \t\tCreated a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/18052>#18052</denchmark-link>\n  to fix this.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "malmaud", "commentT": "2018-12-01T13:58:02Z", "comment_text": "\n \t\tHello, I came up with the same issue when I use tf.import_graph_def:\n AttributeError: 'NoneType' object has no attribute 'outer_context'\n Following the instructions above I switched to import_meta_graph like this:\n `\n def _make_model_and_ops(self, patch_val):\n start = time.time()\n <denchmark-code>    self.image_input_ = tf.placeholder(tf.float32, shape=(None, None, None, 3), name='image_input')\n     saver = tf.train.import_meta_graph(PATH_TO_CKPT + '.meta',\n                                        import_scope=\"detection\",\n                                        input_map={\n                                            'ToFloat_3': self.image_input_\n                                        })\n     saver.restore(self.sess, PATH_TO_CKPT)\n     self.graph = self.sess.graph\n     \n     with self.sess.graph.as_default():\n         tf.set_random_seed(1234)\n         \n         # Tensors are post-fixed with an underscore!\n         #self.image_input_shape_ = tf.placeholder(tf.int32, shape=(1,3), name=\"image_input_shape\")\n         \n         self.eps_ = tf.placeholder(tf.float32, shape=(1), name='eps')\n         \n         # The following commented part is the original code using import_graph_def\n         '''\n         detection_graph_def = tf.GraphDef()\n         with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n             serialized_graph = fid.read()\n             detection_graph_def.ParseFromString(serialized_graph)\n             tf.import_graph_def(detection_graph_def, name='detection',\n                                 input_map={\n                                            'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3': self.image_input_,\n                                            'Preprocessor/map/TensorArrayStack_1/TensorArrayGatherV3': self.image_input_shape_\n                                           })\n         '''\n         \n         # Second-stage Class Loss\n         self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')\n         second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')\n         self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')\n         \n         self.second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),\n                                                                                    logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) \n        \n         # Second-stage bounding boxes\n         self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')\n         \n         grads = tf.gradients(self.second_stage_cls_losses_, [self.image_input_])\n         print(grads)\n         self.unclipped_adv_images_ = self.image_input_ + self.eps_ * tf.sign(grads)\n         self.adv_images_ = tf.clip_by_value(self.unclipped_adv_images_, clip_value_min=0, clip_value_max=255)\n \n \n     elapsed = time.time() - start\n     print(\"Finished loading the model, took {:.0f}s\".format(elapsed))           \n </denchmark-code>\n \n `\n But the problem still exist.\n Does this mean that the information required for tf.gradients does not appear in the check point? If so, what else can I do?\n Thanks, and sorry for my poor English....\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "malmaud", "commentT": "2020-03-14T05:59:09Z", "comment_text": "\n \t\tHaving the same problem as <denchmark-link:https://github.com/ProjectDimlight>@ProjectDimlight</denchmark-link>\n , tried the suggestions above with no luck. Any updates on this?\n \t\t"}}}, "commit": {"commit_id": "1d7c2fa60f717dea7239970d96f7d4bf96842039", "commit_author": "ImSheridan", "commitT": "2018-03-29 11:11:55-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\ops\\control_flow_ops.py", "file_new_name": "tensorflow\\python\\ops\\control_flow_ops.py", "file_complexity": {"file_NLOC": "2123", "file_CCN": "475", "file_NToken": "14996"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "836,837,838", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,forward_ctxt,outer_grad_state", "method_startline": "800", "method_endline": "883", "method_complexity": {"method_NLOC": "59", "method_CCN": "8", "method_NToken": "329", "method_nesting_level": "1"}}}}}}}}