{"BR": {"BR_id": "27455", "BR_author": "LynnHo", "BRopenT": "2019-04-03T09:07:50Z", "BRcloseT": "2019-04-09T21:52:57Z", "BR_text": {"BRsummary": "TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model.", "BRdescription": "\n System information\n \n OS Platform and Distribution: Linux Ubuntu 18.04\n TensorFlow installed from: binary\n TensorFlow version: 2.0.0-alpha0\n Python version: 3.6.8\n \n Describe the current behavior\n I built a keras model with only a tf.nn.relu, but the gradient seems to be None after being decorated by @tf.function\n Code to reproduce the issue\n \n tf.nn.relu + tf.keras.Model + @tf.function (this is the only case that produce None gradient)\n \n import tensorflow as tf\n \n z = tf.keras.Input(())\n h = tf.nn.relu(z)\n m = tf.keras.Model(z, h)\n \n @tf.function\n def f(x):  # with @tf.function\n     with tf.GradientTape() as t:\n         t.watch(x)\n         z = m(x ** 2)\n     return t.gradient(z, x)\n \n print(f(tf.convert_to_tensor(10.0)))\n \n >>> None\n 1.2 tf.nn.relu + tf.keras.Model without @tf.function\n def f(x):  # without @tf.function\n     with tf.GradientTape() as t:\n         t.watch(x)\n         z = m(x ** 2)\n     return t.gradient(z, x)\n \n print(f(tf.convert_to_tensor(10.0)))\n \n >>> tf.Tensor(20.0, shape=(), dtype=float32)\n \n tf.keras.layers.ReLU() + tf.keras.Model + @tf.function\n \n import tensorflow as tf\n \n z = tf.keras.Input(())\n h = tf.keras.layers.ReLU()(z)\n m = tf.keras.Model(z, h)\n \n @tf.function\n def f(x):  # with @tf.function\n     with tf.GradientTape() as t:\n         t.watch(x)\n         z = m(x ** 2)\n     return t.gradient(z, x)\n \n print(f(tf.convert_to_tensor(10.0)))\n \n >>> tf.Tensor(20.0, shape=(), dtype=float32)\n 2.2 tf.keras.layers.ReLU() + tf.keras.Model without @tf.function\n def f(x):  # without @tf.function\n     with tf.GradientTape() as t:\n         t.watch(x)\n         z = m(x ** 2)\n     return t.gradient(z, x)\n \n print(f(tf.convert_to_tensor(10.0)))\n \n >>> tf.Tensor(20.0, shape=(), dtype=float32)\n \n only tf.nn.relu\n \n import tensorflow as tf\n m = tf.nn.relu\n \n @tf.function\n def f(x):  # with @tf.function\n     with tf.GradientTape() as t:\n         t.watch(x)\n         z = m(x ** 2)\n     return t.gradient(z, x)\n \n print(f(tf.convert_to_tensor(10.0)))\n \n >>> tf.Tensor(20.0, shape=(), dtype=float32)\n So, I think its the problem between tf.nn.relu and tf.keras.Model? Besides, tf.nn.tanh has the same problem.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "LynnHo", "commentT": "2019-04-05T21:28:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tomerk>@tomerk</denchmark-link>\n  I think something is broken with the keras graph here since the tape isn't seeing it.\n Can you take a look, or help triage to the right person?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "LynnHo", "commentT": "2019-04-08T23:10:25Z", "comment_text": "\n \t\tI have a fix for this that will be submitted soon.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "LynnHo", "commentT": "2019-04-09T21:52:58Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27455>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27455>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "244cb0b925902a29c6a39c62fd1b80cb3797051b", "commit_author": "A. Unique TensorFlower", "commitT": "2019-04-09 14:47:12-07:00", "commit_complexity": {"commit_NLOC": "0.6785714285714286", "commit_CCN": "0.10714285714285714", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\keras\\engine\\base_layer.py", "file_new_name": "tensorflow\\python\\keras\\engine\\base_layer.py", "file_complexity": {"file_NLOC": "1243", "file_CCN": "302", "file_NToken": "7302"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181", "deleted_lines": null, "method_info": {"method_name": "_make_op", "method_params": "self,inputs", "method_startline": "2154", "method_endline": "2184", "method_complexity": {"method_NLOC": "25", "method_CCN": "6", "method_NToken": "243", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\keras\\layers\\tensorflow_op_layer_test.py", "file_new_name": "tensorflow\\python\\keras\\layers\\tensorflow_op_layer_test.py", "file_complexity": {"file_NLOC": "197", "file_CCN": "27", "file_NToken": "1830"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226", "deleted_lines": null, "method_info": {"method_name": "test_gradient_tape_in_function", "method_params": "self", "method_startline": "205", "method_endline": "226", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "173", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "213,214,215,216,217,218", "deleted_lines": null, "method_info": {"method_name": "test_gradient_tape_in_function.f", "method_params": "x", "method_startline": "213", "method_endline": "218", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "40", "method_nesting_level": "2"}}}}}}}}