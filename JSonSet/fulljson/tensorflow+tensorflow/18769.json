{"BR": {"BR_id": "18769", "BR_author": "rothn", "BRopenT": "2018-04-22T06:03:46Z", "BRcloseT": "2019-05-08T02:17:48Z", "BR_text": {"BRsummary": "InvalidArgumentError for save/restore of variables (same version, same OS, same directory)", "BRdescription": "\n I get an InvalidArgumentError with no further information when I try to save and then restore parts of my model later to continue training it (due to needing my laptop for class).\n Initialization:\n saver = tf.train.Saver({\"embeddings\": embeddings, \"weights\": nce_weights, \"biases\": nce_biases})\n Save:\n saver.save(sess, model_checkpoint_path)\n Load:\n saver.restore(sess, model_checkpoint_path)\n <denchmark-code>2018-04-21 22:45:00.143245: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\n Traceback (most recent call last):\n   File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\n     return fn(*args)\n   File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1312, in _run_fn\n     options, feed_dict, fetch_list, target_list, run_metadata)\n   File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\n     status, run_metadata)\n   File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\n     c_api.TF_GetCode(self.status.status))\n tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\n \t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n \n ... <contains sensitive info> ...\n \n InvalidArgumentError (see above for traceback): /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\n \t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n </denchmark-code>\n \n \n \n Yes, I modified this code (<denchmark-link:https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py>https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py</denchmark-link>\n ) to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce_weights and nce_biases so that training may be resumed.\n \n MacOS 10.13.4 (17E199)\n \n pip on VirtualEnv, according to instructions (<denchmark-link:https://www.tensorflow.org/install/install_mac>https://www.tensorflow.org/install/install_mac</denchmark-link>\n )\n \n 1.7\n \n NA\n \n NA\n \n NA\n \n saver = tf.train.Saver({\"embeddings\": embeddings, \"weights\": nce_weights, \"biases\": nce_biases})\n saver.restore(sess, \"../trained_model/saved_stuff\")\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "rothn", "commentT": "2018-04-22T18:30:04Z", "comment_text": "\n \t\tThank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\n Have I written custom code\n OS Platform and Distribution\n TensorFlow installed from\n TensorFlow version\n Bazel version\n CUDA/cuDNN version\n GPU model and memory\n Exact command to reproduce\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "rothn", "commentT": "2018-04-22T22:26:02Z", "comment_text": "\n \t\tTicket updated with requested information. Will use template in the future!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "rothn", "commentT": "2018-04-25T02:39:06Z", "comment_text": "\n \t\tPossibly related to: <denchmark-link:https://github.com/tensorflow/tensorflow/issues/18640>#18640</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "rothn", "commentT": "2018-04-27T00:36:54Z", "comment_text": "\n \t\tUpdate: I tried this on Windows as well, and I got rid of the stuff for loading the model graph and used the Saver to explicitly restore my variables. On Windows, it fails for the same reason (because I am loading an embedding larger than 2GB) but with a better error message:\n OutOfRangeError (see above for traceback): Read fewer bytes than requested [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "rothn", "commentT": "2018-05-13T00:31:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>\n  bump :)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "rothn", "commentT": "2018-06-06T12:36:16Z", "comment_text": "\n \t\tI am using tensorflow-gpu 1.7.0, and meet exactly the same problem when restoring model in distributed tensorflow environment.\n when the model file is larger than 3GB, the error below occurs:\n InvalidArgumentError (see above for traceback): hdfs://xxxx/model.ckpt-5154361.data-00001-of-00008; Invalid argument\n [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](_recv_save_1/Const_0_S7, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]\n [[Node: save_1/restore_all/NoOp_S10 = _Recv<denchmark-link:>client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:ps/replica:0/task:0/device:GPU:0\", send_device_incarnation=1493071510865629599, tensor_name=\"edge_147_save_1/restore_all/NoOp\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"</denchmark-link>\n ]]\n when I decrease the model size, the error changed:\n tensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested\n [[Node: save_1/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT], _device=\"/job:ps/replica:0/task:1/device:CPU:0\"](_recv_save_1/Const_0_S1, save_1/RestoreV2_3/tensor_names, save_1/RestoreV2_3/shape_and_slices)]]\n List the smaller models:\n -rw-r--r--   3 root supergroup   11893084 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00000-of-00008\n -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00001-of-00008\n -rw-r--r--   3 root supergroup  106483864 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00002-of-00008\n -rw-r--r--   3 root supergroup 5130000440 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.data-00003-of-00008\n -rw-r--r--   3 root supergroup     375688 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00004-of-00008\n -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00005-of-00008\n -rw-r--r--   3 root supergroup  232909600 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00006-of-00008\n -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00007-of-00008\n -rw-r--r--   3 root supergroup       1564 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.index\n -rw-r--r--   3 root supergroup     571996 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.meta\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "rothn", "commentT": "2018-06-08T17:18:28Z", "comment_text": "\n \t\tAny updates, <denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>\n  ?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "rothn", "commentT": "2018-06-11T05:34:43Z", "comment_text": "\n \t\tI think the root cause may be:\n in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/platform/hadoop/hadoop_file_system.cc>https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/platform/hadoop/hadoop_file_system.cc</denchmark-link>\n \n line 213:\n      tSize r = hdfs_->hdfsPread(fs_, file_, static_cast<tOffset>(offset), dst, static_cast<tSize>(n));\n the last param static_cast(n) means the number of bytes required. ie., the tensor's size.\n in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.7/third_party/hadoop/hdfs.h>https://github.com/tensorflow/tensorflow/blob/r1.7/third_party/hadoop/hdfs.h</denchmark-link>\n \n line 75,\n typedef int32_t tSize;    /// size of data for read/write io ops\n tSize is int32, but tensor's size is int64, when the tensor is big enough, overflow occurs.\n My solution is to use partitioned variables for large tensor, then problem solved.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "rothn", "commentT": "2018-06-15T02:06:05Z", "comment_text": "\n \t\tBut I'm not using HDFS -- this is all on my local SSD.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "rothn", "commentT": "2018-07-18T14:14:04Z", "comment_text": "\n \t\tI am having a very similar issue as well. Following <denchmark-link:https://www.tensorflow.org/tutorials/estimators/cnn>this tutorial</denchmark-link>\n , but working with larger images.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "rothn", "commentT": "2018-10-13T03:53:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>\n  seems like we're having issues with large restore? I thought the V2 version of the save restore ops was supposed to fix this. Any ideas?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "rothn", "commentT": "2018-10-15T22:05:30Z", "comment_text": "\n \t\tWe can put >2GB in checkpoints now. <denchmark-link:https://github.com/tensorflow/tensorflow/commit/b8c86c3bbd8271ed968087f24e7fb704103bc733#diff-f4340b63dcfb03e060905682f7471faa>b8c86c3#diff-f4340b63dcfb03e060905682f7471faa</denchmark-link>\n  fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From \"OP_REQUIRES failed at save_restore_v2_ops.cc:184\" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.\n Can someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:\n <denchmark-code>import tensorflow as tf\n from tensorflow.python.ops import io_ops\n \n def main(_):\n   tf.enable_eager_execution()\n   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))\n   path = '/some/network/path'\n   io_ops.save_v2([path], [\"a\"], [\"\"], [large])\n   restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\n   print(tf.reduce_sum(restored))\n \n if __name__ == '__main__':\n   tf.app.run()\n \n </denchmark-code>\n \n Prints:\n tf.Tensor(5e+09, shape=(), dtype=float32)\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "rothn", "commentT": "2018-11-08T13:29:12Z", "comment_text": "\n \t\t\n We can put >2GB in checkpoints now. b8c86c3#diff-f4340b63dcfb03e060905682f7471faa fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From \"OP_REQUIRES failed at save_restore_v2_ops.cc:184\" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.\n Can someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:\n import tensorflow as tf\n from tensorflow.python.ops import io_ops\n \n def main(_):\n   tf.enable_eager_execution()\n   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))\n   path = '/some/network/path'\n   io_ops.save_v2([path], [\"a\"], [\"\"], [large])\n   restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\n   print(tf.reduce_sum(restored))\n \n if __name__ == '__main__':\n   tf.app.run()\n \n Prints:\n tf.Tensor(5e+09, shape=(), dtype=float32)\n \n I've tried this snippet on Mac OS X 10.14.1 + TF 1.12.0\n Got such error:\n \n restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\n 2018-11-08 17:27:05.767089: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument\n Traceback (most recent call last):\n File \"\", line 1, in \n File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1486, in restore_v2\n ctx=_ctx)\n File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1511, in restore_v2_eager_fallback\n attrs=_attrs, ctx=_ctx, name=name)\n File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\n six.raise_from(core._status_to_exception(e.code, message), None)\n File \"\", line 3, in raise_from\n tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument [Op:RestoreV2]\n \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "rothn", "commentT": "2018-11-08T17:21:01Z", "comment_text": "\n \t\tInteresting, thank you for trying that out. So maybe it is a Mac-specific issue.\n Just to check, do you have >20GB of disk and >20GB of RAM free before running that snippet? This could be a badly reported OOM or out-of-disk error I suppose.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "rothn", "commentT": "2018-11-09T11:38:08Z", "comment_text": "\n \t\tI have about 400Gb free space on disk and total 16Gb of RAM (swapfile ON).\n Such issue occurs even if try to load 2.7Gb variable file.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "rothn", "commentT": "2018-11-10T00:32:07Z", "comment_text": "\n \t\tInteresting. I'll find a Mac next week and see if I can reproduce on it.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "rothn", "commentT": "2018-11-22T04:24:27Z", "comment_text": "\n \t\tFinally i found the way how to use save/load embeddings. Variable partitioning is the answer.\n With tf.variable_axis_size_partitioner and max_shard_bytes=2 ** 30 large embeddings loading works well.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "rothn", "commentT": "2019-01-21T05:57:20Z", "comment_text": "\n \t\tThe issue is still there\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "rothn", "commentT": "2019-03-15T00:29:43Z", "comment_text": "\n \t\tProblem still exist when I restore a 18GB model.\n My OS is Windows Server 2016\n Tensorflow 1.13.0 installed from pip\n 512GB memory and hard disk\n Only run on CPU.\n Please advise.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "rothn", "commentT": "2019-03-16T08:21:04Z", "comment_text": "\n \t\tI also put the below in my code\n tf.variable_axis_size_partitioner(\n max_shard_bytes=2**35,\n axis=0,\n bytes_per_string_element=16,\n max_shards=None\n )\n However it still shows below error when restoring the model:\n 2019-03-16 08:12:32.430456: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read fewer bytes than requested\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "rothn", "commentT": "2019-04-05T05:17:47Z", "comment_text": "\n \t\t\n It has been 17 days with no activity and the awaiting response label was assigned. Is this still an issue?\n \n Yes, it is still an issue\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "rothn", "commentT": "2019-05-04T12:41:11Z", "comment_text": "\n \t\tIt has been 29 days with no activity and the awaiting response label was assigned. Is this still an issue?\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "rothn", "commentT": "2019-05-04T17:05:31Z", "comment_text": "\n \t\tYes it\u2019s still an issue\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "rothn", "commentT": "2019-05-08T00:14:27Z", "comment_text": "\n \t\tFinally got a chance to debug this on a mac. Apparently the pread system call, despite taking an eight-byte size_t for its nbytes argument, returns EINVAL if \"The sum of the iov_len values in the iov array overflowed a 32-bit integer.\" And presumably pread is implemented in terms of readv so they have the same limitation.\n I have a change out for review which just limits reads to INT32_MAX on every platform. Seems to work if we do that. I checked that the checkpoints themselves were identical to what gets written on Linux, so existing checkpoints will start working once that change is in.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "rothn", "commentT": "2019-05-08T02:17:49Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=18769>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=18769>No</denchmark-link>\n \n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "rothn", "commentT": "2019-05-22T11:25:04Z", "comment_text": "\n \t\tUh, this is still an issue? I built the latest version of tensorflow from source and it's still happening! I've tried practically everything. One of the most arcane and infuriating errors I have dealt with...\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "rothn", "commentT": "2019-05-22T16:10:54Z", "comment_text": "\n \t\t@Sam-DevZ operating system and version, TF git version, repro instructions? The <denchmark-link:https://github.com/tensorflow/tensorflow/issues/18769#issuecomment-430030868>repro I posted</denchmark-link>\n  works for me on  (nightly) and macOS 10.14.2.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "rothn", "commentT": "2019-05-23T11:18:11Z", "comment_text": "\n \t\tHi! Your snippet doesn't work either. Code:\n import tensorflow as tf\n from tensorflow.python.ops import io_ops\n \n print('Tensorflow version: ' + tf.__version__)\n print('MacOS Version: ' + '10.14.5')\n \n def main(_):\n   tf.enable_eager_execution()\n   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))\n   path = '/Users/clearlycoder/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN'\n   io_ops.save_v2([path], [\"a\"], [\"\"], [large])\n   restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\n   print(tf.reduce_sum(restored))\n \n if __name__ == '__main__':\n   tf.app.run()\n Output:\n <denchmark-code>Tensorflow version: 1.12.2\n MacOS Version: 10.14.5\n 2019-05-23 07:16:10.724624: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument\n Traceback (most recent call last):\n   File \"/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py\", line 16, in <module>\n     tf.app.run()\n   File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n     _sys.exit(main(argv))\n   File \"/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py\", line 12, in main\n     restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\n   File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1486, in restore_v2\n     ctx=_ctx)\n   File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1511, in restore_v2_eager_fallback\n     attrs=_attrs, ctx=_ctx, name=name)\n   File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\n     six.raise_from(core._status_to_exception(e.code, message), None)\n   File \"<string>\", line 3, in raise_from\n tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument [Op:RestoreV2]\n [Finished in 80.3s with exit code 1]\n [cmd: ['/Library/Frameworks/Python.framework/Versions/3.6/bin/python3', '-u', '/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py']]\n [dir: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN]\n [path: /usr/bin:/bin:/usr/sbin:/sbin]\n </denchmark-code>\n \n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "rothn", "commentT": "2019-05-23T11:18:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>\n \n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "rothn", "commentT": "2019-05-23T11:21:02Z", "comment_text": "\n \t\tOh, er, I'm guessing version 0.12.2 is not recent enough. I need 1.14.1-dev20190522, right? Haha.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "rothn", "commentT": "2019-05-23T14:55:50Z", "comment_text": "\n \t\tSorry! It's definitely fixed. I ran:\n <denchmark-code>pip uninstall tensorflow\n pip install tf-nightly\n </denchmark-code>\n \n And now it works perfectly. Sorry about that!\n \t\t"}}}, "commit": {"commit_id": "f2be10a6d278f9a4546fa9cded94074959e67302", "commit_author": "Allen Lavoie", "commitT": "2019-05-07 19:01:17-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\platform\\posix\\posix_file_system.cc", "file_new_name": "tensorflow\\core\\platform\\posix\\posix_file_system.cc", "file_complexity": {"file_NLOC": "321", "file_CCN": "74", "file_NToken": "2045"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "66,67,68,69,70,71,72,73,74,75", "deleted_lines": "65", "method_info": {"method_name": "tensorflow::PosixRandomAccessFile::Read", "method_params": "offset,n,result,scratch", "method_startline": "61", "method_endline": "90", "method_complexity": {"method_NLOC": "27", "method_CCN": "8", "method_NToken": "165", "method_nesting_level": "2"}}}}}}}}