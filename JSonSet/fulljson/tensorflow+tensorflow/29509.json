{"BR": {"BR_id": "29509", "BR_author": "lwu025", "BRopenT": "2019-06-06T18:47:01Z", "BRcloseT": "2020-01-30T22:50:07Z", "BR_text": {"BRsummary": "How to convert a tensorlfow SpaceToBatchND-Conv2D-BatchToSpaceND to a single Conv2D in tflite", "BRdescription": "\n Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\n TensorFlow installed from (source or binary):source\n TensorFlow version (use command below):1.13.1\n Python version: 2.7\n Bazel version (if compiling from source): 0.22.0\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version:\n GPU model and memory:\n \n You can collect some of this information using our environment capture\n <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>\n \n You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: \n \n I'm trying to train my own deeplab model using this <denchmark-link:https://github.com/tensorflow/models/tree/master/research/deeplab>code</denchmark-link>\n  and convert it to tflite.\n My target is to get a model similar to <denchmark-link:https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite>this</denchmark-link>\n \n However, the model is obtained contains operations like:\n <denchmark-link:https://user-images.githubusercontent.com/43549654/59057361-135a7500-884f-11e9-9546-e2bd20e69c95.png></denchmark-link>\n \n SpaceToBatchND and BatchToSpaceND operations are not supported by tflite + opengles backend, they reduced the model's performance on my device.\n In your hosted deeplab model, those three ops are replaced by DEPTHWISE_CONV_2D v2, which has options to set dilation factor. This would be the best solution for me but I'm not sure how to convert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2).\n FYI, I have tried the graph_transforms tool under tensorflow/tools/graph_transforms to flatten the atrous conv. It upsampled the kernels instead of Space_To_Batch + Batch_To_Space. But this transform leads to much more computations that I cannot afford.\n Describe the expected behavior\n convert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2)\n Code to reproduce the issue\n Provide a reproducible test case that is the bare minimum necessary to generate the problem.\n You can try any model under deeplab model zoo for example <denchmark-link:url>http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz</denchmark-link>\n \n Other info / logs\n Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lwu025", "commentT": "2019-09-09T13:21:37Z", "comment_text": "\n \t\tHey, I am currently facing the same problem. One thing that I noticed is when working with quantized model this conversion is being done. Problem is then I end up with a uint8 model.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lwu025", "commentT": "2019-10-10T18:29:03Z", "comment_text": "\n \t\tTOCO has a pass to do this kind of transformation:\n <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc</denchmark-link>\n \n Isn't it working in your case?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lwu025", "commentT": "2020-01-17T10:42:00Z", "comment_text": "\n \t\tI'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lwu025", "commentT": "2020-01-17T17:34:42Z", "comment_text": "\n \t\t\n I'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.\n \n Are you using the old converter or the new MLIR-based converter?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lwu025", "commentT": "2020-01-18T02:59:01Z", "comment_text": "\n \t\tSeems to happen regardless of which converter is chosen. I can ask my client to provide an untrained SavedModel as a repro, if that helps.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "lwu025", "commentT": "2020-01-18T03:11:34Z", "comment_text": "\n \t\tI'm pretty sure it would repro with segmentation specific variant of MobileNet V3, i.e. this one: <denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py>https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py</denchmark-link>\n , using \"large_segmentation\" config: <denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_configs.py#L69>https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_configs.py#L69</denchmark-link>\n .\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "lwu025", "commentT": "2020-01-20T02:33:58Z", "comment_text": "\n \t\tThis is where it bails in my case (in identify_dilated_conv.cc):\n <denchmark-code>120   Operator* bias_add_op = !has_bias_before_bts ? final_op : next_op;\n 121   if (bias_add_op->type != OperatorType::kAdd) {\n 122     // Bias op is required before or after BatchToSpace\n 123     return false;\n 124   }\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "lwu025", "commentT": "2020-01-20T03:21:09Z", "comment_text": "\n \t\tUpon export, a  MobileNet V3 block turns into this:\n <denchmark-link:https://user-images.githubusercontent.com/46361887/72696297-7eac3b00-3af0-11ea-8cf6-d45bfc7e40be.png></denchmark-link>\n \n In its original form, the block is defined as follows: <denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py#L155>https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py#L155</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "lwu025", "commentT": "2020-01-20T03:25:24Z", "comment_text": "\n \t\tIt expects that there will be bias add either before or after BatchToSpaceNd, but it looks like the bias is folded into DepthwiseConv2D. What that Mul is doing there, I don't know.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "lwu025", "commentT": "2020-01-20T04:06:18Z", "comment_text": "\n \t\tMinimal repro:\n #!/usr/bin/env python3\n \n import pathlib\n \n import tensorflow as tf\n from tensorflow import keras\n \n input = keras.Input([128, 128, 3])\n x = keras.layers.Conv2D(8, 5, dilation_rate=2, padding=\"same\", use_bias=False)(input)\n x = keras.layers.BatchNormalization()(x)\n output = keras.layers.ReLU()(x)\n \n m = keras.Model(inputs=input, outputs=output)\n out_dir = pathlib.Path(\"/tmp/minimal_bug_repro\")\n m.save(str(out_dir), save_format=\"tf\")\n \n converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))\n tflite_model = converter.convert()\n output_file = out_dir / \"model.tflite\"\n output_file.write_bytes(tflite_model)\n \n print(f\"Converted model was written to {output_file}\")\n You get the following on the other end:\n <denchmark-link:https://user-images.githubusercontent.com/46361887/72698007-1b71d700-3af7-11ea-9176-bf05f210c17d.png></denchmark-link>\n \n Which is not runnable on the GPU.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "lwu025", "commentT": "2020-01-21T19:36:35Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/depthwise>@depthwise</denchmark-link>\n  for the repro example.\n I'm working on adding support of dilated conv into the MLIR-based converter. I will update this thread when I'm finished.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "lwu025", "commentT": "2020-01-28T23:42:22Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>No</denchmark-link>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "lwu025", "commentT": "2020-01-29T11:56:28Z", "comment_text": "\n \t\tFYI, <denchmark-link:https://github.com/haozha111>@haozha111</denchmark-link>\n , this PR improved the issue, but did not fully fix it. Here's an updated repro which demonstrates the remaining issue:\n <denchmark-code>#!/usr/bin/env python3\n \n import pathlib\n \n import tensorflow as tf\n from tensorflow import keras\n \n input = keras.Input([128, 128, 3])\n x1 = keras.layers.Conv2D(8, 5, dilation_rate=6, padding=\"same\", use_bias=False)(input)\n x1 = keras.layers.BatchNormalization()(x1)\n output1 = keras.layers.ReLU()(x1)\n \n x2 = keras.layers.Conv2D(8, 5, dilation_rate=12, padding=\"same\", use_bias=False)(input)\n x2 = keras.layers.BatchNormalization()(x2)\n output1 = keras.layers.ReLU()(x2)\n \n output = tf.concat([x1, x2], axis=3)\n \n m = keras.Model(inputs=input, outputs=output)\n out_dir = pathlib.Path(\"/tmp/minimal_bug_repro\")\n m.save(str(out_dir), save_format=\"tf\")\n \n converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))\n tflite_model = converter.convert()\n output_file = out_dir / \"model.tflite\"\n output_file.write_bytes(tflite_model)\n \n print(f\"Converted model was written to {output_file}\")\n </denchmark-code>\n \n This produces the following:\n <denchmark-link:https://user-images.githubusercontent.com/46361887/73354717-3bb63a00-424b-11ea-9767-5762e6863f05.png></denchmark-link>\n \n Each of the convs in isolation is fixed, but if I concatenate them we're back to the status quo ante.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "lwu025", "commentT": "2020-01-29T11:57:07Z", "comment_text": "\n \t\tSuch dilations are commonly used in the ASPP module of segmentation models.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "lwu025", "commentT": "2020-01-29T22:53:45Z", "comment_text": "\n \t\tThat's a bit interesting. I haven't tested for this case.\n So do you mean if you have only one conv2d in your graph, then it can be correctly folded?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "lwu025", "commentT": "2020-01-29T22:58:27Z", "comment_text": "\n \t\tLooks like conv2d's by themselves work fine. The simpler repro I posted before looks \"correct\", although I have not tested this in a full blown, trained model yet.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "lwu025", "commentT": "2020-01-30T00:22:24Z", "comment_text": "\n \t\tI tested with your new code, and convert it. Then I'm getting the tflite graph looks like the following:\n <denchmark-link:https://user-images.githubusercontent.com/6316921/73408979-bbc3ba80-42b2-11ea-91ab-46f8a05219f5.png></denchmark-link>\n \n I also have the corresponding tflite file, but not sure how to attach it here.\n This suggests that the graph is converted as expected. I'm wondering if my previous change has been pushed into the nightly already. Maybe you can download tomorrow's nightly and give a try.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "lwu025", "commentT": "2020-01-30T00:25:48Z", "comment_text": "\n \t\tOK, I'll try again. My example was converted using a build directly from master as of last night. Maybe something else got submitted in the interim.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "lwu025", "commentT": "2020-01-30T00:30:43Z", "comment_text": "\n \t\tdid you set converter.experimental_new_converter = True between those two lines?\n converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir)) tflite_model = converter.convert()\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "lwu025", "commentT": "2020-01-30T04:40:35Z", "comment_text": "\n \t\tI'm actually converting as follows, using the master branch as of this afternoon:\n bazel run --copt=\"-Wno-unused-result\" :tflite_convert -- \\\n     --saved_model_dir=/tmp/minimal_bug_repro \\\n     --experimental_new_converter=True --output_file=/tmp/minimal_bug_repro/model.tflite\n The original savedmodel is created with TF 2.1.0 release version, the conversion is done with master tflite_convert.\n Here's the original model and its conversion, in case it's useful in debugging: <denchmark-link:https://storage.googleapis.com/depthwise-temp/minimal_bug_repro.zip>https://storage.googleapis.com/depthwise-temp/minimal_bug_repro.zip</denchmark-link>\n \n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "lwu025", "commentT": "2020-01-30T04:41:31Z", "comment_text": "\n \t\tFor me the graph looks like the image I posted previously, incorrect.\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "lwu025", "commentT": "2020-01-30T19:46:44Z", "comment_text": "\n \t\tI could reproduce the issue with your saved model, but the graph I posted before is from the keras testing code. I'm not sure why there is a difference here.\n Digging it further, I found that there is a difference in the MLIR representation of the two models. For saved model, the IR looks like:\n ...\n %10 = \"tf.Identity\"(%arg0) {T = f32, device = \"\"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>\n %11 = \"tf.Identity\"(%10) {T = f32, device = \"\"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>\n %12 = \"tf.SpaceToBatchND\"(%11, %0, %1) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<36x26x26x3xf32>\n %13 = \"tf.SpaceToBatchND\"(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>\n %14 = \"tf.Conv2D\"(%12, %8) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>\n %15 = \"tf.BatchToSpaceND\"(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\n %y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = \"tf.FusedBatchNormV3\"(%15, %6, %7, %7, %6) {T = f32, U = f32, data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<*xf32>)\n %16 = \"tf.Conv2D\"(%13, %9) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<144x15x15x3xf32>, tensor<5x5x3x8xf32>) -> tensor<144x11x11x8xf32>\n %17 = \"tf.BatchToSpaceND\"(%16, %3, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\n %18 = \"tf.Mul\"(%17, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\n %19 = \"tf.Add\"(%18, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\n %20 = \"tf.ConcatV2\"(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = \"\"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor) -> tensor<1x128x128x16xf32>\n return %20 : tensor<1x128x128x16xf32>\n }<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\n %18 = \"tf.Mul\"(%17, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\n %19 = \"tf.Add\"(%18, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\n %20 = \"tf.ConcatV2\"(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = \"\"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor) -> tensor<1x128x128x16xf32>\n return %20 : tensor<1x128x128x16xf32>\n }\n For the keras test model, the IR is:\n %7 = \"tf.Const\"() {value = dense<12> : tensor<2xi32>} : () -> tensor<2xi32>\n %8 = \"tf.Const\"() {value = dense<[[24, 28], [24, 28]]> : tensor<2x2xi32>} : () -> tensor<2x2xi32>\n %9 = \"tf.Const\"() {value = dense<3> : tensor} : () -> tensor\n %10 = \"tf.Identity\"(%arg0) {T = f32, device = \"\"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>\n %11 = \"tf.SpaceToBatchND\"(%10, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<36x26x26x3xf32>\n %12 = \"tf.Conv2D\"(%11, %2) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>\n %13 = \"tf.BatchToSpaceND\"(%12, %3, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\n %y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = \"tf.FusedBatchNormV3\"(%13, %0, %1, %1, %0) {T = f32, U = f32, data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<*xf32>)\n %14 = \"tf.SpaceToBatchND\"(%10, %7, %8) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>\n %15 = \"tf.Conv2D\"(%14, %6) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<144x15x15x3xf32>, tensor<5x5x3x8xf32>) -> tensor<144x11x11x8xf32>\n %16 = \"tf.BatchToSpaceND\"(%15, %7, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\n %17 = \"tf.Mul\"(%16, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\n %18 = \"tf.Add\"(%17, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\n %19 = \"tf.ConcatV2\"(%y, %18, %9) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = \"\"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor) -> tensor<1x128x128x16xf32>\n return %19 : tensor<1x128x128x16xf32>\n I think the issue is that the IR for saved model is a bit strange, notice those lines:\n %13 = \"tf.SpaceToBatchND\"(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>\n %14 = \"tf.Conv2D\"(%12, %8) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>\n %15 = \"tf.BatchToSpaceND\"(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\n The block_shape parameter before/after the Conv2D op is different, which causes the pattern to not match successfully. Will investigate why the IR is different.\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "lwu025", "commentT": "2020-01-30T20:01:20Z", "comment_text": "\n \t\tHi, Nupur, do you know what might be the cause for the difference between calling the python API and the command line tool?\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "lwu025", "commentT": "2020-01-30T20:34:06Z", "comment_text": "\n \t\tI also tried to convert the model via command line 'tflite_convert' directly (no bazel) in tf-nightly, and the model could be converted correctly. I think the issue is probably with bazel, it might be calling the TF 1 saved model loader which doesn't produce the expected result.\n Can you please try using tf-nightly and then run 'tflite_conveter' tool directly (please avoid using bazel)?\n Thanks.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "lwu025", "commentT": "2020-01-30T20:36:48Z", "comment_text": "\n \t\tJust to be clear, once again, the SavedModel I posted was saved using TF 2.1.0, not the new \"master\" build. It's only the converter that was built from master. Let me try to replicate this with a wheel of TF build off master to see if the model is being saved incorrectly in the first place. Build times being what they are, this will take a bit of time.\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "lwu025", "commentT": "2020-01-30T20:44:35Z", "comment_text": "\n \t\tI think the saved model is generated correctly, it doesn't matter if you generate it from TF 2.1.0 or the master build.\n The issue is with the converter. When you build from master, and then run bazel, even if it's pulling the latest code, I think there is some mis-configuration in bazel that causes the model loading to incorrectly use the old TF 1.x codepath. So what I'm suggesting maybe the easiest approach is to download the tf-nightly, and then call 'tflite_convert' tool directly (in a virtualenv).\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "lwu025", "commentT": "2020-01-30T22:29:04Z", "comment_text": "\n \t\tNightly as of last night LGTM, in both my full deeplab model and the minimal example. Thanks for the quick fix, much appreciated. This was blocking things in unpleasant ways.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "lwu025", "commentT": "2020-01-30T22:50:07Z", "comment_text": "\n \t\tGreat to hear that!\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "lwu025", "commentT": "2020-01-30T22:50:09Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "f54bb6f5578b931d79884302768996ba1073f685", "commit_author": "Haoliang Zhang", "commitT": "2020-01-28 15:32:46-08:00", "commit_complexity": {"commit_NLOC": "0.07272727272727272", "commit_CCN": "0.09090909090909091", "commit_Nprams": "0.8363636363636363"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\compiler\\mlir\\lite\\BUILD", "file_new_name": "tensorflow\\compiler\\mlir\\lite\\BUILD", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "270,286", "deleted_lines": null}}}, "file_1": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tensorflow\\compiler\\mlir\\lite\\tests\\dilated-conv.mlir", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tensorflow\\compiler\\mlir\\lite\\transforms\\dilated_conv.cc", "file_complexity": {"file_NLOC": "21", "file_CCN": "1", "file_NToken": "92"}}, "file_3": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tensorflow\\compiler\\mlir\\lite\\transforms\\dilated_conv.h", "file_complexity": {"file_NLOC": "127", "file_CCN": "28", "file_NToken": "1127"}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\compiler\\mlir\\lite\\transforms\\prepare_tf.cc", "file_new_name": "tensorflow\\compiler\\mlir\\lite\\transforms\\prepare_tf.cc", "file_complexity": {"file_NLOC": "314", "file_CCN": "41", "file_NToken": "2467"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "507,508,509,510,511,512", "deleted_lines": null, "method_info": {"method_name": "mlir::TFL::PrepareTFPass::runOnFunction", "method_params": "", "method_startline": "497", "method_endline": "532", "method_complexity": {"method_NLOC": "19", "method_CCN": "2", "method_NToken": "142", "method_nesting_level": "3"}}}}}}}}