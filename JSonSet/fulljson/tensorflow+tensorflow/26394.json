{"BR": {"BR_id": "26394", "BR_author": "kpe", "BRopenT": "2019-03-06T12:08:17Z", "BRcloseT": "2019-03-18T12:05:58Z", "BR_text": {"BRsummary": "Allow building TF + nvidia GPU targeting &lt; sm35 if XLA is not enabled", "BRdescription": "\n Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo\n TensorFlow installed from (source or binary): source\n TensorFlow version: 1.13.1\n Python version: 3.6.4\n Installed using virtualenv? pip? conda?: pip in venv\n Bazel version (if compiling from source): 0.21\n GCC/Compiler version (if compiling from source): 7.4.0\n CUDA/cuDNN version: 10.0\n GPU model and memory: GTX 650 Ti\n \n I was able to successfully build TF from source with XLA enabled and compute capability 3.0.\n However, when a session is created the python interpreter exits (complaining about insufficient compute capability):\n <denchmark-code>>>> import tensorflow as tf\n >>> tf.Session()\n 2019-03-06 12:49:41.776396: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500130000 Hz\n 2019-03-06 12:49:41.776727: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556553ef5ee0 executing computations on platform Host. Devices:\n 2019-03-06 12:49:41.776741: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n 2019-03-06 12:49:41.809556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n 2019-03-06 12:49:41.810593: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\n 2019-03-06 12:49:41.810666: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\n \n </denchmark-code>\n \n if I reconfigure TF by disabling XLA, and rebuild (again with compute capability 3.0), than TF works fine.\n So I guess, a simple check if compute capability >= 3.5 when XLA is enabled, could at least prevent building non-functional TF.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kpe", "commentT": "2019-03-06T21:28:06Z", "comment_text": "\n \t\tHey, a similar issue was filed a while ago, and a fix has been deployed <denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767>here</denchmark-link>\n . Would be available in the master branch/the next release.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kpe", "commentT": "2019-03-07T10:52:59Z", "comment_text": "\n \t\tI actually found similar issues, but non of them was XLA related. And indeed as you pointed out (<denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767>#25767</denchmark-link>\n ) the config script does no longer allow building TF with compute capability bellow 3.5. Even it looks like if XLA is disabled TF could still be built and function properly with capability 3.0.\n also just for referece - <denchmark-link:https://github.com/tensorflow/tensorflow/issues/24126>#24126</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kpe", "commentT": "2019-03-08T22:46:01Z", "comment_text": "\n \t\t\n as you pointed out (#25767) the config script does no longer allow building TF with compute capability bellow 3.5.\n \n If this is the case, I'm not sure what is the bug here?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "kpe", "commentT": "2019-03-09T00:24:21Z", "comment_text": "\n \t\tThe \"bug\" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.\n What leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "kpe", "commentT": "2019-03-09T00:30:26Z", "comment_text": "\n \t\t\n The \"bug\" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.\n What leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.\n \n Sorry, I'm still confused.\n In the second sentence, I'm understanding that TF built without XLA and run on sm30 does work?  But in the first sentence I'm understanding that you can't build TF with XLA disabled targeting sm30?  Those seem at odds.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "kpe", "commentT": "2019-03-09T00:45:42Z", "comment_text": "\n \t\tWell, the first sentence is - you can build it if you patch configure.py to accept compute capability 3.0 (with disabled XLA). And in sentence two, there are two statements - a) XLA and 3.0 = broken-TF, and b) disabled XLA and 3.0 = working-TF.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "kpe", "commentT": "2019-03-09T00:50:50Z", "comment_text": "\n \t\t....I mean, I had to patch config.py in order to build an TF version for my old GPU (compute capability 3.0).\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "kpe", "commentT": "2019-03-09T00:53:21Z", "comment_text": "\n \t\tOK, so what you'd like is for the TF build team to change config.py so that it doesn't require sm35, if XLA is disabled?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "kpe", "commentT": "2019-03-09T00:59:38Z", "comment_text": "\n \t\tYes, this is what the issue was about. (edit: at least after I learned there was a  >= 3.5 check on the main branch already, preventing TF from being built with 3.0)\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "kpe", "commentT": "2019-03-09T01:04:20Z", "comment_text": "\n \t\tIt may be the case that TF build team has decided that they do not support < sm35 at all, even though you observed that it happened to work in your particular case.  Over to them, this is not really an XLA question.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "kpe", "commentT": "2019-03-09T01:07:26Z", "comment_text": "\n \t\tYes, sorry for the confusion, it was really about, a better config sanity check.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "kpe", "commentT": "2019-03-10T21:21:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/chsigg>@chsigg</denchmark-link>\n  may be able to confirm or deny, but here is what I can say:\n Below compute capability 3.0 TF will NOT work. XLA, or no xla, TF needs features in compute capability 3.0\n Below 3.5 is best effort, we do not test it, or even build it.  It may work, may not work, you are free to use at your own risk.\n Above 3.5 will work.\n 6.0 is the most heavily tested one for us, and any issues we see for 6.0 or newer is promptly triaged.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "kpe", "commentT": "2019-03-11T09:51:37Z", "comment_text": "\n \t\tWe can probably change <denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767>#25767</denchmark-link>\n  to issue an error for compute capability < 3.0 (down from currently < 3.5).\n For compute capability < 3.5, we can issue a warning that XLA is not supported. XLA generates CUDA code for whatever GPU is present at runtime, so anything more than a warning during configuration seems prohibitive.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "kpe", "commentT": "2019-03-11T14:07:56Z", "comment_text": "\n \t\tI can submit another PR if it is required, but I would appreciate it if you could specify the exact information that should be conveyed in the warning. The original PR did issue a warning and not an error. It was changed because it seemed like TensorFlow would surely fail to work below 3.5 (which is not really the case maybe?).\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "kpe", "commentT": "2019-03-18T12:05:58Z", "comment_text": "\n \t\tThe compute capability check is now a warning for < 3.5, and and error for < 3.0. Closing.\n \t\t"}}}, "commit": {"commit_id": "8dc2d0eedac7760deb65254a8ef89878743299d7", "commit_author": "A. Unique TensorFlower", "commitT": "2019-03-12 01:46:54-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "configure.py", "file_new_name": "configure.py", "file_complexity": {"file_NLOC": "1195", "file_CCN": "269", "file_NToken": "6682"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1308,1309,1313,1314,1315", "deleted_lines": "1308,1309", "method_info": {"method_name": "set_tf_cuda_compute_capabilities", "method_params": "environ_cp", "method_startline": "1271", "method_endline": "1326", "method_complexity": {"method_NLOC": "46", "method_CCN": "8", "method_NToken": "168", "method_nesting_level": "0"}}}}}}}}