{"BR": {"BR_id": "35100", "BR_author": "olk", "BRopenT": "2019-12-13T22:06:00Z", "BRcloseT": "2019-12-20T00:22:24Z", "BR_text": {"BRsummary": "Error occurred when finalizing GeneratorDataset iterator", "BRdescription": "\n System information\n \n OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH\n TensorFlow installed from: binary\n TensorFlow version: 2.1.0rc0-1\n Keras version: 2.2.4-tf\n Python version: 3.8\n GPU model and memory: 2x GTX 1080 Ti 11GB\"`\n \n Describe the current behavior\n executing Tensorflow's MNIST handwriting example produces error:\n the error dissapears if the code doesn't use OneDeviceStrategy or MirroredStrategy\n \n W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n \n Code to reproduce the issue\n <denchmark-code>import tensorflow as tf\n  import tensorflow_datasets as tfds\n  import time\n  \n  from tensorflow.keras.optimizers import Adam\n  \n  def build_model():\n      filters = 48\n      units = 24\n      kernel_size = 7\n      learning_rate = 1e-4\n      model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\n        tf.keras.layers.MaxPooling2D(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n      ])\n      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n      return model\n  \n  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n  mnist_train, mnist_test = datasets['train'], datasets['test']\n  \n  num_train_examples = info.splits['train'].num_examples\n  num_test_examples = info.splits['test'].num_examples\n  \n  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n  \n  BUFFER_SIZE = 10000\n  BATCH_SIZE = 32\n  \n  def scale(image, label):\n    image = tf.cast(image, tf.float32)\n    image /= 255\n    return image, label\n  \n  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n  \n  with strategy.scope():\n    model = build_model()\n  \n  epochs=5\n  start = time.perf_counter()\n  model.fit(\n          train_dataset,\n          validation_data=eval_dataset,\n          steps_per_epoch=num_train_examples/epochs,\n          validation_steps=num_test_examples/epochs,\n          epochs=epochs)\n  elapsed = time.perf_counter() - start\n  print('elapsed: {:0.3f}'.format(elapsed))\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "olk", "commentT": "2019-12-16T08:20:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/olk>@olk</denchmark-link>\n , I tried reproducing the reported issue but it worked as expected. Please take a look at the <denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/0f769c920b16c68d0b2c7d238256e0c9/untitled311.ipynb>gist</denchmark-link>\n . Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "olk", "commentT": "2019-12-16T22:04:34Z", "comment_text": "\n \t\tupgraded to TensorFlow version: 2.1.0-rc1 - still get errors\n please note that I execute the example at real hardware (not colab)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "olk", "commentT": "2019-12-17T14:33:12Z", "comment_text": "\n \t\tI guess this issue is related to using Tensorflow with Python-3.8.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "olk", "commentT": "2019-12-17T18:30:53Z", "comment_text": "\n \t\tI've downgraded my system:\n \n Python 3.7.4\n Tensorflow-2.1.0-rc1\n \n Still facing the error:\n \n Train for 30000.0 steps, validate for 5000.0 steps\n Epoch 1/2\n 2019-12-17 19:21:54.361240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n 2019-12-17 19:21:55.824790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n 2019-12-17 19:21:56.980785: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\n Relying on driver to perform ptx compilation. This message will be only logged once.\n 30000/30000 [==============================] - 115s 4ms/step - loss: 0.0856 - accuracy: 0.9761 - val_loss: 0.0376 - val_accuracy: 0.9879\n Epoch 2/2\n 29990/30000 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.99582019-12-17 19:25:28.372294: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n 30000/30000 [==============================] - 111s 4ms/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 0.0375 - val_accuracy: 0.9889\n 2019-12-17 19:25:40.010887: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n 2019-12-17 19:25:40.031138: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n elapsed: 226.391\n \n seams to be related to tensorflow-2.1.0-rc1\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "olk", "commentT": "2019-12-17T23:18:41Z", "comment_text": "\n \t\tI have the same issue.  Originally I was using:\n tensorflow/tensorflow:nightly-gpu-py3\n which has:\n 2.1.0-dev20191106\n Then I tried upgrading tensorflow in the container with:\n <denchmark-link:https://files.pythonhosted.org/packages/a9/fa/8ac34cf1369deb4f523a80eeb86ec0be3dd44139bfb42c45dd3829d6aff5/tf_nightly_gpu-2.1.0.dev20191217-cp36-cp36m-manylinux2010_x86_64.whl>https://files.pythonhosted.org/packages/a9/fa/8ac34cf1369deb4f523a80eeb86ec0be3dd44139bfb42c45dd3829d6aff5/tf_nightly_gpu-2.1.0.dev20191217-cp36-cp36m-manylinux2010_x86_64.whl</denchmark-link>\n \n I still have the same issue.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "olk", "commentT": "2019-12-18T00:12:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/guptapriya>@guptapriya</denchmark-link>\n  <denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>\n  this seems to be an issue related to tf.distribute + tf.keras. In particular, as far as I can tell, the user code does not use  but the error indicates that GeneratorDataset is used. Could you please triage? Thanks.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "olk", "commentT": "2019-12-18T00:36:51Z", "comment_text": "\n \t\tThe error log suggests that the training completed fine, but something at the end caused this error. Neither the training or validation dataset are using generators, so it does seem weird that there is a generator related error. Also it seems like it's just a warning - since the user's print statement at the end \"elapsed..\" did get printed as well.\n <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n  is tf.data.Dataset.from_generator the only time generator_dataset_op is used? Or could there be something else that could trigger it?\n <denchmark-link:https://github.com/rchao>@rchao</denchmark-link>\n  could it be something related to any of the fault tolerance callbacks?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "olk", "commentT": "2019-12-18T15:18:07Z", "comment_text": "\n \t\tI can verify this error with python 3.8 and python-tensorflow-opt-cuda 2.1.0rc1-2 on arch linux.  This error is weirdly not present if you import only the generator from tensorflow, and everything else from Keras.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "olk", "commentT": "2019-12-19T01:16:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/guptapriya>@guptapriya</denchmark-link>\n  I realized that generator dataset is used in multi-device iterator. This seems related to newly added support for cancellation in tf.data.\n The good news is that, as you pointed out, the warning is superfluous. The bad news is that, as far as I can tell, this warning will be present for all tf.distribute jobs in TF 2.1 (given how tf.data cancellation is implemented). I will look into having a fix for this cherrypicked into TF 2.1.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "olk", "commentT": "2019-12-19T01:21:35Z", "comment_text": "\n \t\tAh, great, thanks <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n .\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "olk", "commentT": "2019-12-20T00:22:26Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100>No</denchmark-link>\n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "olk", "commentT": "2020-02-12T02:54:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n  Any update on this? I'm getting this exact message and it looks like my model.fit is not doing it's thing on validation dataset during training.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "olk", "commentT": "2020-02-13T22:40:16Z", "comment_text": "\n \t\tI'm also experiencing this on the official Google Cloud Platform tf2-gpu.2-1.m42 image with Python 3.5.3.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "olk", "commentT": "2020-03-05T00:47:14Z", "comment_text": "\n \t\tIt might help to point out that this error is being printed out once per active GPU.\n 2020-03-05 00:40:55.703069: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "olk", "commentT": "2020-03-08T18:58:20Z", "comment_text": "\n \t\tProblem description\n I am using TensorFlow 2.1.0 for image classification under Centos Linux. As my image training data set is growing, I have to start using a Generator as I do not have enough RAM to hold all pictures. I have coded the Generator based on this <denchmark-link:https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly>tutorial</denchmark-link>\n .\n It seems to work fine, until my program all the sudden gets killed without an error message:\n <denchmark-code>Epoch 6/30\n 2020-03-08 13:28:11.361785: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n 43/43 [==============================] - 54s 1s/step - loss: 5.6839 - accuracy: 0.4669\n Epoch 7/30\n 2020-03-08 13:29:05.511813: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n  7/43 [===>..........................] - ETA: 1:04 - loss: 4.3953 - accuracy: 0.5268Killed\n </denchmark-code>\n \n Looking at the growing memory consumption with linux's top, I suspect a memory leak?\n What I have tried\n \n \n The above suggestion to switch to TF nightly build version. For me it did not help, also downgrading to TF2.0.1 did not help\n \n \n There is a discussion  suggesting that it is important, that 'steps_per_epoch' and 'batch size' correspond (whatever this exactly means) - I played with it without finding any improvement.\n \n \n Trying to narrow down by looking at the size development of all variables in my Generator\n \n \n Relevant code snippets\n <denchmark-code>class DataGenerator(tf.keras.utils.Sequence):\n     'Generates data for Keras'\n     def __init__(self, list_IDs, labels, dir, n_classes):\n         'Initialization'\n         config = configparser.ConfigParser()\n         config.sections()\n         config.read('config.ini')\n \n         self.dim = (int(config['Basics']['PicHeight']),int(config['Basics']['PicWidth']))\n         self.batch_size = int(config['HyperParameter']['batchsize'])\n         self.labels = labels\n         self.list_IDs = list_IDs\n         self.dir = dir\n         self.n_channels = 3\n         self.n_classes = n_classes\n         self.on_epoch_end()        \n \n \n     def __len__(self):\n         'Denotes the number of batches per epoch'\n         return math.floor(len(self.list_IDs) / self.batch_size)\n \n     def __getitem__(self, index):\n         'Generate one batch of data'\n         # Generate indexes of the batch\n         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n \n         # Find list of IDs\n         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n \n         # Generate data\n         X, y = self.__data_generation(list_IDs_temp)\n \n         return X, y, [None]\n </denchmark-code>\n \n being called by\n <denchmark-code>        training_generator = datagenerator.DataGenerator(train_files, labels, dir, len(self.class_names))\n         self.model.fit(x=training_generator,\n                     use_multiprocessing=False,\n                     workers=6, \n                     epochs=self._Epochs, \n                     steps_per_epoch = len(training_generator),\n                     callbacks=[LoggingCallback(self.logger.debug)])\n </denchmark-code>\n \n I have tried running the exact same code under Windows 10, which gives me the following error:\n <denchmark-code>Epoch 9/30\n 2020-03-08 20:49:37.555692: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n 41/41 [==============================] - 75s 2s/step - loss: 2.0167 - accuracy: 0.3133\n Epoch 10/30\n 2020-03-08 20:50:52.986306: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n  1/41 [..............................] - ETA: 2:36 - loss: 1.6237 - accuracy: 0.39062020-03-08 20:50:57.689373: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at matmul_op.cc:480 : Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n 2020-03-08 20:50:57.766163: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n          [[{{node MatMul_6}}]]\n Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n \n  2/41 [>.............................] - ETA: 2:02 - loss: 1.6237 - accuracy: 0.3906Traceback (most recent call last):\n   File \"run.py\", line 83, in <module>\n     main()\n   File \"run.py\", line 70, in main\n     accuracy, num_of_classes = train_Posture(unique_name)\n   File \"run.py\", line 31, in train_Posture\n     acc = neuro.train(picdb, train_ids, test_ids, \"Posture\")\n   File \"A:\\200307 3rd Try\\neuro.py\", line 161, in train\n     callbacks=[LoggingCallback(self.logger.debug)])\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\n     use_multiprocessing=use_multiprocessing)\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\n     total_epochs=epochs)\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\n     batch_outs = execution_function(iterator)\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\n     distributed_function(input_fn))\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\n     result = self._call(*args, **kwds)\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\n     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\n     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\n     self.captured_inputs)\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\n     ctx, args, cancellation_manager=cancellation_manager))\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\n     ctx=ctx)\n   File \"C:\\Users\\Frank\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\n     six.raise_from(core._status_to_exception(e.code, message), None)\n   File \"<string>\", line 3, in raise_from\n tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n          [[node MatMul_6 (defined at A:\\200307 3rd Try\\neuro.py:161) ]]\n Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n  [Op:__inference_distributed_function_764]\n \n Function call stack:\n distributed_function\n \n 2020-03-08 20:51:00.785175: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n </denchmark-code>\n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "olk", "commentT": "2020-03-09T21:03:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Tuxius>@Tuxius</denchmark-link>\n  I seem have the same issue. Should this be reopened? Why is it closed anyway?\n On my side it seems to happen when early stoppping triggers. So it does not cancel the training.\n And I get no OOM message.\n <denchmark-code>156/156 [==============================] - 86s 550ms/step - loss: 0.0676 - acc: 0.9790 - val_loss: 0.7805 - val_acc: 0.8569\n Epoch 17/1000\n 156/156 [==============================] - 86s 550ms/step - loss: 0.0711 - acc: 0.9748 - val_loss: 0.4852 - val_acc: 0.8875\n Epoch 18/1000\n 156/156 [==============================] - 86s 550ms/step - loss: 0.0638 - acc: 0.9772 - val_loss: 1.1247 - val_acc: 0.8371\n 2020-03-09 20:41:21.425818: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n [0.8876654] [5]\n WARNING:tensorflow:sample_weight modes were coerced from\n   {'output': '...'}\n     to\n   ['...']\n Train for 156 steps, validate on 11715 samples\n Epoch 1/1000\n 156/156 [==============================] - 88s 566ms/step - loss: 0.4006 - acc: 0.8377 - val_loss: 1.3430 - val_acc: 0.5214\n Epoch 2/1000\n 156/156 [==============================] - 86s 550ms/step - loss: 0.1554 - acc: 0.9437 - val_loss: 0.7877 - val_acc: 0.8219\n Epoch 3/1000\n </denchmark-code>\n \n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "olk", "commentT": "2020-03-10T12:35:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Tuxius>@Tuxius</denchmark-link>\n  <denchmark-link:https://github.com/PhilipMay>@PhilipMay</denchmark-link>\n  <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n  the same problem here\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "olk", "commentT": "2020-03-11T20:33:57Z", "comment_text": "\n \t\tSince more and more people (<denchmark-link:https://github.com/PhilipMay>@PhilipMay</denchmark-link>\n , <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n , <denchmark-link:https://github.com/drsantos89>@drsantos89</denchmark-link>\n , <denchmark-link:https://github.com/4doge>@4doge</denchmark-link>\n , <denchmark-link:https://github.com/Tuxius>@Tuxius</denchmark-link>\n , ...) report to also still have the same issue I have reopened it <denchmark-link:https://github.com/tensorflow/tensorflow/issues/37515>#37515</denchmark-link>\n \n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "olk", "commentT": "2020-03-17T21:38:46Z", "comment_text": "\n \t\tI have the same problem (using fit_generator)\n I'm using windows 10, python 3.6 and tensorflow 2.1 (cuda 10.1, cudnn 7.6.5)\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "olk", "commentT": "2020-03-24T07:57:02Z", "comment_text": "\n \t\tsame issue here, with CPU, tf 2.1 and python 3.7\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "olk", "commentT": "2020-03-24T18:10:43Z", "comment_text": "\n \t\tsame issue here, scales with validation_freq (occurs on those epochs for which validation is performed.)\n I'm using the tensorflow image tensorflow/tensorflow:2.1.0-gpu-py3 from docker hub: <denchmark-link:https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1>https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1</denchmark-link>\n \n Within the image, the system is 18.04.3 LTS, cuda 10.1.243, python Python 3.6.9\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "olk", "commentT": "2020-03-25T15:03:18Z", "comment_text": "\n \t\tI'm using keras Sequence and saw this issue too. However it's weird if I only use one worker, it works well. I didn't use multiprocessing tho. 2 worker thread will leave the training hang and data show this error\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "olk", "commentT": "2020-03-25T15:55:33Z", "comment_text": "\n \t\tThere was a bug for Keras sequence multi-processing implementation that was fixed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/e918c6e6fab5d0005fcde83d57e92b70343d3553>e918c6e</denchmark-link>\n . This will be available in TF 2.2 and should be already available in TF nightly.\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "olk", "commentT": "2020-03-26T03:35:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>\n  thanks for the heads up! I updated to tf-nightly-gpu and the error:\n  went away.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "olk", "commentT": "2020-03-26T15:38:54Z", "comment_text": "\n \t\tGood to know. Note that the warning and the memory leak are unrelated.\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "olk", "commentT": "2020-03-28T19:01:37Z", "comment_text": "\n \t\t\n @jsimsa thanks for the heads up! I updated to tf-nightly-gpu and the error:\n W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled went away.\n \n I updated today. I confirm that.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "olk", "commentT": "2020-03-28T19:04:26Z", "comment_text": "\n \t\tbut the training result   seems not stable.   train/val  loss/accuracy up and downs too much.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "olk", "commentT": "2020-03-28T23:53:47Z", "comment_text": "\n \t\tFor me the results aren't reproducible from run to run either, even with\n tf.random.set_seed(), but I suspect it has to do with multiple workers for\n my image augmentation generator.\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Sat, Mar 28, 2020, 12:04 PM flydragon2018 ***@***.***> wrote:\n  but the training result seems not stable. train/val loss/accuracy up and\n  downs too much.\n \n  \u2014\n  You are receiving this because you commented.\n  Reply to this email directly, view it on GitHub\n  <#35100 (comment)>,\n  or unsubscribe\n  <https://github.com/notifications/unsubscribe-auth/ABI2DNKEM2P6L6YZLNMF3FLRJZC4XANCNFSM4J2WWO2A>\n  .\n \n \n \n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "olk", "commentT": "2020-06-12T15:55:38Z", "comment_text": "\n \t\tHad the same problem. Memory leak and crash after some number of epochs. Looks like the ModelCheckpoint callback is a culprit. Removing it solved the issue.\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "olk", "commentT": "2020-07-28T16:21:59Z", "comment_text": "\n \t\t\n I guess this issue is related to using Tensorflow with Python-3.8.\n \n It is not related to Python 3.8. I have the same problem with Python 3.7.4\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "olk", "commentT": "2020-08-01T17:15:53Z", "comment_text": "\n \t\tI found a reason for the problem on my computer - YMMV. I was using the ModelCheckpoint callback to save the best model, and if there was a model with that name already in the folder, I got the error. Removing or renaming the model with that name fixed the issue. Windows 10 system, Python 3.7.4.\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "olk", "commentT": "2020-08-10T18:06:18Z", "comment_text": "\n \t\tAdding this code snippet fixes this issue for me when using RTX GPUs:\n <denchmark-code>devices = tf.config.experimental.list_physical_devices('GPU')\n tf.config.experimental.set_memory_growth(devices[0], True)\n </denchmark-code>\n \n This is something I have to do in my training scripts as well. Might help someone \ud83d\udc4d\n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "olk", "commentT": "2020-09-07T01:48:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/610265158/face_landmark/issues/34#issuecomment-615152235>610265158/face_landmark#34 (comment)</denchmark-link>\n \n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "olk", "commentT": "2020-10-03T09:02:54Z", "comment_text": "\n \t\tIdeas from stackoverflow.  I just directly copy the code from deeplearning.ai in colab. A part of it goes like this:\n `train_generator = train_datagen.flow_from_directory(\n 'horse-or-human/',  # This is the source directory for training images\n target_size=(300, 300),  # All images will be resized to 300x300\n batch_size=128,\n # Since we use binary_crossentropy loss, we need binary labels\n class_mode='binary')\n history = model.fit(\n train_generator,\n steps_per_epoch=8,\n epochs=15,\n verbose=1)`\n and there are 1027 images. 128*8=1024, less than 1027. I set steps_per_epoch to 9, the error disappear.\n So, for me the .\n At least this is one of the cases for the error.\n Here is the original answer <denchmark-link:url>https://stackoverflow.com/questions/60000573/error-occurred-when-finalizing-generatordataset-iterator-cancelled-operation-w</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "b6edd34c5858ab0ab4380da774e7e2fd81a92da0", "commit_author": "Jiri Simsa", "commitT": "2019-12-19 16:21:22-08:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 10, "file_old_name": "tensorflow\\core\\kernels\\data\\captured_function.cc", "file_new_name": "tensorflow\\core\\kernels\\data\\captured_function.cc", "file_complexity": {"file_NLOC": "755", "file_CCN": "140", "file_NToken": "5843"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "852,853", "deleted_lines": "852,854", "method_info": {"method_name": "tensorflow::data::CapturedFunction::CapturedFunction", "method_params": "metadata,captured_inputs", "method_startline": "851", "method_endline": "854", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "39", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "484", "deleted_lines": "484", "method_info": {"method_name": "tensorflow::data::CapturedFunction::Create", "method_params": "ctx,metadata,argument_name,out_function", "method_startline": "482", "method_endline": "491", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "89", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "484", "deleted_lines": "484", "method_info": {"method_name": "tensorflow::data::CapturedFunction::Create", "method_params": "ctx,metadata,argument_name,out_function", "method_startline": "482", "method_endline": "491", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "88", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "729", "deleted_lines": "731", "method_info": {"method_name": "tensorflow::data::InstantiatedCapturedFunction::RunInstantiated", "method_params": "args,rets", "method_startline": "714", "method_endline": "750", "method_complexity": {"method_NLOC": "34", "method_CCN": "2", "method_NToken": "247", "method_nesting_level": "2"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "622", "deleted_lines": "623,628", "method_info": {"method_name": "tensorflow::data::InstantiatedCapturedFunction::InstantiatedCapturedFunction", "method_params": "lib,f_handle,ret_types,runner,cancellation_manager,captured_func", "method_startline": "620", "method_endline": "629", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "84", "method_nesting_level": "2"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "496", "deleted_lines": "496", "method_info": {"method_name": "tensorflow::data::CapturedFunction::Create", "method_params": "ctx,metadata,captured_inputs,out_function", "method_startline": "494", "method_endline": "502", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "66", "method_nesting_level": "2"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "496", "deleted_lines": "496", "method_info": {"method_name": "tensorflow::data::CapturedFunction::Create", "method_params": "ctx,metadata,captured_inputs,out_function", "method_startline": "494", "method_endline": "502", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "67", "method_nesting_level": "2"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "622", "deleted_lines": "623", "method_info": {"method_name": "tensorflow::data::InstantiatedCapturedFunction::InstantiatedCapturedFunction", "method_params": "lib,f_handle,ret_types,runner,captured_func", "method_startline": "619", "method_endline": "627", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "75", "method_nesting_level": "2"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "850,852,853", "deleted_lines": "852", "method_info": {"method_name": "tensorflow::data::CapturedFunction::CapturedFunction", "method_params": "metadata,captured_inputs", "method_startline": "849", "method_endline": "853", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "2"}}}, "hunk_9": {"Ismethod": 1, "added_lines": "605", "deleted_lines": "605,606", "method_info": {"method_name": "tensorflow::data::CapturedFunction::Instantiate", "method_params": "ctx,instantiated_captured_function", "method_startline": "527", "method_endline": "607", "method_complexity": {"method_NLOC": "64", "method_CCN": "10", "method_NToken": "548", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\core\\kernels\\data\\captured_function.h", "file_new_name": "tensorflow\\core\\kernels\\data\\captured_function.h", "file_complexity": {"file_NLOC": "138", "file_CCN": "13", "file_NToken": "861"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "112,113,193,200,259", "deleted_lines": "101,113,114,116,195,202,261"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\data\\generator_dataset_op.cc", "file_new_name": "tensorflow\\core\\kernels\\data\\generator_dataset_op.cc", "file_complexity": {"file_NLOC": "173", "file_CCN": "21", "file_NToken": "1135"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "146,148,149", "deleted_lines": "146,147,149,150", "method_info": {"method_name": "tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::GetNextInternal", "method_params": "ctx,out_tensors,end_of_sequence", "method_startline": "120", "method_endline": "153", "method_complexity": {"method_NLOC": "27", "method_CCN": "5", "method_NToken": "157", "method_nesting_level": "4"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\data\\iterator_ops.h", "file_new_name": "tensorflow\\core\\kernels\\data\\iterator_ops.h", "file_complexity": {"file_NLOC": "185", "file_CCN": "18", "file_NToken": "1165"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "77,79", "deleted_lines": "77,79", "method_info": {"method_name": "tensorflow::data::IteratorResource::State::State", "method_params": "flib_def,pflr,flr,iterator", "method_startline": "73", "method_endline": "81", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "80", "method_nesting_level": "4"}}}}}}}}