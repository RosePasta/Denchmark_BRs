{"BR": {"BR_id": "27565", "BR_author": "llan-ml", "BRopenT": "2019-04-06T09:31:57Z", "BRcloseT": "2019-05-03T01:11:15Z", "BR_text": {"BRsummary": "[TF==2.0.0a0] @tf.function raises ValueError when computing gradients", "BRdescription": "\n Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\n TensorFlow version (use command below): pip install tensorflow(-gpu)==2.0.0a0\n Python version: 3.6\n \n Describe the current behavior\n The code executes normally, but raise ValueError when computing gradients (tape.gradient) if I decorate the training function with @tf.function. The traceback is as follows:\n ---------------------------------------------------------------------------\n ValueError                                Traceback (most recent call last)\n ~/Workspaces/fgenl/run.py in ()\n      80     for batch_id in range(num_batches_each_epoch):\n      81         batch_data = data_generator.get_data() # v2\n ---> 82         loss, outputs = train_one_step(batch_data) # v2\n      83         # _, loss, outputs, inputs = sess.run([opt_op, loss_, outputs_, batch_data])\n      84         if loss_metrics is None:\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n     424     # This is the first call of __call__, so we have to initialize.\n     425     initializer_map = {}\n --> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)\n     427     if self._created_variables:\n     428       try:\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\n     368     self._concrete_stateful_fn = (\n     369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n --> 370             *args, **kwds))\n     371\n     372     def invalid_creator_scope(*unused_args, **unused_kwds):\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n    1311     if self._input_signature:\n    1312       args, kwargs = None, None\n -> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)\n    1314     return graph_function\n    1315\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n    1578           or call_context_key not in self._function_cache.missed):\n    1579         self._function_cache.missed.add(call_context_key)\n -> 1580         graph_function = self._create_graph_function(args, kwargs)\n    1581         self._function_cache.primary[cache_key] = graph_function\n    1582         return graph_function, args, kwargs\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n    1510             arg_names=arg_names,\n    1511             override_flat_arg_shapes=override_flat_arg_shapes,\n -> 1512             capture_by_value=self._capture_by_value),\n    1513         self._function_attributes)\n    1514\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n     692                                           converted_func)\n     693\n --> 694       func_outputs = python_func(*func_args, **func_kwargs)\n     695\n     696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\n     315         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n     316         # the function a weak reference to itself to avoid a reference cycle.\n --> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n     318     weak_wrapped_fn = weakref.ref(wrapped_fn)\n     319\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n     684                   optional_features=autograph_options,\n     685                   force_conversion=True,\n --> 686               ), args, kwargs)\n     687\n     688         # Wrapping around a decorator allows checks like tf_inspect.getargspec\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\n     390     return _call_unconverted(f, args, kwargs)\n     391\n --> 392   result = converted_f(*effective_args, **kwargs)\n     393\n     394   # The converted function's closure is simply inserted into the function's\n \n /tmp/tmpx0xgcbu3.py in tf__train_one_step(batch_data)\n       6     outputs = ag__.converted_call(model, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (batch_data,), {})\n       7     loss, info = ag__.converted_call('calculate_loss', loss_object, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (outputs, batch_data), {})\n ----> 8   gradients = ag__.converted_call('gradient', tape, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (loss, model.trainable_variables), {})\n       9   update_list = [(grad, var) for grad, var in ag__.converted_call(zip, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (gradients, model.trainable_variables), {}) if grad is not None]\n      10   ag__.converted_call('apply_gradients', optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (update_list,), {})\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)\n     265\n     266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):\n --> 267     return _call_unconverted(f, args, kwargs)\n     268\n     269   # internal_convert_user_code is for example turned off when issuing a dynamic\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)\n     186     return f.__self__.call(args, kwargs)\n     187\n --> 188   return f(*args, **kwargs)\n     189\n     190\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\n     954         flat_sources,\n     955         output_gradients=output_gradients,\n --> 956         unconnected_gradients=unconnected_gradients)\n     957\n     958     if not self._persistent:\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)\n      70       sources,\n      71       output_gradients,\n ---> 72       compat.as_str(unconnected_gradients.value))\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _aggregate_grads(gradients)\n     565         indexed_slices = ops.IndexedSlices(\n     566             grad,\n --> 567             math_ops.range(grad.shape[0]),\n     568             constant_op.constant(grad.shape.as_list()))\n     569         indexed_slices_list.append(indexed_slices)\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in range(start, limit, delta, dtype, name)\n    1258   with ops.name_scope(name, \"Range\", [start, limit, delta]) as name:\n    1259     start = ops.convert_to_tensor(start, dtype=dtype, name=\"start\")\n -> 1260     limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\n    1261     delta = ops.convert_to_tensor(delta, dtype=dtype, name=\"delta\")\n    1262\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)\n    1048   preferred_dtype = deprecation.deprecated_argument_lookup(\n    1049       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n -> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\n    1051\n    1052\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\n    1106       name=name,\n    1107       preferred_dtype=dtype_hint,\n -> 1108       as_ref=False)\n    1109\n    1110\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\n    1184\n    1185     if ret is None:\n -> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    1187\n    1188     if ret is NotImplemented:\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\n     302                                          as_ref=False):\n     303   _ = as_ref\n --> 304   return constant(v, dtype=dtype, name=name)\n     305\n     306\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\n     243   \"\"\"\n     244   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n --> 245                         allow_broadcast=True)\n     246\n     247\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\n     281       tensor_util.make_tensor_proto(\n     282           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\n --> 283           allow_broadcast=allow_broadcast))\n     284   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\n     285   const_tensor = g.create_op(\n \n ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\n     453   else:\n     454     if values is None:\n --> 455       raise ValueError(\"None values not supported.\")\n     456     # if dtype is provided, forces numpy array to be the type\n     457     # provided if possible.\n \n ValueError: None values not supported.\n \n Describe the expected behavior\n The code should also execute normally when using @tf.function.\n Code to reproduce the issue\n Sorry, I do not have a simple snippet to reproduce this issue. But could you find something in the traceback? See below please.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "llan-ml", "commentT": "2019-04-06T12:18:35Z", "comment_text": "\n \t\tI found a simple script to reproduce this issue, and it seems that the op tf.sparse.sparse_dense_matmul causes this issue.\n # -*- coding: utf-8 -*-\n # @Author  : Lin Lan (ryan.linlan@gmail.com)\n \n from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n import numpy as np\n import scipy as sp\n import tensorflow as tf\n \n \n def sparse_to_tuple(sparse_mx):\n     \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n     def to_tuple(mx):\n         if not sp.sparse.isspmatrix_coo(mx):\n             mx = mx.tocoo()\n         coords = np.vstack((mx.row, mx.col)).transpose()\n         values = mx.data\n         shape = mx.shape\n         return coords, values, shape\n \n     if isinstance(sparse_mx, list):\n         for i in range(len(sparse_mx)):\n             sparse_mx[i] = to_tuple(sparse_mx[i])\n     else:\n         sparse_mx = to_tuple(sparse_mx)\n \n     return sparse_mx\n \n \n def construct_tf_sparse_tensor(sp_sparse_matrix):\n     if not sp.sparse.issparse(sp_sparse_matrix):\n         raise TypeError\n \n     tuple_format = sparse_to_tuple(sp_sparse_matrix)\n     tf_sparse_tensor = tf.sparse.SparseTensor(\n         indices=tuple_format[0],\n         values=tuple_format[1],\n         dense_shape=tuple_format[2])\n     tf_sparse_tensor = tf.sparse.reorder(tf_sparse_tensor)\n     return tf_sparse_tensor\n \n \n weights = tf.Variable(\n     tf.random.uniform([512, 128]),\n     dtype=tf.float32,\n     trainable=True)\n optimizer = tf.optimizers.Adam()\n \n \n @tf.function\n def train(x):\n     with tf.GradientTape() as tape:\n         embeddings = tf.sparse.sparse_dense_matmul(\n             x,\n             weights)\n         batch_embeddings = tf.nn.embedding_lookup(\n             embeddings, [1, 2, 3, 4, 5, 7, 8, 9, 10])\n         # embeddings = tf.nn.embedding_lookup(\n         #     embeddings, list(range(512)))\n         logits = tf.matmul(batch_embeddings, embeddings, transpose_b=True)\n         loss = tf.reduce_mean(logits)\n     gradients = tape.gradient(loss, [weights])\n     optimizer.apply_gradients(zip(gradients, [weights]))\n \n \n random_array = np.random.rand(512, 512)\n sparse_array = sp.sparse.csr_matrix(\n     np.asarray(random_array > 0.5, dtype=np.float32))\n sparse_tensor = construct_tf_sparse_tensor(sparse_array)\n train(sparse_tensor)\n \n To let the above code compute gradients normally, one way is to uncomment embeddings = tf.nn.embedding_lookup(embeddings, list(range(512))).\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "llan-ml", "commentT": "2019-04-08T14:55:17Z", "comment_text": "\n \t\tThe Tape is unable to see the variables.\n So, use tape.watch(embeddings) after sparse_dense_matmul(). (Sparse Tensors cannot be watched, so watching x is not an option).\n That solves the problem.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "llan-ml", "commentT": "2019-04-08T17:36:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/captain-pool>@captain-pool</denchmark-link>\n   The above code works well in eager mode. It only fails when we use  decoration. So, the tape only cannot see the variable with AutoGraph?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "llan-ml", "commentT": "2019-04-08T18:01:29Z", "comment_text": "\n \t\tWell, that is exactly what's happening. For Autograph it works only with watch(). I'm still looking through the codebase to find the reason.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "llan-ml", "commentT": "2019-05-02T01:07:45Z", "comment_text": "\n \t\tThere seems to be a slightly more helpful error in tf-nightly, but it looks like it's unrelated to tape.watch or autograph. The shape of embeddings seems to be partially unknown after the sparse_dense_matmul, and this line fixed in my tests:\n <denchmark-code>        embeddings = tf.sparse.sparse_dense_matmul(\n             x,\n             weights)\n         embeddings.set_shape((512, 128))  # This removes the error.\n         batch_embeddings = tf.nn.embedding_lookup(\n             embeddings, tf.constant([1, 2, 3, 4, 5, 7, 8, 9, 10]))\n </denchmark-code>\n \n Reassigning to triage the tape error.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "llan-ml", "commentT": "2019-05-02T18:15:51Z", "comment_text": "\n \t\tThere is a bug in the backprop code, where it does math_ops.range(grad.shape[0]) which uses the static shape of the grad tensor, which might be known (a number) or None. To use the dynamic shape we need something like math_ops.range(array_ops.shape(grad)[0]).\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "llan-ml", "commentT": "2019-05-03T01:11:16Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27565>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27565>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "110f0610ed0cf52d256e414906cf91d4e9d657e7", "commit_author": "Alexandre Passos", "commitT": "2019-05-02 17:58:38-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\eager\\backprop.py", "file_new_name": "tensorflow\\python\\eager\\backprop.py", "file_complexity": {"file_NLOC": "594", "file_CCN": "114", "file_NToken": "3320"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "571,572", "deleted_lines": "571,572", "method_info": {"method_name": "_aggregate_grads", "method_params": "gradients", "method_startline": "546", "method_endline": "583", "method_complexity": {"method_NLOC": "24", "method_CCN": "9", "method_NToken": "198", "method_nesting_level": "0"}}}}}}}}