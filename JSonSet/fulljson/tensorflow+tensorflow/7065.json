{"BR": {"BR_id": "7065", "BR_author": "SeguinBe", "BRopenT": "2017-01-25T18:40:58Z", "BRcloseT": "2017-05-10T03:03:31Z", "BR_text": {"BRsummary": "pytorch 2.5x faster on VGG16", "BRdescription": "\n <denchmark-h:h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</denchmark-h>\n \n Started on SO, and was told to post here (<denchmark-link:http://stackoverflow.com/questions/41832779/tensorflow-2-5x-slower-than-pytorch-on-vgg16-architecture?noredirect=1#comment70901342_41832779>SO post</denchmark-link>\n )\n <denchmark-h:h3>Environment info</denchmark-h>\n \n Operating System:\n Ubuntu 14.04 + Maxwell Titan X\n Installed version of CUDA and cuDNN:\n CUDA 8.0, cuDNN 5.1\n :~$ ls -l /usr/local/cuda/lib64/libcud*\n -rw-r--r-- 1 root root    558720 Jan 25 08:23 /usr/local/cuda/lib64/libcudadevrt.a\n lrwxrwxrwx 1 root root        16 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\n lrwxrwxrwx 1 root root        19 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n -rwxr-xr-x 1 root root    415432 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0.44\n -rw-r--r-- 1 root root    775162 Jan 25 08:23 /usr/local/cuda/lib64/libcudart_static.a\n lrwxrwxrwx 1 1000 users       13 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\n lrwxrwxrwx 1 1000 users       17 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n -rwxrwxr-x 1 1000 users 79337624 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n -rw-rw-r-- 1 1000 users 69756172 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn_static.a\n Installed from binary pip package :\n \n https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl with an Anaconda distribution\n The output from python -c \"import tensorflow; print(tensorflow.__version__)\":\n \n <denchmark-code>I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\n I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\n 0.12.1\n </denchmark-code>\n \n <denchmark-h:h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</denchmark-h>\n \n Using the following code to do a forward pass on a pretrained VGG16 :\n import tensorflow as tf\n from tensorflow.contrib import slim\n from tensorflow.contrib.slim import nets\n \n tf.reset_default_graph()\n # Use RNG to avoid the feed_dict argument\n input_images = tf.random_uniform((16, 224, 224, 3), maxval=255)  \n preds = nets.vgg.vgg_16(input_images, is_training=False)[0]\n saver = tf.train.Saver()\n \n config = tf.ConfigProto(log_device_placement=True)\n sess = tf.InteractiveSession(config=config)\n saver.restore(sess, './vgg_16.ckpt')\n \n # With jupyter notebook magic\n %timeit sess.run(preds)\n Compared to the pytorch version on the same machine :\n import numpy as np\n import torch\n import torchvision.models as models\n from torch.autograd import Variable\n torch.backends.cudnn.benchmark = True\n \n net = models.vgg16()\n net.cuda()\n \n _in = Variable(torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda())\n \n # With jupyter notebook magic\n %timeit net(_in)\n I get the following results by comparing the frameworks. Surprisingly, there is a small difference with the more complicated resnet-50 while I get a huge gap for the VGG16 architecture which (almost) just uses 3x3 convolutions.\n \n \n \n Model\n TF\n pytorch\n \n \n \n \n VGG16\n 160ms\n 65ms\n \n \n resnet-50\n 58ms\n 48ms\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "SeguinBe", "commentT": "2017-01-25T18:42:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sguada>@sguada</denchmark-link>\n  do you see anything that got added recently that could address performance gap? (maybe some new fused ops?)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "SeguinBe", "commentT": "2017-01-25T18:50:11Z", "comment_text": "\n \t\tcc: <denchmark-link:https://github.com/vincentvanhoucke>@vincentvanhoucke</denchmark-link>\n  in case he knows others working on VGG-like models\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "SeguinBe", "commentT": "2017-01-25T21:18:26Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>\n .  I am going to reproduce the result and get back to you.  Wanted to let you know we are looking at this issue.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "SeguinBe", "commentT": "2017-01-25T21:19:32Z", "comment_text": "\n \t\tthanks\n ps: <denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>\n  is the affected party here\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "SeguinBe", "commentT": "2017-01-25T22:36:18Z", "comment_text": "\n \t\tOne drive-by observation: the setup with\n <denchmark-code>input_images = tf.random_uniform(16, 224, 224, 3), maxval=255)\n </denchmark-code>\n \n ...might be slow because it's invoking the random number generator for every batch. In the PyTorch program, you run the RNG once, outside the timing loop (in the call to np.random.randn()), and reuse its results several times.\n The following might be a fairer comparison:\n <denchmark-code>input_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))\n preds = nets.vgg.vgg_16(input_images, is_training=False)[0]\n sess.run(tf.global_variable_initializer())\n # ...\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "SeguinBe", "commentT": "2017-01-25T22:58:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>\n  tried it already, it does not change the timing at all.\n import tensorflow as tf\n from tensorflow.contrib import slim\n from tensorflow.contrib.slim import nets\n \n tf.reset_default_graph()\n input_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))\n preds = nets.vgg.vgg_16(input_images, is_training=False)[0]\n saver = tf.train.Saver(var_list=[v for v in tf.global_variables() if 'vgg_16' in v.name])\n init_op = tf.variables_initializer([input_images])\n \n config = tf.ConfigProto(log_device_placement=False)\n sess = tf.InteractiveSession(config=config)\n saver.restore(sess, './vgg_16.ckpt')\n sess.run(init_op)\n On a side note, the resnet-50 timing in the end is more like 50ms so basically the same as pytorch.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "SeguinBe", "commentT": "2017-01-26T20:37:08Z", "comment_text": "\n \t\tThe tf version is using the slower NHWC data format. Changing it to NCHW will most likely speed things up.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "SeguinBe", "commentT": "2017-01-27T01:41:58Z", "comment_text": "\n \t\tCan you try?\n with slim.arg_scope([slim.conv2d], data_format='NCHW'):\n preds, _ = nets.vgg.vgg_16(input_images, is_training=False)\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "SeguinBe", "commentT": "2017-01-27T09:51:57Z", "comment_text": "\n \t\tinput_images = tf.Variable(tf.random_uniform((16, 3, 224, 224), maxval=255))\n with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):\n     preds, _ = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\n init_op = tf.global_variables_initializer()\n \n config = tf.ConfigProto(log_device_placement=True)\n sess = tf.InteractiveSession(config=config)\n sess.run(init_op)\n Using data_format='NHWC' and size [X, 224, 224, 3] I still get 160ms and with the data_format='NCHW' it is slightly better at 150ms...\n I have to note that the fc6-fc7 are implemented as convolution in the TF version, will try if modifying them to pure matrix-multiplication changes anything\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "SeguinBe", "commentT": "2017-01-27T10:16:59Z", "comment_text": "\n \t\tSo I think that was mainly the solution, the tensorflow definition of the network was using a convolution instead of the fully connected linear matrix multiplication for fc6 fc7 fc8 (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py#L116>here</denchmark-link>\n ). Did not think originally it would be a big problem but to recapitulate :\n \n \n \n Model\n Timing\n \n \n \n \n TF-slim default\n 160ms\n \n \n TF-slim + NCHW\n 150ms\n \n \n fc layers instead of conv\n 94ms\n \n \n fc layers instead of conv + NCHW\n 82ms\n \n \n pytorch\n 65ms\n \n \n \n There is still a gap but it is definitely more acceptable, should we consider this as resolved?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "SeguinBe", "commentT": "2017-01-27T17:38:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>\n  are the models identical? One way of telling is initializing with same weights and running the computation through. Since they both rely on CuDNN they should get the same timing, 30% slower seems off.\n BTW, you can print out layers and their flops like this, this can sometimes help spot the difference\n <denchmark-code>  tf.contrib.tfprof.model_analyzer.print_model_analysis(\n       tf.get_default_graph(),\n       tfprof_options=tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS)\n </denchmark-code>\n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "SeguinBe", "commentT": "2017-01-27T20:35:03Z", "comment_text": "\n \t\tLooped in by <denchmark-link:https://github.com/sguada>@sguada</denchmark-link>\n .\n The discrepancy between using convolutional vs. fully connected layers should be fixed.\n The convolution kernel correctly calls CuBlas gemm for 1x1 convolutions and NHWC format (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L439>see here</denchmark-link>\n ).\n Using  vs.  should not make a difference for 'fc7' or 'fc8' -- <denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>\n  can you verify that please?\n However, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when\n convolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "SeguinBe", "commentT": "2017-01-28T00:49:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gpapan>@gpapan</denchmark-link>\n  yeah it would be great if it works when the input size and the kernel size are the same and padding is 'VALID'.\n Also, it seems that the optimization won't work with data_format='NCWH', but it should also work, isn't it?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "SeguinBe", "commentT": "2017-01-28T18:09:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/gpapan>@gpapan</denchmark-link>\n  Yes, setting fc7 or/and fc8 as 1x1 conv does not change the timing (both in NHWC and NCHW btw)\n <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>\n  It'd take a bit more time to properly transfer the weights from one framework to another. Though I think that I find the same proportional gap even in the 3x3 convolutional layers. I'll investigate more in the coming days.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "SeguinBe", "commentT": "2017-01-29T22:33:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>\n  Thanks, that's very informative, I will go ahead and submit a change to optimize the branch in which the filter and input activations have the same size.\n <denchmark-link:https://github.com/sguada>@sguada</denchmark-link>\n  I think that cudnn natively supports the NCHW format and handles this special case internally. The last experiment by <denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>\n  also hints in the same direction.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "SeguinBe", "commentT": "2017-01-31T20:00:27Z", "comment_text": "\n \t\tCan someone try it out and get timings with the latest head?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "SeguinBe", "commentT": "2017-01-31T21:21:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>\n  can you please verify that the fix just pushed solves the conv2d vs. fully connected discrepancy issue?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "SeguinBe", "commentT": "2017-02-01T12:43:52Z", "comment_text": "\n \t\tRecent update of <denchmark-link:https://arxiv.org/abs/1608.07249>Benchmarking State-of-the-Art Deep Learning Software Tools</denchmark-link>\n  shows some performance issues. For example, (see table 7)  is significantly (~ 10 times) slower in TF than in other frameworks, an it's even slower at GTX 980 than at GTX 1080. Also, ResNet-50 is ~5.5 times faster in MXNet. Those are most significant differences.\n In addition, LSTM is around 3 times faster in CNTK, and ResNet-56 is twice faster in MXNet.\n Version used was TensorFlow 0.11 (commit <denchmark-link:https://github.com/tensorflow/tensorflow/tree/47dd089db3cd16d76595791b2e8483e2fd0b0a25>47dd089</denchmark-link>\n ) with CUDA 8.0 and cuDNN 5.1\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "SeguinBe", "commentT": "2017-02-01T14:18:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Randl>@Randl</denchmark-link>\n  -- thanks, <denchmark-link:https://github.com/annarev>@annarev</denchmark-link>\n  has been looking at those differences (ps, additional details should probably go to a different github issue since the problems there are different from vgg difference which is almost solved)\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "SeguinBe", "commentT": "2017-05-10T03:03:29Z", "comment_text": "\n \t\tClosing this item.  Comments can still be made.  While it does not always make a difference, you can also try adding .  This seems to make a bigger difference on K80 and it is model dependent.  For Pascal I saw a 9% improvement when testing a Wide ResNet implementation.  Our <denchmark-link:https://www.tensorflow.org/performance/performance_models>benchmark scripts</denchmark-link>\n  also include a VGG16 model, and I asked the team to check that implementation of VGG16 against the findings in this thread.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "SeguinBe", "commentT": "2018-09-07T14:44:19Z", "comment_text": "\n \t\tSo I'm coming back to some old issues and tried again this one:\n <denchmark-h:h2>Setup</denchmark-h>\n \n <denchmark-code>conda create -n deepl python=3.6\n conda activate deepl\n conda install tensorflow-gpu=1.9 jupyter\n conda install pytorch torchvision -c pytorch\n     \n wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\n tar -xvf vgg_16_2016_08_28.tar.gz\n </denchmark-code>\n \n Ubuntu 18.04 + Titan X Maxwell\n TF 1.9, Cuda 9.0, cuDNN 7.1.2 (have to update the drivers to go to 1.10, 9.2, and 7.2)\n <denchmark-h:h2>Commands</denchmark-h>\n \n <denchmark-h:h4>Tensorflow</denchmark-h>\n \n import os\n ## Does not change anything\n #os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n import tensorflow as tf\n from tensorflow.contrib import slim\n from tensorflow.contrib.slim import nets\n import numpy as np\n \n tf.reset_default_graph()\n if False:  # 'NHWC' format\n     # Use RNG to avoid the feed_dict argument\n     input_images = tf.constant(np.random.randn(16, 224, 224, 3).astype(np.float32))\n     net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\n     conv_layer = net[1]['vgg_16/pool5']\n     preds = net[0]\n else:  # 'NCHW' format\n     input_images = tf.constant(np.random.randn(16, 3, 224, 224).astype(np.float32))\n     with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):\n         net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\n         conv_layer = net[1]['vgg_16/pool5']\n         preds = net[0]\n saver = tf.train.Saver()\n \n config = tf.ConfigProto(log_device_placement=True)\n sess = tf.InteractiveSession(config=config)\n saver.restore(sess, './vgg_16.ckpt')\n \n # With jupyter notebook magic\n %timeit sess.run(conv_layer)\n %timeit sess.run(preds)\n <denchmark-h:h4>pyTorch</denchmark-h>\n \n import numpy as np\n import torch\n import torchvision.models as models\n torch.backends.cudnn.benchmark = True\n \n net = models.vgg16()\n net.cuda()\n \n _in = torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda()\n \n # With jupyter notebook magic\n %timeit net.features(_in).data.cpu().numpy()\n %timeit net(_in).data.cpu().numpy()\n <denchmark-h:h2>Results</denchmark-h>\n \n \n \n \n Framework\n TF-NHWC\n TF-NCHW\n pyTorch\n \n \n \n \n pool5 output\n 72.5\n 60.1\n 59.1\n \n \n fc8 output\n 73.6\n 131.0\n 60.8\n \n \n \n <denchmark-h:h3>Conclusion</denchmark-h>\n \n Performance is the same as long as the same data format is used.\n However, as we stated before the TF version is implementing the FC layers as convolutions instead of actual FC layers. For the NHWC case, an optimization makes this difference transparent but it is not the case for the NCHW layout, which explains the difference here. However, this is more about the networks being defined differently than actual performance issues.\n \t\t"}}}, "commit": {"commit_id": "0318cf082ee88ff0e226a5bf7da0487f44d82182", "commit_author": "A. Unique TensorFlower", "commitT": "2017-01-30 12:49:13-08:00", "commit_complexity": {"commit_NLOC": "0.2528301886792453", "commit_CCN": "0.37358490566037733", "commit_Nprams": "0.7358490566037735"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\conv_grad_filter_ops.cc", "file_new_name": "tensorflow\\core\\kernels\\conv_grad_filter_ops.cc", "file_complexity": {"file_NLOC": "562", "file_CCN": "52", "file_NToken": "4368"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495", "deleted_lines": null, "method_info": {"method_name": "tensorflow::Conv2DSlowBackpropFilterOp::Compute", "method_params": "context", "method_startline": "374", "method_endline": "711", "method_complexity": {"method_NLOC": "280", "method_CCN": "33", "method_NToken": "2229", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\conv_grad_input_ops.cc", "file_new_name": "tensorflow\\core\\kernels\\conv_grad_input_ops.cc", "file_complexity": {"file_NLOC": "711", "file_CCN": "66", "file_NToken": "5524"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "632,633,634,635,636,637,638,639,640,641,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661", "deleted_lines": null, "method_info": {"method_name": "tensorflow::Conv2DSlowBackpropInputOp::Compute", "method_params": "context", "method_startline": "547", "method_endline": "888", "method_complexity": {"method_NLOC": "290", "method_CCN": "34", "method_NToken": "2281", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\core\\kernels\\conv_grad_ops_3d.cc", "file_new_name": "tensorflow\\core\\kernels\\conv_grad_ops_3d.cc", "file_complexity": {"file_NLOC": "608", "file_CCN": "57", "file_NToken": "5692"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "408,410,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450", "deleted_lines": "408,410", "method_info": {"method_name": "tensorflow::Conv3DBackpropInputOp<GPUDevice,T>::Compute", "method_params": "context", "method_startline": "389", "method_endline": "605", "method_complexity": {"method_NLOC": "190", "method_CCN": "24", "method_NToken": "1858", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,712", "deleted_lines": null, "method_info": {"method_name": "tensorflow::Conv3DBackpropFilterOp<GPUDevice,T>::Compute", "method_params": "context", "method_startline": "630", "method_endline": "839", "method_complexity": {"method_NLOC": "179", "method_CCN": "21", "method_NToken": "1790", "method_nesting_level": "2"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\core\\kernels\\conv_ops.cc", "file_new_name": "tensorflow\\core\\kernels\\conv_ops.cc", "file_complexity": {"file_NLOC": "584", "file_CCN": "60", "file_NToken": "4710"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "454,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,497,498,499,500,501,502,503,504,505,506,509", "deleted_lines": null, "method_info": {"method_name": "tensorflow::LaunchConv2DOp<GPUDevice,T>::launch", "method_params": "ctx,use_cudnn,cudnn_use_autotune,input_param,filter,row_stride,col_stride,padding,output,data_format", "method_startline": "434", "method_endline": "721", "method_complexity": {"method_NLOC": "250", "method_CCN": "31", "method_NToken": "2101", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "67,68,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102", "deleted_lines": "67,68", "method_info": {"method_name": "tensorflow::LaunchGeneric::launch", "method_params": "ctx,input,filter,row_stride,col_stride,padding,output,data_format", "method_startline": "61", "method_endline": "109", "method_complexity": {"method_NLOC": "39", "method_CCN": "9", "method_NToken": "491", "method_nesting_level": "3"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\conv_ops_3d.cc", "file_new_name": "tensorflow\\core\\kernels\\conv_ops_3d.cc", "file_complexity": {"file_NLOC": "268", "file_CCN": "26", "file_NToken": "2496"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "175,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204", "deleted_lines": "175", "method_info": {"method_name": "tensorflow::LaunchConvOp<GPUDevice,T>::launch", "method_params": "ctx,input_param,filter,strides,padding,output", "method_startline": "138", "method_endline": "353", "method_complexity": {"method_NLOC": "179", "method_CCN": "21", "method_NToken": "1768", "method_nesting_level": "2"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\conv_ops_using_gemm.cc", "file_new_name": "tensorflow\\core\\kernels\\conv_ops_using_gemm.cc", "file_complexity": {"file_NLOC": "351", "file_CCN": "44", "file_NToken": "2646"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "236,237,238,239,240,241,251,252,253,254,255,256,257,258,259,260,261,262,263", "deleted_lines": "236,237,238,239", "method_info": {"method_name": "tensorflow::Im2ColConvFunctor::operator ( )", "method_params": "context,input_data,input_batches,input_height,input_width,input_depth,filter_data,filter_height,filter_width,filter_count,stride_rows,stride_cols,padding,output_data,output_height,output_width", "method_startline": "211", "method_endline": "419", "method_complexity": {"method_NLOC": "157", "method_CCN": "26", "method_NToken": "1198", "method_nesting_level": "3"}}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\python\\kernel_tests\\conv_ops_3d_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\conv_ops_3d_test.py", "file_complexity": {"file_NLOC": "496", "file_CCN": "26", "file_NToken": "2960"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "512,513,514,515,516,517,518,519,520,521,522,523,524,525", "deleted_lines": null, "method_info": {"method_name": "testFilterGradientKernelSizeMatchesInputSize", "method_params": "self", "method_startline": "512", "method_endline": "525", "method_complexity": {"method_NLOC": "14", "method_CCN": "1", "method_NToken": "57", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "527,528,529,530,531,532,533,534,535,536,537,538,539,540", "deleted_lines": null, "method_info": {"method_name": "testInputGradientKernelSizeMatchesInputSize", "method_params": "self", "method_startline": "527", "method_endline": "540", "method_complexity": {"method_NLOC": "14", "method_CCN": "1", "method_NToken": "57", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "245,246,247,248,249,250,251", "deleted_lines": null, "method_info": {"method_name": "testKernelSizeMatchesInputSize", "method_params": "self", "method_startline": "245", "method_endline": "251", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "53", "method_nesting_level": "1"}}}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tensorflow\\python\\kernel_tests\\conv_ops_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\conv_ops_test.py", "file_complexity": {"file_NLOC": "1119", "file_CCN": "117", "file_NToken": "10499"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "560,561,562,563,564,565,566,567,568,569,570,571,572", "deleted_lines": null, "method_info": {"method_name": "testConv2DKernelSizeMatchesInputSizeBackpropInput", "method_params": "self", "method_startline": "560", "method_endline": "572", "method_complexity": {"method_NLOC": "13", "method_CCN": "2", "method_NToken": "105", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "700,701,702,703,704,705,706,707,708,709,710,711", "deleted_lines": null, "method_info": {"method_name": "testConv2DKernelSizeMatchesInputSizeBackpropFilter", "method_params": "self", "method_startline": "700", "method_endline": "711", "method_complexity": {"method_NLOC": "12", "method_CCN": "2", "method_NToken": "115", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019", "deleted_lines": null, "method_info": {"method_name": "testInputGradientKernelSizeMatchesInputSize", "method_params": "self", "method_startline": "1004", "method_endline": "1019", "method_complexity": {"method_NLOC": "16", "method_CCN": "2", "method_NToken": "72", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "406,407,408,409,410,411,412", "deleted_lines": null, "method_info": {"method_name": "testConv2DKernelSizeMatchesInputSize", "method_params": "self", "method_startline": "406", "method_endline": "412", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "53", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036", "deleted_lines": null, "method_info": {"method_name": "testFilterGradientKernelSizeMatchesInputSize", "method_params": "self", "method_startline": "1021", "method_endline": "1036", "method_complexity": {"method_NLOC": "16", "method_CCN": "2", "method_NToken": "72", "method_nesting_level": "1"}}}}}}}}