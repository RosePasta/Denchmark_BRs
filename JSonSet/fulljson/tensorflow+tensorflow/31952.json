{"BR": {"BR_id": "31952", "BR_author": "David-Mao", "BRopenT": "2019-08-25T09:51:25Z", "BRcloseT": "2019-12-02T23:30:48Z", "BR_text": {"BRsummary": "[TF 2.0] tf.gather doesn't work alongside @tf.function", "BRdescription": "\n Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\n Yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n Darwin Kernel Version 18.6.0\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n N/A\n TensorFlow installed from (source or binary):\n binary\n TensorFlow version (use command below):\n 2.0.0-dev20190730\n Python version:\n Python 3.6.8 :: Anaconda, Inc.\n Bazel version (if compiling from source):\n N/A\n GCC/Compiler version (if compiling from source):\n N/A\n CUDA/cuDNN version:\n N/A\n GPU model and memory:\n N/A\n \n Describe the current behavior\n It seems that when tf.gather() is called after a tf.function, the gradient cannot be calculated. The example code blow shows the bug. The code itself raises the following error message:\n \n AssertionError: Expected all args to be Tensors or Variables; but got CompositeTensor\n \n The code will work if we remove the tf.function decorator, or  put the tf.gather line inside the tf.funtion graph.\n Code to reproduce the issue\n <denchmark-code>import numpy as np\n import tensorflow as tf\n \n x = tf.cast(np.random.randn(100, 100), tf.float32)\n z = tf.cast(np.random.randn(1, 100), tf.float32)\n \n layer = tf.keras.layers.Dense(100)\n \n @tf.function  # <- removing this and the code works fine\n def fun(x, layer):\n     y = layer(x)\n     return y\n \n with tf.GradientTape() as tape:\n     y = fun(x, layer)\n     y = tf.gather(y, [0])  # if we put this line inside the function it works fine\n     loss = tf.norm(y - z)\n \n grads = tape.gradient(loss, layer.trainable_variables)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "David-Mao", "commentT": "2019-08-26T05:58:13Z", "comment_text": "\n \t\tI have tried on Colab with TF version 2.0.0-dev20190730, recent nightly version 2.0.0-dev20190825 and was able to reproduce the issue.Please, find the <denchmark-link:https://colab.research.google.com/drive/1P4tejxmGIxKmXM4TFoesuaXTZUEsPpdx>gist</denchmark-link>\n  here.Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "David-Mao", "commentT": "2019-11-27T13:11:30Z", "comment_text": "\n \t\tHi,\n We encountered the same bug, which currently prevents our migration from TF1 to TF2.\n As David-mao said, it works perfectly well in Eager mode.\n The problem arises only when calling tf.gather on tensors returned from a tf.function, and then calculating its gradients.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "David-Mao", "commentT": "2019-11-28T15:26:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/diNatale>@diNatale</denchmark-link>\n  I found a very ugly workaround for this. You wrap  into a graph function:\n <denchmark-code>\n @tf.function\n def gather(x, ind):\n     return tf.gather(x + 0, ind)\n </denchmark-code>\n \n and use this gather instead of tf.gather in your code.  I know it's absurd (the most absurd part is to have + 0 inside it, which is necessary for reasons unclear to me), but it works in my cases.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "David-Mao", "commentT": "2019-11-28T15:43:02Z", "comment_text": "\n \t\tWow!\n \"+ 0\" - of course, how didn't we guess that :)\n It does work for me if I use both the wrapper and the +0\n Is there any explanation for this behaviour?\n thanks\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "David-Mao", "commentT": "2019-12-02T23:30:49Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "d5ee347de231b55f8ef7c11402db1673ff111d53", "commit_author": "Alexandre Passos", "commitT": "2019-12-02 15:16:16-08:00", "commit_complexity": {"commit_NLOC": "0.8461538461538461", "commit_CCN": "0.8461538461538461", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\eager\\backprop_test.py", "file_new_name": "tensorflow\\python\\eager\\backprop_test.py", "file_complexity": {"file_NLOC": "1410", "file_CCN": "246", "file_NToken": "13260"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "313,314", "deleted_lines": null, "method_info": {"method_name": "testFunctionIndexedSlicesGradient.f", "method_params": "x", "method_startline": "313", "method_endline": "314", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "310,311,312,313,314,315,316,317,318,319,320,321", "deleted_lines": null, "method_info": {"method_name": "testFunctionIndexedSlicesGradient", "method_params": "self", "method_startline": "310", "method_endline": "321", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "75", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\eager\\function.py", "file_new_name": "tensorflow\\python\\eager\\function.py", "file_complexity": {"file_NLOC": "1760", "file_CCN": "374", "file_NToken": "11281"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1227,1228,1229,1230,1231,1232", "deleted_lines": null, "method_info": {"method_name": "_wrap_backward_function._backward_function_wrapper", "method_params": "args", "method_startline": "1219", "method_endline": "1251", "method_complexity": {"method_NLOC": "24", "method_CCN": "8", "method_NToken": "126", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "1227,1228,1229,1230,1231,1232", "deleted_lines": null, "method_info": {"method_name": "_wrap_backward_function", "method_params": "self,forward_graph,backward,outputs", "method_startline": "1177", "method_endline": "1253", "method_complexity": {"method_NLOC": "39", "method_CCN": "15", "method_NToken": "255", "method_nesting_level": "1"}}}}}}}}