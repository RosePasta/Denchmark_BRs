{"BR": {"BR_id": "4361", "BR_author": "bsautermeister", "BRopenT": "2016-09-13T18:43:34Z", "BRcloseT": "2017-02-13T17:42:10Z", "BR_text": {"BRsummary": "Update tf.contrib.layers.batch_norm() docs", "BRdescription": "\n Tensorflow version that I use : 0.10 (pip package)\n <denchmark-h:hr></denchmark-h>\n \n I took heavy use of tf.contrib.layers.batch_norm() the last weeks.\n After facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:\n \n #1122\n http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n \n I would suggest to do following improvements to make it more clear:\n 1) Update example in doc-string:\n The example tells in case we use update_collections on its defaults, we have to include this:\n <denchmark-code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n if update_ops:\n     updates = tf.group(update_ops)\n     total_loss = control_flow_ops.with_dependencies([updates], total_loss)\n </denchmark-code>\n \n But this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:\n <denchmark-code>from tensorflow.python import control_flow_ops\n \n update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n if update_ops:\n     updates = tf.tuple(update_ops)\n     total_loss = control_flow_ops.with_dependencies(updates, total_loss)\n </denchmark-code>\n \n As a side question, why do we apply it to the total_loss, and not to the train_op directly, as described in the doc-string text. Added a dependency to total_loss works, but grouping it with the train_op would make the example more clear in my opinion, because we do batch-statistic updates only during training.\n 2) UPDATE_OPS in combination with reuse varscope:\n This is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to UPDATE_OPS nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?\n Or is it required to filter the update-ops after collecting them with update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS), so that each one is executed just once?\n To sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:\n <denchmark-code>if not reuse:\n     # Collect the updates to be computed later.\n     ops.add_to_collections(updates_collections, update_moving_mean)\n     ops.add_to_collections(updates_collections, update_moving_variance)\n </denchmark-code>\n \n In my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling tf.get_collection(tf.GraphKeys.UPDATE_OPS). As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.\n 3) Handling of is_training parameter:\n I have seen a lot of examples people doing something like this in their code to handle the is_training parameter:\n <denchmark-code>def batch_norm_layer(x,train_phase,scope_bn):\n     bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n     updates_collections=None,\n     is_training=True)\n     bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n     updates_collections=None,\n     is_training=False)\n     bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n     return bn\n </denchmark-code>\n \n As far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.\n 4) Usage on Multi-GPU configuration\n a) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).\n b) When I use tf.contrib.batch_norm() within a multi-GPU system, I get an error like this:\n <denchmark-code>InvalidArgumentError: Cannot assign a device to node 'tower_1/inference/ConvStack/x_bn_9/moments/sufficient_statistics/SparseToDense': \n Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel \n for GPU devices is available.\n ...\n </denchmark-code>\n \n Hence, to we have to wrap evey batch_norm() call with tf.device(\"/cpu:0\")? I guess this might have bad impacts on performance, right?\n Thanks!\n PS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know...\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "bsautermeister", "commentT": "2016-09-15T00:10:46Z", "comment_text": "\n \t\tAgree, I believe there is bug in batch_norm.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "bsautermeister", "commentT": "2016-09-15T21:38:57Z", "comment_text": "\n \t\tWith bug in batch_norm, which point's of my list do you actually mean? And could you propose any workaround?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "bsautermeister", "commentT": "2016-09-17T00:16:01Z", "comment_text": "\n \t\tDont know why, I cannot do multi-gpu training when batch_norm moving_avg is applied, but when I update my tf to master version and update my cuda,cudnn, the problem go away.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "bsautermeister", "commentT": "2016-09-23T04:41:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/shlens>@shlens</denchmark-link>\n  Could you take a look at this? Thanks.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "bsautermeister", "commentT": "2016-09-23T15:38:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bsautermeister>@bsautermeister</denchmark-link>\n  would you have a suggested edit on the docstring that would make the layer more clear?\n <denchmark-link:https://github.com/argman>@argman</denchmark-link>\n @, it sounds like your error is fixed, correct?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "bsautermeister", "commentT": "2016-09-24T01:00:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/shlens>@shlens</denchmark-link>\n  , yes, I just update tf to the newest\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "bsautermeister", "commentT": "2016-10-20T02:44:32Z", "comment_text": "\n \t\tIs reuse=True working? Whenever I'm trying 'reuse=True' I get errors like - \"Variable norm0/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\" I'm following the docstring and providing the 'scope' too. As far as I understand, when a variable is to be created using tf.get_variable() and reused, first, it has to be created and then its reuse is to be enabled by using - tf.get_variable_scope().reuse_variables().\n Without \"reuse=True\" in 'tf.contrib.layers.batch_norm()', I think, the right moving mean and variances will not be restored.\n I'm using twnsorflow version 0.11\n Please inform me if this is not the right place to raise this issue. I got to it from <denchmark-link:https://github.com/tensorflow/tensorflow/issues/1122>#1122</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "bsautermeister", "commentT": "2016-10-24T21:29:52Z", "comment_text": "\n \t\tI have the same issue as <denchmark-link:https://github.com/dasabir>@dasabir</denchmark-link>\n  when trying to reuse a batch_norm layer within a variable scope.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "bsautermeister", "commentT": "2016-11-01T05:03:25Z", "comment_text": "\n \t\tFor (2), I agree with <denchmark-link:https://github.com/bsautermeister>@bsautermeister</denchmark-link>\n  because as I believe adding dependences on  looks sound. For some reasons, one may compute loss value (i.e. forward-prop) for validation datapoints; but with dependences on  batch-normalization statistics are also taken from validation set.\n For (3), do we need to share the BN parameters for bn_train and bn_inference? (in the original code different BN variables like beta, gamma are present for those two)\n  def batch_norm_layer(x, train_phase, scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n -  updates_collections=None, is_training=True)\n +  updates_collections=None, is_training=True, scope=scope_bn)\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n -  updates_collections=None, is_training=False)\n +  updates_collections=None, is_training=False, scope=scope_bn, reuse=True)\n    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return bn\n NOTE: I simply ignored the invalid moving average/variance update in the code for simplicity.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "bsautermeister", "commentT": "2016-11-02T07:14:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dasabir>@dasabir</denchmark-link>\n  and <denchmark-link:https://github.com/jfsantos>@jfsantos</denchmark-link>\n  I had same issue. But by speficying the scope_name for batch_norm, the issue was fixed. Under a scope with reusable=True,  will always create new norm_variables and make them reusable which gives you the error. One thing you can do it is to specify the norm_scope name like this . When you reuse this norm layer, just do  or use  under a reusable scope. Hope this is helpful.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "bsautermeister", "commentT": "2016-12-27T09:51:03Z", "comment_text": "\n \t\tI noticed that the docs haven't been updated yet. Would it be useful if the docs instead said:\n update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n with tf.control_dependencies(update_ops):\n     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(total_loss)\n As for proper reuse across multiple data streams, it looks like a shareable version is still <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/normalization.py#L200>in the works</denchmark-link>\n .\n As an aside, to the best of my understanding, the notion of a shareable BN layer should be treated with some care. Depending on the use-case, I think there should be an option to distinguish sharing of the moving averages from the sharing of the beta/gamma parameters <denchmark-link:https://arxiv.org/pdf/1603.09025v4.pdf>as noted here</denchmark-link>\n .\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "bsautermeister", "commentT": "2017-01-27T20:06:49Z", "comment_text": "\n \t\tIs this still a problem with tf.nn.batch_norm?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "bsautermeister", "commentT": "2017-02-13T17:42:10Z", "comment_text": "\n \t\tClosing due to lack of recent activity. Please update the issue if it persists and we will reopen.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "bsautermeister", "commentT": "2018-01-19T11:08:46Z", "comment_text": "\n \t\tWhen you use batch normalization  across multi gpus, how to update variance?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "bsautermeister", "commentT": "2018-04-06T11:57:12Z", "comment_text": "\n \t\tI solve the problem of reusing batch_normalization by specifying reuse=False when first creating bn(I use slim, but it's same to tf.layers.batch_normalization):\n <denchmark-code>scope = tf.get_variable_scope()\n bn1 = slim.batch_norm(input1, decay=0.9, reuse=False, scope=scope, is_training=is_training)\n bn2 = slim.batch_norm(input2, decay=0.9, reuse=True, scope=scope, is_training=is_training)\n </denchmark-code>\n \n You have to specify reuse=False at your first time to create parameters in batch normalization. Or you will get the error info:\n Variable cnn/block1/conv1/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "bsautermeister", "commentT": "2018-06-07T08:41:11Z", "comment_text": "\n \t\tI obey <denchmark-link:https://github.com/wjiangcmu>@wjiangcmu</denchmark-link>\n  's advice, it works.\n the code:\n 33         self.is_training = tf.placeholder(tf.bool, name='MODE')\n // first use:\n 94         self.img_bn1 = tf.cond(self.is_training,\n 95                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = False),\n 96                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True))\n // add update_ops before second ruse, and filter out unrelated update_ops(unrelated moving mean and variance)\n 126         update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n 127         print('update_ops')\n 128         for key in update_ops:\n 129             print(key)\n 131         i2t_update_extra_ops = [elem for elem in update_ops if 'text_feature/attention' not in elem.name]\n // second use:\n 132         self.img_neg_bn1 = batch_norm(self.img_neg_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True)\n // weight update and dependent extra_ops(moving mean and variance)\n 242         self.i2t_optimizer = tf.train.GradientDescentOptimizer(learning_rate )\n 243         i2t_update_grads = self.i2t_optimizer.minimize(self.i2t_loss)\n 244\n 245         i2t_train_ops = [i2t_update_grads] + i2t_update_extra_ops\n 246         self.i2t_updates = tf.group(*i2t_train_ops)\n in addition,  in order to update each batch_norm only once, according to <denchmark-link:https://github.com/bsautermeister>@bsautermeister</denchmark-link>\n  's \"UPDATE_OPS in combination with reuse varscope\", I add the update_ops before the second use each batch_norm, and filter out unrelated update_ops.\n Hope this will be helpful for others.\n \t\t"}}}, "commit": {"commit_id": "a6e6d0aa2cad5e1d50b4f5cbed427a5df9267098", "commit_author": "tvn", "commitT": "2016-09-16 16:16:34-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\contrib\\layers\\python\\layers\\layers.py", "file_new_name": "tensorflow\\contrib\\layers\\python\\layers\\layers.py", "file_complexity": {"file_NLOC": "1055", "file_CCN": "45", "file_NToken": "4973"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "138", "deleted_lines": "138"}}}}}}