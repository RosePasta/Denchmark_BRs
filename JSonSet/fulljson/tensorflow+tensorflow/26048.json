{"BR": {"BR_id": "26048", "BR_author": "robertnishihara", "BRopenT": "2019-02-24T02:14:09Z", "BRcloseT": "2019-03-05T17:13:30Z", "BR_text": {"BRsummary": "Check failure and silent failures with incorrect usage of tf.custom_gradient (in eager mode).", "BRdescription": "\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6\n TensorFlow installed from (source or binary): binary\n TensorFlow version (use command below): v1.12.0-8779-g2ae06ca491 1.13.0-dev20190223 (as well as 1.12.0)\n Python version: Python 3.6.4 :: Anaconda, Inc.\n \n When tf.custom_gradient is used incorrectly (in this case, the returned grad function returns an empty list, the script segfaults.\n import tensorflow as tf\n \n tf.enable_eager_execution()\n \n @tf.custom_gradient\n def identity(x):\n     def grad(dy):\n         return []  # This return value is wrong!\n     return x, grad\n \n x = tf.Variable(1.0)\n with tf.GradientTape() as t:\n     y = identity(x)\n t.gradient(y, [x])\n The t.gradient call fails with\n <denchmark-code>2019-02-23 18:09:14.621207: F ./tensorflow/c/eager/tape.h:642] Check failed: state.op_tape.empty() \n Abort trap: 6\n </denchmark-code>\n \n I think it'd be preferable to raise an exception instead of crashing.\n If I instead return too many values from grad, then the script runs, but this is most likely a bug and should probably raise an exception.\n import tensorflow as tf\n \n tf.enable_eager_execution()\n \n @tf.custom_gradient\n def identity(x):\n     def grad(dy):\n         return 1.0, 2.0  # Too many return values!\n     return x, grad\n \n x = tf.Variable(1.0)\n with tf.GradientTape() as t:\n     y = identity(x)\n t.gradient(y, [x])\n FYI <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>\n \n \t"}, "comments": {}}, "commit": {"commit_id": "710b322a8be78b8aff6b148575fcfe5301f42b64", "commit_author": "Alexandre Passos", "commitT": "2019-03-05 09:11:05-08:00", "commit_complexity": {"commit_NLOC": "0.8571428571428571", "commit_CCN": "0.8571428571428571", "commit_Nprams": "0.8571428571428571"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\c\\eager\\tape.h", "file_new_name": "tensorflow\\c\\eager\\tape.h", "file_complexity": {"file_NLOC": "492", "file_CCN": "102", "file_NToken": "3641"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "642,643,644", "deleted_lines": "642", "method_info": {"method_name": "tensorflow::eager::GradientTape<Gradient,BackwardFunction,TapeTensor>::ComputeGradient", "method_params": "vspace,target_tensor_ids,source_tensor_ids,sources_that_are_targets,output_gradients,result", "method_startline": "475", "method_endline": "671", "method_complexity": {"method_NLOC": "192", "method_CCN": "40", "method_NToken": "1459", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\python\\eager\\backprop_test.py", "file_new_name": "tensorflow\\python\\eager\\backprop_test.py", "file_complexity": {"file_NLOC": "1155", "file_CCN": "207", "file_NToken": "10856"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "135,136,137,138,139,140,141,142,143,144,145,146,147", "deleted_lines": null, "method_info": {"method_name": "testCustomGradientEmptyError", "method_params": "self", "method_startline": "135", "method_endline": "147", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "54", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "139,140", "deleted_lines": null, "method_info": {"method_name": "testCustomGradientEmptyError.testCustomGradientEmptyError.identity.grad", "method_params": "_", "method_startline": "139", "method_endline": "140", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "8", "method_nesting_level": "3"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "138,139,140,141", "deleted_lines": null, "method_info": {"method_name": "testCustomGradientEmptyError.identity", "method_params": "x", "method_startline": "138", "method_endline": "141", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "11", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\ops\\custom_gradient.py", "file_new_name": "tensorflow\\python\\ops\\custom_gradient.py", "file_complexity": {"file_NLOC": "150", "file_CCN": "47", "file_NToken": "1118"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "258,282,283,284,285,286", "deleted_lines": null, "method_info": {"method_name": "_eager_mode_decorator", "method_params": "f,args,kwargs", "method_startline": "253", "method_endline": "294", "method_complexity": {"method_NLOC": "21", "method_CCN": "8", "method_NToken": "184", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "282,283,284,285,286", "deleted_lines": null, "method_info": {"method_name": "_eager_mode_decorator.actual_grad_fn", "method_params": "result_grads", "method_startline": "272", "method_endline": "287", "method_complexity": {"method_NLOC": "15", "method_CCN": "4", "method_NToken": "94", "method_nesting_level": "1"}}}}}}}}