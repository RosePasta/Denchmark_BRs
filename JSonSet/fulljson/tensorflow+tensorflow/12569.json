{"BR": {"BR_id": "12569", "BR_author": "malsulaimi", "BRopenT": "2017-08-24T21:04:12Z", "BRcloseT": "2018-09-09T16:44:11Z", "BR_text": {"BRsummary": "missing Documentation of the method AttentionWrapper.zero_state(...)", "BRdescription": "\n Hello ,\n I have noticed that the method AttentionWrapper.zero_state( batch_size,dtype) does not have any description of its functionality in the  documentation website , below is a reference link :\n <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper>https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper</denchmark-link>\n \n I really hope that this gets fixed , I have spent a couple of days trying to debug a code that I have written until I realized that I was misusing the method .\n thank you\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "malsulaimi", "commentT": "2017-08-24T23:18:58Z", "comment_text": "\n \t\tCan you describe more about how you were misusing the method? (adding <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n  ...)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "malsulaimi", "commentT": "2017-08-25T11:19:37Z", "comment_text": "\n \t\tI did not know that this would return a AttenionWrapperState , I though that this would return a normal initial state  , and thus I was using it as the below :\n <denchmark-code>def decoding_layer(dec_input, encoder_state,\n                    target_sequence_length, max_target_sequence_length,\n                    rnn_size,\n                    num_layers, target_vocab_to_int, target_vocab_size,\n                    batch_size, keep_prob, decoding_embedding_size , encoder_outputs):\n     \"\"\"\n     Create decoding layer\n     :param dec_input: Decoder input\n     :param encoder_state: Encoder state\n     :param target_sequence_length: The lengths of each sequence in the target batch\n     :param max_target_sequence_length: Maximum length of target sequences\n     :param rnn_size: RNN Size\n     :param num_layers: Number of layers\n     :param target_vocab_to_int: Dictionary to go from the target words to an id\n     :param target_vocab_size: Size of target vocabulary\n     :param batch_size: The size of the batch\n     :param keep_prob: Dropout keep probability\n     :param decoding_embedding_size: Decoding embedding size\n     :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n     \"\"\"\n     # 1. Decoder Embedding\n     dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n     dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n \n     # 2. Construct the decoder cell\n     def create_cell(rnn_size):\n         lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n                                             initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n         drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n         return drop\n \n \n     dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n     #dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  \n \n     #attention details \n         attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) \n \n attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)\n \n attn_zero = attn_cell.zero_state(batch_size , tf.float32 )\n \n attn_zero = attn_zero.clone(cell_state = encoder_state)\n \n new_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())\n \n \"\"\"out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n             attn_cell, target_vocab_size, reuse=True\n         )\"\"\"\n \n     #end of attention \n \n     output_layer = Dense(target_vocab_size,\n                          kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n \n     with tf.variable_scope(\"decode\"):\n         train_decoder_out = decoding_layer_train(new_state, attn_cell, dec_embed_input, \n                          target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n \n     with tf.variable_scope(\"decode\", reuse=True):\n         infer_decoder_out = decoding_layer_infer(new_state, attn_cell, dec_embeddings, \n                              target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, \n                              target_vocab_size, output_layer, batch_size, keep_prob)\n \n     return (train_decoder_out, infer_decoder_out)\n \n \"\"\"\n DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n \"\"\"\n #tests.test_decoding_layer(decoding_layer)\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "malsulaimi", "commentT": "2017-08-25T18:35:53Z", "comment_text": "\n \t\tOK, we'll mark it for adding documentation. FWIW we provide source code, and so you can also see the return type for that function (though its not a substitute for documentation, you can always go read the code as well)...\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "malsulaimi", "commentT": "2017-08-25T19:06:14Z", "comment_text": "\n \t\tThats great . Thanks\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "malsulaimi", "commentT": "2017-09-29T23:10:33Z", "comment_text": "\n \t\tAdded better documentation and an example to the docstrings for both BeamSearchDecoder and AttentionWrapper.__init__ and AttentionWrapper.zero_state.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "malsulaimi", "commentT": "2017-09-29T23:10:43Z", "comment_text": "\n \t\tShould show up in a day or two.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "malsulaimi", "commentT": "2018-09-08T18:36:05Z", "comment_text": "\n \t\tNagging Assignee <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>\n : It has been 342 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "malsulaimi", "commentT": "2018-09-09T16:44:07Z", "comment_text": "\n \t\tFixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/aae34fa7e35d9c3931cae49bfc20384dd20dffec>aae34fa</denchmark-link>\n .\n \t\t"}}}, "commit": {"commit_id": "aae34fa7e35d9c3931cae49bfc20384dd20dffec", "commit_author": "Eugene Brevdo", "commitT": "2017-09-29 17:36:01-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\contrib\\seq2seq\\python\\ops\\attention_wrapper.py", "file_new_name": "tensorflow\\contrib\\seq2seq\\python\\ops\\attention_wrapper.py", "file_complexity": {"file_NLOC": "793", "file_CCN": "72", "file_NToken": "3972"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1191,1192,1193,1194,1195", "deleted_lines": null, "method_info": {"method_name": "state_size", "method_params": "self", "method_startline": "1190", "method_endline": "1203", "method_complexity": {"method_NLOC": "9", "method_CCN": "3", "method_NToken": "66", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224", "deleted_lines": null, "method_info": {"method_name": "zero_state", "method_params": "self,batch_size,dtype", "method_startline": "1205", "method_endline": "1255", "method_complexity": {"method_NLOC": "32", "method_CCN": "5", "method_NToken": "206", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\contrib\\seq2seq\\python\\ops\\beam_search_decoder.py", "file_new_name": "tensorflow\\contrib\\seq2seq\\python\\ops\\beam_search_decoder.py", "file_complexity": {"file_NLOC": "420", "file_CCN": "36", "file_NToken": "2806"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,176", "deleted_lines": "133,144"}}}}}}