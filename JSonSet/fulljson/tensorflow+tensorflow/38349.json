{"BR": {"BR_id": "38349", "BR_author": "0x0badc0de", "BRopenT": "2020-04-08T09:30:26Z", "BRcloseT": "2020-10-02T16:47:50Z", "BR_text": {"BRsummary": "`nan` gradient when `tf.where` is used", "BRdescription": "\n Please make sure that this is a bug. As per our\n GitHub Policy,\n we only address code/doc bugs, performance issues, feature requests and\n build/installation issues on GitHub. tag:bug_template\n System information\n \n Have I written custom code (as opposed to using a stock\n example script provided in TensorFlow):  Yes\n OS Platform and Distribution (e.g.,\n Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\n the issue happens on mobile device:\n TensorFlow installed from (source or\n binary): binary\n TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 / v1.12.1-29016-g38797a1c8b 2.2.0-dev20200407\n Python version: 3.7.7\n Bazel version (if compiling from source):\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version: - GPU model and memory:\n \n You can collect some of this information using our environment capture\n <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>\n \n You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: \n Describe the current behavior\n Well-defined function with tf.where has nan gradients at points where tf.where inactive branch is undefined.\n Describe the expected behavior\n Inactive branch should be ignored in gradients calculations.\n Standalone code to reproduce the issue\n Provide a reproducible test case that is the bare minimum necessary to generate\n the problem. If possible, please share a link to Colab/Jupyter/any notebook.\n <denchmark-code>import tensorflow as tf\n \n for ex in range(-3, 3):\n     x = tf.convert_to_tensor(10.**ex)\n     with tf.GradientTape() as g:\n         g.watch(x)\n         y = tf.where(x >= -1., x, tf.math.log1p(-x))\n #         y = tf.where(x >= -1., x, tf.math.log(1.-x))\n #         y = tf.where(x >= -1., x, 1./(1.-x))\n     dy_dx = g.gradient(y, x)\n     print(f'y({x})={y}, dy/dx({x})={dy_dx}')\n </denchmark-code>\n \n All 3 functions above are well defined for positive values used for testing. Still they show no gradient at point 1.. while it has to be equal to 1.\n <denchmark-code>y(0.0010000000474974513)=0.0010000000474974513, dy/dx(0.0010000000474974513)=1.0\n y(0.009999999776482582)=0.009999999776482582, dy/dx(0.009999999776482582)=1.0\n y(0.10000000149011612)=0.10000000149011612, dy/dx(0.10000000149011612)=1.0\n y(1.0)=1.0, dy/dx(1.0)=nan\n y(10.0)=10.0, dy/dx(10.0)=1.0\n y(100.0)=100.0, dy/dx(100.0)=1.0\n </denchmark-code>\n \n Other info / logs Include any logs or source code that would be helpful to\n diagnose the problem. If including tracebacks, please include the full\n traceback. Large logs and files should be attached.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "0x0badc0de", "commentT": "2020-04-08T16:16:55Z", "comment_text": "\n \t\tI have tried on colab with TF version 2.1.0 , 2.2.0-rc2 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.research.google.com/gist/ravikyram/806f63f2cf04070a4601289d7003cf0a/untitled24.ipynb>here</denchmark-link>\n . Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "0x0badc0de", "commentT": "2020-04-08T21:21:52Z", "comment_text": "\n \t\tThis is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.\n You can find more detail here, along with a recipe for how to avoid it: <denchmark-link:https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444>https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444</denchmark-link>\n \n In short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "0x0badc0de", "commentT": "2020-04-08T21:21:54Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>No</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "0x0badc0de", "commentT": "2020-04-08T21:27:19Z", "comment_text": "\n \t\tShouldn't this be documented with big warning in tf.where docs in this case?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "0x0badc0de", "commentT": "2020-04-08T22:25:09Z", "comment_text": "\n \t\tIndeed it should.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "0x0badc0de", "commentT": "2020-04-09T02:42:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>\n  Hello, this is my first time contributing to TensofFlow lib. From the thread I gather you would require the  be updated. If it is so can I work on this?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "0x0badc0de", "commentT": "2020-04-11T18:53:42Z", "comment_text": "\n \t\tHello <denchmark-link:https://github.com/0x0badc0de>@0x0badc0de</denchmark-link>\n  , <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>\n \n Should the updated doc contain a something like a warning? or will a small note at the end, about the input not being Nan will do? Also should the workaround for avoiding it also be added to the doc?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "0x0badc0de", "commentT": "2020-04-11T19:16:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/joemaren>@joemaren</denchmark-link>\n  <denchmark-link:https://github.com/anorak-k>@anorak-k</denchmark-link>\n \n Sorry for the delay. Feel free to send a PR - it's only a matter of adding a paragraph to the docstring.\n The text should be more in the lines of a warning. Something like: Important: if any of the inputs contain NaN values, etc.. And yes, it should include the workaround as well, which is something in the lines of: instead of tf.where(x, ops_that_can_nan(z), ...), write tf.where(x, ops_that_can_nan(tf.where(x, z, safe_value)), ...).\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "0x0badc0de", "commentT": "2020-04-13T10:02:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>\n  I have added the change and raised a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467>#38467</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "0x0badc0de", "commentT": "2020-04-18T22:25:55Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>\n  Thanks for your reply. However, I would like to mention that this behavior also happens when the generated value in the inactive branch  (i.e.  or ). Here is a minimal reproducible example:\n import tensorflow as tf\n \n a = tf.Variable(10.)\n with tf.GradientTape() as tape:\n   out = tf.where(a < 15., a, tf.math.pow(10.0, tf.math.exp(a)))\n   grads = tape.gradient(out, a)\n \n print(grads)\n # tf.Tensor(nan, shape=(), dtype=float32)\n And also if we reverse the condition such that the branch with infinite value is selected, the gradient would be infinite (which is a bit surprising that it does not generate nan instead, like above):\n with tf.GradientTape() as tape:\n   out = tf.where(a > 15., a, tf.math.pow(10.0, tf.math.exp(a)))\n   grads = tape.gradient(out, a)\n \n print(grads)\n # tf.Tensor(inf, shape=(), dtype=float32)\n So this behavior happens for both nan and infinite values in inactive branch. I wish it wasn't like this, because it's a bit unreasonable and makes it impossible to use user-defined ops/functions which generate extremely large values for some input values; hence, that inner tf.where workaround may not be practical always (unfortunately, even gradient clipping does not help with this, because clipping a nan value produces nan in TF).\n CC: <denchmark-link:https://github.com/anorak-k>@anorak-k</denchmark-link>\n  for potential consideration in your PR after <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>\n  confirms this.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "0x0badc0de", "commentT": "2020-04-19T02:27:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mkaze>@mkaze</denchmark-link>\n  that's true - nan, inf and any other special FP value will disrupt the gradient calculation.\n What happens internally is that the gradients are aggregated in this fashion: 1 * <grad of branch taken> + 0 * <grad of branch not taken>. In the former case, you have 0 * inf = nan. In the latter case, you have 1 * inf = inf. I agree it's very confusing, unfortunately a naive fix would add significant overhead to gradient calculations.\n Moreover, the forward calculation doesn't need to result in a nan or inf. You can also get weird results if the gradient alone is nan or inf. For example, the cube root function is defined and well-behaved everywhere, but its derivative at zero is infinite. So this will give you a nan gradient too:\n <denchmark-code>a = tf.Variable(0.0)\n with tf.GradientTape() as tape:\n   out = tf.where(a < 1, a, tf.pow(a, 1.0/3.0))\n   grads = tape.gradient(out, a)\n print(grads)\n </denchmark-code>\n \n I think the tf.where workaround is useful with infinite values as well, so long as the branch not taken is forced to take a gradient that can be safely multiplied by 0. For your example, it would be something like this:\n <denchmark-code>dummy_safe_value = 0\n safe_a = tf.where(a > 15., dummy_safe_value, a)\n out = tf.where(a > 15., a, tf.math.pow(10.0, tf.math.exp(safe_a)))\n </denchmark-code>\n \n I agree that it sometimes can be impractical to do, but in principle it should always be possible as long as you control the inputs to the sensitive functions - all they have to do is force finite values in all the elements that are dropped.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "0x0badc0de", "commentT": "2020-05-07T14:42:20Z", "comment_text": "\n \t\tI want to fix the issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/38349>#38349</denchmark-link>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "0x0badc0de", "commentT": "2020-05-14T15:47:54Z", "comment_text": "\n \t\t\n This is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.\n You can find more detail here, along with a recipe for how to avoid it: https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444\n In short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.\n \n You can simply have it raise a value error if its getting Nan inputs. Or does it not work like that?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "0x0badc0de", "commentT": "2020-05-29T11:17:54Z", "comment_text": "\n \t\tCan I work on this issue if someone isn't now?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "0x0badc0de", "commentT": "2020-05-29T14:36:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tushar-dalal>@tushar-dalal</denchmark-link>\n  The challenge is that verifying for such NaN inputs can be taking on performance. When debugging,  can indeed help with that.\n <denchmark-link:https://github.com/unicorn-io>@unicorn-io</denchmark-link>\n  Feel free to tackle it, but note that it's extremely challenging to solve. That said, there was a PR (<denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467>#38467</denchmark-link>\n ) to add a warning message to the docs of tf.where, it would be useful to revive it.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "0x0badc0de", "commentT": "2020-05-29T16:19:20Z", "comment_text": "\n \t\tI am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "0x0badc0de", "commentT": "2020-06-02T01:38:13Z", "comment_text": "\n \t\t\n I am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue. @mdanatg\n \n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "0x0badc0de", "commentT": "2020-06-02T12:11:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/unicorn-io>@unicorn-io</denchmark-link>\n  You can start by looking at the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/tape.h#L149>gradient code</denchmark-link>\n  and understanding how it works. Then you can reproduce when happens in the case of a where with bad gradients.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "0x0badc0de", "commentT": "2020-06-02T15:26:53Z", "comment_text": "\n \t\tCool I'll get to it\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "0x0badc0de", "commentT": "2020-06-08T09:39:52Z", "comment_text": "\n \t\tHey i would like to work on it. can also help please\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "0x0badc0de", "commentT": "2020-06-17T11:34:28Z", "comment_text": "\n \t\t\n Cool I'll get to it\n \n This bug cannot be fixed as of now it seems.\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "0x0badc0de", "commentT": "2020-06-17T11:51:13Z", "comment_text": "\n \t\tIt's indeed very challenging to fix. However, the documentation of affected ops, like tf.where can still be updated to alert the users about it.\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "0x0badc0de", "commentT": "2020-06-18T05:00:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>\n  isn't <denchmark-link:https://github.com/tensorflow/tensorflow/pull/38497>#38497</denchmark-link>\n  addressing this and is closed?\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "0x0badc0de", "commentT": "2020-06-18T11:40:47Z", "comment_text": "\n \t\tYou mean <denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467>#38467</denchmark-link>\n ? It's closed due to staleness, and it would be useful to revive. By the looks of it it's safe to assume noone else is working on it.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "0x0badc0de", "commentT": "2020-07-01T16:44:28Z", "comment_text": "\n \t\tSeems like its a long time since the last activity. Is this issue still open to be worked on?\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "0x0badc0de", "commentT": "2020-07-01T16:47:47Z", "comment_text": "\n \t\tI think so. There are two parts to it: (1) updating the docs of tf.where, which is fairly straightforward, and (2) actually trying to address the issue, which is a significant undertaking because it involves a rather fundamental issue.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "0x0badc0de", "commentT": "2020-07-08T11:41:29Z", "comment_text": "\n \t\tIs this issue still addressable ?\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "0x0badc0de", "commentT": "2020-07-25T07:42:20Z", "comment_text": "\n \t\tNice to be part of the group.\n Please, have a look to my pull request for the workaround: <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41721>#41721</denchmark-link>\n \n I'm going to work on the main issue too.\n I'll be happy to cooperate with anybody else interested.\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "0x0badc0de", "commentT": "2020-07-27T13:18:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/codeadmin-peritiae>@codeadmin-peritiae</denchmark-link>\n  The PR appears to be empty. Perhaps there's an issue with the git client?\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "0x0badc0de", "commentT": "2020-08-14T14:30:40Z", "comment_text": "\n \t\tJust to follow up on the events. It looks like codeadmin-peritiae had an issue with his original PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41721>#41721</denchmark-link>\n  where he had trouble with his SSH certificate. He then opened up another PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41775>#41775</denchmark-link>\n  which is currently blocked since some of the checks haven't completed. By the looks of it, the documentation update part of this problem is almost completed.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "0x0badc0de", "commentT": "2020-09-28T18:49:30Z", "comment_text": "\n \t\tIs it issue solvable for a beginner ? if yes can I work on it? <denchmark-link:https://github.com/Harsh188>@Harsh188</denchmark-link>\n  <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>\n \n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "0x0badc0de", "commentT": "2020-09-29T02:25:02Z", "comment_text": "\n \t\tI think this issue should be closed, <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41775>#41775</denchmark-link>\n  got merged and fixed the issue with the documentation.\n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "0x0badc0de", "commentT": "2020-10-02T16:47:51Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "d6c0858665de6036de24991b29d74b182cfcf5ae", "commit_author": "codeadmin_peritiae", "commitT": "2020-07-25 09:28:01+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\ops\\array_ops.py", "file_new_name": "tensorflow\\python\\ops\\array_ops.py", "file_complexity": {"file_NLOC": "2265", "file_CCN": "373", "file_NToken": "12946"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504", "deleted_lines": null, "method_info": {"method_name": "where_v2", "method_params": "condition,x,y,name", "method_startline": "4423", "method_endline": "4531", "method_complexity": {"method_NLOC": "10", "method_CCN": "5", "method_NToken": "113", "method_nesting_level": "0"}}}}}}}}