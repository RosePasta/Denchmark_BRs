{"BR": {"BR_id": "19551", "BR_author": "ed-alertedh", "BRopenT": "2018-05-25T05:39:45Z", "BRcloseT": "2018-08-17T19:05:29Z", "BR_text": {"BRsummary": "Cannot use AdagradOptimizer with MirroredStrategy", "BRdescription": "\n <denchmark-h:h3>System information</denchmark-h>\n \n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\n TensorFlow installed from (source or binary): Binary\n TensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0\n Python version: 3.6.3 64-bit\n Bazel version (if compiling from source):\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version: 9.0 / 6.0\n GPU model and memory: Nvidia GTX 1080 8 GB\n Exact command to reproduce: python train_model.py\n \n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n It took me a while to work out what was going on, but it seems that  tf.train.AdagradOptimizer has some specific implementation detail that causes an error when used with MirroredStrategy. I did a spot check with GradientDescentOptimizer and RMSPropOptimizer and they both appear to work in my environment. I'm happy to use a different optimizer as a workaround but I thought at the very least this might save others some time hunting down the cause of the error!\n <denchmark-h:h3>Source code / logs</denchmark-h>\n \n This is almost exactly copied from the example at <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute</denchmark-link>\n  (except for the choice of optimizer)\n <denchmark-code>import tensorflow as tf\n \n def model_fn(features, labels, mode):\n     layer = tf.layers.Dense(1)\n     logits = layer(features)\n \n     if mode == tf.estimator.ModeKeys.PREDICT:\n         predictions = {\"logits\": logits}\n         return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n \n     loss = tf.losses.mean_squared_error(\n             labels=labels, predictions=tf.reshape(logits, []))\n \n     if mode == tf.estimator.ModeKeys.EVAL:\n         return tf.estimator.EstimatorSpec(mode, loss=loss)\n \n     if mode == tf.estimator.ModeKeys.TRAIN:\n         train_op = tf.train.AdagradOptimizer(0.2).minimize(loss)\n         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n \n def input_fn():\n     features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\n     labels = tf.data.Dataset.from_tensors(1.).repeat(100)\n     return tf.data.Dataset.zip((features, labels))\n \n distribution = tf.contrib.distribute.MirroredStrategy()\n config = tf.estimator.RunConfig(train_distribute=distribution)\n classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\n classifier.train(input_fn=input_fn)\n </denchmark-code>\n \n Log output:\n <denchmark-code>2018-05-25 15:30:12.300908: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n 2018-05-25 15:30:14.231174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \n name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.898\n pciBusID: 0000:03:00.0\n totalMemory: 7.93GiB freeMemory: 7.81GiB\n 2018-05-25 15:30:14.557081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \n name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.898\n pciBusID: 0000:81:00.0\n totalMemory: 7.93GiB freeMemory: 7.81GiB\n 2018-05-25 15:30:14.557174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1\n 2018-05-25 15:30:15.198082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n 2018-05-25 15:30:15.198134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 \n 2018-05-25 15:30:15.198142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N N \n 2018-05-25 15:30:15.198145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   N N \n 2018-05-25 15:30:15.198488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7542 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\n 2018-05-25 15:30:15.324510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7543 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1)\n WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqglycjzk\n 2018-05-25 15:30:15.455314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1\n 2018-05-25 15:30:15.455414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n 2018-05-25 15:30:15.455423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 \n 2018-05-25 15:30:15.455427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N N \n 2018-05-25 15:30:15.455431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   N N \n 2018-05-25 15:30:15.455615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 7542 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\n 2018-05-25 15:30:15.455720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 7543 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1)\n Traceback (most recent call last):\n   File \"train_model.py\", line 62, in <module>\n     classifier.train(input_fn=input_fn)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\n     loss = self._train_model(input_fn, hooks, saving_listeners)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\n     return self._train_model_distributed(input_fn, hooks, saving_listeners)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\n     self.config)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 756, in call_for_each_tower\n     return self._call_for_each_tower(fn, *args, **kwargs)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\n     coord.join(threads)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n     six.reraise(*self._exc_info_to_raise)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/six.py\", line 693, in reraise\n     raise value\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n     yield\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\n     self, *merge_args, **merge_kwargs)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 671, in _distributed_apply\n     self._create_slots(var_list)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots\n     with ops.colocate_with(v):\n   File \"/home/aeiq/.pyenv/versions/3.6.3/lib/python3.6/contextlib.py\", line 81, in __enter__\n     return next(self.gen)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4186, in _colocate_with_for_gradient\n     with self.colocate_with(op, ignore_existing):\n   File \"/home/aeiq/.pyenv/versions/3.6.3/lib/python3.6/contextlib.py\", line 81, in __enter__\n     return next(self.gen)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4239, in colocate_with\n     op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1262, in internal_convert_to_tensor_or_indexed_slices\n     value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1104, in internal_convert_to_tensor\n     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 243, in _tensor_conversion\n     assert not as_ref\n AssertionError\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ed-alertedh", "commentT": "2018-06-05T12:40:41Z", "comment_text": "\n \t\tI got the same problem when using the MovingAverageOptimizer\n def model_fn(features, labels, mode):\n     layer = tf.layers.Dense(1)\n     logits = layer(features)\n \n     if mode == tf.estimator.ModeKeys.PREDICT:\n         predictions = {\"logits\": logits}\n         return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n \n     loss = tf.losses.mean_squared_error(\n             labels=labels, predictions=tf.reshape(logits, []))\n \n     if mode == tf.estimator.ModeKeys.EVAL:\n         return tf.estimator.EstimatorSpec(mode, loss=loss)\n \n     if mode == tf.estimator.ModeKeys.TRAIN:\n         optimizer = tf.train.GradientDescentOptimizer(0.02)\n         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\n         train_op = optimizer.minimize(loss)\n         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n \n def input_fn():\n     features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\n     labels = tf.data.Dataset.from_tensors(1.).repeat(100)\n     return tf.data.Dataset.zip((features, labels))\n \n distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\n config = tf.estimator.RunConfig(train_distribute=distribution)\n classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\n classifier.train(input_fn=input_fn)\n The error with some context\n <denchmark-code>AssertionError                            Traceback (most recent call last)\n <ipython-input-114-62e57e2880d4> in <module>()\n      27 config = tf.estimator.RunConfig(train_distribute=distribution)\n      28 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\n ---> 29 classifier.train(input_fn=input_fn)\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n     361 \n     362     saving_listeners = _check_listeners_type(saving_listeners)\n --> 363     loss = self._train_model(input_fn, hooks, saving_listeners)\n     364     logging.info('Loss for final step: %s.', loss)\n     365     return self\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\n     839   def _train_model(self, input_fn, hooks, saving_listeners):\n     840     if self._distribution:\n --> 841       return self._train_model_distributed(input_fn, hooks, saving_listeners)\n     842     else:\n     843       return self._train_model_default(input_fn, hooks, saving_listeners)\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\n     882             labels,  # although this will be None it seems\n     883             model_fn_lib.ModeKeys.TRAIN,\n --> 884             self.config)\n     885 \n     886         # TODO(anjalisridhar): Figure out how to resolve the folowing scaffold\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py in call_for_each_tower(self, fn, *args, **kwargs)\n     754     \"\"\"\n     755     _require_cross_tower_context(self)\n --> 756     return self._call_for_each_tower(fn, *args, **kwargs)\n     757 \n     758   def _call_for_each_tower(self, fn, *args, **kwargs):\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in _call_for_each_tower(self, fn, *args, **kwargs)\n     252       for t in threads:\n     253         t.should_run.set()\n --> 254       coord.join(threads)\n     255 \n     256     return values.regroup({t.device: t.main_result for t in threads})\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\n     387       self._registered_threads = set()\n     388       if self._exc_info_to_raise:\n --> 389         six.reraise(*self._exc_info_to_raise)\n     390       elif stragglers:\n     391         if ignore_live_threads:\n \n /usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)\n     691             if value.__traceback__ is not tb:\n     692                 raise value.with_traceback(tb)\n --> 693             raise value\n     694         finally:\n     695             value = None\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)\n     295     \"\"\"\n     296     try:\n --> 297       yield\n     298     except:  # pylint: disable=bare-except\n     299       self.request_stop(ex=sys.exc_info())\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in run(self)\n     463                 self._captured_var_scope, reuse=self.tower_id > 0), \\\n     464             variable_scope.variable_creator_scope(self.variable_creator_fn):\n --> 465           self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n     466           self.done = True\n     467       finally:\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\n     829 \n     830     logging.info('Calling model_fn.')\n --> 831     model_fn_results = self._model_fn(features=features, **kwargs)\n     832     logging.info('Done calling model_fn.')\n     833 \n \n <ipython-input-114-62e57e2880d4> in model_fn(features, labels, mode)\n      16         optimizer = tf.train.GradientDescentOptimizer(0.02)\n      17         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\n ---> 18         train_op = optimizer.minimize(loss)\n      19         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n      20 \n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n     422 \n     423     return self.apply_gradients(grads_and_vars, global_step=global_step,\n --> 424                                 name=name)\n     425 \n     426   def compute_gradients(self, loss, var_list=None,\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\n      97     if self._sequential_update:\n      98       with ops.control_dependencies([train_op]):\n ---> 99         ma_op = self._ema.apply(var_list)\n     100     else:\n     101       ma_op = self._ema.apply(var_list)\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in apply(self, var_list)\n     423         zero_debias = self._averages[var] in zero_debias_true\n     424         updates.append(assign_moving_average(\n --> 425             self._averages[var], var, decay, zero_debias=zero_debias))\n     426       return control_flow_ops.group(*updates, name=scope)\n     427 \n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)\n      82   with ops.name_scope(name, \"AssignMovingAvg\",\n      83                       [variable, value, decay]) as scope:\n ---> 84     with ops.colocate_with(variable):\n      85       decay = ops.convert_to_tensor(1.0 - decay, name=\"decay\")\n      86       if decay.dtype != variable.dtype.base_dtype:\n \n /usr/lib/python3.5/contextlib.py in __enter__(self)\n      57     def __enter__(self):\n      58         try:\n ---> 59             return next(self.gen)\n      60         except StopIteration:\n      61             raise RuntimeError(\"generator didn't yield\") from None\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\n    4184   def _colocate_with_for_gradient(self, op, gradient_uid,\n    4185                                   ignore_existing=False):\n -> 4186     with self.colocate_with(op, ignore_existing):\n    4187       if gradient_uid is not None and self._control_flow_context is not None:\n    4188         try:\n \n /usr/lib/python3.5/contextlib.py in __enter__(self)\n      57     def __enter__(self):\n      58         try:\n ---> 59             return next(self.gen)\n      60         except StopIteration:\n      61             raise RuntimeError(\"generator didn't yield\") from None\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)\n    4237     if op is not None and not isinstance(op, Operation):\n    4238       # We always want to colocate with the reference op.\n -> 4239       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n    4240 \n    4241     # By default, colocate_with resets the device function stack,\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)\n    1260   else:\n    1261     return internal_convert_to_tensor(\n -> 1262         value, dtype=dtype, name=name, as_ref=as_ref)\n    1263 \n    1264 \n \n /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\n    1102 \n    1103     if ret is None:\n -> 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    1105 \n    1106     if ret is NotImplemented:\n \n /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py in _tensor_conversion(var, dtype, name, as_ref)\n     241   # Try to avoid assignments to and other mutations of MirroredVariable\n     242   # state except through a DistributionStrategy.update() call.\n --> 243   assert not as_ref\n     244   return ops.internal_convert_to_tensor(\n     245       var.get(), dtype=dtype, name=name, as_ref=as_ref)\n \n AssertionError: \n \n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ed-alertedh", "commentT": "2018-06-19T19:10:13Z", "comment_text": "\n \t\tThanks for bringing this to our attention. We will look into why AdagadOptimizer and MovingAverageOptimizer don't work.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ed-alertedh", "commentT": "2018-07-30T12:07:57Z", "comment_text": "\n \t\tHi All,\n I'm getting this same Error with Adagrad (the default for pre-canned tf.estimarot.DNNClassifier)\n Here's my definitions:\n <denchmark-code># Setup MirroredStrategy\n distribution = tf.contrib.distribute.MirroredStrategy()\n run_config = tf.estimator.RunConfig(train_distribute=distribution)\n \n # Define Specs                                                                                                                                                                               \n train_spec_dnn = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords'))\n eval_spec_dnn = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )\n \n \n DNNClassifier = tf.estimator.DNNClassifier(\n     feature_columns = [tf.feature_column.numeric_column(key='feats', dtype=tf.float64, shape=(nDims,))],                                                                    \n     hidden_units = [256, 256, 256, 256],                                                                                                                                 \n     n_classes = 200,\n     model_dir = '/tmp/tf',\n     config = run_config)\n \n </denchmark-code>\n \n Below is the ERROR message:\n <denchmark-code>2018-07-30 11:57:19.736726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3\n 2018-07-30 11:57:19.736801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n 2018-07-30 11:57:19.736818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 \n 2018-07-30 11:57:19.736826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y \n 2018-07-30 11:57:19.736833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y \n 2018-07-30 11:57:19.736840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y \n 2018-07-30 11:57:19.736846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N \n 2018-07-30 11:57:19.737320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n 2018-07-30 11:57:19.737953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 14867 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n 2018-07-30 11:57:19.738083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 14867 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n 2018-07-30 11:57:19.738209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 14867 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n WARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.\n Traceback (most recent call last):\n   File \"train_and_eval.py\", line 137, in <module>\n     tf.estimator.train_and_evaluate(DNNClassifier, train_spec_dnn, eval_spec_dnn)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 439, in train_and_evaluate\n     executor.run()\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 518, in run\n     self.run_local()\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 650, in run_local\n     hooks=train_hooks)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\n     loss = self._train_model(input_fn, hooks, saving_listeners)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\n     return self._train_model_distributed(input_fn, hooks, saving_listeners)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\n     self.config)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 756, in call_for_each_tower\n     return self._call_for_each_tower(fn, *args, **kwargs)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\n     coord.join(threads)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n     six.reraise(*self._exc_info_to_raise)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/six.py\", line 693, in reraise\n     raise value\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n     yield\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\n     self, *merge_args, **merge_kwargs)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 671, in _distributed_apply\n     self._create_slots(var_list)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots\n     with ops.colocate_with(v):\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py\", line 81, in __enter__\n     return next(self.gen)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4186, in _colocate_with_for_gradient\n     with self.colocate_with(op, ignore_existing):\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py\", line 81, in __enter__\n     return next(self.gen)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4239, in colocate_with\n     op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1262, in internal_convert_to_tensor_or_indexed_slices\n     value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1104, in internal_convert_to_tensor\n     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 243, in _tensor_conversion\n     assert not as_ref\n AssertionError\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ed-alertedh", "commentT": "2018-08-15T02:56:29Z", "comment_text": "\n \t\tNagging Assignee <denchmark-link:https://github.com/anj-s>@anj-s</denchmark-link>\n : It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ed-alertedh", "commentT": "2018-12-29T06:54:37Z", "comment_text": "\n \t\t<denchmark-code>optimizer1 = tf.train. GradientDescentOptimizer(learning_rate=FLAGS.tnet_lr)\n optimizer1 = tf.contrib.estimator.TowerOptimizer(optimizer1)\n optimizer2 = tf.train. GradientDescentOptimizer(learning_rate=FLAGS.mnet_lr)\n optimizer2 = tf.contrib.estimator.TowerOptimizer(optimizer2)\n tnet_variables = get_model_variables('tnet')\n mnet_variables = get_model_variables('mnet')\n \n train_op1 = slim.learning.create_train_op(loss,\n                                           optimizer1,\n                                           variables_to_train=tnet_variables,\n                                           summarize_gradients=True)\n train_op2 = slim.learning.create_train_op(loss,\n                                           optimizer2,\n                                           variables_to_train=mnet_variables,\n                                           summarize_gradients=True)\n train_op = tf.group(train_op1, train_op2)\n </denchmark-code>\n \n Above two GradientDescentOptimizers do not work either.\n \t\t"}}}, "commit": {"commit_id": "d0b51d7d9d50dd73c46ba2f2daaa9d26cc0666d0", "commit_author": "Anjali Sridhar", "commitT": "2018-08-17 12:03:13-07:00", "commit_complexity": {"commit_NLOC": "0.14814814814814814", "commit_CCN": "1.0", "commit_Nprams": "0.8888888888888888"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\python\\ops\\variable_scope.py", "file_new_name": "tensorflow\\python\\ops\\variable_scope.py", "file_complexity": {"file_NLOC": "1627", "file_CCN": "168", "file_NToken": "7479"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "45,855,856,857,858,859,860,861,862,863,864,865", "deleted_lines": "840,841,842,857,858"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow\\python\\training\\adagrad.py", "file_new_name": "tensorflow\\python\\training\\adagrad.py", "file_complexity": {"file_NLOC": "83", "file_CCN": "11", "file_NToken": "534"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "73,74,75,76,77,78", "deleted_lines": "73,74,75,76,77,78,79,80", "method_info": {"method_name": "_create_slots", "method_params": "self,var_list", "method_startline": "71", "method_endline": "80", "method_complexity": {"method_NLOC": "10", "method_CCN": "3", "method_NToken": "78", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "83,84,85,86,87,88", "deleted_lines": "83", "method_info": {"method_name": "_init_constant_op.init", "method_params": "", "method_startline": "83", "method_endline": "88", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "30", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "82,83,84,85,86,87,88,89", "deleted_lines": "82,83", "method_info": {"method_name": "_init_constant_op", "method_params": "self,v,dtype", "method_startline": "82", "method_endline": "89", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "13", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\training\\adagrad_test.py", "file_new_name": "tensorflow\\python\\training\\adagrad_test.py", "file_complexity": {"file_NLOC": "271", "file_CCN": "29", "file_NToken": "2824"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336", "deleted_lines": null, "method_info": {"method_name": "testDynamicShapeVariableWithCallableInit", "method_params": "self", "method_startline": "305", "method_endline": "336", "method_complexity": {"method_NLOC": "23", "method_CCN": "4", "method_NToken": "204", "method_nesting_level": "1"}}}}}}}}