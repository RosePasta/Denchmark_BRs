{"BR": {"BR_id": "1563", "BR_author": "sammilei", "BRopenT": "2020-11-30T19:28:33Z", "BRcloseT": "2020-12-09T02:15:39Z", "BR_text": {"BRsummary": "Flag --save-txt returns higher mAP scores", "BRdescription": "\n Repo: b6ed110\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n \n !python test.py --weights \"/content/train_80_20_small/weights/best.pt\" --data /content/ms.yaml --img 416 --task test --verbose --save-txt --iou-thres=0.5\n and\n !python test.py --weights \"/content/train_80_20_small/weights/best.pt\" --data /content/ms.yaml --img 416 --task test --verbose --iou-thres=0.5 \n \n Yield different mAP score\n <denchmark-h:h2>Expected behavior</denchmark-h>\n \n result of command1:\n <denchmark-code>Namespace(augment=False, batch_size=32, conf_thres=0.001, data='/content/ms.yaml', device='', exist_ok=False, img_size=416, iou_thres=0.5, name='exp', project='runs/test', save_conf=False, save_json=False, save_txt=True, single_cls=False, task='test', verbose=True, weights=['/content/train_80_20_small/weights/best.pt'])\n Using torch 1.7.0+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280MB)\n \n Fusing layers... \n Model Summary: 232 layers, 7254609 parameters, 0 gradients\n Scanning '/content/b/labels/val.cache' for images and labels... 1008 found, 0 missing, 1 empty, 0 corrupted: 100% 1008/1008 [00:00<00:00, 6172056.11it/s]\n                Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 32/32 [00:14<00:00,  2.23it/s]\n                  all    1.01e+03    1.01e+03       0.729           1       0.996       0.996\n                    cls1    1.01e+03         187       0.704           1       0.995       0.995\n                    cls2   1.01e+03         405       0.878           1       0.996       0.996\n                    cls3   1.01e+03         206       0.595           1       0.995       0.995\n                    cls4.  1.01e+03         209       0.738           1       0.995       0.995\n Speed: 1.4/2.8/4.2 ms inference/NMS/total per 416x416 image at batch-size 32\n Results saved to runs/test/exp12\n 1008 labels saved to runs/test/exp12/labels\n </denchmark-code>\n \n result of command2:\n <denchmark-code>Namespace(augment=False, batch_size=32, conf_thres=0.001, data='/content/ms.yaml', device='', exist_ok=False, img_size=416, iou_thres=0.5, name='exp', project='runs/test', save_conf=False, save_json=False, save_txt=False, single_cls=False, task='test', verbose=True, weights=['/content/train_80_20_small/weights/best.pt'])\n Using torch 1.7.0+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280MB)\n \n Fusing layers... \n Model Summary: 232 layers, 7254609 parameters, 0 gradients\n Scanning '/content/b/labels/val.cache' for images and labels... 1008 found, 0 missing, 1 empty, 0 corrupted: 100% 1008/1008 [00:00<00:00, 6118463.72it/s]\n                Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 32/32 [00:10<00:00,  2.99it/s]\n                  all    1.01e+03    1.01e+03       0.729       0.941       0.939       0.581\n                 cls1      1.01e+03         187       0.708       0.952        0.95       0.577\n                 cls2     1.01e+03         405       0.875       0.914       0.951       0.582\n                 cls3     1.01e+03         206       0.595       0.971       0.917       0.592\n                 cls4     1.01e+03         209       0.737       0.928       0.939       0.574\n Speed: 1.4/2.3/3.8 ms inference/NMS/total per 416x416 image at batch-size 32\n Results saved to runs/test/exp9\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n YOLOv5 Colab tutorial\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sammilei", "commentT": "2020-11-30T19:42:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sammilei>@sammilei</denchmark-link>\n  thanks for the bug report. Yes I can understand how this could be confusing. We recently made a change to test.py so that --save-txt actually merges predictions with the existing labels to form hybrid autolabels as the new default behavior (if the dataset is unlabelled then the labels would be all predictions).\n I should probably consider separating the two behaviors in the future.\n For the time being the mAP you are seeing is actually correct, it is the hybrid autolabel mAP.\n TODO: separate --save-txt from label merging behavior.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sammilei", "commentT": "2020-11-30T21:17:38Z", "comment_text": "\n \t\tI see. Thanks for the quick answer again!\n I agree with making two different functions: 1. merge auto-labels with ground truth and prediction 2. a stack of prediction only\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sammilei", "commentT": "2020-11-30T21:55:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sammilei>@sammilei</denchmark-link>\n  as a quick fix you can simply set lb=[] instead.\n \n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "sammilei", "commentT": "2020-12-07T12:23:43Z", "comment_text": "\n \t\tHi, I see the same issue here, and still finding it confusing.\n One wants to run \"test\", to \"fairly\" test the ability of the model to predict the ground truth. Is this option equal to save-txt = False ?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "sammilei", "commentT": "2020-12-08T14:58:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bonorico>@bonorico</denchmark-link>\n  thanks for your feedback! This item is on our TODO list to update, in the meantime test.py will produce the correct results when not using --save-txt.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "sammilei", "commentT": "2020-12-09T02:15:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sammilei>@sammilei</denchmark-link>\n  <denchmark-link:https://github.com/bonorico>@bonorico</denchmark-link>\n  this issue should be resolved now in PR <denchmark-link:https://github.com/ultralytics/yolov5/pull/1646>#1646</denchmark-link>\n .\n This PR introduces hybrid autolabelling support in test.py. The auto-labelling options are now:\n \n python test.py --save-txt: traditional auto-labelling\n python test.py --save-hybrid: save hybrid autolabels, combining existing labels with new predictions before NMS (existing predictions given confidence=1.0 before NMS.\n python test.py --save-conf: add confidences to any of the above commands\n \n Regardless of any of the above settings, be aware that auto-labelling works best at very high confidence thresholds, i.e. 0.90 confidence, whereas mAP computation relies on very low confidence threshold, i.e. 0.001, to properly evaluate the area under the PR curve. The two activities are thus essentially mutually exclusive, there is no reason I know of to combine the two into a single test run.\n <denchmark-link:https://user-images.githubusercontent.com/26833433/101565136-4a986d80-3981-11eb-8c08-7e384d8ad9e6.png></denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "sammilei", "commentT": "2020-12-09T13:45:59Z", "comment_text": "\n \t\tHi Glen, thank you for the answer. So basically, drop --save-txt for standard model-testing purposes only...\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "sammilei", "commentT": "2020-12-09T14:48:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bonorico>@bonorico</denchmark-link>\n  --save-txt will simply save labels, mAP is unaffected now.\n --save-hybrid will save hybrid labels, with the correspondingly higher mAP being displayed.\n \t\t"}}}, "commit": {"commit_id": "86f4247515dfb4cccd413c236ff26b31d8e066b4", "commit_author": "Glenn Jocher", "commitT": "2020-12-08 18:15:39-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test.py", "file_new_name": "test.py", "file_complexity": {"file_NLOC": "257", "file_CCN": "1", "file_NToken": "2995"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "36,37", "deleted_lines": "36", "method_info": {"method_name": "test", "method_params": "data,weights,batch_size,imgsz,conf_thres,iou_thres,save_json,single_cls,augment,verbose,model,dataloader,save_dir", "method_startline": "22", "method_endline": "39", "method_complexity": {"method_NLOC": "18", "method_CCN": "1", "method_NToken": "80", "method_nesting_level": "0"}}}}}}}}