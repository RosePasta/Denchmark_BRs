{"BR": {"BR_id": "306", "BR_author": "lorenzomammana", "BRopenT": "2020-07-06T08:38:47Z", "BRcloseT": "2020-07-11T16:50:14Z", "BR_text": {"BRsummary": "labels.npy deletion and recaching on underlying data change", "BRdescription": "\n <denchmark-h:h2>\ud83d\ude80 Feature</denchmark-h>\n \n Add a flag to the train script allowing the user to recache labels.\n <denchmark-h:h2>Motivation</denchmark-h>\n \n I've made a script to convert my labels to the coco correct format, the first time i've lunched the script i made mistakes and the labels were wrong.\n After fixing them and displaying them correctly using an external script i relaunched the train but the labels were still wrong.\n Turns out that even if the label files are changed the script will still load the old cached .npy and obviously the output is going to be incorrect.\n At the moment the only solution is to manually delete the *.npy file and relaunch train.py\n <denchmark-h:h2>Pitch</denchmark-h>\n \n Maybe calling train.py --force-recache or something like this could be a solution to this potential problem. Or at least add to the documentation that this error can occur.\n Tested on Windows, don't know if this issue happens on Linux too (i suppose it does).\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lorenzomammana", "commentT": "2020-07-06T08:39:25Z", "comment_text": "\n \t\tHello <denchmark-link:https://github.com/lorenzomammana>@lorenzomammana</denchmark-link>\n , thank you for your interest in our work! Please visit our <denchmark-link:https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data>Custom Training Tutorial</denchmark-link>\n  to get started, and see our <denchmark-link:https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb>Jupyter Notebook</denchmark-link>\n  <denchmark-link:https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb></denchmark-link>\n , <denchmark-link:https://hub.docker.com/r/ultralytics/yolov5>Docker Image</denchmark-link>\n , and <denchmark-link:https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart>Google Cloud Quickstart Guide</denchmark-link>\n  for example environments.\n If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.\n If this is a custom model or data training question, please note that Ultralytics does not provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:\n \n Cloud-based AI systems operating on hundreds of HD video streams in realtime.\n Edge AI integrated into custom iOS and Android apps for realtime 30 FPS video inference.\n Custom data training, hyperparameter evolution, and model exportation to any destination.\n \n For more information please visit <denchmark-link:https://www.ultralytics.com>https://www.ultralytics.com</denchmark-link>\n .\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lorenzomammana", "commentT": "2020-07-06T17:26:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lorenzomammana>@lorenzomammana</denchmark-link>\n  yes, this is true. The labels are saved in *.npy for faster loading in the future, but if you change the underlying *.txt file labels it will not recache. There is no easy way to detect changes without loading all of the text files. Simply deleting the *.npy file will fix the issue as you've found.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lorenzomammana", "commentT": "2020-07-06T17:28:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lorenzomammana>@lorenzomammana</denchmark-link>\n  or maybe there might be a way to check for necessary recaching. If we had a function that could quickly read the total bytes in a directory, then save that byte-count in the *.npy, a difference there could trigger a recaching?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lorenzomammana", "commentT": "2020-07-06T18:27:40Z", "comment_text": "\n \t\t<denchmark-code>import os\n dir = '../coco/labels/val2017/*.*'\n sum(os.path.getsize(f) for f in glob.glob(dir) if os.path.isfile(f))\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lorenzomammana", "commentT": "2020-07-06T19:08:11Z", "comment_text": "\n \t\tI had exactly the same idea honestly! I can't test your code atm but it does look right to me, are you going to publish the code or do i need to make a pull request?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "lorenzomammana", "commentT": "2020-07-06T19:15:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lorenzomammana>@lorenzomammana</denchmark-link>\n  the entire label caching and loading region of the code needs a rework really. It would be ideal to have the *.npy files be stored dictionaries rather than lists, so that labels could be loaded explicitly like labels['image_134'] rather than labels[0], which may be more failure prone since it is not actually checking the filename but dependent on order, and then the dictionary could include the shapes information as well, which is currently saved in a seperate *.shapes files, which is again order based.\n I might try to do this later on this week, it's a low priority item but would be good to have for the future, as users have run into issues in all the sections above (changing labels while not recaching, shapes files issues, and also having extra images not used during training (i.e. mixing dataset images in a single folder but only calling some with a train.txt), which is not possible now but the dictionary npy would solve I think).\n EDIT: If you'd like to try to do this that would be great. I'd recommend creating a new function in datasets.py to handle this unified *.npy functionality, with integrated change-detection as we mentioned above. Unifying the npy and shapes files would be nice also, one less complication and one less file out there confusing people.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "lorenzomammana", "commentT": "2020-07-10T01:28:46Z", "comment_text": "\n \t\tI think this is a good solution for change detection. Once the single npy file is produced for a dataset, subsequent trainings will scan the images and labels, sum up their bytes into a \"hash\" value which is stored in the actual npy file as well. If the values are equal, the npy is used to load the labels and shapes. If they are different, the npy is deleted and the caching process is repeated.\n def get_hash(files):\n     # Returns a single hash value of a list of files\n     return sum(os.path.getsize(f) for f in files)  # faster\n     # return sum(os.path.getsize(f) for f in files if os.path.isfile(f))  # slower, more robust\n \n # Get hash\n h = get_hash(self.label_files + self.img_files)\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "lorenzomammana", "commentT": "2020-07-10T03:10:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lorenzomammana>@lorenzomammana</denchmark-link>\n  I just pushed a major update <denchmark-link:https://github.com/ultralytics/yolov5/commit/520f5de6f0febf30dd3545793f528c3478c18299>520f5de</denchmark-link>\n  that should resolve all label caching issues. Labels and images are scanned together the first time a dataset is trained, and cached into a labels.cache file now which contains a dictionary of image names, shapes, labels and a unique hash for the cached dataset. Images are scanned for corruption now also as part of the process. All subsequent trainings retrieve the current hash value of the dataset, and if it matches the cached value, then the cached file is used, else the entire dataset is recached and a new file saved. Please try out the changes and let me know if this resolves your issue!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "lorenzomammana", "commentT": "2020-07-10T06:42:45Z", "comment_text": "\n \t\tI'm going to make the big merge from your latest updates to my fork this morning and see if everything is working properly!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "lorenzomammana", "commentT": "2020-07-10T08:15:24Z", "comment_text": "\n \t\tAll right switching between datasets will force label recaching, if the .cache is missing file is created and loaded properly.\n Changing a single digit in a label file will alter the hash and force recaching, seems like it's working perfectly.\n Tested on Ubuntu 18.04, single tesla V100 with multiple datasets, single dataset, mosaic and multi scale.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "lorenzomammana", "commentT": "2020-07-10T18:09:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lorenzomammana>@lorenzomammana</denchmark-link>\n  great!! Thanks for checking.\n The new caching process also verifies each image with PIL img.verify() to help weed out any problems, which is a nice addition. I want to add additional checks, such as rejecting images based on size constraints, too small or too large, etc. Are there any other checks you think should be run on images or labels during caching?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "lorenzomammana", "commentT": "2020-07-11T08:37:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/glenn-jocher>@glenn-jocher</denchmark-link>\n  At the moment i'm not thinking to any particular checks for images except the size constraint as you mentioned.\n For the labels I'm not actually sure what happens if the label width or height exceeds the size of the image, I'm sure that some bounding boxes in my dataset actually exceed the image size (x_center and y_center still in [0, 1]) and they are not detected as errors, i'm also quite confident that this shouldn't be a problem for yolo regression but maybe it's worth mention.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "lorenzomammana", "commentT": "2020-07-11T16:50:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lorenzomammana>@lorenzomammana</denchmark-link>\n  ok. size checks have been integrated now into caching. Will remove TODO label and close now as original issue appears resolved!\n \t\t"}}}, "commit": {"commit_id": "520f5de6f0febf30dd3545793f528c3478c18299", "commit_author": "Glenn Jocher", "commitT": "2020-07-09 20:07:16-07:00", "commit_complexity": {"commit_NLOC": "0.08695652173913043", "commit_CCN": "0.08695652173913043", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "utils\\datasets.py", "file_new_name": "utils\\datasets.py", "file_complexity": {"file_NLOC": "612", "file_CCN": "136", "file_NToken": "7142"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450", "deleted_lines": "428,429,430,431,432,433,436,445,446,447,448,449,450", "method_info": {"method_name": "cache_labels", "method_params": "self,path", "method_startline": "427", "method_endline": "450", "method_complexity": {"method_NLOC": "21", "method_CCN": "6", "method_NToken": "209", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "29,30,31", "deleted_lines": null, "method_info": {"method_name": "get_hash", "method_params": "files", "method_startline": "29", "method_endline": "31", "method_complexity": {"method_NLOC": "2", "method_CCN": "2", "method_NToken": "21", "method_nesting_level": "0"}}}}}}}}