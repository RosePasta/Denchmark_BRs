{"BR": {"BR_id": "1040", "BR_author": "ddkang", "BRopenT": "2019-05-01T04:22:41Z", "BRcloseT": "2019-05-17T02:17:43Z", "BR_text": {"BRsummary": "FORCE-TERMINATE AT Data unpack would read past end of buffer:-26 - error grpcomm_direct.c", "BRdescription": "\n Environment:\n \n Framework: PyTorch\n Framework version: 1.0.1.post2\n Horovod version: 0.16.1\n MPI version: 4.0.0\n CUDA version: Cuda compilation tools, release 9.0, V9.0.176\n NCCL version: N/A\n Python version: 3.5.2\n OS and version: Ubuntu 16.04\n \n Checklist:\n \n Did you search issues to find if somebody asked this question before?\n \n Yes, but there was no resolution: <denchmark-link:https://github.com/horovod/horovod/issues/368>#368</denchmark-link>\n \n Bug report:\n I tried running the pytorch_imagenet_resnet50.py script and got the following error:\n <denchmark-code>--------------------------------------------------------------------------\n An internal error has occurred in ORTE:\n \n [[25215,0],1] FORCE-TERMINATE AT Data unpack would read past end of buffer:-26 - error grpcomm_direct.c(359)\n \n This is something that should be reported to the developers.\n --------------------------------------------------------------------------\n [future5.stanford.edu:12508] [[25215,0],1] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file grpcomm_direct.c at line 355\n </denchmark-code>\n \n I made sure to configure and install OpenMPI as in the dockerfile.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ddkang", "commentT": "2019-05-02T16:18:20Z", "comment_text": "\n \t\tIt does seem that your MPI configuration has some issue. Take a look at this and let me know if it provides any help: <denchmark-link:https://github.com/open-mpi/ompi/issues/4437>open-mpi/ompi#4437</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ddkang", "commentT": "2019-05-02T21:45:59Z", "comment_text": "\n \t\tI am not using hwloc, I didn't see other solutions?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ddkang", "commentT": "2019-05-07T09:46:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ddkang>@ddkang</denchmark-link>\n , can you share output of ?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ddkang", "commentT": "2019-05-07T15:55:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alsrgv>@alsrgv</denchmark-link>\n \n future4:\n <denchmark-code>ii  hwloc-nox                              1.11.2-3                                   amd64        Hierarchical view of the machine - non-X version of utilities\n ii  libhwloc-dev:amd64                     1.11.2-3                                   amd64        Hierarchical view of the machine - static libs and headers\n ii  libhwloc-plugins                       1.11.2-3                                   amd64        Hierarchical view of the machine - plugins\n ii  libhwloc5:amd64                        1.11.2-3                                   amd64        Hierarchical view of the machine - shared libs\n </denchmark-code>\n \n future5:\n <denchmark-code>ii  hwloc-nox                             1.11.2-3                                   amd64        Hierarchical view of the machine - non-X version of utilities\n ii  libhwloc-dev:amd64                    1.11.2-3                                   amd64        Hierarchical view of the machine - static libs and headers\n ii  libhwloc-plugins                      1.11.2-3                                   amd64        Hierarchical view of the machine - plugins\n ii  libhwloc5:amd64                       1.11.2-3                                   amd64        Hierarchical view of the machine - shared libs\n </denchmark-code>\n \n Please let me know if you need anything else!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ddkang", "commentT": "2019-05-16T03:43:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ddkang>@ddkang</denchmark-link>\n , sorry for the delayed response.  Looks like this is the same version as the one flagged in <denchmark-link:https://github.com/open-mpi/ompi/issues/4437>open-mpi/ompi#4437</denchmark-link>\n  as buggy.  Could you try uninstalling hwloc () and reinstalling Open MPI?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ddkang", "commentT": "2019-05-17T01:50:32Z", "comment_text": "\n \t\tThat worked, thank you! It would be nice to have this in the documentation.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ddkang", "commentT": "2019-05-17T02:17:26Z", "comment_text": "\n \t\tGood idea, added to <denchmark-link:https://github.com/horovod/horovod/pull/1084>#1084</denchmark-link>\n . Will close this issue.\n \t\t"}}}, "commit": {"commit_id": "6a84d5dc02a5e243215c69378a513b617f630d36", "commit_author": "Alex Sergeev", "commitT": "2019-05-17 17:08:18-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "README.rst", "file_new_name": "README.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7,39,42,44,45,80,101,185,191,195,197,199,201,387", "deleted_lines": "7,39,42,44,45,80,101,185,189,192,197,198,199,200,203,204,390"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\index.rst", "file_new_name": "docs\\index.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9,10", "deleted_lines": null}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "docs\\mpirun.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\troubleshooting.md", "file_new_name": "docs\\troubleshooting.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "130,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376", "deleted_lines": "130"}}}}}}