{"BR": {"BR_id": "1725", "BR_author": "kangp3", "BRopenT": "2020-02-17T15:36:45Z", "BRcloseT": "2020-02-18T01:55:06Z", "BR_text": {"BRsummary": "PID KeyError in broadcast_optimizer_state", "BRdescription": "\n Environment:\n \n Framework: PyTorch\n Framework version: torch==1.4.0, torchvision==0.5.0\n Horovod version: horovod==0.19.0\n MPI version: Open MPI 4.0.0\n CUDA version: 10.0\n NCCL version: 2.4.2-1+cuda10.0\n Python version: 3.7.6\n OS and version: Ubuntu 18.04\n GCC version: 7.4.0\n \n Checklist:\n \n Did you search issues to find if somebody asked this question before?\n If your question is about hang, did you read this doc?\n If your question is about docker, did you read this doc?\n Did you check if you question is answered in the troubleshooting guide?\n \n Bug report:\n Please describe erroneous behavior you're observing and steps to reproduce it.\n When calling broadcast_optimizer_state we're seeing KeyErrors when trying to access the state_dict for apparently nonexistent pids on all processes, example stack trace attached:\n <denchmark-code>Traceback (most recent call last):\n   ...\n   File ************\n     HVD.broadcast_optimizer_state(optimizer, root_rank=0)\n   File \"/usr/local/lib/python3.7/dist-packages/horovod/torch/__init__.py\", line 572, in broadcast_optimizer_state\n     param_state = state_dict['state'][pid]\n KeyError: 140137585983888\n </denchmark-code>\n \n This is running on a single multi-GPU instance, and all processes are failing the same way (although with different pids). Any pointers appreciated, thanks!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kangp3", "commentT": "2020-02-17T15:56:44Z", "comment_text": "\n \t\tThis script seems to consistently reproduce the error in our environment:\n <denchmark-code>import torch\n import torchvision\n \n import horovod.torch as HVD\n \n HVD.init()\n torch.cuda.set_device(HVD.local_rank())\n torch.cuda.manual_seed(20)\n \n MODEL = torchvision.models.detection.keypointrcnn_resnet50_fpn()\n \n optimizer = torch.optim.SGD(\n     MODEL.parameters(), lr=0.001, weight_decay=1e-6, momentum=0.9, nesterov=True\n )\n \n HVD.broadcast_optimizer_state(optimizer, root_rank=0)\n optimizer = HVD.DistributedOptimizer(\n     optimizer, named_parameters=MODEL.named_parameters(), backward_passes_per_step=1\n )\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kangp3", "commentT": "2020-02-17T21:38:40Z", "comment_text": "\n \t\tThanks for raising this issue <denchmark-link:https://github.com/kangp3>@kangp3</denchmark-link>\n  and thanks for the repro <denchmark-link:https://github.com/amnda-d>@amnda-d</denchmark-link>\n .  Looks like this bug was introduced by <denchmark-link:https://github.com/horovod/horovod/pull/1609>#1609</denchmark-link>\n , where we fixed this method to skip setting state on params that do not require grads, but didn't fix the attempt to broadcast those params.\n <denchmark-link:https://github.com/horovod/horovod/pull/1726>#1726</denchmark-link>\n  should fix the issue, feel free to try it out and let me know how it goes.\n Thanks!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kangp3", "commentT": "2020-02-24T05:52:11Z", "comment_text": "\n \t\twhat model are you training? I encountered with this problem when I was training faster-rcnn.\n \t\t"}}}, "commit": {"commit_id": "705ca38d0e09dbb38034e11445a486ad1acab229", "commit_author": "Travis Addair", "commitT": "2020-02-17 17:55:05-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.5", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\requirements.txt", "file_new_name": "docs\\requirements.txt", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1", "deleted_lines": "1"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "horovod\\torch\\__init__.py", "file_new_name": "horovod\\torch\\__init__.py", "file_complexity": {"file_NLOC": "387", "file_CCN": "97", "file_NToken": "3017"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "572,573,574,575", "deleted_lines": null, "method_info": {"method_name": "broadcast_optimizer_state", "method_params": "optimizer,root_rank", "method_startline": "478", "method_endline": "599", "method_complexity": {"method_NLOC": "48", "method_CCN": "18", "method_NToken": "378", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\test_torch.py", "file_new_name": "test\\test_torch.py", "file_complexity": {"file_NLOC": "1251", "file_CCN": "290", "file_NToken": "10556"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1147,1153,1154,1155,1157", "deleted_lines": "1147,1154", "method_info": {"method_name": "test_broadcast_state_no_grad", "method_params": "self", "method_startline": "1130", "method_endline": "1157", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "189", "method_nesting_level": "1"}}}}}}}}