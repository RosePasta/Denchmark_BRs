{"BR": {"BR_id": "2110", "BR_author": "hoyden", "BRopenT": "2020-07-16T03:07:22Z", "BRcloseT": "2020-07-21T03:24:29Z", "BR_text": {"BRsummary": "Error in computing gradients when using allgather", "BRdescription": "\n Environment:\n \n Framework: TensorFlow\n Framework version: 2.0\n Horovod version:  0.18.2\n \n I am trying to get the median of a tensor computed across all batches and all processes. However, I got an error TypeError: Expected int32, got None of type 'NoneType' instead.It seems that computing gradients does not work well with horovod's allgather operation. A simple illustration of what I would like to achieve is as follows:\n \n with tf.GradientTape() as tape:\n \u2002\u2002\u2002\u2002my_tensor = compute_my_tensor()\n \u2002\u2002\u2002\u2002gathered_my_tensor = hvd.allgather(my_tensor)\n \u2002\u2002\u2002\u2002median = get_median(gathered_my_tensor)\n \u2002\u2002\u2002\u2002loss = get_loss(my_tensor, median, training=True)\n tape = hvd.DistributedGradientTape(tape)\n grads = tape.gradient(loss, trainable_variables)\n optimizer.apply_gradients(zip(grads, trainable_variables))\n \n BTW, when I use eager mode of tensorflow, there will be no error\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hoyden", "commentT": "2020-07-16T21:54:03Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/hoyden>@hoyden</denchmark-link>\n , where does exception get raised from?  And you're saying this only happens when wrapped in a ?\n Do you have a small example script I can run to try and repro the error?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hoyden", "commentT": "2020-07-17T07:17:04Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>\n  for the quich reply! The exception get raised from calculating gradients.And yes, it only happens when wrapped in a , you can repro the error with the follow script:\n <denchmark-code>import tensorflow as tf\n import horovod.tensorflow as hvd\n \n class TestModel(tf.keras.Model):\n \n     def __init__(self):\n         super().__init__()\n         data_description = tf.TensorShape(\n                 [None, 40, 1]\n             )\n         layers = tf.keras.layers\n         input_features = layers.Input(shape=data_description, dtype=tf.float32)\n         inner = layers.Conv2D(\n             filters=128,\n             kernel_size=(3, 3),\n             strides=(2, 2),\n             padding=\"same\",\n             use_bias=False,\n             data_format=\"channels_last\",\n         )(input_features)\n         inner = layers.BatchNormalization()(inner)\n         inner = tf.nn.relu6(inner)\n         inner = layers.Conv2D(\n             filters=128,\n             kernel_size=(3, 3),\n             strides=(2, 2),\n             padding=\"same\",\n             use_bias=False,\n             data_format=\"channels_last\",\n         )(inner)\n         inner = layers.BatchNormalization()(inner)\n \n         inner = tf.nn.relu6(inner)\n         _, _, dim, channels = inner.get_shape().as_list()\n         output_dim = dim * channels\n         inner = layers.Reshape((-1, output_dim))(inner)\n         inner = layers.Dense(512, activation=tf.nn.relu6)(inner)\n         self.x_net = tf.keras.Model(inputs=input_features, outputs=inner, name=\"x_net\")\n         self.final_layer = layers.Dense(128, input_shape=(512,))\n \n     def call(self, inputs, training: bool = None):\n         x0 = inputs\n         x = self.x_net(x0, training=training)\n         x = tf.math.reduce_mean(x, axis=1)\n         y = self.final_layer(x, training=training)\n         return y\n \n     def get_loss(self, logits):\n         loss = self.loss_function(logits)\n         return loss\n \n     def loss_function(self, logits):\n         cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n         hidden1, hidden2 = tf.split(logits, 2, 0)\n         batch_size = tf.shape(hidden1)[0]\n         hidden1_large = hvd.allgather(hidden1)\n         hidden2_large = hvd.allgather(hidden2)\n         enlarged_batch_size = tf.shape(hidden1_large)[0]\n         replica_id = hvd.rank()\n         labels_idx = tf.range(batch_size) + replica_id * batch_size\n         labels = tf.one_hot(labels_idx, enlarged_batch_size * 2)\n         logits_aa = tf.matmul(hidden1, hidden1_large, transpose_b=True)\n         logits_ab = tf.matmul(hidden1, hidden2_large, transpose_b=True)\n         loss = cross_entropy(labels, tf.concat([logits_ab, logits_aa], 1))\n         return loss\n \n class Solver():\n     def __init__(self):\n         self.model = TestModel()\n         self.input_signature = [tf.TensorSpec(\n         shape=(None, None, None, None), dtype=tf.float32\n     )]\n         self.optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n     def train_step(self, inputs):\n \n         with tf.GradientTape() as tape:\n             logits = self.model(inputs, training=True)\n             loss = self.model.get_loss(logits)\n         # Horovod: add Horovod Distributed GradientTape.\n         tape = hvd.DistributedGradientTape(tape)\n         grads = tape.gradient(loss, self.model.trainable_variables)\n         self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n \n     def train(self):\n         train_step = tf.function(self.train_step, input_signature=self.input_signature)\n         x = tf.random.normal([32, 150, 40, 1])\n         hvd.broadcast_variables(self.model.trainable_variables, root_rank=0)\n         hvd.broadcast_variables(self.optimizer.variables(), root_rank=0)\n         train_step(x)\n         tf.print(\"Done\")\n \n if __name__ == \"__main__\":\n     hvd.init()\n     gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n     for gpu in gpus:\n         tf.config.experimental.set_memory_growth(gpu, True)\n     if gpus:\n         tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], \"GPU\")\n     test_solver = Solver()\n     test_solver.train()\n \n \n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hoyden", "commentT": "2020-07-20T21:45:06Z", "comment_text": "\n \t\tThanks for the detailed repro script, <denchmark-link:https://github.com/hoyden>@hoyden</denchmark-link>\n .  That was very helpful to figure out what was going on.  Can you try running with <denchmark-link:https://github.com/horovod/horovod/pull/2121>#2121</denchmark-link>\n  and see if the addresses the issue?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hoyden", "commentT": "2020-07-21T03:02:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>\n  Thanks a lot! It works now!\n \t\t"}}}, "commit": {"commit_id": "459b67df38f9254b3d12cb7df6a49059e3134479", "commit_author": "Travis Addair", "commitT": "2020-07-21 05:57:37-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "horovod\\tensorflow\\mpi_ops.py", "file_new_name": "horovod\\tensorflow\\mpi_ops.py", "file_complexity": {"file_NLOC": "89", "file_CCN": "21", "file_NToken": "668"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "156,157", "deleted_lines": "156,157", "method_info": {"method_name": "_allgather_grad", "method_params": "op,grad", "method_startline": "141", "method_endline": "163", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "96", "method_nesting_level": "0"}}}}}}}}