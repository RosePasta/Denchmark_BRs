{"BR": {"BR_id": "1669", "BR_author": "ahmadki", "BRopenT": "2020-01-14T21:15:07Z", "BRcloseT": "2020-07-13T18:30:00Z", "BR_text": {"BRsummary": "mxnet allgather is borken. it crashed when using GPU and produces wrong results when using CPU", "BRdescription": "\n Environment:\n \n Framework: MXNet\n Framework version: 1.5.1\n Horovod version: built from source\n MPI version: 3.1.1\n CUDA version: 10.2\n NCCL version: 2.5.6\n Python version: 3.6.9\n OS and version: Linux Ubuntu 18.04.3\n GCC version: 7.4.0\n \n Checklist:\n \n Did you search issues to find if somebody asked this question before?\n If your question is about hang, did you read this doc? Yes\n If your question is about docker, did you read this doc?\n Did you check if you question is answered in the troubleshooting guide?\n \n \n Following PR <denchmark-link:https://github.com/horovod/horovod/pull/1639>#1639</denchmark-link>\n ,  in mxnet is still borken. It produces incorrect results when using CPU and crashes when using GPUs\n I installed horovod from source:\n git clone --recurse-submodules -j8 https://github.com/horovod/horovod.git\n cd horovod\n export HOROVOD_GPU_ALLREDUCE=NCCL\n export HOROVOD_NCCL_INCLUDE=/usr/include\n export HOROVOD_NCCL_LIB=/usr/lib/x86_64-linux-gnu\n export HOROVOD_NCCL_LINK=SHARED\n export HOROVOD_WITHOUT_PYTORCH=1\n export HOROVOD_WITHOUT_TENSORFLOW=1\n export HOROVOD_WITH_MXNET=1\n export HOROVOD_WITH_MPI=1\n ln -s /usr/local/cuda/lib64/stubs/libcuda.so ./libcuda.so.1\n export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD\n python setup.py install\n Then ran the following sample code:\n # CUDA_VISIBLE_DEVICES=0,1 mpiexec -n 2 --allow-run-as-root python all_gather_bug.py\n import mxnet as mx\n from mxnet import ndarray\n import horovod.mxnet as hvd\n from mpi4py import MPI\n \n use_gpu = False\n use_mpi4py = False\n \n if use_mpi4py:\n     comm = MPI.COMM_WORLD\n     hvd.init(comm=comm)\n     local_rank = comm.Get_rank()\n else:\n     hvd.init()\n     local_rank = hvd.local_rank()\n \n ctx = mx.gpu(local_rank) if use_gpu else mx.cpu(local_rank)\n mx.random.seed(local_rank)\n \n a = ndarray.random.uniform(shape=[10, 100, 100], ctx=ctx)\n print(\"(before) a.shape = {}\".format(a.shape))\n print(\"(before) a[0]={}\".format(a[0]))\n \n a = hvd.allgather(a)\n print(\"(after) a.shape = {}\".format(a.shape))\n print(\"(after) a[0]={}\".format(a[0]))\n \n mx.nd.waitall()\n hvd.shutdown()\n When using CPU, it looks like rank 0 is overwritting other ranks (ie broadcast instead of allgather):\n <denchmark-code>(before) a.shape = (10, 100, 100)\n (before) a.shape = (10, 100, 100)\n (before) a[0]=\n [[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01\n   4.1702199e-01 9.9718481e-01]\n  [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01\n   2.5926229e-02 9.3154085e-01]\n  [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01\n   2.9090473e-01 1.2132858e-01]\n  ...\n  [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01\n   4.6623814e-01 1.4771296e-01]\n  [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02\n   5.1639861e-01 5.5254018e-01]\n  [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01\n   6.7598689e-01 4.6260124e-01]]\n <NDArray 100x100 @cpu(0)>\n (after) a.shape = (10, 100, 100)\n (before) a[0]=\n [[0.52383333 0.05501367 0.03996297 ... 0.57466674 0.86663705 0.89201903]\n  [0.26314485 0.8478666  0.13140848 ... 0.47916418 0.26825976 0.62353116]\n  [0.60529524 0.56331843 0.08485638 ... 0.32175666 0.5547923  0.8856785 ]\n  ...\n  [0.8128833  0.50981677 0.89495724 ... 0.37793323 0.36698005 0.35633445]\n  [0.12747145 0.10071229 0.580568   ... 0.38354123 0.4238108  0.21727636]\n  [0.54174274 0.33404106 0.6885549  ... 0.9920475  0.7385572  0.32574102]]\n <NDArray 100x100 @cpu(1)>\n (after) a.shape = (10, 100, 100)\n (after) a[0]=\n [[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01\n   4.1702199e-01 9.9718481e-01]\n  [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01\n   2.5926229e-02 9.3154085e-01]\n  [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01\n   2.9090473e-01 1.2132858e-01]\n  ...\n  [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01\n   4.6623814e-01 1.4771296e-01]\n  [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02\n   5.1639861e-01 5.5254018e-01]\n  [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01\n   6.7598689e-01 4.6260124e-01]]\n <NDArray 100x100 @cpu(0)>\n (after) a[0]=\n [[5.4881352e-01 5.9284461e-01 7.1518934e-01 ... 6.0783064e-01\n   4.1702199e-01 9.9718481e-01]\n  [7.2032452e-01 9.3255734e-01 1.1438108e-04 ... 1.8508208e-01\n   2.5926229e-02 9.3154085e-01]\n  [5.4966247e-01 9.4773060e-01 4.3532240e-01 ... 8.3994907e-01\n   2.9090473e-01 1.2132858e-01]\n  ...\n  [2.0300540e-01 3.4745708e-01 6.7227858e-01 ... 9.6727759e-01\n   4.6623814e-01 1.4771296e-01]\n  [5.4340494e-01 6.7115563e-01 2.7836940e-01 ... 1.1400783e-02\n   5.1639861e-01 5.5254018e-01]\n  [5.7066756e-01 9.5268434e-01 2.8474227e-02 ... 3.9755744e-01\n   6.7598689e-01 4.6260124e-01]]\n <NDArray 100x100 @cpu(1)>\n </denchmark-code>\n \n When using GPUs the operation fails:\n <denchmark-code>(before) a.shape = (10, 100, 100)\n (before) a[0]=\n [[0.6686509  0.17409194 0.3850025  ... 0.43011498 0.0661214  0.2502998 ]\n  [0.7005292  0.19000232 0.6673837  ... 0.27718288 0.16084558 0.223108  ]\n  [0.96042585 0.81086403 0.54152083 ... 0.5650488  0.5196334  0.6767488 ]\n  ...\n  [0.96879214 0.9387428  0.04036242 ... 0.13176239 0.3436321  0.47154343]\n  [0.8069018  0.91234195 0.01141495 ... 0.35816687 0.57390726 0.68393874]\n  [0.72049534 0.67948174 0.44702923 ... 0.87448525 0.63809574 0.7006303 ]]\n <NDArray 100x100 @gpu(0)>\n (after) a.shape = (10, 100, 100)\n (before) a.shape = (10, 100, 100)\n (before) a[0]=\n [[0.7685592  0.10232276 0.8685353  ... 0.93769354 0.62144864 0.21535844]\n  [0.85973674 0.3420865  0.6202223  ... 0.5464046  0.41442537 0.32170743]\n  [0.11786121 0.23281038 0.95843846 ... 0.17739207 0.5901362  0.28355032]\n  ...\n  [0.70749086 0.61171615 0.37854642 ... 0.3485002  0.29636437 0.7359518 ]\n  [0.4345038  0.90834665 0.5242443  ... 0.0793817  0.40161872 0.6579807 ]\n  [0.8531954  0.18177992 0.7053579  ... 0.5257004  0.24457276 0.74836564]]\n <NDArray 100x100 @gpu(1)>\n (after) a.shape = (10, 100, 100)\n Traceback (most recent call last):\n   File \"all_gather_bug.py\", line 29, in <module>\n     print(\"(after) a[0]={}\".format(a[0]))\n   File \"/opt/mxnet/python/mxnet/ndarray/ndarray.py\", line 194, in __repr__\n     return '\\n%s\\n<%s %s @%s>' % (str(self.asnumpy()),\n   File \"/opt/mxnet/python/mxnet/ndarray/ndarray.py\", line 1996, in asnumpy\n     ctypes.c_size_t(data.size)))\n   File \"/opt/mxnet/python/mxnet/base.py\", line 252, in check_call\n     raise MXNetError(py_str(_LIB.MXGetLastError()))\n mxnet.base.MXNetError: MPI_Allgatherv failed, see MPI output for details.\n Traceback (most recent call last):\n   File \"all_gather_bug.py\", line 29, in <module>\n     print(\"(after) a[0]={}\".format(a[0]))\n   File \"/opt/mxnet/python/mxnet/ndarray/ndarray.py\", line 194, in __repr__\n     return '\\n%s\\n<%s %s @%s>' % (str(self.asnumpy()),\n   File \"/opt/mxnet/python/mxnet/ndarray/ndarray.py\", line 1996, in asnumpy\n     ctypes.c_size_t(data.size)))\n   File \"/opt/mxnet/python/mxnet/base.py\", line 252, in check_call\n     raise MXNetError(py_str(_LIB.MXGetLastError()))\n mxnet.base.MXNetError: MPI_Allgatherv failed, see MPI output for details.\n --------------------------------------------------------------------------\n Primary job  terminated normally, but 1 process returned\n a non-zero exit code. Per user-direction, the job has been aborted.\n --------------------------------------------------------------------------\n --------------------------------------------------------------------------\n mpiexec detected that one or more processes exited with non-zero status, thus causing\n the job to be terminated. The first process to do so was:\n \n   Process name: [[59002,1],1]\n   Exit code:    1\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ahmadki", "commentT": "2020-01-14T21:25:28Z", "comment_text": "\n \t\tThere is a known issue with MXNet allgather. <denchmark-link:https://github.com/horovod/horovod/issues/1409>#1409</denchmark-link>\n \n We are currently working to resolve it. Will keep you posted. Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ahmadki", "commentT": "2020-01-15T00:19:44Z", "comment_text": "\n \t\tI believe I found the issue.\n The operation receive buffer is defined to have the same size as the send buffer (instead of the sum of sizes from all ranks):\n <denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155>https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155</denchmark-link>\n \n This obviously results in truncated results on CPU and illegal memory access in CUDA.\n I'm working on a PR.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ahmadki", "commentT": "2020-01-15T06:01:48Z", "comment_text": "\n \t\tThanks. That'll be really great. I would love to review when it's ready.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ahmadki", "commentT": "2020-01-28T19:30:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ahmadki>@ahmadki</denchmark-link>\n  Do you need any help with this PR? Thanks!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ahmadki", "commentT": "2020-01-28T19:31:41Z", "comment_text": "\n \t\tThere is another report of this issue (<denchmark-link:https://github.com/horovod/horovod/issues/1519>#1519</denchmark-link>\n ). It would be great if you could provide an ETA for this fix. Thanks!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ahmadki", "commentT": "2020-01-30T23:10:48Z", "comment_text": "\n \t\tSorry for going  AWOL. Incorrect output tensor shape was only the symptom, and the problem is much deeper than I originally though. I got tied up with other things at work and I won't be able to take a second crack at a solution before next week. A summary of my findings so far:\n The allgather output nd-tensor is defined <denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.py#L155>here</denchmark-link>\n , This by it self is not an issue since MPI wrappers  reshape and reallocate the output memory buffers as needed.\n However, because hvd.allgather() is async, the original output tensor is returned immediately without modifications. We would expect subsequent calls to the output tensor to be blocking until it is re-evaluated but this is not the case.\n The output tensor buffer is shallow copied <denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/mxnet/mpi_ops.cc#L127>here</denchmark-link>\n  and changes made to it by <denchmark-link:https://github.com/horovod/horovod/blob/master/horovod/common/ops/collective_operations.h#L91>AllocateOutput</denchmark-link>\n  are not reflected on the original tensor.\n So a solution (probably) should include the creation of a null tensor using ctypes to keep memory ownership at the python level ? and removing the shallow copies.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ahmadki", "commentT": "2020-06-24T04:13:22Z", "comment_text": "\n \t\tAny updates here ? Got the same problem recently.\n \t\t"}}}, "commit": {"commit_id": "cf022be959a7c9431a8415729758b26dec1a87e5", "commit_author": "romerojosh", "commitT": "2020-07-13 11:29:59-07:00", "commit_complexity": {"commit_NLOC": "0.061855670103092786", "commit_CCN": "1.0", "commit_Nprams": "0.7835051546391752"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "horovod\\mxnet\\mpi_ops.cc", "file_new_name": "horovod\\mxnet\\mpi_ops.cc", "file_complexity": {"file_NLOC": "228", "file_CCN": "35", "file_NToken": "1607"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "156,158", "deleted_lines": "153,155", "method_info": {"method_name": "horovod::mxnet::DoHorovodOperationCudaOnCPU", "method_params": "on_complete_ptr,param", "method_startline": "150", "method_endline": "189", "method_complexity": {"method_NLOC": "37", "method_CCN": "4", "method_NToken": "253", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "130,131,132", "deleted_lines": "128,129", "method_info": {"method_name": "horovod::mxnet::PushHorovodOperation", "method_params": "op_type,input,output,name,priority,root_rank", "method_startline": "119", "method_endline": "147", "method_complexity": {"method_NLOC": "22", "method_CCN": "2", "method_NToken": "183", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "197,199,200,201,202,205,207,208,209,210,211,212,213,214,215,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231", "deleted_lines": "194,196,197,200,202,203,204,205,206,208,209", "method_info": {"method_name": "horovod::mxnet::PushHorovodOperationCudaOnCPU", "method_params": "op_type,input,output,name,priority,root_rank", "method_startline": "191", "method_endline": "232", "method_complexity": {"method_NLOC": "29", "method_CCN": "2", "method_NToken": "276", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "73,74,75,77,79,86,102", "deleted_lines": "72,73,75,77,84,100", "method_info": {"method_name": "horovod::mxnet::DoHorovodOperation", "method_params": "on_complete_ptr,param", "method_startline": "68", "method_endline": "117", "method_complexity": {"method_NLOC": "45", "method_CCN": "5", "method_NToken": "326", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "horovod\\mxnet\\mpi_ops.h", "file_new_name": "horovod\\mxnet\\mpi_ops.h", "file_complexity": {"file_NLOC": "69", "file_CCN": "3", "file_NToken": "330"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "52,53,54,59,60,61", "deleted_lines": "54,63", "method_info": {"method_name": "horovod::mxnet::MpiOpsParam::MpiOpsParam", "method_params": "input_tensor,output_tensor,output,cpu_input_tensor,cpu_output_tensor,op_type,op_name,root_rank", "method_startline": "50", "method_endline": "65", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "75", "method_nesting_level": "3"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "61", "deleted_lines": "63,67,68", "method_info": {"method_name": "horovod::mxnet::CreateMpiOpsParam", "method_params": "input_tensor,output_tensor,cpu_tensor,op_type,op_name,root_rank", "method_startline": "61", "method_endline": "69", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "45", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "70,71,72,76,77", "deleted_lines": "68", "method_info": {"method_name": "horovod::mxnet::CreateMpiOpsParam", "method_params": "input_tensor,output_tensor,output,cpu_input_tensor,cpu_output_tensor,op_type,op_name,root_rank", "method_startline": "68", "method_endline": "78", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "56", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "52,53,54", "deleted_lines": "49,54", "method_info": {"method_name": "horovod::mxnet::MpiOpsParam::MpiOpsParam", "method_params": "input_tensor,output_tensor,cpu_tensor,op_type,op_name,root_rank", "method_startline": "47", "method_endline": "58", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "58", "method_nesting_level": "3"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "horovod\\mxnet\\mpi_ops.py", "file_new_name": "horovod\\mxnet\\mpi_ops.py", "file_complexity": {"file_NLOC": "93", "file_CCN": "11", "file_NToken": "729"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "152,153,154,164,165,166", "deleted_lines": "151", "method_info": {"method_name": "allgather", "method_params": "tensor,name,priority", "method_startline": "126", "method_endline": "167", "method_complexity": {"method_NLOC": "14", "method_CCN": "2", "method_NToken": "123", "method_nesting_level": "0"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "test\\test_mxnet.py", "file_new_name": "test\\test_mxnet.py", "file_complexity": {"file_NLOC": "508", "file_CCN": "99", "file_NToken": "4252"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605", "deleted_lines": null, "method_info": {"method_name": "test_horovod_allgather_type_error", "method_params": "self", "method_startline": "582", "method_endline": "605", "method_complexity": {"method_NLOC": "17", "method_CCN": "4", "method_NToken": "119", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557", "deleted_lines": null, "method_info": {"method_name": "test_horovod_allgather_variable_size", "method_params": "self", "method_startline": "524", "method_endline": "557", "method_complexity": {"method_NLOC": "25", "method_CCN": "4", "method_NToken": "245", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580", "deleted_lines": null, "method_info": {"method_name": "test_horovod_allgather_error", "method_params": "self", "method_startline": "559", "method_endline": "580", "method_complexity": {"method_NLOC": "15", "method_CCN": "3", "method_NToken": "98", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522", "deleted_lines": null, "method_info": {"method_name": "test_horovod_allgather", "method_params": "self", "method_startline": "501", "method_endline": "522", "method_complexity": {"method_NLOC": "18", "method_CCN": "3", "method_NToken": "183", "method_nesting_level": "1"}}}}}}}}