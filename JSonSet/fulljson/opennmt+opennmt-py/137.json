{"BR": {"BR_id": "137", "BR_author": "ketranm", "BRopenT": "2017-07-17T23:23:35Z", "BRcloseT": "2017-07-22T21:44:36Z", "BR_text": {"BRsummary": "Beam search bug", "BRdescription": "\n The output of translate.py is inconsistent with different -batch_size option. When I trained on IWSLT data and test on ted2013 de->en, the difference is significant, about 9 BLEU between -batch_size 1 and -batch_size 30.\n I think one of the culprits is in  <denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Translator.py#L201>https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Translator.py#L201</denchmark-link>\n \n When a completed hypothesis is found, we shouldn't advance the beam, otherwise, it will overwrite the best found hypothesis. This can be fixed for example\n def advance(self, wordLk, attnOut):\n     return True if self.is_done \n     numWords = wordLk.size(1)\n     ...\n at the beginning of Beam.advance.\n There is still another bug somewhere, I haven't figured out. I would appreciate for any pointer to fix this bug.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ketranm", "commentT": "2017-07-17T23:35:21Z", "comment_text": "\n \t\tDoes the issue you describe have to do with batch_size? seems like it would always apply.\n When you get a working model, let's setup an integration test to make sure this dataset remains correct.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ketranm", "commentT": "2017-07-18T02:33:42Z", "comment_text": "\n \t\tThere are lots of ways to write batched beam search that have subtle dependencies on the batch size, typically because they allow the fact that one example in the batch has output an EOS to affect the termination of other examples. I'm not super familiar with the OpenNMT implementation though.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ketranm", "commentT": "2017-07-19T22:24:01Z", "comment_text": "\n \t\tno this is clearly a bug. batch beam search must be identical to regular.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ketranm", "commentT": "2017-07-19T22:34:48Z", "comment_text": "\n \t\tIs it a bug if the (yes, incorrect) batched beam search implementation gets slightly better BLEU than regular beam search because it's doing some kind of implicit length normalization? Because that's something I've seen before...\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ketranm", "commentT": "2017-07-19T22:46:40Z", "comment_text": "\n \t\tnot sure what do you mean by implicit length normalization. Current OpenNMT-py considers a beam is finished if EOS is generated at the top of the beam. This is also an issue I wonder is there any reason we don't normalize the log probabilities by sentence length as it's done in Google's paper.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ketranm", "commentT": "2017-07-22T14:39:23Z", "comment_text": "\n \t\tI can confirm that results on my multilingual grapheme-to-phoneme task became markedly better when I reduced my batch_size to 1. Discovering this is a big deal for me; it seems to explain a lot of my trouble reproducing my OpenNMT-lua results in python.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ketranm", "commentT": "2017-07-22T17:27:11Z", "comment_text": "\n \t\tThis sounds more like a padding management issue than a beam search issue. The sequence lengths were not passed to the encoder during translation, thus the encoder's states were incorrect with variable length inputs.\n According to my quick validation test, this is the fix for the issue. Can anyone confirm?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ketranm", "commentT": "2017-07-22T17:35:45Z", "comment_text": "\n \t\tOh, this sounds right to me.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ketranm", "commentT": "2017-07-22T17:53:41Z", "comment_text": "\n \t\tit looks good. I get comparable results now.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "ketranm", "commentT": "2017-07-22T21:44:30Z", "comment_text": "\n \t\tThanks Guilluime. Closing up. Tests coming soon.\n \t\t"}}}, "commit": {"commit_id": "025d3006bfeeee475863a25db56a761d2da50c6f", "commit_author": "Guillaume Klein", "commitT": "2017-07-22 19:21:36+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onmt\\Translator.py", "file_new_name": "onmt\\Translator.py", "file_complexity": {"file_NLOC": "206", "file_CCN": "53", "file_NToken": "1865"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "109", "deleted_lines": "109", "method_info": {"method_name": "translateBatch", "method_params": "self,batch", "method_startline": "104", "method_endline": "243", "method_complexity": {"method_NLOC": "104", "method_CCN": "26", "method_NToken": "1029", "method_nesting_level": "1"}}}}}}}}