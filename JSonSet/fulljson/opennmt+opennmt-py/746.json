{"BR": {"BR_id": "746", "BR_author": "wasiahmad", "BRopenT": "2018-06-05T05:50:59Z", "BRcloseT": "2018-09-03T12:02:38Z", "BR_text": {"BRsummary": "[BUG] dynamic_dict/copy_attn does not work with accum_count &gt; 1", "BRdescription": "\n I am getting the following error when I tried to train transformer model on CNNDM dataset for summarization.\n \n Loading train dataset from data/cnndm/CNNDM.train.3.pt, number of examples: 33018\n Traceback (most recent call last):\n File \"train.py\", line 502, in \n main()\n File \"train.py\", line 494, in main\n train_model(model, fields, optim, data_type, model_opt)\n File \"train.py\", line 257, in train_model\n train_stats = trainer.train(train_iter, epoch, report_func)\n File \"/home/wasiahmad/workspace/projects/OpenNMT-py/onmt/Trainer.py\", line 178, in train\n report_stats, normalization)\n File \"/home/wasiahmad/workspace/projects/OpenNMT-py/onmt/Trainer.py\", line 311, in _gradient_accumulation\n trunc_size, self.shard_size, normalization)\n File \"/home/wasiahmad/workspace/projects/OpenNMT-py/onmt/Loss.py\", line 123, in sharded_compute_loss\n loss, stats = self._compute_loss(batch, **shard)\n File \"/home/wasiahmad/workspace/projects/OpenNMT-py/onmt/modules/CopyGenerator.py\", line 191, in _compute_loss\n batch, self.tgt_vocab, self.cur_dataset.src_vocabs)\n File \"/home/wasiahmad/workspace/projects/OpenNMT-py/onmt/io/TextDataset.py\", line 116, in collapse_copy_scores\n src_vocab = src_vocabs[index]\n IndexError: list index out of range\n \n The training works fine for the first part of the data but whenever it loads the second part (ex., CNNDM.train.3.pt file), it gives index out of range error. Any idea what is hapenning?\n I have downloaded the dataset from the link mentioned <denchmark-link:http://opennmt.net/OpenNMT-py/Summarization.html>here</denchmark-link>\n  and did the preprocessing without any error. Please help.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wasiahmad", "commentT": "2018-06-19T07:29:51Z", "comment_text": "\n \t\tHi, same problem for me. Anyone knows why?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wasiahmad", "commentT": "2018-06-19T16:48:52Z", "comment_text": "\n \t\tI tried a lot but couldn't find the reason behind this error.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wasiahmad", "commentT": "2018-08-03T21:24:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/vince62s>@vince62s</denchmark-link>\n  <denchmark-link:https://github.com/wasiahmad>@wasiahmad</denchmark-link>\n  what was the eventual resolution?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wasiahmad", "commentT": "2018-08-03T21:28:01Z", "comment_text": "\n \t\tdid you try with the lates code + pytorch 0.4 + torchtext 0.3 ?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "wasiahmad", "commentT": "2018-08-03T21:29:28Z", "comment_text": "\n \t\ti tried it on master as of 3 days ago, currently rerunning now\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "wasiahmad", "commentT": "2018-08-04T00:18:56Z", "comment_text": "\n \t\tI tried but didn't work and then stopped trying. It was around 1.5 months ago!!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "wasiahmad", "commentT": "2018-08-30T10:18:46Z", "comment_text": "\n \t\tokay, I can replicate this. We'll look into it.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "wasiahmad", "commentT": "2018-08-30T18:14:20Z", "comment_text": "\n \t\tThanks to <denchmark-link:https://github.com/pltrdy>@pltrdy</denchmark-link>\n  who helped figuring it out.\n The issue is as follow:\n When accum_count > 1, when using copy_attn (and therefore dynamic_dict) there is a mismatch if the batches within the \"true_batch\" = accum_count x batch are accross different shards of the dataset.\n For instance, at the end of the first shard, if we have accum_count = 4, we can have 2 batches loaded from the first shard and the last 2 from the second shard.\n In this context, we have an issue here: <denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/copy_generator.py#L195-L197>https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/copy_generator.py#L195-L197</denchmark-link>\n \n where self.cur_dataset.src_vocabs may refer to the wrong shard.\n This is not easy to solve.\n In the meantime, use only accum_count = 1 with copy_attn and if you can use more GPU.\n I have tried to go back in time and in fact it seems to have never worked.\n So I am a bit confused that the tuto specification with copy_attn + accum_count = 4\n <denchmark-link:https://github.com/srush>@srush</denchmark-link>\n  <denchmark-link:https://github.com/sebastianGehrmann>@sebastianGehrmann</denchmark-link>\n   can you check the config of the model you posted for summarization ?\n Thanks.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "wasiahmad", "commentT": "2018-09-03T12:02:38Z", "comment_text": "\n \t\tfixed in <denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/938>#938</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "f45c7cecf3ebee82692b4a1a708905ca4b52e336", "commit_author": "moses", "commitT": "2018-09-03 13:52:10+02:00", "commit_complexity": {"commit_NLOC": "0.6666666666666666", "commit_CCN": "0.3333333333333333", "commit_Nprams": "0.3333333333333333"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "onmt\\inputters\\inputter.py", "file_new_name": "onmt\\inputters\\inputter.py", "file_complexity": {"file_NLOC": "344", "file_CCN": "73", "file_NToken": "2418"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "445", "deleted_lines": "443,444,445", "method_info": {"method_name": "get_cur_dataset", "method_params": "self", "method_startline": "443", "method_endline": "445", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "10", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "445,450,455", "deleted_lines": "443,444,445,446,449,454,459", "method_info": {"method_name": "_next_dataset_iterator", "method_params": "self,dataset_iter", "method_startline": "443", "method_endline": "459", "method_complexity": {"method_NLOC": "12", "method_CCN": "2", "method_NToken": "70", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onmt\\modules\\copy_generator.py", "file_new_name": "onmt\\modules\\copy_generator.py", "file_complexity": {"file_NLOC": "152", "file_CCN": "10", "file_NToken": "996"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "193", "deleted_lines": "197", "method_info": {"method_name": "_compute_loss", "method_params": "self,batch,output,target,copy_attn,align", "method_startline": "174", "method_endline": "220", "method_complexity": {"method_NLOC": "26", "method_CCN": "2", "method_NToken": "282", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "onmt\\trainer.py", "file_new_name": "onmt\\trainer.py", "file_complexity": {"file_NLOC": "240", "file_CCN": "33", "file_NToken": "1436"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "148,149", "method_info": {"method_name": "train", "method_params": "self,train_iter_fct,valid_iter_fct,train_steps,valid_steps", "method_startline": "110", "method_endline": "210", "method_complexity": {"method_NLOC": "73", "method_CCN": "17", "method_NToken": "455", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "224,225,226", "method_info": {"method_name": "validate", "method_params": "self,valid_iter", "method_startline": "212", "method_endline": "248", "method_complexity": {"method_NLOC": "18", "method_CCN": "3", "method_NToken": "131", "method_nesting_level": "1"}}}}}}}}