{"BR": {"BR_id": "290", "BR_author": "fernandocamargoti", "BRopenT": "2018-08-09T11:22:09Z", "BRcloseT": "2018-08-13T13:48:18Z", "BR_text": {"BRsummary": "loss_std resulting in complex number and breaking Tensorboard", "BRdescription": "\n I'm using torchbearer with PyTorch 0.4 and TensorboardX 1.2. Previously, I was using PyTorch 0.4.1, but I had to downgrade to use the TensorboardX because of a incompatibility with them. After adding the Tensorboard callback, the following error is raised after training for some time:\n {TypeError}can't convert complex to float\n When debugging, I noticed that the add_scalar() of TensorboardX tried to convert the scalar to float and, somehow, the val_loss_std was a complex number. Is there and error in how the std is calculated in order to result in a complex number?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "fernandocamargoti", "commentT": "2018-08-09T12:37:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/fernandocamargoti>@fernandocamargoti</denchmark-link>\n  Thanks for the issue.\n I had a look at the std calculation and I can see that it would go complex if the losses are extremely small, for example, in my testing I get a complex number if I calculate the std([1e-30, 1e-30]). I think pytorch float tensors default to single precision, so this is probably some underflow error.\n We should definitely have a check for this and at least return -1 or something similar instead of blindly returning a complex number. I'll make this change now.\n Are your validation losses very small or do you think the problem is elsewhere?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "fernandocamargoti", "commentT": "2018-08-09T13:22:17Z", "comment_text": "\n \t\tHello, <denchmark-link:https://github.com/MattPainter01>@MattPainter01</denchmark-link>\n .\n I'm not sure it's the case. Have I look at the prints before the error:\n <denchmark-code>0/100(t): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01<00:00,  1.83it/s, running_loss=0.775, precision=0.012, recall=0.8, loss_std=0.0259, loss=0.749]\n 0/100(v): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.99it/s, val_precision=0.012, val_recall=0.757, val_loss_std=0.000117, val_loss=0.67]\n 1/100(t): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  3.54it/s, running_loss=0.722, precision=0.0111, recall=0.189, loss_std=0.0363, loss=0.632]\n 1/100(v): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 16.32it/s, val_precision=0.0108, val_recall=0.0969, val_loss_std=6.22e-05, val_loss=0.507]\n 2/100(t): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.89it/s, running_loss=0.654, precision=0.0101, recall=0.0667, loss_std=0.0483, loss=0.458]\n 2/100(v): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.95it/s, val_precision=0.0133, val_recall=0.0344, val_loss_std=3.43e-21+5.61e-05j, val_loss=0.314]\n Traceback (most recent call last):\n   File \"/home/fernandocamargo/datascience_workspace/recommendation-system/test.py\", line 6, in <module>\n     task.run()\n   File \"/home/fernandocamargo/datascience_workspace/recommendation-system/recommendation/task/base.py\", line 77, in run\n     self.train()\n   File \"/home/fernandocamargo/datascience_workspace/recommendation-system/recommendation/task/base.py\", line 127, in train\n     callbacks=self._get_callbacks())\n   File \"/home/fernandocamargo/anaconda3/envs/recommendation-system/lib/python3.6/site-packages/torchbearer/torchbearer.py\", line 209, in fit_generator\n     _callbacks.on_end_epoch(state)\n   File \"/home/fernandocamargo/anaconda3/envs/recommendation-system/lib/python3.6/site-packages/torchbearer/callbacks/callbacks.py\", line 281, in on_end_epoch\n     self._for_list(lambda callback: callback.on_end_epoch(state))\n   File \"/home/fernandocamargo/anaconda3/envs/recommendation-system/lib/python3.6/site-packages/torchbearer/callbacks/callbacks.py\", line 191, in _for_list\n     function(callback)\n   File \"/home/fernandocamargo/anaconda3/envs/recommendation-system/lib/python3.6/site-packages/torchbearer/callbacks/callbacks.py\", line 281, in <lambda>\n     self._for_list(lambda callback: callback.on_end_epoch(state))\n   File \"/home/fernandocamargo/anaconda3/envs/recommendation-system/lib/python3.6/site-packages/torchbearer/callbacks/tensor_board.py\", line 97, in on_end_epoch\n     self._writer.add_scalar('epoch/' + metric, state[torchbearer.METRICS][metric], state[torchbearer.EPOCH])\n   File \"/home/fernandocamargo/anaconda3/envs/recommendation-system/lib/python3.6/site-packages/tensorboardX/writer.py\", line 272, in add_scalar\n     self.file_writer.add_summary(scalar(tag, scalar_value), global_step)\n   File \"/home/fernandocamargo/anaconda3/envs/recommendation-system/lib/python3.6/site-packages/tensorboardX/summary.py\", line 88, in scalar\n     scalar = float(scalar)\n TypeError: can't convert complex to float\n </denchmark-code>\n \n When the val_loss was 0.507, the val_loss_std was 6.22e-05. But when the val_loss was 0.314, the error happened and the val_loss_std was 3.43e-21+5.61e-05j.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "fernandocamargoti", "commentT": "2018-08-09T13:23:34Z", "comment_text": "\n \t\tMy current workaround was to disable the std for the loss, adding this in my code:\n <denchmark-code>@metrics.default_for_key('loss')\n @metrics.running_mean\n @metrics.mean\n class SimpleLossFactory(metrics.MetricFactory):\n         def build(self):\n             return Loss()\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "fernandocamargoti", "commentT": "2018-08-09T14:20:42Z", "comment_text": "\n \t\tDoes the error still occur when you have more than 1 validation sample?\n For single samples or multiple samples with the same value then precision errors casting from pytorch floats to python floats can give us negative variances.\n For the moment I'll set it to return a variance of 0 for these situations, checkout branch fix/std_complex in the meantime until we merge this.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "fernandocamargoti", "commentT": "2018-08-13T13:48:18Z", "comment_text": "\n \t\tClosed by <denchmark-link:https://github.com/pytorchbearer/torchbearer/pull/296>#296</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "3246e2a447ed544a7f48218095f19f0836fbb5f3", "commit_author": "Matt Painter", "commitT": "2018-08-10 10:15:21+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "20", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\metrics\\test_aggregators.py", "file_new_name": "tests\\metrics\\test_aggregators.py", "file_complexity": {"file_NLOC": "90", "file_CCN": "20", "file_NToken": "943"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "38,39,40,41,42,43,44,45", "deleted_lines": null, "method_info": {"method_name": "test_precision_error", "method_params": "self", "method_startline": "38", "method_endline": "45", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "59", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "torchbearer\\metrics\\aggregators.py", "file_new_name": "torchbearer\\metrics\\aggregators.py", "file_complexity": {"file_NLOC": "114", "file_CCN": "21", "file_NToken": "609"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "139,140,141,142,143", "deleted_lines": "139", "method_info": {"method_name": "process_final", "method_params": "self,args", "method_startline": "131", "method_endline": "143", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "51", "method_nesting_level": "1"}}}}}}}}