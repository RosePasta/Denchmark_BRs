{"BR": {"BR_id": "94", "BR_author": "houchangtao", "BRopenT": "2019-06-12T00:30:05Z", "BRcloseT": "2019-06-20T09:25:59Z", "BR_text": {"BRsummary": "Dynamical Feature with Different Length will Throw Exception", "BRdescription": "\n System: Ubuntu 16.04\n Python: 3.6.4\n mxnet: 1.4.1\n Code to reproduce:\n import pandas as pd\n url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv\"\n df = pd.read_csv(url, header=0, index_col=0)\n from gluonts.dataset.common import ListDataset\n training_data = [\n     {\"start\": pd.Timestamp(df.index[0], freq='5min'), \"target\": df.value[:\"2015-04-05 00:00:00\"], \n      \"feat_dynamic_real\": pd.to_datetime(df[:\"2015-04-05 00:00:00\"].index).dayofweek.values},\n     {\"start\": pd.Timestamp(df.index[0], freq='5min'), \"target\": df.value[:\"2015-04-10 00:00:00\"], \n      \"feat_dynamic_real\": pd.to_datetime(df[:\"2015-04-10 00:00:00\"].index).dayofweek.values}\n ]\n from gluonts.model.deepar import DeepAREstimator\n from gluonts.trainer import Trainer\n from gluonts.distribution import NegativeBinomialOutput, StudentTOutput\n estimator = DeepAREstimator(freq=\"5min\", prediction_length=12, distr_output=NegativeBinomialOutput(),\n                             trainer=Trainer(epochs=10))\n predictor = estimator.train(training_data=training_data)\n Exceptions:\n <denchmark-code>KeyError                                  Traceback (most recent call last)\n <ipython-input-9-7512b68a283e> in <module>()\n       1 estimator = DeepAREstimator(freq=\"5min\", prediction_length=12, distr_output=NegativeBinomialOutput(),\n       2                             trainer=Trainer(epochs=10))\n ----> 3 predictor = estimator.train(training_data=training_data)\n \n /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/model/estimator.py in train(self, training_data)\n     187     def train(self, training_data: Dataset) -> Predictor:\n     188 \n --> 189         training_transformation, trained_net = self.train_model(training_data)\n     190 \n     191         # ensure that the prediction network is created within the same MXNet\n \n /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/model/estimator.py in train_model(self, training_data)\n     180             net=trained_net,\n     181             input_names=get_hybrid_forward_input_names(trained_net),\n --> 182             train_iter=training_data_loader,\n     183         )\n     184 \n \n /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/trainer/_base.py in __call__(self, net, input_names, train_iter)\n     249 \n     250                     with tqdm(train_iter) as it:\n --> 251                         for batch_no, data_entry in enumerate(it, start=1):\n     252                             if self.halt:\n     253                                 break\n \n /usr/local/lib/python3.6/site-packages/tqdm/_tqdm.py in __iter__(self)\n    1003                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n    1004 \n -> 1005             for obj in iterable:\n    1006                 yield obj\n    1007                 # Update and possibly print the progressbar.\n \n /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in __iter__(self)\n     200             ):\n     201                 for batch in self._emit_batches_while_buffer_larger_than(\n --> 202                     self.batch_size - 1\n     203                 ):\n     204                     yield batch\n \n /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in _emit_batches_while_buffer_larger_than(self, thresh)\n     167             self._buffer.shuffle()\n     168         while len(self._buffer) > thresh:\n --> 169             yield self._buffer.next_batch()\n     170 \n     171     def _iterate_forever(\n \n /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in next_batch(self)\n      58             #print(np.asarray(v[:n]).dtype)\n      59             #print(v[:n])\n ---> 60             batch[k] = self.stack(v[:n])\n      61         #batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}\n      62         for key in self._buffers.keys():\n \n /usr/local/lib/python3.6/site-packages/gluonts-0.1.1-py3.6.egg/gluonts/dataset/loader.py in stack(self, xs)\n      70             if data.dtype.kind == 'f':\n      71                 data = data.astype(self.float_type)\n ---> 72             return mx.nd.array(data, dtype=data.dtype, ctx=self.ctx)\n      73         elif isinstance(xs[0], mx.nd.NDArray):\n      74             return mx.nd.stack(*xs)\n \n /usr/local/lib/python3.6/site-packages/mxnet/ndarray/utils.py in array(source_array, ctx, dtype)\n     144         return _sparse_array(source_array, ctx=ctx, dtype=dtype)\n     145     else:\n --> 146         return _array(source_array, ctx=ctx, dtype=dtype)\n     147 \n     148 \n \n /usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in array(source_array, ctx, dtype)\n    2486             except:\n    2487                 raise TypeError('source_array must be array like object')\n -> 2488     arr = empty(source_array.shape, ctx, dtype)\n    2489     arr[:] = source_array\n    2490     return arr\n \n /usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in empty(shape, ctx, dtype)\n    3875     if dtype is None:\n    3876         dtype = mx_real_t\n -> 3877     return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))\n    3878 \n    3879 \n \n /usr/local/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py in _new_alloc_handle(shape, ctx, delay_alloc, dtype)\n     137         ctypes.c_int(ctx.device_id),\n     138         ctypes.c_int(int(delay_alloc)),\n --> 139         ctypes.c_int(int(_DTYPE_NP_TO_MX[np.dtype(dtype).type])),\n     140         ctypes.byref(hdl)))\n     141     return hdl\n \n KeyError: <class 'numpy.object_'>\n </denchmark-code>\n \n Error comes from: \n \n \n gluon-ts/src/gluonts/dataset/loader.py\n \n \n          Line 62\n       in\n       3c6d110\n \n \n \n \n \n \n  data = np.asarray(xs) \n \n \n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "houchangtao", "commentT": "2019-06-12T09:52:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/houchangtao>@houchangtao</denchmark-link>\n  One issue with your example is that you're feeding a list directly as a dataset for training.\n Using the ListDataset would be appropriate here, but this results in an error as well:\n import pandas as pd\n url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv\"\n df = pd.read_csv(url, header=0, index_col=0)\n from gluonts.dataset.common import ListDataset\n training_data = ListDataset(\n     data_iter=[\n         {\"start\": pd.Timestamp(df.index[0], freq='5min'), \"target\": df.value[:\"2015-04-05 00:00:00\"],\n          \"feat_dynamic_real\": pd.to_datetime(df[:\"2015-04-05 00:00:00\"].index).dayofweek.values},\n         {\"start\": pd.Timestamp(df.index[0], freq='5min'), \"target\": df.value[:\"2015-04-10 00:00:00\"],\n          \"feat_dynamic_real\": pd.to_datetime(df[:\"2015-04-10 00:00:00\"].index).dayofweek.values}\n     ],\n     freq=\"5min\"\n )\n from gluonts.model.deepar import DeepAREstimator\n from gluonts.trainer import Trainer\n from gluonts.distribution import NegativeBinomialOutput\n estimator = DeepAREstimator(freq=\"5min\", prediction_length=12, distr_output=NegativeBinomialOutput(),\n                             trainer=Trainer(epochs=10))\n predictor = estimator.train(training_data=training_data)\n Results in:\n <denchmark-code>Traceback (most recent call last):\n   File \"/Users/stellalo/gluon-ts/temp/run_issue_94.py\", line 19, in <module>\n     predictor = estimator.train(training_data=training_data)\n   File \"/Users/stellalo/gluon-ts/src/gluonts/model/estimator.py\", line 189, in train\n     training_transformation, trained_net = self.train_model(training_data)\n   File \"/Users/stellalo/gluon-ts/src/gluonts/model/estimator.py\", line 182, in train_model\n     train_iter=training_data_loader,\n   File \"/Users/stellalo/gluon-ts/src/gluonts/trainer/_base.py\", line 251, in __call__\n     for batch_no, data_entry in enumerate(it, start=1):\n   File \"/Users/stellalo/.virtualenvs/gluonts/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 930, in __iter__\n     for obj in iterable:\n   File \"/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py\", line 195, in __iter__\n     self.batch_size - 1\n   File \"/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py\", line 162, in _emit_batches_while_buffer_larger_than\n     yield self._buffer.next_batch()\n   File \"/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py\", line 54, in next_batch\n     batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}\n   File \"/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py\", line 54, in <dictcomp>\n     batch = {k: self.stack(v[:n]) for k, v in self._buffers.items()}\n   File \"/Users/stellalo/gluon-ts/src/gluonts/dataset/loader.py\", line 62, in stack\n     data = np.asarray(xs)\n   File \"/Users/stellalo/.virtualenvs/gluonts/lib/python3.6/site-packages/numpy/core/numeric.py\", line 492, in asarray\n     return array(a, dtype, copy=False, order=order)\n ValueError: could not broadcast input array from shape (10684) into shape (1)\n \n Process finished with exit code 1\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "houchangtao", "commentT": "2019-06-12T11:17:21Z", "comment_text": "\n \t\tMWE:\n from gluonts.dataset.common import ListDataset\n from gluonts.model.deepar import DeepAREstimator\n from gluonts.trainer import Trainer\n training_data = ListDataset(\n     data_iter=[\n         {\"start\": \"2019-01-01 00:00:00\", \"target\": [1.0, 2.0, 3.0, 4.0],\n          \"feat_dynamic_real\": [1.0, 2.0, 3.0, 4.0]},\n         {\"start\": \"2019-01-01 00:00:00\", \"target\": [1.0, 2.0, 3.0, 4.0, 5.0],\n          \"feat_dynamic_real\": [1.0, 2.0, 3.0, 4.0, 5.0]},\n     ],\n     freq=\"5min\"\n )\n estimator = DeepAREstimator(freq=\"5min\", prediction_length=2,\n                             trainer=Trainer(epochs=10))\n predictor = estimator.train(training_data=training_data)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "houchangtao", "commentT": "2019-06-12T13:50:26Z", "comment_text": "\n \t\tThe problem is that the  field is <denchmark-link:https://github.com/awslabs/gluon-ts/blob/cd5df5814797e602ec4a525d89f4bc84b6e5e82f/src/gluonts/dataset/common.py#L258>processed</denchmark-link>\n  as part of the , but not stacked together with other time-dependent fields <denchmark-link:https://github.com/awslabs/gluon-ts/blob/a8c60fac38296268e5b55afb828568021ff23531/src/gluonts/model/deepar/_estimator.py#L189-L192>in the transformation chain</denchmark-link>\n . Therefore <denchmark-link:https://github.com/awslabs/gluon-ts/blob/cd5df5814797e602ec4a525d89f4bc84b6e5e82f/src/gluonts/dataset/loader.py#L51-L58>when a batch of data is formed</denchmark-link>\n  this field is stacked as-is between different entries in the dataset.\n As far as I can see, solving this issues means either:\n \n (easier) at the beginning of the transformation, explicitly filtering out any fields which will not be consumed\n (harder) completing the transformation chain of DeepAREstimator (and potentially other models), and make it consume all possible fields, possibly defaulting the missing ones (so that no KeyErrors are raised)\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "houchangtao", "commentT": "2019-06-20T09:25:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awslabs/gluon-ts/pull/125>#125</denchmark-link>\n  solves this issues: now features must be explicitly enabled, when constructing the estimator, in order for the model to use them.\n \t\t"}}}, "commit": {"commit_id": "6ec5c828dc7f7cc2d25d6d316e84042069938dd0", "commit_author": "Lorenzo Stella", "commitT": "2019-06-20 11:24:32+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.5384615384615384"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\gluonts\\model\\deepar\\_estimator.py", "file_new_name": "src\\gluonts\\model\\deepar\\_estimator.py", "file_complexity": {"file_NLOC": "258", "file_CCN": "7", "file_NToken": "1005"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "171,172,173,174,175,176,177,179,180,181,182,183,184,185,186,211,212,213,214,215,216", "deleted_lines": "191,193,194,195,196", "method_info": {"method_name": "create_transformation", "method_params": "self", "method_startline": "170", "method_endline": "232", "method_complexity": {"method_NLOC": "61", "method_CCN": "4", "method_NToken": "288", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "108,109,110", "deleted_lines": "107,108,109", "method_info": {"method_name": "__init__", "method_params": "self,str,int,Trainer", "method_startline": "97", "method_endline": "115", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "129", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\gluonts\\model\\seq2seq\\_transform.py", "file_new_name": "src\\gluonts\\model\\seq2seq\\_transform.py", "file_complexity": {"file_NLOC": "83", "file_CCN": "4", "file_NToken": "555"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9,108", "deleted_lines": "9,108"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "src\\gluonts\\transform.py", "file_new_name": "src\\gluonts\\transform.py", "file_complexity": {"file_NLOC": "1080", "file_CCN": "108", "file_NToken": "4612"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "831,839,840,841,842", "deleted_lines": null, "method_info": {"method_name": "_update_cache", "method_params": "self,Timestamp,int", "method_startline": "830", "method_endline": "853", "method_complexity": {"method_NLOC": "24", "method_CCN": "6", "method_NToken": "173", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "380,381,382,383,384", "deleted_lines": null, "method_info": {"method_name": "transform", "method_params": "self,DataEntry", "method_startline": "380", "method_endline": "384", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "34", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "77,79,80,81,82,84", "deleted_lines": "77,79,80,82", "method_info": {"method_name": "shift_timestamp", "method_params": "Timestamp,int", "method_startline": "77", "method_endline": "84", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "88,89", "deleted_lines": null, "method_info": {"method_name": "_shift_timestamp_helper", "method_params": "Timestamp,str,int", "method_startline": "88", "method_endline": "89", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "88,89,90,99", "deleted_lines": "86,95,96", "method_info": {"method_name": "_compute_date_helper", "method_params": "ts,freq,offset", "method_startline": "86", "method_endline": "107", "method_complexity": {"method_NLOC": "6", "method_CCN": "2", "method_NToken": "52", "method_nesting_level": "0"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "377,378", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self", "method_startline": "377", "method_endline": "378", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "77,79,80,81,82", "deleted_lines": "77,79,80,82", "method_info": {"method_name": "compute_date", "method_params": "Timestamp,int", "method_startline": "77", "method_endline": "82", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "0"}}}}}, "file_3": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "test\\model\\test_deepar_cat.py", "file_new_name": "None", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_4": {"file_change_type": "RENAME", "file_Nmethod": 0, "file_old_name": "test\\model\\test_deepar.py", "file_new_name": "test\\model\\test_deepar_lags.py", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_5": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "test\\model\\test_deepar_smoke.py", "file_complexity": {"file_NLOC": "117", "file_CCN": "2", "file_NToken": "595"}}}}}