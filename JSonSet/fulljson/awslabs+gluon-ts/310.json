{"BR": {"BR_id": "310", "BR_author": "lcallot", "BRopenT": "2019-09-13T13:14:46Z", "BRcloseT": "2019-10-11T14:35:24Z", "BR_text": {"BRsummary": "Incorrect number of parameters in DeepARTrainingNetwork", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n The number of parameters in the training network logged by DeepAR is incorrect. The number of parameters\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n from gluonts.model.deepar import DeepAREstimator\n from gluonts.dataset.common import ListDataset\n from gluonts.trainer import Trainer\n \n import numpy as np\n import pandas as pd\n \n url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv\"\n df = pd.read_csv(url, header=0, index_col=0)\n \n training_data = ListDataset(\n     [{\"start\": df.index[0], \"target\": df.value[:\"2015-04-05 00:00:00\"]}],\n     freq = \"5min\"\n )\n \n def count_model_params(net) -> int:\n     params = net.collect_params()\n     num_params = 0\n     for p in params:\n         v = params[p]\n         num_params += np.prod(v.shape)\n     return num_params\n \n trainer = Trainer(epochs=1,batch_size=1,num_batches_per_epoch=1)\n estimator = DeepAREstimator(freq=\"5min\", prediction_length=12, trainer=trainer)\n predictor = estimator.train_model(training_data=training_data)\n npar = count_model_params(net=predictor.trained_net)\n print(f\"Number of parameters counted after training: {npar}\")\n Output\n <denchmark-code>INFO:root:Using CPU\n INFO:root:Start model training\n INFO:root:Number of parameters in DeepARTrainingNetwork: 13463\n INFO:root:Epoch[0] Learning rate is 0.001\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 13.78it/s, avg_epoch_loss=5.52]\n INFO:root:Epoch[0] Elapsed time 0.074 seconds\n INFO:root:Epoch[0] Evaluation metric 'epoch_loss'=5.524269\n INFO:root:Loading parameters from best epoch (0)\n INFO:root:Final loss: 5.524268627166748 (occurred at epoch 0)\n INFO:root:End model training\n \n Number of parameters counted after training: 29743\n </denchmark-code>\n \n Changing the length of lags_seq has no effect on the count before training, but does after.\n trainer = Trainer(epochs=1,batch_size=1,num_batches_per_epoch=1)\n estimator = DeepAREstimator(freq=\"5min\", prediction_length=12, trainer=trainer,\n                            lags_seq=[1])\n predictor = estimator.train_model(training_data=training_data)\n npar = count_model_params(net=predictor.trained_net)\n print(f\"Number of parameters counted after training: {npar}\")\n Output\n <denchmark-code>INFO:root:Using CPU\n INFO:root:Start model training\n INFO:root:Number of parameters in DeepARTrainingNetwork: 13463\n INFO:root:Epoch[0] Learning rate is 0.001\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 13.60it/s, avg_epoch_loss=5.18]\n INFO:root:Epoch[0] Elapsed time 0.075 seconds\n INFO:root:Epoch[0] Evaluation metric 'epoch_loss'=5.181353\n INFO:root:Loading parameters from best epoch (0)\n INFO:root:Final loss: 5.181352615356445 (occurred at epoch 0)\n INFO:root:End model training\n \n Number of parameters counted after training: 24463\n </denchmark-code>\n \n <denchmark-h:h2>Error Message</denchmark-h>\n \n No error\n <denchmark-h:h2>Environment</denchmark-h>\n \n \n Operating system: Sagemaker notebook + training job (Amazon Linux?)\n Python version: 3.7\n GluonTS version:3.3\n \n (Add as much information about your environment as possible, e.g. dependencies versions.)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lcallot", "commentT": "2019-09-13T13:19:01Z", "comment_text": "\n \t\tThe problem is that <denchmark-link:https://github.com/awslabs/gluon-ts/blob/f20cf9e32bd9ae7e96cf87d21925373421fcd55e/src/gluonts/trainer/_base.py#L193-L196>these lines</denchmark-link>\n  should be executed after at least one forward pass, so moved further down in the training loop (with some flag guarding their execution).\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lcallot", "commentT": "2019-09-30T07:15:55Z", "comment_text": "\n \t\tI would like to take this up <denchmark-link:https://github.com/lcallot>@lcallot</denchmark-link>\n  <denchmark-link:https://github.com/lostella>@lostella</denchmark-link>\n   Can you help me in solving this issue?\n Thanks!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lcallot", "commentT": "2019-09-30T08:14:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Patil2099>@Patil2099</denchmark-link>\n  sure! Thanks for the help.\n The issue is in <denchmark-link:https://github.com/awslabs/gluon-ts/blob/22da305e8d52d3fccadd4dd3000859b7c48e62e0/src/gluonts/trainer/_base.py#L193-L196>the lines counting the parameters</denchmark-link>\n : they are invoked before any forward pass on the network is performed, so the number of inputs to the network is unknown, and therfore the number of parameters in the network is inexact.\n The forward pass happens <denchmark-link:https://github.com/awslabs/gluon-ts/blob/22da305e8d52d3fccadd4dd3000859b7c48e62e0/src/gluonts/trainer/_base.py#L252>here</denchmark-link>\n , so you may want to move the parameter counting after this point. Note however that this should happens only once in the training loop, so you can put a safeguard that makes sure they are invoked only at the first iteration.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lcallot", "commentT": "2019-10-11T14:35:23Z", "comment_text": "\n \t\tThis PR <denchmark-link:https://github.com/awslabs/gluon-ts/pull/386>#386</denchmark-link>\n  fixes the issue.\n \t\t"}}}, "commit": {"commit_id": "a63d26315a9fb6737a982a4f3176ad5857b815c5", "commit_author": "timoschowski", "commitT": "2019-10-11 16:32:29+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\gluonts\\trainer\\_base.py", "file_new_name": "src\\gluonts\\trainer\\_base.py", "file_complexity": {"file_NLOC": "253", "file_CCN": "9", "file_NToken": "1137"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "267,268,269,270,271,272,273", "deleted_lines": "192,193,194,195,196,197"}}}}}}