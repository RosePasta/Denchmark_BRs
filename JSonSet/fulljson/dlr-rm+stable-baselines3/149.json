{"BR": {"BR_id": "149", "BR_author": "diditforlulz273", "BRopenT": "2020-08-27T10:57:59Z", "BRcloseT": "2020-09-01T07:52:32Z", "BR_text": {"BRsummary": "PPO clip_fraction logging bug", "BRdescription": "\n I think I've found a minor bug in your PPO algo realisation.\n See, in the main class PPO(OnPolicyAlgorithm), def train() function, after all the loops and main code there is a logging part:\n logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n logger.record(\"train/value_loss\", np.mean(value_losses))\n logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n logger.record(\"train/clip_fraction\", np.mean(clip_fraction))\n logger.record(\"train/loss\", loss.item())\n logger.record(\"train/explained_variance\", explained_var)\n take a closer look at logging variables. While np.mean is used over lists of accumulated values everywhere, for clip fraction you have np.mean(clip_fraction), NOT clip_fractions. So you always log the fraction from the last rollout data batch only, not avg for the full algo iteration.\n I'm pretty certain I'm right. I can make a PR correcting it if you agree.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "diditforlulz273", "commentT": "2020-08-27T11:01:26Z", "comment_text": "\n \t\tHello,\n nice catch.\n \n I can make a PR correcting it if you agree.\n \n please do ;) (don't forget to read the CONTRIBUTING guide first)\n \t\t"}}}, "commit": {"commit_id": "4fd408bec2a25ba3543520317b753d908078fe5e", "commit_author": "Vsevolod Kompantsev", "commitT": "2020-09-01 09:52:31+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\misc\\changelog.rst", "file_new_name": "docs\\misc\\changelog.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "21,402", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "stable_baselines3\\ppo\\ppo.py", "file_new_name": "stable_baselines3\\ppo\\ppo.py", "file_complexity": {"file_NLOC": "211", "file_CCN": "17", "file_NToken": "1275"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "231", "deleted_lines": "231", "method_info": {"method_name": "train", "method_params": "self", "method_startline": "133", "method_endline": "240", "method_complexity": {"method_NLOC": "69", "method_CCN": "12", "method_NToken": "692", "method_nesting_level": "1"}}}}}}}}