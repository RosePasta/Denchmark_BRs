{"BR": {"BR_id": "2965", "BR_author": "przybyszewskiw", "BRopenT": "2020-08-19T11:40:19Z", "BRcloseT": "2020-08-31T22:59:04Z", "BR_text": {"BRsummary": "Shape inference on the Torchvision\u2019s Mask R-CNN causes a segmentation fault", "BRdescription": "\n <denchmark-h:h1>Bug Report</denchmark-h>\n \n <denchmark-h:h3>Is the issue related to model conversion?</denchmark-h>\n \n Probably not.\n <denchmark-h:h3>Describe the bug</denchmark-h>\n \n When I try to run shape inference on the Torchvision's Mask R-CNN it causes a segfault. However, check_model doesn't return any warning/exception.\n <denchmark-h:h3>System information</denchmark-h>\n \n \n OS Platform and Distribution (e.g. Linux Ubuntu 16.04): Linux Ubuntu 18.04.4\n ONNX version (e.g. 1.7):  1.6.0\n Python version: 3.6.10\n PyTorch version: 1.6.0a0+9907a3e\n Torchvision version: 0.7.0a0\n GCC/Compiler version (if compiling from source): N/A\n CMake version: 3.14.0\n Protobuf version: 3.12.2\n Visual Studio version (if applicable): N/A\n \n <denchmark-h:h3>Reproduction instructions</denchmark-h>\n \n <denchmark-code>import torch\n import torchvision\n import onnx\n \n model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n model.eval()\n x = [torch.rand(3, 800, 800)]\n torch.onnx.export(model, x, \"mask_rcnn.onnx\", opset_version = 11)\n onnx_model = onnx.load(\"mask_rcnn.onnx\")\n onnx.checker.check_model(onnx_model)\n onnx.shape_inference.infer_shapes(onnx_model)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Shape inference doesn't cause any errors.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "przybyszewskiw", "commentT": "2020-08-19T18:15:48Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/przybyszewskiw>@przybyszewskiw</denchmark-link>\n ,\n Sorry that current shape inference does not catch the exception properly.\n This segfault bug will be fixed in this PR: <denchmark-link:https://github.com/onnx/onnx/pull/2783>#2783</denchmark-link>\n .\n After applying this PR, I tried to check this model with onnx.checker.check_model('mask_rcnn.onnx', full_check=True) and got the error as follows:\n <denchmark-code>Invalid tensor data type.\n </denchmark-code>\n \n For detailed error messages, I \"shape_inference\" the model and got the error as follows:\n <denchmark-code>Shape inference error(s): (op_type:Gather, name:Gather_2359): [TypeInferenceError] Input 0 expected to have tensor type\n (op_type:Gather, name:Gather_2360): [TypeInferenceError] Input 0 expected to have tensor type\n (op_type:Slice, name:Slice_2389): [TypeInferenceError] Input 0 expected to have tensor type\n (op_type:Slice, name:Slice_2398): [TypeInferenceError] Input 0 expected to have tensor type\n \n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "przybyszewskiw", "commentT": "2020-08-20T10:18:16Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/jcwchen>@jcwchen</denchmark-link>\n ,\n Thank you for your reply. I extended my script, so now it performs an inference in the  as well:\n <denchmark-code>import torch\n import torchvision\n import onnxruntime\n import numpy as np\n import requests\n from io import BytesIO\n from PIL import Image\n \n def to_numpy(tensor):\n     return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n \n def get_image(url):\n     response = requests.get(url)\n     pil_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n     return torch.tensor(np.array(pil_image)).permute(2, 1, 0).to(dtype=torch.float)/255\n \n model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n model.eval()\n \n url = \"http://farm3.staticflickr.com/2469/3915380994_2e611b1779_z.jpg\"\n x = get_image(url)\n \n torch_out = model([x])\n torch.onnx.export(model, [x], \"mask_rcnn.onnx\", opset_version=11, input_names = ['x'])\n \n ort_session = onnxruntime.InferenceSession(\"mask_rcnn.onnx\")\n ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n ort_outs = ort_session.run(None, ort_inputs)\n \n for i, key in enumerate(torch_out[0].keys()):\n     np.testing.assert_allclose(to_numpy(torch_out[0][key]), ort_outs[i], rtol=1e-03, atol=2e-05)\n \n print(\"Exported model passed the test!\")\n </denchmark-code>\n \n It seems like the exported model does the inference correctly, even though the type inference fails. Could you explain this inconsistency?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "przybyszewskiw", "commentT": "2020-08-20T15:59:03Z", "comment_text": "\n \t\tCan you specify what the type inference failure is? As far I know, type inference of onnxruntime must finish successfully so it should print error messages if any.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "przybyszewskiw", "commentT": "2020-08-20T17:19:55Z", "comment_text": "\n \t\tThere is no error when I run the script above. I mentioned the type inference failure you wrote above. How is it that I can run inference on my model without any problem, but when I run shape_inference it segfaults because of TypeInferenceErrors?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "przybyszewskiw", "commentT": "2020-08-20T17:39:05Z", "comment_text": "\n \t\tI think the reason is onnxruntime has its own shape_inference and it only uses part of onnx.shape_inference (node-level). If you use shape inference from onnxruntime successfully, the type inference of your model should be done.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "przybyszewskiw", "commentT": "2020-08-21T11:10:26Z", "comment_text": "\n \t\tAnd what about shape and type inferences used in C api? Do they differ from Python ones?\n I wrote a C++ program that tries to run the onnx model I exported before:\n <denchmark-code>#include <assert.h>\n #include <onnxruntime_c_api.h>\n #include <stdlib.h>\n #include <stdio.h>\n \n \n const OrtApi* g_ort = OrtGetApiBase()->GetApi(ORT_API_VERSION);\n \n void CheckStatus(OrtStatus* status)\n {\n     if (status != NULL) {\n       const char* msg = g_ort->GetErrorMessage(status);\n       fprintf(stderr, \"%s\\n\", msg);\n       g_ort->ReleaseStatus(status);\n       exit(1);\n     }\n }\n \n int main(int argc, char* argv[]) {\n   // initialize  enviroment\n   OrtEnv* env;\n   CheckStatus(g_ort->CreateEnv(ORT_LOGGING_LEVEL_WARNING, \"test\", &env));\n \n   // initialize session options if needed\n   OrtSessionOptions* session_options;\n   CheckStatus(g_ort->CreateSessionOptions(&session_options));\n   g_ort->SetIntraOpNumThreads(session_options, 1);\n \n   // Sets graph optimization level\n   g_ort->SetSessionGraphOptimizationLevel(session_options, ORT_ENABLE_BASIC);\n \n   // create session and load model into memory\n   OrtSession* session;\n   const char* model_path = \"/workspace/mask_rcnn.onnx\";\n   CheckStatus(g_ort->CreateSession(env, model_path, session_options, &session));\n \n   printf(\"done\\n\");\n   return 0;\n }\n </denchmark-code>\n \n and it fails:\n <denchmark-code>Node (ConstantOfShape_1888) Op (ConstantOfShape) [ShapeInferenceError] Invalid shape value: 0\n </denchmark-code>\n \n Do you know why is that? Is there any way to run my ONNX model using C++, knowing that I can run it using Python?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "przybyszewskiw", "commentT": "2020-08-24T23:56:32Z", "comment_text": "\n \t\tThis is a bit strange. As far as I know, the inference used by onnxruntime is the same regardless of whether you use the python or C/C++ API. There is a slightly different implementation of the inference in onnxruntime and ONNX. But even here, if the behavior is different, then one of them needs to be fixed. This seems worth investigating.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "przybyszewskiw", "commentT": "2020-08-31T22:59:04Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/gramalingam>@gramalingam</denchmark-link>\n  for providing a PR to fix the type inference! This issue should be resolved by that PR. <denchmark-link:https://github.com/przybyszewskiw>@przybyszewskiw</denchmark-link>\n  if you still encounter the same error, please reopen it. Thank you.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "przybyszewskiw", "commentT": "2020-09-01T04:50:03Z", "comment_text": "\n \t\t(This might be an onnxruntime issue)\n <denchmark-link:https://github.com/przybyszewskiw>@przybyszewskiw</denchmark-link>\n  One more thing: the gap between Python API and C API for onnxruntime. This is not supposed to happen.\n \n Could you try the following code on your end? Let's see whether it will encounter the same error as C API.\n \n <denchmark-code>import onnxruntime as ort\n so = onnxrt.SessionOptions()\n so.graph_optimization_level = onnxrt.GraphOptimizationLevel.ORT_ENABLE_BASIC\n onnxrt.InferenceSession(\"mask_rcnn.onnx\") \n </denchmark-code>\n \n \n Did you build C API and Python API with the same source code? What is your onnxruntime and onnx version?\n \n Thank you for the help.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "przybyszewskiw", "commentT": "2020-09-02T06:18:39Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jcwchen>@jcwchen</denchmark-link>\n  after the PR I don't have any problems with the  function in Python API. Thanks for that!\n When it comes to this gap between APIs:\n \n I don't encounter any bug while running your code - modifying SessionOptions doesn't change anything;\n Yes, my C and Python APIs are built with the same source code. My onnx version is 1.6.0 and onnxruntime version is 1.3.0.\n \n Can you reproduce that bug? Should I start a new issue for it?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "przybyszewskiw", "commentT": "2020-09-02T06:40:15Z", "comment_text": "\n \t\tThanks for the answer. My previous testing versions of onnxruntime and onnx are different from yours and I will try to reproduce it. Could you try to test the same model with  newer onnxruntime 1.4.0 and onnx 1.7.0 if possible?\n If the Python API and C API still behave differently, feel free to open an issue under onnxruntime. Let's track this problem there. Thanks!\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "przybyszewskiw", "commentT": "2020-09-02T07:24:01Z", "comment_text": "\n \t\tIt looks like when I use onnx 1.7.0 and onnxruntime 1.4.0, model loads correctly in both APIs. Thanks for your help!\n \t\t"}}}, "commit": {"commit_id": "53057782d19a59548705802fca756d87dee032e2", "commit_author": "G. Ramalingam", "commitT": "2020-08-28 16:19:50-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnx\\defs\\tensor\\utils.cc", "file_new_name": "onnx\\defs\\tensor\\utils.cc", "file_complexity": {"file_NLOC": "147", "file_CCN": "35", "file_NToken": "977"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "50", "deleted_lines": "53", "method_info": {"method_name": "ONNX_NAMESPACE::resizeShapeInference", "method_params": "ctx,is_resize_op", "method_startline": "49", "method_endline": "100", "method_complexity": {"method_NLOC": "49", "method_CCN": "14", "method_NToken": "335", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnx\\shape_inference\\implementation.cc", "file_new_name": "onnx\\shape_inference\\implementation.cc", "file_complexity": {"file_NLOC": "432", "file_CCN": "110", "file_NToken": "3154"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "194,196,197,198,200", "deleted_lines": "195,196,197,198,199,200,201", "method_info": {"method_name": "ONNX_NAMESPACE::shape_inference::InferShapesImpl", "method_params": "g,outer_scope_value_types_by_name,opset_imports,check_type,schema_registry,ir_version", "method_startline": "156", "method_endline": "338", "method_complexity": {"method_NLOC": "143", "method_CCN": "37", "method_NToken": "1065", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "onnx\\test\\shape_inference_test.py", "file_new_name": "onnx\\test\\shape_inference_test.py", "file_complexity": {"file_NLOC": "2882", "file_CCN": "337", "file_NToken": "36645"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "2652", "deleted_lines": "2652", "method_info": {"method_name": "test_split_to_sequence", "method_params": "self", "method_startline": "2646", "method_endline": "2654", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "107", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "2714", "deleted_lines": "2714", "method_info": {"method_name": "test_split_to_sequence_split_sizes", "method_params": "self", "method_startline": "2708", "method_endline": "2716", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "109", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "2688", "deleted_lines": "2688", "method_info": {"method_name": "test_split_to_sequence_ignore_keepdims", "method_params": "self", "method_startline": "2682", "method_endline": "2690", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "111", "method_nesting_level": "1"}}}}}}}}