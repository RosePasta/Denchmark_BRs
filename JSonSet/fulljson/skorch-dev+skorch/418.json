{"BR": {"BR_id": "418", "BR_author": "thomasjpfan", "BRopenT": "2019-01-04T19:34:57Z", "BRcloseT": "2019-05-01T12:52:09Z", "BR_text": {"BRsummary": "LRScheduler's batch_idx_ includes validation batches", "BRdescription": "\n LRScheduler's batch_idx_ counts both validation and training batches. This means the learning rate is updated CyclicLR during validation batch steps.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "thomasjpfan", "commentT": "2019-01-08T17:24:27Z", "comment_text": "\n \t\tI would like to use the train_loss or train_batch_size in the history as a proxy for how many training iterations have occurred. The train_loss and train_batch_size are recorded by default in the fit_loop.\n This can be done by adding a parameter to LRScheduler called train_batch_indicator with default value: train_loss.\n I think this would cover 99% of the use cases.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "thomasjpfan", "commentT": "2019-01-09T22:31:53Z", "comment_text": "\n \t\t\n I would like to use the train_loss or train_batch_size in the history as a proxy\n \n Let's try to prevent using proxies, since this creates unnecessary coupling. Would it be enough to look at the  argument passed to  (<denchmark-link:https://github.com/dnouri/skorch/blob/5542209d7d3d1b30bc6bfb87e11b0278fb27f89e/skorch/callbacks/lr_scheduler.py#L148>here</denchmark-link>\n ) and only increment if ?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "thomasjpfan", "commentT": "2019-01-09T23:40:23Z", "comment_text": "\n \t\tUsing training will work as long as the learning rate scheduler records the number of training batches in the history. This way the total number of training batches can be calculated when training resumes.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "thomasjpfan", "commentT": "2019-01-10T16:06:06Z", "comment_text": "\n \t\t\n Using training will work as long as the learning rate scheduler records the number of training batches in the history\n \n You mean it must be in the history in case that after loading the model only, you would like to continue training or are there other cases that I overlook? In that case, should we rather have the net itself count the number of training batches?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "thomasjpfan", "commentT": "2019-01-10T16:26:53Z", "comment_text": "\n \t\t\n in case that after loading the model only, you would like to continue training\n \n This is the use case I am considering.\n \n should we rather have the net itself count the number of training batches?\n \n That works too. We can place the counting logic  in NeuralNet.on_batch_end.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "thomasjpfan", "commentT": "2019-03-15T15:09:23Z", "comment_text": "\n \t\tIf I understand correctly, the current proposal is to have two attributes in NeuralNet:\n \n batch_count_train_\n batch_count_valid_\n both holding the current count of processed batches. The upper bound of this index is not known in advance (since there might be a dynamic amount of batches per epoch) and we can't guarantee that this number is evenly divisible by a fixed batch size (same argument). The last points might get in the way when users expect this to hold (i.e. num_epochs = (batch_count_train + batch_count_valid)/batch_size or something similar).\n \n The purpose of such a counter is to have a global step count. This does not solve the problem of having no local step count, i.e. there is no way to know how many training batches currently have passed without coupling to train_loss in the history (batch_count = len(history[-1, 'batches', :, 'train_loss'])).\n We could, of course, introduce 4 variables, one for global and one for local count or track all local counts in a list but this is just reinventing the wheel (history), I guess.\n Maybe we provide something like this:\n global_train_steps = sum(history[:, 'batches', :, 'is_training'])\n local_train_steps = sum(history[-1, 'batches', :, 'is_training'])\n for which we would need to introduce a new history key (is_training). What I dislike about this is that it feels incredibly wasteful (num_epochs * num_batches * m byte to count 2 numbers).\n So in the end I think that introducing storage to NeuralNet is preferable. We could think about adding these counts to history instead (e.g. as attributes, history.current_train_steps).\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "thomasjpfan", "commentT": "2019-03-16T22:47:51Z", "comment_text": "\n \t\tI would prefer this information to be in the history. This would keep track of state when the history gets loaded from a file.\n What do you think of adding history[:, 'batch_count_train'] and history[:, 'batch_count_valid'] to keep track of the count?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "thomasjpfan", "commentT": "2019-03-17T10:38:19Z", "comment_text": "\n \t\t\n 2. The last points might get in the way when users expect this to hold (i.e. `num_epochs = (batch_count_train + batch_count_valid)/batch_size` or something similar).\n \n \n Do you have an example in mind where someone would make that assumption? Would it be enough to document the behavior somewhere (\"If you want to know the number of training batches, use this key from history...\")\n \n I would prefer this information to be in the history.\n \n History seems to be the right place for this kind of information.\n \n What do you think of adding history[:, 'batch_count_train'] and history[:, 'batch_count_valid'] to keep track of the count?\n \n This looks reasonable. That number would be the non-cumulative number of batches and if you need the total, you should sum them. In theory, the number of batches can be misleading when not taking account of 'train_batch_size' and 'valid_batch_size', but for most practical purposes, that might not be too important.\n \t\t"}}}, "commit": {"commit_id": "e1dc86d5f6a17a63e52964bf12ab9a55dbe17590", "commit_author": "Thomas J Fan", "commitT": "2019-05-01 14:52:09+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGES.md", "file_new_name": "CHANGES.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "skorch\\callbacks\\logging.py", "file_new_name": "skorch\\callbacks\\logging.py", "file_complexity": {"file_NLOC": "261", "file_CCN": "61", "file_NToken": "1203"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "151", "deleted_lines": null, "method_info": {"method_name": "_sorted_keys", "method_params": "self,keys", "method_startline": "132", "method_endline": "161", "method_complexity": {"method_NLOC": "19", "method_CCN": "14", "method_NToken": "146", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "skorch\\callbacks\\lr_scheduler.py", "file_new_name": "skorch\\callbacks\\lr_scheduler.py", "file_complexity": {"file_NLOC": "347", "file_CCN": "51", "file_NToken": "1502"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "148,149", "method_info": {"method_name": "on_batch_end", "method_params": "self,net,kwargs", "method_startline": "148", "method_endline": "149", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "144,146", "deleted_lines": "148,149", "method_info": {"method_name": "on_batch_begin", "method_params": "self,net,training,kwargs", "method_startline": "144", "method_endline": "150", "method_complexity": {"method_NLOC": "7", "method_CCN": "4", "method_NToken": "45", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "152,153,154", "deleted_lines": null, "method_info": {"method_name": "on_batch_end", "method_params": "self,net,training,kwargs", "method_startline": "152", "method_endline": "154", "method_complexity": {"method_NLOC": "3", "method_CCN": "2", "method_NToken": "20", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "144,146", "deleted_lines": "141", "method_info": {"method_name": "on_batch_begin", "method_params": "self,net,kwargs", "method_startline": "141", "method_endline": "146", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "41", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "119,120,121,122", "deleted_lines": "119", "method_info": {"method_name": "on_train_begin", "method_params": "self,net,kwargs", "method_startline": "117", "method_endline": "125", "method_complexity": {"method_NLOC": "9", "method_CCN": "4", "method_NToken": "76", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "skorch\\net.py", "file_new_name": "skorch\\net.py", "file_complexity": {"file_NLOC": "839", "file_CCN": "182", "file_NToken": "4837"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "738,747,748,754,763,764", "deleted_lines": null, "method_info": {"method_name": "fit_loop", "method_params": "self,X,y,epochs,fit_params", "method_startline": "686", "method_endline": "767", "method_complexity": {"method_NLOC": "40", "method_CCN": "8", "method_NToken": "377", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "skorch\\tests\\callbacks\\test_lr_scheduler.py", "file_new_name": "skorch\\tests\\callbacks\\test_lr_scheduler.py", "file_complexity": {"file_NLOC": "409", "file_CCN": "53", "file_NToken": "3179"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "129,130,131,132,133,134", "deleted_lines": null, "method_info": {"method_name": "test_lr_callback_batch_steps_correctly_fallback", "method_params": "self,classifier_module,classifier_data,policy,kwargs", "method_startline": "129", "method_endline": "134", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "12", "method_nesting_level": "1"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "skorch\\tests\\test_net.py", "file_new_name": "skorch\\tests\\test_net.py", "file_complexity": {"file_NLOC": "1676", "file_CCN": "205", "file_NToken": "13287"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088", "deleted_lines": null, "method_info": {"method_name": "test_batch_count", "method_params": "self,net_cls,module_cls,data,batch_size", "method_startline": "2078", "method_endline": "2088", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "96", "method_nesting_level": "1"}}}}}}}}