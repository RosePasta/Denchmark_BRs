{"BR": {"BR_id": "317", "BR_author": "stsievert", "BRopenT": "2018-08-20T23:01:16Z", "BRcloseT": "2018-08-23T08:12:48Z", "BR_text": {"BRsummary": "module weights don't update after copy.deepcopy", "BRdescription": "\n When a model is copied with copy.deepcopy, the weights are frozen and don't update when fit is called.\n Here's an example:\n from sklearn.datasets import make_regression\n import torch.nn as nn\n import torch\n from skorch import NeuralNetRegressor\n import copy\n import toolz\n \n def get_data(n, d):\n     X, y = make_regression(n_features=d, n_samples=n)\n     y -= y.min()\n     y /= y.max()\n     y = y.reshape(-1, 1).astype('float32')\n \n     X = X.astype('float32')\n     return X, y\n \n X, y = get_data(1000, 28 * 28)\n \n class Simple(nn.Module):\n     def __init__(self):\n         super().__init__()\n         self.simple = nn.Sequential(\n             nn.Linear(28 * 28, 1),\n             nn.Sigmoid(),\n         )\n         \n     def forward(self, x):\n         return self.simple(x)\n n1 = NeuralNetRegressor(\n     module=Simple,\n ).initialize()\n \n def check(net, name=''):\n     p = toolz.first(net.module_.parameters()).detach().numpy()\n     print(name + ' first params =', p.flat[:3])\n \n def assert_all_params_equal(net1, net2):\n     for p1, p2 in zip(net1.module_.parameters(), net2.module_.parameters()):\n         assert p1.requires_grad\n         assert p2.requires_grad\n         assert torch.all(p1 == p2)\n     \n check(n1, name=\"n1\")\n n1.partial_fit(X, y)\n check(n1, name=\"n1\")\n \n n2 = copy.deepcopy(n1)\n check(n2, name=\"n2\")\n assert_all_params_equal(n1, n2)\n \n n3 = n2.partial_fit(X, y[::-1])\n \n check(n1, name=\"n1\")\n check(n2, name=\"n2\")\n check(n3, name=\"n3\")\n \n assert_all_params_equal(n2, n3)\n assert_all_params_equal(n1, n2)\n which, when run, outputs this:\n <denchmark-code>n1 first params = [-0.03383204 -0.00945246  0.03146857]\n   epoch    train_loss    valid_loss     dur\n -------  ------------  ------------  ------\n       1        0.0415        0.0529  0.0245\n       2        0.0404        0.0523  0.0251\n       3        0.0394        0.0517  0.0252\n n1 first params = [-0.03349511 -0.00889289  0.02929294]\n n2 first params = [-0.03349511 -0.00889289  0.02929294]\n       4        0.0431        0.0487  0.0250\n       5        0.0431        0.0487  0.0246\n       6        0.0431        0.0487  0.0247\n n1 first params = [-0.03349511 -0.00889289  0.02929294]\n n2 first params = [-0.03349511 -0.00889289  0.02929294]\n n3 first params = [-0.03349511 -0.00889289  0.02929294]\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "stsievert", "commentT": "2018-08-21T00:24:10Z", "comment_text": "\n \t\tThe relevant code is with  and  at <denchmark-link:https://github.com/dnouri/skorch/blob/16d7e52c7e2832f286e8db1d4aa6458d31f3491a/skorch/net.py#L1308-L1351>https://github.com/dnouri/skorch/blob/16d7e52c7e2832f286e8db1d4aa6458d31f3491a/skorch/net.py#L1308-L1351</denchmark-link>\n , which called when the object is pickled.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "stsievert", "commentT": "2018-08-21T00:46:00Z", "comment_text": "\n \t\tI think this is happening because there's a disconnect between the gradients optimizer sees and the gradients the model weights calculate.\n That is, with the same setup as above:\n >>> def check(net, name='', grad=False):\n ...     p = toolz.first(net.module_.parameters()).detach().numpy()\n ...     if not grad:\n ...         print(name + ' first params =', p.flat[:3])\n ...     p = toolz.first(net.module_.parameters()).grad\n ...     if p is not None and grad:\n ...         p = p.numpy()\n ...         print(name + ' first params =', p.flat[:3])\n \n >>> check(n1, name=\"n1\", grad=True)\n n1 first grad params = [-0.00635823 -0.01619332 -0.0140646 ]\n >>> check(n2, name=\"n2\", grad=True)\n n2 first grad params = [ 0.04858593 -0.09615209  0.00394883]\n The weights the optimizer sees are specified in : <denchmark-link:https://github.com/pytorch/pytorch/blob/e449a27646f3b8b44e893e6be09606c4f24f7313/torch/optim/sgd.py#L82>torch/optim/sgd.py#L82</denchmark-link>\n . Requiring the gradients for  does nothing if the optimizer can't see them.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "stsievert", "commentT": "2018-08-21T08:27:23Z", "comment_text": "\n \t\tThanks for reporting this bug. You are right, during the copying process, the parameters in the module and in the optimizer are copied separately, leading to the disconnect, which is why they don't update (though \"frozen\" doesn't quite fit it).\n The easiest solution to me seems to be to add the following lines to the end of __setstate__:\n <denchmark-code>        if self.initialized_:\n             self.initialize_optimizer()\n </denchmark-code>\n \n For your example code, the error is fixed by this snippet (btw. you don't need toolz.first, next does the same here). Could you confirm that this solves your initial problem?\n The deeper problem, however, lies in the fact that we use  on the  and  here: <denchmark-link:https://github.com/dnouri/skorch/blob/be050a15534924e38eaa06480c7f071a2cce16c7/skorch/net.py#L1310-L1316>https://github.com/dnouri/skorch/blob/be050a15534924e38eaa06480c7f071a2cce16c7/skorch/net.py#L1310-L1316</denchmark-link>\n \n The reasoning here was to be able to load parameters to another device later in __getstate__, e.g. when you train on cuda and predict on cpu. We should, however, find a better solution to this, as with the current one, the parameters from module_ and optimizer_ are copied (leading to the bug).\n Even with the proposed fix, we should still avoid the copy because it doubles the memory required for the state. Unfortunately, this will require a more specific solution instead of the more general use of cuda_dependent_attributes_, a solution that will fail if more attributes reference the parameters. But I guess that's still better than the current implementation.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "stsievert", "commentT": "2018-08-21T10:35:51Z", "comment_text": "\n \t\tI digged a little deeper and it seems that a similar issue, the de-coupling of module and optimizer parameters, has tripped some pytorch users in the past:\n <denchmark-link:https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610>https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610</denchmark-link>\n \n I tried to create a minimal example that demonstrates what is required to preserve the connection:\n <denchmark-link:https://gist.github.com/benjamin-work/ca12831877c241935c224a34bcbda713>https://gist.github.com/benjamin-work/ca12831877c241935c224a34bcbda713</denchmark-link>\n \n Critically, saving module and optimizer in <denchmark-link:https://gist.github.com/benjamin-work/ca12831877c241935c224a34bcbda713#file-pytorch_save_and_load-py-L72>one data structure</denchmark-link>\n  instead of two solves the problem, which I admit was surprising to me. This, in conjunction with the use of , could point forward to a clean solution for the issue.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "stsievert", "commentT": "2018-08-21T15:27:24Z", "comment_text": "\n \t\t\n Could you confirm that this solves your initial problem?\n \n The proposed solution solves my initial problem: the weights update. But the optimizer is reinitialized; I don't think any state is preserved. I've let that be; it seems like <denchmark-link:https://github.com/skorch-dev/skorch/issues/317#issuecomment-414629936>#317 (comment)</denchmark-link>\n  is a better solution.\n \n Critically, saving module and optimizer in one data structure ... use of state_dict, could point forward to a clean solution for the issue.\n \n Perfect. See <denchmark-link:https://github.com/skorch-dev/skorch/pull/318>#318</denchmark-link>\n . It does not use , but passes the test I wrote.\n \t\t"}}}, "commit": {"commit_id": "0f7823cb6a27b5d7dfab56f06a6601bf2957594b", "commit_author": "Scott Sievert", "commitT": "2018-08-23 10:12:48+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "skorch\\net.py", "file_new_name": "skorch\\net.py", "file_complexity": {"file_NLOC": "737", "file_CCN": "170", "file_NToken": "4237"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1310,1314,1315,1316,1317,1318", "deleted_lines": "1313,1314,1315,1316", "method_info": {"method_name": "__getstate__", "method_params": "self", "method_startline": "1308", "method_endline": "1320", "method_complexity": {"method_NLOC": "12", "method_CCN": "3", "method_NToken": "79", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1342,1344,1345,1346,1347,1348,1349,1350", "deleted_lines": "1326,1328,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343", "method_info": {"method_name": "__setstate__", "method_params": "self,state", "method_startline": "1322", "method_endline": "1358", "method_complexity": {"method_NLOC": "29", "method_CCN": "6", "method_NToken": "184", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "skorch\\tests\\test_net.py", "file_new_name": "skorch\\tests\\test_net.py", "file_complexity": {"file_NLOC": "1136", "file_CCN": "145", "file_NToken": "9112"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "130,131", "deleted_lines": null, "method_info": {"method_name": "test_train_net_after_copy", "method_params": "self,net_cls,module_cls,data,copy_method", "method_startline": "130", "method_endline": "131", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "13", "method_nesting_level": "1"}}}}}}}}