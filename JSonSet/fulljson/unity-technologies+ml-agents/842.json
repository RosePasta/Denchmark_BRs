{"BR": {"BR_id": "842", "BR_author": "m-ronchi", "BRopenT": "2018-06-12T17:03:28Z", "BRcloseT": "2018-12-17T23:35:19Z", "BR_text": {"BRsummary": "Python exception when agents are done on first action", "BRdescription": "\n code:\n <denchmark-code>public class BugAcademy : Academy {\n \n     public BugAgent[] agents;\n \n     public int turn;\n \n     public override void InitializeAcademy ()\n     {\n         base.InitializeAcademy ();\n         turn = Random.Range (0, agents.Length);\n     }\n }\n \n public class BugAgent : Agent {\n \n     public BugAcademy academy;\n \n     private void FixedUpdate ()\n     {\n         if (academy.turn == System.Array.IndexOf (academy.agents, this)) {\n             RequestDecision ();\n             academy.turn = -1;\n         }\n     }\n \n     public override void CollectObservations ()\n     {\n         AddVectorObs (0);\n     }\n \n     public override void AgentAction (float[] vectorAction, string textAction)\n     {\n         foreach (var a in academy.agents) a.Done ();\n         academy.turn = Random.Range (0, academy.agents.Length);\n     }\n }\n </denchmark-code>\n \n scene setup:\n \n Academy (BugAcademy, has reference to all the agents)\n \n BugBrain (Brain, external, observation space = 1)\n \n \n Agent 1 (BugAgent, brain set to BugBrain)\n Agent 2 (BugAgent, brain set to BugBrain)\n \n environment setup:\n \n python with docker\n connects to editor\n \n expected result: training session runs to completion\n actual result: after a couple hundred actions, python aborts with exception and editor stops playing\n <denchmark-code>Traceback (most recent call last):\n   File \"python/learn.py\", line 81, in <module>\n     tc.start_learning()\n   File \"/execute/python/unitytrainers/trainer_controller.py\", line 259, in start_learning\n     trainer.process_experiences(curr_info, new_info)\n   File \"/execute/python/unitytrainers/ppo/trainer.py\", line 368, in process_experiences\n     self.stats['episode_length'].append(self.episode_steps[agent_id])\n KeyError: -718928\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "m-ronchi", "commentT": "2018-06-14T22:10:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Unity-Technologies/ml-agents/pull/849>#849</denchmark-link>\n  should have hidden the error, but your agent will not be able to learn at all if it is set to done right after it resets. Reinforcement Learning uses the current state of the agent as well as the state following the action taken in order to learn. If the agent resets at every step, it will not learn at all.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "m-ronchi", "commentT": "2018-06-15T09:30:36Z", "comment_text": "\n \t\tHi,\n my training setup is as follows: when an agent makes an action (discrete), I validate that action: there are 40 possible actions, of which only 3 are valid in a given game state.\n the example I posted was a simplification of what happens when the first action the agent does is invalid, and in the early stages (when agents play randomly) there is a 37/40 chance that it is\n if the agent makes an invalid action, what should I do?\n \n punish him (negative reward), then abort the game (reset/done all agents)\n punish him, make a random valid action, continue the game\n right now, I abort the game after 3 consecutive invalid actions (3 is the lower number that doesn't cause the exception)\n there is (or will be) some way to filter invalid actions inside the NN\n \n also, I am calling Done() as soon as the game is ended, does it mean that the agent is not learning from the last move made? should I call an additional RequestDecision() and only reset after that?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "m-ronchi", "commentT": "2018-11-29T00:37:16Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/m-ronchi>@m-ronchi</denchmark-link>\n \n \n You could use Action Masking.\n The fact that 3 is the lowest number of actions the network needs seems odd to me. What are the hyperparameters you are using ?\n The agent will learn from the last move if you call done when the game ends as long as the episode contains more that one experience point.\n \n I hope this helps.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "m-ronchi", "commentT": "2018-11-29T10:08:18Z", "comment_text": "\n \t\tHi,\n I stopped working with ml-agents currently, and have instead taken a Montecarlo approach for my games (with c# jobs), due to the limitations of it at the time (i.e. no action masking and broken tensorflow with IL2CPP).\n I may eventually retry implementing ml-agents in the future (by the way, do you plan to make it compatible with ECS/c# job system? currently is class-based)\n for your second point, I think I was using the default hyperparameters at the time.\n I have this leftover in training-config.yaml, but I don't remember if I set them before or after this issue:\n <denchmark-code>default:\n     trainer: ppo\n     batch_size: 1024\n     beta: 5.0e-3\n     buffer_size: 10240\n     epsilon: 0.2\n     gamma: 0.99\n     hidden_units: 128\n     lambd: 0.95\n     learning_rate: 3.0e-4\n     max_steps: 5.0e4\n     memory_size: 256\n     normalize: false\n     num_epoch: 3\n     num_layers: 2\n     time_horizon: 64\n     sequence_length: 64\n     summary_freq: 1000\n     use_recurrent: false\n     use_curiosity: false\n     curiosity_strength: 0.01\n     curiosity_enc_size: 128\n \n MyBrain:\n     hidden_units: 256\n     max_steps: 5.0e5\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "m-ronchi", "commentT": "2018-11-29T17:44:09Z", "comment_text": "\n \t\tOk, thank you for your answer. I was hoping our implementation of action masking would solve the problem. Our inference solution with TensorFlowSharp is still very experimental and there are a lot of limitations with it like the one you pointed out.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "m-ronchi", "commentT": "2020-01-02T22:43:39Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "a7b6f41fec21d6a59dbccbcfe3d09d42c31a920a", "commit_author": "vincentpierre", "commitT": "2018-06-13 18:15:23-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\unitytrainers\\bc\\trainer.py", "file_new_name": "python\\unitytrainers\\bc\\trainer.py", "file_complexity": {"file_NLOC": "218", "file_CCN": "63", "file_NToken": "2040"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "250,251,252,253", "deleted_lines": "250,251", "method_info": {"method_name": "process_experiences", "method_params": "self,AllBrainInfo,AllBrainInfo", "method_startline": "228", "method_endline": "255", "method_complexity": {"method_NLOC": "21", "method_CCN": "7", "method_NToken": "221", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\unitytrainers\\ppo\\trainer.py", "file_new_name": "python\\unitytrainers\\ppo\\trainer.py", "file_complexity": {"file_NLOC": "364", "file_CCN": "101", "file_NToken": "3896"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "355,356,357,358,362,363", "deleted_lines": "355,356,360", "method_info": {"method_name": "process_experiences", "method_params": "self,AllBrainInfo,AllBrainInfo", "method_startline": "314", "method_endline": "364", "method_complexity": {"method_NLOC": "41", "method_CCN": "10", "method_NToken": "393", "method_nesting_level": "1"}}}}}}}}