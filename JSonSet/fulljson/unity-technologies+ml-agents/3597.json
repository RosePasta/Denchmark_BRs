{"BR": {"BR_id": "3597", "BR_author": "JohnBergago", "BRopenT": "2020-03-09T16:06:41Z", "BRcloseT": "2020-05-15T22:34:54Z", "BR_text": {"BRsummary": "max_step not correctly propagated to gym_unity", "BRdescription": "\n Describe the bug\n It seems like the max_step flag in an agents info dict that is returned from an env.step() never becomes true.\n To Reproduce\n Set up example env\n \n Open the Project project from ml-agents folder.\n Open Basic Scene from Project Explorer Assets > ML-Agents > Examples > Basic > Scenes > Basic\n In Hierarchy click on Basic > Basic Agent\n In Inspector set Max Step in Agent section to 5\n Build this scene and save to envs folder\n \n Test with python\n Run this code:\n import numpy as np\n from gym_unity.envs import UnityEnv\n from mlagents_envs.environment import UnityEnvironment\n from mlagents_envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel\n \n env_file_name = \"path/to/envs/basicbuild/Basic\"\n \n # --- First test gym environment\n env = UnityEnv(env_file_name, 2, no_graphics=False, flatten_branched=False, multiagent=False)\n \n for e in range(2):\n     print(\"Episode \", e)\n     o, d = env.reset(), False\n     steps = 0\n     while not d:\n         o, r, d, info = env.step([[0]] * env.number_agents)\n         steps += 1\n         if (isinstance(d, list)):\n             d = any(d)\n             print(steps, d)\n         else:\n             print(\"Steps: {} \\tReward: {:6.3f} \\tDone: {}\\tMaxStep: {}\".format(steps, r, d, info[\"batched_step_result\"].max_step))\n \n env.close()\n \n \n engine_configuration_channel = EngineConfigurationChannel()\n env = UnityEnvironment(file_name=env_file_name, base_port=5005, seed=1, side_channels=[engine_configuration_channel])\n \n # --- No gym environment according to getting started notebook------------------------------------------\n #Reset the environment\n env.reset()\n \n # Set the default brain to work with\n group_name = env.get_agent_groups()[0]\n group_spec = env.get_agent_group_spec(group_name)\n \n # Set the time scale of the engine\n engine_configuration_channel.set_configuration_parameters(time_scale = 1.0)\n \n for e in range(2):\n     print(\"No Gym Episode \", e)\n     env.reset()\n     step_result = env.get_step_result(group_name)\n     d = False\n     steps = 0\n     while not d:\n         action_size = group_spec.action_size\n         action = np.column_stack([0]*step_result.n_agents())\n         env.set_actions(group_name, action)\n         env.step()\n         step_result = env.get_step_result(group_name)\n \n         r = step_result.reward[0]\n         d = step_result.done[0]\n         max_step = step_result.max_step[0]\n \n         steps += 1\n         if (isinstance(d, list)):\n             d = any(d)\n             print(steps, d)\n         else:\n             print(\"Steps: {}\\tReward: {:5.3f}\\tDone: {}\\tMaxStep: {}\".format(steps, r, d, max_step))\n env.close()\n Replace path/to/envs with the correct path to the built executable.\n The output will look like:\n <denchmark-code>Episode  0\n Steps: 1        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 2        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 3        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 4        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 5        Reward:  0.000  Done: True      MaxStep: [False]\n Episode  1\n Steps: 1        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 2        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 3        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 4        Reward: -0.010  Done: False     MaxStep: [False]\n Steps: 5        Reward:  0.000  Done: True      MaxStep: [False]\n INFO:mlagents_envs:Environment shut down with return code 0.\n INFO:mlagents_envs:Connected to Unity environment with package version 0.14.1-preview and communication version 0.15.0\n INFO:mlagents_envs:Connected new brain:\n My Behavior?team=0\n No Gym Episode  0\n Steps: 1        Reward: -0.010  Done: False     MaxStep: False\n Steps: 2        Reward: -0.010  Done: False     MaxStep: False\n Steps: 3        Reward: -0.010  Done: False     MaxStep: False\n Steps: 4        Reward: -0.010  Done: False     MaxStep: False\n Steps: 5        Reward: 0.000   Done: True      MaxStep: True\n No Gym Episode  1\n Steps: 1        Reward: -0.010  Done: False     MaxStep: False\n Steps: 2        Reward: -0.010  Done: False     MaxStep: False\n Steps: 3        Reward: -0.010  Done: False     MaxStep: False\n Steps: 4        Reward: -0.010  Done: False     MaxStep: False\n Steps: 5        Reward: 0.000   Done: True      MaxStep: True\n </denchmark-code>\n \n The first two episode are from the gym environment which returns done True, but MaxStep False. However it should be True, as the agent didn't solve the task (as can be seen at the reward). However, it works without gym. The last two episode are from a BaseEnv where everything seems to be as it should.\n Environment (please complete the following information):\n \n Ubuntu 18.04\n latest master branch from source\n TensorFlow version: 2.0\n Environment: Basic with max steps\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "JohnBergago", "commentT": "2020-03-09T18:17:26Z", "comment_text": "\n \t\tThanks for sharing the steps for reproducing this issue and the bug report. I'll share with the team and address the issue accordingly!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "JohnBergago", "commentT": "2020-04-14T22:15:24Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/JohnBergago>@JohnBergago</denchmark-link>\n  ,\n Thank you for reporting this bug.\n Our implementation of the  wrapper changed on  since the 0.15.1 release and I think your issue has been resolved. Can you try to reproduce your error on  so I can make sure this is the case?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "JohnBergago", "commentT": "2020-04-18T14:17:25Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/vincentpierre>@vincentpierre</denchmark-link>\n ,\n I tested your new implementation of the environment interfaces. At first it was a little bit confusing, that max_step is only part of the info dict, when a terminal step happened. However, that one was easy to fix. But since you are not supporting multiagent environment anymore (which I was using a lot) I will have to figure out how to write my own wrapper anyway.\n By the way, I just recognized that the LLAPI docs say, that the DecisionStep and TerminalStep contain a done field, but this seems not to be the case.\n Thanks for your effort.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "JohnBergago", "commentT": "2020-04-20T18:24:54Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/JohnBergago>@JohnBergago</denchmark-link>\n \n Thank you for your feedback, the done filed in the LLAPI doc is a mistake, I made a pull request to correct it.\n We deprecated our multi-agent gym wrapper because it was very hard to maintain and did not work for a lot of environments. We encourage users to use the LLAPI directly or write their own wrappers for their specific needs.\n If there is a functionality that you need in the LLAPI that does not exist, please let us know.\n \t\t"}}}, "commit": {"commit_id": "89e4804789003e0b99eab0fb5d0decfd7d1fa65b", "commit_author": "Vincent-Pierre BERGES", "commitT": "2020-04-20 12:06:59-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\Python-API.md", "file_new_name": "docs\\Python-API.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "152,153,177,178,200,201,222,223"}}}}}}