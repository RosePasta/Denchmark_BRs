{"BR": {"BR_id": "1798", "BR_author": "michael20at", "BRopenT": "2019-03-07T19:56:50Z", "BRcloseT": "2019-09-16T18:00:21Z", "BR_text": {"BRsummary": "Unable to shuffle if the fields are not of same length", "BRdescription": "\n Hm, tried ml-agents 0.7 with my new 2080ti (runs great on normal Tensorflow),\n updated Unity to the newest version 2018.3.7f1, training of 3D Balls example via cmd starts fine, but after a few episode (sometimes only a few, sometimes after 9) I get\n \"Unable to shuffle if the fields are not of same length\"!\n What could be the problem?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "michael20at", "commentT": "2019-03-09T08:29:21Z", "comment_text": "\n \t\tHm, it kinda works if I increase the buffer size by ten in the config file, but I still don't know why?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "michael20at", "commentT": "2019-04-02T01:41:53Z", "comment_text": "\n \t\tThis is kind of weird and unexpected, I don't see other people raising this issue, so I guess maybe delete and rerun and try to follow the basic guide step by step might help.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "michael20at", "commentT": "2019-04-02T17:05:21Z", "comment_text": "\n \t\tI also experienced this issue.\n I use ml-agents 0.6 on 1080ti.\n          batch_size:     1536   beta:   0.006   buffer_size:    15360 epsilon:        0.17 gamma:  0.995 hidden_units:   512 lambd:  0.9 learning_rate:  0.00012 max_steps:      5.0e15 normalize:      True num_epoch:      4 num_layers:     4 time_horizon:   768 summary_freq:   2000 use_recurrent:  False use_curiosity:  False\n I saw this phenomenon when I took agent' s life time too long.\n So I guess agent should be Done() before buffer_size.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "michael20at", "commentT": "2019-04-03T22:49:07Z", "comment_text": "\n \t\tAre you using a Numpy version greater than 1.14.1 by any chance? Newer versions of Numpy are known to have similar issues.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "michael20at", "commentT": "2019-04-23T06:08:52Z", "comment_text": "\n \t\tI have the same issue.\n I use ml-agents 0.8.1 on 2070 card which only support CUDA 10.\n CUDA 10 needs TensorFlow >=1.13.0.\n And Numpy 1.14.1 is not support TensorFlow>=1.13.0\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "michael20at", "commentT": "2019-04-23T14:53:00Z", "comment_text": "\n \t\tSame issue. Using Ubuntu Bionic, numpy v 1.16.2\n Trying to track it down, seems a numpy array (advantages) changes shape from (N,) to (1,N) when the ppy trainer.py subtracts the mean from it, about line 325.\n I hacked out a thing to bypass policy updates when that happens. Walker seems to be training now. My 5 yr old approves of the walker. It got up to a reward of 300 so far.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "michael20at", "commentT": "2019-06-03T13:54:51Z", "comment_text": "\n \t\tThis doesnt seem to happen when using Numpy 1.14.5 and tensorflow 1.7 as mentioned in <denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/37d139af636e4a2351751fbf0f2fca5a9ed7457f/ml-agents/setup.py#L33>setup.py</denchmark-link>\n .\n I'm still not sure what the issue is but at least one doesn't get the bug.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "michael20at", "commentT": "2019-07-12T07:12:59Z", "comment_text": "\n \t\tIt should be a compatibility problem with higher version of Tensorflow or Numpy.\n In my situation (py 3.7 && tf 1.13 && np 1.16), simply adding parameter  to all the calls of  in <denchmark-link:https://github.com/Unity-Technologies/ml-agents/blob/master/ml-agents/mlagents/trainers/buffer.py>ml-agents/mlagents/trainers/buffer.py</denchmark-link>\n  could fix this problem.\n But I dont know if that would cause any potential problems.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "michael20at", "commentT": "2019-07-12T17:03:06Z", "comment_text": "\n \t\tThe fields in the buffer should be all numerical, so I don't see an issue with adding the dtype. We'll definitely track this as we upgrade to newer versions of Numpy.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "michael20at", "commentT": "2019-07-17T17:42:25Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/EfveZombie>@EfveZombie</denchmark-link>\n , we found the root cause of this issue and fixed it on the latest  branch. Let us know if it fixes your issue. Thanks!\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "michael20at", "commentT": "2019-09-16T18:00:21Z", "comment_text": "\n \t\tClosing this issue as it was addressed in a recent release of ML-Agents.\n \t\t"}}}, "commit": {"commit_id": "c025086818949707dc5cf47a47b10b86e3d42937", "commit_author": "Ervin T", "commitT": "2019-07-17 10:37:34-07:00", "commit_complexity": {"commit_NLOC": "0.25", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "ml-agents\\mlagents\\trainers\\ppo\\policy.py", "file_new_name": "ml-agents\\mlagents\\trainers\\ppo\\policy.py", "file_complexity": {"file_NLOC": "210", "file_CCN": "29", "file_NToken": "1557"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "195,196", "deleted_lines": null, "method_info": {"method_name": "get_value_estimates", "method_params": "self,BrainInfo,int,bool", "method_startline": "195", "method_endline": "196", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "195,196,197,202,206,207,208,209,210,211,212", "deleted_lines": "193,201,217", "method_info": {"method_name": "get_value_estimates", "method_params": "self,brain_info,idx", "method_startline": "193", "method_endline": "217", "method_complexity": {"method_NLOC": "18", "method_CCN": "7", "method_NToken": "196", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "ml-agents\\mlagents\\trainers\\ppo\\trainer.py", "file_new_name": "ml-agents\\mlagents\\trainers\\ppo\\trainer.py", "file_complexity": {"file_NLOC": "407", "file_CCN": "31", "file_NToken": "2649"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "513", "deleted_lines": "510", "method_info": {"method_name": "get_gae", "method_params": "rewards,value_estimates,value_next,gamma,lambd", "method_startline": "503", "method_endline": "516", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "70", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "ml-agents\\mlagents\\trainers\\tests\\test_ppo.py", "file_new_name": "ml-agents\\mlagents\\trainers\\tests\\test_ppo.py", "file_complexity": {"file_NLOC": "312", "file_CCN": "13", "file_NToken": "2091"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97", "deleted_lines": null, "method_info": {"method_name": "test_ppo_get_value_estimates", "method_params": "mock_communicator,mock_launcher,dummy_config", "method_startline": "71", "method_endline": "97", "method_complexity": {"method_NLOC": "24", "method_CCN": "3", "method_NToken": "181", "method_nesting_level": "0"}}}}}}}}