{"BR": {"BR_id": "624", "BR_author": "fmfn", "BRopenT": "2016-11-11T22:46:06Z", "BRcloseT": "2016-11-25T11:45:04Z", "BR_text": {"BRsummary": "KeyError when adding special tokens", "BRdescription": "\n I am trying to run the example of adding special tokens to the tokenizer and getting the following keyerror:\n <denchmark-code><ipython-input-3-3c4362d5406f> in <module>()\n       8             POS: u'VERB'},\n       9         {\n ---> 10             ORTH: u'me'}])\n      11 assert [w.text for w in nlp(u'gimme that')] == [u'gim', u'me', u'that']\n      12 assert [w.lemma_ for w in nlp(u'gimme that')] == [u'give', u'-PRON-', u'that']\n \n /Users/<user>/venvs/general/lib/python3.5/site-packages/spacy/tokenizer.pyx in spacy.tokenizer.Tokenizer.add_special_case (spacy/tokenizer.cpp:8460)()\n \n /Users/<user>/venvs/general/lib/python3.5/site-packages/spacy/vocab.pyx in spacy.vocab.Vocab.make_fused_token (spacy/vocab.cpp:7879)()\n \n KeyError: 'F'\n </denchmark-code>\n \n The code used is the following:\n <denchmark-code>import spacy\n from spacy.attrs import ORTH, POS, LEMMA\n \n nlp = spacy.load(\"en\", parser=False)\n \n assert [w.text for w in nlp(u'gimme that')] == [u'gimme', u'that']\n nlp.tokenizer.add_special_case(u'gimme',\n     [\n         {\n             ORTH: u'gim',\n             LEMMA: u'give',\n             POS: u'VERB'},\n         {\n             ORTH: u'me'}])\n assert [w.text for w in nlp(u'gimme that')] == [u'gim', u'me', u'that']\n assert [w.lemma_ for w in nlp(u'gimme that')] == [u'give', u'-PRON-', u'that']\n </denchmark-code>\n \n Am I missing something here?\n System info:\n \n MacOS\n python3.5.2\n spacy 1.2.0\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "fmfn", "commentT": "2016-11-11T22:49:25Z", "comment_text": "\n \t\tSorry about this \u2014 the docs got a bit ahead of the code here. The docs describe how the feature should work, and will work shortly (I'll probably fix it over the weekend).\n At the moment you can use the key \"F\" instead of ORTH, \"L\" instead of LEMMA, and \"pos\" instead of POS.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "fmfn", "commentT": "2016-11-11T22:54:10Z", "comment_text": "\n \t\tNice!\n I got it to work by passing 'F' and working backwards, after I traced the make_fused_token method. But \"L\" and \"P\" were extra hidden.\n Thanks for the lightning reply and superb work.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "fmfn", "commentT": "2016-11-11T23:01:13Z", "comment_text": "\n \t\tAfter changing it to:\n <denchmark-code>nlp.tokenizer.add_special_case(\n     u'gimme',\n     [\n         {\n             \"F\": u'gim',\n             \"L\": u'give',\n             \"pos\": u'VERB'\n         },\n         {\n             \"F\": u'me',\n         }\n     ]\n )\n </denchmark-code>\n \n I get:\n <denchmark-code>KeyError                                  Traceback (most recent call last)\n <ipython-input-6-df7b9eb25a34> in <module>()\n       8         },\n       9         {\n ---> 10             \"F\": u'me',\n      11         }\n      12     ]\n \n /Users/<>/venvs/general/lib/python3.5/site-packages/spacy/tokenizer.pyx in spacy.tokenizer.Tokenizer.add_special_case (spacy/tokenizer.cpp:8460)()\n \n /Users/<>/venvs/general/lib/python3.5/site-packages/spacy/vocab.pyx in spacy.vocab.Vocab.make_fused_token (spacy/vocab.cpp:7907)()\n \n /Users/<>/venvs/general/lib/python3.5/site-packages/spacy/morphology.pyx in spacy.morphology.Morphology.assign_tag (spacy/morphology.cpp:3919)()\n \n KeyError: 97\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "fmfn", "commentT": "2016-11-25T11:45:04Z", "comment_text": "\n \t\tThis should now be fixed on master. Thanks for your patience.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "fmfn", "commentT": "2018-05-09T02:38:23Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "1e0f566d9549a117cb273ec8ca996e01ba982f08", "commit_author": "Matthew Honnibal", "commitT": "2016-11-25 12:43:24+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\vocab.pyx", "file_new_name": "spacy\\vocab.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23,24,341,343,344,345,346,347,348", "deleted_lines": "340,341,342,343,344,345,346,347,348"}}}}}}