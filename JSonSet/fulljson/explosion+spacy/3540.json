{"BR": {"BR_id": "3540", "BR_author": "klti", "BRopenT": "2019-04-04T09:02:24Z", "BRcloseT": "2019-08-08T13:09:56Z", "BR_text": {"BRsummary": "Problem with vector attribute after calling Retokenizer.split", "BRdescription": "\n When trying to use split and retokenize like in the docs (<denchmark-link:https://spacy.io/usage/linguistic-features/#retokenization>https://spacy.io/usage/linguistic-features/#retokenization</denchmark-link>\n ),  breaks with an error\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n code to reproduce (nearly the same as example in docs, only made strings unicode and added displacy.render calls)\n <denchmark-code>import spacy\n from spacy import displacy\n \n nlp = spacy.load(\"en_core_web_sm\")\n doc = nlp(u\"I live in NewYork right now\")\n print(\"Before:\", [token.text for token in doc])\n displacy.render(doc)  # works\n displacy.render(doc, style=\"dep\")  # works\n displacy.render(list(doc.sents), style=\"dep\")  # works\n \n with doc.retokenize() as retokenizer:\n     heads = [(doc[3], 1), doc[2]]\n     attrs = {\"POS\": [\"PROPN\", \"PROPN\"], \"DEP\": [\"pobj\", \"compound\"]}\n     retokenizer.split(doc[3], [u\"New\", u\"York\"], heads=heads, attrs=attrs)\n print(\"After:\", [token.text for token in doc])\n displacy.render(doc)  # works\n displacy.render(doc, style=\"dep\")  # works\n displacy.render(list(doc.sents), style=\"dep\")  # breaks\n </denchmark-code>\n \n Output\n <denchmark-code>('Before:', [u'I', u'live', u'in', u'NewYork', u'right', u'now'])\n ('After:', [u'I', u'live', u'in', u'New', u'York', u'right', u'now'])\n Traceback (most recent call last):\n   File \"test_example.py\", line 18, in <module>\n     displacy.render(list(doc.sents), style=\"dep\") # breaks\n   File \"/usr/lib/python2.7/site-packages/spacy/displacy/__init__.py\", line 46, in render\n     docs = [obj if not isinstance(obj, Span) else obj.as_doc() for obj in docs]\n   File \"span.pyx\", line 224, in spacy.tokens.span.Span.as_doc\n   File \"span.pyx\", line 410, in spacy.tokens.span.Span.vector.__get__\n   File \"span.pyx\", line 187, in __iter__\n   File \"token.pyx\", line 392, in spacy.tokens.token.Token.vector.__get__\n IndexError: index 6 is out of bounds for axis 0 with size 6\n </denchmark-code>\n \n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n Python version: 2.7.16\n Platform: Linux-5.0.3-arch1-1-ARCH-x86_64-with-glibc2.2.5\n spaCy version: 2.1.3\n \n P.S.: great library, I love it \ud83d\udc4d\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "klti", "commentT": "2019-04-04T09:03:57Z", "comment_text": "\n \t\tThanks for the report \u2013 based on the error, this looks like it might be a problem with how the vector attribute is handled after splitting.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "klti", "commentT": "2019-08-08T09:15:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/klti>@klti</denchmark-link>\n  : thanks for the report! It was indeed an issue with the  attribute as <denchmark-link:https://github.com/ines>@ines</denchmark-link>\n  pointed out. This should hopefully be fixed by PR <denchmark-link:https://github.com/explosion/spaCy/pull/4097>#4097</denchmark-link>\n .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "klti", "commentT": "2019-09-07T13:42:42Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "963ea5e8d0290e2198b6b36fa79b420f3152f639", "commit_author": "Sofie Van Landeghem", "commitT": "2019-08-08 15:09:44+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "spacy\\tests\\regression\\test_issue3540.py", "file_complexity": {"file_NLOC": "29", "file_CCN": "7", "file_NToken": "436"}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\_retokenize.pyx", "file_new_name": "spacy\\tokens\\_retokenize.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "407,408,409,410,413,414,421,422,423,424", "deleted_lines": null}}}}}}