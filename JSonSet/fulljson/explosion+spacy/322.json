{"BR": {"BR_id": "322", "BR_author": "thricedotted", "BRopenT": "2016-04-06T18:25:37Z", "BRcloseT": "2016-05-04T08:35:36Z", "BR_text": {"BRsummary": "inconsistent sentence boundaries before and after serialization", "BRdescription": "\n I've been running into a problem where a parse's sentence boundaries change after converting it to a bytestring:\n > text = u\"I bought a couch from IKEA. It wasn't very comfortable.\"\n \n > parse = nlp(text)\n \n > parse_from_bytes = Doc(nlp.vocab).from_bytes(parse.to_bytes())\n \n > [s.text for s in parse.sents]\n [u\"I bought a couch from IKEA. It wasn't very comfortable.\"]\n \n > [s.text for s in parse_from_bytes.sents]\n [u'I bought a couch from IKEA.', u\"It wasn't very comfortable.\"]\n \n > parse.to_bytes() == parse_from_bytes.to_bytes()\n True\n This happened to be one where the sentence boundaries were more correct after the conversion, but I have other examples where it actually breaks the parse EDIT: the parse is already broken; in the original, two ROOTs appear in the same sentence, whereas in the from_bytes version, the ROOTs are forced to be in different sentences.\n Not sure if this means there is a bug in the serialization or initial sentence boundary detection!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "thricedotted", "commentT": "2016-04-07T14:38:36Z", "comment_text": "\n \t\tThanks, there's definitely something wrong here.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "thricedotted", "commentT": "2016-04-14T12:06:02Z", "comment_text": "\n \t\t(atting <denchmark-link:https://github.com/wbwseeker>@wbwseeker</denchmark-link>\n  because we were talking about this bug on Slack)\n I've just gone back over the code and realised that I'd forgotten how my transition system works, with respect to the Break transition. It's really not written down anywhere, and it's in fact rather different from the paper that the code cites as inspiration. So, I'll give some background here.\n The intention is that all sentences are connected trees, so there's one word per sentence that is its own head, and that has the label ROOT. Mostly, sentence boundaries are inserted by the Break action. The Break action flags the first word of the buffer as the start of the next sentence. The parser then acts as though the buffer is exhausted until the stack has only one word. That is, it continues parsing using the \"Unshift\" action to connect the stack, until only one word is left. That word then becomes the root of the sentence, it's popped, and parsing continues.\n There is however another way that we can get a sentence boundary. If the buffer is fully exhausted (i.e. we're really at the end of the sentence), the parser might end up with two root words on the stack. It's then allowed to join them with a left or right arc, using the label ROOT. This should be interpreted as saying \"These are both root words, of different sentences. Insert a sentence boundary between them.\" In the code, there's a flag USE_ROOT_ARC_SEGMENT that toggles this behaviour.  It was used as a baseline strategy when I was experimenting with the definition of the Break transition.\n At some point, the code to actually insert the sentence boundaries during this  strategy got dropped. I think the place to make the change should be here: <denchmark-link:https://github.com/spacy-io/spaCy/blob/master/spacy/syntax/arc_eager.pyx#L396>https://github.com/spacy-io/spaCy/blob/master/spacy/syntax/arc_eager.pyx#L396</denchmark-link>\n  . I think all we'll need is something like  here.\n Below you can find the transition sequence taken by the current model for the example sentence. You can see the final R-ROOT action, which connects the two root words. Note that the tokenization problem \"IKEA.\" is the underlying cause for the model's initial mistake here, which is how it ends up trying to use this error-correction mechanism to arrive at the correct parse.\n Another important part of the post-mortem here is that it's really noticeable that I've got a lot of fairly intricate logic in the transition system that has only been supported by informal experiments, and hasn't been written up anywhere. This isn't very satisfying. I really wanted to have a paper that explained the joint sentence boundary detection and parsing mechanism, and presented the whole-document evaluations. But I never got the CoreNLP comparison done, and the priority was always to keep developing. The decisions should at least be written up somewhere, with whatever results are available.\n     >>> import spacy\n     >>> nlp = spacy.load('en')\n     >>> string = u\"I bought a couch from IKEA. It wasn't very comfortable.\"\n     >>> doc = nlp.tokenizer(string)\n     >>> nlp.tagger(doc)\n     >>> with nlp.parser.step_through(doc) as state:\n     ...   while not state.is_final:\n     ...     action = state.predict()\n     ...     print(action)\n     ...     state.transition(action)\n     ... \n     L-nsubj\n     S\n     L-det\n     R-dobj\n     D\n     R-prep\n     R-pobj\n     S\n     L-nsubj\n     D\n     D\n     S\n     R-neg\n     S\n     L-advmod\n     D\n     R-acomp\n     D\n     R-punct\n     R-ROOT\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "thricedotted", "commentT": "2016-04-21T03:24:51Z", "comment_text": "\n \t\tThis bug is hurting one of my projects too that relies on being able to serialize large docs to avoid re-parsing when they are utilized later. Is there any workaround on the outside or internal patch that you can think of?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "thricedotted", "commentT": "2016-04-21T07:51:43Z", "comment_text": "\n \t\tHaven't tested this yet but you could replace the call to doc.sents with\n this:\n def iter_sents(doc):\n for token in doc:\n if token.dep_ == 'ROOT':\n sent_start = token.left_edge\n sent_end = token.right_edge + 1\n yield doc[sent_start : sent_end]\n This might not do what you want though --- this inserts extra sentence\n boundaries. If you're wanting to keep the boundaries set before\n serialisation, you'll have a tougher time. The bug is that the sent_start\n flag isn't being set correctly during parsing in a minority of cases. So\n the planned fix would be to have those sentence boundaries always being\n inserted.\n On Thursday, April 21, 2016, Robert Clewley <denchmark-link:mailto:notifications@github.com>notifications@github.com</denchmark-link>\n \n wrote:\n \n This bug is hurting one of my projects too that relies on being able to\n serialize large docs to avoid re-parsing when they are utilized later. Is\n there any workaround on the outside or internal patch that you can think of?\n \u2014\n You are receiving this because you commented.\n Reply to this email directly or view it on GitHub\n #322 (comment)\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "thricedotted", "commentT": "2016-04-21T12:13:51Z", "comment_text": "\n \t\tYes, I don't want the extra boundaries. Could I extract these edges before serializing and force new sentences with these edges after using __setstate__ and __getstate__?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "thricedotted", "commentT": "2016-04-24T01:57:00Z", "comment_text": "\n \t\tThank you, this does seem to work for now. For future reference, you just need the .i on the two edge attributes to get the required integers.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "thricedotted", "commentT": "2018-05-09T13:12:05Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "7c2d2deaa76f133954e809f10b25164dc9477577", "commit_author": "Matthew Honnibal", "commitT": "2016-04-25 19:41:59+00:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\syntax\\arc_eager.pyx", "file_new_name": "spacy\\syntax\\arc_eager.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "240,282,283,381", "deleted_lines": "25,94,95,238,239,240,241,242,243,244,245,246,247,248,249,250,256,298,299,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449"}}}}}}