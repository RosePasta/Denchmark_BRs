{"BR": {"BR_id": "1634", "BR_author": "AurelienMassiot", "BRopenT": "2017-11-23T13:35:47Z", "BRcloseT": "2018-12-06T14:51:29Z", "BR_text": {"BRsummary": "Problem deserializing Tokenizer on Windows (spaCy 2.0.3)", "BRdescription": "\n Hi,\n When I train a model with spaCy 2.0.3 on my environment 1, everything works well : I can save it, load it, use it.\n However when I try loading it with environment 2, I get the following error :\n <denchmark-code>>>> spacy.load('my_model')\n Traceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\n   File \"C:\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\", line 19, in load\n     return util.load_model(name, **overrides)\n   File \"C:\\Anaconda3\\lib\\site-packages\\spacy\\util.py\", line 116, in load_model\n     return load_model_from_path(Path(name), **overrides)\n   File \"C:\\Anaconda3\\lib\\site-packages\\spacy\\util.py\", line 158, in load_model_from_path\n     return nlp.from_disk(model_path)\n   File \"C:\\Anaconda3\\lib\\site-packages\\spacy\\language.py\", line 626, in from_disk\n     util.from_disk(path, deserializers, exclude)\n   File \"C:\\Anaconda3\\lib\\site-packages\\spacy\\util.py\", line 521, in from_disk\n     reader(path / key)\n   File \"C:\\Anaconda3\\lib\\site-packages\\spacy\\language.py\", line 614, in <lambda>\n     ('tokenizer', lambda p: self.tokenizer.from_disk(p, vocab=False)),\n   File \"tokenizer.pyx\", line 364, in spacy.tokenizer.Tokenizer.from_disk\n   File \"tokenizer.pyx\", line 399, in spacy.tokenizer.Tokenizer.from_bytes\n   File \"C:\\Anaconda3\\lib\\site-packages\\spacy\\util.py\", line 500, in from_bytes\n     msg = msgpack.loads(bytes_data, encoding='utf8')\n   File \"C:\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py\", line 187, in unpackb\n     return _unpacker.unpackb(packed, encoding=encoding, **kwargs)\n   File \"msgpack/_unpacker.pyx\", line 139, in msgpack._unpacker.unpackb (msgpack/_unpacker.cpp:2068)\n TypeError: unhashable type: 'list'\n </denchmark-code>\n \n <denchmark-h:h2>Environment 1 : it works</denchmark-h>\n \n <denchmark-code>* spaCy version      2.0.3\n * Platform           Linux-3.10.0-693.5.2.el7.x86_64-x86_64-with-centos-7.4.1708-Core\n * Python version     3.6.3\n * Models             en\n </denchmark-code>\n \n <denchmark-h:h2>Environment 2 : it doesn't work</denchmark-h>\n \n <denchmark-code>* spaCy version      2.0.3\n * Platform           Windows-2012Server-6.2.9200-SPO\n * Python version     3.6.1\n * Models             en\n </denchmark-code>\n \n 'EN' models are installed on both, spaCy versions are the same, could it be because of Windows ? Or do you have any ideas why I get this error ?\n Thanks a lot !\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "AurelienMassiot", "commentT": "2017-11-23T13:46:52Z", "comment_text": "\n \t\tThanks for the report! It looks like something is going wrong when deserializing the tokenizer:\n <denchmark-code>File \"tokenizer.pyx\", line 399, in spacy.tokenizer.Tokenizer.from_bytes\n </denchmark-code>\n \n In any case, it looks like there might be a problem with the serialization of the tokenizer on Windows. Will look into this! To help us debug: Are you using any custom tokenization rules?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "AurelienMassiot", "commentT": "2017-11-23T14:01:31Z", "comment_text": "\n \t\tThanks for your quick answer,\n I am not using any custom tokenizarion rules I guess, the only things I do for training and saving the model are :\n \n define train data, for example,\n \n <denchmark-code>train_data = [\n     ('Who is Shaka Khan?', {\n         'entities': [(7, 17, 'PERSON')]\n     }),\n     ('I like London and Berlin.', {\n         'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n     })\n ]\n </denchmark-code>\n \n \n \n nlp = spacy.load(\"en\")\n \n \n train a NER with a function pretty similar to the example from spaCy,\n \n \n <denchmark-code>def train_ner(nlp, train_data, output_dir, nb_iterations=50, dropout=0.5):\n     # create the built-in pipeline components and add them to the pipeline\n     # nlp.create_pipe works for built-ins that are registered with spaCy\n     if 'ner' not in nlp.pipe_names:\n         ner = nlp.create_pipe('ner')\n         nlp.add_pipe(ner, last=True)\n     # otherwise, get it so we can add labels\n     else:\n         ner = nlp.get_pipe('ner')\n \n     # add labels\n     for _, annotations in train_data:\n         for ent in annotations.get('entities'):\n             ner.add_label(ent[2])\n \n     # get names of other pipes to disable them during training\n     other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n     with nlp.disable_pipes(*other_pipes):  # only train NER\n         optimizer = nlp.begin_training()\n         for itn in range(nb_iterations):\n             random.shuffle(train_data)\n             losses = {}\n             for text, annotations in train_data:\n                 nlp.update(\n                     [text],  # batch of texts\n                     [annotations],  # batch of annotations\n                     drop=dropout,  # dropout - make it harder to memorise data\n                     sgd=optimizer,  # callable to update weights\n                     losses=losses)\n \n     # Save model\n     if not Path(output_dir).exists():\n         Path(output_dir).mkdir()\n     nlp.to_disk(Path(output_dir))\n     print(\"model saved to: {}\".format(output_dir))\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "AurelienMassiot", "commentT": "2017-11-23T14:20:32Z", "comment_text": "\n \t\tThanks \u2013 definitely looks like a serialization bug then.\n The tests for this are currently incomplete, because the output of msgpack for the tokenizer turned out to be inconsistent, which made it hard to test the way we're testing the other components (e.g. by asserting that the msgpack before and after output are equal). But we should definitely adjust the tests to at least make sure the serialization roundtrip works, so we can test the Windows behaviour properly on Appveyor.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "AurelienMassiot", "commentT": "2018-03-28T10:29:25Z", "comment_text": "\n \t\tI built a model a week ago and successfully loaded it from my Windows 10 with spacy 2.0.7.\n Not sure what updated, I didn't run any pip installs in quite a while, but suddenly I get the same error when using spacy.load as before.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "AurelienMassiot", "commentT": "2018-05-30T21:00:58Z", "comment_text": "\n \t\tJust to add another data point, we're seeing the same issue with spacy 2.0.11, a custom model trained in one machine causes a TypeError: unhashable type: 'list' error when loading it in another. Re-training the model in the second machine makes everything work, so it sounds like somehow a machine-specific \"something\" (?) might be getting used during serialization/deserialization? Reminded me of cookie encryption/decryption issues when a web server farm isn't configured to use the same encryption/decryption key.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "AurelienMassiot", "commentT": "2018-05-31T03:22:54Z", "comment_text": "\n \t\tStrangely enough, a third computer was able to use the same model... Trying to figure out how machine 1 and 3 match and 2 is different, I'll update the thread if I come up with something.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "AurelienMassiot", "commentT": "2018-07-05T10:41:03Z", "comment_text": "\n \t\tAnyone find a solution without adding a new data point/re training the model on the computer?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "AurelienMassiot", "commentT": "2018-07-05T20:21:56Z", "comment_text": "\n \t\tNot me, but coming back to this thread I just thought of something... in my case I'm putting the models in source control (git), so maybe the auto-handling of LF/CRLF characters is messing up the files? The machines where the models failed for us aren't mine so I can't check what their settings look like, but I'll ask the people who own them to check and try with different settings (basically, check-out as-is, commit as-is).\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "AurelienMassiot", "commentT": "2018-07-05T21:14:07Z", "comment_text": "\n \t\tYep, in my case that was the problem! I fixed it by adding a <denchmark-link:https://git-scm.com/docs/gitattributes>.gitattributes file</denchmark-link>\n  to the root of my repo, with something like this:\n <denchmark-code>path/to/a/folder/with/a/spacy/model/** -text\n </denchmark-code>\n \n That \"unsets\" the text attribute, telling git that it should not do CRLF conversion on any files under that path. Once that file is commited to the repo, the easiest solution is to clone the repository again. I also managed to fix the files by running rm .git/index followed by git reset --hard origin/<my-branch> (having the local version of <my-branch> checked out).\n I guess one last thing to consider, is that the files might have been changed by git at commit time, in which case the model might need to be retrained and commited again after adding the .gitattributes file, so it doesn't get modified.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "AurelienMassiot", "commentT": "2018-12-03T11:24:27Z", "comment_text": "\n \t\tHey, I too faced the same issue and this is what fixed me. Follow below steps to resolve the issue in windows platform:\n \n If you have cloned your repository, just delete that.\n run the command in git as: git config --global core.autocrlf false\n now clone your respective repository again and re-run the code\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "AurelienMassiot", "commentT": "2018-12-06T14:45:37Z", "comment_text": "\n \t\ttl;dr: run pip install \"msgpack<0.6.0\" and you should get everything fixed. Alternatively update spaCy, with pip install spacy>=2.0.18\n The issue here is that the msgpack library has changed behaviour around this flag, use_list, and spaCy previously wasn't pinned to a precise enough version of the library to prevent breaking changes. This means that if you install older versions of spaCy, they cease to work, because you're getting a newly released version of msgpack that breaks our code.\n To stop this happening we're now switching our dependencies to our own fork of  and other serialisation utilities, which we're shipping in a library called <denchmark-link:https://github.com/explosion/srsly>srsly</denchmark-link>\n . We have this ready to release on .\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "AurelienMassiot", "commentT": "2019-01-05T14:55:24Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "bd35bf7f09013f57bd42e0f438285a401f461951", "commit_author": "Alex Villarreal", "commitT": "2018-07-09 18:31:37+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "website\\usage\\_install\\_troubleshooting.jade", "file_new_name": "website\\usage\\_install\\_troubleshooting.jade", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199", "deleted_lines": null}}}}}}