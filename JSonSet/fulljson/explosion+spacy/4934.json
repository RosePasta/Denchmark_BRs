{"BR": {"BR_id": "4934", "BR_author": "FallakAsad", "BRopenT": "2020-01-22T09:21:53Z", "BRcloseT": "2020-03-03T12:58:57Z", "BR_text": {"BRsummary": "Changing conv_depth of pretrained model", "BRdescription": "\n I am trying to change conv_depth of the pretrained model and then train it and save it. The training goes fine, however if I load the saved model, I see following error:\n <denchmark-code>AttributeError: 'Residual' object has no attribute 'G'\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"/src/Services/XXX/__init__.py\", line 483, in train_model\n     nlp = load_model(modelName)\n   File \"/src/Services/XXX/__init__.py\", line 593, in load_model\n     nlp = spacy.load(model_dir + \"/model\")\n   File \"/usr/lib64/python3.6/site-packages/spacy/__init__.py\", line 27, in load\n     return util.load_model(name, **overrides)\n   File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 168, in load_model\n     return load_model_from_path(Path(name), **overrides)\n   File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 211, in load_model_from_path\n     return nlp.from_disk(model_path)\n   File \"/usr/lib64/python3.6/site-packages/spacy/language.py\", line 878, in from_disk\n     util.from_disk(path, deserializers, exclude)\n   File \"/usr/lib64/python3.6/site-packages/spacy/util.py\", line 670, in from_disk\n     reader(path / key)\n   File \"/usr/lib64/python3.6/site-packages/spacy/language.py\", line 873, in <lambda>\n     p, exclude=[\"vocab\"]\n   File \"nn_parser.pyx\", line 644, in spacy.syntax.nn_parser.Parser.from_disk\n ValueError: [E149] Error deserializing model. Check that the config used to create the component matches the model being loaded.\n \n </denchmark-code>\n \n Do you think it is possible to change the conv_depth of pretrained model? Here is the part of code where I set conv_depth.\n <denchmark-code>nlp = spacy.load('de_core_news_md')\n ner = nlp.get_pipe('ner')\n optimizer = nlp.begin_training(component_cfg={\"ner\": {\"conv_depth\": 15}})\n // Training code here\n // Saving model code here\n </denchmark-code>\n \n After saving the model cfg looks as follows:\n <denchmark-code>{\n   \"beam_width\":1,\n   \"beam_density\":0.0,\n   \"beam_update_prob\":1.0,\n   \"cnn_maxout_pieces\":3,\n   \"deprecation_fixes\":{\n     \"vectors_name\":\"de_core_news_md.vectors\"\n   },\n   \"nr_class\":106,\n   \"hidden_depth\":1,\n   \"token_vector_width\":96,\n   \"hidden_width\":64,\n   \"maxout_pieces\":2,\n   \"pretrained_vectors\":\"de_core_news_md.vectors\",\n   \"bilstm_depth\":0,\n   \"conv_depth\":15,\n   \"min_action_freq\":30\n \n </denchmark-code>\n \n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n Operating System: openSUSE Leap 15.0\n Python Version Used: Python 3.6.9\n spaCy Version Used: 2.2.1\n Environment Information: not using GPU\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "FallakAsad", "commentT": "2020-01-22T12:03:57Z", "comment_text": "\n \t\tOne of the big problems with spaCy v2 has been the system for passing config around. I used environment variables as a quick hack for experimentation, and we ended up with a system where defaults could be injected at many places, including sometimes overriding saved models. So this is certainly a bug, and there's some chance updating to the latest version will fix it. I'm not sure what's going wrong though --- thanks for including the cfg, that would've been my first question.\n My best guess is that the problem comes in here:\n <denchmark-code>optimizer = nlp.begin_training(component_cfg={\"ner\": {\"conv_depth\": 15}})\n </denchmark-code>\n \n The begin_training method should reinitialise the model weights, so you'll be starting from a new model. I'm not sure whether this is what you intend: if you want to add more layers on top of the pretrained model, you would need to write nlp.resume_training.\n I think the model you're loading in is not respecting the conv depth setting, but it's still saving the setting in the cfg. The cfg and the model architecture don't match, leaving you with a model you can't load back.\n It should be possible to add extra CNN layers to the model, after loading it back, but you'll need to reach into the undocumented internals of the model. We're close to releasing a new version of Thinc that fixes the design problems, and finally includes good docs. It also has a new config system. If you want to try it out, send me an email at <denchmark-link:mailto:matt@explosion.ai>matt@explosion.ai</denchmark-link>\n  .\n By the way, adding CNN layers is unlikely to be the most effective option. You'll be better off increasing token_vector_width, and possibly installing PyTorch and increasing bilstm_depth.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "FallakAsad", "commentT": "2020-01-22T16:06:52Z", "comment_text": "\n \t\tThanks for your answer. I currently have an alternate solutions in my mind,  that is to reinitialize the weights and retrain the model with a larger conv_depth on the same dataset on which de_core_news_md was trained and then fine tune it on my data. that's why I tried using begin_training() function. However, I am not sure if both of the following code snippet are equivalent when training NER component:\n <denchmark-code>nlp = spacy.load('de_core_news_md')\n ner = nlp.get_pipe('ner')\n optimizer = nlp.begin_training()\n // Training code here\n </denchmark-code>\n \n And\n <denchmark-code>nlp = spacy.blank('de')\n ner = nlp.create_pipe('ner')\n nlp.add_pipe(ner)\n optimizer = nlp.begin_training()\n // Training code here\n </denchmark-code>\n \n Does the first code have some pretrained word vectors that will be used in the training of NER even after calling begin_training()? I am wondering if there would be any benefit of loading a 'de_core_news_md' model instead of creating a blank model even after calling begin_training()?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "FallakAsad", "commentT": "2020-02-12T16:07:59Z", "comment_text": "\n \t\tYes, in the first example the pretrained vectors from the de_core_news_md will be used as input for the training, so you should definitely see a difference.\n I tried replicating your original issue with\n <denchmark-code>    nlp = English()\n     ner = nlp.create_pipe(\"ner\")\n     for _, annotations in TRAIN_DATA:\n         for ent in annotations.get(\"entities\"):\n             ner.add_label(ent[2])\n     nlp.add_pipe(ner)\n     optimizer = nlp.begin_training(component_cfg={\"ner\": {\"conv_depth\": 15}})\n </denchmark-code>\n \n Then writing that to file with nlp.to_disk(tmp_dir) and reading back in. I have no issues with this, so I can't replicate your original bug.\n It seems likely this got fixed in a new version of spaCy - I tested with 2.2.3. I'll assume for now that this was fixed. If you do upgrade and the problem persists, please feel free to let me know!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "FallakAsad", "commentT": "2020-02-12T16:11:57Z", "comment_text": "\n \t\tWait, I spoke too soon. I can replicate it when using a model instead of a blank language.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "FallakAsad", "commentT": "2020-02-12T16:43:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/FallakAsad>@FallakAsad</denchmark-link>\n  : is this currently blocking you? Like Matt said, these kind of things will hopefully become much easier to do with spacy 3 that will use the new thinc...\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "FallakAsad", "commentT": "2020-03-02T13:01:00Z", "comment_text": "\n \t\t\n I think the model you're loading in is not respecting the conv depth setting, but it's still saving the setting in the cfg. The cfg and the model architecture don't match, leaving you with a model you can't load back.\n \n That's exactly what happened. Because a Model was already present, the new conv_depth parameter was being ignored (the model left unchanged), but the parameter did get stored in the internal config dictionary, ultimately resulting in inconsistencies and a crash when performing IO.\n PR <denchmark-link:https://github.com/explosion/spaCy/pull/5078>#5078</denchmark-link>\n  will prevent storing the new values when they're not used. This prevents the crash. Actually training a larger model needs to be done with a different approach as discussed above.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "FallakAsad", "commentT": "2020-03-22T20:37:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/svlandeg>@svlandeg</denchmark-link>\n  This is not currently blocking me thanks.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "FallakAsad", "commentT": "2020-04-25T07:53:39Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "a0998868ffe6d0d8d1a610374f537a4f41eda83e", "commit_author": "Sofie Van Landeghem", "commitT": "2020-03-03 13:58:56+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\syntax\\nn_parser.pyx", "file_new_name": "spacy\\syntax\\nn_parser.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "619,620,636", "deleted_lines": "609,619,640"}}}}}}