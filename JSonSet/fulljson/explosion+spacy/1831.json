{"BR": {"BR_id": "1831", "BR_author": "samrensenhouse", "BRopenT": "2018-01-12T00:30:06Z", "BRcloseT": "2018-01-22T18:19:00Z", "BR_text": {"BRsummary": "Custom glove vectors throw tuple index out of range error", "BRdescription": "\n I tried loading in some custom glove vectors using the demo provided here:\n <denchmark-link:https://github.com/stanfordnlp/GloVe/blob/master/demo.sh>https://github.com/stanfordnlp/GloVe/blob/master/demo.sh</denchmark-link>\n \n I then made a directory called vectors with a vectors.50.d.bin inside as well as a vectors.txt\n However, when I use the code below I get an IndexError:tuple index out of range\n <denchmark-code>parser = spacy.load('en_core_web_sm')\n parser.vocab.vectors.from_glove('C:\\dev\\glovepy\\\\vectors')\n spacy_doc = parser('I am happy.')\n for word in spacy_doc:\n    print(t.vector)\n </denchmark-code>\n \n <denchmark-h:h2>Info about spaCy</denchmark-h>\n \n \n spaCy version: 2.0.5\n Platform: Windows-10-10.0.16299-SP0\n Python version: 3.6.3\n Models: en\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "samrensenhouse", "commentT": "2018-01-17T20:01:25Z", "comment_text": "\n \t\tI'm experiencing the same issue.\n I downloaded a trained a GloVe model for Portuguese from <denchmark-link:http://www.nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc>this repository</denchmark-link>\n . It comes as a single  file, so I loaded it using gensim's  and converted it to binary format with the  files, using this command:\n word_vectors.save_word2vec_format('vectors.50.f.bin', fvocab='vocab.txt', binary=True)\n Then I loaded it into spaCy:\n <denchmark-code>nlp = spacy.load('pt')\n nlp.vocab.vectors.from_glove('/path/to/vectors')\n </denchmark-code>\n \n The error happens if I try to read has_vector or vector properties.\n <denchmark-h:h2>Informations</denchmark-h>\n \n \n spaCy version: 2.0.5\n Platform: Ubuntu 16.04\n Python version: 3.6.4\n Model: pt\n GloVe model: GloVe 50 dimensions\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "samrensenhouse", "commentT": "2018-01-18T14:11:43Z", "comment_text": "\n \t\tI think the problem is in self.data.shape[0] * self.data.shape[1], as the GloVe array is shape (some_num,). self.data.shape[1] therefore returns the index out of range error. I don't have a fix for this, though.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "samrensenhouse", "commentT": "2018-01-18T14:34:20Z", "comment_text": "\n \t\thaving the same issue\n <denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n  any workaround? until you get it fixed.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "samrensenhouse", "commentT": "2018-01-22T18:15:23Z", "comment_text": "\n \t\tThanks for the report, especially @Lankey22 for the suggestion.\n Perhaps we need this in from_glove()?\n if self.data.ndim == 1:\n         self.data = self.data.reshape((self.data.size//width, width))\n If so the following mitigation should work for now until the next version:\n nlp = spacy.load('pt')\n nlp.vocab.vectors.from_glove('/path/to/vectors')\n if nlp.vocab.vectors.data.ndim == 1:\n     nlp.vocab.vectors.data = nlp.vocab.vectors.data.reshape((nlp.vocab.vectors.data.size//width, width))\n You'll need to know the width of the vectors you're loading.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "samrensenhouse", "commentT": "2018-02-05T13:12:57Z", "comment_text": "\n \t\tI also came across this issue and I'm using the same workaround. I find it weird that from_glove is using numpy.fromfile. The documentation states that using tofile and fromfile is not suitable for data storage: <denchmark-link:https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.fromfile.html>https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.fromfile.html</denchmark-link>\n \n If you'd use np.load then it would load a 2D array if it was stored as such. np.fromfile always loads a a 1D array. Not 100% sure how GloVe's binary format is stored, but I would expect as a 2D array. I'm loading word2vec embeddings myself and I saved the conversion in a 2D array.\n Another thing that strikes me is that in the documentation it is stated that the dtype in the file format should either be 'f' or 'd'. That means that any file read in this manner will get flattened by np.ascontiguousarray, because neither equal the string 'float32'. After flattening it would get reshaped again to a 2D array. Relevant line is here: \n \n \n spaCy/spacy/vectors.pyx\n \n \n          Line 311\n       in\n       2e7391e\n \n \n \n \n \n \n  if dtype != 'float32': \n \n \n \n \n \n I might have made some wrong assumptions, but it seems to me that this code is not running as efficient as it could. Would be great to hear why certain choices were made. I love working with SpaCy and hope it becomes even better in the future :)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "samrensenhouse", "commentT": "2018-05-08T00:55:09Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "29897ed1b34512bcf1bad11a4c0b83add0602d01", "commit_author": "Matthew Honnibal", "commitT": "2018-01-22 19:18:26+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\vectors.pyx", "file_new_name": "spacy\\vectors.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "313,314", "deleted_lines": null}}}}}}