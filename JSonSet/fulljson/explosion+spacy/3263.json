{"BR": {"BR_id": "3263", "BR_author": "tamarit", "BRopenT": "2019-02-12T13:59:01Z", "BRcloseT": "2019-02-25T18:31:30Z", "BR_text": {"BRsummary": "batch_size parameter is hard-coded in nlp.evaluate()", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n Hi,\n first of all congratulations for your nice tool. It is really powerful.\n I was training a model using CLI commands with relatively small dataset. Then, I increased the size of the training dataset (and consequently of the development dataset) and start getting problems. It was training till suddenly stopped at ~26%, then printed some \"tcmalloc: large alloc ...\" and stopped. My computer is 8 GB of RAM, so I ask a workmate to run the same with his computer with 32GB of RAM. He could run it perfectly. So the problem was related to the memory.\n After some code inspection, prints ,etc I discovered that the problem was here:\n <denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/language.py#L485>https://github.com/explosion/spaCy/blob/master/spacy/language.py#L485</denchmark-link>\n \n The hard-coded batch_size value is too big for my system, so I modified the module placing a smaller value and then I could use the CLI for training without any problem.\n Additionally, the fact I commented above that the training stopped at ~26%, is due to that it is discarding a lot of documents due to max_doc_len default value. This is expected but percentage should be updated accordingly and it is not. This problem  is related to the way the pbar is updated here:\n <denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/cli/train.py#L135>https://github.com/explosion/spaCy/blob/master/spacy/cli/train.py#L135</denchmark-link>\n \n which is not correct since it should take into account the unfiltered batch. I would suggest to add a batch_initial variable just before replacing the value of batch:\n <denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/cli/train.py#L129>https://github.com/explosion/spaCy/blob/master/spacy/cli/train.py#L129</denchmark-link>\n \n So it could be used afterwards when updating pbar.\n Thanks in advance!\n <denchmark-h:h2>Info about spaCy</denchmark-h>\n \n \n Python version: 3.5.2\n Platform: Linux-4.4.0-45-generic-x86_64-with-Ubuntu-16.04-xenial\n spaCy version: 2.0.18\n Models: es\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tamarit", "commentT": "2019-02-17T11:53:20Z", "comment_text": "\n \t\tI think these issues should be resolved in v2.1. Could you try again with spacy-nightly and let me know how you go?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tamarit", "commentT": "2019-02-19T09:26:09Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n  for your prompt answer.\n I have tried spacy-nightly version and I would say that obtained results are even worse with it than with stable version. Using Google Colab with the nigtly version and running the command:\n <denchmark-code>!python3 -m spacy train es  /content/drive/My\\ Drive/models /content/drive/My\\ Drive/data/ner/train.json /content/drive/My\\ Drive/data/ner/test.json -p ner -n 20\n </denchmark-code>\n \n I obtained the following output:\n <denchmark-code>Training pipeline: ['ner']\n Starting with blank model 'es'\n Counting training words (limit=0)\n \n Itn    Dep Loss    NER Loss      UAS    NER P    NER R    NER F    Tag %  Token %  CPU WPS  GPU WPS\n ---  ----------  ----------  -------  -------  -------  -------  -------  -------  -------  -------\n tcmalloc: large alloc 2429968384 bytes == 0xcf56000 @  0x7f4878624001 0x7f48761abf25 0x7f48762107e1 0x7f48762128bf 0x7f48762aad68 0x566103 0x7f47e4a071e9 0x566103 0x7f47e6e4c0d8 0x566103 0x7f47e6e65e34 0x555421 0x7f47e6e2cd4a 0x566103 0x7f47e6e3b922 0x7f47e6e58058 0x7f47e6e23a1a 0x7f47e6e2db15 0x585784 0x5859be 0x555421 0x5a730c 0x503073 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x502540 0x502f3d\n tcmalloc: large alloc 9301024768 bytes == 0x7f45a299a000 @  0x7f4878624001 0x7f48761abf25 0x7f48762107e1 0x7f48762128bf 0x7f48762aad68 0x566103 0x7f47e4a071e9 0x566103 0x7f47e6e4c0d8 0x566103 0x7f47e6e65e34 0x555421 0x7f47e6e2cd4a 0x566103 0x7f47e6e3b922 0x7f47e6e58058 0x7f47e6e23a1a 0x7f47e6e2db15 0x585784 0x5859be 0x555421 0x5a730c 0x503073 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x502540 0x502f3d\n tcmalloc: large alloc 1800478720 bytes == 0xcf56000 @  0x7f4878624001 0x7f48761abf25 0x7f48762107e1 0x7f48762128bf 0x7f48762aad68 0x566103 0x7f47e4a071e9 0x566103 0x7f47e6e4c0d8 0x566103 0x7f47e6e65e34 0x555421 0x7f47e6e2cd4a 0x566103 0x7f47e6e3b922 0x7f47e6e58058 0x7f47e6e23a1a 0x7f47e6e2db15 0x585784 0x5859be 0x555421 0x5a730c 0x503073 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x502540 0x502f3d\n tcmalloc: large alloc 5500887040 bytes == 0x7f45a299a000 @  0x7f4878624001 0x7f48761abf25 0x7f48762107e1 0x7f48762128bf 0x7f48762aad68 0x566103 0x7f47e4a071e9 0x566103 0x7f47e6e4c0d8 0x566103 0x7f47e6e65e34 0x555421 0x7f47e6e2cd4a 0x566103 0x7f47e6e3b922 0x7f47e6e58058 0x7f47e6e23a1a 0x7f47e6e2db15 0x585784 0x5859be 0x555421 0x5a730c 0x503073 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x502540 0x502f3d\n tcmalloc: large alloc 2530091008 bytes == 0x7f45a299a000 @  0x7f4878624001 0x7f48761abf25 0x7f48762107e1 0x7f48762128bf 0x7f48762aad68 0x566103 0x7f47e4a071e9 0x566103 0x7f47e6e4c0d8 0x566103 0x7f47e6e65e34 0x555421 0x7f47e6e2cd4a 0x566103 0x7f47e6e3b922 0x7f47e6e58058 0x7f47e6e23a1a 0x7f47e6e2db15 0x585784 0x5859be 0x555421 0x5a730c 0x503073 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x502540 0x502f3d\n tcmalloc: large alloc 21905645568 bytes == 0x7f407fac0000 @  0x7f4878624001 0x7f48761abf25 0x7f48762107e1 0x7f48762128bf 0x7f48762aad68 0x566103 0x7f47e4a071e9 0x566103 0x7f47e6e4c0d8 0x566103 0x7f47e6e65e34 0x555421 0x7f47e6e2cd4a 0x566103 0x7f47e6e3b922 0x7f47e6e58058 0x7f47e6e23a1a 0x7f47e6e2db15 0x585784 0x5859be 0x555421 0x5a730c 0x503073 0x506859 0x504c28 0x58650d 0x59ebbe 0x507c17 0x504c28 0x502540 0x502f3d\n ^C\n </denchmark-code>\n \n As we can see, there is not percentage (one of the reported problems) and it stills being suddenly stopped (last line ^C is the way Google Colab indicate this). After some code inspection of the spacy-nightly code , I think that maybe the unreliable percentages are fixed (since max_doc_length has been removed) while the hard-coded batch-size in evaluate method (in language.py) is still there (that's probably what is making the whole computation to stop).\n Thanks is advance.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tamarit", "commentT": "2019-02-25T18:31:30Z", "comment_text": "\n \t\tAdded the setting in Language.evaluate()\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "tamarit", "commentT": "2019-03-27T18:31:50Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "f2fae1f186042eadd5b74280382d5836e6df3854", "commit_author": "Matthew Honnibal", "commitT": "2019-02-25 19:30:33+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "spacy\\language.py", "file_new_name": "spacy\\language.py", "file_complexity": {"file_NLOC": "600", "file_CCN": "143", "file_NToken": "4416"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "576,585", "deleted_lines": "576,585", "method_info": {"method_name": "evaluate", "method_params": "self,docs_golds,verbose", "method_startline": "576", "method_endline": "590", "method_complexity": {"method_NLOC": "15", "method_CCN": "6", "method_NToken": "114", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "576,585", "deleted_lines": "576,585", "method_info": {"method_name": "evaluate", "method_params": "self,docs_golds,verbose,batch_size", "method_startline": "576", "method_endline": "590", "method_complexity": {"method_NLOC": "15", "method_CCN": "6", "method_NToken": "118", "method_nesting_level": "1"}}}}}}}}