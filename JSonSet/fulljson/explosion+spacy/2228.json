{"BR": {"BR_id": "2228", "BR_author": "parthshah86", "BRopenT": "2018-04-16T06:54:08Z", "BRcloseT": "2018-05-20T21:57:10Z", "BR_text": {"BRsummary": "excludes parameter behaves differently between to_bytes and from_bytes for Doc", "BRdescription": "\n I am trying to exclude the tensor at serialization as that leads to a huge payload, but at deserialization it creates an issue.\n <denchmark-code>import spacy\n from spacy.tokens import Doc\n nlp = spacy.load('en')\n doc1 = nlp(u'Spacy is a great library for nlp')\n b = doc1.to_bytes(tensor=True)\n doc2 = Doc(nlp.vocab).from_bytes(b, tensor=True)\n \n \n KeyError                                  Traceback (most recent call last)\n <ipython-input-9-c1c3e2da1529> in <module>()\n ----> 1 doc2 = Doc(nlp.vocab).from_bytes(b, tensor=False)\n doc.pyx in spacy.tokens.doc.Doc.from_bytes()\n KeyError: u'tensor'\n \n </denchmark-code>\n \n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n Operating System: MacOS Sierra 2.6 GHz Intel Core i7, 4 Cores\n Python Version Used: 2.7.13\n spaCy Version Used: 2.0.11\n Environment Information:\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "parthshah86", "commentT": "2018-04-17T08:57:51Z", "comment_text": "\n \t\tExperiencing the same issue. This was also brought up before in <denchmark-link:https://github.com/explosion/spaCy/issues/1920>#1920</denchmark-link>\n  (see <denchmark-link:https://github.com/explosion/spaCy/issues/1920#issuecomment-365468782>comment</denchmark-link>\n )\n This happens because the key  is assumed to exist (unlike ) in the  function.\n Starting in line 832 in file spacy/tokens/doc.pyx.\n <denchmark-code>        if 'user_data' not in exclude and 'user_data_keys' in msg:\n             user_data_keys = msgpack.loads(msg['user_data_keys'],\n                                            use_list=False)\n             user_data_values = msgpack.loads(msg['user_data_values'])\n             for key, value in zip(user_data_keys, user_data_values):\n                 self.user_data[key] = value\n \n         cdef attr_t[:, :] attrs\n         cdef int i, start, end, has_space\n         self.sentiment = msg['sentiment']\n         self.tensor = msg['tensor']\n </denchmark-code>\n \n A similar issue occurs when sentiment is disable.\n <denchmark-code>a = nlp('This is a test')\n abytes = a.to_bytes(sentiment=False)\n b = Doc(nlp.vocab).from_bytes(abytes, sentiment=False)\n </denchmark-code>\n \n This produces:\n <denchmark-code>KeyError                                  Traceback (most recent call last)\n <ipython-input-32-cd4b0af97217> in <module>()\n       1 a = nlp('This is a test')\n       2 abytes = a.to_bytes(sentiment=False)\n ----> 3 b = Doc(nlp.vocab).from_bytes(abytes, sentiment=False)\n \n doc.pyx in spacy.tokens.doc.Doc.from_bytes()\n \n KeyError: 'sentiment'\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "parthshah86", "commentT": "2018-05-07T00:16:10Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/therealronnie>@therealronnie</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "parthshah86", "commentT": "2018-06-19T22:45:57Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "6f5ccda19c66be1dc72af79b72dac0b3483741f2", "commit_author": "Mr Roboto", "commitT": "2018-05-01 13:40:22+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": ".github\\contributors\\therealronnie.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\tests\\doc\\test_doc_api.py", "file_new_name": "spacy\\tests\\doc\\test_doc_api.py", "file_complexity": {"file_NLOC": "210", "file_CCN": "48", "file_NToken": "2159"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "111,112,113,114,115,116,117,118,119,120,121", "deleted_lines": null, "method_info": {"method_name": "test_doc_api_serialize", "method_params": "en_tokenizer,text", "method_startline": "104", "method_endline": "121", "method_complexity": {"method_NLOC": "16", "method_CCN": "13", "method_NToken": "222", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\doc.pyx", "file_new_name": "spacy\\tokens\\doc.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "838,839,840,841,842", "deleted_lines": "838,839"}}}}}}