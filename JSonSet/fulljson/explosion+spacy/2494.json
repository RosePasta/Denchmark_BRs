{"BR": {"BR_id": "2494", "BR_author": "arlinajsk", "BRopenT": "2018-06-29T15:14:53Z", "BRcloseT": "2018-07-06T10:41:07Z", "BR_text": {"BRsummary": "Problem with deserializing custom Tokenizer on windows", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n I customize the tokenizer by simply changing the infix:\n infix_re = re.compile(r'''[-~/]''')\n nlp.tokenizer = Tokenizer(nlp.vocab, infix_finditer = infix_re.finditer )\n When I try to save the model to disk, I get this error, complaining about the prefix (which I don't want to set, just wanted to modify the infix bebavior)\n nlp.to_disk( path )\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\language.py\", line 607, in to_disk\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\util.py\", line 514, in to_disk\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\language.py\", line 595, in \n File \"spacy\\tokenizer.pyx\", line 358, in spacy.tokenizer.Tokenizer.to_disk (spacy/tokenizer.cpp:7862)\n with path.open('wb') as file_:\n File \"spacy\\tokenizer.pyx\", line 359, in spacy.tokenizer.Tokenizer.to_disk (spacy/tokenizer.cpp:7789)\n file_.write(self.to_bytes(**exclude))\n File \"spacy\\tokenizer.pyx\", line 388, in spacy.tokenizer.Tokenizer.to_bytes (spacy/tokenizer.cpp:8959)\n return util.to_bytes(serializers, exclude)\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\util.py\", line 496, in to_bytes\n File \"spacy\\tokenizer.pyx\", line 382, in spacy.tokenizer.Tokenizer.to_bytes.lambda2 (spacy/tokenizer.cpp:8386)\n ('prefix_search', lambda: self.prefix_search.self.pattern),\n AttributeError: 'NoneType' object has no attribute 'self'\n Since the tokenizer is working for me, I just thought I'll disable it while saving the model:\n nlp.to_disk( path, disable = ( 'tokenizer', ) )\n Now when I try to load it back, I got this error:\n nlp2 = spacy.load( output_dir )\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy_init_.py\", line 19, in load\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\util.py\", line 119, in load_model\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\util.py\", line 159, in load_model_from_path\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\language.py\", line 638, in from_disk\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\util.py\", line 522, in from_disk\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\language.py\", line 626, in \n File \"spacy\\tokenizer.pyx\", line 371, in spacy.tokenizer.Tokenizer.from_disk (spacy/tokenizer.cpp:8195)\n self.from_bytes(bytes_data, **exclude)\n File \"spacy\\tokenizer.pyx\", line 406, in spacy.tokenizer.Tokenizer.from_bytes (spacy/tokenizer.cpp:9890)\n msg = util.from_bytes(bytes_data, deserializers, exclude)\n File \"ext\\vc12_win32\\lib\\python2.7\\site-packages\\spacy\\util.py\", line 501, in from_bytes\n File \"ext3\\noarch\\pylib\\site-packages\\msgpack_numpy.py\", line 187, in unpackb\n File \"msgpack/_unpacker.pyx\", line 211, in msgpack._unpacker.unpackb\n UnpackValueError: Unpack failed: error = 0\n Things work fine without the custom tokenizer behavior all together (the custom tokenizer works and the NER is identified correctly)\n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n Operating System: Windows 7\n Python Version Used: 2.7\n spaCy Version Used: 2.0.9 / 2.0.10\n Environment Information:\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "arlinajsk", "commentT": "2018-06-29T17:06:26Z", "comment_text": "\n \t\tThanks for the report \u2013 a similar issue actually  <denchmark-link:https://support.prodi.gy/t/how-to-save-a-custom-tokenizer/661/4>came up</denchmark-link>\n , so we've been working on the tokenizer serialization (see the  branch in <denchmark-link:https://github.com/explosion/spaCy/commit/f08c871adf6f126c2ea7112804c813b977bcb167>f08c871</denchmark-link>\n  and see <denchmark-link:https://github.com/explosion/spaCy/commit/526be4082329d16ecf7b1fa40b81f2008396a325>526be40</denchmark-link>\n ).\n Basically, the problem is that the prefix, suffix, infix and token match rules default to None, which works fine while you're using the tokenizer. But when the tokenizer is saved, spaCy falsely assumes that they're all functions, so it doesn't serialize correctly (and without the tokenizer, spaCy complains when you try to load the model back in).\n In your case, you want to use custom infix rules and the default prefix and suffix rules, right? If so, you could do the following as a workaround:\n def custom_tokenizer(nlp):\n     infix_re = re.compile(r'''[-~/]''')\n     return Tokenizer(nlp.vocab,\n                      prefix_search=nlp.tokenizer.prefix_search,\n                      suffix_search=nlp.tokenizer.suffix_search,\n                      token_match=nlp.tokenizer.token_match,\n                      infix_finditer=infix_re.finditer)\n \n nlp.tokenizer = custom_tokenizer(nlp)\n This will set all other tokenizer rules to the defaults of your nlp object.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "arlinajsk", "commentT": "2018-06-29T17:22:19Z", "comment_text": "\n \t\tThe workaround works! thank you <denchmark-link:https://github.com/ines>@ines</denchmark-link>\n !\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "arlinajsk", "commentT": "2018-08-05T10:52:33Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "38e07ade4c2e940567c4d6f54674a9fcedf920c6", "commit_author": "ines", "commitT": "2018-07-06 12:40:51+02:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\tests\\serialize\\test_serialize_tokenizer.py", "file_new_name": "spacy\\tests\\serialize\\test_serialize_tokenizer.py", "file_complexity": {"file_NLOC": "31", "file_CCN": "6", "file_NToken": "254"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "17,18,19,20,21,22", "deleted_lines": null, "method_info": {"method_name": "test_serialize_custom_tokenizer", "method_params": "en_vocab,en_tokenizer", "method_startline": "17", "method_endline": "22", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "38", "method_nesting_level": "0"}}}}}}}}