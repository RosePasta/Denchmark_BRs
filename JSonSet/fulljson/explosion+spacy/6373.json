{"BR": {"BR_id": "6373", "BR_author": "wlwg", "BRopenT": "2020-11-10T22:33:57Z", "BRcloseT": "2020-11-23T11:29:36Z", "BR_text": {"BRsummary": "textcat training is not deterministic with gpu enabled", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n This is related to <denchmark-link:https://github.com/explosion/spaCy/issues/6177>#6177</denchmark-link>\n . I can verify that when using CPU, the training losses/weights for textcat can be deterministic with . However, if I enable GPU via , the training losses/weights become different every time.\n <denchmark-code>import spacy\n spacy.require_gpu()\n \n for _ in range(2):\n     spacy.util.fix_random_seed(0)\n \n     model = spacy.load('en_core_web_sm')\n \n     model.add_pipe(model.create_pipe('textcat'))\n     model.remove_pipe('parser')\n     model.remove_pipe('tagger')\n \n     cat = model.get_pipe('textcat')\n     cat.add_label(\"dog\")\n     cat.add_label(\"donut\")\n \n     model.begin_training()\n     print(model(\"What even is?\").cats)\n </denchmark-code>\n \n Output:\n <denchmark-code>{'dog': 0.2501096725463867, 'donut': 0.3427947163581848}\n {'dog': 0.9567031860351562, 'donut': 0.9506585001945496}\n </denchmark-code>\n \n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n Operating System: Linux\n Python Version Used: 3.6.9\n spaCy Version Used: latest on master (git sha: 320a8b1)\n Environment Information: Google Colab\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wlwg", "commentT": "2020-11-11T09:41:13Z", "comment_text": "\n \t\tHmm, I can't reproduce this.\n Can you double-check by explicitly uninstalling spacy in colab before installing from master? It's possible that the default spacy install isn't being replaced/uninstalled cleanly when you install from source.\n What do you see in spacy.git_info.GIT_VERSION?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wlwg", "commentT": "2020-11-11T16:29:25Z", "comment_text": "\n \t\tAnd what is your thinc version?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wlwg", "commentT": "2020-11-13T20:33:55Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/adrianeboyd>@adrianeboyd</denchmark-link>\n  <denchmark-link:https://github.com/svlandeg>@svlandeg</denchmark-link>\n \n : \n :  \n : \n I just wrote up a more detailed script: <denchmark-link:https://colab.research.google.com/drive/1lVJpVE-SS85jQP3LdkuZkhKvpBA0EuXM?usp=sharing>https://colab.research.google.com/drive/1lVJpVE-SS85jQP3LdkuZkhKvpBA0EuXM?usp=sharing</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wlwg", "commentT": "2020-11-16T16:17:14Z", "comment_text": "\n \t\tHmm, I do think there may be a bug of some sort here in spacy v2. Locally and with the colab example above I get consistent results within multiple CPU and GPU runs (also with our quick internal test cases related to this), but the CPU and GPU results are not similar to each other, and if I extend the training a bit I do get different results for multiple GPU runs. We will look into it!\n In better news, with spacy v3 I get the same results on both (minus some float rounding differences, of course).\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "wlwg", "commentT": "2020-11-18T12:40:09Z", "comment_text": "\n \t\tI'd be happy to look into this further, but I can't reproduce... :(\n If I run this on either CPU or GPU, I just keep getting consistent results, after installing a clean copy of spacy[cuda101].\n I can run the training loop 200 times, just keep getting the same result.\n The only thing I can think of right now, that this happens on Linux and not Windows? Though that makes little sense to me. <denchmark-link:https://github.com/adrianeboyd>@adrianeboyd</denchmark-link>\n  : you couldn't replicate at first either - what exactly did you change to replicate this?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "wlwg", "commentT": "2020-11-19T13:26:54Z", "comment_text": "\n \t\tHere's my test script (just adapted a bit from the one in the colab example):\n import spacy\n from spacy.util import minibatch, compounding\n \n def train():\n     spacy.util.fix_random_seed(0)\n     model = spacy.blank(\"en\")\n \n     model.add_pipe(model.create_pipe(\"textcat\"))\n \n     cat = model.get_pipe(\"textcat\")\n     cat.add_label(\"dog\")\n     cat.add_label(\"donut\")\n \n     x_train = [f\"example {i}\" for i in range(1000)]\n     y_train = [{\"cats\": {\"dog\": i/1000, \"donut\": 1 - i/1000}} for i in range(1000)]\n     train_data = list(zip(x_train, y_train))\n \n     optimizer = model.begin_training()\n     for i in range(10):\n         batches = minibatch(train_data, size=compounding(16, 64, 1.001))\n         losses = {}\n         for batch in batches:\n             x_batch, y_batch = zip(*batch)\n             model.update(x_batch, y_batch, sgd=optimizer, drop=0, losses=losses)\n         print(i, \"loss:\", losses[\"textcat\"])\n     print(\"example 10:\", model(\"example 10\").cats)\n     print()\n \n if __name__ == \"__main__\":\n     print(\"1st time CPU:\")\n     train()\n     print(\"2nd time CPU:\")\n     train()\n     print(\"\\nEnabling GPU\\n\")\n     spacy.require_gpu()\n     print(\"1st time GPU:\")\n     train()\n     print(\"2nd time GPU:\")\n     train()\n Output:\n <denchmark-code>1st time CPU:\n 0 loss: 0.020526510332956605\n 1 loss: 0.2192715626588324\n 2 loss: 0.1541586974939264\n 3 loss: 0.21435572720838536\n 4 loss: 0.1982542650088135\n 5 loss: 0.19825033005452042\n 6 loss: 0.19787737677813766\n 7 loss: 0.016827800470196053\n 8 loss: 0.02887996903154999\n 9 loss: 0.02469563187116819\n example 10: {'dog': 0.001906172838062048, 'donut': 0.6181842684745789}\n \n 2nd time CPU:\n 0 loss: 0.020526510332956605\n 1 loss: 0.2192715626588324\n 2 loss: 0.1541586974939264\n 3 loss: 0.21435572720838536\n 4 loss: 0.1982542650088135\n 5 loss: 0.19825033005452042\n 6 loss: 0.19787737677813766\n 7 loss: 0.016827800470196053\n 8 loss: 0.02887996903154999\n 9 loss: 0.02469563187116819\n example 10: {'dog': 0.001906172838062048, 'donut': 0.6181842684745789}\n \n \n Enabling GPU\n \n 1st time GPU:\n 0 loss: 0.022869700213050237\n 1 loss: 0.06781688092814875\n 2 loss: 0.15603950362856267\n 3 loss: 0.029185388615587726\n 4 loss: 0.04577569641696755\n 5 loss: 0.03271988184133079\n 6 loss: 0.030841199260066787\n 7 loss: 0.016764739026257303\n 8 loss: 0.023379557263069728\n 9 loss: 0.020565684088069247\n example 10: {'dog': 0.15584374964237213, 'donut': 0.9999545812606812}\n \n 2nd time GPU:\n 0 loss: 0.022846033180030645\n 1 loss: 0.07457155887192357\n 2 loss: 0.1533858735638205\n 3 loss: 0.03846120528942265\n 4 loss: 0.030317590604681754\n 5 loss: 0.022946339027839713\n 6 loss: 0.040068494405659294\n 7 loss: 0.023592466532136314\n 8 loss: 0.02665060829349386\n 9 loss: 0.021907005400862545\n example 10: {'dog': 0.15843163430690765, 'donut': 0.9288136959075928}\n </denchmark-code>\n \n I tested in a new venv with everything from wheels except spacy (from master as of now). example 10 is the model cats output for the text \"example 10\".\n example 10 for a few more GPU runs:\n <denchmark-code>{'dog': 0.2435295134782791, 'donut': 0.9999375343322754}\n {'dog': 0.4791581332683563, 'donut': 0.9981231093406677}\n {'dog': 0.6463608145713806, 'donut': 0.016409972682595253}\n {'dog': 0.14756248891353607, 'donut': 0.9230985045433044}\n </denchmark-code>\n \n : <denchmark-link:https://github.com/explosion/spaCy/files/5566924/freeze.txt>freeze.txt</denchmark-link>\n \n I redid the test with v3 and the results are a bit more variable than I thought between CPU and GPU, but they're not that different across GPU runs.\n <denchmark-code>CPU: {'dog': 0.0654868334531784, 'donut': 0.9892733693122864}\n GPU 1: {'dog': 0.022449197247624397, 'donut': 0.9723042249679565}\n GPU 2: {'dog': 0.02237524650990963, 'donut': 0.9726961255073547}\n GPU 3: {'dog': 0.022426428273320198, 'donut': 0.9722701907157898}\n GPU 4: {'dog': 0.02197781391441822, 'donut': 0.9722147583961487}\n </denchmark-code>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "wlwg", "commentT": "2020-11-19T16:56:51Z", "comment_text": "\n \t\tThanks Adriane - the original script didn't have a model.update function which prevented reproducing this.\n I was able to finally track this down to the  layer of the CNN model in the default textcat architecture. PR <denchmark-link:https://github.com/explosion/spaCy/pull/6411>#6411</denchmark-link>\n  should fix this - but it requires an update of Thinc to 7.4.3 (to be released).\n \t\t"}}}, "commit": {"commit_id": "2af31a8c8dad61e1985949905e6419811dde22ac", "commit_author": "Sofie Van Landeghem", "commitT": "2020-11-23 12:29:35+01:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\_ml.py", "file_new_name": "spacy\\_ml.py", "file_complexity": {"file_NLOC": "784", "file_CCN": "166", "file_NToken": "6425"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "649,691", "deleted_lines": "649,691", "method_info": {"method_name": "build_text_classifier", "method_params": "nr_class,width,cfg", "method_startline": "639", "method_endline": "712", "method_complexity": {"method_NLOC": "69", "method_CCN": "5", "method_NToken": "487", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\tests\\regression\\test_issue6177.py", "file_new_name": "spacy\\tests\\regression\\test_issue6177.py", "file_complexity": {"file_NLOC": "24", "file_CCN": "3", "file_NToken": "193"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "26,27,28,29,31,37", "deleted_lines": "14,27,29,35", "method_info": {"method_name": "test_issue6177", "method_params": "", "method_startline": "8", "method_endline": "37", "method_complexity": {"method_NLOC": "21", "method_CCN": "3", "method_NToken": "174", "method_nesting_level": "0"}}}}}}}}