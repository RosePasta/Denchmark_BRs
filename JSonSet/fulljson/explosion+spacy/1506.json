{"BR": {"BR_id": "1506", "BR_author": "honnibal", "BRopenT": "2017-11-07T23:37:40Z", "BRcloseT": "2017-11-11T17:39:49Z", "BR_text": {"BRsummary": "KeyError during stream processing", "BRdescription": "\n <denchmark-link:https://github.com/ligser>@ligser</denchmark-link>\n  writes on Gitter:\n <denchmark-code>Hello all! I have some troubles with spacy-nightly==2.0.0rc1 (a18 has same behavior) with en_core_web_lg model. When I run nlp.pipe with a generator of a texts I get the exception: KeyError: 4405041669077156115..\n Exception raised after amount of texts (average 10000).\n Stacktrace looks like that:\n     nlp.pipe((c.content_text for c in texts), batch_size=24, n_threads=8)\n   File \"doc.pyx\", line 375, in spacy.tokens.doc.Doc.text.__get__\n   File \"doc.pyx\", line 232, in __iter__ \n   File \"token.pyx\", line 178, in spacy.tokens.token.Token.text_with_ws.__get__\n   File \"strings.pyx\", line 116, in spacy.strings.StringStore.__getitem__\n KeyError: 4405041669077156115\n That looks like a bug with a StringStore cleanup or something related (maybe shared string-store that clean-up by one of threads?).\n My code just get a texts from mysql split it to texts and ids and do: for id, doc in zip(ids_gen,nlp.pipe(docs_gen, ...)).\n </denchmark-code>\n \n I think this is likely due to the solution added in spaCy 2 to address the streaming data memory growth.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "honnibal", "commentT": "2017-11-08T16:59:23Z", "comment_text": "\n \t\tI made some experiments and reproduce similar error as test case. I'm very new in python, and code looks ugly and work slow, but it cause error that looks same (but have different stacktrace):\n <denchmark-code># coding: utf8\n from __future__ import unicode_literals\n \n import random\n import string\n \n from ...lang.en import English\n \n \n def test_issue1506():\n     nlp = English()\n \n     def random_string_generator(string_length, limit):\n         for _ in range(limit):\n             yield ''.join(\n                 random.choice(string.digits + string.ascii_letters + '. ') for _ in range(string_length))\n \n     for i, d in zip(\n         (i for i in range(20007)),\n         nlp.pipe(random_string_generator(600, 20007))\n     ):\n         str(d.text)\n </denchmark-code>\n \n <denchmark-h:h2>Info about spaCy</denchmark-h>\n \n \n spaCy version: 2.0.2.dev0\n Platform: Darwin-17.2.0-x86_64-i386-64bit\n Python version: 3.6.2\n \n Stacktrace:\n <denchmark-code>spacy/language.py:554: in pipe\n     for doc in docs:\n spacy/language.py:534: in <genexpr>\n     docs = (self.make_doc(text) for text in texts)\n spacy/language.py:357: in make_doc\n     return self.tokenizer(text)\n tokenizer.pyx:106: in spacy.tokenizer.Tokenizer.__call__\n     ???\n tokenizer.pyx:156: in spacy.tokenizer.Tokenizer._tokenize\n     ???\n tokenizer.pyx:235: in spacy.tokenizer.Tokenizer._attach_tokens\n     ???\n doc.pyx:547: in spacy.tokens.doc.Doc.push_back\n     ???\n morphology.pyx:81: in spacy.morphology.Morphology.assign_untagged\n     ???\n _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n \n >   ???\n E   KeyError: 10868232842057966403\n \n strings.pyx:116: KeyError\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "honnibal", "commentT": "2017-11-08T19:23:14Z", "comment_text": "\n \t\tAlso when I revert from the  changes from PR <denchmark-link:https://github.com/explosion/spaCy/pull/1424>#1424</denchmark-link>\n  \u2014 that test light green.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "honnibal", "commentT": "2017-11-09T20:51:41Z", "comment_text": "\n \t\tI'm having a similar issue when accessing token.lemma_ for some tokens.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "honnibal", "commentT": "2017-11-09T22:14:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/andharris>@andharris</denchmark-link>\n  Do you still have an example text by any chance?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "honnibal", "commentT": "2017-11-10T12:37:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ines>@ines</denchmark-link>\n  Here's a reproducible script:\n import spacy\n import thinc.extra.datasets\n \n \n def main():\n     nlp = spacy.blank('en')\n     data, _ = thinc.extra.datasets.imdb()\n     corpus = (i[0] for i in data)\n     docs = nlp.pipe(corpus)\n     lemmas = [[token.lemma_ for token in doc] for doc in docs]\n     print(\"Parsed lemmas for {} docs in corpus\".format(len(lemmas)))\n \n \n if __name__ == '__main__':\n     main()\n Info:\n \n spacy: 2.0.2\n python: 3.6.2\n \n Stacktrace:\n <denchmark-code>Traceback (most recent call last):\n   File \"spacy_bug.py\", line 15, in <module>\n     main()\n   File \"spacy_bug.py\", line 10, in main\n     lemmas = [[token.lemma_ for token in doc] for doc in docs]\n   File \"spacy_bug.py\", line 10, in <listcomp>\n     lemmas = [[token.lemma_ for token in doc] for doc in docs]\n   File \".../venv/lib/python3.6/site-packages/spacy/language.py\", line 554, in pipe\n     for doc in docs:\n   File \".../venv/lib/python3.6/site-packages/spacy/language.py\", line 534, in <genexpr>\n     docs = (self.make_doc(text) for text in texts)\n   File \".../venv/lib/python3.6/site-packages/spacy/language.py\", line 357, in make_doc\n     return self.tokenizer(text)\n   File \"tokenizer.pyx\", line 106, in spacy.tokenizer.Tokenizer.__call__\n   File \"tokenizer.pyx\", line 156, in spacy.tokenizer.Tokenizer._tokenize\n   File \"tokenizer.pyx\", line 235, in spacy.tokenizer.Tokenizer._attach_tokens\n   File \"doc.pyx\", line 547, in spacy.tokens.doc.Doc.push_back\n   File \"morphology.pyx\", line 81, in spacy.morphology.Morphology.assign_untagged\n   File \"strings.pyx\", line 116, in spacy.strings.StringStore.__getitem__\n KeyError: 5846064049184721376\n </denchmark-code>\n \n Interestingly if I replace docs = nlp.pipe(corpus) with docs = (nlp.tokenizer(doc) for doc in corpus) I no longer get the error. Not user why this works though and the other fails.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "honnibal", "commentT": "2017-11-10T12:55:29Z", "comment_text": "\n \t\t(nlp.tokenizer(doc) for doc in corpus) don't clean up StringStore \u2014 looks like that cause error.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "honnibal", "commentT": "2017-11-10T13:24:26Z", "comment_text": "\n \t\tAlso, I try to work around that case. And looks like it working...\n <denchmark-code>        original_strings_data = self.vocab.strings.to_bytes()\n         nr_seen = 0\n         for doc in docs:\n             yield doc\n             recent_refs.add(doc)\n             if nr_seen < 10000:\n                 old_refs.add(doc)\n                 nr_seen += 1\n             elif len(old_refs) == 0:\n                 # All the docs in the 'old' set have expired, so the only\n                 # difference between the backup strings and the current\n                 # string-store should be obsolete. We therefore swap out the\n                 # old strings data.\n                 old_refs, recent_refs = recent_refs, old_refs\n                 tmp = self.vocab.strings.to_bytes()\n                 self.vocab.strings.from_bytes(original_strings_data)\n                 original_strings_data = tmp\n                 nr_seen = 0\n </denchmark-code>\n \n I try to not track string manually and just swype it by lowlevel data.\n Maybe that:\n <denchmark-code>for word in doc:\n     recent_strings.add(word.text)\n </denchmark-code>\n \n not track all strings? (Looks like it does not track lemmas at all)\n Or maybe I did not see something wrong with my code, and it just not cleans up?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "honnibal", "commentT": "2017-11-10T13:48:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ligser>@ligser</denchmark-link>\n  : You're exactly right. It's not adding the lemmas or other new strings --- just the word text.\n Periodically we need to do:\n <denchmark-code>current = original + recent\n </denchmark-code>\n \n Currently we're getting recent by just tracking doc.text. It might be best to add something to the StringStore, but I'm worried that this adds more state that can be lost in serialisation, causing confuing results.\n What if we had:\n <denchmark-code>recent = current - previous\n current, previous = (original + recent), current\n </denchmark-code>\n \n This seems like it should work.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "honnibal", "commentT": "2017-11-10T14:19:02Z", "comment_text": "\n \t\tIf I properly understand what you mean, that code does that:\n <denchmark-code>        origin_strings = list(self.vocab.strings)\n         previous_strings = list()\n         nr_seen = 0\n         for doc in docs:\n             yield doc\n             recent_refs.add(doc)\n             if nr_seen < 10000:\n                 old_refs.add(doc)\n                 nr_seen += 1\n             elif len(old_refs) == 0:\n                 # All the docs in the 'old' set have expired, so the only\n                 # difference between the backup strings and the current\n                 # string-store should be obsolete. We therefore swap out the\n                 # old strings data.\n                 old_refs, recent_refs = recent_refs, old_refs\n                 current_strings = list(self.vocab.strings)\n                 recent_strings = [item for item in current_strings if item not in previous_strings]\n                 self.vocab.strings._reset_and_load(recent_strings + origin_strings)\n                 previous_strings = current_strings\n                 nr_seen = 0\n </denchmark-code>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "honnibal", "commentT": "2017-11-10T14:47:24Z", "comment_text": "\n \t\tBut that not work.\n Because if I subtract previous from current \u2014 I lost strings that presented in both (created at previous and used at current too), but I shouldn't do that. I try to think little more about strings that can be wiped out, looks like I understand things too literally.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "honnibal", "commentT": "2017-11-10T15:17:25Z", "comment_text": "\n \t\tLooks like at that level pipe just cannot decide which strings are fresh and which obsolete.\n In your solution \u2014 you try to track truly recent strings. The only problem there is not completed list of words, because of lemmas and other changes to StringStore. That solution can work if you know how to track all of the strings.\n In my solution with tmp var just not caused any cleanup \u2014 I just try to work in Nth iteration with N-1th string store and it works because of luck.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "honnibal", "commentT": "2017-11-10T15:52:33Z", "comment_text": "\n \t\tI think there can be used another version of StringStore class (PipedStringStore for example) that holds two different stores \u2014 \u00abold\u00bb and \u00abnew\u00bb and know about iterations.\n When we start new iteration it swaps StringStore and clean-up new one. When we work with PipedStringStore \u2014 it tries to use \u00abnew\u00bb StringStore and if the key does not exist here \u2014 it fallback to \u00abold\u00bb and copy value that exists in \u00abold\u00bb and not in \u00abnew\u00bb.\n After \u00abiteration\u00bb it discards \u00abold\u00bb with values that not used in that \u00abiteration\u00bb.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "honnibal", "commentT": "2018-05-08T08:27:55Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "3c600adf23e2ed08bd91bbd5dec3972f8f723492", "commit_author": "Roman Domrachev", "commitT": "2017-11-11 03:11:27+03:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\language.py", "file_new_name": "spacy\\language.py", "file_complexity": {"file_NLOC": "477", "file_CCN": "111", "file_NToken": "3452"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15,551,552,553,562,564", "deleted_lines": "550,551,552,556,557,563,564,565,566,567,568,569"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\strings.pxd", "file_new_name": "spacy\\strings.pxd", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "3,27", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\strings.pyx", "file_new_name": "spacy\\strings.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7,115,173,185,248,252,253,254,255,256,257,258,259,260,261,262,263,264,265,281", "deleted_lines": null}}}, "file_3": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "spacy\\tests\\regression\\test_issue1506.py", "file_complexity": {"file_NLOC": "18", "file_CCN": "7", "file_NToken": "131"}}}}}