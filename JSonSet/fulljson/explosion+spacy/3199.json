{"BR": {"BR_id": "3199", "BR_author": "sotnyk", "BRopenT": "2019-01-26T10:20:10Z", "BRcloseT": "2019-02-08T17:33:33Z", "BR_text": {"BRsummary": "TypeError for sentences' noun_chunks if \"it_core_news_sm\" is used", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n <denchmark-code>import spacy\n from spacy.lang.it.examples import sentences as sents_it\n from spacy.lang.fr.examples import sentences as sents_fr\n \n fr_nlp = spacy.load('fr_core_news_sm')\n fr_doc = fr_nlp(sents_fr[0])\n \n it_nlp = spacy.load('it_core_news_sm')\n it_doc = it_nlp(sents_it[0])\n \n print(list(list(fr_doc.sents)[0].noun_chunks))\n print(list(list(it_doc.sents)[0].noun_chunks))\n </denchmark-code>\n \n For French (also have been checked small English and German vocabularies) we get:\n \n [de dollard]\n \n For Italian:\n <denchmark-code>TypeError                                 Traceback (most recent call last)\n <ipython-input-5-03cc3443c5ec> in <module>\n ----> 1 list(list(it_doc.sents)[0].noun_chunks)\n \n span.pyx in __get__()\n \n TypeError: 'NoneType' object is not callable\n </denchmark-code>\n \n Additional issue:\n it_doc.noun_chunks simple returns empty list without any elements. But one or more noun chunks is expected.\n <denchmark-h:h2>Your Environment</denchmark-h>\n \n <denchmark-h:h2>Info about spaCy</denchmark-h>\n \n \n spaCy version: 2.0.16\n Platform: Windows-10-10.0.17763-SP0\n Python version: 3.6.8\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sotnyk", "commentT": "2019-01-26T13:01:04Z", "comment_text": "\n \t\tThanks for the report \u2013 I really thought we had this fixed in the nightly, but I was able to reproduce the error there as well.\n The problem is that Italian doesn't have a noun chunk iterator and spaCy doesn't fail gracefully here. For Doc objects, I think we already fixed this \u2013 but the sentences in doc.sents are Span objects, and it seems like Span.noun_chunks still defaults to None for the noun chunks iterator (instead of just returning no noun chunks).\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sotnyk", "commentT": "2019-01-27T17:45:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ines>@ines</denchmark-link>\n  , Thank you for explanation.\n So, I should implement noun chunks recognition in Italian texts \"by hand\", using POS tagging feature.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sotnyk", "commentT": "2019-01-30T21:37:42Z", "comment_text": "\n \t\tIf you do end up implementing Italian noun chunks, that'd be really cool and we'd definitely appreciate a pull request \ud83d\ude42 (Maybe you can take inspiration from one of the other languages, like Spanish?)\n In the meantime, fixing Span.noun_chunks like Doc.noun_chunks (if no noun chunks are available) should also solve this and prevent spaCy from raising an error.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "sotnyk", "commentT": "2019-03-10T17:43:35Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "ea07f3022e618a19341105c5fe06c4995739ef4e", "commit_author": "Ines Montani", "commitT": "2019-02-08 18:33:16+01:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "spacy\\tests\\regression\\test_issue3199.py", "file_complexity": {"file_NLOC": "7", "file_CCN": "1", "file_NToken": "62"}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\span.pyx", "file_new_name": "spacy\\tokens\\span.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "406,407,408", "deleted_lines": "406,407"}}}}}}