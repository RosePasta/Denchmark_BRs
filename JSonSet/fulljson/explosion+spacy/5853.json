{"BR": {"BR_id": "5853", "BR_author": "thefirebanks", "BRopenT": "2020-08-01T02:25:58Z", "BRcloseT": "2020-08-03T11:53:16Z", "BR_text": {"BRsummary": "Noun chunks not being updated appropriately according to multiple sentences in paragraph", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n Hello! First of all, this awesome package! I am trying to do 2 things:\n \n Divide a string into sentences\n Get the noun_chunks of each sentence\n \n I am aiming to do this in a variety of languages, but primarily experimenting with English, Spanish and Portuguese.\n Issue 1: With the Spanish model, I get the following behavior:\n \n \n Sentence 1:  En res\u00famen AR es un servicio que auxilia los tecnicos de campo a obtener ayuda en la atenci\u00f3n de casos     complejos o que necesite de un soporte de un especialista.\n Noun chunks:  [AR, un servicio, los tecnicos, campo, ayuda, la atenci\u00f3n, casos, complejos, un soporte, un especialista]\n \n \n Sentence 2:  Se puede con eso utilizar del smartphone para compartir de manera segura la imagen del device permitindo que se haga notaciones en la pantalla facilitando el reparo.\n Noun chunks:  [AR, un servicio, los tecnicos, campo, ayuda, la atenci\u00f3n, casos, complejos, un soporte]\n \n \n As you can see, the noun chunks for sentence 2 seem to be referring to the first sentence, as if when I call sentence.noun_chunks I would still be referring to the past sentence.\n <denchmark-link:https://user-images.githubusercontent.com/31460539/89092012-f7733600-d362-11ea-82c8-89a8cc80297e.png></denchmark-link>\n \n Issue 2: With the Portuguese model, I get the following sentence (same code as above only with the portuguese model pt_core_news_sm loaded:\n \n \n Sentence 1: Qual \u00e9 o link para o estudo de Forrester sobre MVS em Portugues?\n noun chunks: [[]]\n \n \n Sentence 2: Aproveite a leitura e compartilhe com seus clientes.\n noun chunks: [[]]\n \n \n Am I doing something wrong or is this expected behavior? Maybe because it's a different language? In English things seem to be working relatively well. Thank you very much!\n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n Operating System: MacOS Catalina 10.15.5\n Python Version Used: 3.7\n spaCy Version Used:  2.3.2\n Environment Information:\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "thefirebanks", "commentT": "2020-08-03T07:02:43Z", "comment_text": "\n \t\tThanks for the report, this does look like a bug for Spanish. We'll look into it!\n Not all languages have an implemented noun_chunks method and there isn't one yet for Portuguese. Since it relies on UD POS tags and dependencies, it's possible that once the Spanish noun_chunks is fixed, you can adapt it easily to also work for Portuguese, since I would guess that Portuguese noun chunks are quite similar to Spanish noun chunks.\n \t\t"}}}, "commit": {"commit_id": "cd59979ab446d7613ec7df5d5737539464918edf", "commit_author": "Adriane Boyd", "commitT": "2020-08-03 13:53:15+02:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\lang\\es\\syntax_iterators.py", "file_new_name": "spacy\\lang\\es\\syntax_iterators.py", "file_complexity": {"file_NLOC": "53", "file_CCN": "18", "file_NToken": "358"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "23", "deleted_lines": "23,24", "method_info": {"method_name": "noun_chunks", "method_params": "doclike", "method_startline": "8", "method_endline": "30", "method_complexity": {"method_NLOC": "21", "method_CCN": "8", "method_NToken": "177", "method_nesting_level": "0"}}}}}}}}