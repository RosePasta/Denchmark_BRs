{"BR": {"BR_id": "4440", "BR_author": "Wirg", "BRopenT": "2019-10-14T21:06:39Z", "BRcloseT": "2020-02-16T16:17:10Z", "BR_text": {"BRsummary": "\"Copying\" a Doc with to_array/from_array does not yield the same SENT_START", "BRdescription": "\n <denchmark-h:h2>Context</denchmark-h>\n \n I am playing around building new s in a pipeline to remove sentences (<denchmark-link:https://stackoverflow.com/questions/58368208/filtering-out-sentences-between-sentence-segmentation-and-other-spacy-components?noredirect=1#comment103103941_58368208>https://stackoverflow.com/questions/58368208/filtering-out-sentences-between-sentence-segmentation-and-other-spacy-components?noredirect=1#comment103103941_58368208</denchmark-link>\n ).\n I noticed that I could not make  work with . I expect to have the same sentences that what I had previously and end up with one sentence by token.\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n import spacy\n from spacy.tokens import Doc\n from spacy import attrs\n import numpy as np\n \n ATTRIBUTES_TO_RESTORE = [attrs.ORTH, attrs.LEMMA, attrs.SHAPE, attrs.IS_STOP, attrs.DEP, attrs.LOWER, attrs.POS, attrs.TAG, attrs.IS_ALPHA, attrs.SENT_START]\n \n def copy_doc(doc: Doc) -> Doc:\n     return Doc(\n         doc.vocab,\n         words=list(map(str, doc)),\n         spaces=list(map(lambda token: token.whitespace_ != '', doc))\n     ).from_array(ATTRIBUTES_TO_RESTORE, doc.to_array(ATTRIBUTES_TO_RESTORE))\n \n \n nlp = spacy.load('en_core_web_sm')\n doc = nlp(\"This is a short sentence. Too short. This is a really really long sentence with a lot of junk.\")\n doc_copy = copy_doc(doc)\n for attr in ATTRIBUTES_TO_RESTORE:\n     print(attrs.NAMES[attr])\n     # Expect no errors, raise for attrs.SENT_START\n     np.testing.assert_array_equal(doc.to_array([attr]), doc_copy.to_array([attr]))\n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n spaCy version: 2.2.0\n Platform: Linux-4.4.0-18362-Microsoft-x86_64-with-Ubuntu-18.04-bionic\n Python version: 3.6.7\n \n EDIT:\n After some fiddling, it seems like I should not be using attrs.SENT_START but attrs.HEAD in my case as I use dep.\n It seems like I should probably have an error message :\n \n \n \n spaCy/spacy/tokens/doc.pyx\n \n \n         Lines 785 to 786\n       in\n       f2d2247\n \n \n \n \n \n \n  if SENT_START in attrs and HEAD in attrs: \n \n \n \n  raise ValueError(Errors.E032) \n \n \n \n \n \n So :\n \n ATTRIBUTES_TO_RESTORE = [attrs.SENT_START] works if I don't have HEAD and/or DEP\n ATTRIBUTES_TO_RESTORE = [attrs.HEAD, attrs.DEP] works if I don't have SENT_START\n \n I guess that the previous snippet should be replaced by something like :\n         if SENT_START in attrs and (HEAD in attrs or DEP in attrs):\n             raise ValueError(Errors.E032)\n Should I do a PR ?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Wirg", "commentT": "2019-10-15T08:10:54Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/Wirg>@Wirg</denchmark-link>\n , thanks for the report and the detailed analysis! I think you're right: your original case should have thrown an error because  shouldn't be used if the document is parsed. It would be good to make this function more robust and the error message more transparent. If you create the PR - feel free to include your original case as a regression test in <denchmark-link:https://github.com/explosion/spaCy/tree/master/spacy/tests/regression>https://github.com/explosion/spaCy/tree/master/spacy/tests/regression</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Wirg", "commentT": "2019-10-18T08:46:43Z", "comment_text": "\n \t\tIf you haven't seen it already, check out Span.as_doc() which shows how to handle a lot of the related issues (SENT_START vs. HEAD/DEP, adjusting dependency arcs, etc.).\n \n \n \n spaCy/spacy/tokens/span.pyx\n \n \n         Lines 203 to 242\n       in\n       258eb9e\n \n \n \n \n \n \n  def as_doc(self, bint copy_user_data=False): \n \n \n \n  \"\"\"Create a `Doc` object with a copy of the `Span`'s data. \n \n \n \n   \n \n \n \n          copy_user_data (bool): Whether or not to copy the original doc's user data. \n \n \n \n          RETURNS (Doc): The `Doc` copy of the span. \n \n \n \n   \n \n \n \n          DOCS: https://spacy.io/api/span#as_doc \n \n \n \n   \"\"\" \n \n \n \n  # TODO: make copy_user_data a keyword-only argument (Python 3 only) \n \n \n \n          words = [t.text for t in self] \n \n \n \n          spaces = [bool(t.whitespace_) for t in self] \n \n \n \n          cdef Doc doc = Doc(self.doc.vocab, words=words, spaces=spaces) \n \n \n \n          array_head = [LENGTH, SPACY, LEMMA, ENT_IOB, ENT_TYPE, ENT_KB_ID] \n \n \n \n  if self.doc.is_tagged: \n \n \n \n              array_head.append(TAG) \n \n \n \n  # If doc parsed add head and dep attribute \n \n \n \n  if self.doc.is_parsed: \n \n \n \n              array_head.extend([HEAD, DEP]) \n \n \n \n  # Otherwise add sent_start \n \n \n \n  else: \n \n \n \n              array_head.append(SENT_START) \n \n \n \n          array = self.doc.to_array(array_head) \n \n \n \n          array = array[self.start : self.end] \n \n \n \n  self._fix_dep_copy(array_head, array) \n \n \n \n          doc.from_array(array_head, array) \n \n \n \n          doc.noun_chunks_iterator = self.doc.noun_chunks_iterator \n \n \n \n          doc.user_hooks = self.doc.user_hooks \n \n \n \n          doc.user_span_hooks = self.doc.user_span_hooks \n \n \n \n          doc.user_token_hooks = self.doc.user_token_hooks \n \n \n \n          doc.vector = self.vector \n \n \n \n          doc.vector_norm = self.vector_norm \n \n \n \n          doc.tensor = self.doc.tensor[self.start : self.end] \n \n \n \n  for key, value in self.doc.cats.items(): \n \n \n \n  if hasattr(key, \"__len__\") and len(key) == 3: \n \n \n \n                  cat_start, cat_end, cat_label = key \n \n \n \n  if cat_start == self.start_char and cat_end == self.end_char: \n \n \n \n                      doc.cats[cat_label] = value \n \n \n \n  if copy_user_data: \n \n \n \n              doc.user_data = self.doc.user_data \n \n \n \n  return doc \n \n \n \n \n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Wirg", "commentT": "2020-02-13T08:20:03Z", "comment_text": "\n \t\tJust coming back to this...\n I think that the original check (for HEAD) is correct because DEP alone doesn't provide any sentence boundaries, so the conflict is only between SENT_START and HEAD.\n The problem is that DEP alone sets self.is_parsed and then set_children_from_heads() sees lots of 0 heads and modifies all the sentence boundaries. I think is_parsed should just be determined from HEAD and not DEP.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Wirg", "commentT": "2020-03-17T16:37:16Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "5b102963bf67b6f49fe1c88d1e6fe9f337e6a621", "commit_author": "adrianeboyd", "commitT": "2020-02-16 17:17:09+01:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\tests\\doc\\test_doc_api.py", "file_new_name": "spacy\\tests\\doc\\test_doc_api.py", "file_complexity": {"file_NLOC": "262", "file_CCN": "48", "file_NToken": "2531"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307", "deleted_lines": null, "method_info": {"method_name": "test_doc_from_array_sent_starts", "method_params": "en_vocab", "method_startline": "277", "method_endline": "307", "method_complexity": {"method_NLOC": "28", "method_CCN": "7", "method_NToken": "296", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\doc.pyx", "file_new_name": "spacy\\tokens\\doc.pyx", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "816", "deleted_lines": "816"}}}}}}