{"BR": {"BR_id": "3716", "BR_author": "avmgithub", "BRopenT": "2017-11-15T16:28:51Z", "BRcloseT": "2020-05-26T16:09:17Z", "BR_text": {"BRsummary": "[ppc64le] test_torch.py test failing in test_qr", "BRdescription": "\n Running on Ubuntu 16.04 ppc64le , when running test_torch.py  test fails in test_qr.  Test only fails when running on Virtual Machine.\n $ python -m unittest test_torch.TestTorch.test_qr\n check_qr 1\n check_qr 2\n check_qr 3\n check big matrix 4\n F\n FAIL: test_qr (test_torch.TestTorch)\n Traceback (most recent call last):\n File \"/home/ubuntu/builder/jenkins/pytorch/pytorch/test/common.py\", line 53, in wrapper\n fn(*args, **kwargs)\n File \"/home/ubuntu/builder/jenkins/pytorch/pytorch/test/test_torch.py\", line 2127, in test_qr\n self.assertEqual(a, a_qr, prec=1e-3)\n File \"/home/ubuntu/builder/jenkins/pytorch/pytorch/test/common.py\", line 204, in assertEqual\n assertTensorsEqual(x, y)\n File \"/home/ubuntu/builder/jenkins/pytorch/pytorch/test/common.py\", line 196, in assertTensorsEqual\n self.assertLessEqual(max_err, prec, message)\n AssertionError: 5.990425851987622 not less than or equal to 0.001 :\n <denchmark-h:hr></denchmark-h>\n \n Ran 1 test in 0.457s\n FAILED (failures=1)\n This only fails when running on a virtual machine.  Baremetal systems do not seem to exhibit the failure. Also this particular line seems to affect the failure:\n a = torch.randn(1000, 1000)  after the comment # check big matrix\n If I change this to a = torch.randn(130, 130),   in most cases the test passes but there are random failures.\n If someone can please point me to where the torch.randn  code is , I'll see if I can figure this out.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "avmgithub", "commentT": "2017-11-16T04:47:06Z", "comment_text": "\n \t\tthe torch.randn code eventually links to\n <denchmark-link:https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorRandom.c#L80>https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorRandom.c#L80</denchmark-link>\n \n <denchmark-link:https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THRandom.c#L253>https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THRandom.c#L253</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "avmgithub", "commentT": "2017-12-08T18:12:42Z", "comment_text": "\n \t\tHi,\n I found that the problem is in the LAPACK implementation in OpenBLAS. OpenBLAS 0.2.19 has issues detecting the correct number of CPUs and this leads to some threading issues as far as I can understand. This happens on Virtual Machines and also on other systems where there are CPU limitations. As of today Conda provides OpenBLAS 0.2.19 on ppc64le. RHEL 7.4 ships with OpenBLAS 0.2.20 and Ubuntu 16.04 ships with OpenBLAS 0.2.18. I tested both RHEL 7.4 and Ubuntu 16.04 and it works fine with them.\n The PR in OpenBLAS that fixes the problem is <denchmark-link:https://github.com/xianyi/OpenBLAS/pull/981>xianyi/OpenBLAS#981</denchmark-link>\n .\n Edit: To summarize: the problem is in OpenBLAS 0.2.19, earlier and later versions work fine.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "avmgithub", "commentT": "2018-05-09T09:21:52Z", "comment_text": "\n \t\tI see a similar issue with pytorch v0.4.0, I am using OpenBLAS v0.2.20.\n <denchmark-code>======================================================================\n FAIL: test_qr (test_torch.TestTorch)\n ----------------------------------------------------------------------\n Traceback (most recent call last):\n   File \"common.py\", line 66, in wrapper\n     fn(*args, **kwargs)\n   File \"test_torch.py\", line 3133, in test_qr\n     self.assertEqual(a, a_qr, prec=1e-3)\n   File \"common.py\", line 243, in assertEqual\n     assertTensorsEqual(x, y)\n   File \"common.py\", line 235, in assertTensorsEqual\n     self.assertLessEqual(max_err, prec, message)\n AssertionError: tensor(6.6219) not less than or equal to 0.001\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "avmgithub", "commentT": "2018-05-09T11:53:50Z", "comment_text": "\n \t\tI'm also using openBlas 0.2.20.\n $ rpm -qa | grep blas\n libopenblas-0.2.20-2528.e2e928f.ppc64le\n Are you using the version that comes from the OS  or from PowerAI\n I do not see the problem on my Power9 RH7.4\n cpuinfo\n processor       : 159\n cpu             : POWER9 (raw), altivec supported\n clock           : 1983.000000MHz\n revision        : 2.1 (pvr 004e 1201)\n $ uname -a\n Linux p95a28.pbm.ihost.com 4.11.0-44.5.1.el7a.ppc64le <denchmark-link:https://github.com/pytorch/pytorch/issues/1>#1</denchmark-link>\n  SMP Thu Jan 18 02:18:03 EST 2018 ppc64le ppc64le ppc64le GNU/Linux\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "avmgithub", "commentT": "2018-05-09T15:24:00Z", "comment_text": "\n \t\tWe're using a PowerAI build of OpenBLAS 0.2.20 (a bit newer than yours and not published yet libopenblas-0.2.20-2593.b59753e.ppc64le).\n Some particulars in case this rings any bells with anyone...\n We can force the failure with a simple test, based on the failing test_qr testcase:\n <denchmark-code>#!/usr/bin/env python\n \n import torch\n \n for sz in [ 127, 128, 129, 130, 500 ]:\n         print(\"Size = {}\".format(sz))\n         for i in range(10):\n                 a = torch.randn(sz, sz)\n                 q, r = torch.qr(a)\n                 a_qr = torch.mm(q, r)\n                 m = float(max(max(x) for x in a - a_qr))\n                 print(\"max diff = {0:.6f}{1}\".format(m, \" FAIL!\" if m > 0.001 else \"\"))\n </denchmark-code>\n \n The failure doesn't seem to occur with matrix sizes of 128x128 or smaller, but will occur with increasing frequency as the matrix size grows. With 129x129, it fails maybe 1 in 10; 500x500 around 9 in 10.\n Not clear whether the problem is coming from torch.qr() or torch.mm() (or both). But presumably one of them is getting into some code that chooses a different algorithm at the 128x128 size cutoff (maybe based on particulars of the build or runtime environment).\n Doesn't fail for us on our Power8 machines but does on Power9, with same OpenBLAS and PyTorch binaries.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "avmgithub", "commentT": "2018-05-09T21:51:23Z", "comment_text": "\n \t\tProblem doesn't seem to reproduce with environment variable OMP_NUM_THREADS=1 set, so it's presumably an issue with OpenBLAS multi-threading that shows up on Power9.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "avmgithub", "commentT": "2018-06-29T13:37:13Z", "comment_text": "\n \t\tI am able to reproduce this issue with the following snippet:\n <denchmark-code>import torch\n a = torch.randn(1000,1000)\n q,r = torch.qr(a)\n m,tau = torch.geqrf(a)\n a_qr = torch.orgqr(m,tau)\n m_triu = torch.triu(m)\n y = float(max(max(x) for x in q - a_qr))\n z = float(max(max(x) for x in r - m_triu))\n \n print(\"Q max diff = {0:.6f}{1}\".format(y, \" FAIL!\" if y > 0.001 else \"\"))\n print(\"R max diff = {0:.6f}{1}\".format(z, \" FAIL!\" if z > 0.001 else \"\"))\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "avmgithub", "commentT": "2020-05-26T16:09:16Z", "comment_text": "\n \t\tGiven <denchmark-link:https://github.com/pytorch/pytorch/issues/3716#issuecomment-350332992>#3716 (comment)</denchmark-link>\n  I am optimistically closing this as fixed.\n \t\t"}}}, "commit": {"commit_id": "ae534dc9780ed5e327cc5c0a9d4685293a5573d3", "commit_author": "ycao", "commitT": "2020-05-08 17:56:36-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "test\\jit\\test_builtins.py", "file_new_name": "test\\jit\\test_builtins.py", "file_complexity": {"file_NLOC": "101", "file_CCN": "25", "file_NToken": "698"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "82,83,84,85", "deleted_lines": null, "method_info": {"method_name": "test_del_multiple_operands.del_list_multiple_operands", "method_params": "x", "method_startline": "82", "method_endline": "85", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "3"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "90,91,92,93", "deleted_lines": null, "method_info": {"method_name": "test_del_multiple_operands.del_dict_multiple_operands", "method_params": "x", "method_startline": "90", "method_endline": "93", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "3"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93", "deleted_lines": null, "method_info": {"method_name": "test_del_multiple_operands", "method_params": "self", "method_startline": "77", "method_endline": "93", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "53", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "torch\\jit\\frontend.py", "file_new_name": "torch\\jit\\frontend.py", "file_complexity": {"file_NLOC": "568", "file_CCN": "147", "file_NToken": "4993"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "288,289,290,291,292", "deleted_lines": null, "method_info": {"method_name": "build_Delete", "method_params": "ctx,stmt", "method_startline": "287", "method_endline": "293", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "62", "method_nesting_level": "1"}}}}}}}}