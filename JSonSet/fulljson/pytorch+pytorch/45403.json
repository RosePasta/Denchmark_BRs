{"BR": {"BR_id": "45403", "BR_author": "mruberry", "BRopenT": "2020-09-28T00:21:06Z", "BRcloseT": "2020-10-05T17:19:56Z", "BR_text": {"BRsummary": "test_torch.py/test_inverse_cuda causes illegal memory access on some platforms", "BRdescription": "\n Run the test -> illegal memory access on CUDA (on some platforms).\n This is indicative of the operation illegally accessing memory.\n Likely (almost definitely) caused by <denchmark-link:https://github.com/pytorch/pytorch/pull/42403>#42403</denchmark-link>\n .\n <denchmark-h:h2>Environment</denchmark-h>\n \n PyTorch version: 1.7.0a0+db4690c\n Is debug build: True\n CUDA used to build PyTorch: 10.1\n ROCM used to build PyTorch: N/A\n OS: Ubuntu 18.04.3 LTS (x86_64)\n GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n Clang version: 8.0.0 (tags/RELEASE_800/final)\n CMake version: version 3.14.0\n Python version: 3.8 (64-bit runtime)\n Is CUDA available: True\n CUDA runtime version: 10.1.105\n GPU models and configuration:\n GPU 0: Quadro GP100\n GPU 1: Quadro GP100\n Nvidia driver version: 440.82\n cuDNN version: Could not collect\n HIP runtime version: N/A\n MIOpen runtime version: N/A\n Versions of relevant libraries:\n [pip3] numpy==1.19.1\n [pip3] torch==1.7.0a0\n [pip3] torchvision==0.7.0\n [conda] blas                      1.0                         mkl\n [conda] magma-cuda101             2.5.2                         1    pytorch\n [conda] mkl                       2020.1                      217\n [conda] mkl-include               2020.1                      217\n [conda] mkl-service               2.3.0            py38he904b0f_0\n [conda] mkl_fft                   1.2.0            py38h23d657b_0\n [conda] mkl_random                1.1.1            py38h0573a6f_0\n [conda] numpy                     1.19.1           py38hbc911f0_0\n [conda] numpy-base                1.19.1           py38hfa32c7d_0\n [conda] torch                     1.7.0a0                   dev_0    \n [conda] torchvision               0.7.0                    pypi_0    pypi\n cc <denchmark-link:https://github.com/ezyang>@ezyang</denchmark-link>\n  <denchmark-link:https://github.com/gchanan>@gchanan</denchmark-link>\n  <denchmark-link:https://github.com/zou3519>@zou3519</denchmark-link>\n  <denchmark-link:https://github.com/mruberry>@mruberry</denchmark-link>\n  <denchmark-link:https://github.com/VitalyFedyunin>@VitalyFedyunin</denchmark-link>\n  <denchmark-link:https://github.com/vincentqb>@vincentqb</denchmark-link>\n  <denchmark-link:https://github.com/vishwakftw>@vishwakftw</denchmark-link>\n  <denchmark-link:https://github.com/jianyuh>@jianyuh</denchmark-link>\n  <denchmark-link:https://github.com/nikitaved>@nikitaved</denchmark-link>\n  <denchmark-link:https://github.com/pearu>@pearu</denchmark-link>\n \n cc <denchmark-link:https://github.com/xwang233>@xwang233</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mruberry", "commentT": "2020-09-28T01:28:30Z", "comment_text": "\n \t\tWhen running the test in isolation I get:\n <denchmark-code>\n ======================================================================\n ERROR: test_inverse_cuda (__main__.TestTorchDeviceTypeCUDA)\n ----------------------------------------------------------------------\n Traceback (most recent call last):\n  File \"/private/home/mruberry/git/pytorch/torch/testing/_internal/common_utils.py\", line 822, in wrapper\n   method(*args, **kwargs)\n  File \"/private/home/mruberry/git/pytorch/torch/testing/_internal/common_utils.py\", line 822, in wrapper\n   method(*args, **kwargs)\n  File \"/private/home/mruberry/git/pytorch/torch/testing/_internal/common_device_type.py\", line 273, in instantiated_test\n   result = test_fn(self, *args)\n  File \"/private/home/mruberry/git/pytorch/torch/testing/_internal/common_device_type.py\", line 508, in dep_fn\n   return fn(slf, device, *args, **kwargs)\n  File \"/private/home/mruberry/git/pytorch/torch/testing/_internal/common_device_type.py\", line 508, in dep_fn\n   return fn(slf, device, *args, **kwargs)\n  File \"test/test_torch.py\", line 6091, in test_inverse\n   matrices_inverse = torch.inverse(matrices)\n RuntimeError: cusolver error: 6, when calling `cusolverDnDgetrs( handle, CUBLAS_OP_N, n, nrhs, dA, lda, ipiv, ret, ldb, info)`\n ----------------------------------------------------------------------\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mruberry", "commentT": "2020-09-28T02:22:53Z", "comment_text": "\n \t\tThanks for the test.\n The test at line 6091 of test_torch.py is equivalent to this\n import torch\n print(torch.__version__)\n \n x = torch.randn(2, 3, 3, dtype=torch.double, device='cuda').permute(0, 2, 1)\n y = torch.inverse(x)\n \n print(y)\n Since this is a batch 2 matrix, it's handled by cusolver after the PR. Before the PR, it was MAGMA. I tried this with CUDA 11.0 on RTX 2070 Super and there was no error.\n For batch size = 2 matrix, after the PR, cusolver matrix inverse kernels are executed in parallel using different CUDA streams. This can greatly improve the performance and should be fine in nearly all cases. Since <denchmark-link:https://github.com/mruberry>@mruberry</denchmark-link>\n  also tried the repro with , then parallel execution is not the problem.\n I can only think of the cusolver library function being the problem. Perhaps we need to modify the guard here\n \n \n \n pytorch/aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h\n \n \n         Lines 10 to 13\n       in\n       d75c402\n \n \n \n \n \n \n  #if defined(CUDART_VERSION) && CUDART_VERSION >= 10000 \n \n \n \n  // some cusolver functions doesn't work well on cuda 9.2, cusolver is used on cuda >= 10.0 \n \n \n \n  #define USE_CUSOLVER \n \n \n \n  #endif \n \n \n \n \n \n If USE_CUSOLVER is not defined, MAGMA implementation will be used.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "mruberry", "commentT": "2020-09-28T17:05:45Z", "comment_text": "\n \t\tIt does not repro with cuda 10.1.243, we should disable cusolver for 10.1.105. <denchmark-link:https://github.com/xwang233>@xwang233</denchmark-link>\n  is there a way to distinguish between patch releases in ifdefs?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "mruberry", "commentT": "2020-09-28T17:39:04Z", "comment_text": "\n \t\tI think they have different CUDA_VERSION macros. I'll check that.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "mruberry", "commentT": "2020-09-28T18:54:47Z", "comment_text": "\n \t\tHere are the cusolver header files on my machine\n <denchmark-code>$ cd /usr/local\n \n $ ls -lahd cuda-*\n drwxr-xr-x. 17 root root 4.0K Sep 28 11:47 cuda-10.1.105\n drwxr-xr-x. 17 root root 4.0K Sep 28 11:38 cuda-10.1.168\n drwxr-xr-x. 15 root root 4.0K Sep 28 11:13 cuda-10.1.243\n drwxr-xr-x. 18 root root 4.0K Mar  5  2020 cuda-10.2\n drwxr-xr-x.  7 root root 4.0K Aug 13 13:42 cuda-11.0\n \n $ rg -i cusolver_ver_ cuda-*\n cuda-10.2/targets/x86_64-linux/include/cusolver_common.h\n 76:#define CUSOLVER_VER_MAJOR 10\n 77:#define CUSOLVER_VER_MINOR 3\n 78:#define CUSOLVER_VER_PATCH 0\n 79:#define CUSOLVER_VER_BUILD 89\n 80:#define CUSOLVER_VERSION (CUSOLVER_VER_MAJOR * 1000 + \\\n 81:                        CUSOLVER_VER_MINOR *  100 + \\\n 82:                        CUSOLVER_VER_PATCH)\n \n cuda-11.0/targets/x86_64-linux/include/cusolver_common.h\n 76:#define CUSOLVER_VER_MAJOR 10\n 77:#define CUSOLVER_VER_MINOR 4\n 78:#define CUSOLVER_VER_PATCH 0\n 79:#define CUSOLVER_VER_BUILD 191\n 80:#define CUSOLVER_VERSION (CUSOLVER_VER_MAJOR * 1000 + \\\n 81:                        CUSOLVER_VER_MINOR *  100 + \\\n 82:                        CUSOLVER_VER_PATCH)\n \n cuda-10.1.243/targets/x86_64-linux/include/cusolver_common.h\n 76:#define CUSOLVER_VER_MAJOR 10\n 77:#define CUSOLVER_VER_MINOR 2\n 78:#define CUSOLVER_VER_PATCH 0\n 79:#define CUSOLVER_VER_BUILD 243\n 80:#define CUSOLVER_VERSION (CUSOLVER_VER_MAJOR * 1000 + \\\n 81:                        CUSOLVER_VER_MINOR *  100 + \\\n 82:                        CUSOLVER_VER_PATCH)\n </denchmark-code>\n \n To only enable cusolver for cuda >= 10.1.243, we can change the guard to\n #if defined(CUDART_VERSION) && defined(CUSOLVER_VERSION) && CUSOLVER_VERSION >= 10200\n The first CUDART_VERSION is used to guard ROCm.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "mruberry", "commentT": "2020-09-28T19:56:51Z", "comment_text": "\n \t\tCool, can you please submit a PR?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "mruberry", "commentT": "2020-09-28T19:58:09Z", "comment_text": "\n \t\tSure, thanks!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "mruberry", "commentT": "2020-10-05T06:38:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/xwang233>@xwang233</denchmark-link>\n  we can close this now, right?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "mruberry", "commentT": "2020-10-05T07:10:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mruberry>@mruberry</denchmark-link>\n  The PR has been merged, so it should solve the problem you saw on 10.1.105. If you have verified that and don't see a crash elsewhere, feel free to close it. Thanks!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "mruberry", "commentT": "2020-10-05T17:19:55Z", "comment_text": "\n \t\tVerified the test now passes.\n \t\t"}}}, "commit": {"commit_id": "df0de780c32f91cfcdb153595ac456cc629c01ba", "commit_author": "Xiao Wang", "commitT": "2020-09-29 09:25:20-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "aten\\src\\ATen\\native\\cuda\\BatchLinearAlgebraLib.h", "file_new_name": "aten\\src\\ATen\\native\\cuda\\BatchLinearAlgebraLib.h", "file_complexity": {"file_NLOC": "9", "file_CCN": "0", "file_NToken": "27"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "10,11", "deleted_lines": "10,11"}}}}}}