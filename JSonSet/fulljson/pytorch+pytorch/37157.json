{"BR": {"BR_id": "37157", "BR_author": "mcarilli", "BRopenT": "2020-04-23T17:35:28Z", "BRcloseT": "2020-05-07T01:21:15Z", "BR_text": {"BRsummary": "CUBLAS_STATUS_EXECUTION_FAILED for ConvTranspose2d backward with FP16 inputs", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n While helping <denchmark-link:https://github.com/vfdev-5>@vfdev-5</denchmark-link>\n  integrate  with <denchmark-link:https://github.com/pytorch/ignite>Ignite</denchmark-link>\n , he asked me to debug a CycleGan+Amp example.  I saw weird nondeterministic errors.  Eventually I narrowed down a repro with ConvTranspose2d, FP16 inputs, and shapes taken from cyclegan.  It fails with illegal memory accesses in .  (The repro does not involve  or <denchmark-link:https://github.com/vfdev-5>@vfdev-5</denchmark-link>\n  's script at all, so I think these are innocent.)\n I don't know the cause yet, the failure may or may not occur based on input addresses, and I'm not sure what distinguishes \"bad\" input addresses.\n Posting for visibility.  I will check for similar issues and keep digging.  Related:\n \n #31690, which discusses how nonzero output_padding can force ConvTranspose2d to use the \"slow conv\" backend (im2col+col2im+cublas GEMM) instead of cudnn.\n #23545, same failing call (ConvTranspose2d with FP16 inputs, even the same shapes as my repro!) but fails with different symptoms.  The \"fix\" (9130ab3) is a one-line change that replaced a macro with a hardcoded character, so any deep memory movement problems likely weren't affected.\n #15584, also cyclegan backward() failure, but failure came from cudnn.  Not resolved.\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n This may not work on a different system, but out.sum().backward() reliably fails for me at iteration 48.\n import torch\n \n dtype = torch.half\n \n for i in range(0,500):\n     torch.cuda.manual_seed(17)\n     # dummy tensors to fuzz where ConvTranspose2d inputs get allocated\n     dummy0 = torch.empty((7777*i,), device=\"cuda\", dtype=dtype)\n     a = torch.randn((6, 256, 50, 50), device=\"cuda\", dtype=dtype)\n     dummy1 = torch.empty((7777*i,), device=\"cuda\", dtype=dtype)\n     # shape from CycleGAN\n     m = torch.nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True).cuda().to(dtype)\n     dummy2 = torch.empty((7777*i,), device=\"cuda\", dtype=dtype)\n     out = m(a)\n     out.sum().backward()\n     print(i,\n           \"weight grad sum = \", m.weight.grad.data.double().sum().item() if m.weight.grad is not None else None,\n           \"bias grad sum = \", m.bias.grad.data.double().sum().item() if m.bias.grad is not None else None)\n     m.zero_grad()\n Stack trace with CUDA_LAUNCH_BLOCKING=1\n (weight grad and bias grad sums are deterministic for the first 47 iterations)\n <denchmark-code>weight grad sum =  1366953.875 bias grad sum =  98304.0\n 47 torch.Size([6, 128, 100, 100]) 140174484123648 140173632753664 140173635366912\n 140173633344000\n weight grad sum =  1366953.875 bias grad sum =  98304.0\n 48 torch.Size([6, 128, 100, 100]) 140174476443648 140173636979712 140173633344000\n 140173637570048\n Traceback (most recent call last):\n   File \"transpose2d.py\", line 131, in <module>\n     out.sum().backward()\n   File \"/home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/tensor.py\", line 198, in backward\n     torch.autograd.backward(self, gradient, retain_graph, create_graph)\n   File \"/home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 100, in backward\n     allow_unreachable=True)  # allow_unreachable flag\n RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)` (gemm<c10::Half> at ../aten/src/ATen/cuda/CUDABlas.cpp:226)\n frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7f7d201cbe1a in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libc10.so)\n frame #1: <unknown function> + 0x35ccb46 (0x7f7d239b2b46 in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\n frame #2: <unknown function> + 0x35cd4d9 (0x7f7d239b34d9 in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\n frame #3: at::native::(anonymous namespace)::slow_conv_transpose2d_acc_grad_parameters_cuda_template(at::Tensor const&, at::Tensor const&, at::Tensor&, at::Tensor&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, int) + 0xe46 (0x7f7d22946446 in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\n frame #4: at::native::slow_conv_transpose2d_backward_cuda(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, at::Tensor const&, at::Tensor const&, std::array<bool, 3ul>) + 0x422 (0x7f7d2294c3d2 in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\n frame #5: <unknown function> + 0x3649d49 (0x7f7d23a2fd49 in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\n frame #6: <unknown function> + 0x365927d (0x7f7d23a3f27d in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\n frame #7: <unknown function> + 0x2bae96a (0x7f7d43cbe96a in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n frame #8: <unknown function> + 0xf39d3d (0x7f7d42049d3d in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n frame #9: torch::autograd::generated::SlowConvTranspose2DBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x520 (0x7f7d4391f490 in /home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\n ...\n </denchmark-code>\n \n cuda-memcheck --tool memcheck python transpose2d.py reports a bunch of invalid reads like\n <denchmark-code> Invalid __global__ read of size 2\n =========     at 0x00001340 in void gemv2T_kernel_val<int, int, __half, __half, float, int=128, int=16, int=2, int=2, bool=0, cublasGemvParams<cublasGemvTensorStridedBatched<__half const >, cublasGemvTensorStridedBatched<__half>, float>>(__half const , float, float)\n =========     by thread (112,0,0) in block (768,0,0)\n =========     Address 0x7f58ea60000e is out of bounds\n =========     Device Frame:void gemv2T_kernel_val<int, int, __half, __half, float, int=128, int=16, int=2, int=2, bool=0, cublasGemvParams<cublasGemvTensorStridedBatched<__half const >, cublasGemvTensorStridedBatched<__half>, float>>(__half const , float, float) (void gemv2T_kernel\n _val<int, int, __half, __half, float, int=128, int=16, int=2, int=2, bool=0, cublasGemvParams<cublasGemvTensorStridedBatched<__half const >, cublasGemvTensorStridedBatched<__half>, float>>(__half const , float, float) : 0x1340)\n ...\n =========     Host Frame:/home/mcarilli/anaconda3/envs/python373/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so (_ZN2at6native97_GLOBAL__N__73_tmpxft_00003c73_00000000_7_NaiveConvolutionTranspose2d_compute_61_cpp1_ii_1a68c69d55slow_conv_transpose2d_acc_grad_parameters_cuda_\n templateERKNS_6TensorES4_RS2_S5_S4_S4_N3c108ArrayRefIlEES8_S8_S8_S8_i + 0xe46) [0x2560446]\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n Cublas is being loaded from a vanilla public cuda 10.2 install:\n <denchmark-code>(python373) $ LD_DEBUG=libs python transpose2d.py 2>&1 | grep cublas\n       ...\n       8876:     calling init: /home/mcarilli/cuda_versions/10.2_files/cuda-toolkit/lib64/libcublasLt.so.10\n       8876:     calling init: /home/mcarilli/cuda_versions/10.2_files/cuda-toolkit/lib64/libcublas.so.10\n </denchmark-code>\n \n Pytorch is based on a recent-ish master (__version__ is unique because it's a source build from a local commit with debugging print statements).\n cc <denchmark-link:https://github.com/ezyang>@ezyang</denchmark-link>\n  <denchmark-link:https://github.com/gchanan>@gchanan</denchmark-link>\n  <denchmark-link:https://github.com/zou3519>@zou3519</denchmark-link>\n  <denchmark-link:https://github.com/csarofeen>@csarofeen</denchmark-link>\n  <denchmark-link:https://github.com/ptrblck>@ptrblck</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mcarilli", "commentT": "2020-04-23T21:52:16Z", "comment_text": "\n \t\tHi pri because it is a crash.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mcarilli", "commentT": "2020-04-24T02:07:53Z", "comment_text": "\n \t\tAt the <denchmark-link:https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu#L736>spot where the error occurs</denchmark-link>\n , the gemv arguments have the expected shape.  If replace the gemv call with dummy in-place s applied to the argument tensors, the repro runs end to end under cuda-memcheck without errors, so the argument tensors appear properly allocated.\n Looks more and more like a misconfigured cublas call or an internal cublas error.  Tomorrow i'll figure out which.\n \t\t"}}}, "commit": {"commit_id": "35693e9b4b09de02a18f0840ec491726f2e61fe2", "commit_author": "Michael Carilli", "commitT": "2020-05-06 18:19:30-07:00", "commit_complexity": {"commit_NLOC": "0.34782608695652173", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "aten\\src\\ATen\\cuda\\CUDABlas.cpp", "file_new_name": "aten\\src\\ATen\\cuda\\CUDABlas.cpp", "file_complexity": {"file_NLOC": "262", "file_CCN": "45", "file_NToken": "1397"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "357,358,359,360,361,363", "deleted_lines": null, "method_info": {"method_name": "at::cuda::blas::gemv<at::BFloat16>", "method_params": "CUDABLAS_GEMV_ARGTYPES", "method_startline": "356", "method_endline": "364", "method_complexity": {"method_NLOC": "9", "method_CCN": "3", "method_NToken": "88", "method_nesting_level": "3"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,351", "deleted_lines": "323,324,325,326,327,328,330,336,337,338,339,340,341,343", "method_info": {"method_name": "at::cuda::blas::gemv<at::Half>", "method_params": "CUDABLAS_GEMV_ARGTYPES", "method_startline": "322", "method_endline": "352", "method_complexity": {"method_NLOC": "9", "method_CCN": "3", "method_NToken": "88", "method_nesting_level": "3"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "aten\\src\\ATen\\native\\Blas.cpp", "file_new_name": "aten\\src\\ATen\\native\\Blas.cpp", "file_complexity": {"file_NLOC": "75", "file_CCN": "24", "file_NToken": "1031"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "68", "method_info": {"method_name": "at::native::addmv_out", "method_params": "result,self,mat,vec,beta,alpha", "method_startline": "47", "method_endline": "75", "method_complexity": {"method_NLOC": "23", "method_CCN": "8", "method_NToken": "297", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "aten\\src\\ATen\\native\\cuda\\Blas.cu", "file_new_name": "aten\\src\\ATen\\native\\cuda\\Blas.cu", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "11,33,34,35,36,37,38,39,40,41", "deleted_lines": "9,12,13,14,15,16,17,18,19,20,21,43,44,45,46,47"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "test\\test_torch.py", "file_new_name": "test\\test_torch.py", "file_complexity": {"file_NLOC": "14192", "file_CCN": "2230", "file_NToken": "188975"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "15006,15007,15008,15009,15010,15011,15012,15013,15014,15015,15016,15017,15018,15019,15020,15021,15022,15023,15024,15025", "deleted_lines": null, "method_info": {"method_name": "test_addmv_rowmajor_colmajor_incx_incy_lda._test", "method_params": "use_out,row_major,incx,incy,lda_tail", "method_startline": "15006", "method_endline": "15025", "method_complexity": {"method_NLOC": "16", "method_CCN": "3", "method_NToken": "225", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "14997,14998,14999,15000,15001,15002,15003,15004,15005,15006,15007,15008,15009,15010,15011,15012,15013,15014,15015,15016,15017,15018,15019,15020,15021,15022,15023,15024,15025,15026,15027,15028", "deleted_lines": null, "method_info": {"method_name": "test_addmv_rowmajor_colmajor_incx_incy_lda", "method_params": "self,device,dtype", "method_startline": "14997", "method_endline": "15028", "method_complexity": {"method_NLOC": "10", "method_CCN": "2", "method_NToken": "171", "method_nesting_level": "1"}}}}}}}}