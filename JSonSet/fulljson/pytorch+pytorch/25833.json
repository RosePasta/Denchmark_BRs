{"BR": {"BR_id": "25833", "BR_author": "Wizaron", "BRopenT": "2019-09-08T13:19:17Z", "BRcloseT": "2019-10-27T13:00:40Z", "BR_text": {"BRsummary": "CUDNN implementation of CTCLoss does not handle gradients from subsequent operations", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n CUDNN implementation of CTCLoss does not handle gradients from subsequent operations.\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n <denchmark-code>import torch\n \n class Architecture(torch.nn.Module):\n \n     def __init__(self, n_features, n_classes):\n         super(Architecture, self).__init__()\n \n         self.n_features = n_features\n         self.n_classes = n_classes\n \n         self.cls = torch.nn.Linear(self.n_features, self.n_classes)\n \n     def forward(self, x):\n         ts, bs = x.shape[:2]\n \n         x = x.view(ts * bs, self.n_features)\n         x = self.cls(x).view(ts, bs, self.n_classes)\n         x = torch.nn.functional.log_softmax(x, dim=-1)\n         return x\n \n BATCH_SIZE = 2\n GT_LENGTH = 3\n N_TIMESTEPS = GT_LENGTH * 2\n N_FEATURES = 5\n N_CLASSES = 4\n \n def get_model():\n     arch = Architecture(N_FEATURES, N_CLASSES)\n     arch.train()\n \n     return arch\n \n def get_data():\n     # Data\n     features = torch.normal(mean=torch.zeros((N_TIMESTEPS, BATCH_SIZE, N_FEATURES), dtype=torch.float32))\n     pred_lengths = N_TIMESTEPS * torch.ones((BATCH_SIZE,), dtype=torch.int32)\n \n     targets = torch.randint(1, N_CLASSES, size=(BATCH_SIZE * GT_LENGTH,), dtype=torch.int32)\n     target_lengths = GT_LENGTH * torch.ones((BATCH_SIZE,), dtype=torch.int32)\n \n     return features, pred_lengths, targets, target_lengths\n \n def cast_data(features, pred_lengths, targets, target_lengths, device, dtype):\n     features = features.to(device)\n     pred_lengths = pred_lengths.to(dtype)\n     targets = targets.to(dtype).to(device)\n     target_lengths = target_lengths.to(dtype)\n \n     if dtype == torch.int32:\n         targets = targets.to(torch.device(\"cpu\"))\n \n     return features, pred_lengths, targets, target_lengths\n \n def run(model, data, device, dtype, loss_mult=1.0):\n     if device == torch.device(\"cpu\"):\n         print(\"\\n# CTC CPU     : device {} - dtype {} - mul {}\".format(device, dtype, loss_mult))\n     elif device == torch.device(\"cuda\") and dtype == torch.int32:\n         print(\"\\n# CTC CUDNN   : device {} - dtype {} - mul {}\".format(device, dtype, loss_mult))\n     elif device == torch.device(\"cuda\") and dtype == torch.long:\n         print(\"\\n# CTC REGULAR : device {} - dtype {} - mul {}\".format(device, dtype, loss_mult))\n \n     model = model.to(device)\n     features, pred_lengths, targets, target_lengths = cast_data(*data, device, dtype)\n     preds = model(features)\n \n     # Loss\n     loss = torch.nn.functional.ctc_loss(preds, targets, pred_lengths, target_lengths)\n \n     loss = loss_mult * loss\n \n     print('LOSS : ', loss)\n \n     model.zero_grad()\n     loss.backward()\n \n     for param in model.parameters():\n         print('GRAD : ', param.grad.abs().mean())\n \n     model.zero_grad()\n \n data = get_data()\n model = get_model()\n \n print(\"----- CPU -----\")\n run(model, data, torch.device(\"cpu\"), torch.int32)\n run(model, data, torch.device(\"cpu\"), torch.long)\n run(model, data, torch.device(\"cpu\"), torch.int32, 0.0)\n run(model, data, torch.device(\"cpu\"), torch.long, 0.0)\n \n print(\"\\n----- GPU -----\")\n run(model, data, torch.device(\"cuda\"), torch.int32)\n run(model, data, torch.device(\"cuda\"), torch.long)\n run(model, data, torch.device(\"cuda\"), torch.int32, 0.0)\n run(model, data, torch.device(\"cuda\"), torch.long, 0.0)\n </denchmark-code>\n \n <denchmark-h:h2>Expected behavior</denchmark-h>\n \n We expect the gradients to be zeroed out when we multiply the loss by zero, which is not the case for cudnn implementation of ctcloss.\n We can see that, from the following output, gradients are not affected by mul when we are using cudnn implementation.\n <denchmark-code>----- CPU -----\n \n # CTC CPU     : device cpu - dtype torch.int32 - mul 1.0\n LOSS :  tensor(1.5330, grad_fn=<MulBackward0>)\n GRAD :  tensor(0.0887)\n GRAD :  tensor(0.1541)\n \n # CTC CPU     : device cpu - dtype torch.int64 - mul 1.0\n LOSS :  tensor(1.5330, grad_fn=<MulBackward0>)\n GRAD :  tensor(0.0887)\n GRAD :  tensor(0.1541)\n \n # CTC CPU     : device cpu - dtype torch.int32 - mul 0.0\n LOSS :  tensor(0., grad_fn=<MulBackward0>)\n GRAD :  tensor(0.)\n GRAD :  tensor(0.)\n \n # CTC CPU     : device cpu - dtype torch.int64 - mul 0.0\n LOSS :  tensor(0., grad_fn=<MulBackward0>)\n GRAD :  tensor(0.)\n GRAD :  tensor(0.)\n \n ----- GPU -----\n \n # CTC CUDNN   : device cuda - dtype torch.int32 - mul 1.0\n LOSS :  tensor(1.5330, device='cuda:0', grad_fn=<MulBackward0>)\n GRAD :  tensor(4.4561, device='cuda:0')\n GRAD :  tensor(6.0174, device='cuda:0')\n \n # CTC REGULAR : device cuda - dtype torch.int64 - mul 1.0\n LOSS :  tensor(1.5330, device='cuda:0', grad_fn=<MulBackward0>)\n GRAD :  tensor(0.0887, device='cuda:0')\n GRAD :  tensor(0.1541, device='cuda:0')\n \n # CTC CUDNN   : device cuda - dtype torch.int32 - mul 0.0\n LOSS :  tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n GRAD :  tensor(4.4561, device='cuda:0')\n GRAD :  tensor(6.0174, device='cuda:0')\n \n # CTC REGULAR : device cuda - dtype torch.int64 - mul 0.0\n LOSS :  tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n GRAD :  tensor(0., device='cuda:0')\n GRAD :  tensor(0., device='cuda:0')\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n PyTorch version: 1.2.0\n Is debug build: No\n CUDA used to build PyTorch: 9.2.148\n OS: Ubuntu 16.04.6 LTS\n GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\n CMake version: version 3.5.1\n Python version: 3.6\n Is CUDA available: Yes\n CUDA runtime version: Could not collect\n GPU models and configuration: GPU 0: GeForce GTX 980M\n Nvidia driver version: 418.40.04\n cuDNN version: Probably one of the following:\n /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\n /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.6.1\n Versions of relevant libraries:\n [pip] numpy==1.15.4\n [pip] torch==1.2.0\n [pip] torchgeometry==0.1.2rc1\n [pip] torchvision==0.4.0a0+9232c4a\n [conda] blas                      1.0                         mkl\n [conda] mkl                       2019.1                      144\n [conda] mkl_fft                   1.0.6            py36hd81dba3_0\n [conda] mkl_random                1.0.2            py36hd81dba3_0\n [conda] pytorch                   1.2.0           py3.6_cuda9.2.148_cudnn7.6.2_0    pytorch\n [conda] torchvision               0.4.0                 py36_cu92    pytorch\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Wizaron", "commentT": "2019-09-09T21:44:45Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/t-vi>@t-vi</denchmark-link>\n \n If cudnn is trashing some buffers I'm not sure if there's much we can do about this :(\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Wizaron", "commentT": "2019-09-10T04:44:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml#L1388>https://github.com/pytorch/pytorch/blob/master/tools/autograd/derivatives.yaml#L1388</denchmark-link>\n \n seems like we need to multiply grad_out there.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Wizaron", "commentT": "2019-10-27T13:00:40Z", "comment_text": "\n \t\tFixed by: Use grad_out for cudnn CTC loss <denchmark-link:https://github.com/pytorch/pytorch/pull/27039>#27039</denchmark-link>\n \n Thank you for reporting!\n \t\t"}}}, "commit": {"commit_id": "f461184505149560803855f3a40d9e0e54c64826", "commit_author": "Thomas Viehmann", "commitT": "2019-10-15 11:36:37-07:00", "commit_complexity": {"commit_NLOC": "0.3246753246753247", "commit_CCN": "0.8051948051948052", "commit_Nprams": "0.2597402597402597"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "aten\\src\\ATen\\cudnn\\Descriptors.h", "file_new_name": "aten\\src\\ATen\\cudnn\\Descriptors.h", "file_complexity": {"file_NLOC": "199", "file_CCN": "33", "file_NToken": "1215"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "264,265,266,267,268,269,270", "deleted_lines": null, "method_info": {"method_name": "at::native::AT_CUDA_APICTCLossDescriptor::setEx", "method_params": "datatype,normMode,gradMode", "method_startline": "264", "method_endline": "270", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "3"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "aten\\src\\ATen\\native\\LossCTC.cpp", "file_new_name": "aten\\src\\ATen\\native\\LossCTC.cpp", "file_complexity": {"file_NLOC": "295", "file_CCN": "55", "file_NToken": "3015"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "381,382", "deleted_lines": null, "method_info": {"method_name": "at::native::ctc_loss", "method_params": "log_probs,targets,input_lengths,target_lengths,BLANK,reduction,zero_infinity", "method_startline": "377", "method_endline": "386", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "171", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "343,344,345,353,354,355,356,357,358,359,360,361", "deleted_lines": "342,343,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,371", "method_info": {"method_name": "at::native::ctc_loss", "method_params": "log_probs,targets,input_lengths,target_lengths,BLANK,reduction,zero_infinity", "method_startline": "341", "method_endline": "374", "method_complexity": {"method_NLOC": "29", "method_CCN": "6", "method_NToken": "254", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "aten\\src\\ATen\\native\\cudnn\\LossCTC.cpp", "file_new_name": "aten\\src\\ATen\\native\\cudnn\\LossCTC.cpp", "file_complexity": {"file_NLOC": "103", "file_CCN": "13", "file_NToken": "762"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "15,16,17,18,19,20,21,22", "deleted_lines": null, "method_info": {"method_name": "at::native::_use_cudnn_ctc_loss", "method_params": "log_probs,targets,input_lengths,target_lengths,BLANK", "method_startline": "15", "method_endline": "22", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "26", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "101,102,103,104,105,106,107,109,110,111,112,113,114,115,116,117,118,119,124,125,126,127,128,129,130,131,132,133,134,135,136,137", "deleted_lines": "71,73,74,75,76,82,83,84,85,86", "method_info": {"method_name": "at::native::_cudnn_ctc_loss", "method_params": "log_probs_t,targets_t,input_lengths_,target_lengths_,BLANK,deterministic,zero_infinity", "method_startline": "70", "method_endline": "139", "method_complexity": {"method_NLOC": "56", "method_CCN": "2", "method_NToken": "459", "method_nesting_level": "2"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "aten\\src\\ATen\\native\\native_functions.yaml", "file_new_name": "aten\\src\\ATen\\native\\native_functions.yaml", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "97,98,99,100,101,102", "deleted_lines": null}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\test_autograd.py", "file_new_name": "test\\test_autograd.py", "file_complexity": {"file_NLOC": "2992", "file_CCN": "543", "file_NToken": "30340"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758", "deleted_lines": null, "method_info": {"method_name": "test_ctc_loss_cudnn", "method_params": "self,device", "method_startline": "3738", "method_endline": "3758", "method_complexity": {"method_NLOC": "20", "method_CCN": "1", "method_NToken": "236", "method_nesting_level": "1"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "test\\test_nn.py", "file_new_name": "test\\test_nn.py", "file_complexity": {"file_NLOC": "7862", "file_CCN": "1257", "file_NToken": "88877"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8952", "deleted_lines": "8952"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tools\\autograd\\derivatives.yaml", "file_new_name": "tools\\autograd\\derivatives.yaml", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1385", "deleted_lines": "1385"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tools\\autograd\\templates\\Functions.cpp", "file_new_name": "tools\\autograd\\templates\\Functions.cpp", "file_complexity": {"file_NLOC": "1530", "file_CCN": "351", "file_NToken": "14975"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "2456,2457,2458,2459,2460,2461,2462,2463,2464,2465", "deleted_lines": null, "method_info": {"method_name": "torch::autograd::generated::_cudnn_ctc_loss_backward", "method_params": "grad_out,loss,raw_grad,zero_infinity", "method_startline": "2456", "method_endline": "2465", "method_complexity": {"method_NLOC": "10", "method_CCN": "2", "method_NToken": "95", "method_nesting_level": "4"}}}}}}}}