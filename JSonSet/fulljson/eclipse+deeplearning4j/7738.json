{"BR": {"BR_id": "7738", "BR_author": "forever1078", "BRopenT": "2019-05-15T07:47:34Z", "BRcloseT": "2019-05-21T07:47:24Z", "BR_text": {"BRsummary": "exception when changing the number of dataset using BalancedPathFilter", "BRdescription": "\n I want to control the dataset size using BalancedPathFilter(new Random(seed), labelMaker, 20000, numLabels, 6000).The total dataset size is 60000,but I only want to use 20000 to train.\n However,\n java.lang.RuntimeException: java.lang.UnsupportedOperationException: Cannot do conversion to one hot using batched reader: 10 output classes, but array.size(1) is 4 (must be equal to 1 or numClasses = 10)\n at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:430)\n Caused by: java.lang.UnsupportedOperationException: Cannot do conversion to one hot using batched reader: 10 output classes, but array.size(1) is 4 (must be equal to 1 or numClasses = 10)\n at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertWritablesBatched(RecordReaderMultiDataSetIterator.java:413)\n at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertFeaturesOrLabels(RecordReaderMultiDataSetIterator.java:359)\n at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.nextMultiDataSet(RecordReaderMultiDataSetIterator.java:332)\n at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next(RecordReaderMultiDataSetIterator.java:212)\n at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:364)\n at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:439)\n at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:84)\n at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:404)\n How can I control the dataset size for training\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "forever1078", "commentT": "2019-05-15T09:09:53Z", "comment_text": "\n \t\tPlease show the code you have used.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "forever1078", "commentT": "2019-05-15T10:41:23Z", "comment_text": "\n \t\tjust the MnistClassifier example\n <denchmark-code> val trainData = File(rootDir + \"mnist_png/training\")\n  val randNumGen = Random(1234)\n  val trainSplit = FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, Random(1234))\n  val labelMaker = ParentPathLabelGenerator() // use parent directory name as the image label\n  val pathFilter = BalancedPathFilter(Random(42),labelMaker,20000,10,6000)\n  val split = trainSplit.sample(pathFilter,1.0)\n  val trainRR = ImageRecordReader(28, 28, 1, labelMaker)\n \n  trainRR.initialize(split[0])\n  samplesUsedInTraining = split[0].length().toInt()\n \n  val trainIter = RecordReaderDataSetIterator(trainRR, 54, 1, 10)\n \n  // pixel values from 0-255 to 0-1 (min-max scaling)\n  val imageScaler = ImagePreProcessingScaler()\n  imageScaler.fit(trainIter)\n  trainIter.preProcessor = imageScaler\n  model!!.fit(trainIter)\n </denchmark-code>\n \n the errors are\n <denchmark-code>java.lang.RuntimeException: java.lang.UnsupportedOperationException: Cannot do conversion to one hot using batched reader: 10 output classes, but array.size(1) is 4 (must be equal to 1 or numClasses = 10)\n         at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:430)\n      Caused by: java.lang.UnsupportedOperationException: Cannot do conversion to one hot using batched reader: 10 output classes, but array.size(1) is 4 (must be equal to 1 or numClasses = 10)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertWritablesBatched(RecordReaderMultiDataSetIterator.java:413)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertFeaturesOrLabels(RecordReaderMultiDataSetIterator.java:359)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.nextMultiDataSet(RecordReaderMultiDataSetIterator.java:332)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next(RecordReaderMultiDataSetIterator.java:212)\n         at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:364)\n         at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:439)\n         at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:84)\n         at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:404)\n </denchmark-code>\n \n when I print the trainRR.numLabels(),it is less than 10.But how can I solve it?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "forever1078", "commentT": "2019-05-15T20:02:29Z", "comment_text": "\n \t\tHmmm, I think there's a question in RandomPathFilter.filter() method.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "forever1078", "commentT": "2019-05-16T08:33:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Charele>@Charele</denchmark-link>\n  Good idea!\n <denchmark-link:https://github.com/forever1078>@forever1078</denchmark-link>\n  Does RandomPathFilter work or are you having issues only with BalancedPathFilter?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "forever1078", "commentT": "2019-05-16T11:05:06Z", "comment_text": "\n \t\tRandomPathFilter has the same issue\n <denchmark-code> val trainData = File(rootDir + \"mnist_png/training\")\n  val randNumGen = Random(1234)\n  val trainSplit = FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, Random(1234))\n  val labelMaker = ParentPathLabelGenerator() // use parent directory name as the image label\n  val pathFilter = RandomPathFilter(Random(42),labelMaker,20000)\n  val split = trainSplit.sample(pathFilter,1.0)\n  val trainRR = ImageRecordReader(28, 28, 1, labelMaker)\n \n  trainRR.initialize(split[0])\n  samplesUsedInTraining = split[0].length().toInt()\n \n  val trainIter = RecordReaderDataSetIterator(trainRR, 54, 1, 10)\n \n  // pixel values from 0-255 to 0-1 (min-max scaling)\n  val imageScaler = ImagePreProcessingScaler()\n  imageScaler.fit(trainIter)\n  trainIter.preProcessor = imageScaler\n  model!!.fit(trainIter)\n </denchmark-code>\n \n <denchmark-code>java.lang.RuntimeException: java.lang.UnsupportedOperationException: Cannot do conversion to one hot using batched reader: 10 output classes, but array.size(1) is 4 (must be equal to 1 or numClasses = 10)\n         at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:430)\n      Caused by: java.lang.UnsupportedOperationException: Cannot do conversion to one hot using batched reader: 10 output classes, but array.size(1) is 4 (must be equal to 1 or numClasses = 10)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertWritablesBatched(RecordReaderMultiDataSetIterator.java:413)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.convertFeaturesOrLabels(RecordReaderMultiDataSetIterator.java:359)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.nextMultiDataSet(RecordReaderMultiDataSetIterator.java:332)\n         at org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.next(RecordReaderMultiDataSetIterator.java:212)\n         at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:364)\n         at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:439)\n         at org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.next(RecordReaderDataSetIterator.java:84)\n         at org.deeplearning4j.datasets.iterator.AsyncDataSetIterator$AsyncPrefetchThread.run(AsyncDataSetIterator.java:404)\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "forever1078", "commentT": "2019-05-16T11:30:00Z", "comment_text": "\n \t\tIt sounds like your dataset only has 4 classes, not 10. Make sure you have 10 classes in there.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "forever1078", "commentT": "2019-05-16T13:30:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/saudet>@saudet</denchmark-link>\n  It's true,I want to use a part of the MNist dataset. So I try to use the BalancedPathFilter or RandomPathFilter to select a part of the dataset. However the selected samples only contains 4 classes.\n How can I select a part of dataset that contains all classes?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "forever1078", "commentT": "2019-05-18T13:46:05Z", "comment_text": "\n \t\tI see. I think that could be fixed by shuffling the paths before filtering instead of after in :\n <denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/datavec/datavec-api/src/main/java/org/datavec/api/io/filters/RandomPathFilter.java#L66>https://github.com/deeplearning4j/deeplearning4j/blob/master/datavec/datavec-api/src/main/java/org/datavec/api/io/filters/RandomPathFilter.java#L66</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "forever1078", "commentT": "2019-05-20T11:07:42Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/saudet>@saudet</denchmark-link>\n  yes, I think so, Maybe we should shuffle before the \"maxPaths\" rule.\n But I find another question in BalancedPathFilter.filter(),\n I can't get the expected number of paths.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "forever1078", "commentT": "2019-05-20T13:27:05Z", "comment_text": "\n \t\tval trainSplit =\n new FileSplit(new File(\"c:\\\\mnist_png\\\\training\"), NativeImageLoader.ALLOWED_FORMATS, new Random())\n val pathFilter = new BalancedPathFilter(new Random(), null, null);\n val splits = trainSplit.sample(pathFilter, 1.0)\n val trainRR =\n new ImageRecordReader(28, 28, 1, new ParentPathLabelGenerator())\n trainRR.initialize(splits(0))\n val trainIter =\n new RecordReaderDataSetIterator(trainRR, 1, 1, 10)\n println(\"I get: \" + trainIter.size)\n I think I should get the full dataset,it's 60000.I can't\n o.n.n.Nd4jBlas - Number of threads used for BLAS: 4\n o.n.l.a.o.e.DefaultOpExecutioner - Backend used: [CPU]; OS: [Windows 7]\n o.n.l.a.o.e.DefaultOpExecutioner - Cores: [8]; Memory: [3.5GB];\n o.n.l.a.o.e.DefaultOpExecutioner - Blas vendor: [MKL]\n I get: 54210\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "forever1078", "commentT": "2019-05-21T06:18:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Charele>@Charele</denchmark-link>\n  That's normal for BalancedPathFilter, use RandomPathFilter.\n \t\t"}}}, "commit": {"commit_id": "73f93ef7edca3d10eb7a6ed9af4ef5f0522b4710", "commit_author": "Samuel Audet", "commitT": "2019-05-21 16:46:44+09:00", "commit_complexity": {"commit_NLOC": "0.05555555555555555", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "datavec\\datavec-api\\src\\main\\java\\org\\datavec\\api\\io\\filters\\RandomPathFilter.java", "file_new_name": "datavec\\datavec-api\\src\\main\\java\\org\\datavec\\api\\io\\filters\\RandomPathFilter.java", "file_complexity": {"file_NLOC": "45", "file_CCN": "12", "file_NToken": "299"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "68,69,70,71,73", "deleted_lines": "68,76", "method_info": {"method_name": "RandomPathFilter::filter", "method_params": "paths", "method_startline": "67", "method_endline": "82", "method_complexity": {"method_NLOC": "14", "method_CCN": "5", "method_NToken": "115", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "datavec\\datavec-api\\src\\test\\java\\org\\datavec\\api\\split\\InputSplitTests.java", "file_new_name": "datavec\\datavec-api\\src\\test\\java\\org\\datavec\\api\\split\\InputSplitTests.java", "file_complexity": {"file_NLOC": "106", "file_CCN": "7", "file_NToken": "811"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152", "deleted_lines": null, "method_info": {"method_name": "InputSplitTests::testSampleNoBias", "method_params": "", "method_startline": "134", "method_endline": "152", "method_complexity": {"method_NLOC": "17", "method_CCN": "4", "method_NToken": "151", "method_nesting_level": "1"}}}}}}}}