{"BR": {"BR_id": "3066", "BR_author": "dreamflasher", "BRopenT": "2020-12-09T16:15:55Z", "BRcloseT": "2021-01-07T19:53:55Z", "BR_text": {"BRsummary": "wandb integration failing with latest wandb library", "BRdescription": "\n Describe the bug\n wandb introduced a breaking change with their library, when calling wandb.log() \u2013 previously this would not increase the step, but now it does. Currently the integration increases this counter on the first call, and consecutive calls fail because the provided step (not increased) does not match with the increased step counter.\n To Reproduce\n Use the WandbCallback\n Error with full stack trace\n Place between these lines with triple backticks:\n <denchmark-code>wandb: WARNING Step must only increase in log calls.  Step 6422 < 6423; dropping {'epoch': 20}.\n </denchmark-code>\n \n Additional context\n I recommend to remove the internal step counter and let wandb take care of that. I'll propose a PR asap.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dreamflasher", "commentT": "2020-12-10T18:45:51Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/dreamflasher>@dreamflasher</denchmark-link>\n  ,\n Do you have any reproducible code you could share so that we can see the issue?\n Providing the step explicitly ensures we associate validation and training data to the correct step.\n With the proposed PR, wandb.step would increase after the end of each training batch. It means that validation metrics would be associated with the next step.\n If you log custom data, the recommended way is to create wandb_process with the new @typedispatch as shown in the notebook (maybe we can better document it).\n Otherwise you would just use commit=False in your own calls to wandb.log.\n Let me know if it works for you.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dreamflasher", "commentT": "2020-12-10T20:45:09Z", "comment_text": "\n \t\tThanks a lot! This is happening in a bigger integrated project, I'll try to distill a simpler piece of code for reproduction. In my code I always call wandb.log with commit=False.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dreamflasher", "commentT": "2020-12-17T12:31:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  I wasn't able to create a concise example, but I believe I found the root cause \u2013 actually what I already wrote in the initial message \u2013 that you changed the wandb.log api (always increase), but at four places in the code it shouldn't be increased.\n Here's the PR: <denchmark-link:https://github.com/fastai/fastai/pull/3080>#3080</denchmark-link>\n \n Does this make sense to you and is this the expected behaviour?\n On my end this fixes the error.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dreamflasher", "commentT": "2020-12-17T17:05:20Z", "comment_text": "\n \t\tI'm still surprised it would solve your problem:\n \n commit=True submits metrics as well as previously uncommitted metrics (for example when using commit=False or step), then increments the wandb step. When False it just adds the metrics to dict of uncommitted values for future commit.\n step gives direct access to wandb step and prevents from logging at a previous step. When logging at a future step, it will commit all previous steps, and not commit only the current step.\n \n I'm thinking that there is a mismatch between the logging from the callback and your custom logging.\n You could:\n \n Option 1 (recommended unless impossible): log data mainly using wandb_process and let the callback handle the proper step, taking advantage of type_dispatch\n Option 2: use commit=False and don't provide any step in your custom logging, this will automatically match the last step logged by the callback\n Option 3: if you have to provide a step (maybe you log something before the callback), you can access it through wandb.run.step. You need to be careful here because WandbCallback keeps its own internal counter self._wandb_step which it increases at after_batch, right before logging.\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "dreamflasher", "commentT": "2020-12-17T17:18:06Z", "comment_text": "\n \t\tThanks a lot for your feedback. I removed all custom logging and still getting the error described above. So the root cause is very likely the WandbCallback (I ran all our tests and with the proposed changes it's working). Did you  have a look at the PR? You increase the step count twice at the end of the epoch, is that intentional? At the end of an epoch first after_batch and then after_epoch are being called. You don't increase the callback counter, but you do increase (implicitly) the wandb counter by calling wandb.log without commit, which defaults to True, and thus increases the count, right?\n It looks to me like the auto increment with wandb.log() is detrimental, because now it's not possible anymore to submit but not to increase the count?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "dreamflasher", "commentT": "2020-12-17T17:37:21Z", "comment_text": "\n \t\tI did have a look at the PR. I also performed a few small tests to check commit and step worked as expected.\n Basically you're not supposed to provide both step and commit.\n When step is provided, typically commit is at its default which is None.\n Setting it to False will actually call the exact same functions.\n commit is forced to True only when step is None (also default value)\n That is why I don't understand why it would change the behavior in your code.\n Relevant code section is <denchmark-link:https://github.com/wandb/client/blob/52f31e00a83e887d94cd46a08b2fe2652670f5c9/wandb/sdk/wandb_run.py#L794-L813>here</denchmark-link>\n .\n after_batch increases the step to get it ready for current logging (it is initially set at -1).\n I'm not sure I understand where we increase the counter at after_epoch as the step is manually given.\n Here are other interesting links:\n \n my test checking internal behavior of wandb depending on commit: colab\n tests in fastai library, 2 training loops are performed and there is no output warning: see end of notebook\n \n Is the issue happening at your first fit loop or are you having additional training loops within a same run?\n For debugging, you could try to print wandb.run.step and WandbCallback._wandb_step.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "dreamflasher", "commentT": "2020-12-17T17:53:42Z", "comment_text": "\n \t\tIt happens after the first epoch, no previous runs before, only one init call:\n <denchmark-code>epoch     train_loss  valid_loss  accuracy  brier_score  ece       nll       err  time\n 0         7.291822    6.423790    0.891155  0.022018     0.056384  6.423790  0.122854           05:20     wandb: WARNING Step must only increase in log calls.  Step 232 < 233; dropping {'epoch': 1}.\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "dreamflasher", "commentT": "2020-12-17T17:59:15Z", "comment_text": "\n \t\tI could take a closer look if you can share any code.\n Can you share your W&B run? When you look at each of the metrics logged on your panel, is there any that is not logged through fastai callback?\n Otherwise for debugging, you could try to print wandb.run.step and WandbCallback._wandb_step.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "dreamflasher", "commentT": "2020-12-18T11:45:15Z", "comment_text": "\n \t\tSometimes it happens after the first epoch, no previous runs before, only one init call:\n <denchmark-code>epoch     train_loss  valid_loss  accuracy  brier_score  ece       nll       err  time\n 0         7.291822    6.423790    0.891155  0.022018     0.056384  6.423790  0.122854           05:20     wandb: WARNING Step must only increase in log calls.  Step 232 < 233; dropping {'epoch': 1}.\n </denchmark-code>\n \n I restored the original fastai code, removed all our custom wandb.log calls, and now the problem occurs after the first run.\n The 5th epoch finishes and then I get wandb: WARNING Step must only increase in log calls.  Step 1164 < 1165; dropping {'epoch': 5}..\n Also in this run we have multiple consecutive wandb.init calls \u2013 but it already happens in the first run (I can turn the multiple runs off, if that helps debugging).\n Thank you so much for your continued help! Yes, the wandb run is: imachines/mitl-regression-tests/90xkcy38.\n The code should be uploaded by default right?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "dreamflasher", "commentT": "2020-12-20T02:06:30Z", "comment_text": "\n \t\tI would guess it happens when logging your validation data. If you have a call to wandb.log, make sure to use commit=False.\n Feel free to share your code privately if you want me to take a closer look.\n It's not always uploaded depending on your settings.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "dreamflasher", "commentT": "2020-12-20T03:23:39Z", "comment_text": "\n \t\tI don't have a single wandb.log call in there as previously mentioned.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "dreamflasher", "commentT": "2021-01-04T19:14:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jph00>@jph00</denchmark-link>\n  Please reopen\n <denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  Here's a reproducible example of what's failing:\n <denchmark-code>import wandb\n from fastai.callback.wandb import WandbCallback\n from fastai.vision.all import *\n \n \n def label_func(f):\n     return f[0].isupper()\n \n \n path = untar_data(URLs.PETS)\n files = get_image_files(path / \"images\")\n dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))\n wandb.init()\n learn = cnn_learner(dls, resnet34, metrics=error_rate, cbs=[WandbCallback(log_model=False)])\n \n learn.fit_one_cycle(3, slice(0.001, 0.03))\n \n learn.validate()\n </denchmark-code>\n \n The problem is with learn.validate() \u2013 took me a while to extract this.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "dreamflasher", "commentT": "2021-01-04T20:48:53Z", "comment_text": "\n \t\tThanks a lot for this <denchmark-link:https://github.com/dreamflasher>@dreamflasher</denchmark-link>\n \n I actually never used this function.\n I'll need to take a look to understand the difference between validate and validation from the fit loop.\n I imagine you would expect to log validation metrics, though it's tricky to know what should be done here as you could have the same metrics already associated to the last step from your fit loop.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "dreamflasher", "commentT": "2021-01-05T13:43:30Z", "comment_text": "\n \t\tWe use learn.validate() for two reasons\n \n To get the results of the best run and log this to wandb (wandb shows the last result, but as we are keeping the best model for future use, we want to see the best numbers in wandb (according to validation metric)).\n We do multiple training runs and want to average metrics across runs (monte-carlo cross-validation), and at the end report those average metrics (wandb doesn't support two levels of grouping so far, we need one for MCCV runs and the other for multiple active learning runs)\n \n In the example above we don't do any logging so far, and the error already occurs. But yes, likely the next issue will be to do the logging without screwing up wandb. But let's start with fixing learn.validate() \u2013 it looks to me that during this no logging to wandb should be the default?\n Currently\n <denchmark-code>def after_batch(self):\n         \"Log hyper-parameters and training loss\"\n         if self.training:\n </denchmark-code>\n \n Maybe do the if self.training: guard also in def after_epoch(self):?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "dreamflasher", "commentT": "2021-01-05T22:09:15Z", "comment_text": "\n \t\t\n Maybe do the if self.training: guard also in def after_epoch(self):?\n \n I don't think it would work because we want to log validation metrics there, for which self.training=False.\n I believe the solution may be to remove <denchmark-link:https://github.com/fastai/fastai/blob/870409f78df41e04a2be378197e41f8c2e1ab371/fastai/callback/wandb.py#L123>this line</denchmark-link>\n .\n Its purpose is to force commit the last data at the end of training (happens automatically in a script but not necessarily in notebooks), which also auto-increment wandb internal step.\n I think we would want to log the validation data (though it may overwrite previous validation data logged for this step, which should be the same unless the data changed).\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "dreamflasher", "commentT": "2021-01-06T21:43:24Z", "comment_text": "\n \t\tIt looks like this solves the issue on our end, did you run it through your test suite? Shall I create a PR?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "dreamflasher", "commentT": "2021-01-07T13:56:26Z", "comment_text": "\n \t\tJust pulled the last fastai version and now I am getting this error with the above example:\n <denchmark-code>Traceback (most recent call last):\n   File \"wandbtest.py\", line 16, in <module>\n     learn.fit_one_cycle(3, slice(0.001, 0.03))\n   File \"/home/ubuntu/git/fastai/fastai/callback/schedule.py\", line 112, in fit_one_cycle\n     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 211, in fit\n     self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 160, in _with_events\n     try: self(f'before_{event_type}');  f()\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 202, in _do_fit\n     self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 160, in _with_events\n     try: self(f'before_{event_type}');  f()\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 196, in _do_epoch\n     self._do_epoch_train()\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 188, in _do_epoch_train\n     self._with_events(self.all_batches, 'train', CancelTrainException)\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 160, in _with_events\n     try: self(f'before_{event_type}');  f()\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 166, in all_batches\n     for o in enumerate(self.dl): self.one_batch(*o)\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 184, in one_batch\n     self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 162, in _with_events\n     self(f'after_{event_type}');  final()\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 141, in __call__\n     def __call__(self, event_name): L(event_name).map(self._call_one)\n   File \"/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/foundation.py\", line 154, in map\n     def map(self, f, *args, gen=False, **kwargs): return self._new(map_ex(self, f, *args, gen=gen, **kwargs))\n   File \"/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py\", line 641, in map_ex\n     return list(res)\n   File \"/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py\", line 631, in __call__\n     return self.func(*fargs, **kwargs)\n   File \"/home/ubuntu/git/fastai/fastai/learner.py\", line 145, in _call_one\n     for cb in self.cbs.sorted('order'): cb(event_name)\n   File \"/home/ubuntu/git/fastai/fastai/callback/core.py\", line 44, in __call__\n     if self.run and _run: res = getattr(self, event_name, noop)()\n   File \"/home/ubuntu/git/fastai/fastai/callback/wandb.py\", line 92, in after_batch\n     wandb.log({'epoch': self._wandb_epoch, 'train_loss': to_detach(self.smooth_loss.clone()), 'raw_loss': to_detach(self.loss.clone()), **hypers}, step=s\n elf._wandb_step)\n   File \"/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py\", line 378, in __getattr__\n     if attr is not None: return getattr(attr,k)\n   File \"/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py\", line 378, in __getattr__\n     if attr is not None: return getattr(attr,k)\n   File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 778, in __getattr__\n     raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n torch.nn.modules.module.ModuleAttributeError: 'Sequential' object has no attribute 'smooth_loss'\n </denchmark-code>\n \n cc <denchmark-link:https://github.com/jph00>@jph00</denchmark-link>\n  was there a recent change that could explain this new error?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "dreamflasher", "commentT": "2021-01-07T14:28:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  How about this: <denchmark-link:https://github.com/fastai/fastai/pull/3129>#3129</denchmark-link>\n  \u2013 we can do the last empty log, but keeping the step_count \u2013 this solves it on our side too, and shouldn't change anything in your tests? I also included a fix for the above error. Do you approve?\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "dreamflasher", "commentT": "2021-01-07T17:33:41Z", "comment_text": "\n \t\tSorry just saw this - I fixed that this morning.\n \t\t"}}}, "commit": {"commit_id": "4bd5a4176bdfd1aaa531f73a1332db784279ccd6", "commit_author": "Marcel Ackermann", "commitT": "2021-01-07 11:53:54-08:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fastai\\callback\\wandb.py", "file_new_name": "fastai\\callback\\wandb.py", "file_complexity": {"file_NLOC": "213", "file_CCN": "74", "file_NToken": "2256"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "123,124", "deleted_lines": "123", "method_info": {"method_name": "after_fit", "method_params": "self", "method_startline": "114", "method_endline": "124", "method_complexity": {"method_NLOC": "11", "method_CCN": "6", "method_NToken": "102", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "nbs\\70_callback.wandb.ipynb", "file_new_name": "nbs\\70_callback.wandb.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "204,205", "deleted_lines": "204"}}}}}}