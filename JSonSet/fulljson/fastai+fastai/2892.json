{"BR": {"BR_id": "2892", "BR_author": "muellerzr", "BRopenT": "2020-10-21T02:48:59Z", "BRcloseT": "2020-11-03T21:41:48Z", "BR_text": {"BRsummary": "Learn.load and LRFinder not functioning properly for the optimizer states", "BRdescription": "\n Currently there is a bug where if you run  twice in a row, you will get very different results such as below:\n <denchmark-link:https://user-images.githubusercontent.com/7831895/96666881-f7f9e800-1325-11eb-858a-9eae1ae82b12.png></denchmark-link>\n \n This sort of pattern is common in models that already have trained weights. Since we know that the models weights are stored away, my investigation led me to believe that something would be wrong with how we are loading in the optimizer in learn.load.\n The stem of the issue is the fact that if self.opt is none we call create_opt and then pass this new opt into load_model as seen below:\n def load(self, file, with_opt=None, device=None, **kwargs):\n         if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n         if self.opt is None: self.create_opt()\n         file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n         load_model(file, self.model, self.opt, device=device, **kwargs)\n         return self\n What winds up happening is the same optimizer state is being used, despite us trying to load in new weights because self.opt is not None when it goes in. The proposed fix is that if with_opt is None, we set self.opt to None. This allows for a much better graph that we could expect:\n @patch\n def load(self:Learner, file, with_opt=None, device=None, **kwargs):\n     if device is None and hasattr(self.dls, 'device'): device = self.dls.device\n     if with_opt is None: self.opt=None\n     file = join_path_file(file, self.path/self.model_dir, ext='.pth')\n     load_model(file, self.model, self.opt, device=device, **kwargs)\n     return self\n <denchmark-link:https://user-images.githubusercontent.com/7831895/96667077-5b841580-1326-11eb-856a-c35537a094c6.png></denchmark-link>\n \n Let me know if this fix looks okay to you <denchmark-link:https://github.com/jph00>@jph00</denchmark-link>\n , or if you believe there is a better solution for addressing the issue\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "muellerzr", "commentT": "2020-10-21T03:16:12Z", "comment_text": "\n \t\tCurrently it doesn't seem to work with mixed precision, here is the stack trace:\n ---------------------------------------------------------------------------\n AttributeError                            Traceback (most recent call last)\n <ipython-input-20-979fb3ad48c2> in <module>\n       6     load_model(file, self.model, self.opt, device=device, **kwargs)\n       7     return self\n ----> 8 learn.lr_find()\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/schedule.py in lr_find(self, start_lr, end_lr, num_it, stop_div, show_plot, suggestions)\n     226     n_epoch = num_it//len(self.dls.train) + 1\n     227     cb=LRFinder(start_lr=start_lr, end_lr=end_lr, num_it=num_it, stop_div=stop_div)\n --> 228     with self.no_logging(): self.fit(n_epoch, cbs=cb)\n     229     if show_plot: self.recorder.plot_lr_find()\n     230     if suggestions:\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/logargs.py in _f(*args, **kwargs)\n      54         init_args.update(log)\n      55         setattr(inst, 'init_args', init_args)\n ---> 56         return inst if to_return else f(*args, **kwargs)\n      57     return _f\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)\n     205             self.opt.set_hypers(lr=self.lr if lr is None else lr)\n     206             self.n_epoch = n_epoch\n --> 207             self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n     208 \n     209     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)\n     155         try:       self(f'before_{event_type}')       ;f()\n     156         except ex: self(f'after_cancel_{event_type}')\n --> 157         finally:   self(f'after_{event_type}')        ;final()\n     158 \n     159     def all_batches(self):\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in __call__(self, event_name)\n     131     def ordered_cbs(self, event): return [cb for cb in sort_by_run(self.cbs) if hasattr(cb, event)]\n     132 \n --> 133     def __call__(self, event_name): L(event_name).map(self._call_one)\n     134 \n     135     def _call_one(self, event_name):\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in map(self, f, *args, **kwargs)\n     270              else f.format if isinstance(f,str)\n     271              else f.__getitem__)\n --> 272         return self._new(map(g, self))\n     273 \n     274     def filter(self, f, negate=False, **kwargs):\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in _new(self, items, *args, **kwargs)\n     216     @property\n     217     def _xtra(self): return None\n --> 218     def _new(self, items, *args, **kwargs): return type(self)(items, *args, use_list=None, **kwargs)\n     219     def __getitem__(self, idx): return self._get(idx) if is_indexer(idx) else L(self._get(idx), use_list=None)\n     220     def copy(self): return self._new(self.items.copy())\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in __call__(cls, x, *args, **kwargs)\n     197     def __call__(cls, x=None, *args, **kwargs):\n     198         if not args and not kwargs and x is not None and isinstance(x,cls): return x\n --> 199         return super().__call__(x, *args, **kwargs)\n     200 \n     201 # Cell\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in __init__(self, items, use_list, match, *rest)\n     207         if items is None: items = []\n     208         if (use_list is not None) or not _is_array(items):\n --> 209             items = list(items) if use_list else _listify(items)\n     210         if match is not None:\n     211             if is_coll(match): match = len(match)\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in _listify(o)\n     114     if isinstance(o, list): return o\n     115     if isinstance(o, str) or _is_array(o): return [o]\n --> 116     if is_iter(o): return list(o)\n     117     return [o]\n     118 \n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastcore/foundation.py in __call__(self, *args, **kwargs)\n     177             if isinstance(v,_Arg): kwargs[k] = args.pop(v.i)\n     178         fargs = [args[x.i] if isinstance(x, _Arg) else x for x in self.pargs] + args[self.maxi+1:]\n --> 179         return self.fn(*fargs, **kwargs)\n     180 \n     181 # Cell\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in _call_one(self, event_name)\n     135     def _call_one(self, event_name):\n     136         assert hasattr(event, event_name), event_name\n --> 137         [cb(event_name) for cb in sort_by_run(self.cbs)]\n     138 \n     139     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/learner.py in <listcomp>(.0)\n     135     def _call_one(self, event_name):\n     136         assert hasattr(event, event_name), event_name\n --> 137         [cb(event_name) for cb in sort_by_run(self.cbs)]\n     138 \n     139     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/core.py in __call__(self, event_name)\n      42                (self.run_valid and not getattr(self, 'training', False)))\n      43         res = None\n ---> 44         if self.run and _run: res = getattr(self, event_name, noop)()\n      45         if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit\n      46         return res\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/fp16.py in after_fit(self)\n     123     def after_fit(self):\n     124         if not hasattr(self,'master_pgs'): return\n --> 125         _copy_state(self.learn.opt, self.master_pgs, self.model_pgs)\n     126         self.learn.opt.param_lists  = self.old_pgs\n     127         delattr(self, \"master_pgs\")\n \n ~/anaconda3/envs/fastai/lib/python3.8/site-packages/fastai/callback/fp16.py in _copy_state(opt, pgs1, pgs2)\n      58 # Cell\n      59 def _copy_state(opt, pgs1, pgs2):\n ---> 60     opt.param_lists = pgs2\n      61     for pg1,pg2 in zip(pgs1, pgs2):\n      62         for p1,p2 in zip(pg1, pg2): opt.state[p2] = copy_clone(opt.state.pop(p1, {}))\n \n AttributeError: 'NoneType' object has no attribute 'param_lists'\n I don't know enough about fp16 to know where to start debugging or the obvious red flags\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "muellerzr", "commentT": "2020-10-21T07:28:54Z", "comment_text": "\n \t\tI have the same issue here.\n Also maybe related, some models with fp16 diverge if I do a lr_find before calling fit. So I do lr_find, then recreate the learner and call fit afterwards.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "muellerzr", "commentT": "2020-11-03T21:38:40Z", "comment_text": "\n \t\tYou didn't provide a repro so I don't actually know if I fixed it - but I made a change to load back the orig opt after lr_find, so hopefully it's fixed now. I couldn't repro the fp16 issue at all. Here's what I did\n <denchmark-code>from fastai.vision.all import *\n \n path = untar_data(URLs.PETS)\n files = get_image_files(path/\"images\")\n def label_func(f): return f[0].isupper()\n dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224), n_workers=8)\n learn = cnn_learner(dls, resnet34, metrics=error_rate).to_fp16()\n learn.lr_find()\n </denchmark-code>\n \n Please reopen if there's still a problem and you can provide a minimal repro.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "muellerzr", "commentT": "2020-11-17T21:48:42Z", "comment_text": "\n \t\tI I have similar issue and am using the lastest version v2.1.5.\n If I insert a lr_find in front of the fit, it will change the training result. Please see screenshot below\n <denchmark-link:https://user-images.githubusercontent.com/26617297/99454500-5e0c7600-298b-11eb-9da2-392fb989e3a4.png></denchmark-link>\n \n reproducible code:\n <denchmark-code>from fastai.vision.all import *\n def is_cat(x): return x[0].isupper()\n path = untar_data(URLs.PETS)/'images'\n set_seed(42,True)\n dls = ImageDataLoaders.from_name_func(\n     path, get_image_files(path), valid_pct=0.2, seed=42,\n     label_func=is_cat, item_tfms=Resize(224),shuffle_train= False)\n learn = cnn_learner(dls, resnet34, metrics=error_rate)\n learn.fit(1)\n </denchmark-code>\n \n <denchmark-code>set_seed(42,True)\n dls = ImageDataLoaders.from_name_func(\n     path, get_image_files(path), valid_pct=0.2, seed=42,\n     label_func=is_cat, item_tfms=Resize(224),shuffle_train= False)\n learn = cnn_learner(dls, resnet34, metrics=error_rate)\n learn.lr_find()\n \n learn.fit(1)\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "muellerzr", "commentT": "2021-01-08T10:43:08Z", "comment_text": "\n \t\tthe issue is still there on latest 2.2.3\n \t\t"}}}, "commit": {"commit_id": "2b058673fd26733bd68b1dc0e0b53e58247278b3", "commit_author": "Jeremy Howard", "commitT": "2020-11-03 13:41:33-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fastai\\callback\\schedule.py", "file_new_name": "fastai\\callback\\schedule.py", "file_complexity": {"file_NLOC": "170", "file_CCN": "46", "file_NToken": "2008"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "198", "deleted_lines": "198", "method_info": {"method_name": "after_fit", "method_params": "self", "method_startline": "194", "method_endline": "199", "method_complexity": {"method_NLOC": "6", "method_CCN": "2", "method_NToken": "50", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fastai\\learner.py", "file_new_name": "fastai\\learner.py", "file_complexity": {"file_NLOC": "485", "file_CCN": "152", "file_NToken": "5287"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "50", "deleted_lines": "50", "method_info": {"method_name": "load_model", "method_params": "file,model,opt,with_opt,device,strict", "method_startline": "41", "method_endline": "54", "method_complexity": {"method_NLOC": "14", "method_CCN": "9", "method_NToken": "130", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "nbs\\13a_learner.ipynb", "file_new_name": "nbs\\13a_learner.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "213", "deleted_lines": "213"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "nbs\\14_callback.schedule.ipynb", "file_new_name": "nbs\\14_callback.schedule.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "612,633,786,787,842,865,866,925,926,1046,1047,1100,1313,1318,1358,1370,1424", "deleted_lines": "612,633,786,787,842,865,866,925,926,1046,1047,1100,1313,1318"}}}}}}