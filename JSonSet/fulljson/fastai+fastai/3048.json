{"BR": {"BR_id": "3048", "BR_author": "marii-moe", "BRopenT": "2020-12-02T09:30:20Z", "BRcloseT": "2020-12-02T14:35:20Z", "BR_text": {"BRsummary": "Gradient Accumulation + Mixed Precision shows artificially high training loss", "BRdescription": "\n Example of issue on forum: <denchmark-link:https://forums.fast.ai/t/what-is-the-v2-equivalent-of-accumulatescheduler/66409/13>https://forums.fast.ai/t/what-is-the-v2-equivalent-of-accumulatescheduler/66409/13</denchmark-link>\n \n Please confirm you have the latest versions of fastai, fastcore, fastscript, and nbdev prior to reporting a bug (delete one): YES\n Describe the bug\n The bug occurs when Gradient Accumulation and the MixedPrecision Callback are both used. Gradient Accumulation run before Mixed Precision and causes the after_backwards to not be run, meaning that the loss is not unscaled before it is logged. This means that very large losses such as 6000000+ to be logged.\n To Reproduce\n Steps to reproduce the behavior:\n Run this code:\n <denchmark-code>seed=random.randint(0,2**32-1)\n \n with no_random(seed): \n     db=synth_dbunch(bs=8,n_train=1,n_valid=1,cuda=True)\n     learn = synth_learner(data=db)\n     learn.fit(1, lr=0.01)\n #start without gradient overflow\n max_loss_scale=2048.0\n with no_random(seed): \n     db=synth_dbunch(bs=1,n_train=8,n_valid=8,cuda=True)\n     learn = synth_learner(data=db,cbs=[GradientAccumulation(n_acc=8)])\n     learn.to_fp16(max_loss_scale=max_loss_scale)\n     learn.fit(1, lr=0.01)\n </denchmark-code>\n \n The training loss will be very high, 5000+ for fp16. fp32 will be reasonable.\n Expected behavior\n Similar training loss between fp32 and fp16 versions. <2 difference in loss.\n Additional context\n I have already gotten a fix, bill be submitting it in pull request soon.\n \t"}, "comments": {}}, "commit": {"commit_id": "d5599538539e8c8cc2bcde5cd378a241c8270232", "commit_author": "Marii", "commitT": "2020-12-02 06:35:20-08:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "fastai\\callback\\fp16.py", "file_new_name": "fastai\\callback\\fp16.py", "file_complexity": {"file_NLOC": "132", "file_CCN": "53", "file_NToken": "1335"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101", "deleted_lines": "98,104", "method_info": {"method_name": "after_backward", "method_params": "self", "method_startline": "97", "method_endline": "115", "method_complexity": {"method_NLOC": "17", "method_CCN": "10", "method_NToken": "149", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "120,121,122", "deleted_lines": "121", "method_info": {"method_name": "after_batch", "method_params": "self", "method_startline": "120", "method_endline": "122", "method_complexity": {"method_NLOC": "2", "method_CCN": "2", "method_NToken": "20", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "nbs\\18_callback.fp16.ipynb", "file_new_name": "nbs\\18_callback.fp16.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "562,566,592,611,612,613,628,698,699,704,705,710,711,759,760,765,766,771,772,834,835,840,841,846,847,893,894,899,900,905,906,1006,1007,1012,1013,1018,1019", "deleted_lines": "562,566,589,595,612,696,697,702,703,708,709,757,758,763,764,769,770,832,833,838,839,844,845,891,892,897,898,903,904,1004,1005,1010,1011,1016,1017"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "nbs\\18a_callback.training.ipynb", "file_new_name": "nbs\\18a_callback.training.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "39,40,41,42,43,44,45,46,47,48,146,216,217,245,246,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,403,404,432,433,438,439,577,578,625,626", "deleted_lines": "39,137,207,208,236,237,293,294,299,300,328,329,334,335,473,474,521,522"}}}}}}