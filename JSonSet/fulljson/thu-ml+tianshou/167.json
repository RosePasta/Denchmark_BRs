{"BR": {"BR_id": "167", "BR_author": "wiljohnhong", "BRopenT": "2020-07-26T13:59:29Z", "BRcloseT": "2020-07-27T07:55:14Z", "BR_text": {"BRsummary": "why the value of `collect_per_step` passed into `n_episode`?", "BRdescription": "\n \n \n \n tianshou/tianshou/trainer/onpolicy.py\n \n \n          Line 87\n       in\n       bd9c3c7\n \n \n \n \n \n \n  result = train_collector.collect(n_episode=collect_per_step, \n \n \n \n \n \n I see that the parameter collect_per_step in onpolicy_trainer() function refers to \"the number of frames the collector would collect before the network update\", but its value is finally passed into the n_episode parameter in collect(), I get quite confused about this. Do I misunderstand anything?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wiljohnhong", "commentT": "2020-07-27T00:38:33Z", "comment_text": "\n \t\tSorry. Indeed it should be \"the number of episodes the collector would collect before the network update\". It's my fault and I'll fix it right now. Thank you for pointing out!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wiljohnhong", "commentT": "2020-07-28T02:27:35Z", "comment_text": "\n \t\tthanks, and I recommend that tianshou can allow us to choose whether to collect a fixed num of episodes or a fixed num of timesteps to collect in collect(), since in some environments an episode may consist of a large num of timesteps and we do not want to train after finishing the whole episode, and in some other cases the length of an episode could vary greatly.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wiljohnhong", "commentT": "2020-07-28T03:06:04Z", "comment_text": "\n \t\t\n I recommend that tianshou can allow us to choose whether to collect a fixed num of episodes or a fixed num of timesteps to collect in collect(), since in some environments an episode may consist of a large num of timesteps and we do not want to train after collect the whole episode, and in some other cases the length of an episode could vary greatly.\n \n If I have time I'll fix it.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wiljohnhong", "commentT": "2020-07-28T08:02:49Z", "comment_text": "\n \t\t\n in some environments an episode may consist of a large num of timesteps and we do not want to train after finishing the whole episode\n \n I think there is an easy workaround without modifying Tianshou: you can truncate the environment to finish after a certain amount of steps. However, in many cases, the reward is very sparse (only available when the episode finishes) and training with partial episodes makes no sense (the rewards are all zero). Therefore, I think only allowing collecting full episodes works in most cases. If one indeed needs to train with partial episodes, they can wrap the environment to finish after a certain amount of steps.\n \t\t"}}}, "commit": {"commit_id": "b7a4015db71b7014cfddeca31eb40829ee4646a9", "commit_author": "Trinkle23897", "commitT": "2020-07-27 16:54:14+08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\pong_a2c.py", "file_new_name": "examples\\pong_a2c.py", "file_complexity": {"file_NLOC": "86", "file_CCN": "10", "file_NToken": "837"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "26", "deleted_lines": "26", "method_info": {"method_name": "get_args", "method_params": "", "method_startline": "17", "method_endline": "44", "method_complexity": {"method_NLOC": "26", "method_CCN": "2", "method_NToken": "322", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\pong_ppo.py", "file_new_name": "examples\\pong_ppo.py", "file_complexity": {"file_NLOC": "91", "file_CCN": "10", "file_NToken": "865"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "26", "deleted_lines": "26", "method_info": {"method_name": "get_args", "method_params": "", "method_startline": "17", "method_endline": "44", "method_complexity": {"method_NLOC": "27", "method_CCN": "2", "method_NToken": "340", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tianshou\\data\\collector.py", "file_new_name": "tianshou\\data\\collector.py", "file_complexity": {"file_NLOC": "320", "file_CCN": "21", "file_NToken": "1780"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "262,265", "deleted_lines": "261,265,266,267"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tianshou\\policy\\base.py", "file_new_name": "tianshou\\policy\\base.py", "file_complexity": {"file_NLOC": "206", "file_CCN": "7", "file_NToken": "645"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "89,90", "deleted_lines": "89,90,91"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tianshou\\trainer\\onpolicy.py", "file_new_name": "tianshou\\trainer\\onpolicy.py", "file_complexity": {"file_NLOC": "138", "file_CCN": "1", "file_NToken": "676"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "43,44,45", "deleted_lines": "43,44,45"}}}}}}