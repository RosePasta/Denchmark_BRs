{"BR": {"BR_id": "785", "BR_author": "busycalibrating", "BRopenT": "2020-11-17T08:49:22Z", "BRcloseT": "2020-11-22T09:54:34Z", "BR_text": {"BRsummary": "RandomAffine with 2 values for scale specified", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug:  RandomAffine and scale</denchmark-h>\n \n I'm getting the following error when using RandomAffine initialized with a tuple of two values for scale:\n <denchmark-code>TypeError: scale_y should be a float number or a tuple with length 2 whose values between (-inf, inf).Got tensor([]).\n </denchmark-code>\n \n I haven't quite ironed down the cause of the bug, but I think this happens when exactly 4 entries of a batch are selected to have the scaling operation applied to them. Then, we hit <denchmark-link:https://github.com/kornia/kornia/blob/0b599d4b372c5d3dd72d7be0855dd7987b4171fd/kornia/augmentation/random_generator/random_generator.py#L190>this line</denchmark-link>\n  and enter the  function, which then fails because I only specified a tuple of two values for scale. I can confirm I don't run into this issue when I specify 4 values for .\n This looks related to <denchmark-link:https://github.com/kornia/kornia/pull/714>#714</denchmark-link>\n  but I'm running Kornia 0.4.1 which includes this fix so it may still be relevant?\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n Essentially, I instantiated a RandomAffine class\n # The only transformation I'm applying to my image prior to stuffing them into the kornia 'function'\n transform = transforms.Compose([transforms.ToTensor()])\n \n transform_fcn = torch.nn.Sequential(\n     K.augmentation.RandomAffine(degrees=(-45., 45.), \n                                 scale=(0.8, 1.4), \n                                 shear=(0., 0.15), \n                                 return_transform=False, p=1),   # for sake of argument\n     K.augmentation.RandomHorizontalFlip(),\n     K.augmentation.Normalize(mean=0.5, std=0.5)\n )\n To emphasis this issue, I instantiated a dataloader with a batch size of 4 and then tried to pass this through the augmentation:\n <denchmark-code>loader = DataLoader(train_ds, batch_size=4)\n x = next(iter(loader))\n out = transform_fcn(x)\n </denchmark-code>\n \n This resulted in the stack trace below.\n <denchmark-code>---------------------------------------------------------------------------\n TypeError                                 Traceback (most recent call last)\n <ipython-input-30-cf0a915755df> in <module>\n ----> 1 out = transform_fcn(x[0])\n \n ~/default-env/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n     725             result = self._slow_forward(*input, **kwargs)\n     726         else:\n --> 727             result = self.forward(*input, **kwargs)\n     728         for hook in itertools.chain(\n     729                 _global_forward_hooks.values(),\n \n ~/default-env/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input)\n     115     def forward(self, input):\n     116         for module in self:\n --> 117             input = module(input)\n     118         return input\n     119 \n \n ~/default-env/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n     725             result = self._slow_forward(*input, **kwargs)\n     726         else:\n --> 727             result = self.forward(*input, **kwargs)\n     728         for hook in itertools.chain(\n     729                 _global_forward_hooks.values(),\n \n ~/default-env/lib/python3.8/site-packages/kornia/augmentation/base.py in forward(self, input, params, return_transform)\n     196             return_transform = self.return_transform\n     197         if params is None:\n --> 198             params = self.__forward_parameters__(batch_shape, self.p, self.p_batch, self.same_on_batch)\n     199         if 'batch_prob' not in params:\n     200             params['batch_prob'] = torch.tensor([True] * batch_shape[0])\n \n ~/default-env/lib/python3.8/site-packages/kornia/augmentation/base.py in __forward_parameters__(self, batch_shape, p, p_batch, same_on_batch)\n      92             batch_prob = batch_prob.repeat(batch_shape[0])\n      93         # selectively param gen\n ---> 94         return self.__selective_param_gen__(batch_shape, batch_prob)\n      95 \n      96     def apply_func(self, input: torch.Tensor, params: Dict[str, torch.Tensor],\n \n ~/default-env/lib/python3.8/site-packages/kornia/augmentation/base.py in __selective_param_gen__(self, batch_shape, to_apply)\n      63     def __selective_param_gen__(\n      64             self, batch_shape: torch.Size, to_apply: torch.Tensor) -> Dict[str, torch.Tensor]:\n ---> 65         _params = self.generate_parameters(\n      66             torch.Size((int(to_apply.sum().item()), *batch_shape[1:])))\n      67         if _params is None:\n \n ~/default-env/lib/python3.8/site-packages/kornia/augmentation/augmentation.py in generate_parameters(self, batch_shape)\n     483 \n     484     def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n --> 485         return rg.random_affine_generator(\n     486             batch_shape[0], batch_shape[-2], batch_shape[-1], self.degrees, self.translate, self.scale, self.shear,\n     487             self.same_on_batch)\n \n ~/default-env/lib/python3.8/site-packages/kornia/augmentation/random_generator/random_generator.py in random_affine_generator(batch_size, height, width, degrees, translate, scale, shear, same_on_batch)\n     173         _scale = _adapted_uniform((batch_size,), scale[0], scale[1], same_on_batch).unsqueeze(1).repeat(1, 2)\n     174         if len(_scale) == 4:\n --> 175             _joint_range_check(cast(torch.Tensor, scale[2:]), \"scale_y\")\n     176             _scale[:, 1] = _adapted_uniform((batch_size,), scale[2], scale[3], same_on_batch)\n     177     else:\n \n ~/default-env/lib/python3.8/site-packages/kornia/augmentation/utils/param_validation.py in _joint_range_check(ranged_factor, name, bounds)\n      45             raise ValueError(f\"{name}[0] should be smaller than {name}[1] got {ranged_factor}\")\n      46     else:\n ---> 47         raise TypeError(\n      48             f\"{name} should be a float number or a tuple with length 2 whose values between {bounds}.\"\n      49             f\"Got {ranged_factor}.\")\n \n TypeError: scale_y should be a float number or a tuple with length 2 whose values between (-inf, inf).Got tensor([]).\n </denchmark-code>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n \n Ubuntu 20.04\n Tried PyTorch 1.6.0 and 1.7.0 (via pip)\n Kornia 0.4.1\n Python 3.8\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "busycalibrating", "commentT": "2020-11-17T09:00:17Z", "comment_text": "\n \t\tCan you try master instead? I do not have this issue on my local machine.\n Seems like it has been fixed in <denchmark-link:https://github.com/kornia/kornia/pull/757>#757</denchmark-link>\n .\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "busycalibrating", "commentT": "2020-11-17T09:04:29Z", "comment_text": "\n \t\tMaster has the same issue\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "busycalibrating", "commentT": "2020-11-17T10:56:45Z", "comment_text": "\n \t\tI have tried on Colab. It should be fine.\n ! pip install --force-reinstall git+https://github.com/kornia/kornia.git\n \n import kornia as K\n import torch\n x = K.augmentation.RandomAffine(\n     degrees=(-45., 45.), scale=(0.8, 1.4), shear=(0., 0.15), return_transform=False, p=1)   # for sake of argument\n x(torch.rand(5, 5))\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "busycalibrating", "commentT": "2020-11-17T16:57:39Z", "comment_text": "\n \t\tTry this:\n ! pip install --force-reinstall git+https://github.com/kornia/kornia.git\n \n import kornia as K\n import torch\n \n x = K.augmentation.RandomAffine(\n     degrees=(-45., 45.), \n     scale=(0.8, 1.4), \n     shear=(0., 0.15), \n     return_transform=False, p=1)   # for sake of argument\n \n x(torch.rand(4, 1, 5, 5))\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "busycalibrating", "commentT": "2020-11-17T17:37:05Z", "comment_text": "\n \t\tI see the problem. It happens only when batchsize == 4 due to a typo. Fixed in <denchmark-link:https://github.com/kornia/kornia/pull/786>#786</denchmark-link>\n .\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "busycalibrating", "commentT": "2020-11-17T17:41:36Z", "comment_text": "\n \t\tAwesome, thanks for the quick fix!\n \t\t"}}}, "commit": {"commit_id": "fdfff5572927e1928aa11648655b1a4996d8de84", "commit_author": "shijianjian", "commitT": "2020-11-22 17:54:33+08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "kornia\\augmentation\\random_generator\\random_generator.py", "file_new_name": "kornia\\augmentation\\random_generator\\random_generator.py", "file_complexity": {"file_NLOC": "732", "file_CCN": "15", "file_NToken": "4704"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "190", "deleted_lines": "190"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "test\\augmentation\\test_random_generator.py", "file_new_name": "test\\augmentation\\test_random_generator.py", "file_complexity": {"file_NLOC": "952", "file_CCN": "74", "file_NToken": "13888"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "277", "deleted_lines": "277"}}}}}}