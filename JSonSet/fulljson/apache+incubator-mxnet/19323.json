{"BR": {"BR_id": "19323", "BR_author": "zjost", "BRopenT": "2020-10-09T20:37:48Z", "BRcloseT": "2020-11-06T19:07:13Z", "BR_text": {"BRsummary": "Variable sequence length not handled correctly for BiDirectional layers", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n There are a couple of different issues related to the use of  in the <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/rnn/rnn_layer.py#L40>_RNNLayer</denchmark-link>\n .\n \n It doesn't seem to be usable by GRU and RNN, but only LSTM.  This is more thoroughly documented here\n For LSTM, it only seems to work properly when using GPU.  When using CPU, padding additional elements impacts the output despite passing in the same sequence length.  This means the output of a particular sequence would change depending on the maximum sequence length of the batch.\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n <denchmark-code>#ctx = [mx.cpu()]\n ctx = [mx.gpu(0)]\n class TestModel(gluon.nn.HybridBlock):\n     def __init__(self, bidirectional=True):\n         super(TestModel, self).__init__(prefix=\"TestModel_\")\n         with self.name_scope():\n             self.rnn = gluon.rnn.LSTM(hidden_size=1, bidirectional=bidirectional, use_sequence_length=True)\n     \n     def hybrid_forward(self, F, x, x_len):\n         x = x.expand_dims(2) # add a feature dimension\n         x = x.transpose((1, 0, 2)) # to make in (max_sequence_length, batch_size, other_feature_dims)\n         out = self.rnn(x, sequence_length=x_len)\n         out = F.SequenceLast(out, sequence_length=x_len, use_sequence_length=True)\n         return out\n \n net = TestModel(bidirectional=True)\n net.initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n \n pad_val = -1\n example_codes = [[1,2], [1,pad_val]]\n example_len = [2,1]\n x_input = mx.nd.array(example_codes).as_in_context(ctx[0])\n x_len_input = mx.nd.array(example_len).as_in_context(ctx[0])\n mx.random.seed(0)\n \n # Original\n out1 = net(x_input, x_len_input)\n \n # Extra padding on first token\n x_input2 = mx.nd.array([k+[pad_val] for k in example_codes]).as_in_context(ctx[0])\n out2 = net(x_input2, x_len_input)\n \n # Note:  out1 != out2 when ctx = CPU for the backward cell\n </denchmark-code>\n \n <denchmark-h:h3>Steps to reproduce</denchmark-h>\n \n Run the above code with CPU and GPU context and observe the output of the second column (i.e. from the backward LSTM cell).\n <denchmark-h:h2>What have you tried to solve it?</denchmark-h>\n \n I've found that if you use x = F.SequenceMask(x, sequence_length=x_len, use_sequence_length=True) before passing to the RNN, the outputs match.  This might suggest that the CPU implementation reverses the entire padded sequence for the backward LSTM cell, rather than just reversing the first x_len elements.\n Note:  I suspect #14208 is relevant given that the intended behavior works only for GPU/LSTM\n <denchmark-h:h2>Environment</denchmark-h>\n \n \n Environment Information\n ----------Python Info----------\n Version      : 3.6.5\n Compiler     : GCC 7.2.0\n Build        : ('default', 'Apr 29 2018 16:14:56')\n Arch         : ('64bit', '')\n ------------Pip Info-----------\n Version      : 10.0.1\n Directory    : /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/pip\n ----------MXNet Info-----------\n Version      : 1.6.0\n Directory    : /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet\n Commit Hash   : 6eec9da55c5096079355d1f1a5fa58dcf35d6752\n Library      : ['/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/libmxnet.so']\n Build features:\n \u2714 CUDA\n \u2714 CUDNN\n \u2714 NCCL\n \u2714 CUDA_RTC\n \u2716 TENSORRT\n \u2714 CPU_SSE\n \u2714 CPU_SSE2\n \u2714 CPU_SSE3\n \u2714 CPU_SSE4_1\n \u2714 CPU_SSE4_2\n \u2716 CPU_SSE4A\n \u2714 CPU_AVX\n \u2716 CPU_AVX2\n \u2714 OPENMP\n \u2716 SSE\n \u2714 F16C\n \u2716 JEMALLOC\n \u2714 BLAS_OPEN\n \u2716 BLAS_ATLAS\n \u2716 BLAS_MKL\n \u2716 BLAS_APPLE\n \u2714 LAPACK\n \u2714 MKLDNN\n \u2714 OPENCV\n \u2716 CAFFE\n \u2716 PROFILER\n \u2714 DIST_KVSTORE\n \u2716 CXX14\n \u2716 INT64_TENSOR_SIZE\n \u2714 SIGNAL_HANDLER\n \u2716 DEBUG\n \u2716 TVM_OP\n ----------System Info----------\n Platform     : Linux-4.14.198-152.320.amzn2.x86_64-x86_64-with-glibc2.9\n system       : Linux\n node         : ip-172-31-46-69.us-west-2.compute.internal\n release      : 4.14.198-152.320.amzn2.x86_64\n version      : #1 SMP Wed Sep 23 23:57:28 UTC 2020\n ----------Hardware Info----------\n machine      : x86_64\n processor    : x86_64\n Architecture:        x86_64\n CPU op-mode(s):      32-bit, 64-bit\n Byte Order:          Little Endian\n CPU(s):              32\n On-line CPU(s) list: 0-31\n Thread(s) per core:  2\n Core(s) per socket:  16\n Socket(s):           1\n NUMA node(s):        1\n Vendor ID:           GenuineIntel\n CPU family:          6\n Model:               79\n Model name:          Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n Stepping:            1\n CPU MHz:             2700.082\n CPU max MHz:         3000.0000\n CPU min MHz:         1200.0000\n BogoMIPS:            4600.04\n Hypervisor vendor:   Xen\n Virtualization type: full\n L1d cache:           32K\n L1i cache:           32K\n L2 cache:            256K\n L3 cache:            46080K\n NUMA node0 CPU(s):   0-31\n Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt\n ----------Network Test----------\n Setting timeout: 10\n Timing for MXNet: https://github.com/apache/incubator-mxnet, DNS: 0.0028 sec, LOAD: 0.6319 sec.\n Timing for Gluon Tutorial(en): http://gluon.mxnet.io, DNS: 0.1442 sec, LOAD: 0.0671 sec.\n Error open Gluon Tutorial(cn): https://zh.gluon.ai, <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)>, DNS finished in 0.07044124603271484 sec.\n Timing for FashionMNIST: https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz, DNS: 0.0106 sec, LOAD: 0.1394 sec.\n Timing for PYPI: https://pypi.python.org/pypi/pip, DNS: 0.0031 sec, LOAD: 0.3234 sec.\n Error open Conda: https://repo.continuum.io/pkgs/free/, HTTP Error 403: Forbidden, DNS finished in 0.001977682113647461 sec.\n ----------Environment----------\n KMP_DUPLICATE_LIB_OK=\"True\"\n KMP_INIT_AT_FORK=\"FALSE\"\n KMP_AFFINITY=\"granularity=fine,compact,1,0\"\n OMP_NUM_THREADS=\"16\"\n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "zjost", "commentT": "2020-10-09T20:38:42Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/anko-intel>@anko-intel</denchmark-link>\n  <denchmark-link:https://github.com/TaoLv>@TaoLv</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "zjost", "commentT": "2020-10-14T10:54:38Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/zjost>@zjost</denchmark-link>\n  currently CPU does not support  in RNN layers. It seems that this information is missing in MKLDNN execution path but when you run this code with  environment variable you will get the following error: .\n Your solution to use F.SequenceMask(x, sequence_length=x_len, use_sequence_length=True) is equivalent to setting pad_val = 0 instead of -1. However, it's not proper solution and it happened to work by accident. Padding with 0s yields correct result for bidirectional RNN layers only if all biases are equal to 0 which is the case here (default initializer for bias is zero). You can check it by changing LSTM layer initialization in your model to:\n <denchmark-code>self.rnn = gluon.rnn.LSTM(hidden_size=1, bidirectional=bidirectional, input_size=1, use_sequence_length=True,\n                           h2h_bias_initializer='one', i2h_bias_initializer='one')\n </denchmark-code>\n \n For now, my suggestion would be to either use batch_size=1 or group sentences into batches of equal length.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "zjost", "commentT": "2020-10-14T16:34:59Z", "comment_text": "\n \t\tTo be clear, this is a problem for us because we use SageMaker and it makes it such that the same record gets different scores when it's invoked via the endpoint as a single record vs in a Batch Transform job.  Running Batch Transform 1 record at a time takes way too long and we can't control how SageMaker splits the batches.\n Also, any comment on point 1, about how this only seems to work with LSTM, not RNN/GRU, even when using correct cuDNN?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "zjost", "commentT": "2020-10-15T09:00:55Z", "comment_text": "\n \t\tRegarding point 1. it seems that for now only LSTM supports  parameter. It was introduced by this PR: <denchmark-link:https://github.com/apache/incubator-mxnet/pull/14208>#14208</denchmark-link>\n  and it causes some argument order problem when using other RNN types. However, I'm not sure if this is bug or just lack of implementation on GPU side because I'm only familiar with CPU code and as I said before,  is not supported here.\n I understand your concerns about performance of using batches of 1. If you want to get correct results of bidirectional RNN layers while running bigger batches you can create BidirectionalRNN layer by yourself with two RNN layers and concat after. Example of such layer:\n ctx = [mx.cpu()]\n \n class CustomBidirectionalRNNLayer(gluon.nn.HybridBlock):\n     def __init__(self, hidden_size):\n         super(CustomBidirectionalRNNLayer, self).__init__(prefix=\"bidir_rnn_\")\n         with self.name_scope():\n             self.rnn_l2r = gluon.rnn.LSTM(hidden_size=hidden_size, bidirectional=False, prefix='l2r',\n                                          h2h_bias_initializer='one', i2h_bias_initializer='one')\n             self.rnn_r2l = gluon.rnn.LSTM(hidden_size=hidden_size, bidirectional=False, prefix='r2l',\n                                          h2h_bias_initializer='one', i2h_bias_initializer='one')\n     \n     def hybrid_forward(self, F, x, x_len):\n         l2r_out = self.rnn_l2r(x)\n         r2l_out = self.rnn_r2l(F.SequenceReverse(x, sequence_length=x_len, use_sequence_length=True))\n         out = F.concat(l2r_out, r2l_out, dim=2)\n         return out\n \n     \n class TestModel(gluon.nn.HybridBlock):\n     def __init__(self):\n         super(TestModel, self).__init__(prefix=\"TestModel_\")\n         with self.name_scope():\n             self.bidir_rnn = CustomBidirectionalRNNLayer(hidden_size=1)\n             \n             \n     def hybrid_forward(self, F, x, x_len):\n         x = x.expand_dims(2) # add a feature dimension\n         x = x.transpose((1, 0, 2)) # to make in (max_sequence_length, batch_size, other_feature_dims)\n         out = self.bidir_rnn(x, x_len)\n         out = F.SequenceLast(out, sequence_length=x_len, use_sequence_length=True)\n         return out\n     \n net = TestModel()\n net.initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n \n pad_val = 0\n example_codes = [[1,2], [1,pad_val]]\n example_len = [2,1]\n x_input = mx.nd.array(example_codes).as_in_context(ctx[0])\n x_len_input = mx.nd.array(example_len).as_in_context(ctx[0])\n mx.random.seed(0)\n \n # Original\n out1 = net(x_input, x_len_input)\n \n # Extra padding on first token\n x_input2 = mx.nd.array([k+[pad_val] for k in example_codes]).as_in_context(ctx[0])\n out2 = net(x_input2, x_len_input)\n This solution also solves point 1. because use_sequence_length is used only in F.SequenceReverse/F.SequenceLast and not in RNN operator so it doesn't give any error. Let me know if you are fine with such workaround.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "zjost", "commentT": "2020-10-15T15:57:42Z", "comment_text": "\n \t\tThanks for taking the time to show an implementation of this.\n Do you think a warning should be added to the documentation regarding the use of bidirectional?  I don't think it's clear that this will not have intended behavior unless sequences are of equal length in a batch.  Particularly so because if you start tracing down the base classes, it seems as though e.g. GRU will pass along the use_sequence_length kwarg.\n Regarding <denchmark-link:https://github.com/apache/incubator-mxnet/pull/14208>#14208</denchmark-link>\n , I'm not sure why this fails for other RNN types since the code changes appear to be primarily to the  base class rather than the LSTMCell.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "zjost", "commentT": "2020-10-16T08:41:58Z", "comment_text": "\n \t\tAbsolutely there should be some note in the documentation about use_sequence_length working only on GPU. For bidirectional=False it does not harm users but using it with bidirectional will lead to incorrect results. Also there should be a mention that it works only for LSTM right now.\n I will try to explain you the problem why changing _RNN base class works for LSTM and not for GRU and RNN. The problem lies on the graph framework level (NNVM). Here the arguments of RNN are registered:\n \n \n \n incubator-mxnet/src/operator/rnn.cc\n \n \n         Lines 414 to 418\n       in\n       d0ceecb\n \n \n \n \n \n \n  .add_argument(\"state_cell\", \"NDArray-or-Symbol\", \n \n \n \n  \"initial cell state for LSTM networks (only for LSTM)\") \n \n \n \n  .add_argument(\"sequence_length\", \"NDArray-or-Symbol\", \n \n \n \n  \"Vector of valid sequence lengths for each element in batch. (Only used if\" \n \n \n \n  \" use_sequence_length kwarg is True)\") \n \n \n \n \n \n As you can see these 2 arguments are optional where cell_state exists only in LSTM and sequence_length is present where user set use_sentence_length to True. Listing of input arguments is defined by:\n \n \n \n incubator-mxnet/src/operator/rnn.cc\n \n \n         Lines 37 to 53\n       in\n       d0ceecb\n \n \n \n \n \n \n  DMLC_REGISTER_PARAMETER(RNNParam); \n \n \n \n  static inline std::vector<std::string> ListArguments(const RNNParam& param_) { \n \n \n \n  // All RNNs start off with same 3 input arguments \n \n \n \n    std::vector<std::string> arguments{\"data\", \"parameters\", \"state\"}; \n \n \n \n  \n \n \n \n  // LSTMs also have an additional state_cell argument \n \n \n \n  if (param_.mode == rnn_enum::kLstm) { \n \n \n \n      arguments.emplace_back(\"state_cell\"); \n \n \n \n    } \n \n \n \n  \n \n \n \n  // All RNNs have option of additional sequence_length argument \n \n \n \n  if (param_.use_sequence_length) { \n \n \n \n      arguments.emplace_back(\"sequence_length\"); \n \n \n \n    } \n \n \n \n  \n \n \n \n  return arguments; \n \n \n \n  } \n \n \n \n \n \n as well as in the enum:\n \n \n \n incubator-mxnet/src/operator/rnn-inl.h\n \n \n         Lines 55 to 56\n       in\n       d0ceecb\n \n \n \n \n \n \n  namespace rnn_enum { \n \n \n \n  enum RNNOpInputs {kData, kParams, kState, kStateCell, kSequenceLength}; \n \n \n \n \n \n Problem here is that conditional argument cannot exist without the previous ones being available. So this way if we use sequence_length it expects the previous one too (which is cell_state that exists only in LSTM). We can change ordering of these 2 arguments in the code. This way RNN/GRU won't crash while used with use_sequence_length but LSTM will crash when not used with use_sequence_length because it needs cell_state that is after. I'm not familiar with any workaround for that but I don't think we should focus on that now since GRU/RNN doesn't have any kernel with use_sequence_length anyway.\n BTW sorry, I've made a little mistake with my CustomBidirectionalLayer in my previous post. To make it work correctly you have to reverse back r2l output before concat:\n class CustomBidirectionalRNNLayer(gluon.nn.HybridBlock):\n     def __init__(self, hidden_size):\n         super(CustomBidirectionalRNNLayer, self).__init__(prefix=\"bidir_rnn_\")\n         with self.name_scope():\n             self.rnn_l2r = gluon.rnn.LSTM(hidden_size=hidden_size, bidirectional=False, prefix='l2r',\n                                          h2h_bias_initializer='one', i2h_bias_initializer='one')\n             self.rnn_r2l = gluon.rnn.LSTM(hidden_size=hidden_size, bidirectional=False, prefix='r2l',\n                                          h2h_bias_initializer='one', i2h_bias_initializer='one')\n     \n     def hybrid_forward(self, F, x, x_len):\n         l2r_out = self.rnn_l2r(x)\n         r2l_out = self.rnn_r2l(F.SequenceReverse(x, sequence_length=x_len, use_sequence_length=True))\n         out = F.concat(l2r_out, F.SequenceReverse(r2l_out, sequence_length=x_len, use_sequence_length=True), dim=2)\n         return out\n For now I'll prepare a PR so the code fails when run with use_sequence_length on CPU so users will be notified.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "zjost", "commentT": "2020-11-06T10:02:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/leezu>@leezu</denchmark-link>\n  I think we can close it since <denchmark-link:https://github.com/apache/incubator-mxnet/pull/19466>#19466</denchmark-link>\n  has been merged\n \t\t"}}}, "commit": {"commit_id": "087f6ff8bdc7335ea7b27b057d37efd9fac12b61", "commit_author": "Adam", "commitT": "2020-11-04 14:42:34-08:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.6666666666666666"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\nn\\mkldnn\\mkldnn_rnn-inl.h", "file_new_name": "src\\operator\\nn\\mkldnn\\mkldnn_rnn-inl.h", "file_complexity": {"file_NLOC": "322", "file_CCN": "41", "file_NToken": "2305"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "476,477,478,479", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::SupportMKLDNNRnn", "method_params": "param,input_dtype", "method_startline": "476", "method_endline": "479", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "28", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\operator\\rnn.cc", "file_new_name": "src\\operator\\rnn.cc", "file_complexity": {"file_NLOC": "366", "file_CCN": "44", "file_NToken": "2400"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "200,201,202", "deleted_lines": "200", "method_info": {"method_name": "mxnet::op::RNNStorageType", "method_params": "attrs,dev_mask,dispatch_mode,in_attrs,out_attrs", "method_startline": "195", "method_endline": "205", "method_complexity": {"method_NLOC": "11", "method_CCN": "2", "method_NToken": "88", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "249", "deleted_lines": "247", "method_info": {"method_name": "mxnet::op::CreateRNNState", "method_params": "attrs,ctx,in_shapes,in_types", "method_startline": "232", "method_endline": "267", "method_complexity": {"method_NLOC": "32", "method_CCN": "7", "method_NToken": "253", "method_nesting_level": "2"}}}}}}}}