{"BR": {"BR_id": "19022", "BR_author": "Zha0q1", "BRopenT": "2020-08-26T23:11:01Z", "BRcloseT": "2020-11-09T10:35:53Z", "BR_text": {"BRsummary": "MKLDNN numpy rnn core dump", "BRdescription": "\n With the following script mxnet will core dump on y.backward(). My build is master with cuda off mkldnn on. I tried to build mkldnn off and the script wouldn't core dump then.\n def test_rnn():\n     INT_OVERFLOW = 2**10\n     def batch_check(x, modes, params):\n         for m, p in zip(modes, params):\n             state = np.random.normal(0, 1, (1, 4, 1))\n             x.attach_grad()\n             state.attach_grad()\n             x.attach_grad()\n             p.attach_grad()\n \n             with mx.autograd.record():\n                 y = npx.rnn(data=x, parameters=p, mode=m, \\\n                     state=state, state_size=1, num_layers=1)\n             assert y.shape == (INT_OVERFLOW, 4, 1)\n             assert type(y[0]).__name__ == 'ndarray'\n             y.backward()\n             print(state.grad)\n     data = np.random.normal(0, 1, (INT_OVERFLOW, 4, 4))\n     modes = ['rnn_relu', 'rnn_tanh', 'gru']\n     params = [np.random.normal(0, 1, (7,)), \\\n         np.random.normal(0, 1, (7,)), \\\n         np.random.normal(0, 1, (21,))]\n     batch_check(data, modes, params)               \n This will trigger two possible error messages:\n Sometimes it's:\n <denchmark-code>ubuntu@ip-172-31-38-169:~/incubator-mxnet$ python rnn.py \n [22:40:24] ../src/storage/storage.cc:198: Using Pooled (Naive) StorageManager for CPU\n corrupted size vs. prev_size\n Aborted (core dumped)\n </denchmark-code>\n \n Other times:\n <denchmark-code>ubuntu@ip-172-31-38-169:~/incubator-mxnet$ python rnn.py \n [21:57:52] ../src/storage/storage.cc:198: Using Pooled (Naive) StorageManager for CPU\n malloc_consolidate(): invalid chunk size\n Aborted (core dumped)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Zha0q1", "commentT": "2020-08-26T23:11:42Z", "comment_text": "\n \t\tGDB backtrace:\n when the error is\n <denchmark-code>ubuntu@ip-172-31-38-169:~/incubator-mxnet$ python rnn.py \n [22:40:24] ../src/storage/storage.cc:198: Using Pooled (Naive) StorageManager for CPU\n corrupted size vs. prev_size\n Aborted (core dumped)\n </denchmark-code>\n \n <denchmark-code>corrupted size vs. prev_size\n \n Thread 21 \"python\" received signal SIGABRT, Aborted.\n [Switching to Thread 0x7fff692dd700 (LWP 78491)]\n __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\n 51\t../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\n (gdb) bt\n #0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\n #1  0x00007ffff78058b1 in __GI_abort () at abort.c:79\n #2  0x00007ffff784e907 in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7ffff797bdfa \"%s\\n\")\n     at ../sysdeps/posix/libc_fatal.c:181\n #3  0x00007ffff785597a in malloc_printerr (str=str@entry=0x7ffff7979efd \"corrupted size vs. prev_size\")\n     at malloc.c:5350\n #4  0x00007ffff7855b7c in malloc_consolidate (av=av@entry=0x7fff4c000020) at malloc.c:4456\n #5  0x00007ffff7859848 in _int_malloc (av=av@entry=0x7fff4c000020, bytes=bytes@entry=5120) at malloc.c:3703\n #6  0x00007ffff785a55b in _int_memalign (av=0x7fff4c000020, alignment=64, bytes=<optimized out>)\n     at malloc.c:4694\n #7  0x00007ffff786002a in _mid_memalign (address=<optimized out>, bytes=5016, alignment=<optimized out>)\n     at malloc.c:3314\n #8  __posix_memalign (memptr=0x7fff692da9c0, alignment=<optimized out>, size=5016) at malloc.c:5369\n #9  0x00007fffe64adad2 in dnnl::impl::malloc (size=5016, alignment=64)\n     at ../3rdparty/mkldnn/src/common/utils.cpp:99\n #10 0x00007fffe63e4ff8 in dnnl::impl::c_compatible::operator new (sz=5016)\n     at ../3rdparty/mkldnn/src/common/nstl.hpp:40\n #11 0x00007fffe69cd594 in dnnl::impl::cpu::jit_uni_reorder_t::pd_t::create (reorder_pd=0x7fff692daef8, \n     engine=0x555557207880, attr=0x7fff4c086fc0, src_engine=0x555557207880, src_md=0x7fff692daf20, \n     dst_engine=0x555557207880, dst_md=0x7fff692db1e0) at ../3rdparty/mkldnn/src/cpu/jit_uni_reorder.cpp:1086\n #12 0x00007fffe69c5e76 in dnnl::impl::cpu::jit_uni_reorder_create (reorder_pd=0x7fff692daef8, \n     engine=0x555557207880, attr=0x7fff4c086fc0, src_engine=0x555557207880, src_md=0x7fff692daf20, \n     dst_engine=0x555557207880, dst_md=0x7fff692db1e0) at ../3rdparty/mkldnn/src/cpu/jit_uni_reorder.cpp:1247\n #13 0x00007fffe64a1e8f in dnnl_reorder_primitive_desc_create (reorder_pd=0x7fff692daef8, \n ---Type <return> to continue, or q <return> to quit---\n     src_md=0x7fff692daf20, src_engine=0x555557207880, dst_md=0x7fff692db1e0, dst_engine=0x555557207880, \n     attr=0x7fff4c086fc0) at ../3rdparty/mkldnn/src/common/reorder.cpp:73\n #14 0x00007fffdb0a263c in dnnl::reorder::primitive_desc::primitive_desc (this=0x7fff692db4e0, src=..., \n     dst=..., attr=...) at ../3rdparty/mkldnn/include/dnnl.hpp:3166\n #15 0x00007fffdb0a274e in dnnl::reorder::reorder (this=0x7fff692db540, src=..., dst=..., attr=...)\n     at ../3rdparty/mkldnn/include/dnnl.hpp:3217\n #16 0x00007fffdbfe3213 in mxnet::op::MKLDNNMemoryReorder (src=..., dst=...)\n     at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:399\n #17 0x00007fffdbfda681 in mxnet::op::MKLDNNRnnBackward::SetNativeWeightsGrads (this=0x7fff4c077390)\n     at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:867\n #18 0x00007fffdbfe34f3 in mxnet::op::RegisterMKLDNNRnn<mxnet::op::MKLDNNRnnBackward> (rnn=...)\n     at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:991\n \n #19 0x00007fffdbfde69f in mxnet::op::MKLDNNRnnOp::Backward (this=0x55555731fbf0, ctx=..., \n     inputs=std::vector of length 5, capacity 5 = {...}, req=std::vector of length 3, capacity 3 = {...}, \n     outputs=std::vector of length 3, capacity 3 = {...}) at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:1203\n #20 0x00007fffe31da173 in mxnet::op::RNNStatefulGradComputeExCPU (state_ptr=..., ctx=..., \n     inputs=std::vector of length 5, capacity 5 = {...}, req=std::vector of length 3, capacity 3 = {...}, \n     outputs=std::vector of length 3, capacity 3 = {...}) at ../src/operator/rnn.cc:284\n #21 0x00007fffdac7f26b in std::_Function_handler<void (mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&), void (*)(mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)>::_M_invoke(std::_Any_data const&, mxnet::OpStatePtr const&, mxnet::OpC---Type <return> to continue, or q <return> to quit---\n ontext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&) (__functor=..., __args#0=..., __args#1=..., __args#2=std::vector of length 5, capacity 5 = {...}, \n     __args#3=std::vector of length 3, capacity 3 = {...}, \n     __args#4=std::vector of length 3, capacity 3 = {...}) at /usr/include/c++/7/bits/std_function.h:316\n #22 0x00007fffdadcd088 in std::function<void (mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)>::operator()(mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&) const (this=0x555557295ce8, __args#0=..., __args#1=..., \n     __args#2=std::vector of length 5, capacity 5 = {...}, \n     __args#3=std::vector of length 3, capacity 3 = {...}, \n     __args#4=std::vector of length 3, capacity 3 = {...}) at /usr/include/c++/7/bits/std_function.h:706\n #23 0x00007fffdae42905 in mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (__closure=0x555557295be0, rctx=..., on_complete=...)\n     at ../src/imperative/./imperative_utils.h:758\n #24 0x00007fffdae42b02 in mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::Nod---Type <return> to continue, or q <return> to quit---\n eAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#2}::operator()(mxnet::RunContext) const (\n     __closure=0x555557295be0, rctx=...) at ../src/imperative/./imperative_utils.h:772\n #25 0x00007fffdae48f9e in std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#2}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&) (__functor=..., __args#0=...)\n     at /usr/include/c++/7/bits/std_function.h:316\n #26 0x00007fffdad9f5a2 in std::function<void (mxnet::RunContext)>::operator()(mxnet::RunContext) const (\n     this=0x555557194200, __args#0=...) at /usr/include/c++/7/bits/std_function.h:706\n #27 0x00007fffdada8260 in mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (\n     __closure=0x5555572dab50, ctx=..., on_complete=...) at ../src/engine/./threaded_engine.h:537\n #28 0x00007fffdadac835 in std::_Function_handler<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&, mxnet::engine::CallbackOnComplete&&) (__functor=..., \n ---Type <return> to continue, or q <return> to quit---\n     __args#0=..., __args#1=...) at /usr/include/c++/7/bits/std_function.h:316\n #29 0x00007fffdada03d0 in std::function<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete)>::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (this=0x55555731c630, __args#0=..., \n     __args#1=...) at /usr/include/c++/7/bits/std_function.h:706\n #30 0x00007fffdadb4fd4 in mxnet::engine::ThreadedEngine::ExecuteOprBlock (this=0x555557319ac0, run_ctx=..., \n     opr_block=0x55555731e1b8) at ../src/engine/./threaded_engine.h:381\n #31 0x00007fffdadb8964 in mxnet::engine::ThreadedEnginePerDevice::CPUWorker<(dmlc::ConcurrentQueueType)0> (\n     this=0x555557319ac0, ctx=..., block=0x5555563ad5c0, \n     ready_event=std::shared_ptr<dmlc::ManualEvent> (use count 2, weak count 0) = {...})\n     at ../src/engine/threaded_engine_perdevice.cc:304\n #32 0x00007fffdadb62f4 in mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}::operator()(dmlc::ManualEvent) const (__closure=0x55555731bf40, \n     ready_event=std::shared_ptr<dmlc::ManualEvent> (use count 2, weak count 0) = {...})\n     at ../src/engine/threaded_engine_perdevice.cc:120\n #33 0x00007fffdadbecc0 in std::_Function_handler<void (std::shared_ptr<dmlc::ManualEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr<dmlc::ManualEvent>&&) (__functor=..., __args#0=...) at /usr/include/c++/7/bits/std_function.h:316\n #34 0x00007fffdadbd927 in std::function<void (std::shared_ptr<dmlc::ManualEvent>)>::operator()(std::shared_ptr<dmlc::ManualEvent>) const (this=0x55555724b428, __args#0=std::shared_ptr<dmlc::ManualEvent> (empty) = {...})\n     at /usr/include/c++/7/bits/std_function.h:706\n #35 0x00007fffdadbb605 in std::__invoke_impl<void, std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> >(std::__invoke_other, std::function<void (std::shared_ptr<dmlc::ManualEvent---Type <return> to continue, or q <return> to quit---\n >)>&&, std::shared_ptr<dmlc::ManualEvent>&&) (__f=...) at /usr/include/c++/7/bits/invoke.h:60\n #36 0x00007fffdadb7643 in std::__invoke<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> >(std::function<void (std::shared_ptr<dmlc::ManualEvent>)>&&, std::shared_ptr<dmlc::ManualEvent>&&) (__fn=...) at /usr/include/c++/7/bits/invoke.h:95\n #37 0x00007fffdadc4fc9 in std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > >::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (\n     this=0x55555724b418) at /usr/include/c++/7/thread:234\n #38 0x00007fffdadc4f31 in std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > >::operator()() (this=0x55555724b418)\n     at /usr/include/c++/7/thread:243\n #39 0x00007fffdadc4ed0 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > > >::_M_run() (this=0x55555724b410)\n     at /usr/include/c++/7/thread:186\n #40 0x00007fffd3e196df in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n #41 0x00007ffff7bbd6db in start_thread (arg=0x7fff692dd700) at pthread_create.c:463\n #42 0x00007ffff78e6a3f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\n </denchmark-code>\n \n <denchmark-code></denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Zha0q1", "commentT": "2020-08-26T23:12:34Z", "comment_text": "\n \t\tGDB backtrace:\n when the error is\n <denchmark-code>ubuntu@ip-172-31-38-169:~/incubator-mxnet$ python rnn.py \n [21:57:52] ../src/storage/storage.cc:198: Using Pooled (Naive) StorageManager for CPU\n malloc_consolidate(): invalid chunk size\n Aborted (core dumped)\n </denchmark-code>\n \n The backtrace is identical\n <denchmark-code>[New Thread 0x7fff13ffe880 (LWP 78359)]\n malloc_consolidate(): invalid chunk size\n \n Thread 21 \"python\" received signal SIGABRT, Aborted.\n [Switching to Thread 0x7fff68adc700 (LWP 78344)]\n __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\n 51\t../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\n (gdb) bt\n #0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\n #1  0x00007ffff78058b1 in __GI_abort () at abort.c:79\n #2  0x00007ffff784e907 in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7ffff797bdfa \"%s\\n\")\n     at ../sysdeps/posix/libc_fatal.c:181\n #3  0x00007ffff785597a in malloc_printerr (\n     str=str@entry=0x7ffff797d650 \"malloc_consolidate(): invalid chunk size\") at malloc.c:5350\n #4  0x00007ffff7855c1e in malloc_consolidate (av=av@entry=0x7fff58000020) at malloc.c:4441\n #5  0x00007ffff7859848 in _int_malloc (av=av@entry=0x7fff58000020, bytes=bytes@entry=5120) at malloc.c:3703\n #6  0x00007ffff785a55b in _int_memalign (av=0x7fff58000020, alignment=64, bytes=<optimized out>)\n     at malloc.c:4694\n #7  0x00007ffff786002a in _mid_memalign (address=<optimized out>, bytes=5016, alignment=<optimized out>)\n     at malloc.c:3314\n #8  __posix_memalign (memptr=0x7fff68ad99c0, alignment=<optimized out>, size=5016) at malloc.c:5369\n #9  0x00007fffe64adad2 in dnnl::impl::malloc (size=5016, alignment=64)\n     at ../3rdparty/mkldnn/src/common/utils.cpp:99\n #10 0x00007fffe63e4ff8 in dnnl::impl::c_compatible::operator new (sz=5016)\n     at ../3rdparty/mkldnn/src/common/nstl.hpp:40\n #11 0x00007fffe69cd594 in dnnl::impl::cpu::jit_uni_reorder_t::pd_t::create (reorder_pd=0x7fff68ad9ef8, \n     engine=0x555557207540, attr=0x7fff58086f80, src_engine=0x555557207540, src_md=0x7fff68ad9f20, \n     dst_engine=0x555557207540, dst_md=0x7fff68ada1e0) at ../3rdparty/mkldnn/src/cpu/jit_uni_reorder.cpp:1086\n #12 0x00007fffe69c5e76 in dnnl::impl::cpu::jit_uni_reorder_create (reorder_pd=0x7fff68ad9ef8, \n     engine=0x555557207540, attr=0x7fff58086f80, src_engine=0x555557207540, src_md=0x7fff68ad9f20, \n     dst_engine=0x555557207540, dst_md=0x7fff68ada1e0) at ../3rdparty/mkldnn/src/cpu/jit_uni_reorder.cpp:1247\n #13 0x00007fffe64a1e8f in dnnl_reorder_primitive_desc_create (reorder_pd=0x7fff68ad9ef8, \n ---Type <return> to continue, or q <return> to quit---\n     src_md=0x7fff68ad9f20, src_engine=0x555557207540, dst_md=0x7fff68ada1e0, dst_engine=0x555557207540, \n     attr=0x7fff58086f80) at ../3rdparty/mkldnn/src/common/reorder.cpp:73\n #14 0x00007fffdb0a263c in dnnl::reorder::primitive_desc::primitive_desc (this=0x7fff68ada4e0, src=..., \n     dst=..., attr=...) at ../3rdparty/mkldnn/include/dnnl.hpp:3166\n #15 0x00007fffdb0a274e in dnnl::reorder::reorder (this=0x7fff68ada540, src=..., dst=..., attr=...)\n     at ../3rdparty/mkldnn/include/dnnl.hpp:3217\n #16 0x00007fffdbfe3213 in mxnet::op::MKLDNNMemoryReorder (src=..., dst=...)\n     at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:399\n #17 0x00007fffdbfda681 in mxnet::op::MKLDNNRnnBackward::SetNativeWeightsGrads (this=0x7fff58077340)\n     at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:867\n #18 0x00007fffdbfe34f3 in mxnet::op::RegisterMKLDNNRnn<mxnet::op::MKLDNNRnnBackward> (rnn=...)\n     at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:991\n #19 0x00007fffdbfde69f in mxnet::op::MKLDNNRnnOp::Backward (this=0x55555731fbc0, ctx=..., \n     inputs=std::vector of length 5, capacity 5 = {...}, req=std::vector of length 3, capacity 3 = {...}, \n     outputs=std::vector of length 3, capacity 3 = {...}) at ../src/operator/nn/mkldnn/mkldnn_rnn.cc:1203\n #20 0x00007fffe31da173 in mxnet::op::RNNStatefulGradComputeExCPU (state_ptr=..., ctx=..., \n     inputs=std::vector of length 5, capacity 5 = {...}, req=std::vector of length 3, capacity 3 = {...}, \n     outputs=std::vector of length 3, capacity 3 = {...}) at ../src/operator/rnn.cc:284\n #21 0x00007fffdac7f26b in std::_Function_handler<void (mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&), void (*)(mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)>::_M_invoke(std::_Any_data const&, mxnet::OpStatePtr const&, mxnet::OpC---Type <return> to continue, or q <return> to quit---\n ontext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&) (__functor=..., __args#0=..., __args#1=..., __args#2=std::vector of length 5, capacity 5 = {...}, \n     __args#3=std::vector of length 3, capacity 3 = {...}, \n     __args#4=std::vector of length 3, capacity 3 = {...}) at /usr/include/c++/7/bits/std_function.h:316\n #22 0x00007fffdadcd088 in std::function<void (mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)>::operator()(mxnet::OpStatePtr const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&) const (this=0x555556a60738, __args#0=..., __args#1=..., \n     __args#2=std::vector of length 5, capacity 5 = {...}, \n     __args#3=std::vector of length 3, capacity 3 = {...}, \n     __args#4=std::vector of length 3, capacity 3 = {...}) at /usr/include/c++/7/bits/std_function.h:706\n #23 0x00007fffdae42905 in mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (__closure=0x555556a60630, rctx=..., on_complete=...)\n     at ../src/imperative/./imperative_utils.h:758\n #24 0x00007fffdae42b02 in mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::Nod---Type <return> to continue, or q <return> to quit---\n eAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#2}::operator()(mxnet::RunContext) const (\n     __closure=0x555556a60630, rctx=...) at ../src/imperative/./imperative_utils.h:772\n #25 0x00007fffdae48f9e in std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#2}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&) (__functor=..., __args#0=...)\n     at /usr/include/c++/7/bits/std_function.h:316\n #26 0x00007fffdad9f5a2 in std::function<void (mxnet::RunContext)>::operator()(mxnet::RunContext) const (\n     this=0x5555570658d0, __args#0=...) at /usr/include/c++/7/bits/std_function.h:706\n #27 0x00007fffdada8260 in mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (\n     __closure=0x5555572e61a0, ctx=..., on_complete=...) at ../src/engine/./threaded_engine.h:537\n #28 0x00007fffdadac835 in std::_Function_handler<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete), mxnet::engine::ThreadedEngine::BulkFlush()::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&, mxnet::engine::CallbackOnComplete&&) (__functor=..., \n ---Type <return> to continue, or q <return> to quit---\n     __args#0=..., __args#1=...) at /usr/include/c++/7/bits/std_function.h:316\n #29 0x00007fffdada03d0 in std::function<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete)>::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const (this=0x55555731c630, __args#0=..., \n     __args#1=...) at /usr/include/c++/7/bits/std_function.h:706\n #30 0x00007fffdadb4fd4 in mxnet::engine::ThreadedEngine::ExecuteOprBlock (this=0x555557319780, run_ctx=..., \n     opr_block=0x55555731e1b8) at ../src/engine/./threaded_engine.h:381\n #31 0x00007fffdadb8964 in mxnet::engine::ThreadedEnginePerDevice::CPUWorker<(dmlc::ConcurrentQueueType)0> (\n     this=0x555557319780, ctx=..., block=0x5555563ad5c0, \n     ready_event=std::shared_ptr<dmlc::ManualEvent> (use count 2, weak count 0) = {...})\n     at ../src/engine/threaded_engine_perdevice.cc:304\n #32 0x00007fffdadb62f4 in mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}::operator()(dmlc::ManualEvent) const (__closure=0x55555731bc00, \n     ready_event=std::shared_ptr<dmlc::ManualEvent> (use count 2, weak count 0) = {...})\n     at ../src/engine/threaded_engine_perdevice.cc:120\n #33 0x00007fffdadbecc0 in std::_Function_handler<void (std::shared_ptr<dmlc::ManualEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#1}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr<dmlc::ManualEvent>&&) (__functor=..., __args#0=...) at /usr/include/c++/7/bits/std_function.h:316\n #34 0x00007fffdadbd927 in std::function<void (std::shared_ptr<dmlc::ManualEvent>)>::operator()(std::shared_ptr<dmlc::ManualEvent>) const (this=0x5555571f1258, __args#0=std::shared_ptr<dmlc::ManualEvent> (empty) = {...})\n     at /usr/include/c++/7/bits/std_function.h:706\n #35 0x00007fffdadbb605 in std::__invoke_impl<void, std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> >(std::__invoke_other, std::function<void (std::shared_ptr<dmlc::ManualEvent---Type <return> to continue, or q <return> to quit---\n >)>&&, std::shared_ptr<dmlc::ManualEvent>&&) (__f=...) at /usr/include/c++/7/bits/invoke.h:60\n #36 0x00007fffdadb7643 in std::__invoke<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> >(std::function<void (std::shared_ptr<dmlc::ManualEvent>)>&&, std::shared_ptr<dmlc::ManualEvent>&&) (__fn=...) at /usr/include/c++/7/bits/invoke.h:95\n #37 0x00007fffdadc4fc9 in std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > >::_M_invoke<0ul, 1ul>(std::_Index_tuple<0ul, 1ul>) (\n     this=0x5555571f1248) at /usr/include/c++/7/thread:234\n #38 0x00007fffdadc4f31 in std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > >::operator()() (this=0x5555571f1248)\n     at /usr/include/c++/7/thread:243\n #39 0x00007fffdadc4ed0 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)>, std::shared_ptr<dmlc::ManualEvent> > > >::_M_run() (this=0x5555571f1240)\n     at /usr/include/c++/7/thread:186\n #40 0x00007fffd3e196df in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n #41 0x00007ffff7bbd6db in start_thread (arg=0x7fff68adc700) at pthread_create.c:463\n #42 0x00007ffff78e6a3f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Zha0q1", "commentT": "2020-08-27T00:03:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/TaoLv>@TaoLv</denchmark-link>\n  <denchmark-link:https://github.com/pengzhao-intel>@pengzhao-intel</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Zha0q1", "commentT": "2020-08-30T20:23:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/TaoLv>@TaoLv</denchmark-link>\n  can you please take a look?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Zha0q1", "commentT": "2020-08-31T00:42:43Z", "comment_text": "\n \t\t\n @TaoLv can you please take a look?\n \n Yes, team is looking into this issue. <denchmark-link:https://github.com/anko-intel>@anko-intel</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Zha0q1", "commentT": "2020-09-21T08:47:35Z", "comment_text": "\n \t\tThis issue will be fixed with oneDNN v1.7 release (planned October 23). We have partial workaround for this and if this is urgent we can open pull request.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "Zha0q1", "commentT": "2020-11-09T10:28:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Zha0q1>@Zha0q1</denchmark-link>\n  <denchmark-link:https://github.com/pengzhao-intel>@pengzhao-intel</denchmark-link>\n  <denchmark-link:https://github.com/szha>@szha</denchmark-link>\n  Fix is already merged in all branches - Can we close this issue?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "Zha0q1", "commentT": "2020-11-09T10:35:53Z", "comment_text": "\n \t\tclosing as <denchmark-link:https://github.com/bgawrych>@bgawrych</denchmark-link>\n  mentioned.\n \t\t"}}}, "commit": {"commit_id": "7c86f4813c66e221f7d78b9783360941adaa7779", "commit_author": "bgawrych", "commitT": "2020-10-27 15:52:42+08:00", "commit_complexity": {"commit_NLOC": "0.4117647058823529", "commit_CCN": "0.9411764705882353", "commit_Nprams": "0.4117647058823529"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "src\\operator\\nn\\mkldnn\\mkldnn_rnn.cc", "file_new_name": "src\\operator\\nn\\mkldnn\\mkldnn_rnn.cc", "file_complexity": {"file_NLOC": "917", "file_CCN": "200", "file_NToken": "8688"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "833,834,837,838", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::MKLDNNRnnBackward::SetNativeWeightsGrads", "method_params": "", "method_startline": "832", "method_endline": "841", "method_complexity": {"method_NLOC": "10", "method_CCN": "3", "method_NToken": "82", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "602,608", "deleted_lines": "593,599", "method_info": {"method_name": "mxnet::op::MKLDNNRnnForwardTraining::SetTrnMem", "method_params": "fwd", "method_startline": "589", "method_endline": "622", "method_complexity": {"method_NLOC": "26", "method_CCN": "6", "method_NToken": "284", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "732,740", "deleted_lines": "723,731", "method_info": {"method_name": "mxnet::op::MKLDNNRnnBackward::FetchDataWeightsMem", "method_params": "fwd", "method_startline": "719", "method_endline": "753", "method_complexity": {"method_NLOC": "32", "method_CCN": "8", "method_NToken": "266", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "857,858,860,861", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::MKLDNNRnnBackward::CommitWeightsGrads", "method_params": "diff_weights,diff_bias,req,dtype", "method_startline": "851", "method_endline": "952", "method_complexity": {"method_NLOC": "88", "method_CCN": "12", "method_NToken": "825", "method_nesting_level": "2"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "54,55,56,57", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::CheckMemDescEquality", "method_params": "left,right", "method_startline": "54", "method_endline": "57", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "39", "method_nesting_level": "2"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "774,781", "deleted_lines": "765,772", "method_info": {"method_name": "mxnet::op::MKLDNNRnnBackward::SetWeightsGradsMem", "method_params": "", "method_startline": "755", "method_endline": "803", "method_complexity": {"method_NLOC": "45", "method_CCN": "6", "method_NToken": "429", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\python\\mkl\\test_mkldnn.py", "file_new_name": "tests\\python\\mkl\\test_mkldnn.py", "file_complexity": {"file_NLOC": "619", "file_CCN": "115", "file_NToken": "7310"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756", "deleted_lines": null, "method_info": {"method_name": "test_rnn", "method_params": "", "method_startline": "730", "method_endline": "756", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "66", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753", "deleted_lines": null, "method_info": {"method_name": "test_rnn.batch_check", "method_params": "seq_length,state_size,batch_size,input_size", "method_startline": "735", "method_endline": "753", "method_complexity": {"method_NLOC": "18", "method_CCN": "2", "method_NToken": "262", "method_nesting_level": "1"}}}}}}}}