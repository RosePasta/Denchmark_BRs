{"BR": {"BR_id": "16338", "BR_author": "reminisce", "BRopenT": "2019-09-30T21:14:22Z", "BRcloseT": "2019-10-31T18:10:07Z", "BR_text": {"BRsummary": "Reduce op throws \"too many resources requested for launch\"", "BRdescription": "\n When MXNet is compiled in  mode, reduce ops on GPUs may throw the error \"too many resources requested for launch\" at kernel launch. See <denchmark-link:https://github.com/apache/incubator-mxnet/pull/16294#issuecomment-536727538>#16294 (comment)</denchmark-link>\n  for root cause.\n This issue is filed to track the progress of fixing the problem.\n Error message <denchmark-link:https://github.com/apache/incubator-mxnet/pull/16294#issuecomment-536266618>example</denchmark-link>\n :\n <denchmark-code>(base) ubuntu@ip-172-31-16-49:~/incubator-mxnet$ nosetests -s --verbose tests/python/gpu/test_operator_gpu.py:test_np_sum\n [INFO] Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=342263604 to reproduce.\n test_operator_gpu.test_np_sum ... [08:06:44] ../src/base.cc:84: Upgrade advisory: this mxnet has been built against cuDNN lib version 7401, which is older than the oldest version tested by CI (7600).  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n [INFO] Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1216105730 to reproduce.\n ERROR\n \n ======================================================================\n ERROR: test_operator_gpu.test_np_sum\n ----------------------------------------------------------------------\n Traceback (most recent call last):\n   File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/nose/case.py\", line 197, in runTest\n     self.test(*self.arg)\n   File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/nose/util.py\", line 620, in newfunc\n     return func(*arg, **kw)\n   File \"/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/common.py\", line 177, in test_new\n     orig_test(*args, **kwargs)\n   File \"/home/ubuntu/incubator-mxnet/python/mxnet/util.py\", line 307, in _with_np_shape\n     return func(*args, **kwargs)\n   File \"/home/ubuntu/incubator-mxnet/python/mxnet/util.py\", line 491, in _with_np_array\n     return func(*args, **kwargs)\n   File \"/home/ubuntu/incubator-mxnet/tests/python/gpu/../unittest/test_numpy_op.py\", line 264, in test_np_sum\n     assert_almost_equal(y.asnumpy(), expected_ret, rtol=1e-3 if dtype == 'float16' else 1e-3,\n   File \"/home/ubuntu/incubator-mxnet/python/mxnet/ndarray/ndarray.py\", line 2504, in asnumpy\n     ctypes.c_size_t(data.size)))\n   File \"/home/ubuntu/incubator-mxnet/python/mxnet/base.py\", line 254, in check_call\n     raise MXNetError(py_str(_LIB.MXGetLastError()))\n mxnet.base.MXNetError: [08:06:54] /home/ubuntu/incubator-mxnet/src/operator/nn/././../tensor/./broadcast_reduce-inl.cuh:528: Check failed: err == cudaSuccess (7 vs. 0) : Name: reduce_kernel_M1 ErrStr:too many resources requested for launch\n Stack trace:\n   [bt] (0) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x32) [0x7f81a9b7fb82]\n   [bt] (1) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(void mxnet::op::broadcast::ReduceImpl<mxnet::op::mshadow_op::sum, 2, float, mshadow::half::half_t, mshadow::half::half_t, mxnet::op::mshadow_op::identity>(CUstream_st*, mxnet::TBlob const&, mxnet::OpReqType, mxnet::TBlob const&, mshadow::Tensor<mshadow::gpu, 1, char> const&, mxnet::op::broadcast::ReduceImplConfig<2> const&)+0x820) [0x7f81aa184e10]\n   [bt] (2) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(void mxnet::op::broadcast::Reduce<mxnet::op::mshadow_op::sum, 2, mshadow::half::half_t, mxnet::op::mshadow_op::identity, true>(mshadow::Stream<mshadow::gpu>*, mxnet::TBlob const&, mxnet::OpReqType, mshadow::Tensor<mshadow::gpu, 1, char> const&, mxnet::TBlob const&)+0x539) [0x7f81aa187eb9]\n   [bt] (3) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(void mxnet::op::ReduceAxesComputeImpl<mshadow::gpu, mxnet::op::mshadow_op::sum, true, false, mxnet::op::mshadow_op::identity>(mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, mxnet::TShape const&)+0x13e9) [0x7f81aa868649]\n   [bt] (4) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(void mxnet::op::NumpyReduceAxesCompute<mshadow::gpu, mxnet::op::mshadow_op::sum, true, false, mxnet::op::mshadow_op::identity>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0x4ac) [0x7f81aa97a26c]\n   [bt] (5) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x2a6) [0x7f81ac1cdc16]\n   [bt] (6) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0x17) [0x7f81ac1cde67]\n   [bt] (7) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(+0x39d3f4e) [0x7f81ac127f4e]\n   [bt] (8) /home/ubuntu/incubator-mxnet/python/mxnet/../../build/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x5cf) [0x7f81ac13418f]\n \n \n -------------------- >> begin captured logging << --------------------\n root: INFO: NumPy-shape semantics has been activated in your code. This is required for creating and manipulating scalar and zero-size tensors, which were not supported in MXNet before, as in the official NumPy library. Please DO NOT manually deactivate this semantics while using `mxnet.numpy` and `mxnet.numpy_extension` modules.\n common: INFO: Setting module np/mx/python random seeds, use MXNET_MODULE_SEED=342263604 to reproduce.\n root: INFO: NumPy-shape semantics has been activated in your code. This is required for creating and manipulating scalar and zero-size tensors, which were not supported in MXNet before, as in the official NumPy library. Please DO NOT manually deactivate this semantics while using `mxnet.numpy` and `mxnet.numpy_extension` modules.\n common: INFO: Setting test np/mx/python random seeds, use MXNET_TEST_SEED=1216105730 to reproduce.\n --------------------- >> end captured logging << ---------------------\n \n ----------------------------------------------------------------------\n Ran 1 test in 9.612s\n \n FAILED (errors=1)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "reminisce", "commentT": "2019-09-30T21:14:25Z", "comment_text": "\n \t\tHey, this is the MXNet Label Bot.\n Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.\n \t\t"}}}, "commit": {"commit_id": "979e610281312da99173a494057ed68fff7ee4b2", "commit_author": "Przemyslaw Tredak", "commitT": "2019-10-31 11:10:06-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\operator\\tensor\\broadcast_reduce-inl.cuh", "file_new_name": "src\\operator\\tensor\\broadcast_reduce-inl.cuh", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "265,281", "deleted_lines": null}}}}}}