{"BR": {"BR_id": "14383", "BR_author": "antinucleon", "BRopenT": "2019-03-10T07:27:08Z", "BRcloseT": "2020-09-03T05:08:54Z", "BR_text": {"BRsummary": "MXNET_BACKWARD_DO_MIRROR is broken", "BRdescription": "\n MIRROR feature is broken for a while. Is there any plan to fix this?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "antinucleon", "commentT": "2019-03-10T07:27:11Z", "comment_text": "\n \t\tHey, this is the MXNet Label Bot.\n Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.\n Here are my recommended labels: Bug\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "antinucleon", "commentT": "2019-03-17T06:51:05Z", "comment_text": "\n \t\tContribution is welcome!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "antinucleon", "commentT": "2019-03-17T06:51:56Z", "comment_text": "\n \t\trelated PR <denchmark-link:https://github.com/apache/incubator-mxnet/pull/11472>#11472</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "antinucleon", "commentT": "2019-03-19T00:07:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mxnet-label-bot>@mxnet-label-bot</denchmark-link>\n  add [Gluon, Bug]\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "antinucleon", "commentT": "2019-03-19T09:37:38Z", "comment_text": "\n \t\tRepo:\n <denchmark-code>(venv) bingxu@bingxu-mbp:~/research/hiconv-tvm/resnet-50$ python mx_memory.py \n 3317 MB\n (venv) bingxu@bingxu-mbp:~/research/hiconv-tvm/resnet-50$ MXNET_BACKWARD_DO_MIRROR=1 python mx_memory.py \n 4496 MB\n (venv) bingxu@bingxu-mbp:~/research/hiconv-tvm/resnet-50$ \n </denchmark-code>\n \n Open mirror will cost more memory.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "antinucleon", "commentT": "2019-04-09T22:00:16Z", "comment_text": "\n \t\tThis seems to be incorrectly labelled as Gluon. Removing the gluon label.\n <denchmark-link:https://github.com/mxnet-label-bot>@mxnet-label-bot</denchmark-link>\n  Update [Bug, Environment Variables]\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "antinucleon", "commentT": "2019-06-19T10:39:15Z", "comment_text": "\n \t\t<denchmark-h:h1>mxnet version:1.4.0, resnext101</denchmark-h>\n \n I think the following code for MXNET_BACKWARD_DO_MIRROR has problem which is from .//src/executor/graph_executor.cc.\n it will do not use mirror even the MXNET_BACKWARD_DO_MIRROR is on unless the force_mirroring is set.\n and...it will use more gpu memory when MXNET_BACKWARD_DO_MIRROR is set.\n  260   int do_mirror = dmlc::GetEnv(\"MXNET_BACKWARD_DO_MIRROR\", 0);\n  261   auto need_mirror = [do_mirror](const nnvm::Node& node) -> int {\n  262     if (node.is_variable()) return 0;\n  263     const std::string& type = node.attrs.op->name;\n  264     if (type == \"Dropout\") return false;\n  265     if (get_node_attr(node, \"__force_mirroring__\", false)) return true;\n  266     if (do_mirror == 0) return false;\n  267     if (type == \"Convolution\") return false;\n  268     if (type == \"FullyConnected\") return false;\n  269     if (type == \"Concat\") return false;\n  270     if (type == \"SoftmaxOutput\") return false;\n  271     if (type == \"BatchNorm\") return false;\n  272     if (type == \"CuDNNBatchNorm\") return false;\n  273     return true;\n  274   };\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "antinucleon", "commentT": "2019-06-19T19:34:21Z", "comment_text": "\n \t\tI can't express more about how important this feature is. Please fix it, and get same memory cost to version 0.7. <denchmark-link:https://github.com/dmlc/mxnet-memonger>https://github.com/dmlc/mxnet-memonger</denchmark-link>\n  <denchmark-link:https://arxiv.org/abs/1604.06174>https://arxiv.org/abs/1604.06174</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "antinucleon", "commentT": "2019-07-09T22:49:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/antinucleon>@antinucleon</denchmark-link>\n  Given below is the outputs that I got from the LSTM-based NMT model (from the  toolkit):\n <denchmark-h:h3>Baseline</denchmark-h>\n \n <denchmark-code>[INFO:root] Global Step[50] Epoch[0] Batch [50]\tSpeed: 261.10 samples/sec\tperplexity=921.880075\tMemory Usage (MB): pid_21652=6179, \tPE Usage (W, J): dev_0=78.96,1935.41, \n [INFO:root] Global Step[100] Epoch[0] Batch [100]\tSpeed: 285.67 samples/sec\tperplexity=616.976020\tMemory Usage (MB): pid_21652=6179, \tPE Usage (W, J): dev_0=82.94,3793.57, \n [INFO:root] Global Step[150] Epoch[0] Batch [150]\tSpeed: 294.01 samples/sec\tperplexity=496.731365\tMemory Usage (MB): pid_21652=6601, \tPE Usage (W, J): dev_0=95.77,5878.26, \n [INFO:root] Global Step[200] Epoch[0] Batch [200]\tSpeed: 310.77 samples/sec\tperplexity=424.378748\tMemory Usage (MB): pid_21652=6601, \tPE Usage (W, J): dev_0=77.22,7468.53, \n [INFO:root] Global Step[250] Epoch[0] Batch [250]\tSpeed: 282.17 samples/sec\tperplexity=369.385264\tMemory Usage (MB): pid_21652=6601, \tPE Usage (W, J): dev_0=87.60,9455.43, \n [INFO:root] Global Step[300] Epoch[0] Batch [300]\tSpeed: 294.64 samples/sec\tperplexity=321.364135\tMemory Usage (MB): pid_21652=6601, \tPE Usage (W, J): dev_0=82.16,11240.07, \n </denchmark-code>\n \n <denchmark-h:h3>BACKWARD_DO_MIRROR=1</denchmark-h>\n \n <denchmark-code>[INFO:root] Global Step[50] Epoch[0] Batch [50]\tSpeed: 151.09 samples/sec\tperplexity=949.961463\tMemory Usage (MB): pid_20928=2425, \tPE Usage (W, J): dev_0=84.69,3587.42, \n [INFO:root] Global Step[100] Epoch[0] Batch [100]\tSpeed: 170.88 samples/sec\tperplexity=625.173421\tMemory Usage (MB): pid_20928=2425, \tPE Usage (W, J): dev_0=76.74,6461.51, \n [INFO:root] Global Step[150] Epoch[0] Batch [150]\tSpeed: 178.00 samples/sec\tperplexity=499.439886\tMemory Usage (MB): pid_20928=2475, \tPE Usage (W, J): dev_0=84.37,9494.95, \n [INFO:root] Global Step[200] Epoch[0] Batch [200]\tSpeed: 195.40 samples/sec\tperplexity=426.799941\tMemory Usage (MB): pid_20928=2475, \tPE Usage (W, J): dev_0=79.16,12087.66, \n [INFO:root] Global Step[250] Epoch[0] Batch [250]\tSpeed: 169.05 samples/sec\tperplexity=371.365061\tMemory Usage (MB): pid_20928=2475, \tPE Usage (W, J): dev_0=81.68,15179.92, \n [INFO:root] Global Step[300] Epoch[0] Batch [300]\tSpeed: 180.27 samples/sec\tperplexity=323.268620\tMemory Usage (MB): pid_20928=2475, \tPE Usage (W, J): dev_0=73.94,17805.00, \n </denchmark-code>\n \n We can see from the output logs that with the same number of global steps, both roughly reach the same training quality. The memory footprint of doing backward mirroring is around 1/3 of that in the baseline, but at the same time this comes with around 40% performance drop.\n I am currently still investigating on the cause of such huge performance drop. At the same time, if you have any specific benchmark of interest (preferrably small benchmark like the one you commented above because we are still in the debugging phase). Please kindly let me know. Thanks.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "antinucleon", "commentT": "2019-07-09T22:52:03Z", "comment_text": "\n \t\tMy implementation is available here: <denchmark-link:https://github.com/UofT-EcoSystem/nnvm/blob/cce19c328427de4eacd51178d233ce23a3e3d79d/src/pass/gradient.cc#L144>https://github.com/UofT-EcoSystem/nnvm/blob/cce19c328427de4eacd51178d233ce23a3e3d79d/src/pass/gradient.cc#L144</denchmark-link>\n \n Any feedback or comment is much appreciated. Thanks.\n \t\t"}}}, "commit": {"commit_id": "7d4f2f3b1045fd515bca6fe0fb38d94ef4f2ff60", "commit_author": "Bojian Zheng", "commitT": "2019-11-13 09:28:19-08:00", "commit_complexity": {"commit_NLOC": "0.3333333333333333", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\executor\\graph_executor.cc", "file_new_name": "src\\executor\\graph_executor.cc", "file_complexity": {"file_NLOC": "1737", "file_CCN": "337", "file_NToken": "16002"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "367,368", "method_info": {"method_name": "mxnet::exec::GraphExecutor::InitFullGraph", "method_params": "symbol,grad_req_types", "method_startline": "324", "method_endline": "387", "method_complexity": {"method_NLOC": "57", "method_CCN": "19", "method_NToken": "545", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "src\\operator\\nn\\cudnn\\cudnn_batch_norm-inl.h", "file_new_name": "src\\operator\\nn\\cudnn\\cudnn_batch_norm-inl.h", "file_complexity": {"file_NLOC": "211", "file_CCN": "23", "file_NToken": "1657"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "126,127,128,129,130,131,132,144,167,168,169,170", "deleted_lines": "136", "method_info": {"method_name": "mxnet::op::CuDNNBatchNormOp::Forward", "method_params": "ctx,in_data,req,out_data,aux_states", "method_startline": "72", "method_endline": "171", "method_complexity": {"method_NLOC": "84", "method_CCN": "7", "method_NToken": "686", "method_nesting_level": "3"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "245,246,247", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::CuDNNBatchNormOp::Backward", "method_params": "ctx,inputs,req,outputs", "method_startline": "173", "method_endline": "248", "method_complexity": {"method_NLOC": "63", "method_CCN": "8", "method_NToken": "610", "method_nesting_level": "3"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "58", "deleted_lines": null, "method_info": {"method_name": "mxnet::op::CuDNNBatchNormOp::CuDNNBatchNormOp", "method_params": "", "method_startline": "50", "method_endline": "59", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "57", "method_nesting_level": "3"}}}}}}}}