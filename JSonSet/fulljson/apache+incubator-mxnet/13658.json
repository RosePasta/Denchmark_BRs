{"BR": {"BR_id": "13658", "BR_author": "jermainewang", "BRopenT": "2018-12-16T17:43:29Z", "BRcloseT": "2018-12-22T02:31:29Z", "BR_text": {"BRsummary": "Converting MX array to DLPack crashes when MX array goes out-of-scope", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n Converting MX NDArray to DLPack, then to other framework's DLPack-compatible NDArray causes memory corruption when the origin MX NDArray goes out-of-scope.\n <denchmark-h:h2>Environment info (Required)</denchmark-h>\n \n <denchmark-code>----------Python Info----------\n Version      : 3.5.2\n Compiler     : GCC 5.4.0 20160609\n Build        : ('default', 'Nov 12 2018 13:43:14')\n Arch         : ('64bit', '')\n ------------Pip Info-----------\n Version      : 18.1\n Directory    : /usr/local/lib/python3.5/dist-packages/pip\n ----------MXNet Info-----------\n Version      : 1.4.0\n Directory    : /usr/local/lib/python3.5/dist-packages/mxnet\n Commit Hash   : 1f73c5d9d308a690b57ea1b474d2ba99ca06c476\n ----------System Info----------\n Platform     : Linux-4.19.4-arch1-1-ARCH-x86_64-with-Ubuntu-16.04-xenial\n system       : Linux\n node         : 17d02f89890e\n release      : 4.19.4-arch1-1-ARCH\n version      : #1 SMP PREEMPT Fri Nov 23 09:06:58 UTC 2018\n ----------Hardware Info----------\n machine      : x86_64\n processor    : x86_64\n Architecture:          x86_64\n CPU op-mode(s):        32-bit, 64-bit\n Byte Order:            Little Endian\n CPU(s):                8\n On-line CPU(s) list:   0-7\n Thread(s) per core:    2\n Core(s) per socket:    4\n Socket(s):             1\n NUMA node(s):          1\n Vendor ID:             GenuineIntel\n CPU family:            6\n Model:                 62\n Model name:            Intel(R) Xeon(R) CPU E5-1620 v2 @ 3.70GHz\n Stepping:              4\n CPU MHz:               1812.064\n CPU max MHz:           3900.0000\n CPU min MHz:           1200.0000\n BogoMIPS:              7384.55\n Virtualization:        VT-x\n Hypervisor vendor:     vertical\n Virtualization type:   full\n L1d cache:             32K\n L1i cache:             32K\n L2 cache:              256K\n L3 cache:              10240K\n NUMA node0 CPU(s):     0-7\n Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts flush_l1d\n ----------Network Test----------\n Setting timeout: 10\n Timing for MXNet: https://github.com/apache/incubator-mxnet, DNS: 0.0056 sec, LOAD: 0.4655 sec.\n Timing for Gluon Tutorial(en): http://gluon.mxnet.io, DNS: 0.0277 sec, LOAD: 0.4154 sec.\n Timing for PYPI: https://pypi.python.org/pypi/pip, DNS: 0.0039 sec, LOAD: 0.1236 sec.\n Timing for Gluon Tutorial(cn): https://zh.gluon.ai, DNS: 0.1844 sec, LOAD: 1.0354 sec.\n Timing for Conda: https://repo.continuum.io/pkgs/free/, DNS: 0.0045 sec, LOAD: 0.0329 sec.\n Timing for FashionMNIST: https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz, DNS: 0.0125 sec, LOAD: 0.6710 sec.\n </denchmark-code>\n \n Package used (Python/R/Scala/Julia): Python\n <denchmark-h:h2>Error Message:</denchmark-h>\n \n <denchmark-code>Segmentation fault: 11\n \n Stack trace returned 10 entries:\n [bt] (0) /usr/local/lib/python3.5/dist-packages/mxnet/libmxnet.so(+0x1fef5a) [0x7f4a09186f5a]\n [bt] (1) /usr/local/lib/python3.5/dist-packages/mxnet/libmxnet.so(+0x31383b6) [0x7f4a0c0c03b6]\n [bt] (2) /lib/x86_64-linux-gnu/libc.so.6(+0x354b0) [0x7f4a259324b0]\n [bt] (3) /usr/local/lib/python3.5/dist-packages/torch/lib/libcaffe2.so(at::TypeDefault::tensorFromBlob(void*, c10::ArrayRef<long>, c10::ArrayRef<long>, std::function<void (void*)> const&) const+0x61) [0x7f4996c4c741]\n [bt] (4) /usr/local/lib/python3.5/dist-packages/torch/lib/libcaffe2.so(at::fromDLPack(DLManagedTensor const*)+0x29f) [0x7f4996871e2f]\n [bt] (5) /usr/local/lib/python3.5/dist-packages/torch/lib/libtorch_python.so(THPModule_fromDLPack(_object*, _object*)+0x41) [0x7f49e2e2f341]\n [bt] (6) python3(PyEval_EvalFrameEx+0x4d06) [0x53b486]\n [bt] (7) python3(PyEval_EvalFrameEx+0x4b14) [0x53b294]\n [bt] (8) python3() [0x53fc97]\n [bt] (9) python3(PyEval_EvalCode+0x1f) [0x5409bf]\n </denchmark-code>\n \n <denchmark-h:h2>Minimum reproducible example</denchmark-h>\n \n import mxnet as mx\n from torch.utils import dlpack\n \n def foo():\n     x = mx.nd.array([0, 5], dtype='int64')\n     dl = x.to_dlpack_for_read()\n     return dlpack.from_dlpack(dl)\n \n for i in range(10):\n     y = foo()\n     y.numpy()\n Torch version v1.0.0\n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n (Paste the commands you ran that produced the error.)\n \n Use a ubuntu 16.04 image (with mx and torch installed)\n Run the above code\n \n <denchmark-h:h2>What have you tried to solve it?</denchmark-h>\n \n Found this bug in DGL project <denchmark-link:https://github.com/dmlc/dgl/pull/312>dmlc/dgl#312</denchmark-link>\n  . Tried:\n \n MXArray -> DLPack -> DGL Array :  FAILED\n MXArray -> DLPack -> MXArray : SUCCEED\n MXArray -> DLPack -> Torch Tensor : FAILED\n Torch Tensor -> DLPack -> DGL Array : SUCCEED\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jermainewang", "commentT": "2018-12-16T17:43:32Z", "comment_text": "\n \t\tHey, this is the MXNet Label Bot.\n Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jermainewang", "commentT": "2018-12-17T06:44:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tqchen>@tqchen</denchmark-link>\n  <denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  could you please take a look at this problem? Thanks\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jermainewang", "commentT": "2018-12-17T08:11:03Z", "comment_text": "\n \t\tDLPack allow strides field to be , for mxnet do do not need to store strides, as we only support compact tensor for now.\n <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/include/mxnet/tensor_blob.h#L409>https://github.com/apache/incubator-mxnet/blob/master/include/mxnet/tensor_blob.h#L409</denchmark-link>\n \n However, PyTorch doesn't accept the DLPack whose strides is nullptr.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jermainewang", "commentT": "2018-12-17T14:27:33Z", "comment_text": "\n \t\tthe error might be trickier. For DGL code below, the code is undeterministic. We have to run it multiple times before we can see crash. It seems that some memory in dlpack exported from MXNet isn't referenced. However, if I use , it works fine. <denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  do you have any suggestions?\n import os\n os.environ['DGLBACKEND'] = 'mxnet'\n import mxnet as mx\n import numpy as np\n import dgl\n \n def foo():\n     x = mx.nd.array([0, 5], dtype='int64')\n     dl = x.to_dlpack_for_read()\n     return dgl.ndarray.from_dlpack(dl)\n \n for i in range(10):\n     y = foo()\n     y.asnumpy()\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jermainewang", "commentT": "2018-12-17T15:46:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  This explains the torch case thank you. In DGL, we actually handled this:\n <denchmark-link:https://github.com/dmlc/dgl/blob/632d598c77af616278bc0f2144a14958678dcbae/src/runtime/ndarray.cc#L83-L89>https://github.com/dmlc/dgl/blob/632d598c77af616278bc0f2144a14958678dcbae/src/runtime/ndarray.cc#L83-L89</denchmark-link>\n \n In DGL, we also only support contiguous tensor, so  is never used elsewhere. The NDArray class is borrowed from TVM project. I wonder whether this bug happens in TVM too.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jermainewang", "commentT": "2018-12-17T15:49:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/zheng-da>@zheng-da</denchmark-link>\n   can accept the DLPack whose  is .\n Could you please try the <denchmark-link:https://github.com/apache/incubator-mxnet/pull/13663>MXNet PR</denchmark-link>\n  which fills the strides of DLPack?\n The code you wrote works fine in .\n I couldn't reproduce the error.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jermainewang", "commentT": "2018-12-17T15:51:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jermainewang>@jermainewang</denchmark-link>\n  Let me test it for TVM.\n import tvm\n import mxnet as mx\n \n tvm_a = tvm.ndarray.array([1, 2, 3])\n tvm_pack = tvm_a.to_dlpack()\n \n mx_a = mx.nd.from_dlpack(tvm_pack)\n print(mx_a)\n \n mx_b = mx.nd.array([4, 5, 6])\n mx_pack = mx_b.to_dlpack_for_write()\n tvm_b = tvm.nd.from_dlpack(mx_pack)\n print(tvm_b)\n <denchmark-code>mxnet==1.5.0b20181216\n tvm: 0.5.dev\n </denchmark-code>\n \n It works fine in TVM.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "jermainewang", "commentT": "2018-12-18T03:35:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  , try following steps:\n \n Use a ubuntu 16.04 image.\n Create file t.py with following codes:\n \n import mxnet as mx\n import numpy as np\n import tvm\n \n def foo():\n   x = mx.nd.array([0, 5], dtype='int64')\n   dl = x.to_dlpack_for_read()\n   return tvm.nd.from_dlpack(dl)\n \n for i in range(10):\n   y = foo()\n   y.asnumpy()\n \n Run it with for i in {0..100}; do echo $i && python3 t.py || break ; done\n \n I used a ubuntu docker image and could reproduce the error.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "jermainewang", "commentT": "2018-12-18T09:08:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jermainewang>@jermainewang</denchmark-link>\n \n I use Arch Linux, Python 3.7.1, MXNet 1.5.0 installed by pip,and TVM 0.5.dev.\n There is no any error. It's strange.\n I will test it on the ubuntu server and docker.\n Could you provide the error messeage in the test for TVM?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "jermainewang", "commentT": "2018-12-18T10:29:41Z", "comment_text": "\n \t\tit seems the bug only appears in Ubuntu 16.04, if I remember correctly. We tested in Ubuntu 18.04, and it works fine.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "jermainewang", "commentT": "2018-12-18T16:28:35Z", "comment_text": "\n \t\tYeah, the bug does not occur on my arch machine too.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "jermainewang", "commentT": "2018-12-18T16:29:54Z", "comment_text": "\n \t\tError message:\n <denchmark-code>root@17d02f89890e:/tmp/dgl# for i in {0..100}; do echo $i && python3 tt-tvm.py || break ; done\n 0\n 1\n 2\n 3\n Traceback (most recent call last):\n   File \"tt-tvm.py\", line 12, in <module>\n     y.asnumpy()\n   File \"/tmp/tvm/python/tvm/_ffi/ndarray.py\", line 264, in asnumpy\n     check_call(_LIB.TVMArrayCopyToBytes(self.handle, data, nbytes))\n   File \"/tmp/tvm/python/tvm/_ffi/base.py\", line 72, in check_call\n     raise TVMError(py_str(_LIB.TVMGetLastError()))\n tvm._ffi.base.TVMError: [16:29:28] /tmp/tvm/src/runtime/ndarray.cc:256: Check failed: arr_size == nbytes (16330860332007321568 vs. 16) TVMArrayCopyToBytes: size mismatch\n \n Stack trace returned 10 entries:\n [bt] (0) /tmp/tvm/build/libtvm.so(dmlc::StackTrace[abi:cxx11](unsigned long)+0x1fd) [0x7fe7c8dd7f6d]\n [bt] (1) /tmp/tvm/build/libtvm.so(TVMArrayCopyToBytes+0x665) [0x7fe7c9407415]\n [bt] (2) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7fe80aa3fe20]\n [bt] (3) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7fe80aa3f88b]\n [bt] (4) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a) [0x7fe80aa3a01a]\n [bt] (5) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(+0x9fcb) [0x7fe80aa2dfcb]\n [bt] (6) python3(PyObject_Call+0x47) [0x5c20e7]\n [bt] (7) python3(PyEval_EvalFrameEx+0x4ed6) [0x53b656]\n [bt] (8) python3(PyEval_EvalFrameEx+0x4b14) [0x53b294]\n [bt] (9) python3() [0x53fc97]\n </denchmark-code>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "jermainewang", "commentT": "2018-12-19T00:55:54Z", "comment_text": "\n \t\tReproduce the error on Ubuntu16.04\n I print the datas of DLTensor in include/tvm/runtime/ndarray.h \n  inline size_t GetDataSize(const DLTensor& arr) {                                                                                        \n    size_t size = 1;                                                                                                                      \n    cout << \"DLTensor Ptr: \" << &arr << endl;                                                                                             \n    cout << \"Shape Ptr: \" << arr.shape << endl;                      \n    cout << \"ndim: \" << arr.ndim << endl;                                                                     \n    for (tvm_index_t i = 0; i < arr.ndim; ++i) {                                                                                          \n      cout << \"shape[\" << i << \"] = \" << arr.shape[i] << endl;                                                                            \n      size *= static_cast<size_t>(arr.shape[i]);                                                                                          \n    }                                                                                                                                     \n    cout << \"Bits: \" << int(arr.dtype.bits) << endl;                                                                                      \n    cout << \"lanes: \" << int(arr.dtype.lanes) << endl;                                                                                    \n    size *= (arr.dtype.bits * arr.dtype.lanes + 7) / 8;                                                                                   \n    return size;                                                                                                                          \n  }                                                       \n Error Message:\n <denchmark-code>DLTensor Ptr: 0x3496040\n Shape Ptr: 0x34839f0\n ndim: 1\n shape[0] = -2992783055023189668\n Bits: 64\n lanes: 1\n </denchmark-code>\n \n In MXNet, include/mxnet/tensor_blob.h\n   inline void SetDLTensor(int dev_mask, int dev_id) {\n     dltensor_.data = dptr_;\n     dltensor_.ctx = DLContext{static_cast<DLDeviceType>(dev_mask), dev_id};\n     dltensor_.ndim = shape_.ndim();\n     dltensor_.dtype = DTypeTransform(type_flag_);\n     dltensor_.shape = shape_.data();\n     dltensor_.strides = nullptr;\n     dltensor_.byte_offset = 0;\n   }\n It seems that TShape object shape_ has been reallocated, because shape_ is saved in a TBlob instance, however Tblob is mutable in NDArray.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "jermainewang", "commentT": "2018-12-19T17:20:54Z", "comment_text": "\n \t\tThat's strange. I thought the mutable is for the data pointer while the shape array should not be changed.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "jermainewang", "commentT": "2018-12-20T04:27:08Z", "comment_text": "\n \t\tI have fixed the bug in PR <denchmark-link:https://github.com/apache/incubator-mxnet/pull/13698>#13698</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "f2115881235f66640c5a59810c0a523136b08963", "commit_author": "JackieWu", "commitT": "2018-12-21 10:03:54-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ndarray\\ndarray.cc", "file_new_name": "src\\ndarray\\ndarray.cc", "file_complexity": {"file_NLOC": "1720", "file_CCN": "376", "file_NToken": "16465"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "333,336", "deleted_lines": "335,336,337", "method_info": {"method_name": "mxnet::NDArray::ToDLPack", "method_params": "", "method_startline": "332", "method_endline": "342", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "96", "method_nesting_level": "1"}}}}}}}}