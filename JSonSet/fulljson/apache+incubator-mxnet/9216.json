{"BR": {"BR_id": "9216", "BR_author": "wkcn", "BRopenT": "2017-12-28T07:54:53Z", "BRcloseT": "2018-03-10T01:36:59Z", "BR_text": {"BRsummary": "Loss of Precision in BatchNorm and output_var may be wrong", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n \n BatchNorm loses a little precision\n the output_var in BatchNorm may be wrong\n \n <denchmark-h:h2>Environment Info</denchmark-h>\n \n OS: Arch Linux 4.14.8\n MXNet: 1.0.0 and 1.0.1 (the latest version, CPU version)\n <denchmark-h:h2>Build Config</denchmark-h>\n \n make -j8 USE_OPENCV=1 USE_BLAS=openblas\n <denchmark-h:hr></denchmark-h>\n \n Hi, there.\n I converted <denchmark-link:https://github.com/KaimingHe/deep-residual-networks>ResNet Model on Caffe</denchmark-link>\n  to <denchmark-link:https://github.com/wkcn/resnet-v1-mx>ResNet model on MXNet</denchmark-link>\n .\n And I found that the output results between Caffe and MXNet are different.\n The reason is that the computations of Caffe and MXNet are different.\n For the BatchNorm in Caffe, the output is (x - mean(x)) / sqrt(var(x) + eps).\n For the BatchNorm in MXNet, the output is (x - mean(x)) * factor, and factor = 1.0 / sqrt(var(x) + eps).\n I think the method in MXNet will lose a little precision but bring the higher performance (Reduce the times of division).\n At the same time, I found that the output_var in BatchNorm may be wrong.\n The output_var is invstd, namely the multiplicative inverse of the standard deviation. I think it should be the variance.\n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n Here is <denchmark-link:https://github.com/wkcn/test_mxnet_bn>my testing code</denchmark-link>\n .\n I compare three outputs:\n \n numpy (compute manally)\n caffe\n mxnet\n \n <denchmark-code>caffe and numpy 0.0 0.0\n caffe and mx 16.0 2.36527e-07\n numpy and mx 16.0 2.36527e-07\n </denchmark-code>\n \n The first column is the maxmimum absolute error, and the second column is the maxmimum relative error.\n <denchmark-h:h1>What I have tried to solve it</denchmark-h>\n \n I change the BatchNorm implement in MXNet, and the output is below:\n <denchmark-code>caffe and numpy 0.0 0.0\n caffe and mx 0.0 0.0\n numpy and mx 0.0 0.0\n </denchmark-code>\n \n The modified BatchNorm(cpu) code is <denchmark-link:https://github.com/wkcn/incubator-mxnet/commit/5ecd4882bc043cf059e962f7ce488270bafa07c7>here</denchmark-link>\n .\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wkcn", "commentT": "2017-12-30T00:18:24Z", "comment_text": "\n \t\tWhat's the result if we use this line instead: <denchmark-link:https://github.com/wkcn/test_mxnet_bn/blob/master/compute.py#L15>https://github.com/wkcn/test_mxnet_bn/blob/master/compute.py#L15</denchmark-link>\n . I think it may be the same as the MXNet version. Also, the rel_error is relatively small and should not be a problem.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wkcn", "commentT": "2017-12-30T05:34:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n  If using the line 15th, the result will be the same as that of MXNet.\n The rel_error is relatively small.\n However, there may be a bug.\n The <denchmark-link:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html?highlight=batchnorm#mxnet.symbol.BatchNorm>documents</denchmark-link>\n  said that\n output_mean_var (boolean, optional, default=0) \u2013 Output All,normal mean and var\n <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147>https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147</denchmark-link>\n \n The output_var is  now, I think it should be .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wkcn", "commentT": "2017-12-30T05:53:37Z", "comment_text": "\n \t\tLooks like a bug, I'm not sure. <denchmark-link:https://github.com/piiswrong>@piiswrong</denchmark-link>\n  <denchmark-link:https://github.com/cjolivier01>@cjolivier01</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wkcn", "commentT": "2018-01-02T01:20:21Z", "comment_text": "\n \t\tCaffe and MXNet will get the same BN output as I've tested before.\n Please be caution that CUDNN cannot handle BN when eps <= 1e-5. In this situation, you can set cudnn_off=true in BN of MXNet.\n <denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  ,Models of <denchmark-link:https://github.com/apache/incubator-mxnet/pull/9215>#9215</denchmark-link>\n  get a very lower error rate.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "wkcn", "commentT": "2018-01-02T03:58:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/chinakook>@chinakook</denchmark-link>\n  Thank you!\n I wrote <denchmark-link:https://github.com/wkcn/test_mxnet_bn/tree/ndarray>a new code</denchmark-link>\n  to test.\n And I found that using CUDNN and not using CUDNN on Caffe get the same BN output.\n But there is a little difference between the CPU result and the GPU result on Caffe.\n <denchmark-code>max absolute error, max relative error, mean error\n 8.0, 3.4666334e-07, 0.024640804\n </denchmark-code>\n \n The comparison between MXNet and Caffe is showed below.\n Using the GPU result of Caffe.\n <denchmark-code>('context: ', cpu(0))\n ('cudnn_off: ', True)\n ndarray: y = gamma * [(x - mean) / sqrt(var + eps)] + beta\n \n                 max absolute error, max relative error, mean error\n ('caffe and ndarray', 8.0, 3.4666334e-07, 0.024640804)\n ('caffe and mx', 16.0, 3.4666334e-07, 0.057109512)\n ('ndarray and mx', 16.0, 2.3652711e-07, 0.038025968)\n </denchmark-code>\n \n <denchmark-code>('context: ', gpu(0))\n ('cudnn_off: ', True)\n ndarray: y = gamma * [(x - mean) / sqrt(var + eps)] + beta\n \n                 max absolute error, max relative error, mean error\n ('caffe and ndarray', 8.0, 3.4666334e-07, 0.024640804)\n ('caffe and mx', 16.0, 2.3644267e-07, 0.052587245)\n ('ndarray and mx', 16.0, 2.4033051e-07, 0.054628547)\n </denchmark-code>\n \n <denchmark-code>('context: ', gpu(0))\n ('cudnn_off: ', False)\n ndarray: y = gamma * [(x - mean) / sqrt(var + eps)] + beta\n \n                 max absolute error, max relative error, mean error\n ('caffe and ndarray', 8.0, 3.4666334e-07, 0.024640804)\n ('caffe and mx', 16.0, 2.3644267e-07, 0.052587245)\n ('ndarray and mx', 16.0, 2.4033051e-07, 0.054628547)\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "wkcn", "commentT": "2018-01-02T04:05:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  Can you confirm that the output is invstd instead of var?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "wkcn", "commentT": "2018-01-02T04:06:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wkcn>@wkcn</denchmark-link>\n  These errors are too small and will cause no problem. However, there is certainly a bug if the output is invstd instead of var.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "wkcn", "commentT": "2018-01-02T04:44:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n \n Thank you!\n I'm sure that the output_var is invstd instead of variance.\n <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147>https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L147</denchmark-link>\n \n <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L161>https://github.com/apache/incubator-mxnet/blob/master/src/operator/nn/batch_norm.cc#L161</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "wkcn", "commentT": "2018-01-02T21:18:56Z", "comment_text": "\n \t\tthe documentation should be fixed\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "wkcn", "commentT": "2018-01-05T18:31:07Z", "comment_text": "\n \t\tI find it will be improper to output the invstd as the value will be undefined when batch_size=1. We should consider to change it back to output the variance.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "wkcn", "commentT": "2018-01-05T18:41:42Z", "comment_text": "\n \t\tJust FYI, CUDNN stores invstd for moving_variance rather than actual variance, if I remember correctly.\n This can be seen in the unit test not checking for those being the same when CUDNN is enabled: \n \n \n incubator-mxnet/tests/cpp/operator/batchnorm_test.cc\n \n \n          Line 409\n       in\n       f9fd88b\n \n \n \n \n \n \n  #if MXNET_USE_CUDNN != 1 /* CUDNN takes a different approach here on first pass */ \n \n \n \n \n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "wkcn", "commentT": "2018-01-05T18:52:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cjolivier01>@cjolivier01</denchmark-link>\n  I see, we need to test the correctness of the mean and \"var\" as we can fetch these two values by turning on . <denchmark-link:https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.BatchNorm>https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.BatchNorm</denchmark-link>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "wkcn", "commentT": "2018-01-05T18:55:47Z", "comment_text": "\n \t\tI believe that it was stored as invstd in order to avoid calculating it back and forth between forward and backward pass, but I am open to it being adjusted as long as everything in tests/cpp/batchnorm_test.cc still passes.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "wkcn", "commentT": "2018-01-05T19:05:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/cjolivier01>@cjolivier01</denchmark-link>\n  In that case, it should be okay to keep the invstd format. We need to test the case when  is turned on.\n \t\t"}}}, "commit": {"commit_id": "279ccb1c77d7fdfb2652a4f6574466ea1ecb3a09", "commit_author": "Xingjian Shi", "commitT": "2018-03-09 17:36:58-08:00", "commit_complexity": {"commit_NLOC": "0.15547703180212014", "commit_CCN": "0.3250883392226148", "commit_Nprams": "0.09540636042402827"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\api\\python\\gluon\\nn.md", "file_new_name": "docs\\api\\python\\gluon\\nn.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\api\\python\\ndarray\\ndarray.md", "file_new_name": "docs\\api\\python\\ndarray\\ndarray.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "643", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\api\\python\\symbol\\symbol.md", "file_new_name": "docs\\api\\python\\symbol\\symbol.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "644", "deleted_lines": null}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\mxnet\\gluon\\nn\\basic_layers.py", "file_new_name": "python\\mxnet\\gluon\\nn\\basic_layers.py", "file_complexity": {"file_NLOC": "595", "file_CCN": "66", "file_NToken": "2467"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "584,585,586,587,588,589,590,591", "deleted_lines": null, "method_info": {"method_name": "__repr__", "method_params": "self", "method_startline": "584", "method_endline": "591", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "78", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "564,565,566", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,axis,epsilon,center,scale,beta_initializer,gamma_initializer,in_channels,prefix,params", "method_startline": "564", "method_endline": "566", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "580,581,582", "deleted_lines": null, "method_info": {"method_name": "hybrid_forward", "method_params": "self,F,data,gamma,beta", "method_startline": "580", "method_endline": "582", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\nn\\batch_norm-inl.h", "file_new_name": "src\\operator\\nn\\batch_norm-inl.h", "file_complexity": {"file_NLOC": "287", "file_CCN": "37", "file_NToken": "2298"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "82", "deleted_lines": "82", "method_info": {"method_name": "mxnet::op::BatchNormParam::DMLC_DECLARE_PARAMETER", "method_params": "BatchNormParam", "method_startline": "69", "method_endline": "87", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "124", "method_nesting_level": "3"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\operator\\nn\\batch_norm.cc", "file_new_name": "src\\operator\\nn\\batch_norm.cc", "file_complexity": {"file_NLOC": "458", "file_CCN": "59", "file_NToken": "3904"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "513,514", "deleted_lines": "513"}}}, "file_6": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "src\\operator\\nn\\layer_norm-inl.h", "file_complexity": {"file_NLOC": "229", "file_CCN": "15", "file_NToken": "2256"}}, "file_7": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "src\\operator\\nn\\layer_norm.cc", "file_complexity": {"file_NLOC": "111", "file_CCN": "4", "file_NToken": "781"}}, "file_8": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "src\\operator\\nn\\layer_norm.cu", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_9": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\python\\unittest\\test_gluon.py", "file_new_name": "tests\\python\\unittest\\test_gluon.py", "file_complexity": {"file_NLOC": "654", "file_CCN": "91", "file_NToken": "6850"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "385,386,387", "deleted_lines": null, "method_info": {"method_name": "test_layernorm", "method_params": "", "method_startline": "385", "method_endline": "387", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "28", "method_nesting_level": "0"}}}}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\python\\unittest\\test_operator.py", "file_new_name": "tests\\python\\unittest\\test_operator.py", "file_complexity": {"file_NLOC": "4189", "file_CCN": "654", "file_NToken": "53512"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "2449,2450,2451,2452,2453,2454", "deleted_lines": null, "method_info": {"method_name": "test_layer_norm", "method_params": "", "method_startline": "2449", "method_endline": "2454", "method_complexity": {"method_NLOC": "6", "method_CCN": "5", "method_NToken": "78", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427", "deleted_lines": null, "method_info": {"method_name": "check_layer_normalization.npy_layer_norm", "method_params": "data,gamma,beta,axis,eps", "method_startline": "2417", "method_endline": "2427", "method_complexity": {"method_NLOC": "11", "method_CCN": "3", "method_NToken": "122", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447", "deleted_lines": null, "method_info": {"method_name": "check_layer_normalization", "method_params": "in_shape,axis,eps,dtype", "method_startline": "2416", "method_endline": "2447", "method_complexity": {"method_NLOC": "21", "method_CCN": "2", "method_NToken": "297", "method_nesting_level": "0"}}}}}}}}