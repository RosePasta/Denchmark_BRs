{"BR": {"BR_id": "1002", "BR_author": "clampert", "BRopenT": "2018-12-06T15:45:25Z", "BRcloseT": "2018-12-06T16:57:43Z", "BR_text": {"BRsummary": "Assert fails on machine with more than 8 GPUs", "BRdescription": "\n I'm describing a problem of version 0.9.0.1-24-gef334e31, hash 'ef334e31c34bab909d2c7469d3905347a0d5137a' on machines with more than 8 GPUs.\n I ran the imagenet-resnet.py example with unmodified source on a 10 GPU machine:\n python3 mygit/tensorpack/examples/ResNet/imagenet-resnet.py --data /localhome/chl/ILSVRC2012/ -d 50 --batch 320\n The code fails, and with good reason:\n The class SyncMultiGPUTrainerReplicated will activate hierarchical mode for 8 GPUs or more:\n train/trainers.py:176      mode = 'hierarchical' if len(gpus) >= 8 else 'nccl'\n Afterwards, in the routine allreduce_grads_hierarchical will fail because an assert allows only exactly 8 GPUs:\n graph_builder/utils.py:187     assert num_gpu == 8, num_gpu\n <denchmark-h:hr></denchmark-h>\n \n <denchmark-h:h2>log of /dev/stderr</denchmark-h>\n \n Traceback (most recent call last):\n File \"mygit/tensorpack/examples/ResNet/imagenet-resnet.py\", line 148, in \n launch_train_with_config(config, trainer)\n File \"/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/train/interface.py\", line 87, in launch_train_with_config\n model._build_graph_get_cost, model.get_optimizer)\n File \"/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/utils/argtools.py\", line 176, in wrapper\n return func(*args, **kwargs)\n File \"/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/train/tower.py\", line 204, in setup_graph\n train_callbacks = self._setup_graph(input, get_cost_fn, get_opt_fn)\n File \"/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/train/trainers.py\", line 186, in _setup_graph\n self._make_get_grad_fn(input, get_cost_fn, get_opt_fn), get_opt_fn)\n File \"/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/graph_builder/training.py\", line 251, in build\n packed_grads, raw_devices, average=self._average)\n File \"/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/tfutils/scope_utils.py\", line 94, in wrapper\n return func(*args, **kwargs)\n File \"/nfs/scistore12/chlgrp/chl/.local/lib/python3.5/site-packages/tensorpack/graph_builder/utils.py\", line 187, in allreduce_grads_hierarchical\n assert num_gpu == 8, num_gpu\n AssertionError: 10\n <denchmark-h:hr></denchmark-h>\n \n <denchmark-h:h2>log of /dev/stdout</denchmark-h>\n \n \ufffd[32m[1206 16:21:16 @logger.py:85]\ufffd[0m Argv: mygit/tensorpack/examples/ResNet/imagenet-resnet.py --data /localhome/chl/ILSVRC2012/ -d 50 --batch 320\n \ufffd[32m[1206 16:21:16 @imagenet-resnet.py:64]\ufffd[0m Running on 10 towers. Batch size per tower: 32\n \ufffd[32m[1206 16:21:16 @fs.py:100]\ufffd[0m \ufffd[5m\ufffd[31mWRN\ufffd[0m Env var $TENSORPACK_DATASET not set, using /nfs/scistore12/chlgrp/chl/tensorpack_data for datasets.\n \ufffd[32m[1206 16:21:17 @imagenet_utils.py:106]\ufffd[0m \ufffd[5m\ufffd[31mWRN\ufffd[0m DataFlow may become the bottleneck when too few processes are used.\n \ufffd[32m[1206 16:21:17 @parallel.py:313]\ufffd[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n \ufffd[32m[1206 16:21:17 @ilsvrc.py:128]\ufffd[0m [ILSVRC12] Assuming directory /localhome/chl/ILSVRC2012/val has 'original' structure.\n \ufffd[32m[1206 16:21:18 @training.py:52]\ufffd[0m [DataParallel] Training a model of 10 towers.\n \ufffd[32m[1206 16:21:18 @interface.py:46]\ufffd[0m Automatically applying StagingInput on the DataFlow.\n \ufffd[32m[1206 16:21:18 @input_source.py:219]\ufffd[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n \ufffd[32m[1206 16:21:18 @training.py:112]\ufffd[0m Building graph for training tower 0 on device /gpu:0 ...\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m conv0 input: [None, 3, 224, 224]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m conv0 output: [None, 64, 112, 112]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m pool0 input: [None, 64, 112, 112]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m pool0 output: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block0/conv1 input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block0/conv1 output: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block0/conv2 input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block0/conv2 output: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block0/conv3 input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block0/conv3 output: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block0/convshortcut input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block0/convshortcut output: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block1/conv1 input: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block1/conv1 output: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block1/conv2 input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block1/conv2 output: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block1/conv3 input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block1/conv3 output: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block2/conv1 input: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block2/conv1 output: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block2/conv2 input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block2/conv2 output: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group0/block2/conv3 input: [None, 64, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group0/block2/conv3 output: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block0/conv1 input: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block0/conv1 output: [None, 128, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block0/conv2 input: [None, 128, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block0/conv2 output: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block0/conv3 input: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block0/conv3 output: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block0/convshortcut input: [None, 256, 56, 56]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block0/convshortcut output: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block1/conv1 input: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block1/conv1 output: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block1/conv2 input: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block1/conv2 output: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block1/conv3 input: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block1/conv3 output: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block2/conv1 input: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block2/conv1 output: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block2/conv2 input: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block2/conv2 output: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block2/conv3 input: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block2/conv3 output: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block3/conv1 input: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block3/conv1 output: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block3/conv2 input: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block3/conv2 output: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group1/block3/conv3 input: [None, 128, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group1/block3/conv3 output: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block0/conv1 input: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block0/conv1 output: [None, 256, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block0/conv2 input: [None, 256, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block0/conv2 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block0/conv3 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block0/conv3 output: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block0/convshortcut input: [None, 512, 28, 28]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block0/convshortcut output: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block1/conv1 input: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block1/conv1 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block1/conv2 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block1/conv2 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block1/conv3 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block1/conv3 output: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block2/conv1 input: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block2/conv1 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block2/conv2 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block2/conv2 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block2/conv3 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block2/conv3 output: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block3/conv1 input: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block3/conv1 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block3/conv2 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block3/conv2 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block3/conv3 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:129]\ufffd[0m group2/block3/conv3 output: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:18 @registry.py:121]\ufffd[0m group2/block4/conv1 input: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group2/block4/conv1 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group2/block4/conv2 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group2/block4/conv2 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group2/block4/conv3 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group2/block4/conv3 output: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group2/block5/conv1 input: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group2/block5/conv1 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group2/block5/conv2 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group2/block5/conv2 output: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group2/block5/conv3 input: [None, 256, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group2/block5/conv3 output: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block0/conv1 input: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block0/conv1 output: [None, 512, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block0/conv2 input: [None, 512, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block0/conv2 output: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block0/conv3 input: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block0/conv3 output: [None, 2048, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block0/convshortcut input: [None, 1024, 14, 14]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block0/convshortcut output: [None, 2048, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block1/conv1 input: [None, 2048, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block1/conv1 output: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block1/conv2 input: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block1/conv2 output: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block1/conv3 input: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block1/conv3 output: [None, 2048, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block2/conv1 input: [None, 2048, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block2/conv1 output: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block2/conv2 input: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block2/conv2 output: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m group3/block2/conv3 input: [None, 512, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m group3/block2/conv3 output: [None, 2048, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m gap input: [None, 2048, 7, 7]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m gap output: [None, 2048]\n \ufffd[32m[1206 16:21:19 @registry.py:121]\ufffd[0m linear input: [None, 2048]\n \ufffd[32m[1206 16:21:19 @registry.py:129]\ufffd[0m linear output: [None, 1000]\n \ufffd[32m[1206 16:21:19 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:19 @regularize.py:19]\ufffd[0m The following tensors will be regularized: conv0/W:0, group0/block0/conv1/W:0, group0/block0/conv2/W:0, group0/block0/conv3/W:0, group0/block0/convshortcut/W:0, group0/block1/conv1/W:0, group0/block1/conv2/W:0, group0/block1/conv3/W:0, group0/block2/conv1/W:0, group0/block2/conv2/W:0, group0/block2/conv3/W:0, group1/block0/conv1/W:0, group1/block0/conv2/W:0, group1/block0/conv3/W:0, group1/block0/convshortcut/W:0, group1/block1/conv1/W:0, group1/block1/conv2/W:0, group1/block1/conv3/W:0, group1/block2/conv1/W:0, group1/block2/conv2/W:0, group1/block2/conv3/W:0, group1/block3/conv1/W:0, group1/block3/conv2/W:0, group1/block3/conv3/W:0, group2/block0/conv1/W:0, group2/block0/conv2/W:0, group2/block0/conv3/W:0, group2/block0/convshortcut/W:0, group2/block1/conv1/W:0, group2/block1/conv2/W:0, group2/block1/conv3/W:0, group2/block2/conv1/W:0, group2/block2/conv2/W:0, group2/block2/conv3/W:0, group2/block3/conv1/W:0, group2/block3/conv2/W:0, group2/block3/conv3/W:0, group2/block4/conv1/W:0, group2/block4/conv2/W:0, group2/block4/conv3/W:0, group2/block5/conv1/W:0, group2/block5/conv2/W:0, group2/block5/conv3/W:0, group3/block0/conv1/W:0, group3/block0/conv2/W:0, group3/block0/conv3/W:0, group3/block0/convshortcut/W:0, group3/block1/conv1/W:0, group3/block1/conv2/W:0, group3/block1/conv3/W:0, group3/block2/conv1/W:0, group3/block2/conv2/W:0, group3/block2/conv3/W:0, linear/W:0\n \ufffd[32m[1206 16:21:20 @training.py:112]\ufffd[0m Building graph for training tower 1 on device /gpu:1 ...\n \ufffd[32m[1206 16:21:21 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:22 @training.py:112]\ufffd[0m Building graph for training tower 2 on device /gpu:2 ...\n \ufffd[32m[1206 16:21:23 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:24 @training.py:112]\ufffd[0m Building graph for training tower 3 on device /gpu:3 ...\n \ufffd[32m[1206 16:21:25 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:26 @training.py:112]\ufffd[0m Building graph for training tower 4 on device /gpu:4 ...\n \ufffd[32m[1206 16:21:27 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:28 @training.py:112]\ufffd[0m Building graph for training tower 5 on device /gpu:5 ...\n \ufffd[32m[1206 16:21:29 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:30 @training.py:112]\ufffd[0m Building graph for training tower 6 on device /gpu:6 ...\n \ufffd[32m[1206 16:21:31 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:32 @training.py:112]\ufffd[0m Building graph for training tower 7 on device /gpu:7 ...\n \ufffd[32m[1206 16:21:33 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:34 @training.py:112]\ufffd[0m Building graph for training tower 8 on device /gpu:8 ...\n \ufffd[32m[1206 16:21:35 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:36 @training.py:112]\ufffd[0m Building graph for training tower 9 on device /gpu:9 ...\n \ufffd[32m[1206 16:21:37 @regularize.py:90]\ufffd[0m regularize_cost() found 54 variables to regularize.\n \ufffd[32m[1206 16:21:38 @utils.py:360]\ufffd[0m Will pack 161 gradients of total dimension=25557032 into 10 splits.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "clampert", "commentT": "2018-12-06T16:58:19Z", "comment_text": "\n \t\tThe mode was now changed to \"nccl\".\n Actually I don't have such machine to test if \"nccl\" works for 10 GPUs. Let me know if anything goes wrong.\n \t\t"}}}, "commit": {"commit_id": "bb2262de517fa0ffef9cf13dde1fe9a1fe9cb093", "commit_author": "Yuxin Wu", "commitT": "2018-12-06 08:57:29-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorpack\\dataflow\\imgaug\\base.py", "file_new_name": "tensorpack\\dataflow\\imgaug\\base.py", "file_complexity": {"file_NLOC": "102", "file_CCN": "34", "file_NToken": "739"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "52", "deleted_lines": "52,53", "method_info": {"method_name": "augment_return_params", "method_params": "self,d", "method_startline": "45", "method_endline": "54", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "15", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorpack\\train\\trainers.py", "file_new_name": "tensorpack\\train\\trainers.py", "file_complexity": {"file_NLOC": "305", "file_CCN": "46", "file_NToken": "1701"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "168,177", "deleted_lines": "176", "method_info": {"method_name": "__init__", "method_params": "self,gpus,average,mode,use_nccl", "method_startline": "159", "method_endline": "181", "method_complexity": {"method_NLOC": "10", "method_CCN": "5", "method_NToken": "92", "method_nesting_level": "1"}}}}}}}}