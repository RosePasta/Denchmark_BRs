{"BR": {"BR_id": "2503", "BR_author": "iamtrask", "BRopenT": "2019-08-14T12:55:13Z", "BRcloseT": "2019-09-10T16:03:00Z", "BR_text": {"BRsummary": "URGENT: bug in encrypted autograd", "BRdescription": "\n Describe the bug\n For some reason, calling loss.backward() on a pointer using normal autograd then breaks our ability to call loss.backward() on an encrypted loss variable even on two totally separate examples.\n To Reproduce\n <denchmark-code>import torch\n import torch as th\n import torch.nn as nn\n import torch.nn.functional as F\n import torch.optim as optim\n import syft as sy\n \n # Set everything up\n hook = sy.TorchHook(torch) \n \n big_hospital = sy.VirtualWorker(hook, id=\"big_hospital2\")\n small_hospital = sy.VirtualWorker(hook, id=\"small_hospital2\")\n crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider2\")\n \n # A Toy Model\n class Net(nn.Module):\n     def __init__(self):\n         super(Net, self).__init__()\n         self.fc1 = nn.Linear(2, 2)\n         self.fc2 = nn.Linear(2, 1)\n \n     def forward(self, x):\n         x = self.fc1(x)\n         x = F.relu(x)\n         x = self.fc2(x)\n         return x\n     \n def federated():\n     # A Toy Dataset\n     data = th.tensor([[0,0],[0,1],[1,0],[1,1.]])\n     target = th.tensor([[0],[0],[1],[1.]])\n \n     model = Net()\n \n     # Training Logic\n     opt = optim.SGD(params=model.parameters(),lr=0.1)\n \n     data = data.send(big_hospital)\n     target = target.send(big_hospital)\n \n     # NEW) send model to correct worker\n     model.send(data.location)\n \n     # 1) erase previous gradients (if they exist)\n     opt.zero_grad()\n \n     # 2) make a prediction\n     pred = model(data)\n \n     # 3) calculate how much we missed\n     loss = ((pred - target)**2).sum()\n \n     # 4) figure out which weights caused us to miss\n     loss.backward()\n     \n     print(\"Done!\")\n     \n def encrypted():\n     # A Toy Dataset\n     data2 = th.tensor([[0,0],[0,1],[1,0],[1,1.]])\n     target2 = th.tensor([[0],[0],[1],[1.]])\n \n     model2 = Net()\n \n     # We encode everything\n     data2 = data2.fix_precision().share(big_hospital, small_hospital, crypto_provider=crypto_provider, requires_grad=True)\n     target2 = target2.fix_precision().share(big_hospital, small_hospital, crypto_provider=crypto_provider, requires_grad=True)\n     model2 = model2.fix_precision().share(big_hospital, small_hospital, crypto_provider=crypto_provider, requires_grad=True)\n \n     opt2 = optim.SGD(params=model2.parameters(),lr=0.1).fix_precision()\n \n \n     # 1) erase previous gradients (if they exist)\n     opt2.zero_grad()\n \n     # 2) make a prediction\n     pred2 = model2(data2)\n \n     # 3) calculate how much we missed\n     loss2 = ((pred2 - target2)**2).sum()\n \n     # 4) figure out which weights caused us to miss\n     loss2.backward()\n \n #     # 5) change those weights\n #     opt2.step()\n \n #     # 6) print our progress\n #     print(loss2.get().float_precision())\n         \n     print(\"Done\")\n     \n run_broken = True\n \n # make sure to re-start your jupyter notebook / environment with each test.\n if(run_broken):\n     # Breaks\n     federated()\n     encrypted() # breaks here - something about loss2.backward() causes the federated() demo to break \n else:\n     # Works fine\n     encrypted()\n     federated()\n </denchmark-code>\n \n Throws error:\n <denchmark-code>---------------------------------------------------------------------------\n AttributeError                            Traceback (most recent call last)\n <ipython-input-1-db6dbbeffaa2> in <module>()\n     100     # Breaks\n     101     federated()\n --> 102     encrypted() # breaks here - something about loss2.backward() causes the federated() demo to break\n     103 else:\n     104     # Works fine\n \n <ipython-input-1-db6dbbeffaa2> in encrypted()\n      84 \n      85     # 4) figure out which weights caused us to miss\n ---> 86     loss2.backward()\n      87 \n      88 #     # 5) change those weights\n \n /Users/atrask/anaconda/lib/python3.6/site-packages/syft-0.1.22a1-py3.6.egg/syft/frameworks/torch/hook/hook.py in overloaded_native_method(self, *args, **kwargs)\n     683                 # Put back the wrappers where needed\n     684                 response = syft.frameworks.torch.hook_args.hook_response(\n --> 685                     method_name, response, wrap_type=type(self), new_self=self\n     686                 )\n     687 \n \n /Users/atrask/anaconda/lib/python3.6/site-packages/syft-0.1.22a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py in hook_response(attr, response, wrap_type, wrap_args, new_self)\n     243         response_hook_function = hook_method_response_functions[attr_id]\n     244         # Try running it\n --> 245         new_response = response_hook_function(response)\n     246 \n     247     except (IndexError, KeyError, AssertionError):  # Update the function in cas of an error\n \n /Users/atrask/anaconda/lib/python3.6/site-packages/syft-0.1.22a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py in <lambda>(x)\n     502         f = many_fold\n     503 \n --> 504     return lambda x: f(lambdas, x)\n     505 \n     506 \n \n /Users/atrask/anaconda/lib/python3.6/site-packages/syft-0.1.22a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py in two_fold(lambdas, args, **kwargs)\n     520 \n     521 def two_fold(lambdas, args, **kwargs):\n --> 522     return lambdas[0](args[0], **kwargs), lambdas[1](args[1], **kwargs)\n     523 \n     524 \n \n /Users/atrask/anaconda/lib/python3.6/site-packages/syft-0.1.22a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py in <lambda>(i)\n     480         if isinstance(r, (list, tuple))  # if the rule is a list or tuple.\n     481         # Last if not, rule is probably == 1 so use type to return the right transformation.\n --> 482         else lambda i: backward_func[wrap_type](i, **wrap_args)\n     483         for a, r in zip(response, rules)  # And do this for all the responses / rules provided\n     484     ]\n \n /Users/atrask/anaconda/lib/python3.6/site-packages/syft-0.1.22a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py in <lambda>(i)\n      73 backward_func = {\n      74     TorchTensor: lambda i: i.wrap(),\n ---> 75     torch.Tensor: lambda i: i.wrap(),\n      76     torch.nn.Parameter: lambda i: torch.nn.Parameter(data=i),\n      77     PointerTensor: lambda i: i,\n \n AttributeError: 'NoneType' object has no attribute 'wrap'\n </denchmark-code>\n \n Additional context\n latest version of PySyft from PyPI ('0.1.22a1')\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "iamtrask", "commentT": "2019-08-14T13:04:22Z", "comment_text": "\n \t\tIs this the error you get?\n <denchmark-code>File \"PySyft/syft/frameworks/torch/hook/hook_args.py\", line 75, in <lambda>\n     torch.Tensor: lambda i: i.wrap()\n AttributeError: 'NoneType' object has no attribute 'wrap'\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "iamtrask", "commentT": "2019-08-14T13:04:31Z", "comment_text": "\n \t\tYup\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "iamtrask", "commentT": "2019-08-14T13:09:20Z", "comment_text": "\n \t\tI will try what happens if you force the function to not be taken from the dictionary in the hook. There might be a conflict there.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "iamtrask", "commentT": "2019-08-14T13:10:38Z", "comment_text": "\n \t\tYou rock. Thank you <denchmark-link:https://github.com/midokura-silvia>@midokura-silvia</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "iamtrask", "commentT": "2019-08-14T13:14:55Z", "comment_text": "\n \t\t<denchmark-code>    # Try this\n     federated()\n     \n     sy.frameworks.torch.hook.hook_args.hook_method_args_functions = {}\n     sy.frameworks.torch.hook.hook_args.hook_method_response_functions = {}\n     sy.frameworks.torch.hook.hook_args.get_tensor_type_functions = {}\n \n     encrypted() # runs through\n </denchmark-code>\n \n \t\t"}}}, "commit": {"commit_id": "241c859a3d04982abe286670b48550af3a5f12ac", "commit_author": "Th\u00e9o Ryffel", "commitT": "2019-09-10 18:02:59+02:00", "commit_complexity": {"commit_NLOC": "0.7142857142857143", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "syft\\frameworks\\torch\\hook\\hook_args.py", "file_new_name": "syft\\frameworks\\torch\\hook\\hook_args.py", "file_complexity": {"file_NLOC": "56", "file_CCN": "0", "file_NToken": "256"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "58", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "syft\\frameworks\\torch\\tensors\\interpreters\\autograd.py", "file_new_name": "syft\\frameworks\\torch\\tensors\\interpreters\\autograd.py", "file_complexity": {"file_NLOC": "222", "file_CCN": "52", "file_NToken": "1323"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "258,259,260,261,262,263,264,265,266,267,268,269,270", "deleted_lines": "258,259,260,261", "method_info": {"method_name": "get", "method_params": "self", "method_startline": "255", "method_endline": "271", "method_complexity": {"method_NLOC": "11", "method_CCN": "3", "method_NToken": "56", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "test\\torch\\hook\\test_hook_args.py", "file_new_name": "test\\torch\\hook\\test_hook_args.py", "file_complexity": {"file_NLOC": "64", "file_CCN": "7", "file_NToken": "680"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113", "deleted_lines": null, "method_info": {"method_name": "test_backward_multiple_use.encrypted", "method_params": "", "method_startline": "75", "method_endline": "113", "method_complexity": {"method_NLOC": "22", "method_CCN": "1", "method_NToken": "243", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "36,37,38", "deleted_lines": null, "method_info": {"method_name": "test_backward_multiple_use.forward", "method_params": "self,x", "method_startline": "36", "method_endline": "38", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "17", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116", "deleted_lines": null, "method_info": {"method_name": "test_backward_multiple_use", "method_params": "workers", "method_startline": "19", "method_endline": "116", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "53", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "32,33,34", "deleted_lines": null, "method_info": {"method_name": "test_backward_multiple_use.__init__", "method_params": "self", "method_startline": "32", "method_endline": "34", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "2"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73", "deleted_lines": null, "method_info": {"method_name": "test_backward_multiple_use.federated", "method_params": "", "method_startline": "40", "method_endline": "73", "method_complexity": {"method_NLOC": "15", "method_CCN": "1", "method_NToken": "190", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\torch\\tensors\\test_autograd.py", "file_new_name": "test\\torch\\tensors\\test_autograd.py", "file_complexity": {"file_NLOC": "450", "file_CCN": "34", "file_NToken": "5835"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499", "deleted_lines": null, "method_info": {"method_name": "test_share_with_requires_grad", "method_params": "workers", "method_startline": "476", "method_endline": "499", "method_complexity": {"method_NLOC": "15", "method_CCN": "3", "method_NToken": "126", "method_nesting_level": "0"}}}}}}}}