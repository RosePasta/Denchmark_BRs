{"BR": {"BR_id": "67", "BR_author": "qiulesun", "BRopenT": "2018-06-09T01:28:43Z", "BRcloseT": "2018-09-26T22:59:45Z", "BR_text": {"BRsummary": "No module named cpp_extension", "BRdescription": "\n Hi, I got the error named No module named cpp_extension (from torch.utils.cpp_extension import load) when I run the quick demo <denchmark-link:http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package>http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package</denchmark-link>\n . The version of python and torch are 2.7 and 0.3.1 respectively. How can I handle it?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "qiulesun", "commentT": "2018-06-09T18:18:37Z", "comment_text": "\n \t\t0.3.1 is way too old. Please install PyTorch master branch > 0.5.0\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "qiulesun", "commentT": "2018-06-15T06:22:06Z", "comment_text": "\n \t\tThe version of python and torch are updated to 3.6 and 0.4.0 respectively. Follow the link you provided <denchmark-link:https://www.claudiokuenzler.com/blog/756/install-newer-ninja-build-tools-ubuntu-14.04-trusty#.WxYrvFMvzJw>https://www.claudiokuenzler.com/blog/756/install-newer-ninja-build-tools-ubuntu-14.04-trusty#.WxYrvFMvzJw</denchmark-link>\n , I install ninja 1.8.2. However, when I run again the quick demo <denchmark-link:http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package>http://hangzh.com/PyTorch-Encoding/experiments/segmentation.html#install-package</denchmark-link>\n , I got another error. How can I solve it? I believe your papers and code can make me interested in semantic segmentation tasks.\n root@hh-Z97X-UD3H:/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master# python quick_demo.py\n Traceback (most recent call last):\n File \"/usr/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 576, in _build_extension_module\n ['ninja', '-v'], stderr=subprocess.STDOUT, cwd=build_directory)\n File \"/usr/anaconda3/lib/python3.6/subprocess.py\", line 336, in check_output\n **kwargs).stdout\n File \"/usr/anaconda3/lib/python3.6/subprocess.py\", line 418, in run\n output=stdout, stderr=stderr)\n subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n File \"demo.py\", line 2, in \n import encoding\n File \"/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/init.py\", line 13, in \n from . import nn, functions, dilated, parallel, utils, models, datasets\n File \"/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/nn/init.py\", line 12, in \n from .encoding import *\n File \"/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/nn/encoding.py\", line 18, in \n from ..functions import scaledL2, aggregate\n File \"/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/functions/init.py\", line 2, in \n from .encoding import *\n File \"/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/functions/encoding.py\", line 13, in \n from .. import lib\n File \"/media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/init.py\", line 12, in \n ], build_directory=cpu_path, verbose=False)\n File \"/usr/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 501, in load\n _build_extension_module(name, build_directory)\n File \"/usr/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 582, in _build_extension_module\n name, error.output.decode()))\n RuntimeError: Error building extension 'enclib_cpu': [1/2] c++ -MMD -MF roi_align_cpu.o.d -DTORCH_EXTENSION_NAME=enclib_cpu -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/anaconda3/include/python3.6m -fPIC -std=c++11 -c /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp -o roi_align_cpu.o\n FAILED: roi_align_cpu.o\n c++ -MMD -MF roi_align_cpu.o.d -DTORCH_EXTENSION_NAME=enclib_cpu -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/anaconda3/include/python3.6m -fPIC -std=c++11 -c /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp -o roi_align_cpu.o\n In file included from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/ArrayRef.h:18:0,\n from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/ScalarType.h:5,\n from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Scalar.h:11,\n from /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/ATen.h:6,\n from /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:1:\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp: In function \u2018at::Tensor ROIAlignForwardCPU(const at::Tensor&, const at::Tensor&, int64_t, int64_t, double, int64_t)\u2019:\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:388:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(input.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:388:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(input.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:389:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:389:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:390:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(input.ndimension() == 4);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:390:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(input.ndimension() == 4);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:391:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.ndimension() == 2);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:391:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.ndimension() == 2);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:392:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.size(1) == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:392:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.size(1) == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:404:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(roi_cols == 4 || roi_cols == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:404:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(roi_cols == 4 || roi_cols == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:409:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(input.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:409:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(input.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:410:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:410:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp: In function \u2018at::Tensor ROIAlignBackwardCPU(const at::Tensor&, const at::Tensor&, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, double, int64_t)\u2019:\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:444:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:444:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:445:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.ndimension() == 2);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:445:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.ndimension() == 2);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:446:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.size(1) == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:446:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.size(1) == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:451:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(roi_cols == 4 || roi_cols == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:451:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(roi_cols == 4 || roi_cols == 5);\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:18: error: expected primary-expression before \u2018(\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:456:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:281:62: error: expected primary-expression before \u2018)\u2019 token\n throw at::Error({func, FILE, LINE}, VA_ARGS)\n ^\n /usr/anaconda3/lib/python3.6/site-packages/torch/lib/include/ATen/Error.h:285:5: note: in expansion of macro \u2018AT_ERROR\u2019\n AT_ERROR(VA_ARGS);   \n ^\n /media/hh/0bfd0eaf-cf46-48b3-915a-aa317b67d9ec/PyTorch-Encoding/PyTorch-Encoding-master/encoding/lib/cpu/roi_align_cpu.cpp:456:3: note: in expansion of macro \u2018AT_ASSERT\u2019\n AT_ASSERT(bottom_rois.is_contiguous());\n ^\n ninja: build stopped: subcommand failed.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "qiulesun", "commentT": "2018-06-15T06:26:36Z", "comment_text": "\n \t\tThis package depend on a slightly higher version than PyTroch 0.4.0. Please follow the instructions to install pytorch from source <denchmark-link:https://github.com/pytorch/pytorch#from-source>https://github.com/pytorch/pytorch#from-source</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "qiulesun", "commentT": "2018-06-19T07:53:14Z", "comment_text": "\n \t\tIn your paper, the sentence ''The ground truth labels for SE-loss are generated by \u201cunique\u201d operation finding the categories presented in the given ground-truth segmentation mask.'' means that every input image has multiple labels. As far as I know, the binary cross entroy loss can handle binary class or multi-class task rather than multi-labels.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "qiulesun", "commentT": "2018-06-19T13:26:54Z", "comment_text": "\n \t\tI didn\u2019t get the difference between multi class and multi labels. Could you please explain in detail?\n Btw, the NN already has sigmoid activation\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "qiulesun", "commentT": "2018-06-20T00:58:53Z", "comment_text": "\n \t\tMulticlass classification means a classification task with more than two classes; e.g., classify a set of images of fruits which may be oranges, apples, or pears. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time.\n Multilabel classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these.\n I note that the  NN has sigmoid activation. I hold the question that, in your case, the input image has multiple labels or one.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "qiulesun", "commentT": "2018-06-20T01:27:55Z", "comment_text": "\n \t\tThe presence of the object categories is indeed a multi-label task. Each category is predicted independently using a binary prediction. I hope it can address your concern.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "qiulesun", "commentT": "2018-06-20T01:32:39Z", "comment_text": "\n \t\tPlease refer to the docs for binary cross entropy loss <denchmark-link:https://pytorch.org/docs/stable/nn.html?highlight=bceloss#torch.nn.BCELoss>https://pytorch.org/docs/stable/nn.html?highlight=bceloss#torch.nn.BCELoss</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "qiulesun", "commentT": "2018-06-20T03:48:33Z", "comment_text": "\n \t\tIn binary classification, the number of classes equals 2. The object categories in an input image are more than 2 (figure 2 in paper). So I don't understand why binary cross entropy loss is empolyed and ''Each category is predicted independently using a binary prediction. ''\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "qiulesun", "commentT": "2018-06-20T04:42:55Z", "comment_text": "\n \t\tEach category is a binary classification problem. For 150 categories, there 150 individual binary classification problem. I hope this explanation  is clear enough. If you still have difficulties, feel free to ask questions in Chinese.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "qiulesun", "commentT": "2018-06-20T07:31:07Z", "comment_text": "\n \t\tThank you for your patience.  Your explanation is clear. The binary cross entropy loss can handle the multi-label classification task. Its target is something like [1,0,0,1,0...]. Sigmoid, unlike softmax don't give probability distribution around NCLASS as output, but independent probabilities.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "qiulesun", "commentT": "2018-06-20T13:23:14Z", "comment_text": "\n \t\tYou\u2019re welcome. That is correct.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "qiulesun", "commentT": "2018-06-24T07:58:39Z", "comment_text": "\n \t\tI am really sorroy for disturbing you again. I shouldn't ask the question about installation PyTorch from source, but I have no idea to solve it. Can you help me to fix it out?\n System Info\uff1a\n How you installed PyTorch (conda, pip, source): source\n Build command you used (if compiling from source): python setup.py install\n OS: ubuntu14.04\n PyTorch version: master\n Python version: 3.6\n CUDA/cuDNN version: cuda8.0+cudnn5.0\n GPU models and configuration: GTX1080Ti\n GCC version (if compiling from source): 4.9.4\n CMake version: 3.7.2\n ############################################################\n Issue description\uff1a\n 3 errors detected in the compilation of \"/tmp/tmpxft_00002a14_00000000-7_THCTensorMath.cpp1.ii\".\n CMake Error at caffe2_gpu_generated_THCTensorMath.cu.o.Release.cmake:279 (message):\n Error generating file\n /media/hh/pytorch_dir/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/./caffe2_gpu_generated_THCTensorMath.cu.o\n make[2]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCTensorMath.cu.o] Error 1\n make[1]: *** [caffe2/CMakeFiles/caffe2_gpu.dir/all] Error 2\n make: *** [all] Error 2\n Failed to run 'bash tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn nccl caffe2 nanopb libshm gloo THD c10d'\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "qiulesun", "commentT": "2018-06-24T16:51:34Z", "comment_text": "\n \t\tTry install the dependencies as following first:\n <denchmark-code>export CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n \n # Install basic dependencies\n conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing\n conda install -c mingfeima mkldnn\n \n # Add LAPACK support for the GPU\n conda install -c pytorch magma-cuda80 # or magma-cuda90 if CUDA 9\n </denchmark-code>\n \n You may want to ask on PyTorch repo for further help\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "qiulesun", "commentT": "2018-06-26T12:20:01Z", "comment_text": "\n \t\tAre the models you released (model_zoo.py) all trained with two Context Encoding Modules?  Can you detail the MS evaluation in the table 1?\n <denchmark-code>models = {\n      'encnet_resnet50_pcontext': get_encnet_resnet50_pcontext,\n     'encnet_resnet101_pcontext': get_encnet_resnet101_pcontext,\n     'encnet_resnet50_ade': get_encnet_resnet50_ade,\n     }\n </denchmark-code>\n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "qiulesun", "commentT": "2018-06-26T15:16:17Z", "comment_text": "\n \t\tWe only use one Context Encoding Module now, which is more efficient and makes the model compatible with EncNetV2.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "qiulesun", "commentT": "2018-07-01T06:30:43Z", "comment_text": "\n \t\tCan Ubuntu, Mac and Windows os all run the released codes?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "qiulesun", "commentT": "2018-07-01T19:16:58Z", "comment_text": "\n \t\tIt mainly depends on the PyTorch. If the pytorch is compiled successfully on your system, there won't be a problem. I am using both Mac and Ubuntu. Note that PyTorch master branch is required.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "qiulesun", "commentT": "2018-07-03T03:03:14Z", "comment_text": "\n \t\tThe comand (e.g., CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset PContext --model EncNet --aux --se-loss --backbone resnet101)  for training the model means training resnet101 from scratch or finetuning resnet101?\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "qiulesun", "commentT": "2018-07-03T03:10:46Z", "comment_text": "\n \t\tresnet101 is pretrained from ImageNet.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "qiulesun", "commentT": "2018-07-03T09:46:27Z", "comment_text": "\n \t\tI used the comand (CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset PContext --model EncNet --aux --se-loss) for training the model resnet50. However, when it ran to the epoch12, I stopped it. Next, I restart it and find unluckily it has ran from epoch0 rather than epoch12. What should I do to run it from epoch12?\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "qiulesun", "commentT": "2018-07-03T14:50:26Z", "comment_text": "\n \t\tPlease resume by adding command --resume path/to/checkpoint.pth.tar\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "qiulesun", "commentT": "2018-07-06T03:21:47Z", "comment_text": "\n \t\tThank you. I have another interest. When does PyTroch 0.4.0 meets the requirements of running released code ?\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "qiulesun", "commentT": "2018-07-06T18:34:43Z", "comment_text": "\n \t\tThis package won't be compatible with PyTroch 0.4.0, but it will be compatible with next stable release.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "qiulesun", "commentT": "2018-07-13T02:32:31Z", "comment_text": "\n \t\tQuestion about selayer, why does the selayer have no sigmoid activation function?\n (encmodule): EncModule(\n (encoding): Sequential(\n (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n (2): ReLU(inplace)\n (3): Encoding(N x 512=>32x512)\n (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n (5): ReLU(inplace)\n (6): Mean()\n )\n (fc): Sequential(\n (0): Linear(in_features=512, out_features=512, bias=True)\n (1): Sigmoid()\n )\n (selayer): Linear(in_features=512, out_features=59, bias=True)\n )\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "qiulesun", "commentT": "2018-07-13T02:35:51Z", "comment_text": "\n \t\tThat is the prediction layer for minimizing SE-Loss.\n The  function is applied during the loss calculation <denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/customize.py#L65>https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/customize.py#L65</denchmark-link>\n \n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "qiulesun", "commentT": "2018-08-03T17:22:58Z", "comment_text": "\n \t\tSorry for bothering you agian, I have no idea with next errors when I run CUDA_VISIBLE_DEVICES=0,1 python train.py --dataset pcontext --model encnet --aux --se-loss.\n And import encoding gets similar errors.\n OS: ubuntu14.04\n Pytorch version: 0.5.0 (from source)\n Python version: 3.6\n CUDA: 8.0\n cudnn: 6.0.21\n GPU: 2 1080\n /usr/local/anaconda3/bin/python3.6 /media/cv-pc-00/QL_480G/sql/pytorch_dir/PyTorch-Encoding/experiments/segmentation/train.py --dataset PContext --model EncNet --se-loss\n \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n Traceback (most recent call last):\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 742, in _build_extension_module\n ['ninja', '-v'], stderr=subprocess.STDOUT, cwd=build_directory)\n File \"/usr/local/anaconda3/lib/python3.6/subprocess.py\", line 336, in check_output\n **kwargs).stdout\n File \"/usr/local/anaconda3/lib/python3.6/subprocess.py\", line 418, in run\n output=stdout, stderr=stderr)\n subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n File \"/media/cv-pc-00/QL_480G/sql/pytorch_dir/PyTorch-Encoding/experiments/segmentation/train.py\", line 17, in \n import encoding.utils as utils\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/encoding/init.py\", line 13, in \n from . import nn, functions, dilated, parallel, utils, models, datasets\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/encoding/nn/init.py\", line 12, in \n from .encoding import *\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/encoding/nn/encoding.py\", line 18, in \n from ..functions import scaledL2, aggregate, pairwise_cosine\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/encoding/functions/init.py\", line 2, in \n from .encoding import *\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/encoding/functions/encoding.py\", line 14, in \n from .. import lib\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/init.py\", line 20, in \n ], build_directory=gpu_path, verbose=False)\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 496, in load\n with_cuda=with_cuda)\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 664, in _jit_compile\n _build_extension_module(name, build_directory)\n File \"/usr/local/anaconda3/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 748, in _build_extension_module\n name, error.output.decode()))\n RuntimeError: Error building extension 'enclib_gpu': [1/4] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu -o roi_align_kernel.cuda.o\n FAILED: roi_align_kernel.cuda.o\n /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu -o roi_align_kernel.cuda.o\n nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(373): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(373): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(420): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/roi_align_kernel.cu(420): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n 4 errors detected in the compilation of \"/tmp/tmpxft_0000662c_00000000-7_roi_align_kernel.cpp1.ii\".\n [2/4] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu -o encoding_kernel.cuda.o\n FAILED: encoding_kernel.cuda.o\n /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu -o encoding_kernel.cuda.o\n nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(315): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(341): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(364): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/encoding_kernel.cu(391): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n 4 errors detected in the compilation of \"/tmp/tmpxft_00006623_00000000-7_encoding_kernel.cpp1.ii\".\n [3/4] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu -o syncbn_kernel.cuda.o\n FAILED: syncbn_kernel.cuda.o\n /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=enclib_gpu -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/TH -I/usr/local/anaconda3/lib/python3.6/site-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/local/anaconda3/include/python3.6m --compiler-options '-fPIC' -std=c++11 -c /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu -o syncbn_kernel.cuda.o\n nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(183): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(217): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(249): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n /usr/local/anaconda3/lib/python3.6/site-packages/encoding/lib/gpu/syncbn_kernel.cu(272): error: class \"at::Context\" has no member \"getCurrentCUDAStream\"\n 4 errors detected in the compilation of \"/tmp/tmpxft_00006627_00000000-7_syncbn_kernel.cpp1.ii\".\n ninja: build stopped: subcommand failed.\n Process finished with exit code 1\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "qiulesun", "commentT": "2018-08-03T17:40:59Z", "comment_text": "\n \t\tHi, That is because the PyTorch updates in backend.\n \n Could you change at::Context:: getCurrentCUDAStream  to cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n Also add #include <ATen/cuda/CUDAContext.h>\n \n This will be fixed in next version.\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "qiulesun", "commentT": "2018-08-04T08:42:53Z", "comment_text": "\n \t\tThanks for your attention. It does work! However, three warnings occur,  do that matter?\n \n \n /usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1940:\n UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.   warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n \n \n /usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1025:\n UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n \n \n /usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52:\n UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n warnings.warn(warning.format(ret))\n \n \n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "qiulesun", "commentT": "2018-08-05T17:52:39Z", "comment_text": "\n \t\tThe deprecate warning is okay for now.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "qiulesun", "commentT": "2018-08-07T03:08:49Z", "comment_text": "\n \t\tProblem with debugging the backward method of Function class\n Hi, aggregate(A, X, C) and scaledL2(X, C, S) in encoding.functions.encoding.py implement the forward and backwark of your custom function. I want to debug their forward and backwark and the pycharm-community-2018.1.4 I used on Ubuntu 16.04 LTS has allowed me debug the forward step by step. However, I could not debug backward function like forward equipped with 2 1080 GPU.\n Could you tell me is it possilbe and how to address it? (ps: for my own  custom functions based on your codes, I also face the same problem)\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "qiulesun", "commentT": "2018-08-07T17:54:45Z", "comment_text": "\n \t\tYou can directly call the backend function for debugging <denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/functions/encoding.py#L77>https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/functions/encoding.py#L77</denchmark-link>\n \n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "qiulesun", "commentT": "2018-08-08T13:40:24Z", "comment_text": "\n \t\tFor my special case, I want to run the codes with one GPU (ps: my machine is equipped with 2 GPUs), for example debugging the codes, etc.\n Do the codes support a single GPU operation even if the machine is equipped with 2 GPUs?\n Is the default multi GPU running if the machine is equipped with multiple  GPUs?\n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "qiulesun", "commentT": "2018-08-08T17:38:20Z", "comment_text": "\n \t\tCUDA_VISIBLE_DEVICES=0 python train.py ...\n \t\t"}, "comments_34": {"comment_id": 35, "comment_author": "qiulesun", "commentT": "2018-08-09T00:50:45Z", "comment_text": "\n \t\tQuestion 1\n I use pycharm-community-2018.1.4 to make it easier to debug the codes and CUDA_VISIBLE_DEVICES=0 --dataset PContext --model EncNet --se-loss is given in debug configurations.\n However, I get the error  train.py: error: unrecognized arguments: CUDA_VISIBLE_DEVICES=0\n When I use the pycharm-community-2018.1.4 to debug the codes with a single GPU, I should do what next ?\n Connected to pydev debugger (build 181.5087.37)\n usage: train.py [-h] [--model MODEL] [--backbone BACKBONE] [--dataset DATASET]\n [--data-folder DATA_FOLDER] [--workers N] [--aux] [--se-loss]\n [--epochs N] [--start_epoch N] [--batch-size N]\n [--test-batch-size N] [--lr LR] [--lr-scheduler LR_SCHEDULER]\n [--momentum M] [--weight-decay M] [--no-cuda] [--seed S]\n [--resume RESUME] [--checkname CHECKNAME]\n [--model-zoo MODEL_ZOO] [--ft] [--pre-class PRE_CLASS] [--ema]\n [--eval] [--no-val] [--test-folder TEST_FOLDER]\n train.py: error: unrecognized arguments: CUDA_VISIBLE_DEVICES=0\n Question 2\n args.lr = lrs[args.dataset.lower()] / 16 * args.batch_size in option.py means that the lr is relate to batch_size you give. Is that the lr not fixed depending on the batch_size (GPU memory)?\n In my experiments, I set the args.lr = lrs[args.dataset.lower()], is it reasonable and feasible, does it respect your paper and intentions?\n Question 3\n For multi-size evaluation, the 27th line base_size=576, crop_size=608 (base_size less than crop_size) in encoding/models/base.py should be base_size=608, crop_size=576?\n Previously, you set base_size=520, crop_size=480 and now you change them to base_size=576, crop_size=608. I hold the view that crop_size less than base_size seems reasonable. What settings should I follow to reproduce your results?\n I am looking forward to your reply.\n \t\t"}, "comments_35": {"comment_id": 36, "comment_author": "qiulesun", "commentT": "2018-08-09T16:41:27Z", "comment_text": "\n \t\tQ1: please use the terminal to launch the program.\n Q2: That is a kind of standard setting for LR. When increasing the batch size, people typically increase the LR accordingly.\n Q3. That is a bug. It will be fixed in next release.\n \t\t"}, "comments_36": {"comment_id": 37, "comment_author": "qiulesun", "commentT": "2018-08-09T17:43:41Z", "comment_text": "\n \t\tFor the Q2 above, due to the limited GPU memory, the batch size has to be small (typically less than 16) unfortunately. It means that  I have to use smaller LR according to the standard setting, i.e., args.lr = lrs[args.dataset.lower()] / 16 * args.batch_size ?\n \t\t"}, "comments_37": {"comment_id": 38, "comment_author": "qiulesun", "commentT": "2018-08-09T17:46:48Z", "comment_text": "\n \t\tYes. If the batch size is too small, the model will get worse result, because the working batch size for batch normalization is small.\n \t\t"}, "comments_38": {"comment_id": 39, "comment_author": "qiulesun", "commentT": "2018-08-10T09:36:22Z", "comment_text": "\n \t\tI only have 2 1080 GPUs with a total of 16G memory. The batch size is small less than 16 in my experiments. Can I alleviate this side effect (the model will get worse result you said) by using larger LR and set args.lr = lrs[args.dataset.lower()], independent of batch size?\n \t\t"}, "comments_39": {"comment_id": 40, "comment_author": "qiulesun", "commentT": "2018-08-10T19:01:38Z", "comment_text": "\n \t\tThe batch size matters for segmentation task, due to working batch size for the Synchronize Batch Normalization. For batch size =16 yields the best performance.\n \t\t"}, "comments_40": {"comment_id": 41, "comment_author": "qiulesun", "commentT": "2018-08-12T12:36:08Z", "comment_text": "\n \t\tWhat is the main difference between encoding.nn.BatchNorm1d and  encoding.nn.BatchNorm2d?\n \t\t"}, "comments_41": {"comment_id": 42, "comment_author": "qiulesun", "commentT": "2018-08-13T17:19:39Z", "comment_text": "\n \t\tsame as torch.nn.BatchNorm1d and  torch.nn.BatchNorm2d\n \t\t"}, "comments_42": {"comment_id": 43, "comment_author": "qiulesun", "commentT": "2018-08-24T03:16:41Z", "comment_text": "\n \t\tI have two questions.\n (1) For cos ans poly lr schedules, every batch (iter) has a different lr rather than them in one epoch has same lr. Is that right?\n (2) For cifar10 recognition, the scaling factor s_k is not learnt but randomly sampling from a uniform distribution between 0 and 1, which is different from segmentation tasks. Is that right?\n \t\t"}, "comments_43": {"comment_id": 44, "comment_author": "qiulesun", "commentT": "2018-09-19T09:04:56Z", "comment_text": "\n \t\tI'm sorry for disturbing you again.\n Your work is very encouraging to me. I notice that the scaled_l2 and aggregate opertors of the proposed encoding layer are implemented by C++  language.  Duo to I am not good at it, could you share the corresponding implementation using python code  if you want?\n \t\t"}, "comments_44": {"comment_id": 45, "comment_author": "qiulesun", "commentT": "2018-09-19T13:58:13Z", "comment_text": "\n \t\tWe change LR every iter.\n The cifar experiment use shake-out like regularization.\n Scaled L2 and aggregate are easy to implement in python, but that will be memory consuming.\n \t\t"}, "comments_45": {"comment_id": 46, "comment_author": "qiulesun", "commentT": "2018-09-21T03:00:47Z", "comment_text": "\n \t\tquestion 1:\n Sorry to ask the stupid question.\n The augmented pascal voc 2012 has 11533 images in trainval.txt rather than 10582 used in paper.  It's troubled me. And I do not get the information about how to augment the 1464 trainging images of pascal voc 2012 to result in 10582 ones. In other words, I do not get the relationship between the pascal voc 2012 and its augmented  version. Could I fortunately know your opinion?\n If you think this question is not worth answering, I can understand completely.\n \n As far as I known, Group norm (<denchmark-link:https://arxiv.org/pdf/1803.08494.pdf>https://arxiv.org/pdf/1803.08494.pdf</denchmark-link>\n ) is independent of batch size, much suitable for semantic segmentation task, which requires small batches constrained by memory consumption.\n Could you consider employing it in your updated version?\n \t\t"}, "comments_46": {"comment_id": 47, "comment_author": "qiulesun", "commentT": "2018-09-21T17:37:03Z", "comment_text": "\n \t\tQ1. For VOC experiments, first pretrained on COCO, then finetune on \"pascal_aug\" and finally on \"pascal_voc\". I am releasing the training detail for reproducing VOC experiments this weekend.\n Q2. Group Norm still has inferior performance comparing to BN. You can easily use that by changing the code a little bit.\n \t\t"}, "comments_47": {"comment_id": 48, "comment_author": "qiulesun", "commentT": "2018-09-24T13:55:12Z", "comment_text": "\n \t\t\n I see base_size=608 and crop_size=576 in the training log of EncNet_ResNet50_ADE, (<denchmark-link:https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log>https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log</denchmark-link>\n ), however, the base_size and crop_size are set to 520 and 480 respectively in <denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/datasets/base.py#L17>https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/datasets/base.py#L17</denchmark-link>\n .\n It's troubled me. Does the special case for ADE20K use base_size=608 and crop_size=576 and use  base_size=520 and crop_size=480 for PASCAL Context and PASCAL VOC12 ?\n \n Besides, base_size=576 and crop_size=608 in <denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/models/base.py#L27>https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/models/base.py#L27</denchmark-link>\n  is only to multiscale test ?\n \t\t"}, "comments_48": {"comment_id": 49, "comment_author": "qiulesun", "commentT": "2018-09-24T16:51:37Z", "comment_text": "\n \t\tThere are some bugs in existing code. I am updating them soon.\n \t\t"}, "comments_49": {"comment_id": 50, "comment_author": "qiulesun", "commentT": "2018-09-26T01:17:42Z", "comment_text": "\n \t\t\n As mentioned above,  there are some bugs in existing code. I still have a question.\n The EncNet_ResNet50_ADE achieves 79.9 pixAcc and 41.2 mIoU at the last row in the table  (<denchmark-link:https://hangzhang.org/PyTorch-Encoding/experiments/segmentation.html>https://hangzhang.org/PyTorch-Encoding/experiments/segmentation.html</denchmark-link>\n ), however, from the training log file (<denchmark-link:https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log>https://raw.githubusercontent.com/zhanghang1989/image-data/master/encoding/segmentation/logs/encnet_resnet50_ade.log</denchmark-link>\n )  I see that it obtains 78.0 pixAcc and 40.2 mIoU lower than the results you reported.\n Is this because you use the multi-scale testing strategy on ADE20K val set?  Or something else ?\n \t\t"}, "comments_50": {"comment_id": 51, "comment_author": "qiulesun", "commentT": "2018-09-26T17:12:44Z", "comment_text": "\n \t\tThe validation during the training is using center crop, only for monitoring the training process.\n \t\t"}}}, "commit": {"commit_id": "f8919197bd53b027b131a891dd7b917e615a6743", "commit_author": "Hang Zhang", "commitT": "2018-09-26 15:59:44-07:00", "commit_complexity": {"commit_NLOC": "0.4151436031331593", "commit_CCN": "0.6579634464751958", "commit_Nprams": "0.49477806788511747"}, "changed_files": {"file_0": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "docs\\source\\_static\\js\\hidebib.js", "file_complexity": {"file_NLOC": "38", "file_CCN": "10", "file_NToken": "231"}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\_templates\\layout.html", "file_new_name": "docs\\source\\_templates\\layout.html", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "6", "deleted_lines": "6"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\experiments\\segmentation.rst", "file_new_name": "docs\\source\\experiments\\segmentation.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "26,27,28,29,34,41,42,43,44,45,46,47,48,49,50,51,52,53,82,83,84,85,86,87,88,89,90,91,92,93,94,140,142,143,144,145,147", "deleted_lines": "30,37,38,39,40,41,42,43,44,45,119,121,122,123,125"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\experiments\\texture.rst", "file_new_name": "docs\\source\\experiments\\texture.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "20,22,23,24,25,26,27,34,42", "deleted_lines": "20,22,29,37,59,60"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\functions.rst", "file_new_name": "docs\\source\\functions.rst", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23,26", "deleted_lines": "23,26"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\__init__.py", "file_new_name": "encoding\\__init__.py", "file_complexity": {"file_NLOC": "3", "file_CCN": "0", "file_NToken": "24"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13", "deleted_lines": "13"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\datasets\\__init__.py", "file_new_name": "encoding\\datasets\\__init__.py", "file_complexity": {"file_NLOC": "17", "file_CCN": "1", "file_NToken": "85"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "2,7,10,15", "deleted_lines": null}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "encoding\\datasets\\ade20k.py", "file_new_name": "encoding\\datasets\\ade20k.py", "file_complexity": {"file_NLOC": "92", "file_CCN": "18", "file_NToken": "731"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "93,94,99,101,108,109,110", "deleted_lines": "104,105", "method_info": {"method_name": "_get_ade20k_pairs", "method_params": "folder,split", "method_startline": "72", "method_endline": "111", "method_complexity": {"method_NLOC": "25", "method_CCN": "3", "method_NToken": "207", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "61,62", "deleted_lines": "61,62", "method_info": {"method_name": "_mask_transform", "method_params": "self,mask", "method_startline": "60", "method_endline": "62", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "1"}}}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "encoding\\datasets\\base.py", "file_new_name": "encoding\\datasets\\base.py", "file_complexity": {"file_NLOC": "86", "file_CCN": "19", "file_NToken": "807"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "40,41", "deleted_lines": null, "method_info": {"method_name": "make_pred", "method_params": "self,x", "method_startline": "40", "method_endline": "41", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "13", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "78,79,80,81", "method_info": {"method_name": "_sync_transform", "method_params": "self,img,mask", "method_startline": "61", "method_endline": "99", "method_complexity": {"method_NLOC": "32", "method_CCN": "7", "method_NToken": "377", "method_nesting_level": "1"}}}}}, "file_9": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\datasets\\cityscapes.py", "file_complexity": {"file_NLOC": "148", "file_CCN": "34", "file_NToken": "1514"}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "encoding\\datasets\\coco.py", "file_new_name": "encoding\\datasets\\coco.py", "file_complexity": {"file_NLOC": "116", "file_CCN": "14", "file_NToken": "825"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "46,47", "deleted_lines": "39,41,44,46,47,48,51,52,53", "method_info": {"method_name": "__getitem__", "method_params": "self,index", "method_startline": "39", "method_endline": "61", "method_complexity": {"method_NLOC": "21", "method_CCN": "5", "method_NToken": "189", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "73,74", "deleted_lines": "71,72", "method_info": {"method_name": "_gen_seg_mask", "method_params": "self,target,h,w", "method_startline": "66", "method_endline": "81", "method_complexity": {"method_NLOC": "16", "method_CCN": "4", "method_NToken": "160", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "14", "deleted_lines": "13,14", "method_info": {"method_name": "__init__", "method_params": "self,root", "method_startline": "13", "method_endline": "14", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "35", "method_nesting_level": "1"}}}}}, "file_11": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "encoding\\datasets\\pascal_aug.py", "file_new_name": "encoding\\datasets\\pascal_aug.py", "file_complexity": {"file_NLOC": "63", "file_CCN": "10", "file_NToken": "562"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "18,19", "deleted_lines": "18,19", "method_info": {"method_name": "__init__", "method_params": "self,root,split,mode,transform,target_transform,kwargs", "method_startline": "18", "method_endline": "19", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "26", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "18,19", "deleted_lines": "18,19", "method_info": {"method_name": "__init__", "method_params": "self,root", "method_startline": "18", "method_endline": "19", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "35", "method_nesting_level": "1"}}}}}, "file_12": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "encoding\\datasets\\pascal_voc.py", "file_new_name": "encoding\\datasets\\pascal_voc.py", "file_complexity": {"file_NLOC": "73", "file_CCN": "11", "file_NToken": "631"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "82,83", "deleted_lines": null, "method_info": {"method_name": "pred_offset", "method_params": "self", "method_startline": "82", "method_endline": "83", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "7", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "19,20", "deleted_lines": "19,20", "method_info": {"method_name": "__init__", "method_params": "self,root,split,mode,transform,target_transform,kwargs", "method_startline": "19", "method_endline": "20", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "26", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": null, "deleted_lines": "68,71", "method_info": {"method_name": "__getitem__", "method_params": "self,index", "method_startline": "51", "method_endline": "73", "method_complexity": {"method_NLOC": "19", "method_CCN": "7", "method_NToken": "167", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "19,20", "deleted_lines": "19,20", "method_info": {"method_name": "__init__", "method_params": "self,root", "method_startline": "19", "method_endline": "20", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "35", "method_nesting_level": "1"}}}}}, "file_13": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "encoding\\datasets\\pcontext.py", "file_new_name": "encoding\\datasets\\pcontext.py", "file_complexity": {"file_NLOC": "85", "file_CCN": "15", "file_NToken": "797"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "92,95", "method_info": {"method_name": "__getitem__", "method_params": "self,index", "method_startline": "71", "method_endline": "97", "method_complexity": {"method_NLOC": "22", "method_CCN": "7", "method_NToken": "185", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "51", "method_info": {"method_name": "_class_to_index", "method_params": "self,mask", "method_startline": "48", "method_endline": "55", "method_complexity": {"method_NLOC": "6", "method_CCN": "2", "method_NToken": "71", "method_nesting_level": "1"}}}}}, "file_14": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "encoding\\dilated\\resnet.py", "file_new_name": "encoding\\dilated\\resnet.py", "file_complexity": {"file_NLOC": "215", "file_CCN": "28", "file_NToken": "1820"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "139,140,141,142,143,144,145,146,147,148,149,150,151,152,167", "deleted_lines": "138,139,141,142,143,158", "method_info": {"method_name": "__init__", "method_params": "self,block,layers,num_classes,dilated,norm_layer", "method_startline": "138", "method_endline": "167", "method_complexity": {"method_NLOC": "29", "method_CCN": "5", "method_NToken": "376", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "135,136", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,block,layers,num_classes,dilated,deep_base,norm_layer", "method_startline": "135", "method_endline": "136", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "27", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "282,283,284", "deleted_lines": null, "method_info": {"method_name": "resnet152", "method_params": "pretrained,root,kwargs", "method_startline": "274", "method_endline": "285", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "69", "method_nesting_level": "0"}}}}}, "file_15": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\functions\\__init__.py", "file_new_name": "encoding\\functions\\__init__.py", "file_complexity": {"file_NLOC": "4", "file_CCN": "0", "file_NToken": "16"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4", "deleted_lines": null}}}, "file_16": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\functions\\customize.py", "file_complexity": {"file_NLOC": "42", "file_CCN": "2", "file_NToken": "66"}}, "file_17": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "encoding\\functions\\encoding.py", "file_new_name": "encoding\\functions\\encoding.py", "file_complexity": {"file_NLOC": "91", "file_CCN": "12", "file_NToken": "431"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "83,95", "deleted_lines": "83,84,96", "method_info": {"method_name": "scaledL2", "method_params": "X,C,S", "method_startline": "83", "method_endline": "96", "method_complexity": {"method_NLOC": "14", "method_CCN": "1", "method_NToken": "22", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "69", "deleted_lines": "69", "method_info": {"method_name": "forward", "method_params": "ctx,X,C,S", "method_startline": "65", "method_endline": "71", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "60", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "26", "deleted_lines": "26", "method_info": {"method_name": "forward", "method_params": "ctx,A,X,C", "method_startline": "20", "method_endline": "27", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "58", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "79", "deleted_lines": "79", "method_info": {"method_name": "backward", "method_params": "ctx,gradSL", "method_startline": "74", "method_endline": "80", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "75", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "35", "deleted_lines": "35", "method_info": {"method_name": "backward", "method_params": "ctx,gradE", "method_startline": "30", "method_endline": "36", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "69", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "82,83,95", "deleted_lines": "82,83,84", "method_info": {"method_name": "scaled_l2", "method_params": "X,C,S", "method_startline": "82", "method_endline": "95", "method_complexity": {"method_NLOC": "14", "method_CCN": "1", "method_NToken": "22", "method_nesting_level": "0"}}}}}, "file_18": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "encoding\\functions\\syncbn.py", "file_new_name": "encoding\\functions\\syncbn.py", "file_complexity": {"file_NLOC": "60", "file_CCN": "10", "file_NToken": "325"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "49", "deleted_lines": "49", "method_info": {"method_name": "forward", "method_params": "ctx,input,mean,std,gamma,beta", "method_startline": "44", "method_endline": "50", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "74", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "29", "deleted_lines": "29", "method_info": {"method_name": "forward", "method_params": "ctx,input", "method_startline": "24", "method_endline": "30", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "48", "method_nesting_level": "1"}}}}}, "file_19": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\__init__.py", "file_new_name": "encoding\\lib\\__init__.py", "file_complexity": {"file_NLOC": "22", "file_CCN": "0", "file_NToken": "217"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9,10,11,12,14,18,21,24", "deleted_lines": "9,10,15"}}}, "file_20": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\cpu\\.ninja_deps", "file_new_name": "encoding\\lib\\cpu\\.ninja_deps", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_21": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\cpu\\.ninja_log", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_22": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\cpu\\build.ninja", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_23": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\cpu\\encoding_cpu.cpp", "file_complexity": {"file_NLOC": "43", "file_CCN": "4", "file_NToken": "570"}}, "file_24": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\cpu\\encoding_cpu.o", "file_new_name": "encoding\\lib\\cpu\\encoding_cpu.o", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_25": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\cpu\\nms_cpu.cpp", "file_complexity": {"file_NLOC": "82", "file_CCN": "15", "file_NToken": "853"}}, "file_26": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\cpu\\operator.cpp", "file_complexity": {"file_NLOC": "14", "file_CCN": "1", "file_NToken": "142"}}, "file_27": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\cpu\\operator.h", "file_complexity": {"file_NLOC": "63", "file_CCN": "0", "file_NToken": "359"}}, "file_28": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\cpu\\operator.o", "file_new_name": "encoding\\lib\\cpu\\operator.o", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_29": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "encoding\\lib\\cpu\\roi_align_cpu.cpp", "file_new_name": "encoding\\lib\\cpu\\roi_align_cpu.cpp", "file_complexity": {"file_NLOC": "385", "file_CCN": "48", "file_NToken": "2615"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "380,412", "deleted_lines": "380,412", "method_info": {"method_name": "ROIAlignForwardCPU", "method_params": "input,bottom_rois,pooled_height,pooled_width,spatial_scale,sampling_ratio", "method_startline": "380", "method_endline": "429", "method_complexity": {"method_NLOC": "38", "method_CCN": "2", "method_NToken": "266", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "380,412", "deleted_lines": "380,412", "method_info": {"method_name": "ROIAlign_Forward_CPU", "method_params": "input,bottom_rois,pooled_height,pooled_width,spatial_scale,sampling_ratio", "method_startline": "380", "method_endline": "429", "method_complexity": {"method_NLOC": "38", "method_CCN": "2", "method_NToken": "266", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "432,458", "deleted_lines": "432,458", "method_info": {"method_name": "ROIAlignBackwardCPU", "method_params": "bottom_rois,grad_output,b_size,channels,height,width,pooled_height,pooled_width,spatial_scale,sampling_ratio", "method_startline": "432", "method_endline": "476", "method_complexity": {"method_NLOC": "38", "method_CCN": "2", "method_NToken": "225", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "432,458", "deleted_lines": "432,458", "method_info": {"method_name": "ROIAlign_Backward_CPU", "method_params": "bottom_rois,grad_output,b_size,channels,height,width,pooled_height,pooled_width,spatial_scale,sampling_ratio", "method_startline": "432", "method_endline": "476", "method_complexity": {"method_NLOC": "38", "method_CCN": "2", "method_NToken": "225", "method_nesting_level": "0"}}}}}, "file_30": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\cpu\\setup.py", "file_new_name": "encoding\\lib\\cpu\\setup.py", "file_complexity": {"file_NLOC": "16", "file_CCN": "0", "file_NToken": "51"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8,10,11,12", "deleted_lines": "8"}}}, "file_31": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\cpu\\syncbn_cpu.cpp", "file_complexity": {"file_NLOC": "50", "file_CCN": "7", "file_NToken": "409"}}, "file_32": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\cpu\\syncbn_cpu.o", "file_new_name": "encoding\\lib\\cpu\\syncbn_cpu.o", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_33": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "encoding\\lib\\gpu\\common.h", "file_new_name": "encoding\\lib\\gpu\\common.h", "file_complexity": {"file_NLOC": "174", "file_CCN": "50", "file_NToken": "1375"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150", "deleted_lines": null, "method_info": {"method_name": "reduceN", "method_params": "op,b,k,d,N", "method_startline": "117", "method_endline": "150", "method_complexity": {"method_NLOC": "27", "method_CCN": "8", "method_NToken": "198", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186", "deleted_lines": null, "method_info": {"method_name": "reduceK", "method_params": "op,b,i,d,K", "method_startline": "153", "method_endline": "186", "method_complexity": {"method_NLOC": "27", "method_CCN": "8", "method_NToken": "198", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114", "deleted_lines": null, "method_info": {"method_name": "reduceD", "method_params": "op,b,i,k,D", "method_startline": "81", "method_endline": "114", "method_complexity": {"method_NLOC": "27", "method_CCN": "8", "method_NToken": "198", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224", "deleted_lines": null, "method_info": {"method_name": "reduceBN", "method_params": "op,k,d,B,N", "method_startline": "189", "method_endline": "224", "method_complexity": {"method_NLOC": "30", "method_CCN": "9", "method_NToken": "214", "method_nesting_level": "0"}}}}}, "file_34": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\gpu\\encoding_kernel.cu", "file_new_name": "encoding\\lib\\gpu\\encoding_kernel.cu", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "2,3,82,101,169,195,218,245", "deleted_lines": "1,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,228,247,315,341,364,391,396,397"}}}, "file_35": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\gpu\\encodingv2_kernel.cu", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_36": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "encoding\\lib\\gpu\\nms_kernel.cu", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_37": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "encoding\\lib\\gpu\\operator.cpp", "file_new_name": "encoding\\lib\\gpu\\operator.cpp", "file_complexity": {"file_NLOC": "22", "file_CCN": "1", "file_NToken": "214"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "4,5,6,15,16,17,18,19,20,21,22", "deleted_lines": "4,5", "method_info": {"method_name": "PYBIND11_MODULE", "method_params": "TORCH_EXTENSION_NAME,m", "method_startline": "3", "method_endline": "23", "method_complexity": {"method_NLOC": "21", "method_CCN": "1", "method_NToken": "212", "method_nesting_level": "0"}}}}}, "file_38": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\gpu\\operator.h", "file_new_name": "encoding\\lib\\gpu\\operator.h", "file_complexity": {"file_NLOC": "96", "file_CCN": "0", "file_NToken": "570"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4,12,24,25,26,27,28,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113", "deleted_lines": "4,12"}}}, "file_39": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\gpu\\roi_align_kernel.cu", "file_new_name": "encoding\\lib\\gpu\\roi_align_kernel.cu", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "2,350,374,379,396,421,426", "deleted_lines": "349,373,378,395,420,425"}}}, "file_40": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\gpu\\setup.py", "file_new_name": "encoding\\lib\\gpu\\setup.py", "file_complexity": {"file_NLOC": "17", "file_CCN": "0", "file_NToken": "53"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "10,13", "deleted_lines": null}}}, "file_41": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\lib\\gpu\\syncbn_kernel.cu", "file_new_name": "encoding\\lib\\gpu\\syncbn_kernel.cu", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "2,3,184,218,250,253,273,276", "deleted_lines": "1,183,217,249,252,272,275"}}}, "file_42": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "encoding\\models\\base.py", "file_new_name": "encoding\\models\\base.py", "file_complexity": {"file_NLOC": "162", "file_CCN": "31", "file_NToken": "1695"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "27", "deleted_lines": "27", "method_info": {"method_name": "__init__", "method_params": "self,nclass,backbone,aux,se_loss,dilated,norm_layer,base_size,crop_size,mean,456,std,224,root", "method_startline": "26", "method_endline": "28", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "59", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "27", "deleted_lines": "27", "method_info": {"method_name": "__init__", "method_params": "self,nclass,backbone,aux,se_loss,dilated,norm_layer,base_size,crop_size,mean,456,std,224,root", "method_startline": "26", "method_endline": "28", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "59", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "102,103", "deleted_lines": null, "method_info": {"method_name": "parallel_forward", "method_params": "self,inputs,kwargs", "method_startline": "87", "method_endline": "104", "method_complexity": {"method_NLOC": "11", "method_CCN": "7", "method_NToken": "160", "method_nesting_level": "1"}}}}}, "file_43": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "encoding\\models\\encnet.py", "file_new_name": "encoding\\models\\encnet.py", "file_complexity": {"file_NLOC": "225", "file_CCN": "16", "file_NToken": "1255"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "189,190", "deleted_lines": "189", "method_info": {"method_name": "get_encnet_resnet101_pcontext", "method_params": "pretrained,root,kwargs", "method_startline": "172", "method_endline": "190", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "46,47", "method_info": {"method_name": "__init__", "method_params": "self,in_channels,nclass,ncodes,se_loss,norm_layer", "method_startline": "44", "method_endline": "61", "method_complexity": {"method_NLOC": "16", "method_CCN": "2", "method_NToken": "157", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "169,170", "deleted_lines": "170", "method_info": {"method_name": "get_encnet_resnet50_pcontext", "method_params": "pretrained,root,kwargs", "method_startline": "152", "method_endline": "170", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230", "deleted_lines": null, "method_info": {"method_name": "get_encnet_resnet101_ade", "method_params": "pretrained,root,kwargs", "method_startline": "212", "method_endline": "230", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250", "deleted_lines": null, "method_info": {"method_name": "get_encnet_resnet152_ade", "method_params": "pretrained,root,kwargs", "method_startline": "232", "method_endline": "250", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "0"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "209,210", "deleted_lines": "208", "method_info": {"method_name": "get_encnet_resnet50_ade", "method_params": "pretrained,root,kwargs", "method_startline": "192", "method_endline": "210", "method_complexity": {"method_NLOC": "19", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "0"}}}}}, "file_44": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\models\\fcn.py", "file_new_name": "encoding\\models\\fcn.py", "file_complexity": {"file_NLOC": "125", "file_CCN": "9", "file_NToken": "523"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "104", "deleted_lines": "104,105"}}}, "file_45": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "encoding\\models\\model_store.py", "file_new_name": "encoding\\models\\model_store.py", "file_complexity": {"file_NLOC": "88", "file_CCN": "12", "file_NToken": "469"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "59,60,62", "deleted_lines": "55,57", "method_info": {"method_name": "get_model_file", "method_params": "name,root", "method_startline": "33", "method_endline": "81", "method_complexity": {"method_NLOC": "46", "method_CCN": "6", "method_NToken": "232", "method_nesting_level": "0"}}}}}, "file_46": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "encoding\\models\\model_zoo.py", "file_new_name": "encoding\\models\\model_zoo.py", "file_complexity": {"file_NLOC": "21", "file_CCN": "2", "file_NToken": "122"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33,36", "deleted_lines": null, "method_info": {"method_name": "get_model", "method_params": "name,kwargs", "method_startline": "11", "method_endline": "42", "method_complexity": {"method_NLOC": "16", "method_CCN": "2", "method_NToken": "96", "method_nesting_level": "0"}}}}}, "file_47": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\models\\psp.py", "file_new_name": "encoding\\models\\psp.py", "file_complexity": {"file_NLOC": "71", "file_CCN": "8", "file_NToken": "505"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "61", "deleted_lines": "61"}}}, "file_48": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "encoding\\nn\\customize.py", "file_new_name": "encoding\\nn\\customize.py", "file_complexity": {"file_NLOC": "151", "file_CCN": "22", "file_NToken": "1243"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "43", "deleted_lines": "43", "method_info": {"method_name": "__init__", "method_params": "self,se_loss,se_weight,nclass,aux,aux_weight,weight,size_average,ignore_index", "method_startline": "42", "method_endline": "44", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "43", "deleted_lines": "43", "method_info": {"method_name": "__init__", "method_params": "self,se_loss,se_weight,nclass,aux,aux_weight,weight,size_average,ignore_index", "method_startline": "42", "method_endline": "44", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "65,72", "deleted_lines": "65,72", "method_info": {"method_name": "forward", "method_params": "self,inputs", "method_startline": "53", "method_endline": "73", "method_complexity": {"method_NLOC": "21", "method_CCN": "5", "method_NToken": "251", "method_nesting_level": "1"}}}}}, "file_49": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "encoding\\nn\\encoding.py", "file_new_name": "encoding\\nn\\encoding.py", "file_complexity": {"file_NLOC": "259", "file_CCN": "24", "file_NToken": "1299"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "93,95,98,102,103", "deleted_lines": "94,95,98,99,103,104", "method_info": {"method_name": "forward", "method_params": "self,X", "method_startline": "90", "method_endline": "106", "method_complexity": {"method_NLOC": "12", "method_CCN": "3", "method_NToken": "134", "method_nesting_level": "1"}}}}}, "file_50": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "encoding\\utils\\__init__.py", "file_new_name": "encoding\\utils\\__init__.py", "file_complexity": {"file_NLOC": "10", "file_CCN": "0", "file_NToken": "62"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13", "deleted_lines": "13"}}}, "file_51": {"file_change_type": "MODIFY", "file_Nmethod": 9, "file_old_name": "encoding\\utils\\metrics.py", "file_new_name": "encoding\\utils\\metrics.py", "file_complexity": {"file_NLOC": "90", "file_CCN": "14", "file_NToken": "758"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "64,70,71,72,73,74", "deleted_lines": null, "method_info": {"method_name": "batch_pix_accuracy", "method_params": "output,target", "method_startline": "64", "method_endline": "79", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "95", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "50,51,52,53,54", "deleted_lines": null, "method_info": {"method_name": "get", "method_params": "self", "method_startline": "50", "method_endline": "54", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "60", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "82,89,93,94", "deleted_lines": null, "method_info": {"method_name": "batch_intersection_union", "method_params": "output,target,nclass", "method_startline": "82", "method_endline": "105", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "182", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "56,57,58,59,60,61", "deleted_lines": null, "method_info": {"method_name": "reset", "method_params": "self", "method_startline": "56", "method_endline": "61", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "26", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53", "deleted_lines": "30,37,41,42", "method_info": {"method_name": "batch_intersection_union", "method_params": "predict,target,nclass", "method_startline": "30", "method_endline": "53", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "172", "method_nesting_level": "0"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "15,16,17,18,19,20,21,22,23,24,25,26,27", "deleted_lines": "14,20,21,22", "method_info": {"method_name": "batch_pix_accuracy", "method_params": "predict,target", "method_startline": "14", "method_endline": "27", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "85", "method_nesting_level": "0"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "18,19,20,21", "deleted_lines": "20,21", "method_info": {"method_name": "__init__", "method_params": "self,nclass", "method_startline": "18", "method_endline": "21", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "26", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48", "deleted_lines": "30,37,41,42", "method_info": {"method_name": "update", "method_params": "self,labels,preds", "method_startline": "23", "method_endline": "48", "method_complexity": {"method_NLOC": "15", "method_CCN": "6", "method_NToken": "101", "method_nesting_level": "1"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "24,25,26,27,28,29,30,31,32,33,34", "deleted_lines": "30", "method_info": {"method_name": "update.evaluate_worker", "method_params": "self,label,pred", "method_startline": "24", "method_endline": "34", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "59", "method_nesting_level": "2"}}}}}, "file_52": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "encoding\\utils\\pallete.py", "file_new_name": "encoding\\utils\\pallete.py", "file_complexity": {"file_NLOC": "32", "file_CCN": "8", "file_NToken": "2707"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "24", "deleted_lines": "24", "method_info": {"method_name": "get_mask_pallete", "method_params": "npimg,dataset", "method_startline": "13", "method_endline": "26", "method_complexity": {"method_NLOC": "11", "method_CCN": "5", "method_NToken": "81", "method_nesting_level": "0"}}}}}, "file_53": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "experiments\\recognition\\dataset\\minc.py", "file_new_name": "experiments\\recognition\\dataset\\minc.py", "file_complexity": {"file_NLOC": "103", "file_CCN": "17", "file_NToken": "957"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "97,99", "deleted_lines": "97,99", "method_info": {"method_name": "__init__", "method_params": "self,args", "method_startline": "78", "method_endline": "108", "method_complexity": {"method_NLOC": "29", "method_CCN": "2", "method_NToken": "271", "method_nesting_level": "1"}}}}}, "file_54": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "experiments\\recognition\\main.py", "file_new_name": "experiments\\recognition\\main.py", "file_complexity": {"file_NLOC": "138", "file_CCN": "19", "file_NToken": "1055"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "99", "deleted_lines": "99", "method_info": {"method_name": "main", "method_params": "", "method_startline": "33", "method_endline": "171", "method_complexity": {"method_NLOC": "57", "method_CCN": "10", "method_NToken": "432", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "99", "deleted_lines": "99", "method_info": {"method_name": "main.train", "method_params": "epoch", "method_startline": "81", "method_endline": "105", "method_complexity": {"method_NLOC": "23", "method_CCN": "3", "method_NToken": "216", "method_nesting_level": "1"}}}}}, "file_55": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "experiments\\recognition\\model\\download_models.py", "file_new_name": "experiments\\recognition\\model\\download_models.py", "file_complexity": {"file_NLOC": "4", "file_CCN": "0", "file_NToken": "24"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5", "deleted_lines": "5"}}}, "file_56": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "experiments\\recognition\\model\\mynn.py", "file_new_name": "experiments\\recognition\\model\\mynn.py", "file_complexity": {"file_NLOC": "208", "file_CCN": "27", "file_NToken": "1799"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "90,91,92,93,94,95,96,97,98,99,100,101,102", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,channel,K,reduction", "method_startline": "90", "method_endline": "102", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "122", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "104,105,106,107", "deleted_lines": null, "method_info": {"method_name": "forward", "method_params": "self,x", "method_startline": "104", "method_endline": "107", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "43", "method_nesting_level": "1"}}}}}, "file_57": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "experiments\\recognition\\option.py", "file_new_name": "experiments\\recognition\\option.py", "file_complexity": {"file_NLOC": "53", "file_CCN": "3", "file_NToken": "491"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "43,44", "deleted_lines": "43,44", "method_info": {"method_name": "__init__", "method_params": "self", "method_startline": "15", "method_endline": "67", "method_complexity": {"method_NLOC": "45", "method_CCN": "1", "method_NToken": "452", "method_nesting_level": "1"}}}}}, "file_58": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "experiments\\segmentation\\option.py", "file_new_name": "experiments\\segmentation\\option.py", "file_complexity": {"file_NLOC": "99", "file_CCN": "7", "file_NToken": "799"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "28,30,32,33,37,38,41,42", "deleted_lines": "28,30,71,72,73,75,76", "method_info": {"method_name": "__init__", "method_params": "self", "method_startline": "12", "method_endline": "86", "method_complexity": {"method_NLOC": "66", "method_CCN": "1", "method_NToken": "600", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "94,95,99,108,109,116", "deleted_lines": "96", "method_info": {"method_name": "parse", "method_params": "self", "method_startline": "88", "method_endline": "117", "method_complexity": {"method_NLOC": "29", "method_CCN": "6", "method_NToken": "186", "method_nesting_level": "1"}}}}}, "file_59": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "experiments\\segmentation\\test.py", "file_new_name": "experiments\\segmentation\\test.py", "file_complexity": {"file_NLOC": "71", "file_CCN": "12", "file_NToken": "692"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "71,72,73,74,75,76,78,79,80,81,82", "deleted_lines": "71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,87,88,89,90,91,92,96,97", "method_info": {"method_name": "test.eval_batch", "method_params": "image,dst,evaluator,eval_mode", "method_startline": "71", "method_endline": "97", "method_complexity": {"method_NLOC": "24", "method_CCN": "5", "method_NToken": "218", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "63,64,65,67,70,71,72,73,74,75,76,78,79,80,81,82", "deleted_lines": "24,25,26,27,67,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85", "method_info": {"method_name": "test", "method_params": "args", "method_startline": "24", "method_endline": "85", "method_complexity": {"method_NLOC": "53", "method_CCN": "12", "method_NToken": "576", "method_nesting_level": "0"}}}}}, "file_60": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "experiments\\segmentation\\train.py", "file_new_name": "experiments\\segmentation\\train.py", "file_complexity": {"file_NLOC": "148", "file_CCN": "19", "file_NToken": "1323"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "39,63,64,67,68,69,90,91,92", "deleted_lines": "39,63,64,65,66,67,68,69,72", "method_info": {"method_name": "__init__", "method_params": "self,args", "method_startline": "30", "method_endline": "96", "method_complexity": {"method_NLOC": "57", "method_CCN": "10", "method_NToken": "607", "method_nesting_level": "1"}}}}}, "file_61": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "scripts\\prepare_cityscapes.py", "file_complexity": {"file_NLOC": "40", "file_CCN": "4", "file_NToken": "252"}}, "file_62": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "scripts\\prepare_coco.py", "file_new_name": "scripts\\prepare_coco.py", "file_complexity": {"file_NLOC": "48", "file_CCN": "5", "file_NToken": "269"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "27,28,29,30", "deleted_lines": "27,28,29,30", "method_info": {"method_name": "download_coco", "method_params": "path,overwrite", "method_startline": "19", "method_endline": "37", "method_complexity": {"method_NLOC": "14", "method_CCN": "2", "method_NToken": "78", "method_nesting_level": "0"}}}}}, "file_63": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "scripts\\prepare_minc.py", "file_complexity": {"file_NLOC": "35", "file_CCN": "3", "file_NToken": "249"}}, "file_64": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "setup.py", "file_new_name": "setup.py", "file_complexity": {"file_NLOC": "74", "file_CCN": "3", "file_NToken": "357"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "21", "deleted_lines": "21"}}}, "file_65": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\unit_test\\test_function.py", "file_complexity": {"file_NLOC": "188", "file_CCN": "14", "file_NToken": "2596"}}, "file_66": {"file_change_type": "MODIFY", "file_Nmethod": 8, "file_old_name": "tests\\unit_test\\test_module.py", "file_new_name": "tests\\unit_test\\test_module.py", "file_complexity": {"file_NLOC": "75", "file_CCN": "17", "file_NToken": "855"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "63,64,65,66,67,68,69", "method_info": {"method_name": "test_sum_square", "method_params": "", "method_startline": "63", "method_endline": "69", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "83", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "48", "deleted_lines": "48,49,50,51,52,62,63,64,65,66,67,68,69,70,71", "method_info": {"method_name": "testSyncBN._check_batchnorm_result", "method_params": "bn1,bn2,input,is_train,cuda", "method_startline": "48", "method_endline": "84", "method_complexity": {"method_NLOC": "21", "method_CCN": "3", "method_NToken": "202", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "48,94,95", "deleted_lines": "47,48,49,50,51,52,62,63,64,65,66,67,68,69,70,71,85,86,87,88,89,90,91,92,93,94,95", "method_info": {"method_name": "testSyncBN", "method_params": "", "method_startline": "47", "method_endline": "95", "method_complexity": {"method_NLOC": "10", "method_CCN": "2", "method_NToken": "130", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "48", "deleted_lines": "40,41,42,43,44,45,46,47,48,49,50", "method_info": {"method_name": "test_scaledL2", "method_params": "", "method_startline": "40", "method_endline": "50", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "150", "method_nesting_level": "0"}}}, "hunk_4": {"Ismethod": 1, "added_lines": null, "deleted_lines": "103,104,105,106,107,108,109,110,111,112,113,114,115,116", "method_info": {"method_name": "test_syncbn_func", "method_params": "", "method_startline": "103", "method_endline": "116", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "211", "method_nesting_level": "0"}}}, "hunk_5": {"Ismethod": 1, "added_lines": null, "deleted_lines": "27,28,29,30,31,32,33,34,35,36,37", "method_info": {"method_name": "test_aggregate", "method_params": "", "method_startline": "27", "method_endline": "37", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "154", "method_nesting_level": "0"}}}, "hunk_6": {"Ismethod": 1, "added_lines": null, "deleted_lines": "120", "method_info": {"method_name": "testSyncBN._checkBatchNormResult", "method_params": "bn1,bn2,input,is_train,cuda", "method_startline": "120", "method_endline": "156", "method_complexity": {"method_NLOC": "21", "method_CCN": "3", "method_NToken": "202", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": "94,95", "deleted_lines": "86,87,88,89,90,91,92,93,94,95,96,97,98,99,100", "method_info": {"method_name": "test_syncbn", "method_params": "", "method_startline": "86", "method_endline": "100", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "130", "method_nesting_level": "0"}}}}}, "file_67": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\unit_test\\test_utils.py", "file_complexity": {"file_NLOC": "23", "file_CCN": "1", "file_NToken": "256"}}}}}