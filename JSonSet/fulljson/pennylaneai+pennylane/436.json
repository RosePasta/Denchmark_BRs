{"BR": {"BR_id": "436", "BR_author": "ycchen1989", "BRopenT": "2019-11-27T20:41:33Z", "BRcloseT": "2020-02-18T06:01:33Z", "BR_text": {"BRsummary": "Classical preprocessing before Amplitude Encoding", "BRdescription": "\n <denchmark-h:h4>Issue description</denchmark-h>\n \n Description of the issue - include code snippets and screenshots here\n if relevant. You may use the following template below\n \n \n Expected behavior: (What you expect to happen)\n I placed a pytorch classical node before the quantum node.\n In the classical node, there is just a scaling on the input, while the scaling parameters are subject to optimization process. The scaled classical data are then sent into the quantum node.\n \n \n Actual behavior: (What actually happens)\n In the loss.backward() process, however, the error message says that the parameters cannot be differentiated. It seems that the qnode cannot handle input vectors which are already carry torch gradient information? If I change the angle parameter in the qnode into a keyword argument, the error message disappears, of course, the scaling parameters do not update!\n \n \n <denchmark-h:h4>Source code and tracebacks</denchmark-h>\n \n Please include any additional code snippets and error tracebacks related\n to the issue here.\n The classical node:\n class ClassicalScaling:\n \tdef __init__(self, var_Q_circuit):\n \t\tself.var_Q_circuit = var_Q_circuit\n \tdef forward(self, angles):\n \t\treturn self.var_Q_circuit * angles\n The quantum node:\n class VQC:\n \tdef __init__(\n \t\t\tself,\n \t\t\tnum_of_input= 10,\n \t\t\tnum_of_output= 10,\n \t\t\tnum_of_wires = 10,\n \t\t\tvar_Q_circuit = None,\n \t\t\tvar_Q_bias = None):\n \n \t\tself.var_Q_circuit = var_Q_circuit\n \t\tself.var_Q_bias = var_Q_bias\n \t\tself.num_of_input = num_of_input\n \t\tself.num_of_output = num_of_output\n \t\tself.num_of_wires = num_of_wires\n \t\tself.dev = qml.device('default.qubit', wires = num_of_wires)\n \n \tdef circuit(self, angles):\n \t\t@qml.qnode(self.dev, interface='torch')\n \t\tdef _circuit(var_Q_circuit, angles):\n \t\t\tqml.QubitStateVector(angles, wires=list(range(self.num_of_wires)))\n \t\t\tweights = var_Q_circuit\n \t\t\tfor W in weights:\n \t\t\t\tself._layer(W)\n \t\t\treturn [qml.expval.PauliZ(k) for k in range(self.num_of_output)]\n \n \t\treturn _circuit(self.var_Q_circuit, angles)\n \n \tdef _forward(self, angles):\n \t\tangles = angles / torch.clamp(torch.sqrt(torch.sum(angles ** 2)), min = 1e-9)\n \t\traw_output = self.circuit(angles)\n \n \t\treturn raw_output\n \n \tdef forward(self, angles):\n \t\tfw = self._forward(angles)\n \t\treturn fw\n The initial scaling parameters:\n var_Q_circuit = Variable(torch.ones(1024, dtype = torch.double), requires_grad=True)\n The error messages:\n   File \"/Users/ycchen/Desktop/Programming_Exercise/QISKIT_EXERCISE/PENNYLANE/PyTorchAcceleration/TorchPennyLane/lib/python3.7/site-packages/pennylane/qnode.py\", line 875, in jacobian\n     raise ValueError('Cannot differentiate wrt parameter(s) {}.'.format(bad))\n ValueError: Cannot differentiate wrt parameter(s) {240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263}.\n <denchmark-h:h4>Additional information</denchmark-h>\n \n The input dim is 1024 which is to be encoded into 10 qubit quantum amplitudes.\n Any additional information, configuration or data that might be necessary\n to reproduce the issue.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ycchen1989", "commentT": "2019-11-27T21:22:19Z", "comment_text": "\n \t\tThanks for reporting <denchmark-link:https://github.com/ycchen1989>@ycchen1989</denchmark-link>\n  .\n Would you mind also posting how you \"run\" VQE and \"ClassicalScaling\" exactly, i.e. in what way you invoke the loss.backward() statement?\n At the moment your code only shows the two classes and the initial parameters. It would be really helpful to have a minimum code example that reproduces the error when I run it.\n Thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ycchen1989", "commentT": "2019-11-28T18:25:03Z", "comment_text": "\n \t\tminimum code example:\n import os\n import argparse\n \n import pennylane as qml\n from pennylane import numpy as np\n # import numpy as np\n # from pennylane.optimize import NesterovMomentumOptimizer\n \n import matplotlib.pyplot as plt\n from datetime import datetime\n \n import torch\n import torch.nn as nn \n from torch.autograd import Variable\n \n import pickle\n \n dtype = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n device = 'cuda' if torch.cuda.is_available() else 'cpu'\n \n ###\n \n class ClassicalScaling:\n \tdef __init__(self, var_Q_circuit):\n \t\tself.var_Q_circuit = var_Q_circuit\n \tdef forward(self, angles):\n \t\treturn self.var_Q_circuit * angles\n \n class VQC:\n \tdef __init__(\n \t\t\tself,\n \t\t\tnum_of_input= 10,\n \t\t\tnum_of_output= 2,\n \t\t\tnum_of_wires = 10,\n \t\t\tvar_Q_circuit = None,\n \t\t\tvar_Q_bias = None):\n \n \t\tself.var_Q_circuit = var_Q_circuit\n \t\tself.var_Q_bias = var_Q_bias\n \t\tself.num_of_input = num_of_input\n \t\tself.num_of_output = num_of_output\n \t\tself.num_of_wires = num_of_wires\n \t\tself.dev = qml.device('default.qubit', wires = num_of_wires)\n \n \tdef _layer(self, W):\n \t\t\"\"\" Single layer of the variational classifier.\n \n \t\tArgs:\n \t\t\tW (array[float]): 2-d array of variables for one layer\n \n \t\t\"\"\"\n \n \t\t# W = W.numpy()\n \n \t\tqml.CNOT(wires=[0, 1])\n \t\tqml.CNOT(wires=[1, 2])\n \t\tqml.CNOT(wires=[2, 3])\n \t\tqml.CNOT(wires=[3, 4])\n \t\tqml.CNOT(wires=[4, 5])\n \t\tqml.CNOT(wires=[5, 6])\n \t\tqml.CNOT(wires=[6, 7])\n \t\tqml.CNOT(wires=[7, 8])\n \t\tqml.CNOT(wires=[8, 9])\n \t\t# Another CNOT which is not in MNIST experiments!\n \t\tqml.CNOT(wires=[9, 0])\n \n \n \t\tqml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n \t\tqml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n \t\tqml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n \t\tqml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n \t\tqml.Rot(W[4, 0], W[4, 1], W[4, 2], wires=4)\n \t\tqml.Rot(W[5, 0], W[5, 1], W[5, 2], wires=5)\n \t\tqml.Rot(W[6, 0], W[6, 1], W[6, 2], wires=6)\n \t\tqml.Rot(W[7, 0], W[7, 1], W[7, 2], wires=7)\n \t\tqml.Rot(W[8, 0], W[8, 1], W[8, 2], wires=8)\n \t\tqml.Rot(W[9, 0], W[9, 1], W[9, 2], wires=9)\n \n \tdef circuit(self, angles):\n \t\t@qml.qnode(self.dev, interface='torch')\n \t\tdef _circuit(var_Q_circuit, angles):\n \t\t\tqml.QubitStateVector(angles, wires=list(range(self.num_of_wires)))\n \t\t\tweights = var_Q_circuit\n \t\t\tfor W in weights:\n \t\t\t\tself._layer(W)\n \t\t\treturn [qml.expval.PauliZ(k) for k in range(self.num_of_output)]\n \n \t\treturn _circuit(self.var_Q_circuit, angles)\n \n \tdef _forward(self, angles):\n \t\tangles = angles / torch.clamp(torch.sqrt(torch.sum(angles ** 2)), min = 1e-9)\n \t\traw_output = self.circuit(angles)\n \n \t\tm = nn.Softmax(dim=0)\n \n \t\tclamp = 1e-9 * torch.ones(self.num_of_output).type(dtype).to(device)\n \n \t\tnormalized_output = torch.max(raw_output, clamp)\n \n \t\toutput = m(normalized_output)\n \t\t\n \t\treturn output\n \n \tdef forward(self, angles):\n \t\tfw = self._forward(angles)\n \t\treturn fw\n \n ###\t\n \n def lost_function_cross_entropy(labels, predictions):\n \t## numpy array\n \tloss = nn.CrossEntropyLoss()\n \toutput = loss(predictions, labels)\n \tprint(\"LOSS AVG: \",output)\n \treturn output\n \n \n \n def cost(VQC, X, Y):\n \t\"\"\"Cost (error) function to be minimized.\"\"\"\n \n \t# predictions = torch.stack([variational_classifier(var_Q_circuit = var_Q_circuit, var_Q_bias = var_Q_bias, angles=item) for item in X])\n \n \t## This method still not fully use the CPU resource...\n \tloss = nn.CrossEntropyLoss()\n \toutput = loss(torch.stack([VQC.forward(item) for item in X]), Y)\n \tprint(\"LOSS AVG: \",output)\n \treturn output\n \n def train_epoch(opt, VQC, X, Y, batch_size):\n \tlosses = []\n \tfor i in range(5):\n \t\tbatch_index = np.random.randint(0, len(X), (batch_size, ))\n \t\tX_train_batch = X[batch_index]\n \t\tY_train_batch = Y[batch_index]\n \t\t# opt.step(closure)\n \t\topt.zero_grad()\n \t\tprint(\"CALCULATING LOSS...\")\n \t\tloss = cost(VQC = VQC, X = X_train_batch, Y = Y_train_batch)\n \t\tprint(\"BACKWARD..\")\n \t\tloss.backward()\n \t\tlosses.append(loss.data.cpu().numpy())\n \t\topt.step()\n \t# \t\tprint(\"LOSS IN CLOSURE: \", loss)\n \t\tprint(\"FINISHED OPT.\")\n \t\t# print(\"CALCULATING PREDICTION.\")\n \tlosses = np.array(losses)\n \treturn losses.mean()\n \n class stackedCircuit:\n \tdef __init__(self, scaling_part, vqc):\n \t\tself.scaling_part = scaling_part\n \t\tself.vqc = vqc\n \n \tdef forward(self, single_item):\n \t\tres_temp = self.scaling_part.forward(single_item)\n \t\tres_temp = self.vqc.forward(res_temp)\n \t\treturn res_temp\n \n \n def main(batch_size, epoch_num):\n \tvar_C_scaling = Variable(torch.ones(1024, dtype = torch.double), requires_grad=True)\n \tscaling_part = ClassicalScaling(var_Q_circuit = var_C_scaling)\n \n \tnum_of_input = 10\n \tnum_of_output = 10\n \tnum_layers = 2\n \tnum_qubits = 10\n \n \tvar_Q_circuit = Variable(torch.tensor(0.01 * np.random.randn(num_layers, num_qubits, 3), device=device).type(dtype), requires_grad=True)\n \tvqc = VQC(num_of_input = num_of_input, num_of_output = num_of_output, num_of_wires = num_qubits, var_Q_circuit = var_Q_circuit, var_Q_bias = None)\n \n \tstacked = stackedCircuit(scaling_part, vqc)\n \n \tparams = [var_C_scaling, var_Q_circuit]\n \n \n \topt = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n \n \n \tx_for_train = torch.tensor(np.random.randn(10000, 1024), device=device).type(dtype)\n \ty_for_train = torch.tensor(np.zeros(10000), device=device).type(torch.LongTensor)\n \n \tx_for_train = x_for_train.type(dtype)\n \n \tfor it in range(epoch_num):\n \t\t# Need to save data\n \t\tavg_loss_in_epoch = train_epoch(opt, stacked, x_for_train, y_for_train, batch_size)\n \t\n \n \n if __name__ == '__main__':\n \t\n \tmain(batch_size = 10, epoch_num = 100)\n \n \n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ycchen1989", "commentT": "2019-11-28T22:50:57Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/ycchen1989>@ycchen1989</denchmark-link>\n  !\n I see that in your code you using a rather old version of PennyLane. Would you mind installing the latest one first and see if it resolves the issue? That might save me the time it takes to turn your code into a minimum example and start debugging.\n Also check the latest templates functions in the <denchmark-link:https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.embeddings.AmplitudeEmbedding.html>API</denchmark-link>\n , which explains what parts are differentiable and which ones are not.\n PS: For future reference, a minimum working example usually consists of only a few lines of code needed to reproduce the issue. That allows us to respond a lot quicker and better!\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ycchen1989", "commentT": "2019-11-28T22:52:34Z", "comment_text": "\n \t\tNote, tf you run your code with the latest PennyLane version, you will have to change qml.expval.PauliZ(k) to qml.expval(qml.PauliZ(k)), which is the new signature\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ycchen1989", "commentT": "2019-11-29T00:03:13Z", "comment_text": "\n \t\tThanks for replying.\n I just install the latest version of pennylane(v0.7) and modify the expectation value to\n [qml.expval(qml.PauliZ(k)) for k in range(self.num_of_output)]\n and also change the\n qml.QubitStateVector(angles, wires=list(range(self.num_of_wires)))\n to\n qml.templates.embeddings.AmplitudeEmbedding(angles, wires=list(range(self.num_of_wires)))\n but the error messages persist.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ycchen1989", "commentT": "2019-11-29T20:33:16Z", "comment_text": "\n \t\tThanks for trying, I will have a proper look asap then.\n If you can shorten the example to a few lines I will be much faster in helping you, and I would be extremely grateful. It is difficult to dig through 200 lines of code that I have not written...\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ycchen1989", "commentT": "2019-11-30T04:18:34Z", "comment_text": "\n \t\tThanks for replying. I tried to shorten the code to about 100 lines. Hope it helps.\n import pennylane as qml\n from pennylane import numpy as np\n \n import matplotlib.pyplot as plt\n from datetime import datetime\n \n import torch\n import torch.nn as nn \n from torch.autograd import Variable\n \n dtype = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n device = 'cuda' if torch.cuda.is_available() else 'cpu'\n \n ###\n class VQC:\n \tdef __init__(\n \t\t\tself,\n \t\t\tnum_of_input= 10,\n \t\t\tnum_of_output= 2,\n \t\t\tnum_of_wires = 10,\n \t\t\tvar_Q_circuit = None,\n \t\t\tvar_C_scaling = None\n \t\t\t):\n \n \t\tself.var_Q_circuit = var_Q_circuit\n \t\tself.var_C_scaling = var_C_scaling\n \t\tself.num_of_input = num_of_input\n \t\tself.num_of_output = num_of_output\n \t\tself.num_of_wires = num_of_wires\n \t\tself.dev = qml.device('default.qubit', wires = num_of_wires)\n \n \tdef _layer(self, W):\n \n \t\tqml.CNOT(wires=[0, 1])\n \t\tqml.CNOT(wires=[1, 2])\n \t\tqml.CNOT(wires=[2, 3])\n \t\tqml.CNOT(wires=[3, 4])\n \t\tqml.CNOT(wires=[4, 5])\n \t\tqml.CNOT(wires=[5, 6])\n \t\tqml.CNOT(wires=[6, 7])\n \t\tqml.CNOT(wires=[7, 8])\n \t\tqml.CNOT(wires=[8, 9])\n \t\tqml.CNOT(wires=[9, 0])\n \n \n \t\tqml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n \t\tqml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n \t\tqml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n \t\tqml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n \t\tqml.Rot(W[4, 0], W[4, 1], W[4, 2], wires=4)\n \t\tqml.Rot(W[5, 0], W[5, 1], W[5, 2], wires=5)\n \t\tqml.Rot(W[6, 0], W[6, 1], W[6, 2], wires=6)\n \t\tqml.Rot(W[7, 0], W[7, 1], W[7, 2], wires=7)\n \t\tqml.Rot(W[8, 0], W[8, 1], W[8, 2], wires=8)\n \t\tqml.Rot(W[9, 0], W[9, 1], W[9, 2], wires=9)\n \n \tdef circuit(self, angles):\n \t\t@qml.qnode(self.dev, interface='torch')\n \t\tdef _circuit(var_Q_circuit, angles):\n \t\t\tqml.templates.embeddings.AmplitudeEmbedding(angles, wires=list(range(self.num_of_wires)))\n \t\t\tweights = var_Q_circuit\n \t\t\tfor W in weights:\n \t\t\t\tself._layer(W)\n \t\t\treturn [qml.expval(qml.PauliZ(k)) for k in range(self.num_of_output)]\n \t\treturn _circuit(self.var_Q_circuit, angles)\n \n \tdef forward(self, angles):\n \t\tprint(angles)\n \t\tangles = self.var_C_scaling * angles\n \t\tprint(angles)\n \t\tangles = angles / torch.clamp(torch.sqrt(torch.sum(angles ** 2)), min = 1e-9)\n \t\traw_output = self.circuit(angles)\n \t\tm = nn.Softmax(dim=0)\n \t\tclamp = 1e-9 * torch.ones(self.num_of_output).type(dtype).to(device)\n \t\tnormalized_output = torch.max(raw_output, clamp)\n \t\toutput = m(normalized_output)\n \t\treturn output\n \n def cost(VQC, X, Y):\n \tloss = nn.CrossEntropyLoss()\n \toutput = loss(torch.stack([VQC.forward(item) for item in X]), Y)\n \tprint(\"LOSS AVG: \",output)\n \treturn output\n \n def main(batch_size, epoch_num):\n \tnum_of_input = 10\n \tnum_of_output = 2\n \tnum_layers = 4\n \tnum_qubits = 10\n \tvar_C_scaling = Variable(torch.ones(1024, dtype = torch.double), requires_grad=True)\n \tvar_Q_circuit = Variable(torch.tensor(0.01 * np.random.randn(num_layers, num_qubits, 3), device=device).type(dtype), requires_grad=True)\n \tvqc = VQC(num_of_input = num_of_input, num_of_output = num_of_output, num_of_wires = num_qubits, var_Q_circuit = var_Q_circuit, var_C_scaling = var_C_scaling)\n \n \tparams = [var_C_scaling, var_Q_circuit]\n \n \topt = torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n \n \tx_for_train = torch.tensor(np.random.randn(10, 1024), device=device).type(dtype)\n \ty_for_train = torch.tensor(np.zeros(10), device=device).type(torch.LongTensor)\n \n \tx_for_train = x_for_train.type(dtype)\n \n \topt.zero_grad()\n \tloss = cost(VQC = vqc, X = x_for_train, Y = y_for_train)\n \tloss.backward()\n \topt.step()\n \n if __name__ == '__main__':\n \tmain(batch_size = 10, epoch_num = 100)\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ycchen1989", "commentT": "2019-12-02T10:30:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ycchen1989>@ycchen1989</denchmark-link>\n  I think I've bumped into this problem before as well. What I think is happening is that PennyLane is trying to optimize the  argument to  because it's not passed as a keyword argument. Changing  to a keyword parameter should fix it:\n def circuit(self, angles):\n     @qml.qnode(self.dev, interface='torch')\n     def _circuit(var_Q_circuit, angles=None):\n             qml.templates.embeddings.AmplitudeEmbedding(angles, wires=list(range(self.num_of_wires)))\n             weights = var_Q_circuit\n             for W in weights:\n                     self._layer(W)\n             return [qml.expval(qml.PauliZ(k)) for k in range(self.num_of_output)]\n     return _circuit(self.var_Q_circuit, angles=angles)\n It's documented <denchmark-link:https://pennylane.ai/qml/tutorial/tutorial_advanced_usage.html#keyword-arguments>here</denchmark-link>\n .\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ycchen1989", "commentT": "2019-12-02T12:42:49Z", "comment_text": "\n \t\tBut if i set the angle to keyword argument, the classical scaling part won't be updated by gradient descent.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "ycchen1989", "commentT": "2019-12-02T14:32:15Z", "comment_text": "\n \t\tI'm not sure then, I managed to get mixed classical and quantum parameter optimization working by extending nn.Module and defining self.quantum_parameters = torch.nn.Parameter(self._generate_quantum_parameters()) in the constructor. I then just initialize the optimizer as optim.SGD(model.parameters(), ...).\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "ycchen1989", "commentT": "2019-12-04T15:07:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ycchen1989>@ycchen1989</denchmark-link>\n  first of all, thanks for your patience, and thanks <denchmark-link:https://github.com/soudy>@soudy</denchmark-link>\n  for pointing out that a keyword argument would solve the problem.\n <denchmark-link:https://github.com/ycchen1989>@ycchen1989</denchmark-link>\n , I think I can reproduce your problem with the following minimum working example (please reduce your code in future to something as small :) ):\n import pennylane as qml\n import numpy as np\n from pennylane.templates import AmplitudeEmbedding\n \n dev = qml.device('default.qubit', wires=2)\n features = np.array([1/2, 1 / 2, 1/2, 1/2])\n \n @qml.qnode(dev)\n def circuit(f):\n     AmplitudeEmbedding(features=f, wires=range(2))\n     return qml.expval(qml.PauliZ(0))\n \n g = qml.grad(circuit, argnum=[0])\n g(features)\n The issue is that computing gradients with respect to the features of AmplitudeEmbedding is not working.\n To give some background, while for most embeddings (including AngleEmbedding, for example), the features can be \"learnt\", for others (i.e. BasisEmbedding) this is theoretically not possible. AmplitudeEmbedding lies somewhere in between the two: It is theoretically possible for some implementations.\n In fact, the current implementation should support gradients. The template calls the operation QubitStateVector, which at the moment always calls the state preparation template MottonnenStatePreparation which is a rather convoluted quantum algorithm. I will try to track down where gradient calculation fails, and maybe I can make it work.\n Also, we only recently overhauled the templates, and are still working on updating the documentation. It will be made clear in the description of each template how and if differentiation works with each argument. Sorry for not having this up sooner!\n I hope this helps to understand the issue. Thanks for making me aware of it.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "ycchen1989", "commentT": "2019-12-04T19:30:52Z", "comment_text": "\n \t\tAnother Update: Looking deeper into this, MottonnenStatePreparation uses a lot of postprocessing to compute angles and gates for the state preparation circuit. It is very unlikely that we can make this differentiable (and even if we did, it would involve a huge computational graph).\n To summarize, AmplitudeEmbedding circuits are under the hood so complex that computing gradients with respect to features is not feasible for now.\n If you code it up yourself, I recommend actually doing all preprocessing in a classical node (i.e. a normal python function),  and then have the quantum node just use the calculated gate parameters.\n I will make it a lot clearer in the documentation, and throw an error if a user tries to feed features as positional arguments to a qnode!\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "ycchen1989", "commentT": "2020-01-04T08:19:46Z", "comment_text": "\n \t\tHello, I reviewed some of my previous codes, I found that I can do the gradient as I mentioned in this post with version 0.4.0 pennylane!\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "ycchen1989", "commentT": "2020-01-04T20:18:57Z", "comment_text": "\n \t\tThanks for the update <denchmark-link:https://github.com/ycchen1989>@ycchen1989</denchmark-link>\n ! At some point since v0.4.0, the gradient computation of  must have broken. Does the error message return for v0.5.0?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "ycchen1989", "commentT": "2020-01-05T00:12:02Z", "comment_text": "\n \t\tIt can only run successfully on 0.4.0. I tried on 0.5.0 but failed.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "ycchen1989", "commentT": "2020-02-18T06:01:32Z", "comment_text": "\n \t\tClosing this, since the warning has been added to the documentation, and differentiating AmplitudeEmbedding's feature input is no longer intended behaviour.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "ycchen1989", "commentT": "2020-07-05T02:16:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mariaschuld>@mariaschuld</denchmark-link>\n  <denchmark-link:https://github.com/josh146>@josh146</denchmark-link>\n  Thanks for the answers so far. If one wants to stack up a quantum layer after a classical layer, the output of the classical layer (which is a function of its weights and therefore we need to track its gradient) is going to be the input of the quantum layer. What is the procedure then? Because as far as the discussion goes, these values cannot be encoded into the amplitudes of a quantum layer.\n For making my question more clear, imagine we are classifying Mnist dataset, pytorch is being used, device is \"default.qubit\" and the interface=\"torch\".\n we have the output of a convolution layer as (1 * 1 * 16 * 16) meaning there is one batch of one channel of 16*16 pixels. This output can be flatten into (1, 256). What is the best way to encode these 256 tunable variables into a quantum layer parameters?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "ycchen1989", "commentT": "2020-07-06T07:41:20Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/mamadpierre>@mamadpierre</denchmark-link>\n !\n Almost all other embeddings other than AmplitudeEmbedding are differentiable, you seem to have picked the one that is a bit more tricky :) And there are theoretical reasons for this:\n \n AmplitudeEmbedding is in some sense very special, because it first needs to parse a classical input vector into a large number of angles and gate sequences to implement the arbitrary state preparation. In other words, while the maths is easy, the implementation is non-trivial.\n The embedding is also far from hardware efficient, and you will naturally get very long circuits that will take long to train for anything but very few qubits. This is why we allow simulators to \"hack\" the circuit, and replace all those gates by just a single procedure that initialises the state vector in the desired way (using QubitStateVector), but this is what makes things non-differentiable on some devices.\n Lastly, as you know AmplitudeEmbedding maps the output of a classical layer to a quantum state that is identical (up to normalisation) to this output. The quantum circuit after the embedding then applies a unitary transformation on this output. In many cases you could therefore just train a linear layer of a neural net and get something a lot more powerful (since you are not limited to unitary matrices).\n \n In short: unless you have a good theoretical reason to use this embedding (and there are some, I agree), maybe you want to choose another one?\n Having said all that, if you  a good theoretical reason to use amplitude encoding, you could try to use the template <denchmark-link:https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.state_preparations.MottonenStatePreparation.html>MottonnenStatePreparation</denchmark-link>\n  instead, which is what  calls on hardware devices where we cannot apply our non-differentiable hack anyways. It is just an arbitrary state preparation routine. As far as I know, the template is differentiable.\n I hope this helps?\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "ycchen1989", "commentT": "2020-07-06T16:46:43Z", "comment_text": "\n \t\tThanks for your time. Your thoughts are useful. As you said we are away from the hardware efficient implementation of the method. The reason one may want to use AmplitudeEmbedding is its exponential reduction in the number of qubits being used.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "ycchen1989", "commentT": "2020-07-06T17:38:56Z", "comment_text": "\n \t\tYes, that's true. But you'd also get an exponential reduction in the qubits used with other embeddings. As an extreme example, if you encode all inputs into a single qubit, you reduce the number of qubits to a constant! The crux for ML is whether this is actually inducing an interesting representation of your features, which in the case of amplitude encoding is questionable...\n Of course I don't know your application, so don't let that detain you. Just good to think about what actually happens with the data, quantum computer or not :)\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "ycchen1989", "commentT": "2020-07-06T19:01:17Z", "comment_text": "\n \t\tMaybe I am missing something. but I don't think we can. As mentioned in the qml.templates documents (<denchmark-link:https://pennylane.readthedocs.io/en/stable/code/qml_templates.html>here</denchmark-link>\n ), other embeddings need same order of qubits as the features of the data. If you encode all the features into one qubit, via for instance AngleEmbedding, then you have many angle rotations upon each other, making the features dependent of each other. But if you want an angle per feature of the data to learn a representation of it, then . Maybe, I am totally wrong.\n BTW, my application is hybrid Classic-Quantum ML, I want to see whether any quantum circuit can help in the middle of the way to the process of learning.\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "ycchen1989", "commentT": "2020-07-09T09:37:55Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/mamadpierre>@mamadpierre</denchmark-link>\n , no, you are totally right: If you want one feature per qubit (which most of PennyLane's templates assume) then you need . My comment was more for argument's sake: No one stops you from writing your own template which uses a logarithmic number of qubits in the size of the data - AmplitudeEmbedding has no monopoly claims on this, and it is doing something very trivial to the original features.\n I strongly advise treating the embedding not as something to pick blindly, but to be extremely aware of how the quantum state depends on the original features - this may explain a lot of the results you see in the experiments! It may be also a lot more valueable to collect good data on how different embeddings behave, rather than claiming some improvements over randomly chosen classical architectures where the fairness of the comparison is necessarily very hard to judge.\n An interesting angle is also <denchmark-link:https://arxiv.org/abs/1907.02085>in this paper</denchmark-link>\n , where everything is encoded into one qubit, but with learnable rotations in between. Another trainable embedding is PennyLane's <denchmark-link:https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.embeddings.QAOAEmbedding.html>QAOAEmbedding</denchmark-link>\n .\n If you find interesting circuits to embedd data, feel free to make a PR and <denchmark-link:https://pennylane.readthedocs.io/en/stable/development/adding_templates.html>add them to PennyLane</denchmark-link>\n . :)\n \t\t"}}}, "commit": {"commit_id": "c61183d9e62119e33c4daaa2256511a0587914c2", "commit_author": "Maria Schuld", "commitT": "2019-12-05 18:29:37-05:00", "commit_complexity": {"commit_NLOC": "0.16666666666666666", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "pennylane\\templates\\embeddings.py", "file_new_name": "pennylane\\templates\\embeddings.py", "file_complexity": {"file_NLOC": "209", "file_CCN": "27", "file_NToken": "885"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "155,157,158,159", "deleted_lines": "149,151,152,167,168,169,170,171,178", "method_info": {"method_name": "BasisEmbedding", "method_params": "features,wires", "method_startline": "149", "method_endline": "180", "method_complexity": {"method_NLOC": "25", "method_CCN": "3", "method_NToken": "71", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "183,184,185,187,190,194,195,200,202,203,207,215,225,227", "deleted_lines": "183,184,186,189,190,194,195,200,202,203,207,208,216,226", "method_info": {"method_name": "DisplacementEmbedding", "method_params": "features,wires,method,c", "method_startline": "183", "method_endline": "227", "method_complexity": {"method_NLOC": "37", "method_CCN": "4", "method_NToken": "141", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "230,231,232,234,237,238,242,243,248,250,251,255,256,264,274,276", "deleted_lines": "231,232,233,235,238,242,243,248,250,251,255,263,273,275", "method_info": {"method_name": "SqueezingEmbedding", "method_params": "features,wires,method,c", "method_startline": "230", "method_endline": "276", "method_complexity": {"method_NLOC": "38", "method_CCN": "4", "method_NToken": "141", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "41,42,43,44,45,46,94", "deleted_lines": "54", "method_info": {"method_name": "AmplitudeEmbedding", "method_params": "features,wires,pad,normalize", "method_startline": "31", "method_endline": "95", "method_complexity": {"method_NLOC": "53", "method_CCN": "9", "method_NToken": "285", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pennylane\\templates\\utils.py", "file_new_name": "pennylane\\templates\\utils.py", "file_complexity": {"file_NLOC": "75", "file_CCN": "39", "file_NToken": "556"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33,35,36,37", "deleted_lines": "33,34", "method_info": {"method_name": "_check_no_variable", "method_params": "arg,arg_str,msg", "method_startline": "25", "method_endline": "42", "method_complexity": {"method_NLOC": "10", "method_CCN": "7", "method_NToken": "81", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\test_templates_utils.py", "file_new_name": "tests\\test_templates_utils.py", "file_complexity": {"file_NLOC": "152", "file_CCN": "19", "file_NToken": "1918"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "188", "deleted_lines": "188", "method_info": {"method_name": "test_check_no_variables", "method_params": "self,arg", "method_startline": "186", "method_endline": "188", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "18", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "193,194", "deleted_lines": "193,194", "method_info": {"method_name": "test_check_no_variables_exception", "method_params": "self,arg", "method_startline": "191", "method_endline": "194", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "30", "method_nesting_level": "1"}}}}}}}}