{"BR": {"BR_id": "2479", "BR_author": "haixpham", "BRopenT": "2017-10-11T20:21:26Z", "BRcloseT": "2018-02-23T23:24:24Z", "BR_text": {"BRsummary": "CBFDeserializer crashing issue", "BRdescription": "\n I have been using CTFDeserializer with success so far. But with large datasets, the text file grows too big. I tried to switch to CBF, by converting CTF file to CBF with the \"ctf2bin\" tool, which reduces CTF file size from 16GB to CBF file size of 4GB. The reader is defined as:\n def create_reader(path, is_training=True):\n     return C.io.MinibatchSource(C.io.CBFDeserializer(path, C.io.StreamDefs(\n         features = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False),\n         labels = C.io.StreamDef(field='labels', shape=label_dim, is_sparse=False)\n     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n However, the training program crashes at specific times. E.g, if minibatch_size = 200, it crashes at 29th epoch, and at 2nd epoch when minibatch_size=300.\n Note that the same training code with the same data loaded by CTFDeserializer works perfectly. So, the culprit would be somewhere in CBFDeserializer, or I didn't use it correctly.\n In either case, can someone give me a workaround to handle big dataset?\n edit: I'm using CTNK 2.2 GPU, Python 3.6 on Windows.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "haixpham", "commentT": "2017-10-12T01:50:06Z", "comment_text": "\n \t\tCan you please provide us your crash output to debug further?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "haixpham", "commentT": "2017-10-12T01:57:02Z", "comment_text": "\n \t\tI'm not sure if CTNK did log something somewhere. If it does please tell me where to look for it.\n I use Python API, so I couldn't trace the error. It is reported by Visual studio as Memory access violation.\n Furthermore, I repeat the training a few times. Every time I tried with minibatch=200, it crashes at 29th epoch, and 2nd epoch with minibatch=300.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "haixpham", "commentT": "2017-11-10T18:34:12Z", "comment_text": "\n \t\tI'm encountering a similar problem with a .cbf dataset that I'm working with. After reading a fixed number of minibatches, I get a segmentation fault with no other message/logs. However, training is working just fine with the .ctf format. I can provide access to the dataset in both formats if needed.\n \t\t"}}}, "commit": {"commit_id": "695bdf7bdfe06e072c85be9d2c4edec6fd7f1314", "commit_author": "KeDengMS", "commitT": "2018-02-23 11:21:23-08:00", "commit_complexity": {"commit_NLOC": "0.02564102564102564", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "Source\\CNTKv2LibraryDll\\API\\CNTKLibraryExperimental.h", "file_new_name": "Source\\CNTKv2LibraryDll\\API\\CNTKLibraryExperimental.h", "file_complexity": {"file_NLOC": "82", "file_CCN": "6", "file_NToken": "474"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "60,61,62,63,91", "deleted_lines": "160"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "Source\\Readers\\CNTKBinaryReader\\BinaryDataChunk.h", "file_new_name": "Source\\Readers\\CNTKBinaryReader\\BinaryDataChunk.h", "file_complexity": {"file_NLOC": "67", "file_CCN": "13", "file_NToken": "404"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48", "deleted_lines": null, "method_info": {"method_name": "CNTK::BinaryDataChunk::~BinaryDataChunk", "method_params": "", "method_startline": "28", "method_endline": "48", "method_complexity": {"method_NLOC": "18", "method_CCN": "5", "method_NToken": "72", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "Tests\\EndToEndTests\\UnitTests\\CNTKv2Library\\run-test", "file_new_name": "Tests\\EndToEndTests\\UnitTests\\CNTKv2Library\\run-test", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "29", "deleted_lines": null}}}, "file_3": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "Tests\\UnitTests\\ReaderTests\\Data\\CNTKBinaryReader\\Simple_jagged_sequence.bin", "file_new_name": "Tests\\UnitTests\\ReaderTests\\Data\\CNTKBinaryReader\\Simple_jagged_sequence.bin", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "Tests\\UnitTests\\V2LibraryTests\\MinibatchSourceTest.cpp", "file_new_name": "Tests\\UnitTests\\V2LibraryTests\\MinibatchSourceTest.cpp", "file_complexity": {"file_NLOC": "342", "file_CCN": "61", "file_NToken": "2229"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "422", "deleted_lines": null, "method_info": {"method_name": "CNTK::Test::BOOST_AUTO_TEST_CASE", "method_params": "CBFDeserializer", "method_startline": "419", "method_endline": "423", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "14", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321", "deleted_lines": null, "method_info": {"method_name": "CNTK::Test::TestCBFSweepBoundary", "method_params": "", "method_startline": "300", "method_endline": "321", "method_complexity": {"method_NLOC": "20", "method_CCN": "3", "method_NToken": "112", "method_nesting_level": "2"}}}}}}}}