{"BR": {"BR_id": "3259", "BR_author": "chris922", "BRopenT": "2019-05-14T15:58:47Z", "BRcloseT": "2019-05-17T18:28:25Z", "BR_text": {"BRsummary": "ambassador and vizier-core not starting", "BRdescription": "\n I am using current master (commit <denchmark-link:https://github.com/kubeflow/kubeflow/commit/58022fbe26d9fc70a271519c8ae0fbe0f860eb57>58022fb</denchmark-link>\n ) and installed kubeflow on a previously created AWS EKS cluster (thus not using  provided by kubeflow). (3x t3.large nodes, tested in regions  and )\n IAM roles for ALB are in place as well.\n Nevertheless I am not able to get the ambassador and vizier-core pods running.\n Just a bunch of responses for various commands:\n Running pods (ns: kubeflow)\n <denchmark-code>$ kubectl -nkubeflow get po\n NAME                                                        READY     STATUS             RESTARTS   AGE\n alb-ingress-controller-596688f5bd-9ls7c                     2/2       Running            1          46m\n ambassador-796bcd74b5-4p949                                 1/2       CrashLoopBackOff   13         46m\n ambassador-796bcd74b5-gb4pl                                 1/2       CrashLoopBackOff   13         46m\n ambassador-796bcd74b5-rxvvb                                 1/2       CrashLoopBackOff   9          46m\n app-controller-c6d849967-rmc6q                              2/2       Running            0          46m\n argo-ui-bd754894c-twrdk                                     2/2       Running            0          46m\n centraldashboard-799849c495-qtxzk                           2/2       Running            0          46m\n jupyter-0                                                   2/2       Running            0          45m\n jupyter-web-app-56b6bf5bdf-mrt9x                            2/2       Running            0          46m\n katib-ui-7f49d9d8c6-4qdkc                                   2/2       Running            0          46m\n metacontroller-0                                            1/2       Running            0          45m\n minio-5955bbd58c-w6kmk                                      2/2       Running            0          46m\n ml-pipeline-7c697c9ffd-mcbzj                                2/2       Running            6          46m\n ml-pipeline-persistenceagent-7bffd45959-7wwfr               2/2       Running            0          46m\n ml-pipeline-scheduledworkflow-68f4b77777-gdjxx              2/2       Running            0          46m\n ml-pipeline-ui-5c947b8c57-x9gxk                             2/2       Running            0          46m\n ml-pipeline-viewer-controller-deployment-6b77d4c8cf-bmwgv   2/2       Running            1          46m\n mysql-854c579944-7z9qp                                      2/2       Running            0          46m\n notebooks-controller-68f9578d4c-js8jq                       2/2       Running            1          46m\n profiles-7db5484769-trbg4                                   2/2       Running            1          46m\n pytorch-operator-dc7b79d5c-gdk2h                            2/2       Running            0          46m\n spartakus-volunteer-95694bbf-r7q6b                          2/2       Running            0          46m\n studyjob-controller-54cdf77cc9-j7bb6                        2/2       Running            1          46m\n tf-job-dashboard-76558cbb57-6sgdd                           2/2       Running            0          45m\n tf-job-operator-757658b5cd-9h9qj                            2/2       Running            0          45m\n vizier-core-744646bc44-77hhq                                1/2       CrashLoopBackOff   7          13m\n vizier-core-rest-5d86bbf7bd-7hqts                           2/2       Running            0          45m\n vizier-db-6586c85f4d-m4krn                                  2/2       Running            0          45m\n vizier-suggestion-bayesianoptimization-5c4dc96968-vk28b     2/2       Running            0          45m\n vizier-suggestion-grid-6ddf58868-9vbfw                      2/2       Running            0          45m\n vizier-suggestion-hyperband-56f9689bbb-xthhj                2/2       Running            0          45m\n vizier-suggestion-random-674d55dc56-t5mmn                   2/2       Running            0          45m\n workflow-controller-694c8b98c4-mltrs                        2/2       Running            1          45m\n </denchmark-code>\n \n Running pods (ns: istio-system)\n <denchmark-code>$ kubectl -nistio-system get po\n NAME                                                           READY     STATUS                       RESTARTS   AGE\n grafana-5c45779547-nhl4x                                       1/1       Running                      0          36m\n istio-citadel-6d7f8c5d-wzb9c                                   1/1       Running                      0          36m\n istio-cleanup-secrets-release-1.1-20190111-09-15-hh4ps         0/1       Completed                    0          36m\n istio-egressgateway-6f8b444984-p5mt5                           1/1       Running                      0          36m\n istio-galley-685ffb6bf4-4pmcc                                  1/1       Running                      0          36m\n istio-grafana-post-install-release-1.1-20190111-09-15-tnqs5    0/1       Completed                    0          36m\n istio-ingressgateway-7f7878c979-hv256                          1/1       Running                      0          36m\n istio-pilot-5d69c5d77c-lwcgr                                   2/2       Running                      0          36m\n istio-policy-59467fccc9-jdqkr                                  2/2       Running                      2          36m\n istio-security-post-install-release-1.1-20190111-09-15-l85hl   0/1       Completed                    0          36m\n istio-sidecar-injector-57d76787db-rqblh                        1/1       Running                      0          36m\n istio-telemetry-7c84b58bb5-ts24d                               2/2       Running                      2          36m\n istio-tracing-5445d89986-r5l95                                 1/1       Running                      0          36m\n kiali-69d7f779c8-rtmdg                                         0/1       CreateContainerConfigError   0          36m\n prometheus-5467b4b887-bbwlh                                    1/1       Running                      0          36m\n servicegraph-6ddb7d7d7-pccpp                                   1/1       Running                      0          36m\n </denchmark-code>\n \n (kiali not starting due to missing kiali-secret, but I doubt this is a problem)\n Describe vizier-core pod\n <denchmark-code>$ kubectl -nkubeflow describe po vizier-core-744646bc44-77hhq\n Name:               vizier-core-744646bc44-77hhq\n Namespace:          kubeflow\n Priority:           0\n PriorityClassName:  <none>\n Node:               ip-10-0-0-82.eu-west-1.compute.internal/10.0.0.82\n Start Time:         Tue, 14 May 2019 16:44:45 +0200\n Labels:             app=vizier\n                     app.kubernetes.io/name=app\n                     component=core\n                     pod-template-hash=744646bc44\n Annotations:        sidecar.istio.io/status={\"version\":\"13b1f7694262f17b8ed7fc57080d5da3c9280d53d717f3290ef10d460f335d2b\",\"initContainers\":[\"istio-init\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs...\n Status:             Running\n IP:                 10.0.0.178\n Controlled By:      ReplicaSet/vizier-core-744646bc44\n Init Containers:\n   istio-init:\n     Container ID:  docker://7c145dfcd34aeb2a0822b0893b48cc8871cad1556cdc2256e75e39f92551b260\n     Image:         gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15\n     Image ID:      docker-pullable://gcr.io/istio-release/proxy_init@sha256:e661adbfb76a29a8b41ebe166eb5d4fd927d3fe5bbc7b62907ab4ae56407c208\n     Port:          <none>\n     Host Port:     <none>\n     Args:\n       -p\n       15001\n       -u\n       1337\n       -m\n       REDIRECT\n       -i\n       *\n       -x\n \n       -b\n       6789\n       -d\n       15020\n     State:          Terminated\n       Reason:       Completed\n       Exit Code:    0\n       Started:      Tue, 14 May 2019 16:44:47 +0200\n       Finished:     Tue, 14 May 2019 16:44:54 +0200\n     Ready:          True\n     Restart Count:  0\n     Limits:\n       cpu:     10m\n       memory:  10Mi\n     Requests:\n       cpu:        10m\n       memory:     10Mi\n     Environment:  <none>\n     Mounts:       <none>\n Containers:\n   vizier-core:\n     Container ID:  docker://1e7f08118a6e07b929254c86e0f5cba8c47cb42c8bf06923662d5e9b168ef6d5\n     Image:         gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd\n     Image ID:      docker-pullable://gcr.io/kubeflow-images-public/katib/vizier-core@sha256:12d0a1196f49f4d492f01ec8028ee400d0be4f3b083787b04c2232dc2e28af2c\n     Port:          6789/TCP\n     Host Port:     0/TCP\n     Command:\n       ./vizier-manager\n     State:          Waiting\n       Reason:       CrashLoopBackOff\n     Last State:     Terminated\n       Reason:       Error\n       Exit Code:    2\n       Started:      Tue, 14 May 2019 16:53:48 +0200\n       Finished:     Tue, 14 May 2019 16:54:23 +0200\n     Ready:          False\n     Restart Count:  7\n     Liveness:       exec [/bin/grpc_health_probe -addr=:6789] delay=10s timeout=1s period=10s #success=1 #failure=3\n     Readiness:      exec [/bin/grpc_health_probe -addr=:6789] delay=5s timeout=1s period=10s #success=1 #failure=3\n     Environment:\n       MYSQL_ROOT_PASSWORD:  <set to the key 'MYSQL_ROOT_PASSWORD' in secret 'vizier-db-secrets'>  Optional: false\n     Mounts:\n       /var/run/secrets/kubernetes.io/serviceaccount from default-token-tjmzq (ro)\n   istio-proxy:\n     Container ID:  docker://5f0ecac6df0a8f75e096ba29d3f8de325f5e98138aad00d6dd8cbc843214e17e\n     Image:         gcr.io/istio-release/proxyv2:release-1.1-20190111-09-15\n     Image ID:      docker-pullable://gcr.io/istio-release/proxyv2@sha256:325ee8ba87ce70b13dfbc9cdd0358989911f7e517ae37ba391b2c237489c973d\n     Port:          15090/TCP\n     Host Port:     0/TCP\n     Args:\n       proxy\n       sidecar\n       --configPath\n       /etc/istio/proxy\n       --binaryPath\n       /usr/local/bin/envoy\n       --serviceCluster\n       vizier.kubeflow\n       --drainDuration\n       45s\n       --parentShutdownDuration\n       1m0s\n       --discoveryAddress\n       istio-pilot.istio-system:15010\n       --zipkinAddress\n       zipkin.istio-system:9411\n       --connectTimeout\n       10s\n       --proxyAdminPort\n       15000\n       --controlPlaneAuthPolicy\n       NONE\n       --statusPort\n       15020\n       --applicationPorts\n       6789\n     State:          Running\n       Started:      Tue, 14 May 2019 16:44:55 +0200\n     Ready:          True\n     Restart Count:  0\n     Requests:\n       cpu:      10m\n     Readiness:  http-get http://:15020/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30\n     Environment:\n       POD_NAME:                      vizier-core-744646bc44-77hhq (v1:metadata.name)\n       POD_NAMESPACE:                 kubeflow (v1:metadata.namespace)\n       INSTANCE_IP:                    (v1:status.podIP)\n       ISTIO_META_POD_NAME:           vizier-core-744646bc44-77hhq (v1:metadata.name)\n       ISTIO_META_CONFIG_NAMESPACE:   kubeflow (v1:metadata.namespace)\n       ISTIO_META_INTERCEPTION_MODE:  REDIRECT\n       ISTIO_METAJSON_LABELS:         {\"app\":\"vizier\",\"app.kubernetes.io/name\":\"app\",\"component\":\"core\",\"pod-template-hash\":\"744646bc44\"}\n \n     Mounts:\n       /etc/certs/ from istio-certs (ro)\n       /etc/istio/proxy from istio-envoy (rw)\n       /var/run/secrets/kubernetes.io/serviceaccount from default-token-tjmzq (ro)\n Conditions:\n   Type              Status\n   Initialized       True\n   Ready             False\n   ContainersReady   False\n   PodScheduled      True\n Volumes:\n   default-token-tjmzq:\n     Type:        Secret (a volume populated by a Secret)\n     SecretName:  default-token-tjmzq\n     Optional:    false\n   istio-envoy:\n     Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n     Medium:  Memory\n   istio-certs:\n     Type:        Secret (a volume populated by a Secret)\n     SecretName:  istio.default\n     Optional:    true\n QoS Class:       Burstable\n Node-Selectors:  <none>\n Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                  node.kubernetes.io/unreachable:NoExecute for 300s\n Events:\n   Type     Reason     Age                From                                              Message\n   ----     ------     ----               ----                                              -------\n   Normal   Scheduled  13m                default-scheduler                                 Successfully assigned kubeflow/vizier-core-744646bc44-77hhq to ip-10-0-0-82.eu-west-1.compute.internal\n   Normal   Pulled     13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image \"gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15\" already present on machine\n   Normal   Created    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container\n   Normal   Started    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container\n   Normal   Created    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container\n   Normal   Pulled     13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image \"gcr.io/istio-release/proxyv2:release-1.1-20190111-09-15\" already present on machine\n   Normal   Started    13m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container\n   Normal   Killing    12m                kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Killing container with id docker://vizier-core:Container failed liveness probe.. Container will be killed and recreated.\n   Normal   Created    12m (x2 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container\n   Normal   Pulled     12m (x2 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image \"gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd\" already present on machine\n   Normal   Started    12m (x2 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container\n   Warning  Unhealthy  12m (x7 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Readiness probe failed: timeout: failed to connect service \":6789\" within 1s\n   Warning  Unhealthy  12m (x5 over 13m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Liveness probe failed: timeout: failed to connect service \":6789\" within 1s\n   Warning  BackOff    3m (x26 over 9m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Back-off restarting failed container\n </denchmark-code>\n \n Logs vizier-core\n <denchmark-code>$ kubectl -nkubeflow logs vizier-core-744646bc44-77hhq vizier-core                                                                                                                                                 \n [mysql] 2019/05/14 14:45:54 packets.go:36: unexpected EOF\n </denchmark-code>\n \n Describe ambassador pod\n <denchmark-code>$ kubectl -nkubeflow describe po ambassador-796bcd74b5-4p949\n Name:               ambassador-796bcd74b5-4p949\n Namespace:          kubeflow\n Priority:           0\n PriorityClassName:  <none>\n Node:               ip-10-0-0-82.eu-west-1.compute.internal/10.0.0.82\n Start Time:         Tue, 14 May 2019 16:11:53 +0200\n Labels:             app.kubernetes.io/name=app\n                     pod-template-hash=796bcd74b5\n                     service=ambassador\n Annotations:        sidecar.istio.io/status={\"version\":\"13b1f7694262f17b8ed7fc57080d5da3c9280d53d717f3290ef10d460f335d2b\",\"initContainers\":[\"istio-init\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs...\n Status:             Running\n IP:                 10.0.0.206\n Controlled By:      ReplicaSet/ambassador-796bcd74b5\n Init Containers:\n   istio-init:\n     Container ID:  docker://00d470ba3f099417a21a1660045f4a7b083c0389c76e0a9c87bd22e7475b6170\n     Image:         gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15\n     Image ID:      docker-pullable://gcr.io/istio-release/proxy_init@sha256:e661adbfb76a29a8b41ebe166eb5d4fd927d3fe5bbc7b62907ab4ae56407c208\n     Port:          <none>\n     Host Port:     <none>\n     Args:\n       -p\n       15001\n       -u\n       1337\n       -m\n       REDIRECT\n       -i\n       *\n       -x\n \n       -b\n \n       -d\n       15020\n     State:          Terminated\n       Reason:       Completed\n       Exit Code:    0\n       Started:      Tue, 14 May 2019 16:14:51 +0200\n       Finished:     Tue, 14 May 2019 16:14:57 +0200\n     Ready:          True\n     Restart Count:  5\n     Limits:\n       cpu:     10m\n       memory:  10Mi\n     Requests:\n       cpu:        10m\n       memory:     10Mi\n     Environment:  <none>\n     Mounts:       <none>\n Containers:\n   ambassador:\n     Container ID:   docker://cbe4cd544bb744f2a4a465d28bfdced88c97da4d9a5d20e7a3faec30a1716a8c\n     Image:          quay.io/datawire/ambassador:0.37.0\n     Image ID:       docker-pullable://quay.io/datawire/ambassador@sha256:54b8cf9d1294b91e255f275efe9ddd33544edd29325e02e606c00cf4e5304154\n     Port:           <none>\n     Host Port:      <none>\n     State:          Waiting\n       Reason:       CrashLoopBackOff\n     Last State:     Terminated\n       Reason:       Error\n       Exit Code:    1\n       Started:      Tue, 14 May 2019 16:57:38 +0200\n       Finished:     Tue, 14 May 2019 16:57:42 +0200\n     Ready:          False\n     Restart Count:  13\n     Limits:\n       cpu:     1\n       memory:  400Mi\n     Requests:\n       cpu:      200m\n       memory:   100Mi\n     Liveness:   http-get http://:8877/ambassador/v0/check_alive delay=30s timeout=1s period=30s #success=1 #failure=3\n     Readiness:  http-get http://:8877/ambassador/v0/check_ready delay=30s timeout=1s period=30s #success=1 #failure=3\n     Environment:\n       AMBASSADOR_NAMESPACE:  kubeflow (v1:metadata.namespace)\n     Mounts:\n       /var/run/secrets/kubernetes.io/serviceaccount from ambassador-token-smjq7 (ro)\n   istio-proxy:\n     Container ID:  docker://b296e41760bc00b12b90b05e1223c110b44f99939362ec10a30ab8bc7929ddcf\n     Image:         gcr.io/istio-release/proxyv2:release-1.1-20190111-09-15\n     Image ID:      docker-pullable://gcr.io/istio-release/proxyv2@sha256:325ee8ba87ce70b13dfbc9cdd0358989911f7e517ae37ba391b2c237489c973d\n     Port:          15090/TCP\n     Host Port:     0/TCP\n     Args:\n       proxy\n       sidecar\n       --configPath\n       /etc/istio/proxy\n       --binaryPath\n       /usr/local/bin/envoy\n       --serviceCluster\n       istio-proxy.kubeflow\n       --drainDuration\n       45s\n       --parentShutdownDuration\n       1m0s\n       --discoveryAddress\n       istio-pilot.istio-system:15010\n       --zipkinAddress\n       zipkin.istio-system:9411\n       --connectTimeout\n       10s\n       --proxyAdminPort\n       15000\n       --controlPlaneAuthPolicy\n       NONE\n       --statusPort\n       15020\n       --applicationPorts\n \n     State:          Running\n       Started:      Tue, 14 May 2019 16:15:16 +0200\n     Ready:          True\n     Restart Count:  0\n     Requests:\n       cpu:      10m\n     Readiness:  http-get http://:15020/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30\n     Environment:\n       POD_NAME:                      ambassador-796bcd74b5-4p949 (v1:metadata.name)\n       POD_NAMESPACE:                 kubeflow (v1:metadata.namespace)\n       INSTANCE_IP:                    (v1:status.podIP)\n       ISTIO_META_POD_NAME:           ambassador-796bcd74b5-4p949 (v1:metadata.name)\n       ISTIO_META_CONFIG_NAMESPACE:   kubeflow (v1:metadata.namespace)\n       ISTIO_META_INTERCEPTION_MODE:  REDIRECT\n       ISTIO_METAJSON_LABELS:         {\"app.kubernetes.io/name\":\"app\",\"pod-template-hash\":\"796bcd74b5\",\"service\":\"ambassador\"}\n \n     Mounts:\n       /etc/certs/ from istio-certs (ro)\n       /etc/istio/proxy from istio-envoy (rw)\n       /var/run/secrets/kubernetes.io/serviceaccount from ambassador-token-smjq7 (ro)\n Conditions:\n   Type              Status\n   Initialized       True\n   Ready             False\n   ContainersReady   False\n   PodScheduled      True\n Volumes:\n   ambassador-token-smjq7:\n     Type:        Secret (a volume populated by a Secret)\n     SecretName:  ambassador-token-smjq7\n     Optional:    false\n   istio-envoy:\n     Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n     Medium:  Memory\n   istio-certs:\n     Type:        Secret (a volume populated by a Secret)\n     SecretName:  istio.ambassador\n     Optional:    true\n QoS Class:       Burstable\n Node-Selectors:  <none>\n Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                  node.kubernetes.io/unreachable:NoExecute for 300s\n Events:\n   Type     Reason     Age                 From                                              Message\n   ----     ------     ----                ----                                              -------\n   Normal   Scheduled  48m                 default-scheduler                                 Successfully assigned kubeflow/ambassador-796bcd74b5-4p949 to ip-10-0-0-82.eu-west-1.compute.internal\n   Normal   Pulling    48m                 kubelet, ip-10-0-0-82.eu-west-1.compute.internal  pulling image \"gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15\"\n   Normal   Pulled     47m                 kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Successfully pulled image \"gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15\"\n   Normal   Created    46m (x5 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Created container\n   Warning  Failed     46m (x5 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Error: failed to start container \"istio-init\": Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused \"process_linux.go:402: container init caused \\\"process_linux.go:367: setting cgroup config for procHooks process caused \\\\\\\"failed to write 10485760 to memory.limit_in_bytes: write /sys/fs/cgroup/memory/kubepods/burstable/pod39aa57ba-7652-11e9-84fe-027424f7c2d4/istio-init/memory.limit_in_bytes: device or resource busy\\\\\\\"\\\"\": unknown\n   Normal   Pulled     46m (x4 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image \"gcr.io/istio-release/proxy_init:release-1.1-20190111-09-15\" already present on machine\n   Warning  BackOff    45m (x9 over 47m)   kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Back-off restarting failed container\n   Normal   Started    23m (x10 over 44m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Started container\n   Normal   Pulled     17m (x10 over 44m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Container image \"quay.io/datawire/ambassador:0.37.0\" already present on machine\n   Warning  BackOff    3m (x196 over 44m)  kubelet, ip-10-0-0-82.eu-west-1.compute.internal  Back-off restarting failed container\n </denchmark-code>\n \n Logs ambassador\n <denchmark-code>$ kubectl -nkubeflow logs ambassador-796bcd74b5-4p949 ambassador\n 2019-05-14 14:47:17 kubewatch 0.37.0 INFO: generating config with gencount 1 (8 changes)\n /usr/lib/python3.6/site-packages/pkg_resources/__init__.py:1298: UserWarning: /ambassador is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable).\n   warnings.warn(msg, UserWarning)\n 2019-05-14 14:47:18 kubewatch 0.37.0 INFO: Scout reports {\"latest_version\": \"0.61.0\", \"application\": \"ambassador\", \"notices\": [], \"cached\": false, \"timestamp\": 1557845237.926627}\n [2019-05-14 14:47:18.564][13][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)\n [2019-05-14 14:47:18.566][13][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized\n [2019-05-14 14:47:18.566][13][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)\n [2019-05-14 14:47:18.586][13][info][config] source/server/configuration_impl.cc:94] loading tracing configuration\n [2019-05-14 14:47:18.586][13][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration\n AMBASSADOR: starting diagd\n AMBASSADOR: starting Envoy\n AMBASSADOR: waiting\n PIDS: 17:diagd 18:envoy 19:kubewatch\n starting hot-restarter with target: /ambassador/start-envoy.sh\n forking and execing new child process at epoch 0\n forked new child process with PID=20\n got SIGCHLD\n PID=20 exited with code=1\n Due to abnormal exit, force killing all child processes and exiting\n exiting due to lack of child processes\n AMBASSADOR: envoy exited with status 1\n Here's the envoy.json we were trying to run with:\n {\n   \"listeners\": [\n \n     {\n       \"address\": \"tcp://0.0.0.0:80\",\n \n       \"filters\": [\n         {\n           \"type\": \"read\",\n           \"name\": \"http_connection_manager\",\n           \"config\": {\"codec_type\": \"auto\",\n             \"stat_prefix\": \"ingress_http\",\n             \"access_log\": [\n               {\n                 \"format\": \"ACCESS [%START_TIME%] \\\"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\"%REQ(X-FORWARDED-FOR)%\\\" \\\"%REQ(USER-AGENT)%\\\" \\\"%REQ(X-REQUEST-ID)%\\\" \\\"%REQ(:AUTHORITY)%\\\" \\\"%UPSTREAM_HOST%\\\"\\n\",\n                 \"path\": \"/dev/fd/1\"\n               }\n             ],\n \n             \"route_config\": {\n               \"virtual_hosts\": [\n                 {\n                   \"name\": \"backend\",\n                   \"domains\": [\"*\"],\"routes\": [\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/ambassador/v0/check_ready\",\"prefix_rewrite\": \"/ambassador/v0/check_ready\",\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_127_0_0_1_8877\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/ambassador/v0/check_alive\",\"prefix_rewrite\": \"/ambassador/v0/check_alive\",\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_127_0_0_1_8877\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/ambassador/v0/\",\"prefix_rewrite\": \"/ambassador/v0/\",\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_127_0_0_1_8877\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 300000,\"prefix\": \"/pipeline\",\"use_websocket\": true,\"prefix_rewrite\": \"/pipeline\",\"cluster\": \"cluster_ml_pipeline_ui_kubeflow\"\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/jupyter/\",\"prefix_rewrite\": \"/\",\"request_headers_to_add\": [{\"key\": \"x-forwarded-prefix\", \"value\": \"/jupyter\"}],\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_jupyter_web_app_kubeflow\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/tfjobs/\",\"prefix_rewrite\": \"/tfjobs/\",\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_tf_job_dashboard_kubeflow\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/katib/\",\"prefix_rewrite\": \"/katib/\",\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_katib_ui_kubeflow\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 300000,\"prefix\": \"/user/\",\"use_websocket\": true,\"prefix_rewrite\": \"/user/\",\"cluster\": \"cluster_jupyter_lb_kubeflow\"\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/argo/\",\"prefix_rewrite\": \"/\",\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_argo_ui_kubeflow\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 300000,\"prefix\": \"/hub/\",\"use_websocket\": true,\"prefix_rewrite\": \"/hub/\",\"cluster\": \"cluster_jupyter_lb_kubeflow\"\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 300000,\"prefix\": \"/data\",\"use_websocket\": true,\"prefix_rewrite\": \"/data\",\"cluster\": \"cluster_ml_pipeline_ui_kubeflow\"\n \n \n                     }\n                     ,\n \n                     {\n                       \"timeout_ms\": 3000,\"prefix\": \"/\",\"prefix_rewrite\": \"/\",\"weighted_clusters\": {\n                               \"clusters\": [\n \n                                     { \"name\": \"cluster_centraldashboard_kubeflow\", \"weight\": 100.0 }\n \n                               ]\n                           }\n \n \n                     }\n \n \n                   ]\n                 }\n               ]\n             },\n             \"filters\": [\n               {\n                 \"name\": \"cors\",\n                 \"config\": {}\n               },{\"type\": \"decoder\",\n                 \"name\": \"router\",\n                 \"config\": {}\n               }\n             ]\n           }\n         }\n       ]\n     }\n   ],\n   \"admin\": {\n     \"address\": \"tcp://127.0.0.1:8001\",\n     \"access_log_path\": \"/tmp/admin_access_log\"\n   },\n \n   \"cluster_manager\": {\n     \"clusters\": [\n       {\n         \"name\": \"cluster_127_0_0_1_8877\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://127.0.0.1:8877\"\n           }\n \n         ]},\n       {\n         \"name\": \"cluster_argo_ui_kubeflow\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://argo-ui.kubeflow:80\"\n           }\n \n         ]},\n       {\n         \"name\": \"cluster_centraldashboard_kubeflow\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://centraldashboard.kubeflow:80\"\n           }\n \n         ]},\n       {\n         \"name\": \"cluster_jupyter_lb_kubeflow\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://jupyter-lb.kubeflow:80\"\n           }\n \n         ]},\n       {\n         \"name\": \"cluster_jupyter_web_app_kubeflow\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://jupyter-web-app.kubeflow:80\"\n           }\n \n         ]},\n       {\n         \"name\": \"cluster_katib_ui_kubeflow\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://katib-ui.kubeflow:80\"\n           }\n \n         ]},\n       {\n         \"name\": \"cluster_ml_pipeline_ui_kubeflow\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://ml-pipeline-ui.kubeflow:80\"\n           }\n \n         ]},\n       {\n         \"name\": \"cluster_tf_job_dashboard_kubeflow\",\n         \"connect_timeout_ms\": 3000,\n         \"type\": \"strict_dns\",\n         \"lb_type\": \"round_robin\",\n         \"hosts\": [\n           {\n             \"url\": \"tcp://tf-job-dashboard.kubeflow:80\"\n           }\n \n         ]}\n \n     ]\n   },\n \n   \"statsd_udp_ip_address\": \"127.0.0.1:8125\",\n   \"stats_flush_interval_ms\": 1000\n }AMBASSADOR: shutting down (1)\n </denchmark-code>\n \n Could this here be the error? I saw the same error also on other pods, but it looks like they were able to start successfully after a few retries.. just the ambassador one keeps logging this event.\n <denchmark-code>Error: failed to start container \"istio-init\": Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused \"process_linux.go:402: container init caused \\\"process_linux.go:367: setting cgroup config for procHooks process caused \\\\\\\"failed to write 10485760 to memory.limit_in_bytes: write /sys/fs/cgroup/memory/kubepods/burstable/pod39aa57ba-7652-11e9-84fe-027424f7c2d4/istio-init/memory.limit_in_bytes: device or resource busy\\\\\\\"\\\"\": unknown\n </denchmark-code>\n \n There is enough memory available on all nodes, there is also only kubeflow installed on the nodes, no other applications/pods running.\n Security group nodes:\n Outbound to *: All traffic allowed\n Inbound from node: All traffic allowed\n Inbound from EKS control: 443, 1025 - 65535\n Security group EKS control:\n Outbound to *: All traffic allowed\n Inbound from node: 443\n Do you need some other information?\n EDIT: Just used v0.5.1 and the same issue occurs.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "chris922", "commentT": "2019-05-14T15:58:51Z", "comment_text": "\n \t\tIssue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.71. Please mark this comment with \ud83d\udc4d or \ud83d\udc4e to give our bot feedback!\n Links: <denchmark-link:https://github.com/marketplace/issue-label-bot>app homepage</denchmark-link>\n , <denchmark-link:https://mlbot.net/data/kubeflow/kubeflow>dashboard</denchmark-link>\n  and <denchmark-link:https://github.com/hamelsmu/MLapp>code</denchmark-link>\n  for this bot.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "chris922", "commentT": "2019-05-14T18:00:45Z", "comment_text": "\n \t\tI confirm that I encountered the same error when trying to deploy kubeflow with Istio enabled, using platform=none.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "chris922", "commentT": "2019-05-14T19:13:55Z", "comment_text": "\n \t\tIt seems that you have istio injector on for kubeflow namespace.\n We haven't done that yet, do you manually change it?\n Istio sidecar injector cannot be apply to ambassador (because they are both envoy). So you should exclude ambassador (you can add annotation to the pod).\n Not sure about vizier though.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "chris922", "commentT": "2019-05-14T19:39:08Z", "comment_text": "\n \t\tThanks for the fast answers!\n \n It seems that you have istio injector on for kubeflow namespace.\n We haven't done that yet, do you manually change it?\n \n I haven't done this manually, I will try to disable this tomorrow.. just had a quick look and it seems that the kfctl.sh script activated it:\n \n \n \n kubeflow/scripts/aws/util.sh\n \n \n         Lines 154 to 157\n       in\n       4b1ae00\n \n \n \n \n \n \n  kubectl apply -f ${KUBEFLOW_K8S_MANIFESTS_DIR}/istio-crds.yaml \n \n \n \n  kubectl apply -f ${KUBEFLOW_K8S_MANIFESTS_DIR}/istio-noauth.yaml \n \n \n \n  \n \n \n \n  kubectl label namespace ${K8S_NAMESPACE} istio-injection=enabled --overwrite \n \n \n \n \n \n Is the istio injection required to get the istio-ingress working?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "chris922", "commentT": "2019-05-15T13:20:47Z", "comment_text": "\n \t\tI removed the label and all pods started successfully!\n For me personally the ticket can be closed, but maybe you want to remove the istio injection until it is available? <denchmark-link:https://github.com/Jeffwan>@Jeffwan</denchmark-link>\n  you added the injection according to git-blame, WDYT?\n If you like I can provide a PR that just removes the line that sets the istio-injection label.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "chris922", "commentT": "2019-05-15T15:59:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/chris922>@chris922</denchmark-link>\n  I make the change in 0.5 and recently istio changes comes in master branch. I will have a check and come back to you soon.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "chris922", "commentT": "2019-05-15T20:38:44Z", "comment_text": "\n \t\tConfigMap  policy has been changed from  to   in this commit.\n <denchmark-link:https://github.com/kubeflow/kubeflow/commit/2455ba6134dff4556471a3cafd9bcdef3e0fb241#diff-1ac92c9005d2565157922d5a08fe8166>2455ba6#diff-1ac92c9005d2565157922d5a08fe8166</denchmark-link>\n \n That's the reason enabled namespace  + enabled configmap will automatically inject sidecars.\n In 0.5, istio is not default installed in kubeflow and AWS solutions install it by default to integrate with services. enabled namespace + disabled configmap will make users determine if they'd like to use istio or not by adding pod annotation sidecar.istio.io/inject.\n <denchmark-link:https://github.com/lluunn>@lluunn</denchmark-link>\n  <denchmark-link:https://github.com/kunmingg>@kunmingg</denchmark-link>\n   Any reason to turn on  ? I need to do corresponding changes for AWS.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "chris922", "commentT": "2019-05-15T20:48:05Z", "comment_text": "\n \t\tThat's part of 0.6's multi-user support\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "chris922", "commentT": "2019-05-15T20:57:07Z", "comment_text": "\n \t\t\n That's part of 0.6's multi-user support\n \n <denchmark-link:https://github.com/lluunn>@lluunn</denchmark-link>\n \n Does it mean users won't deploy any workloads needs istio in kubeflow namespace? I notice code base still have some references for . If istio inject is disabled at kubeflow namespace level, this will definitely not work unless it's deployed in other namespaces.\n Another concern is all the resources deployed in namespace enabled injection will have sidecar?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "chris922", "commentT": "2019-05-15T21:00:40Z", "comment_text": "\n \t\tI don't quite understand the question and concern.\n The plan is to turn on sidecar whenever makes sense and use istio features.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "chris922", "commentT": "2019-05-15T21:36:08Z", "comment_text": "\n \t\t\n I don't quite understand the question and concern.\n The plan is to turn on sidecar whenever makes sense and use istio features.\n \n True. There're many ways to turn on sidecar injection.\n My question is if using disable for ConfigMap and adjust namespace and pod policy is better?\n Or use enable for ConfigMap is better? This major difference is default behavior.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "chris922", "commentT": "2019-05-16T00:16:31Z", "comment_text": "\n \t\tI think we want the default is on so that everything is in istio mesh.\n Those cannot be in istio mesh we can use pod annotation to exclude them.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "chris922", "commentT": "2019-05-16T01:15:44Z", "comment_text": "\n \t\t\n I think we want the default is on so that everything is in istio mesh.\n Those cannot be in istio mesh we can use pod annotation to exclude them.\n \n OK. Then we will make some changes on AWS side. <denchmark-link:https://github.com/chris922>@chris922</denchmark-link>\n  could you file a PR to update kubeflow namespace label?  Another improvement is use same version istio as kfctl version.\n \n \n Thanks a lot! I can help review.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "chris922", "commentT": "2019-09-14T03:08:38Z", "comment_text": "\n \t\tHello,\n I got the same error after installing kubernetes and kuberflow on my server machine.\n Three ambassador pods are stuck in CrashLoopBackOff status, vizier-core is also same.\n Ambassador log says envoy exited with code 1.\n The problem is I'm very new to kubernetes & kubeflow; I don't know how to \"manually disable\" istio-sidecar-injector.\n For installation, I followed the steps in <denchmark-link:https://github.com/NVIDIA/deepops>https://github.com/NVIDIA/deepops</denchmark-link>\n , particularly the instructions on <denchmark-link:https://github.com/NVIDIA/deepops/blob/master/docs/kubernetes-cluster.md>https://github.com/NVIDIA/deepops/blob/master/docs/kubernetes-cluster.md</denchmark-link>\n .\n The instruction is to use ansible to remotely install kubernetes and kubeflow to the server machine.\n Please, anyone let me know which file(s) to correct and how.\n If pointing to a specific file is difficult, please give me some clues what to look for.\n Thank you very much for helping me.\n \t\t"}}}, "commit": {"commit_id": "e1d26f6f03db221a85b44f7bc3e0fd8c4ef1df95", "commit_author": "Christian Bandowski", "commitT": "2019-05-17 11:28:24-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "scripts\\aws\\util.sh", "file_new_name": "scripts\\aws\\util.sh", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "149,156,157", "deleted_lines": "149,150,157"}}}}}}