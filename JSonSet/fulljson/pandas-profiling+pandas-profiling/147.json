{"BR": {"BR_id": "147", "BR_author": "geoHeil", "BRopenT": "2018-12-09T12:50:18Z", "BRcloseT": "2019-05-31T04:57:30Z", "BR_text": {"BRsummary": "pyarrow pandas dataframe buffer source array is read-only exception", "BRdescription": "\n When a pyArrow backed pandas parquet Dataframe is used with pandas-profiling:\n I get the stack trace below. However, when using a CSV backed data frame with the same data pandas-profiling works just fine.\n <denchmark-code>--------------------------------------------------------------------------\n ValueError                                Traceback (most recent call last)\n <ipython-input-9-4f6d7d159b1b> in <module>\n ----> 1 pandas_profiling.ProfileReport(aa)\n \n /opt/conda/lib/python3.6/site-packages/pandas_profiling/__init__.py in __init__(self, df, **kwargs)\n      64         sample = kwargs.get('sample', df.head())\n      65 \n ---> 66         description_set = describe(df, **kwargs)\n      67 \n      68         self.html = to_html(sample,\n \n /opt/conda/lib/python3.6/site-packages/pandas_profiling/describe.py in describe(df, bins, check_correlation, correlation_threshold, correlation_overrides, check_recoded, pool_size, **kwargs)\n     413         'table': table_stats,\n     414         'variables': variable_stats.T,\n --> 415         'freq': {k: (base.get_groupby_statistic(df[k])[0] if variable_stats[k].type != base.S_TYPE_UNSUPPORTED else None) for k in df.columns},\n     416         'correlations': {'pearson': dfcorrPear, 'spearman': dfcorrSpear}\n     417     }\n \n /opt/conda/lib/python3.6/site-packages/pandas_profiling/describe.py in <dictcomp>(.0)\n     413         'table': table_stats,\n     414         'variables': variable_stats.T,\n --> 415         'freq': {k: (base.get_groupby_statistic(df[k])[0] if variable_stats[k].type != base.S_TYPE_UNSUPPORTED else None) for k in df.columns},\n     416         'correlations': {'pearson': dfcorrPear, 'spearman': dfcorrSpear}\n     417     }\n \n /opt/conda/lib/python3.6/site-packages/pandas_profiling/base.py in get_groupby_statistic(data)\n      45         return _VALUE_COUNTS_MEMO[data.name]\n      46 \n ---> 47     value_counts_with_nan = data.value_counts(dropna=False)\n      48     value_counts_without_nan = value_counts_with_nan.loc[value_counts_with_nan.index.dropna()]\n      49     distinct_count_with_nan = value_counts_with_nan.count()\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/base.py in value_counts(self, normalize, sort, ascending, bins, dropna)\n    1036         from pandas.core.algorithms import value_counts\n    1037         result = value_counts(self, sort=sort, ascending=ascending,\n -> 1038                               normalize=normalize, bins=bins, dropna=dropna)\n    1039         return result\n    1040 \n \n /opt/conda/lib/python3.6/site-packages/pandas/core/algorithms.py in value_counts(values, sort, ascending, normalize, bins, dropna)\n     721 \n     722     if sort:\n --> 723         result = result.sort_values(ascending=ascending)\n     724 \n     725     if normalize:\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/series.py in sort_values(self, axis, ascending, inplace, kind, na_position)\n    2494             raise ValueError('invalid na_position: {!r}'.format(na_position))\n    2495 \n -> 2496         result = self._constructor(arr[sortedIdx], index=self.index[sortedIdx])\n    2497 \n    2498         if inplace:\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py in __getitem__(self, key)\n    2095         result = getitem(key)\n    2096         if not is_scalar(result):\n -> 2097             return promote(result)\n    2098         else:\n    2099             return result\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/indexes/category.py in _shallow_copy(self, values, categories, ordered, dtype, **kwargs)\n     206             dtype = self.dtype if dtype is None else dtype\n     207             return super(CategoricalIndex, self)._shallow_copy(\n --> 208                 values=values, dtype=dtype, **kwargs)\n     209         if categories is None:\n     210             categories = self.categories\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py in _shallow_copy(self, values, **kwargs)\n     516         if not len(values) and 'dtype' not in kwargs:\n     517             attributes['dtype'] = self.dtype\n --> 518         return self._simple_new(values, **attributes)\n     519 \n     520     def _shallow_copy_with_infer(self, values=None, **kwargs):\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/indexes/category.py in _simple_new(cls, values, name, categories, ordered, dtype, **kwargs)\n     182 \n     183         values = cls._create_categorical(cls, values, categories, ordered,\n --> 184                                          dtype=dtype)\n     185         result._data = values\n     186         result.name = name\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/indexes/category.py in _create_categorical(self, data, categories, ordered, dtype)\n     173             if isinstance(dtype, CategoricalDtype):\n     174                 # we want to silently ignore dtype='category'\n --> 175                 data = data._set_dtype(dtype)\n     176         return data\n     177 \n \n /opt/conda/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _set_dtype(self, dtype)\n     728         \"\"\"\n     729         codes = _recode_for_categories(self.codes, self.categories,\n --> 730                                        dtype.categories)\n     731         return type(self)(codes, dtype=dtype, fastpath=True)\n     732 \n \n /opt/conda/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _recode_for_categories(codes, old_categories, new_categories)\n    2461         # All null anyway, so just retain the nulls\n    2462         return codes.copy()\n -> 2463     indexer = coerce_indexer_dtype(new_categories.get_indexer(old_categories),\n    2464                                    new_categories)\n    2465     new_codes = take_1d(indexer, codes.copy(), fill_value=-1)\n \n /opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance)\n    3257                                  'backfill or nearest reindexing')\n    3258 \n -> 3259             indexer = self._engine.get_indexer(target._ndarray_values)\n    3260 \n    3261         return _ensure_platform_int(indexer)\n \n pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_indexer()\n \n pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.lookup()\n \n /opt/conda/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper()\n \n /opt/conda/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__()\n \n ValueError: buffer source array is read-only\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "geoHeil", "commentT": "2019-05-30T17:46:45Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/geoHeil>@geoHeil</denchmark-link>\n  ,\n Thank you for reporting this issue. Testing it with a recent version of pandas-profiling works just fine for me, in the myriad of ways I have tested it. This code works for example and seems to be comparable to what you mention in your post:\n <denchmark-code>import pandas as pd\n from pandas_profiling import ProfileReport\n \n \n # https://github.com/pandas-profiling/pandas-profiling/issues/147\n def test_issue147():\n     \"Data from https://github.com/Teradata/kylo/raw/master/samples/sample-data/parquet/userdata2.parquet\"\n     df = pd.read_parquet(r\"userdata2.parquet\", engine='pyarrow')\n     report = ProfileReport(df, title=\"PyArrow with Pandas Parquet Backend\")\n     html = report.to_html()\n     assert type(html) == str and '<p class=\"h2\">Dataset info</p>' in html\n </denchmark-code>\n \n It could be that recent updates have solved the problem, or that your dataset contains more than I am now testing. In any case, the first step would be to take another shot at the dataset with the latest pandas-profiling.  If the error persists, please let us know.\n Kind regards,\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "geoHeil", "commentT": "2019-05-31T04:57:30Z", "comment_text": "\n \t\tIndeed. I will close it now.\n \t\t"}}}, "commit": {"commit_id": "d6c85be96b2d7279075678eccd832fc133e9999f", "commit_author": "sbrugman", "commitT": "2019-05-30 20:30:37+02:00", "commit_complexity": {"commit_NLOC": "0.29411764705882354", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": ".gitignore", "file_new_name": ".gitignore", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "71,72", "deleted_lines": "71"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\view\\formatters.html", "file_new_name": "docs\\view\\formatters.html", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27,111,163", "deleted_lines": "110,162"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pandas_profiling\\view\\formatters.py", "file_new_name": "pandas_profiling\\view\\formatters.py", "file_complexity": {"file_NLOC": "80", "file_CCN": "10", "file_NToken": "256"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "86", "deleted_lines": "85", "method_info": {"method_name": "fmt", "method_params": "value", "method_startline": "74", "method_endline": "86", "method_complexity": {"method_NLOC": "13", "method_CCN": "2", "method_NToken": "35", "method_nesting_level": "0"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "requirements-test.txt", "file_new_name": "requirements-test.txt", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5", "deleted_lines": null}}}, "file_4": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\issues\\test_issue147.py", "file_complexity": {"file_NLOC": "14", "file_CCN": "2", "file_NToken": "90"}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\test_describe.py", "file_new_name": "tests\\test_describe.py", "file_complexity": {"file_NLOC": "637", "file_CCN": "28", "file_NToken": "3451"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "62,63,64,65,66,67,68,69,70,71,72,73,74,75", "deleted_lines": "62,63,92,93", "method_info": {"method_name": "test_recoding_reject", "method_params": "recoding_data", "method_startline": "60", "method_endline": "94", "method_complexity": {"method_NLOC": "34", "method_CCN": "8", "method_NToken": "242", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "104,105,106,107,108,109,110,111,112,113,114,115,116,117,118", "deleted_lines": null, "method_info": {"method_name": "test_cramers_reject", "method_params": "recoding_data", "method_startline": "97", "method_endline": "137", "method_complexity": {"method_NLOC": "38", "method_CCN": "8", "method_NToken": "293", "method_nesting_level": "0"}}}}}}}}