{"BR": {"BR_id": "574", "BR_author": "AlexRux", "BRopenT": "2020-09-25T10:01:44Z", "BRcloseT": "2020-09-25T14:50:07Z", "BR_text": {"BRsummary": "Causal Padding", "BRdescription": "\n <denchmark-h:h3>Describe the bug</denchmark-h>\n \n \"causal\" padding with QuantConv1D doesn't work. It reduces the dimension.\n I got this summary:\n <denchmark-code>+model stats--------------------------------------------------------------------------------------+\n | Layer                     Input prec.           Outputs  # 4-bit  # 32-bit  Memory  32-bit MACs |\n |                                 (bit)                        x 1       x 1    (kB)              |\n +-------------------------------------------------------------------------------------------------+\n | input_1                             -  ((None, 29, 1),)        0         0       0            ? |\n | quant_conv1d                        -      (-1, 28, 12)       24         0    0.01            ? |\n | activation                          -      (-1, 28, 12)        0         0       0            ? |\n | quant_conv1d_1                      -      (-1, 26, 16)      384         0    0.19            ? |\n | activation_1                        -      (-1, 26, 16)        0         0       0            ? |\n | quant_conv1d_2                      -      (-1, 22, 20)      640         0    0.31            ? |\n | activation_2                        -      (-1, 22, 20)        0         0       0            ? |\n | quant_conv1d_3                      -      (-1, 21, 24)      960         0    0.47            ? |\n | activation_3                        -      (-1, 21, 24)        0         0       0            ? |\n | global_average_pooling1d            -          (-1, 24)        0         0       0            ? |\n | flatten                             -          (-1, 24)        0         0       0            0 |\n | quant_dense                         -           (-1, 4)       96         4    0.06           96 |\n | activation_4                        -           (-1, 4)        0         0       0            ? |\n +-------------------------------------------------------------------------------------------------+\n | Total                                                       2104         4    1.04           96 |\n +-------------------------------------------------------------------------------------------------+\n </denchmark-code>\n \n Described by this:\n <denchmark-code>kwargs = dict(\n                   kernel_quantizer=kernel_quantizer,\n                   kernel_constraint=kernel_constraint,\n                   padding=\"causal\",\n                   use_bias = False,\n     )\n \n     x = tf.keras.Input(shape=(Time_series_length, 1))\n \n     ConvL1 = lq.layers.QuantConv1D(filters=12, kernel_size=2, dilation_rate=1, **kwargs)(x)\n     ConvL1 = tf.keras.layers.Activation(activation)(ConvL1)\n \n     ConvL2 = lq.layers.QuantConv1D(filters=16, kernel_size=2, dilation_rate=2, **kwargs)(ConvL1)\n     ConvL2 = tf.keras.layers.Activation(activation)(ConvL2)\n \n     ConvL3 = lq.layers.QuantConv1D(filters=20, kernel_size=2, dilation_rate=4, **kwargs)(ConvL2)\n     ConvL3 = tf.keras.layers.Activation(activation)(ConvL3)\n \n     ConvL4 = lq.layers.QuantConv1D(filters=24, kernel_size=2, dilation_rate=1, **kwargs)(ConvL3)\n     ConvL4 = tf.keras.layers.Activation(activation)(ConvL4)\n \n     final = tf.keras.layers.GlobalAveragePooling1D()(ConvL4)\n     final = tf.keras.layers.Flatten()(final)\n     final = lq.layers.QuantDense(classi, kernel_quantizer=kernel_quantizer, kernel_constraint=kernel_constraint)(\n         final)\n     final = tf.keras.layers.Activation(\"softmax\")(final)\n </denchmark-code>\n \n But using tf.keras.layers.Conv1D instead of lq.layers.QuantConv1D I got this:\n <denchmark-code>+model stats-----------------------------------------------------------------------------+\n | Layer                     Input prec.           Outputs  # 32-bit  Memory  32-bit MACs |\n |                                 (bit)                         x 1    (kB)              |\n +----------------------------------------------------------------------------------------+\n | input_1                             -  ((None, 29, 1),)         0       0            ? |\n | conv1d                              -      (-1, 29, 12)        24    0.09            ? |\n | activation                          -      (-1, 29, 12)         0       0            ? |\n | conv1d_1                            -      (-1, 29, 16)       384    1.50            ? |\n | activation_1                        -      (-1, 29, 16)         0       0            ? |\n | conv1d_2                            -      (-1, 29, 20)       640    2.50            ? |\n | activation_2                        -      (-1, 29, 20)         0       0            ? |\n | conv1d_3                            -      (-1, 29, 24)       960    3.75            ? |\n | activation_3                        -      (-1, 29, 24)         0       0            ? |\n | global_average_pooling1d            -          (-1, 24)         0       0            ? |\n | flatten                             -          (-1, 24)         0       0            0 |\n | quant_dense                         -           (-1, 4)        96    0.38           96 |\n | activation_4                        -           (-1, 4)         0       0            ? |\n +----------------------------------------------------------------------------------------+\n | Total                                                        2104    8.22           96 |\n +----------------------------------------------------------------------------------------+\n </denchmark-code>\n \n They should be the same. What can I do to fix it? I need quantizations and causal padding.\n TensorFlow version: 2.1\n Larq version: 10.0\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "AlexRux", "commentT": "2020-09-25T12:07:56Z", "comment_text": "\n \t\tI was able to reproduce your issue.\n This is actually caused by the fact that TensorFlow  <denchmark-link:https://github.com/tensorflow/tensorflow/blob/25fba035f3e453d94490932096282c7b0624bbb3/tensorflow/python/keras/layers/convolutional.py#L204>explicitely checks for the class name in the Conv1D layer</denchmark-link>\n  which prevent subclasses of  to use causal padding.\n This bug has been fixed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/17b63987e58a08ecaa950c2668a5870a399cfaba>tensorflow/tensorflow@17b6398</denchmark-link>\n , so upgrading to TensorFlow 2.3 should solve your issue.\n I've personally never used causal padding in Keras so please let us know if this fixes your issue.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "AlexRux", "commentT": "2020-09-25T14:34:02Z", "comment_text": "\n \t\tFixed, thank you.\n \t\t"}}}, "commit": {"commit_id": "394a9086fea0a3eac982a689c5ca2918dbe1e017", "commit_author": "Lukas Geiger", "commitT": "2020-09-25 13:42:16+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "larq\\layers_base.py", "file_new_name": "larq\\layers_base.py", "file_complexity": {"file_NLOC": "203", "file_CCN": "47", "file_NToken": "1301"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "106,107,108,109", "deleted_lines": "106,107,108", "method_info": {"method_name": "__init__", "method_params": "self,args,pad_values,kwargs", "method_startline": "103", "method_endline": "109", "method_complexity": {"method_NLOC": "7", "method_CCN": "5", "method_NToken": "79", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "larq\\layers_test.py", "file_new_name": "larq\\layers_test.py", "file_complexity": {"file_NLOC": "285", "file_CCN": "20", "file_NToken": "1969"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "279,280,281", "deleted_lines": null, "method_info": {"method_name": "test_conv1d_non_zero_padding_raises", "method_params": "self", "method_startline": "279", "method_endline": "281", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "38", "method_nesting_level": "1"}}}}}}}}