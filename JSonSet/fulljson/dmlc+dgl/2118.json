{"BR": {"BR_id": "2118", "BR_author": "Junyoungpark", "BRopenT": "2020-08-27T12:20:44Z", "BRcloseT": "2020-08-30T01:27:29Z", "BR_text": {"BRsummary": "Maybe a bug on reading-out node and edge features?", "BRdescription": "\n Hi,\n I found a sketchy issue while I do the node readout and edge readout and do the backpropgation.\n <denchmark-code>import dgl\n import torch\n \n if __name__ == '__main__':\n     src, dst = (0, 1, 2), (2, 1, 0)\n     g = dgl.graph((src, dst))\n     g.ndata['feat'] = torch.ones(3, 2)\n     g.edata['feat'] = torch.ones(3, 3)\n \n     nh_enc = torch.nn.Linear(2, 2)\n     g.ndata['h'] = nh_enc(g.ndata['feat'])\n \n     eh_enc = torch.nn.Linear(3, 3)\n     g.edata['h'] = eh_enc(g.edata['feat'])\n \n     n_readout = dgl.readout_nodes(g, 'h', op='sum')\n     e_readout = dgl.readout_edges(g, 'h', op='sum')\n \n     inpt = torch.cat([n_readout, e_readout], dim=-1)\n \n     m = torch.nn.Linear(5, 1)\n \n     out = m(inpt)\n     out.backward()\n \n </denchmark-code>\n \n on ubuntu 18.04 + dgl-cu101 0.5.0 + torch 1.5.1\n The above code give me an error as follows:\n Using backend: pytorch\n Traceback (most recent call last):\n File \"/home/silab9/dev/ScheduleNet/check_dgl_readouts.py\", line 24, in \n out.backward()\n File \"/home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/torch/tensor.py\", line 198, in backward\n torch.autograd.backward(self, gradient, retain_graph, create_graph)\n File \"/home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/torch/autograd/init.py\", line 98, in backward\n Variable._execution_engine.run_backward(\n RuntimeError: [21:16:12] /opt/dgl/src/array/kernel.cc:39: Check failed: arrays[i].IsContiguous(): Expect U_data to be a contiguous tensor\n Stack trace:\n [bt] (0) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x22) [0x7fd30ca89e02]\n [bt] (1) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(+0x8f91fa) [0x7fd30cbbe1fa]\n [bt] (2) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(+0x8fec24) [0x7fd30cbc3c24]\n [bt] (3) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/libdgl.so(DGLFuncCall+0x52) [0x7fd30d190232]\n [bt] (4) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x19f87) [0x7fd3d24fbf87]\n [bt] (5) /home/silab9/.pyenv/versions/gpu_torch/lib/python3.8/site-packages/dgl/_ffi/_cy3/core.cpython-38-x86_64-linux-gnu.so(+0x1a5fe) [0x7fd3d24fc5fe]\n [bt] (6) /home/silab9/.pyenv/versions/gpu_torch/bin/python(_PyObject_MakeTpCall+0x91) [0x5629f25dbbd1]\n [bt] (7) /home/silab9/.pyenv/versions/gpu_torch/bin/python(_PyEval_EvalFrameDefault+0x6264) [0x5629f25c9f24]\n [bt] (8) /home/silab9/.pyenv/versions/gpu_torch/bin/python(+0x5dac7) [0x5629f25c2ac7]\n But when I do only backprop on either one of node or edge readouts, It works perfectly fine.\n <denchmark-code>    src, dst = (0, 1, 2), (2, 1, 0)\n     g = dgl.graph((src, dst))\n     g.ndata['feat'] = torch.ones(3, 2)\n     g.edata['feat'] = torch.ones(3, 3)\n \n     nh_enc = torch.nn.Linear(2, 2)\n     g.ndata['h'] = nh_enc(g.ndata['feat'])\n \n     eh_enc = torch.nn.Linear(3, 3)\n     g.edata['h'] = eh_enc(g.edata['feat'])\n \n     n_readout = dgl.readout_nodes(g, 'h', op='sum')\n \n     inpt = n_readout\n \n     m = torch.nn.Linear(2, 1)\n \n     out = m(inpt)\n     out.backward()\n </denchmark-code>\n \n Is this behavior is a bug or my mistake?\n Thanks,\n Jun\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Junyoungpark", "commentT": "2020-08-27T13:33:05Z", "comment_text": "\n \t\tThanks for reporting this, I'll take a look.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Junyoungpark", "commentT": "2020-08-29T15:36:53Z", "comment_text": "\n \t\tThe problem seems to be related to backward function of GSpMM.\n When I print the shape and stride of dZ at the beginning of GSpMM.backward() I found the shape is (1, 3) but the stride is (5, 1), which doesn't make sense.\n May need more time to figure out the reason.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Junyoungpark", "commentT": "2020-08-29T16:15:49Z", "comment_text": "\n \t\tOK I think I know what is going on here.  Probably it's how PyTorch and NumPy etc handles strides of a view of an array with size-1 axes.\n Normally, the strides on an axis of a contiguous array should be a cumulative product of the sizes of the succeeding axes:\n torch.randn((2, 3, 4)).stride()    # (12, 4, 1)\n DGL determines whether an array is contiguous by this rule.\n However, this is not true for views of tensors which have size 1 on the first few axes:\n torch.randn((2, 5))[:, :3].stride()    # (5, 1)\n torch.randn((2, 5))[:, :3].contiguous().stride()    # (3, 1)\n torch.randn((1, 5))[:, :3].stride()    # (5, 1)\n torch.randn((1, 5))[:, :3].contiguous().stride()    # (5, 1) ???\n In fact, we can also observe the same phenomenon in NumPy arrays:\n np.ascontiguousarray(np.random.randn(1, 5)[:, :3]).strides    # (40, 8)\n As per the bug, the reason is that the input of GSpMM.backward() in this case is actually a view of a 1xN tensor, which falls into the corner case above.\n We should change the implementation of IsContiguous() accordingly.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Junyoungpark", "commentT": "2020-08-30T01:27:29Z", "comment_text": "\n \t\tThis should be fixed in <denchmark-link:https://github.com/dmlc/dgl/pull/2127>#2127</denchmark-link>\n  .\n \t\t"}}}, "commit": {"commit_id": "7816c5a2493e205809e32c333b62ce7e7be634d8", "commit_author": "Zihao Ye", "commitT": "2020-08-30 09:26:03+08:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\runtime\\ndarray.cc", "file_new_name": "src\\runtime\\ndarray.cc", "file_complexity": {"file_NLOC": "416", "file_CCN": "59", "file_NToken": "3178"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "140,141,142,143,144,145,146,147,148,149,151", "deleted_lines": "140,141,142,143,145", "method_info": {"method_name": "dgl::runtime::NDArray::IsContiguous", "method_params": "", "method_startline": "136", "method_endline": "152", "method_complexity": {"method_NLOC": "15", "method_CCN": "5", "method_NToken": "102", "method_nesting_level": "2"}}}}}}}}