{"BR": {"BR_id": "879", "BR_author": "ce39906", "BRopenT": "2020-06-19T02:00:56Z", "BRcloseT": "2020-06-28T02:01:35Z", "BR_text": {"BRsummary": "[LightGBM] Train Lambdamart failed with \"org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1\"", "BRdescription": "\n \n Hi, <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  , I'm using mmlspark0.18.1 to train a ranking job.\n This below is the main training flow.\n <denchmark-code>def run(self):\n         train_df = self.load_svm_rank_data()\n         df = train_df.repartition(8, 'query_id')\n         model = LightGBMRanker(\n             parallelism='data_parallel',\n             #  parallelism='voting_parallel',\n             objective='lambdarank',\n             boostingType='gbdt',\n             numIterations=500,\n             learningRate=0.1,\n             # For recall task, 511,8 is enough.\n             # numLeaves=511,\n             # maxDepth=8,\n             numLeaves=1023,\n             maxDepth=10,\n             earlyStoppingRound=0,\n             maxPosition=20,\n             #minSumHessianInLeaf=0.0005,\n             minSumHessianInLeaf=0.001,\n             lambdaL1=0.01,\n             lambdaL2=0.01,\n             isProvideTrainingMetric=True,\n             #  baggingSeed=3,\n             #  boostFromAverage=True,\n             #  categoricalSlotIndexes=None,\n             #  categoricalSlotNames=None,\n             defaultListenPort=49650,\n             #  defaultListenPort=12400,\n             featuresCol='features',\n             groupCol='query_id',\n             #  initScoreCol=None,\n             labelCol='label',\n             #  labelGain=[],\n             #  modelString='',\n             numBatches=0,\n             #  predictionCol='prediction',\n             timeout=600000.0,\n             #  useBarrierExecutionMode=False,\n             #  validationIndicatorCol=None,\n             verbosity=1,\n             #  weightCol=None,\n         ).fit(df)\n </denchmark-code>\n \n This below is the spark job config.\n <denchmark-code>/opt/meituan/spark-2.2/bin/spark-submit     --deploy-mode cluster --queue root.zw03_training.hadoop-map.training --executor-cores 40 --num-executors 8 --master yarn --driver-memory 8G --files /opt/meituan/spark-2.2/conf/hive-site.xml --executor-memory 16G --files /opt/tmp/etl/remote_file/session_D5E23EBC14BCEA4F_pysparkjar_00877de2cbfbf624dca5ac527f415c9e/city_province_list --repositories http://pixel.sankuai.com/repository/group-releases,http://pixel.sankuai.com/repository/mtdp --conf spark.yarn.maxAppAttempts=1 --conf spark.task.cpus=40 --conf spark.sql.autoBroadcastJoinThreshold=-1 --conf spark.kryoserializer.buffer.max=1024m --conf spark.driver.maxResultSize=10G --conf spark.executor.instances=8 --conf spark.hadoop.parquet.enable.summary-metadata=false --conf spark.executor.heartbeatInterval=30s --conf spark.default.parallelism=1024 --conf spark.sql.hive.metastorePartitionPruning=true --conf spark.yarn.driver.memoryOverhead=8096 --conf spark.sql.orc.filterPushdown=true --conf spark.sql.parquet.filterPushdown=true --conf spark.sql.shuffle.partitions=1024 --conf spark.sql.orc.splits.include.file.footer=true --conf spark.jars.packages=com.microsoft.ml.spark:mmlspark_2.11:0.18.1 --conf spark.sql.orc.cache.stripe.details.size=10000 --conf spark.sql.parquet.mergeSchema=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.yarn.executor.memoryOverhead=60G --conf spark.yarn.am.extraJavaOptions=\"-DappIdentify=hope_3375504 -Dport=AppMaster \" --conf spark.driver.extraJavaOptions=\"-DappIdentify=hope_3375504 -Dport=Driver -XX:PermSize=128M -XX:MaxPermSize=256M \" --conf spark.executor.extraJavaOptions=\"-DappIdentify=hope_3375504 -Dport=Executor \"          --name huobaochong:/opt/meituan/20200616/topk_train_v2/shanghai/topk_train/topk_train.hope     --conf spark.job.owner=huobaochong     --conf spark.client.host=zw02-data-msp-launcher13.mt     --conf spark.job.type=mtmsp     --conf spark.flowid=D5E23EBC14BCEA4F     --conf spark.yarn.app.tags.flowid=D5E23EBC14BCEA4F     --conf spark.yarn.app.tags.schedulejobid=cantor-6177712     --conf spark.yarn.app.tags.scheduleinstanceid=     --conf spark.yarn.app.tags.scheduleplanid=     --conf spark.yarn.app.tags.onceexecid=once-exec-6163959     --conf spark.yarn.app.tags.rm.taskcode=hope:huobaochong:/opt/meituan/20200616/topk_train_v2/shanghai/topk_train/topk_train.hope     --conf spark.yarn.app.tags.rm.taskname=huobaochong:/opt/meituan/20200616/topk_train_v2/shanghai/topk_train/topk_train.hope     --conf spark.yarn.app.tags.rm.tasktype=hope     --conf spark.yarn.app.tags.mtmspCompileVersion=0     --conf spark.yarn.job.priority=1     --conf spark.hive.mt.metastore.audit.id=SPARK-MTMSP-D5E23EBC14BCEA4F     --conf spark.hadoop.hive.mt.metastore.audit.id=SPARK-MTMSP-D5E23EBC14BCEA4F     --conf spark.hbo.enabled=true     --conf spark.executor.cantorEtlIncreaseMemory.enabled=true     /opt/tmp/etl/remote_file/session_D5E23EBC14BCEA4F_pysparkjar_00877de2cbfbf624dca5ac527f415c9e/topk_train.py     20200615190316-v0.0.3_china-20200505-20200520-common-staging shangha\n </denchmark-code>\n \n This below is the error info.\n <denchmark-link:https://user-images.githubusercontent.com/10767336/85087961-7a0bc000-b211-11ea-89f0-4e2c21cf1f61.png></denchmark-link>\n \n stdout from the driver node:\n \n py4j.protocol.Py4JJavaError: An error occurred while calling o149.fit.\n : org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.1 failed 4 times, most recent failure: Lost task 3.3 in stage 1.1 (TID 13502, zw03-data-hdp-dn-cpu0244.mt, executor 9): java.net.ConnectException: Connection refused (Connection refused)\n at java.net.PlainSocketImpl.socketConnect(Native Method)\n at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n at java.net.Socket.connect(Socket.java:589)\n at java.net.Socket.connect(Socket.java:538)\n at java.net.Socket.(Socket.java:434)\n at java.net.Socket.(Socket.java:211)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.getNetworkInitNodes(TrainUtils.scala:324)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$15.apply(TrainUtils.scala:398)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$15.apply(TrainUtils.scala:393)\n at com.microsoft.ml.spark.core.env.StreamUtilities$.using(StreamUtilities.scala:28)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.trainLightGBM(TrainUtils.scala:392)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:85)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:85)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:196)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:193)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:834)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:834)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:43)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:43)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:89)\n at org.apache.spark.scheduler.Task.run(Task.scala:110)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:363)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n at java.lang.Thread.run(Thread.java:745)\n Driver stacktrace:\n at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1576)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1564)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563)\n at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1563)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:822)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:822)\n at scala.Option.foreach(Option.scala:257)\n at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:822)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1794)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1746)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1735)\n at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:634)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:2157)\n at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1033)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n at org.apache.spark.rdd.RDD.reduce(RDD.scala:1015)\n at org.apache.spark.sql.Dataset.reduce(Dataset.scala:1460)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$class.innerTrain(LightGBMBase.scala:90)\n at com.microsoft.ml.spark.lightgbm.LightGBMRanker.innerTrain(LightGBMRanker.scala:25)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$class.train(LightGBMBase.scala:38)\n at com.microsoft.ml.spark.lightgbm.LightGBMRanker.train(LightGBMRanker.scala:25)\n at com.microsoft.ml.spark.lightgbm.LightGBMRanker.train(LightGBMRanker.scala:25)\n at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n at py4j.Gateway.invoke(Gateway.java:280)\n at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n at py4j.commands.CallCommand.execute(CallCommand.java:79)\n at py4j.GatewayConnection.run(GatewayConnection.java:214)\n at java.lang.Thread.run(Thread.java:745)\n Caused by: java.net.ConnectException: Connection refused (Connection refused)\n at java.net.PlainSocketImpl.socketConnect(Native Method)\n at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n at java.net.Socket.connect(Socket.java:589)\n at java.net.Socket.connect(Socket.java:538)\n at java.net.Socket.(Socket.java:434)\n at java.net.Socket.(Socket.java:211)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.getNetworkInitNodes(TrainUtils.scala:324)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$15.apply(TrainUtils.scala:398)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$15.apply(TrainUtils.scala:393)\n at com.microsoft.ml.spark.core.env.StreamUtilities$.using(StreamUtilities.scala:28)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.trainLightGBM(TrainUtils.scala:392)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:85)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:85)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:196)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:193)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:834)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:834)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:43)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:43)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:89)\n at org.apache.spark.scheduler.Task.run(Task.scala:110)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:363)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n ... 1 more\n \n Errors from executors like this below.\n <denchmark-link:https://user-images.githubusercontent.com/10767336/85088396-9b20e080-b212-11ea-99af-c757ad50c5c0.png></denchmark-link>\n \n Info (please complete the following information):\n \n MMLSpark Version: mmlspark_2.11:0.18.1\n Spark Version: 2.2\n Spark Platform: spark with yarn\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ce39906", "commentT": "2020-06-19T02:41:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  based on the error:\n <denchmark-link:https://user-images.githubusercontent.com/24683184/85091035-86adfb00-b1b4-11ea-8eb3-72ff375106fa.png></denchmark-link>\n \n I think this is an out of memory error:\n <denchmark-link:https://stackoverflow.com/questions/28901123/why-do-spark-jobs-fail-with-org-apache-spark-shuffle-metadatafetchfailedexceptio>https://stackoverflow.com/questions/28901123/why-do-spark-jobs-fail-with-org-apache-spark-shuffle-metadatafetchfailedexceptio</denchmark-link>\n \n I'm not sure if LightGBM is running out of memory or if it is a cluster configuration issue, but I will guess/assume that it is the former.\n Could you try running on the latest version?  I believe there were some memory optimizations.  I will need to look into more ways to reduce memory.  Are you perchance running LightGBM several times over?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ce39906", "commentT": "2020-06-19T02:41:50Z", "comment_text": "\n \t\tHere is a link to the latest version, if it helps:\n <denchmark-code>Maven Coordinates\n com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1-96-fce3c952-SNAPSHOT\n Maven Resolver\n https://mmlspark.azureedge.net/maven\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ce39906", "commentT": "2020-06-19T03:48:42Z", "comment_text": "\n \t\t\n Here is a link to the latest version, if it helps:\n Maven Coordinates\n com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1-96-fce3c952-SNAPSHOT\n Maven Resolver\n https://mmlspark.azureedge.net/maven\n \n \n Our cluster can only connect the inner network, as this <denchmark-link:https://github.com/Azure/mmlspark/issues/818>#818</denchmark-link>\n  , could you post a link for me to download the jar?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ce39906", "commentT": "2020-06-19T04:41:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  sure, I think this is it:\n <denchmark-link:https://mmlspark.azureedge.net/maven/com/microsoft/ml/spark/mmlspark_2.11/1.0.0-rc1-96-fce3c952-SNAPSHOT/mmlspark_2.11-1.0.0-rc1-96-fce3c952-SNAPSHOT.jar>https://mmlspark.azureedge.net/maven/com/microsoft/ml/spark/mmlspark_2.11/1.0.0-rc1-96-fce3c952-SNAPSHOT/mmlspark_2.11-1.0.0-rc1-96-fce3c952-SNAPSHOT.jar</denchmark-link>\n \n And you will need the newer version of lightgbm:\n <denchmark-link:https://repo.maven.apache.org/maven2/com/microsoft/ml/lightgbm/lightgbmlib/2.3.180/lightgbmlib-2.3.180.jar>https://repo.maven.apache.org/maven2/com/microsoft/ml/lightgbm/lightgbmlib/2.3.180/lightgbmlib-2.3.180.jar</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ce39906", "commentT": "2020-06-19T04:45:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n   If it is still failing, can you give me more information on the dataset size? What is the dimensionality of the dataset - the number of rows and columns?  Is the dataset dense or sparse?  This might help diagnose the out of memory issue and help me figure out if there is some bug in the scala layer.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ce39906", "commentT": "2020-06-19T06:28:58Z", "comment_text": "\n \t\t\n @ce39906 If it is still failing, can you give me more information on the dataset size? What is the dimensionality of the dataset - the number of rows and columns? Is the dataset dense or sparse? This might help diagnose the out of memory issue and help me figure out if there is some bug in the scala layer.\n \n The dataset is about 160GB with 101 columns and 112441295 rows.\n The dataset is dense.\n My spark conf is like this below.\n \n /opt/meituan/spark-2.2/bin/spark-submit     --deploy-mode cluster --queue root.zw03_training.hadoop-map.training --executor-cores 40 --num-executors 8 --master yarn --driver-memory 8G --files /opt/meituan/spark-2.2/conf/hive-site.xml --executor-memory 16G --files /opt/tmp/etl/remote_file/session_D5E23EBC14BCEA4F_pysparkjar_00877de2cbfbf624dca5ac527f415c9e/city_province_list --repositories http://pixel.sankuai.com/repository/group-releases,http://pixel.sankuai.com/repository/mtdp --conf spark.yarn.maxAppAttempts=1 --conf spark.task.cpus=40 --conf spark.sql.autoBroadcastJoinThreshold=-1 --conf spark.kryoserializer.buffer.max=1024m --conf spark.driver.maxResultSize=10G --conf spark.executor.instances=8 --conf spark.hadoop.parquet.enable.summary-metadata=false --conf spark.executor.heartbeatInterval=30s --conf spark.default.parallelism=1024 --conf spark.sql.hive.metastorePartitionPruning=true --conf spark.yarn.driver.memoryOverhead=8096 --conf spark.sql.orc.filterPushdown=true --conf spark.sql.parquet.filterPushdown=true --conf spark.sql.shuffle.partitions=1024 --conf spark.sql.orc.splits.include.file.footer=true --conf spark.jars.packages=com.microsoft.ml.spark:mmlspark_2.11:0.18.1 --conf spark.sql.orc.cache.stripe.details.size=10000 --conf spark.sql.parquet.mergeSchema=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.yarn.executor.memoryOverhead=60G --conf spark.yarn.am.extraJavaOptions=\"-DappIdentify=hope_3375504 -Dport=AppMaster \" --conf spark.driver.extraJavaOptions=\"-DappIdentify=hope_3375504 -Dport=Driver -XX:PermSize=128M -XX:MaxPermSize=256M \" --conf spark.executor.extraJavaOptions=\"-DappIdentify=hope_3375504 -Dport=Executor \"          --name huobaochong:/opt/meituan/20200616/topk_train_v2/shanghai/topk_train/topk_train.hope     --conf spark.job.owner=huobaochong     --conf spark.client.host=zw02-data-msp-launcher13.mt     --conf spark.job.type=mtmsp     --conf spark.flowid=D5E23EBC14BCEA4F     --conf spark.yarn.app.tags.flowid=D5E23EBC14BCEA4F     --conf spark.yarn.app.tags.schedulejobid=cantor-6177712     --conf spark.yarn.app.tags.scheduleinstanceid=     --conf spark.yarn.app.tags.scheduleplanid=     --conf spark.yarn.app.tags.onceexecid=once-exec-6163959     --conf spark.yarn.app.tags.rm.taskcode=hope:huobaochong:/opt/meituan/20200616/topk_train_v2/shanghai/topk_train/topk_train.hope     --conf spark.yarn.app.tags.rm.taskname=huobaochong:/opt/meituan/20200616/topk_train_v2/shanghai/topk_train/topk_train.hope     --conf spark.yarn.app.tags.rm.tasktype=hope     --conf spark.yarn.app.tags.mtmspCompileVersion=0     --conf spark.yarn.job.priority=1     --conf spark.hive.mt.metastore.audit.id=SPARK-MTMSP-D5E23EBC14BCEA4F     --conf spark.hadoop.hive.mt.metastore.audit.id=SPARK-MTMSP-D5E23EBC14BCEA4F     --conf spark.hbo.enabled=true     --conf spark.executor.cantorEtlIncreaseMemory.enabled=true     /opt/tmp/etl/remote_file/session_D5E23EBC14BCEA4F_pysparkjar_00877de2cbfbf624dca5ac527f415c9e/topk_train.py     20200615190316-v0.0.3_china-20200505-20200520-common-staging shangha\n \n should I increase executor's memory or executor.memoryOverhead ?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ce39906", "commentT": "2020-06-19T07:30:08Z", "comment_text": "\n \t\tAnother question, must I repartition the data frame to have the same partitions with the number of executors before training?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ce39906", "commentT": "2020-06-19T15:54:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n   Great question!  That is a large dataset for the given cluster size, assuming lower bound it is at least:\n 112,441,295 rows  * 101 columns * 8 bytes per double value=  ~90.8 GB\n in memory.  On top of that spark needs to use memory for scheduling.\n For LightGBM, we need to load all data in memory for training; the data is already repartitioned prior to training.  The DataFrame in scala is converted to native representation, and there is around 2X original dataset size overhead for this, so total size in memory should be about 3X.  I think it is possible to make this 2X in the future, so we will only need to hold this data in native memory and original DataFrame.\n Besides that, we also need to configure the cluster well, otherwise it can effectively have a lot less memory than what the cluster can actually provide.  Increasing execution memory will definitely help, if there is unused memory.  In yarn, I always pay a lot of attention to the number of cores, the amount of memory per machine and the number of machines, and based on that I try to make a good estimate for the number of executors, especially number of executors per machine.  If you have one executor per machine, and it has just 4GB of memory when the machine has 64 GB available, then obviously you would be using just 6% of the cluster's available memory, so you have to be very careful with the configuration.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ce39906", "commentT": "2020-06-19T15:58:43Z", "comment_text": "\n \t\tThese parameters seem very important:\n <denchmark-code>-executor-cores 40 --num-executors 8 --master yarn --driver-memory 8G --files /opt/meituan/spark-2.2/conf/hive-site.xml --executor-memory 16G --\n </denchmark-code>\n \n I also see you have a very high spark.yarn.executor.memoryOverhead=60G\n This seems a bit high to me, based on:\n <denchmark-link:https://spark.apache.org/docs/latest/configuration.html>https://spark.apache.org/docs/latest/configuration.html</denchmark-link>\n \n The default is:\n <denchmark-code>executorMemory * 0.10, with minimum of 384 \n </denchmark-code>\n \n <denchmark-link:https://user-images.githubusercontent.com/24683184/85153442-379fae80-b224-11ea-8445-5c12feff3536.png></denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "ce39906", "commentT": "2020-06-19T19:09:28Z", "comment_text": "\n \t\tCan you send me more info on your cluster configuration?  How many machines do you have, how much RAM does each machine have and how many cores are there?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "ce39906", "commentT": "2020-06-22T02:03:42Z", "comment_text": "\n \t\t\n @ce39906 sure, I think this is it:\n https://mmlspark.azureedge.net/maven/com/microsoft/ml/spark/mmlspark_2.11/1.0.0-rc1-96-fce3c952-SNAPSHOT/mmlspark_2.11-1.0.0-rc1-96-fce3c952-SNAPSHOT.jar\n And you will need the newer version of lightgbm:\n https://repo.maven.apache.org/maven2/com/microsoft/ml/lightgbm/lightgbmlib/2.3.180/lightgbmlib-2.3.180.jar\n \n hi, <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  , I tried your new version jar, and it uses less memory. I config executor memory to 35g, executro.memoryOverHead to 20g,  executor-num 8 , for the first time the job is successful, I tried a second time, the job failed for the memory issue. I think memory config is not appropriate for now, and I will try more config.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "ce39906", "commentT": "2020-06-22T02:17:59Z", "comment_text": "\n \t\t\n Can you send me more info on your cluster configuration? How many machines do you have, how much RAM does each machine have and how many cores are there?\n \n The training queue in our company is managed by another team, the training queue is confined to 1048cpu and about 4000g memory. As running the jobs, I saw two kinds of executor machine, 40cpu with 128g memory and 64cpu with 256memory.\n Another question, should I try large executor memory and little memoryOverHead? the native part is running in off-heap memory or in-heap memory? and what's the relationship with off-heap memory and executor.memoryOverHead?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "ce39906", "commentT": "2020-06-22T06:07:33Z", "comment_text": "\n \t\t<denchmark-link:https://user-images.githubusercontent.com/10767336/85253562-1263a800-b491-11ea-8d72-9a39cf302ee5.png></denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/10767336/85253862-a33a8380-b491-11ea-8d06-4026b75df136.png></denchmark-link>\n \n hi, <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  , another confusing question, I configured with 4 executors, why there is only one task in the training stage?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "ce39906", "commentT": "2020-06-23T15:55:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  great questions!\n <denchmark-code>Another question, should I try large executor memory and little memoryOverHead?\n </denchmark-code>\n \n I may be wrong about my interpretation of those parameters (and I have contributed to apache spark but only to SparkML), but my understanding is that memoryOverhead is not used for the pipeline so you actually want to keep it as small as possible, eg on spark page it states:\n <denchmark-link:https://spark.apache.org/docs/latest/configuration.html>https://spark.apache.org/docs/latest/configuration.html</denchmark-link>\n \n <denchmark-code>This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes. \n </denchmark-code>\n \n My understanding is that you really want to keep spark.executor.memory as high as possible on the machines.\n <denchmark-code>the native part is running in off-heap memory or in-heap memory? and what's the relationship with off-heap memory and executor.memoryOverHead?\n </denchmark-code>\n \n The native part is allocated mostly in the native heap, it's definitely not on the stack.  Hmm, I'm not sure about the relationship with off-heap memory and executor.memoryOverHead.  In practice, I have always tried to maximize executor memory and minimize memoryOverhead, but different types of clusters (yarn, mesos, spark standalone) may use these variables differently.  The doc even mentions:\n <denchmark-code>This option is currently supported on YARN and Kubernetes. \n </denchmark-code>\n \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "ce39906", "commentT": "2020-06-24T02:04:45Z", "comment_text": "\n \t\t\n @ce39906 great questions!\n Another question, should I try large executor memory and little memoryOverHead?\n \n I may be wrong about my interpretation of those parameters (and I have contributed to apache spark but only to SparkML), but my understanding is that memoryOverhead is not used for the pipeline so you actually want to keep it as small as possible, eg on spark page it states:\n https://spark.apache.org/docs/latest/configuration.html\n This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%). This option is currently supported on YARN and Kubernetes. \n \n My understanding is that you really want to keep spark.executor.memory as high as possible on the machines.\n the native part is running in off-heap memory or in-heap memory? and what's the relationship with off-heap memory and executor.memoryOverHead?\n \n The native part is allocated mostly in the native heap, it's definitely not on the stack. Hmm, I'm not sure about the relationship with off-heap memory and executor.memoryOverHead. In practice, I have always tried to maximize executor memory and minimize memoryOverhead, but different types of clusters (yarn, mesos, spark standalone) may use these variables differently. The doc even mentions:\n This option is currently supported on YARN and Kubernetes. \n \n \n <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  , Thanks for your reply, that's a great help to me.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "ce39906", "commentT": "2020-06-24T02:22:41Z", "comment_text": "\n \t\t\n \n \n hi, @imatiach-msft , another confusing question, I configured with 4 executors, why there is only one task in the training stage?\n \n Hi, <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  , I found this issue <denchmark-link:https://github.com/Azure/mmlspark/issues/747>#747</denchmark-link>\n , I think this issue is similar to mine. I think this issue happens when requesting large memory machines which may cost more time. So when running the method below\n <denchmark-code>val numExecutorCores = ClusterUtil.getNumExecutorCores(dataset, numCoresPerExec, log) \n </denchmark-code>\n \n the requested machines are pending and the value  is set with 1.\n Like the method mentioned is <denchmark-link:https://github.com/Azure/mmlspark/issues/747>#747</denchmark-link>\n , I want to do control the number of reasonable num workers  based on your latest version code (com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1-96-fce3c952-SNAPSHOT\n ) which made a lot of memory optimization. Where can I get the source code of the latest version? That's will be very grateful.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "ce39906", "commentT": "2020-06-24T02:55:32Z", "comment_text": "\n \t\thi <denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  , I know there was a recent PR from <denchmark-link:https://github.com/ocworld>@ocworld</denchmark-link>\n  related to getNumExecutorCores, not sure if it would impact your setup though:\n <denchmark-link:https://github.com/Azure/mmlspark/pull/855>#855</denchmark-link>\n \n It does seem like you should have more tasks, perhaps getNumExecutorCores is not returning the correct value in your case.\n I do agree it would be good to add a parameter to control the number of tasks.\n \"Where can I get the source code of the latest version?\"\n The source code is on github master branch, the latest package should be the same as posted above:\n <denchmark-code>Maven Coordinates\n com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1-96-fce3c952-SNAPSHOT\n \n Maven Resolver\n https://mmlspark.azureedge.net/maven\n </denchmark-code>\n \n Hmm, let me see if I can implement something for you quickly to control the number of tasks, as it does seem like a useful parameter.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "ce39906", "commentT": "2020-06-24T03:33:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  I've created a PR here to allow users to override the number of tasks as you and <denchmark-link:https://github.com/Azure/mmlspark/issues/747>#747</denchmark-link>\n  suggested:\n <denchmark-link:https://github.com/Azure/mmlspark/pull/881>#881</denchmark-link>\n \n Will update maven coordinates here once I get a green build.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "ce39906", "commentT": "2020-06-24T04:56:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  new build finished for the PR:\n <denchmark-code>Maven Coordinates\n \n com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1-91-a13271d7-SNAPSHOT\n Maven Resolver\n \n https://mmlspark.azureedge.net/maven\n </denchmark-code>\n \n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "ce39906", "commentT": "2020-06-24T05:54:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  Can you show me your driver log about ClusterUtils to check getNumExecutorCores.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "ce39906", "commentT": "2020-06-24T06:27:10Z", "comment_text": "\n \t\t\n @ce39906 Can you show me your driver log about ClusterUtils to check getNumExecutorCores.\n \n Hi, <denchmark-link:https://github.com/ocworld>@ocworld</denchmark-link>\n  , this is the evidence.\n <denchmark-link:https://user-images.githubusercontent.com/10767336/85508478-b9c61380-b626-11ea-9430-386133af9f1a.png></denchmark-link>\n \n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "ce39906", "commentT": "2020-06-24T07:57:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  Thank you for sharing your logs.\n I found the reason from your configuration.\n spark.task.cpus=40 is set.\n \"spark.task.cpus\" means \"Number of cores to allocate for each task.\"\n (<denchmark-link:https://spark.apache.org/docs/2.4.5/configuration.html>https://spark.apache.org/docs/2.4.5/configuration.html</denchmark-link>\n )\n The number of workers per executor is equals to the number of tasks per executor.\n So, if you want to create 40 workers per executor, spark.task.cpus=1 and spark.executor.cores=40 should be set.\n if reproducing it when setting spark.task.cpus=1, please, let me know about it.\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "ce39906", "commentT": "2020-06-24T07:59:49Z", "comment_text": "\n \t\tIn spark, the number of tasks per executor = spark.executor.cores / spark.task.cpus\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "ce39906", "commentT": "2020-06-24T10:09:05Z", "comment_text": "\n \t\tIn addition, about the number of executors, \"Could not retrieve executors from blockmanager\" and \"Using default case = 1 executors\" are shown in the logs.\n <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  I think that getExecutors() cannot retrieve executors in the spark session in this dataset.\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "ce39906", "commentT": "2020-06-24T10:47:33Z", "comment_text": "\n \t\t\n @ce39906 new build finished for the PR:\n Maven Coordinates\n \n com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1-91-a13271d7-SNAPSHOT\n Maven Resolver\n \n https://mmlspark.azureedge.net/maven\n \n \n Hi, <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  , what 's the version of the corresponding LightGBM? I'm using com.microsoft.ml.lightgbm:lightgbmlib:2.3.180 and got an core dump like this below:\n <denchmark-link:https://user-images.githubusercontent.com/10767336/85541912-218e5580-b64b-11ea-84c2-361cfebfafb1.png></denchmark-link>\n \n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "ce39906", "commentT": "2020-06-24T10:49:04Z", "comment_text": "\n \t\t\n In spark, the number of tasks per executor = spark.executor.cores / spark.task.cpus\n \n I want to conf with one task on an executor with 40 CPUs\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "ce39906", "commentT": "2020-06-24T11:45:58Z", "comment_text": "\n \t\t\n \n In spark, the number of tasks per executor = spark.executor.cores / spark.task.cpus\n \n I want to conf with one task on an executor with 40 CPUs\n \n <denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  Ok, \"ClusterUtils calculated....\" is right in the last red bock. In my undestanding, numCores in ClusterUtils means num of tasks, while NumExecutorCores is executor cores.\n Anyway, it is strange to me your second red block. I assume that they are printed in spark.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "ce39906", "commentT": "2020-06-24T12:05:27Z", "comment_text": "\n \t\t\n \n \n In spark, the number of tasks per executor = spark.executor.cores / spark.task.cpus\n \n I want to conf with one task on an executor with 40 CPUs\n \n @ce39906 Ok, \"ClusterUtils calculated....\" is right in the last red bock. In my undestanding, numCores in ClusterUtils means num of tasks, while NumExecutorCores is executor cores.\n Anyway, it is strange to me your second red block. I assume that they are printed in spark.\n \n Yes, the log in the second red block is printed in spark.\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "ce39906", "commentT": "2020-06-24T15:14:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n \n \"what 's the version of the corresponding LightGBM? I'm using com.microsoft.ml.lightgbm:lightgbmlib:2.3.180 \"\n Yes, that should be the new version of LightGBM.  I'm not sure what is causing the error though.  Is there any way I could try to reproduce this issue?  What is the cluster than you are using, is the dataset private and is there some way I could get the code to reproduce this?\n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "ce39906", "commentT": "2020-06-25T04:39:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  I took a closer look at the logs and <denchmark-link:https://github.com/ocworld>@ocworld</denchmark-link>\n  is exactly right, the spark.task.cpus is set to 40, so you are using 40 cpus in a single task - which explains why you are only seeing one task (with 40 CPUs in it).  Maybe LightGBM is not able to use as much parallelism in this scenario, I will need to take a closer look at the native code, but I would recommend trying spark.task.cpus=1, which would create 40 tasks instead.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "ce39906", "commentT": "2020-06-25T04:42:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n \n for reference, please see logic here:\n <denchmark-link:https://github.com/Azure/mmlspark/blob/master/src/main/scala/com/microsoft/ml/spark/core/utils/ClusterUtil.scala#L25>https://github.com/Azure/mmlspark/blob/master/src/main/scala/com/microsoft/ml/spark/core/utils/ClusterUtil.scala#L25</denchmark-link>\n \n Also see relevant config docs:\n <denchmark-link:https://spark.apache.org/docs/latest/configuration.html>https://spark.apache.org/docs/latest/configuration.html</denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/24683184/85654787-b6f60d80-b67c-11ea-88fa-10ef835289bc.png></denchmark-link>\n \n and relevant stackoverflow thread:\n <denchmark-link:https://stackoverflow.com/questions/37545069/what-is-the-difference-between-spark-task-cpus-and-executor-cores>https://stackoverflow.com/questions/37545069/what-is-the-difference-between-spark-task-cpus-and-executor-cores</denchmark-link>\n \n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "ce39906", "commentT": "2020-06-25T06:26:06Z", "comment_text": "\n \t\t\n @ce39906 I took a closer look at the logs and @ocworld is exactly right, the spark.task.cpus is set to 40, so you are using 40 cpus in a single task - which explains why you are only seeing one task (with 40 CPUs in it). Maybe LightGBM is not able to use as much parallelism in this scenario, I will need to take a closer look at the native code, but I would recommend trying spark.task.cpus=1, which would create 40 tasks instead.\n \n Ok, I will try it.\n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "ce39906", "commentT": "2020-06-25T06:43:46Z", "comment_text": "\n \t\t\n hy you are only seeing one task (with 40 CPUs in it\n the spark.task.cpus is set to 40, so you are using 40 cpus in a single task - which explains why you are only seeing one task (with 40 CPUs in it)\n I don't think only seeing one task is caused by this. Only seeing one task is caused by\n \n <denchmark-code>val numExecutorCores = ClusterUtil.getNumExecutorCores(dataset, numCoresPerExec, log)\n </denchmark-code>\n \n this method returns 1 when the requested machines are pending.\n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "ce39906", "commentT": "2020-06-25T13:31:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  it still returns one?  Can you post the logs?  It seems somehow the setting change \"spark.task.cpus=1\" is not getting registered perhaps?\n \t\t"}, "comments_34": {"comment_id": 35, "comment_author": "ce39906", "commentT": "2020-06-25T13:32:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  can you post your spark conf like above again?\n <denchmark-code>My spark conf is like this below.\n ...\n </denchmark-code>\n \n \t\t"}, "comments_35": {"comment_id": 36, "comment_author": "ce39906", "commentT": "2020-06-25T15:09:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ce39906>@ce39906</denchmark-link>\n  also, did you try the numTasks parameter I added in the new PR I sent you?\n <denchmark-link:https://github.com/Azure/mmlspark/pull/881>#881</denchmark-link>\n \n Did that change the number of tasks?\n \t\t"}, "comments_36": {"comment_id": 37, "comment_author": "ce39906", "commentT": "2020-06-26T06:12:42Z", "comment_text": "\n \t\t\n @ce39906 also, did you try the numTasks parameter I added in the new PR I sent you?\n #881\n Did that change the number of tasks?\n \n <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  , sorry about replying late, these days are the Dragon Boat Festival holidays.\n here is my mainly spark conf\n <denchmark-code>master = yarn-cluster\n driver-memory = 8G\n driver-cores = 4\n executor-memory = 100G\n executor-cores = 40\n is_dynamic_allocation = false\n num-executors = 4\n \n [option_env_args]\n spark.executor.instances = 4\n spark.task.cpus = 1\n spark.jars.packages = com.microsoft.ml.lightgbm:lightgbmlib:2.3.180,com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1-91-a13271d7-SNAPSHOT\n \n spark.yarn.maxAppAttempts = 1\n spark.driver.maxResultSize = 10G\n spark.yarn.driver.memoryOverhead = 4096\n spark.yarn.executor.memoryOverhead = 120G\n spark.sql.orc.filterPushdown = true\n spark.sql.orc.splits.include.file.footer = true\n spark.sql.orc.cache.stripe.details.size = 10000\n spark.sql.hive.metastorePartitionPruning = true\n \n spark.hadoop.parquet.enable.summary-metadata = false\n spark.sql.parquet.mergeSchema = false\n spark.sql.parquet.filterPushdown = true\n spark.sql.hive.metastorePartitionPruning = true\n \n spark.sql.autoBroadcastJoinThreshold = -1\n \n spark.sql.shuffle.partitions = 2048\n spark.default.parallelism = 2048\n spark.serializer = org.apache.spark.serializer.KryoSerializer\n spark.kryoserializer.buffer.max = 1024m\n spark.executor.heartbeatInterval = 30s\n spark.network.timeout = 800s\n spark.executor.extraJavaOptions=\"-XX:+UseG1GC -XX:-UseGCOverheadLimit\"\n </denchmark-code>\n \n did you try the numTasks parameter I added in the new PR I sent you?\n Yes,  I set numTask parameter to 40(cores of each executor) * 4 (number of executors) = 160, and the training stage had 160 tasks.\n The job with these parameters succeeds.\n I'm trying other memory configs about executor.memory and executor.memory.overhead\n \t\t"}}}, "commit": {"commit_id": "7409ba58f1ef25be349c19cf429c880c8d7eb4dc", "commit_author": "Ilya Matiach", "commitT": "2020-06-28 20:37:41-04:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMBase.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMBase.scala", "file_complexity": {"file_NLOC": "143", "file_CCN": "27", "file_NToken": "1322"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "147,148,149,150,151,152", "deleted_lines": "150,151", "method_info": {"method_name": "getTrainingCols", "method_params": "", "method_startline": "132", "method_endline": "152", "method_complexity": {"method_NLOC": "14", "method_CCN": "3", "method_NToken": "155", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "152,153,154,155,156,157,158,159,160,161,162,163,164,165", "deleted_lines": "166", "method_info": {"method_name": "getCategoricalIndexes", "method_params": "DataFrame", "method_startline": "152", "method_endline": "166", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "64", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "169,170,171,172,173,174,175,190", "deleted_lines": "166,167,168,169,170", "method_info": {"method_name": "innerTrain", "method_params": "Int", "method_startline": "166", "method_endline": "219", "method_complexity": {"method_NLOC": "39", "method_CCN": "5", "method_NToken": "380", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMParams.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMParams.scala", "file_complexity": {"file_NLOC": "261", "file_CCN": "44", "file_NToken": "2244"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "78", "deleted_lines": null, "method_info": {"method_name": "setNumTasks", "method_params": "Int", "method_startline": "78", "method_endline": "79", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "71,72", "deleted_lines": null, "method_info": {"method_name": "setRepartitionByGroupingColumn", "method_params": "Boolean", "method_startline": "70", "method_endline": "72", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\test\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\split1\\VerifyLightGBMClassifier.scala", "file_new_name": "src\\test\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\split1\\VerifyLightGBMClassifier.scala", "file_complexity": {"file_NLOC": "506", "file_CCN": "30", "file_NToken": "4226"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "306,307,308,309,310", "deleted_lines": null}}}}}}