{"BR": {"BR_id": "1829", "BR_author": "QiJune", "BRopenT": "2020-03-13T00:04:07Z", "BRcloseT": "2020-03-19T14:28:26Z", "BR_text": {"BRsummary": "worker pod crashes when get task", "BRdescription": "\n <denchmark-code>2020-03-12 22:03:32.300671: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\n Traceback (most recent call last):\n   File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n     \"__main__\", mod_spec)\n   File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n     exec(code, run_globals)\n   File \"/elasticdl/python/worker/main.py\", line 44, in <module>\n     main()\n   File \"/elasticdl/python/worker/main.py\", line 40, in main\n     worker.run()\n   File \"/elasticdl/python/worker/worker.py\", line 1144, in run\n     self._train_and_evaluate()\n   File \"/elasticdl/python/worker/worker.py\", line 1074, in _train_and_evaluate\n     self._minibatch_size, err_msg\n   File \"/elasticdl/python/worker/task_data_service.py\", line 86, in report_record_done\n     task = self._pending_tasks[0]\n IndexError: deque index out of range\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "QiJune", "commentT": "2020-03-13T00:16:09Z", "comment_text": "\n \t\tIt seems that timeout mechanisim does not worker too. I will debug this.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "QiJune", "commentT": "2020-03-13T02:01:58Z", "comment_text": "\n \t\tI describe the pod:\n <denchmark-code>Status:       Failed\n State:          Terminated\n       Reason:       Error\n       Exit Code:    1\n       Started:      Thu, 12 Mar 2020 17:07:32 +0800\n       Finished:     Fri, 13 Mar 2020 06:04:06 +0800\n </denchmark-code>\n \n The exit code is 1.\n And the master pod log:\n <denchmark-code>   for event in stream:\n [2020-03-13 01:58:50,535] [INFO] [k8s_instance_manager.py:233:_event_cb] Got event ADDED, phase Running for pod: elasticdl-test-edl-dban-qitian910-worker-1\n [2020-03-13 01:58:50,538] [INFO] [k8s_instance_manager.py:233:_event_cb] Got event ADDED, phase Failed for pod: elasticdl-test-edl-dban-qitian910-worker-6\n [2020-03-13 01:58:50,540] [INFO] [k8s_instance_manager.py:233:_event_cb] Got event ADDED, phase Running for pod: elasticdl-test-edl-dban-qitian910-ps-2\n </denchmark-code>\n \n The master find the failed pod.\n It seems that we only handle exit_code 137\n \n \n \n elasticdl/elasticdl/python/master/k8s_instance_manager.py\n \n \n         Lines 251 to 259\n       in\n       d002f4a\n \n \n \n \n \n \n  if ( \n \n \n \n  evt_type == \"MODIFIED\" \n \n \n \n  and phase == \"Failed\" \n \n \n \n  and evt_obj.status.container_statuses \n \n \n \n  and evt_obj.status.container_statuses[0].state.terminated \n \n \n \n  and evt_obj.status.container_statuses[ \n \n \n \n  0 \n \n \n \n      ].state.terminated.exit_code \n \n \n \n  == 137 \n \n \n \n \n \n So this failed worker pod is not handled.\n \t\t"}}}, "commit": {"commit_id": "95c74b28c867dd5986724ad77d7802386624528d", "commit_author": "QI JUN", "commitT": "2020-03-19 22:28:25+08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "elasticdl\\python\\worker\\task_data_service.py", "file_new_name": "elasticdl\\python\\worker\\task_data_service.py", "file_complexity": {"file_NLOC": "164", "file_CCN": "44", "file_NToken": "903"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "86,87,88", "deleted_lines": null, "method_info": {"method_name": "report_record_done", "method_params": "self,count,err_msg", "method_startline": "75", "method_endline": "110", "method_complexity": {"method_NLOC": "24", "method_CCN": "8", "method_NToken": "160", "method_nesting_level": "1"}}}}}}}}