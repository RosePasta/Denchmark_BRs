{"BR": {"BR_id": "2678", "BR_author": "andrewredd", "BRopenT": "2020-07-23T12:44:45Z", "BRcloseT": "2020-10-05T11:33:47Z", "BR_text": {"BRsummary": "training_epoch_end seems to fail when returning nothing", "BRdescription": "\n I'm trying to log weight histograms to tensorboard at the end of each training epoch. I have the following code:\n <denchmark-code>    def training_epoch_end(self, outputs):\n         self.log_hists()\n </denchmark-code>\n \n This is in line with the documentation. \"If you don't need to display anything, Don't return anything\"\n However when this function runs I get the following error:\n <denchmark-code>...\n File \"/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 499, in run_training_epoch\n \u2502    self.run_training_epoch_end(epoch_output)\n \u2502  File \"/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 533, in run_training_epoch_end\n \u2502    _processed_outputs = self.process_output(epoch_output)\n \u2502  File \"/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/logging.py\", line 106, in process_output\n \u2502    for k, v in output.items():\n \u2502AttributeError: 'NoneType' object has no attribute 'items'\n \u2502Exception ignored in: <object repr() failed>\n \u2502Traceback (most recent call last):\n \u2502  File \"/venv/lib/python3.6/site-packages/tqdm/std.py\", line 1086, in __del__\n \u2502  File \"/venv/lib/python3.6/site-packages/tqdm/std.py\", line 1293, in close\n \u2502  File \"/venv/lib/python3.6/site-packages/tqdm/std.py\", line 1471, in display\n \u2502  File \"/venv/lib/python3.6/site-packages/tqdm/std.py\", line 1089, in __repr__\n \u2502  File \"/venv/lib/python3.6/site-packages/tqdm/std.py\", line 1433, in format_dict\n \u2502TypeError: 'NoneType' object is not iterable```\n \n The fix that I've found is to return a dictionary according to the expected typed return, as follows:\n \n </denchmark-code>\n \n <denchmark-code>def training_epoch_end(self, outputs):\n     self.log_hists()\n     return {'dummy': torch.Tensor()}\n </denchmark-code>\n \n <denchmark-code>\n This seems like a bandaid rather than a true fix since I don't need to return anything.\n \n 1) Is there a better way to log the histograms to tensorboard?\n 2) Should I be doing something different in this function?\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "andrewredd", "commentT": "2020-07-23T12:45:34Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "andrewredd", "commentT": "2020-07-24T16:59:03Z", "comment_text": "\n \t\ttraining_epoch_end by default doesn't log histogram.\n If you want to plot histogram on tensorboard, try self.logger.experiment.add_histogram().\n It's <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/experiment_logging.html#tensorboard>the docs</denchmark-link>\n .\n PL supports every method of  class with .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "andrewredd", "commentT": "2020-07-24T18:09:50Z", "comment_text": "\n \t\tThanks Jeff!\n \n The log_hists function that I have in the example is logging the entire the\n model weights, the biases and the gradients and it does use the referenced\n method.  Is the fact that if I don't return something from the function a\n bug or user error?\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Fri, Jul 24, 2020 at 12:59 PM Jeff Yang ***@***.***> wrote:\n  training_epoch_end by default doesn't log histogram.\n  If you want to plot histogram on tensorboard, try\n  self.logger.experiment.add_histogram().\n \n  It's the docs\n  <https://pytorch-lightning.readthedocs.io/en/latest/experiment_logging.html#tensorboard>\n  .\n  PL supports every method of SummaryWriter class with\n  self.logger.experiment.summary_writer_methods.\n \n  \u2014\n  You are receiving this because you authored the thread.\n  Reply to this email directly, view it on GitHub\n  <#2678 (comment)>,\n  or unsubscribe\n  <https://github.com/notifications/unsubscribe-auth/ALQO4BRY7WOVCZWYLZDUSPLR5G4WPANCNFSM4PFV6LKA>\n  .\n \n \n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "andrewredd", "commentT": "2020-07-27T09:23:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/andrewredd>@andrewredd</denchmark-link>\n  sorry for the delayed reply, currently we need to return empty dict in  to make it work. But, Lightning will be supporting  in the future.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "andrewredd", "commentT": "2020-08-26T19:10:32Z", "comment_text": "\n \t\tRunning into the same issue\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "andrewredd", "commentT": "2020-09-22T16:11:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  are we going to support return None?\n \t\t"}}}, "commit": {"commit_id": "b014223f72ee457285fa3eb336d1d4039cedb651", "commit_author": "William Falcon", "commitT": "2020-10-05 07:33:46-04:00", "commit_complexity": {"commit_NLOC": "0.35294117647058826", "commit_CCN": "0.8235294117647058", "commit_Nprams": "0.5588235294117647"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "file_complexity": {"file_NLOC": "479", "file_CCN": "146", "file_NToken": "3746"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "339,340,341,342", "deleted_lines": null, "method_info": {"method_name": "_process_training_step_output", "method_params": "self,training_step_output,split_batch", "method_startline": "336", "method_endline": "378", "method_complexity": {"method_NLOC": "24", "method_CCN": "7", "method_NToken": "163", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "666,667,668", "deleted_lines": null, "method_info": {"method_name": "run_training_batch", "method_params": "self,batch,batch_idx,dataloader_idx", "method_startline": "608", "method_endline": "735", "method_complexity": {"method_NLOC": "68", "method_CCN": "21", "method_NToken": "548", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "744,745,746,747", "deleted_lines": null, "method_info": {"method_name": "training_step_and_backward", "method_params": "self,split_batch,batch_idx,opt_idx,optimizer,hiddens", "method_startline": "737", "method_endline": "754", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "71", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "827,828,829", "deleted_lines": null, "method_info": {"method_name": "process_train_step_outputs", "method_params": "self,all_train_step_outputs,early_stopping_accumulator,checkpoint_accumulator", "method_startline": "816", "method_endline": "846", "method_complexity": {"method_NLOC": "14", "method_CCN": "10", "method_NToken": "117", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "311,312,313", "deleted_lines": null, "method_info": {"method_name": "training_step", "method_params": "self,split_batch,batch_idx,opt_idx,hiddens", "method_startline": "294", "method_endline": "334", "method_complexity": {"method_NLOC": "29", "method_CCN": "3", "method_NToken": "175", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "44", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,trainer", "method_startline": "39", "method_endline": "46", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "49", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\trainer\\data_flow\\test_train_loop_flow_scalar_1_0.py", "file_new_name": "tests\\trainer\\data_flow\\test_train_loop_flow_scalar_1_0.py", "file_complexity": {"file_NLOC": "150", "file_CCN": "21", "file_NToken": "839"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "196,197,198,199", "deleted_lines": null, "method_info": {"method_name": "test_train_step_no_return.training_step", "method_params": "self,batch,batch_idx", "method_startline": "196", "method_endline": "199", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "41", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "201,202", "deleted_lines": null, "method_info": {"method_name": "test_train_step_no_return.training_epoch_end", "method_params": "self,outputs", "method_startline": "201", "method_endline": "202", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "16", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217", "deleted_lines": null, "method_info": {"method_name": "test_train_step_no_return", "method_params": "tmpdir", "method_startline": "191", "method_endline": "217", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "74", "method_nesting_level": "0"}}}}}}}}