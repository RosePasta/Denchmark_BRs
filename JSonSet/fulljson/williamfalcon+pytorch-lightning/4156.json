{"BR": {"BR_id": "4156", "BR_author": "Borda", "BRopenT": "2020-10-14T21:59:10Z", "BRcloseT": "2020-10-15T14:53:43Z", "BR_text": {"BRsummary": "crashing dump for hparams containing fsspec", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n If you pass a model as PL model argument and dum this model, it crashes...\n <denchmark-h:h2>Please reproduce using [the BoringModel and post here]</denchmark-h>\n \n Failing several models in Bolts\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/runs/1254957648>https://github.com/PyTorchLightning/pytorch-lightning-bolts/runs/1254957648</denchmark-link>\n \n sample of failing test:\n <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/ccdc9f9952153abf9b12f00e05294fa332d5f424/tests/models/test_vision.py#L8-L31>https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/ccdc9f9952153abf9b12f00e05294fa332d5f424/tests/models/test_vision.py#L8-L31</denchmark-link>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n <denchmark-code>        cls = type(data)\n         if cls in copyreg.dispatch_table:\n             reduce = copyreg.dispatch_table[cls](data)\n         elif hasattr(data, '__reduce_ex__'):\n >           reduce = data.__reduce_ex__(2)\n E           TypeError: __reduce_ex__() takes exactly one argument (0 given)\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Borda", "commentT": "2020-10-14T22:44:22Z", "comment_text": "\n \t\tseems that there is something special about Bolts's , any idea <denchmark-link:https://github.com/nateraw>@nateraw</denchmark-link>\n ?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Borda", "commentT": "2020-10-14T23:02:49Z", "comment_text": "\n \t\tThis is the exact error I ran into in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/pull/264>my PR</denchmark-link>\n  as well. I don't know what's causing it \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Borda", "commentT": "2020-10-14T23:03:21Z", "comment_text": "\n \t\tok, so the more trace is that DM has a pointer to Trainer and somewhere in Trainer there is <fsspec.implementations.local.LocalFileSystem object at 0x13ba296a0> which causes this problem...\n EDIT: MNISTDataModule -> Trainer -> CheckPoint -> fsspec\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Borda", "commentT": "2020-10-14T23:51:15Z", "comment_text": "\n \t\t<denchmark-code>{'datamodule': <pl_bolts.datamodules.mnist_datamodule.MNISTDataModule object at 0x13b991dd8>, 'embed_dim': 16, 'heads': 2, 'layers': 2, 'pixels': 28, 'vocab_size': 16, 'num_classes': 10, 'classify': False, 'batch_size': 64, 'learning_rate': 0.01, 'steps': 25000, 'data_dir': '.', 'num_workers': 8}\n vvv\n <pl_bolts.datamodules.mnist_datamodule.MNISTDataModule object at 0x13b991dd8>\n vvv\n <pytorch_lightning.trainer.trainer.Trainer object at 0x12ec29780>\n vvv\n [<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x13ba29630>, <pytorch_lightning.callbacks.progress.ProgressBar object at 0x13ba29668>]\n vvv\n <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x13ba29630>\n vvv\n <fsspec.implementations.local.LocalFileSystem object at 0x13ba296a0>\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Borda", "commentT": "2020-10-15T02:56:19Z", "comment_text": "\n \t\tshould hparams be a more formal concept in the lightning module? given how complex objects can be passed to the module or the trainer, should users explicitly save set the hyperparameters their module uses? and should lightning skip saving them if they're not set? i wonder how the frame capture logic for arguments is going to hold up over time.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Borda", "commentT": "2020-10-15T07:08:48Z", "comment_text": "\n \t\t\n should hparams be a more formal concept in the lightning module? given how complex objects can be passed to the module or the trainer, should users explicitly save set the hyperparameters their module uses? and should lightning skip saving them if they're not set? i wonder how the frame capture logic for arguments is going to hold up over time.\n \n good questions, we do not want to limit users much, I made the two following fixes:\n \n save only parameters which can be saved and some is invalid skip it and warn user, this is just preventing to crash in runtime, #4158\n the problem came from runtimeTTrainer linking to used instances so it is impossible to do any static check or other checks before training starts, also this runtime changes are not needed for Init state reconstruction, see we save just the initial hparams, #4163\n \n note that each of this PR solves just a part of the issue \ud83d\udc30\n \t\t"}}}, "commit": {"commit_id": "4204ef7b533be7fe64e753372147aa204290540b", "commit_author": "Jirka Borovec", "commitT": "2020-10-15 16:53:42+02:00", "commit_complexity": {"commit_NLOC": "0.6086956521739131", "commit_CCN": "0.6086956521739131", "commit_Nprams": "0.8695652173913043"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "26", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\saving.py", "file_new_name": "pytorch_lightning\\core\\saving.py", "file_complexity": {"file_NLOC": "304", "file_CCN": "41", "file_NToken": "1361"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "377,378,379,380,381,382,383,384,385,386,387,388,390", "deleted_lines": "375,378", "method_info": {"method_name": "save_hparams_to_yaml", "method_params": "config_yaml,dict", "method_startline": "348", "method_endline": "390", "method_complexity": {"method_NLOC": "35", "method_CCN": "10", "method_NToken": "250", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\models\\test_hparams.py", "file_new_name": "tests\\models\\test_hparams.py", "file_complexity": {"file_NLOC": "354", "file_CCN": "59", "file_NToken": "2808"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "591,592,593,594,595,596,597,598,599,600,601", "deleted_lines": null, "method_info": {"method_name": "test_model_with_fsspec_as_parameter", "method_params": "tmpdir", "method_startline": "591", "method_endline": "601", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "50", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "586,587,588", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,my_path,any_param", "method_startline": "586", "method_endline": "588", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "23", "method_nesting_level": "1"}}}}}}}}