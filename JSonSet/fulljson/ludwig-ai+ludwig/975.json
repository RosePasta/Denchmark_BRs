{"BR": {"BR_id": "975", "BR_author": "mangleddata", "BRopenT": "2020-10-26T20:35:37Z", "BRcloseT": "2020-11-06T01:04:07Z", "BR_text": {"BRsummary": "Train throws AttributeError: module 'tensorflow.keras.losses' has no attribute 'softmax_cross_entropy'", "BRdescription": "\n Describe the bug\n ludwig train fails with error \"AttributeError: module 'tensorflow.keras.losses' has no attribute 'softmax_cross_entropy'\n To Reproduce\n I can build a reproducer after cleaning data, but want to see if there's some library mismatch that you can spot.\n Environment (please complete the following information):\n \n OS: 18.04.1-Ubuntu\n Version 18\n Python version 3.7.6\n Ludwig version 0.3\n Tensorflow version 2.3.1\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mangleddata", "commentT": "2020-10-26T23:22:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mangleddata>@mangleddata</denchmark-link>\n  the Ludwig / TF / Python versions look fine to me.\n You can use the data synthesizer commad to create a dataset that looks like yours to reproduce this.\n I'm guessing you are useing class weights,is that correct? In that case I may actually know what the issue is.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mangleddata", "commentT": "2020-10-27T00:56:11Z", "comment_text": "\n \t\tI created a reproducer with all the files. Please see\n <denchmark-link:https://github.com/mangleddata/repro_cw>https://github.com/mangleddata/repro_cw</denchmark-link>\n \n Indeed, I was using class weights. So I poked around and with strong google skills, I modified softmax_cross_entropy_with_class_weighting to use tf.compat.v1.losses.softmax_cross_entropy function and it started making progress.\n <denchmark-code>def softmax_cross_entropy_with_class_weighting(logits, one_hot_labels,\n                                                class_weights,\n                                                labels_smoothing=0.0):\n     class_weights_const = tf.expand_dims(\n         tf.constant(class_weights, dtype=tf.float32), 0)\n     sample_weights = tf.reduce_sum(\n         tf.multiply(one_hot_labels, class_weights_const), 1)\n     if False:\n         return tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels,\n                                                logits=logits,\n                                                label_smoothing=labels_smoothing,\n                                                weights=sample_weights,\n                                                reduction=tf.losses.Reduction.NONE)\n     else:\n         return tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=one_hot_labels,\n                                                logits=logits,\n                                                label_smoothing=labels_smoothing,\n                                                weights=sample_weights,\n                                                reduction=tf.losses.Reduction.NONE)\n </denchmark-code>\n \n However, when I tried on multi GPU, it failed with timeout after few epochs. I recall similar issues in the past and had created an evenly divisible dataset - but hitting same issue. Any thoughts on this one ? The reproducer may not show this hang behaviour since I trimmed the dataset a bit.\n Training:  72% 3141/4336 [00:36<00:14, 84.87it/s][1,0]:[2020-10-26 22:26:23.932368: E /tmp/pip-install-f5oc3rbb/horovod/horovod/common/stall_inspector.cc:103] One or more tensors were submitted to be reduced, gathered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks are trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock.\n [1,0]:Missing ranks:\n [1,0]:0!: [HorovodBroadcast_count_0, HorovodBroadcast_ecd_category_output_feature_1_classifier_1_dense_2_bias_0, HorovodBroadcast_ecd_category_output_feature_1_classifier_1_dense_2_kernel_0, HorovodBroadcast_ecd_category_output_feature_2_classifier_2_dense_3_bias_0, HorovodBroadcast_ecd_category_output_feature_2_classifier_2_dense_3_kernel_0, HorovodBroadcast_ecd_category_output_feature_3_classifier_3_dense_4_bias_0 ...]\n [1,0]:One or more rank (marked by \"!\") is stalled for longer than 300 seconds. Will shutdown.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "mangleddata", "commentT": "2020-10-27T01:58:31Z", "comment_text": "\n \t\tThis sound like a separate issue (one <denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>\n  can probably help with), but could you try without horovod and report on that? It would be great to help me reproduce. It's likely even a small dataset will reproduce the issue.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "mangleddata", "commentT": "2020-10-27T03:49:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/w4nderlust>@w4nderlust</denchmark-link>\n  I reduced the file size and was able to reproduce the issue (module 'tensorflow.keras.losses' has no attribute 'softmax_cross_entropy). Could you check if run.sh from repo above reproduces on your end ?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "mangleddata", "commentT": "2020-10-27T23:56:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mangleddata>@mangleddata</denchmark-link>\n  thank you, I was able to reproduce, Will look into it.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "mangleddata", "commentT": "2020-11-06T01:04:07Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ludwig-ai/ludwig/pull/994>#994</denchmark-link>\n  should have fixed the issue, so I'm closing. <denchmark-link:https://github.com/mangleddata>@mangleddata</denchmark-link>\n  Please confirm if this solves your specific issue, if not I'll reopen.\n \t\t"}}}, "commit": {"commit_id": "99d12ee54e1fbbe5d7c5f09e9fa6d1988f35f680", "commit_author": "Piero Molino", "commitT": "2020-11-05 17:02:47-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "ludwig\\data\\preprocessing.py", "file_new_name": "ludwig\\data\\preprocessing.py", "file_complexity": {"file_NLOC": "1367", "file_CCN": "71", "file_NToken": "5095"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995", "deleted_lines": "969,970,971,972,973,974,975,976,977,978,984", "method_info": {"method_name": "build_metadata", "method_params": "dataset_df,features,global_preprocessing_parameters", "method_startline": "941", "method_endline": "998", "method_complexity": {"method_NLOC": "49", "method_CCN": "6", "method_NToken": "192", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "ludwig\\features\\set_feature.py", "file_new_name": "ludwig\\features\\set_feature.py", "file_complexity": {"file_NLOC": "273", "file_CCN": "22", "file_NToken": "1431"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "202,207", "deleted_lines": null, "method_info": {"method_name": "_setup_loss", "method_params": "self", "method_startline": "200", "method_endline": "209", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "37", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "328,330", "deleted_lines": null, "method_info": {"method_name": "populate_defaults", "method_params": "output_feature", "method_startline": "326", "method_endline": "335", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "78", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "ludwig\\modules\\loss_modules.py", "file_new_name": "ludwig\\modules\\loss_modules.py", "file_complexity": {"file_NLOC": "345", "file_CCN": "26", "file_NToken": "1988"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "146", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,feature_loss,name", "method_startline": "144", "method_endline": "147", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "11", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "246,247,248,249,250,251", "deleted_lines": "243,244,245", "method_info": {"method_name": "mean_confidence_penalty", "method_params": "probabilities,num_classes", "method_startline": "243", "method_endline": "254", "method_complexity": {"method_NLOC": "11", "method_CCN": "1", "method_NToken": "84", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "381,382,383,384,385,386", "deleted_lines": null, "method_info": {"method_name": "weighted_sigmoid_cross_entropy", "method_params": "logits,vector_labels,class_weights,labels_smoothing,kwargs", "method_startline": "381", "method_endline": "386", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "16", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "153,154,155,156", "deleted_lines": "152,153,155", "method_info": {"method_name": "call", "method_params": "self,y,y_pred", "method_startline": "152", "method_endline": "158", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "36", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "ludwig\\modules\\metric_modules.py", "file_new_name": "ludwig\\modules\\metric_modules.py", "file_complexity": {"file_NLOC": "379", "file_CCN": "53", "file_NToken": "2763"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "175,176,177", "deleted_lines": "175,176"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\integration_tests\\test_experiment.py", "file_new_name": "tests\\integration_tests\\test_experiment.py", "file_complexity": {"file_NLOC": "684", "file_CCN": "51", "file_NToken": "3875"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "201,202,203,204,205,206,207,208,209", "deleted_lines": null, "method_info": {"method_name": "test_experiment_multiclass_with_class_weights", "method_params": "csv_filename", "method_startline": "201", "method_endline": "209", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "61", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "211,212,213,214,215,216,217,218,219,220", "deleted_lines": null, "method_info": {"method_name": "test_experiment_multilabel_with_class_weights", "method_params": "csv_filename", "method_startline": "211", "method_endline": "220", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "63", "method_nesting_level": "0"}}}}}}}}