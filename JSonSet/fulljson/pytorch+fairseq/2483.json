{"BR": {"BR_id": "2483", "BR_author": "joeqi0370", "BRopenT": "2020-08-15T08:52:46Z", "BRcloseT": "2020-08-25T00:36:19Z", "BR_text": {"BRsummary": "Can not train the Translator.  Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'", "BRdescription": "\n <denchmark-h:h2>What is your question?</denchmark-h>\n \n Can not train the translator\n <denchmark-h:h4>Code</denchmark-h>\n \n The commands I runned:\n fairseq-preprocess --source-lang en --target-lang de --trainpref D:\\fairseq\\test_translateEN-DE\\data\\train --validpref D:\\fairseq\\test_translateEN-DE\\data\\val --testpref D:\\fairseq\\test_translateEN-DE\\data\\test --destdir D:\\fairseq\\test_translateEN-DE\\model.bin\n #Preprocess seems ok\n ##########################\n 2020-08-15 16:38:32 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types\n 2020-08-15 16:38:33 | INFO | fairseq_cli.preprocess | [en] D:\\fairseq\\test_translateEN-DE\\data\\train.en: 10000 sents, 234434 tokens, 0.0% replaced by \n 2020-08-15 16:38:33 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types\n 2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] D:\\fairseq\\test_translateEN-DE\\data\\val.en: 1419 sents, 40092 tokens, 10.5% replaced by \n 2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] Dictionary: 24992 types\n 2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [en] D:\\fairseq\\test_translateEN-DE\\data\\test.en: 1581 sents, 34996 tokens, 11.2% replaced by \n 2020-08-15 16:38:34 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types\n 2020-08-15 16:38:35 | INFO | fairseq_cli.preprocess | [de] D:\\fairseq\\test_translateEN-DE\\data\\train.de: 10000 sents, 223074 tokens, 0.0% replaced by \n 2020-08-15 16:38:35 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types\n 2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] D:\\fairseq\\test_translateEN-DE\\data\\val.de: 1419 sents, 40031 tokens, 17.0% replaced by \n 2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] Dictionary: 35808 types\n 2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | [de] D:\\fairseq\\test_translateEN-DE\\data\\test.de: 1581 sents, 34635 tokens, 16.3% replaced by \n 2020-08-15 16:38:36 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to D:\\fairseq\\test_translateEN-DE\\model.bin\n ##########################\n But the following command reports error\n fairseq-train D:\\fairseq\\test_translateEN-DE\\model.bin --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 --arch fconv_iwslt_de_en --save-dir D:\\fairseq\\test_translateEN-DE\\\n Error info:\n ##########################\n 2020-08-15 16:39:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n 2020-08-15 16:39:08 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None\n 2020-08-15 16:39:08 | INFO | fairseq.trainer | no existing checkpoint found D:\\fairseq\\test_translateEN-DE\\checkpoint_last.pt\n 2020-08-15 16:39:08 | INFO | fairseq.trainer | loading train data for epoch 1\n 2020-08-15 16:39:08 | INFO | fairseq.data.data_utils | loaded 10000 examples from: D:\\fairseq\\test_translateEN-DE\\model.bin\\train.en-de.en\n 2020-08-15 16:39:08 | INFO | fairseq.data.data_utils | loaded 10000 examples from: D:\\fairseq\\test_translateEN-DE\\model.bin\\train.en-de.de\n 2020-08-15 16:39:08 | INFO | fairseq.tasks.translation | D:\\fairseq\\test_translateEN-DE\\model.bin train en-de 10000 examples\n Traceback (most recent call last):\n File \"D:\\Python\\Anaconda3\\envs\\NMT\\Scripts\\fairseq-train-script.py\", line 33, in \n sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 333, in cli_main\n distributed_utils.call_main(args, main)\n File \"d:\\fairseq\\fairseq\\fairseq\\distributed_utils.py\", line 189, in call_main\n main(args, **kwargs)\n File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 109, in main\n extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\n File \"d:\\fairseq\\fairseq\\fairseq\\checkpoint_utils.py\", line 187, in load_checkpoint\n epoch_itr = trainer.get_train_iterator(\n File \"d:\\fairseq\\fairseq\\fairseq\\trainer.py\", line 335, in get_train_iterator\n return self.task.get_batch_iterator(\n File \"d:\\fairseq\\fairseq\\fairseq\\tasks\\fairseq_task.py\", line 214, in get_batch_iterator\n batch_sampler = dataset.batch_by_size(\n File \"d:\\fairseq\\fairseq\\fairseq\\data\\fairseq_dataset.py\", line 118, in batch_by_size\n return data_utils.batch_by_size(\n File \"d:\\fairseq\\fairseq\\fairseq\\data\\data_utils.py\", line 256, in batch_by_size\n return batch_by_size_fast(\n File \"fairseq\\data\\data_utils_fast.pyx\", line 27, in fairseq.data.data_utils_fast.batch_by_size_fast\n cpdef list batch_by_size_fast(\n ValueError: Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'\n ##########################\n <denchmark-h:h4>What have you tried?</denchmark-h>\n \n \n I have tried to remove all &amps; tokens from the texts and make sure that the lines are equal in training files.\n change the --arch from fconv_iwslt_de_en to other values\n \n <denchmark-h:h4>What's your environment?</denchmark-h>\n \n \n fairseq Version (e.g., 1.0 or master): master\n PyTorch Version (e.g., 1.0)  : 1.2\n OS (e.g., Linux):  Windows 10\n How you installed fairseq (pip, source): install from source\n Build command you used (if compiling from source):\n git clone https://github.com/pytorch/fairseq\n cd fairseq\n python setup.py build_ext --inplace\n pip install --editable ./\n \n git clone <denchmark-link:https://github.com/roy-ht/editdistance>https://github.com/roy-ht/editdistance</denchmark-link>\n \n cd editdistance\n pip install --editable ./\n \n Python version: 3.8.5\n CUDA/cuDNN version: cudatoolkit 10.0.130\n GPU models and configuration: rank   0: capabilities =  6.1  ; total memory = 12.000 GB ; name = TITAN X (Pascal)\n Any other relevant information:\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "joeqi0370", "commentT": "2020-08-19T12:08:52Z", "comment_text": "\n \t\tsame here, any update?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "joeqi0370", "commentT": "2020-08-19T18:53:45Z", "comment_text": "\n \t\tWe don't support PyTorch 1.2 (>= 1.4 only) and Windows support is a bit flaky. <denchmark-link:https://github.com/hungviet0304>@hungviet0304</denchmark-link>\n  are you also on Windows?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "joeqi0370", "commentT": "2020-08-19T18:58:28Z", "comment_text": "\n \t\tAh, I think I know the issue though. Will submit a fix shortly.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "joeqi0370", "commentT": "2020-08-20T03:24:50Z", "comment_text": "\n \t\tYes, I have tried to run on Windows and this problem probably appear only on Windows. I also have tried to run on other OS - works perfectly.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "joeqi0370", "commentT": "2020-08-20T03:42:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/myleott>@myleott</denchmark-link>\n   oh,I also pulled the newest commit (master) and re-installed faisreq but the problem still shows up\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "joeqi0370", "commentT": "2020-08-20T14:28:15Z", "comment_text": "\n \t\tI only just merged the fix this morning (<denchmark-link:https://github.com/pytorch/fairseq/commit/adbd89fd4be9e68100bf9a4ba9eed1e7fb2e4040>adbd89f</denchmark-link>\n ). Can you try again?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "joeqi0370", "commentT": "2020-08-21T01:01:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/myleott>@myleott</denchmark-link>\n   It seems the problem is still there.\n \n I nuked everthing I have changed locally\n git reset --hard HEAD\n git clean -xffd\n git pull\n \n result output:\n <denchmark-code>remote: Enumerating objects: 139, done.\n remote: Counting objects: 100% (139/139), done.\n remote: Compressing objects: 100% (49/49), done.\n remote: Total 142 (delta 100), reused 126 (delta 90), pack-reused 3\n Receiving objects: 100% (142/142), 73.81 KiB | 123.00 KiB/s, done.\n Resolving deltas: 100% (101/101), completed with 53 local objects.\n From https://github.com/pytorch/fairseq\n    bd20dbda..83d701ac  master      -> origin/master\n  + 10608fd3...e3464e77 itr_bounds  -> origin/itr_bounds  (forced update)\n  * [new branch]        itr_bounds2 -> origin/itr_bounds2\n  * [new branch]        misc_fixes  -> origin/misc_fixes\n Updating bd20dbda..83d701ac\n Fast-forward\n  README.md                                          |   3 +\n  docs/getting_started.rst                           |   3 +\n  examples/constrained_decoding/README.md            | 124 +++++\n  examples/constrained_decoding/normalize.py         |  26 ++\n  examples/constrained_decoding/tok.py               |  31 ++\n  examples/translation_moe/src/translation_moe.py    |   3 +-\n  examples/wav2vec/README.md                         |  17 +-\n  examples/wav2vec/libri_labels.py                   |   4 +-\n  fairseq/__init__.py                                |   2 +\n  fairseq/clib/libnat_cuda/edit_dist.cu              |  36 +-\n  fairseq/criterions/legacy_masked_lm.py             |  18 +-\n  fairseq/criterions/masked_lm.py                    |   2 +-\n  fairseq/criterions/wav2vec_criterion.py            |   2 +-\n  fairseq/data/audio/raw_audio_dataset.py            |   2 +-\n  fairseq/data/data_utils_fast.pyx                   |  37 +-\n  fairseq/data/dictionary.py                         |   2 +-\n  fairseq/data/iterators.py                          |  34 +-\n  fairseq/data/language_pair_dataset.py              |  16 +\n  fairseq/data/token_block_utils_fast.pyx            |   4 +-\n  fairseq/file_io.py                                 |  10 +\n  fairseq/iterative_refinement_generator.py          |   6 +-\n  .../modules/transformer_sentence_encoder_layer.py  |   6 +-\n  fairseq/models/nat/levenshtein_utils.py            |   2 +-\n  fairseq/models/wav2vec/wav2vec2.py                 |   3 +\n  fairseq/modules/adaptive_softmax.py                |   2 +-\n  fairseq/optim/fp16_optimizer.py                    |  46 +-\n  fairseq/options.py                                 |   2 +\n  fairseq/scoring/bleu.py                            |  24 +-\n  fairseq/scoring/wer.py                             |   3 +-\n  fairseq/search.py                                  | 341 +++++++++++++-\n  fairseq/sequence_generator.py                      | 102 ++++-\n  fairseq/tasks/fairseq_task.py                      |  11 +-\n  fairseq/tasks/language_modeling.py                 |   5 +-\n  fairseq/tasks/masked_lm.py                         |  11 +-\n  fairseq/tasks/multilingual_translation.py          |   8 +-\n  fairseq/tasks/sentence_prediction.py               |  14 +-\n  fairseq/tasks/translation.py                       |  15 +-\n  fairseq/tasks/translation_from_pretrained_bart.py  |   6 +-\n  fairseq/tasks/translation_lev.py                   |   6 +-\n  fairseq/tasks/translation_multi_simple_epoch.py    |   8 +-\n  fairseq/token_generation_constraints.py            | 500 +++++++++++++++++++++\n  fairseq/trainer.py                                 |  12 +-\n  fairseq/utils.py                                   |   4 +-\n  fairseq_cli/eval_lm.py                             |   2 +-\n  fairseq_cli/generate.py                            |   6 +-\n  fairseq_cli/interactive.py                         |  85 +++-\n  scripts/constraints/extract.py                     |  83 ++++\n  scripts/constraints/validate.py                    |  33 ++\n  tests/test_constraints.py                          | 254 +++++++++++\n  tests/test_fp16_optimizer.py                       |  77 ++++\n  tests/test_inference_dropout.py                    |   5 +\n  tests/test_train.py                                |  11 +-\n  52 files changed, 1888 insertions(+), 181 deletions(-)\n  create mode 100644 examples/constrained_decoding/README.md\n  create mode 100755 examples/constrained_decoding/normalize.py\n  create mode 100755 examples/constrained_decoding/tok.py\n  create mode 100644 fairseq/token_generation_constraints.py\n  create mode 100755 scripts/constraints/extract.py\n  create mode 100755 scripts/constraints/validate.py\n  create mode 100755 tests/test_constraints.py\n  create mode 100644 tests/test_fp16_optimizer.py\n </denchmark-code>\n \n \n \n reinstall the fairseq with:\n pip install --editable ./\n \n \n the pre-process command:\n fairseq-preprocess --source-lang de --target-lang en --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test --destdir data-bin/iwslt14.tokenized.de-en\n \n \n train the model again(The latest version ask me to input optimizer, so I add \"--reset-optimizer\" parameter)\n \n \n <denchmark-code>Traceback (most recent call last):\n   File \"D:\\Python\\Anaconda3\\envs\\Fairseq\\Scripts\\fairseq-train-script.py\", line 33, in <module>\n     sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n   File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 333, in cli_main\n     distributed_utils.call_main(args, main)\n   File \"d:\\fairseq\\fairseq\\fairseq\\distributed_utils.py\", line 189, in call_main\n     main(args, **kwargs)\n   File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 109, in main\n     extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\n   File \"d:\\fairseq\\fairseq\\fairseq\\checkpoint_utils.py\", line 163, in load_checkpoint\n     extra_state = trainer.load_checkpoint(\n   File \"d:\\fairseq\\fairseq\\fairseq\\trainer.py\", line 275, in load_checkpoint\n     self._build_optimizer()\n   File \"d:\\fairseq\\fairseq\\fairseq\\trainer.py\", line 212, in _build_optimizer\n     self._optimizer = optim.build_optimizer(self.args, params)\n   File \"d:\\fairseq\\fairseq\\fairseq\\registry.py\", line 36, in build_x\n     raise ValueError('--{} is required!'.format(registry_name))\n ValueError: --optimizer is required!\n </denchmark-code>\n \n final command\uff1a\n fairseq-train data-bin/iwslt14.tokenized.de-en --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 --arch fconv_iwslt_de_en --save-dir checkpoints/fconv --reset-optimizer\n which outputs the same error:\n <denchmark-code>Traceback (most recent call last):\n   File \"D:\\Python\\Anaconda3\\envs\\Fairseq\\Scripts\\fairseq-train-script.py\", line 33, in <module>\n     sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n   File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 333, in cli_main\n     distributed_utils.call_main(args, main)\n   File \"d:\\fairseq\\fairseq\\fairseq\\distributed_utils.py\", line 189, in call_main\n     main(args, **kwargs)\n   File \"d:\\fairseq\\fairseq\\fairseq_cli\\train.py\", line 109, in main\n     extra_state, epoch_itr = checkpoint_utils.load_checkpoint(args, trainer)\n   File \"d:\\fairseq\\fairseq\\fairseq\\checkpoint_utils.py\", line 187, in load_checkpoint\n     epoch_itr = trainer.get_train_iterator(\n   File \"d:\\fairseq\\fairseq\\fairseq\\trainer.py\", line 335, in get_train_iterator\n     return self.task.get_batch_iterator(\n   File \"d:\\fairseq\\fairseq\\fairseq\\tasks\\fairseq_task.py\", line 214, in get_batch_iterator\n     batch_sampler = dataset.batch_by_size(\n   File \"d:\\fairseq\\fairseq\\fairseq\\data\\fairseq_dataset.py\", line 118, in batch_by_size\n     return data_utils.batch_by_size(\n   File \"d:\\fairseq\\fairseq\\fairseq\\data\\data_utils.py\", line 256, in batch_by_size\n     return batch_by_size_fast(\n   File \"fairseq\\data\\data_utils_fast.pyx\", line 28, in fairseq.data.data_utils_fast.batch_by_size_fast\n ValueError: Buffer dtype mismatch, expected 'DTYPE_t' but got 'long'\n </denchmark-code>\n \n In addition:\n \n I have try the command in Anaconda console and Cygwin's Terminal, both failed.\n I have tried fairseq 0.9 latest stable version, everything is OK. I am satisfied with the frist trained result.\n \n Oh, Maybe it is time for me to drop Windows 10 @@!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "joeqi0370", "commentT": "2020-08-24T14:58:52Z", "comment_text": "\n \t\tI'm running into this, too. It seems like a stray default dtype is lingering somewhere in the code that's causing an issue.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "joeqi0370", "commentT": "2020-08-24T18:54:04Z", "comment_text": "\n \t\tI tracked this down. The issue is as expected: default (non-portable) dtypes in FairseqDataset#ordered_indices are causing this issue. I've included the patch below; I'm happy to submit a PR in a few hours or someone else can take it if they'd like:\n diff --git fairseq/data/fairseq_dataset.py fairseq/data/fairseq_dataset.py\n index 2c972127..f196aff1 100644\n --- fairseq/data/fairseq_dataset.py\n +++ fairseq/data/fairseq_dataset.py\n @@ -50,7 +50,7 @@ class FairseqDataset(torch.utils.data.Dataset, EpochListening):\n      def ordered_indices(self):\n          \"\"\"Return an ordered list of indices. Batches will be constructed based\n          on this order.\"\"\"\n -        return np.arange(len(self))\n +        return np.arange(len(self), dtype=np.int64)\n \n      @property\n      def supports_prefetch(self):\n diff --git fairseq/data/language_pair_dataset.py fairseq/data/language_pair_dataset.py\n index aed54d61..fba3d37b 100644\n --- fairseq/data/language_pair_dataset.py\n +++ fairseq/data/language_pair_dataset.py\n @@ -372,9 +372,9 @@ class LanguagePairDataset(FairseqDataset):\n          \"\"\"Return an ordered list of indices. Batches will be constructed based\n          on this order.\"\"\"\n          if self.shuffle:\n -            indices = np.random.permutation(len(self))\n +            indices = np.random.permutation(len(self)).astype(np.int64)\n          else:\n -            indices = np.arange(len(self))\n +            indices = np.arange(len(self), dtype=np.int64)\n          if self.buckets is None:\n              # sort by target length, then source length\n              if self.tgt_sizes is not None:\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "joeqi0370", "commentT": "2020-08-25T04:27:28Z", "comment_text": "\n \t\tIt works perfectly. Thanks so much <denchmark-link:https://github.com/myleott>@myleott</denchmark-link>\n  , <denchmark-link:https://github.com/erip>@erip</denchmark-link>\n  !!!\n \t\t"}}}, "commit": {"commit_id": "226f0e45391ac1dcacb72ff68b523bf7b2ebceda", "commit_author": "Elijah Rippeth", "commitT": "2020-08-24 17:36:08-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\data\\fairseq_dataset.py", "file_new_name": "fairseq\\data\\fairseq_dataset.py", "file_complexity": {"file_NLOC": "84", "file_CCN": "26", "file_NToken": "531"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "53", "deleted_lines": "53", "method_info": {"method_name": "ordered_indices", "method_params": "self", "method_startline": "50", "method_endline": "53", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "22", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\data\\language_pair_dataset.py", "file_new_name": "fairseq\\data\\language_pair_dataset.py", "file_complexity": {"file_NLOC": "318", "file_CCN": "52", "file_NToken": "2328"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "375,377", "deleted_lines": "375,377", "method_info": {"method_name": "ordered_indices", "method_params": "self", "method_startline": "371", "method_endline": "390", "method_complexity": {"method_NLOC": "15", "method_CCN": "4", "method_NToken": "125", "method_nesting_level": "1"}}}}}}}}