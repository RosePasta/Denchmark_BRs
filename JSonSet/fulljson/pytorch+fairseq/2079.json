{"BR": {"BR_id": "2079", "BR_author": "zixiliuUSC", "BRopenT": "2020-04-30T06:33:08Z", "BRcloseT": "2020-05-04T14:16:42Z", "BR_text": {"BRsummary": "training transformer model with normalize-before generates extremely high ppl and a \"list assignment index out of range\" error", "BRdescription": "\n <denchmark-h:h2>\u2753 Questions and Help</denchmark-h>\n \n <denchmark-h:h4>What is your question?</denchmark-h>\n \n I use fairseq transformer model to build a grammar error correction model. The dataset I use is Lang8 and Conll2014. The two datasets are prepared as translation task in which source file is sentences with gramma error and target file is corrected sentences. Different sentences is seperated by '\\n'. The two dataset is concatonated and preprocessed by subword-nmt and then binarized by fairseq-preprocess.\n The problem I meet is that I train the model with or without flags, --decoder-normalize-before  --encoder-normalize-before, and get very different result and a strange error.\n <denchmark-h:h4>Code</denchmark-h>\n \n Train with --decoder-normalize-before  --encoder-normalize-before\n <denchmark-code>python train.py temp/bpe/bin \\\n     --save-dir checkpoints/transformer --arch transformer \\\n     --activation-fn relu \\\n     --dropout 0.1 --attention-dropout 0.1 --activation-dropout 0.1 \\\n     --encoder-embed-dim 256 \\\n     --encoder-ffn-embed-dim 256 --encoder-layers 4 \\\n     --encoder-attention-heads 4 \\\n     --decoder-embed-dim 256 --decoder-ffn-embed-dim 256 \\\n     --decoder-layers 4 \\\n     --decoder-attention-heads 4 \\\n     --encoder-layerdrop 0.1 --decoder-layerdrop 0.1 \\\n     --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\n     --max-tokens 5000 --task translation        \\\n     --keep-best-checkpoints 10 \\\n     --bpe subword_nmt \\\n     --update-freq 8 \\\n     --patience 8 \\\n     --no-save-optimizer-state \\\n     --best-checkpoint-metric ppl \\\n     --decoder-normalize-before --encoder-normalize-before \\\n     --ddp-backend no_c10d\n </denchmark-code>\n \n Training log: Since the error apeared in epoch4, I directly reran the script. And got the same error after three epoch.\n <denchmark-code>bash train1.sh \n 2020-04-29 21:50:54 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:14321\n 2020-04-29 21:50:54 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:14321\n 2020-04-29 21:50:55 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 1\n 2020-04-29 21:50:55 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 0\n 2020-04-29 21:50:57 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.1, best_checkpoint_metric='ppl', bpe='subword_nmt', bpe_codes=None, bpe_separator='@@', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='temp/bpe/bin', data_buffer_size=0, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=256, decoder_input_dim=256, decoder_layerdrop=0.1, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:14321', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=256, encoder_layerdrop=0.1, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=10, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5000, max_tokens_valid=5000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=8, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[8], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n 2020-04-29 21:50:57 | INFO | fairseq.tasks.translation | [src] dictionary: 48947 types\n 2020-04-29 21:50:57 | INFO | fairseq.tasks.translation | [tgt] dictionary: 48613 types\n 2020-04-29 21:50:57 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.src\n 2020-04-29 21:50:57 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.tgt\n 2020-04-29 21:50:57 | INFO | fairseq.tasks.translation | temp/bpe/bin valid src-tgt 8204 examples\n 2020-04-29 21:50:58 | INFO | fairseq_cli.train | TransformerModel(\n   (encoder): TransformerEncoder(\n     (embed_tokens): Embedding(48947, 256, padding_idx=1)\n     (embed_positions): SinusoidalPositionalEmbedding()\n     (layers): ModuleList(\n       (0): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (1): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (2): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (3): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n     )\n     (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n   )\n   (decoder): TransformerDecoder(\n     (embed_tokens): Embedding(48613, 256, padding_idx=1)\n     (embed_positions): SinusoidalPositionalEmbedding()\n     (layers): ModuleList(\n       (0): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (1): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (2): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (3): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n     )\n     (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (output_projection): Linear(in_features=256, out_features=48613, bias=False)\n   )\n )\n 2020-04-29 21:50:58 | INFO | fairseq_cli.train | model transformer, criterion CrossEntropyCriterion\n 2020-04-29 21:50:58 | INFO | fairseq_cli.train | num. model params: 41642240 (num. trained: 41642240)\n 2020-04-29 21:50:58 | INFO | fairseq_cli.train | training on 2 GPUs\n 2020-04-29 21:50:58 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None\n 2020-04-29 21:50:58 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/transformer/checkpoint_last.pt\n 2020-04-29 21:50:58 | INFO | fairseq.trainer | loading train data for epoch 1\n 2020-04-29 21:50:58 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.src\n 2020-04-29 21:50:58 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.tgt\n 2020-04-29 21:50:58 | INFO | fairseq.tasks.translation | temp/bpe/bin train src-tgt 1130841 examples\n 2020-04-29 21:51:05 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n epoch 001:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, *, Number alpha)\n /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, *, Number alpha)\n epoch 001 | valid on 'valid' subset | loss 4.067 | ppl 16.77 | wps 119770 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                        \n epoch 001 | valid on 'valid' subset | loss 4.067 | ppl 16.77 | wps 119685 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                        \n epoch 001 | loss 6.743 | ppl 107.11 | wps 59759.2 | ups 0.79 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.564 | clip 0 | train_wall 236 | wall 276                       \n epoch 002:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 21:55:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint1.pt (epoch 1 @ 210 updates, score 16.77) (writing took 0.49035206900043704 seconds)\n epoch 001 | loss 6.743 | ppl 107.11 | wps 59649.1 | ups 0.78 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.564 | clip 0 | train_wall 236 | wall 276                       \n epoch 002 | valid on 'valid' subset | loss 1.464 | ppl 2.76 | wps 121245 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2.76                                                         \n epoch 002 | valid on 'valid' subset | loss 1.464 | ppl 2.76 | wps 118177 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2.76                                                         \n epoch 002 | loss 1.986 | ppl 3.96 | wps 57927.6 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.262 | clip 0 | train_wall 243 | wall 551                         \n epoch 003:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:00:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint2.pt (epoch 2 @ 420 updates, score 2.76) (writing took 3.3108816809999553 seconds)\n epoch 002 | loss 1.986 | ppl 3.96 | wps 57341.3 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.262 | clip 0 | train_wall 243 | wall 555                         \n epoch 003 | valid on 'valid' subset | loss 1.184 | ppl 2.27 | wps 120108 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 2.27                                                         \n epoch 003 | valid on 'valid' subset | loss 1.184 | ppl 2.27 | wps 117797 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 2.27                                                         \n epoch 003 | loss 1.224 | ppl 2.34 | wps 57350.6 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.137 | clip 0 | train_wall 246 | wall 830                         \n epoch 004:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:04:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint3.pt (epoch 3 @ 630 updates, score 2.27) (writing took 3.109906989000592 seconds)\n epoch 003 | loss 1.224 | ppl 2.34 | wps 57392.5 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.137 | clip 0 | train_wall 243 | wall 833                         \n Traceback (most recent call last):                                                                                                                                                          \n   File \"train.py\", line 11, in <module>\n     cli_main()\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 355, in cli_main\n     nprocs=args.distributed_world_size,\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n     while not context.join():\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\n     raise Exception(msg)\n Exception: \n \n -- Process 1 terminated with the following error:\n Traceback (most recent call last):\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n     fn(i, *args)\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 324, in distributed_main\n     main(args, init_distributed=True)\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 117, in main\n     valid_losses = train(args, trainer, task, epoch_itr, max_update)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\n     return func(*args, **kwds)\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 187, in train\n     log_output = trainer.train_step(samples)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\n     return func(*args, **kwds)\n   File \"/home/zixi/EE-599/fairseq/fairseq/trainer.py\", line 379, in train_step\n     ignore_grad=is_dummy_batch,\n   File \"/home/zixi/EE-599/fairseq/fairseq/tasks/fairseq_task.py\", line 341, in train_step\n     loss, sample_size, logging_output = criterion(model, sample)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/criterions/cross_entropy.py\", line 29, in forward\n     net_output = model(**sample['net_input'])\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/legacy_distributed_data_parallel.py\", line 86, in forward\n     return self.module(*inputs, **kwargs)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 272, in forward\n     return_all_hiddens=return_all_hiddens,\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 498, in forward\n     encoder_states[-1] = x\n IndexError: list assignment index out of range\n </denchmark-code>\n \n <denchmark-code>$ bash train1.sh \n 2020-04-29 22:29:25 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19129\n 2020-04-29 22:29:25 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19129\n 2020-04-29 22:29:26 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 1\n 2020-04-29 22:29:26 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 0\n 2020-04-29 22:29:29 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.1, best_checkpoint_metric='ppl', bpe='subword_nmt', bpe_codes=None, bpe_separator='@@', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='temp/bpe/bin', data_buffer_size=0, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=256, decoder_input_dim=256, decoder_layerdrop=0.1, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19129', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=256, encoder_layerdrop=0.1, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=10, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5000, max_tokens_valid=5000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=8, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[8], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n 2020-04-29 22:29:29 | INFO | fairseq.tasks.translation | [src] dictionary: 48947 types\n 2020-04-29 22:29:29 | INFO | fairseq.tasks.translation | [tgt] dictionary: 48613 types\n 2020-04-29 22:29:29 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.src\n 2020-04-29 22:29:29 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.tgt\n 2020-04-29 22:29:29 | INFO | fairseq.tasks.translation | temp/bpe/bin valid src-tgt 8204 examples\n 2020-04-29 22:29:29 | INFO | fairseq_cli.train | TransformerModel(\n   (encoder): TransformerEncoder(\n     (embed_tokens): Embedding(48947, 256, padding_idx=1)\n     (embed_positions): SinusoidalPositionalEmbedding()\n     (layers): ModuleList(\n       (0): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (1): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (2): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (3): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n     )\n     (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n   )\n   (decoder): TransformerDecoder(\n     (embed_tokens): Embedding(48613, 256, padding_idx=1)\n     (embed_positions): SinusoidalPositionalEmbedding()\n     (layers): ModuleList(\n       (0): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (1): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (2): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (3): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n     )\n     (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n     (output_projection): Linear(in_features=256, out_features=48613, bias=False)\n   )\n )\n 2020-04-29 22:29:29 | INFO | fairseq_cli.train | model transformer, criterion CrossEntropyCriterion\n 2020-04-29 22:29:29 | INFO | fairseq_cli.train | num. model params: 41642240 (num. trained: 41642240)\n 2020-04-29 22:29:29 | INFO | fairseq_cli.train | training on 2 GPUs\n 2020-04-29 22:29:29 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None\n 2020-04-29 22:29:29 | INFO | fairseq.trainer | loaded checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 3 @ 0 updates)\n 2020-04-29 22:29:29 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n 2020-04-29 22:29:29 | INFO | fairseq.trainer | loading train data for epoch 3\n 2020-04-29 22:29:29 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.src\n 2020-04-29 22:29:30 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.tgt\n 2020-04-29 22:29:30 | INFO | fairseq.tasks.translation | temp/bpe/bin train src-tgt 1130841 examples\n epoch 004:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, *, Number alpha)\n /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, *, Number alpha)\n epoch 004 | valid on 'valid' subset | loss 1.075 | ppl 2.11 | wps 121131 | wpb 6793.9 | bsz 341.8 | num_updates 210 | best_ppl 2.11                                                         \n epoch 004 | valid on 'valid' subset | loss 1.075 | ppl 2.11 | wps 121005 | wpb 6793.9 | bsz 341.8 | num_updates 210 | best_ppl 2.11                                                         \n epoch 004 | loss 1.182 | ppl 2.27 | wps 57897.8 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.138 | clip 0 | train_wall 479 | wall 0                           \n epoch 005:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:34:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint4.pt (epoch 4 @ 210 updates, score 2.11) (writing took 3.373271124999519 seconds)\n epoch 004 | loss 1.182 | ppl 2.27 | wps 57536.8 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.138 | clip 0 | train_wall 479 | wall 0                           \n epoch 005 | valid on 'valid' subset | loss 0.997 | ppl 2 | wps 121011 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2                                                               \n epoch 005 | loss 1.005 | ppl 2.01 | wps 57402.3 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.1 | clip 0 | train_wall 245 | wall 0                             \n epoch 005 | valid on 'valid' subset | loss 0.997 | ppl 2 | wps 117614 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 2                                                               \n epoch 005: 100%|\u2589| 209/210 [04:34<00:01,  1.33s/it, loss=0.99, ppl=1.99, wps=58585.5, ups=0.77, wpb=76273.2, bsz=5474.4, num_updates=400, lr=0.005, gnorm=0.098, clip=0, train_wall=116, wal2020-04-29 22:38:47 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint5.pt (epoch 5 @ 420 updates, score 2.0) (writing took 3.413826895999591 seconds)\n epoch 005 | loss 1.005 | ppl 2.01 | wps 57391.1 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.1 | clip 0 | train_wall 242 | wall 0                             \n epoch 006 | valid on 'valid' subset | loss 0.963 | ppl 1.95 | wps 120335 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 1.95                                                         \n                                                                                                                                                                                            epoch 006 | loss 0.954 | ppl 1.94 | wps 56921.8 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.089 | clip 0 | train_wall 248 | wall 0                            \n epoch 006 | valid on 'valid' subset | loss 0.963 | ppl 1.95 | wps 117267 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 1.95                                                         \n epoch 007:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:43:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer/checkpoint6.pt (epoch 6 @ 630 updates, score 1.95) (writing took 3.180665442998361 seconds)\n epoch 006 | loss 0.954 | ppl 1.94 | wps 56971.1 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.089 | clip 0 | train_wall 244 | wall 0                           \n Traceback (most recent call last):                                                                                                                                                          \n   File \"train.py\", line 11, in <module>\n     cli_main()\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 355, in cli_main\n     nprocs=args.distributed_world_size,\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n     while not context.join():\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\n     raise Exception(msg)\n Exception: \n \n -- Process 1 terminated with the following error:\n Traceback (most recent call last):\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n     fn(i, *args)\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 324, in distributed_main\n     main(args, init_distributed=True)\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 117, in main\n     valid_losses = train(args, trainer, task, epoch_itr, max_update)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\n     return func(*args, **kwds)\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 187, in train\n     log_output = trainer.train_step(samples)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/contextlib.py\", line 74, in inner\n     return func(*args, **kwds)\n   File \"/home/zixi/EE-599/fairseq/fairseq/trainer.py\", line 379, in train_step\n     ignore_grad=is_dummy_batch,\n   File \"/home/zixi/EE-599/fairseq/fairseq/tasks/fairseq_task.py\", line 341, in train_step\n     loss, sample_size, logging_output = criterion(model, sample)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/criterions/cross_entropy.py\", line 29, in forward\n     net_output = model(**sample['net_input'])\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/legacy_distributed_data_parallel.py\", line 86, in forward\n     return self.module(*inputs, **kwargs)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 272, in forward\n     return_all_hiddens=return_all_hiddens,\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/zixi/EE-599/fairseq/fairseq/models/transformer.py\", line 498, in forward\n     encoder_states[-1] = x\n IndexError: list assignment index out of range\n \n </denchmark-code>\n \n Train without these two flags. The error doesn't raise anymore, but the ppl in this training is extremely bad. It seems the model learns nothing.\n Training script:\n <denchmark-code>python train.py temp/bpe/bin \\\n     --save-dir checkpoints/transformer1 --arch transformer \\\n     --activation-fn relu \\\n     --dropout 0.1 --attention-dropout 0.1 --activation-dropout 0.1 \\\n     --encoder-embed-dim 256 \\\n     --encoder-ffn-embed-dim 256 --encoder-layers 4 \\\n     --encoder-attention-heads 4 \\\n     --decoder-embed-dim 256 --decoder-ffn-embed-dim 256 \\\n     --decoder-layers 4 \\\n     --decoder-attention-heads 4 \\\n     --encoder-layerdrop 0.1 --decoder-layerdrop 0.1 \\\n     --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\n     --max-tokens 5000 --task translation        \\\n     --keep-best-checkpoints 10 \\\n     --bpe subword_nmt \\\n     --update-freq 8 \\\n     --patience 8 \\\n     --no-save-optimizer-state \\\n     --best-checkpoint-metric ppl \\\n     --ddp-backend no_c10d\n \n </denchmark-code>\n \n <denchmark-code>bash train.sh\n 2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:10163\n 2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:10163\n 2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 1\n 2020-04-29 22:53:37 | INFO | fairseq.distributed_utils | initialized host zixi-MS-7B79 as rank 0\n 2020-04-29 22:53:40 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.1, best_checkpoint_metric='ppl', bpe='subword_nmt', bpe_codes=None, bpe_separator='@@', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=25, cpu=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='temp/bpe/bin', data_buffer_size=0, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=4, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=256, decoder_input_dim=256, decoder_layerdrop=0.1, decoder_layers=4, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:10163', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=256, encoder_layerdrop=0.1, encoder_layers=4, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=10, keep_interval_updates=-1, keep_last_epochs=-1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5000, max_tokens_valid=5000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=True, no_scale_embedding=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=8, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer1', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[8], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)\n 2020-04-29 22:53:40 | INFO | fairseq.tasks.translation | [src] dictionary: 48947 types\n 2020-04-29 22:53:40 | INFO | fairseq.tasks.translation | [tgt] dictionary: 48613 types\n 2020-04-29 22:53:40 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.src\n 2020-04-29 22:53:40 | INFO | fairseq.data.data_utils | loaded 8204 examples from: temp/bpe/bin/valid.src-tgt.tgt\n 2020-04-29 22:53:40 | INFO | fairseq.tasks.translation | temp/bpe/bin valid src-tgt 8204 examples\n 2020-04-29 22:53:40 | INFO | fairseq_cli.train | TransformerModel(\n   (encoder): TransformerEncoder(\n     (embed_tokens): Embedding(48947, 256, padding_idx=1)\n     (embed_positions): SinusoidalPositionalEmbedding()\n     (layers): ModuleList(\n       (0): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (1): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (2): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (3): TransformerEncoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n     )\n   )\n   (decoder): TransformerDecoder(\n     (embed_tokens): Embedding(48613, 256, padding_idx=1)\n     (embed_positions): SinusoidalPositionalEmbedding()\n     (layers): ModuleList(\n       (0): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (1): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (2): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n       (3): TransformerDecoderLayer(\n         (self_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (encoder_attn): MultiheadAttention(\n           (k_proj): Linear(in_features=256, out_features=256, bias=True)\n           (v_proj): Linear(in_features=256, out_features=256, bias=True)\n           (q_proj): Linear(in_features=256, out_features=256, bias=True)\n           (out_proj): Linear(in_features=256, out_features=256, bias=True)\n         )\n         (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n         (fc1): Linear(in_features=256, out_features=256, bias=True)\n         (fc2): Linear(in_features=256, out_features=256, bias=True)\n         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n       )\n     )\n     (output_projection): Linear(in_features=256, out_features=48613, bias=False)\n   )\n )\n 2020-04-29 22:53:40 | INFO | fairseq_cli.train | model transformer, criterion CrossEntropyCriterion\n 2020-04-29 22:53:40 | INFO | fairseq_cli.train | num. model params: 41641216 (num. trained: 41641216)\n 2020-04-29 22:53:41 | INFO | fairseq_cli.train | training on 2 GPUs\n 2020-04-29 22:53:41 | INFO | fairseq_cli.train | max tokens per GPU = 5000 and max sentences per GPU = None\n 2020-04-29 22:53:41 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/transformer1/checkpoint_last.pt\n 2020-04-29 22:53:41 | INFO | fairseq.trainer | loading train data for epoch 1\n 2020-04-29 22:53:41 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.src\n 2020-04-29 22:53:41 | INFO | fairseq.data.data_utils | loaded 1130841 examples from: temp/bpe/bin/train.src-tgt.tgt\n 2020-04-29 22:53:41 | INFO | fairseq.tasks.translation | temp/bpe/bin train src-tgt 1130841 examples\n epoch 001:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:53:48 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n epoch 001:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, *, Number alpha)\n /opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, *, Number alpha)\n epoch 001 | valid on 'valid' subset | loss 9.883 | ppl 944.34 | wps 123133 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                       \n epoch 001 | valid on 'valid' subset | loss 9.883 | ppl 944.34 | wps 120444 | wpb 6793.9 | bsz 341.8 | num_updates 210                                                                       \n epoch 001 | loss 9.305 | ppl 632.7 | wps 60234.8 | ups 0.79 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.414 | clip 0 | train_wall 234 | wall 274                        \n epoch 002:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 22:58:15 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint1.pt (epoch 1 @ 210 updates, score 944.34) (writing took 0.5157724929995311 seconds)\n epoch 001 | loss 9.305 | ppl 632.7 | wps 60118.6 | ups 0.79 | wpb 76004.5 | bsz 5385 | num_updates 210 | lr 0.005 | gnorm 0.414 | clip 0 | train_wall 234 | wall 275                        \n epoch 002:  44%|\u258d| 92/210 [01:57<02:29,  1.27s/it, loss=9.176, ppl=578.29, wps=57866.5, ups=0.76, wpb=75663.3, bsz=5346.4, num_updates=300, lr=0.005, gnorm=0.23, clip=0, train_wall=113, was=0.76, wpb=75663.3, bsz=5346.4, num_updates=300, lr=0.005, gnorm=0.23, clip=0, train_wallepoch 002:  44%|\u258d| 92/210 [01:58<02:29,  1.27s/it, loss=9.176, ppl=578.29, wps=57866.3, ups=0.76, wpb=75663.3, bsz=5346.4, num_updates=300, lr=0.005, gnorm=0.23, clip=0, train_wallepoch 002 | valid on 'valid' subset | loss 11.463 | ppl 2822.93 | wps 121199 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 944.34                                                   \n epoch 002 | valid on 'valid' subset | loss 11.463 | ppl 2822.93 | wps 120872 | wpb 6793.9 | bsz 341.8 | num_updates 420 | best_ppl 944.34                                                   \n epoch 002 | loss 9.112 | ppl 553.22 | wps 58548.5 | ups 0.77 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 241 | wall 547                       \n epoch 003:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 23:02:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint2.pt (epoch 2 @ 420 updates, score 2822.93) (writing took 1.710711199000798 seconds)\n epoch 002 | loss 9.112 | ppl 553.22 | wps 58292.6 | ups 0.77 | wpb 76004.5 | bsz 5385 | num_updates 420 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 239 | wall 548                       \n epoch 003 | valid on 'valid' subset | loss 11.745 | ppl 3433.39 | wps 118206 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 944.34                                                   \n epoch 003 | valid on 'valid' subset | loss 11.745 | ppl 3433.39 | wps 119624 | wpb 6793.9 | bsz 341.8 | num_updates 630 | best_ppl 944.34                                                   \n epoch 003 | loss 8.952 | ppl 495.12 | wps 57402.9 | ups 0.76 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.279 | clip 0 | train_wall 245 | wall 825                       \n epoch 004:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 23:07:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint3.pt (epoch 3 @ 630 updates, score 3433.39) (writing took 2.4265237050021824 seconds)\n epoch 003 | loss 8.952 | ppl 495.12 | wps 57254.1 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 630 | lr 0.005 | gnorm 0.279 | clip 0 | train_wall 244 | wall 827                       \n epoch 004 | valid on 'valid' subset | loss 11.445 | ppl 2787.56 | wps 117655 | wpb 6793.9 | bsz 341.8 | num_updates 840 | best_ppl 944.34                                                   \n epoch 004 | valid on 'valid' subset | loss 11.445 | ppl 2787.56 | wps 121146 | wpb 6793.9 | bsz 341.8 | num_updates 840 | best_ppl 944.34\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:02<00:00, 11.55it/s]\n epoch 004 | loss 8.884 | ppl 472.4 | wps 57052 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 840 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 248 | wall 1104                         \n epoch 005:   0%|                                                                                                                                                    | 0/210 [00:00<?, ?it/s]2020-04-29 23:12:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/transformer1/checkpoint4.pt (epoch 4 @ 840 updates, score 2787.56) (writing took 1.7434893480021856 seconds)\n epoch 004 | loss 8.884 | ppl 472.4 | wps 57191.2 | ups 0.75 | wpb 76004.5 | bsz 5385 | num_updates 840 | lr 0.005 | gnorm 0.256 | clip 0 | train_wall 245 | wall 1106                       \n epoch 005:  11%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                                           | 23/210 [00:32<04:04,  1.31s/it]^CTraceback (most recent call last):\n   File \"train.py\", line 11, in <module>\n     cli_main()\n   File \"/home/zixi/EE-599/fairseq/fairseq_cli/train.py\", line 355, in cli_main\n     nprocs=args.distributed_world_size,\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_processes\n     while not context.join():\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 78, in join\n     timeout=timeout,\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n     ready = selector.select(timeout)\n   File \"/home/zixi/anaconda3/envs/ROC/lib/python3.7/selectors.py\", line 415, in select\n     fd_event_list = self._selector.poll(timeout)\n KeyboardInterrupt\n \n </denchmark-code>\n \n <denchmark-h:h4>What have you tried?</denchmark-h>\n \n I check my data again and I think there is nothing wrong with the data. As for the bugs, I have no idea.\n <denchmark-h:h4>What's your environment?</denchmark-h>\n \n \n fairseq Version : master, I install fairseq using git clone and pip install --editable .\n PyTorch Version: 1.5\n OS (e.g., Linux): Ubuntu 18.04 LTS\n How you installed fairseq: pip install --editable .\n Build command you used (if compiling from source): pip install --editable .\n Python version: 3.7.6\n CUDA/cuDNN version: 10.1\n GPU models and configuration: RTX-2070 and RTX-2060s, I use two cards to train the model.\n Any other relevant information: none\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "zixiliuUSC", "commentT": "2020-04-30T11:16:34Z", "comment_text": "\n \t\tThe normalize before flags move the layernorm to the beginning of each transformer block and often helps the model learn. If the ppl is poor without these flags, then you could try decreasing the learning rate, applying gradient clipping, using a learning rate warmup, etc. Or, just keep the flags :)\n The other index assignment traceback looks like a bug. I\u2019ll take a look.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "zixiliuUSC", "commentT": "2020-04-30T19:44:05Z", "comment_text": "\n \t\tThis looks like a bug specific to layerdrop. Will submit a fix shortly.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "zixiliuUSC", "commentT": "2020-05-01T04:11:50Z", "comment_text": "\n \t\t\n This looks like a bug specific to layerdrop. Will submit a fix shortly.\n \n Thank you\n \t\t"}}}, "commit": {"commit_id": "89d18af12792442f8ce5df86027aaa3f908240ec", "commit_author": "Myle Ott", "commitT": "2020-05-04 07:16:30-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 11, "file_old_name": "fairseq\\models\\transformer.py", "file_new_name": "fairseq\\models\\transformer.py", "file_complexity": {"file_NLOC": "741", "file_CCN": "100", "file_NToken": "4925"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1089,1090,1091,1092,1093", "method_info": {"method_name": "transformer_align", "method_params": "args", "method_startline": "1089", "method_endline": "1093", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "45", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "334,335,336", "method_info": {"method_name": "forward", "method_params": "self,src_tokens,src_lengths,prev_output_tokens", "method_startline": "334", "method_endline": "336", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "30", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1097,1098,1099,1100", "method_info": {"method_name": "transformer_wmt_en_de_big_align", "method_params": "args", "method_startline": "1097", "method_endline": "1100", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "33", "method_nesting_level": "0"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "579,580,581,582,583,584,585,586,587", "deleted_lines": "566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581", "method_info": {"method_name": "__init__", "method_params": "self,args,dictionary,embed_tokens,no_encoder_attn", "method_startline": "530", "method_endline": "633", "method_complexity": {"method_NLOC": "89", "method_CCN": "16", "method_NToken": "573", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": null, "deleted_lines": "155,156", "method_info": {"method_name": "add_args", "method_params": "parser", "method_startline": "94", "method_endline": "172", "method_complexity": {"method_NLOC": "74", "method_CCN": "1", "method_NToken": "617", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": null, "deleted_lines": "306,307,308,309,310", "method_info": {"method_name": "__init__", "method_params": "self,encoder,decoder,args", "method_startline": "306", "method_endline": "310", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "44", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "579,580", "deleted_lines": "566,567,568,569,570,571,572,573,574,575,576,577,578,579,580", "method_info": {"method_name": "buffered_future_mask", "method_params": "self,tensor", "method_startline": "566", "method_endline": "580", "method_complexity": {"method_NLOC": "15", "method_CCN": "5", "method_NToken": "115", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": null, "deleted_lines": "325,326,327,328,329,330,331,332", "method_info": {"method_name": "build_model", "method_params": "cls,args,task", "method_startline": "325", "method_endline": "332", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "36", "method_nesting_level": "1"}}}, "hunk_8": {"Ismethod": 1, "added_lines": "344,345,346,347,348,349,350,351", "deleted_lines": "309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361", "method_info": {"method_name": "__init__", "method_params": "self,args,dictionary,embed_tokens", "method_startline": "309", "method_endline": "361", "method_complexity": {"method_NLOC": "45", "method_CCN": "9", "method_NToken": "282", "method_nesting_level": "1"}}}, "hunk_9": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1023", "method_info": {"method_name": "base_architecture", "method_params": "args", "method_startline": "990", "method_endline": "1031", "method_complexity": {"method_NLOC": "40", "method_CCN": "1", "method_NToken": "391", "method_nesting_level": "0"}}}, "hunk_10": {"Ismethod": 1, "added_lines": "344", "deleted_lines": "338,339,340,341,342,343,344", "method_info": {"method_name": "forward_decoder", "method_params": "self,prev_output_tokens,encoder_out,incremental_state,features_only,extra_args", "method_startline": "338", "method_endline": "344", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "21", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "fairseq\\models\\transformer_align.py", "file_complexity": {"file_NLOC": "71", "file_CCN": "7", "file_NToken": "420"}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "fairseq\\modules\\__init__.py", "file_new_name": "fairseq\\modules\\__init__.py", "file_complexity": {"file_NLOC": "63", "file_CCN": "0", "file_NToken": "220"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "20,53", "deleted_lines": null}}}, "file_3": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "fairseq\\modules\\layer_drop.py", "file_complexity": {"file_NLOC": "35", "file_CCN": "5", "file_NToken": "99"}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\test_binaries.py", "file_new_name": "tests\\test_binaries.py", "file_complexity": {"file_NLOC": "1047", "file_CCN": "93", "file_NToken": "5585"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "251", "method_info": {"method_name": "test_transformer_cross_self_attention", "method_params": "self", "method_startline": "238", "method_endline": "253", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "81", "method_nesting_level": "1"}}}}}}}}