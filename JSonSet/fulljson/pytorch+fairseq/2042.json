{"BR": {"BR_id": "2042", "BR_author": "mohammadKhalifa", "BRopenT": "2020-04-22T08:11:49Z", "BRcloseT": "2020-05-01T11:05:58Z", "BR_text": {"BRsummary": "Masked LM training with BERT fails", "BRdescription": "\n Training with a masked lm objective as follows:\n fairseq-train $DATA_DIR \\\n \t\t--task masked_lm --criterion masked_lm\\\n \t\t--arch bert_base \\\n \t\t--optimizer adam --adam-betas '(0.9,0.98)' --adam-eps 1e-6 --clip-norm 1.0 \\\n \t\t--lr-scheduler polynomial_decay --lr $PEAK_LR --warmup-updates $WARMUP_UPDATES --total-num-update $TOTAL_UPDATES \\\n \t\t--dropout 0.1 --weight-decay 0.01 \\\n \t\t--max-tokens $TOKENS_PER_SAMPLE --update-freq $UPDATE_FREQ \\\n \t\t--max-update $TOTAL_UPDATES --log-format simple --log-interval 1 --save-interval-updates 1000\\\n \t\t--valid-subset valid \\\n \t\t--mask-prob 0.25\\\n \t\t--random-token-prob 0.2\\\n \t\t--skip-invalid-size-inputs-valid-test \\\n Gives me the following exception:\n <denchmark-code>File \"/home/mkhalifa/py36/bin/fairseq-train\", line 11, in <module>\n     load_entry_point('fairseq', 'console_scripts', 'fairseq-train')()\n   File \"/home/mkhalifa/fairseq/fairseq_cli/train.py\", line 321, in cli_main\n     main(args)\n   File \"/home/mkhalifa/fairseq/fairseq_cli/train.py\", line 96, in main\n     train(args, trainer, task, epoch_itr)\n   File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py\", line 52, in inner\n     return func(*args, **kwds)\n   File \"/home/mkhalifa/fairseq/fairseq_cli/train.py\", line 176, in train\n     log_output = trainer.train_step(samples)\n   File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/contextlib.py\", line 52, in inner\n     return func(*args, **kwds)\n   File \"/home/mkhalifa/fairseq/fairseq/trainer.py\", line 319, in train_step\n     ignore_grad=is_dummy_batch,\n   File \"/home/mkhalifa/fairseq/fairseq/tasks/fairseq_task.py\", line 337, in train_step\n     loss, sample_size, logging_output = criterion(model, sample)\n   File \"/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/mkhalifa/fairseq/fairseq/criterions/masked_lm.py\", line 52, in forward\n     ignore_index=self.padding_idx,\n   File \"/home/mkhalifa/py36/lib/python3.6/site-packages/torch/nn/functional.py\", line 1836, in nll_loss\n     .format(input.size(0), target.size(0)))\n ValueError: Expected input batch_size (892) to match target batch_size (223).\n \n </denchmark-code>\n \n The same error occurs when using other masked_lm architectures such as xlm_base. Roberta architectures work fine.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mohammadKhalifa", "commentT": "2020-04-22T14:43:33Z", "comment_text": "\n \t\tConfirmed that I can reproduce this on my end.  CC <denchmark-link:https://github.com/ngoyal2707>@ngoyal2707</denchmark-link>\n  <denchmark-link:https://github.com/myleott>@myleott</denchmark-link>\n , it seems that <denchmark-link:https://github.com/pytorch/fairseq/commit/718677ebb044e27aaf1a30640c2f7ab6b8fa8509>718677e</denchmark-link>\n  breaks .  Specifically,  doesn't do anything with the  parameter in its  method.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mohammadKhalifa", "commentT": "2020-04-23T09:52:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lematt1991>@lematt1991</denchmark-link>\n  <denchmark-link:https://github.com/ngoyal2707>@ngoyal2707</denchmark-link>\n  <denchmark-link:https://github.com/myleott>@myleott</denchmark-link>\n \n I created a pull request <denchmark-link:https://github.com/pytorch/fairseq/pull/2050>#2050</denchmark-link>\n  that fixed the issue\n \t\t"}}}, "commit": {"commit_id": "1f2bf68f95f2676a600cabb549180c974f8c3a56", "commit_author": "Muhammad", "commitT": "2020-05-01 04:06:02-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "fairseq\\models\\masked_lm.py", "file_new_name": "fairseq\\models\\masked_lm.py", "file_complexity": {"file_NLOC": "247", "file_CCN": "30", "file_NToken": "1776"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "191,221,222,223", "deleted_lines": "191,231", "method_info": {"method_name": "forward", "method_params": "self,src_tokens,segment_labels,unused", "method_startline": "191", "method_endline": "242", "method_complexity": {"method_NLOC": "23", "method_CCN": "6", "method_NToken": "171", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "191,221,222,223", "deleted_lines": "191,231", "method_info": {"method_name": "forward", "method_params": "self,src_tokens,segment_labels,masked_tokens,unused", "method_startline": "191", "method_endline": "244", "method_complexity": {"method_NLOC": "25", "method_CCN": "7", "method_NToken": "189", "method_nesting_level": "1"}}}}}}}}