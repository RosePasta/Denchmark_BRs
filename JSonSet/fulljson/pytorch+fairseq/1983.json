{"BR": {"BR_id": "1983", "BR_author": "noe", "BRopenT": "2020-04-08T19:15:44Z", "BRcloseT": "2020-04-14T17:28:32Z", "BR_text": {"BRsummary": "Sampling from GeneratorHubInterface does not generate anything", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When calling function  from , as specified in the <denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/language_model>language modeling readme</denchmark-link>\n , the result is always the same prefix passed as argument to the function, with no extra tokens sampled from the LM.\n Internally, the problem is that, when the sentence is converted to token IDs, the EOS token ID is added. When the model is presented with a prefix that \"has already ended\", it just generates another EOS and finishes.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Just follow the code to sample from an LM from the <denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/language_model>language modeling readme</denchmark-link>\n .\n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>from fairseq.models.transformer_lm import TransformerLanguageModel\n custom_lm = TransformerLanguageModel.from_pretrained('/path/to/model/dir', 'checkpoint100.pt')\n custom_lm.sample('Barack Obama', beam=1)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The returned sampled sentences have more tokens apart from the ones already supplied as argument to sample.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n fairseq Version: master:\n PyTorch Version: 1.4.0\n OS: Ubuntu 16.04.2 LTS\n How you installed fairseq: from source\n Python version: 3.7.4\n CUDA/cuDNN version: none\n GPU models and configuration: none\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "noe", "commentT": "2020-04-08T19:17:10Z", "comment_text": "\n \t\tI would be happy to provide a PR.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "noe", "commentT": "2020-04-09T02:41:21Z", "comment_text": "\n \t\tsame issue\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "noe", "commentT": "2020-04-09T19:07:56Z", "comment_text": "\n \t\tI got a similar issue here, I copied the code from template: <denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/language_model>https://github.com/pytorch/fairseq/tree/master/examples/language_model</denchmark-link>\n \n <denchmark-code>lm.sample('Barack Obama', sampling=True, sampling_topk=10, temperature=0.9)\n >>> \"Barack Obama\"\n </denchmark-code>\n \n It won't generate any longer sentence except 'Barack Obama'.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "noe", "commentT": "2020-04-13T22:14:24Z", "comment_text": "\n \t\tAh, thanks for flagging. I mistakenly broke this in <denchmark-link:https://github.com/pytorch/fairseq/commit/7c0ab23d14882d77ae5017ee71085925c5c03373>7c0ab23</denchmark-link>\n .\n Will submit a fix shortly.\n \t\t"}}}, "commit": {"commit_id": "88daeb748b31ad27de6c34630968e0fc191e4326", "commit_author": "Myle Ott", "commitT": "2020-04-14 10:28:15-07:00", "commit_complexity": {"commit_NLOC": "0.5714285714285714", "commit_CCN": "1.0", "commit_Nprams": "0.5714285714285714"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\data\\strip_token_dataset.py", "file_new_name": "fairseq\\data\\strip_token_dataset.py", "file_complexity": {"file_NLOC": "12", "file_CCN": "6", "file_NToken": "101"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "17,18,19,20,21", "deleted_lines": "17", "method_info": {"method_name": "__getitem__", "method_params": "self,index", "method_startline": "15", "method_endline": "21", "method_complexity": {"method_NLOC": "7", "method_CCN": "5", "method_NToken": "67", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\tasks\\language_modeling.py", "file_new_name": "fairseq\\tasks\\language_modeling.py", "file_complexity": {"file_NLOC": "210", "file_CCN": "28", "file_NToken": "1132"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "207,208,209,210,211,212,213,214,215,216,217,227,228,229,230", "deleted_lines": "206,207,208,209,210,211,212,213,214,215,216", "method_info": {"method_name": "build_dataset_for_inference", "method_params": "self,src_tokens,src_lengths,kwargs", "method_startline": "201", "method_endline": "241", "method_complexity": {"method_NLOC": "35", "method_CCN": "2", "method_NToken": "198", "method_nesting_level": "1"}}}}}}}}