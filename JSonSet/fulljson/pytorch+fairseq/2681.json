{"BR": {"BR_id": "2681", "BR_author": "joshim5", "BRopenT": "2020-10-01T18:37:14Z", "BRcloseT": "2020-11-03T22:07:15Z", "BR_text": {"BRsummary": "Performance deviation when saving/loading megatron checkpoints", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Loss changes when saving / loading checkpoints trained with megatron.\n The problem seems like it may be amplified with --memory-efficient-fp16.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Train a model without stopping (top) and again resuming from an intermediate checkpoint (bottom). When model-parallel is not turned on, the loss curves look the same before/after loading checkpoints.\n <denchmark-code>python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std3-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10 --memory-efficient-fp16 \n \n 2020-10-01 11:27:18 | INFO | train_inner | epoch 001:      1 / 39 loss=0.932, ppl=1.91, wps=0, ups=0, wpb=2048, bsz=16, num_updates=1, lr=1.25e-07, gnorm=0.046, loss_scale=128, train_wall=1, wall=5\n 2020-10-01 11:27:18 | INFO | train_inner | epoch 001:      2 / 39 loss=0.941, ppl=1.92, wps=20281, ups=9.9, wpb=2048, bsz=16, num_updates=2, lr=2.5e-07, gnorm=0.056, loss_scale=128, train_wall=0, wall=5\n 2020-10-01 11:27:18 | INFO | train_inner | epoch 001:      3 / 39 loss=0.933, ppl=1.91, wps=33879, ups=16.53, wpb=2048, bsz=16, num_updates=3, lr=3.75e-07, gnorm=0.052, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      4 / 39 loss=0.957, ppl=1.94, wps=32845, ups=16.02, wpb=2048, bsz=16, num_updates=4, lr=5e-07, gnorm=0.057, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      5 / 39 loss=0.92, ppl=1.89, wps=29635.2, ups=14.46, wpb=2048, bsz=16, num_updates=5, lr=6.25e-07, gnorm=0.057, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      6 / 39 loss=0.935, ppl=1.91, wps=30384.9, ups=14.82, wpb=2048, bsz=16, num_updates=6, lr=7.5e-07, gnorm=0.047, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      7 / 39 loss=0.945, ppl=1.92, wps=33652.5, ups=16.42, wpb=2048, bsz=16, num_updates=7, lr=8.75e-07, gnorm=0.075, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      8 / 39 loss=0.95, ppl=1.93, wps=33616.1, ups=16.4, wpb=2048, bsz=16, num_updates=8, lr=1e-06, gnorm=0.061, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | train_inner | epoch 001:      9 / 39 loss=0.947, ppl=1.93, wps=34367.3, ups=16.76, wpb=2048, bsz=16, num_updates=9, lr=1.125e-06, gnorm=0.044, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | train_inner | epoch 001:     10 / 39 loss=0.931, ppl=1.91, wps=31126.4, ups=15.19, wpb=2048, bsz=16, num_updates=10, lr=1.25e-06, gnorm=0.045, loss_scale=128, train_wall=0, wall=6\n 2020-10-01 11:27:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n 2020-10-01 11:27:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.94 | ppl 1.92 | wps 66250.8 | wpb 1013.2 | bsz 7.9 | num_updates 10\n 2020-10-01 11:27:25 | INFO | fairseq_cli.train | begin save checkpoint\n 2020-10-01 11:27:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std3-check/checkpoint_1_10.pt (epoch 1 @ 10 updates, score 0.94) (writing took 4.070859680883586 seconds)\n 2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     11 / 39 loss=0.943, ppl=1.92, wps=197.6, ups=0.1, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, loss_scale=128, train_wall=0, wall=16\n 2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     12 / 39 loss=0.925, ppl=1.9, wps=32072.8, ups=15.65, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.047, loss_scale=128, train_wall=0, wall=16\n 2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     13 / 39 loss=0.931, ppl=1.91, wps=37135.9, ups=18.11, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=16\n 2020-10-01 11:27:29 | INFO | train_inner | epoch 001:     14 / 39 loss=0.93, ppl=1.91, wps=37413.2, ups=18.25, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, loss_scale=128, train_wall=0, wall=17\n 2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     15 / 39 loss=0.916, ppl=1.89, wps=32179.6, ups=15.69, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=17\n 2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     16 / 39 loss=0.927, ppl=1.9, wps=38443.5, ups=18.75, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.038, loss_scale=128, train_wall=0, wall=17\n 2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     17 / 39 loss=0.931, ppl=1.91, wps=35584.3, ups=17.36, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.046, loss_scale=128, train_wall=0, wall=17\n 2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     18 / 39 loss=0.931, ppl=1.91, wps=32496, ups=16.88, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.051, loss_scale=128, train_wall=0, wall=17\n 2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     19 / 39 loss=0.908, ppl=1.88, wps=30532.6, ups=14.89, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.052, loss_scale=128, train_wall=0, wall=17\n 2020-10-01 11:27:30 | INFO | train_inner | epoch 001:     20 / 39 loss=0.94, ppl=1.92, wps=37303.4, ups=18.2, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.06, loss_scale=128, train_wall=0, wall=17\n \n python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std3-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128  --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10 --memory-efficient-fp16 \n \n 2020-10-01 11:28:02 | INFO | train_inner | epoch 001:     11 / 39 loss=0.943, ppl=1.92, wps=174.9, ups=0.09, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, loss_scale=128, train_wall=1, wall=0\n 2020-10-01 11:28:02 | INFO | train_inner | epoch 001:     12 / 39 loss=0.925, ppl=1.9, wps=7341.3, ups=3.58, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.047, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     13 / 39 loss=0.931, ppl=1.91, wps=36062.9, ups=17.59, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     14 / 39 loss=0.93, ppl=1.91, wps=38712.9, ups=18.88, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     15 / 39 loss=0.916, ppl=1.89, wps=33434.7, ups=16.31, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.049, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     16 / 39 loss=0.927, ppl=1.9, wps=31454.2, ups=15.34, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.038, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     17 / 39 loss=0.931, ppl=1.91, wps=33455.7, ups=16.32, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.046, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     18 / 39 loss=0.931, ppl=1.91, wps=33471, ups=17.4, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.051, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     19 / 39 loss=0.908, ppl=1.88, wps=34046.9, ups=16.61, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.052, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | train_inner | epoch 001:     20 / 39 loss=0.94, ppl=1.92, wps=36189.7, ups=17.65, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.06, loss_scale=128, train_wall=0, wall=0\n 2020-10-01 11:28:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n 2020-10-01 11:28:08 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.91 | ppl 1.88 | wps 67429.2 | wpb 1013.2 | bsz 7.9 | num_updates 20 | best_loss 0.91\n </denchmark-code>\n \n As another check, see the same thing without -mem-efficient-fp16:\n <denchmark-code>python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std4-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10\n \n 2020-10-01 11:29:37 | INFO | fairseq.trainer | begin training epoch 1\n 2020-10-01 11:29:42 | INFO | train_inner | epoch 001:      1 / 39 loss=0.932, ppl=1.91, wps=0, ups=0, wpb=2048, bsz=16, num_updates=1, lr=1.25e-07, gnorm=0.046, train_wall=2, wall=6\n 2020-10-01 11:29:43 | INFO | train_inner | epoch 001:      2 / 39 loss=0.941, ppl=1.92, wps=4584.9, ups=2.24, wpb=2048, bsz=16, num_updates=2, lr=2.5e-07, gnorm=0.056, train_wall=0, wall=6\n 2020-10-01 11:29:43 | INFO | train_inner | epoch 001:      3 / 39 loss=0.932, ppl=1.91, wps=7799.8, ups=3.81, wpb=2048, bsz=16, num_updates=3, lr=3.75e-07, gnorm=0.052, train_wall=0, wall=7\n 2020-10-01 11:29:43 | INFO | train_inner | epoch 001:      4 / 39 loss=0.955, ppl=1.94, wps=5649.1, ups=2.76, wpb=2048, bsz=16, num_updates=4, lr=5e-07, gnorm=0.057, train_wall=0, wall=7\n 2020-10-01 11:29:44 | INFO | train_inner | epoch 001:      5 / 39 loss=0.915, ppl=1.89, wps=6558.9, ups=3.2, wpb=2048, bsz=16, num_updates=5, lr=6.25e-07, gnorm=0.057, train_wall=0, wall=7\n 2020-10-01 11:29:44 | INFO | train_inner | epoch 001:      6 / 39 loss=0.928, ppl=1.9, wps=5850.9, ups=2.86, wpb=2048, bsz=16, num_updates=6, lr=7.5e-07, gnorm=0.047, train_wall=0, wall=8\n 2020-10-01 11:29:44 | INFO | train_inner | epoch 001:      7 / 39 loss=0.933, ppl=1.91, wps=5723.3, ups=2.79, wpb=2048, bsz=16, num_updates=7, lr=8.75e-07, gnorm=0.075, train_wall=0, wall=8\n 2020-10-01 11:29:45 | INFO | train_inner | epoch 001:      8 / 39 loss=0.935, ppl=1.91, wps=6092.8, ups=2.97, wpb=2048, bsz=16, num_updates=8, lr=1e-06, gnorm=0.061, train_wall=0, wall=8\n 2020-10-01 11:29:45 | INFO | train_inner | epoch 001:      9 / 39 loss=0.927, ppl=1.9, wps=6992.4, ups=3.41, wpb=2048, bsz=16, num_updates=9, lr=1.125e-06, gnorm=0.042, train_wall=0, wall=9\n 2020-10-01 11:29:45 | INFO | train_inner | epoch 001:     10 / 39 loss=0.908, ppl=1.88, wps=5788.6, ups=2.83, wpb=2048, bsz=16, num_updates=10, lr=1.25e-06, gnorm=0.045, train_wall=0, wall=9\n 2020-10-01 11:29:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n 2020-10-01 11:29:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.907 | ppl 1.87 | wps 17267.7 | wpb 1013.2 | bsz 7.9 | num_updates 10\n 2020-10-01 11:29:55 | INFO | fairseq_cli.train | begin save checkpoint\n 2020-10-01 11:29:59 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std4-check/checkpoint_1_10.pt (epoch 1 @ 10 updates, score 0.907) (writing took 4.3799392599612474 seconds)\n 2020-10-01 11:29:59 | INFO | train_inner | epoch 001:     11 / 39 loss=0.911, ppl=1.88, wps=149.6, ups=0.07, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, train_wall=0, wall=23\n 2020-10-01 11:30:00 | INFO | train_inner | epoch 001:     12 / 39 loss=0.887, ppl=1.85, wps=5720.2, ups=2.79, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.046, train_wall=0, wall=23\n 2020-10-01 11:30:00 | INFO | train_inner | epoch 001:     13 / 39 loss=0.893, ppl=1.86, wps=5695.3, ups=2.78, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.047, train_wall=0, wall=23\n 2020-10-01 11:30:00 | INFO | train_inner | epoch 001:     14 / 39 loss=0.881, ppl=1.84, wps=6004.7, ups=2.93, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, train_wall=0, wall=24\n 2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     15 / 39 loss=0.861, ppl=1.82, wps=5793.5, ups=2.83, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.048, train_wall=0, wall=24\n 2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     16 / 39 loss=0.868, ppl=1.83, wps=7520.7, ups=3.67, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.036, train_wall=0, wall=24\n 2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     17 / 39 loss=0.867, ppl=1.82, wps=7838.1, ups=3.83, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.043, train_wall=0, wall=25\n 2020-10-01 11:30:01 | INFO | train_inner | epoch 001:     18 / 39 loss=0.863, ppl=1.82, wps=6999.9, ups=3.64, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.046, train_wall=0, wall=25\n 2020-10-01 11:30:02 | INFO | train_inner | epoch 001:     19 / 39 loss=0.825, ppl=1.77, wps=7828.4, ups=3.82, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.047, train_wall=0, wall=25\n 2020-10-01 11:30:02 | INFO | train_inner | epoch 001:     20 / 39 loss=0.853, ppl=1.81, wps=7703.4, ups=3.76, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.056, train_wall=0, wall=25\n \n python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std4-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)'  --lr 0.0005 --lr-scheduler inverse_sqrt --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch roberta_large --update-freq 2 --save-interval-updates 10\n \n 2020-10-01 11:30:38 | INFO | fairseq.trainer | begin training epoch 1\n 2020-10-01 11:30:44 | INFO | train_inner | epoch 001:     11 / 39 loss=0.911, ppl=1.88, wps=138.4, ups=0.07, wpb=2048, bsz=16, num_updates=11, lr=1.375e-06, gnorm=0.055, train_wall=1, wall=0\n 2020-10-01 11:30:44 | INFO | train_inner | epoch 001:     12 / 39 loss=0.887, ppl=1.85, wps=5156, ups=2.52, wpb=2048, bsz=16, num_updates=12, lr=1.5e-06, gnorm=0.046, train_wall=0, wall=0\n 2020-10-01 11:30:45 | INFO | train_inner | epoch 001:     13 / 39 loss=0.893, ppl=1.86, wps=5299, ups=2.59, wpb=2048, bsz=16, num_updates=13, lr=1.625e-06, gnorm=0.047, train_wall=0, wall=0\n 2020-10-01 11:30:45 | INFO | train_inner | epoch 001:     14 / 39 loss=0.881, ppl=1.84, wps=6331.9, ups=3.09, wpb=2048, bsz=16, num_updates=14, lr=1.75e-06, gnorm=0.059, train_wall=0, wall=0\n 2020-10-01 11:30:45 | INFO | train_inner | epoch 001:     15 / 39 loss=0.861, ppl=1.82, wps=5262.7, ups=2.57, wpb=2048, bsz=16, num_updates=15, lr=1.875e-06, gnorm=0.048, train_wall=0, wall=0\n 2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     16 / 39 loss=0.868, ppl=1.83, wps=5776.1, ups=2.82, wpb=2048, bsz=16, num_updates=16, lr=2e-06, gnorm=0.036, train_wall=0, wall=0\n 2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     17 / 39 loss=0.867, ppl=1.82, wps=8960.5, ups=4.37, wpb=2048, bsz=16, num_updates=17, lr=2.125e-06, gnorm=0.043, train_wall=0, wall=0\n 2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     18 / 39 loss=0.863, ppl=1.82, wps=11617.3, ups=6.04, wpb=1922, bsz=16, num_updates=18, lr=2.25e-06, gnorm=0.046, train_wall=0, wall=0\n 2020-10-01 11:30:46 | INFO | train_inner | epoch 001:     19 / 39 loss=0.825, ppl=1.77, wps=5859.4, ups=2.86, wpb=2048, bsz=16, num_updates=19, lr=2.375e-06, gnorm=0.047, train_wall=0, wall=0\n 2020-10-01 11:30:47 | INFO | train_inner | epoch 001:     20 / 39 loss=0.853, ppl=1.81, wps=5785.5, ups=2.82, wpb=2048, bsz=16, num_updates=20, lr=2.5e-06, gnorm=0.056, train_wall=0, wall=0\n </denchmark-code>\n \n With Megatron (and no memory-efficient-fp16) the curves look slightly different:\n <denchmark-code>python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check9    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\n \n 2020-10-01 11:52:42 | INFO | train_inner | epoch 001:      1 / 78 loss=0.962, ppl=1.95, wps=0, ups=0, wpb=1024, bsz=8, num_updates=1, lr=2.24975e-07, gnorm=0.058, train_wall=2, wall=8\n 2020-10-01 11:52:42 | INFO | train_inner | epoch 001:      2 / 78 loss=0.981, ppl=1.97, wps=2233.4, ups=2.18, wpb=1024, bsz=8, num_updates=2, lr=3.4995e-07, gnorm=0.067, train_wall=0, wall=8\n 2020-10-01 11:52:43 | INFO | train_inner | epoch 001:      3 / 78 loss=0.953, ppl=1.94, wps=2413.9, ups=2.36, wpb=1024, bsz=8, num_updates=3, lr=4.74925e-07, gnorm=0.065, train_wall=0, wall=9\n 2020-10-01 11:52:43 | INFO | train_inner | epoch 001:      4 / 78 loss=0.996, ppl=2, wps=3858.7, ups=3.77, wpb=1024, bsz=8, num_updates=4, lr=5.999e-07, gnorm=0.06, train_wall=0, wall=9\n 2020-10-01 11:52:43 | INFO | train_inner | epoch 001:      5 / 78 loss=0.953, ppl=1.94, wps=3928.6, ups=3.84, wpb=1024, bsz=8, num_updates=5, lr=7.24875e-07, gnorm=0.047, train_wall=0, wall=9\n 2020-10-01 11:52:44 | INFO | train_inner | epoch 001:      6 / 78 loss=0.951, ppl=1.93, wps=3806, ups=3.72, wpb=1024, bsz=8, num_updates=6, lr=8.4985e-07, gnorm=0.054, train_wall=0, wall=9\n 2020-10-01 11:52:44 | INFO | train_inner | epoch 001:      7 / 78 loss=0.936, ppl=1.91, wps=2803.3, ups=2.74, wpb=1024, bsz=8, num_updates=7, lr=9.74825e-07, gnorm=0.056, train_wall=0, wall=10\n 2020-10-01 11:52:44 | INFO | train_inner | epoch 001:      8 / 78 loss=0.998, ppl=2, wps=2721.6, ups=2.66, wpb=1024, bsz=8, num_updates=8, lr=1.0998e-06, gnorm=0.066, train_wall=0, wall=10\n 2020-10-01 11:52:45 | INFO | train_inner | epoch 001:      9 / 78 loss=0.974, ppl=1.96, wps=2264.3, ups=2.21, wpb=1024, bsz=8, num_updates=9, lr=1.22478e-06, gnorm=0.062, train_wall=0, wall=11\n 2020-10-01 11:52:45 | INFO | train_inner | epoch 001:     10 / 78 loss=0.959, ppl=1.94, wps=2275, ups=2.22, wpb=1024, bsz=8, num_updates=10, lr=1.34975e-06, gnorm=0.07, train_wall=0, wall=11\n 2020-10-01 11:52:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n 2020-10-01 11:53:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.929 | ppl 1.9 | wps 6851.3 | wpb 509.6 | bsz 4 | num_updates 10\n 2020-10-01 11:53:02 | INFO | fairseq_cli.train | begin save checkpoint\n 2020-10-01 11:53:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std-check9/checkpoint_1_10-model_part-0.pt (epoch 1 @ 10 updates, score 0.929) (writing took 2.5728800734505057 seconds)\n 2020-10-01 11:53:05 | INFO | train_inner | epoch 001:     11 / 78 loss=0.939, ppl=1.92, wps=52.2, ups=0.05, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.057, train_wall=0, wall=31\n 2020-10-01 11:53:05 | INFO | train_inner | epoch 001:     12 / 78 loss=0.938, ppl=1.92, wps=2308.6, ups=2.25, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, train_wall=0, wall=31\n 2020-10-01 11:53:06 | INFO | train_inner | epoch 001:     13 / 78 loss=0.877, ppl=1.84, wps=2285.4, ups=2.23, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.089, train_wall=0, wall=31\n 2020-10-01 11:53:06 | INFO | train_inner | epoch 001:     14 / 78 loss=0.887, ppl=1.85, wps=2493.2, ups=2.43, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.05, train_wall=0, wall=32\n 2020-10-01 11:53:07 | INFO | train_inner | epoch 001:     15 / 78 loss=0.867, ppl=1.82, wps=2378.8, ups=2.32, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.052, train_wall=0, wall=32\n 2020-10-01 11:53:07 | INFO | train_inner | epoch 001:     16 / 78 loss=0.891, ppl=1.85, wps=2488.1, ups=2.43, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.06, train_wall=0, wall=33\n 2020-10-01 11:53:07 | INFO | train_inner | epoch 001:     17 / 78 loss=0.887, ppl=1.85, wps=3965, ups=3.87, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.059, train_wall=0, wall=33\n 2020-10-01 11:53:08 | INFO | train_inner | epoch 001:     18 / 78 loss=0.862, ppl=1.82, wps=3881, ups=3.79, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.085, train_wall=0, wall=33\n 2020-10-01 11:53:08 | INFO | train_inner | epoch 001:     19 / 78 loss=0.876, ppl=1.83, wps=4705.1, ups=4.59, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.061, train_wall=0, wall=33\n 2020-10-01 11:53:08 | INFO | train_inner | epoch 001:     20 / 78 loss=0.818, ppl=1.76, wps=3002.5, ups=2.93, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.051, train_wall=0, wall=34\n \n python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check9    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\n \n 2020-10-01 11:53:45 | INFO | train_inner | epoch 001:     11 / 78 loss=0.936, ppl=1.91, wps=44.1, ups=0.04, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.057, train_wall=2, wall=0\n 2020-10-01 11:53:45 | INFO | train_inner | epoch 001:     12 / 78 loss=0.94, ppl=1.92, wps=3380.6, ups=3.3, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, train_wall=0, wall=0\n 2020-10-01 11:53:45 | INFO | train_inner | epoch 001:     13 / 78 loss=0.877, ppl=1.84, wps=4735.6, ups=4.62, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.088, train_wall=0, wall=0\n 2020-10-01 11:53:46 | INFO | train_inner | epoch 001:     14 / 78 loss=0.887, ppl=1.85, wps=4641.6, ups=4.53, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.05, train_wall=0, wall=0\n 2020-10-01 11:53:46 | INFO | train_inner | epoch 001:     15 / 78 loss=0.867, ppl=1.82, wps=2367.4, ups=2.31, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.051, train_wall=0, wall=0\n 2020-10-01 11:53:46 | INFO | train_inner | epoch 001:     16 / 78 loss=0.891, ppl=1.85, wps=3147, ups=3.07, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.06, train_wall=0, wall=0\n 2020-10-01 11:53:47 | INFO | train_inner | epoch 001:     17 / 78 loss=0.888, ppl=1.85, wps=4429.3, ups=4.32, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.059, train_wall=0, wall=0\n 2020-10-01 11:53:47 | INFO | train_inner | epoch 001:     18 / 78 loss=0.863, ppl=1.82, wps=2539.5, ups=2.48, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.085, train_wall=0, wall=0\n 2020-10-01 11:53:47 | INFO | train_inner | epoch 001:     19 / 78 loss=0.877, ppl=1.84, wps=3398.7, ups=3.32, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.061, train_wall=0, wall=0\n 2020-10-01 11:53:48 | INFO | train_inner | epoch 001:     20 / 78 loss=0.817, ppl=1.76, wps=2572.9, ups=2.51, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.051, train_wall=0, wall=0\n </denchmark-code>\n \n However, with Megatron (+ memory-efficient-fp16) the curves look even further different:\n <denchmark-code>python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\n \n 2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      2 / 78 loss_scale=32, train_wall=0, wall=6\n 2020-10-01 10:59:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n 2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      3 / 78 loss_scale=16, train_wall=0, wall=6\n 2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      4 / 78 loss=1.006, ppl=2.01, wps=0, ups=0, wpb=1024, bsz=8, num_updates=1, lr=2.24975e-07, gnorm=0.056, loss_scale=16, train_wall=0, wall=6\n 2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      5 / 78 loss=0.97, ppl=1.96, wps=10604.8, ups=10.35, wpb=1024, bsz=8, num_updates=2, lr=3.4995e-07, gnorm=0.053, loss_scale=16, train_wall=0, wall=6\n 2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      6 / 78 loss=0.959, ppl=1.94, wps=15286.4, ups=14.91, wpb=1024, bsz=8, num_updates=3, lr=4.74925e-07, gnorm=0.057, loss_scale=16, train_wall=0, wall=7\n 2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      7 / 78 loss=0.966, ppl=1.95, wps=15897.1, ups=15.51, wpb=1024, bsz=8, num_updates=4, lr=5.999e-07, gnorm=0.057, loss_scale=16, train_wall=0, wall=7\n 2020-10-01 10:59:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n 2020-10-01 10:59:29 | INFO | train_inner | epoch 001:      8 / 78 loss=None, ppl=0, wps=0, ups=0, wpb=None, bsz=None, num_updates=None, lr=None, gnorm=None, loss_scale=8, train_wall=0, wall=7\n 2020-10-01 10:59:30 | INFO | train_inner | epoch 001:      9 / 78 loss=1.022, ppl=2.03, wps=16224.8, ups=15.83, wpb=1024, bsz=8, num_updates=5, lr=7.24875e-07, gnorm=0.066, loss_scale=8, train_wall=0, wall=7\n 2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     10 / 78 loss=1.008, ppl=2.01, wps=14974.3, ups=14.61, wpb=1024, bsz=8, num_updates=6, lr=8.4985e-07, gnorm=0.074, loss_scale=8, train_wall=0, wall=7\n 2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     11 / 78 loss=1.01, ppl=2.01, wps=14921.3, ups=14.56, wpb=1024, bsz=8, num_updates=7, lr=9.74825e-07, gnorm=0.059, loss_scale=8, train_wall=0, wall=7\n 2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     12 / 78 loss=1.008, ppl=2.01, wps=16674.8, ups=16.27, wpb=1024, bsz=8, num_updates=8, lr=1.0998e-06, gnorm=0.07, loss_scale=8, train_wall=0, wall=7\n 2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     13 / 78 loss=0.951, ppl=1.93, wps=15649, ups=15.26, wpb=1024, bsz=8, num_updates=9, lr=1.22478e-06, gnorm=0.098, loss_scale=8, train_wall=0, wall=7\n 2020-10-01 10:59:30 | INFO | train_inner | epoch 001:     14 / 78 loss=0.965, ppl=1.95, wps=16871.8, ups=16.46, wpb=1024, bsz=8, num_updates=10, lr=1.34975e-06, gnorm=0.063, loss_scale=8, train_wall=0, wall=7\n 2020-10-01 10:59:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n 2020-10-01 10:59:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.985 | ppl 1.98 | wps 34385.5 | wpb 509.6 | bsz 4 | num_updates 10\n 2020-10-01 10:59:36 | INFO | fairseq_cli.train | begin save checkpoint\n 2020-10-01 10:59:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/std-check/checkpoint_1_10-model_part-0.pt (epoch 1 @ 10 updates, score 0.985) (writing took 3.1006576996296644 seconds)\n 2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     15 / 78 loss=0.959, ppl=1.94, wps=110.3, ups=0.11, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.063, loss_scale=8, train_wall=0, wall=16\n 2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     16 / 78 loss=0.994, ppl=1.99, wps=15599.7, ups=15.22, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=16\n 2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     17 / 78 loss=0.991, ppl=1.99, wps=16336.3, ups=15.94, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.066, loss_scale=8, train_wall=0, wall=16\n 2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     18 / 78 loss=0.974, ppl=1.96, wps=13780.4, ups=13.45, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.085, loss_scale=8, train_wall=0, wall=17\n 2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     19 / 78 loss=1.013, ppl=2.02, wps=16322.8, ups=15.92, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.067, loss_scale=8, train_wall=0, wall=17\n 2020-10-01 10:59:39 | INFO | train_inner | epoch 001:     20 / 78 loss=0.968, ppl=1.96, wps=16886.4, ups=16.47, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=17\n 2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     21 / 78 loss=0.969, ppl=1.96, wps=15092.8, ups=14.73, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.065, loss_scale=8, train_wall=0, wall=17\n 2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     22 / 78 loss=0.932, ppl=1.91, wps=15917.8, ups=15.53, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.065, loss_scale=8, train_wall=0, wall=17\n 2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     23 / 78 loss=0.963, ppl=1.95, wps=16381.6, ups=15.98, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.069, loss_scale=8, train_wall=0, wall=17\n 2020-10-01 10:59:40 | INFO | train_inner | epoch 001:     24 / 78 loss=0.992, ppl=1.99, wps=16278, ups=15.88, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.073, loss_scale=8, train_wall=0, wall=17\n 2020-10-01 10:59:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n \n \n python fairseq_train.py --task masked_lm /checkpoint/bioseq_nonsecure/model-parallel-data/tiny_sample_valid_ur50-bin  --dataset-impl fasta  --save-dir checkpoints/std-check    --dropout 0.1   --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0   --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07   --tokens-per-sample 128 --sample-break-mode none   --max-tokens 128 --memory-efficient-fp16 --no-progress-bar --log-interval 1 --seed 4 --max-epoch 1 --max-update 50 --encoder-layers 4  --arch model_parallel_roberta_large --model-parallel-size 2 --update-freq 2 --save-interval-updates 10\n \n 2020-10-01 11:00:10 | INFO | train_inner | epoch 001:     15 / 78 loss=0.959, ppl=1.94, wps=85.2, ups=0.08, wpb=1024, bsz=8, num_updates=11, lr=1.47473e-06, gnorm=0.063, loss_scale=8, train_wall=1, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     16 / 78 loss=0.993, ppl=1.99, wps=8795.3, ups=8.58, wpb=1024, bsz=8, num_updates=12, lr=1.5997e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     17 / 78 loss=0.99, ppl=1.99, wps=17000.1, ups=16.59, wpb=1024, bsz=8, num_updates=13, lr=1.72468e-06, gnorm=0.066, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     18 / 78 loss=0.977, ppl=1.97, wps=16906.7, ups=16.49, wpb=1024, bsz=8, num_updates=14, lr=1.84965e-06, gnorm=0.085, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     19 / 78 loss=1.014, ppl=2.02, wps=16565.4, ups=16.16, wpb=1024, bsz=8, num_updates=15, lr=1.97463e-06, gnorm=0.068, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     20 / 78 loss=0.967, ppl=1.95, wps=17070, ups=16.65, wpb=1024, bsz=8, num_updates=16, lr=2.0996e-06, gnorm=0.062, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     21 / 78 loss=0.97, ppl=1.96, wps=17756.2, ups=17.32, wpb=1024, bsz=8, num_updates=17, lr=2.22458e-06, gnorm=0.066, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     22 / 78 loss=0.932, ppl=1.91, wps=16320.7, ups=15.92, wpb=1024, bsz=8, num_updates=18, lr=2.34955e-06, gnorm=0.065, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     23 / 78 loss=0.963, ppl=1.95, wps=17310.4, ups=16.89, wpb=1024, bsz=8, num_updates=19, lr=2.47453e-06, gnorm=0.068, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | train_inner | epoch 001:     24 / 78 loss=0.992, ppl=1.99, wps=17010.6, ups=16.58, wpb=1024, bsz=8, num_updates=20, lr=2.5995e-06, gnorm=0.073, loss_scale=8, train_wall=0, wall=0\n 2020-10-01 11:00:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n 2020-10-01 11:00:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.945 | ppl 1.92 | wps 33933.2 | wpb 509.6 | bsz 4 | num_updates 20 | best_loss 0.945\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n See commands above\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Loss curve should not change when saving/reloading a checkpoint.\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>Collecting environment information...\n PyTorch version: 1.5.0a0+4ff3872\n Is debug build: No\n CUDA used to build PyTorch: 10.1\n \n OS: Ubuntu 18.04.3 LTS\n GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n CMake version: version 3.10.2\n \n Python version: 3.6\n Is CUDA available: Yes\n CUDA runtime version: 10.1.105\n GPU models and configuration:\n GPU 0: Quadro GP100\n GPU 1: Quadro GP100\n \n Nvidia driver version: 418.116.00\n cuDNN version: Could not collect\n \n Versions of relevant libraries:\n [pip] msgpack-numpy==0.4.5\n [pip] numpy==1.18.3\n [pip] numpydoc==0.9.2\n [pip] pytorch-lightning==0.8.1\n [pip] pytorch-pretrained-bert==0.6.2\n [pip] pytorch-transformers==1.1.0\n [pip] torch==1.5.0a0+4ff3872\n [conda] blas                      1.0                         mkl\n [conda] libblas                   3.8.0                    15_mkl    conda-forge\n [conda] libcblas                  3.8.0                    15_mkl    conda-forge\n [conda] liblapack                 3.8.0                    15_mkl    conda-forge\n [conda] magma-cuda101             2.5.2                         1    pytorch\n [conda] mkl                       2020.1                      217\n [conda] mkl-include               2020.0                      166\n [conda] mkl-service               2.3.0            py36he904b0f_0\n [conda] mkl_fft                   1.0.15           py36ha843d7b_0\n [conda] mkl_random                1.1.0            py36hd6b4f25_0\n [conda] pytorch-lightning         0.8.1                     <pip>\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "joshim5", "commentT": "2020-10-03T00:25:11Z", "comment_text": "\n \t\tI believe this is due to the megatron code maintaining its own random number state, which is not managed by fairseq.\n Some upcoming changes to this code will make fairseq manage all of the RNG state, which should fix this issue (and also enable us to support model parallel on TPUs).\n \t\t"}}}, "commit": {"commit_id": "b120fbbe8fdb6fc8412149916fe09c54757bdaf6", "commit_author": "Joshua Meier", "commitT": "2020-11-03 14:07:06-08:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "fairseq\\model_parallel\\megatron_trainer.py", "file_new_name": "fairseq\\model_parallel\\megatron_trainer.py", "file_complexity": {"file_NLOC": "67", "file_CCN": "10", "file_NToken": "323"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "75,76,77,78,79,80,81", "deleted_lines": null, "method_info": {"method_name": "load_checkpoint", "method_params": "self,filename,reset_optimizer,reset_lr_scheduler,optimizer_overrides,reset_meters", "method_startline": "75", "method_endline": "81", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "22", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "69,70,71,72,73", "deleted_lines": null, "method_info": {"method_name": "save_checkpoint", "method_params": "self,filename,extra_state", "method_startline": "69", "method_endline": "73", "method_complexity": {"method_NLOC": "4", "method_CCN": "1", "method_NToken": "33", "method_nesting_level": "1"}}}}}}}}