{"BR": {"BR_id": "1860", "BR_author": "loopdigga96", "BRopenT": "2020-03-18T07:22:18Z", "BRcloseT": "2020-03-21T18:16:32Z", "BR_text": {"BRsummary": "wmt19 model cannot run on gpu except #0.", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n I am running <denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/wmt19>tutorial</denchmark-link>\n . I successfully loaded model from hub and tried to run it on second gpu (id=1). Which raised an exception that data and model are stored on different GPUs. With gpu(id=0) works fine.\n <denchmark-code>Using cache found in /home/vlad/.cache/torch/hub/pytorch_fairseq_master\n Loading codes from /home/vlad/.cache/torch/pytorch_fairseq/0695ef328ddefcb8cbcfabc3196182f59c0e41e0468b10cc0db2ae9c91881fcc.bb1be17de4233e13870bd7d6065bfdb03fca0a51dd0f5d0b7edf5c188eda71f1/bpecodes ...\n Read 30000 codes from the codes file.\n Traceback (most recent call last):\n   File \"/home/vlad/Documents/coding/experiments/paper-analyzer-Tretyak_Internship/papers/experiments/scientific_generation/scripts/paraphrase.py\", line 46, in <module>\n     en2de.translate(['hello'])\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 126, in translate\n     return self.sample(sentences, beam, verbose, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 132, in sample\n     batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/hub_utils.py\", line 165, in generate\n     translations = self.task.inference_step(generator, self.models, batch)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/fairseq_task.py\", line 351, in inference_step\n     return generator.generate(models, sample, prefix_tokens=prefix_tokens)\n   File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\n     return func(*args, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 93, in generate\n     return self._generate(model, sample, **kwargs)\n   File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\n     return func(*args, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 276, in _generate\n     tokens[:, :step + 1], encoder_outs, temperature=self.temperature,\n   File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/autograd/grad_mode.py\", line 49, in decorate_no_grad\n     return func(*args, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 549, in forward_decoder\n     temperature=temperature,\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/sequence_generator.py\", line 568, in _decode_one\n     tokens, encoder_out=encoder_out, incremental_state=self.incremental_states[model],\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/fairseq_model.py\", line 274, in forward_decoder\n     return self.decoder(prev_output_tokens, **kwargs)\n   File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py\", line 704, in forward\n     alignment_heads=alignment_heads,\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/models/transformer.py\", line 807, in extract_features\n     need_head_weights=bool((idx == alignment_layer)),\n   File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/transformer_layer.py\", line 297, in forward\n     need_head_weights=need_head_weights,\n   File \"/home/vlad/Documents/envs/p3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/home/vlad/.cache/torch/hub/pytorch_fairseq_master/fairseq/modules/multihead_attention.py\", line 325, in forward\n     key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float(\"-inf\")\n RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorMasked.cu:28\n \n Process finished with exit code 1\n \n </denchmark-code>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>import torch\n \n en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de',\n                        checkpoint_file='model1.pt:model2.pt:model3.pt:model4.pt',\n                        tokenizer='moses', bpe='fastbpe').to(torch.device('cuda:1'))\n \n result = en2de.translate(['hello'])\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Run without errors\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n fairseq Version==0.9.0\n PyTorch Version ==1.4.0\n OS (e.g., Linux): Ubuntu 18.04\n How you installed fairseq (pip, source): pip\n Build command you used (if compiling from source):\n Python version: 3.6\n CUDA/cuDNN version: 10.2\n GPU models and configuration: RTX 2080 x 2\n Any other relevant information:\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "loopdigga96", "commentT": "2020-10-11T03:59:14Z", "comment_text": "\n \t\tI still have this question when using gpu other than gpu: 0, have the problem been solved ?\n \t\t"}}}, "commit": {"commit_id": "bee6d71646c22eb552ec7d78d439729b38dfa55b", "commit_author": "Myle Ott", "commitT": "2020-03-21 11:16:24-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "fairseq\\modules\\layer_norm.py", "file_new_name": "fairseq\\modules\\layer_norm.py", "file_complexity": {"file_NLOC": "29", "file_CCN": "9", "file_NToken": "234"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "19,20", "deleted_lines": "19", "method_info": {"method_name": "forward", "method_params": "self,x", "method_startline": "18", "method_endline": "20", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "28", "method_nesting_level": "2"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "fairseq\\modules\\multihead_attention.py", "file_new_name": "fairseq\\modules\\multihead_attention.py", "file_complexity": {"file_NLOC": "393", "file_CCN": "20", "file_NToken": "2805"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "381,382,383,384,389,390,391,392", "deleted_lines": "381,382,383,384,389,390,391"}}}}}}