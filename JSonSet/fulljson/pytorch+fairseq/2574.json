{"BR": {"BR_id": "2574", "BR_author": "Alvant", "BRopenT": "2020-09-04T12:35:32Z", "BRcloseT": "2020-10-14T16:32:36Z", "BR_text": {"BRsummary": "Signature of apply_sparse_mask method in MultiheadAttention", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n In MultiheadAttention module,  method signature\n <denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L445>https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L445</denchmark-link>\n \n is invalid (but it still works). It is written like a function, not a class method.\n I suggest fixing the signature from\n def apply_sparse_mask(attn_weights, ...\n to\n def apply_sparse_mask(self, attn_weights, ...\n and the line <denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L320>https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L320</denchmark-link>\n  from\n attn_weights = MultiheadAttention.apply_sparse_mask(...)\n to\n attn_weights = self.apply_sparse_mask(...)\n Why am I suggesting using an instance method instead of a class- or staticmethod?\n I noticed that SparseMultiHeadAttention redefines this  method <denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sparse_multihead_attention.py#L101>https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sparse_multihead_attention.py#L101</denchmark-link>\n .\n So it seems that  signature would be more appropriate to use in MultiheadAttention.\n <denchmark-h:h3>P.S.</denchmark-h>\n \n It is not quite a bug, but it is something not quite right.\n <denchmark-h:h3>P.P.S.</denchmark-h>\n \n If the idea is OK, I could make a pullrequest.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Alvant", "commentT": "2020-09-08T14:38:00Z", "comment_text": "\n \t\tNice catch!  Definitely agree with your proposed fix, feel free to submit a PR, thanks!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Alvant", "commentT": "2020-09-08T20:44:28Z", "comment_text": "\n \t\tOk! Here <denchmark-link:https://github.com/pytorch/fairseq/pull/2587>#2587</denchmark-link>\n \n but for some reason the tests did not run after submitting the pull request...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Alvant", "commentT": "2020-09-09T20:48:16Z", "comment_text": "\n \t\tI think it might be because there was a lint error that got merged somehow.  It should be fixed now if you want to rebase.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Alvant", "commentT": "2020-10-15T16:27:52Z", "comment_text": "\n \t\t\ud83c\udf88 \ud83d\ude03\n \t\t"}}}, "commit": {"commit_id": "5e831033069b52b09905e0bf8ba104d016e04efd", "commit_author": "Vasiliy Alekseev", "commitT": "2020-10-14 09:32:23-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "examples\\speech_recognition\\models\\vggtransformer.py", "file_new_name": "examples\\speech_recognition\\models\\vggtransformer.py", "file_complexity": {"file_NLOC": "725", "file_CCN": "86", "file_NToken": "3951"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "462", "deleted_lines": "462", "method_info": {"method_name": "parse_transformer_sampling", "method_params": "self,transformer_sampling,num_layers", "method_startline": "437", "method_endline": "473", "method_complexity": {"method_NLOC": "21", "method_CCN": "7", "method_NToken": "102", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "397,399", "deleted_lines": "397,399", "method_info": {"method_name": "validate_transformer_config", "method_params": "self,transformer_config", "method_startline": "392", "method_endline": "401", "method_complexity": {"method_NLOC": "10", "method_CCN": "3", "method_NToken": "57", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "fairseq\\modules\\multihead_attention.py", "file_new_name": "fairseq\\modules\\multihead_attention.py", "file_complexity": {"file_NLOC": "400", "file_CCN": "21", "file_NToken": "2907"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "445", "deleted_lines": "445", "method_info": {"method_name": "apply_sparse_mask", "method_params": "self,attn_weights,int,int,int", "method_startline": "445", "method_endline": "446", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "21", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "445", "deleted_lines": "445", "method_info": {"method_name": "apply_sparse_mask", "method_params": "attn_weights,int,int,int", "method_startline": "445", "method_endline": "446", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "1"}}}}}}}}