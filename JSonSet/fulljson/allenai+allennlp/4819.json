{"BR": {"BR_id": "4819", "BR_author": "tmcclintock", "BRopenT": "2020-11-25T17:31:11Z", "BRcloseT": "2020-12-05T06:48:58Z", "BR_text": {"BRsummary": "Rename token.py to avoid bugs in certain Python versions", "BRdescription": "\n <denchmark-h:h2>Checklist</denchmark-h>\n \n \n  I have verified that the issue exists against the master branch of AllenNLP.\n  I have read the relevant section in the contribution guide on reporting bugs.\n  I have checked the issues list for similar or identical bug reports.\n  I have checked the pull requests list for existing proposed fixes.\n  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.\n  I have included in the \"Description\" section below a traceback from any exceptions related to this bug.\n  I have included in the \"Related issues or possible duplicates\" section below all related issues and possible duplicate issues (If there are none, check this box anyway).\n  I have included in the \"Environment\" section below the name of the operating system and Python version that I was using when I discovered this bug.\n  I have included in the \"Environment\" section below the output of pip freeze.\n  I have included in the \"Steps to reproduce\" section below a minimally reproducible example.\n \n <denchmark-h:h2>Description</denchmark-h>\n \n In certain version of Python (verified with 3.7.8) attempting to import from the file token.py causes a circular import because there is an identically named file in the dependency tree of the dataclasses module.\n \n Python traceback:\n \n Traceback (most recent call last):\n   File \"token.py\", line 1, in <module>\n     from dataclasses import dataclass\n   File \"/opt/anaconda3/envs/allentest/lib/python3.7/dataclasses.py\", line 5, in <module>\n     import inspect\n   File \"/opt/anaconda3/envs/allentest/lib/python3.7/inspect.py\", line 40, in <module>\n     import linecache\n   File \"/opt/anaconda3/envs/allentest/lib/python3.7/linecache.py\", line 11, in <module>\n     import tokenize\n   File \"/opt/anaconda3/envs/allentest/lib/python3.7/tokenize.py\", line 35, in <module>\n     from token import *\n   File \"/Users/tmcclintock/Github/allennlp/allennlp/data/tokenizers/token.py\", line 1, in <module>\n     from dataclasses import dataclass\n ImportError: cannot import name 'dataclass' from 'dataclasses' (/opt/anaconda3/envs/allentest/lib/python3.7/dataclasses.py)\n \n \n \n <denchmark-h:h2>Related issues or possible duplicates</denchmark-h>\n \n I was not able to find duplicate issue in the open or closed issues or in the current PRs.\n <denchmark-h:h2>Environment</denchmark-h>\n \n OS: OSX\n Python version: 3.7.8\n \n Output of pip freeze:\n \n certifi==2020.11.8\n \n \n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n \n Example source:\n \n Begin by creating a fresh environment, cloning, and changing into the relevant directory:\n conda create --name allentest python=3.7.8\n conda activate allentest\n git clone https://github.com/allenai/allennlp.git\n cd allennlp/allennlp/data/tokenizers/\n Attempt to run the file in question:\n python token.py\n \n \n <denchmark-h:h2>Proposed solution</denchmark-h>\n \n The fix is easy, in that you can just rename the file so there is no collision mv token.py token_class.py and update import statements.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tmcclintock", "commentT": "2020-11-25T18:28:04Z", "comment_text": "\n \t\tHuh, that's interesting. The error still happens when you run\n python allennlp/data/tokenizers/token.py\n from the root of the repo, but doesn't happen when import from a Python interpreter:\n python\n >>> from allennlp.data.tokenizers import token\n >>> # all good\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tmcclintock", "commentT": "2020-11-25T19:28:32Z", "comment_text": "\n \t\tYes, it's very subtle. The same error occurs in Python 3.9.0 (just checked).\n I think it speaks to how \"token\" is overloaded. Collisions were inevitable I guess...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tmcclintock", "commentT": "2020-11-25T21:14:25Z", "comment_text": "\n \t\tOk, yes, let's make that change. Would you like to make the PR <denchmark-link:https://github.com/tmcclintock>@tmcclintock</denchmark-link>\n ?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "tmcclintock", "commentT": "2020-11-25T21:40:13Z", "comment_text": "\n \t\tWill do. May take until the weekend though.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "tmcclintock", "commentT": "2020-11-25T21:41:06Z", "comment_text": "\n \t\tNo rush.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "tmcclintock", "commentT": "2020-12-05T06:53:32Z", "comment_text": "\n \t\tMy bad. Got busy.\n \t\t"}}}, "commit": {"commit_id": "6c3238ec8e714960174171decc47f9c34d1941f2", "commit_author": "Evan Pete Walsh", "commitT": "2020-12-04 22:48:56-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "16,17,18,19,20", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "allennlp\\common\\util.py", "file_new_name": "allennlp\\common\\util.py", "file_complexity": {"file_NLOC": "488", "file_CCN": "110", "file_NToken": "2482"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "72", "deleted_lines": "72", "method_info": {"method_name": "sanitize", "method_params": "Any", "method_startline": "66", "method_endline": "107", "method_complexity": {"method_NLOC": "32", "method_CCN": "13", "method_NToken": "204", "method_nesting_level": "0"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\__init__.py", "file_new_name": "allennlp\\data\\__init__.py", "file_complexity": {"file_NLOC": "14", "file_CCN": "0", "file_NToken": "107"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "12", "deleted_lines": "12,13"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\dataset_readers\\dataset_utils\\span_utils.py", "file_new_name": "allennlp\\data\\dataset_readers\\dataset_utils\\span_utils.py", "file_complexity": {"file_NLOC": "356", "file_CCN": "30", "file_NToken": "1637"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5", "deleted_lines": "5"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\fields\\namespace_swapping_field.py", "file_new_name": "allennlp\\data\\fields\\namespace_swapping_field.py", "file_complexity": {"file_NLOC": "47", "file_CCN": "7", "file_NToken": "260"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8", "deleted_lines": "8"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\fields\\text_field.py", "file_new_name": "allennlp\\data\\fields\\text_field.py", "file_complexity": {"file_NLOC": "129", "file_CCN": "32", "file_NToken": "831"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "16", "deleted_lines": "16"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\token_indexers\\elmo_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\elmo_indexer.py", "file_complexity": {"file_NLOC": "128", "file_CCN": "17", "file_NToken": "624"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7", "deleted_lines": "7"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\token_indexers\\pretrained_transformer_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\pretrained_transformer_indexer.py", "file_complexity": {"file_NLOC": "196", "file_CCN": "28", "file_NToken": "1022"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9", "deleted_lines": "9,10"}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\token_indexers\\pretrained_transformer_mismatched_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\pretrained_transformer_mismatched_indexer.py", "file_complexity": {"file_NLOC": "109", "file_CCN": "15", "file_NToken": "555"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9", "deleted_lines": "9"}}}, "file_9": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\token_indexers\\single_id_token_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\single_id_token_indexer.py", "file_complexity": {"file_NLOC": "96", "file_CCN": "9", "file_NToken": "431"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7", "deleted_lines": "7"}}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\token_indexers\\spacy_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\spacy_indexer.py", "file_complexity": {"file_NLOC": "55", "file_CCN": "5", "file_NToken": "287"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "10", "deleted_lines": "10"}}}, "file_11": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\token_indexers\\token_characters_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\token_characters_indexer.py", "file_complexity": {"file_NLOC": "123", "file_CCN": "10", "file_NToken": "687"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "11", "deleted_lines": "11,12"}}}, "file_12": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\token_indexers\\token_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\token_indexer.py", "file_complexity": {"file_NLOC": "98", "file_CCN": "10", "file_NToken": "353"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "7", "deleted_lines": "7"}}}, "file_13": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\__init__.py", "file_new_name": "allennlp\\data\\tokenizers\\__init__.py", "file_complexity": {"file_NLOC": "12", "file_CCN": "0", "file_NToken": "81"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "6,7", "deleted_lines": "6"}}}, "file_14": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\character_tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\character_tokenizer.py", "file_complexity": {"file_NLOC": "73", "file_CCN": "12", "file_NToken": "331"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5", "deleted_lines": "5"}}}, "file_15": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\letters_digits_tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\letters_digits_tokenizer.py", "file_complexity": {"file_NLOC": "17", "file_CCN": "2", "file_NToken": "95"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "6", "deleted_lines": "6"}}}, "file_16": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\pretrained_transformer_tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\pretrained_transformer_tokenizer.py", "file_complexity": {"file_NLOC": "387", "file_CCN": "22", "file_NToken": "1957"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9", "deleted_lines": "9"}}}, "file_17": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\spacy_tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\spacy_tokenizer.py", "file_complexity": {"file_NLOC": "116", "file_CCN": "14", "file_NToken": "486"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8", "deleted_lines": "8"}}}, "file_18": {"file_change_type": "RENAME", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\token.py", "file_new_name": "allennlp\\data\\tokenizers\\token_class.py", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_19": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\tokenizer.py", "file_complexity": {"file_NLOC": "70", "file_CCN": "6", "file_NToken": "157"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5", "deleted_lines": "5"}}}, "file_20": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\whitespace_tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\whitespace_tokenizer.py", "file_complexity": {"file_NLOC": "20", "file_CCN": "2", "file_NToken": "81"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5", "deleted_lines": "5"}}}, "file_21": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\data\\dataset_readers\\dataset_utils\\span_utils_test.py", "file_new_name": "tests\\data\\dataset_readers\\dataset_utils\\span_utils_test.py", "file_complexity": {"file_NLOC": "221", "file_CCN": "14", "file_NToken": "2476"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "6", "deleted_lines": "6,7"}}}, "file_22": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\data\\tokenizers\\letters_digits_tokenizer_test.py", "file_new_name": "tests\\data\\tokenizers\\letters_digits_tokenizer_test.py", "file_complexity": {"file_NLOC": "57", "file_CCN": "10", "file_NToken": "261"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "2", "deleted_lines": "2,3"}}}, "file_23": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\data\\tokenizers\\spacy_tokenizer_test.py", "file_new_name": "tests\\data\\tokenizers\\spacy_tokenizer_test.py", "file_complexity": {"file_NLOC": "106", "file_CCN": "21", "file_NToken": "557"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4", "deleted_lines": "4,5"}}}}}}