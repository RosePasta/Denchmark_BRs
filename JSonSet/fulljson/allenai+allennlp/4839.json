{"BR": {"BR_id": "4839", "BR_author": "dcfidalgo", "BRopenT": "2020-12-04T16:17:48Z", "BRcloseT": "2020-12-16T02:09:45Z", "BR_text": {"BRsummary": "Superfluous warning when extending the vocab in the `Embedding`", "BRdescription": "\n <denchmark-h:h2>Checklist</denchmark-h>\n \n \n  I have verified that the issue exists against the master branch of AllenNLP.\n  I have read the relevant section in the contribution guide on reporting bugs.\n  I have checked the issues list for similar or identical bug reports.\n  I have checked the pull requests list for existing proposed fixes.\n  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.\n  I have included in the \"Description\" section below a traceback from any exceptions related to this bug.\n  I have included in the \"Related issues or possible duplicates\" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).\n  I have included in the \"Environment\" section below the name of the operating system and Python version that I was using when I discovered this bug.\n  I have included in the \"Environment\" section below the output of pip freeze.\n  I have included in the \"Steps to reproduce\" section below a minimally reproducible example.\n \n <denchmark-h:h2>Description</denchmark-h>\n \n If one creates an allennlp.modules.token_embedders.embedding.Embedding without a pretrained_file, you still get a warning when extending the vocab that no pretrained_file is found. I would expect the warning only to trigger if one specified a pretrained_file when creating the Embedding or when an extension_pretrained_file is passed on to Embedding.extend_vocab.\n I would be more than happy to provide a PR if you think this is actually and issue and should be addressed.\n \n Python traceback:\n \n WARNING:root:Embedding at model_path, None cannot locate the pretrained_file.  If you are fine-tuning and want to use using pretrained_file for embedding extension, please pass the mapping by --embedding-sources argument.\n \n \n \n \n <denchmark-h:h2>Related issues or possible duplicates</denchmark-h>\n \n \n None\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n OS: Ubuntu 20.04\n Python version: 3.8.0\n \n Output of pip freeze:\n \n attrs==20.3.0\n blis==0.7.3\n boto3==1.16.29\n botocore==1.19.29\n catalogue==1.0.0\n certifi==2020.11.8\n chardet==3.0.4\n click==7.1.2\n cymem==2.0.4\n dataclasses==0.6\n filelock==3.0.12\n future==0.18.2\n h5py==3.1.0\n idna==2.10\n iniconfig==1.1.1\n jmespath==0.10.0\n joblib==0.17.0\n jsonnet==0.17.0\n jsonpickle==1.4.2\n murmurhash==1.0.4\n nltk==3.5\n numpy==1.19.4\n overrides==3.1.0\n packaging==20.7\n plac==1.1.3\n pluggy==0.13.1\n preshed==3.0.4\n protobuf==3.14.0\n py==1.9.0\n pyparsing==2.4.7\n pytest==6.1.2\n python-dateutil==2.8.1\n regex==2020.11.13\n requests==2.25.0\n s3transfer==0.3.3\n sacremoses==0.0.43\n scikit-learn==0.23.2\n scipy==1.5.4\n sentencepiece==0.1.91\n six==1.15.0\n spacy==2.3.4\n srsly==1.0.4\n tensorboardX==2.1\n thinc==7.4.3\n threadpoolctl==2.1.0\n tokenizers==0.9.3\n toml==0.10.2\n torch==1.7.0\n tqdm==4.54.0\n transformers==3.5.1\n typing-extensions==3.7.4.3\n urllib3==1.26.2\n wasabi==0.8.0\n \n \n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n \n Example source:\n \n from allennlp.data import Token, Instance, Vocabulary\n from allennlp.data.fields import TextField\n from allennlp.data.token_indexers import SingleIdTokenIndexer\n from allennlp.modules.token_embedders.embedding import Embedding\n \n instance = Instance({\"token\": TextField([Token(\"test\")], {\"tokens\": SingleIdTokenIndexer()})})\n vocab = Vocabulary.from_instances([instance])\n \n instance2 = Instance({\"token\": TextField([Token(\"this\")], {\"tokens\": SingleIdTokenIndexer()})})\n vocab2 = Vocabulary.from_instances([instance, instance2])\n \n embedder = Embedding(1, vocab=vocab)\n embedder.extend_vocab(vocab2)\n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dcfidalgo", "commentT": "2020-12-04T17:43:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dcfidalgo>@dcfidalgo</denchmark-link>\n  Hi, thanks for catching this! You are most welcome to submit a PR!\n \t\t"}}}, "commit": {"commit_id": "832901e8cb00ab0f415649bb2e52ffbab4a5ae5d", "commit_author": "David Fidalgo", "commitT": "2020-12-15 18:09:44-08:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "40", "deleted_lines": "40"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\modules\\token_embedders\\embedding.py", "file_new_name": "allennlp\\modules\\token_embedders\\embedding.py", "file_complexity": {"file_NLOC": "507", "file_CCN": "33", "file_NToken": "2228"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "256,288,289,290,292,293,294,295,296,297,298,299", "deleted_lines": "256,288,289,290,291,292,293,294,295,296,297,299,300"}}}}}}