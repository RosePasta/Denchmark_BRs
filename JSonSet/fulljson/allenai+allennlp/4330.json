{"BR": {"BR_id": "4330", "BR_author": "epwalsh", "BRopenT": "2020-06-05T22:18:19Z", "BRcloseT": "2020-07-09T18:50:13Z", "BR_text": {"BRsummary": "Transformer tokenizers cause deadlocks when dataset reader is lazy and dataloader num_workers &gt; 0", "BRdescription": "\n <denchmark-h:h2>Checklist</denchmark-h>\n \n \n  I have verified that the issue exists against the master branch of AllenNLP.\n  I have read the relevant section in the contribution guide on reporting bugs.\n  I have checked the issues list for similar or identical bug reports.\n  I have checked the pull requests list for existing proposed fixes.\n  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.\n  I have included in the \"Description\" section below a traceback from any exceptions related to this bug.\n  I have included in the \"Related issues or possible duplicates\" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).\n  I have included in the \"Environment\" section below the name of the operating system and Python version that I was using when I discovered this bug.\n  I have included in the \"Environment\" section below the output of pip freeze.\n  I have included in the \"Steps to reproduce\" section below a minimally reproducible example.\n \n <denchmark-h:h2>Description</denchmark-h>\n \n Dataset readers that use a PretrainedTransformerTokenizer can cause a deadlock when used lazily and with num_workers > 0 in the Dataloader.\n \n Python traceback:\n \n ...\n 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n 2020-06-05 15:04:14,980 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n 2020-06-05 15:04:14,982 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n 2020-06-05 15:04:14,982 - INFO - allennlp.training.trainer - Beginning training.\n 2020-06-05 15:04:14,982 - INFO - allennlp.training.trainer - Epoch 0/9\n 2020-06-05 15:04:14,982 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 3119.212\n 2020-06-05 15:04:14,989 - INFO - allennlp.common.file_utils - checking cache for https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl at /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687\n 2020-06-05 15:04:14,989 - INFO - allennlp.common.file_utils - waiting to acquire lock on /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687\n 2020-06-05 15:04:14,989 - INFO - filelock - Lock 140643240348696 acquired on /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687.lock\n 2020-06-05 15:04:14,989 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl is up-to-date\n 2020-06-05 15:04:14,989 - INFO - filelock - Lock 140643240348696 released on /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687.lock\n 2020-06-05 15:04:14,990 - INFO - allennlp_models.pair_classification.dataset_readers.snli - Reading SNLI instances from jsonl dataset at: /home/epwalsh/.allennlp/cache/144d0e8739288dbcde80c238baf94dde44c4ed58da59c92cf48ccce5b649574a.07ccd1720fbe07137833cd627398fb7e2687bdcbad963d71cae8c97a37f76687\n 2020-06-05 15:04:15,010 - INFO - allennlp.common.file_utils - checking cache for https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_test.jsonl at /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92\n 2020-06-05 15:04:15,011 - INFO - allennlp.common.file_utils - waiting to acquire lock on /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92\n 2020-06-05 15:04:15,011 - INFO - filelock - Lock 140642731300176 acquired on /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92.lock\n 2020-06-05 15:04:15,011 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_test.jsonl is up-to-date\n 2020-06-05 15:04:15,011 - INFO - filelock - Lock 140642731300176 released on /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92.lock\n 2020-06-05 15:04:15,011 - INFO - allennlp_models.pair_classification.dataset_readers.snli - Reading SNLI instances from jsonl dataset at: /home/epwalsh/.allennlp/cache/321a276d553780889b3d4276a1ce372c54c405f8e57917cdfa993feb5a0783a2.02752124e6570073d50876f7f66a32191d9787dcb992c56e79f558cebf5faf92\n 2020-06-05 15:04:15,026 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 29\n 2020-06-05 15:04:15,031 - INFO - allennlp.training.trainer - Training\n 2020-06-05 15:04:15,133 - INFO - allennlp.common.file_utils - checking cache for https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl at /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484\n 2020-06-05 15:04:15,133 - INFO - allennlp.common.file_utils - waiting to acquire lock on /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484\n 2020-06-05 15:04:15,134 - INFO - filelock - Lock 140642730869648 acquired on /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484.lock\n 2020-06-05 15:04:15,134 - INFO - allennlp.common.file_utils - cache of https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl is up-to-date\n 2020-06-05 15:04:15,134 - INFO - filelock - Lock 140642730869648 released on /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484.lock\n 2020-06-05 15:04:15,134 - INFO - allennlp_models.pair_classification.dataset_readers.snli - Reading SNLI instances from jsonl dataset at: /home/epwalsh/.allennlp/cache/e00019e4314663e308d583b0fce3b28548b1935143669947b5642779f366dce0.a6d9333199c8569c358962009242171183a893a532b6a5267c91fd3ea0ca6484\n # hangs forever here\n \n \n \n <denchmark-h:h2>Related issues or possible duplicates</denchmark-h>\n \n \n I'm pretty sure this has to do with huggingface/tokenizers#187\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n OS: Ubuntu 18.04\n Python version: 3.6.5\n \n Output of pip freeze:\n \n aiohttp==3.6.2\n alabaster==0.7.11\n -e git+git@github.com:epwalsh/allennlp.git@902d36a520dd75fd82ebaa014799ed8fa6d02e2e#egg=allennlp\n -e git+git@github.com:allenai/allennlp-beaker.git@d3afc6e23ae22ea434aff6b9296f9d6e17fc2b45#egg=allennlp_beaker\n -e git+git@github.com:allenai/allennlp-models.git@efe66bc086c95a78c7779933b82e1382b63a3ee1#egg=allennlp_models\n apex==0.1\n appdirs==1.4.3\n argh==0.26.2\n asn1crypto==0.24.0\n aspy.yaml==1.3.0\n astroid==2.3.0\n async-timeout==3.0.0\n atomicwrites==1.1.5\n attrs==19.3.0\n aws-xray-sdk==0.95\n awscli==1.18.40\n Babel==2.6.0\n backcall==0.1.0\n black==19.10b0\n bleach==2.1.3\n blis==0.4.1\n boto==2.49.0\n boto3==1.13.16\n botocore==1.16.16\n catalogue==1.0.0\n cattrs==0.9.0\n certifi==2020.4.5.1\n cffi==1.11.5\n cfgv==2.0.1\n chardet==3.0.4\n click==7.1.2\n click-completion==0.5.0\n click-spinner==0.1.10\n codecov==2.1.3\n colorama==0.3.9\n conllu==3.0\n cookies==2.2.1\n coverage==5.1\n coveralls==1.5.1\n crayons==0.1.2\n cryptography==2.3.1\n cycler==0.10.0\n cymem==2.0.3\n cytoolz==0.9.0.1\n dash==1.5.1\n dash-auth==1.3.2\n dash-bootstrap-components==0.7.2\n dash-core-components==1.4.0\n dash-daq==0.1.4\n dash-html-components==1.0.1\n dash-renderer==1.2.0\n dash-table==4.5.0\n dataclasses==0.7\n decorator==4.3.0\n dill==0.2.8.2\n docker==3.5.0\n docker-pycreds==0.3.0\n docopt==0.6.2\n docutils==0.15.2\n ecdsa==0.13\n editdistance==0.4\n en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n entrypoints==0.3\n filelock==3.0.12\n flake8==3.8.1\n flaky==3.6.1\n Flask==1.0.2\n Flask-Caching==1.7.2\n Flask-Compress==1.4.0\n Flask-Cors==3.0.7\n Flask-Login==0.4.1\n Flask-SeaSurf==0.2.2\n ftfy==5.5.0\n future==0.18.2\n gevent==1.3.6\n greenlet==0.4.14\n gunicorn==19.9.0\n h5py==2.10.0\n hide-code==0.5.2\n html5lib==1.0.1\n identify==1.4.7\n idna==2.9\n idna-ssl==1.1.0\n imagesize==1.0.0\n importlib-metadata==1.6.0\n importlib-resources==1.0.2\n inotify==0.2.10\n ipykernel==4.8.2\n ipython==6.5.0\n ipython-genutils==0.2.0\n ipywidgets==7.4.0\n isort==4.3.4\n itsdangerous==0.24\n jedi==0.16.0\n jeepney==0.4.3\n Jinja2==2.11.1\n jmespath==0.10.0\n joblib==0.15.1\n jsondiff==1.1.1\n jsonnet==0.16.0\n jsonpickle==1.4.1\n jsonschema==2.6.0\n jupyter==1.0.0\n jupyter-client==5.2.3\n jupyter-console==5.2.0\n jupyter-contrib-core==0.3.3\n jupyter-core==4.4.0\n jupyter-nbextensions-configurator==0.4.0\n keyring==21.2.1\n kiwisolver==1.0.1\n lazy-object-proxy==1.3.1                                                                                                                                                                                  \n livereload==2.5.2\n lunr==0.5.6\n Markdown==3.2.1\n markdown-include==0.5.1\n MarkupSafe==1.0\n mathy-pydoc==0.6.7\n matplotlib==3.2.1\n mccabe==0.6.1\n mistune==0.8.3\n mkdocs==1.1\n mkdocs-material==5.2.0\n mkdocs-material-extensions==1.0\n mock==2.0.0\n more-itertools==8.3.0\n moto==1.3.4\n msgpack==0.5.6\n msgpack-numpy==0.4.3.1\n multidict==4.7.6\n murmurhash==1.0.2\n mypy==0.770\n mypy-extensions==0.4.3\n nbconvert==5.3.1\n nbformat==4.4.0\n neovim==0.2.6\n -e git+git@github.com:epwalsh/nlp-models.git@23232ea470503e5a6453aca2bb9a22b84de6848d#egg=nlpete\n nltk==3.5\n nodeenv==1.3.3\n nose==1.3.7\n notebook==5.6.0\n nr.collections==0.0.1\n nr.databind==0.0.4\n nr.databind.core==0.0.14\n nr.databind.json==0.0.9\n nr.interface==0.0.2\n nr.metaclass==0.0.5\n nr.parsing.date==0.1.0\n nr.pylang.utils==0.0.2\n nr.stream==0.0.3\n numpy==1.18.4\n numpydoc==0.8.0\n nvidia-ml-py3==7.352.0\n overrides==3.0.0\n packaging==20.4\n pandocfilters==1.4.2\n parsimonious==0.8.0\n parso==0.6.2\n pathspec==0.7.0\n pathtools==0.1.2\n pbr==4.2.0\n pdfkit==0.6.1\n pexpect==4.6.0\n pickleshare==0.7.4\n pkg-resources==0.0.0\n pkginfo==1.4.2\n plac==1.1.3\n plotly==4.2.1                                                                                                                                                                                               [50/1861]\n pluggy==0.13.1\n port-for==0.3.1\n pre-commit==2.3.0\n preshed==3.0.2\n prometheus-client==0.3.1\n prompt-toolkit==1.0.15\n protobuf==3.12.1\n ptyprocess==0.6.0\n py==1.8.1\n py-rouge==1.1\n py3nvml==0.2.5\n pyaml==17.12.1\n pyasn1==0.4.4\n pycodestyle==2.6.0\n pycparser==2.18\n pycryptodome==3.6.5\n pycycle==0.0.8\n pydoc-markdown @ git+https://github.com/NiklasRosenstein/pydoc-markdown.git@f0bf8af1db4f11581c19d206d4ed1ab34b4854c1\n pydocstyle==5.0.2\n pyflakes==2.2.0\n Pygments==2.5.2\n pylint==2.4.1\n pymdown-extensions==7.0\n pypandoc==1.4\n pyparsing==2.4.7\n pytest==5.4.2\n pytest-cov==2.8.1\n python-dateutil==2.8.1\n python-jose==2.0.2\n pytorch-pretrained-bert==0.6.1\n pytz==2017.3\n PyYAML==5.3\n pyzmq==17.1.2\n qtconsole==4.3.1\n readme-renderer==26.0\n regex==2020.5.14\n registrable==0.0.1\n requests==2.23.0\n requests-toolbelt==0.8.0\n responses==0.10.14\n retrying==1.3.3\n rsa==3.4.2\n ruamel.yaml==0.16.10\n ruamel.yaml.clib==0.2.0\n s3transfer==0.3.3\n sacremoses==0.0.43\n scikit-learn==0.23.1\n scipy==1.4.1\n SecretStorage==3.1.2\n semantic-version==2.8.5\n Send2Trash==1.5.0\n sentencepiece==0.1.91\n shellingham==1.2.8\n simplegeneric==0.8.1\n six==1.15.0\n snowballstemmer==1.2.1\n spacy==2.2.4\n Sphinx==2.2.0\n sphinx-autobuild==0.7.1\n sphinx-rtd-theme==0.4.1\n sphinxcontrib-applehelp==1.0.1\n sphinxcontrib-devhelp==1.0.1\n sphinxcontrib-htmlhelp==1.0.2\n sphinxcontrib-jsmath==1.0.1\n sphinxcontrib-qthelp==1.0.2\n sphinxcontrib-serializinghtml==1.1.3\n sqlparse==0.2.4\n srsly==1.0.2\n tensorboardX==2.0\n terminado==0.8.1\n testpath==0.3.1\n thinc==7.4.0\n threadpoolctl==2.0.0\n tokenizers==0.7.0\n toml==0.10.0\n toolz==0.9.0\n torch==1.5.0\n tornado==5.1\n tqdm==4.46.1\n traitlets==4.3.2\n transformers==2.9.1\n twine==3.1.1\n typed-ast==1.4.0\n typing==3.7.4.1\n typing-extensions==3.7.4\n ua-parser==0.8.0\n ujson==1.35\n Unidecode==1.0.22\n urllib3==1.25.9\n virtualenv==16.7.5\n wasabi==0.6.0\n watchdog==0.10.2\n wcwidth==0.1.9\n webencodings==0.5.1\n websocket-client==0.49.0\n Werkzeug==0.14.1\n widgetsnbextension==3.4.0\n word2number==1.1\n wrapt==1.10.11\n xmltodict==0.11.0\n yarl==1.2.6\n zipp==3.1.0\n \n \n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n Run allennlp train on this config:\n \n Example source:\n \n local transformer_model = \"roberta-large\";\n local transformer_dim = 1024;\n local cls_is_last_token = false;\n \n {\n   \"dataset_reader\":{\n     \"type\": \"snli\",\n     \"lazy\": true,\n     \"tokenizer\": {\n       \"type\": \"pretrained_transformer\",\n       \"model_name\": transformer_model,\n       \"add_special_tokens\": false\n     },\n     \"token_indexers\": {\n       \"tokens\": {\n         \"type\": \"pretrained_transformer\",\n         \"model_name\": transformer_model,\n         \"max_length\": 40\n       }\n     }\n   },\n   \"train_data_path\": \"https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_train.jsonl\",\n   \"validation_data_path\": \"https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl\",\n   \"test_data_path\": \"https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_test.jsonl\",\n   \"model\": {\n     \"type\": \"basic_classifier\",\n     \"text_field_embedder\": {\n       \"token_embedders\": {\n         \"tokens\": {\n           \"type\": \"pretrained_transformer\",\n           \"model_name\": transformer_model,\n           \"max_length\": 512\n         }\n       }\n     },\n     \"seq2vec_encoder\": {\n        \"type\": \"cls_pooler\",\n        \"embedding_dim\": transformer_dim,\n        \"cls_is_last_token\": cls_is_last_token\n     },\n     \"feedforward\": {\n       \"input_dim\": transformer_dim,\n       \"num_layers\": 1,\n       \"hidden_dims\": transformer_dim,\n       \"activations\": \"tanh\"\n     },\n     \"dropout\": 0.1,\n     \"namespace\": \"tags\"\n   },\n   \"data_loader\": {\n     \"batch_size\" : 8,\n     \"num_workers\": true,\n   },\n   \"trainer\": {\n     \"num_epochs\": 10,\n     \"cuda_device\" : -1,\n     \"validation_metric\": \"+accuracy\",\n     \"learning_rate_scheduler\": {\n       \"type\": \"slanted_triangular\",\n       \"cut_frac\": 0.06\n     },\n     \"optimizer\": {\n       \"type\": \"huggingface_adamw\",\n       \"lr\": 2e-5,\n       \"weight_decay\": 0.1,\n     }\n   }\n }\n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "epwalsh", "commentT": "2020-06-05T22:45:06Z", "comment_text": "\n \t\tOur thinking is that we would have a very hard time resolving this issue ourselves, and it'll take work from huggingface's tokenizers repo to really fix this.  The suggested workaround is to just always avoid setting num_workers when using a huggingface tokenizer.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "epwalsh", "commentT": "2020-06-08T18:28:55Z", "comment_text": "\n \t\tHere's an even simpler repro example btw:\n from allennlp.data import DataLoader, Vocabulary\n from allennlp.data.tokenizers import PretrainedTransformerTokenizer\n from allennlp.data.token_indexers import PretrainedTransformerIndexer\n from allennlp_models.pair_classification.dataset_readers import SnliReader\n \n reader = SnliReader(\n     lazy=True,\n     max_instances=10,\n     tokenizer=PretrainedTransformerTokenizer(\n         model_name=\"roberta-large\", add_special_tokens=False\n     ),\n     token_indexers={\n         \"tokens\": PretrainedTransformerIndexer(\n             model_name=\"roberta-large\", max_length=40\n         )\n     },\n )\n \n print(\"initialized dataset\")\n ds = reader.read(\"https://allennlp.s3.amazonaws.com/datasets/snli/snli_1.0_dev.jsonl\")\n \n print(\"creating vocab\")\n vocab = Vocabulary.from_instances(ds)\n \n ds.index_with(vocab)\n \n print(\"reading instances\")\n # this will hang on the next line unless you remove the `num_workers=1` bit\n for instance in DataLoader(ds, num_workers=1):\n     print(instance)\n print(\"done\")\n I believe this confirms the issue is from <denchmark-link:https://github.com/huggingface/tokenizers/issues/187>huggingface/tokenizers#187</denchmark-link>\n .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "epwalsh", "commentT": "2020-06-15T22:27:12Z", "comment_text": "\n \t\tThere is a PR open now with tokenizers that provides a work-around for this: <denchmark-link:https://github.com/huggingface/tokenizers/pull/306>huggingface/tokenizers#306</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "epwalsh", "commentT": "2020-06-29T16:53:13Z", "comment_text": "\n \t\tThis is now fixed in the latest release of tokenizers (version 0.8.0), but we'll have to wait until it makes it upstream into a transformers release.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "epwalsh", "commentT": "2020-07-06T02:21:41Z", "comment_text": "\n \t\tI'm facing the same issue; is the solution to just wait for huggingface's release that will be incorporated here?\n Does this mean that AllenNLP 1.0 cannot be used for any code that uses the PretrainedTransformerTokenizer?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "epwalsh", "commentT": "2020-07-06T04:54:15Z", "comment_text": "\n \t\tJust don't set num_workers > 0 with a lazy dataset.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "epwalsh", "commentT": "2020-07-06T04:55:41Z", "comment_text": "\n \t\tSomeone from the group tried num_workers = 0 and it didn't work. I will try and report.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "epwalsh", "commentT": "2020-07-06T04:58:21Z", "comment_text": "\n \t\tWe have lots of models that train just fine with transformers, so it definitely is not a global problem. Make sure you have an up to date release, also.\n \t\t"}}}, "commit": {"commit_id": "b9a91646bd97942609b545794e889b16ba8e05a5", "commit_author": "dependabot-preview[bot]", "commitT": "2020-07-09 11:50:12-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27,47", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\data\\tokenizers\\pretrained_transformer_tokenizer.py", "file_new_name": "allennlp\\data\\tokenizers\\pretrained_transformer_tokenizer.py", "file_complexity": {"file_NLOC": "382", "file_CCN": "17", "file_NToken": "1837"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "131,133", "deleted_lines": "131,133,188"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "setup.py", "file_new_name": "setup.py", "file_complexity": {"file_NLOC": "59", "file_CCN": "0", "file_NToken": "187"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "67", "deleted_lines": "67"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\data\\token_indexers\\pretrained_transformer_indexer_test.py", "file_new_name": "tests\\data\\token_indexers\\pretrained_transformer_indexer_test.py", "file_complexity": {"file_NLOC": "169", "file_CCN": "16", "file_NToken": "1193"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "58", "deleted_lines": "58", "method_info": {"method_name": "test_as_array_produces_token_sequence_roberta", "method_params": "self", "method_startline": "54", "method_endline": "66", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "79", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "74,83", "deleted_lines": "74,83", "method_info": {"method_name": "test_as_array_produces_token_sequence_roberta_sentence_pair", "method_params": "self", "method_startline": "68", "method_endline": "83", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "96", "method_nesting_level": "1"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\data\\tokenizers\\pretrained_transformer_tokenizer_test.py", "file_new_name": "tests\\data\\tokenizers\\pretrained_transformer_tokenizer_test.py", "file_complexity": {"file_NLOC": "283", "file_CCN": "32", "file_NToken": "1432"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "123,124,125,134", "deleted_lines": "123,132", "method_info": {"method_name": "test_token_idx_bert_cased", "method_params": "self", "method_startline": "117", "method_endline": "140", "method_complexity": {"method_NLOC": "24", "method_CCN": "3", "method_NToken": "111", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "16", "deleted_lines": "16", "method_info": {"method_name": "test_splits_roberta", "method_params": "self", "method_startline": "10", "method_endline": "27", "method_complexity": {"method_NLOC": "17", "method_CCN": "2", "method_NToken": "58", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "146", "deleted_lines": "144", "method_info": {"method_name": "test_token_idx_roberta", "method_params": "self", "method_startline": "142", "method_endline": "163", "method_complexity": {"method_NLOC": "22", "method_CCN": "3", "method_NToken": "103", "method_nesting_level": "1"}}}}}}}}