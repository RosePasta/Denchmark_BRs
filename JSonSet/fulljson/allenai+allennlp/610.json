{"BR": {"BR_id": "610", "BR_author": "ChristophAlt", "BRopenT": "2017-12-14T00:16:28Z", "BRcloseT": "2018-01-08T17:17:35Z", "BR_text": {"BRsummary": "Data pipeline tutorial issues", "BRdescription": "\n While working through the data pipeline tutorial (<denchmark-link:http://allennlp.org/tutorials/data-pipeline>http://allennlp.org/tutorials/data-pipeline</denchmark-link>\n ), I noticed a couple of things:\n \n \n when installing allennlp v0.2.3 via pip (python 3.6.1), the tutorial fails, because at this point Dataset has no as_tensor_dict(..) method.\n \n \n in the second example, in\n \n \n <denchmark-code>print(vocab.get_index_to_token_vocabulary(\"tokens\"), \"\\n\")\n .\n print(vocab.get_index_to_token_vocabulary(\"chars\"), \"\\n\")\n </denchmark-code>\n \n \"vocab\" should be replaced with \"word_and_char_vocab\"\n \n there is a missing comma between \"good\" and \".\" in line\n \n <denchmark-code>review2 = TextField(list(map(Token, [\"This\", \"movie\", \"was\", \"quite\", \"slow\", \"but\", \"good\" \".\"])), token_indexers={\"tokens\": SingleIdTokenIndexer()})\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ChristophAlt", "commentT": "2017-12-14T00:27:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/joelgrus>@joelgrus</denchmark-link>\n , <denchmark-link:https://github.com/schmmd>@schmmd</denchmark-link>\n , issue 1 is a fundamental problem with how our systems are set up.  In order to merge a PR that updates an API, I have to update the notebook tutorials so that CI passes.  Those tutorials then get deployed on merge, which makes them out of sync with our pip version.  I don't think we actually want to automatically deploy our tutorials, or we need to have them versioned.\n For 2 and 3, I'm not sure how these are passing tests.  <denchmark-link:https://github.com/DeNeutoy>@DeNeutoy</denchmark-link>\n , shouldn't the notebook test be failing if there's invalid syntax?  Oh, nevermind.  2 is because there's prior state from earlier notebook cells (jupyter fail...), and 3 is because two strings next to each other are concatenated in python, so it's actually valid.  We can get these fixed.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ChristophAlt", "commentT": "2017-12-14T01:47:43Z", "comment_text": "\n \t\tthe tutorials are going to be out of sync one way or another, so I think having them versioned is the right approach. let me think about the right way to do this\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ChristophAlt", "commentT": "2017-12-14T16:19:31Z", "comment_text": "\n \t\tMy approach would be:\n \n \n Move our release process to a TeamCity configuration so it's scripted.\n \n \n Trigger deploying tutorials on successful release.  Tutorial deployment can update the tutorials to explicitly mention the pip installation they work for (latest).\n \n \n Have regular point releases weekly on Mondays.\n \n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ChristophAlt", "commentT": "2017-12-14T18:37:24Z", "comment_text": "\n \t\tCan we at least disable tutorial pushes now, and rollback the tutorials to the 0.2.3 release commit?  That way we'll have working tutorials while we figure out a better long term solution.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ChristophAlt", "commentT": "2017-12-14T18:40:30Z", "comment_text": "\n \t\tThat should be easy.\n <denchmark-link:https://github.com/joelgrus>@joelgrus</denchmark-link>\n  are you able to roll back the tutorials?\n I will add deploying tutorials to our release process, and we can do it manually for now.\n Peace.  Michael\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ChristophAlt", "commentT": "2017-12-14T18:41:55Z", "comment_text": "\n \t\tshould be easy\n \t\t"}}}, "commit": {"commit_id": "e82bbcabd5223275f8ba80d1077fa260a0a56542", "commit_author": "Mark Neumann", "commitT": "2018-01-08 17:17:34+00:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tutorials\\notebooks\\data_pipeline.ipynb", "file_new_name": "tutorials\\notebooks\\data_pipeline.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "31,32,54,61,71,88,95,115,116,121,142,149,150,157,158,163,165,168,169,170,196,197,237,244,245,252,253,255,256,288,295,296,297,298,320,340", "deleted_lines": "31,32,33,34,56,63,73,90,97,117,118,119,120,125,146,153,154,161,162,167,169,172,173,174,200,201,241,248,249,256,257,259,260,292,299,300,301,302,324,344"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tutorials\\notebooks\\embedding_tokens.ipynb", "file_new_name": "tutorials\\notebooks\\embedding_tokens.ipynb", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "35,36,51,52,62,66,82,89,90,97,99,120,121,131,132,142,167,261", "deleted_lines": "35,36,37,38,53,54,55,56,66,70,86,93,94,101,103,124,125,135,136,137,138,148,173,267"}}}}}}