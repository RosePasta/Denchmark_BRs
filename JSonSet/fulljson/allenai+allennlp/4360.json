{"BR": {"BR_id": "4360", "BR_author": "koren-v", "BRopenT": "2020-06-15T14:10:01Z", "BRcloseT": "2020-11-02T22:21:23Z", "BR_text": {"BRsummary": "Can't get correct output using .saliency_interpret_from_json() from Interpret", "BRdescription": "\n Hi, I'm trying to get the importance of each word in a sentence while sentiment classification but always get the same result for different inputs. The Code:\n <denchmark-code>from allennlp.predictors.predictor import Predictor\n import allennlp_models.classification\n from allennlp.interpret.saliency_interpreters import (\n     SaliencyInterpreter,\n     SimpleGradient,\n     SmoothGradient,\n     IntegratedGradient,\n )\n \n predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/sst-roberta-large-2020.06.08.tar.gz\")\n simple_grad = SimpleGradient(predictor)\n \n input_json = {'sentence' : \"a very well-made, funny and entertaining picture.\"}\n \n simple_grad.saliency_interpret_from_json(input_json)\n </denchmark-code>\n \n gives:\n <denchmark-code>{'instance_1': {'grad_input_1': [1.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0]}}\n </denchmark-code>\n \n So what can be the reason of this problem?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "koren-v", "commentT": "2020-06-15T16:00:04Z", "comment_text": "\n \t\tNo idea what's causing the issue yet, but I have confirmed that this is a bug.  I get the correct output with rc3, but it's broken with rc6.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "koren-v", "commentT": "2020-06-15T17:55:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/matt-gardner>@matt-gardner</denchmark-link>\n  Thanks for your answer.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "koren-v", "commentT": "2020-06-15T19:37:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/matt-gardner>@matt-gardner</denchmark-link>\n  Is it possible way to load this model in rc3? Because using the same code with this version gives:\n <denchmark-code>ConfigurationError: sst_tokens not in acceptable choices for validation_dataset_reader.type: ['conll2003', 'interleaving', 'sequence_tagging', 'sharded', 'babi', 'text_classification_json']. You should either use the --include-package flag to make sure the correct module is loaded, or use a fully qualified class name in your config file like {\"model\": \"my_module.models.MyModel\"} to have it imported automatically.\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "koren-v", "commentT": "2020-06-15T19:59:58Z", "comment_text": "\n \t\tYeah, just include this line in your script: from allennlp_models import sentiment.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "koren-v", "commentT": "2020-06-30T21:54:05Z", "comment_text": "\n \t\tPossibly related: <denchmark-link:https://github.com/allenai/allennlp-models/pull/85>allenai/allennlp-models#85</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "koren-v", "commentT": "2020-08-18T16:18:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/matt-gardner>@matt-gardner</denchmark-link>\n  this is just a friendly ping to make sure you haven't forgotten about this issue \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "koren-v", "commentT": "2020-08-18T19:42:51Z", "comment_text": "\n \t\tI thought the issue might have been resolved with recent changes to the SST model / tokenization stuff.  But I just ran the model on master (both this and the models repo), and had the same issue.  This is still not resolved.\n We're giving a tutorial at EMNLP on interpreting predictions, and this should definitely be fixed before then.  I'm putting this into the 1.2 milestone, but only so we don't forget about it; it really should be in a 1.3 or a 2.1 milestone, or something, but those don't exist yet.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "koren-v", "commentT": "2020-10-09T23:10:37Z", "comment_text": "\n \t\tJust because it hasn't been mentioned yet: The problem is that it takes the gradients at the top of the transformer, not the bottom, when using the mismatched embedder. Since the pooler takes only the first token ([CLS]), all the gradients are there.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "koren-v", "commentT": "2020-10-15T21:40:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/matt-gardner>@matt-gardner</denchmark-link>\n , do you have an idea of when you will get to this?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "koren-v", "commentT": "2020-10-15T22:15:59Z", "comment_text": "\n \t\tI have some thoughts on how to fix this.  If I'm lucky, I can maybe get to it tomorrow; if not, I'm not sure, but I might be able to make it happen next week.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "koren-v", "commentT": "2020-11-02T23:19:09Z", "comment_text": "\n \t\tI couldn't get the demo to work end-to-end, but I have confirmed that the example from in here works now as expected.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "koren-v", "commentT": "2020-11-07T00:01:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dirkgr>@dirkgr</denchmark-link>\n  can you share which example works?  i am still having trouble getting this working.  i tried 1.0.0rc3 (as suggested way up above) and see this error:  \n this still seems broken.\n i tried 1.1.0, 1.2.0, and master and i see the incorrect array [1.0, 0.0, 0.0, ...].\n here is my code:  <denchmark-link:https://github.com/data-science-on-aws/workshop/blob/1958164/07_train/wip/99_AllenNLP_RoBERTa_Prediction.ipynb>https://github.com/data-science-on-aws/workshop/blob/1958164/07_train/wip/99_AllenNLP_RoBERTa_Prediction.ipynb</denchmark-link>\n \n any help would be appreciated!  hoping to get a working demo of this soon.\n should we re-open this?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "koren-v", "commentT": "2020-11-07T01:08:07Z", "comment_text": "\n \t\tYou have to install  from both allennlp and allennlp-models. Then copy-and-paste the code from the description above. When I do this, I see reasonable numbers. I also made a test for this here: <denchmark-link:https://github.com/allenai/allennlp-models/pull/163>allenai/allennlp-models#163</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "b7cec515448f46daec9a76551d0a3342ef51b27b", "commit_author": "Matt Gardner", "commitT": "2020-11-02 14:21:23-08:00", "commit_complexity": {"commit_NLOC": "0.875", "commit_CCN": "0.9464285714285714", "commit_Nprams": "0.35714285714285715"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "8,11,12,13,14,15,16,17,18,41", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "allennlp\\interpret\\saliency_interpreters\\integrated_gradient.py", "file_new_name": "allennlp\\interpret\\saliency_interpreters\\integrated_gradient.py", "file_complexity": {"file_NLOC": "70", "file_CCN": "16", "file_NToken": "539"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "79,80,87,89,92,93,108,109", "deleted_lines": "78,81", "method_info": {"method_name": "_integrate_gradients", "method_params": "self,Instance", "method_startline": "72", "method_endline": "116", "method_complexity": {"method_NLOC": "28", "method_CCN": "7", "method_NToken": "212", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "42,54,59,60,61", "deleted_lines": "41,53,58,60,61", "method_info": {"method_name": "_register_forward_hook", "method_params": "self,int,List", "method_startline": "41", "method_endline": "61", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "38", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "54", "deleted_lines": "53", "method_info": {"method_name": "_register_forward_hook.forward_hook", "method_params": "module,inputs,output", "method_startline": "50", "method_endline": "56", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "43", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "42,54,59,60,61,62,63,64,65,67,68,69,70", "deleted_lines": "53,58,60,61,70", "method_info": {"method_name": "_register_hooks", "method_params": "self,int,List,List", "method_startline": "42", "method_endline": "70", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "74", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "59,60,61,62", "deleted_lines": "60,61", "method_info": {"method_name": "_register_hooks.get_token_offsets", "method_params": "module,inputs,outputs", "method_startline": "59", "method_endline": "62", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "29", "method_nesting_level": "2"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "54", "deleted_lines": "53", "method_info": {"method_name": "_register_hooks.forward_hook", "method_params": "module,inputs,output", "method_startline": "51", "method_endline": "57", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "39", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "allennlp\\interpret\\saliency_interpreters\\saliency_interpreter.py", "file_new_name": "allennlp\\interpret\\saliency_interpreters\\saliency_interpreter.py", "file_complexity": {"file_NLOC": "49", "file_CCN": "3", "file_NToken": "229"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "41,42", "deleted_lines": null, "method_info": {"method_name": "_aggregate_token_embeddings", "method_params": "", "method_startline": "41", "method_endline": "42", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "19", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "allennlp\\interpret\\saliency_interpreters\\simple_gradient.py", "file_new_name": "allennlp\\interpret\\saliency_interpreters\\simple_gradient.py", "file_complexity": {"file_NLOC": "50", "file_CCN": "9", "file_NToken": "391"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "67,68,69,70", "deleted_lines": null, "method_info": {"method_name": "_register_hooks.get_token_offsets", "method_params": "module,inputs,outputs", "method_startline": "67", "method_endline": "70", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "29", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "27,28,29,30,32,34,35,39,40,41,49", "deleted_lines": "24,25,26,30,32,43,51", "method_info": {"method_name": "saliency_interpret_from_json", "method_params": "self,JsonDict", "method_startline": "18", "method_endline": "55", "method_complexity": {"method_NLOC": "25", "method_CCN": "5", "method_NToken": "214", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "57", "deleted_lines": "51,59,61,62,64", "method_info": {"method_name": "_register_forward_hook", "method_params": "self,List", "method_startline": "51", "method_endline": "64", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "34", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "65", "deleted_lines": "64", "method_info": {"method_name": "_register_hooks.forward_hook", "method_params": "module,inputs,output", "method_startline": "64", "method_endline": "65", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "28", "method_nesting_level": "2"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "57,65,67,68,69,70,72,73,74,75,76,77,78", "deleted_lines": "59,61,62,64", "method_info": {"method_name": "_register_hooks", "method_params": "self,List,List", "method_startline": "57", "method_endline": "78", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "70", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": null, "deleted_lines": "59", "method_info": {"method_name": "_register_forward_hook.forward_hook", "method_params": "module,inputs,output", "method_startline": "58", "method_endline": "59", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "32", "method_nesting_level": "2"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "allennlp\\nn\\util.py", "file_new_name": "allennlp\\nn\\util.py", "file_complexity": {"file_NLOC": "1534", "file_CCN": "153", "file_NToken": "7645"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1808,1809", "deleted_lines": null, "method_info": {"method_name": "get_token_offsets_from_text_field_inputs", "method_params": "", "method_startline": "1808", "method_endline": "1809", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759", "deleted_lines": null, "method_info": {"method_name": "find_text_field_embedder", "method_params": "Module", "method_startline": "1748", "method_endline": "1759", "method_complexity": {"method_NLOC": "11", "method_CCN": "3", "method_NToken": "52", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "1774,1782,1783,1784,1785,1786,1787", "deleted_lines": "1766,1767,1768,1769,1770,1771,1772,1773,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788", "method_info": {"method_name": "find_embedding_layer", "method_params": "Module", "method_startline": "1762", "method_endline": "1805", "method_complexity": {"method_NLOC": "33", "method_CCN": "11", "method_NToken": "188", "method_nesting_level": "0"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "allennlp\\predictors\\predictor.py", "file_new_name": "allennlp\\predictors\\predictor.py", "file_complexity": {"file_NLOC": "288", "file_CCN": "33", "file_NToken": "1317"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "178,179,180,181", "deleted_lines": null, "method_info": {"method_name": "_register_embedding_gradient_hooks.get_token_offsets", "method_params": "module,inputs,outputs", "method_startline": "178", "method_endline": "181", "method_complexity": {"method_NLOC": "4", "method_CCN": "2", "method_NToken": "31", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "37", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,Model,DatasetReader,bool", "method_startline": "31", "method_endline": "37", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "71", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176", "deleted_lines": null, "method_info": {"method_name": "_register_embedding_gradient_hooks.hook_layers", "method_params": "module,grad_in,grad_out", "method_startline": "153", "method_endline": "176", "method_complexity": {"method_NLOC": "12", "method_CCN": "2", "method_NToken": "110", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "140,141,146,147,148,149,150,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,183,184,185,187,188", "deleted_lines": "145,147,149,150", "method_info": {"method_name": "_register_embedding_gradient_hooks", "method_params": "self,embedding_gradients", "method_startline": "138", "method_endline": "188", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "60", "method_nesting_level": "1"}}}}}}}}