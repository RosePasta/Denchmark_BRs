{"BR": {"BR_id": "4612", "BR_author": "dwadden", "BRopenT": "2020-08-28T00:59:28Z", "BRcloseT": "2020-09-01T15:27:53Z", "BR_text": {"BRsummary": "PretrainedTransformerMismatchedIndexer fails silently when given empty strings as input.", "BRdescription": "\n <denchmark-h:h2>Checklist</denchmark-h>\n \n \n  I have verified that the issue exists against the master branch of AllenNLP.\n  I have read the relevant section in the contribution guide on reporting bugs.\n  I have checked the issues list for similar or identical bug reports.\n  I have checked the pull requests list for existing proposed fixes.\n  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.\n  I have included in the \"Description\" section below a traceback from any exceptions related to this bug.\n  I have included in the \"Related issues or possible duplicates\" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).\n  I have included in the \"Environment\" section below the name of the operating system and Python version that I was using when I discovered this bug.\n  I have included in the \"Environment\" section below the output of pip freeze.\n  I have included in the \"Steps to reproduce\" section below a minimally reproducible example.\n \n <denchmark-h:h2>Description</denchmark-h>\n \n When the PretrainedTransformerMismatchedEmbedder is used to embed an empty string '', running backward on the output of the embedder produces nan gradients in the transformer parameters, but it's tough to track down where they came from. It's easier to see with an example; see \"Steps to Reproduce\".\n Suggested fix: The PretrainedTransformerMismatchedEmbedder should throw an error or give a warning when given an empty string as input. I'm happy to implement this if I can get some guidance on the appropriate files to change.\n The output of pip freeze is long and (I think) uninformative, so I've moved it to the bottom of this bug report.\n <denchmark-h:h2>Related issues or possible duplicates</denchmark-h>\n \n \n None\n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n Runnable example below. The first example will not produce nan gradients, but the second one will.\n <denchmark-code>import torch\n \n from allennlp import data\n from allennlp.data import fields\n from allennlp import modules\n \n \n def check_nan_grads(words):\n     \"Encode a list of words, take a gradient, and check for NaN's.\"\n     print(f\"Checking {words}.\")\n     # Create indexer and embedder.\n     tok_indexers = {\"bert\": data.token_indexers.PretrainedTransformerMismatchedIndexer(\n         \"bert-base-cased\")}\n     token_embedder = modules.token_embedders.PretrainedTransformerMismatchedEmbedder(\n         \"bert-base-cased\")\n     embedder = modules.text_field_embedders.BasicTextFieldEmbedder({\"bert\": token_embedder})\n \n     # Convert words to tensor dict.\n     vocab = data.Vocabulary()\n     text_field = fields.TextField(\n         [data.Token(word) for word in words], tok_indexers)\n     text_field.index(vocab)\n     token_tensor = text_field.as_tensor(text_field.get_padding_lengths())\n     tensor_dict = text_field.batch_tensors([token_tensor])\n \n     # Run forward pass. We need a scalar to take the gradient of, so just take the mean of the\n     # embeddings.\n     output = embedder(tensor_dict)\n     loss = output.mean()\n     loss.backward()\n \n     # Check whether this produces an NaN in the model parameters.\n     for name, param in embedder.named_parameters():\n         grad = param.grad\n         if grad is not None and torch.any(torch.isnan(param.grad)):\n             print(\"Found NaN grad.\")\n             print(\"Offending tensor_dict:\")\n             print(tensor_dict)\n             print()\n             return\n \n     print(\"No NaN's.\")\n     print()\n \n \n ####################\n \n # This works fine.\n example_safe = [\"An\", \"example\"]\n check_nan_grads(example_safe)\n \n # This produces NaN grads because of the empty string.\n example_bad_empty = [\"An\", \"\", \"example\"]\n check_nan_grads(example_bad_empty)\n \n # This produces NaN grads because there's a weird character the indexer doesn't know about.\n weird_character = \"\\uf732\\uf730\\uf730\\uf733\"\n print(f\"Weird character: {weird_character}.\")\n example_bad_unicode = [\"A\", weird_character, \"example\"]\n check_nan_grads(example_bad_unicode)\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n OS: Linux\n Python version: 3.7.6\n \n \n ```\n -e git+https://github.com/allenai/allennlp.git@<denchmark-link:https://github.com/allenai/allennlp/commit/73220d71cd5990f38747e50e64674a5166347e52>73220d7</denchmark-link>\n #egg=allennlp\n -e git+https://github.com/allenai/allennlp-models.git@a730fed9424bcbe21186fc7866b195ea9ac7ecc5#egg=allennlp_models\n attrs==19.3.0\n autopep8 @ file:///tmp/build/80754af9/autopep8_1592412889138/work\n backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n beautifulsoup4==4.8.1\n blis==0.4.1\n boto3==1.14.20\n botocore==1.17.20\n catalogue==1.0.0\n certifi==2020.6.20\n chardet==3.0.4\n click==7.1.2\n conllu==3.0\n cymem==2.0.3\n decorator==4.4.2\n docutils==0.15.2\n filelock==3.0.12\n flake8==3.8.3\n flaky==3.7.0\n future==0.18.2\n h5py==2.10.0\n idna==2.10\n importlib-metadata @ file:///tmp/build/80754af9/importlib-metadata_1593446408836/work\n ipdb==0.11\n ipython==7.9.0\n ipython-genutils==0.2.0\n javapackages==4.3.2\n jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1592619900914/work\n jmespath==0.10.0\n joblib==0.16.0\n jsonnet @ file:///home/conda/feedstock_root/build_artifacts/jsonnet_1590349750875/work\n jsonpickle==1.4.1\n lxml==4.5.2\n mccabe==0.6.1\n more-itertools==8.4.0\n murmurhash==1.0.2\n mypy @ file:///tmp/build/80754af9/mypy_1593442617121/work\n mypy-extensions==0.4.3\n nltk==3.5\n numpy==1.19.0\n overrides==3.1.0\n packaging==20.4\n pandas==0.25.2\n parso==0.7.0\n pexpect==4.8.0\n pickleshare==0.7.5\n plac==1.1.3\n pluggy==0.13.1\n preshed==3.0.2\n prompt-toolkit==2.0.10\n protobuf==3.12.2\n psutil==5.7.0\n ptyprocess==0.6.0\n py==1.9.0\n py-rouge==1.1\n pycodestyle==2.6.0\n pyflakes==2.2.0\n Pygments==2.6.1\n pyparsing==2.4.7\n pytest==5.4.3\n python-dateutil==2.8.1\n python-Levenshtein==0.12.0\n pytz==2020.1\n PyXB==1.2.4\n regex==2020.6.8\n requests==2.24.0\n responses==0.10.15\n rope==0.17.0\n s3transfer==0.3.3\n sacremoses==0.0.43\n scikit-learn==0.23.1\n scipy==1.5.1\n semantic-version==2.8.5\n sentencepiece==0.1.91\n six @ file:///home/conda/feedstock_root/build_artifacts/six_1590081179328/work\n soupsieve==2.0.1\n spacy==2.2.4\n srsly==1.0.2\n tensorboardX==2.1\n thinc==7.4.0\n threadpoolctl==2.1.0\n tokenizers==0.8.1rc1\n toml @ file:///tmp/build/80754af9/toml_1592853716807/work\n torch==1.6.0\n tqdm==4.47.0\n traitlets==4.3.3\n transformers==3.0.2\n typed-ast==1.4.1\n typing-extensions @ file:///tmp/build/80754af9/typing_extensions_1592847887441/work\n urllib3==1.25.9\n wasabi==0.7.0\n wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1592931742287/work\n word2number==1.1\n zipp==3.1.0\n ```\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dwadden", "commentT": "2020-08-28T02:08:44Z", "comment_text": "\n \t\tThanks for the thorough bug report!  Just FYI, I fixed the pip freeze output to be what was intended; you can see that it's now in a drop down, so it's not so annoying.\n Can you also include what print(tensor_dict) looks like?  I'm not sure what will happen with that empty string.  Is it just all padding, and that's what's causing nans?  I'm guessing so.\n I agree that throwing an error somewhere is the right fix.  I'm not certain where that error should be, but right now I'm thinking probably in <denchmark-link:https://github.com/allenai/allennlp/blob/e840a589afc4bfdac0165a8650145259a7603807/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py#L376-L389>the tokenizer</denchmark-link>\n .  Seeing the output of  might make me change my mind, though.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dwadden", "commentT": "2020-08-28T22:19:21Z", "comment_text": "\n \t\tAh, thanks for fixing pip freeze.\n It turns out that, in addition to empty strings, weird unicode characters can also mess things up; presumably they're not recognized by the token indexer. I added an example in the code snippet above to show what happens.\n I've included a tensor_dict printout below. I think it might be the -1 entries in offsets that's messing things up. Let me know what you think is the best spot to throw an error.\n <denchmark-code>Checking ['An', '', 'example'].\n Found NaN grad.\n Offending tensor_dict:\n {'bert': {'mask': tensor([[True, True, True]]),\n           'offsets': tensor([[[ 1,  1],\n                               [-1, -1],\n                               [ 2,  2]]]),\n           'token_ids': tensor([[ 101, 1760, 1859,  102]]),\n           'type_ids': tensor([[0, 0, 0, 0]]),\n           'wordpiece_mask': tensor([[True, True, True, True]])}}\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dwadden", "commentT": "2020-08-28T23:44:56Z", "comment_text": "\n \t\tOk, figured it out.  We need a torch.clamp_min(..., 1) on this line: \n \n \n allennlp/allennlp/modules/token_embedders/pretrained_transformer_mismatched_embedder.py\n \n \n          Line 108\n       in\n       e840a58\n \n \n \n \n \n \n  orig_embeddings = span_embeddings_sum / span_embeddings_len \n \n \n \n \n \n That resolves the issue.  Can you open a PR for this that includes a simple test based on the minimal example you gave above?  That would be awesome.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dwadden", "commentT": "2020-08-30T04:10:24Z", "comment_text": "\n \t\tI changed that line to orig_embeddings = torch.clamp_min(span_embeddings_sum / span_embeddings_len, 1), but I'm still getting nan gradients (I wrote a unit test for this). Is there something else that needs to be changed?\n I've got a PR for this, but I didn't want to submit since the test suite wouldn't pass if the PR were accepted as is. Let me know if I should submit anyhow.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "dwadden", "commentT": "2020-08-30T06:03:53Z", "comment_text": "\n \t\tSorry I wasn't specific enough; it's the division by zero that's the problem, so you need to clamp the lengths to a min of 1.  So, orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1).\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "dwadden", "commentT": "2020-08-30T23:45:04Z", "comment_text": "\n \t\tPR submitted: <denchmark-link:https://github.com/allenai/allennlp/pull/4615>#4615</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "711afaa7720ebaba4a3739c1753c02568039993d", "commit_author": "David Wadden", "commitT": "2020-09-01 08:27:52-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "14,15,22,69", "deleted_lines": "66"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder.py", "file_new_name": "allennlp\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder.py", "file_complexity": {"file_NLOC": "96", "file_CCN": "3", "file_NToken": "282"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "108", "deleted_lines": "108"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder_test.py", "file_new_name": "tests\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder_test.py", "file_complexity": {"file_NLOC": "141", "file_CCN": "14", "file_NToken": "1289"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179", "deleted_lines": null, "method_info": {"method_name": "test_exotic_tokens_no_nan_grads", "method_params": "self", "method_startline": "148", "method_endline": "179", "method_complexity": {"method_NLOC": "23", "method_CCN": "5", "method_NToken": "213", "method_nesting_level": "1"}}}}}}}}