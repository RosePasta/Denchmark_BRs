{"BR": {"BR_id": "4428", "BR_author": "AutoTemp", "BRopenT": "2020-07-01T08:06:24Z", "BRcloseT": "2020-09-08T20:38:20Z", "BR_text": {"BRsummary": "ModuleNotFoundError when num_workers&gt;0 & DistributeDataParallel & lazy read", "BRdescription": "\n <denchmark-h:h2>Checklist</denchmark-h>\n \n \n  I have verified that the issue exists against the master branch of AllenNLP.\n  I have read the relevant section in the contribution guide on reporting bugs.\n  I have checked the issues list for similar or identical bug reports.\n  I have checked the pull requests list for existing proposed fixes.\n  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.\n  I have included in the \"Description\" section below a traceback from any exceptions related to this bug.\n  I have included in the \"Related issues or possible duplicates\" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).\n  I have included in the \"Environment\" section below the name of the operating system and Python version that I was using when I discovered this bug.\n  I have included in the \"Environment\" section below the output of pip freeze.\n  I have included in the \"Steps to reproduce\" section below a minimally reproducible example.\n \n <denchmark-h:h2>Description</denchmark-h>\n \n When num_workers>0, DistributeDataParallel and lazy read, allennlp report ModuleNotFoundError.\n ('my_text_classifier' is include-package)\n \n Python traceback:\n \n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/xxx/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/xxx/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_text_classifier'\n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/xxx/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/xxx/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_text_classifier'\n \n \n \n <denchmark-h:h2>Related issues or possible duplicates</denchmark-h>\n \n \n #3924\n (ModuleNotFoundError also mentioned in the issue but the solution does not work)\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n OS: Linux\n Python version: 3.7.0\n \n Output of pip freeze:\n \n rsa==3.4.2\n s3transfer==0.3.3\n sacremoses==0.0.43\n scikit-learn==0.20.0\n scipy==1.5.0\n Send2Trash==1.5.0\n sentencepiece==0.1.91\n six==1.15.0\n spacy==2.1.9\n sqlparse==0.3.1\n srsly==1.0.2\n tensorboardX==2.0\n terminado==0.8.3\n testpath==0.4.4\n thinc==7.0.8\n tokenizers==0.7.0\n torch==1.5.1\n tqdm==4.47.0\n transformers==2.11.0\n Unidecode==1.1.1\n urllib3==1.25.9\n wasabi==0.6.0\n wcwidth==0.2.5\n webencodings==0.5.1\n widgetsnbextension==3.5.1\n word2number==1.1\n zipp==3.1.0\n \n \n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n \n use the example in allennlp-guide-examples/quick_start\n In 'my_text_classifier.jsonnet', set lazy=true in reader, add distributed info, and set num_workers=2 in data_loader\n (details are as follow)\n run \"allennlp train my_text_classifier.jsonnet -s demo --include-package my_text_classifier\"\n \n when remove --include-package and turn into .allennlp_plugins (with one line: my_text_classifier)\n also same error\n remove any of num_workers>0 & DistributeDataParallel & lazy read, the project is ok.\n \n Example source:\n \n allennlp train my_text_classifier.jsonnet -s demo --include-package my_text_classifier\n (also error when use .allennlp_plugins)\n \n my_text_classifier.jsonnet:\n \n {\n     \"dataset_reader\" : {\n         \"type\": \"classification-tsv\",\n         \"token_indexers\": {\n             \"tokens\": {\n                 \"type\": \"single_id\"\n             }\n         },\n         \"lazy\": true\n     },\n     \"train_data_path\": \"data/movie_review/train.tsv\",\n     \"validation_data_path\": \"data/movie_review/dev.tsv\",\n     \"model\": {\n         \"type\": \"simple_classifier\",\n         \"embedder\": {\n             \"token_embedders\": {\n                 \"tokens\": {\n                     \"type\": \"embedding\",\n                     \"embedding_dim\": 10\n                 }\n             }\n         },\n         \"encoder\": {\n             \"type\": \"bag_of_embeddings\",\n             \"embedding_dim\": 10\n         }\n     },\n     \"data_loader\": {\n         \"batch_size\": 8,\n         \"num_workers\": 2\n \n     },\n     \"trainer\": {\n         \"optimizer\": \"adam\",\n         \"num_epochs\": 5\n     },\n     \"distributed\":{\n         \"cuda_devices\": [0,2],\n         \"num_nodes\": 1\n     }\n }\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "AutoTemp", "commentT": "2020-07-01T15:21:24Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/AutoTemp>@AutoTemp</denchmark-link>\n , thank you for the detailed bug report :)\n Could you tell me if that exception was raised before or after the DistributedDataParallel workers were spawned? Or if you could just share your training log output, I should be able to tell.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "AutoTemp", "commentT": "2020-07-02T01:45:34Z", "comment_text": "\n \t\t\n Hi @AutoTemp, thank you for the detailed bug report :)\n Could you tell me if that exception was raised before or after the DistributedDataParallel workers were spawned? Or if you could just share your training log output, I should be able to tell.\n \n For the above example, the outputs are as follow:\n \n Python traceback:\n \n 2020-07-01 08:00:58,070 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n 2020-07-01 08:00:59,169 - INFO - root - Switching to distributed training mode since multiple GPUs are configuredMaster is at: 1\n 27.0.0.1:29500 | Rank of this node: 0 | Number of workers in this node: 2 | Number of nodes: 1 | World size: 2\n 2020-07-01 08:00:59,169 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n 2020-07-01 08:00:59,169 - INFO - allennlp.common.params - dataset_reader.type = classification-tsv\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.lazy = True\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.tokenizer = None\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id\n 2020-07-01 08:00:59,170 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = False\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.feature_name = text\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.default_value = THIS IS A REALLY\n UNLIKELY VALUE THAT HAS TO BE A STRING\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - dataset_reader.max_tokens = None\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - train_data_path = data/movie_review/train.tsv\n 2020-07-01 08:00:59,171 - INFO - allennlp.training.util - Reading training data from data/movie_review/train.tsv\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - validation_dataset_reader = None\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - validation_data_path = data/movie_review/dev.tsv\n 2020-07-01 08:00:59,171 - INFO - allennlp.training.util - Reading validation data from data/movie_review/dev.tsv\n 2020-07-01 08:00:59,171 - INFO - allennlp.common.params - vocabulary.type = from_instances\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.min_count = None\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.pretrained_files = None\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.padding_token = @@PADDING@@\n 2020-07-01 08:00:59,172 - INFO - allennlp.common.params - vocabulary.oov_token = @@UNKNOWN@@\n 2020-07-01 08:00:59,172 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n 1600it [00:01, 834.85it/s]\n 200it [00:00, 793.34it/s]]\n 1800it [00:02, 829.82it/s]\n 2020-07-01 08:01:01,406 - INFO - allennlp.training.util - writing the vocabulary to demo/vocabulary.\n 2020-07-01 08:01:01,407 - INFO - filelock - Lock 140568736572080 acquired on demo/vocabulary/.lock\n 2020-07-01 08:01:01,499 - INFO - filelock - Lock 140568736572080 released on demo/vocabulary/.lock\n 2020-07-01 08:01:01,499 - INFO - allennlp.training.util - done creating vocab\n 2020-07-01 08:01:02,293 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n 2020-07-01 08:01:02,300 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n 2020-07-01 08:01:02,996 - INFO - allennlp.common.params - Rank 0 | random_seed = 13370\n 2020-07-01 08:01:02,996 - INFO - allennlp.common.params - Rank 0 | numpy_seed = 1337\n 2020-07-01 08:01:02,996 - INFO - allennlp.common.params - Rank 0 | pytorch_seed = 133\n 2020-07-01 08:01:03,108 - INFO - allennlp.common.checks - Rank 0 | Pytorch version: 1.5.1\n 2020-07-01 08:01:03,150 - INFO - root - Rank 0 | Process group of world size 2 initialized for distributed training in worker 0\n 2020-07-01 08:01:03,150 - INFO - allennlp.common.params - Rank 0 | type = default\n 2020-07-01 08:01:03,151 - INFO - allennlp.common.params - Rank 0 | dataset_reader.type = classification-tsv\n 2020-07-01 08:01:03,151 - INFO - allennlp.common.params - Rank 0 | dataset_reader.lazy = True\n 2020-07-01 08:01:03,151 - INFO - allennlp.common.params - Rank 0 | dataset_reader.cache_directory = None\n 2020-07-01 08:01:03,151 - INFO - allennlp.common.params - Rank 0 | dataset_reader.max_instances = None\n 2020-07-01 08:01:03,151 - INFO - allennlp.common.params - Rank 0 | dataset_reader.manual_distributed_sharding = False\n 2020-07-01 08:01:03,151 - INFO - allennlp.common.params - Rank 0 | dataset_reader.manual_multi_process_sharding = False\n 2020-07-01 08:01:03,151 - INFO - allennlp.common.params - Rank 0 | dataset_reader.tokenizer = None\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.type = single_id\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.namespace = tokens\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.lowercase_tokens = False\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.start_tokens = None\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.end_tokens = None\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.feature_name = text\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.default_value = THIS IS\n A REALLY UNLIKELY VALUE THAT HAS TO BE A STRING\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.token_indexers.tokens.token_min_padding_length\n  = 0\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | dataset_reader.max_tokens = None\n 2020-07-01 08:01:03,152 - INFO - allennlp.common.params - Rank 0 | train_data_path = data/movie_review/train.tsv\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | datasets_for_vocab_creation = None\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | validation_dataset_reader = None\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | validation_data_path = data/movie_review/dev.tsv\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | validation_data_loader = None\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | test_data_path = None\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | evaluate_on_test = False\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | batch_weight_key =\n 2020-07-01 08:01:03,153 - INFO - allennlp.training.util - Rank 0 | Reading training data from data/movie_review/train.tsv\n 2020-07-01 08:01:03,153 - INFO - allennlp.training.util - Rank 0 | Reading validation data from data/movie_review/dev.tsv\n 2020-07-01 08:01:03,153 - INFO - allennlp.common.params - Rank 0 | vocabulary.type = from_files\n 2020-07-01 08:01:03,154 - INFO - allennlp.common.params - Rank 0 | vocabulary.directory = demo/vocabulary\n 2020-07-01 08:01:03,154 - INFO - allennlp.common.params - Rank 0 | vocabulary.padding_token = @@PADDING@@\n 2020-07-01 08:01:03,154 - INFO - allennlp.common.params - Rank 0 | vocabulary.oov_token = @@UNKNOWN@@\n 2020-07-01 08:01:03,154 - INFO - allennlp.data.vocabulary - Rank 0 | Loading token dictionary from demo/vocabulary.\n 2020-07-01 08:01:03,354 - INFO - filelock - Rank 0 | Lock 140121523643560 acquired on demo/vocabulary/.lock\n 2020-07-01 08:01:03,397 - INFO - filelock - Rank 0 | Lock 140121523643560 released on demo/vocabulary/.lock\n 2020-07-01 08:01:03,398 - INFO - allennlp.common.params - Rank 0 | model.type = simple_classifier\n 2020-07-01 08:01:03,398 - INFO - allennlp.common.params - Rank 0 | model.embedder.type = basic\n 2020-07-01 08:01:03,398 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.type = embedding\n 2020-07-01 08:01:03,399 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.embedding_dim = 10\n 2020-07-01 08:01:03,399 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.num_embeddings = None\n 2020-07-01 08:01:03,399 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.projection_dim = None\n 2020-07-01 08:01:03,399 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.weight = None\n 2020-07-01 08:01:03,400 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.padding_index = None\n 2020-07-01 08:01:03,400 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.trainable = True\n 2020-07-01 08:01:03,400 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.max_norm = None\n 2020-07-01 08:01:03,400 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.norm_type = 2.0\n 2020-07-01 08:01:03,401 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.scale_grad_by_freq = Fa\n lse\n 2020-07-01 08:01:03,401 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.sparse = False\n 2020-07-01 08:01:03,401 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.vocab_namespace = token\n s\n 2020-07-01 08:01:03,401 - INFO - allennlp.common.params - Rank 0 | model.embedder.token_embedders.tokens.pretrained_file = None\n 2020-07-01 08:01:03,405 - INFO - allennlp.common.params - Rank 0 | model.encoder.type = bag_of_embeddings\n 2020-07-01 08:01:03,406 - INFO - allennlp.common.params - Rank 0 | model.encoder.embedding_dim = 10\n 2020-07-01 08:01:03,406 - INFO - allennlp.common.params - Rank 0 | model.encoder.averaged = False\n 2020-07-01 08:01:03,407 - WARNING - root - Rank 0 | vocabulary serialization directory demo/vocabulary is not empty\n 2020-07-01 08:01:03,407 - INFO - filelock - Rank 0 | Lock 140121523644288 acquired on demo/vocabulary/.lock\n 2020-07-01 08:01:03,497 - INFO - filelock - Rank 0 | Lock 140121523644288 released on demo/vocabulary/.lock\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.type = default\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.batch_size = 8\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.shuffle = False\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.sampler = None\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.batch_sampler = None\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.num_workers = 2\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.pin_memory = False\n 2020-07-01 08:01:03,497 - INFO - allennlp.common.params - Rank 0 | data_loader.drop_last = False\n 2020-07-01 08:01:03,498 - INFO - allennlp.common.params - Rank 0 | data_loader.timeout = 0\n 2020-07-01 08:01:03,498 - INFO - allennlp.common.params - Rank 0 | data_loader.worker_init_fn = None\n 2020-07-01 08:01:03,498 - INFO - allennlp.common.params - Rank 0 | data_loader.multiprocessing_context = None\n 2020-07-01 08:01:03,498 - INFO - allennlp.common.params - Rank 0 | data_loader.batches_per_epoch = None\n /home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/data/dataloader.py:74: UserWarning: Using multi-pro\n cess data loading with a lazy dataset could lead to deadlocks with certain tokenizers. See:\n   https://github.com/allenai/allennlp/issues/4330\n \n   UserWarning,\n 2020-07-01 08:01:04,068 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n 2020-07-01 08:01:04,258 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_text_classifier'\n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_text_classifier'\n ^CTraceback (most recent call last):\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/bin/allennlp\", line 8, in <module>\n     sys.exit(run())\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/__main__.py\", line 19, in run\n     main(prog=\"allennlp\")\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/__init__.py\", line 92, in main\n     args.func(args)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/train.py\", line 112, in train_mode\n l_from_args\n     dry_run=args.dry_run,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/train.py\", line 171, in train_mode\n l_from_file\n     dry_run=dry_run,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/train.py\", line 295, in train_mode\n l\n     nprocs=num_procs,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_\n processes\n     while not context.join():\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 78, in join\n     timeout=timeout,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n     ready = selector.select(timeout)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/selectors.py\", line 415, in select\n     fd_event_list = self._selector.poll(timeout)\n KeyboardInterrupt\n ^CError in atexit._run_exitfuncs:\n Traceback (most recent call last):\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n \n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "AutoTemp", "commentT": "2020-07-02T02:00:01Z", "comment_text": "\n \t\tFor my own experiments (not the simple example of reproduction above), the outputs are as follow:\n \n Python traceback:\n \n 2020-07-02 01:48:37,802 - INFO - allennlp.common.params - Rank 0 | data_loader.type = default\n 2020-07-02 01:48:37,803 - INFO - allennlp.common.params - Rank 0 | data_loader.batch_size = 64\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.shuffle = False\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.sampler = None\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.batch_sampler = None\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.num_workers = 1\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.pin_memory = True\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.drop_last = False\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.timeout = 0\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.worker_init_fn = None\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.multiprocessing_context = None\n 2020-07-02 01:48:37,804 - INFO - allennlp.common.params - Rank 0 | data_loader.batches_per_epoch = None\n 2020-07-02 01:48:37,841 - INFO - my_package.train_model - Rank 0 | Construct trainer\n 2020-07-02 01:48:37,842 - INFO - allennlp.common.params - Rank 0 | trainer.type = gradient_descent\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.patience = None\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.validation_metric = -loss\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.num_epochs = 20\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.cuda_device = 0\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.grad_norm = None\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.grad_clipping = None\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.distributed = True\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.world_size = 2\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.num_gradient_accumulation_steps = 1\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.opt_level = None\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.no_grad = None\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.grad_clipping = None\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.distributed = True\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.world_size = 2\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.num_gradient_accumulation_steps = 1\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.opt_level = None\n 2020-07-02 01:48:37,843 - INFO - allennlp.common.params - Rank 0 | trainer.no_grad = None\n 2020-07-02 01:48:37,844 - INFO - allennlp.common.params - Rank 0 | trainer.learning_rate_scheduler = None\n 2020-07-02 01:48:37,844 - INFO - allennlp.common.params - Rank 0 | trainer.momentum_scheduler = None\n 2020-07-02 01:48:37,844 - INFO - allennlp.common.params - Rank 0 | trainer.tensorboard_writer = None\n 2020-07-02 01:48:37,844 - INFO - allennlp.common.params - Rank 0 | trainer.moving_average = None\n 2020-07-02 01:48:37,844 - INFO - allennlp.common.params - Rank 0 | trainer.checkpointer = None\n 2020-07-02 01:48:37,844 - INFO - allennlp.common.params - Rank 0 | trainer.batch_callbacks = None\n 2020-07-02 01:48:37,844 - INFO - allennlp.common.params - Rank 0 | trainer.epoch_callbacks = None\n 2020-07-02 01:48:38,686 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n 2020-07-02 01:48:38,703 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n 2020-07-02 01:48:38,729 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n 2020-07-02 01:48:38,730 - INFO - transformers.file_utils - PyTorch version 1.5.1 available.\n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_package'\n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_package'\n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_package'\n Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n     exitcode = _main(fd)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n     self = reduction.pickle.load(from_parent)\n ModuleNotFoundError: No module named 'my_package'\n Traceback (most recent call last):\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/bin/allennlp\", line 8, in <module>\n     sys.exit(run())\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/__main__.py\", line 19, in run\n     main(prog=\"allennlp\")\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/__init__.py\", line 92, in main\n     args.func(args)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/train.py\", line 112, in train_mode\n l_from_args\n     dry_run=args.dry_run,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/train.py\", line 171, in train_mode\n l_from_file\n     dry_run=dry_run,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/train.py\", line 295, in train_mode\n l\n     nprocs=num_procs,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 200, in spawn\n     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 158, in start_\n processes\n     while not context.join():\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 119, in join\n     raise Exception(msg)\n Exception:\n \n -- Process 1 terminated with the following error:\n Traceback (most recent call last):\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n     fn(i, *args)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/commands/train.py\", line 418, in _train_wor\n ker\n     params=params, serialization_dir=serialization_dir, local_rank=process_rank,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/common/from_params.py\", line 580, in from_p\n arams\n     **extras,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/common/from_params.py\", line 611, in from_p\n arams\n     return constructor_to_call(**kwargs)  # type: ignore\n   File \"/home/sunxin/project/gector_allennlp/my_package/train_model.py\", line 66, in from_partial_objects\n     model=model_, data_loader=data_loader_, validation_data_loader=validation_data_loader_,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/common/lazy.py\", line 46, in construct\n     return self._constructor(**kwargs)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/common/from_params.py\", line 446, in constr\n uctor\n     return value_cls.from_params(params=deepcopy(popped_params), **constructor_extras)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/common/from_params.py\", line 580, in from_p\n arams\n     **extras,\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/common/from_params.py\", line 611, in from_p\n arams\n     return constructor_to_call(**kwargs)  # type: ignore\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/allennlp/training/trainer.py\", line 1056, in from_pa\n rtial_objects\n model = model.cuda(cuda_device)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 307, in cuda\n     return self._apply(lambda t: t.cuda(device))\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 203, in _apply\n     module._apply(fn)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 203, in _apply\n     module._apply(fn)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 203, in _apply\n     module._apply(fn)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 225, in _apply\n     param_applied = fn(param)\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 307, in <lambda>\n     return self._apply(lambda t: t.cuda(device))\n   File \"/home/sunxin/anaconda3/envs/allennlp1.0/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66\n , in handler\n     _error_if_any_worker_fails()\n RuntimeError: DataLoader worker (pid 56579) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerun\n ning with num_workers=0 may give better error trace.\n \n \n \n \n when revising num_workers=0, there are no errors.\n \n I suspect the problem is in this code(train.py - GradientDescentTrainer - from_partial_objects):\n <denchmark-code>        if cuda_device >= 0:\n             # Moving model to GPU here so that the optimizer state gets constructed on\n             # the right device.\n             model = model.cuda(cuda_device)\n </denchmark-code>\n \n since I wrap them with 'try except', it can catch an exception.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "AutoTemp", "commentT": "2020-07-06T22:34:55Z", "comment_text": "\n \t\tI'm still 100% sure what is causing the issue, but I've narrowed it down and found a work-around.\n The work-around is to ensure your custom modules (like my_package or my_text_classifier in these examples) are in your Python path. There are two ways to do this:\n \n Append the parent directories of these modules to the PYTHONPATH environment variable. If you're running allennlp from the same directory as your custom modules, just do this: PYTHONPATH=./ allennlp train ....\n Install your custom modules into your Python environment. This means you need a setup.py file. Once you have that, you can just install each of your custom modules as \"editable\", with pip install -e ., or python setup.py develop.\n \n The underlying issue has something to do with the interaction between multiprocessing and pickle, since multiprocessing uses pickle to spawn and communicate across processes.\n In particular, when the DataLoader spawns loader workers, it pickle-dumps the DatasetReader implemented in my_text_classifier so that the spawned workers can each pickle-load the DatasetReader. And this is where it's failing: the workers try to load this DatasetReader object, but they can't find the my_text_classifier module.\n What's weird is that pickle should be able to find the my_text_classifier module since it's in the current working directory, and usually it can. In fact, like you said, it finds it just fine when you're not using distributed training. But, for some reason, within a data loader worker within a distributed training worker, pickle is failing to resolve the my_text_classifier module.\n For anyone interested in pursuing this further, here's a stand-alone reproducible example: <denchmark-link:https://github.com/epwalsh/allennlp-issue-4428/tree/master>https://github.com/epwalsh/allennlp-issue-4428/tree/master</denchmark-link>\n .\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "AutoTemp", "commentT": "2020-07-08T03:04:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/epwalsh>@epwalsh</denchmark-link>\n  Thanks for your detailed answer!!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "AutoTemp", "commentT": "2020-09-07T19:33:05Z", "comment_text": "\n \t\tThis issue is still valid and keeps happening. Any plans to fix it? I can confirm that PYTHONPATH=. fixes.\n \t\t"}}}, "commit": {"commit_id": "bf3206a28cd504d91ccd4a8fdbd07cbf549e2f2f", "commit_author": "Dirk Groeneveld", "commitT": "2020-09-08 13:38:19-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "allennlp\\common\\plugins.py", "file_new_name": "allennlp\\common\\plugins.py", "file_complexity": {"file_NLOC": "53", "file_CCN": "11", "file_NToken": "209"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "50,51,52,53,54,55", "deleted_lines": null, "method_info": {"method_name": "import_plugins", "method_params": "", "method_startline": "46", "method_endline": "68", "method_complexity": {"method_NLOC": "19", "method_CCN": "6", "method_NToken": "88", "method_nesting_level": "0"}}}}}}}}