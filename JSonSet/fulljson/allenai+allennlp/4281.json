{"BR": {"BR_id": "4281", "BR_author": "arthurdeschamps", "BRopenT": "2020-05-25T10:35:28Z", "BRcloseT": "2020-06-03T00:04:23Z", "BR_text": {"BRsummary": "\"TypeError: not a sequence\" on simple coreference resolution", "BRdescription": "\n Describe the bug\n I get a TypeError: not a sequence when trying to predict this simple string: \"Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university.\"\n Full stacktrace:\n <denchmark-code>Traceback (most recent call last):\n   File \"/home/arthur/question-generation/models/sg_dqg.py\", line 156, in <module>\n     preprocess(args.ds)\n   File \"/home/arthur/question-generation/models/sg_dqg.py\", line 124, in preprocess\n     coreferences = coreference_resolution(evidences_list)\n   File \"/home/arthur/question-generation/models/sg_dqg.py\", line 74, in coreference_resolution\n     document=\"Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university.\"\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp_models/coref/coref_predictor.py\", line 65, in predict\n     return self.predict_json({\"document\": document})\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/predictors/predictor.py\", line 48, in predict_json\n     return self.predict_instance(instance)\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/predictors/predictor.py\", line 171, in predict_instance\n     outputs = self._model.forward_on_instance(instance)\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/models/model.py\", line 142, in forward_on_instance\n     return self.forward_on_instances([instance])[0]\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/models/model.py\", line 167, in forward_on_instances\n     model_input = util.move_to_device(dataset.as_tensor_dict(), cuda_device)\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/batch.py\", line 139, in as_tensor_dict\n     for field, tensors in instance.as_tensor_dict(lengths_to_use).items():\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/instance.py\", line 99, in as_tensor_dict\n     tensors[field_name] = field.as_tensor(padding_lengths[field_name])\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/fields/text_field.py\", line 103, in as_tensor\n     self._indexed_tokens[indexer_name], indexer_lengths[indexer_name]\n   File \"/home/arthur/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/allennlp/data/token_indexers/pretrained_transformer_mismatched_indexer.py\", line 96, in as_padded_tensor_dict\n     offsets_tokens, offsets_padding_lengths, default_value=lambda: (0, 0)\n TypeError: not a sequence\n </denchmark-code>\n \n To Reproduce\n Run this simple piece of code :\n <denchmark-code>predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n predictor.predict(\n document=\"Besides its prominence in sports, Notre Dame is also a large, four-year, highly residential research University, and is consistently ranked among the top twenty universities in the United States  and as a major global university.\"\n )\n </denchmark-code>\n \n Expected behavior\n The function should return the proper resolved coreferences.\n System (please complete the following information):\n \n OS: Linux\n Python version: 3.7.7\n AllenNLP version: 1.0.0rc4\n PyTorch version: 1.5.0\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T17:38:47Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/arthurdeschamps>@arthurdeschamps</denchmark-link>\n , I did some debugging and a found that the root of the issue comes from there being some  values in the  list from your stacktrace, which causes  to fail when trying to turn this list into a tensor because it doesn't know how to handle the  values. These  values come from this line:\n <denchmark-link:https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py#L360>https://github.com/allenai/allennlp/blob/master/allennlp/data/tokenizers/pretrained_transformer_tokenizer.py#L360</denchmark-link>\n \n <denchmark-link:https://github.com/dirkgr>@dirkgr</denchmark-link>\n  any ideas how we should fix this?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T20:49:13Z", "comment_text": "\n \t\tThis is something that has to be handled in the model (or in the original tokenizer).\n Recall: _intra_word_tokenize takes an existing tokenization of a string, and cuts the tokens into word pieces suitable for a transformer. What should happen when the existing tokenization produces tokens that have zero word pieces? I can't answer that in general. It depends on your case.\n If your answer is \"That should never happen.\", then look at the cases where it happens anyways and find out why. Maybe the original tokenizer produces tokens that are nothing but spaces? Zero-length tokens? But sometimes there is a legitimate reason for this, and you need to handle it somehow downstream.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T20:58:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dirkgr>@dirkgr</denchmark-link>\n  in any case, I think we need to improve our error message here. Seems to me like  should raise an exception when a token results in zero word pieces?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T21:34:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/arthurdeschamps>@arthurdeschamps</denchmark-link>\n  is this case you have extra whitespace in your document:\n <denchmark-code>...he United States  and as...\n                    ^^\n </denchmark-code>\n \n Nevertheless, the error message should be improved here. See <denchmark-link:https://github.com/allenai/allennlp/pull/4291>#4291</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T21:52:43Z", "comment_text": "\n \t\t\n Seems to me like _intra_word_tokenize should raise an exception when a token results in zero word pieces?\n \n Sometimes this is a case you want to handle specifically. If we throw an exception, you could never deal with that case.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T22:15:55Z", "comment_text": "\n \t\t\n Sometimes this is a case you want to handle specifically. If we throw an exception, you could never deal with that case.\n \n My issue with not throwing an exception is that there are several places that rely on this function returning all non-Nonetype offsets. This issue brought to light one of these places.\n mypy should really have caught this before in our CI checks because this is a typing error on our part.\n That said, the other option would be to raise an exception downstream in all of the places that use these offsets when  is encountered. I didn't go that route with <denchmark-link:https://github.com/allenai/allennlp/pull/4291>#4291</denchmark-link>\n  because I couldn't find a single example where a  offset was actually handled.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T23:45:53Z", "comment_text": "\n \t\tWhat are all of those places that expect it?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T23:49:01Z", "comment_text": "\n \t\t\n What are all of those places that expect it?\n \n \n https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/pretrained_transformer_mismatched_indexer.py#L96\n https://github.com/allenai/allennlp-models/blob/master/allennlp_models/coref/util.py#L105\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T23:50:09Z", "comment_text": "\n \t\tCan we fix it by adding a fixed embedding to the mismatched embedder for these cases, which gets substituted when there is nothing else to add?\n I'm looking at the other case right now.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "arthurdeschamps", "commentT": "2020-05-26T23:53:52Z", "comment_text": "\n \t\tFor the coref case, it would be pretty messed up if the mention starts or ends in a token that has no word pieces. I'd be OK just letting it crash in that case, or just throwing an exception right there.\n We're losing a little bit of generality if we do this. The proper fix would be to scan forward from the start token until we find one that's not None, and scan backwards from the end token. But I think the case where that's necessary is quite rare, and possibly not worth the complexity.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "arthurdeschamps", "commentT": "2020-05-27T02:50:19Z", "comment_text": "\n \t\tMight be relevant: <denchmark-link:https://github.com/allenai/allennlp/issues/3779>#3779</denchmark-link>\n , esp. the part where it was re-opened. Specifically see <denchmark-link:https://github.com/allenai/allennlp/pull/3808/files#r381036253>https://github.com/allenai/allennlp/pull/3808/files#r381036253</denchmark-link>\n \n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "arthurdeschamps", "commentT": "2020-05-27T02:53:25Z", "comment_text": "\n \t\t\n \n \n allennlp/tests/data/tokenizers/pretrained_transformer_tokenizer_test.py\n \n \n          Line 225\n       in\n       8ff47d3\n \n \n \n \n \n \n  def test_intra_word_tokenize_whitespaces(self): \n \n \n \n \n  was supposed to test this behavior.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "arthurdeschamps", "commentT": "2020-05-29T01:14:47Z", "comment_text": "\n \t\tI proposed a fix at <denchmark-link:https://github.com/allenai/allennlp/pull/4301>#4301</denchmark-link>\n .\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "arthurdeschamps", "commentT": "2020-06-03T00:05:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/allenai/allennlp/pull/4301>#4301</denchmark-link>\n  fixes this issue, in the sense that you don't get an exception anymore. But instead it puts a zero vector into your embeddings. If the model isn't trained for that possibility, anything could happen. There is no guarantee it will give good performance.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "arthurdeschamps", "commentT": "2020-07-10T10:36:17Z", "comment_text": "\n \t\tI found that upgrading to huggingface transformers 3.0.0 facilitates this error\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "arthurdeschamps", "commentT": "2020-07-10T12:04:39Z", "comment_text": "\n \t\tWhat do you mean by \"facilitates\"?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "arthurdeschamps", "commentT": "2020-07-10T18:07:54Z", "comment_text": "\n \t\tSorry I should have given you more to go off - really big fan of you guys!\n I upgraded to transformers==3.0.1 without changing any code and got the same type error. When I reverted versions of transformers<3.0.0 the same code worked fine.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "arthurdeschamps", "commentT": "2020-07-13T09:52:23Z", "comment_text": "\n \t\tI just tried running the code from the issue description at the top on the latest master of allennlp and allennlp-models, and transformers==3.0.2. I had no problems. What are you running?\n \n really big fan of you guys!\n \n Thanks! We appreciate it :-)\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "arthurdeschamps", "commentT": "2020-07-13T09:53:09Z", "comment_text": "\n \t\tActually, if you could put repro steps into the description of a new issue, that would be great. Then we don't have to have a discussion on a closed issue, which might get lost easily.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "arthurdeschamps", "commentT": "2020-07-14T07:51:52Z", "comment_text": "\n \t\tThat's strange. I think transformers introduced a few tokenizer fixes between 3.0.1 and 3.0.2, but might also be something else. I can try repro this in the next week or two as have some hard deadlines approaching and have not recorded all the steps I took to fix this error as well as I could have...\n \t\t"}}}, "commit": {"commit_id": "fc47bf6ae5c0df6d473103d459b75fa7edbdd979", "commit_author": "Dirk Groeneveld", "commitT": "2020-06-02 17:04:22-07:00", "commit_complexity": {"commit_NLOC": "0.02857142857142857", "commit_CCN": "0.6571428571428571", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "allennlp\\data\\token_indexers\\pretrained_transformer_mismatched_indexer.py", "file_new_name": "allennlp\\data\\token_indexers\\pretrained_transformer_mismatched_indexer.py", "file_complexity": {"file_NLOC": "94", "file_CCN": "15", "file_NToken": "527"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "66,67,68,69,70", "deleted_lines": null, "method_info": {"method_name": "tokens_to_indices", "method_params": "self,Vocabulary", "method_startline": "62", "method_endline": "79", "method_complexity": {"method_NLOC": "12", "method_CCN": "6", "method_NToken": "133", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder.py", "file_new_name": "allennlp\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder.py", "file_complexity": {"file_NLOC": "75", "file_CCN": "3", "file_NToken": "238"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "89,90,91", "deleted_lines": null}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder_test.py", "file_new_name": "tests\\modules\\token_embedders\\pretrained_transformer_mismatched_embedder_test.py", "file_complexity": {"file_NLOC": "113", "file_CCN": "9", "file_NToken": "1036"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "101", "deleted_lines": "98", "method_info": {"method_name": "test_long_sequence_splitting_end_to_end", "method_params": "self", "method_startline": "56", "method_endline": "101", "method_complexity": {"method_NLOC": "37", "method_CCN": "3", "method_NToken": "343", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141", "deleted_lines": null, "method_info": {"method_name": "test_token_without_wordpieces", "method_params": "self", "method_startline": "102", "method_endline": "141", "method_complexity": {"method_NLOC": "34", "method_CCN": "3", "method_NToken": "324", "method_nesting_level": "1"}}}}}}}}