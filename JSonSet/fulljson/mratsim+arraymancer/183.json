{"BR": {"BR_id": "183", "BR_author": "mratsim", "BRopenT": "2017-12-22T23:22:15Z", "BRcloseT": "2018-05-01T10:49:31Z", "BR_text": {"BRsummary": "MNIST OpenMP accuracy 25% less than single threaded", "BRdescription": "\n Compiling the <denchmark-link:https://github.com/mratsim/Arraymancer/blob/8a92d825ae436e868c82709ade7553a8a6716816/examples/ex02_handwritten_digits_recognition.nim>MNIST example</denchmark-link>\n  with OpenMP leads to much worse accuracy for no apparent reason.\n After the first epoch\n Random seed 1337: Single-thread: 76.2%, OpenMP 51%\n Random seed 42: Single-thread: 83.2%, OpenMP 57.5%\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mratsim", "commentT": "2017-12-23T08:43:25Z", "comment_text": "\n \t\tAdded <denchmark-link:https://github.com/mratsim/Arraymancer/blob/061b2d07f8904db5c14a662f020fbcfb7e4cbac3/examples/ex02_handwritten_digits_recognition.nim>loss tracking</denchmark-link>\n  to MNIST example.\n Log:\n <denchmark-h:h2>No OpenMP</denchmark-h>\n \n <denchmark-code>Epoch is: 0\n Batch id: 0\n Loss is:  132.9124755859375\n Epoch is: 0\n Batch id: 200\n Loss is:  2.301989078521729\n Epoch is: 0\n Batch id: 400\n Loss is:  1.155071973800659\n Epoch is: 0\n Batch id: 600\n Loss is:  1.043337464332581\n Epoch is: 0\n Batch id: 800\n Loss is:  0.58299720287323\n Epoch is: 0\n Batch id: 1000\n Loss is:  0.5417937040328979\n Epoch is: 0\n Batch id: 1200\n Loss is:  0.6955615282058716\n Epoch is: 0\n Batch id: 1400\n Loss is:  0.4742314517498016\n Epoch is: 0\n Batch id: 1600\n Loss is:  0.3307125866413116\n Epoch is: 0\n Batch id: 1800\n Loss is:  0.6455222368240356\n \n Epoch #0 done. Testing accuracy\n Accuracy: 83.24999999999999%\n Loss:     0.5828457295894622\n \n \n Epoch is: 1\n Batch id: 0\n Loss is:  0.5344035029411316\n Epoch is: 1\n Batch id: 200\n Loss is:  0.4455387890338898\n Epoch is: 1\n Batch id: 400\n Loss is:  0.1642555445432663\n Epoch is: 1\n Batch id: 600\n Loss is:  0.5191419124603271\n Epoch is: 1\n Batch id: 800\n Loss is:  0.2091695368289948\n Epoch is: 1\n Batch id: 1000\n Loss is:  0.2661008834838867\n Epoch is: 1\n Batch id: 1200\n Loss is:  0.405451238155365\n Epoch is: 1\n Batch id: 1400\n Loss is:  0.1397259384393692\n Epoch is: 1\n Batch id: 1600\n Loss is:  0.526863694190979\n Epoch is: 1\n Batch id: 1800\n Loss is:  0.5916416645050049\n \n Epoch #1 done. Testing accuracy\n Accuracy: 88.49000000000001%\n Loss:     0.3582650691270828\n </denchmark-code>\n \n <denchmark-h:h2>openMP</denchmark-h>\n \n <denchmark-code>Epoch is: 0\n Batch id: 0\n Loss is:  132.9124603271484\n Epoch is: 0\n Batch id: 200\n Loss is:  2.315118074417114\n Epoch is: 0\n Batch id: 400\n Loss is:  1.185595512390137\n Epoch is: 0\n Batch id: 600\n Loss is:  0.9815522432327271\n Epoch is: 0\n Batch id: 800\n Loss is:  0.6072715520858765\n Epoch is: 0\n Batch id: 1000\n Loss is:  0.6047156453132629\n Epoch is: 0\n Batch id: 1200\n Loss is:  0.7644815444946289\n Epoch is: 0\n Batch id: 1400\n Loss is:  0.4162880778312683\n Epoch is: 0\n Batch id: 1600\n Loss is:  0.3775918483734131\n Epoch is: 0\n Batch id: 1800\n Loss is:  0.5572935938835144\n \n Epoch #0 done. Testing accuracy\n Accuracy: 57.50999999999999%\n Loss:     0.5141938149929046\n \n \n Epoch is: 1\n Batch id: 0\n Loss is:  0.4935077428817749\n Epoch is: 1\n Batch id: 200\n Loss is:  0.4984779953956604\n Epoch is: 1\n Batch id: 400\n Loss is:  0.1299190074205399\n Epoch is: 1\n Batch id: 600\n Loss is:  0.4471000134944916\n Epoch is: 1\n Batch id: 800\n Loss is:  0.240534171462059\n Epoch is: 1\n Batch id: 1000\n Loss is:  0.2717002034187317\n Epoch is: 1\n Batch id: 1200\n Loss is:  0.374997079372406\n Epoch is: 1\n Batch id: 1400\n Loss is:  0.112208716571331\n Epoch is: 1\n Batch id: 1600\n Loss is:  0.6015350818634033\n Epoch is: 1\n Batch id: 1800\n Loss is:  0.5499600172042847\n \n Epoch #1 done. Testing accuracy\n Accuracy: 60.31999999999999%\n Loss:     0.3561002880334854\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mratsim", "commentT": "2017-12-23T09:05:30Z", "comment_text": "\n \t\tAfter disabling OpenMP in im2col, maxpool2D, softmax, accuracy_score with no change, the most likely candidate is argmax (which is also used in maxpool2D)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "mratsim", "commentT": "2018-05-01T10:49:31Z", "comment_text": "\n \t\tIssue was the parallel version of Argmax, I suppose their was a race in the parallel \"fold_enumerateAxis_inline\" template.\n It has been deleted along with all the 3 extra procs needed to implement argmax in a functional manner. New version doesn't exhibit the issue. It is not parallel but as it uses less temporaries (passing from a tensor of tuples to a tuple of tensors), it might even be faster.\n Result on MNIST:\n Without OpenMP\n <denchmark-code>Epoch is: 0\n Batch id: 0\n Loss is:  194.3991851806641\n Epoch is: 0\n Batch id: 200\n Loss is:  2.60599946975708\n Epoch is: 0\n Batch id: 400\n Loss is:  1.708131313323975\n Epoch is: 0\n Batch id: 600\n Loss is:  1.061241149902344\n Epoch is: 0\n Batch id: 800\n Loss is:  0.8607467412948608\n Epoch is: 0\n Batch id: 1000\n Loss is:  0.9292868375778198\n Epoch is: 0\n Batch id: 1200\n Loss is:  0.6178927421569824\n Epoch is: 0\n Batch id: 1400\n Loss is:  0.4008050560951233\n Epoch is: 0\n Batch id: 1600\n Loss is:  0.2450754344463348\n Epoch is: 0\n Batch id: 1800\n Loss is:  0.3787734508514404\n \n Epoch #0 done. Testing accuracy\n Accuracy: 84.24999999999999%\n Loss:     0.4853884726762772\n \n \n Epoch is: 1\n Batch id: 0\n Loss is:  0.8319419622421265\n Epoch is: 1\n Batch id: 200\n Loss is:  0.3116425573825836\n Epoch is: 1\n Batch id: 400\n Loss is:  0.232885867357254\n Epoch is: 1\n Batch id: 600\n Loss is:  0.3877259492874146\n Epoch is: 1\n Batch id: 800\n Loss is:  0.3621436357498169\n Epoch is: 1\n Batch id: 1000\n Loss is:  0.5054937601089478\n Epoch is: 1\n Batch id: 1200\n Loss is:  0.4431287050247192\n Epoch is: 1\n Batch id: 1400\n Loss is:  0.2153264284133911\n Epoch is: 1\n Batch id: 1600\n Loss is:  0.1401071697473526\n Epoch is: 1\n Batch id: 1800\n Loss is:  0.3415909707546234\n \n Epoch #1 done. Testing accuracy\n Accuracy: 87.91%\n Loss:     0.3657706841826439\n </denchmark-code>\n \n With OpenMP\n <denchmark-code>Epoch is: 0\n Batch id: 0\n Loss is:  194.3992156982422\n Epoch is: 0\n Batch id: 200\n Loss is:  2.586019992828369\n Epoch is: 0\n Batch id: 400\n Loss is:  1.865354180335999\n Epoch is: 0\n Batch id: 600\n Loss is:  1.339139461517334\n Epoch is: 0\n Batch id: 800\n Loss is:  0.9255489110946655\n Epoch is: 0\n Batch id: 1000\n Loss is:  0.8845529556274414\n Epoch is: 0\n Batch id: 1200\n Loss is:  0.5737345814704895\n Epoch is: 0\n Batch id: 1400\n Loss is:  0.4271677136421204\n Epoch is: 0\n Batch id: 1600\n Loss is:  0.3307865262031555\n Epoch is: 0\n Batch id: 1800\n Loss is:  0.3299965560436249\n \n Epoch #0 done. Testing accuracy\n Accuracy: 81.76000000000001%\n Loss:     0.6043894469738007\n \n \n Epoch is: 1\n Batch id: 0\n Loss is:  0.9704902172088623\n Epoch is: 1\n Batch id: 200\n Loss is:  0.2412533462047577\n Epoch is: 1\n Batch id: 400\n Loss is:  0.2090668380260468\n Epoch is: 1\n Batch id: 600\n Loss is:  0.5054131746292114\n Epoch is: 1\n Batch id: 800\n Loss is:  0.4721413254737854\n Epoch is: 1\n Batch id: 1000\n Loss is:  0.5908526182174683\n Epoch is: 1\n Batch id: 1200\n Loss is:  0.3866634964942932\n Epoch is: 1\n Batch id: 1400\n Loss is:  0.1679804921150208\n Epoch is: 1\n Batch id: 1600\n Loss is:  0.3216101229190826\n Epoch is: 1\n Batch id: 1800\n Loss is:  0.2575283646583557\n \n Epoch #1 done. Testing accuracy\n Accuracy: 90.58999999999999%\n Loss:     0.2933134615421295\n </denchmark-code>\n \n \t\t"}}}, "commit": {"commit_id": "3d1377214453198382b3f7e8d896b379ba02f00e", "commit_author": "mratsim", "commitT": "2018-05-01 12:37:12+02:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\accessors.nim", "file_new_name": "src\\tensor\\accessors.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "335", "deleted_lines": "335"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\aggregate.nim", "file_new_name": "src\\tensor\\aggregate.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15,21,151,152,153,155,156,157,158,159,160,162,163,164,165,166", "deleted_lines": "15,150,152,153,154,155,156,157,158,159,160,161,163,164,165"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\higher_order_foldreduce.nim", "file_new_name": "src\\tensor\\higher_order_foldreduce.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "56", "deleted_lines": "56,63,64,65,66,67,68,69,70,71,72,73,74"}}}, "file_3": {"file_change_type": "DELETE", "file_Nmethod": 0, "file_old_name": "src\\tensor\\private\\p_aggregate.nim", "file_new_name": "None", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tests\\tensor\\test_aggregate.nim", "file_new_name": "tests\\tensor\\test_aggregate.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "106,107,108,109,110,111,112,113", "deleted_lines": "106"}}}}}}