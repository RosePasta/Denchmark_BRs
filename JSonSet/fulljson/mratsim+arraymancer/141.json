{"BR": {"BR_id": "141", "BR_author": "mratsim", "BRopenT": "2017-11-01T13:04:02Z", "BRcloseT": "2017-11-05T00:12:59Z", "BR_text": {"BRsummary": "Bottleneck: 30% time spent in genericSeqAssign in slicer and sigmoid_cross_entropy", "BRdescription": "\n Following <denchmark-link:https://github.com/mratsim/Arraymancer/commit/9834ad0212d5a53c702cf2abfd628dff61828391>9834ad0</denchmark-link>\n  the xor benchmark is now spending 3 sec out of 11s doing GenericSeqAssign.\n <denchmark-link:https://user-images.githubusercontent.com/22738317/32275867-1d0e6924-bf0d-11e7-8361-3137a232483a.png></denchmark-link>\n \n Culprits:\n \n slicers\n \n proc slicer[T](t: AnyTensor[T],\n                 slices: varargs[SteppedSlice],\n                 ellipsis: Ellipsis): AnyTensor[T] {.noInit,noSideEffect.}=\n   ## Take a Tensor, SteppedSlices and Ellipsis\n   ## Returns:\n   ##    A copy of the original Tensor\n   ##    Offset and strides are changed to achieve the desired effect.\n \n   result = t\n   let full_slices = @slices & newSeqWith(t.rank - slices.len, span)\n   slicerT(result, full_slices)\n \n proc slicer[T](t: AnyTensor[T],\n                 ellipsis: Ellipsis,\n                 slices: varargs[SteppedSlice]\n                 ): AnyTensor[T] {.noInit,noSideEffect.}=\n   ## Take a Tensor, Ellipsis and SteppedSlices\n   ## Returns:\n   ##    A copy of the original Tensor\n   ##    Offset and strides are changed to achieve the desired effect.\n \n   result = t\n   let full_slices = newSeqWith(t.rank - slices.len, span) & @slices\n   slicerT(result, full_slices)\n \n proc slicer[T](t: AnyTensor[T],\n                 slices1: varargs[SteppedSlice],\n                 ellipsis: Ellipsis,\n                 slices2: varargs[SteppedSlice]\n                 ): AnyTensor[T] {.noInit,noSideEffect.}=\n   ## Take a Tensor, Ellipsis and SteppedSlices\n   ## Returns:\n   ##    A copy of the original Tensor\n   ##    Offset and strides are changed to achieve the desired effect.\n \n   result = t\n   let full_slices = concat(@slices1,\n                             newSeqWith(t.rank - slices1.len - slices2.len, span),\n                             @slices2)\n   slicerT(result, full_slices)\n And sigmoid_cross_entropy from nn:\n N_NIMCALL(tyObject_VariablecolonObjectType__V9arbGi37a9bYRP6noahxnkg*, sigmoid_cross_entropy_JUee17nUtwwG9crOhwAktAw)(tyObject_VariablecolonObjectType__V9arbGi37a9bYRP6noahxnkg* a, tyObject_Tensor_YVEir6VZKk3q2MAtip9aD6w* target) {\n \ttyObject_VariablecolonObjectType__V9arbGi37a9bYRP6noahxnkg* result;\n \ttyObject_SigmoidCrossEntropyLosscolonObjectType__UFgVZehjkS6ZKN0TA9bM9a6Q* gate;\n \ttyObject_NodecolonObjectType__p32yf8YodYmaGoPmH50AWw* node;\n \tNI T1_;\n \ttyObject_LosscolonObjectType__AuSc1kjvf0sy9bIrj0fYzpQ* T2_;\n \tresult = (tyObject_VariablecolonObjectType__V9arbGi37a9bYRP6noahxnkg*)0;\n \tgate = (tyObject_SigmoidCrossEntropyLosscolonObjectType__UFgVZehjkS6ZKN0TA9bM9a6Q*)0;\n \tgate = (tyObject_SigmoidCrossEntropyLosscolonObjectType__UFgVZehjkS6ZKN0TA9bM9a6Q*) newObj((&NTI_HqKVGU8O1Eg0L2tk6CR9bTA_), sizeof(tyObject_SigmoidCrossEntropyLosscolonObjectType__UFgVZehjkS6ZKN0TA9bM9a6Q));\n \t(*gate).Sup.Sup.m_type = (&NTI_UFgVZehjkS6ZKN0TA9bM9a6Q_);\n \t(*gate).Sup.Sup.arity = ((NI) 1);\n \tasgnRef((void**) (&(*gate).cache), a);\n \t(*gate).Sup.target.shape = (*target).shape;\n \t(*gate).Sup.target.strides = (*target).strides;\n \t(*gate).Sup.target.offset = (*target).offset;\n \tgenericSeqAssign((&(*gate).Sup.target.data), (*target).data, (&NTI_4Xyxy0Om14N6K1l5e9bUPSQ_));   // <<<----------------- HERE\n \tnode = (tyObject_NodecolonObjectType__p32yf8YodYmaGoPmH50AWw*)0;\n \tnode = (tyObject_NodecolonObjectType__p32yf8YodYmaGoPmH50AWw*) newObj((&NTI_u2b9cqonYlV8r9bWdpfPYhKQ_), sizeof(tyObject_NodecolonObjectType__p32yf8YodYmaGoPmH50AWw));\n \tasgnRef((void**) (&(*node).gate), gate);\n \tasgnRef((void**) (&(*node).parents[(((NI) 0))- 0]), a);\n \t(*(*a).tape).nodes = (tySequence_vShYhtvHQtyhCu8g2tVy6Q*) incrSeqV2(&((*(*a).tape).nodes)->Sup, sizeof(tyObject_NodecolonObjectType__p32yf8YodYmaGoPmH50AWw*));\n \tT1_ = (*(*a).tape).nodes->Sup.len++;\n \tasgnRef((void**) (&(*(*a).tape).nodes->data[T1_]), node);\n \tT2_ = (tyObject_LosscolonObjectType__AuSc1kjvf0sy9bIrj0fYzpQ*)0;\n \tT2_ = &gate->Sup;\n \tresult = forward_qAuDJ4FSE2zIoYPtuAdhHg(T2_, a, target);\n \tasgnRef((void**) (&(*result).ancestor), node);\n \tasgnRef((void**) (&(*node).child), result);\n \treturn result;\n }\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mratsim", "commentT": "2017-11-01T13:15:37Z", "comment_text": "\n \t\tSigmoid_cross_entropy useless alloc fixed in <denchmark-link:https://github.com/mratsim/Arraymancer/commit/aa7a803822572ee2de7d836ae84a15deebb81832>aa7a803</denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/22738317/32276356-1e7b644a-bf0f-11e7-86f8-d844d3617660.png></denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mratsim", "commentT": "2017-11-03T21:50:37Z", "comment_text": "\n \t\tThe previous implementation of sparse_softmax_cross_entropy was also heavily affected.\n <denchmark-link:https://user-images.githubusercontent.com/22738317/32397222-0f458416-c0e9-11e7-9837-e556b340f8b6.png></denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "mratsim", "commentT": "2017-11-03T22:54:26Z", "comment_text": "\n \t\t<denchmark-h:h2>The plan:</denchmark-h>\n \n Similar to metadataArray introduce a sliceArray type allocated on the stack.\n Note: varargs[SteppedSlices] should not catch a sliceArray.\n <denchmark-h:h2>How?</denchmark-h>\n \n Modify metadataArray to be derived from a generic base type and slice Array should be the same:\n type\n   FooArray*[T] = object\n     data*: array[MAXRANK, T]\n     len*: int\n \n   MetadataArray = FooArray[int]\n   SliceArray = FooArray[SteppedSlices]\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "mratsim", "commentT": "2017-11-05T00:12:59Z", "comment_text": "\n \t\tIn fine, the main issue in softmax_cross_entropy was forgetting to use unsafeSlice.\n In any case heap allocated / GC-managed memory has been reduced to just the Tensor data. Everything else is allocated on the stack, it is now possible to use unsafeSlicing within an OpenMP loop. Memory usage is also more predictable and there is much less risk of fragmentation due to allocating/free small seq (2 or 3 elements slices) in rapid succession.\n Gains in terms of performance is inconclusive. At least there is no loss.\n Closed by <denchmark-link:https://github.com/mratsim/Arraymancer/commit/d553685f5d30c7ed7192615e897467732e9695ab>d553685</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "d553685f5d30c7ed7192615e897467732e9695ab", "commit_author": "Mamy Ratsimbazafy", "commitT": "2017-11-05 00:01:24+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "benchmarks\\implementation\\logsumexp.nim", "file_new_name": "benchmarks\\implementation\\logsumexp.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245", "deleted_lines": "179"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "benchmarks\\implementation\\softmax_cross_entropy_bench.nim", "file_new_name": "benchmarks\\implementation\\softmax_cross_entropy_bench.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19,20,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,103,115,119,121,128,129,130,131,132,133,134,135,136,137,139,152,154,165,179,180,181,182,184,185,187,205,206,210,214,227,233,250,251,285,288,295,296,297,298,301,302,303,304,307,308,309,310,312,313,314,315,320,321,322,323,324,325,326,327,328,329,335,336,337,338,340,341,342,343,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427", "deleted_lines": "19,31,40,52,56,58,65,66,67,68,69,70,72,85,87,98,112,113,115,116,118,136,137,141,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,178,228,231,238,239,240,241,242,243,244,245,246,247,248,250,251,252,253,255,256,257,258,261,262,263,264,265,267,268,269,270,271,276,277,278,279,285,286,287,288,290,291,292,293,305,306,307"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\autograd\\gates_basic.nim", "file_new_name": "src\\autograd\\gates_basic.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "30,31,32,33,34,35"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\autograd\\gates_blas.nim", "file_new_name": "src\\autograd\\gates_blas.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": null, "deleted_lines": "29,30,31,32,33,34"}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\nn\\layers\\linear.nim", "file_new_name": "src\\nn\\layers\\linear.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "45,46,47,54,69,70", "deleted_lines": "45,46,47,54,69,70"}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\nn\\loss\\sigmoid_cross_entropy.nim", "file_new_name": "src\\nn\\loss\\sigmoid_cross_entropy.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27", "deleted_lines": "27"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\nn_primitives\\nnp_sigmoid_cross_entropy.nim", "file_new_name": "src\\nn_primitives\\nnp_sigmoid_cross_entropy.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "32", "deleted_lines": "32"}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\nn_primitives\\nnp_softmax_cross_entropy.nim", "file_new_name": "src\\nn_primitives\\nnp_softmax_cross_entropy.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "28,29,69,70,99,100,101,103,105,109,179,180,183,184", "deleted_lines": "28,29,69,70,99,100,102,104,108,109,110,111,112,113,114,115,185,186,188,189,190,192,193"}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\private\\nested_containers.nim", "file_new_name": "src\\private\\nested_containers.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "15,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67", "deleted_lines": "18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,48"}}}, "file_9": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\accessors_macros_syntax.nim", "file_new_name": "src\\tensor\\accessors_macros_syntax.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "16,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105", "deleted_lines": null}}}, "file_10": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\backend\\metadataArray.nim", "file_new_name": "src\\tensor\\backend\\metadataArray.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "17,18,19,20,21,22,23,25,28,39,47,55,60,63,81,87,93,99,111,115,119,123,128,137,142,152,163,170,174,183,188,195,203,211,214,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235", "deleted_lines": "18,20,21,23,34,42,50,55,58,76,82,88,94,106,110,114,118,123,132,137,147,158,165,169,178,183,190,198,206,209,215"}}}, "file_11": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\private\\p_accessors_macros_read.nim", "file_new_name": "src\\tensor\\private\\p_accessors_macros_read.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "20,24,53,64,77,91,92,93,96,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,184,200,217,218,219", "deleted_lines": "23,52,63,76,90,91,92,95,120,136,153,154,155"}}}, "file_12": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\private\\p_accessors_macros_write.nim", "file_new_name": "src\\tensor\\private\\p_accessors_macros_write.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "17,30,37,47,58,70,71,72,84,106,116,128,129,130,156,165,175,176,177,222,235,249,250,251", "deleted_lines": "29,36,46,57,69,70,71,83,105,115,127,128,129,155,164,174,175,176,221,234,248,249,250"}}}, "file_13": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\private\\p_checks.nim", "file_new_name": "src\\tensor\\private\\p_checks.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "21,78", "deleted_lines": "21,78,79,80,81"}}}, "file_14": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\private\\p_shapeshifting.nim", "file_new_name": "src\\tensor\\private\\p_shapeshifting.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "55", "deleted_lines": "55"}}}, "file_15": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\shapeshifting.nim", "file_new_name": "src\\tensor\\shapeshifting.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "274,320,329", "deleted_lines": "274,320,329"}}}, "file_16": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\tensor\\term_rewriting.nim", "file_new_name": "src\\tensor\\term_rewriting.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "16,82,84,85,118,119", "deleted_lines": "81,83,116"}}}}}}