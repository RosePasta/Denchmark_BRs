{"BR": {"BR_id": "327", "BR_author": "mratsim", "BRopenT": "2018-12-09T13:07:23Z", "BRcloseT": "2018-12-16T11:16:06Z", "BR_text": {"BRsummary": "Custom NN types - \"undeclared field 'grad'", "BRdescription": "\n When creating a custom NN type if it doesn't use generics you run into issues with the autograd backward method:\n For example\n type ShakespeareNet = object\n   # Embedding weight = Encoder\n   encoder_w: Variable[Tensor[float32]]\n \n   # GRU RNN = Internal representation\n   gru_W3s0, gru_W3sN, gru_U3s, gru_bW3s, bU3s: Variable[Tensor[float32]]\n \n   # Linear layer weight = Decoder\n   decoder_w: Variable[Tensor[float32]]\n Will trigger an error here:\n \n \n \n Arraymancer/src/autograd/gates_basic.nim\n \n \n         Lines 30 to 31\n       in\n       89cd3f4\n \n \n \n \n \n \n  method backward*[TT](self: AddGate[TT], payload: Payload[TT]): SmallDiffs[TT] {.noInit, inline.}= \n \n \n \n  let gradient = payload.variable.grad \n \n \n \n \n \n <denchmark-code>yourfile.nim(71, 22) template/generic instantiation of `backward` from here\n ../src/autograd/gates_basic.nim(32, 34) Error: undeclared field: 'grad'\n </denchmark-code>\n \n This happens when the generic types and its method can be materialised early.\n The workaround is to use a generic type ...\n type ShakespeareNet[TT] = object\n   # Embedding weight = Encoder\n   encoder_w: Variable[TT]\n \n   # GRU RNN = Internal representation\n   gru_W3s0, gru_W3sN, gru_U3s, gru_bW3s, bU3s: Variable[TT]\n \n   # Linear layer weight = Decoder\n   decoder_w: Variable[TT]\n Renaming the payload field from variable to something else to avoid collision might also help.\n Removing  altogether would also be a solution (<denchmark-link:https://github.com/mratsim/Arraymancer/issues/302>#302</denchmark-link>\n )\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "mratsim", "commentT": "2018-12-13T00:18:35Z", "comment_text": "\n \t\tTrying to change the following\n type\n   Sgd*[TT] = object\n     ## Stochastic gradient descent\n     params: seq[Variable[TT]]\n     lr: float32 # Learning rate.\n into\n type\n   Sgd*[T] = object\n     ## Stochastic gradient descent\n     params: seq[Variable[Tensor[T]]]\n     lr: T # Learning rate.\n also triggers the same issue at the same location.\n Changing payload.variable field to a name that is not used at all didn't fix the issue:\n \n \n \n Arraymancer/src/autograd/ag_data_structure.nim\n \n \n         Lines 81 to 86\n       in\n       3ae364d\n \n \n \n \n \n \n  PayloadKind* = enum \n \n \n \n    pkVar, pkSeq \n \n \n \n  Payload*[TT] = object \n \n \n \n  case kind*: PayloadKind \n \n \n \n  of pkVar: variable*: Variable[TT] \n \n \n \n  of pkSeq: sequence*: seq[Variable[TT]] \n \n \n \n \n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "mratsim", "commentT": "2018-12-16T11:16:06Z", "comment_text": "\n \t\tfixed by <denchmark-link:https://github.com/mratsim/Arraymancer/pull/333>#333</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "edf22b13800e7154db49a845868ee4778f9fb42c", "commit_author": "mratsim", "commitT": "2018-12-16 00:25:46+01:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\nn\\optimizers\\optimizers.nim", "file_new_name": "src\\nn\\optimizers\\optimizers.nim", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19,21,22", "deleted_lines": "19,21,22"}}}}}}