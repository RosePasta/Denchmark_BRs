{"BR": {"BR_id": "281", "BR_author": "ryanjulian", "BRopenT": "2018-08-14T01:07:08Z", "BRcloseT": "2018-10-30T22:50:14Z", "BR_text": {"BRsummary": "policy_ent_coeff does not seem to work in tf/NPO", "BRdescription": "\n I can choose any value (or GaussianMLPPolicy params) I want, and it doesn't change the policy standard deviation. e.g. -1.0 won't cause the policy stddev to fall, and 1.0 won't cause it to rise. This may be because a gradient path is broken between the reward function and the policy std network.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ryanjulian", "commentT": "2018-08-14T20:20:55Z", "comment_text": "\n \t\tWhat param do you set to -1.0? init_std?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ryanjulian", "commentT": "2018-08-14T20:24:17Z", "comment_text": "\n \t\tMy assertion was\n if policy_ent_coeff = -1.0, then during training I should see the policy entropy decrease rapidly (if most environment rewards are small)\n likewise, if policy_ent_coeff = 1.0, then I should see policy entropy increase rapidly.\n this is because the optimizer should be able to differentiate through the loss function all the way to the std_network of the policy. the term policy_ent_coeff tunes the magnitude of this gradient.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ryanjulian", "commentT": "2018-08-14T20:29:52Z", "comment_text": "\n \t\tThings to check:\n \n Assuming your augmented rewards equation looks like r + (policy_ent_coeff * policy_entropy), ensure that policy_entropy is differentiable (symbolic) all the way back to the std_network of the policy. This means that your loss function retrieves policy_entropy by purely by feeding actions and observations, not by using the recorded mean/std (e.g. agent_infos, dist_infos).\n Make sure that dist_infos are only used for forward pass calculations (e.g. KL divergence and likelihood ratio)\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ryanjulian", "commentT": "2018-10-17T17:50:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/CatherineSue>@CatherineSue</denchmark-link>\n  were you able to reproduce this?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ryanjulian", "commentT": "2018-10-17T18:05:47Z", "comment_text": "\n \t\tYes. I reproduced the bug.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ryanjulian", "commentT": "2018-10-25T17:51:55Z", "comment_text": "\n \t\tFor this issue, let's use\n \n MaxEnt (e.g. augment the reward with an entropy term and a coefficient): \\hat r = r + \\alpha_1 * H\n log-likelihood estimator: H ~ -logli(action| policy(state))\n Turn off the gradient: H = tf.stop_gradient(H)\n \n \t\t"}}}, "commit": {"commit_id": "26c14a3e943e219b9202ceaacc72b6caffe2b8c8", "commit_author": "Chang Su", "commitT": "2018-10-30 15:44:55-07:00", "commit_complexity": {"commit_NLOC": "0.13333333333333333", "commit_CCN": "1.0", "commit_Nprams": "0.8"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "examples\\tf\\ppo_pendulum.py", "file_new_name": "examples\\tf\\ppo_pendulum.py", "file_complexity": {"file_NLOC": "40", "file_CCN": "1", "file_NToken": "182"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "30", "deleted_lines": "30,31", "method_info": {"method_name": "run_task", "method_params": "_", "method_startline": "21", "method_endline": "45", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "111", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "garage\\tf\\algos\\npo.py", "file_new_name": "garage\\tf\\algos\\npo.py", "file_complexity": {"file_NLOC": "382", "file_CCN": "32", "file_NToken": "2371"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "102,103", "deleted_lines": "98", "method_info": {"method_name": "optimize_policy", "method_params": "self,itr,samples_data", "method_startline": "77", "method_endline": "109", "method_complexity": {"method_NLOC": "28", "method_CCN": "1", "method_NToken": "252", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "37,39,40", "deleted_lines": "37", "method_info": {"method_name": "__init__", "method_params": "self,pg_loss,clip_range,optimizer,optimizer_args,name,policy,policy_ent_coeff,use_softplus_entropy,use_neg_logli_entropy,stop_entropy_gradient,kwargs", "method_startline": "30", "method_endline": "41", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "54", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "366,367,368,369,375,376,377,378,379,380,381,382,383,384,393,394,395,396", "deleted_lines": "367,368,376", "method_info": {"method_name": "_build_entropy_term", "method_params": "self,i", "method_startline": "359", "method_endline": "403", "method_complexity": {"method_NLOC": "37", "method_CCN": "5", "method_NToken": "207", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "37,39", "deleted_lines": "37", "method_info": {"method_name": "__init__", "method_params": "self,pg_loss,clip_range,optimizer,optimizer_args,name,policy,policy_ent_coeff,use_softplus_entropy,kwargs", "method_startline": "30", "method_endline": "39", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "46", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "334", "deleted_lines": "329", "method_info": {"method_name": "_build_policy_loss", "method_params": "self,i", "method_startline": "226", "method_endline": "357", "method_complexity": {"method_NLOC": "107", "method_CCN": "11", "method_NToken": "659", "method_nesting_level": "1"}}}}}}}}