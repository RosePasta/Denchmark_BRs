{"BR": {"BR_id": "18", "BR_author": "jyhong836", "BRopenT": "2018-11-15T20:42:19Z", "BRcloseT": "2018-11-20T20:00:25Z", "BR_text": {"BRsummary": "What does the `reset_states` do?", "BRdescription": "\n It seems that the method reset_state in metrics resets the stored values. However, I am not sure when it should be used. Is it for resetting states at the end of each epoch?\n According to my understanding, the keras-metrics is designed to avoid the incorrect approximation of recall on each batch. Thus, a practical solution is computing the metrics on the end of each epoch independently.\n But in the <denchmark-link:https://github.com/netrack/keras-metrics/blob/master/README.md>README.md</denchmark-link>\n , the given example is\n import keras\n import keras_metrics\n \n model = models.Sequential()\n model.add(keras.layers.Dense(1, activation=\"sigmoid\", input_dim=2))\n model.add(keras.layers.Dense(1, activation=\"softmax\"))\n \n model.compile(optimizer=\"sgd\",\n               loss=\"binary_crossentropy\",\n               metrics=[keras_metrics.precision(), keras_metrics.recall()])\n which directly pass the keras_metrics.recall() as metrics for batch-based usage. The problem in the demo is that the states may* not be resetted. Therefore, the recall value of each epoch will be dependent on previous epochs.\n * I am not sure if the reset_states method is called at the end of each epoch.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jyhong836", "commentT": "2018-11-16T08:24:18Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/jyhong836>@jyhong836</denchmark-link>\n . The  indeed is called on each epoch, see <denchmark-link:https://github.com/keras-team/keras/blob/2.2.4/keras/engine/training_arrays.py#L145>https://github.com/keras-team/keras/blob/2.2.4/keras/engine/training_arrays.py#L145</denchmark-link>\n .\n It seems the only thing with that: metrics should be marked as stateful, while they are not. Thank you for noticing that, I'll prepare appropriate changes.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jyhong836", "commentT": "2018-11-16T13:42:31Z", "comment_text": "\n \t\tThank you for your reply. You are right. I didn't notice the calling in training_arrays.py.\n BTW, the unit test case is actually unconvincing. The correctness of the true positive, false negative and etc. values are not tested.\n There has been a stateful metric test, i.e., <denchmark-link:https://github.com/keras-team/keras/blob/75a35032e194a2d065b0071a9e786adf6cee83ea/tests/keras/metrics_test.py#L124>test_stateful_metrics</denchmark-link>\n , in the official package but only for binary true positive test. You may refer to that.\n Your package is really useful. Thank you for your contribution.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jyhong836", "commentT": "2018-11-17T02:14:56Z", "comment_text": "\n \t\tI try to run the unit test in the pull request <denchmark-link:https://github.com/netrack/keras-metrics/pull/19>#19</denchmark-link>\n  . I compare the false_positive value aganist below function:\n def ref_false_pos(y_true, y_pred):\n     return np.sum(np.logical_and(np.round(y_pred)==1, y_true == 0))\n y_pred = model.predict(x)\n expected_fp = ref_false_pos(y, y_pred)\n The values will not be equal occassionally. Even if I fixed the random seed by  np.random.seed(2334), the inequality still happens occasionally.\n Is there any explanation for this stochastics?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jyhong836", "commentT": "2018-11-17T08:09:55Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jyhong836>@jyhong836</denchmark-link>\n , could you, please post an example of run with failing test (output or input data). Unfortunately, I can't reproduce this issue after merging pull request <denchmark-link:https://github.com/netrack/keras-metrics/pull/19>#19</denchmark-link>\n .\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jyhong836", "commentT": "2018-11-17T14:49:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ybubnov>@ybubnov</denchmark-link>\n  I post my test at <denchmark-link:https://github.com/jyhong836/keras-metrics>jyhong836/keras-metrics</denchmark-link>\n . But I am not sure if you can reproduce the result. I also include a  file. Please put it under your working directory.\n Currently, I can reproduce the error on my Macbook, macos 10.14, tensorflow 1.5.0 (cpu version), keras 2.2.4 & 2.1.6. However, I cannot reproduce it on another linux computer, with tensorflow 1.4.0 (GPU version), keras 2.1.6. I am not sure if it is the version issue or the computer issue.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jyhong836", "commentT": "2018-11-19T13:45:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jyhong836>@jyhong836</denchmark-link>\n , I've tried to run your tests on my Linux machine and I've managed to reproduce an issue with the tensorflow  version. There is no issue with tensoflow  though.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jyhong836", "commentT": "2018-11-19T14:23:38Z", "comment_text": "\n \t\tTests are failing both when model is loaded from temp_model.hdf5 and after model fitting.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "jyhong836", "commentT": "2018-11-19T15:29:04Z", "comment_text": "\n \t\tSo I guess there is some bug in tensorflow <=1.5.0.\n Which tensorflow do you use? GPU or CPU version?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "jyhong836", "commentT": "2018-11-20T06:13:02Z", "comment_text": "\n \t\tI'm able to reproduce an issue with tensorflow 1.6.0 and 1.7.0 as well. I'm using CPU version (from pip repository, so not optimized for AVX2 and FMA instructions).\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "jyhong836", "commentT": "2018-11-20T20:00:25Z", "comment_text": "\n \t\tI think there is no better solution than upgrading. I will close the issue.\n \t\t"}}}, "commit": {"commit_id": "f12f3358a458357d5abf3eeb2d05aae46aa554e2", "commit_author": "Yasha Bubnov", "commitT": "2018-11-16 17:37:54+03:00", "commit_complexity": {"commit_NLOC": "0.42105263157894735", "commit_CCN": "1.0", "commit_Nprams": "0.5789473684210527"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "keras_metrics\\__init__.py", "file_new_name": "keras_metrics\\__init__.py", "file_complexity": {"file_NLOC": "2", "file_CCN": "0", "file_NToken": "9"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1", "deleted_lines": "1"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "keras_metrics\\metrics.py", "file_new_name": "keras_metrics\\metrics.py", "file_complexity": {"file_NLOC": "165", "file_CCN": "28", "file_NToken": "1307"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "178", "method_info": {"method_name": "__init__", "method_params": "self,name,kwargs", "method_startline": "173", "method_endline": "178", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "56", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "190,191,192,193,194,195,196", "deleted_lines": "189,190", "method_info": {"method_name": "__call__", "method_params": "self,y_true,y_pred", "method_startline": "186", "method_endline": "196", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "90", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": null, "deleted_lines": "205", "method_info": {"method_name": "__init__", "method_params": "self,name,kwargs", "method_startline": "200", "method_endline": "205", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "56", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "41,51", "deleted_lines": "39,49", "method_info": {"method_name": "_categorical", "method_params": "self,y_true,y_pred,dtype", "method_startline": "36", "method_endline": "53", "method_complexity": {"method_NLOC": "10", "method_CCN": "3", "method_NToken": "83", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "11,12,16", "deleted_lines": "14", "method_info": {"method_name": "__init__", "method_params": "self,label,kwargs", "method_startline": "9", "method_endline": "20", "method_complexity": {"method_NLOC": "8", "method_CCN": "2", "method_NToken": "73", "method_nesting_level": "1"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\test_metrics.py", "file_new_name": "tests\\test_metrics.py", "file_complexity": {"file_NLOC": "52", "file_CCN": "1", "file_NToken": "432"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "13,22,23,27,29,30,31,32,33,36,37,38,39,42,43,44,46,47,48,57,58,59,60,61,62,63,64,65,66,67", "deleted_lines": "19,20,24,26,27,30,31,34,35,37,38,39,48,49,50", "method_info": {"method_name": "test_metrics", "method_params": "self", "method_startline": "11", "method_endline": "67", "method_complexity": {"method_NLOC": "43", "method_CCN": "1", "method_NToken": "399", "method_nesting_level": "1"}}}}}}}}