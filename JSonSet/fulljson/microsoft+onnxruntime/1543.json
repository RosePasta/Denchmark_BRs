{"BR": {"BR_id": "1543", "BR_author": "hossein1387", "BRopenT": "2019-08-01T15:42:42Z", "BRcloseT": "2019-11-06T19:34:09Z", "BR_text": {"BRsummary": "Error running quantized onnx model", "BRdescription": "\n \n Using <denchmark-link:https://github.com/microsoft/onnxruntime/tree/e26e11b9f7f7b1d153d9ce2ac160cffb241e4ded/onnxruntime/python/tools/quantization>Quantization tool</denchmark-link>\n   I quantized  and got  . However, when I try to run the quantized model I get:\n RuntimeError: [ONNXRuntimeError] : 1 : GENERAL ERROR : Load model from VGG_Quant.onnx failed:[ShapeInferenceError] Incompatible dimensions \n Running the original onnx model (VGG.onnx) with the same setup (same dataset) does not produce any error. The error occur when I try to create an InferenceSession, here is how I try to run my code:\n <denchmark-code> options = onnxrt.SessionOptions()\n  sess = self.onnxrt.InferenceSession('VGG_Quant.onnx', options)\n </denchmark-code>\n \n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS x86_64 GNU/Linux\n ONNX Runtime installed from (source or binary): pip\n ONNX Runtime version: 0.4.0\n Python version: Python 3.7.3\n \n The original model was downloaded from Pytorch zoo models and then converted to onnx (which again runs perfectly fine with onnxruntime).\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hossein1387", "commentT": "2019-08-01T16:47:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hossein1387>@hossein1387</denchmark-link>\n  Can you share the fp32 onnx model and the quantization script input params which you chose while quantizing the model.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hossein1387", "commentT": "2019-08-01T16:54:58Z", "comment_text": "\n \t\tHere is the original <denchmark-link:https://drive.google.com/open?id=1WxTGyE947Gzv_zh6gOxpv18KdA1h9z9i>fp32 model</denchmark-link>\n , and here is the <denchmark-link:https://drive.google.com/open?id=1_9UWIMNuKhhm_X3_hUK740pc0iDeo0Mn>quantized version</denchmark-link>\n . I quantized the model using the following code:\n <denchmark-code>import onnx\n from quantize import quantize, QuantizationMode\n \n # Load the onnx model\n model = onnx.load('VGG.onnx')\n # Quantize\n quantized_model = quantize(model, quantization_mode=QuantizationMode.IntegerOps)\n # Save the quantized model\n onnx.save(quantized_model, 'VGG_Quant.onnx')\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hossein1387", "commentT": "2019-08-02T18:54:20Z", "comment_text": "\n \t\tI think there is a serious bug with the quantizer module. I trained an MLP model and passed the trained model through the quantizer and realized the quantizer did not do anything. After going through the code I found out that the quantizer only quantizes the Conv and MatMul nodes. I don't know why a GEMM operator (which can be found in MLP and FC layers) is not a MatMul. Anyway, I then tried to train a network with only one Conv layer and a FC  layer. The following shows the graph of my network:\n <denchmark-link:https://user-images.githubusercontent.com/6366236/62392372-4bbbbe00-b535-11e9-994f-22f3917cf5cc.png></denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/6366236/62392380-4f4f4500-b535-11e9-8f1e-e8fbfd27d0cc.png></denchmark-link>\n \n On top is the fp32 version and the bottom graph shows the quantized version. The model passed through the quantizer successfully but again I was unable to run the model using the onnx runtime and got the same error as before.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hossein1387", "commentT": "2019-08-02T21:15:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hossein1387>@hossein1387</denchmark-link>\n  : Thanks for the detailed info...\n We are working towards strengthening support for quantization including the quantization tooling.\n I will update the quantization script to include GEMM as well and will update you once I root cause the shape inference bug in the quantized model.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "hossein1387", "commentT": "2019-08-06T00:44:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hossein1387>@hossein1387</denchmark-link>\n  : The reason for shape inference failure is an invalid default value in quantize script which is not supported by runtime yet... We do plan to add per-channel quantization support but it is not available today...\n My PR reference above should resolve this issue and in the meanwhile you can also try this instead:\n quantized_model = quantize(model, quantization_mode=QuantizationMode.IntegerOps, per_channel=False)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "hossein1387", "commentT": "2019-08-06T04:18:10Z", "comment_text": "\n \t\tThanks for the update, I ran two models with and without quantization, both models are VGG like and both are using CIFAR100, here is some results:\n \n \n \n \u00a0\n Model1\n \n \n \n \n \n \u00a0\n FP32\n Quantized\n \n \n Accuracy\n 72.28%\n 72.27%\n \n \n Exec Time\n 14.4ms\n 53.9ms\n \n \n Size\n 77MB\n 19MB\n \n \n \n \n \n \n \u00a0\n Model2\n \n \n \n \n \n \u00a0\n FP32\n Quantized\n \n \n Accuracy\n 73.99%\n 73.97%\n \n \n Exec Time\n 7.6ms\n 50.29ms\n \n \n Size\n 20MB\n 5.2MB\n \n \n \n I dont understand why the quantized version is taking much longer than original model? Shouldn't the 8 bit quantized model take less than the FP32 model?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "hossein1387", "commentT": "2019-08-16T14:57:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/askhade>@askhade</denchmark-link>\n  any idea on why I am gettig these results?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "hossein1387", "commentT": "2019-08-21T20:09:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hossein1387>@hossein1387</denchmark-link>\n  : which platform are you running on? We don't have optimized kernel support for windows yet... this work is in progress. On Linux the perf should be better than windows but we only support single threaded kernels...\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "hossein1387", "commentT": "2019-09-09T22:56:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hossein1387>@hossein1387</denchmark-link>\n  could you provide more info on this if you still require assistance?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "hossein1387", "commentT": "2019-09-11T14:59:05Z", "comment_text": "\n \t\tThanks for your responses <denchmark-link:https://github.com/askhade>@askhade</denchmark-link>\n  <denchmark-link:https://github.com/faxu>@faxu</denchmark-link>\n .\n Here is my platform information:\n <denchmark-code>python version: 3.7.3\n python build version: ('default', 'Mar 27 2019 22:11:17')\n python compiler version: GCC 7.3.0\n python implementation: CPython\n os: Linux\n os kernel version: #201806252030 SMP Tue Jun 26 00:33:17 UTC 2018\n os release version: 4.17.3-041703-generic\n os platform: Linux-4.17.3-041703-generic-x86_64-with-debian-stretch-sid\n linux distribution: Debian\n uname: uname_result(system='Linux', node='TANDEM-TL0275U', release='4.17.3-041703-generic', version='#201806252030 SMP Tue Jun 26 00:33:17 UTC 2018', machine='x86_64', processor='x86_64')\n architecture: ('64bit', '')\n machine: x86_64\n \n </denchmark-code>\n \n When I check my quantized model, the onnx graph has many more operations compared to the original  floating point model. I am not sure why that is and why cant we just use 8bit operations/operators, but as a results of this design choice, the 8bit model execution time is much more than the floating point model. It would be awsome if someone explain to me about these design choices.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "hossein1387", "commentT": "2019-09-29T08:29:13Z", "comment_text": "\n \t\tI have the same problem and same question. How can I get inference acceleration on onnxruntime?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "hossein1387", "commentT": "2019-10-30T21:16:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hossein1387>@hossein1387</denchmark-link>\n  :\n Regarding the extra node additions : ONNX does not have a lot of quantized operators yet so we need to resort to FP32 to 8 bit conversions in between... We are planning to add more ops to the quantized ops list which will improve this situation and we are also adding fusions to fuse these extra nodes into single nodes... Both of these will reduce the number of ops we add.\n Regarding achieving acceleration with quantized models... As part ort 1.0 release we added optimized kernels for matmul operations however optimized kernel work for convolutions is still in progress... Once this is done then the model should experience significant speedup than today...\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "hossein1387", "commentT": "2019-11-01T15:39:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/askhade>@askhade</denchmark-link>\n  Thank you very much for your response.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "hossein1387", "commentT": "2020-04-21T09:51:00Z", "comment_text": "\n \t\t\n Thanks for the update, I ran two models with and without quantization, both models are VGG like and both are using CIFAR100, here is some results:\n \u00a0\tModel1\n \u00a0\tFP32 Quantized\n Accuracy\t72.28%\t72.27%\n Exec Time\t14.4ms\t53.9ms\n Size\t77MB\t19MB\n \u00a0\tModel2\n \u00a0\tFP32 Quantized\n Accuracy\t73.99%\t73.97%\n Exec Time\t7.6ms\t50.29ms\n Size\t20MB\t5.2MB\n I dont understand why the quantized version is taking much longer than original model? Shouldn't the 8 bit quantized model take less than the FP32 model?\n \n I have the same issue, quantized model double lower and take more time\n \t\t"}}}, "commit": {"commit_id": "16087f313360d81d8a3c208101f7b5297d0c4315", "commit_author": "Ashwini Khade", "commitT": "2019-08-05 21:39:37-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "onnxruntime\\python\\tools\\quantization\\README.md", "file_new_name": "onnxruntime\\python\\tools\\quantization\\README.md", "file_complexity": {"file_NLOC": "None", "file_CCN": "None", "file_NToken": "None"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "65", "deleted_lines": "65"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "onnxruntime\\python\\tools\\quantization\\quantize.py", "file_new_name": "onnxruntime\\python\\tools\\quantization\\quantize.py", "file_complexity": {"file_NLOC": "593", "file_CCN": "124", "file_NToken": "5124"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "993", "deleted_lines": "993", "method_info": {"method_name": "quantize", "method_params": "model,per_channel,nbits,quantization_mode,static,asymmetric_input_types,input_quantization_params,output_quantization_params", "method_startline": "993", "method_endline": "1055", "method_complexity": {"method_NLOC": "16", "method_CCN": "3", "method_NToken": "129", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "993", "deleted_lines": "993", "method_info": {"method_name": "quantize", "method_params": "model,per_channel,nbits,quantization_mode,static,asymmetric_input_types,input_quantization_params,output_quantization_params", "method_startline": "993", "method_endline": "1055", "method_complexity": {"method_NLOC": "16", "method_CCN": "3", "method_NToken": "129", "method_nesting_level": "0"}}}}}}}}