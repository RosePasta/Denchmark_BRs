{"BR": {"BR_id": "1034", "BR_author": "GuanLuo", "BRopenT": "2019-05-15T03:31:36Z", "BRcloseT": "2019-06-04T23:32:28Z", "BR_text": {"BRsummary": "Can't Create Model Sessions on Different GPU", "BRdescription": "\n Describe the bug\n I am on a 2 GPU system, I tried to create one session on each GPU (so 2 sessions in total). I get the following error when it's creating the second session (i.e. creating the first session on GPU 0, and the second session on GPU1)\n <denchmark-code>Failed to get allocator for location: OrtAllocatorInfo: [ name:Cuda id:0 mem_type:0 type:1]\n </denchmark-code>\n \n Changing the creation order (first session on GPU1, second session on GPU0) will gives the same kind of error, but with different id\n <denchmark-code>Failed to get allocator for location: OrtAllocatorInfo: [ name:Cuda id:1 mem_type:0 type:1]\n </denchmark-code>\n \n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\n ONNX Runtime installed from (source or binary): source\n ONNX Runtime version: 0.4.0\n Python version:\n Visual Studio version (if applicable):\n GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\n CUDA/cuDNN version: 10.1.105 / 7.5.0.56\n GPU model and memory: Tesla P100, 16 GB\n \n To Reproduce\n \n Compile the code (modified from c_api_example) below, and run the executable\n the model file \"int32_int32_int32.onnx\" is just a simple model that takes two inputs, returns two outputs for addition and subtraction. Will provide the generation script if requested, but the model itself seems irrelevant to the issue\n \n <denchmark-code>#include <assert.h>\n #include <core/providers/cuda/cuda_provider_factory.h>\n #include <core/session/onnxruntime_c_api.h>\n #include <stdio.h>\n #include <stdlib.h>\n #include <iostream>\n #include <vector>\n \n //*****************************************************************************\n // helper function to check for status\n #define CHECK_STATUS(expr)                               \\\n   {                                                      \\\n     OrtStatus* onnx_status = (expr);                     \\\n     if (onnx_status != NULL) {                           \\\n       const char* msg = OrtGetErrorMessage(onnx_status); \\\n       fprintf(stderr, \"%s\\n\", msg);                      \\\n       OrtReleaseStatus(onnx_status);                     \\\n       exit(1);                                           \\\n     }                                                    \\\n   }\n \n int\n main(int argc, char* argv[])\n {\n   //===================== Env Scope ======================\n   // initialize  enviroment...one enviroment per process\n   // enviroment maintains thread pools and other state info\n   OrtEnv* env = nullptr;\n   // [TODO] look into OrtCreateEnvWithCustomLogger()\n   CHECK_STATUS(OrtCreateEnv(ORT_LOGGING_LEVEL_WARNING, \"test\", &env));\n \n   //===================== Model Scope ======================\n   // session option also specifies GPU device\n   // but other session option can be set \"globally\", just clone the session\n   // when setting actual device\n   OrtSessionOptions* session_options = OrtCreateSessionOptions();\n   OrtSetSessionThreadPoolSize(session_options, 1);\n \n   // disable graph optimization\n   OrtSetSessionGraphOptimizationLevel(session_options, 0);\n \n   //===================== Session Scope ======================\n   // create multiple sessions and load models into memory\n   std::vector<const char*> model_paths;\n   {\n     // two instances of the same model\n     model_paths.emplace_back();\n     model_paths.back() = \"int32_int32_int32.onnx\";\n     model_paths.emplace_back();\n     model_paths.back() = \"int32_int32_int32.onnx\";\n   }\n \n   std::vector<OrtSession*> sessions(model_paths.size());\n   for (size_t idx = 0; idx < model_paths.size(); idx++) {\n     // To deploy on different device, need to set provider on cloned option\n     // as you can remove a execution provider once it is appended\n     OrtSessionOptions* context_options =\n         OrtCloneSessionOptions(session_options);\n     \n     // Use CUDA, device 0. CPU if not set.\n     // Use \"idx\" as we know we have two models and two devices\n     OrtSessionOptionsAppendExecutionProvider_CUDA(context_options, idx);\n     std::cout << \"here\" << std::endl;\n     // [TODO] change session options for different session\n     CHECK_STATUS(OrtCreateSession(\n         env, model_paths[idx], context_options, &sessions[idx]));\n     std::cout << \"after here\" << std::endl;\n     // session-wise\n     OrtReleaseSessionOptions(context_options);\n   }\n }\n </denchmark-code>\n \n Expected behavior\n Expect to be able to create session on different GPU at the same time. Right now, I can create two sessions on the same GPU (GPU0 only or GPU1 only).\n Screenshots\n If applicable, add screenshots to help explain your problem.\n Additional context\n Add any other context about the problem here.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "GuanLuo", "commentT": "2019-05-15T17:22:38Z", "comment_text": "\n \t\tYes, it's a known problem.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "GuanLuo", "commentT": "2019-05-17T21:11:35Z", "comment_text": "\n \t\tWe should fix the following warning:\n <denchmark-code>/home/chasun/src/onnxruntime/onnxruntime/core/providers/cuda/cuda_allocator.h:24:13: warning: private field 'device_id_' is not used [-Wunused-private-field]\n   const int device_id_;\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "GuanLuo", "commentT": "2019-05-19T00:43:21Z", "comment_text": "\n \t\tI think the problem is in thread_local of per_thread_context in CUDA execution provider. <denchmark-link:https://github.com/GuanLuo>@GuanLuo</denchmark-link>\n  can you try create the two sessions in two different threads and see if the problem still exists?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "GuanLuo", "commentT": "2019-05-20T16:21:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/KeDengMS>@KeDengMS</denchmark-link>\n  So I just spawn a thread to execute ? Or the life time of the thread needs to be longer than that? Like once the session created, to run the model, I also need to run it in the same thread.\n Anyway, I will try that.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "GuanLuo", "commentT": "2019-05-28T18:07:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/GuanLuo>@GuanLuo</denchmark-link>\n , yes please try spawn a thread to execute OrtCreateSession for different devices. Your inference code should run on the same thread too. Please let me know what do you find.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "GuanLuo", "commentT": "2019-05-30T03:05:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/KeDengMS>@KeDengMS</denchmark-link>\n  Yes! It works if I spawn a new thread to execute OrtCreateSession, the following is what I changed in my code snap. Notice that I didn't run my inference code on the same thread, and everything seems fine. Is that expected? Or there is pitfall for doing that?\n auto status = std::async(std::launch::async, &OrtCreateSession,\n         env, model_paths[idx], context_options, &sessions[idx]);\n CHECK_STATUS(status.get());\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "GuanLuo", "commentT": "2019-05-30T06:07:55Z", "comment_text": "\n \t\tThanks for the confirmation, will work on a fix to it. Your workaround is OK, as long as the inference threads for different GPUs are separated.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "GuanLuo", "commentT": "2019-05-30T16:15:28Z", "comment_text": "\n \t\tI just realized I wasn't being clear. I mean my inference code is run on the main thread (sessions are created on different threads via async), and the inference code send one request to each session.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "GuanLuo", "commentT": "2019-05-31T23:51:38Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/GuanLuo>@GuanLuo</denchmark-link>\n , I found your <denchmark-link:https://github.com/NVIDIA/tensorrt-inference-server/commit/fbbbeefd24ee537b3074bd055444755fd7889b81>workaround</denchmark-link>\n . Will try my fix with TRTIS later.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "GuanLuo", "commentT": "2019-06-04T23:37:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/GuanLuo>@GuanLuo</denchmark-link>\n  can you try the fix and see if your issue is solved?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "GuanLuo", "commentT": "2019-06-05T02:25:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/KeDengMS>@KeDengMS</denchmark-link>\n  I tried with my sample code and the issue is resolved! I can deploy models on different GPUs without using threading. However, I am not going to try it on our server code until there is new release version for ONNX Runtime, hope you understand that.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "GuanLuo", "commentT": "2019-06-05T18:42:23Z", "comment_text": "\n \t\tAwesome, thanks for the confirmation.\n \t\t"}}}, "commit": {"commit_id": "7c4494a0bc3baad95a783c6759c753f6b5a51b5e", "commit_author": "KeDengMS", "commitT": "2019-06-04 16:32:27-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cuda\\cuda_allocator.cc", "file_new_name": "onnxruntime\\core\\providers\\cuda\\cuda_allocator.cc", "file_complexity": {"file_NLOC": "51", "file_CCN": "12", "file_NToken": "302"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "23", "deleted_lines": "23", "method_info": {"method_name": "onnxruntime::CUDAAllocator::CheckDevice", "method_params": "", "method_startline": "17", "method_endline": "25", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "29", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cuda\\cuda_allocator.h", "file_new_name": "onnxruntime\\core\\providers\\cuda\\cuda_allocator.h", "file_complexity": {"file_NLOC": "24", "file_CCN": "1", "file_NToken": "158"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "14", "deleted_lines": "14", "method_info": {"method_name": "onnxruntime::CUDAAllocator::CUDAAllocator", "method_params": "device_id", "method_startline": "14", "method_endline": "14", "method_complexity": {"method_NLOC": "1", "method_CCN": "1", "method_NToken": "20", "method_nesting_level": "2"}}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 7, "file_old_name": "onnxruntime\\core\\providers\\cuda\\cuda_execution_provider.cc", "file_new_name": "onnxruntime\\core\\providers\\cuda\\cuda_execution_provider.cc", "file_complexity": {"file_NLOC": "1002", "file_CCN": "93", "file_NToken": "12704"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "117,118,119,120,121,122,123,124,125,126,127,128", "deleted_lines": "117,118,119,120,121,122,123,124,125", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::ReleasePerThreadStuffs", "method_params": "", "method_startline": "117", "method_endline": "130", "method_complexity": {"method_NLOC": "13", "method_CCN": "5", "method_NToken": "89", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "49,50,51,52,53", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::PerThreadContext::PerThreadContext", "method_params": "device_id", "method_startline": "45", "method_endline": "54", "method_complexity": {"method_NLOC": "9", "method_CCN": "1", "method_NToken": "85", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "182,183", "deleted_lines": "170,171,172,173,174,175,176,177,178,179,180,187", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::OnRunStart", "method_params": "", "method_startline": "161", "method_endline": "187", "method_complexity": {"method_NLOC": "23", "method_CCN": "5", "method_NToken": "169", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "152", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::AddDeferredReleaseCPUPtr", "method_params": "p", "method_startline": "148", "method_endline": "159", "method_complexity": {"method_NLOC": "9", "method_CCN": "2", "method_NToken": "70", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "137", "deleted_lines": "140", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::GetAllocator", "method_params": "id,mem_type", "method_startline": "132", "method_endline": "141", "method_complexity": {"method_NLOC": "7", "method_CCN": "2", "method_NToken": "43", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": "99,100,101,103,104,105,107,108,109,110,111,112,113,114,115", "deleted_lines": "99,101,103,104,113,114,115", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::GetPerThreadContext", "method_params": "", "method_startline": "99", "method_endline": "115", "method_complexity": {"method_NLOC": "16", "method_CCN": "4", "method_NToken": "131", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": "191", "deleted_lines": "189", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::OnRunEnd", "method_params": "", "method_startline": "189", "method_endline": "197", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "59", "method_nesting_level": "1"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "onnxruntime\\core\\providers\\cuda\\cuda_execution_provider.h", "file_new_name": "onnxruntime\\core\\providers\\cuda\\cuda_execution_provider.h", "file_complexity": {"file_NLOC": "115", "file_CCN": "18", "file_NToken": "711"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "46", "deleted_lines": "46,47", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::PerThreadCublasHandle", "method_params": "", "method_startline": "45", "method_endline": "47", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "13", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "129,130,131", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::PerThreadContext::GetAllocator", "method_params": "", "method_startline": "129", "method_endline": "131", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "3"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "60", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::GetConstOnes", "method_params": "count", "method_startline": "59", "method_endline": "61", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "21", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "50", "deleted_lines": "49", "method_info": {"method_name": "onnxruntime::CUDAExecutionProvider::PerThreadCudnnHandle", "method_params": "", "method_startline": "49", "method_endline": "51", "method_complexity": {"method_NLOC": "3", "method_CCN": "1", "method_NToken": "14", "method_nesting_level": "2"}}}}}}}}