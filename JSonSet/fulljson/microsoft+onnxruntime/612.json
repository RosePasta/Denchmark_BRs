{"BR": {"BR_id": "612", "BR_author": "xadupre", "BRopenT": "2019-03-13T15:33:45Z", "BRcloseT": "2019-04-09T17:55:32Z", "BR_text": {"BRsummary": "TfidfVectorizer fails with empty strings", "BRdescription": "\n Describe the bug\n I'm trying to write a converter for TfIdfVectorizer when analyser=='char'. I use the regular expression to split a sentance into characters with '.' (does not work) but '[^\\n]' works. With the first one, I get the following error:\n INVALID_ARGUMENT : Input shape must have either [C] or [B,C] dimensions where C > 0 and B > 0\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\n ONNX Runtime installed from (source or binary): master branch from github\n ONNX Runtime version: master branch from github\n Python version: 3.7\n \n To Reproduce\n \n clone https://github.com/xadupre/sklearn-onnx/tree/tfc\n run test test_model_tfidf_vectorizer11_dot in tests/test_SklearnTfidfVectorizerConverterChar.py\n \n Expected behavior\n onnxruntime should return outputs.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xadupre", "commentT": "2019-03-13T17:05:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/xadupre>@xadupre</denchmark-link>\n  There are few things I want you to note.\n \n \n Current implementation of tokenexp is the opposite of what it should be. I have created a PR below.\n \n \n If you want to split something into individual characters the Tokenizer spec provides for a char tokenization mode which is enabled when you supply a single empty string as separator.\n \n \n The rest below I believe are python related issues.\n \n Per spec Tokenizer accepts UTF-8 strings for both input, output and attributes. This applies as well to StringNormalizer and TfIdfVectorizer.  It looks like pybind11 will automatically convert python strings to utf-8, so it should be good.\n Per spec the regex supported (or intended to be supported) is BRE\n The above error message is a result of TypeAndShapeInference function failure. It expects input as  Tensor of strings with the aforementioned dimensions. Will check what our python expects.\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xadupre", "commentT": "2019-03-13T22:26:15Z", "comment_text": "\n \t\tI have submitted <denchmark-link:https://github.com/microsoft/onnxruntime/pull/617>#617</denchmark-link>\n  to address this.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "xadupre", "commentT": "2019-03-14T17:07:30Z", "comment_text": "\n \t\tI tried but that does not fix the issue. StringNormalizer can handle empty strings, Tokenizer too, but not TfidfVectorizer. It should handle empty strings and return an empty results.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "xadupre", "commentT": "2019-03-14T17:12:58Z", "comment_text": "\n \t\tAnother issue maybe introduced with changes <denchmark-link:https://github.com/microsoft/onnxruntime/pull/617>#617</denchmark-link>\n  is the following.\n <denchmark-code>corpus = numpy.array([\n         'This is the first document.',\n         'This document is the second document.',\n         ]).reshape((2, 1))\n vect = TfidfVectorizer(ngram_range=(1, 1), norm=None,\n                        analyzer='word', token_pattern=\".{1,2}\")\n vect.fit(corpus.ravel())\n pred = vect.transform(corpus.ravel())\n model_onnx = convert_sklearn(vect, 'TfidfVectorizer',\n                              [('input', StringTensorType([1, 1]))])\n </denchmark-code>\n \n The ONNX pipeline includes a couple of nodes which produce the following outputs\n <denchmark-code>-------------- normalized (StringNormalizer)\n ['this is the first document.']\n -------------- tokenized (Tokenizer)\n [['th' 'is' ' i' 's ' 'th' 'e ' 'fi' 'rs' 't ' 'do' 'cu' 'me' 'nt' '.']]\n -------------- flattened (Flatten)\n [['th' 'is' ' i' 's ' 'th' 'e ' 'fi' 'rs' 't ' 'do' 'cu' 'me' 'nt' '.']]\n -------------- tfidfvectorizer (TfidfVectorizer with regex = '.{1,2}'\n 2019-03-14 18:08:44.0281985 [E:onnxruntime:InferenceSession, inference_session.cc:541 onnxruntime::InferenceSession::Impl::Initialize] Exception during initialization: onnxruntime\\onnxruntime\\core\\providers\\cpu\\nn\\tfidfvectorizer.cc:371 onnxruntime::TfIdfVectorizer::TfIdfVectorizer (before_insert + ngrams) == impl_->str_set_.size() was false. poll_strings duplicate 1-grams detected\n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "xadupre", "commentT": "2019-03-15T16:58:04Z", "comment_text": "\n \t\tI found two bugs while converting a TfIdfVectorizer. The first one comes from scikit-learn which creates ambiguities when trying to retrieve n-grams if the tokens contain spaces. The other bug comes from the tokenizer which loses one dimension if the expression to tokenize is empty. The code is the following:\n <denchmark-code>corpus = np.array([\n         'This is the first document.',\n         'This document is the second document.',\n         'And this is the third one.',\n         ' ',\n         ]).reshape((4, 1))\n vect = TfidfVectorizer(ngram_range=(1, 2), norm=None)\n vect.fit(corpus.ravel())\n pred = vect.transform(corpus.ravel())\n from skl2onnx import convert_sklearn\n from skl2onnx.common.data_types import StringTensorType\n \n model_onnx = convert_sklearn(vect, 'TfidfVectorizer',\n                              [('input', StringTensorType([1, 1]))])\n </denchmark-code>\n \n I then checked the intermediate outputs for two strings, an non-empty string and an empty string (differencs in yellow). When the string is empty, the function return an null dimension.\n <denchmark-code>  if (max_tokens == 0) {\n     output_dims.push_back(0);\n     TensorShape output_shape(output_dims);\n     ctx->Output(0, output_shape);\n     return Status::OK();\n   }\n </denchmark-code>\n \n But if mark_ is true, we would expect to have that string surrounded by start/end markers (this code is after the first one).\n <denchmark-code>  if (mark_) {\n     max_tokens += 2;  // Start/end markers as separate tokens\n   }\n </denchmark-code>\n \n I compared the results for the intermediate nodes (graph is below)\n with input: [array(['And this is the third one.'], dtype='<U37')]\n <denchmark-code>-> node 'normalized'\n ['and this is the third one.']\n -> node 'tokenized'\n [['and' 'this' 'is' 'the' 'third' 'one']]\n -> node 'flattened'\n [['and' 'this' 'is' 'the' 'third' 'one']]\n -> node 'variable'\n [[1.9162908 1.9162908 0.        0.        0.        0.        1.2231436\n   1.2231436 1.9162908 0.        0.        1.2231436 0.        0.\n   1.9162908 1.9162908 1.9162908 1.2231436 0.        1.5108256]]\n </denchmark-code>\n \n with input: [array([' '], dtype='<U37')]\n <denchmark-code>-> node 'normalized'\n [' ']\n -> node 'tokenized'\n 2019-03-15 17:37:20.3345248 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes\n []\n -> node 'flattened'\n 2019-03-15 17:37:20.3356089 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes\n 2019-03-15 17:37:20.3356310 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes\n []\n -> node 'variable'\n </denchmark-code>\n \n for the following pipeline...\n <denchmark-link:https://user-images.githubusercontent.com/22452781/54448320-b9647a80-474b-11e9-9242-e3ac0d70ba5f.png></denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "xadupre", "commentT": "2019-03-22T23:03:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/xadupre>@xadupre</denchmark-link>\n  I think we discussed this sufficiently with Wei-Sheng. Pls, talk to me if you feel there still an issue with regards to empty strings.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "xadupre", "commentT": "2019-03-25T22:17:08Z", "comment_text": "\n \t\tSupporting empty input shape in TfidfVectorizer looks reasonable to me. For example, if we tokenized a document (i.e., a very long string) into a [C]-tensor, but unfortunately remove all the elements in StringNormalizer, and finally TfidfVectorizer gets [0]-tensor. What should be the TFIDF-representation of this document? I'd guess that representation is a [D]-vector where all elements are zeros, because the count of every vocabulary is 0. Note that D is the dictionary's max index in TfidfVectorizer.\n \t\t"}}}, "commit": {"commit_id": "ccd7e801a05d9819ba43386b0623c988f3dd5c72", "commit_author": "Xavier Dupr\u00e9", "commitT": "2019-04-09 10:55:24-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cpu\\nn\\tfidfvectorizer.cc", "file_new_name": "onnxruntime\\core\\providers\\cpu\\nn\\tfidfvectorizer.cc", "file_complexity": {"file_NLOC": "471", "file_CCN": "89", "file_NToken": "3593"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "460,462,466,474,475,476,477,478,479,480,481,482,483,484,485", "deleted_lines": "456,457,458,459,464,466,470,473,474", "method_info": {"method_name": "onnxruntime::TfIdfVectorizer::ComputeImpl", "method_params": "ctx", "method_startline": "437", "method_endline": "563", "method_complexity": {"method_NLOC": "101", "method_CCN": "20", "method_NToken": "667", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "onnxruntime\\test\\providers\\cpu\\nn\\tfidfvectorizer_test.cc", "file_new_name": "onnxruntime\\test\\providers\\cpu\\nn\\tfidfvectorizer_test.cc", "file_complexity": {"file_NLOC": "500", "file_CCN": "29", "file_NToken": "5536"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip01_Empty_Dim2", "method_startline": "139", "method_endline": "159", "method_complexity": {"method_NLOC": "17", "method_CCN": "1", "method_NToken": "179", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim1Fail", "method_startline": "73", "method_endline": "93", "method_complexity": {"method_NLOC": "17", "method_CCN": "1", "method_NToken": "164", "method_nesting_level": "2"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim2", "method_startline": "117", "method_endline": "137", "method_complexity": {"method_NLOC": "17", "method_CCN": "1", "method_NToken": "179", "method_nesting_level": "2"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim1Success", "method_startline": "95", "method_endline": "115", "method_complexity": {"method_NLOC": "17", "method_CCN": "1", "method_NToken": "177", "method_nesting_level": "2"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182", "deleted_lines": null, "method_info": {"method_name": "onnxruntime::test::TEST", "method_params": "TfIdfVectorizerTest,Int32_TF_onlyBigrams_Skip0_Empty_Dim2N", "method_startline": "161", "method_endline": "182", "method_complexity": {"method_NLOC": "18", "method_CCN": "1", "method_NToken": "195", "method_nesting_level": "2"}}}}}}}}