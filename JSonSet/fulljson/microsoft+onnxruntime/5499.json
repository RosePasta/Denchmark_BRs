{"BR": {"BR_id": "5499", "BR_author": "j-paulus", "BRopenT": "2020-10-15T10:53:35Z", "BRcloseT": "2020-11-24T19:14:42Z", "BR_text": {"BRsummary": "Augmented nodes in calibration for static quantization returning float when something expects int64", "BRdescription": "\n Describe the bug\n I have converted a model from torch to ONNX (opset=11) and now experimenting with quantization to see potential benefits.\n \n Unquantized model works.\n Dynamic quantization works and the resulting model works.\n Static quantization does not work, but crashes as the routines try to load the augmented model for collecting calibration data.\n \n The full stack trace is:\n <denchmark-code>  File \"/Users/name/opt/anaconda3/envs/onnx/lib/python3.7/site-packages/onnxruntime/quantization/quantize.py\", line 171, in quantize_static\n     nodes_to_exclude)\n   File \"/Users/name/opt/anaconda3/envs/onnx/lib/python3.7/site-packages/onnxruntime/quantization/calibrate.py\", line 258, in calibrate\n     dict_for_quantization = calibrater.get_intermediate_outputs()\n   File \"/Users/name/opt/anaconda3/envs/onnx/lib/python3.7/site-packages/onnxruntime/quantization/calibrate.py\", line 110, in get_intermediate_outputs\n     session = onnxruntime.InferenceSession(self.augmented_model_path, None)\n   File \"/Users/name/opt/anaconda3/envs/onnx/lib/python3.7/site-packages/onnxruntime/capi/session.py\", line 195, in __init__\n     self._create_inference_session(providers, provider_options)\n   File \"/Users/name/opt/anaconda3/envs/onnx/lib/python3.7/site-packages/onnxruntime/capi/session.py\", line 200, in _create_inference_session\n     sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)\n onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from augmented_model.onnx failed:Type Error: Type (tensor(float)) of output arg (230_ReduceMin) of node (230_ReduceMin) does not match expected type (tensor(int64)).\n </denchmark-code>\n \n The node name (\"230_ReduceMin\") varies depending on the run, so I assume the same problem is in all of them or the routine using the model. To my understanding, these nodes are the ones added by ONNXCalibrater.augment_graph(). In that function the added nodes have type TensorProto.FLOAT, e.g., line 81:\n            added_outputs.append(helper.make_tensor_value_info(reduce_min_node.output[0], TensorProto.FLOAT, ()))\n Why is some part of the code expecting int64 datatype when the augmented nodes explicitly use float?\n Urgency\n Not urgent.\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\n ONNX Runtime installed from (source or binary): binary from pip\n ONNX Runtime version: 1.5.1\n Python version: 3.7.9\n Visual Studio version (if applicable): N/A\n GCC/Compiler version (if compiling from source): N/A\n CUDA/cuDNN version: N/A\n GPU model and memory: N/A\n \n To Reproduce\n Unable to attach the real model, but will try to create a minimal example triggering the bug.\n The code relevant to the quantization call is:\n <denchmark-code>cdr = CalibrationDataProvider()\n \n quantize_static(model_input='model.onnx', model_output='model_quant.onnx', calibration_data_reader=cdr)\n </denchmark-code>\n \n The implementation of CalibrationDataProvider is irrelevant as it is not called before the crash occurs.\n Expected behavior\n Model quantization to run without crashing.\n Screenshots\n If applicable, add screenshots to help explain your problem.\n Additional context\n Add any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.\n Edit: the number in the node is the original name\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "j-paulus", "commentT": "2020-10-16T09:30:19Z", "comment_text": "\n \t\tUpdate: Same crash when using 1.5.2.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "j-paulus", "commentT": "2020-10-23T18:57:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/j-paulus>@j-paulus</denchmark-link>\n , thanks for reporting the issue. Could you please share the model for debugging?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "j-paulus", "commentT": "2020-10-26T07:26:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yufenglee>@yufenglee</denchmark-link>\n , Unfortunately, I am not allowed to share the real model where I'm encountering this issue. And since this is of a non-urgent priority, I haven't had the time to create a minimal model exhibiting the problem. This is still on my todo-list, but I cannot promise anything.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "j-paulus", "commentT": "2020-10-26T13:18:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yufenglee>@yufenglee</denchmark-link>\n , This code below should trigger the error.\n It appears to be related to inquiring tensor shape and using that in operations. The size if stored as int64 tensors and since the quantization statistics computation blindly assumes everything to be float, there will be problems.\n Please note, that the issue is not only for reshape(), but also other operations using the size information as a variable. This was just an easy way to demonstrate the problem.\n <denchmark-code>import numpy as np\n import torch\n from onnxruntime.quantization import quantize_dynamic, quantize_static, QuantType\n from onnxruntime.quantization.calibrate import CalibrationDataReader\n \n class CalibrationDataProvider(CalibrationDataReader):\n     def __init__(self):\n         super(CalibrationDataProvider, self).__init__()\n         self.counter = 0\n \n     def get_next(self):\n         if self.counter > 2:\n             return None\n         else:\n             self.counter += 1\n             return {'x': np.random.randn(2, 4).astype(np.float32)}\n \n class Model(torch.nn.Module):\n     def __init__(self):\n         super().__init__()\n         self.n = int(1)\n \n     def forward(self, x):\n         f = x.shape[0]\n         y = x.reshape(-1, f)\n         return y\n \n model = Model().float()\n \n dummy_input = (torch.randn(2, 4), )\n torch.onnx.export(\n     model,\n     dummy_input,\n     'model.onnx',\n     input_names=('x',),\n     export_params=True,\n     training=False,\n     opset_version=11)\n \n cdr = CalibrationDataProvider()\n \n quantize_static(model_input='model.onnx',\n                 model_output='model_q.onnx',\n                 calibration_data_reader=cdr)\n \n </denchmark-code>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "j-paulus", "commentT": "2020-11-24T19:14:42Z", "comment_text": "\n \t\tIssue was fixed with <denchmark-link:https://github.com/microsoft/onnxruntime/pull/5704>#5704</denchmark-link>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "j-paulus", "commentT": "2020-12-15T08:28:20Z", "comment_text": "\n \t\tNow that 1.6.0 including the fix was released, I was able to verify that the bug was fixed. Thank you!\n \t\t"}}}, "commit": {"commit_id": "5c4543e194be0b7ff753b24e5d5c32949fbde5c3", "commit_author": "Yufeng Li", "commitT": "2020-11-04 23:55:48-08:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\python\\tools\\quantization\\calibrate.py", "file_new_name": "onnxruntime\\python\\tools\\quantization\\calibrate.py", "file_complexity": {"file_NLOC": "149", "file_CCN": "48", "file_NToken": "1183"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "58,59,60,70,71,72,73,74,75", "deleted_lines": "66,67,68,69,70,71", "method_info": {"method_name": "augment_graph", "method_params": "self", "method_startline": "50", "method_endline": "99", "method_complexity": {"method_NLOC": "34", "method_CCN": "13", "method_NToken": "339", "method_nesting_level": "1"}}}}}}}}