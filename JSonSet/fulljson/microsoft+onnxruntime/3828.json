{"BR": {"BR_id": "3828", "BR_author": "harrysummer", "BRopenT": "2020-05-05T20:10:52Z", "BRcloseT": "2020-05-06T07:05:17Z", "BR_text": {"BRsummary": "cuDNN error when running multi-thread evaluation", "BRdescription": "\n We have some code to call the same InferenceSession in multiple threads. And we randomly see the following crashes:\n \n Non-zero status code returned while running Conv node. Name:'onnx_op_2' Status Message: CUDNN error executing cudnnConvolutionForward(CudnnHandle(), &alpha, s_.x_tensor, x_data, s_.filter_desc, w_data, s_.conv_desc, s_.algo, workspace.get(), s_.workspace_bytes, &beta, s_.y_tensor, y_data)\n \n \n Non-zero status code returned while running Conv node. Name:'' Status Message: CUDNN error executing cudnnAddTensor(CudnnHandle(), &alpha, s_.b_tensor, b_data, &alpha, s_.y_tensor, y_data)\n \n We figured out this is a thread-safety problem in CUDA covolution implementation. Here are the detail explanation:\n In the compute function of Conv, a mutex guards the following block in cuda/nn/conv.cc:\n \n \n \n onnxruntime/onnxruntime/core/providers/cuda/nn/conv.cc\n \n \n         Lines 55 to 169\n       in\n       5dfc91d\n \n \n \n \n \n \n  { \n \n \n \n    std::lock_guard<OrtMutex> lock(s_.mutex); \n \n \n \n  // TODO: add a global cache if need to handle cases for multiple frames running simultaneuously with different batch_size \n \n \n \n  bool input_dims_changed = (s_.last_x_dims != x_dims); \n \n \n \n  bool w_dims_changed = (s_.last_w_dims != w_dims); \n \n \n \n  if (input_dims_changed || w_dims_changed) { \n \n \n \n  if (input_dims_changed) \n \n \n \n        s_.last_x_dims = x_dims; \n \n \n \n  \n \n \n \n  if (w_dims_changed) { \n \n \n \n        s_.last_w_dims = w_dims; \n \n \n \n        s_.cached_benchmark_results.clear(); \n \n \n \n      } \n \n \n \n  \n \n \n \n  const int64_t N = X->Shape()[0]; \n \n \n \n  const int64_t M = W->Shape()[0]; \n \n \n \n  \n \n \n \n  ORT_RETURN_IF_ERROR(conv_attrs_.ValidateInputShape(X, W)); \n \n \n \n  \n \n \n \n      std::vector<int64_t> kernel_shape; \n \n \n \n  ORT_RETURN_IF_ERROR(conv_attrs_.ComputeKernelShape(W->Shape(), kernel_shape)); \n \n \n \n  auto rank = kernel_shape.size(); \n \n \n \n      std::vector<int64_t> pads(conv_attrs_.pads); \n \n \n \n  if (pads.empty()) { \n \n \n \n        pads.resize(rank * 2, 0); \n \n \n \n      } \n \n \n \n      std::vector<int64_t> dilations(conv_attrs_.dilations); \n \n \n \n  if (dilations.empty()) { \n \n \n \n        dilations.resize(rank, 1); \n \n \n \n      } \n \n \n \n      std::vector<int64_t> strides(conv_attrs_.strides); \n \n \n \n  if (strides.empty()) { \n \n \n \n        strides.resize(rank, 1); \n \n \n \n      } \n \n \n \n  \n \n \n \n      std::vector<int64_t> y_dims; \n \n \n \n      y_dims.insert(y_dims.begin(), {N, M}); \n \n \n \n  ORT_RETURN_IF_ERROR(conv_attrs_.InferOutputShape<true>(x_shape.Slice(2), kernel_shape, \n \n \n \n                                                             strides, dilations, &pads, &y_dims)); \n \n \n \n      s_.y_dims = y_dims; \n \n \n \n      Tensor* Y = context->Output(0, TensorShape(s_.y_dims)); \n \n \n \n      y_data = reinterpret_cast<CudaT*>(Y->template MutableData<T>()); \n \n \n \n  \n \n \n \n  // special case when there is a dim value of 0 in the shape. \n \n \n \n  if (Y->Shape().Size() == 0) \n \n \n \n  return Status::OK(); \n \n \n \n  \n \n \n \n      std::vector<int64_t> x_dims_cudnn = x_dims; \n \n \n \n      std::vector<int64_t> y_dims_cudnn = y_dims; \n \n \n \n  if (rank < 2) { \n \n \n \n  // cudnn only takes 4D or 5D input, so pad dimensions if needed \n \n \n \n        x_dims_cudnn.push_back(1); \n \n \n \n        y_dims_cudnn.push_back(1); \n \n \n \n        w_dims.push_back(1); \n \n \n \n        pads.insert(pads.begin() + rank, 0); \n \n \n \n        pads.insert(pads.end(), 0); \n \n \n \n        kernel_shape.push_back(1); \n \n \n \n        strides.push_back(1); \n \n \n \n        dilations.push_back(1); \n \n \n \n      } \n \n \n \n  ORT_RETURN_IF_ERROR(s_.x_tensor.Set(x_dims_cudnn, CudnnTensor::GetDataType<CudaT>())); \n \n \n \n  ORT_RETURN_IF_ERROR(s_.y_tensor.Set(y_dims_cudnn, CudnnTensor::GetDataType<CudaT>())); \n \n \n \n  \n \n \n \n  if (w_dims_changed) \n \n \n \n  ORT_RETURN_IF_ERROR(s_.filter_desc.Set(w_dims, CudnnTensor::GetDataType<CudaT>())); \n \n \n \n  \n \n \n \n      cudnnConvolutionMode_t mode = CUDNN_CROSS_CORRELATION; \n \n \n \n  ORT_RETURN_IF_ERROR(s_.conv_desc.Set(kernel_shape.size(), pads, strides, dilations, \n \n \n \n                                           mode, CudnnTensor::GetDataType<CudaT>())); \n \n \n \n  CUDNN_RETURN_IF_ERROR(cudnnSetConvolutionGroupCount(s_.conv_desc, gsl::narrow_cast<int>(conv_attrs_.group))); \n \n \n \n  \n \n \n \n  if (has_bias) { \n \n \n \n  const Tensor* B = context->Input<Tensor>(2); \n \n \n \n  const auto& b_shape = B->Shape(); \n \n \n \n  ORT_RETURN_IF_NOT(b_shape.NumDimensions() == 1, \"bias should be 1D\"); \n \n \n \n        std::vector<int64_t> b_dims(2 + kernel_shape.size()); \n \n \n \n        b_dims[0] = 1;           // N \n \n \n \n        b_dims[1] = b_shape[0];  // C \n \n \n \n  for (size_t i = 0; i < kernel_shape.size(); i++) b_dims[2 + i] = 1; \n \n \n \n  \n \n \n \n  ORT_RETURN_IF_ERROR(s_.b_tensor.Set(b_dims, CudnnTensor::GetDataType<CudaT>())); \n \n \n \n      } \n \n \n \n  \n \n \n \n  if (!s_.cached_benchmark_results.contains(x_dims_cudnn)) { \n \n \n \n        IAllocatorUniquePtr<void> algo_search_workspace = GetScratchBuffer<void>(AlgoSearchWorkspaceSize); \n \n \n \n  \n \n \n \n  // set math type to tensor core before algorithm search \n \n \n \n  if (std::is_same<T, MLFloat16>::value) \n \n \n \n  CUDNN_RETURN_IF_ERROR(cudnnSetConvolutionMathType(s_.conv_desc, CUDNN_TENSOR_OP_MATH)); \n \n \n \n  \n \n \n \n        cudnnConvolutionFwdAlgoPerf_t perf; \n \n \n \n  int algo_count = 1; \n \n \n \n  CUDNN_RETURN_IF_ERROR(cudnnFindConvolutionForwardAlgorithmEx( \n \n \n \n  CudnnHandle(), \n \n \n \n            s_.x_tensor, \n \n \n \n            x_data, \n \n \n \n            s_.filter_desc, \n \n \n \n            w_data, \n \n \n \n            s_.conv_desc, \n \n \n \n            s_.y_tensor, \n \n \n \n            y_data, \n \n \n \n  1, \n \n \n \n            &algo_count, \n \n \n \n            &perf, \n \n \n \n            algo_search_workspace.get(), \n \n \n \n            AlgoSearchWorkspaceSize)); \n \n \n \n        s_.cached_benchmark_results.insert(x_dims_cudnn, {perf.algo, perf.memory, perf.mathType}); \n \n \n \n      } \n \n \n \n  \n \n \n \n  const auto& perf = s_.cached_benchmark_results.at(x_dims_cudnn); \n \n \n \n  CUDNN_RETURN_IF_ERROR(cudnnSetConvolutionMathType(s_.conv_desc, perf.mathType)); \n \n \n \n      s_.algo = perf.algo; \n \n \n \n      s_.workspace_bytes = perf.memory; \n \n \n \n    } \n \n \n \n  } \n \n \n \n \n \n However, just below this block, there are calls to cuDNN API using the shared cuDNN state s_:\n \n \n \n onnxruntime/onnxruntime/core/providers/cuda/nn/conv.cc\n \n \n          Line 185\n       in\n       5dfc91d\n \n \n \n \n \n \n  CUDNN_RETURN_IF_ERROR(cudnnConvolutionForward(CudnnHandle(), \n \n \n \n \n \n \n \n \n onnxruntime/onnxruntime/core/providers/cuda/nn/conv.cc\n \n \n          Line 202\n       in\n       5dfc91d\n \n \n \n \n \n \n  CUDNN_RETURN_IF_ERROR(cudnnAddTensor(CudnnHandle(), &alpha, s_.b_tensor, b_data, &alpha, s_.y_tensor, y_data)); \n \n \n \n \n \n Therefore, when multi-thread evaluation hits cudnnConvolutionForward or cudnnAddTensor at the same time, the cuDNN call may fail when the shapes of tensors are different on different thread. Our solution for this is extend the mutex scope to include to two cuDNN calls.\n \t"}, "comments": {}}, "commit": {"commit_id": "b45ce925428efce1008b40adb9603cfb57500306", "commit_author": "Hariharan Seshadri", "commitT": "2020-05-06 00:05:15-07:00", "commit_complexity": {"commit_NLOC": "None", "commit_CCN": "None", "commit_Nprams": "None"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "onnxruntime\\core\\providers\\cuda\\nn\\conv.cc", "file_new_name": "onnxruntime\\core\\providers\\cuda\\nn\\conv.cc", "file_complexity": {"file_NLOC": "185", "file_CCN": "24", "file_NToken": "1578"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "170,171,172,173,174,176,177,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202", "deleted_lines": "169,171,172,173,174,175,177,178,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202", "method_info": {"method_name": "onnxruntime::cuda::Conv<T>::ComputeInternal", "method_params": "context", "method_startline": "37", "method_endline": "206", "method_complexity": {"method_NLOC": "138", "method_CCN": "18", "method_NToken": "1292", "method_nesting_level": "2"}}}}}}}}