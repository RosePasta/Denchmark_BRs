{"BR": {"BR_id": "1900", "BR_author": "jimthompson5802", "BRopenT": "2020-05-31T01:19:30Z", "BRcloseT": "2020-06-02T23:17:03Z", "BR_text": {"BRsummary": "BeamSearchDecoder with non LSTM cells raises ValueError exception", "BRdescription": "\n System information\n \n OS Platform and Distribution: Tensorflow Docker Container based on image tensorflow/tensorflow:2.2.0-jupyter\n TensorFlow version and how it was installed (source or binary): 2.2.0\n TensorFlow-Addons version and how it was installed (source or binary): tfa-nightly  0.11.0.dev20200520032238\n Python version: 3.6.9\n Is GPU used? (yes/no): no\n \n Describe the bug\n Working with <denchmark-link:https://github.com/w4nderlust>@w4nderlust</denchmark-link>\n  on this PR <denchmark-link:https://github.com/ludwig-ai/ludwig/pull/699>ludwig-ai/ludwig#699</denchmark-link>\n   we encountered an issue with seq2seq beam search.  If we use  or  with beam search and no Atttention we see this error:\n <denchmark-code>ValueError: The two structures don't have the same nested structure.\n First structure: type=list str=[<tf.Tensor: shape=(384, 256), dtype=float32, numpy=\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]\n Second structure: type=int str=256\n More specifically: Substructure \"type=list str=[<tf.Tensor: shape=(384, 256), dtype=float32, numpy=\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>]\" is a sequence, while substructure \"type=int str=256\" is not\n \n </denchmark-code>\n \n No error occurs when we use LSTMCell with beam search and no Attention.\n Refer this <denchmark-link:https://github.com/uber/ludwig/pull/699#issuecomment-636403380>posting</denchmark-link>\n  for more context.\n Code to reproduce the issue\n Minimal Working example (NOTE: updated minimal working example to simplify code)\n <denchmark-code>import numpy as np\n import tensorflow as tf\n from tensorflow import keras\n import tensorflow_addons as tfa\n \n # ValueError exception raised for both SimpleRNNCell and GRUCell\n \n #DECODER_CELL_TYPE = keras.layers.SimpleRNNCell  # Raises ValueError Exception\n #DECODER_CELL_TYPE = keras.layers.GRUCell        # Raises ValueError Exception\n DECODER_CELL_TYPE = keras.layers.LSTMCell        # Does not raise ValueError Exception\n \n VOCAB_SIZE = 100\n EMBED_SIZE = 10\n RNN_UNITS = 256\n INPUT_SEQUENCE_SIZE = 10\n OUTPUT_SEQUENCE_SIZE = 15\n BATCH_SIZE = 128\n BEAM_WIDTH = 3\n SIMULATE_LSTM_ENCODER = False\n \n #============== simulate output from encoder ========================\n encoder_outputs = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)\n if SIMULATE_LSTM_ENCODER:\n     encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32),\n                      tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]\n else:\n     encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]\n \n \n \n # ================ Setup Decoder =====================\n embeddings_dec = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE)\n decoder_cell = DECODER_CELL_TYPE(RNN_UNITS)\n output_layer = keras.layers.Dense(VOCAB_SIZE)\n \n GO_SYMBOL = VOCAB_SIZE - 1\n END_SYMBOL = 0\n batch_size = BATCH_SIZE\n encoder_sequence_length = tf.convert_to_tensor(\n     np.array([INPUT_SEQUENCE_SIZE] * BATCH_SIZE),\n     tf.int32\n )\n \n decoder_input = tf.expand_dims(\n     [GO_SYMBOL] * batch_size, 1)\n start_tokens = tf.fill([batch_size], GO_SYMBOL)\n end_token = END_SYMBOL\n \n decoder_emb_inp = embeddings_dec(decoder_input)\n \n if DECODER_CELL_TYPE._keras_api_names[0] != 'keras.layers.LSTMCell':\n     # adjust for non LSTMCell decoder\n     encoder_state = [encoder_state[0]]\n \n \n #================= setup for beam search ==================\n decoder_initial_state = decoder_cell.get_initial_state(\n     batch_size=BATCH_SIZE,\n     dtype=tf.float32\n )\n \n if not isinstance(decoder_initial_state, list):\n     decoder_initial_state = [decoder_initial_state]\n \n tiled_decoder_initial_state = tfa.seq2seq.tile_batch(\n     decoder_initial_state,\n     multiplier=BEAM_WIDTH\n )\n \n decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n     cell=decoder_cell,\n     beam_width=BEAM_WIDTH,\n     output_layer=output_layer\n )\n \n #================= perform beam search  =====================\n decoder_embedding_matrix = embeddings_dec.variables[0]\n \n (\n     first_finished,\n     first_inputs,\n     first_state\n ) = decoder.initialize(\n     decoder_embedding_matrix,\n     start_tokens=start_tokens,\n     end_token=end_token,\n     initial_state=tiled_decoder_initial_state\n )\n \n inputs = first_inputs\n state = first_state\n \n for j in range(OUTPUT_SEQUENCE_SIZE):\n     outputs, next_state, next_inputs, finished = decoder.step(\n         j, inputs, state, training=False)\n     inputs = next_inputs\n     state = next_state\n \n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jimthompson5802", "commentT": "2020-05-31T01:22:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/dynamicwebpaige>@dynamicwebpaige</denchmark-link>\n  this is another blocker, similar to the similar problem that was solved before.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jimthompson5802", "commentT": "2020-06-02T23:28:25Z", "comment_text": "\n \t\tThank you for the quick resolution!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jimthompson5802", "commentT": "2020-06-03T02:49:30Z", "comment_text": "\n \t\tThank you as well.  I can confirm the fix addresses the issue we encountered.\n \t\t"}}}, "commit": {"commit_id": "fe5adac35a601cdae3cd6247070cfb4d8fe242bc", "commit_author": "Guillaume Klein", "commitT": "2020-06-02 16:17:02-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\seq2seq\\beam_search_decoder.py", "file_new_name": "tensorflow_addons\\seq2seq\\beam_search_decoder.py", "file_complexity": {"file_NLOC": "754", "file_CCN": "80", "file_NToken": "4305"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "572,573,574", "deleted_lines": null, "method_info": {"method_name": "step", "method_params": "self,time,inputs,state,training,name", "method_startline": "537", "method_endline": "602", "method_complexity": {"method_NLOC": "47", "method_CCN": "3", "method_NToken": "301", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\seq2seq\\tests\\beam_search_decoder_test.py", "file_new_name": "tensorflow_addons\\seq2seq\\tests\\beam_search_decoder_test.py", "file_complexity": {"file_NLOC": "526", "file_CCN": "24", "file_NToken": "4564"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "530,531", "deleted_lines": null, "method_info": {"method_name": "test_dynamic_decode_rnn", "method_params": "cell_class,time_major,has_attention,with_alignment_history", "method_startline": "530", "method_endline": "531", "method_complexity": {"method_NLOC": "2", "method_CCN": "1", "method_NToken": "9", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "527,528,529,530,531,532,550", "deleted_lines": "527,545", "method_info": {"method_name": "test_dynamic_decode_rnn", "method_params": "time_major,has_attention,with_alignment_history", "method_startline": "527", "method_endline": "628", "method_complexity": {"method_NLOC": "90", "method_CCN": "4", "method_NToken": "549", "method_nesting_level": "0"}}}}}}}}