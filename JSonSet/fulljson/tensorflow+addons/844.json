{"BR": {"BR_id": "844", "BR_author": "wuliytTaotao", "BRopenT": "2020-01-09T11:40:47Z", "BRcloseT": "2020-07-22T16:42:56Z", "BR_text": {"BRsummary": "How to use AdamW correctly?", "BRdescription": "\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\n TensorFlow version and how it was installed (source or binary): 2.0\n TensorFlow-Addons version and how it was installed (source or/ binary): 0.6\n Python version: 3.5\n Is GPU used? (yes/no): yes\n \n Describe the bug\n There seems to be no examples showing how to use AdamW with learning rate scheduler normally, so I try to use AdamW like the code below. The code is correct with adam, but with AdamW with learning rate decay, it doesn't work.\n Can anyone give a right example using the AdamW with learning rate decay?\n Code to reproduce the issue\n import tensorflow as tf\n import os\n from tensorflow_addons.optimizers import AdamW\n \n if __name__ == '__main__':\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n \n     gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n     for gpu in gpus:\n         tf.config.experimental.set_memory_growth(gpu, enable=True)\n     print(gpus)\n     cifar10 = tf.keras.datasets.cifar10\n \n     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n     x_train, x_test = x_train / 255.0, x_test / 255.0\n \n     model = tf.keras.models.Sequential([\n         tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n         tf.keras.layers.AveragePooling2D(),\n         tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n         tf.keras.layers.AveragePooling2D(),\n         tf.keras.layers.Flatten(),\n         tf.keras.layers.Dense(10, activation='softmax')\n     ])\n \n     step = tf.Variable(0, trainable=False)\n     schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [10000, 15000], [1e-0, 1e-1, 1e-2])\n     # lr and wd can be a function or a tensor\n     lr = 1e-3 * schedule(step)\n     wd = lambda: 1e-4 * schedule(step)\n     print(lr)\n     print(wd)\n     \n     # PiecewiseConstantDecay also doesn't seem to work properly\n     optimizer = AdamW(learning_rate=lr, weight_decay=wd)\n     # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n \n     model.compile(optimizer=optimizer,\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n \n     model.fit(x_train, y_train, epochs=40, validation_split=0.1)\n \n     model.evaluate(x_test,  y_test, verbose=2)\n Other info / logs\n <denchmark-code>Traceback (most recent call last):\n   File \"tmp2.py\", line 42, in <module>\n     model.fit(x_train, y_train, epochs=40, validation_split=0.1)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\n     use_multiprocessing=use_multiprocessing)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\n     total_epochs=epochs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\n     batch_outs = execution_function(iterator)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\n     distributed_function(input_fn))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\n     result = self._call(*args, **kwds)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\n     return self._stateless_fn(*args, **kwds)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\n     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\n     self.captured_inputs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\n     ctx, args, cancellation_manager=cancellation_manager)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\n     ctx=ctx)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\n     six.raise_from(core._status_to_exception(e.code, message), None)\n   File \"<string>\", line 2, in raise_from\n tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation sequential/conv2d/Conv2D/ReadVariableOp: Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:GPU:0' because no supported kernel for GPU devices is available.\n Colocation Debug Info:\n Colocation group had the following types and supported devices: \n Root Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\n RealDiv: GPU CPU XLA_CPU XLA_GPU \n LogicalAnd: GPU CPU XLA_CPU XLA_GPU \n _Arg: GPU CPU XLA_CPU XLA_GPU \n ReadVariableOp: GPU CPU XLA_CPU XLA_GPU \n Greater: GPU CPU XLA_CPU XLA_GPU \n Sub: GPU CPU XLA_CPU XLA_GPU \n Const: GPU CPU XLA_CPU XLA_GPU \n Pack: GPU CPU XLA_CPU XLA_GPU \n LessEqual: GPU CPU XLA_CPU XLA_GPU \n Identity: GPU CPU XLA_CPU XLA_GPU \n Cast: GPU CPU XLA_CPU XLA_GPU \n Sum: GPU CPU XLA_CPU XLA_GPU \n ResourceApplyAdam: GPU CPU XLA_CPU XLA_GPU \n Mul: GPU CPU XLA_CPU XLA_GPU \n Sqrt: GPU CPU XLA_CPU XLA_GPU \n AssignSubVariableOp: GPU CPU XLA_CPU XLA_GPU \n AddV2: GPU CPU XLA_CPU XLA_GPU \n Pow: GPU CPU XLA_CPU XLA_GPU \n \n Colocation members, user-requested devices, and framework assigned devices, if any:\n   sequential_conv2d_conv2d_readvariableop_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n   adamw_adamw_update_resourceapplyadam_m (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n   adamw_adamw_update_resourceapplyadam_v (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n   sequential/conv2d/Conv2D/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/Const (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/Const_1 (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/Const_2 (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/Const_3 (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/Const_4 (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/LessEqual (LessEqual) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/Greater (Greater) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/Greater_1 (Greater) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/LessEqual_1 (LessEqual) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/and (LogicalAnd) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/preds_c (Pack) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/Cast (Cast) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/Const (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/num_true_conds (Sum) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/n_true_conds (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/LessEqual (LessEqual) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/Assert/Const (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/Assert/AssertGuard/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/PiecewiseConstant/case/cond/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/mul/x (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/mul (Mul) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/mul_1/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/mul_1 (Mul) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/AssignSubVariableOp (AssignSubVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Identity_1 (Identity) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/add/y (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/add (AddV2) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Cast (Cast) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Identity_2 (Identity) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Identity_3 (Identity) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Pow (Pow) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Pow_1 (Pow) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub/x (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub (Sub) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Sqrt (Sqrt) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub_1/x (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub_1 (Sub) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/truediv (RealDiv) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/mul_2 (Mul) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/Const (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub_2/x (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub_2 (Sub) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub_3/x (Const) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/sub_3 (Sub) /job:localhost/replica:0/task:0/device:GPU:0\n   AdamW/AdamW/update/ResourceApplyAdam (ResourceApplyAdam) /job:localhost/replica:0/task:0/device:GPU:0\n \n Op: ReadVariableOp\n Node attrs: dtype=DT_FLOAT\n Registered kernels:\n   device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n   device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_BFLOAT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n   device='GPU'\n   device='CPU'\n   device='XLA_CPU'\n   device='XLA_GPU'\n \n \t [[{{node sequential/conv2d/Conv2D/ReadVariableOp}}]] [Op:__inference_distributed_function_2137]\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "wuliytTaotao", "commentT": "2020-01-09T17:02:20Z", "comment_text": "\n \t\tcc code owner: <denchmark-link:https://github.com/PhilJd>@PhilJd</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "wuliytTaotao", "commentT": "2020-01-10T07:28:51Z", "comment_text": "\n \t\tWhen I run the above code on the CPU only, no error is reported. But another problem arises, learning rate decay and weight decay do not work.\n I found that when using model.fit(), tf.optimizers.schedules.PiecewiseConstantDecay should be used as a parameter to learning_rate like below:\n schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-3 1e-4, 1e-5])\n optimizer = tf.keras.optimizers.Adam(learning_rate=schedule)\n \n model.compile(optimizer=optimizer,\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n model.fit(x_train, y_train, epochs=40, validation_split=0.1)\n So I tried to use AdamW as well,  learning rate decay works, but the weight decay doesn't work:\n step = tf.Variable(0, trainable=False)\n schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-3 1e-4, 1e-5])\n wd = lambda: 1e-1 * schedule(step)\n \n # weight decay cannot be changed with schedule\n optimizer = tf.keras.optimizers.AdamW(learning_rate=schedule, weight_decay=wd)\n \n model.compile(optimizer=optimizer,\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n model.fit(x_train, y_train, epochs=40, validation_split=0.1)\n When weight decay doesn't change with learning rate schedule, learning curve may be like this:\n <denchmark-link:https://user-images.githubusercontent.com/34088353/72133806-4f455200-33bd-11ea-9e80-359873758071.png></denchmark-link>\n \n Hope someone can tell me how to do it right, thanks!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "wuliytTaotao", "commentT": "2020-01-10T09:04:04Z", "comment_text": "\n \t\tIt seems like keras treats instances of  separately (in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L529>_get_hyper</denchmark-link>\n )\n Could you try to create a second schedule and see if that works? I.e., something along the lines:\n <denchmark-code>schedule_lr = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-3, 1e-4, 1e-5])\n schedule_wd = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-4, 1e-5, 1e-6])\n optimizer = tf.keras.optimizers.AdamW(learning_rate=schedule_lr, weight_decay=schedule_wd)\n </denchmark-code>\n \n Thanks :)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "wuliytTaotao", "commentT": "2020-01-10T10:14:38Z", "comment_text": "\n \t\t\n It seems like keras treats instances of learning_rate_schedule.LearningRateSchedule separately (in _get_hyper)\n Could you try to create a second schedule and see if that works? I.e., something along the lines:\n schedule_lr = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-3, 1e-4, 1e-5])\n schedule_wd = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-4, 1e-5, 1e-6])\n optimizer = tf.keras.optimizers.AdamW(learning_rate=schedule_lr, weight_decay=schedule_wd)\n \n Thanks :)\n \n It doesn't work:\n <denchmark-code>Traceback (most recent call last):\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 324, in _AssertCompatible\n     fn(values)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 263, in inner\n     _ = [_check_failed(v) for v in nest.flatten(values)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 264, in <listcomp>\n     if not isinstance(v, expected_types)]\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 248, in _check_failed\n     raise ValueError(v)\n ValueError: <tensorflow.python.keras.optimizer_v2.learning_rate_schedule.PiecewiseConstantDecay object at 0x7f57d72eef60>\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"tmp2.py\", line 48, in <module>\n     model.fit(x_train, y_train, epochs=40, validation_split=0.1)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\n     use_multiprocessing=use_multiprocessing)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\n     total_epochs=epochs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\n     batch_outs = execution_function(iterator)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\n     distributed_function(input_fn))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\n     result = self._call(*args, **kwds)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\n     self._initialize(args, kwds, add_initializers_to=initializer_map)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\n     *args, **kwds))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\n     graph_function, _, _ = self._maybe_define_function(args, kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\n     graph_function = self._create_graph_function(args, kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\n     capture_by_value=self._capture_by_value),\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\n     func_outputs = python_func(*func_args, **func_kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\n     return weak_wrapped_fn().__wrapped__(*args, **kwds)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 73, in distributed_function\n     per_replica_function, args=(model, x, y, sample_weights))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 760, in experimental_run_v2\n     return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1787, in call_for_each_replica\n     return self._call_for_each_replica(fn, args, kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2132, in _call_for_each_replica\n     return fn(*args, **kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\n     return func(*args, **kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in train_on_batch\n     output_loss_metrics=model._output_loss_metrics)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 311, in train_on_batch\n     output_loss_metrics=output_loss_metrics))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 272, in _process_single_batch\n     model.optimizer.apply_gradients(zip(grads, trainable_weights))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py\", line 153, in apply_gradients\n     grads_and_vars, name=name)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 441, in apply_gradients\n     kwargs={\"name\": name})\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1917, in merge_call\n     return self._merge_call(merge_fn, args, kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1924, in _merge_call\n     return merge_fn(self._strategy, *args, **kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 485, in _distributed_apply\n     var, apply_grad_to_update_var, args=(grad,), group=False))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1530, in update\n     return self._update(var, fn, args, kwargs, group)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2142, in _update\n     return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2148, in _update_non_slot\n     result = fn(*args, **kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 467, in apply_grad_to_update_var\n     update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py\", line 173, in _resource_apply_dense\n     with tf.control_dependencies([self._decay_weights_op(var)]):\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py\", line 158, in _decay_weights_op\n     self._get_hyper('weight_decay', var.dtype) * var,\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/variables.py\", line 1079, in _run_op\n     return tensor_oper(a.value(), *args, **kwargs)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py\", line 924, in r_binary_op_wrapper\n     x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\")\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 1184, in convert_to_tensor\n     return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 1242, in convert_to_tensor_v2\n     as_ref=False)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\n     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\n     return constant(v, dtype=dtype, name=name)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\n     allow_broadcast=True)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\n     allow_broadcast=allow_broadcast))\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 449, in make_tensor_proto\n     _AssertCompatible(values, dtype)\n   File \"/home/yetao/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 331, in _AssertCompatible\n     (dtype.name, repr(mismatch), type(mismatch).__name__))\n TypeError: Expected float32, got <tensorflow.python.keras.optimizer_v2.learning_rate_schedule.PiecewiseConstantDecay object at 0x7f57d72eef60> of type 'PiecewiseConstantDecay' instead.\n </denchmark-code>\n \n It says weight decay needs to be float32, rather than PiecewiseConstantDecay object, but why learning rate could be?\n And I saw someplace implementing the weight decay with learning rate schedule by $wd_t = wd* lr_t / lr$, this seems like a good way to implement it, but I'm not familiar with the implementation of TF2.0.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "wuliytTaotao", "commentT": "2020-01-10T10:21:09Z", "comment_text": "\n \t\tThanks for trying! I hope to find some time on the weekend to take a closer look.\n I've avoided the model.fit function so far as I feel it does too much under the hood but I guess now's the time to dive in ;)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "wuliytTaotao", "commentT": "2020-01-10T12:09:19Z", "comment_text": "\n \t\tInspire by <denchmark-link:https://github.com/sajadn/AdamW/blob/master/DecoupleWeightDecay.py>https://github.com/sajadn/AdamW/blob/master/DecoupleWeightDecay.py</denchmark-link>\n , I find a way using callback to monitor the weight decay along with the learning rate schedule on the begin of each epoch, and the code below can implement the AdamW with learning rate schelude on epochs (not each update):\n import tensorflow as tf\n import os\n from tensorflow_addons.optimizers import AdamW\n \n import numpy as np\n \n from tensorflow.python.keras import backend as K\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.keras.callbacks import Callback\n \n \n def lr_schedule(epoch):\n     \"\"\"Learning Rate Schedule\n     Learning rate is scheduled to be reduced after 20, 30 epochs.\n     Called automatically every epoch as part of callbacks during training.\n     # Arguments\n         epoch (int): The number of epochs\n     # Returns\n         lr (float32): learning rate\n     \"\"\"\n     lr = 1e-3\n \n     if epoch >= 30:\n         lr *= 1e-2\n     elif epoch >= 20:\n         lr *= 1e-1\n     print('Learning rate: ', lr)\n     return lr\n \n \n def wd_schedule(epoch):\n     \"\"\"Weight Decay Schedule\n     Weight decay is scheduled to be reduced after 20, 30 epochs.\n     Called automatically every epoch as part of callbacks during training.\n     # Arguments\n         epoch (int): The number of epochs\n     # Returns\n         wd (float32): weight decay\n     \"\"\"\n     wd = 1e-4\n \n     if epoch >= 30:\n         wd *= 1e-2\n     elif epoch >= 20:\n         wd *= 1e-1\n     print('Weight decay: ', wd)\n     return wd\n \n \n # just copy the implement of LearningRateScheduler, and then change the lr with weight_decay\n @keras_export('keras.callbacks.WeightDecayScheduler')\n class WeightDecayScheduler(Callback):\n     \"\"\"Weight Decay Scheduler.\n \n     Arguments:\n         schedule: a function that takes an epoch index as input\n             (integer, indexed from 0) and returns a new\n             weight decay as output (float).\n         verbose: int. 0: quiet, 1: update messages.\n \n     ```python\n     # This function keeps the weight decay at 0.001 for the first ten epochs\n     # and decreases it exponentially after that.\n     def scheduler(epoch):\n       if epoch < 10:\n         return 0.001\n       else:\n         return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n \n     callback = WeightDecayScheduler(scheduler)\n     model.fit(data, labels, epochs=100, callbacks=[callback],\n               validation_data=(val_data, val_labels))\n     ```\n     \"\"\"\n \n     def __init__(self, schedule, verbose=0):\n         super(WeightDecayScheduler, self).__init__()\n         self.schedule = schedule\n         self.verbose = verbose\n \n     def on_epoch_begin(self, epoch, logs=None):\n         if not hasattr(self.model.optimizer, 'weight_decay'):\n             raise ValueError('Optimizer must have a \"weight_decay\" attribute.')\n         try:  # new API\n             weight_decay = float(K.get_value(self.model.optimizer.weight_decay))\n             weight_decay = self.schedule(epoch, weight_decay)\n         except TypeError:  # Support for old API for backward compatibility\n             weight_decay = self.schedule(epoch)\n         if not isinstance(weight_decay, (float, np.float32, np.float64)):\n             raise ValueError('The output of the \"schedule\" function '\n                              'should be float.')\n         K.set_value(self.model.optimizer.weight_decay, weight_decay)\n         if self.verbose > 0:\n             print('\\nEpoch %05d: WeightDecayScheduler reducing weight '\n                   'decay to %s.' % (epoch + 1, weight_decay))\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n         logs['weight_decay'] = K.get_value(self.model.optimizer.weight_decay)\n \n \n if __name__ == '__main__':\n     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n \n     gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n     for gpu in gpus:\n         tf.config.experimental.set_memory_growth(gpu, enable=True)\n     print(gpus)\n     cifar10 = tf.keras.datasets.cifar10\n \n     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n     x_train, x_test = x_train / 255.0, x_test / 255.0\n \n     model = tf.keras.models.Sequential([\n         tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n         tf.keras.layers.AveragePooling2D(),\n         tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n         tf.keras.layers.AveragePooling2D(),\n         tf.keras.layers.Flatten(),\n         tf.keras.layers.Dense(10, activation='softmax')\n     ])\n \n     optimizer = AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0))\n     # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n \n     tb_callback = tf.keras.callbacks.TensorBoard(os.path.join('logs', 'adamw'),\n                                                  profile_batch=0)\n     lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n     wd_callback = WeightDecayScheduler(wd_schedule)\n \n     model.compile(optimizer=optimizer,\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n \n     model.fit(x_train, y_train, epochs=40, validation_split=0.1,\n               callbacks=[tb_callback, lr_callback, wd_callback])\n \n     model.evaluate(x_test, y_test, verbose=2)\n This can be a simple example of using AdamW with tf.keras.\n But if someone want to use learning rate decay every update of weights, like tf.optimizers.schedules.PiecewiseConstantDecay, it cannot be achieved with the code above.\n <denchmark-link:https://github.com/PhilJd>@PhilJd</denchmark-link>\n  Thanks!\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "wuliytTaotao", "commentT": "2020-01-10T20:28:11Z", "comment_text": "\n \t\t\n It seems like keras treats instances of learning_rate_schedule.LearningRateSchedule separately (in _get_hyper)\n Could you try to create a second schedule and see if that works? I.e., something along the lines:\n schedule_lr = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-3, 1e-4, 1e-5])\n schedule_wd = tf.optimizers.schedules.PiecewiseConstantDecay(\n         [1407*20, 1407*30], [1e-4, 1e-5, 1e-6])\n optimizer = tf.keras.optimizers.AdamW(learning_rate=schedule_lr, weight_decay=schedule_wd)\n \n Thanks :)\n \n I would think we have to do something like this to weight_decay if we want to pass an instance of LearningRateSchedule into it.\n <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L678-L688>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L678-L688</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "wuliytTaotao", "commentT": "2020-01-12T15:49:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/WindQAQ>@WindQAQ</denchmark-link>\n  I agree with you. If we do not establish a connection between  and  through initial values such as $wd_t = wd_{init} \\cdot lr_t / lr_{init}$, then we must do the same schedule for both, but the current code does not support the case where  is . I also think that  needs to support the type.\n BTW, the callback method mentioned above can be used normally.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "wuliytTaotao", "commentT": "2020-01-13T21:14:56Z", "comment_text": "\n \t\t\n @WindQAQ I agree with you. If we do not establish a connection between weight_decay and learning_rate through initial values such as $wd_t = wd_{init} \\cdot lr_t / lr_{init}$, then we must do the same schedule for both, but the current code does not support the case where weight_decay is learning_rate_schedule.LearningRateSchedule. I also think that weight_decay needs to support the type learning_rate_schedule.LearningRateSchedule.\n BTW, the callback method mentioned above can be used normally.\n \n Agree +1. As there is another request in <denchmark-link:https://github.com/tensorflow/addons/issues/865>#865</denchmark-link>\n , <denchmark-link:https://github.com/PhilJd>@PhilJd</denchmark-link>\n  do you think we should support decaying  param? Thank you.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "wuliytTaotao", "commentT": "2020-02-08T15:53:17Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wuliytTaotao>@wuliytTaotao</denchmark-link>\n  <denchmark-link:https://github.com/WindQAQ>@WindQAQ</denchmark-link>\n \n Hi, when mixprecision training ,the code above with WD scheduler doesn't work\n in on_epoch_begin(self, epoch, logs)\n 13     def on_epoch_begin(self, epoch, logs=None):\n 14         if not hasattr(self.model.optimizer, 'weight_decay'):\n ---> 15             raise ValueError('Optimizer must have a \"weight_decay\" attribute.')\n 16         try:  # new API\n 17             weight_decay = float(K.get_value(self.model.optimizer.weight_decay))\n ValueError: Optimizer must have a \"weight_decay\" attribute.\n and because of :\n optimizer.weight_decay\n <tf.Variable 'weight_decay:0' shape=() dtype=float64, numpy=0.0002>\n model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n model.optimizer\n <tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer.LossScaleOptimizer at 0x29619c8a2c8>\n and by the way :\n per step update WD and lr for ADAM is unnessasary ,because ADAM can adjust lr automatically inside an epoch. and WD is aimed to \"Decouple Weight Decay Regularization\" (original paper)with loss function and lr.\n above all ,with epoch level update is more than sufficient.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "wuliytTaotao", "commentT": "2020-02-08T16:10:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AlexWang1900>@AlexWang1900</denchmark-link>\n  Please list your code completely and give your TF version.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "wuliytTaotao", "commentT": "2020-02-08T16:28:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/wuliytTaotao>@wuliytTaotao</denchmark-link>\n \n <denchmark-code>tf.keras.mixed_precision.experimental.set_policy('mixed_float16')\n optimizer = tfa.optimizers.AdamW(learning_rate=lr_schedule(0), weight_decay=wd_schedule(0),amsgrad=False)\n model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n model.optimizer\n </denchmark-code>\n \n <tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer.LossScaleOptimizer at 0x29619c8a2c8>\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "wuliytTaotao", "commentT": "2020-02-21T08:20:25Z", "comment_text": "\n \t\tAnything moved regarding the bug with GPU ?\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "wuliytTaotao", "commentT": "2020-02-25T14:48:02Z", "comment_text": "\n \t\tHey guys,\n While facing the similar issue, until there is a PR, here is a workaround I found that works for either .fit() (classic keras usage) or in a custom use of the AdamW object.\n I first create the AdamW object as opt then assign a lambda function returning the value of wd_schedule(opt.iterations) as weight_decay attribute. This allows to update the weight decay value commonly with the optimizer's number of iterations.\n Here is a snippet of code for the case of training scheme using .fit() :\n <denchmark-code>lr_schedule = tf.optimizers.schedules.ExponentialDecay(1e-4, 100, 0.9)\n wd_schedule = tf.optimizers.schedules.ExponentialDecay(5e-5, 100, 0.9)\n opt = AdamW(learning_rate=lr_schedule, weight_decay=lambda : None)\n opt.weight_decay = lambda : wd_schedule(opt.iterations)\n mlp.compile(\n   optimizer=opt,\n   loss=tf.keras.losses.BinaryCrossentropy())\n </denchmark-code>\n \n If I create a tf.keras.callback.CallBack to ensure that the value of weight decay do change:\n <denchmark-code>class DecayHistory(tf.keras.callbacks.Callback):\n     def on_train_begin(self, logs={}):\n         self.lr = []\n         self.wd = []\n     def on_batch_end(self, batch, logs={}):\n         self.lr.append(self.model.optimizer.lr(self.model.optimizer.iterations))\n         self.wd.append(self.model.optimizer.weight_decay)\n </denchmark-code>\n \n I obtain the expected behavior as shown in the following plot :\n <denchmark-link:https://user-images.githubusercontent.com/29702745/75257561-7036fa80-57e5-11ea-87a0-a621faab4669.png></denchmark-link>\n \n PS : <denchmark-link:https://github.com/wuliytTaotao>@wuliytTaotao</denchmark-link>\n  's  solution can be updated at each step by using  instead of \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "wuliytTaotao", "commentT": "2020-03-27T12:20:54Z", "comment_text": "\n \t\t\n lr_schedule = tf.optimizers.schedules.ExponentialDecay(1e-4, 100, 0.9)\n I obtain the expected behavior as shown in the following plot :\n \n \n You specify 100 decay steps in the code, but in the plot decay continues for the entire plot range (more than 2000 steps). Can you clarify this discrepancy or update the code, please?\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "wuliytTaotao", "commentT": "2020-03-27T12:51:18Z", "comment_text": "\n \t\t\n \n lr_schedule = tf.optimizers.schedules.ExponentialDecay(1e-4, 100, 0.9)\n I obtain the expected behavior as shown in the following plot :\n \n \n You specify 100 decay steps in the code, but in the plot decay continues for the entire plot range (more than 2000 steps). Can you clarify this discrepancy or update the code, please?\n \n Hi,\n This is inherent to the way  is built .\n Indeed if you look at the documentation (<denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay>https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay</denchmark-link>\n ), how  work is not very clear.\n Though what it does is that the decay_steps correspond to how many step it takes to to get from a learning rate lr to a learning rate of value decay_rate * lr.\n To have a concrete example, lets take the parameters of the learning rate scheduler above with initial_learning_rate = 1e-4, decay_steps = 100 , decay_rate = 0.9  :\n \n After 100 step : learning_rate_step_100 = 0.9 * initial_learning_rate\n After 200 step : learning_rate_step_200 = 0.9 * learning_rate_step_100\n And so on...\n \n Contrary to some other schedulers (such as Cosine scheduler) , Exponential Decay is infinite. The general formula if staircase=False is :\n lr(step) = (decay_rate ** (step / decay_steps) )* initial_learning_rate\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "wuliytTaotao", "commentT": "2020-03-27T13:48:39Z", "comment_text": "\n \t\tMany thanks!\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "wuliytTaotao", "commentT": "2020-07-09T04:10:16Z", "comment_text": "\n \t\tOne can also follow this <denchmark-link:https://github.com/tensorflow/addons/pull/1974>#1974</denchmark-link>\n  to make  support scheduler. Feel free to open an PR and request my review if anyone is interested in it. Thanks.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "wuliytTaotao", "commentT": "2020-07-21T20:25:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hugoych>@hugoych</denchmark-link>\n  While, your selected solution works for the schedule, it doesn't allow the optimizier to be serialized anymore, due to this line:\n \n \n \n addons/tensorflow_addons/optimizers/weight_decay_optimizers.py\n \n \n          Line 95\n       in\n       5f618fd\n \n \n \n \n \n \n  {\"weight_decay\": self._serialize_hyperparameter(\"weight_decay\"),} \n \n \n \n \n \n With your solution, the parameter is a callable, but it returns a tensor - following the function self. _serialize_hyperparameter:\n <denchmark-link:https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L861>https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L861</denchmark-link>\n \n A callable is resolved differently, than a tensor or a function - the fix is to revert the order of operations (first resolve the callable, then check if it's a tensor or a custom object (e.g.: in this case a learning rate scheduler)\n Until the proper schedules are implemented, this solution can be used in conjunction with yours (this is for SGDW, but the same can be done for AdamW)\n `\n <denchmark-code>class SerializableSGDW(tfa.optimizers.SGDW):\n     def get_config(self):\n         config = tf.keras.optimizers.SGD.get_config(self)\n     \n         config.update(\n             {\"weight_decay\": self._fixed_serialize_hyperparameter(\"weight_decay\"),}\n         )\n     \n         return config\n \n     def _fixed_serialize_hyperparameter(self, hyperparameter_name):\n         \"\"\"Serialize a hyperparameter that can be a float, callable, or Tensor.\"\"\"\n         value = self._hyper[hyperparameter_name]\n     \n         # First resolve the callable\n         if callable(value):\n             value = value()\n     \n         if isinstance(value, tf.keras.optimizers.schedules.LearningRateSchedule):\n             return tf.keras.optimizers.schedules.serialize(value)\n     \n         if tensor_util.is_tensor(value):\n             return backend.get_value(value)\n     \n         return value\n </denchmark-code>\n \n `\n Note however, after loading the model weight_decay will be variable, and no longer the scheduler\n \t\t"}}}, "commit": {"commit_id": "b9f9ac5cc54c9c2169a8197d0d61adcb42b764e2", "commit_author": "MHStadler", "commitT": "2020-07-22 09:42:55-07:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "0.7142857142857143"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\optimizers\\tests\\weight_decay_optimizers_test.py", "file_new_name": "tensorflow_addons\\optimizers\\tests\\weight_decay_optimizers_test.py", "file_complexity": {"file_NLOC": "282", "file_CCN": "24", "file_NToken": "2316"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "254,255,256,257,258,259,260,261,262,263,264,265,266", "deleted_lines": null, "method_info": {"method_name": "test_keras_fit_with_schedule", "method_params": "", "method_startline": "254", "method_endline": "266", "method_complexity": {"method_NLOC": "12", "method_CCN": "1", "method_NToken": "132", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "375,376,377,378,379,380,381,382,383,384", "deleted_lines": null, "method_info": {"method_name": "test_serialization_with_wd_schedule", "method_params": "", "method_startline": "375", "method_endline": "384", "method_complexity": {"method_NLOC": "10", "method_CCN": "1", "method_NToken": "80", "method_nesting_level": "0"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tensorflow_addons\\optimizers\\weight_decay_optimizers.py", "file_new_name": "tensorflow_addons\\optimizers\\weight_decay_optimizers.py", "file_complexity": {"file_NLOC": "309", "file_CCN": "27", "file_NToken": "927"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "177", "deleted_lines": null, "method_info": {"method_name": "_decay_weights_sparse_op", "method_params": "self,var,indices", "method_startline": "175", "method_endline": "179", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "62", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "78,79,80", "deleted_lines": "78,79", "method_info": {"method_name": "__init__", "method_params": "self,FloatTensorLike,kwargs", "method_startline": "74", "method_endline": "87", "method_complexity": {"method_NLOC": "5", "method_CCN": "1", "method_NToken": "50", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "172", "deleted_lines": null, "method_info": {"method_name": "_decay_weights_op", "method_params": "self,var", "method_startline": "170", "method_endline": "173", "method_complexity": {"method_NLOC": "4", "method_CCN": "3", "method_NToken": "49", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": "181,182,183,184,185", "deleted_lines": null, "method_info": {"method_name": "_decayed_wd", "method_params": "self,var_dtype", "method_startline": "181", "method_endline": "185", "method_complexity": {"method_NLOC": "5", "method_CCN": "2", "method_NToken": "50", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": "97,98,99,100,101,102,103,104,105,106,107,108,109,110,111", "deleted_lines": null, "method_info": {"method_name": "from_config", "method_params": "cls,config,custom_objects", "method_startline": "97", "method_endline": "111", "method_complexity": {"method_NLOC": "12", "method_CCN": "5", "method_NToken": "97", "method_nesting_level": "1"}}}}}}}}