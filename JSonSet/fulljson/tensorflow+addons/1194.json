{"BR": {"BR_id": "1194", "BR_author": "georgesterpu", "BRopenT": "2020-03-02T00:26:21Z", "BRcloseT": "2020-03-06T09:24:12Z", "BR_text": {"BRsummary": "Error when using a mask in RNN + AttentionWrapper cell", "BRdescription": "\n System information\n \n OS Platform and Distribution:  manjaro linux testing\n TensorFlow version: pypi tf-nightly 2.2.0.dev20200228\n TensorFlow-Addons version: pypi tfa-nightly 0.9.0.dev20200228\n Python version: 3.7.6\n Is GPU used?: no\n \n Describe the bug\n A ValueError is raised when using a mask in a RNN layer taking a RNN cell wrapped with an AttentionWrapper.\n Code to reproduce the issue\n import tensorflow_addons as tfa\n import tensorflow as tf\n \n bs = 4\n cell = tf.keras.layers.LSTMCell(3)\n mechanism = tfa.seq2seq.LuongAttention(units=3)\n cell = tfa.seq2seq.AttentionWrapper(cell, mechanism)\n layer = tf.keras.layers.RNN(cell)\n \n \n @tf.function\n def test(masked=False):\n     seq_len = tf.random.uniform(shape=(), minval=2, maxval=10, dtype=tf.int32)\n     data = tf.ones(shape=(bs, seq_len, 3))\n     mechanism.setup_memory(data)\n \n     if masked:\n         mask = tf.sequence_mask(lengths=bs * [seq_len])\n     else:\n         mask = None\n \n     return layer(data, mask=mask)\n \n \n test(masked=False); print('Test 1 passed')\n test(masked=True); print('Test 2 passed')\n Other info / logs\n Test 1 passed\n Traceback (most recent call last):\n   File \"/run/media/zenbook/work/phd/work/__/pb1.py\", line 26, in <module>\n     test(masked=True); print('Test 2 passed')\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n     result = self._call(*args, **kwds)\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2419, in __call__\n     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\n     graph_function = self._create_graph_function(args, kwargs)\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\n     capture_by_value=self._capture_by_value),\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\n     func_outputs = python_func(*func_args, **func_kwargs)\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\n     return weak_wrapped_fn().__wrapped__(*args, **kwds)\n   File \"/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\n     raise e.ag_error_metadata.to_exception(e)\n ValueError: in user code:\n \n     /run/media/zenbook/work/phd/work/__/pb1.py:22 test  *\n         return layer(data, mask=mask)\n     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:652 __call__  **\n         return super(RNN, self).__call__(inputs, **kwargs)\n     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:926 __call__\n         outputs = call_fn(cast_inputs, *args, **kwargs)\n     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:793 call\n         zero_output_for_mask=self.zero_output_for_mask)\n     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4205 rnn\n         **while_loop_kwargs)\n     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop\n         back_prop=back_prop)\n     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:229 while_loop\n         len_orig_loop_vars], expand_composites=True))\n     /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:1143 _check_shapes_compat\n         \"specify a less-specific shape.\" % (input_t.name, shape, t.shape))\n \n     ValueError: Input tensor 'rnn/AttentionWrapperZeroState/zeros_3:0' enters the loop with shape (), but has shape (4, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\n \n \n Process finished with exit code 1\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "georgesterpu", "commentT": "2020-03-02T14:19:38Z", "comment_text": "\n \t\tThis is because the Keras RNN assumes that each tensor in the state has a batch dimension. However, the time field in AttentionWrapperState is just a scalar tensor.\n I think we could work around not having this time field. <denchmark-link:https://github.com/georgesterpu>@georgesterpu</denchmark-link>\n  Do you want to look into that?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "georgesterpu", "commentT": "2020-03-02T15:36:18Z", "comment_text": "\n \t\tThanks, <denchmark-link:https://github.com/guillaumekln>@guillaumekln</denchmark-link>\n .\n Assuming that the problem comes from the  field -  uses it to write the current alignment slice to a  when the  flag is True. I can see several possibilities, such as:\n \n pass the current loop index from outside (the time parameter of the _step function declared inside keras.backend.rnn ?\n manage the loop counter as a member of the AttentionWrapper class\n extend the TensorArray class to support an append operation, since AttentionWrapper never leapfrogs over indices and only writes sequentially anyway.\n \n Unfortunately, as a final year student, I lack the throughput to carry on with a more detailed investigation - this is why I created the issue in the first place \u263a\ufe0f\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "georgesterpu", "commentT": "2020-03-02T16:50:37Z", "comment_text": "\n \t\tThanks looking into workarounds. Actually, as alignment_history has a dynamic size and we only append to it, we can do something like:\n ta = ta.write(ta.size(), elem)\n \n Unfortunately, as a final year student, I lack the throughput to carry on with a more detailed investigation - this is why I created the issue in the first place relaxed\n \n No worries! Thank you for finding these issues. I can make a PR as I already have a fix locally.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "georgesterpu", "commentT": "2020-03-02T17:30:36Z", "comment_text": "\n \t\tAlso, any chance that\n 4) the shape_invariants parameter of the while_loop in keras.backend.rnn could be configured conveniently to work with an AttentionWrapperState ?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "georgesterpu", "commentT": "2020-03-02T17:40:12Z", "comment_text": "\n \t\tIf you dive into the K.rnn code you'll find that it applies a tf.where on each state tensor, including our time scalar. This is the issue. See the relevant code here:\n <denchmark-link:https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L4070>https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L4070</denchmark-link>\n \n compared to the Addons decoder code that ignores scalar tensors:\n \n \n \n addons/tensorflow_addons/seq2seq/decoder.py\n \n \n          Line 455\n       in\n       fcaa672\n \n \n \n \n \n \n  # TensorArrays and scalar states get passed through. \n \n \n \n \n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "georgesterpu", "commentT": "2020-03-06T17:26:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/guillaumekln>@guillaumekln</denchmark-link>\n  thanks a lot !\n There is a corner case where this doesn't work: when setting  in the  constructor. . Crashing under both eager and graph modes.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "georgesterpu", "commentT": "2020-03-06T17:28:09Z", "comment_text": "\n \t\timport tensorflow_addons as tfa\n import tensorflow as tf\n import os\n \n os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n \n cell = tf.keras.layers.LSTMCell(3)\n mechanism = tfa.seq2seq.LuongAttention(units=3)\n cell = tfa.seq2seq.AttentionWrapper(cell, mechanism, alignment_history=True)\n layer = tf.keras.layers.RNN(cell)\n \n \n # @tf.function\n def mytest():\n     var_len = tf.random.uniform(shape=(), minval=2, maxval=10, dtype=tf.int32)\n     lengths = tf.random.uniform(shape=(var_len,), minval=1, maxval=var_len + 1, dtype=tf.int32)\n     data = tf.ones(shape=(var_len, var_len, 3))\n     mask = tf.sequence_mask(lengths, maxlen=var_len)\n     mechanism.setup_memory(data, lengths)\n \n     return layer(data, mask=mask)\n \n mytest()\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "georgesterpu", "commentT": "2020-03-06T17:43:17Z", "comment_text": "\n \t\tIt seems the Keras RNN backend does not accept TensorArray in the inputs. <denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>\n  Is there any plans for Keras to support TensorArray in cell states?\n \t\t"}}}, "commit": {"commit_id": "8305b1b16bfbaf1ff6009a96487117f916fabfe5", "commit_author": "Guillaume Klein", "commitT": "2020-03-06 10:24:11+01:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "0.0", "commit_Nprams": "0.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "file_new_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "file_complexity": {"file_NLOC": "1275", "file_CCN": "147", "file_NToken": "6478"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1918", "method_info": {"method_name": "state_size", "method_params": "self", "method_startline": "1909", "method_endline": "1930", "method_complexity": {"method_NLOC": "16", "method_CCN": "5", "method_NToken": "94", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "2085,2086,2087", "deleted_lines": "2089,2101", "method_info": {"method_name": "call", "method_params": "self,inputs,state,kwargs", "method_startline": "2001", "method_endline": "2109", "method_complexity": {"method_NLOC": "68", "method_CCN": "9", "method_NToken": "357", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": null, "deleted_lines": "1986", "method_info": {"method_name": "get_initial_state", "method_params": "self,inputs,batch_size,dtype", "method_startline": "1932", "method_endline": "2003", "method_complexity": {"method_NLOC": "52", "method_CCN": "7", "method_NToken": "274", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 10, "file_old_name": "tensorflow_addons\\seq2seq\\attention_wrapper_test.py", "file_new_name": "tensorflow_addons\\seq2seq\\attention_wrapper_test.py", "file_complexity": {"file_NLOC": "870", "file_CCN": "37", "file_NToken": "6600"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "763", "method_info": {"method_name": "testLuongScaled", "method_params": "self", "method_startline": "743", "method_endline": "779", "method_complexity": {"method_NLOC": "35", "method_CCN": "1", "method_NToken": "260", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "922", "method_info": {"method_name": "testLuongMonotonicNotNormalized", "method_params": "self", "method_startline": "903", "method_endline": "942", "method_complexity": {"method_NLOC": "38", "method_CCN": "1", "method_NToken": "284", "method_nesting_level": "1"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "999,1000,1001,1003,1008", "deleted_lines": null, "method_info": {"method_name": "test_attention_state_with_variable_length_input", "method_params": "self", "method_startline": "993", "method_endline": "1008", "method_complexity": {"method_NLOC": "13", "method_CCN": "1", "method_NToken": "151", "method_nesting_level": "1"}}}, "hunk_3": {"Ismethod": 1, "added_lines": null, "deleted_lines": "689", "method_info": {"method_name": "testBahdanauNormalized", "method_params": "self", "method_startline": "671", "method_endline": "705", "method_complexity": {"method_NLOC": "33", "method_CCN": "1", "method_NToken": "263", "method_nesting_level": "1"}}}, "hunk_4": {"Ismethod": 1, "added_lines": null, "deleted_lines": "648", "method_info": {"method_name": "testBahdanauNotNormalized", "method_params": "self", "method_startline": "627", "method_endline": "669", "method_complexity": {"method_NLOC": "42", "method_CCN": "1", "method_NToken": "312", "method_nesting_level": "1"}}}, "hunk_5": {"Ismethod": 1, "added_lines": null, "deleted_lines": "838", "method_info": {"method_name": "testBahdanauMonotonicNotNormalized", "method_params": "self", "method_startline": "818", "method_endline": "859", "method_complexity": {"method_NLOC": "40", "method_CCN": "1", "method_NToken": "296", "method_nesting_level": "1"}}}, "hunk_6": {"Ismethod": 1, "added_lines": null, "deleted_lines": "726", "method_info": {"method_name": "testLuongNotNormalized", "method_params": "self", "method_startline": "707", "method_endline": "741", "method_complexity": {"method_NLOC": "33", "method_CCN": "1", "method_NToken": "249", "method_nesting_level": "1"}}}, "hunk_7": {"Ismethod": 1, "added_lines": null, "deleted_lines": "880", "method_info": {"method_name": "testBahdanauMonotonicNormalized", "method_params": "self", "method_startline": "861", "method_endline": "901", "method_complexity": {"method_NLOC": "40", "method_CCN": "1", "method_NToken": "299", "method_nesting_level": "1"}}}, "hunk_8": {"Ismethod": 1, "added_lines": null, "deleted_lines": "799", "method_info": {"method_name": "testNotUseAttentionLayer", "method_params": "self", "method_startline": "781", "method_endline": "816", "method_complexity": {"method_NLOC": "34", "method_CCN": "1", "method_NToken": "262", "method_nesting_level": "1"}}}, "hunk_9": {"Ismethod": 1, "added_lines": null, "deleted_lines": "964", "method_info": {"method_name": "testLuongMonotonicScaled", "method_params": "self", "method_startline": "944", "method_endline": "985", "method_complexity": {"method_NLOC": "40", "method_CCN": "1", "method_NToken": "295", "method_nesting_level": "1"}}}}}}}}