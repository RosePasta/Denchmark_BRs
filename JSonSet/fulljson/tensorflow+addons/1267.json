{"BR": {"BR_id": "1267", "BR_author": "eoinoreilly30", "BRopenT": "2020-03-09T16:32:33Z", "BRcloseT": "2020-04-04T17:34:13Z", "BR_text": {"BRsummary": "Missing argument in apply_gradients() in AdamW optimizer", "BRdescription": "\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\n TensorFlow version and how it was installed (source or binary): tf-nightly 2.2.0-dev20200309 (pip install)\n TensorFlow-Addons version and how it was installed (source or binary): 0.8.3 (pip install)\n Python version: 3.6\n Is GPU used? (yes/no): yes\n \n Describe the bug\n When running model.compile() with AdamW optimizer, a type error is thrown saying:\n apply_gradients() got an unexpected keyword argument 'all_reduce_sum_gradients'\n This can be fixed by adding in the argument to apply_gradients() in tensorflow_addons/optimizers/weight_decay_optimizers.py\n Code to reproduce the issue\n <denchmark-link:https://colab.research.google.com/drive/1A6X8yYii5M8BDqwAFvoFglrTIqWvwQLm>https://colab.research.google.com/drive/1A6X8yYii5M8BDqwAFvoFglrTIqWvwQLm</denchmark-link>\n \n Other info / logs\n TypeError: in user code:\n <denchmark-code>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:503 train_function  *\n     outputs = self.distribute_strategy.experimental_run_v2(\n /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:920 experimental_run_v2  **\n     return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2254 call_for_each_replica\n     return self._call_for_each_replica(fn, args, kwargs)\n /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2615 _call_for_each_replica\n     return fn(*args, **kwargs)\n /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:473 train_step  **\n     _minimize(tape, self.optimizer, loss, self.trainable_variables)\n /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1737 _minimize\n     all_reduce_sum_gradients=False)\n \n TypeError: apply_gradients() got an unexpected keyword argument 'all_reduce_sum_gradients'\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "eoinoreilly30", "commentT": "2020-03-09T17:19:11Z", "comment_text": "\n \t\tYou code works with tensorflow==2.1.0 and tfa-nightly.\n We currently target the stable version of tensorflow (2.1.0) to, as you can see, keep the sanity of our devs. So we don't expect everything to work with tf-nightly.\n But what you are reporting is concerning. Either :\n \n the bug comes from tensorflow and they're not being backward compatible there\n We use some undocumented/private API in AdamW and we should remove them.\n \n In all cases let's keep this issue open.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T15:31:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/PhilJd>@PhilJd</denchmark-link>\n  could you look into it? It would be nice to fix it before the 0.9 release.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T15:44:25Z", "comment_text": "\n \t\tThanks for flagging this. I just checked, the signature of the apply_gradients function has changed indeed. PR <denchmark-link:https://github.com/tensorflow/addons/pull/1181>#1181</denchmark-link>\n  was an attempt to fix this but broke backward compatibility. I'll prepare a PR in a minute.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T15:50:29Z", "comment_text": "\n \t\tThanks a lot. One thing I don't understand is that if when doing the subclassing, we used only public APIs, the breakage shouldn't happend right?\n <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#write_a_customized_optimizer_2>https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#write_a_customized_optimizer_2</denchmark-link>\n \n It seems we shouldn't override apply_gradients.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T16:07:05Z", "comment_text": "\n \t\tThe problem is that we need to pass a list with the variables to decay. I put this into apply_gradients and not into e.g. constructor as the variables to decay are also passed there.\n Atm I don't see a backward compatible way of getting rid of overwriting the apply_gradients function.\n I think it's easier to add **kwargs to the function signature, allowing us to be backward compatible.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T16:09:45Z", "comment_text": "\n \t\tYep you're right. For now, let's fix the compatibility with TF 2.2.x with **kwargs. Then we can look into how to use only public API to reimplement it. If we don't do it at some point, we'll have to update this file at every TF release haha\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T16:21:43Z", "comment_text": "\n \t\tHm, technically we're just wrapping a public API, which seems perfectly valid to me.\n I'd argue that the problem is on the Keras side, with an optimizer wrapping the public API in an incompatible way, requiring special treatment in the training loop <denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.2.0-rc1/tensorflow/python/keras/engine/training.py#L1735>here</denchmark-link>\n .\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T16:33:01Z", "comment_text": "\n \t\tWe shouldn't override methods which are public and not meant to be overwritten. For example, if method x is public and not meant to be overwritten, and we overwrite it anyway, adding a new argument y, it's working for the moment. Then, since x is a public API, the TF team can also add a new argument z. Then we're in conflict on the API level with what does tensorflow.\n But yeah the piece of code you're showing is unfortunate. There are some issues on both side. We can discuss at the meeting today if using composition over subclassing is possible, and if yes, then we may want to go this way and break backward compatibility.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T16:41:03Z", "comment_text": "\n \t\tWe should discuss this one for sure.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T19:17:54Z", "comment_text": "\n \t\tHm, I just realized I took a look at the wrong snapshot for optimizer (2.1) and didn't get the full picture, sorry about that.\n I guess we can get a long way with **kwargs (I think it's likely the TF team will only add new arguments with a default, which wouldn't require changes here) but I'd be happy to approach this differently if backward compatibility is less of a concern.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "eoinoreilly30", "commentT": "2020-04-03T21:32:53Z", "comment_text": "\n \t\tYep, so we discussed this at the SIG keras and SIG addons meeting, the decision was that we do quick fixes and keep backward compatibility until the new RFC for keras optimizers is created and implemented. Once it's implemented, we'll refactor everything, possibly do backward incompatible changes and make everything compliant with the new API.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "eoinoreilly30", "commentT": "2020-04-04T17:35:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/PhilJd>@PhilJd</denchmark-link>\n   As said during the sig  keras meeting, there is going to be an RFC concerning the new API for optimizers in <denchmark-link:https://github.com/tensorflow/community/pulls>https://github.com/tensorflow/community/pulls</denchmark-link>\n  . When it's implemented, we'll likely refactor the weight decay using this API, could you take a look and get involved in this RFC when it's out?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "eoinoreilly30", "commentT": "2020-04-04T20:20:36Z", "comment_text": "\n \t\tSure, feel free to ping me if I happen to miss the announcement :)\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "eoinoreilly30", "commentT": "2020-04-11T11:11:19Z", "comment_text": "\n \t\twe still face the issue with TensorFlow version and how it was installed (source or binary): tf-nightly 2.2.0-dev20200408 (pip install) and stable version 2.1\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "eoinoreilly30", "commentT": "2020-04-11T11:20:19Z", "comment_text": "\n \t\tWhich tensorflow-addons version are you using? I guess it's in tfa-nightly only so far.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "eoinoreilly30", "commentT": "2020-04-11T11:21:25Z", "comment_text": "\n \t\tShould work with tensorflow addons 0.9.x too\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "eoinoreilly30", "commentT": "2020-06-28T09:32:23Z", "comment_text": "\n \t\tI'm getting a similar error,\n <denchmark-code>    TypeError: tf__apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'\n </denchmark-code>\n \n This is from attempting a custom model\n <denchmark-code>from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer as lso\n from tensorflow.python.distribute import parameter_server_strategy\n \n def _minimize(strategy, tape, optimizer, loss, trainable_variables):\n     with tape:\n         if isinstance(optimizer, lso.LossScaleOptimizer):\n             loss = optimizer.get_scaled_loss(loss)\n \n     gradients = tape.gradient(loss, trainable_variables)\n     gradients = [(ClipIfNotNone(grad)) for grad in gradients]\n     gradients = [(ClipIfNotNone2(grad)) for grad in gradients]\n     # Whether to aggregate gradients outside of optimizer. This requires support\n     # of the optimizer and doesn't work with ParameterServerStrategy and\n     # CentralStroageStrategy.\n     aggregate_grads_outside_optimizer = (\n         optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access\n         not isinstance(strategy.extended,\n                         parameter_server_strategy.ParameterServerStrategyExtended))\n \n     if aggregate_grads_outside_optimizer:\n         # We aggregate gradients before unscaling them, in case a subclass of\n         # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be\n         # done on scaled gradients, not unscaled gradients, for numeric stability.\n         gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n                                                     trainable_variables))\n     if isinstance(optimizer, lso.LossScaleOptimizer):\n         gradients = optimizer.get_unscaled_gradients(gradients)\n     gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access\n     if trainable_variables:\n         if aggregate_grads_outside_optimizer:\n             optimizer.apply_gradients(\n                 zip(gradients, trainable_variables),\n                 experimental_aggregate_gradients=False)\n         else:\n             optimizer.apply_gradients(zip(gradients, trainable_variables))\n \n class CustomModel(tf.keras.Model):\n     def train_step(self, data):\n         # Unpack the data. Its structure depends on your model and\n         # on what you pass to `fit()`.\n         x = data\n         y = tf.constant([1.0], dtype=tf.float32)\n \n         with tf.GradientTape() as tape:\n             y_pred = self(x, training=True)  # Forward pass\n             # Compute the loss value\n             # (the loss function is configured in `compile()`)\n             loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n         \n         _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n                 self.trainable_variables)\n \n         self.compiled_metrics.update_state(y, y_pred, sample_weight)\n         return {m.name: m.result() for m in self.metrics}\n </denchmark-code>\n \n This is with the AdamW optimizer as well.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "eoinoreilly30", "commentT": "2020-06-28T09:55:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Santosh-Gupta>@Santosh-Gupta</denchmark-link>\n  you need . Check <denchmark-link:https://github.com/tensorflow/addons/pull/1924>#1924</denchmark-link>\n \n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "eoinoreilly30", "commentT": "2020-06-28T10:09:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bhack>@bhack</denchmark-link>\n  where do the  go? I'm looking at <denchmark-link:https://github.com/tensorflow/addons/pull/1924>#1924</denchmark-link>\n  but still not sure. Is it at ? so ?\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "eoinoreilly30", "commentT": "2020-06-28T10:23:04Z", "comment_text": "\n \t\tFor AdamW <denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/weight_decay_optimizers.py#L130-L181>https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/weight_decay_optimizers.py#L130-L181</denchmark-link>\n \n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "eoinoreilly30", "commentT": "2020-06-28T10:58:27Z", "comment_text": "\n \t\tInstalling the latest tfa add-ons seemed to fix the issue, though now I'm getting an AttributeError: Tensor.name is meaningless when eager execution is enabled. error, which I believe is related to training on TPUs and not the AdamW optimizer.\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "eoinoreilly30", "commentT": "2020-06-29T19:34:32Z", "comment_text": "\n \t\tI solved it, Just upgrade to new version tfa 0.10.0\n pip install tensorflow_addons==0.10.0\n \t\t"}}}, "commit": {"commit_id": "99dabd10540087523a4130735f732a865d93f529", "commit_author": "Phil", "commitT": "2020-04-04 19:34:12+02:00", "commit_complexity": {"commit_NLOC": "1.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\optimizers\\weight_decay_optimizers.py", "file_new_name": "tensorflow_addons\\optimizers\\weight_decay_optimizers.py", "file_complexity": {"file_NLOC": "296", "file_CCN": "22", "file_NToken": "801"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "129,141,142,143", "deleted_lines": "129,150", "method_info": {"method_name": "apply_gradients", "method_params": "self,grads_and_vars,name,decay_var_list", "method_startline": "129", "method_endline": "150", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "52", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "129,141,142,143,153", "deleted_lines": "129,150", "method_info": {"method_name": "apply_gradients", "method_params": "self,grads_and_vars,name,decay_var_list,kwargs", "method_startline": "129", "method_endline": "153", "method_complexity": {"method_NLOC": "5", "method_CCN": "3", "method_NToken": "58", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\optimizers\\weight_decay_optimizers_test.py", "file_new_name": "tensorflow_addons\\optimizers\\weight_decay_optimizers_test.py", "file_complexity": {"file_NLOC": "251", "file_CCN": "21", "file_NToken": "1791"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "276,277,278,279,280,281,282,283", "deleted_lines": null, "method_info": {"method_name": "testKerasFit", "method_params": "self", "method_startline": "276", "method_endline": "283", "method_complexity": {"method_NLOC": "7", "method_CCN": "1", "method_NToken": "111", "method_nesting_level": "1"}}}}}}}}