{"BR": {"BR_id": "2283", "BR_author": "vlawhern", "BRopenT": "2020-12-10T20:52:38Z", "BRcloseT": "2020-12-15T05:14:21Z", "BR_text": {"BRsummary": "Can't save keras model weights when using LayerNormLSTMCell and Eager mode due to non-unique weight names", "BRdescription": "\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\n TensorFlow version and how it was installed (source or binary): TF 2.2, installed with Anaconda through conda package manager\n TensorFlow-Addons version and how it was installed (source or binary): 0.11.2, pip install\n Python version: 3.7.6\n Is GPU used? (yes/no): Yes\n \n Describe the bug\n When using a model with LayerNormLSTMCell with Eager Execution enabled, you cannot save the model weights due to it having non-unique weight names. Apparently this is a known Keras issue (see <denchmark-link:https://github.com/tensorflow/tensorflow/issues/27688#issuecomment-595950270>tensorflow/tensorflow#27688 (comment)</denchmark-link>\n ).\n For example, this code works just fine:\n import tensorflow as tf\n from tensorflow_addons.rnn import LayerNormLSTMCell\n from tensorflow.keras.layers import LSTMCell\n \n import numpy as np\n \n x_train = np.random.randn(1000, 10, 20)\n y_train = np.random.randint(0, 1, x_train.shape[0])\n \n model = tf.keras.Sequential([\n     tf.keras.layers.RNN(LSTMCell(10), input_shape = (10,20)),\n     tf.keras.layers.Dense(2),\n     tf.keras.layers.Activation('softmax')\n ])\n \n \n model.compile(optimizer='adam',\n               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n               metrics=['sparse_categorical_accuracy'])\n \n model.fit(x_train, y_train, epochs=1)\n model.save_weights('/tmp/weights.h5')\n while this code fails\n import tensorflow as tf\n from tensorflow_addons.rnn import LayerNormLSTMCell\n from tensorflow.keras.layers import LSTMCell\n \n import numpy as np\n \n x_train = np.random.randn(1000, 10, 20)\n y_train = np.random.randint(0, 1, x_train.shape[0])\n \n model = tf.keras.Sequential([\n     tf.keras.layers.RNN(LayerNormLSTMCell(10), input_shape = (10,20)),\n     tf.keras.layers.Dense(2),\n     tf.keras.layers.Activation('softmax')\n ])\n \n \n model.compile(optimizer='adam',\n               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n               metrics=['sparse_categorical_accuracy'])\n \n model.fit(x_train, y_train, epochs=1)\n model.save_weights('/tmp/weights.h5')\n with the following error:\n RuntimeError: Unable to create link (name already exists)\n I believe this is happening due to the LayerNormLSTMCell weight names not being unique. Here are the weight names; weights 3,5,7 and 4,6,8 each have the same name.\n model.layers[0].weights[0].name\n Out[7]: 'rnn_2/kernel:0'\n \n model.layers[0].weights[1].name\n Out[8]: 'rnn_2/recurrent_kernel:0'\n \n model.layers[0].weights[2].name\n Out[9]: 'rnn_2/bias:0'\n \n model.layers[0].weights[3].name\n Out[10]: 'rnn_2/gamma:0'\n \n model.layers[0].weights[4].name\n Out[11]: 'rnn_2/beta:0'\n \n model.layers[0].weights[5].name\n Out[12]: 'rnn_2/gamma:0'\n \n model.layers[0].weights[6].name\n Out[13]: 'rnn_2/beta:0'\n \n model.layers[0].weights[7].name\n Out[14]: 'rnn_2/gamma:0'\n \n model.layers[0].weights[8].name\n Out[15]: 'rnn_2/beta:0'\n The suggested solution is to disable eager execution (via tf.compat.v1.disable_eager_execution()) which does allow both code snippets to work; however, in my use case I run into severe performance degradation this way. If possible I'd like to be able to save model weights when use LayerNormLSTMCell with Eager mode.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "vlawhern", "commentT": "2020-12-10T22:22:55Z", "comment_text": "\n \t\tSeems that the name of  layer does not propagate successfully. Hi <denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>\n , can you confirm if this is a keras upstream issue and is there any workaround on this?\n <denchmark-link:https://colab.research.google.com/drive/1M9BUa8t0ynV8qPQzGh8T_OH2IpKMnog8?usp=sharing>https://colab.research.google.com/drive/1M9BUa8t0ynV8qPQzGh8T_OH2IpKMnog8?usp=sharing</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "vlawhern", "commentT": "2020-12-10T23:22:31Z", "comment_text": "\n \t\tI think its a layerNormCell issue, since the duplicated name is caused by kernel_norm/recurrent_norm/state_norm sub layers. More specifically, build() method of LayerNormLSTMCell should call kernel_norm.build() with a name scope, so that the weights created will the proper name to identify them.\n <denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/rnn/layer_norm_lstm_cell.py#L157>https://github.com/tensorflow/addons/blob/master/tensorflow_addons/rnn/layer_norm_lstm_cell.py#L157</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "vlawhern", "commentT": "2020-12-10T23:26:10Z", "comment_text": "\n \t\tSounds good. So the name passed into layer does nothing for name scope?\n <denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/rnn/layer_norm_lstm_cell.py#L196-L202>https://github.com/tensorflow/addons/blob/master/tensorflow_addons/rnn/layer_norm_lstm_cell.py#L196-L202</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "vlawhern", "commentT": "2020-12-10T23:28:24Z", "comment_text": "\n \t\tIt should, but since we manually invoke the build() method for those sublayers, and that is causing the problem for missing the name. If you take a look for the base_layer in keras, there are quite some logic around build() method before it is invoked.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "vlawhern", "commentT": "2020-12-10T23:35:20Z", "comment_text": "\n \t\tThanks for clarification!\n \t\t"}}}, "commit": {"commit_id": "3ca4decde94b4c07632c5b9750a442d155b1faea", "commit_author": "Qianli Scott Zhu", "commitT": "2020-12-14 20:54:08-08:00", "commit_complexity": {"commit_NLOC": "0.2727272727272727", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\rnn\\layer_norm_lstm_cell.py", "file_new_name": "tensorflow_addons\\rnn\\layer_norm_lstm_cell.py", "file_complexity": {"file_NLOC": "140", "file_CCN": "10", "file_NToken": "734"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "158,159,160,161,162", "deleted_lines": "158,159", "method_info": {"method_name": "build.maybe_build_sublayer", "method_params": "sublayer,build_shape", "method_startline": "158", "method_endline": "162", "method_complexity": {"method_NLOC": "5", "method_CCN": "2", "method_NToken": "38", "method_nesting_level": "2"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "157,158,159,160,161,162,163,164,165,166", "deleted_lines": "157,158,159", "method_info": {"method_name": "build", "method_params": "self,input_shape", "method_startline": "155", "method_endline": "166", "method_complexity": {"method_NLOC": "6", "method_CCN": "1", "method_NToken": "72", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\rnn\\tests\\layer_norm_lstm_cell_test.py", "file_new_name": "tensorflow_addons\\rnn\\tests\\layer_norm_lstm_cell_test.py", "file_complexity": {"file_NLOC": "110", "file_CCN": "5", "file_NToken": "1022"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141", "deleted_lines": null, "method_info": {"method_name": "test_build", "method_params": "", "method_startline": "126", "method_endline": "141", "method_complexity": {"method_NLOC": "16", "method_CCN": "1", "method_NToken": "155", "method_nesting_level": "0"}}}}}}}}