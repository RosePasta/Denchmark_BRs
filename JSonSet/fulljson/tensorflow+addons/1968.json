{"BR": {"BR_id": "1968", "BR_author": "avnx", "BRopenT": "2020-07-03T18:10:08Z", "BRcloseT": "2020-07-11T05:25:47Z", "BR_text": {"BRsummary": "Error with mixed precision", "BRdescription": "\n System information\n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\n TensorFlow version and how it was installed (source or binary):  https://github.com/horovod/horovod/blob/master/Dockerfile.gpu (changed TENSORFLOW_VERSION=2.2.0 and NCCL_VERSION=2.5.6-1+cuda10.1)\n TensorFlow-Addons version and how it was installed (source or binary): nightly (0.11.0-dev)\n Python version: 3.6.9\n Is GPU used? (yes/no): yes\n \n Code doesn't work when applying mixed precision.\n Am I doing something wrong?\n Code to reproduce the issue\n <denchmark-code>import tensorflow as tf\n import tensorflow_addons as tfa\n from tensorflow.keras.mixed_precision import experimental as mixed_precision\n \n policy = mixed_precision.Policy('mixed_float16')\n mixed_precision.set_policy(policy)\n \n embedding_layer = tf.keras.layers.Embedding(37,\n                                             37,\n                                             name='embedding')\n \n rnn_cell = tf.keras.layers.LSTMCell(10)\n attention_mechanism = tfa.seq2seq.BahdanauAttention(units=10, name='BahdanauAtt')\n attention_wrapper = tfa.seq2seq.AttentionWrapper(rnn_cell,\n                                                  attention_mechanism,\n                                                  attention_layer_size=10,\n                                                  name='Att_wrapper')\n mlp = tf.keras.layers.Dense(14, name='output_mlp')\n decoder_cell = tfa.seq2seq.BasicDecoder(attention_wrapper,\n                                         output_layer=mlp,\n                                         sampler=tfa.seq2seq.ScheduledEmbeddingTrainingSampler(0.1,\n                                                                                               embedding_layer),\n                                         name='train_decoder')\n \n @tf.function\n def step(inputs, encoder_outputs):\n     \n     attention_mechanism.setup_memory(encoder_outputs)\n     decoder_initial_state = attention_wrapper.get_initial_state(batch_size=32,\n                                                                 dtype=encoder_outputs.dtype)\n \n     embeddings = embedding_layer(inputs)\n \n     outputs = decoder_cell(embeddings,\n                        initial_state=decoder_initial_state,\n                        training=True)\n     return outputs\n \n encoder_outputs = tf.random.uniform([32, 17, 256], dtype=tf.float16)\n inputs = tf.random.uniform([32, 27], maxval=37, dtype=tf.int32)\n step(inputs, encoder_outputs)\n \n \n </denchmark-code>\n \n Other info / logs\n <denchmark-code>/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py:163 call  *\n         self,\n     /usr/local/lib/python3.6/dist-packages/typeguard/__init__.py:262 wrapper  *\n         retval = func(*args, **kwargs)\n     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py:397 body  *\n         (next_outputs, decoder_state, next_inputs, decoder_finished) = decoder.step(\n     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/basic_decoder.py:134 step  *\n         cell_outputs, cell_state = self.cell(inputs, state, training=training)\n     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/attention_wrapper.py:2052 call  *\n         attention, alignments, next_attention_state = self._attention_fn(\n     /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/attention_wrapper.py:1595 _compute_attention  *\n         context_ = tf.matmul(expanded_alignments, attention_mechanism.values)\n     /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper  **\n         return target(*args, **kwargs)\n     /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:2946 matmul\n         a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n     /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:1526 batch_mat_mul_v2\n         \"BatchMatMulV2\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n     /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:506 _apply_op_helper\n         inferred_from[input_arg.type_attr]))\n \n     TypeError: Input 'y' of 'BatchMatMulV2' Op has type float16 that does not match type float32 of argument 'x'.\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "avnx", "commentT": "2020-07-08T17:07:23Z", "comment_text": "\n \t\tThe first issue is that we are forcing  at <denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper.py#L559-L560>https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper.py#L559-L560</denchmark-link>\n .\n Then we have a similar issue at <denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/sampler.py#L394-L400>https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/sampler.py#L394-L400</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "avnx", "commentT": "2020-07-08T17:48:51Z", "comment_text": "\n \t\tYes, we should fix this. The seq2seq layers were designed before the new dtype policy.\n While working on a fix, I find it a bit unexpected that tf.keras.layers.Embedding does not return float16 here.\n \t\t"}}}, "commit": {"commit_id": "8bad1e45f4ddfb0f774955de93b43f45a8f8c960", "commit_author": "Guillaume Klein", "commitT": "2020-07-10 22:25:46-07:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow_addons\\conftest.py", "file_new_name": "tensorflow_addons\\conftest.py", "file_complexity": {"file_NLOC": "14", "file_CCN": "0", "file_NToken": "33"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "file_new_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "file_complexity": {"file_NLOC": "1225", "file_CCN": "147", "file_NToken": "6262"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1722,1728", "deleted_lines": "113,114,559,560,738,739,1096,1097,1277,1278,1737"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\seq2seq\\sampler.py", "file_new_name": "tensorflow_addons\\seq2seq\\sampler.py", "file_complexity": {"file_NLOC": "551", "file_CCN": "85", "file_NToken": "3536"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "393,394,395", "deleted_lines": null, "method_info": {"method_name": "next_inputs", "method_params": "self,time,outputs,state,sample_ids", "method_startline": "381", "method_endline": "407", "method_complexity": {"method_NLOC": "8", "method_CCN": "1", "method_NToken": "75", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "393,394,395", "deleted_lines": null, "method_info": {"method_name": "next_inputs.maybe_sample", "method_params": "", "method_startline": "386", "method_endline": "403", "method_complexity": {"method_NLOC": "17", "method_CCN": "1", "method_NToken": "128", "method_nesting_level": "2"}}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\seq2seq\\tests\\attention_wrapper_test.py", "file_new_name": "tensorflow_addons\\seq2seq\\tests\\attention_wrapper_test.py", "file_complexity": {"file_NLOC": "772", "file_CCN": "38", "file_NToken": "6173"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "544,549,555,556,559,561,563,568", "deleted_lines": "544,550,551,554,556,558,563", "method_info": {"method_name": "test_bahdanau_not_normalized", "method_params": "", "method_startline": "542", "method_endline": "579", "method_complexity": {"method_NLOC": "37", "method_CCN": "1", "method_NToken": "286", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "620,625,631,632,635,637,639", "deleted_lines": "618,624,625,628,630,632", "method_info": {"method_name": "test_luong_not_normalized", "method_params": "", "method_startline": "618", "method_endline": "649", "method_complexity": {"method_NLOC": "30", "method_CCN": "1", "method_NToken": "240", "method_nesting_level": "0"}}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\seq2seq\\tests\\decoder_test.py", "file_new_name": "tensorflow_addons\\seq2seq\\tests\\decoder_test.py", "file_complexity": {"file_NLOC": "117", "file_CCN": "9", "file_NToken": "880"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157", "deleted_lines": null, "method_info": {"method_name": "test_dynamic_decode_rnn_with_scheduled_embedding_training_sampler", "method_params": "", "method_startline": "133", "method_endline": "157", "method_complexity": {"method_NLOC": "22", "method_CCN": "1", "method_NToken": "167", "method_nesting_level": "0"}}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\utils\\test_utils.py", "file_new_name": "tensorflow_addons\\utils\\test_utils.py", "file_complexity": {"file_NLOC": "150", "file_CCN": "40", "file_NToken": "1035"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "106,107,108,109,110,111", "deleted_lines": null, "method_info": {"method_name": "run_with_mixed_precision_policy", "method_params": "request", "method_startline": "106", "method_endline": "111", "method_complexity": {"method_NLOC": "6", "method_CCN": "3", "method_NToken": "52", "method_nesting_level": "0"}}}}}}}}