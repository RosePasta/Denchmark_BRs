{"BR": {"BR_id": "693", "BR_author": "danijar", "BRopenT": "2019-11-12T09:50:49Z", "BRcloseT": "2019-12-26T18:10:20Z", "BR_text": {"BRsummary": "Lost gradient after clipping in focal loss?", "BRdescription": "\n \n \n \n addons/tensorflow_addons/losses/focal_loss.py\n \n \n          Line 132\n       in\n       776b751\n \n \n \n \n \n \n  y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon()) \n \n \n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "danijar", "commentT": "2019-11-12T17:06:39Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/AakashKumarNain>@AakashKumarNain</denchmark-link>\n  code owner\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "danijar", "commentT": "2019-11-12T17:19:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/danijar>@danijar</denchmark-link>\n  any sample code to reproduce?\n If it is the case though, then I suspect that epsilon added to y_pred in this line\n p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n should solve the issue. Need to test it though.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "danijar", "commentT": "2019-11-14T05:15:22Z", "comment_text": "\n \t\tI just read through the code and wanted to highlight a potential problem. Looking at K.clip, it doesn't seem to provide a gradient for values that have been clipped. In this is indeed the case, a fix would be\n clipped = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n y_pred = y_pred - tf.stop_gradient(y_pred) + tf.stop_gradient(clipped)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "danijar", "commentT": "2019-11-14T06:51:32Z", "comment_text": "\n \t\tThank you <denchmark-link:https://github.com/danijar>@danijar</denchmark-link>\n  for looking into this and providing a fix. I will look into it once again and we will make the necessary code changes. Thank you again\n <denchmark-link:https://github.com/WindQAQ>@WindQAQ</denchmark-link>\n  <denchmark-link:https://github.com/facaiy>@facaiy</denchmark-link>\n  <denchmark-link:https://github.com/seanpmorgan>@seanpmorgan</denchmark-link>\n  sounds good?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "danijar", "commentT": "2019-11-20T19:14:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AakashKumarNain>@AakashKumarNain</denchmark-link>\n  Sorry for the late response. I'm fine with this. Maybe provide a testcase to verify the patch is needed!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "danijar", "commentT": "2019-12-10T19:10:26Z", "comment_text": "\n \t\tSorry for the delay. I was on vacation. I will check and if needed, fix this during this weekend\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "danijar", "commentT": "2019-12-12T10:49:23Z", "comment_text": "\n \t\tK.clip is just clip by value. It doesn't seem to affect the gradients, hence no fix is needed IMO. Though looking for any additional inputs\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "danijar", "commentT": "2019-12-12T18:12:29Z", "comment_text": "\n \t\tWhat do you mean? Clip by value is not differentiable.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "danijar", "commentT": "2019-12-12T20:26:01Z", "comment_text": "\n \t\tThe same thing is being used here in  calculations.\n <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4558-L4588>https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4558-L4588</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "danijar", "commentT": "2019-12-12T21:38:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/danijar>@danijar</denchmark-link>\n  I understand the confusion.\n <denchmark-link:https://github.com/tensorflow/tensorflow/blob/f7f4049ded8ad2bfbaecde1ff2d42460cc9fa762/tensorflow/python/ops/clip_ops.py#L38>tf.clip_by_value</denchmark-link>\n  uses  and  currently (ignoring the TODO).\n Both  and  have their gradients defined <denchmark-link:https://github.com/tensorflow/tensorflow/blob/606a542c81df033d493d25e32b9f175b947fcbbb/tensorflow/core/ops/math_grad.cc#L617>here</denchmark-link>\n . So yes, as <denchmark-link:https://github.com/AakashKumarNain>@AakashKumarNain</denchmark-link>\n  mentioned, using  shouldn't be a problem for differentiation.\n As for the gradient, it'll be 0 outside the range of min_value and max_value.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "danijar", "commentT": "2019-12-15T18:40:34Z", "comment_text": "\n \t\tExactly. Is it supposed to be 0 outside of the range?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "danijar", "commentT": "2019-12-15T20:24:29Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/danijar>@danijar</denchmark-link>\n  My bad, you're right. A clip is used in BCE before taking a  for numerical stability. That won't be needed for focal loss.\n Funnily enough in the tf.keras implementation of  BCE, that does both a clip followed by an output + eps. I think the addition of eps after the clip won't be needed.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "danijar", "commentT": "2019-12-16T04:12:18Z", "comment_text": "\n \t\tMy main point was that the gradient should probably be patched back onto the clipped tensor. Anyway, I'm not a user of the module.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "danijar", "commentT": "2019-12-17T16:20:18Z", "comment_text": "\n \t\tWe will patch it.\n \t\t"}}}, "commit": {"commit_id": "732b18fa55c299e1abad8ba7ac281e55ff5f7b3e", "commit_author": "Aakash Kumar Nain", "commitT": "2019-12-26 23:28:04+05:18", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "1.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow_addons\\losses\\focal_loss.py", "file_new_name": "tensorflow_addons\\losses\\focal_loss.py", "file_complexity": {"file_NLOC": "105", "file_CCN": "4", "file_NToken": "474"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "45,46,47,113,115,123,124,125,126,127,129,130,134,136,138,139,140,144,148,151", "deleted_lines": "45,46,47,113,115,123,125,126,130,132,134,135,136,140,144,147,148"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow_addons\\losses\\focal_loss_test.py", "file_new_name": "tensorflow_addons\\losses\\focal_loss_test.py", "file_complexity": {"file_NLOC": "72", "file_CCN": "5", "file_NToken": "661"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "96,97,110", "deleted_lines": "94,107", "method_info": {"method_name": "test_without_logits", "method_params": "self", "method_startline": "84", "method_endline": "111", "method_complexity": {"method_NLOC": "20", "method_CCN": "1", "method_NToken": "213", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "62,63,64,65,80", "deleted_lines": "62,63,78", "method_info": {"method_name": "test_with_logits", "method_params": "self", "method_startline": "47", "method_endline": "81", "method_complexity": {"method_NLOC": "26", "method_CCN": "1", "method_NToken": "255", "method_nesting_level": "1"}}}}}}}}