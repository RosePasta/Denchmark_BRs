{"BR": {"BR_id": "36", "BR_author": "bamos", "BRopenT": "2015-10-25T17:07:47Z", "BRcloseT": "2016-01-11T21:46:58Z", "BR_text": {"BRsummary": "Training: inconsistent tensor size", "BRdescription": "\n <denchmark-code>training(master*)$ ./main.lua -data ~/openface/data/casia-facescrub/dlib-affine-224-split/ -imgDim 224 -batchSize 20 -nDonkeys -1\n \n ==> doing epoch on training data:\n ==> online epoch # 1\n \n /home/bamos/torch/install/bin/luajit: inconsistent tensor size at /home/bamos/torch/pkg/torch/lib/TH/generic/THTensorCopy.c:7\n stack traceback:\n         [C]: at 0x7fecb5c9aa60\n         [C]: in function '__newindex'\n         /home/bamos/openface/training/train.lua:132: in function 'f2'\n         /home/bamos/openface/training/data.lua:36: in function 'addjob'\n         /home/bamos/openface/training/train.lua:48: in function 'train'\n         ./main.lua:35: in main chunk\n         [C]: in function 'dofile'\n         ...amos/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:131: in main chunk\n         [C]: at 0x00406670\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "bamos", "commentT": "2015-10-26T20:08:49Z", "comment_text": "\n \t\thi, any clue how to resolve it? I have the same error.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "bamos", "commentT": "2015-10-26T20:41:33Z", "comment_text": "\n \t\tHi, yes, I just looked at this again and it's because I forgot to specify the nn2 model for images sized 224 and the training was defaulting to the nn4 model, which only supports inputs of 96x96. Because of this, the training code was passing in a tensor with 20 images, sized 20,3,224,224 and was receiving back a tensor sized 500,128 instead of 20,128.\n \n  This error is very confusing and I'll add a check to prevent mis-matched sizes like this.\n \n -Brandon.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "bamos", "commentT": "2015-10-28T07:43:07Z", "comment_text": "\n \t\tWhat is the format of the training data in the folder ~/openface/data/casia-facescrub/dlib-affine-224-split/ ?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "bamos", "commentT": "2015-10-28T11:53:54Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/Prithviraj7>@Prithviraj7</denchmark-link>\n  -  and  subdirectories containing directories of people with images.\n This can be produced with steps 1 and 2 of 'Training new models' in the README:\n <denchmark-h:hr></denchmark-h>\n \n \n \n Create a directory for your raw images so that images from different\n people are in different subdirectories. The names of the labels or\n images do not matter, and each person can have a different amount of images.\n The images should be formatted as jpg or png and have\n a lowercase extension.\n $ tree data/mydataset/raw\n person-1\n \u251c\u2500\u2500 image-1.jpg\n \u251c\u2500\u2500 image-2.png\n ...\n \u2514\u2500\u2500 image-p.png\n \n ...\n \n person-m\n \u251c\u2500\u2500 image-1.png\n \u251c\u2500\u2500 image-2.jpg\n ...\n \u2514\u2500\u2500 image-q.png\n \n \n \n Preprocess the raw images, change 8 to however many\n separate processes you want to run:\n for N in {1..8}; do ./util/align-dlib.py <path-to-raw-data> align affine <path-to-aligned-data> --size 96 &; done.\n Prune out directories with less than N (I use 10) images\n per class with ./util/prune-dataset.py <path-to-aligned-data> --numImagesThreshold <N> and\n then split the dataset into train and val subdirectories\n with ./util/create-train-val-split.py <path-to-aligned-data> <validation-ratio>.\n \n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "bamos", "commentT": "2016-01-11T21:46:58Z", "comment_text": "\n \t\tI've added an imgDim attribute to model definitions. The training code will error if it's not set or if the command-line option is incorrectly set. I decided to keep the command-line option so the user is aware of this resizing and in case a user's loading from a pre-trained model that they don't have a model definition for.\n -Brandon.\n \t\t"}}}, "commit": {"commit_id": "0479d0e3459633489a3a2a0110be6bd938fbd8b6", "commit_author": "Brandon Amos", "commitT": "2016-01-11 16:44:50-05:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.0", "commit_Nprams": "1.0"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "training\\donkey.lua", "file_new_name": "training\\donkey.lua", "file_complexity": {"file_NLOC": "60", "file_CCN": "2", "file_NToken": "401"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "28,29", "deleted_lines": "28,29"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "training\\model.lua", "file_new_name": "training\\model.lua", "file_complexity": {"file_NLOC": "91", "file_CCN": "10", "file_NToken": "504"}, "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "85,86", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "training\\opts.lua", "file_new_name": "training\\opts.lua", "file_complexity": {"file_NLOC": "49", "file_CCN": "7", "file_NToken": "377"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "48", "deleted_lines": "48", "method_info": {"method_name": "M.parse", "method_params": "arg", "method_startline": "11", "method_endline": "65", "method_complexity": {"method_NLOC": "42", "method_CCN": "6", "method_NToken": "334", "method_nesting_level": "0"}}}}}}}}