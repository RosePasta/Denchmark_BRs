{"BR": {"BR_id": "392", "BR_author": "lalitpagaria", "BRopenT": "2020-09-17T16:25:43Z", "BRcloseT": "2020-09-21T11:37:55Z", "BR_text": {"BRsummary": "Faiss document store create duplicate vector_ids", "BRdescription": "\n Describe the bug\n Faiss document store will generate duplicate vector ids when number of documents in write_documents and update_embeddings functions is greater than configured index_buffer_size.\n Error message\n No error message\n Expected behavior\n All written documents should have unique vector_ids\n Additional context\n Using enumerate index as vector_id causing this issue.\n <denchmark-code>for vector_id, doc in enumerate(document_objects[i: i + self.index_buffer_size]): \n </denchmark-code>\n \n My <denchmark-link:https://github.com/deepset-ai/haystack/pull/385>PR</denchmark-link>\n  will fix this issue, because of refactoring of vector_is generation logic.\n To Reproduce\n Following test data will reproduce this issue\n <denchmark-code>documents = [\n         {\"name\": \"name_1\", \"text\": \"text_1\", \"embedding\": np.random.rand(768).astype(np.float32)},\n         {\"name\": \"name_2\", \"text\": \"text_2\", \"embedding\": np.random.rand(768).astype(np.float32)},\n         {\"name\": \"name_3\", \"text\": \"text_3\", \"embedding\": np.random.rand(768).astype(np.float32)},\n     ]\n \n     document_store = FAISSDocumentStore(sql_url=\"sqlite:///haystack_test_faiss.db\", index_buffer_size=len(documents) - 1)\n     document_store.delete_all_documents()\n     document_store.write_documents(documents)\n     documents_indexed = document_store.get_all_documents()\n \n     # test if number of documents is correct\n     assert len(documents_indexed) == len(documents)\n \n     # test if two docs have same vector_is assigned\n     vector_ids = set()\n     for i, doc in enumerate(documents_indexed):\n         vector_ids.add(doc.meta[\"vector_id\"])\n     assert len(vector_ids) == len(documents)\n </denchmark-code>\n \n System:\n All system\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lalitpagaria", "commentT": "2020-09-17T16:26:34Z", "comment_text": "\n \t\tIt will be fixed by this PR <denchmark-link:https://github.com/deepset-ai/haystack/pull/385>#385</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lalitpagaria", "commentT": "2020-09-18T06:32:14Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/lalitpagaria>@lalitpagaria</denchmark-link>\n  ,\n Good catch! I can reproduce this bug. I saw <denchmark-link:https://github.com/deepset-ai/haystack/pull/385>#385</denchmark-link>\n , but I think for this bug we can have a simpler solution. I will therefore create a separate PR to not mix it with the bigger changes of <denchmark-link:https://github.com/deepset-ai/haystack/pull/385>#385</denchmark-link>\n . We will need a more careful review and discussion there.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lalitpagaria", "commentT": "2020-09-21T11:37:55Z", "comment_text": "\n \t\tFixed by <denchmark-link:https://github.com/deepset-ai/haystack/pull/395>#395</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "4c503158a7555becf5b67473ce0bcd55413b5544", "commit_author": "Malte Pietsch", "commitT": "2020-09-18 12:52:22+02:00", "commit_complexity": {"commit_NLOC": "0.0", "commit_CCN": "0.8823529411764706", "commit_Nprams": "0.9411764705882353"}, "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "haystack\\document_store\\faiss.py", "file_new_name": "haystack\\document_store\\faiss.py", "file_complexity": {"file_NLOC": "139", "file_CCN": "32", "file_NToken": "1140"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "66,73,77,81", "deleted_lines": "72", "method_info": {"method_name": "write_documents", "method_params": "self,None", "method_startline": "52", "method_endline": "82", "method_complexity": {"method_NLOC": "24", "method_CCN": "12", "method_NToken": "235", "method_nesting_level": "1"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "127,130,131,135,136,137,138,139", "deleted_lines": "126,130,131,132,133,134", "method_info": {"method_name": "update_embeddings", "method_params": "self,BaseRetriever,None", "method_startline": "106", "method_endline": "144", "method_complexity": {"method_NLOC": "24", "method_CCN": "8", "method_NToken": "237", "method_nesting_level": "1"}}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "test\\test_faiss.py", "file_new_name": "test\\test_faiss.py", "file_complexity": {"file_NLOC": "52", "file_CCN": "10", "file_NToken": "590"}, "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "9,16,17,25,26,27,28,29,30,31,32,41", "deleted_lines": "28", "method_info": {"method_name": "test_faiss_write_docs", "method_params": "document_store,index_buffer_size", "method_startline": "9", "method_endline": "41", "method_complexity": {"method_NLOC": "19", "method_CCN": "5", "method_NToken": "233", "method_nesting_level": "0"}}}, "hunk_1": {"Ismethod": 1, "added_lines": "45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83", "deleted_lines": null, "method_info": {"method_name": "test_faiss_update_docs", "method_params": "document_store,index_buffer_size", "method_startline": "45", "method_endline": "83", "method_complexity": {"method_NLOC": "25", "method_CCN": "5", "method_NToken": "273", "method_nesting_level": "0"}}}, "hunk_2": {"Ismethod": 1, "added_lines": "8,9,16,17,25,26,27,28", "deleted_lines": "6,28", "method_info": {"method_name": "test_faiss_indexing", "method_params": "document_store", "method_startline": "6", "method_endline": "28", "method_complexity": {"method_NLOC": "14", "method_CCN": "2", "method_NToken": "160", "method_nesting_level": "0"}}}}}}}}