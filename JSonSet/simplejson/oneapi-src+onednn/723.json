{"BR": {"BR_id": "723", "BR_author": "affranchi", "BRopenT": "2020-04-29T22:21:38Z", "BRcloseT": "2020-05-07T14:26:04Z", "BR_text": {"BRsummary": "Squeeze dimension?", "BRdescription": "\n Hello,\n I'm trying to squeeze dimension.\n For example, if I squeeze 2x2x2 tensor into 2x2 tensor with index 1 for the second dimension,\n (0,0) -> return (0,1,0)\n (0,1) -> return (0,1,1)\n (1,0) -> return (1,1,0)\n (1,1) -> return (1,1,0)\n I thought that I can archive this by combining submemory_desc and reshape like the below.\n <denchmark-code>auto mem_desc = dnnl::memory::desc({2,2,2}, f32, tag::abc);\n auto mem = dnnl::memory(mem_desc, engine);\n \n auto squeezed_desc = mem_desc.submemory_desc({2,1,2}, {0,1,0}).reshape({2,2});\n </denchmark-code>\n \n However, what I found is that this squeezed descriptor returns (0,0,0), (0,0,1), (0,1,0), (0,1,1) for reshaped index (0,0), (0,1), (1,0), (1,1). Also, if I do\n <denchmark-code>auto squeezed_desc = mem_desc.submemory_desc({2,1,2}, {0,0,0}).reshape({2,2});\n </denchmark-code>\n \n It returns (0,0,0), (0,0,1), (0,1,0), (0,1,1) values for reshaped index (0,0), (0,1), (1,0), (1,1).\n Does it happen because this reshape doesn't make sense in physical layout? If so, is there any better way to squeeze dimension?\n In addition to the question, I'm curious about submemory_desc behavior.\n I'm trying to do 2x2 matrix multiplication and store the results in 4x4 matrix.\n <denchmark-code>src_md = dnnl::memory::desc({2,2}, f32, tag::ab);\n weights_md = dnnl::memory::desc({2,2}, f32, tag::ab);\n dst_md = dnnl::memory::desc({4,4}, f32, tag::ab);\n \n // operator descriptor\n auto matmul_d = dnnl::matmul::desc(src_md, weights_md, dnnl::memory::desc(), dst_md.submemory_desc({2,2}, {2,2}));\n ...\n // execute..\n ...\n </denchmark-code>\n \n What I expect here is that the results will be placed in (2,2), (2,3), (3,2), (3,3) area. But looks it's always placed in (0,0), (0,1), (1,0), (1,1) in 4x4 matrix.\n Looks reorder copies submemory_desc area to the other memory. Does it mean that I need to copy memory to use submemory for other operators?\n Please, let me know if I misunderstood something.\n Thanks,\n Kenny\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "affranchi", "commentT": "2020-05-02T22:39:46Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/affranchi>@affranchi</denchmark-link>\n ,\n \n I thought that I can archive this by combining submemory_desc and reshape like the below.\n auto mem_desc = dnnl::memory::desc({2,2,2}, f32, tag::abc);\n auto mem = dnnl::memory(mem_desc, engine);\n \n auto squeezed_desc = mem_desc.submemory_desc({2,1,2}, {0,1,0}).reshape({2,2});\n \n \n To me the the code that you showed does exactly what you wanted:\n #include <stdio.h>\n #include \"dnnl.hpp\"\n \n using namespace dnnl;\n \n using dt = memory::data_type;\n using tag = memory::format_tag;\n \n memory::dim phys_offset(memory::desc &md, memory::dims position) {\n     // assumption: md has plain format, i.e. no dimensions are blocked\n     memory::dim offset = md.data.offset0; // do not forget about base offset\n     for (int d = 0; d < (int)md.dims().size(); ++d) {\n         offset += position[d] * md.data.format_desc.blocking.strides[d];\n     }\n     return offset;\n }\n \n int main() {\n     memory::desc mem_desc({2,2,2}, dt::f32, tag::abc);\n     memory::desc squeezed_desc = mem_desc.submemory_desc({2,1,2}, {0,1,0}).reshape({2,2});\n \n     printf(\"(position) --> offset\\n\");\n     for (int y: {0, 1})\n     for (int x: {0, 1})\n         printf(\"(%d,%d) --> %d\\n\", y, x, (int)phys_offset(squeezed_desc, {y, x}));\n \n     engine eng(engine::kind::cpu, 0);\n \n     std::vector<float> mem_data = {0, 1, 2, 3, 4, 5, 6, 7};\n     std::vector<float> out_data = {9, 9, 9, 9};\n \n     memory mem(mem_desc, eng, mem_data.data());\n     memory::desc out_desc({2, 2}, dt::f32, tag::ab);\n     memory out(out_desc, eng, out_data.data());\n \n     reorder({eng, squeezed_desc, eng, out_desc}).execute(stream(eng), mem, out);\n     printf(\"check out:\\n\"); for(auto o: out_data) printf(\"%.0f\\n\", o);\n \n     return 0;\n }\n \n $ g++ -std=c++11 gh723.cpp -ldnnl && ./a.out\n (position) --> offset\n (0,0) --> 2\n (0,1) --> 3\n (1,0) --> 6\n (1,1) --> 7\n check out:\n 2\n 3\n 6\n 7\n So as you initially planned:\n <denchmark-code>(0,0) -> return (0,1,0)  // phys offset in the original buffer --> 2\n (0,1) -> return (0,1,1)  // phys offset in the original buffer --> 3\n (1,0) -> return (1,1,0)  // phys offset in the original buffer --> 6\n (1,1) -> return (1,1,1)  // phys offset in the original buffer --> 7\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "affranchi", "commentT": "2020-05-02T22:59:18Z", "comment_text": "\n \t\t\n What I expect here is that the results will be placed in (2,2), (2,3), (3,2), (3,3) area. But looks it's always placed in (0,0), (0,1), (1,0), (1,1) in 4x4 matrix.\n \n This is a bug. With the following fix [1] the example does what you expected it to do.\n Thanks for the report! We will fix this.\n #include <algorithm>\n #include <cmath>\n #include <iostream>\n #include <string>\n #include <vector>\n \n #include \"dnnl.hpp\"\n \n using namespace dnnl;\n \n using tag = memory::format_tag;\n using dt = memory::data_type;\n \n int main() {\n     dnnl::engine engine(engine::kind::cpu, 0);\n     dnnl::stream engine_stream(engine);\n \n     std::vector<float> src_data = {1, 2, 3, 4};\n     std::vector<float> wei_data = {1, 0, 0, 1}; // identity matrix\n     std::vector<float> dst_data(4 * 4, 0.f);\n \n     auto src_md = memory::desc({2, 2}, dt::f32, tag::ab);\n     auto wei_md = memory::desc({2, 2}, dt::f32, tag::ab);\n     auto dst_md = memory::desc({4, 4}, dt::f32, tag::ab);\n \n     auto src_mem = memory(src_md, engine, src_data.data());\n     auto wei_mem = memory(wei_md, engine, wei_data.data());\n     auto dst_mem = memory(dst_md, engine, dst_data.data());\n \n     // Create operation descriptor\n     auto matmul_d = matmul::desc(src_md, wei_md, {}, dst_md.submemory_desc({2,2}, {2,2}));\n     auto matmul_pd = matmul::primitive_desc(matmul_d, engine);\n     auto matmul_prim = matmul(matmul_pd);\n \n     // Primitive arguments.\n     std::unordered_map<int, memory> matmul_args;\n     matmul_args.insert({DNNL_ARG_SRC, src_mem});\n     matmul_args.insert({DNNL_ARG_WEIGHTS, wei_mem});\n     matmul_args.insert({DNNL_ARG_DST, dst_mem});\n     matmul_prim.execute(engine_stream, matmul_args);\n     engine_stream.wait();\n \n     for (int i = 0; i < 4; ++i)\n     for (int j = 0; j < 4; ++j)\n         printf(\"%.0f%c\", dst_data[i * 4 + j], j == 3 ? '\\n' : ' ');\n \n     return 0;\n }\n $  g++ -std=c++11 gh723_matmul.cpp -ldnnl && ./a.out\n 0 0 0 0\n 0 0 0 0\n 0 0 1 2\n 0 0 3 4\n [1] The fix: we forgot to shift the original pointers by offset0. The strides (leading dimensions) were computed correctly though.\n diff --git a/src/cpu/matmul/gemm_f32_matmul.cpp b/src/cpu/matmul/gemm_f32_matmul.cpp\n index f0e77b00e..69df852a3 100644\n --- a/src/cpu/matmul/gemm_f32_matmul.cpp\n +++ b/src/cpu/matmul/gemm_f32_matmul.cpp\n @@ -117,17 +117,23 @@ status_t gemm_f32_matmul_t::pd_t::check_and_configure_attributes() {\n  }\n \n  status_t gemm_f32_matmul_t::execute_ref(const exec_ctx_t &ctx) const {\n -    const auto src = CTX_IN_MEM(const src_data_t *, DNNL_ARG_SRC);\n -    const auto weights = CTX_IN_MEM(const weights_data_t *, DNNL_ARG_WEIGHTS);\n -    const auto bias = CTX_IN_MEM(const char *, DNNL_ARG_BIAS);\n +    auto src = CTX_IN_MEM(const src_data_t *, DNNL_ARG_SRC);\n +    auto weights = CTX_IN_MEM(const weights_data_t *, DNNL_ARG_WEIGHTS);\n +    auto bias = CTX_IN_MEM(const char *, DNNL_ARG_BIAS);\n      auto dst = CTX_OUT_MEM(dst_data_t *, DNNL_ARG_DST);\n \n      DEFINE_SCALES_BUFFER(scales);\n \n      const auto src_d = ctx.memory_mdw(DNNL_ARG_SRC, pd()->src_md());\n      const auto weights_d = ctx.memory_mdw(DNNL_ARG_WEIGHTS, pd()->weights_md());\n +    const auto bias_d = ctx.memory_mdw(DNNL_ARG_BIAS, pd()->weights_md(1));\n      const auto dst_d = ctx.memory_mdw(DNNL_ARG_DST, pd()->dst_md());\n \n +    src += src_d.offset0();\n +    weights += weights_d.offset0();\n +    if (bias) bias += bias_d.offset0();\n +    dst += dst_d.offset0();\n +\n      const gemm_based::params_t &params = pd()->params();\n \n      const auto &dst_bd = dst_d.blocking_desc();\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "affranchi", "commentT": "2020-05-03T16:31:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emfomenk>@emfomenk</denchmark-link>\n  Thank you for your answer and your work for fix.\n Looks it's not merged yet. Could you let me know the ticket link so that I can track when it's merged? Or, could you let me know if it's merged?\n Thanks,\n Kenny\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "affranchi", "commentT": "2020-05-03T17:03:59Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/affranchi>@affranchi</denchmark-link>\n ,\n You can track this issue, which we will close with the corresponding fix promotion.\n \t\t"}}}, "commit": {"commit_id": "a1c2fdefbd12dedf078bf50dd09c1d29424817c7", "commit_author": "Fomenko, Evarist M", "commitT": "2020-05-05 17:02:04-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\cpu\\matmul\\gemm_bf16_matmul.cpp", "file_new_name": "src\\cpu\\matmul\\gemm_bf16_matmul.cpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "133,134,135,140,141,142,143,144,145,146,147,148,149,150", "deleted_lines": "133,134,135,148,149,150,151", "method_info": {"method_name": "dnnl::impl::cpu::matmul::gemm_bf16_matmul_t<dst_type>::execute_ref", "method_params": "ctx", "method_startline": "131", "method_endline": "260"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\cpu\\matmul\\gemm_f32_matmul.cpp", "file_new_name": "src\\cpu\\matmul\\gemm_f32_matmul.cpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "120,121,122,129,132,133,134,135,136,137", "deleted_lines": "120,121,122", "method_info": {"method_name": "dnnl::impl::cpu::matmul::gemm_f32_matmul_t::execute_ref", "method_params": "ctx", "method_startline": "119", "method_endline": "217"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\cpu\\matmul\\gemm_x8s8s32x_matmul.cpp", "file_new_name": "src\\cpu\\matmul\\gemm_x8s8s32x_matmul.cpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "150,151,152,160,161,162,163,164,165,166,167,168,169,170", "deleted_lines": "150,151,152,169,170,171,172", "method_info": {"method_name": "dnnl::impl::cpu::matmul::gemm_x8s8s32x_matmul_t<src_type,weights_type,dst_type>::execute_ref", "method_params": "ctx", "method_startline": "146", "method_endline": "324"}}}}}}}