{"BR": {"BR_id": "575", "BR_author": "hamletbatista", "BRopenT": "2019-11-20T19:55:43Z", "BRcloseT": "2020-02-27T23:42:36Z", "BR_text": {"BRsummary": "Can't parse SavedModel to use in TensorflowJs", "BRdescription": "\n Describe the bug\n A clear and concise description of what the bug is.\n I'm trying to export a trained model so I can run inference using TensorflowJs, but the exported .pb doesn't work with the TensorflowJs converter tool. I get this error:\n WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.init (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n Instructions for updating:\n If using Keras pass *_constraint arguments to layers.\n Traceback (most recent call last):\n File \"/usr/local/bin/tensorflowjs_converter\", line 8, in \n sys.exit(pip_main())\n File \"/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py\", line 638, in pip_main\n main([' '.join(sys.argv[1:])])\n File \"/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py\", line 642, in main\n convert(argv[0].split(' '))\n File \"/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py\", line 591, in convert\n strip_debug_ops=args.strip_debug_ops)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py\", line 419, in convert_tf_saved_model\n model = load(saved_model_dir, saved_model_tags)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load.py\", line 519, in load\n return load_internal(export_dir, tags)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load.py\", line 550, in load_internal\n root = load_v1_in_v2.load(export_dir, tags)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py\", line 239, in load\n return loader.load(tags=tags)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py\", line 222, in load\n signature_functions = self._extract_signatures(wrapped, meta_graph_def)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/load_v1_in_v2.py\", line 138, in _extract_signatures\n signature_fn = wrapped.prune(feeds=feeds, fetches=fetches)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/wrap_function.py\", line 320, in prune\n sources=flat_feeds + self.graph.internal_captures)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/lift_to_graph.py\", line 260, in lift_to_graph\n add_sources=add_sources))\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/op_selector.py\", line 413, in map_subgraph\n % (repr(init_tensor), repr(op), _path_from(op, init_tensor, sources)))\n tensorflow.python.ops.op_selector.UnliftableError: A SavedModel signature needs an input for each placeholder the signature's outputs use. An output for signature 'predict' depends on a placeholder which is not an input (i.e. the placeholder is not fed a value).\n Unable to lift tensor <tf.Tensor 'Category0/predictions_Category0/predictions_Category0:0' shape=(?,) dtype=int64> because it depends transitively on placeholder <tf.Operation 'is_training' type=Placeholder> via at least one path, e.g.: Category0/predictions_Category0/predictions_Category0 (ArgMax) <- Category0/predictions_Category0/add (Add) <- Category0/predictions_Category0/MatMul (MatMul) <- concat_combiner/concat_combiner (Identity) <- concat_combiner/concat (Identity) <- Questions/Questions (Identity) <- Questions/dropout/cond/Merge (Merge) <- Questions/dropout/cond/dropout/mul_1 (Mul) <- Questions/dropout/cond/dropout/Cast (Cast) <- Questions/dropout/cond/dropout/GreaterEqual (GreaterEqual) <- Questions/dropout/cond/dropout/rate (Const) <- Questions/dropout/cond/switch_t (Identity) <- Questions/dropout/cond/Switch (Switch) <- is_training (Placeholder)\n \n Steps to reproduce the behavior:\n You can follow my steps in this colab notebook <denchmark-link:https://colab.research.google.com/drive/1c1REIK3G5FzwuCxmO8R0xA_0ODDlC57z#scrollTo=vNudSgJAZ7JB>https://colab.research.google.com/drive/1c1REIK3G5FzwuCxmO8R0xA_0ODDlC57z#scrollTo=vNudSgJAZ7JB</denchmark-link>\n \n Please provide code, yaml definition file and a sample of data in order to entirely reproduce the issue.\n Issues that are not reproducible will be ignored.\n Everything is in the Colab notebook.\n Expected behavior\n A clear and concise description of what you expected to happen.\n I am hoping to load the trained model in TensorflowJs\n Screenshots\n If applicable, add screenshots to help explain your problem.\n Environment (please complete the following information):\n \n OS: macOS\n Version: Catalina\n Python version: 3.6.8\n Ludwig version: 0.2.1\n \n \n Add any other context about the problem here.\n I tried the ideas in this article <denchmark-link:https://github.com/ludwig-ai/ludwig/issues/329#issuecomment-548854347>#329 (comment)</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hamletbatista", "commentT": "2019-11-21T19:52:53Z", "comment_text": "\n \t\tThis is not a functionality we currently support, so will mark it as enhancement, but at the same time from the log it seems the problem has to do with the way we save the SavedModel rather than a tfjs specific thing, so we will investigate the issue, and that could potentially solve the problem for tfjs too.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hamletbatista", "commentT": "2019-11-26T00:13:46Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/w4nderlust>@w4nderlust</denchmark-link>\n  thanks\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hamletbatista", "commentT": "2020-02-04T18:05:44Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/hamletbatista>@hamletbatista</denchmark-link>\n , would you mind providing the model_definition.yaml which is referenced in the codelab? Thanks\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hamletbatista", "commentT": "2020-02-04T21:08:29Z", "comment_text": "\n \t\tActually never mind, I see it in the template.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "hamletbatista", "commentT": "2020-02-04T22:25:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ydudin3>@ydudin3</denchmark-link>\n  glad to know. Please let me know if are able to get this to work\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "hamletbatista", "commentT": "2020-02-06T01:37:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hamletbatista>@hamletbatista</denchmark-link>\n  it seems from the logs that output tensors get appended to the input_tensors list\n For example printing input_tensors yields:\n {'Category0': <tf.Tensor 'Category0/Category0_placeholder:0' shape=(?,) dtype=int64>, 'Category2': <tf.Tensor 'Category2/Category2_placeholder:0' shape=(?,) dtype=int64>, 'Questions': <tf.Tensor 'Questions/Questions_placeholder:0' shape=(?, ?) dtype=int32>}\n It looks like get_tensors function has these few lines in it:\n for output_feature in model_definition['output_features']: input_tensors[output_feature['name']] = getattr(model, output_feature['name'])\n Is this intentional? I wonder if that's what's causing model load failure.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "hamletbatista", "commentT": "2020-02-06T20:41:23Z", "comment_text": "\n \t\tThat is in the cell after \"try it again\". Not sure why you are running it that way instead of using model.save_savedmodel().\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "hamletbatista", "commentT": "2020-02-06T21:02:34Z", "comment_text": "\n \t\t\n That is in the cell after \"try it again\". Not sure why you are running it that way instead of using model.save_savedmodel().\n \n I tried that first. See cells above.\n <denchmark-link:https://user-images.githubusercontent.com/1514243/73978311-133cd880-48fa-11ea-92d8-ba268f75744c.png></denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "hamletbatista", "commentT": "2020-02-06T21:04:42Z", "comment_text": "\n \t\t\n @hamletbatista it seems from the logs that output tensors get appended to the input_tensors list\n For example printing input_tensors yields:\n {'Category0': <tf.Tensor 'Category0/Category0_placeholder:0' shape=(?,) dtype=int64>, 'Category2': <tf.Tensor 'Category2/Category2_placeholder:0' shape=(?,) dtype=int64>, 'Questions': <tf.Tensor 'Questions/Questions_placeholder:0' shape=(?, ?) dtype=int32>}\n It looks like get_tensors function has these few lines in it:\n for output_feature in model_definition['output_features']: input_tensors[output_feature['name']] = getattr(model, output_feature['name'])\n Is this intentional? I wonder if that's what's causing model load failure.\n \n I haven't touched this code in a while, but I added links in the comments to where I was finding suggestions to fix the issue.\n The suggestion came from this comment it appears <denchmark-link:https://github.com/ludwig-ai/ludwig/issues/329#issuecomment-508777581>#329 (comment)</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "hamletbatista", "commentT": "2020-02-06T21:06:14Z", "comment_text": "\n \t\tI took a different route to solve this using HuggingFace's library, but yours would make the tutorial much simpler to follow\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "hamletbatista", "commentT": "2020-02-06T21:24:25Z", "comment_text": "\n \t\t\n I tried that first. See cells above.\n \n Yes but it is commented out, and I don't see errors there, what was wrong with it?\n \n I took a different route to solve this using HuggingFace's library, but yours would make the tutorial much simpler to follow\n \n We are adding import of Huggingface's transformer library in the next version of Ludwig. Wonder how are you serving it as even the smaller distilled model is really expensive to use at inference time.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "hamletbatista", "commentT": "2020-02-06T23:12:33Z", "comment_text": "\n \t\t\n \n I tried that first. See cells above.\n \n Yes but it is commented out, and I don't see errors there, what was wrong with it?\n \n There was no error or stack trace. The issue was the file generated seemed corrupted or incomplete when I tried to load it.\n I will give it another try over the weekend.\n \n \n I took a different route to solve this using HuggingFace's library, but yours would make the tutorial much simpler to follow\n \n We are adding import of Huggingface's transformer library in the next version of Ludwig. Wonder how are you serving it as even the smaller distilled model is really expensive to use at inference time.\n \n Yes. I have that problem and this is mostly a learning exercise to teach marketers, not for production use. I have in my queue to investigate this research next <denchmark-link:https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/>https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/</denchmark-link>\n \n It seems to solve that issue.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "hamletbatista", "commentT": "2020-02-07T01:20:05Z", "comment_text": "\n \t\t\n There was no error or stack trace. The issue was the file generated seemed corrupted or incomplete when I tried to load it.\n \n Got it, but you can understand that I need to se the error you were getting about corrupted file, otherwise it's difficult to figure out what the problem is.\n Ideally you could provide a minimal self contained reproducible zip containing either data and a python script (data can be generated with the data/dataset_sythesizer.py script if you can't share it) or data + yaml file + command to run it.\n \n Yes. I have that problem and this is mostly a learning exercise to teach marketers, not for production use. I have in my queue to investigate this research next https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/\n It seems to solve that issue.\n \n It was tested on 3 layers bert, the latency is much higher on the full model. Still, it's a step forward ;)\n Anyway, I thought you usecase was fast inference at deployment time, but if your goal is just demoing, and you don't care about a super scalable inference pipeline, then you can train a model with Ludwig and then serve it with\n <denchmark-code>ludwig serve --model_path path/to/trained/model\n </denchmark-code>\n \n and it will launch a REST API server you can query easily. More info in the <denchmark-link:https://uber.github.io/ludwig/user_guide/#serve>User Guide</denchmark-link>\n .\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "hamletbatista", "commentT": "2020-02-07T09:24:36Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/w4nderlust>@w4nderlust</denchmark-link>\n  just FYI:  works incorrectly now. That's because of wrong placeholders' names <denchmark-link:https://github.com/ludwig-ai/ludwig/issues/329#issuecomment-548854347>#329 (comment)</denchmark-link>\n \n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "hamletbatista", "commentT": "2020-02-07T12:45:38Z", "comment_text": "\n \t\t\n \n There was no error or stack trace. The issue was the file generated seemed corrupted or incomplete when I tried to load it.\n \n Got it, but you can understand that I need to se the error you were getting about corrupted file, otherwise it's difficult to figure out what the problem is.\n Ideally you could provide a minimal self contained reproducible zip containing either data and a python script (data can be generated with the data/dataset_sythesizer.py script if you can't share it) or data + yaml file + command to run it.\n \n Yes. I will have time over the weekend :)\n \n \n Yes. I have that problem and this is mostly a learning exercise to teach marketers, not for production use. I have in my queue to investigate this research next https://cloudblogs.microsoft.com/opensource/2020/01/21/microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/\n It seems to solve that issue.\n \n It was tested on 3 layers bert, the latency is much higher on the full model. Still, it's a step forward ;)\n \n Interesting. I will see if I can get decent accuracy. Thanks for the insights.\n \n Anyway, I thought you usecase was fast inference at deployment time, but if your goal is just demoing, and you don't care about a super scalable inference pipeline, then you can train a model with Ludwig and then serve it with\n ludwig serve --model_path path/to/trained/model\n \n and it will launch a REST API server you can query easily. More info in the User Guide.\n \n I need to run the model in JS to embed in Google Sheets and Excel. Fetching from a serving URL would be my fall back option.\n Thanks\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "hamletbatista", "commentT": "2020-02-07T18:51:52Z", "comment_text": "\n \t\t\n @w4nderlust just FYI: save_savemodel works incorrectly now. That's because of wrong placeholders' names #329 (comment)\n \n Thanks, yes we are working on it. <denchmark-link:https://github.com/ydudin3>@ydudin3</denchmark-link>\n \n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "hamletbatista", "commentT": "2020-02-27T23:42:36Z", "comment_text": "\n \t\tThe merged PR should have solved the issue. There's also an integration test for SavedModel now that shows how to load and save SavedModels and what kind of preprocessing and postprocessing you need to do in order to map data to tensors and prediction tensors to data: <denchmark-link:https://github.com/uber/ludwig/blob/master/tests/integration_tests/test_savedmodel.py>https://github.com/uber/ludwig/blob/master/tests/integration_tests/test_savedmodel.py</denchmark-link>\n . Let us know if you have further problems.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "hamletbatista", "commentT": "2020-02-28T13:40:36Z", "comment_text": "\n \t\tThanks. I will check this out. This was sorely needed!\n \t\t"}}}, "commit": {"commit_id": "4039f93e605ed929e4f4a75c725bb70e148df92b", "commit_author": "Piero Molino", "commitT": "2020-02-27 15:39:59-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "ludwig\\cli.py", "file_new_name": "ludwig\\cli.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "106,107,108,109", "deleted_lines": null, "method_info": {"method_name": "export", "method_params": "self", "method_startline": "106", "method_endline": "109"}}, "hunk_1": {"Ismethod": 1, "added_lines": "111,112,113,114", "deleted_lines": null, "method_info": {"method_name": "saved_model_predict", "method_params": "self", "method_startline": "111", "method_endline": "114"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "ludwig\\models\\model.py", "file_new_name": "ludwig\\models\\model.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1454,1465,1466", "deleted_lines": null, "method_info": {"method_name": "load", "method_params": "load_path,gpus,gpu_fraction,use_horovod", "method_startline": "1454", "method_endline": "1467"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1402,1403,1404,1405,1406,1408,1409,1410,1412,1413,1414,1415,1416,1417,1418,1419,1420,1422,1423,1424,1425,1426,1427,1428,1429,1430,1432,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448", "deleted_lines": "1403,1404,1405,1406,1409,1410,1411,1412,1413,1416,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1435", "method_info": {"method_name": "save_savedmodel", "method_params": "self,save_path", "method_startline": "1400", "method_endline": "1448"}}, "hunk_2": {"Ismethod": 1, "added_lines": "1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446", "deleted_lines": "1435", "method_info": {"method_name": "load", "method_params": "load_path,use_horovod", "method_startline": "1435", "method_endline": "1446"}}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\integration_tests\\test_savedmodel.py"}}}}