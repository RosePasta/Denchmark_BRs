{"BR": {"BR_id": "131", "BR_author": "dathudeptrai", "BRopenT": "2020-04-24T02:03:02Z", "BRcloseT": "2020-04-24T04:45:14Z", "BR_text": {"BRsummary": "Possible BUG may make your code not achieve the best performance. (background noise, etc.)", "BRdescription": "\n Hi <denchmark-link:https://github.com/kan-bayashi>@kan-bayashi</denchmark-link>\n  , there is a bug i think that make ur code cann't achieve the best performance for both melgan and PWG. That is after training 1 step generator, you should re-compute y_ then use this y_ for discriminator, but seem ur code not do that. In my experiment, re-compute y_ is crucial for obtain best quality. I have my tensorflow code for melgan, i can get the same performance as ur code but just around 2M steps from scratch (don't need PWG auxiliary loss to help convergence speed), but if i don't recompute y_, my tf code at 2M steps not good as 2M steps when recompute y_\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dathudeptrai", "commentT": "2020-04-24T03:59:59Z", "comment_text": "\n \t\tThank you for your suggestions!\n I'm not sure which is the standard for GAN training, but it is better to change the update manner since your experiments shows better performance.\n I will make PR.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:01:47Z", "comment_text": "\n \t\tBTW, how was the final quality? Is it improved? or just related to convergence speed?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:09:14Z", "comment_text": "\n \t\tI think that make sense, discriminator should use newest y_ for training, that make discriminator penalize generator better. U can ref two implementation on github (<denchmark-link:https://github.com/seungwonpark/melgan/blob/master/utils/train.py/#L85>https://github.com/seungwonpark/melgan/blob/master/utils/train.py/#L85</denchmark-link>\n ) and official code (<denchmark-link:https://github.com/descriptinc/melgan-neurips/blob/master/scripts/train.py/#L156>https://github.com/descriptinc/melgan-neurips/blob/master/scripts/train.py/#L156</denchmark-link>\n ). Note that official code training discriminator before generator but it's still re-compute D-fake for training generator. The quality on my language (vietnamese) is improve significantly. I'm training Ljspeech now on melgan only, i have 1M4 val audio here. I believe that the same improvement can achieve with PWG since the manner correctly.\n <denchmark-link:https://github.com/kan-bayashi/ParallelWaveGAN/files/4526567/1460000steps.zip>1460000steps.zip</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:18:57Z", "comment_text": "\n \t\tThank you for sharing samples!\n It seems very good :) I will follow your suggested manner!\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:22:21Z", "comment_text": "\n \t\tBTW, i'm creating TF framework for Speech, now focus TTS :)), just like ur ESPNet :D. TF now training faster than pytorch with exact parameter, batch_size, batch_max_step, the improvement on inference speed you knew before: D.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:34:29Z", "comment_text": "\n \t\tThat sounds nice :)\n But I love pytorch so I want facebook to make it faster \ud83d\ude80\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:37:16Z", "comment_text": "\n \t\thaha BTW, what is the license of this repo and ur ESPNET :)), i cann't retrain anything, there are many model and dataset, i plan to provide some training script, pretrained on my TF framework and the rest is convert from ur pytorch pretrained then support for inference only :D. Can i do that ?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:39:48Z", "comment_text": "\n \t\tThat's very cool.\n This repository is MIT license and ESPnet is Apache 2.0, so you can do it!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "dathudeptrai", "commentT": "2020-04-24T04:44:45Z", "comment_text": "\n \t\tok that nice, i will make TF models and pytorch models synchronized and use the same preprocesing here so atleast user can use pytorch for training and TF for inference :D. Again, thank for ur hard work :)).\n \t\t"}}}, "commit": {"commit_id": "aabd569d807823d0285779330f98aeb6e05e01dc", "commit_author": "kan-bayashi", "commitT": "2020-04-24 13:26:40+09:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "parallel_wavegan\\bin\\train.py", "file_new_name": "parallel_wavegan\\bin\\train.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "222,223,224", "deleted_lines": null, "method_info": {"method_name": "_train_step", "method_params": "self,batch", "method_startline": "157", "method_endline": "265"}}}}}}}