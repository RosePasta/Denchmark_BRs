{"BR": {"BR_id": "503", "BR_author": "KeAWang", "BRopenT": "2019-02-07T03:29:16Z", "BRcloseT": "2019-02-07T19:18:29Z", "BR_text": {"BRsummary": "Fast pred var? More like 1/3 as fast pred var", "BRdescription": "\n It seems that after commit <denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/0ff64169766bd22d1d8cae35ef0ab3f0d6f16e2e>0ff6416</denchmark-link>\n , fast_pred_var is now 3x slower.\n Using the following code\n <denchmark-code>model.eval()\n likelihood.eval()\n import time\n \n for i in range(10):\n     start = time.time()\n     with gpytorch.settings.fast_pred_var():\n         preds = model(test_x)\n     print(time.time() - start)\n </denchmark-code>\n \n I get\n <denchmark-code>0.05411267280578613\n 0.0015854835510253906\n 0.0012111663818359375\n 0.0011920928955078125\n 0.0011920928955078125\n 0.0011932849884033203\n 0.0012063980102539062\n 0.0011904239654541016\n 0.0011909008026123047\n 0.00119781494140625\n </denchmark-code>\n \n from the previous commit <denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/a1e8bcc26a4b432776620b242023adb7acf206e3>a1e8bcc</denchmark-link>\n .\n On the other hand, I get\n <denchmark-code>0.05980277061462402\n 0.003302335739135742\n 0.0028259754180908203\n 0.0028007030487060547\n 0.0028228759765625\n 0.002866983413696289\n 0.0028259754180908203\n 0.0033342838287353516\n 0.0032806396484375\n 0.003268718719482422\n </denchmark-code>\n \n from the breaking commit <denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/0ff64169766bd22d1d8cae35ef0ab3f0d6f16e2e>0ff6416</denchmark-link>\n .\n The code used to train the model is taken from the official fast_pred_var notebook:\n <denchmark-code>import math\n import torch\n import gpytorch\n from matplotlib import pyplot as plt\n \n # Make plots inline\n %matplotlib inline\n \n import urllib.request\n import os.path\n from scipy.io import loadmat\n from math import floor\n \n if not os.path.isfile('skillcraft.mat'):\n     print('Downloading \\'skillcraft\\' UCI dataset...')\n     urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1xQ1vgx_bOsLDQ3RLbJwPxMyJHW7U9Eqd', 'skillcraft.mat')\n     \n data = torch.Tensor(loadmat('skillcraft.mat')['data'])\n X = data[:, :-1]\n X = X - X.min(0)[0]\n X = 2 * (X / X.max(0)[0]) - 1\n y = data[:, -1]\n \n # Use the first 80% of the data for training, and the last 20% for testing.\n train_n = int(floor(0.4*len(X)))\n \n train_x = X[:train_n, :].contiguous().cuda()\n train_y = y[:train_n].contiguous().cuda()\n \n test_x = X[train_n:, :].contiguous().cuda()\n test_y = y[train_n:].contiguous().cuda()\n \n class GPRegressionModel(gpytorch.models.ExactGP):\n         def __init__(self, train_x, train_y, likelihood):\n             super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n             self.mean_module = gpytorch.means.ConstantMean()\n             self.covar_module = gpytorch.kernels.ScaleKernel(\n                 gpytorch.kernels.RBFKernel()\n             )\n             \n         def forward(self, x):\n             mean_x = self.mean_module(x)\n             covar_x = self.covar_module(x)\n             return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n \n likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n model = GPRegressionModel(train_x, train_y, likelihood).cuda()\n \n # Find optimal model hyperparameters\n model.train()\n likelihood.train()\n \n # Use the adam optimizer\n optimizer = torch.optim.Adam([\n     {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n ], lr=0.1)\n \n # \"Loss\" for GPs - the marginal log likelihood\n mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n \n training_iterations = 20\n def train():\n     for i in range(training_iterations):\n         optimizer.zero_grad()\n         output = model(train_x)\n         loss = -mll(output, train_y)\n         loss.backward()\n         print('Iter %d/%d - Loss: %.3f' % (i + 1,\n                                            training_iterations,\n                                            loss.item()))\n         optimizer.step()\n </denchmark-code>\n \n On the other hand, this does not appear to affect forward passes without the fast_pred_var option. Although that could just because forwards without fast_pred_var are already very slow.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "KeAWang", "commentT": "2019-02-07T03:50:49Z", "comment_text": "\n \t\tMy best guess: the kernel matrix computation for k_x*X is taking slightly longer (about 0.002s longer) as a result of something in the commit. Maybe something gets optimized poorly by the JIT when things are inlined? <denchmark-link:https://github.com/gpleiss>@gpleiss</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "KeAWang", "commentT": "2019-02-07T19:18:29Z", "comment_text": "\n \t\tClosed by <denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/507>#507</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "d009c7757a48abad6906373190db1eb30e82098f", "commit_author": "Alex Wang", "commitT": "2019-02-07 13:06:13-05:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "gpytorch\\kernels\\kernel.py", "file_new_name": "gpytorch\\kernels\\kernel.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "236,237,238,239", "deleted_lines": null, "method_info": {"method_name": "__getstate__", "method_params": "self", "method_startline": "236", "method_endline": "239"}}, "hunk_1": {"Ismethod": 1, "added_lines": "241,242", "deleted_lines": "242", "method_info": {"method_name": "__setstate__", "method_params": "self,d", "method_startline": "241", "method_endline": "242"}}, "hunk_2": {"Ismethod": 1, "added_lines": "236,237,238,239,240,241,242,243", "deleted_lines": "242", "method_info": {"method_name": "_covar_dist", "method_params": "self,x1,x2,diag,batch_dims,square_dist,postprocess_func,params", "method_startline": "235", "method_endline": "243"}}, "hunk_3": {"Ismethod": 1, "added_lines": "251", "deleted_lines": null, "method_info": {"method_name": "_covar_dist", "method_params": "self,x1,x2,diag,batch_dims,square_dist,dist_postprocess_func,params", "method_startline": "244", "method_endline": "252"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "gpytorch\\kernels\\rbf_kernel.py", "file_new_name": "gpytorch\\kernels\\rbf_kernel.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "99", "deleted_lines": "99", "method_info": {"method_name": "forward", "method_params": "self,x1,x2,diag,params", "method_startline": "96", "method_endline": "100"}}}}}}}