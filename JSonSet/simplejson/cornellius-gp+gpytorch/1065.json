{"BR": {"BR_id": "1065", "BR_author": "LemonPi", "BRopenT": "2020-03-01T22:15:35Z", "BRcloseT": "2020-04-07T17:08:47Z", "BR_text": {"BRsummary": "[Bug] Lazy Tensor Internal Indexing Error for Large Variance Evaluation (MultitaskMultivariateNormal)", "BRdescription": "\n <denchmark-h:h1>\ud83d\udc1b Bug</denchmark-h>\n \n When calculating the variance of a MultitaskMultivariateNormal, evaluating self.lazy_covariance_matrix.diag() returns an indexing error when the number of query points is too large. When the number of data points is small (<150), inside the MultitaskMultivariateNormal we have\n <denchmark-code>self.lazy_covariance_matrix.lazy_tensors\n Out[2]: \n (<gpytorch.lazy.non_lazy_tensor.NonLazyTensor at 0x125d53350>,\n  <gpytorch.lazy.added_diag_lazy_tensor.AddedDiagLazyTensor at 0x12cd99050>)\n </denchmark-code>\n \n But when we increase the number of query points we get\n <denchmark-code>self.lazy_covariance_matrix.lazy_tensors\n Out[4]: \n (<gpytorch.lazy.interpolated_lazy_tensor.InterpolatedLazyTensor at 0x131338410>,\n  <gpytorch.lazy.matmul_lazy_tensor.MatmulLazyTensor at 0x131338550>,\n  <gpytorch.lazy.block_diag_lazy_tensor.BlockDiagLazyTensor at 0x131338290>,\n  <gpytorch.lazy.diag_lazy_tensor.DiagLazyTensor at 0x131338610>)\n </denchmark-code>\n \n Having different implementation strategy depending on size is fine, but the strategy for large batches errors out when evaluating variance.\n <denchmark-h:h2>To reproduce</denchmark-h>\n \n Pretty much the Batch Independent Multioutput GP <denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/03_Multitask_Exact_GPs/Batch_Independent_Multioutput_GP.html>tutorial</denchmark-link>\n  but increase the  to 200 (no problems if you use 51 as in the tutorial)\n ** Code snippet to reproduce **\n import math\n import torch\n import gpytorch\n from matplotlib import pyplot as plt\n \n train_x = torch.linspace(0, 1, 100)\n ntest = 200\n \n train_y = torch.stack([\n     torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.shape[0]) * 0.2,\n     torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.shape[0]) * 0.8,\n ], -1)\n ny = train_y.shape[1]\n \n \n class BatchIndependentMultitaskGPModel(gpytorch.models.ExactGP):\n     def __init__(self, train_x, train_y, likelihood):\n         super().__init__(train_x, train_y, likelihood)\n         self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([ny]))\n         self.covar_module = gpytorch.kernels.ScaleKernel(\n             gpytorch.kernels.RBFKernel(batch_shape=torch.Size([ny])),\n             batch_shape=torch.Size([ny])\n         )\n \n     def forward(self, x):\n         mean_x = self.mean_module(x)\n         covar_x = self.covar_module(x)\n         return gpytorch.distributions.MultitaskMultivariateNormal.from_batch_mvn(\n             gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n         )\n \n \n likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=ny)\n model = BatchIndependentMultitaskGPModel(train_x, train_y, likelihood)\n \n training_iterations = 50\n \n # Find optimal model hyperparameters\n model.train()\n likelihood.train()\n \n # Use the adam optimizer\n optimizer = torch.optim.Adam([\n     {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n ], lr=0.1)\n \n # \"Loss\" for GPs - the marginal log likelihood\n mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n \n for i in range(training_iterations):\n     optimizer.zero_grad()\n     output = model(train_x)\n     loss = -mll(output, train_y)\n     loss.backward()\n     print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n     optimizer.step()\n # Set into eval mode\n model.eval()\n likelihood.eval()\n \n # Initialize plots\n f, axes = plt.subplots(1, ny, figsize=(8, 3))\n \n # Make predictions\n with torch.no_grad():\n     test_x = torch.linspace(0, 1, ntest)\n     xx = torch.stack([test_x, torch.linspace(0, 2, test_x.shape[0])], -1)\n     predictions = likelihood(model(test_x))\n     mean = predictions.mean\n     lower, upper = predictions.confidence_region()\n \n # This contains predictions for both tasks, flattened out\n # The first half of the predictions is for the first task\n # The second half is for the second task\n \n for i in range(train_y.shape[1]):\n     # Plot training data as black stars\n     axes[i].plot(train_x.detach().numpy(), train_y[:, i].detach().numpy(), 'k*')\n     # Predictive mean as blue line\n     axes[i].plot(test_x.numpy(), mean[:, i].numpy(), 'b')\n     # Shade in confidence\n     axes[i].fill_between(test_x.numpy(), lower[:, i].numpy(), upper[:, i].numpy(), alpha=0.5)\n     axes[i].set_ylim([-3, 3])\n axes[-1].legend(['Observed Data', 'Mean', 'Confidence'])\n axes[-1].set_title('Observed Values (Likelihood)')\n plt.show()\n ** Stack trace/error message **\n <denchmark-code>Traceback (most recent call last):\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 88, in _getitem\n     x1 = x1[(*batch_indices, row_index, dim_index)]\n IndexError: too many indices for tensor of dimension 2\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"/Users/johnsonzhong/Research/meta_contact/simulation/temp.py\", line 70, in <module>\n     lower, upper = predictions.confidence_region()\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/distributions/multivariate_normal.py\", line 80, in confidence_region\n     std2 = self.stddev.mul_(2)\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/distributions/distribution.py\", line 111, in stddev\n     return self.variance.sqrt()\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/distributions/multitask_multivariate_normal.py\", line 219, in variance\n     var = super().variance\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/distributions/multivariate_normal.py\", line 189, in variance\n     diag = self.lazy_covariance_matrix.diag()\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/sum_lazy_tensor.py\", line 96, in diag\n     return sum(lazy_tensor.diag().contiguous() for lazy_tensor in self.lazy_tensors)\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/sum_lazy_tensor.py\", line 96, in <genexpr>\n     return sum(lazy_tensor.diag().contiguous() for lazy_tensor in self.lazy_tensors)\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/interpolated_lazy_tensor.py\", line 387, in diag\n     return super(InterpolatedLazyTensor, self).diag()\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py\", line 839, in diag\n     return self[..., row_col_iter, row_col_iter]\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py\", line 1716, in __getitem__\n     res = self._get_indices(row_index, col_index, *batch_indices)\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/interpolated_lazy_tensor.py\", line 110, in _get_indices\n     *[batch_index.view(*batch_index.shape, 1, 1) for batch_index in batch_indices],\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/block_interleaved_lazy_tensor.py\", line 58, in _get_indices\n     res = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices, row_index_block)\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py\", line 294, in _get_indices\n     base_lazy_tensor = self._getitem(_noop_index, _noop_index, *batch_indices)._expand_batch(final_shape)\n   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py\", line 97, in _getitem\n     f\"Attempting to tensor index a non-batch matrix's batch dimensions. \"\n RuntimeError: Attempting to tensor index a non-batch matrix's batch dimensions. Got batch index {batch_indices} but my shape was {self.shape}\n </denchmark-code>\n \n <denchmark-h:h2>Expected Behavior</denchmark-h>\n \n Calculating variance should work the same regardless of number of query points.\n <denchmark-h:h2>System information</denchmark-h>\n \n \n \n  gpytorch 1.01\n \n \n  pytorch 1.3.1\n \n \n  MacOS 10.15.3 \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "LemonPi", "commentT": "2020-03-01T23:31:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jacobrgardner>@jacobrgardner</denchmark-link>\n , <denchmark-link:https://github.com/gpleiss>@gpleiss</denchmark-link>\n  for 100 train points this happens exactly when switching eval points form 156 -> 157. I.e. exactly when the full covariance matrix grows > 256. Probably not a coincidence...\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "LemonPi", "commentT": "2020-03-02T00:32:25Z", "comment_text": "\n \t\tYup I suspect that it's switching implementation for larger matrices (suggested by the change in number of self.lazy_tensors), which is totally fine, but there's something wrong with the large matrix implementation\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "LemonPi", "commentT": "2020-03-03T21:06:23Z", "comment_text": "\n \t\tThis seems to be only an issue with the BatchIndependentMultitaskGPModel from the tutorial. The Multitask one (allow correlated outputs) works fine.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "LemonPi", "commentT": "2020-03-04T00:19:27Z", "comment_text": "\n \t\t:( My guess is this is probably related to <denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1024>#1024</denchmark-link>\n  and <denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1039>#1039</denchmark-link>\n  ... I'll work on this issue tomorrow.\n \t\t"}}}, "commit": {"commit_id": "50eded402a7152b74c20fc86aa49e0afa31ffea9", "commit_author": "Geoff Pleiss", "commitT": "2020-04-07 10:29:06-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "gpytorch\\lazy\\block_lazy_tensor.py", "file_new_name": "gpytorch\\lazy\\block_lazy_tensor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98", "deleted_lines": null, "method_info": {"method_name": "_getitem", "method_params": "self,row_index,col_index,batch_indices", "method_startline": "64", "method_endline": "98"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "gpytorch\\lazy\\lazy_tensor.py", "file_new_name": "gpytorch\\lazy\\lazy_tensor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "216", "deleted_lines": "216", "method_info": {"method_name": "_getitem", "method_params": "self,row_index,col_index,batch_indices", "method_startline": "182", "method_endline": "238"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "gpytorch\\utils\\getitem.py", "file_new_name": "gpytorch\\utils\\getitem.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "171,172,173,174,175", "deleted_lines": null, "method_info": {"method_name": "_is_noop_index", "method_params": "index", "method_startline": "171", "method_endline": "175"}}}}}}}