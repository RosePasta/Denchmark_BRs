{"BR": {"BR_id": "313", "BR_author": "tanyuqian", "BRopenT": "2020-07-06T03:03:45Z", "BRcloseT": "2020-07-21T20:04:11Z", "BR_text": {"BRsummary": "A bug in GPT2Tokenizer.", "BRdescription": "\n GPT2Tokenizer fails to recover a sentence \"BART is a seq2seq model.\" with encoded ids of it. The output sentence is \"BART is a seqseq model.\". It should be related to numbers' processing.\n A script to show the bug is here: <denchmark-link:https://github.com/tanyuqian/texar-pytorch/blob/master/examples/bart/gpt2_tokenizer_bug.py>https://github.com/tanyuqian/texar-pytorch/blob/master/examples/bart/gpt2_tokenizer_bug.py</denchmark-link>\n \n \t"}, "comments": {}}, "commit": {"commit_id": "4d08a8c2d92f67ca54723a849a34beaf40a59cf6", "commit_author": "Pengzhi Gao", "commitT": "2020-07-21 16:04:10-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "texar\\torch\\data\\tokenizers\\gpt2_tokenizer.py", "file_new_name": "texar\\torch\\data\\tokenizers\\gpt2_tokenizer.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "129,143", "deleted_lines": "129,143,144"}}}}}}