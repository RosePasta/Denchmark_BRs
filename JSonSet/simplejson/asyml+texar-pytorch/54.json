{"BR": {"BR_id": "54", "BR_author": "TomNong", "BRopenT": "2019-06-22T18:49:56Z", "BRcloseT": "2019-06-25T23:31:40Z", "BR_text": {"BRsummary": "AttentionRNNDecoder param initial_state wrapped in AttentionWrapperState", "BRdescription": "\n Hi <denchmark-link:https://github.com/gpengzhi>@gpengzhi</denchmark-link>\n  , we noticed that it would be too heavy to wrap the AttentionRNNDecoder forward method parameter \"initial_state\" into AttentionWrapperState. The position is here: <denchmark-link:https://github.com/asyml/texar-pytorch/blob/master/texar/modules/decoders/rnn_decoders.py#L518>https://github.com/asyml/texar-pytorch/blob/master/texar/modules/decoders/rnn_decoders.py#L518</denchmark-link>\n . Would you consider making it as a regular tensor? Thanks!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "TomNong", "commentT": "2019-06-22T18:57:29Z", "comment_text": "\n \t\tGood point. <denchmark-link:https://github.com/TomNong>@TomNong</denchmark-link>\n  I think we could simplify this part. Let me think about the code refactoring on the part.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "TomNong", "commentT": "2019-06-22T18:58:05Z", "comment_text": "\n \t\tIn the TF version, the user only needs to provide the actual cell state (not the wrapper state) as initial_state, and the state is wrapped into an AttentionWrapperState in AttentionRNNDecoder.initialize. We could follow similar logic here.\n Note that this breaks the type annotations in the method signature, as initial_state is no longer of type Optional[State] where State is the first type argument of RNNDecoder. Consider changing the annotation of initial_state to Optional[RNNCellBase] which allows arbitrary cell types. mypy should not report errors since the signature is already # type: ignore'd.\n \t\t"}}}, "commit": {"commit_id": "567aaa1589d08bfed086467f258d17d1138a5b51", "commit_author": "Pengzhi Gao", "commitT": "2019-06-25 16:59:34-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "examples\\seq2seq_attn\\seq2seq_attn.py", "file_new_name": "examples\\seq2seq_attn\\seq2seq_attn.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "139", "deleted_lines": "139", "method_info": {"method_name": "main._train_epoch", "method_params": "", "method_startline": "132", "method_endline": "143"}}, "hunk_1": {"Ismethod": 1, "added_lines": "139", "deleted_lines": "139", "method_info": {"method_name": "main", "method_params": "", "method_startline": "119", "method_endline": "182"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "texar\\core\\attention_mechanism.py", "file_new_name": "texar\\core\\attention_mechanism.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "34,928", "deleted_lines": "34,928"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "texar\\modules\\decoders\\rnn_decoders.py", "file_new_name": "texar\\modules\\decoders\\rnn_decoders.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "539,540,541,542,543,544,545,546,547,548,549", "deleted_lines": null, "method_info": {"method_name": "forward", "method_params": "self,Tensor,None,None,None,None,None,None,bool,None,kwargs", "method_startline": "539", "method_endline": "551"}}, "hunk_1": {"Ismethod": 1, "added_lines": "513,514", "deleted_lines": "513,514,515,516,517,518,519,520,521,522", "method_info": {"method_name": "forward", "method_params": "self,Tensor,None,None,None,None,None,None,bool,None,kwargs", "method_startline": "513", "method_endline": "524"}}, "hunk_2": {"Ismethod": 1, "added_lines": "503,504,505,506", "deleted_lines": null, "method_info": {"method_name": "_get_batch_size_fn", "method_params": "x", "method_startline": "503", "method_endline": "506"}}, "hunk_3": {"Ismethod": 1, "added_lines": "489,490,491,492,493,494,495", "deleted_lines": null, "method_info": {"method_name": "initialize", "method_params": "self,Helper", "method_startline": "489", "method_endline": "495"}}}}}}}