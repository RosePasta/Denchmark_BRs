{"BR": {"BR_id": "118", "BR_author": "astariul", "BRopenT": "2020-03-04T06:02:14Z", "BRcloseT": "2020-03-10T21:03:24Z", "BR_text": {"BRsummary": "TypeError: FP16_DeepSpeedZeroOptimizer is not an Optimizer", "BRdescription": "\n I'm trying to use 1-Cycle scheduler, but I meet the following error :\n \n TypeError: FP16_DeepSpeedZeroOptimizer is not an Optimizer\n \n <denchmark-h:hr></denchmark-h>\n \n Here is my configuration file :\n <denchmark-code>{\n     \"train_batch_size\": 64,\n     \"train_micro_batch_size_per_gpu\": 1,\n     \"gradient_accumulation_steps\": 16,\n     \"optimizer\": {\n         \"type\": \"Adam\",\n         \"params\": {\n             \"lr\": 3e-05,\n             \"betas\": [\n                 0.9,\n                 0.999\n             ],\n             \"eps\": 1e-8,\n             \"weight_decay\": 0.01\n         }\n     },\n     \"gradient_clipping\": 0.1,\n     \"scheduler\": {\n         \"type\": \"OneCycle\",\n         \"params\": {\n             \"cycle_first_step_size\": 16000,\n             \"cycle_first_stair_count\": 8000,\n             \"decay_step_size\": 16000,\n             \"cycle_min_lr\": 1e-06,\n             \"cycle_max_lr\": 3e-05,\n             \"decay_lr_rate\": 1e-07,\n             \"cycle_min_mom\": 0.85,\n             \"cycle_max_mom\": 0.99,\n             \"decay_mom_rate\": 0.0\n         }\n     },\n     \"zero_optimization\": true,\n     \"disable_allgather\": true,\n     \"fp16\": {\n         \"enabled\": true,\n         \"loss_scale\": 0,\n         \"min_loss_scale\": 1\n     }\n }\n </denchmark-code>\n \n When using another Scheduler (with FP16), I meet no problem.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "astariul", "commentT": "2020-03-04T06:37:32Z", "comment_text": "\n \t\tThanks for reporting this bug. We will take a look at this as soon as possible. I just created two test cases that reproduce the error (one with ZeRO and one with FP16 but no ZeRO).\n <denchmark-link:https://github.com/microsoft/DeepSpeed/blob/jeffra/onecycle_bug/tests/unit/test_fp16.py#L147-L246>https://github.com/microsoft/DeepSpeed/blob/jeffra/onecycle_bug/tests/unit/test_fp16.py#L147-L246</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "astariul", "commentT": "2020-03-05T06:21:18Z", "comment_text": "\n \t\tNote : I have the same error when I try the same configuration and LRRangeTest as scheduler.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "astariul", "commentT": "2020-03-13T10:38:47Z", "comment_text": "\n \t\tI'm still meeting this issue in deepspeed/deepspeed:latest. How should I update the docker image to pull latest code from source ?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "astariul", "commentT": "2020-03-13T14:42:52Z", "comment_text": "\n \t\tHi @Colanim, it should be up to date. Can you tell us this info from inside your docker container?\n python -c 'import deepspeed; print(\"deepspeed info:\", deepspeed.__version__, deepspeed.__git_branch__, deepspeed.__git_hash__)'\n Also I just looked at the lasted docker build, it prints this same version info and it looks to be aligned with the latest March 12th commit (<denchmark-link:https://github.com/microsoft/DeepSpeed/commit/3d3f8d36a4e8c0b7e6358bccd90254fc7424ffcb>3d3f8d3</denchmark-link>\n ): <denchmark-link:https://dev.azure.com/DeepSpeedMSFT/DeepSpeed/_build/results?buildId=416&view=logs&j=3dc8fd7e-4368-5a92-293e-d53cefc8c4b3&t=a1aa9649-a94b-5ac4-3f5e-9bb6223edb04&l=1717>https://dev.azure.com/DeepSpeedMSFT/DeepSpeed/_build/results?buildId=416&view=logs&j=3dc8fd7e-4368-5a92-293e-d53cefc8c4b3&t=a1aa9649-a94b-5ac4-3f5e-9bb6223edb04&l=1717</denchmark-link>\n \n ** info: 0.1.0 master 3d3f8d3\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "astariul", "commentT": "2020-03-14T10:07:08Z", "comment_text": "\n \t\tMy bad, I didn't pull the latest image..\n After doing docker pull deepspeed/deepspeed:latest, it's working \ud83d\udc4d\n \t\t"}}}, "commit": {"commit_id": "1c0b326e772e20082772fdcd10e3f95ed481d555", "commit_author": "Olatunji Ruwase", "commitT": "2020-03-10 14:03:23-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "deepspeed\\pt\\deepspeed_lr_schedules.py", "file_new_name": "deepspeed\\pt\\deepspeed_lr_schedules.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "531,532,533,534,535,536", "deleted_lines": null, "method_info": {"method_name": "_initialize_lr", "method_params": "self,optimizer,cycle_min_lr,cycle_max_lr,decay_lr_rate,last_batch_iteration", "method_startline": "531", "method_endline": "536"}}, "hunk_1": {"Ismethod": 1, "added_lines": "288,289,290,291,292,293,294,295,296", "deleted_lines": null, "method_info": {"method_name": "get_torch_optimizer", "method_params": "optimizer", "method_startline": "288", "method_endline": "296"}}, "hunk_2": {"Ismethod": 1, "added_lines": "513,514,515,516,517,518", "deleted_lines": null, "method_info": {"method_name": "_initialize_cycle", "method_params": "self,cycle_first_step_size,cycle_second_step_size,cycle_first_stair_count,cycle_second_stair_count,decay_step_size", "method_startline": "513", "method_endline": "518"}}, "hunk_3": {"Ismethod": 1, "added_lines": "546,547,548,549,550,551", "deleted_lines": null, "method_info": {"method_name": "_initialize_momentum", "method_params": "self,optimizer,cycle_min_mom,cycle_max_mom,decay_mom_rate,last_batch_iteration", "method_startline": "546", "method_endline": "551"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tests\\unit\\test_fp16.py", "file_new_name": "tests\\unit\\test_fp16.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "182,183,184,185,186,187,188,189,190,191,192,193", "deleted_lines": null, "method_info": {"method_name": "test_adam_fp16_onecycle_compatibility._test_adam_fp16_onecycle_compatibility", "method_params": "args,model,hidden_dim", "method_startline": "182", "method_endline": "193"}}, "hunk_1": {"Ismethod": 1, "added_lines": "198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248", "deleted_lines": null, "method_info": {"method_name": "test_adam_fp16_zero_onecycle_compatibility", "method_params": "tmpdir", "method_startline": "198", "method_endline": "248"}}, "hunk_2": {"Ismethod": 1, "added_lines": "147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195", "deleted_lines": null, "method_info": {"method_name": "test_adam_fp16_onecycle_compatibility", "method_params": "tmpdir", "method_startline": "147", "method_endline": "195"}}, "hunk_3": {"Ismethod": 1, "added_lines": "233,234,235,236,237,238,239,240,241,242,243,244", "deleted_lines": null, "method_info": {"method_name": "test_adam_fp16_zero_onecycle_compatibility._test_adam_fp16_zero_onecycle_compatibility", "method_params": "args,model,hidden_dim", "method_startline": "233", "method_endline": "244"}}}}}}}