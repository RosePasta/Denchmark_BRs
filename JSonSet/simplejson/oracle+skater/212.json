{"BR": {"BR_id": "212", "BR_author": "alay18", "BRopenT": "2018-03-26T09:32:12Z", "BRcloseT": "2018-06-17T09:30:57Z", "BR_text": {"BRsummary": "Kernel dies when running plot_partial_dependence", "BRdescription": "\n Hello,\n I'm currently having trouble running plot_partial_dependence for my XGBoost model.\n My kernel dies when I try to run the script, and this happens also if I run partial_dependence:\n The kernel appears to have died. It will restart automatically.\n In terminal, it says:\n malloc: *** error for object 0x100007f87ba95b8c: pointer being freed was not allocated\n *** set a breakpoint in malloc_error_break to debug\n There doesn't seem to be an issue when I run plot_partial_dependence for my random forest model.\n Could someone please look into this? Thanks in advance for your help!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "alay18", "commentT": "2018-04-07T21:07:54Z", "comment_text": "\n \t\tLooks like this is an issue with multiprocessing/xgboost compatibility, see <denchmark-link:https://github.com/dmlc/xgboost/issues/2163>dmlc/xgboost#2163</denchmark-link>\n \n <denchmark-link:https://github.com/pramitchoudhary>@pramitchoudhary</denchmark-link>\n  might be worth checking to see if changing the spawner to a forkserver will be effective. Would need to check that thats a stable spawner on the major platforms.\n <denchmark-link:https://github.com/alay18>@alay18</denchmark-link>\n  in mean time, can you try setting n_jobs=1 if you're running pdp with an xgboost model?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "alay18", "commentT": "2018-04-09T08:25:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/aikramer2>@aikramer2</denchmark-link>\n  Hey, I've tried setting n_jobs = -1 but the same thing happened. Please let me know if there's any updates on your side.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "alay18", "commentT": "2018-04-09T16:08:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alay18>@alay18</denchmark-link>\n  try n_jobs=1, not -1. n_jobs=-1 will use as many processes as are available.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "alay18", "commentT": "2018-04-09T16:14:52Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/aikramer2>@aikramer2</denchmark-link>\n  thanks for pointing that out, just tried n_jobs = 1, the kernel still died.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "alay18", "commentT": "2018-04-11T20:59:56Z", "comment_text": "\n \t\tthanks, <denchmark-link:https://github.com/aikramer2>@aikramer2</denchmark-link>\n . <denchmark-link:https://github.com/alay18>@alay18</denchmark-link>\n  did the issue resolve when using n_jobs=1? or are you still getting errors?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "alay18", "commentT": "2018-04-12T11:51:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pramitchoudhary>@pramitchoudhary</denchmark-link>\n  hello, unfortunately using n_jobs = 1 did not resolve the issue.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "alay18", "commentT": "2018-04-23T17:56:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alay18>@alay18</denchmark-link>\n  will it be possible to share the code(or example snippet) for the XGBoost model that you are trying to use. Apologies for the slow response, have been beaten down with releasing new features and documentation. Let's get this resolved.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "alay18", "commentT": "2018-04-24T13:25:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pramitchoudhary>@pramitchoudhary</denchmark-link>\n  here it is:\n #specify XGB for inner loop - grid search\n xgb = XGBRegressor(objective='reg:linear', booster='gbtree', n_jobs= -1)\n #XGB parameters for grid search\n xgb_grid = {\"max_depth\" : [6],\n \"learning_rate\" : [0.2],\n \"gamma\" : [0],\n \"n_estimators\" : [150],\n \"min_child_weight\" : [1],\n \"base_score\" : [0.5],\n \"subsample\" : [1],\n \"max_delta_step\" : [0],\n \"colsample_bytree\" : [0.5],\n \"colsample_bylevel\" : [0.4],\n \"reg_alpha\" : [0],\n \"reg_lambda\" : [60],\n \"scale_pos_weight\" : [1]\n }\n clf = GridSearchCV(xgb, param_grid = xgb_grid, cv = inner_cv, n_jobs = -1)\n clf.fit(X, y)\n nested_score = cross_validate(clf, X, y, cv=outer_cv, n_jobs= -1, scoring = ('r2', 'neg_mean_squared_error'))\n nested_scores_mean = nested_score['test_r2'].mean()\n nested_rmse_mean = (np.sqrt(-nested_score['test_neg_mean_squared_error'])).mean()\n print (nested_scores_mean)\n print (nested_rmse_mean)\n #Skater feature importance\n skater_model = InMemoryModel(clf.predict, examples = X)\n interpreter = Interpretation()\n interpreter.load_data(X)\n pdp = interpreter.partial_dependence.plot_partial_dependence(\n feature_ids = ['marksandspencer'], modelinstance = skater_model, grid_resolution = 30, n_jobs = 1)\n plt.show()\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "alay18", "commentT": "2018-04-25T15:42:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/aikramer2>@aikramer2</denchmark-link>\n  will you have time to take a look at this and see if we can resolve it or is it something that we would need help from the XGBoost guys?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "alay18", "commentT": "2018-05-16T21:06:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/aikramer2>@aikramer2</denchmark-link>\n  Did you get a chance to take a look at this issue?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "alay18", "commentT": "2018-06-16T17:35:42Z", "comment_text": "\n \t\tlooking into this, apologies for all the delays.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "alay18", "commentT": "2018-06-17T07:55:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alay18>@alay18</denchmark-link>\n  <denchmark-link:https://github.com/aikramer2>@aikramer2</denchmark-link>\n  alright still a little blind on how XgBosst is handling concurrency in-terms of splitting the trees internally but there seems to be some reported issues in the way XgBoost handles thread pool internally and python's multiprocessing libraries. For more information checkout the <denchmark-link:http://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux>FAQ</denchmark-link>\n . The link that <denchmark-link:https://github.com/aikramer2>@aikramer2</denchmark-link>\n  had referenced earlier contains good history on this thread. Also, you might also get a dead kernel issue with  and (I was not able to get it working).\n Now, to get this working with a workaround, the below mentioned code worked for me,\n %matplotlib inline\n import matplotlib.pyplot\n from sklearn.datasets import load_boston, load_breast_cancer\n from sklearn.ensemble import GradientBoostingRegressor\n from sklearn.linear_model import LogisticRegression, LinearRegression\n import matplotlib.pyplot as plt\n <denchmark-h:h4>Reference for customizing matplotlib: https://matplotlib.org/users/style_sheets.html</denchmark-h>\n \n plt.style.use('ggplot')\n import pandas as pd\n import numpy as np\n <denchmark-h:h1>Load Bosting housing data</denchmark-h>\n \n regressor_data = load_boston()\n <denchmark-h:h1>Get information about the data</denchmark-h>\n \n print(regressor_data.DESCR)\n regressor_X = regressor_data.data\n regressor_y = regressor_data.target\n from xgboost import XGBRegressor\n from sklearn.model_selection import GridSearchCV\n from sklearn.model_selection import cross_validate\n <denchmark-h:h1>XGRegressor example</denchmark-h>\n \n xgb = XGBRegressor(objective='reg:linear', booster='gbtree', n_jobs= -1)\n #XGB parameters for grid search\n xgb_grid = {\"max_depth\" : [6],\n \"learning_rate\" : [0.2],\n \"gamma\" : [0],\n \"n_estimators\" : [150],\n \"min_child_weight\" : [1],\n \"base_score\" : [0.5],\n \"subsample\" : [1],\n \"max_delta_step\" : [0],\n \"colsample_bytree\" : [0.5],\n \"colsample_bylevel\" : [0.4],\n \"reg_alpha\" : [0],\n \"reg_lambda\" : [60],\n \"scale_pos_weight\" : [1]\n }\n clf = GridSearchCV(xgb, param_grid = xgb_grid, cv = 3, n_jobs = -1)\n clf.fit(regressor_X, regressor_y)\n <denchmark-h:h1>Model Evaluation</denchmark-h>\n \n from sklearn.metrics import mean_squared_error\n from sklearn.metrics import r2_score\n y_hat = clf.predict(regressor_X)\n print(mean_squared_error(regressor_y, y_hat))\n print(r2_score(regressor_y, y_hat))\n <denchmark-h:h1>Model Inference</denchmark-h>\n \n from skater.core.explanations import Interpretation\n interpreter = Interpretation(regressor_X, feature_names=regressor_data.feature_names)\n from skater.model import InMemoryModel\n annotated_model = InMemoryModel(clf.predict, examples=regressor_X)\n print(\"Number of classes: {}\".format(annotated_model.n_classes))\n print(\"Input shape: {}\".format(annotated_model.input_shape))\n print(\"Model Type: {}\".format(annotated_model.model_type))\n print(\"Output Shape: {}\".format(annotated_model.output_shape))\n print(\"Output Type: {}\".format(annotated_model.output_type))\n print(\"Returns Probabilities: {}\".format(annotated_model.probability))\n print(\"2-way partial dependence plots\")\n <denchmark-h:h1>Features can passed as a tuple for 2-way partial plot</denchmark-h>\n \n pdp_features = [('DIS', 'RM')]\n interpreter.partial_dependence.plot_partial_dependence(\n pdp_features, annotated_model, grid_resolution=30, n_jobs=1\n )\n print(\"1-way partial dependence plots\")\n <denchmark-h:h1>or as independent features for 1-way partial plots</denchmark-h>\n \n pdp_features = ['DIS', 'RM']\n interpreter.partial_dependence.plot_partial_dependence(\n pdp_features, annotated_model, grid_resolution=30, progressbar=False, n_jobs=1, with_variance=True\n )\n Will add as an example notebook as well, so that its easier for other folks.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "alay18", "commentT": "2018-06-17T09:30:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alay18>@alay18</denchmark-link>\n   Added a notebook example <denchmark-link:https://github.com/datascienceinc/Skater/blob/master/examples/xgboost_regression_example.ipynb>here</denchmark-link>\n \n Marking this issue as resolved. For a better solution, will open a different ticket focusing on resolving the thread pool handling. Feel free to reopen the issue if things still dont work on your end.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "alay18", "commentT": "2019-03-14T01:37:04Z", "comment_text": "\n \t\tI'm still having this issue on some notebooks\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "alay18", "commentT": "2019-03-14T01:51:08Z", "comment_text": "\n \t\t\n @alay18 Added a notebook example here\n Marking this issue as resolved. For a better solution, will open a different ticket focusing on resolving the thread pool handling. Feel free to reopen the issue if things still dont work on your end.\n \n FYI this kills the kernel\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "alay18", "commentT": "2019-10-25T08:34:41Z", "comment_text": "\n \t\tI have this issue too when using xgboost.sklearn API. It never shows anything when I try to generate plots inside a notebook\n \t\t"}}}, "commit": {"commit_id": "9a484258ec6bd469f904865b9fbed54e13edc55b", "commit_author": "Pramit Choudhary", "commitT": "2018-06-17 02:27:30-07:00", "changed_files": {"file_0": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "examples\\xgboost_regression_example.ipynb"}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "skater\\core\\global_interpretation\\partial_dependence.py", "file_new_name": "skater\\core\\global_interpretation\\partial_dependence.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "382", "deleted_lines": "382"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "skater\\model\\local_model.py", "file_new_name": "skater\\model\\local_model.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "121", "deleted_lines": null}}}}}}