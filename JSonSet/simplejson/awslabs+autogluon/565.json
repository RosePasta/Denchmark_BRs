{"BR": {"BR_id": "565", "BR_author": "schinto", "BRopenT": "2020-07-19T16:36:13Z", "BRcloseT": "2020-07-30T15:11:09Z", "BR_text": {"BRsummary": "Is the calculation of the RMSE of test data correct?", "BRdescription": "\n Is the RMSE calculation in evaluate_prediction in autogluon version 0.0.11 correct?\n From a prediction with the test data:\n <denchmark-code>y_pred = predictor.predict(X_raw)\n perf = predictor.evaluate_predictions(\n         y_true=y_test, y_pred=y_pred, auxiliary_metrics=True\n     )\n </denchmark-code>\n \n I got:\n <denchmark-code>Evaluation: root_mean_squared_error on test data: 1.9002197479504435\n Evaluations on test data:\n {\n     \"root_mean_squared_error\": 1.9002197479504435,\n     \"mean_absolute_error\": 0.2822201467381229,\n     \"explained_variance_score\": 0.9041746987036652,\n     \"r2_score\": 0.9041688612988515,\n     \"pearson_correlation\": 0.9509362562666829,\n     \"mean_squared_error\": 0.17589136840340483,\n     \"median_absolute_error\": 0.18991703222021486\n }\n </denchmark-code>\n \n When the RMSE is calculated using\n <denchmark-code>from sklearn.metrics import mean_squared_error, r2_score\n import numpy as np\n \n def rmse(y_pred, y_true):\n     return np.sqrt(mean_squared_error(y_pred, y_true)) \n \n print(f'RMSE: {rmse(y_test, y_pred):.3f}')\n print(f'MSE: {mean_squared_error(y_test, y_pred):.3f}')\n print(f'r2 score: {r2_score(y_test, y_pred):.3f}')\n </denchmark-code>\n \n the following values are returned:\n <denchmark-code>RMSE: 0.419\n MSE: 0.176\n r2 score: 0.904\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "schinto", "commentT": "2020-07-19T18:22:06Z", "comment_text": "\n \t\tInteresting... Could you also do predictor.leaderboard(X_raw_with_labels)? That will indicate what is internally being used by AutoGluon.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "schinto", "commentT": "2020-07-19T18:30:10Z", "comment_text": "\n \t\tThis is the AutoGluon source code for rmse (Same as in <denchmark-link:https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python>https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python</denchmark-link>\n ):\n <denchmark-code>def rmse_func(predictions, targets):\n     return np.sqrt(((predictions - targets) ** 2).mean())\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "schinto", "commentT": "2020-07-19T18:37:28Z", "comment_text": "\n \t\tI've ran a test on this and it seems to be consistent (differing from your example). Could you double-check your code and ensure you ran everything in the same cell?\n On 0.0.12 (No changes to RMSE):\n <denchmark-code>predictor = task.fit(\n     # time_limits=120,\n     train_data=X,\n     label=LABEL,\n     eval_metric='root_mean_squared_error',\n     hyperparameters={\n         'CAT': {},\n     },\n )\n \n path_test = path_prefix + 'test_data.csv'\n X_test = load_pd.load(path_test)\n \n predictor.leaderboard(X_test)\n \n y_test = X_test[LABEL]\n y_pred = predictor.predict(X_test)\n perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n from sklearn.metrics import mean_squared_error, r2_score\n import numpy as np\n \n def rmse(y_pred, y_true):\n     return np.sqrt(mean_squared_error(y_pred, y_true))\n \n print(f'RMSE: {rmse(y_test, y_pred):.3f}')\n print(f'MSE: {mean_squared_error(y_test, y_pred):.3f}')\n print(f'r2 score: {r2_score(y_test, y_pred):.3f}')\n \n </denchmark-code>\n \n Output:\n <denchmark-code>                     model    score_test     score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer\n 0        CatboostRegressor -19219.966533 -26231.021279        0.071855       0.067640  77.554613                 0.071855                0.067640          77.554613            0       True\n 1  weighted_ensemble_k0_l1 -19219.966533 -26231.021279        0.075469       0.069419  77.559314                 0.003614                0.001779           0.004701            1       True\n Evaluation: root_mean_squared_error on test data: 19219.966533285162\n Evaluations on test data:\n {\n     \"root_mean_squared_error\": 19219.966533285162,\n     \"mean_absolute_error\": 13203.561436971126,\n     \"explained_variance_score\": 0.9259382784632942,\n     \"r2_score\": 0.9256836316437639,\n     \"pearson_correlation\": 0.96245422205405,\n     \"mean_squared_error\": 369407113.5406017,\n     \"median_absolute_error\": 9687.979841075066\n }\n RMSE: 19219.967\n MSE: 369407113.541\n r2 score: 0.926\n \n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "schinto", "commentT": "2020-07-20T07:33:28Z", "comment_text": "\n \t\tSee below for the output from the leaderboard.\n <denchmark-code>predictor.leaderboard(test_data)\n \n model  score_test  score_val  pred_time_test  pred_time_val      fit_time  pred_time_test_marginal  pred_time_val_marginal fit_time_marginal  stack_level  can_infer\n 0               weighted_ensemble_k0_l2   -0.419394  -0.419480     1371.710608     363.260315  85731.478442                 0.017222                0.001277           1.162603            2       True\n 1          LightGBMRegressor_STACKER_l1   -0.419805  -0.421002     1291.514437     346.914402  52674.690803                 5.264961                2.719512          49.868780            1       True\n 2    LightGBMRegressorCustom_STACKER_l1   -0.419831  -0.420828     1292.150959     347.095359  52745.572411                 5.901483                2.900468         120.750388            1       True\n 3          CatboostRegressor_STACKER_l1   -0.421231  -0.423362     1287.817158     345.872373  52695.846922                 1.567682                1.677483          71.024898            1       True\n 4   RandomForestRegressorMSE_STACKER_l1   -0.422732  -0.424340     1308.213786     349.527967  63250.733135                21.964311                5.333077       10625.911111            1       True\n 5     ExtraTreesRegressorMSE_STACKER_l1   -0.423172  -0.423621     1336.994950     350.628498  74862.760662                50.745474                6.433608       22237.938638            1       True\n 6               weighted_ensemble_k0_l1   -0.426322  -0.425883       57.560714      16.361131  16316.717391                 0.015704                0.001499           1.403781            1       True\n 7    LightGBMRegressorCustom_STACKER_l0   -0.428854  -0.431920       14.277234       5.369839   1295.655222                14.277234                5.369839        1295.655222            0       True\n 8          LightGBMRegressor_STACKER_l0   -0.432707  -0.440154        7.679609       3.904525    362.878872                 7.679609                3.904525         362.878872            0       True\n 9          CatboostRegressor_STACKER_l0   -0.432829  -0.437915        1.056855       1.666364   1155.844561                 1.056855                1.666364        1155.844561            0       True\n 10    ExtraTreesRegressorMSE_STACKER_l0   -0.469459  -0.467605       34.531311       5.418904  13500.934955                34.531311                5.418904       13500.934955            0       True\n 11  RandomForestRegressorMSE_STACKER_l0   -0.512019  -0.508367       35.587046       5.374662   7508.789899                35.587046                5.374662        7508.789899            0       True\n 12        NeuralNetRegressor_STACKER_l1   -0.635227  -1.591328     1582.314277     434.648647  83989.564337               296.064801               90.453757       31364.742313            1       True\n 13   KNeighborsRegressorDist_STACKER_l1   -0.753965  -0.768754     1818.558200     461.193521  52739.027242               532.308724              116.998631         114.205219            1       True\n 14   KNeighborsRegressorDist_STACKER_l0   -0.758689  -0.774370      365.134637     116.263620    113.223741               365.134637              116.263620         113.223741            0       True\n 15   KNeighborsRegressorUnif_STACKER_l1   -0.781376  -0.795930     1619.714213     461.372831  52739.213231               333.464738              117.177941         114.391208            1       True\n 16   KNeighborsRegressorUnif_STACKER_l0   -0.787333  -0.802268      348.599119     116.399790    113.495155               348.599119              116.399790         113.495155            0       True\n 17        NeuralNetRegressor_STACKER_l0   -0.884560  -3.940324      479.383664      89.797186  28573.999618               479.383664               89.797186       28573.999618            0       True\n </denchmark-code>\n \n The output from evaluate_predictions is still:\n Evaluation: root_mean_squared_error on test data: 1.9002197479504435\n Two other observation:\n A) I trained 14 regression models (training data had on average 24000 samples and 1243 features) using:\n <denchmark-code>predictor = task.fit(\n         train_data=train_data,\n         label=label_column,\n         output_directory=output_directory,\n         auto_stack=True,\n         feature_prune=True,\n         verbosity=0,\n     )\n </denchmark-code>\n \n NeuralNetRegressor_STACKER_l0 always had the highest RMSE.\n B) In one of my first runs the training dataset contained a couple of Inf and -Inf y-values, which crashed the training without giving a meaningful error message.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "schinto", "commentT": "2020-07-20T23:54:16Z", "comment_text": "\n \t\tThanks for the clarification! I believe I have narrowed down that this has to be a bug. I also suspect that predictor.evaluate(test_data) will produce the correct value, and that the bug purely lies in predictor.evaluate_predictions because it uses separate logic from what the rest of the code uses to calculate scores.\n Contributions are welcome to try to refactor predictor.evaluate_predictions logic to re-use the other internal scoring logic. If nobody contributes a fix, I will try to find time eventually to resolve this.\n <denchmark-link:https://github.com/schinto>@schinto</denchmark-link>\n  To help the process along, could you provide a MWE of this defect in action? A dummy dataset generated through a few lines of code should suffice, along with the relevant calls to the defective logic + the correct logic. Also make sure to use v0.0.12 when constructing the example, it is possible it was fixed in 0.0.12 since you mentioned you were using 0.0.11. Regarding your other comments: A: This is a very deep topic regarding model behavior on unique datasets, and should exist in a separate issue if you believe that the neural network should be doing better on this problem (Along with ideally a NN example that fairs better than what AG produced). B: This is a valid defect that should be fixed, could you create a separate issue with a minimal example so we can work to resolve?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "schinto", "commentT": "2020-07-22T07:48:24Z", "comment_text": "\n \t\tMinimum Working Example illustrating the problem with root_mean_squared_error:\n <denchmark-code>import pandas as pd\n import numpy as np\n from sklearn.datasets import make_regression\n from sklearn.model_selection import train_test_split\n import autogluon as ag\n from autogluon import TabularPrediction as task\n from sklearn.metrics import mean_squared_error, r2_score\n \n \n print(f'Autogluon version: {ag.__version__}')\n \n # Make simulated data for regression\n X,y = make_regression(n_samples=100, n_features=6, n_informative=3,\n n_targets=1, tail_strength=0.5, noise=0.02,\n shuffle=True, coef=False, random_state=42)\n \n xcols = [ f'x{i:04}' for i in range(X.shape[1]) ]\n ycols = ['y']\n df = pd.concat([pd.DataFrame(y, columns=ycols), pd.DataFrame(X, columns=xcols)], axis=1)\n \n # Split dataframe in training and test data\n train_data, test_data = train_test_split(df, train_size=0.75, random_state=42)\n X_train, y_train = train_data.drop(columns=['y']), train_data['y']\n X_test,  y_test  = test_data.drop(columns=['y']), test_data['y']\n \n # Train model\n label = 'y'\n predictor = task.fit(\n         time_limits=120,\n         train_data=train_data,\n         label=label,\n         auto_stack=True,\n         feature_prune=True,\n         verbosity=0,\n )\n \n # predictor.leaderboard()\n y_pred = predictor.predict(X_test)\n \n perf = predictor.evaluate_predictions(\n     y_true=y_test, y_pred=y_pred, auxiliary_metrics=True\n )\n \n print('Compare performance metrics:')\n print('(A) Performance metrics from autogluon')\n perf_df = pd.DataFrame(perf.values(), index=perf.keys(), columns=['value'])\n print(perf_df.round(decimals=3))\n \n # Calculate root mean square error from sklearn's mean_squared_error\n def rmse(y_pred, y_true):\n     return np.sqrt(mean_squared_error(y_pred, y_true))\n \n print('\\n(B) Performance metrics using sklearn function')\n print(f'root_mean_squared_error: {rmse(y_test, y_pred):.3f}')\n print(f'mean_squared_error: {mean_squared_error(y_test, y_pred):.3f}')\n print(f'r2_score: {r2_score(y_test, y_pred):.3f}')\n \n # lboard = predictor.leaderboard(test_data, silent=True)\n # lboard.sort_values(by='score_val', ascending=False)\n </denchmark-code>\n \n Output:\n <denchmark-code>Autogluon version: 0.0.11\n Compare performance metrics:\n (A) Performance metrics from autogluon\n                             value\n root_mean_squared_error   108.738\n mean_absolute_error        10.569\n explained_variance_score    0.970\n r2_score                    0.964\n pearson_correlation         0.992\n mean_squared_error        232.442\n median_absolute_error       7.206\n \n (B) Performance metrics using sklearn functions\n root_mean_squared_error: 15.246\n mean_squared_error: 232.442\n r2_score: 0.964\n </denchmark-code>\n \n sqrt(232.442) = 15.246 => root_mean_squared_error returned by function evaluate_predictions is wrong.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "schinto", "commentT": "2020-07-22T17:23:15Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/schinto>@schinto</denchmark-link>\n  Awesome, thanks!\n \t\t"}}}, "commit": {"commit_id": "ad5a890b43a0d42c470a6c299f0750dcec367f4f", "commit_author": "Nick Erickson", "commitT": "2020-07-30 08:11:08-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "autogluon\\task\\tabular_prediction\\predictor.py", "file_new_name": "autogluon\\task\\tabular_prediction\\predictor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "137", "deleted_lines": null, "method_info": {"method_name": "predict_proba", "method_params": "self,dataset,model,as_pandas,as_multiclass", "method_startline": "135", "method_endline": "165"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "autogluon\\utils\\tabular\\ml\\learner\\abstract_learner.py", "file_new_name": "autogluon\\utils\\tabular\\ml\\learner\\abstract_learner.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "383,387,392,417,418,419,429,442,443,444,445,446,451", "deleted_lines": "379,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,404,405,430,431,432,442,447,448,449", "method_info": {"method_name": "evaluate", "method_params": "self,y_true,y_pred,silent,auxiliary_metrics,detailed_report,high_always_good", "method_startline": "371", "method_endline": "458"}}}}}}}