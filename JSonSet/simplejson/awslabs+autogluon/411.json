{"BR": {"BR_id": "411", "BR_author": "Innixma", "BRopenT": "2020-04-11T22:27:02Z", "BRcloseT": "2020-04-22T22:47:50Z", "BR_text": {"BRsummary": "Tabular: os.fork() called by MXNet NN model during prediction causing OOM", "BRdescription": "\n This OOM error was caused during training of Albert OpenML dataset (Large) on m5.2xlarge. This OOM error has never been encountered before. (Happened 3 times on latest benchmark run on Albert and Airlines datasets).\n The error happened during the setup for training the weighted ensemble with bagging and stacking disabled. Therefore, each model needs to do predictions on the validation data in turn (The amount of validation data is very small, less than 10,000, and therefore should not cause issues). For whatever reason, TabularNN is forking the process, which likely leads to a doubling of memory usage.\n Question: Why are we running OOM during a stage that should require very minimal memory usage, and why are we forking the process?\n Stack Trace, note that the memory check shortly prior to this indicates 32% memory usage:\n <denchmark-code>[INFO] [amlb.utils.process:01:07:58.029] CPU Utilization: 41.7%\n [INFO] [amlb.utils.process:01:07:58.029] Memory Usage: 32.1%\n [INFO] [amlb.utils.process:01:07:58.029] Disk Usage: 15.7%\n [ERROR] [amlb.benchmark:01:08:55.237] [Errno 12] Cannot allocate memory\n Traceback (most recent call last):\n   File \"/home/ubuntu/workspace/automlbenchmark/amlb/benchmark.py\", line 391, in run\n     meta_result = framework.run(self._dataset, task_config)\n   File \"/home/ubuntu/workspace/automlbenchmark/frameworks/autogluon_nobag/__init__.py\", line 4, in run\n     return run(*args, **kwargs)\n   File \"/home/ubuntu/workspace/automlbenchmark/frameworks/autogluon_nobag/exec.py\", line 16, in run\n     return exec_template.run(dataset=dataset, config=config, parameters=parameters)\n   File \"/home/ubuntu/workspace/automlbenchmark/frameworks/autogluon/exec_template.py\", line 82, in run\n     **fit_params\n   File \"/home/ubuntu/workspace/autogluon/autogluon/task/tabular_prediction/tabular_prediction.py\", line 425, in fit\n     hyperparameters=hyperparameters, time_limit=time_limits_orig, save_data=cache_data, verbosity=verbosity)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/learner/default_learner.py\", line 99, in fit\n     hyperparameters=hyperparameters)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/trainer/auto_trainer.py\", line 39, in train\n     self.train_multi_and_ensemble(X_train, y_train, X_test, y_test, models, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/trainer/abstract_trainer.py\", line 507, in train_multi_and_ensemble\n     self.train_multi_levels(X_train, y_train, X_test, y_test, models=models, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, level_start=0, level_end=self.stack_ensemble_levels)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/trainer/abstract_trainer.py\", line 522, in train_multi_levels\n     self.stack_new_level(X=X_train, y=y_train, X_test=X_test, y_test=y_test, models=models, level=level, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, time_limit_core=time_limit_core, time_limit_aux=time_limit_aux)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/trainer/abstract_trainer.py\", line 533, in stack_new_level\n     aux_models = self.stack_new_level_aux(X=X_test, y=y_test, fit=False, level=level+1, time_limit=time_limit_aux)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/trainer/abstract_trainer.py\", line 568, in stack_new_level_aux\n     X_train_stack_preds = self.get_inputs_to_stacker(X, level_start=0, level_end=level, fit=fit)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/trainer/abstract_trainer.py\", line 797, in get_inputs_to_stacker\n     X = dummy_stackers[level+1].preprocess(X=X, preprocess=False, fit=False, compute_base_preds=True)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/models/ensemble/stacker_ensemble_model.py\", line 92, in preprocess\n     y_pred_proba = base_model.predict_proba(X)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_model.py\", line 384, in predict_proba\n     return self._predict_tabular_data(new_data=X, process=True, predict_proba=True)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_model.py\", line 398, in _predict_tabular_data\n     new_data = self.process_test_data(new_data, batch_size=self.batch_size, num_dataloading_workers=self.num_dataloading_workers, labels=None)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_model.py\", line 486, in process_test_data\n     problem_type=self.problem_type, labels=labels, is_test=True)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_dataset.py\", line 150, in __init__\n     self.generate_dataset_and_dataloader(data_list=data_list)\n   File \"/home/ubuntu/workspace/autogluon/autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_dataset.py\", line 158, in generate_dataset_and_dataloader\n     num_workers=self.num_dataloading_workers)  # no need to shuffle test data\n   File \"/home/ubuntu/virtual/automlbenchmark/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\", line 642, in __init__\n     initargs=[self._dataset, is_np_shape(), is_np_array()])\n   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/context.py\", line 119, in Pool\n     context=self.get_context())\n   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 174, in __init__\n     self._repopulate_pool()\n   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n     w.start()\n   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 105, in start\n     self._popen = self._Popen(self)\n   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n     return Popen(process_obj)\n   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n     self._launch(process_obj)\n   File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n     self.pid = os.fork()\n OSError: [Errno 12] Cannot allocate memory\n \n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Innixma", "commentT": "2020-04-11T22:29:20Z", "comment_text": "\n \t\tpip freeze output:\n <denchmark-code>attrs==19.3.0\n # Editable install with no version control (autogluon==0.0.7b11042020)\n -e /home/ubuntu/workspace/autogluon\n # Editable install with no version control (autogluon-utils==0.0.1)\n -e /home/ubuntu/workspace/autogluon-utils\n Babel==2.8.0\n bcrypt==3.1.7\n boto3==1.12.36\n botocore==1.15.36\n catboost==0.22\n certifi==2020.4.5.1\n cffi==1.14.0\n chardet==3.0.4\n click==7.1.1\n cloudpickle==1.3.0\n colorama==0.4.3\n ConfigSpace==0.4.10\n cryptography==2.9\n cycler==0.10.0\n Cython==0.29.16\n dask==2.6.0\n deap==1.3.1\n debtcollector==2.0.0\n decorator==4.4.2\n distributed==2.6.0\n docutils==0.15.2\n fasteners==0.15\n fire==0.3.0\n fsspec==0.7.1\n future==0.18.2\n gluoncv==0.7.0b20200405\n gluonnlp==0.8.1\n graphviz==0.8.4\n h2o==3.30.0.1\n HeapDict==1.0.1\n idna==2.9\n importlib-metadata==1.6.0\n ipython-genutils==0.2.0\n iso8601==0.1.12\n jmespath==0.9.5\n joblib==0.14.1\n jsonschema==3.2.0\n jupyter-core==4.6.3\n kaggle==1.5.6\n kiwisolver==1.2.0\n liac-arff==2.3\n lightgbm==2.3.1\n matplotlib==3.2.1\n mock==4.0.2\n monotonic==1.5\n more-itertools==8.2.0\n msgpack==1.0.0\n mxnet==1.6.0\n nbformat==5.0.5\n netaddr==0.7.19\n netifaces==0.10.9\n networkx==2.4\n nose==1.3.7\n numpy==1.18.2\n openml==0.7.0\n oslo.concurrency==4.0.1\n oslo.config==8.0.1\n oslo.i18n==4.0.0\n oslo.utils==4.1.0\n packaging==20.3\n pandas==0.25.3\n paramiko==2.7.1\n pbr==5.4.4\n Pillow==6.2.1\n plotly==4.6.0\n pluggy==0.13.1\n portalocker==1.6.0\n protobuf==3.11.3\n protobuf3-to-dict==0.1.5\n psutil==5.7.0\n py==1.8.1\n pyaml==20.4.0\n pyarrow==0.16.0\n pycparser==2.20\n PyNaCl==1.3.0\n pyparsing==2.4.6\n pyrsistent==0.16.0\n pytest==5.4.1\n python-dateutil==2.8.1\n python-slugify==4.0.0\n pytz==2019.3\n PyYAML==5.3.1\n requests==2.23.0\n retry==0.9.2\n retrying==1.3.3\n rfc3986==1.3.2\n ruamel.yaml==0.16.10\n ruamel.yaml.clib==0.2.0\n s3fs==0.4.2\n s3transfer==0.3.3\n sagemaker==1.55.0\n scikit-learn==0.20.4\n scikit-optimize==0.8.dev0\n scipy==1.2.3\n six==1.14.0\n smdebug-rulesconfig==0.1.2\n sortedcontainers==2.1.0\n stevedore==1.32.0\n stopit==1.1.2\n tabulate==0.8.7\n tblib==1.6.0\n termcolor==1.1.0\n text-unidecode==1.3\n toolz==0.10.0\n tornado==6.0.4\n TPOT==0.11.1\n tqdm==4.45.0\n traitlets==4.3.3\n typing==3.7.4.1\n update-checker==0.16\n urllib3==1.25.8\n wcwidth==0.1.9\n wrapt==1.12.1\n xmltodict==0.12.0\n zict==2.0.0\n zipp==3.1.0\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Innixma", "commentT": "2020-04-13T18:33:22Z", "comment_text": "\n \t\tTo clarify, you never saw this error previously show up on Albert/Airlines before this issue was posted, right?  Or do you think that in the previous runs, the NN never got a chance to produce this error due to time limits, and it is now producing the error because of increased efficiency in the other models?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "Innixma", "commentT": "2020-04-13T21:02:47Z", "comment_text": "\n \t\tI believe the latter given more thought. This run was on non-bagged, so every model trains 10x faster in a sense. It is also possible that it didn't run OOM on bagged version because it failed in the try/except part of the code during training (NN predicts on validation in fit() with bagging, which the exception can get caught and NN gets skipped).\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "Innixma", "commentT": "2020-04-13T23:08:30Z", "comment_text": "\n \t\tOne fix that should avoid the os.fork() / multiprocessing is to change the self.num_dataloading_workers in these lines:\n \n \n \n autogluon/autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_model.py\n \n \n         Lines 172 to 174\n       in\n       7e551a2\n \n \n \n \n \n \n  self.num_dataloading_workers = max(1, int(kwargs['num_cpus']/2.0)) \n \n \n \n  else: \n \n \n \n  self.num_dataloading_workers = 1 \n \n \n \n \n \n to simply always have:\n self.num_dataloading_workers = 0\n This just supposes extra available threads will be utilized by the installed MXNet (MKL) as appropriate for the network computations rather than data-loading. It might be too unsafe for us to presume we can more aggressively ask for extra dataloading_worker threads for arbitrary heterogeneous datasets, especially when the available memory differs during training/validation/prediction phases based on outside context.\n If num_dataloading_workers = 0 fixes the issue, we could discuss setting num_dataloading_workers > 0 during training and num_dataloading_workers = 0 during prediction.  It's just unfortunately hard to come up with a single setting that exploits efficiency gains where possible but still works on any dataset.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "Innixma", "commentT": "2020-04-14T23:01:09Z", "comment_text": "\n \t\tDo you know why it is running OOM? Is the fork causing memory doubling? Does it call fork for every core? Would it make sense to multi-thread instead? Forking and doubling memory would also be a significant time overhead, which would actually slow down most inference.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "Innixma", "commentT": "2020-04-16T23:32:16Z", "comment_text": "\n \t\tDiscussed offline with <denchmark-link:https://github.com/jwmueller>@jwmueller</denchmark-link>\n  , according to the existing use case, there's no reason to use multiprocessing workers, since each worker process has to keep a copy of original ndarray.\n Two choices here:\n \n Stick with num_workers=0 for DataLoader, to avoid duplicate data in child processes\n Use num_workers>0, together with thread_pool=True, this will not bypass GIL, but can significantly reduce the memory usage, while potentially improve the IO\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "Innixma", "commentT": "2020-04-16T23:35:37Z", "comment_text": "\n \t\tThanks for your input!  I think we should first try thread_pool=True to try and mitigate potential slowdowns.\n Here is PR:  <denchmark-link:https://github.com/awslabs/autogluon/pull/416>#416</denchmark-link>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "Innixma", "commentT": "2020-04-17T01:22:22Z", "comment_text": "\n \t\tI was able to demonstrate NN slowdown on the current setting by creating a large list in memory before calling task.fit():\n long_lst = [i for i in range(200000000)]\n On my Mac, this 7 GB list increased inference time by ~20% consistently for the NN. This would probably be much more severe on a larger machine with more cores and more data in memory. Note that the other ML models were not impacted when LongLst was in memory.\n Inference Time by Scenario:\n Normal: 0.48s\n LongLst: 0.60s\n ThreadPool: 0.37s (Not affected by LongLst)\n NumWorkers0: 0.13s (Not affected by LongLst)\n I will plan to run a full benchmark with ThreadPool and NumWorkers0 settings to see how they compare to the current settings.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "Innixma", "commentT": "2020-04-17T02:12:28Z", "comment_text": "\n \t\tInteresting... after delving into it a bit, I also found the DataLoader behaves in counterintuitive ways.  Most of the public MXNet material regarding DataLoader practices is with regards to datasets stored on-disk rather than in-memory like the ArrayDataset we operate on.  Older MXNet models used to usually use DataIterator rather than DataLoader, but this documentation specifically recommends DataLoader over DataIterator:\n <denchmark-link:https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/data/datasets.html#Appendix:-Upgrading-from-Module-DataIter-to-Gluon-DataLoader>https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/data/datasets.html#Appendix:-Upgrading-from-Module-DataIter-to-Gluon-DataLoader</denchmark-link>\n \n It seems we may end up in the strange situation where the NN could plausibly run at optimal performance using num_workers = 0 rather than > 0, and using mxnet (single-thread) rather than mxnet-mkl (multi-thread)...\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "Innixma", "commentT": "2020-04-17T15:59:27Z", "comment_text": "\n \t\tBenchmark Results:\n <denchmark-code>\n                                framework  time_train_s  metric_error  time_infer_s       acc       auc   logloss  bestdiff  loss_rescaled      rank  rank=1_count  rank=2_count  rank=3_count  rank>3_count  error_count\n 0    autogluon_ag_nn_nostack_numwork0_4h    283.258974      0.330012      1.147583  0.831875  0.849475  0.562289  0.003097       0.025641  1.923077             4            34             1             0            0\n 1             autogluon_ag_nn_nostack_4h    244.100000      0.329844      1.204280  0.832035  0.849426  0.561840  0.002009       0.066691  2.025641             0            38             1             0            0\n 2  autogluon_ag_nn_nostack_threadpool_4h    883.158974      0.329747      1.817375  0.832004  0.849349  0.561519  0.001360       0.083982  2.051282             1            35             3             0            0\n \n </denchmark-code>\n \n This suggests to use the current setting during training, and numwork0 for inference, or perhaps numwork0 for both if we want to lean towards memory stability. thread_pool=True should be avoided, at least for m5.2xlarge instances.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "Innixma", "commentT": "2020-04-20T00:06:39Z", "comment_text": "\n \t\tI tested with an additional configuration (called _mixed in the benchmark): training set is kept as is, but validation set and test sets for inference use numworkers = 0. This achieves the best result with no downsides compared to the other methods:\n <denchmark-code>                               framework  time_train_s  metric_error  time_infer_s       acc       auc   logloss  bestdiff  loss_rescaled      rank  rank=1_count  rank=2_count  rank=3_count  rank>3_count  error_count\n 0    autogluon_ag_nn_nostack_numwork0_4h    290.444737      0.338697      1.174810  0.827697  0.842307  0.562289  0.003178       0.026316  2.381579             4             0            33             1            0\n 1             autogluon_ag_nn_nostack_4h    250.284211      0.338524      1.229560  0.827861  0.842256  0.561840  0.002062       0.054820  2.513158             0             2            36             0            0\n 2       autogluon_ag_nn_nostack_mixed_4h    243.205263      0.338736      1.173114  0.827857  0.842221  0.562271  0.002979       0.089021  2.539474             0             2            35             1            1\n 3  autogluon_ag_nn_nostack_threadpool_4h    905.784211      0.338425      1.859594  0.827830  0.842176  0.561519  0.001396       0.082441  2.565789             1             1            33             3            0\n \n </denchmark-code>\n \n Note that the inference speedup is much more significant than this data suggests, as the times are dominated by the large datasets. Most of the smaller datasets see upwards of a 400% inference speedup because of this change, and likely an even larger % improvement for online inference.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "Innixma", "commentT": "2020-04-20T00:19:19Z", "comment_text": "\n \t\tPlease see <denchmark-link:https://github.com/awslabs/autogluon/pull/422>#422</denchmark-link>\n  for the proposed solution.\n \t\t"}}}, "commit": {"commit_id": "04866e5a976c471df1c9272b9be15132f986ead9", "commit_author": "Nick Erickson", "commitT": "2020-04-20 12:23:57-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "autogluon\\utils\\tabular\\ml\\models\\tabular_nn\\tabular_nn_model.py", "file_new_name": "autogluon\\utils\\tabular\\ml\\models\\tabular_nn\\tabular_nn_model.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "465", "deleted_lines": "462", "method_info": {"method_name": "generate_datasets", "method_params": "self,X_train,y_train,params,X_test,y_test", "method_startline": "444", "method_endline": "468"}}, "hunk_1": {"Ismethod": 1, "added_lines": "404", "deleted_lines": "401", "method_info": {"method_name": "_predict_tabular_data", "method_params": "self,new_data,process,predict_proba", "method_startline": "394", "method_endline": "442"}}, "hunk_2": {"Ismethod": 1, "added_lines": "93", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,str,str,str,objective_func,stopping_metric,hyperparameters,features", "method_startline": "69", "method_endline": "98"}}, "hunk_3": {"Ismethod": 1, "added_lines": "179,180", "deleted_lines": null, "method_info": {"method_name": "fit", "method_params": "self,X_train,Y_train,X_test,Y_test,time_limit,reporter,kwargs", "method_startline": "161", "method_endline": "214"}}}}}}}