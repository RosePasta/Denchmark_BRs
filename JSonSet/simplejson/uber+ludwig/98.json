{"BR": {"BR_id": "98", "BR_author": "ctpmp", "BRopenT": "2019-02-15T18:59:40Z", "BRcloseT": "2019-03-21T02:06:08Z", "BR_text": {"BRsummary": "Image classification: could not broadcast input array from shape into shape", "BRdescription": "\n Receiving this error trying to run a simple image classification example\n <denchmark-code>  File \"/Users/colinpetit/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ludwig/features/image_feature.py\", line 101, in add_feature_data\n     data[feature['name']][i, :, :, :] = img\n ValueError: could not broadcast input array from shape (743,620,3) into shape (383,442,4)\n </denchmark-code>\n \n model_definition_file:\n <denchmark-code>input_features:\n   -\n     name: img\n     type: image\n     encoder: stacked_cnn\n output_features:\n   -\n     name: type\n     type: category\n training:\n   epochs: 10\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ctpmp", "commentT": "2019-02-15T19:18:12Z", "comment_text": "\n \t\tI have to specify this better in the docs: Either your images are all of the same size, or you have to resize them to the same size by specifying the ,  and  parameters as explained <denchmark-link:https://uber.github.io/ludwig/user_guide/#image-features-preprocessing>here</denchmark-link>\n . Let me know if this solves your issue.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ctpmp", "commentT": "2019-02-16T21:50:09Z", "comment_text": "\n \t\tI've set a resize as follows\n input_features:\n     -\n         name: image_path\n         type: image\n         encoder: stacked_cnn\n         resize_method: crop_or_pad\n         height: 500\n         width: 500\n \n output_features:\n     -\n         name: class\n         type: category\n However I still get an error.\n ValueError: could not broadcast input array from shape (500,500,4) into shape (500,500,3)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ctpmp", "commentT": "2019-02-17T01:00:50Z", "comment_text": "\n \t\tInteresting, that 4 as the last dimension of your image, may mean images with an alpha channel, something I may have not considered. Can you send me an image form you dataset that has a fourth channel so that I can reproduce the issue?\n <denchmark-link:https://github.com/ydudin3>@ydudin3</denchmark-link>\n  could you please try to take a look at this too?\n Another remark: I would suggest not exceeding 224x224 as a image size, as othersize model training would require a lot of memory and it will be really slow. <denchmark-link:https://github.com/ydudin3>@ydudin3</denchmark-link>\n  maybe we can add something about it in the documentation, plus moving all parameters from the feature itself to preprocessing as it happens for other features and impose a defaul maximum size, so that if the dimension of images is above that size images are resized users get a warning printed on the console.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ctpmp", "commentT": "2019-02-17T12:33:27Z", "comment_text": "\n \t\tIt is the Stanford dog breeds data set, I couldn't find the one or two with an alpha channel but I think that was the issue.\n I just removed the alpha channel with <denchmark-link:https://stackoverflow.com/a/52962485>this</denchmark-link>\n .\n And yes, size was far too big! 112x112 was getting quick results.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ctpmp", "commentT": "2019-02-17T19:57:32Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/mattarderne>@mattarderne</denchmark-link>\n  for confirming it, will work on a patch that solves this issue, it will be pretty straightforward.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "ctpmp", "commentT": "2019-03-07T08:06:27Z", "comment_text": "\n \t\tHello,  <denchmark-link:https://github.com/w4nderlust>@w4nderlust</denchmark-link>\n \n Can I update the docs <denchmark-link:https://uber.github.io/ludwig/user_guide/#image-features-preprocessing>image-preprocessing</denchmark-link>\n   with your suggestion\n \n \n Ludwig supports grayscale and color images, image type is automatically inferred. During preprocessing raw image files are transformed into numpy ndarrays and saved in the hdf5 format. Images should have the same size (keep the image size under 224x224 pixels). If they have different sizes they should be converted to the same size by specifing the resize_method, width,  and height parameters of image feature preprocessing.\n \n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "ctpmp", "commentT": "2019-03-07T19:09:07Z", "comment_text": "\n \t\tThanks for collecting this. Wait a bit as we are going to change a few things, including the support for images with alpha channels.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "ctpmp", "commentT": "2019-03-07T22:39:13Z", "comment_text": "\n \t\tSo all parameters have been moved to preprocessing, <denchmark-link:https://github.com/ludwig-ai/ludwig/pull/141>#141</denchmark-link>\n  . Now there is a PR for dealing with the 4th channel and image sizes too. Stay tuned.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "ctpmp", "commentT": "2019-03-21T02:09:17Z", "comment_text": "\n \t\tI added a temporary workaround for images with a 4th channel. We will improve over this solution (which basically just ignores the alpha channel), but at the moment should fix this issue. <denchmark-link:https://github.com/mattarderne>@mattarderne</denchmark-link>\n  and <denchmark-link:https://github.com/ctpmp>@ctpmp</denchmark-link>\n  can you please confirm that with the code on master this issue is solved?\n \t\t"}}}, "commit": {"commit_id": "ec52e8262f9bf7a6738f84d4efb20f44c90bfee4", "commit_author": "piero", "commitT": "2019-03-20 19:05:56-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "ludwig\\features\\image_feature.py", "file_new_name": "ludwig\\features\\image_feature.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "31,150,151,152", "deleted_lines": "30"}}}}}}