{"BR": {"BR_id": "950", "BR_author": "murthyrudra", "BRopenT": "2018-09-06T16:15:20Z", "BRcloseT": "2019-01-27T07:49:39Z", "BR_text": {"BRsummary": "\"-fix_word_vecs_enc\" triggers an error when reloading an existing model", "BRdescription": "\n Hi, I was training the Transformer model with the options as specified in <denchmark-link:link>http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model</denchmark-link>\n . The training works fine, but the problem arises when i stop the training and resume the training using the option . I get the following error:\n <denchmark-code>    Traceback (most recent call last):\n       File \"train.py\", line 40, in <module>\n         main(opt)\n       File \"train.py\", line 27, in main\n         single_main(opt)\n       File \"/home/rudra/OpenNMT-py/onmt/train_single.py\", line 113, in main\n         optim = build_optim(model, opt, checkpoint)\n       File \"/home/rudra/OpenNMT-py/onmt/utils/optimizers.py\", line 62, in build_optim\n         optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n       File \"/home/rudra/miniconda3/envs/py36/lib/python3.6/site-packages/torch/optim/optimizer.py\", line         107, in load_state_dict\n         raise ValueError(\"loaded state dict contains a parameter group \"\n     ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group\n </denchmark-code>\n \n The error occurs when the model tries to load the optimizer state in the File utils/optimizers.py line 62. Could anyone please help me in debugging this error?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "murthyrudra", "commentT": "2018-09-06T16:45:53Z", "comment_text": "\n \t\tGive your command lines, both the first initial training and the train_from\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "murthyrudra", "commentT": "2018-09-06T16:57:10Z", "comment_text": "\n \t\tHi, the initial command I ran was\n <denchmark-code>CUDA_VISIBLE_DEVICES=1 python train.py -data data/demo -save_model model-transformer-baseline \n -pre_word_vecs_enc data/embeddings.enc.pt -word_vec_size 300 -gpuid 1 -encoder_type transformer \n -decoder_type transformer -layers 5 -rnn_size 300 -transformer_ff 500 -heads 5 -fix_word_vecs_enc \n -max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens -normalization tokens \n -accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 \n -learning_rate 1 -max_grad_norm 0 -param_init 0  -param_init_glorot -label_smoothing 0.1 \n -valid_steps 5000 -save_checkpoint_steps 5000\n </denchmark-code>\n \n I stopped the training after 5000 steps and ran the following command\n <denchmark-code>CUDA_VISIBLE_DEVICES=1 python train.py -data data/demo -save_model model-transformer-baseline \n -pre_word_vecs_enc data/embeddings.enc.pt -word_vec_size 300 -gpuid 1 -encoder_type transformer \n -decoder_type transformer -layers 5 -rnn_size 300 -transformer_ff 500 -heads 5 -fix_word_vecs_enc \n -max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens -normalization tokens \n -accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 \n -learning_rate 1 -max_grad_norm 0 -param_init 0  -param_init_glorot -label_smoothing 0.1 \n -valid_steps 5000 -save_checkpoint_steps 5000 -train_from model-transformer-baseline_step_5000.pt\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "murthyrudra", "commentT": "2018-09-06T18:05:13Z", "comment_text": "\n \t\tit might not be theproblem but you are not on master. line 62 of optimizer.py is not the same.\n git pull and repost the error.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "murthyrudra", "commentT": "2018-09-07T03:34:59Z", "comment_text": "\n \t\tHi, I cloned the master branch and ran the model using the same command as earlier. I am getting the same error. I am using PyTorch 0.4.0 and Cuda 9.0\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "murthyrudra", "commentT": "2018-09-07T05:33:48Z", "comment_text": "\n \t\tI need the error trace to see the lines number\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "murthyrudra", "commentT": "2018-09-07T05:36:11Z", "comment_text": "\n \t\tHi, the error occurs when the optimizer tries to load the saved state, specifically in the code optim/optimizer.py in <denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py>https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py</denchmark-link>\n  line 107. I tried printing the contents of  and  and the  has length of 218 whereas  has length of 217 and the individual contents are in the attached file\n <denchmark-link:https://github.com/OpenNMT/OpenNMT-py/files/2359639/error.txt>error.txt</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "murthyrudra", "commentT": "2018-09-07T05:40:39Z", "comment_text": "\n \t\tHi, Please find the error traceback:\n <denchmark-code>traceback (most recent call last):\n   File \"train.py\", line 40, in <module>\n     main(opt)\n   File \"train.py\", line 27, in main\n     single_main(opt)\n   File \"/home/rudra/NMT/OpenNMT/OpenNMT-py/onmt/train_single.py\", line 112, in main\n     optim = build_optim(model, opt, checkpoint)\n   File \"/home/rudra/NMT/OpenNMT/OpenNMT-py/onmt/utils/optimizers.py\", line 51, in build_optim\n     optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n   File \"/home/rudra/miniconda3/envs/py36/lib/python3.6/site-packages/torch/optim/optimizer.py\", line 107, in load_state_dict\n raise ValueError(\"loaded state dict contains a parameter group \"\n ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "murthyrudra", "commentT": "2018-09-07T06:34:05Z", "comment_text": "\n \t\tcan  you remove this -pre_word_vecs_enc data/embeddings.enc.pt (and maybe the -fix_word_vecs_enc) from the second command line ?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "murthyrudra", "commentT": "2018-09-07T06:54:43Z", "comment_text": "\n \t\tHi, I am getting the same error. The error occurs when it tries to load the previously stored Optimizer's state. I did try bypassing the loading of Optimizer's state, but that results in perplexity being Nan values.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "murthyrudra", "commentT": "2018-09-07T07:05:31Z", "comment_text": "\n \t\tJust as a sanity check can you please retrain from scratch without -pre_word_vecs_enc -fix_word_vecs_enc\n save after afew steps, and then do a train_from ?\n thanks\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "murthyrudra", "commentT": "2018-09-07T08:26:20Z", "comment_text": "\n \t\tHi, If I train without specifying pre-trained word embeddings using pre_word_vecs_enc and the -fix_word_vecs_enc option and retraining using -train_from, the model works well. There is no error and the perplexity values seems good.\n Now, I tried specifying only pre-trained word embeddings using pre_word_vecs_enc and retraining after some steps, the model works well, with no error. The problem arises only when I specify the argument fix_word_vecs_enc.\n Thanks\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "murthyrudra", "commentT": "2019-01-27T03:19:34Z", "comment_text": "\n \t\tThe problem has beed solved by pull request <denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/1211>#1211</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "6a8b4b1b4cc1877e6267893e8f065a7126196045", "commit_author": "Zhenxin Fu", "commitT": "2019-01-24 11:37:35+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "onmt\\model_builder.py", "file_new_name": "onmt\\model_builder.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "295,298", "deleted_lines": "291,294", "method_info": {"method_name": "build_base_model", "method_params": "model_opt,fields,gpu,checkpoint", "method_startline": "174", "method_endline": "303"}}, "hunk_1": {"Ismethod": 1, "added_lines": "44,45,46,58,59", "deleted_lines": "55", "method_info": {"method_name": "build_embeddings", "method_params": "opt,word_field,feat_fields,for_encoder", "method_startline": "28", "method_endline": "61"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "onmt\\modules\\embeddings.py", "file_new_name": "onmt\\modules\\embeddings.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "98", "deleted_lines": "98", "method_info": {"method_name": "__init__", "method_params": "self,word_vec_size,word_vocab_size,word_padding_idx,position_encoding,feat_merge,feat_vec_exponent,feat_vec_size,feat_padding_idx,feat_vocab_sizes,dropout,sparse", "method_startline": "89", "method_endline": "98"}}, "hunk_1": {"Ismethod": 1, "added_lines": "98,99", "deleted_lines": "98", "method_info": {"method_name": "__init__", "method_params": "self,word_vec_size,word_vocab_size,word_padding_idx,position_encoding,feat_merge,feat_vec_exponent,feat_vec_size,feat_padding_idx,feat_vocab_sizes,dropout,sparse,fix_word_vecs", "method_startline": "89", "method_endline": "99"}}, "hunk_2": {"Ismethod": 1, "added_lines": "173", "deleted_lines": "186,187", "method_info": {"method_name": "load_pretrained_vectors", "method_params": "self,emb_file", "method_startline": "173", "method_endline": "189"}}, "hunk_3": {"Ismethod": 1, "added_lines": "173", "deleted_lines": "169,186,187", "method_info": {"method_name": "load_pretrained_vectors", "method_params": "self,emb_file,fixed", "method_startline": "169", "method_endline": "187"}}}}}}}