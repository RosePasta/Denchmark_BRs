{"BR": {"BR_id": "580", "BR_author": "kongwei9901", "BRopenT": "2019-05-08T17:45:12Z", "BRcloseT": "2019-12-11T19:39:51Z", "BR_text": {"BRsummary": "Reshap error for SHAP calculation", "BRdescription": "\n Hi Scott,\n We got a reshape error when trying to test SHAP on our data. Have you seen something similar?\n ValueError: cannot reshape array of size 207506055 into shape (255235,0,815)\n Also please see similar errors reported here\n <denchmark-link:https://github.com/dmlc/xgboost/issues/4276>dmlc/xgboost#4276</denchmark-link>\n \n <denchmark-link:https://discuss.xgboost.ai/t/scala-spark-xgboost-v0-81-shap-problem/817/2>https://discuss.xgboost.ai/t/scala-spark-xgboost-v0-81-shap-problem/817/2</denchmark-link>\n \n Let me know if you need to more information to investigate.\n Best,\n Wei\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kongwei9901", "commentT": "2019-05-09T14:39:38Z", "comment_text": "\n \t\tError is pointing to the XGBoost code:\n ...anaconda3/lib/python3.7/site-packages/shap/explainers/tree.py in shap_values(self, X, y, tree_limit, approximate)\n 177                 phi = self.model.original_model.predict(\n 178                     X, ntree_limit=tree_limit, pred_contribs=True,\n --> 179                     approx_contribs=approximate, validate_features=False\n 180                 )\n 181\n .../anaconda3/lib/python3.7/site-packages/xgboost/core.py in predict(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\n 1310                     preds = preds.reshape(nrow, data.num_col() + 1)\n 1311                 else:\n -> 1312                     preds = preds.reshape(nrow, ngroup, data.num_col() + 1)\n 1313             else:\n 1314                 preds = preds.reshape(nrow, chunk_size)\n ValueError: cannot reshape array of size 207506055 into shape (255235,0,815)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kongwei9901", "commentT": "2019-05-31T08:13:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/devs>@devs</denchmark-link>\n  Any updates??\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kongwei9901", "commentT": "2019-07-26T14:11:00Z", "comment_text": "\n \t\tHappened to me as well. With a bigger impact dataset it does not happen, looks like there should be a reshape(nrow, ngroup, data.num_col() + 1, -1) to make lumpy figure out the third dimension automatically? It's an xgboost issue\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "kongwei9901", "commentT": "2019-07-27T02:42:19Z", "comment_text": "\n \t\tHey, thanks for reporting this! Sorry I lost track of the issue thread. I think this problem was fixed in the latest XGBoost master. If not though, I can look back into it. What versions are you using of XGboost when you see the problem?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "kongwei9901", "commentT": "2019-07-27T13:58:44Z", "comment_text": "\n \t\tHi Scott,\n We were using xgboost 0.81 and 0.82. Let me know if you need more information to investigate. We will try xgboost 0.9 from our end as well. Thanks for your time!\n Best,\n Wei\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "kongwei9901", "commentT": "2019-07-27T14:53:00Z", "comment_text": "\n \t\tI am using version 0.90 of xgboost (py3.6)\n \n  Andy\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "kongwei9901", "commentT": "2019-08-13T07:30:03Z", "comment_text": "\n \t\tI got the same reshape error.  Have you fixed the problem?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "kongwei9901", "commentT": "2019-08-22T14:52:47Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/slundberg>@slundberg</denchmark-link>\n , same problem, with xgboost 0.90(py3.6), tried two version of shap  - 0.29.1 and 0.29.3\n It's quite popular problem\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "kongwei9901", "commentT": "2019-08-31T16:17:00Z", "comment_text": "\n \t\tCan anyone share a simple notebook demonstrating the problem? I\u2019ll try and debug anything I can reproduce. Thanks!\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "kongwei9901", "commentT": "2019-09-20T13:30:31Z", "comment_text": "\n \t\tHello, are there solutions to this issue ?\n Thanks\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "kongwei9901", "commentT": "2019-09-22T05:34:44Z", "comment_text": "\n \t\tI'm not sure if this is related to: <denchmark-link:https://github.com/slundberg/shap/issues/743>#743</denchmark-link>\n \n But as I mentioned, I am experiencing a similar issue with LightGBM:\n shap_values = shap.TreeExplainer(clf).shap_values(dataset.iloc[:10000,:])\n <denchmark-code>ValueError                                Traceback (most recent call last)\n <timed exec> in <module>\n \n ~/anaconda3/envs/kaggle_env/lib/python3.7/site-packages/shap/explainers/tree.py in shap_values(self, X, y, tree_limit, approximate)\n     196                     phi = np.concatenate((-phi, phi), axis=-1)\n     197                 if phi.shape[1] != X.shape[1] + 1:\n --> 198                     phi = phi.reshape(X.shape[0], phi.shape[1]//(X.shape[1]+1), X.shape[1]+1)\n     199 \n     200             elif self.model.model_type == \"catboost\": # thanks to the CatBoost team for implementing this...\n \n ValueError: cannot reshape array of size 23020000 into shape (10000,1,1155)\n </denchmark-code>\n \n where dataset.shape is: (472432, 1154)\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "kongwei9901", "commentT": "2019-09-24T20:01:21Z", "comment_text": "\n \t\tI am facing the same issue. The code works for one model and doesn't work for another one.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "kongwei9901", "commentT": "2019-09-25T17:04:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pyotam>@pyotam</denchmark-link>\n  that looks like it is with a LightGBM model. I have not been able to reproduce this on my end, so if anyone has a notebook that shows this I would be happy to debug it.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "kongwei9901", "commentT": "2019-09-26T16:31:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/slundberg>@slundberg</denchmark-link>\n  I sent you a small notebook.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "kongwei9901", "commentT": "2019-10-23T21:56:51Z", "comment_text": "\n \t\tIn my case it turned out that I was removing some of the features during training and not removing them at the time of prediction + SHAP computation. It is strange that the model was able to do the prediction! but thankfully SHAP computation complained!\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "kongwei9901", "commentT": "2019-12-03T12:16:58Z", "comment_text": "\n \t\tI'll +1 <denchmark-link:https://github.com/saberian>@saberian</denchmark-link>\n 's comment, using XGBoost 0.9 and sklearn 0.22 I had the same error message in a complex Notebook, it turns out I was passing the raw  (with all features) to shap but had trained XGBoost on a  of features.\n The error message didn't make this clear:\n <denchmark-code>shap_values = explainer.shap_values(X_train) # missing column mask\n ...\n ValueError: cannot reshape array of size 93058 into shape (13294,0,33)\n (from xgboost/core.py)\n </denchmark-code>\n \n Possibly it might help others to add a hint inside SHAP to spot this user-error?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "kongwei9901", "commentT": "2019-12-11T19:39:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ianozsvald>@ianozsvald</denchmark-link>\n  good idea, I added note to the exception that points to this issue. Closing this since I think this problem now only comes from passing bad data matrices, and we now have a good error message for this.\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "kongwei9901", "commentT": "2020-01-10T12:05:49Z", "comment_text": "\n \t\tI cannot get.\n Shap is adding 3 extra columns or who...\n I have  a shape (1559458, 634), but Shap see it as (1559458, 637)..\n <denchmark-link:https://user-images.githubusercontent.com/28587948/72152002-cdecc000-33ba-11ea-973c-0269457639d0.png></denchmark-link>\n \n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "kongwei9901", "commentT": "2020-01-10T15:30:30Z", "comment_text": "\n \t\t\n I cannot get.\n Shap is adding 3 extra columns or who...\n I have a shape (1559458, 634), but Shap see it as (1559458, 637)..\n \n \n looks like my booster from scala contains smth more than needed..\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "kongwei9901", "commentT": "2020-05-12T15:55:27Z", "comment_text": "\n \t\tI had the same problem with LGBM and turned out to be pretty easy to solve:\n I was passing more columns than the ones the model was trained with.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "kongwei9901", "commentT": "2020-08-25T07:26:34Z", "comment_text": "\n \t\tYes. The reason behind is that you are passing more columns in the explainer than the trained-model\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "kongwei9901", "commentT": "2020-09-17T07:32:42Z", "comment_text": "\n \t\tI faced the same problem with lightGBM\uff0c the reason is that the columns in the explainer is not the same as the trained_model, is it right?  If so, that means we must gaurantee  the length of  columns in the explainner equals to the length of columns in the trained model\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "kongwei9901", "commentT": "2020-10-12T12:32:28Z", "comment_text": "\n \t\tlook like this bug is still continued while doing SHAP on XGB Classifier.\n ValueError: This reshape error is often caused by passing a bad data matrix to SHAP. See <denchmark-link:https://github.com/slundberg/shap/issues/580>#580</denchmark-link>\n \n Traceback:\n File \"c:\\users\\tushi\\anaconda3\\lib\\site-packages\\streamlit\\ScriptRunner.py\", line 322, in _run_script\n exec(code, module.)\n File \"D:\\Project_CGI_INSOFE\\pickle\\cgi.py\", line 55, in \n shap_values = explainer.shap_values(X_train_sample).reshape(-1,1)\n File \"c:\\users\\tushi\\anaconda3\\lib\\site-packages\\shap\\explainers_tree.py\", line 278, in shap_values\n \"See <denchmark-link:https://github.com/slundberg/shap/issues/580>#580</denchmark-link>\n \") from e\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "kongwei9901", "commentT": "2020-12-15T15:52:51Z", "comment_text": "\n \t\tI confirm that this is a very strange error. I am particularly facing it when resampling:\n # Quick test that everything works ok:\n dtest = xgb.DMatrix(features)\n model.predict(dtest)\n print('features are ok!')\n print(features.shape)\n \n # Shap implementation of model\n explainer = shap.TreeExplainer(model)\n \n X = shap.sample(features, 100)\n print( type(X), X.shape )\n \n model.predict(xgb.DMatrix(X))\n shap_values = explainer.shap_values(xgb.DMatrix(X))\n When I run it I get this:\n features are ok!\n (544401, 92)\n <class 'pandas.core.frame.DataFrame'> (100, 92)\n Traceback (most recent call last):\n   File \"/home/vladimir/.local/share/virtualenvs/ICFES-SocioEconomico-eS63OEPi/lib/python3.8/site-packages/shap/explainers/_tree.py\", line 280, in shap_values\n     phi = self.model.original_model.predict(\n   File \"/home/vladimir/.local/share/virtualenvs/ICFES-SocioEconomico-eS63OEPi/lib/python3.8/site-packages/xgboost/core.py\", line 1397, in predict\n     preds = preds.reshape(nrow, ngroup, data.num_col() + 1)\n ValueError: cannot reshape array of size 23400 into shape (100,2,93)\n \n The above exception was the direct cause of the following exception:\n \n Traceback (most recent call last):\n   File \"shapsummaryplots.py\", line 122, in <module>\n     shap_summary_plots(2014, 2)\n   File \"shapsummaryplots.py\", line 97, in shap_summary_plots\n     shap_values = explainer.shap_values(xgb.DMatrix(X))\n   File \"/home/vladimir/.local/share/virtualenvs/ICFES-SocioEconomico-eS63OEPi/lib/python3.8/site-packages/shap/explainers/_tree.py\", line 285, in shap_values\n     raise ValueError(\"This reshape error is often caused by passing a bad data matrix to SHAP. \" \\\n ValueError: This reshape error is often caused by passing a bad data matrix to SHAP. See https://github.com/slundberg/shap/issues/580\n so out of nowhere it seems that SHAP is passing new data to XGBoost? (ValueError: cannot reshape array of size 23400 into shape (100,2,93))\n Definitely I'm passing the correct number of columns, no matter if I sample or not the features dataframe, as model.predict(xgb.DMatrix(X)) works ok, but then when getting the shap values with the explainer it fails!\n My shap version is 0.37 and my xgboost version is 1.2.1\n \t\t"}}}, "commit": {"commit_id": "4625b2e9d53e5e102f6aeee7b9ee7f6dffe63ae3", "commit_author": "Scott Lundberg", "commitT": "2019-12-11 11:36:38-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "shap\\explainers\\tree.py", "file_new_name": "shap\\explainers\\tree.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "226,227,228,229,230", "deleted_lines": "226", "method_info": {"method_name": "shap_values", "method_params": "self,X,y,tree_limit,approximate,check_additivity", "method_startline": "154", "method_endline": "320"}}}}}}}