{"BR": {"BR_id": "541", "BR_author": "vfdev-5", "BRopenT": "2019-06-02T00:47:55Z", "BRcloseT": "2019-09-21T08:22:00Z", "BR_text": {"BRsummary": "Possibly a bug when try to simulate lr scheduling values", "BRdescription": "\n The following code fails:\n lr_scheduler = create_lr_scheduler_with_warmup(\n     StepLR(optimizer, step_size=2.4, gamma=0.97),\n     warmup_start_value=0.0,\n     warmup_end_value=scaled_lr,\n     warmup_duration=5,\n     output_simulated_values=[None] * 50\n )\n with the error\n <denchmark-code>ignite/ignite/contrib/handlers/param_scheduler.py in _replicate_lr_scheduler(lr_scheduler, new_optimizer_param_groups)\n     489         t = torch.zeros([1], requires_grad=True)\n     490         dummy_optimizer = optimizer_cls([t], lr=0.1)\n --> 491         dummy_optimizer.load_state_dict(lr_scheduler.optimizer.state_dict())\n     492         if new_optimizer_param_groups is not None:\n     493             dummy_optimizer.param_groups = new_optimizer_param_groups\n \n /opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py in load_state_dict(self, state_dict)\n     113         saved_lens = (len(g['params']) for g in saved_groups)\n     114         if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n --> 115             raise ValueError(\"loaded state dict contains a parameter group \"\n     116                              \"that doesn't match the size of optimizer's group\")\n     117 \n \n ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group\n </denchmark-code>\n \n In this case optimizer has a real model inside.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "vfdev-5", "commentT": "2019-06-02T11:20:33Z", "comment_text": "\n \t\tAnother observation: we need to multiply by 1.25 warmup_end_value in order to end the warmup with warmup_end_value value. This should be fixed.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "vfdev-5", "commentT": "2019-07-30T01:03:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/vfdev-5>@vfdev-5</denchmark-link>\n  could you provide some more code to reproduce this issue?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "vfdev-5", "commentT": "2019-07-30T08:49:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/anmolsjoshi>@anmolsjoshi</denchmark-link>\n  following snippet should reproduce the error:\n import torch\n from torch.optim.lr_scheduler import StepLR\n \n from ignite.contrib.handlers import create_lr_scheduler_with_warmup\n \n from torchvision.models.resnet import resnet18\n \n model = resnet18()\n \n optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n \n output_simulated_values = [None] * 50\n \n lr_scheduler = create_lr_scheduler_with_warmup(\n     StepLR(optimizer, step_size=2.4, gamma=0.97),\n     warmup_start_value=0.0,\n     warmup_end_value=0.01,\n     warmup_duration=5,\n     output_simulated_values=output_simulated_values\n )\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "vfdev-5", "commentT": "2019-07-31T01:34:04Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/vfdev-5>@vfdev-5</denchmark-link>\n  updating <denchmark-link:https://github.com/pytorch/ignite/blob/master/ignite/contrib/handlers/param_scheduler.py#L490>this line of code</denchmark-link>\n  to the code below seems to resolve the issue and the tests are still passing.\n Do you know if this correctly resolves the issue? I don't want things to silently fail.\n dummy_optimizer = optimizer_cls(lr_scheduler.optimizer.param_groups[0][\"params\"], lr=0.1)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "vfdev-5", "commentT": "2019-07-31T01:35:56Z", "comment_text": "\n \t\tCould you clarify <denchmark-link:https://github.com/pytorch/ignite/issues/541#issuecomment-498022062>this comment</denchmark-link>\n  further?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "vfdev-5", "commentT": "2019-07-31T07:00:56Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/anmolsjoshi>@anmolsjoshi</denchmark-link>\n , the only problem I feel with this fix\n dummy_optimizer = optimizer_cls(lr_scheduler.optimizer.param_groups[0][\"params\"], lr=0.1)\n is that we pass the real model's parameters to the replicated optimizer and lr scheduler. Maybe the thing wont work if there more than 1 param_groups ?\n I was thinking about whether the following line is really necessary :\n \n \n \n ignite/ignite/contrib/handlers/param_scheduler.py\n \n \n          Line 491\n       in\n       c5dda81\n \n \n \n \n \n \n  dummy_optimizer.load_state_dict(lr_scheduler.optimizer.state_dict()) \n \n \n \n \n \n maybe can omit it ?\n Concerning <denchmark-link:https://github.com/pytorch/ignite/issues/541#issuecomment-498022062>#541 (comment)</denchmark-link>\n , I think I fixed this <denchmark-link:https://github.com/pytorch/ignite/pull/556>here</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "vfdev-5", "commentT": "2019-08-02T03:56:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/vfdev-5>@vfdev-5</denchmark-link>\n  commenting that line out caused other tests to fail. <denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L24-L25>This error</denchmark-link>\n  is being raised.\n Could you explain <denchmark-link:https://github.com/pytorch/ignite/blob/master/tests/ignite/contrib/handlers/test_param_scheduler.py#L338-L339>this test</denchmark-link>\n ?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "vfdev-5", "commentT": "2019-08-02T07:04:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/anmolsjoshi>@anmolsjoshi</denchmark-link>\n  this should check this condition:\n \n \n I agree that it would be better to write it as :\n with pytest.raises(ValueError, match=\"Optimizer passed to lr_scheduler should have a single param group\"):\n         LRScheduler.simulate_values(num_events=100, lr_scheduler=lr_scheduler)\n \n commenting that line out caused other tests to fail. This error is being raised.\n \n we can possibly set manually initial_lr into dummy optimizer ?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "vfdev-5", "commentT": "2019-08-23T06:48:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/anmolsjoshi>@anmolsjoshi</denchmark-link>\n  any updates on this ?\n \t\t"}}}, "commit": {"commit_id": "5bb629ad2fca56d5e6abf6fdfa0e6a7a6526309e", "commit_author": "vfdev", "commitT": "2019-09-21 10:21:59+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "ignite\\contrib\\handlers\\param_scheduler.py", "file_new_name": "ignite\\contrib\\handlers\\param_scheduler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "490,493,494", "deleted_lines": "490,491", "method_info": {"method_name": "_replicate_lr_scheduler", "method_params": "lr_scheduler,new_optimizer_param_groups", "method_startline": "484", "method_endline": "500"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\ignite\\contrib\\handlers\\test_param_scheduler.py", "file_new_name": "tests\\ignite\\contrib\\handlers\\test_param_scheduler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733", "deleted_lines": null, "method_info": {"method_name": "test_create_lr_scheduler_with_warmup_with_real_model", "method_params": "dummy_model_factory", "method_startline": "715", "method_endline": "733"}}}}}}}