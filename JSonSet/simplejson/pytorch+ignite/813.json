{"BR": {"BR_id": "813", "BR_author": "sdesrozis", "BRopenT": "2020-02-28T08:35:32Z", "BRcloseT": "2020-05-29T15:04:33Z", "BR_text": {"BRsummary": "CosineAnnealingWarmRestarts is not compatible with LRScheduler.simulate_values()", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug description</denchmark-h>\n \n CosineAnnealingWarmRestarts is not compatible with LRScheduler.simulate_values(). Precisely, a object of type CosineAnnealingWarmRestarts can't be replicated by LRScheduler._replicate_lr_scheduler(). It works for CosineAnnealingLR.\n For example :\n <denchmark-code>lr_scheduler = CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=10, eta_min=1e-3)\n lr_values = LRScheduler.simulate_values(num_events=50, lr_scheduler=lr_scheduler)\n </denchmark-code>\n \n produces the following error\n <denchmark-code>Traceback (most recent call last):\n   File \"tutorials/misc/lr_schedulers.py\", line 56, in <module>\n     lr_values = LRScheduler.simulate_values(num_events=50, lr_scheduler=lr_scheduler)\n   File \"/work/desrozis/Softwares/conda/envs/focus-light/lib/python3.7/site-packages/ignite/contrib/handlers/param_scheduler.py\", line 606, in simulate_values\n     copy_lr_scheduler = LRScheduler._replicate_lr_scheduler(lr_scheduler)\n   File \"/work/desrozis/Softwares/conda/envs/focus-light/lib/python3.7/site-packages/ignite/contrib/handlers/param_scheduler.py\", line 627, in _replicate_lr_scheduler\n     copy_lr_scheduler = lr_scheduler_cls(optimizer=dummy_optimizer, **kwargs)\n TypeError: __init__() got an unexpected keyword argument 'T_i'\n </denchmark-code>\n \n This issue is due to the assumption that lr_scheduler.__state_dict__ contains arguments of method CosineAnnealingWarmRestarts.__init__(). A workaround could be to remove T_i in a similar way to base_lrs and last_epoch but it's not very satisfactory...\n Hope it helps to have a better and stonger ignite.\n <denchmark-h:h2>Environment</denchmark-h>\n \n \n PyTorch Version (e.g., 1.4): 1.4\n Ignite Version (e.g., 0.3.0): 0.3.0\n OS (e.g., Linux): Linux\n How you installed Ignite (conda, pip, source): conda\n Python version: 3.7.6\n Any other relevant information:\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sdesrozis", "commentT": "2020-02-28T10:05:02Z", "comment_text": "\n \t\tThanks for the report <denchmark-link:https://github.com/sdesrozis>@sdesrozis</denchmark-link>\n  ! I'll investigate the issue and push a fix.\n EDIT: Seems like implementation of CosineAnnealingWarmRestarts is a bit different from other LR schedulers, e.g. CosineAnnealingLR. This implies that it wont work even for LR scheduling (not simulation of lr values). The difference is in step and get_lr implementations. In CosineAnnealingWarmRestarts, get_lr does not account on optimizer's lr vs CosineAnnealingLR and others. So, get_lr is stateless but internally in ignite, we use only get_lr without calling step. Finally, this gives a constant learning rate.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sdesrozis", "commentT": "2020-04-01T20:03:35Z", "comment_text": "\n \t\tWhat about copy.deepcopy() ? Does it work on a _LRScheduler ? It could replace the home made replication ?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sdesrozis", "commentT": "2020-05-26T12:54:57Z", "comment_text": "\n \t\tThe bug is also with MultiplicativeLR. The way to replicate _LRScheduler is really not safe. This scheduler have an argument lr_lambda but it stores in self.lr_lambdas. To replicate, we use self.__state_dict__ and pass it to __init__. It can't works.\n IMO plot values with replication is very tricky.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "sdesrozis", "commentT": "2020-05-26T12:56:58Z", "comment_text": "\n \t\t\n What about copy.deepcopy() ? Does it work on a _LRScheduler ? It could replace the home made replication ?\n \n copy.deepcopy() will replicate params of the related optimizer, not a good idea.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "sdesrozis", "commentT": "2020-05-26T12:59:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/vfdev-5>@vfdev-5</denchmark-link>\n  maybe we should accept that we can't use a  after ploting values ? (and do not replicate but can plot for every schedulers).\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "sdesrozis", "commentT": "2020-05-26T13:03:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sdesrozis>@sdesrozis</denchmark-link>\n  how about the same approach as used with lr finders etc :\n \n save initial state dicts\n do something\n restore init state\n ?\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "sdesrozis", "commentT": "2020-05-26T13:05:22Z", "comment_text": "\n \t\tYou save my day \ud83d\ude0a I check if it\u2019s ok.\n \t\t"}}}, "commit": {"commit_id": "b6af7764e5c2dfdb11a722023090aa8bab527094", "commit_author": "Sylvain Desroziers", "commitT": "2020-05-29 17:04:32+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 13, "file_old_name": "ignite\\contrib\\handlers\\param_scheduler.py", "file_new_name": "ignite\\contrib\\handlers\\param_scheduler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012", "method_info": {"method_name": "_replicate_scheduler", "method_params": "scheduler,save_history", "method_startline": "994", "method_endline": "1012"}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "652,653,654,655,656,657,658,659,660,661,662,663,664", "method_info": {"method_name": "_replicate_lr_scheduler", "method_params": "lr_scheduler", "method_startline": "652", "method_endline": "664"}}, "hunk_2": {"Ismethod": 1, "added_lines": "563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595", "deleted_lines": "552,553,554,555,557,558,559,560,561,562,563,564,565,566,567,594", "method_info": {"method_name": "simulate_values", "method_params": "cls,num_events,schedulers,durations,param_names,kwargs", "method_startline": "546", "method_endline": "595"}}, "hunk_3": {"Ismethod": 1, "added_lines": "454,455,456,457,458,459,460,461,462,463", "deleted_lines": "452", "method_info": {"method_name": "__init__", "method_params": "self,schedulers,durations,save_history", "method_startline": "428", "method_endline": "469"}}, "hunk_4": {"Ismethod": 1, "added_lines": "666,667,668,669,670,671,672,676,677,678,679,680,681,682,684,685,686,687,688,689,691,692,693,694,695", "deleted_lines": "655,656,657,658,659,660,661,662,663,664", "method_info": {"method_name": "simulate_values", "method_params": "cls,num_events,lr_scheduler,kwargs", "method_startline": "655", "method_endline": "695"}}, "hunk_5": {"Ismethod": 1, "added_lines": "594,595", "deleted_lines": "594", "method_info": {"method_name": "__init__", "method_params": "self,lr_scheduler,save_history,kwds", "method_startline": "594", "method_endline": "608"}}, "hunk_6": {"Ismethod": 1, "added_lines": "947,948,951,952,953,954,955", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,None,save_history", "method_startline": "929", "method_endline": "955"}}, "hunk_7": {"Ismethod": 1, "added_lines": "966,967,968", "deleted_lines": null, "method_info": {"method_name": "save_history", "method_params": "self,value", "method_startline": "966", "method_endline": "968"}}, "hunk_8": {"Ismethod": 1, "added_lines": "1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052", "deleted_lines": "1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036", "method_info": {"method_name": "simulate_values", "method_params": "cls,num_events,schedulers,kwargs", "method_startline": "1017", "method_endline": "1052"}}, "hunk_9": {"Ismethod": 1, "added_lines": "622", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,lr_scheduler,save_history,kwargs", "method_startline": "622", "method_endline": "636"}}, "hunk_10": {"Ismethod": 1, "added_lines": null, "deleted_lines": "513", "method_info": {"method_name": "__call__", "method_params": "self,engine,name", "method_startline": "509", "method_endline": "515"}}, "hunk_11": {"Ismethod": 1, "added_lines": "1029,1030,1031,1032,1033,1034,1035,1036", "deleted_lines": "1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036", "method_info": {"method_name": "_replicate_optimizer", "method_params": "optimizer", "method_startline": "1023", "method_endline": "1036"}}, "hunk_12": {"Ismethod": 1, "added_lines": "962,963", "deleted_lines": null, "method_info": {"method_name": "save_history", "method_params": "self", "method_startline": "962", "method_endline": "963"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 10, "file_old_name": "tests\\ignite\\contrib\\handlers\\test_param_scheduler.py", "file_new_name": "tests\\ignite\\contrib\\handlers\\test_param_scheduler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "597,598", "deleted_lines": null, "method_info": {"method_name": "test_lr_scheduler_asserts", "method_params": "", "method_startline": "592", "method_endline": "598"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1153,1154,1155,1156", "deleted_lines": null, "method_info": {"method_name": "test_param_group_scheduler", "method_params": "", "method_startline": "1129", "method_endline": "1167"}}, "hunk_2": {"Ismethod": 1, "added_lines": "310,311,312,313,314", "deleted_lines": null, "method_info": {"method_name": "test_concat_scheduler_asserts", "method_params": "", "method_startline": "279", "method_endline": "314"}}, "hunk_3": {"Ismethod": 1, "added_lines": "659,660,661", "deleted_lines": "625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641", "method_info": {"method_name": "test_lr_scheduler", "method_params": "", "method_startline": "601", "method_endline": "661"}}, "hunk_4": {"Ismethod": 1, "added_lines": "1113,1114,1115,1121,1122,1123,1124,1125,1126", "deleted_lines": null, "method_info": {"method_name": "test_param_group_scheduler_asserts", "method_params": "", "method_startline": "1081", "method_endline": "1126"}}, "hunk_5": {"Ismethod": 1, "added_lines": "1153,1154,1155", "deleted_lines": null, "method_info": {"method_name": "test_param_group_scheduler._test", "method_params": "lr_schedulers,optimizer", "method_startline": "1130", "method_endline": "1155"}}, "hunk_6": {"Ismethod": 1, "added_lines": "344,345", "deleted_lines": null, "method_info": {"method_name": "test_concat_scheduler_state_dict", "method_params": "", "method_startline": "317", "method_endline": "345"}}, "hunk_7": {"Ismethod": 1, "added_lines": "21,22", "deleted_lines": null, "method_info": {"method_name": "test_param_scheduler_asserts.get_param", "method_params": "self", "method_startline": "21", "method_endline": "22"}}, "hunk_8": {"Ismethod": 1, "added_lines": "19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37", "deleted_lines": null, "method_info": {"method_name": "test_param_scheduler_asserts", "method_params": "", "method_startline": "19", "method_endline": "37"}}, "hunk_9": {"Ismethod": 1, "added_lines": "683,684", "deleted_lines": null, "method_info": {"method_name": "test_piecewiselinear_asserts", "method_params": "", "method_startline": "666", "method_endline": "684"}}}}}}}