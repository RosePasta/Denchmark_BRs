{"BR": {"BR_id": "278", "BR_author": "jasonliu747", "BRopenT": "2020-07-29T03:18:26Z", "BRcloseT": "2020-08-22T12:27:01Z", "BR_text": {"BRsummary": "Different memory consumption among GPUs", "BRdescription": "\n Describe the bug\n When running a training, either on single or multiple nodes, the memory of first GPU card will always be consumed more than others. Looks like it only happened after upgrading BytePS version to v0.2.4.\n To Reproduce\n Steps to reproduce the behavior:\n <denchmark-code>apiVersion: \"kubeflow.org/v1beta1\"\n kind: \"MXJob\"\n metadata:\n   name: \"byteps-mxnet-job\"\n spec:\n   jobMode: MXTrain\n   mxReplicaSpecs:\n     Scheduler:\n       replicas: 1\n       restartPolicy: Never\n       template:\n         spec:\n           containers:\n             - name: mxnet\n               image: bytepsimage/mxnet:v0.2.4\n               command: [\"bpslaunch\"]\n     Server:\n       replicas: 1\n       restartPolicy: Never\n       template:\n         spec:\n           containers:\n             - name: mxnet\n               image: bytepsimage/mxnet:v0.2.4\n               command: [\"bpslaunch\"]\n               resources:\n                 requests:\n                   cpu: 4\n     Worker:\n       replicas: 2\n       restartPolicy: Never\n       template:\n         spec:\n           containers:\n             - name: mxnet\n               image: bytepsimage/mxnet:v0.2.4\n               command: [\"bpslaunch\"]\n               args: [\"python3\", \"/usr/local/byteps/example/mxnet/train_imagenet_byteps.py\", \"--benchmark\", \"1\", \"--batch-size=32\", \"--num-epochs=1\"]\n               volumeMounts:\n               - mountPath: /dev/shm\n                 name: dshm\n               resources:\n                 limits:\n                   nvidia.com/gpu: 8\n           volumes:\n           - name: dshm\n             emptyDir:\n               medium: Memory\n </denchmark-code>\n \n \n <denchmark-link:https://user-images.githubusercontent.com/24452340/88752517-b7f2f100-d18c-11ea-8729-4dd6102498b9.png></denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jasonliu747", "commentT": "2020-07-30T01:39:17Z", "comment_text": "\n \t\tI am not sure why PID 35529-35546 also uses memory on GPU0. Is your  identical with this <denchmark-link:https://github.com/bytedance/byteps/blob/master/example/mxnet/train_imagenet_byteps.py>example</denchmark-link>\n ?\n \n Looks like it only happened after upgrading BytePS version to v0.2.4.\n \n If it means v0.2.3 has no such issue, it looks weird. Because <denchmark-link:https://github.com/bytedance/byteps/commit/c16efff5aa5722738978c93da4ca86b2f8c00634>c16efff</denchmark-link>\n  and <denchmark-link:https://github.com/bytedance/byteps/commit/9e3b46d3c5508f0cc1b19540564c124f3a2fc0eb>9e3b46d</denchmark-link>\n  do not seem to be relevant to this problem..\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jasonliu747", "commentT": "2020-07-30T02:43:06Z", "comment_text": "\n \t\t\n I am not sure why PID 35529-35546 also uses memory on GPU0. Is your train_imagenet_byteps.py identical with this example?\n \n Yes, it's totally identical.\n \n If it means v0.2.3 has no such issue, it looks weird.\n \n I'm not sure which version caused this issue. The last version I used before v0.2.4 was v0.2.0.  And I just double checked, there is indeed no such issue in v0.2.0.\n <denchmark-link:https://user-images.githubusercontent.com/24452340/88874323-62cce300-d251-11ea-9430-6aa230818811.png></denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jasonliu747", "commentT": "2020-07-30T04:03:05Z", "comment_text": "\n \t\t\n I am not sure why PID 35529-35546 also uses memory on GPU0. Is your train_imagenet_byteps.py identical with this example?\n \n Looks like it only happened after upgrading BytePS version to v0.2.4.\n \n If it means v0.2.3 has no such issue, it looks weird. Because c16efff and 9e3b46d do not seem to be relevant to this problem..\n \n Are u sure? I have been experiencing this problem since I started using byteps.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jasonliu747", "commentT": "2020-07-30T10:19:13Z", "comment_text": "\n \t\tIt happens when training IMAGENET. Interestingly, <denchmark-link:https://github.com/vycezhong/byteps/blob/gradient_compression/example/mxnet/train_gluon_imagenet_byteps_gc.py>my script</denchmark-link>\n  using  also suffers this.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jasonliu747", "commentT": "2020-07-30T10:46:15Z", "comment_text": "\n \t\tYes, I can reproduce this with v0.2.4 on MXNet.. But PyTorch and TF benchmarks are fine. So I think the problem is in the MXNet example code (or maybe the plugins). We should investigate this.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jasonliu747", "commentT": "2020-08-19T08:02:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ymjiang>@ymjiang</denchmark-link>\n  any update on this issue?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jasonliu747", "commentT": "2020-08-22T07:07:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jasonliu747>@jasonliu747</denchmark-link>\n   It comes from . After setting  the problem is gone.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "jasonliu747", "commentT": "2020-08-22T12:27:57Z", "comment_text": "\n \t\tFixed by <denchmark-link:https://github.com/vycezhong>@vycezhong</denchmark-link>\n . Closing this.\n \t\t"}}}, "commit": {"commit_id": "c5a10545e8c2c8a75ff29be943befe57c81f8a1e", "commit_author": "Yuchen Zhong", "commitT": "2020-08-22 20:27:00+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "example\\mxnet\\common\\data_byteps.py", "file_new_name": "example\\mxnet\\common\\data_byteps.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "119,121,149,150,168,169", "deleted_lines": "119,121,149,167", "method_info": {"method_name": "get_rec_iter", "method_params": "args,rank", "method_startline": "111", "method_endline": "170"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "example\\mxnet\\common\\fit_byteps.py", "file_new_name": "example\\mxnet\\common\\fit_byteps.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "161", "deleted_lines": "161", "method_info": {"method_name": "fit", "method_params": "args,network,data_loader,kwargs", "method_startline": "142", "method_endline": "329"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "example\\mxnet\\train_gluon_imagenet_byteps_gc.py", "file_new_name": "example\\mxnet\\train_gluon_imagenet_byteps_gc.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "273,274,292,293", "deleted_lines": "273,291", "method_info": {"method_name": "main", "method_params": "", "method_startline": "134", "method_endline": "546"}}, "hunk_1": {"Ismethod": 1, "added_lines": "273,274,292,293", "deleted_lines": "273,291", "method_info": {"method_name": "main.get_data_rec", "method_params": "rec_train,rec_train_idx,rec_val,rec_val_idx,batch_size,num_workers", "method_startline": "228", "method_endline": "295"}}}}}}}