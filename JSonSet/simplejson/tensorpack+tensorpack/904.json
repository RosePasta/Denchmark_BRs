{"BR": {"BR_id": "904", "BR_author": "thuzhf", "BRopenT": "2018-09-21T10:04:49Z", "BRcloseT": "2018-09-21T20:14:52Z", "BR_text": {"BRsummary": "AttributeError: 'TrainLoop' object has no attribute 'starting_epoch'", "BRdescription": "\n The problem occurred when L318 below is executed if stats is not None (e.g. when you restore training from a log dir). The full error message is attached in the end.\n \n \n \n tensorpack/tensorpack/callbacks/monitor.py\n \n \n         Lines 309 to 318\n       in\n       36b05bb\n \n \n \n \n \n \n  def _setup_graph(self): \n \n \n \n  stats = JSONWriter.load_existing_json() \n \n \n \n  self._fname = os.path.join(logger.get_logger_dir(), JSONWriter.FILENAME) \n \n \n \n  if stats is not None: \n \n \n \n  try: \n \n \n \n  epoch = stats[-1]['epoch_num'] + 1 \n \n \n \n  except Exception: \n \n \n \n  epoch = None \n \n \n \n  \n \n \n \n  starting_epoch = self.trainer.loop.starting_epoch \n \n \n \n \n \n It can be seen self.starting_epoch is defined in L48 below.\n \n \n \n tensorpack/tensorpack/train/base.py\n \n \n         Lines 44 to 48\n       in\n       36b05bb\n \n \n \n \n \n \n  def config(self, steps_per_epoch, starting_epoch, max_epoch): \n \n \n \n  \"\"\" \n \n \n \n          Configure the loop given the settings. \n \n \n \n          \"\"\" \n \n \n \n  self.starting_epoch = int(starting_epoch) \n \n \n \n \n \n And config() is called in L241 below in the main_loop() function.\n \n \n \n tensorpack/tensorpack/train/base.py\n \n \n         Lines 233 to 241\n       in\n       36b05bb\n \n \n \n \n \n \n  def main_loop(self, steps_per_epoch, starting_epoch, max_epoch): \n \n \n \n  \"\"\" \n \n \n \n          Run the main training loop. \n \n \n \n   \n \n \n \n          Args: \n \n \n \n              steps_per_epoch, starting_epoch, max_epoch (int): \n \n \n \n          \"\"\" \n \n \n \n  with self.sess.as_default(): \n \n \n \n  self.loop.config(steps_per_epoch, starting_epoch, max_epoch) \n \n \n \n \n \n And setup_callbacks() (L289 below) is executed before main_loop() (L291 below).\n \n \n \n tensorpack/tensorpack/train/base.py\n \n \n         Lines 274 to 291\n       in\n       36b05bb\n \n \n \n \n \n \n  def train(self, \n \n \n \n  callbacks, monitors, \n \n \n \n  session_creator, session_init, \n \n \n \n  steps_per_epoch, starting_epoch=1, max_epoch=9999999): \n \n \n \n  \"\"\" \n \n \n \n          Implemented by three lines: \n \n \n \n   \n \n \n \n          .. code-block:: python \n \n \n \n   \n \n \n \n              self.setup_callbacks(callbacks, monitors) \n \n \n \n              self.initialize(session_creator, session_init) \n \n \n \n              self.main_loop(steps_per_epoch, starting_epoch, max_epoch) \n \n \n \n   \n \n \n \n          You can call those methods by yourself to have better control on details if needed. \n \n \n \n          \"\"\" \n \n \n \n  self.setup_callbacks(callbacks, monitors) \n \n \n \n  self.initialize(session_creator, session_init) \n \n \n \n  self.main_loop(steps_per_epoch, starting_epoch, max_epoch) \n \n \n \n \n \n And L318 in the first file (monitor.py) will be finally executed in the calling of setup_callbacks() above. Actually you already have load_existing_epoch_number  in L298 below.\n \n \n \n tensorpack/tensorpack/callbacks/monitor.py\n \n \n          Line 298\n       in\n       36b05bb\n \n \n \n \n \n \n  def load_existing_epoch_number(): \n \n \n \n \n \n And the above function is only called in the following class:\n \n \n \n tensorpack/tensorpack/train/config.py\n \n \n         Lines 177 to 240\n       in\n       36b05bb\n \n \n \n \n \n \n  class AutoResumeTrainConfig(TrainConfig): \n \n \n \n  \"\"\" \n \n \n \n      Same as :class:`TrainConfig`, but does the following to automatically \n \n \n \n      resume from training: \n \n \n \n   \n \n \n \n      1. If a checkpoint was found in :meth:`logger.get_logger_dir()`, set \n \n \n \n         `session_init` option to load it. \n \n \n \n      2. If a JSON history was found in :meth:`logger.get_logger_dir()`, try to \n \n \n \n         load the epoch number from it and set the `starting_epoch` option to \n \n \n \n         continue training. \n \n \n \n   \n \n \n \n      You can choose to let the above two option to either overwrite or \n \n \n \n      not overwrite user-provided arguments, as explained below. \n \n \n \n      \"\"\" \n \n \n \n  def __init__(self, always_resume=True, **kwargs): \n \n \n \n  \"\"\" \n \n \n \n          Args: \n \n \n \n              always_resume (bool): If False, user-provided arguments \n \n \n \n                  `session_init` and `starting_epoch` will take priority. \n \n \n \n                  Otherwise, resume will take priority. \n \n \n \n              kwargs: same as in :class:`TrainConfig`. \n \n \n \n   \n \n \n \n          Note: \n \n \n \n              The main goal of this class is to let a training job to resume \n \n \n \n              without changing any line of code or command line arguments. \n \n \n \n              So it's useful to let resume take priority over user-provided arguments sometimes: \n \n \n \n   \n \n \n \n              If your training starts from a pre-trained model, \n \n \n \n              you would want it to use user-provided model loader at the \n \n \n \n              beginning, but a \"resume\" model loader when the job was \n \n \n \n              interrupted and restarted. \n \n \n \n          \"\"\" \n \n \n \n  if always_resume or 'session_init' not in kwargs: \n \n \n \n  sessinit = self._get_sessinit_resume() \n \n \n \n  if sessinit is not None: \n \n \n \n  path = sessinit.path \n \n \n \n  if 'session_init' in kwargs: \n \n \n \n  logger.info(\"Found checkpoint at {}. \" \n \n \n \n  \"session_init arguments will be overwritten.\".format(path)) \n \n \n \n  else: \n \n \n \n  logger.info(\"Will load checkpoint at {}.\".format(path)) \n \n \n \n  kwargs['session_init'] = sessinit \n \n \n \n  \n \n \n \n  if always_resume or 'starting_epoch' not in kwargs: \n \n \n \n  last_epoch = self._get_last_epoch() \n \n \n \n  if last_epoch is not None: \n \n \n \n  now_epoch = last_epoch + 1 \n \n \n \n  logger.info(\"Found history statistics from JSON. \" \n \n \n \n  \"Overwrite the starting epoch to epoch #{}.\".format(now_epoch)) \n \n \n \n  kwargs['starting_epoch'] = now_epoch \n \n \n \n  \n \n \n \n  super(AutoResumeTrainConfig, self).__init__(**kwargs) \n \n \n \n  \n \n \n \n  def _get_sessinit_resume(self): \n \n \n \n  logdir = logger.get_logger_dir() \n \n \n \n  if not logdir: \n \n \n \n  return None \n \n \n \n  path = os.path.join(logdir, 'checkpoint') \n \n \n \n  if not tf.gfile.Exists(path): \n \n \n \n  return None \n \n \n \n  return SaverRestore(path) \n \n \n \n  \n \n \n \n  def _get_last_epoch(self): \n \n \n \n  return JSONWriter.load_existing_epoch_number() \n \n \n \n \n \n Even if I changed TrainConfig() to AutoResumeTrainConfig(), the same problem still occurred due to the same reason. Although self.starting_epoch is also defined below, it is the config's attribute instead of the train.loop's:\n \n \n \n tensorpack/tensorpack/train/config.py\n \n \n          Line 146\n       in\n       7b8728f\n \n \n \n \n \n \n  self.starting_epoch = int(starting_epoch) \n \n \n \n \n \n I think some easy workarounds will be:\n \n Defining starting_epoch earlier as the train.loop's attribute (that way AutoResumeTrainConfig() will probably work).\n Changing L318 in the first file (monitor.py) to use function load_existing_epoch_number() (that way maybe will make AutoResumeTrainConfig() useless, but this is handy because users can always use TrainConfig()).\n \n However, to be consistent with the logic of your Trainer.train() function (including 3 functions), it seems it needs more consideration on how to fix this.\n The following is the full error message:\n <denchmark-code>Traceback (most recent call last):\n   File \"main.py\", line 307, in <module>\n     main()\n   File \"main.py\", line 303, in main\n     launch_train_with_config(config, trainer)\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/interface.py\", line 95, in launch_train_with_config\n     extra_callbacks=config.extra_callbacks)\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py\", line 319, in train_with_defaults\n     steps_per_epoch, starting_epoch, max_epoch)\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py\", line 289, in train\n     self.setup_callbacks(callbacks, monitors)\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/utils/argtools.py\", line 181, in wrapper\n     return func(*args, **kwargs)\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/train/base.py\", line 189, in setup_callbacks\n     self._callbacks.setup_graph(weakref.proxy(self))\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/base.py\", line 52, in setup_graph\n     self._setup_graph()\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/group.py\", line 70, in _setup_graph\n     cb.setup_graph(self.trainer)\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/monitor.py\", line 55, in setup_graph\n     self._setup_graph()\n   File \"/data/fangzhang/anaconda3/lib/python3.6/site-packages/tensorpack/callbacks/monitor.py\", line 318, in _setup_graph\n     starting_epoch = self.trainer.loop.starting_epoch\n AttributeError: 'TrainLoop' object has no attribute 'starting_epoch'\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "thuzhf", "commentT": "2018-09-21T15:07:13Z", "comment_text": "\n \t\tThis is an issue I fixed before but forgot to take a note about it. Unfortunately it was introduced again lately..\n I will think about how to properly address this.\n \t\t"}}}, "commit": {"commit_id": "9ce85aae9f63919087cfdae76238d094ab5c35f5", "commit_author": "Yuxin Wu", "commitT": "2018-09-21 13:14:26-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorpack\\callbacks\\inference.py", "file_new_name": "tensorpack\\callbacks\\inference.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "81,82,83", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorpack\\callbacks\\monitor.py", "file_new_name": "tensorpack\\callbacks\\monitor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "311,312,313", "deleted_lines": null, "method_info": {"method_name": "_setup_graph", "method_params": "self", "method_startline": "310", "method_endline": "313"}}, "hunk_1": {"Ismethod": 1, "added_lines": "315,324,325,332,335,340,344", "deleted_lines": "324,327,328,333,336,337,338,339,340,341,343,344", "method_info": {"method_name": "_before_train", "method_params": "self", "method_startline": "315", "method_endline": "345"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "tensorpack\\dataflow\\dataset\\cifar.py", "file_new_name": "tensorpack\\dataflow\\dataset\\cifar.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "155", "deleted_lines": "152,156", "method_info": {"method_name": "__init__", "method_params": "self,train_or_test,shuffle,dir", "method_startline": "152", "method_endline": "158"}}, "hunk_1": {"Ismethod": 1, "added_lines": "88,107", "deleted_lines": "88", "method_info": {"method_name": "__init__", "method_params": "self,train_or_test,shuffle,dir,cifar_classnum", "method_startline": "88", "method_endline": "107"}}, "hunk_2": {"Ismethod": 1, "added_lines": "88,107,108,109", "deleted_lines": "88", "method_info": {"method_name": "__init__", "method_params": "self,train_or_test,shuffle,dir,cifar_classnum", "method_startline": "88", "method_endline": "110"}}, "hunk_3": {"Ismethod": 1, "added_lines": "155,159", "deleted_lines": "156", "method_info": {"method_name": "__init__", "method_params": "self,train_or_test,shuffle,dir", "method_startline": "155", "method_endline": "161"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorpack\\train\\config.py", "file_new_name": "tensorpack\\train\\config.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "225", "deleted_lines": "225", "method_info": {"method_name": "__init__", "method_params": "self,always_resume,kwargs", "method_startline": "191", "method_endline": "228"}}}}}}}