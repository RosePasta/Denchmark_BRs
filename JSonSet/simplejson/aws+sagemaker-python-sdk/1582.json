{"BR": {"BR_id": "1582", "BR_author": "ChaiBapchya", "BRopenT": "2020-06-12T07:42:04Z", "BRcloseT": "2020-06-17T17:59:38Z", "BR_text": {"BRsummary": "local_gpu with distributed training for single instance multi-gpu distributed training", "BRdescription": "\n Describe the bug\n Upon testing for local-session [sagemaker], single instance, multi-gpu distributed training\n It fails at\n <denchmark-code>Input\n training_instance_type = 'local_gpu', distributions = {'mpi': {'enabled': True, 'processes_per_host': 4}}\n </denchmark-code>\n \n Stack Trace\n <denchmark-code>    def warn_if_parameter_server_with_multi_gpu(training_instance_type, distributions):\n         \"\"\"Warn the user that training will not fully leverage all the GPU\n         cores if parameter server is enabled and a multi-GPU instance is selected.\n         Distributed training with the default parameter server setup doesn't\n         support multi-GPU instances.\n \n         Args:\n             training_instance_type (str): A string representing the type of training instance selected.\n             distributions (dict): A dictionary with information to enable distributed training.\n                 (Defaults to None if distributed training is not enabled.) For example:\n \n                 .. code:: python\n \n                     {\n                         'parameter_server':\n                         {\n                             'enabled': True\n                         }\n                     }\n \n \n         \"\"\"\n         if training_instance_type == \"local\" or distributions is None:\n             return\n \n         is_multi_gpu_instance = (\n >           training_instance_type.split(\".\")[1].startswith(\"p\")\n             and training_instance_type not in SINGLE_GPU_INSTANCE_TYPES\n         )\n E       IndexError: list index out of range\n \n .tox/py37/lib/python3.7/site-packages/sagemaker/fw_utils.py:620: IndexError\n </denchmark-code>\n \n To reproduce\n <denchmark-code>tox -e py37 -- tests/integ/test_horovod_mx.py\n </denchmark-code>\n \n \n Build custom docker image :\n https://github.com/ChaiBapchya/sagemaker-mxnet-training-toolkit/tree/mx_hvd_mpi\n custom Sagemaker python SDK build from source : https://github.com/ChaiBapchya/sagemaker-python-sdk/tree/mx_estimator_horovod_mpi\n \n Expected behavior\n For running Distributed training on single instance multi-gpu for mpi-based horovod, I encounter this error.\n Since I'm using horovod [mpi] this warning isn't relevant.\n I suggest we should also add local_gpu here\n <denchmark-code>if training_instance_type in [\"local\",\"local_gpu\"] or distributions is None:\n             return\n </denchmark-code>\n \n System information\n A description of your system. Please provide:\n \n SageMaker Python SDK version: Build from source [1.62.1.dev0]\n Framework name (eg. PyTorch) or algorithm (eg. KMeans):MXNet\n Framework version:1.6.0\n Python version:3\n CPU or GPU:GPU\n Custom Docker image (Y/N):Y\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ChaiBapchya", "commentT": "2020-06-17T17:59:37Z", "comment_text": "\n \t\twarning message update has been released in <denchmark-link:https://github.com/aws/sagemaker-python-sdk/releases/tag/v1.65.0>https://github.com/aws/sagemaker-python-sdk/releases/tag/v1.65.0</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "af7f75ae336f0481e52bb968e4cc6df91b1bac2c", "commit_author": "Chuyang", "commitT": "2020-06-16 19:41:06-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\sagemaker\\fw_utils.py", "file_new_name": "src\\sagemaker\\fw_utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "628,629,630", "deleted_lines": "628,629,630", "method_info": {"method_name": "warn_if_parameter_server_with_multi_gpu", "method_params": "training_instance_type,distributions", "method_startline": "602", "method_endline": "637"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\unit\\test_fw_utils.py", "file_new_name": "tests\\unit\\test_fw_utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1277,1278,1279,1280,1281,1282,1283,1284", "deleted_lines": null, "method_info": {"method_name": "test_warn_if_parameter_server_with_local_multi_gpu", "method_params": "caplog", "method_startline": "1277", "method_endline": "1284"}}}}}}}