{"BR": {"BR_id": "326", "BR_author": "lehahoang", "BRopenT": "2019-07-19T13:55:52Z", "BRcloseT": "2019-10-25T08:32:32Z", "BR_text": {"BRsummary": "Issue of re-evaluation of the quantized checkpoint", "BRdescription": "\n Hi,\n After successfully quantizing the model, I want to re-evaluate the model by using this command line:\n $ python3 compress_classifier.py --arch resnet20_cifar ../data.cifar --qebw 8 --qeba 8 --resume-from quantized_checkpoint.pth.tar --evaluate\n However, an error is raised unexpectedly. The log looks like:\n <denchmark-code>Log file for this run: /homes/lhoang/distiller/examples/classifier_compression/logs/2019.07.19-154314/2019.07.19-154314.log\n => creating a resnet20_cifar model with the cifar10 dataset\n \n --------------------------------------------------------\n Logging to TensorBoard - remember to execute the server:\n > tensorboard --logdir='./logs'\n \n => loading checkpoint quantized_checkpoint.pth.tar\n => Checkpoint contents:\n +--------------------+-------------+----------------+\n | Key                | Type        | Value          |\n |--------------------+-------------+----------------|\n | arch               | str         | resnet20_cifar |\n | compression_sched  | dict        |                |\n | epoch              | int         | 0              |\n | extras             | dict        |                |\n | quantizer_metadata | dict        |                |\n | state_dict         | OrderedDict |                |\n +--------------------+-------------+----------------+\n \n => Checkpoint['extras'] contents:\n +----------------+--------+---------+\n | Key            | Type   |   Value |\n |----------------+--------+---------|\n | quantized_top1 | float  |   86.36 |\n +----------------+--------+---------+\n \n Loaded compression schedule from checkpoint (epoch 0)\n Loaded quantizer metadata from the checkpoint\n \n WARNING:\n No stats file passed - Dynamic quantization will be used\n At the moment, this mode isn't as fully featured as stats-based quantization, and the accuracy results obtained are likely not as representative of real-world results.\n Specifically:\n   * Not all modules types are supported in this mode. Unsupported modules will remain in FP32.\n   * Optimizations for quantization of layers followed by Relu/Tanh/Sigmoid are only supported when statistics are used.\n END WARNING\n \n Weight bits:  8\n THIS LINE IS TRULY EXECUTED - CHECKPOINT2\n Per-layer quantization parameters saved to logs/2019.07.19-154314/layer_quant_params.yaml\n \n Log file for this run: /homes/lhoang/distiller/examples/classifier_compression/logs/2019.07.19-154314/2019.07.19-154314.log\n Traceback (most recent call last):\n   File \"compress_classifier.py\", line 791, in <module>\n     main()\n   File \"compress_classifier.py\", line 176, in main\n     model, args.resumed_checkpoint_path, model_device=args.device)\n   File \"/homes/lhoang/distiller/distiller/apputils/checkpoint.py\", line 164, in load_checkpoint\n     quantizer.prepare_model()\n   File \"/homes/lhoang/distiller/distiller/quantization/range_linear.py\", line 951, in prepare_model\n     raise ValueError('PostTrainLinearQuantizer requires dummy input in order to perform certain optimizations')\n ValueError: PostTrainLinearQuantizer requires dummy input in order to perform certain optimizations\n </denchmark-code>\n \n It looks like we need to pass \"dummy_input\" so that it can be executed.\n Does it make sense to give the stats file for static quantization during the re-evaluation phase of a quantized model?\n By the way, it did not happen when I used the previous source code.\n Thank you.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lehahoang", "commentT": "2019-07-20T21:53:01Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/lehahoang>@lehahoang</denchmark-link>\n  ,\n That is indeed a bug that appeared from our last update to PostTrainLinearQuantizer, I will work on this soon enough to fix it.\n In the meantime - I would suggest saving the whole model in the checkpoint, instead only its' state_dict like we do in compress_classifier.py, and that would save you the trouble of loading and quantizing.\n Cheers,\n Lev\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lehahoang", "commentT": "2019-07-22T09:43:52Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/levzlotnik>@levzlotnik</denchmark-link>\n \n The above issue does not exist if I use the previous source code.\n However, there is another issue of re-evaluating the quantized checkpoint if I use the previous source code.\n For Resnet20_cifar model, the classification accuracy of re-evaluating is the same to the case which we do quantization before evaluation.\n For VGG-f (f=11, 16...), there is a significant drop of classification accuracy of re-evaluating.\n The next part describes what I have done:\n Step 1: Training a model\n $ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --epochs 10\n Step 2: Collecting stats for quantization\n $ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --resume-from logs/2019.07.22-111804/checkpoint.pth.tar --evaluate --qe-calibration 0.1\n Step 3: Do quantization\n $ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --resume-from logs/2019.07.22-111804/checkpoint.pth.tar --evaluate --quantize-eval --qe-stats-file logs/2019.07.22-112206/acts_quantization_stats.yaml --qe-per-channel\n ==> Top1: 78.590    Top5: 98.570    Loss: 0.615\n Step 4: Re-evaluating the checkpoint\n  $ CUDA_VISIBLE_DEVICE=0,1 python3 compress_classifier.py --arch vgg11_cifar ../data.cifar --resume-from logs/2019.07.22-112303/quantized_checkpoint.pth.tar --evaluate\n ==> Top1: 54.680    Top5: 87.620    Loss: 1.376\n Again, the difference in classification accuracy does not appear if we are working with Resnet20_cifar model\n As I do not really know how it is going under the hood of Distiller, I am a bit confused of the results.\n I guess this is a bug of loading quantized checkpoints. Please fix it.\n Thanks\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lehahoang", "commentT": "2019-07-22T11:14:56Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/lehahoang>@lehahoang</denchmark-link>\n \n The flows under the hood are as follows:\n \n When quantizing \"from scratch\":\n \n Create FP32 \"vanilla\" module\n Quantize the module according to quantization command line arguments\n Evaluate\n Save checkpoint with quantized parameters and quantization metadata\n \n \n When loading a checkpoint:\n \n Create FP32 \"vanilla\" module\n Load checkpoint file\n If checkpoint contains quantization metadata:\n \n Quantize module according to the quantization metadata\n \n \n Load model parameters (\"state_dict\") from checkpoint\n Evaluate\n \n \n \n (Note that the 2 flows aren't mutually exclusive, i.e. you could load the checkpoint and then quantize again as if it's \"from scratch\". Which could lead to unexpected results. But the way you ran it is fine).\n As you can see, resuming from \"quantized_checkpoint.pth.tar\" doesn't really \"save\" you anything, since we always start with a FP32 model and then quantize it. Admittedly, when I originally added the dumping of the quantized checkpoint, it was more to allow the user to inspect the quantized parameters of the model. Resuming from it was not a use case I was thinking about, since you could just re-run the same quantization \"from scratch\" and get the exact same results.\n Since none of this is really documented, I can definitely see why you'd expect resuming to work.\n To summarize:\n \n Regarding the need to pass \"dummy_input\" - this is indeed a change I made recently, and failed to take care of in the resume flow. We'll fix that, as @levzlotnik said\n We'll look into the issue you have with VGG. Even with my \"disclaimer\" above, I don't see why this would happen. We need to make sure there isn't a more serious issue hiding here\n To avoid confusion, we'll make sure loading from a post-train quantization checkpoint just works\n \n For now, if you want to re-create post-train quantization results, just re-run the same command. You can see the command line used for each run in the log file. Look for a line that starts with Command line: near the top of the log file.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lehahoang", "commentT": "2019-07-22T19:42:52Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/guyjacob>@guyjacob</denchmark-link>\n \n Thanks for your instant reply.\n As I break down the source code of  \"evaluate_model\" defined in  compress_classifier.py, I  assume that the models passed to the \"test\" and dumped to checkpoints in the next code section should be the same. That's why I expect the resuming from quantized checkpoints should work when I load it.\n <denchmark-code>     if args.quantize_eval:\n         model.cpu()\n         quantizer = quantization.PostTrainLinearQuantizer.from_args(model, args)\n         quantizer.prepare_model(distiller.get_dummy_input(input_shape=model.input_shape))\n         model.to(args.device)\n \n     top1, _, _ = test(test_loader, model, criterion, loggers, activations_collectors, args=args)\n \n     if args.quantize_eval:\n         checkpoint_name = 'quantized'\n         apputils.save_checkpoint(0, args.arch, model, optimizer=None, scheduler=scheduler,\n                                  name='_'.join([args.name, checkpoint_name]) if args.name else checkpoint_name,\n                                  dir=msglogger.logdir, extras={'quantized_top1': top1})\n </denchmark-code>\n \n As far as I undersatnd your flow, you save all the parameters like Pytorch does including \"the wrapper\" module. Loading the saved quantized checkpoint should reproduce the classification accuracy. Please correct me if I am wrong.\n I need post-training quantization models for other projects, not only for emulating the quantization as you and your colleagues planed from the start.\n Thank you for your strong support.\n Best regards,\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lehahoang", "commentT": "2019-10-25T08:32:32Z", "comment_text": "\n \t\tClosing due to staleness. Please re-open if you need further information or assistance.\n \t\t"}}}, "commit": {"commit_id": "6782ccaeae0c02b542c4050d55abc585e34dd1bd", "commit_author": "Lev Zlotnik", "commitT": "2019-07-23 18:19:47+03:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "distiller\\apputils\\checkpoint.py", "file_new_name": "distiller\\apputils\\checkpoint.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "164", "deleted_lines": "164"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "distiller\\quantization\\quantizer.py", "file_new_name": "distiller\\quantization\\quantizer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "216", "deleted_lines": null, "method_info": {"method_name": "prepare_model", "method_params": "self,dummy_input", "method_startline": "193", "method_endline": "249"}}}}}}}