{"BR": {"BR_id": "115", "BR_author": "dabadee", "BRopenT": "2019-01-01T19:03:40Z", "BRcloseT": "2019-01-21T15:09:21Z", "BR_text": {"BRsummary": "NNZ (Sparse) turn to 0 on Quantizing", "BRdescription": "\n I am doing the following\n \n \n Train simplenet on cifar\n python3 compress_classifier.py --arch simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.01 --epochs 100\n \n \n Prune using AGP\n python3 compress_classifier.py -a simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.01 --compress=agp_simplenet75.yaml --resume logs/2019.01.01-134735/checkpoint.pth.tar --epochs 25\n \n \n Quantize using QuantAwareTrainRangeLinearQuantizer\n python3 compress_classifier.py -a simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.001 --compress=quant.yaml --resume logs/2019.01.01-183251/checkpoint.pth.tar --epochs 20\n \n \n On the very first step both conv layers' weights and their associated float_weight, scale, and zero point layers become 100% sparse with 0 NNZ sparse values.\n Is it a bug or am I going in wrong direction ?\n My yaml files are as follows\n agp_simplenet75.yaml\n <denchmark-code>version: 1\n pruners:\n   fc1_pruner:\n     class: 'AutomatedGradualPruner'\n     initial_sparsity : 0.05\n     final_sparsity: 0.75\n     weights: ['module.fc1.weight']\n \n   fc2_pruner:\n     class: 'AutomatedGradualPruner'\n     initial_sparsity : 0.05\n     final_sparsity: 0.75\n     weights: ['module.fc2.weight']\n  \n   fc3_pruner:\n     class: 'AutomatedGradualPruner'\n     initial_sparsity : 0.05\n     final_sparsity: 0.75\n     weights: ['module.fc3.weight']\n \n   conv1_pruner:\n     class: 'AutomatedGradualPruner'\n     initial_sparsity : 0.03\n     final_sparsity: 0.75\n     weights: ['module.conv1.weight']\n \n   conv2_pruner:\n     class: 'AutomatedGradualPruner'\n     initial_sparsity : 0.03\n     final_sparsity: 0.75\n     weights: ['module.conv2.weight']\n \n \n lr_schedulers:\n   # Learning rate decay scheduler\n    pruning_lr:\n      class: ExponentialLR\n      gamma: 0.9\n \n \n policies:\n   - pruner:\n       instance_name : 'conv1_pruner'\n     starting_epoch: 100\n     ending_epoch: 110\n     frequency: 2\n \n   - pruner:\n       instance_name : 'conv2_pruner'\n     starting_epoch: 100\n     ending_epoch: 110\n     frequency: 2\n \n   - pruner:\n       instance_name : 'fc1_pruner'\n     starting_epoch: 100\n     ending_epoch: 120\n     frequency: 2\n \n   - pruner:\n       instance_name : 'fc2_pruner'\n     starting_epoch: 100\n     ending_epoch: 120\n     frequency: 2\n \n   - pruner:\n       instance_name: 'fc3_pruner'\n     starting_epoch: 100\n     ending_epoch: 120\n     frequency: 2\n \n   - lr_scheduler:\n       instance_name: pruning_lr\n     starting_epoch: 101\n     ending_epoch: 180\n     frequency: 1\n </denchmark-code>\n \n quant.yaml\n <denchmark-code>quantizers:\n   linear_quantizer:\n     class: QuantAwareTrainRangeLinearQuantizer\n     bits_activations: 8\n     bits_weights: 8\n     mode: 'ASYMMETRIC_UNSIGNED'  # Can try \"SYMMETRIC\" as well\n     ema_decay: 0.999   # Decay value for exponential moving average tracking of activation ranges\n     per_channel_wts: True\n \n policies:\n     - quantizer:\n         instance_name: linear_quantizer\n       # For now putting a large range here, which should cover both training from scratch or resuming from some\n       # pre-trained checkpoint at some unknown epoch\n       starting_epoch: 0\n       ending_epoch: 300\n       frequency: 1\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dabadee", "commentT": "2019-01-09T15:44:46Z", "comment_text": "\n \t\tcan any one help with this ? or just give me some pointers to look at\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dabadee", "commentT": "2019-01-10T15:11:26Z", "comment_text": "\n \t\tIt is a bug indeed. After pruning, we have entire channels with value 0, and this isn't handled properly. The calculated scale factor for these channels is inf, and in turn the quantized values are nan, and the end result is what you see.\n I'll be pushing several quantization related changes in the coming days, so will fix this along with those.\n Apologies for taking so long to respond.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dabadee", "commentT": "2019-01-21T15:09:10Z", "comment_text": "\n \t\tShould be fixed by <denchmark-link:https://github.com/NervanaSystems/distiller/commit/10ce938cf010f2cbc4865f13efdd10d7bd5f63f9>this commit</denchmark-link>\n .\n Also, note that the  model you were experimenting with called ReLU via the functional PyTorch API. However, for the  to be able to detect layers and replace them with a quantized implementation, they must be instantiated as modules. The way it was, only the weights in  were being quantized, and the activations were left untouched. I also pushed a <denchmark-link:https://github.com/NervanaSystems/distiller/commit/24381169f2bb70c953cfc765ae342d98ef8a3153>commit</denchmark-link>\n  modifying the model accordingly.\n Closing this, please re-open if there are anymore issues.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dabadee", "commentT": "2019-01-21T15:16:15Z", "comment_text": "\n \t\tgreat, I will check it out today\n \t\t"}}}, "commit": {"commit_id": "10ce938cf010f2cbc4865f13efdd10d7bd5f63f9", "commit_author": "Guy Jacob", "commitT": "2019-01-21 16:48:06+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "distiller\\quantization\\q_utils.py", "file_new_name": "distiller\\quantization\\q_utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "31,32,33,34,35,38,39,40,41,42,43,44,45,46,47", "deleted_lines": "34,35,37,38,39,40", "method_info": {"method_name": "symmetric_linear_quantization_params", "method_params": "num_bits,saturation_val", "method_startline": "30", "method_endline": "48"}}, "hunk_1": {"Ismethod": 1, "added_lines": "20,21,22,23,24,25,26,27", "deleted_lines": "23,24,25,26,27", "method_info": {"method_name": "_prep_saturation_val_tensor", "method_params": "sat_val", "method_startline": "20", "method_endline": "27"}}}}, "file_1": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\test_quant_utils.py"}}}}