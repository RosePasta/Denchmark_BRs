{"BR": {"BR_id": "3344", "BR_author": "karlhigley", "BRopenT": "2020-04-12T15:44:42Z", "BRcloseT": "2020-04-15T14:43:05Z", "BR_text": {"BRsummary": "`test_instantiate_tfe_layer` is flaky", "BRdescription": "\n Describe the bug\n This test semi-regularly fails\n To Reproduce\n Run the test (or full suite) until it fails.\n Expected behavior\n The test should reliably pass.\n Screenshots\n <denchmark-code>        # compares results and raises error if not equal up to 0.001\n >       np.testing.assert_allclose(actual, expected, rtol=0.001)\n E       AssertionError: \n E       Not equal to tolerance rtol=0.001, atol=0\n E       \n E       Mismatched elements: 4 / 20 (20%)\n E       Max absolute difference: 0.00031287\n E       Max relative difference: 0.00110612\n E        x: array([[-0.163999, -1.074379, -1.394909, -0.963268,  0.888889],\n E              [-0.163999, -1.074379, -1.394909, -0.963268,  0.888889],\n E              [-0.163999, -1.074379, -1.394909, -0.963268,  0.888889],\n E              [-0.163999, -1.074379, -1.394909, -0.963268,  0.888889]])\n E        y: array([[-0.164181, -1.074468, -1.395135, -0.9634  ,  0.889202],\n E              [-0.164181, -1.074468, -1.395135, -0.9634  ,  0.889202],\n E              [-0.164181, -1.074468, -1.395135, -0.9634  ,  0.889202],...\n </denchmark-code>\n \n Additional context\n This started being an issue when the order of the test suite was randomized (specifically to shake out flaky tests like this.)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "karlhigley", "commentT": "2020-04-12T17:51:38Z", "comment_text": "\n \t\tThere are two potential issues here:\n \n Randomized functions in tests should be seeded, since integration tests should be deterministic. By randomizing the order of tests, the seed that these tolerance values are tuned against might be getting reset.\n The rtol value here is working against the test definition, which is what makes the error dependent on seed.\n \n To explain 2 a bit more:The loss of precision from TFE's fixed point representation truncating floating points is relatively worse for values closer to 0, and sampling kernel_initializer from a standard normal means we have a high likelihood of sampling numbers near 0. Relative error between the expected and actual value of the kernel will thus increase for numbers near 0 (whereas absolute error should be better).\n If you want to randomize testing order, I'd double check that you're setting seeds after sampling the testcase order (so that each test can still be deterministic). I'd additionally either change the kernel_initializer to a distribution that doesn't have an expected value at 0 (normal -> uniform), or switch from rtol to atol (this is less satisfying but should at least make the test less susceptible to precision errors).\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "karlhigley", "commentT": "2020-04-12T18:08:12Z", "comment_text": "\n \t\tThe issue that prompted randomizing the test case order was that changes to the directory structure (without code changes) could cause test failures, since the tests are run in alphabetical order and randomized functions in the tests are inconsistently (re-)seeded.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "karlhigley", "commentT": "2020-04-12T18:13:41Z", "comment_text": "\n \t\t(For anyone picking up this issue, the test randomization is performed with <denchmark-link:https://github.com/pytest-dev/pytest-randomly>pytest-randomly</denchmark-link>\n .)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "karlhigley", "commentT": "2020-04-13T13:44:17Z", "comment_text": "\n \t\t\n randomized functions in the tests are inconsistently (re-)seeded\n \n Seems like a great use case for <denchmark-link:https://docs.pytest.org/en/latest/fixture.html>pytest fixtures</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "karlhigley", "commentT": "2020-04-13T14:34:12Z", "comment_text": "\n \t\tNot sure I follow. Are you suggesting using fixtures to set random seeds?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "karlhigley", "commentT": "2020-04-13T16:57:55Z", "comment_text": "\n \t\tYes, assuming there's a reason that no global seed is used.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "karlhigley", "commentT": "2020-04-14T14:47:49Z", "comment_text": "\n \t\tThere is a global seed used, which is set to the timestamp of the start of the test run. (There are actually multiple random seeds at play though (the Python random seed, the PyTorch random seed, and the Numpy random seed), and I'm not sure they all get set. \ud83e\udd14\n Setting fixed random seeds at the beginning of a test run doesn't solve the issue though, because the order of the tests is not fixed, so the numbers generated for each individual test may vary. If individual tests can't be written in a fashion that makes them (pretty much) deterministic despite the randomness within, they can set whatever random seeds they need to.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "karlhigley", "commentT": "2020-04-14T18:43:46Z", "comment_text": "\n \t\t\n Setting fixed random seeds at the beginning of a test run doesn't solve the issue though, because the order of the tests is not fixed, so the numbers generated for each individual test may vary.\n \n I see, I misunderstood. Dynamically setting the seed based on timestamp is not what I'd consider \"fixed seed\" because it doesn't allow for exact reproducibility of a test run across time & machines (which is what all unit/integration tests should strive for imo). But at least the problem is clear now, we need same numbers across runs for this issue to be fixed\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "karlhigley", "commentT": "2020-04-15T14:42:01Z", "comment_text": "\n \t\tIt does allow for reproducibility, because each test run prints out the random seed at the beginning and you can pass it back in with a flag.\n IMO, we need tests that are resilient to receiving different numbers, because the tests are already not guaranteed to run in the same order on every run (due to package path changing refactors.) I'd much rather intentionally deal with the problem of tests that flake (by randomizing their order and fixing issues) than have unrelated tests fail unexpectedly while refactoring due to order dependence of the test suite.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "karlhigley", "commentT": "2020-04-15T14:43:05Z", "comment_text": "\n \t\tAnyway, <denchmark-link:https://github.com/OpenMined/PySyft/pull/3358>#3358</denchmark-link>\n  adjusts the thresholds to make this test pass reliably, so this game of whack-a-mole has shifted to other parts of the code base.\n \t\t"}}}, "commit": {"commit_id": "f8dbe92bedd2b8faceadfbc9af4e8fa859a5556c", "commit_author": "Karl Higley", "commitT": "2020-04-14 14:09:20-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\keras\\test_sequential.py", "file_new_name": "test\\keras\\test_sequential.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "54", "deleted_lines": "54", "method_info": {"method_name": "test_instantiate_tfe_layer", "method_params": "", "method_startline": "13", "method_endline": "54"}}}}}}}