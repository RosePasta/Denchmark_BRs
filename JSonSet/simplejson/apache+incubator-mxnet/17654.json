{"BR": {"BR_id": "17654", "BR_author": "ZheyuYe", "BRopenT": "2020-02-21T15:38:56Z", "BRcloseT": "2020-02-25T21:40:45Z", "BR_text": {"BRsummary": "[LayerNorm] Missing the mismatch cues of in_channels", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n It seems that LayerNorm could work through even the setting of in_channels is wrong. As seen in the reproducible code snippet below, I am setting the parameters in_channels as 768 purposely in all cases which are unmatched receiving a input whose whose dimension of last axis is 1024. However, only the last of the three error cases would produce a \"reasonable\" error message.\n I'm not entirely clear about the underlying implementation of nn.LayerNorm, and it make no sense to me that the first two cases are properly executable. I am wondering is there any chance to recheck the LayerNorm to generating an error message to infrom the user of the mismatch. It is now apparent that error messages occur only when there are other layers attached and the model is hybridized.\n The above thinking and experimental process were inspired by a typo in the [SQUAD fine-tuning scripts of XLNET, which may need to be corrected. Surprisingly, this is a runable script even if the units size of xlnet large is 1024.\n <denchmark-link:https://github.com/dmlc/gluon-nlp/blob/137e6b16bc1e672c6963a1e2ed754357e5a2ba11/scripts/language_model/model/qa.py#L37-L46>https://github.com/dmlc/gluon-nlp/blob/137e6b16bc1e672c6963a1e2ed754357e5a2ba11/scripts/language_model/model/qa.py#L37-L46</denchmark-link>\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n <denchmark-code>import mxnet as mx\n from mxnet.gluon import HybridBlock,nn\n mx.npx.set_np()\n \n class Foobar(HybridBlock):\n     def __init__(self, units, prefix=None, params=None):\n         super(Foobar, self).__init__(prefix=prefix, params=params)\n         self.dense = nn.Dense(1, flatten=False)\n         self.layernorm = nn.LayerNorm(epsilon=1e-12, in_channels=768)\n     def hybrid_forward(self, F, x):\n         out = self.layernorm(x)\n         return out\n \n class Foo(HybridBlock):\n     def __init__(self, units, prefix=None, params=None):\n         super(Foo, self).__init__(prefix=prefix, params=params)\n         self.dense = nn.Dense(1, flatten=False)\n         self.layernorm = nn.LayerNorm(epsilon=1e-12, in_channels=768)\n     def hybrid_forward(self, F, x):\n         out = self.layernorm(x)\n         out = self.dense(out)\n         return out\n \n foo_0 = Foobar(units=1024)\n foo_0.initialize(ctx=mx.gpu())\n foo_0.hybridize()\n out = foo_0(mx.np.random.normal(0,1,size=(10,1024), ctx=mx.gpu()))\n \n foo_1 = Foo(units=1024)\n foo_1.initialize(ctx=mx.gpu())\n out = foo_1(mx.np.random.normal(0,1,size=(10,1024), ctx=mx.gpu()))\n \n foo_2 = Foo(units=1024)\n foo_2.initialize(ctx=mx.gpu())\n foo_2.hybridize()\n out = foo_2(mx.np.random.normal(0,1,size=(10,1024), ctx=mx.gpu()))\n \n </denchmark-code>\n \n <denchmark-h:h3>Error Message</denchmark-h>\n \n <denchmark-code>DeferredInitializationError: Parameter 'dense2_weight' has not been initialized yet because initialization was deferred. Actual initialization happens during the first forward pass. Please pass one batch of data through the network before accessing Parameters. You can also avoid deferred initialization by specifying in_units, num_features, etc., for network layers.\n \n During handling of the above exception, another exception occurred:\n AssertionError: Expected shape (1024,) is incompatible with given shape (768,).\n </denchmark-code>\n \n <denchmark-h:h2>Comments</denchmark-h>\n \n <denchmark-link:https://github.com/sxjscience>@sxjscience</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ZheyuYe", "commentT": "2020-02-21T16:59:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ZheyuYe>@ZheyuYe</denchmark-link>\n  The C++ side implementation of the shape inferring logic is here: \n \n The python side is here: \n \n \n incubator-mxnet/python/mxnet/gluon/nn/basic_layers.py\n \n \n         Lines 609 to 614\n       in\n       9dcf71d\n \n \n \n \n \n \n  self.gamma = self.params.get('gamma', grad_req='write' if scale else 'null', \n \n \n \n  shape=(in_channels,), init=gamma_initializer, \n \n \n \n  allow_deferred_init=True) \n \n \n \n  self.beta = self.params.get('beta', grad_req='write' if center else 'null', \n \n \n \n  shape=(in_channels,), init=beta_initializer, \n \n \n \n  allow_deferred_init=True) \n \n \n \n \n \n The problem should be to check the shape of gamma and beta:\n \n \n \n incubator-mxnet/src/operator/nn/layer_norm.cc\n \n \n         Lines 56 to 57\n       in\n       9dcf71d\n \n \n \n \n \n \n  in_shape->at(layernorm::kGamma) = mxnet::TShape(Shape1(channelCount)); \n \n \n \n  in_shape->at(layernorm::kBeta) = mxnet::TShape(Shape1(channelCount)); \n \n \n \n \n \n Would you try to investigate the issue? You can append std::cout << in_shape->at(layernorm::kGamma), which should not be empty when in_channel is given.\n I think one way to solve the prioblem is to use the same SHAPE_ASSIGN_CHECK as here:\n \n \n \n incubator-mxnet/src/operator/numpy/np_where_op.cc\n \n \n          Line 42\n       in\n       9dcf71d\n \n \n \n \n \n \n  SHAPE_ASSIGN_CHECK(*out_attrs, 0, operand1); \n \n \n \n \n \n \t\t"}}}, "commit": {"commit_id": "ce8a616c5b24b21238d83499bbb092ff43c87279", "commit_author": "Xingjian Shi", "commitT": "2020-02-25 13:40:44-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\operator\\nn\\layer_norm.cc", "file_new_name": "src\\operator\\nn\\layer_norm.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "55,56,57,58,59,60", "deleted_lines": "55,56,57,58", "method_info": {"method_name": "mxnet::op::LayerNormShape", "method_params": "attrs,in_shape,out_shape", "method_startline": "39", "method_endline": "68"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\python\\unittest\\test_gluon.py", "file_new_name": "tests\\python\\unittest\\test_gluon.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "897,898,899,900,901,902,903", "deleted_lines": "897", "method_info": {"method_name": "test_layernorm", "method_params": "", "method_startline": "894", "method_endline": "903"}}}}}}}