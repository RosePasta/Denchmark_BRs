{"BR": {"BR_id": "19026", "BR_author": "sxjscience", "BRopenT": "2020-08-27T06:56:44Z", "BRcloseT": "2020-08-31T05:08:20Z", "BR_text": {"BRsummary": "[Bug] RTC Failed to compile", "BRdescription": "\n <denchmark-link:https://github.com/ptrendx>@ptrendx</denchmark-link>\n  Let me create an issue and track it in the issue.\n It is reproducible by running the <denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/tests/test_attention_cell.py>https://github.com/dmlc/gluon-nlp/blob/master/tests/test_attention_cell.py</denchmark-link>\n  script on GPU with GluonNLP.\n Error Message:\n <denchmark-code>>           raise get_last_ffi_error()\n E           mxnet.base.MXNetError: Traceback (most recent call last):\n E             File \"../src/operator/fusion/fused_op.cu\", line 647\n E           MXNetError: Check failed: compileResult == NVRTC_SUCCESS (6 vs. 0) : NVRTC Compilation failed. Please set environment variable MXNET_USE_FUSION to 0.\n E           clip_Cast_kernel.cu(814): error: identifier \"inf\" is undefined\n E           \n E           clip_Cast_kernel.cu(795): warning: variable \"ndim_output1\" was declared but never referenced\n E           \n E           clip_Cast_kernel.cu(797): warning: variable \"ndim_output0\" was declared but never referenced\n E           \n E           clip_Cast_kernel.cu(798): warning: variable \"ndim_input_0\" was declared but never referenced\n E           \n E           clip_Cast_kernel.cu(333): warning: variable \"op::SQRT_2\" was declared but never referenced\n E           \n E           clip_Cast_kernel.cu(393): warning: variable \"op::pi\" was declared but never referenced\n E           \n E           1 error detected in the compilation of \"clip_Cast_kernel.cu\".\n \n </denchmark-code>\n \n Code file obtained after setting MXNET_RTC_VERBOSE =1:\n struct __align__(2) __half {\n   __host__ __device__ __half() { }\n   unsigned short __x;\n };\n /* Definitions of intrinsics */\n __device__ inline __half __float2half(const float f) {\n   __half val;\n  asm(\"{  cvt.rn.f16.f32 %0, %1;}\\n\" : \"=h\"(val.__x) : \"f\"(f));\n   return val;\n }\n __device__ inline float __half2float(const __half h) {\n   float val;\n  asm(\"{  cvt.f32.f16 %0, %1;}\\n\" : \"=f\"(val) : \"h\"(h.__x));\n   return val;\n }\n \n typedef __half half;\n \n template <typename DType>\n struct AccType {\n   using type = DType;\n \n   __device__ static inline type from(const DType& val) {\n     return val;\n   }\n \n   __device__ static inline DType to(type val) {\n     return val;\n   }\n \n };\n \n template<>\n struct AccType<half> {\n   using type = float;\n \n   __device__ static inline type from(const half& val) {\n     return __half2float(val);\n   }\n \n   __device__ static inline half to(type val) {\n     return __float2half(val);\n   }\n };\n \n \n using float32 = float;\n using float64 = double;\n using float16 = half;\n using uint8 = unsigned char;\n using int8 = char;\n using int32 = int;\n using int64 = long long;\n \n static_assert(sizeof(float32) == 4, \"Size of float32 is expected to be 4B\");\n static_assert(sizeof(float64) == 8, \"Size of float64 is expected to be 8B\");\n static_assert(sizeof(float16) == 2, \"Size of float16 is expected to be 2B\");\n static_assert(sizeof(uint8) == 1, \"Size of uint8 is expected to be 1B\");\n static_assert(sizeof(int8) == 1, \"Size of int8 is expected to be 1B\");\n static_assert(sizeof(int32) == 4, \"Size of int32 is expected to be 4B\");\n static_assert(sizeof(int64) == 8, \"Size of int64 is expected to be 8B\");\n \n typedef int32 index_t;\n \n // bool and int8 need to be accumulated in index_t\n // but bool needs to be treated in the special way\n // for ops like bitwise_not\n struct bool_t {\n   index_t value;\n \n   __device__ inline bool_t(const index_t& v) : value(v) {}\n   __device__ inline bool_t(const volatile index_t& v) : value(v) {}\n   __device__ inline bool_t() : value(0) {}\n \n   __device__ inline operator index_t() const volatile { return value; }\n   __device__ inline bool_t& operator= (const index_t& v) {\n     value = v;\n     return *this;\n   }\n   __device__ inline volatile bool_t& operator= (const index_t& v) volatile {\n     value = v;\n     return *this;\n   }\n   __device__ inline bool_t& operator= (const volatile index_t& v) {\n     value = v;\n     return *this;\n   }\n };\n template<>\n struct AccType<bool> {\n   using type = bool_t;\n \n   __device__ static inline type from(const bool& val) {\n     return val;\n   }\n \n   __device__ static inline bool to(type val) {\n     return val;\n   }\n };\n \n template<>\n struct AccType<int8> {\n   using type = index_t;\n \n   __device__ static inline type from(const int8& val) {\n     return val;\n   }\n \n   __device__ static inline int8 to(type val) {\n     return val;\n   }\n };\n \n template<>\n struct AccType<uint8> {\n   using type = index_t;\n \n   __device__ static inline type from(const uint8& val) {\n     return val;\n   }\n \n   __device__ static inline uint8 to(type val) {\n     return val;\n   }\n };\n \n namespace type_util {\n \n struct false_type {\n   static constexpr bool value = false;\n };\n \n struct true_type {\n   static constexpr bool value = true;\n };\n \n // is_integral\n template <typename T> struct is_integral : false_type {};\n template <> struct is_integral<uint8> : true_type {};\n template <> struct is_integral<int8>  : true_type {};\n template <> struct is_integral<int32> : true_type {};\n template <> struct is_integral<int64> : true_type {};\n template <> struct is_integral<bool>  : true_type {};\n template <> struct is_integral<bool_t>  : true_type {};\n \n // is_unsigned\n template <typename T> struct is_unsigned : false_type {};\n template <> struct is_unsigned<uint8> : true_type {};\n template <> struct is_unsigned<bool>  : true_type {};\n template <> struct is_unsigned<bool_t>  : true_type {};\n \n // is_same\n template <typename T, typename U>\n struct is_same : false_type {};\n template <typename T> struct is_same<T, T> : true_type {};\n \n // has_double\n template <typename... T> struct has_double : false_type {};\n \n template <typename A, typename... B>\n struct has_double<A, B...> {\n     static constexpr bool value = is_same<A, double>::value ||\n                                   has_double<B...>::value;\n };\n \n // has_double_or_integral\n template <typename... T> struct has_double_or_integral : false_type {};\n \n template <typename A, typename... B>\n struct has_double_or_integral<A, B...> {\n     static constexpr bool value = is_same<A, double>::value ||\n                                   is_integral<A>::value ||\n                                   has_double_or_integral<B...>::value;\n };\n \n template <bool b>\n struct enable_if {};\n \n template <>\n struct enable_if<true> {\n   using type = void;\n };\n \n template <typename T, typename U, class Enable = void>\n struct mixed_type;\n \n template <typename T>\n struct mixed_type<T, float64, typename enable_if<!is_same<float64, T>::value>::type> {\n   using type = float64;\n };\n \n template <typename T>\n struct mixed_type<float64, T> {\n   using type = float64;\n };\n \n template <typename T>\n struct mixed_type<T, float32, typename enable_if<!is_same<float64, T>::value &&\n                                                  !is_same<float32, T>::value>::type> {\n   using type = float32;\n };\n \n template <typename T>\n struct mixed_type<float32, T, typename enable_if<!is_same<float64, T>::value>::type> {\n   using type = float32;\n };\n \n template <typename T>\n struct mixed_type<T, float16, typename enable_if<is_same<float16, T>::value ||\n                                                  is_integral<T>::value>::type> {\n   using type = float16;\n };\n \n template <typename T>\n struct mixed_type<float16, T, typename enable_if<is_integral<T>::value>::type> {\n   using type = float16;\n };\n \n template <typename T, typename U>\n struct mixed_type<T, U, typename enable_if<is_integral<T>::value &&\n                                            is_integral<U>::value &&\n                                            !is_same<U, bool_t>::value &&\n                                            sizeof(T) <= sizeof(U)>::type> {\n   using type = U;\n };\n \n template <typename T, typename U>\n struct mixed_type<U, T, typename enable_if<is_integral<T>::value &&\n                                            is_integral<U>::value &&\n                                            !is_same<U, bool_t>::value &&\n                                            sizeof(T) < sizeof(U)>::type> {\n   using type = U;\n };\n \n template <typename T>\n struct mixed_type<T, bool_t, typename enable_if<is_integral<T>::value &&\n                                                 sizeof(T) < sizeof(bool_t)>::type> {\n   using type = index_t;\n };\n \n template <typename T>\n struct mixed_type<bool_t, T, typename enable_if<is_integral<T>::value &&\n                                                 sizeof(T) < sizeof(bool_t)>::type> {\n   using type = index_t;\n };\n \n template <typename T>\n struct mixed_type<T, bool_t, typename enable_if<is_integral<T>::value &&\n                                                 sizeof(T) == sizeof(bool_t)>::type> {\n   using type = T;\n };\n \n }  // namespace type_util\n \n \n enum class OpReqType {\n   kNullOp,\n   kWriteTo,\n   kWriteInplace,\n   kAddTo\n };\n \n constexpr int kRTCMaxThreadsPerBlock = 512;\n \n namespace util {\n \n constexpr int MAX_DIM = 5;\n \n template <int ndim>\n __device__ inline void unravel_dot(const index_t idx, const index_t (&shape)[MAX_DIM],\n   const index_t (&stridej)[MAX_DIM], const index_t (&stridek)[MAX_DIM], index_t* j, index_t* k) {\n   *j = 0;\n   *k = 0;\n   #pragma unroll\n   for (index_t i = ndim-1, idx_t = idx; i >=0; --i) {\n     const auto tmp = idx_t / shape[i];\n     const auto coord = idx_t - tmp*shape[i];\n     *j += coord*stridej[i];\n     *k += coord*stridek[i];\n     idx_t = tmp;\n   }\n }\n \n template<int ndim>\n __device__ inline index_t unravel_dot(const index_t idx, const index_t (&shape)[MAX_DIM],\n   const index_t (&stride)[MAX_DIM]) {\n   index_t ret = 0;\n   #pragma unroll\n   for (index_t i = ndim-1, j = idx; i >=0; --i) {\n     auto tmp = j / shape[i];\n     ret += (j - tmp*shape[i])*stride[i];\n     j = tmp;\n   }\n   return ret;\n }\n \n template<int ndim>\n __device__ inline index_t unravel_ravel(const index_t idx, const index_t (&shape1)[MAX_DIM],\n                                         const index_t (&shape2)[MAX_DIM]) {\n   index_t ret = 0;\n   index_t total_shape = 1;\n #pragma unroll\n   for (index_t i = ndim-1, j = idx; i >=0; --i) {\n     if (i != ndim - 1) {\n       total_shape *= shape2[i + 1];\n     }\n     auto tmp = j / shape1[i];\n     const index_t coord = j - tmp*shape1[i];\n     ret += total_shape * (shape2[i] > coord) * coord;\n     j = tmp;\n   }\n   return ret;\n }\n \n template<int ndim, int ndim2>\n __device__ inline index_t ravel(const index_t (&coord)[ndim], const index_t (&shape)[ndim2]) {\n   index_t ret = 0;\n #pragma unroll\n   for (int i = 0; i < ndim; ++i) {\n     ret = ret * shape[i] + (shape[i] > coord[i]) * coord[i];\n   }\n   return ret;\n }\n \n template<int ndim, int ndim2>\n __device__ inline void unravel(const index_t idx,\n                                const index_t (&shape)[ndim2],\n                                index_t (&coord)[ndim]) {\n #pragma unroll\n   for (index_t i = ndim-1, j = idx; i >=0; --i) {\n     auto tmp = j / shape[i];\n     coord[i] = j - tmp*shape[i];\n     j = tmp;\n   }\n }\n \n template <typename DType>\n __device__ inline bool isinf(volatile const DType &val) {\n   return false;\n }\n \n template <>\n __device__ inline bool isinf(volatile const float &val) {\n   return ::isinf(val);\n }\n \n template <>\n __device__ inline bool isinf(volatile const double &val) {\n   return ::isinf(val);\n }\n \n template <>\n __device__ inline bool isinf(volatile const long double &val) {\n   return ::isinf(val);\n }\n \n template <>\n __device__ inline bool isinf(volatile const float16 &val) {\n   return ::isinf(__half2float(const_cast<const float16&>(val)));\n }\n \n template <typename DType>\n __device__ inline bool isnan(volatile const DType &val) {\n   return false;\n }\n \n template <>\n __device__ inline bool isnan(volatile const float &val) {\n   return ::isnan(val);\n }\n \n template <>\n __device__ inline bool isnan(volatile const double &val) {\n   return ::isnan(val);\n }\n \n template <>\n __device__ inline bool isnan(volatile const long double &val) {\n   return ::isnan(val);\n }\n \n template <>\n __device__ inline bool isnan(volatile const float16 &val) {\n   return ::isnan(__half2float(const_cast<const float16&>(val)));\n }\n \n }  // namespace util\n \n \n constexpr double DBL_MAX = 1.7976931348623157081e+308;\n \n namespace op {\n \n namespace special_functions {\n \n template<typename DType>\n __device__ inline static DType trigamma(DType x);\n \n template<>\n __device__ inline double trigamma<double>(double x) {\n   double PI(3.14159265358979323846);\n   double sign = +1;\n   double result = 0;\n   if (x < 0.5) {\n     sign = -1;\n     const double sin_pi_x = sin(PI * x);\n     result -= (PI * PI) / (sin_pi_x * sin_pi_x);\n     x = 1 - x;\n   }\n   for (int i = 0; i < 6; ++i) {\n     result += 1 / (x * x);\n     x += 1;\n   }\n   const double ixx = 1 / (x*x);\n   result += (1 + 1 / (2*x) + ixx * (1./6 - ixx * (1./30 - ixx * (1./42)))) / x;\n   return sign * result;\n }\n \n template<>\n __device__ inline float trigamma<float>(float x) {\n   float PI(3.14159265358979323846);\n   float sign = +1;\n   float result = 0;\n   if (x < 0.5f) {\n     sign = -1;\n     const float sin_pi_x = sinf(PI * x);\n     result -= (PI * PI) / (sin_pi_x * sin_pi_x);\n     x = 1 - x;\n   }\n   for (int i = 0; i < 6; ++i) {\n     result += 1 / (x * x);\n     x += 1;\n   }\n   const float ixx = 1 / (x*x);\n   result += (1 + 1 / (2*x) + ixx * (1.f/6 - ixx * (1.f/30 - ixx * (1.f/42)))) / x;\n   return sign * result;\n }\n \n struct cephes {\n   /*\n    * Helper to evaluate a polynomial given an array of coefficients.\n    */\n   template <typename DType>\n   __device__ inline static DType polevl(DType x, const DType coef[], int N) {\n     DType ans;\n     DType const *p;\n     int i;\n \n     p = coef;\n     ans = *p++;\n \n     i = N;\n     do {\n       ans = ans * x  +  *p++;\n     } while ( --i );\n \n     return( ans );\n   }\n \n \n   /*\n    * Helper function for psi that handles double/float specific differences\n    * in the algorithm.\n    */\n   template<typename DType>\n   __device__ inline static DType psi_helper(DType s);\n \n   /*\n    *\n    *\tPsi (digamma) function\n    *\n    *\n    * SYNOPSIS:\n    *\n    * float x, y, psif();\n    *\n    * y = psif( x );\n    *\n    *\n    * DESCRIPTION:\n    *\n    *              d      -\n    *   psi(x)  =  -- ln | (x)\n    *              dx\n    *\n    * is the logarithmic derivative of the gamma function.\n    * For integer x,\n    *                   n-1\n    *                    -\n    * psi(n) = -EUL  +   >  1/k.\n    *                    -\n    *                   k=1\n    *\n    * This formula is used for 0 < n <= 10.  If x is negative, it\n    * is transformed to a positive argument by the reflection\n    * formula  psi(1-x) = psi(x) + pi cot(pi x).\n    * For general positive x, the argument is made greater than 10\n    * using the recurrence  psi(x+1) = psi(x) + 1/x.\n    * Then the following asymptotic expansion is applied:\n    *\n    *                           inf.   B\n    *                            -      2k\n    * psi(x) = log(x) - 1/2x -   >   -------\n    *                            -        2k\n    *                           k=1   2k x\n    *\n    * where the B2k are Bernoulli numbers.\n    *\n    * ACCURACY:\n    *    Absolute error,  relative when |psi| > 1 :\n    * arithmetic   domain     # trials      peak         rms\n    *    IEEE      -33,0        30000      8.2e-7      1.2e-7\n    *    IEEE      0,33        100000      7.3e-7      7.7e-8\n    *\n    * ERROR MESSAGES:\n    *     message         condition      value returned\n    * psi singularity    x integer <=0      MAXNUMF\n    */\n   template<typename DType>\n   __device__ inline static DType psi(DType x) {\n     DType p, q, nz, s, w, y;\n     int i, n, negative;\n \n     DType EUL(0.57721566490153286061);\n     DType PI(3.14159265358979323846);\n \n     negative = 0;\n     nz = 0.0;\n \n     if ( x <= 0.0 ) {\n       negative = 1;\n       q = x;\n       p = ::floor(q);\n       if ( p == q ) {\n         return DBL_MAX;\n       }\n       /* Remove the zeros of tan(PI x)\n        * by subtracting the nearest integer from x\n        */\n       nz = q - p;\n       if ( nz != 0.5 ) {\n         if ( nz > 0.5 ) {\n           p += 1.0;\n           nz = q - p;\n         }\n         nz = PI/::tan(PI*nz);\n       } else {\n         nz = 0.0;\n       }\n       x = 1.0 - x;\n     }\n \n     /* check for positive integer up to 10 */\n     if ( (x <= 10.0) && (x == ::floor(x)) ) {\n       y = 0.0;\n       n = x;\n       for ( i = 1; i < n; i++ ) {\n         w = i;\n         y += 1.0/w;\n       }\n       y -= EUL;\n       goto done;\n     }\n \n     s = x;\n     w = 0.0;\n     while ( s < 10.0 ) {\n       w += 1.0/s;\n       s += 1.0;\n     }\n \n     y = psi_helper(s);\n \n     y = logf(s)  -  (0.5/s)  -  y  -  w;\n \n done:\n \n     if ( negative ) {\n       y -= nz;\n     }\n \n     return(y);\n   }\n };\n \n \n template<>\n __device__ inline double cephes::psi_helper<double>(double s) {\n   double z;\n   const double A[] = {\n     8.33333333333333333333E-2,\n     -2.10927960927960927961E-2,\n     7.57575757575757575758E-3,\n     -4.16666666666666666667E-3,\n     3.96825396825396825397E-3,\n     -8.33333333333333333333E-3,\n     8.33333333333333333333E-2\n   };\n \n   if ( s < 1.0e17 ) {\n     z = 1.0/(s * s);\n     return z * cephes::polevl<double>(z, A, 6);\n   } else {\n     return 0.0;\n   }\n }\n \n template<>\n __device__ inline float cephes::psi_helper<float>(float s) {\n   float z;\n   const float A[] = {\n     -4.16666666666666666667E-3f,\n     3.96825396825396825397E-3f,\n     -8.33333333333333333333E-3f,\n     8.33333333333333333333E-2f\n   };\n \n   if ( s < 1.0e8 ) {\n     z = 1.0/(s * s);\n     return z * cephes::polevl<float>(z, A, 3);\n   } else {\n     return 0.0;\n   }\n }\n }  // namespace special_functions\n }  // namespace op\n \n \n \n namespace vector {\n \n template <int size>\n struct VectorType {\n     static_assert(size <= 32, \"VectorType needs to have size of at most 32B\");\n };\n \n template <>\n struct VectorType<1> {\n   using type = char;\n };\n \n template <>\n struct VectorType<2> {\n   using type = short;\n };\n \n \n template <>\n struct VectorType<4> {\n   using type = int;\n };\n \n template <>\n struct VectorType<8> {\n   using type = long long;\n };\n \n template <>\n struct VectorType<16> {\n   using type = ulonglong2;\n };\n \n template <>\n struct VectorType<32> {\n   using type = ulonglong4;\n };\n \n template <typename DType>\n __device__ inline DType add_elem(const DType& x, const DType& y) {\n   return x + y;\n }\n \n template <>\n __device__ inline half add_elem(const half& x, const half& y) {\n   return __float2half(__half2float(x) + __half2float(y));\n }\n \n /* \\brief Helper class that enables storing multiple values of type DType\n           as 1 value of type LType.\n */\n template <typename DType, int n>\n class VectorizedStorage {\n  public:\n   using LType = typename VectorType<sizeof(DType) * n>::type;\n   constexpr static int nvec = n;\n   union vectorized_storage {\n     LType aligned;\n     DType separate[nvec];  // NOLINT(*)\n \n     inline __device__ vectorized_storage() {}\n     inline __device__ ~vectorized_storage() {}\n   } scratch_;\n \n   inline __device__ VectorizedStorage() {}\n   inline __device__ VectorizedStorage (const VectorizedStorage<DType, n>& y2) {\n       scratch_.aligned = y2.scratch_.aligned;\n   }\n   inline __device__ VectorizedStorage (const LType &y2) {\n       scratch_.aligned = y2;\n   }\n   inline __device__ VectorizedStorage<DType, n>& operator+=(\n       const VectorizedStorage<DType, n>& rhs) {\n     #pragma unroll\n     for (int i = 0; i < nvec; ++i) {\n       scratch_.separate[i] = add_elem(scratch_.separate[i], rhs.scratch_.separate[i]);\n     }\n     return *this;\n   }\n   inline __device__ ~VectorizedStorage() {}\n };\n \n // Returns const LType is DType is const\n template <typename DType, typename LType>\n struct select_const {\n   using type = LType;\n };\n \n template <typename DType, typename LType>\n struct select_const<const DType, LType> {\n   using type = const LType;\n };\n \n template <typename DType>\n struct remove_const {\n   using type = DType;\n };\n \n template <typename DType>\n struct remove_const<const DType> {\n   using type = DType;\n };\n \n \n /* \\brief Helper class that enables accessing multiple values of type DType\n           as 1 value of type LType. Additional aligned template argument\n           allows performance optimizations if the pointer and the size of\n           the allocation is aligned to sizeof(LType) / sizeof(DType) elements.\n */\n template <typename DType, int nvec, bool aligned = false>\n class VectorizedAccessor {\n  public:\n   using StorageType = VectorizedStorage<typename remove_const<DType>::type,\n                                         nvec>;\n   using LType = typename select_const<DType, typename StorageType::LType>::type;\n   StorageType storage_;\n \n   LType* aligned_ptr_;\n   DType* unaligned_ptr_;\n   int alignment_;\n   index_t n_elems_;\n \n   inline __device__ VectorizedAccessor(DType* const ptr, const index_t size) {\n     unaligned_ptr_ = ptr;\n     if (aligned) {\n       alignment_ = 0;\n       aligned_ptr_ = reinterpret_cast<LType*>(ptr);\n       n_elems_ = (size + nvec- 1) / nvec;\n     } else {\n       size_t ptr_as_number = reinterpret_cast<size_t>(ptr);\n       alignment_ = (ptr_as_number % sizeof(LType)) / sizeof(DType);\n       aligned_ptr_ = reinterpret_cast<LType*>(ptr - alignment_);\n       n_elems_ = (size + alignment_ + nvec - 1) / nvec;\n     }\n   }\n \n   /* \\brief Alignment of the input pointer in elements. */\n   inline __device__ int alignment() const {\n     return alignment_;\n   }\n \n   /* \\brief Access to separate elements. */\n   inline __device__ DType* separate() {\n     return storage_.scratch_.separate;\n   }\n \n   /* \\brief Number of aligned elements that span the entire input tensor. */\n   inline __device__ index_t num_aligned_elements() const {\n     return n_elems_;\n   }\n \n   /* \\brief Load values from the input.\n      \\param id Aligned index of the element.\n      \\param N size of the tensor.\n   */\n   inline __device__ void load(const index_t id, const index_t N) {\n     if (aligned) {\n       storage_.scratch_.aligned = aligned_ptr_[id];\n     } else {\n       if (id > 0 && id < n_elems_ - 1) {\n         storage_.scratch_.aligned = aligned_ptr_[id];\n       } else {\n #pragma unroll\n         for (int j = 0; j < nvec; ++j) {\n           DType* ptr = reinterpret_cast<DType*>(&(aligned_ptr_[id])) + j;\n           if (reinterpret_cast<size_t>(ptr) >= reinterpret_cast<size_t>(unaligned_ptr_) &&\n               reinterpret_cast<size_t>(ptr) < reinterpret_cast<size_t>(unaligned_ptr_ + N)) {\n             storage_.scratch_.separate[j] = *ptr;\n           }\n         }\n       }\n     }\n   }\n };\n \n /* \\brief Class used for vectorized read-only access. */\n template <typename DType, int nvec, bool aligned = false>\n class VectorizedLoader : public VectorizedAccessor<const DType, nvec, aligned> {\n  public:\n   inline __device__ VectorizedLoader(const DType* ptr, const index_t N) :\n     VectorizedAccessor<const DType, nvec, aligned>(ptr, N) {\n   }\n };\n \n /* \\brief Class used for vectorized writable access. */\n template <typename DType, int nvec, bool aligned = false>\n class VectorizedStorer : public VectorizedAccessor<DType, nvec, aligned> {\n  public:\n   inline __device__ VectorizedStorer(DType* ptr, const index_t N) :\n     VectorizedAccessor<DType, nvec, aligned>(ptr, N) {\n   }\n \n   /* \\brief Store values to the output.\n      \\param id Aligned index of the element.\n      \\param N size of the tensor.\n   */\n   inline __device__ void store(const index_t id, const index_t N) {\n     if (aligned) {\n       this->aligned_ptr_[id] = this->storage_.scratch_.aligned;\n     } else {\n       if (id > 0 && id < this->n_elems_ - 1) {\n         this->aligned_ptr_[id] = this->storage_.scratch_.aligned;\n       } else {\n #pragma unroll\n         for (int j = 0; j < nvec; ++j) {\n           DType* ptr = reinterpret_cast<DType*>(&(this->aligned_ptr_[id])) + j;\n           if (reinterpret_cast<size_t>(ptr) >= reinterpret_cast<size_t>(this->unaligned_ptr_) &&\n               reinterpret_cast<size_t>(ptr) < reinterpret_cast<size_t>(this->unaligned_ptr_ + N)) {\n             *ptr = this->storage_.scratch_.separate[j];\n           }\n         }\n       }\n     }\n   }\n };\n \n }  // namespace vector\n \n \n \n \n #define INT_MAX (2147483647)\n \n namespace op {\n \n template <typename DType>\n struct LoadType {\n   using Type = DType;\n };\n \n template <>\n struct LoadType<half> {\n   using Type = float;\n };\n \n template <typename DType>\n __device__ inline typename LoadType<DType>::Type load(const DType input) {\n   return input;\n }\n \n template <>\n __device__ inline float load(const half input) {\n   return __half2float(input);\n }\n \n template <typename DType1, typename DType2>\n __device__ inline DType1 store(const DType2 input, DType1* ref) {\n   return input;\n }\n \n template <typename DType>\n __device__ inline half store(const DType input, half* ref) {\n   return __float2half(input);\n }\n \n template <int ndim>\n struct Shape {\n    int x[ndim];\n    size_t size;\n    __device__ inline const int& operator [](const int i) const {\n        return x[i];\n    }\n    __device__ inline int& operator [](const int i) {\n        return x[i];\n    }\n    __device__ inline void set(const int def) {\n        #pragma unroll\n        for (int i = 0; i < ndim; i++) {\n            x[i] = def;\n        }\n    }\n };\n \n template <>\n struct Shape<0> {\n    size_t size;\n };\n \n template <int nvec, typename DType, int ndim>\n __device__ inline vector::VectorizedStorage<DType, nvec> load_index(const DType * input, int i,\n                                                                     const Shape<ndim> &shape) {\n   using V = vector::VectorizedStorage<DType, nvec>;\n   if (i < shape.size) {\n     const auto* vector_input = reinterpret_cast<const typename V::LType *>(input + i);\n     return V(*vector_input);\n   } else {\n     return V({0});\n   }\n }\n \n template <int nvec, typename DType, int ndim>\n __device__ inline vector::VectorizedStorage<DType, nvec> global_load_index(const DType * input,\n                     int i, const Shape<ndim> &shape) {\n   using V = vector::VectorizedStorage<DType, nvec>;\n   if (i < shape.size) {\n     const auto* vector_input = reinterpret_cast<const typename V::LType *>(input + i);\n     return V(__ldg(vector_input));\n   } else {\n     return V({0});\n   }\n }\n \n template <int nvec, typename DType, int ndim>\n __device__ inline vector::VectorizedStorage<DType, nvec> load_slice(const DType * input,\n                                                                     const Shape<ndim>& shape,\n                                                                     Shape<ndim> begin,\n                                                                     Shape<ndim> end,\n                                                                     int offset) {\n   int idx[nvec];\n \n   Shape<ndim> ref_strides;\n   Shape<ndim> strides;\n   ref_strides[ndim-1] = 1;\n   strides[ndim-1] = 1;\n   #pragma unroll\n   for (int dim = ndim-1; dim >=0; dim--) {\n     if (begin[dim] < 0) begin[dim] = shape[dim] + begin[dim];\n     if (end[dim] < 0) end[dim] = shape[dim] + end[dim];\n     if (end[dim] == INT_MAX) end[dim] = shape[dim];\n     if (dim > 0) {\n       ref_strides[dim-1] = ref_strides[dim] * (end[dim] - begin[dim]);\n       strides[dim-1] = strides[dim] * shape[dim];\n     }\n   }\n   #pragma unroll\n   for (int j = 0; j < nvec; j++) {\n     idx[j] = 0;\n     int ref_idx = offset + j;\n     #pragma unroll\n     for (int dim = 0; dim < ndim; dim++) {\n        int stride = ref_strides[dim];\n        if (shape[dim] > 1) {\n          idx[j] += (ref_idx / stride + begin[dim]) * strides[dim];\n        }\n        ref_idx = ref_idx % stride;\n     }\n   }\n   vector::VectorizedStorage<DType, nvec> ret;\n   #pragma unroll\n   for (int j = 0; j < nvec; j++) {\n       ret.scratch_.separate[j] = *(input + idx[j]);\n   }\n   return ret;\n }\n \n template <int nvec, typename DType, int ndim>\n __device__ inline vector::VectorizedStorage<DType, nvec> fast_load_slice(const DType * input,\n                                                                          const Shape<ndim>& shape,\n                                                                          Shape<ndim> begin,\n                                                                          Shape<ndim> end,\n                                                                          int offset) {\n   int idx = 0;\n \n   Shape<ndim> ref_strides;\n   Shape<ndim> strides;\n   ref_strides[ndim-1] = 1;\n   strides[ndim-1] = 1;\n   #pragma unroll\n   for (int dim = ndim-1; dim >=0; dim--) {\n     if (begin[dim] < 0) begin[dim] = shape[dim] + begin[dim];\n     if (end[dim] < 0) end[dim] = shape[dim] + end[dim];\n     if (end[dim] == INT_MAX) end[dim] = shape[dim];\n     if (dim > 0) {\n       ref_strides[dim-1] = ref_strides[dim] * (end[dim] - begin[dim]);\n       strides[dim-1] = strides[dim] * shape[dim];\n     }\n   }\n   int ref_idx = offset;\n   #pragma unroll\n   for (int dim = 0; dim < ndim; dim++) {\n      int stride = ref_strides[dim];\n      if (shape[dim] > 1) {\n        idx += (ref_idx / stride + begin[dim]) * strides[dim];\n      }\n      ref_idx = ref_idx % stride;\n   }\n   return global_load_index<nvec>(input, idx, shape);\n }\n \n template <int nvec, typename DType, int ndim>\n __device__ inline void store_index(const vector::VectorizedStorage<DType, nvec> value, int i,\n                         DType * output, const Shape<ndim>& shape) {\n   if (i < (shape.size + nvec - 1) / nvec) {\n     auto vector_output = reinterpret_cast<\n                           typename vector::VectorizedStorage<DType, nvec>::LType *>(output);\n     vector_output[i] = value.scratch_.aligned;\n   }\n }\n \n template <int nvec, typename DType, int ndim>\n __device__ inline void store_add_index(const vector::VectorizedStorage<DType, nvec> value, int i,\n                             DType * output, const Shape<ndim>& shape) {\n   if (i < (shape.size + nvec - 1) / nvec) {\n     auto vector_output = reinterpret_cast<\n                           typename vector::VectorizedStorage<DType, nvec>::LType *>(output);\n     vector::VectorizedStorage<DType, nvec> ret(vector_output[i]);\n     ret += value;\n     vector_output[i] = ret.scratch_.aligned;\n   }\n }\n \n }  // namespace op\n \n \n namespace op {\n \n template <typename DType>\n __device__ inline bool isnan(const DType val) {\n   return util::isnan(val);\n }\n \n template <typename DType>\n __device__ inline bool_t isinf(const DType val) {\n   return util::isinf(val);\n }\n \n template <typename DType>\n __device__ inline bool_t isposinf(const DType val) {\n   return util::isinf(val) && (val > 0);\n }\n \n template <typename DType>\n __device__ inline bool_t isneginf(const DType val) {\n   return util::isinf(val) && (val < 0);\n }\n \n template <typename DType>\n __device__ inline bool_t isfinite(const DType val) {\n   return !op::isnan(val) && !op::isinf(val);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n add(const DType a, const DType2 b) {\n   return a + b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n sub(const DType a, const DType2 b) {\n   return a - b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rsub(const DType a, const DType2 b) {\n   return b - a;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n mul(const DType a, const DType2 b) {\n   return a * b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n div(const DType a, const DType2 b) {\n   return a / b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rdiv(const DType a, const DType2 b) {\n   return b / a;\n }\n \n #define DEFINE_BINARY_MATH_FUNC(name, double_version, float_version) \\\n template <typename DType, typename DType2> \\\n __device__ inline typename type_util::mixed_type<DType, DType2>::type \\\n name (const DType a, const DType2 b) { \\\n   if (type_util::has_double_or_integral<DType, DType2>::value) { \\\n     return double_version ((double)a, (double)b); \\\n   } else { \\\n     return float_version ((float)a, (float)b); \\\n   } \\\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n power (const DType a, const DType2 b) {\n   if (type_util::has_double<DType, DType2>::value) {\n     return ::pow ((double)a, (double)b); \\\n   } else {\n     return ::powf ((float)a, (float)b);\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rpow(const DType a, const DType2 b) {\n   return power(b, a);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n max(const DType a, const DType2 b) {\n   if (isnan(a)) return a;\n   return a > b ? a : b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n fmax(const DType a, const DType2 b) {\n   if (isnan(b)) return a;\n   return a > b ? a : b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n min(const DType a, const DType2 b) {\n   if (isnan(a)) return a;\n   return a < b ? a : b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n fmin(const DType a, const DType2 b) {\n   if (isnan(b)) return a;\n   return a < b ? a : b;\n }\n \n DEFINE_BINARY_MATH_FUNC(hypot, ::hypot, ::hypotf)\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n mod(const DType a, const DType2 b) {\n   if (b == 0) {\n     return 0;\n   }\n   const double ad = static_cast<double>(a);\n   const double bd = static_cast<double>(b);\n   if (bd < 0) {\n     if (ad < 0) {\n       return -::fmod(-ad, -bd);\n     } else {\n       return ::fmod(ad, -bd) +\n              (::fmod(ad, -bd) != 0 ? bd : 0);\n     }\n   } else {\n     if (ad < 0) {\n       return -::fmod(-ad, bd) +\n               (::fmod(-ad, bd) != 0 ? bd : 0);\n     } else {\n       return ::fmod(ad, bd);\n     }\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n fmod(const DType a, const DType2 b) {\n   if (b == 0) {\n     return 0;\n   }\n   return ::fmod(static_cast<double>(a), static_cast<double>(b));\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rmod(const DType a, const DType2 b) {\n   return op::mod(b, a);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rfmod(const DType a, const DType2 b) {\n   return op::fmod(b, a);\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a == real_b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType not_equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a != real_b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType greater(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a > real_b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType greater_equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a >= real_b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType less(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a < real_b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType less_equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a <= real_b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool_t np_equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a == real_b ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool_t np_not_equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a != real_b ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool_t np_greater(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a > real_b ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool_t np_greater_equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a >= real_b ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool_t np_less(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a < real_b ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool_t np_less_equal(const DType a, const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a <= real_b ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType logical_and(const DType a, const DType2 b) {\n   return a && b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType logical_or(const DType a, const DType2 b) {\n   return a || b ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType logical_xor(const DType a, const DType2 b) {\n   return ((a || b) && !(a && b)) ? 1 : 0;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType copysign(const DType a, const DType2 b) {\n   return (a >= 0 && b >= 0) || (a < 0 && b < 0) ? a : -a;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType2 rcopysign(const DType a, const DType2 b) {\n   return copysign(b, a);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n lcm(const DType a, const DType2 b) {\n   if (type_util::is_integral<DType>::value &&\n       type_util::is_integral<DType2>::value) {\n     DType A = a;\n     DType2 B = b;\n     // minus cases.\n     if (a < 0) {\n       A = -a;\n     }\n     if (b < 0) {\n       B = -b;\n     }\n     // handle zero-valued cases.\n     DType c;\n     if (a == 0 || b == 0) {\n       c = 0;\n     } else {\n       DType tmp;\n       DType tmp_a = A;\n       DType tmp_b = B;\n       if (A < B) {\n         tmp = A;\n         A = B;\n         B = tmp;\n       }\n       while (A % B != 0) {\n         A = A % B;\n         tmp = A;\n         A = B;\n         B = tmp;\n       }\n       c = tmp_a / B * tmp_b;\n     }\n     return c;\n   } else {\n     return 0;\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type bitwise_xor(const DType a,\n                                                                        const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a ^ real_b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type bitwise_or(const DType a,\n                                                                        const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a | real_b;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type bitwise_and(const DType a,\n                                                                        const DType2 b) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   const mixed_type real_a = a;\n   const mixed_type real_b = b;\n   return real_a & real_b;\n }\n \n DEFINE_BINARY_MATH_FUNC(arctan2, ::atan2, ::atan2f)\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rarctan2(const DType a, const DType2 b) {\n   return arctan2(b, a);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n ldexp(const DType a, const DType2 b) {\n   if (type_util::has_double_or_integral<DType, DType2>::value) {\n     return a * ::pow(2.0, static_cast<double>(b));\n   } else {\n     return a * ::powf(2.0f, static_cast<float>(b));\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rldexp(const DType a, const DType2 b) {\n   return ldexp(b, a);\n }\n \n #undef DEFINE_BINARY_MATH_FUNC\n \n template <typename DType, typename DType2>\n __device__ inline bool np_logical_and(const DType val, const DType2 val2) {\n   return (val && val2) ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool np_logical_or(const DType val, const DType2 val2) {\n   return (val || val2) ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline bool np_logical_xor(const DType val, const DType2 val2) {\n   return ((val || val2) && !(val && val2)) ? true : false;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType left(const DType left_val, const DType2 right_val) {\n   return left_val;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType2 right(const DType left_val, const DType2 right_val) {\n   return right_val;\n }\n \n }  // namespace op\n \n \n namespace op {\n \n template <typename DType>\n __device__ inline DType identity(const DType val) {\n   return val;\n }\n \n template <typename DType>\n __device__ inline DType negation(const DType val) {\n   return -val;\n }\n \n template <typename OutType, typename DType>\n __device__ inline typename LoadType<OutType>::Type cast(const DType val) {\n   return static_cast<typename LoadType<OutType>::Type>(val);\n }\n \n // activations\n \n template <typename DType>\n __device__ inline DType relu(const DType val) {\n   return (isnan(val) || val > 0) ? val : 0;\n }\n \n template <typename DType>\n __device__ inline DType sigmoid(const DType val) {\n   if (type_util::has_double_or_integral<DType>::value) {\n     return 1./(1 + ::exp(-val));\n   } else {\n     return 1.f/(1 + expf(-val));\n   }\n }\n \n template <typename DType>\n __device__ inline DType softrelu(const DType val) {\n   if (type_util::has_double_or_integral<DType>::value) {\n     return ::log(1 + ::exp(val));\n   } else {\n     return logf(1 + expf(val));\n   }\n }\n \n template <typename DType>\n __device__ inline DType softsign(const DType val) {\n   if (type_util::has_double_or_integral<DType>::value) {\n     return val / (1 + fabs(val));\n   } else {\n     return val / (1 + fabsf(val));\n   }\n }\n \n // exp and log\n \n #define DEFINE_UNARY_MATH_FUNC(name, double_version, float_version) \\\n template <typename DType> \\\n __device__ inline DType name (const DType a) { \\\n   if (type_util::has_double_or_integral<DType>::value) { \\\n     return double_version ((double)a); \\\n   } else { \\\n     return float_version (a); \\\n   } \\\n }\n \n DEFINE_UNARY_MATH_FUNC(exp, ::exp, ::expf)\n DEFINE_UNARY_MATH_FUNC(expm1, ::expm1, ::expm1f)\n DEFINE_UNARY_MATH_FUNC(log, ::log, ::logf)\n DEFINE_UNARY_MATH_FUNC(log10, ::log10, ::log10f)\n DEFINE_UNARY_MATH_FUNC(log2, ::log2, ::log2f)\n DEFINE_UNARY_MATH_FUNC(log1p, ::log1p, ::log1pf)\n \n // trigonometric\n \n constexpr double pi = 3.14159265358979323846;\n \n template <typename DType>\n __device__ inline DType degrees(const DType val) {\n   if (type_util::has_double_or_integral<DType>::value) {\n     return (val / pi) * 180;\n   } else {\n     return (val / static_cast<float>(pi)) * 180.f;\n   }\n }\n \n template <typename DType>\n __device__ inline DType radians(const DType val) {\n   if (type_util::has_double_or_integral<DType>::value) {\n     return (val / 180.0) * pi;\n   } else {\n     return (val / 180.0f) * static_cast<float>(pi);\n   }\n }\n \n DEFINE_UNARY_MATH_FUNC(sin, ::sin, ::sinf)\n DEFINE_UNARY_MATH_FUNC(cos, ::cos, ::cosf)\n DEFINE_UNARY_MATH_FUNC(tan, ::tan, ::tanf)\n DEFINE_UNARY_MATH_FUNC(arcsin, ::asin, ::asinf)\n DEFINE_UNARY_MATH_FUNC(arccos, ::acos, ::acosf)\n DEFINE_UNARY_MATH_FUNC(arctan, ::atan, ::atanf)\n \n DEFINE_UNARY_MATH_FUNC(sinh, ::sinh, ::sinhf)\n DEFINE_UNARY_MATH_FUNC(cosh, ::cosh, ::coshf)\n DEFINE_UNARY_MATH_FUNC(tanh, ::tanh, ::tanhf)\n DEFINE_UNARY_MATH_FUNC(arcsinh, ::asinh, ::asinhf)\n DEFINE_UNARY_MATH_FUNC(arccosh, ::acosh, ::acoshf)\n DEFINE_UNARY_MATH_FUNC(arctanh, ::atanh, ::atanhf)\n \n // sqrt\n \n DEFINE_UNARY_MATH_FUNC(sqrt, ::sqrt, ::sqrtf)\n DEFINE_UNARY_MATH_FUNC(rsqrt, ::rsqrt, ::rsqrtf)\n DEFINE_UNARY_MATH_FUNC(cbrt, ::cbrt, ::cbrtf)\n DEFINE_UNARY_MATH_FUNC(rcbrt, ::rcbrt, ::rcbrtf)\n \n template <typename DType>\n __device__ inline DType square(const DType val) {\n   return val * val;\n }\n \n template <typename DType, typename... DTypes>\n __device__ inline typename LoadType<DType>::Type zero(const DType val, const DTypes... args) {\n   return 0;\n }\n \n template <typename DType>\n __device__ inline typename LoadType<DType>::Type zero() {\n   return 0;\n }\n \n template <typename DType, typename... DTypes>\n __device__ inline typename LoadType<DType>::Type one(const DType val, const DTypes... args) {\n   return 1;\n }\n \n template <typename DType>\n __device__ inline typename LoadType<DType>::Type one() {\n   return 1;\n }\n \n template <typename DType, typename... DTypes>\n __device__ inline typename LoadType<DType>::Type negone(const DType val, const DTypes... args) {\n   return -1;\n }\n \n template <typename DType>\n __device__ inline typename LoadType<DType>::Type negone() {\n   return -1;\n }\n \n template <typename DType>\n __device__ inline DType round(const DType val) {\n   if (type_util::has_double<DType>::value) {\n     return ::round((double)val);\n   } else if (type_util::is_integral<DType>::value) {\n     return val;\n   } else {\n     return ::roundf(val);\n   }\n }\n \n template <typename DType>\n __device__ inline DType floor(const DType val) {\n   if (type_util::has_double<DType>::value) {\n     return ::floor((double)val);\n   } else if (type_util::is_integral<DType>::value) {\n     return val;\n   } else {\n     return ::floorf(val);\n   }\n }\n \n template <typename DType>\n __device__ inline DType ceil(const DType val) {\n   if (type_util::has_double<DType>::value) {\n     return ::ceil((double)val);\n   } else if (type_util::is_integral<DType>::value) {\n     return val;\n   } else {\n     return ::ceilf(val);\n   }\n }\n \n template <typename DType>\n __device__ inline DType rint(const DType val) {\n   if (type_util::has_double<DType>::value) {\n     return ::rint((double)val);\n   } else if (type_util::is_integral<DType>::value) {\n     return val;\n   } else {\n     return ::rintf(val);\n   }\n }\n \n template <typename DType>\n __device__ inline DType fix(const DType val) {\n   const auto f = floor(val);\n   const auto c = ceil(val);\n   return (f > 0 ? f : -f) < (c > 0 ? c : -c) ? f : c;\n }\n \n template <typename DType>\n __device__ inline DType trunc(const DType val) {\n   if (type_util::has_double<DType>::value) {\n     return ::trunc((double)val);\n   } else if (type_util::is_integral<DType>::value) {\n     return val;\n   } else {\n     return ::truncf(val);\n   }\n }\n \n template <typename DType>\n __device__ inline DType clip(const DType val, const float a_min, const float a_max) {\n   return max(min(val, a_max), a_min);\n }\n \n template <typename DType>\n __device__ inline DType sign(const DType val) {\n   if (val < 0) return -1;\n   return val > 0 ? 1 : 0;\n }\n \n template <typename DType>\n __device__ inline DType reciprocal(const DType val) {\n   return 1.0f / val;\n }\n \n DEFINE_UNARY_MATH_FUNC(abs, ::fabs, ::fabsf)\n DEFINE_UNARY_MATH_FUNC(gamma, ::tgamma, ::tgammaf)\n DEFINE_UNARY_MATH_FUNC(gammaln, ::lgamma, ::lgammaf)\n DEFINE_UNARY_MATH_FUNC(erf, ::erf, ::erff)\n DEFINE_UNARY_MATH_FUNC(erfinv, ::erfinv, ::erfinvf)\n \n template <typename DType>\n __device__ inline DType gelu(const DType val) {\n   return 0.5f * val * (1.0f + op::erf(val / op::sqrt(2.0f)));\n }\n \n template <typename DType1, typename DType2>\n __device__ inline DType1 smooth_l1(const DType1 val, const DType2 scalar) {\n   const auto bsq = scalar * scalar;\n   const auto ibsq = 1.0f / bsq;\n   if (val > ibsq) {\n     return val - 0.5f * ibsq;\n   } else if (val < -ibsq) {\n     return -val - 0.5f * ibsq;\n   } else {\n     return 0.5f * val * val * bsq;\n   }\n }\n \n template <typename DType>\n __device__ inline DType digamma(const DType val) {\n   if (type_util::has_double_or_integral<DType>::value) {\n     return special_functions::cephes::psi<double>(val);\n   } else {\n     return special_functions::cephes::psi<float>(val);\n   }\n }\n \n template <typename DType>\n __device__ inline DType logical_not(const DType val) {\n   return val != DType(0) ? DType(0) : DType(1);\n }\n \n template <typename DType>\n __device__ inline bool_t np_logical_not(const DType val) {\n   return !static_cast<bool>(val);\n }\n \n #undef DEFINE_UNARY_MATH_FUNC\n \n template <typename DType>\n __device__ inline DType bitwise_not(const DType a) {\n   if (type_util::is_same<DType, bool_t>::value) {\n     return !a;\n   } else {\n     return ~static_cast<int64>(a);\n   }\n }\n \n }  // namespace op\n \n \n \n \n namespace op {\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_relu(const DTypeGrad grad, const DType val) {\n   if (isnan(val)) return val;\n   return val > 0 ? grad : 0;\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_sigmoid(const DTypeGrad grad, const DType out) {\n   return grad * out * (1 - out);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_softrelu(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad * sigmoid(v);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_softsign(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   const auto ap1 = 1 + op::abs(v);\n   return grad / (ap1 * ap1);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_abs(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad * op::sign(v);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_exp(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad * op::exp(v);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_expm1(const DTypeGrad grad, const DType val) {\n   return backward_exp(grad, val);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_log(const DTypeGrad grad, const DType val) {\n   return grad / val;\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_log10(const DTypeGrad grad, const DType val) {\n   return grad / (val * op::log(static_cast<DTypeGrad>(10)));\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_log2(const DTypeGrad grad, const DType val) {\n   return grad / (val * op::log(static_cast<DTypeGrad>(2)));\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_log1p(const DTypeGrad grad, const DType val) {\n   return grad / (1 + val);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_sin(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad * op::cos(v);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_cos(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return -grad * op::sin(v);\n }\n \n // Uses output from tan\n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_tan(const DTypeGrad grad, const DType out) {\n   return grad * (out * out + 1);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_arcsin(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad / op::sqrt(1 - v*v);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_arccos(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return -grad / op::sqrt(1 - v*v);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_arctan(const DTypeGrad grad, const DType val) {\n   return grad / (1 + val*val);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_degrees(const DTypeGrad grad, const DType /* val */) {\n   return op::degrees(grad);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_radians(const DTypeGrad grad, const DType /* val */) {\n   return op::radians(grad);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_sinh(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad * op::cosh(v);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_cosh(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad * op::sinh(v);\n }\n \n // Uses tanh output\n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_tanh(const DTypeGrad grad, const DType out) {\n   return grad * (1 - out * out);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_arcsinh(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad / op::sqrt(v * v + 1);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_arccosh(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   return grad / op::sqrt(v * v - 1);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_arctanh(const DTypeGrad grad, const DType val) {\n   return grad / (1 - val * val);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_sqrt(const DTypeGrad grad, const DType out) {\n   return 0.5 * grad / out;\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_rsqrt(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   const auto inv = 1 / v;\n   return -0.5 * grad * op::sqrt(inv) * inv;\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_cbrt(const DTypeGrad grad, const DType out) {\n   return grad / (3.0f * out * out);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_rcbrt(const DTypeGrad grad, const DType val) {\n   const typename type_util::mixed_type<DTypeGrad, DType>::type v = val;\n   const auto inv = 1 / v;\n   return -1.f/3.f * grad * op::cbrt(inv) * inv;\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_square(const DTypeGrad grad, const DType val) {\n   return 2 * val * grad;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rdiv_grad(const DType val,\n           const DType2 val2) {\n   return -val2 / (val * val);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n div_grad(const DType val,\n          const DType2 val2) {\n   const typename type_util::mixed_type<DType, DType2>::type temp = val2;\n   return op::reciprocal(temp);\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType div_rgrad(const DType val,\n                                   const DType2 val2) {\n   return -val / (val2 * val2);\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType mod_grad(const DType val,\n                                  const DType2 val2) {\n   if (type_util::is_integral<DType>::value) {\n     return 0;\n   } else {\n     return 1;\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType mod_rgrad(const DType val,\n                                   const DType2 val2) {\n   if (type_util::is_integral<DType>::value) {\n     return 0;\n   } else {\n     return -op::floor(val / val2);\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType rmod_grad(const DType val,\n                                   const DType2 val2) {\n   if (type_util::is_integral<DType>::value) {\n     return 0;\n   } else {\n     return -op::floor(val2 / val);\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n power_grad(const DType val,\n            const DType2 val2) {\n   return op::power(val, val2 - 1.f) * val2;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n power_rgrad(const DType val,\n             const DType2 val2) {\n   const typename type_util::mixed_type<DType, DType2>::type temp = val;\n   return op::power(val, val2) * op::log(temp);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rpower_grad(const DType val,\n             const DType2 val2) {\n   const typename type_util::mixed_type<DType, DType2>::type temp = val2;\n   return val * op::log(temp);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n hypot_grad_left(const DType val,\n                 const DType2 val2) {\n   return val / op::hypot(val, val2);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n hypot_grad_right(const DType val,\n                  const DType2 val2) {\n   return val2 / op::hypot(val, val2);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n copysign_grad(const DType val,\n               const DType2 val2) {\n   return (val >= 0 && val2 >= 0) || (val < 0 && val2 < 0) ? 1 : -1;\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n arctan2_grad(const DType val,\n              const DType2 val2) {\n   return val2 / (val * val + val2 * val2);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rarctan2_grad(const DType val,\n               const DType2 val2) {\n   return val / (val * val + val2 * val2);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n arctan2_rgrad(const DType val,\n               const DType2 val2) {\n   return -rarctan2_grad(val, val2);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n ldexp_grad(const DType val,\n            const DType2 val2) {\n   return op::power(static_cast<DType>(2), val2);\n }\n \n template <typename DType, typename DType2>\n __device__ inline typename type_util::mixed_type<DType, DType2>::type\n rldexp_grad(const DType val,\n             const DType2 val2) {\n   using mixed_type = typename type_util::mixed_type<DType, DType2>::type;\n   return val2 * op::power(static_cast<mixed_type>(2), val) * op::log(static_cast<mixed_type>(2));\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_clip(const DTypeGrad grad, const DType val,\n               const float a_min, const float a_max) {\n   if (val > a_max || val < a_min) {\n     return 0;\n   } else {\n     return grad;\n   }\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_reciprocal(const DTypeGrad grad, const DType val) {\n   return -grad / (val * val);\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_erf(const DTypeGrad grad, const DType val) {\n   using mixed_type = typename type_util::mixed_type<DTypeGrad, DType>::type;\n   const mixed_type v = val;\n   constexpr mixed_type my_pi = pi;\n   return 2.0f / op::sqrt(my_pi) * op::exp(-(v*v)) * grad;\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_erfinv(const DTypeGrad grad, const DType val) {\n   using mixed_type = typename type_util::mixed_type<DTypeGrad, DType>::type;\n   constexpr mixed_type my_pi = pi;\n   const mixed_type g = grad;\n   const mixed_type v = val;\n   return 0.5f * op::sqrt(my_pi) * op::exp(v * v) * g;\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_gamma(const DTypeGrad grad, const DType val) {\n   using mixed_type = typename type_util::mixed_type<DTypeGrad, DType>::type;\n   const mixed_type v = val;\n   if (type_util::is_same<DTypeGrad, double>::value) {\n     return grad * op::gamma(v) * op::special_functions::cephes::psi<double>(v);\n   } else {\n     return grad * op::gamma(v) * op::special_functions::cephes::psi<float>(v);\n   }\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_gammaln(const DTypeGrad grad, const DType val) {\n   using mixed_type = typename type_util::mixed_type<DTypeGrad, DType>::type;\n   const mixed_type v = val;\n   if (type_util::is_same<DTypeGrad, double>::value) {\n     return grad * op::special_functions::cephes::psi<double>(v);\n   } else {\n     return grad * op::special_functions::cephes::psi<float>(v);\n   }\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_digamma(const DTypeGrad grad, const DType val) {\n   using mixed_type = typename type_util::mixed_type<DTypeGrad, DType>::type;\n   const mixed_type v = val;\n   if (type_util::is_same<DTypeGrad, double>::value) {\n     return grad * op::special_functions::trigamma<double>(v);\n   } else {\n     return grad * op::special_functions::trigamma<float>(v);\n   }\n }\n \n template <typename DType, typename DTypeGrad>\n __device__ inline typename type_util::mixed_type<DTypeGrad, DType>::type\n backward_gelu(const DTypeGrad grad, const DType val) {\n   return 0.5f * (grad + grad * op::erf(val / op::sqrt(2.0f)) +\n                  val * backward_erf(grad, val / op::sqrt(2.0f)) / op::sqrt(2.0f));\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType smooth_l1_grad(const DType val, const DType2 scalar) {\n   auto bsq = scalar * scalar;\n   auto ibsq = 1.0f / bsq;\n   if (val > ibsq) {\n     return 1;\n   } else if (val < -ibsq) {\n     return -1;\n   } else {\n     return bsq * val;\n   }\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType2 xelu_grad(const DType val,\n                                    const DType2 val2) {\n   return (val > 0) ? 1 : val2;\n }\n \n template <typename DType, typename DType2>\n __device__ inline DType prelu_grad(const DType val,\n                                    const DType2 val2) {\n   return (val > 0) ? 0 : val;\n }\n \n }  // namespace op\n \n \n \n \n namespace red {\n \n /*! \\brief sum reducer */\n struct sum {\n   /*! \\brief do reduction into dst */\n   template<typename DType, typename DType2>\n   __device__ inline static void Reduce(volatile DType& dst,  volatile DType2 src) {\n     dst = op::add(dst, src);\n   }\n \n   /*! \\brief do stable reduction into dst */\n   template<typename DType, typename DType2>\n   __device__ inline static void Reduce(volatile DType& dst,  volatile DType2 src,\n                                        volatile DType& residual) {\n     DType y = op::sub(src, residual);\n     DType t = dst + y;\n     if (util::isinf(t)) {\n       residual = 0;\n     } else {\n       residual = (t - dst) - y;\n     }\n     dst = t;\n   }\n   /*! \\brief combine the results of two reducers */\n   template<typename DType>\n   __device__ inline static void Merge(volatile DType& dst_val, volatile DType& src_val) {\n     Reduce(dst_val, src_val);\n   }\n   /*! \\brief combine the results of two reducers */\n   template<typename DType>\n   __device__ inline static void Merge(volatile DType& dst_val, volatile DType& dst_residual,\n                                       volatile DType& src_val, volatile DType& src_residual) {\n     DType t1 = dst_val + src_val;\n     if (util::isinf(t1)) {\n       dst_val = t1;\n       dst_residual = 0;\n     } else {\n       DType e = t1 - dst_val;\n       DType t2 = ((src_val - e) + (dst_val - (t1 - e))) + dst_residual + src_residual;\n       dst_val = t1 + t2;\n       dst_residual = t2 - (dst_val - t1);\n     }\n   }\n   /*! \\brief finalize reduction result */\n   template<typename DType>\n   __device__ inline static void Finalize(volatile DType& dst) {}\n   /*! \\brief finalize reduction result */\n   template<typename DType>\n   __device__ inline static void Finalize(volatile DType& dst, volatile DType& none) {}\n   /*!\n    *\\brief set the initial value during reduction\n    */\n   template<typename DType>\n   __device__ inline static void SetInitValue(DType &initv) {\n     initv = 0;\n   }\n   /*!\n    *\\brief set the initial value during reduction\n    */\n   template<typename DType>\n   __device__ inline static void SetInitValue(DType &initv, DType &residual) {\n     SetInitValue(initv);\n     residual = 0;\n   }\n };\n }  // namespace red\n \n \n using DType_output1 = float;\n static const int ndim_output1 = 2;\n using DType_output0 = int;\n static const int ndim_output0 = 2;\n static const int ndim_input_0 = 2;\n using DType_input_0 = int;\n static const int nvec = 1;\n \n __launch_bounds__(512)\n __global__ void FusedKernel_clip_Cast(size_t N,  const op::Shape<2> input_0_shape,  const op::Shape<2> output0_shape,  const op::Shape<2> output1_shape, DType_input_0* input_0, DType_output0* output0, DType_output1* output1) {\n \n const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n for (int i = tid; i < N; i+= gridDim.x * blockDim.x) {\n     int offset = i*nvec;\n \n const auto vec_input_0 = op::load_index<nvec>(input_0, offset, input_0_shape);\n vector::VectorizedStorage<DType_output0, nvec> vec_output0;\n vector::VectorizedStorage<DType_output1, nvec> vec_output1;\n for (int j = 0; j < nvec; j++ ) {\n const auto temp0 = op::load(vec_input_0.scratch_.separate[j]);\n const auto temp2 = op::clip(temp0, 0, inf);\n const auto temp4 = op::cast<float32>(temp2);\n vec_output0.scratch_.separate[j] = op::store(temp2, output0);\n vec_output1.scratch_.separate[j] = op::store(temp4, output1);\n }\n op::store_index(vec_output0, i, output0, output0_shape);\n op::store_index(vec_output1, i, output1, output1_shape);\n \n }\n }\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sxjscience", "commentT": "2020-08-28T00:03:19Z", "comment_text": "\n \t\tOk, I believe  is generated by the ffi for  here: <denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/src/api/operator/tensor/matrix_op.cc#L52>https://github.com/apache/incubator-mxnet/blob/master/src/api/operator/tensor/matrix_op.cc#L52</denchmark-link>\n \n I will make PR with support for fusion of clip without a_min or a_max parameters tomorrow.\n \t\t"}}}, "commit": {"commit_id": "e2aacce02b2e09d2bed832810f4aade4d750afcb", "commit_author": "Przemyslaw Tredak", "commitT": "2020-08-30 22:08:18-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\common\\cuda\\rtc\\forward_functions-inl.h", "file_new_name": "src\\common\\cuda\\rtc\\forward_functions-inl.h", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "838,839,840,841,842,843,844", "deleted_lines": "838"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\common\\cuda\\rtc\\special_functions-inl.h", "file_new_name": "src\\common\\cuda\\rtc\\special_functions-inl.h", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "54,55,56", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\python\\gpu\\test_fusion.py", "file_new_name": "tests\\python\\gpu\\test_fusion.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "163,164,165", "deleted_lines": null, "method_info": {"method_name": "check_unary_ops", "method_params": "", "method_startline": "67", "method_endline": "169"}}}}}}}