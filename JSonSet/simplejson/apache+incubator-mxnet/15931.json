{"BR": {"BR_id": "15931", "BR_author": "hzfan", "BRopenT": "2019-08-16T18:03:44Z", "BRcloseT": "2019-08-19T06:44:01Z", "BR_text": {"BRsummary": "TBlob bug about dltensor", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n TBlob does not disable/overload the default copy constructor/assignment, so the default one can be used. This results in shallow copy of dltensor_ (which is a field of type DLTensor in TBlob, see <denchmark-link:https://github.com/apache/incubator-mxnet/blob/5a4c01bac9afd4e75227a0b4b1231bceffb204df/include/mxnet/tensor_blob.h#L415>here</denchmark-link>\n ) and memory leak.\n <denchmark-h:h2>Environment info (Required)</denchmark-h>\n \n Python 3.7.3\n Built from source (master at <denchmark-link:https://github.com/apache/incubator-mxnet/commit/5a4c01bac9afd4e75227a0b4b1231bceffb204df>5a4c01b</denchmark-link>\n )\n <denchmark-h:h2>Minimum reproducible example</denchmark-h>\n \n To reproduce this error, I made a minor change to the function <denchmark-link:https://github.com/apache/incubator-mxnet/blob/5a4c01bac9afd4e75227a0b4b1231bceffb204df/src/operator/numpy/np_dot-inl.h#L39>NumpyDotForward</denchmark-link>\n  (in src/operator/numpy/np_dot-inl.h) for illustration.\n Here is the function after my modification.\n I modified one line, and added two lines (denoted by comments):\n <denchmark-code>template<typename xpu>\n inline void NumpyDotForward(const nnvm::NodeAttrs& attrs,\n                             const OpContext& ctx,\n                             const std::vector<TBlob>& inputs,\n                             const std::vector<OpReqType>& req,\n                             const std::vector<TBlob>& outputs) {\n   using namespace mshadow;\n   using namespace mxnet_op;\n \n   CHECK_EQ(inputs.size(), 2U);\n   CHECK_EQ(outputs.size(), 1U);\n \n   const TBlob& a = inputs[0];\n   const TBlob& b = inputs[1];\n   // const TBlob& out = outputs[0];\n   TBlob out = outputs[0];  // ** changed by me **\n   const mxnet::TShape a_shape = a.shape_;\n   const mxnet::TShape b_shape = b.shape_;\n   out = out.reshape(out.shape_);  // ** added by me **\n   out = TBlob(out.dltensor());  // ** added by me **\n   MSHADOW_REAL_TYPE_SWITCH(out.type_flag_, DType, {\n     if (b_shape.ndim() < 3) {\n       // Case 1, 2, 3, 4, 5: a is N-D array (N >= 1) and b is vector or matrix, sum product\n       //        over the last axis of a and the first axis of b\n       TensordotIntAxesImpl<xpu>(1, ctx, a, b, out, req[0]);\n     } else {\n       // Case 3, 5.5: a is N-D array and b is M-D array (M > 2), sum product over the last axis\n       //         of a and the 2nd-to-last axis of b\n       const Tuple<int> a_axes_summed({a_shape.ndim() - 1});\n       const Tuple<int> b_axes_summed({b_shape.ndim() - 2});\n       TensordotImpl<xpu>(a_axes_summed, b_axes_summed, ctx, a, b, out, req);\n     }\n   });\n }\n </denchmark-code>\n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n \n replace NumpyDotForward with the above one\n build\n run the following\n \n <denchmark-code>from mxnet import np\n a = np.array([[1, 2, 3], [4, 5, 6]])\n b = np.array([[1, 1], [1, 1], [1, 1]])\n np.dot(a, b)\n </denchmark-code>\n \n The expected result is\n <denchmark-code>array([[ 6.,  6.],\n        [15., 15.]])\n </denchmark-code>\n \n But the real result is\n <denchmark-code>array([[0., 0.],\n        [0., 0.]])\n </denchmark-code>\n \n <denchmark-h:h2>Possible cause of this problem</denchmark-h>\n \n TBlob.dltensor_.shape is a pointer. When TBlob b is assigned to TBlob a, the pointer gets shallow copied:\n <denchmark-code>a.dltensor_.shape = b.dltensor_.shape\n </denchmark-code>\n \n But b.dltensor_.shape points to b.shape_.data(). So when b is a temporary variable (like the return value of TBlob.reshape()), b.shape_.data() gets destroyed after the function returns. Now a.dltensor_.shape points to invalid memory.\n <denchmark-h:h2>Quick fix (IMO)</denchmark-h>\n \n \n disable default assignment/copy constructor (declare them with private)\n overload them and use SetDLTensor to avoid shallow copy\n \n <denchmark-h:h2>Comments</denchmark-h>\n \n This bug has nothing to do with np.dot. I just used it for illustration.\n Thank <denchmark-link:https://github.com/yzhliu>@yzhliu</denchmark-link>\n  <denchmark-link:https://github.com/reminisce>@reminisce</denchmark-link>\n  <denchmark-link:https://github.com/haojin2>@haojin2</denchmark-link>\n  for help.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hzfan", "commentT": "2019-08-16T18:03:47Z", "comment_text": "\n \t\tHey, this is the MXNet Label Bot.\n Thank you for submitting the issue! I will try and suggest some labels so that the appropriate MXNet community members can help resolve it.\n Here are my recommended labels: Bug\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hzfan", "commentT": "2019-08-16T18:08:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/mxnet-label-bot>@mxnet-label-bot</denchmark-link>\n  add [bug]\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hzfan", "commentT": "2019-08-17T06:10:13Z", "comment_text": "\n \t\tI too encountered the illegal-memory-access error probably resulted from the root cause revealed here (I called TBlob::reshape). I think overriding assignment operator and copy constructor with SetDLTensor() explicitly called is reasonable.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hzfan", "commentT": "2019-08-18T09:02:55Z", "comment_text": "\n \t\tI created a fix in <denchmark-link:https://github.com/apache/incubator-mxnet/pull/15937#issue-308348737>#15937 (comment)</denchmark-link>\n  by overriding assignment operator and copy constructor with SetDLTensor() explicitly called, as <denchmark-link:https://github.com/reminisce>@reminisce</denchmark-link>\n  suggested.\n As <denchmark-link:https://github.com/yzhliu>@yzhliu</denchmark-link>\n   mentioned, this is not backward compatible. I'm not sure whether previous use of TBlob assignment will be affected. But after all, I think overriding it is necessary. Otherwise the dltensor converted from TBlob, which is heavily used when TVM is incorporated, will be corrupted.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "hzfan", "commentT": "2019-08-19T06:44:01Z", "comment_text": "\n \t\tFixed by <denchmark-link:https://github.com/apache/incubator-mxnet/pull/15937>#15937</denchmark-link>\n  . Issue closed.\n \t\t"}}}, "commit": {"commit_id": "64a0502d625bf63ddccdf551d24cbf3b5cb47c10", "commit_author": "Haozheng Fan", "commitT": "2019-08-18 23:00:43-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "include\\mxnet\\tensor_blob.h", "file_new_name": "include\\mxnet\\tensor_blob.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "179,180,181,182,183,184,185", "deleted_lines": null, "method_info": {"method_name": "mxnet::TBlob::operator =", "method_params": "src", "method_startline": "179", "method_endline": "185"}}, "hunk_1": {"Ismethod": 1, "added_lines": "155,156,157", "deleted_lines": null, "method_info": {"method_name": "mxnet::TBlob::TBlob", "method_params": "src", "method_startline": "155", "method_endline": "157"}}}}}}}