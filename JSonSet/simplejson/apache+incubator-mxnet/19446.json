{"BR": {"BR_id": "19446", "BR_author": "bgawrych", "BRopenT": "2020-10-29T13:55:04Z", "BRcloseT": "2020-11-02T18:02:40Z", "BR_text": {"BRsummary": "net.optimize_for doesn't work with numpy semantics", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n Forward pass after calling optimize_for with specific backend doesn't work. I'm not sure what this error mean, but found a way to overcome this (ugly way :))\n Problem occurs on master and 1.x branches\n <denchmark-h:h3>Error Message</denchmark-h>\n \n <denchmark-code>Traceback (most recent call last):\n   File \"../d.py\", line 23, in <module>\n     print(net(a, b))\n   File \"/home/bgawrych/Desktop/mxnet/python/mxnet/gluon/block.py\", line 1407, in __call__\n     return super().__call__(x, *args)\n   File \"/home/bgawrych/Desktop/mxnet/python/mxnet/gluon/block.py\", line 716, in __call__\n     _check_all_np_ndarrays(out)\n   File \"/home/bgawrych/Desktop/mxnet/python/mxnet/gluon/utils.py\", line 480, in _check_all_np_ndarrays\n     raise TypeError(\"Block's output ndarrays/symbols must be of type `mxnet.numpy.ndarray`\"\n TypeError: Block's output ndarrays/symbols must be of type `mxnet.numpy.ndarray` or `mxnet.symbol.numpy._Symbol`, while got output type <class 'mxnet.ndarray.ndarray.NDArray'>\n </denchmark-code>\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n <denchmark-code>import mxnet as mx\n from mxnet.gluon import HybridBlock\n \n mx.npx.set_np()\n \n class TestBlock(HybridBlock):\n     def __init__(self):\n         super(TestBlock, self).__init__()\n         self.d = mx.gluon.nn.Dense(1)\n     def hybrid_forward(self, F, a, b, *args):\n         res = self.d.hybrid_forward(F, a, b)\n         return res\n \n a = mx.np.random.uniform(low=-1, high=1, size=(1,1))\n b = mx.np.random.uniform(low=-1, high=1, size=(1,1))\n \n net = TestBlock()\n net.initialize()\n net.hybridize()\n \n print(net(a, b))\n net.optimize_for(a, b, backend=\"MKLDNN\")\n #print(net(a, b)) # <---- this line doesn't work now - we need to reload symbol with JSON\n inputs, sym = net._cached_graph\n sym = mx.sym.np._symbol.load_json(sym.tojson())\n x = mx.gluon.SymbolBlock(sym, [mx.sym.var('data0'), mx.sym.var('data1')], net.collect_params())\n \n print(x(a, b))\n </denchmark-code>\n \n <denchmark-h:h2>What have you tried to solve it?</denchmark-h>\n \n \n Add ConvertShapeAttrToNumPyCompatible(&g); in MXOptimizeForBackend- doesn't help\n \n <denchmark-link:https://github.com/samskalicky>@samskalicky</denchmark-link>\n  maybe you will be able to help\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "bgawrych", "commentT": "2020-10-30T16:36:10Z", "comment_text": "\n \t\tThanks for the code to reproduce <denchmark-link:https://github.com/bgawrych>@bgawrych</denchmark-link>\n , I can reproduce on teh v1.8.x branch. will take a look\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "bgawrych", "commentT": "2020-10-30T17:08:45Z", "comment_text": "\n \t\tAfter checking with <denchmark-link:https://github.com/leezu>@leezu</denchmark-link>\n  it looks like we need to do something like:\n <denchmark-code># Partition the graph.                                                                                                               \n out = out.optimize_for(self._backend, arg_dict, aux_dict, ctx, **self._backend_opts)\n \n # convert to numpy symbol if needed                                                                                                  \n if _mx_npx.is_np_array():\n     out = out.as_np_ndarray()\n \n #update cached graph with partitioned graph                                                                                          \n self._cached_graph = data, out\n </denchmark-code>\n \n here where we call optimize_for in the Gluon block:\n \n \n \n incubator-mxnet/python/mxnet/gluon/block.py\n \n \n          Line 1039\n       in\n       0faecf0\n \n \n \n \n \n \n  out = out.optimize_for(self._backend, arg_dict, aux_dict, ctx, **self._backend_opts) \n \n \n \n \n \n With this change it seems to be working. I'll create a PR with this on v1.x and master branches. I guess there hasnt been any testing of Numpy functionality with optimize_for yet....\n \t\t"}}}, "commit": {"commit_id": "33d94f1d59335f504ed5b9a7b32f0e81a5d5da56", "commit_author": "Sam Skalicky", "commitT": "2020-11-02 10:02:39-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\mxnet\\gluon\\block.py", "file_new_name": "python\\mxnet\\gluon\\block.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1041,1042,1043,1044", "deleted_lines": null, "method_info": {"method_name": "_build_cache", "method_params": "self,args", "method_startline": "994", "method_endline": "1089"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\python\\unittest\\test_numpy_gluon.py", "file_new_name": "tests\\python\\unittest\\test_numpy_gluon.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "424,425,426", "deleted_lines": null, "method_info": {"method_name": "test_optimize_for.__init__", "method_params": "self", "method_startline": "424", "method_endline": "426"}}, "hunk_1": {"Ismethod": 1, "added_lines": "427,428,429", "deleted_lines": null, "method_info": {"method_name": "test_optimize_for.hybrid_forward", "method_params": "self,F,a,b,args", "method_startline": "427", "method_endline": "429"}}, "hunk_2": {"Ismethod": 1, "added_lines": "422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440", "deleted_lines": null, "method_info": {"method_name": "test_optimize_for", "method_params": "", "method_startline": "422", "method_endline": "440"}}}}}}}