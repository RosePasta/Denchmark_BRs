{"BR": {"BR_id": "19286", "BR_author": "szha", "BRopenT": "2020-10-03T20:48:19Z", "BRcloseT": "2020-10-06T18:11:45Z", "BR_text": {"BRsummary": "infer shape error in hybridized block for zero-size input", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n For a hybridized HybridBlock, the new forward interface throws an error for zero-size input\n <denchmark-h:h3>Error Message</denchmark-h>\n \n <denchmark-code>MXNetError: MXNetError: Shape inconsistent, Provided = [2,4,0,128], inferred shape=[2,4,4,128]\n </denchmark-code>\n \n \n Complete error\n ---> 17 model(np.zeros((2, 2, 4, 0, 128)))\n ~/mxnet/python/mxnet/util.py in _with_np_shape(*args, **kwargs)\n 296         def _with_np_shape(*args, **kwargs):\n 297             with np_shape(active=True):\n --> 298                 return func(*args, **kwargs)\n 299         return _with_np_shape\n 300     else:\n ~/mxnet/python/mxnet/util.py in _with_np_array(*args, **kwargs)\n 480         def _with_np_array(*args, **kwargs):\n 481             with np_array(active=True):\n --> 482                 return func(*args, **kwargs)\n 483         return _with_np_array\n 484     else:\n ~/mxnet/python/mxnet/gluon/block.py in call(self, x, *args)\n 1427\n 1428             with x.ctx:\n -> 1429                 return self._call_cached_op(x, *args)\n 1430\n 1431     def forward(self, x, *args):\n ~/mxnet/python/mxnet/util.py in _with_np_shape(*args, **kwargs)\n 296         def _with_np_shape(*args, **kwargs):\n 297             with np_shape(active=True):\n --> 298                 return func(*args, **kwargs)\n 299         return _with_np_shape\n 300     else:\n ~/mxnet/python/mxnet/util.py in _with_np_array(*args, **kwargs)\n 480         def _with_np_array(*args, **kwargs):\n 481             with np_array(active=True):\n --> 482                 return func(*args, **kwargs)\n 483         return _with_np_array\n 484     else:\n ~/mxnet/python/mxnet/gluon/block.py in _call_cached_op(self, *args)\n 1096     def _call_cached_op(self, *args):\n 1097         if self._cached_op is None:\n -> 1098             self._build_cache(*args)\n 1099         assert self._cached_op, \"Gluon failed to build the cache. \" \n 1100                                 \"This should never happen. \" \\\n ~/mxnet/python/mxnet/util.py in _with_np_shape(*args, **kwargs)\n 296         def _with_np_shape(*args, **kwargs):\n 297             with np_shape(active=True):\n --> 298                 return func(*args, **kwargs)\n 299         return _with_np_shape\n 300     else:\n ~/mxnet/python/mxnet/util.py in _with_np_array(*args, **kwargs)\n 480         def _with_np_array(*args, **kwargs):\n 481             with np_array(active=True):\n --> 482                 return func(*args, **kwargs)\n 483         return _with_np_array\n 484     else:\n ~/mxnet/python/mxnet/gluon/block.py in _build_cache(self, *args)\n 993\n 994     def _build_cache(self, *args):\n --> 995         data, out = self._get_graph(*args)\n 996         data_names = {data.name: i for i, data in enumerate(data)}\n 997         params = {p.var().name: p for p in self.collect_params().values()}\n ~/mxnet/python/mxnet/util.py in _with_np_shape(*args, **kwargs)\n 296         def _with_np_shape(*args, **kwargs):\n 297             with np_shape(active=True):\n --> 298                 return func(*args, **kwargs)\n 299         return _with_np_shape\n 300     else:\n ~/mxnet/python/mxnet/util.py in _with_np_array(*args, **kwargs)\n 480         def _with_np_array(*args, **kwargs):\n 481             with np_array(active=True):\n --> 482                 return func(*args, **kwargs)\n 483         return _with_np_array\n 484     else:\n ~/mxnet/python/mxnet/gluon/block.py in _get_graph(self, *args)\n 989                 return self._get_graph_v1(*args)\n 990             else:  # Gluon 2 based on deferred compute mode\n --> 991                 return self._get_graph_v2(*args)\n 992         return self._cached_graph\n 993\n ~/mxnet/python/mxnet/util.py in _with_np_shape(*args, **kwargs)\n 296         def _with_np_shape(*args, **kwargs):\n 297             with np_shape(active=True):\n --> 298                 return func(*args, **kwargs)\n 299         return _with_np_shape\n 300     else:\n ~/mxnet/python/mxnet/util.py in _with_np_array(*args, **kwargs)\n 480         def _with_np_array(*args, **kwargs):\n 481             with np_array(active=True):\n --> 482                 return func(*args, **kwargs)\n 483         return _with_np_array\n 484     else:\n ~/mxnet/python/mxnet/gluon/block.py in _get_graph_v2(self, *args)\n 978             args = _regroup(flatten_args, self._in_format)\n 979             with autograd.pause(), dc.context():\n --> 980                 out = super().call(*args)\n 981             flatten_out, self._out_format = _flatten(out, \"output\")\n 982             symbol_outputs = dc.get_symbol(flatten_out, sym_cls=type(symbol_inputs[0]))\n ~/mxnet/python/mxnet/gluon/block.py in call(self, *args)\n 709             hook(self, args)\n 710\n --> 711         out = self.forward(*args)\n 712\n 713         for hook in self._forward_hooks.values():\n ~/mxnet/python/mxnet/util.py in _with_np_shape(*args, **kwargs)\n 296         def _with_np_shape(*args, **kwargs):\n 297             with np_shape(active=True):\n --> 298                 return func(*args, **kwargs)\n 299         return _with_np_shape\n 300     else:\n ~/mxnet/python/mxnet/util.py in _with_np_array(*args, **kwargs)\n 480         def _with_np_array(*args, **kwargs):\n 481             with np_array(active=True):\n --> 482                 return func(*args, **kwargs)\n 483         return _with_np_array\n 484     else:\n  in forward(self, x)\n 8         super().init()\n 9     def forward(self, x):\n ---> 10         return x[0]\n 11\n 12\n ~/mxnet/python/mxnet/numpy/multiarray.py in getitem(self, key)\n 743                     'index {} is out of bounds for axis 0 with size {}'.format(\n 744                         key, shape[0]))\n --> 745             return self._at(key)\n 746         elif isinstance(key, py_slice):\n 747             # Unlike numpy/_symbol.py, calls MXNDArraySlice64 writable memory\n ~/mxnet/python/mxnet/ndarray/ndarray.py in _at(self, idx)\n 1423         else:\n 1424             check_call(_LIB.MXNDArrayAt(\n -> 1425                 self.handle, ctypes.c_uint32(idx), ctypes.byref(handle)))\n 1426         return self.class(handle=handle, writable=self.writable)\n 1427\n ~/mxnet/python/mxnet/base.py in check_call(ret)\n 244     \"\"\"\n 245     if ret != 0:\n --> 246         raise get_last_ffi_error()\n 247\n 248\n MXNetError: MXNetError: Shape inconsistent, Provided = [2,4,0,128], inferred shape=[2,4,4,128]\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n Run the following script\n <denchmark-code>from mxnet import np, npx, gluon, use_np\n \n npx.set_np()\n \n @use_np\n class TestModel(gluon.HybridBlock):\n     def __init__(self):\n         super().__init__()\n     def forward(self, x):\n         return x[0]\n \n \n model = TestModel()\n model.initialize()\n model.hybridize()\n \n model(np.zeros((2, 2, 4, 0, 128)))\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n \n Environment Information\n ----------Python Info----------\n ('Version      :', '3.7.8')\n ('Compiler     :', 'GCC 4.2.1 Compatible Apple LLVM 11.0.3 (clang-1103.0.29.20) (-macos10.15-objc-')\n ('Build        :', ('default', 'Jun  5 2020 22:59:21'))\n ('Arch         :', ('64bit', ''))\n ------------Pip Info-----------\n No corresponding pip install for current python.\n ----------MXNet Info-----------\n No MXNet installed.\n ----------System Info----------\n ('Platform     :', 'Darwin-19.6.0-x86_64-i386-64bit')\n ('system       :', 'Darwin')\n ('node         :', 'a483e79ab3ab')\n ('release      :', '19.6.0')\n ('version      :', 'Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64')\n ----------Hardware Info----------\n ('machine      :', 'x86_64')\n ('processor    :', 'i386')\n machdep.cpu.brand_string: Intel(R) Core(TM) i7-8569U CPU @ 2.80GHz\n machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C\n machdep.cpu.leaf7_features: RDWRFSGS TSC_THREAD_OFFSET SGX BMI1 AVX2 SMEP BMI2 ERMS INVPCID FPU_CSDS MPX RDSEED ADX SMAP CLFSOPT IPT MDCLEAR TSXFA IBRS STIBP L1DF SSBD\n machdep.cpu.extfeatures: SYSCALL XD 1GBPAGE EM64T LAHF LZCNT PREFETCHW RDTSCP TSCI\n ----------Network Test----------\n Setting timeout: 10\n Timing for MXNet: https://github.com/apache/incubator-mxnet, DNS: 0.0040 sec, LOAD: 2.5430 sec.\n Timing for PYPI: https://pypi.python.org/pypi/pip, DNS: 0.6464 sec, LOAD: 9.0609 sec.\n Timing for FashionMNIST: https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz, DNS: 0.0006 sec, LOAD: 2.1972 sec.\n Error open Conda: https://repo.continuum.io/pkgs/free/, HTTP Error 403: Forbidden, DNS finished in 0.000483989715576 sec.\n Timing for Gluon Tutorial(en): http://gluon.mxnet.io, DNS: 0.0009 sec, LOAD: 2.8254 sec.\n Error open Gluon Tutorial(cn): https://zh.gluon.ai, <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:727)>, DNS finished in 0.00119805335999 sec.\n ----------Environment----------\n CC=\"/usr/local/opt/llvm/bin/clang\"\n CXX=\"/usr/local/opt/llvm/bin/clang++\"\n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "szha", "commentT": "2020-10-05T18:41:23Z", "comment_text": "\n \t\tThis bug is related to legacy (non-numpy) reshape operator. Reshape with 0 implies \"copy this dimension from input\".\n We can see that all shapes are correct prior to infer_shape pass:\n <denchmark-code>frame #6: 0x00007fff3bd1cb8b libmxnet.so`mxnet::imperative::SetShapeType(ctx=0x00007fffffff8f18, attrs=0x00007fffffff97a8, inputs=size=1, outputs=size=1, dispatch_mode=0x00007fffffff8f24) at imperative_utils.h:208:26\n    205        common::ConvertToNumpyShape(&in_shapes);\n    206        common::ConvertToNumpyShape(&out_shapes);\n    207      }\n -> 208      const bool success = infershape[attrs.op](attrs, &in_shapes, &out_shapes);\n    209      if (!success) {\n    210        std::stringstream os;\n    211        os << \"Operator \" << attrs.op->name << \" inferring shapes failed.\\n\";\n (lldb) parray 5 in_shapes[0].data_heap_\n (long *) $15 = 0x0000555555f45700 {\n   (long) [0] = 1\n   (long) [1] = 2\n   (long) [2] = 4\n   (long) [3] = 0\n   (long) [4] = 128\n }\n (lldb) p out_shapes                                                                                                                                                                    (mxnet::ShapeVector) $16 = size=1 {\n   [0] = {\n     mxnet::Tuple<long> = {\n       ndim_ = 4\n       num_heap_allocated_ = 0\n       data_stack_ = ([0] = 2, [1] = 4, [2] = 0, [3] = 128)\n       data_heap_ = 0x0000000000000000\n     }\n   }\n }\n </denchmark-code>\n \n But after infer_shape, 0 is replaced by 4.\n <denchmark-code>frame #1: 0x00007fff4968c1be libmxnet.so`mxnet::op::ReshapeShape(attrs=0x00007fffffff97a8, in_attrs=0x00005555569cbb68, out_attrs=0x00005555569cbb80) at matrix_op-inl.h:235:3\n    232      << \"Target: \" << oshape\n    233      << \"\\nSource: \" << dshape;\n    234  #endif\n -> 235    SHAPE_ASSIGN_CHECK(*out_attrs, 0, oshape);\n    236    return ReverseReshapeInferShape(&(*in_attrs)[0], (*out_attrs)[0]);\n    237  }\n    238\n (lldb) p oshape\n (mxnet::TShape) $17 = {\n   mxnet::Tuple<long> = {\n     ndim_ = 4\n     num_heap_allocated_ = 0\n     data_stack_ = ([0] = 2, [1] = 4, [2] = 4, [3] = 128)\n     data_heap_ = 0x0000000000000000\n   }\n }\n </denchmark-code>\n \n \n \n \n incubator-mxnet/src/operator/tensor/matrix_op-inl.h\n \n \n         Lines 194 to 237\n       in\n       f732530\n \n \n \n \n \n \n  inline bool ReshapeShape(const nnvm::NodeAttrs& attrs, \n \n \n \n                           mxnet::ShapeVector *in_attrs, \n \n \n \n                           mxnet::ShapeVector *out_attrs) { \n \n \n \n  const ReshapeParam& param_ = nnvm::get<ReshapeParam>(attrs.parsed); \n \n \n \n  CHECK_EQ(in_attrs->size(), 1U) << \"Input: [data]\"; \n \n \n \n  CHECK_EQ(out_attrs->size(), 1U); \n \n \n \n    mxnet::TShape &dshape = (*in_attrs)[0]; \n \n \n \n  if (!mxnet::ndim_is_known(dshape)) return false; \n \n \n \n    mxnet::TShape oshape; \n \n \n \n  if (param_.shape.ndim() != 0) { \n \n \n \n      oshape = InferReshapeShape(param_.shape, dshape, param_.reverse); \n \n \n \n    } else if (param_.target_shape.ndim() != -1) { \n \n \n \n  LOG(INFO) << \"Using target_shape will be deprecated.\"; \n \n \n \n      oshape = param_.target_shape; \n \n \n \n  int neg_count = 0; \n \n \n \n  index_t inf_idx = 0; \n \n \n \n  index_t start_idx = param_.keep_highest ? 1 : 0; \n \n \n \n  if (param_.keep_highest) { \n \n \n \n        oshape[0] = dshape[0]; \n \n \n \n      } \n \n \n \n  for (int i = start_idx; i < oshape.ndim(); ++i) { \n \n \n \n  if (oshape[i] == 0) { \n \n \n \n          neg_count++; \n \n \n \n          inf_idx = i; \n \n \n \n        } \n \n \n \n      } \n \n \n \n  if (neg_count == 1) { \n \n \n \n        oshape[inf_idx] = 1; \n \n \n \n        oshape[inf_idx] = dshape.Size() / oshape.Size(); \n \n \n \n      } \n \n \n \n    } else { \n \n \n \n  return shape_is_known((*out_attrs)[0]) \n \n \n \n             && ReverseReshapeInferShape(&(*in_attrs)[0], (*out_attrs)[0]); \n \n \n \n    } \n \n \n \n  ReverseReshapeInferShape(&dshape, oshape); \n \n \n \n  #if 0 \n \n \n \n    CHECK_EQ(oshape.Size(), dshape.Size()) \n \n \n \n      << \"Target shape size is different to source. \" \n \n \n \n      << \"Target: \" << oshape \n \n \n \n      << \"\\nSource: \" << dshape; \n \n \n \n  #endif \n \n \n \n  SHAPE_ASSIGN_CHECK(*out_attrs, 0, oshape); \n \n \n \n  return ReverseReshapeInferShape(&(*in_attrs)[0], (*out_attrs)[0]); \n \n \n \n  } \n \n \n \n \n \n The root cause is that MXNDArrayAt, MXNDArrayReshape and MXNDArraySlice do not sufficiently distinguish between the numpy and non-numpy mode and always record legacy operators.\n We need to update the recording step to record numpy / legacy operator based on if numpy / legacy mode is enabled:\n \n \n \n incubator-mxnet/src/ndarray/ndarray.cc\n \n \n         Lines 302 to 308\n       in\n       72eff9b\n \n \n \n \n \n \n  nnvm::NodeAttrs attrs; \n \n \n \n  attrs.op = nnvm::Op::Get(\"Reshape\");; \n \n \n \n  std::ostringstream os; \n \n \n \n  os << shape; \n \n \n \n  attrs.dict.insert({\"shape\", os.str()}); \n \n \n \n  attrs.op->attr_parser(&attrs); \n \n \n \n  std::vector<NDArray*> inputs(1, this), outputs(1, &ret); \n \n \n \n \n \n \t\t"}}}, "commit": {"commit_id": "7c61b4ba6687f61035458afcb0c3ab5d24b6571f", "commit_author": "Leonard Lausen", "commitT": "2020-10-06 11:11:44-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\ndarray\\ndarray.cc", "file_new_name": "src\\ndarray\\ndarray.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "305,306,307,308,309,310,311", "deleted_lines": "303,306", "method_info": {"method_name": "mxnet::NDArray::ReshapeWithRecord", "method_params": "shape", "method_startline": "273", "method_endline": "321"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\python\\unittest\\test_deferred_compute.py", "file_new_name": "tests\\python\\unittest\\test_deferred_compute.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "583,584", "deleted_lines": null, "method_info": {"method_name": "test_indexing_empty_shape.forward", "method_params": "self,x", "method_startline": "583", "method_endline": "584"}}, "hunk_1": {"Ismethod": 1, "added_lines": "580,581,582,583,584,585,586,587,588,589,590,591,592", "deleted_lines": null, "method_info": {"method_name": "test_indexing_empty_shape", "method_params": "", "method_startline": "580", "method_endline": "592"}}}}}}}