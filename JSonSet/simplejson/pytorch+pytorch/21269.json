{"BR": {"BR_id": "21269", "BR_author": "syed-ahmed", "BRopenT": "2019-06-03T02:36:12Z", "BRcloseT": "2019-06-06T17:09:41Z", "BR_text": {"BRsummary": "test_array_adaptor and test_from_cuda_array_interface_active_device fails with numba version 0.44.0", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n test_array_adaptor and test_from_cuda_array_interface_active_device fails with numba version 0.44.0\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Run test_numba_integration with numba 0.44.0\n \n <denchmark-code>Running test_numba_integration ... [2019-06-01 01:10:22.416268]\n test_active_device (__main__.TestNumbaIntegration)\n 'as_cuda_array' tensor device must match active numba context. ... ok\n test_array_adaptor (__main__.TestNumbaIntegration)\n Torch __cuda_array_adaptor__ exposes tensor data to numba.cuda. ... FAIL\n test_conversion_errors (__main__.TestNumbaIntegration)\n Numba properly detects array interface for tensor.Tensor variants. ... ok\n test_cuda_array_interface (__main__.TestNumbaIntegration)\n torch.Tensor exposes __cuda_array_interface__ for cuda tensors. ... ok\n test_from_cuda_array_interface (__main__.TestNumbaIntegration)\n torch.as_tensor() and torch.tensor() supports the __cuda_array_interface__ protocol. ... ok\n test_from_cuda_array_interface_active_device (__main__.TestNumbaIntegration)\n torch.as_tensor() tensor device must match active numba context. ... FAIL\n test_from_cuda_array_interface_lifetime (__main__.TestNumbaIntegration)\n torch.as_tensor(obj) tensor grabs a reference to obj so that the lifetime of obj exceeds the tensor ... ok\n \n ======================================================================\n FAIL: test_array_adaptor (__main__.TestNumbaIntegration)\n Torch __cuda_array_adaptor__ exposes tensor data to numba.cuda.\n ----------------------------------------------------------------------\n Traceback (most recent call last):\n   File \"test_numba_integration.py\", line 123, in test_array_adaptor\n     torch.arange(10).to(dt).numpy()\n AssertionError: TypeError not raised\n \n ======================================================================\n FAIL: test_from_cuda_array_interface_active_device (__main__.TestNumbaIntegration)\n torch.as_tensor() tensor device must match active numba context.\n ----------------------------------------------------------------------\n Traceback (most recent call last):\n   File \"test_numba_integration.py\", line 345, in test_from_cuda_array_interface_active_device\n     torch.as_tensor(numba_ary, device=torch.device(\"cuda\", 1))\n AssertionError: RuntimeError not raised\n \n ----------------------------------------------------------------------\n Ran 7 tests in 2.401s\n \n FAILED (failures=2)\n Traceback (most recent call last):\n   File \"run_test_new.py\", line 434, in <module>\n     main()\n   File \"run_test_new.py\", line 426, in main\n     raise RuntimeError(message)\n RuntimeError: test_numba_integration failed!\n </denchmark-code>\n \n <denchmark-h:h2>Expected behavior</denchmark-h>\n \n Tests in question pass\n <denchmark-h:h2>Environment</denchmark-h>\n \n <denchmark-code>Collecting environment information...\n PyTorch version: 1.2.0a0+0885dd2\n Is debug build: No\n CUDA used to build PyTorch: 10.1.168\n \n OS: Ubuntu 16.04.6 LTS\n GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\n CMake version: version 3.5.1\n \n Python version: 3.6\n Is CUDA available: Yes\n CUDA runtime version: 10.1.168\n GPU models and configuration: \n GPU 0: Tesla V100-DGXS-16GB\n GPU 1: Tesla V100-DGXS-16GB\n GPU 2: Tesla V100-DGXS-16GB\n GPU 3: Tesla V100-DGXS-16GB\n \n Nvidia driver version: 410.48\n cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\n \n Versions of relevant libraries:\n [pip] msgpack-numpy==0.4.3.2\n [pip] numpy==1.16.4\n [pip] torch==1.2.0a0+0885dd2\n [pip] torchtext==0.4.0\n [pip] torchvision==0.2.1\n [conda] magma-cuda100             2.1.0                         5    local\n [conda] mkl                       2019.1                      144  \n [conda] mkl-include               2019.1                      144  \n [conda] torch                     1.2.0a0+0885dd2          pypi_0    pypi\n [conda] torchtext                 0.4.0                    pypi_0    pypi\n [conda] torchvision               0.2.1                    pypi_0    pypi\n </denchmark-code>\n \n <denchmark-h:h2>Additional context</denchmark-h>\n \n Test passes with numba version 0.43.1. Also looks like 0.44.0 was added in conda couple days ago?: <denchmark-link:https://anaconda.org/numba/numba>https://anaconda.org/numba/numba</denchmark-link>\n .\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "syed-ahmed", "commentT": "2019-06-03T13:38:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/madsbk>@madsbk</denchmark-link>\n  do you think you could take a look?\n Not sure if this is an upstream problem or not; would have to read the release notes for 0.44.0.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "syed-ahmed", "commentT": "2019-06-03T14:55:25Z", "comment_text": "\n \t\tSure, I can take a look\n \t\t"}}}, "commit": {"commit_id": "ee15ad1bd614186ce48b776f9bda570a758e20da", "commit_author": "Mads R. B. Kristensen", "commitT": "2019-06-06 10:06:41-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\test_numba_integration.py", "file_new_name": "test\\test_numba_integration.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "120,121,122,123,124,125", "method_info": {"method_name": "test_array_adaptor", "method_params": "self", "method_startline": "105", "method_endline": "165"}}}}}}}