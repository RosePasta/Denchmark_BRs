{"BR": {"BR_id": "3705", "BR_author": "hanhanW", "BRopenT": "2020-11-03T20:40:32Z", "BRcloseT": "2020-11-04T23:18:42Z", "BR_text": {"BRsummary": "Compile error in llvm-ir when fusing gather ops with other ops.", "BRdescription": "\n Hit an issue in LLVM-IR compilation when enable fusion on gather ops and other ops in <denchmark-link:https://github.com/google/iree/pull/3682>#3682</denchmark-link>\n \n With release BUILD, there is a log LLVM IR fails to verify, but a module is still generated. Thus it doesn't crash in compilation time.\n \n \n \n iree/iree/compiler/Dialect/HAL/Target/LLVM/IR/LLVMIRTarget.cpp\n \n \n         Lines 59 to 64\n       in\n       f52156c\n \n \n \n \n \n \n  auto llvmModule = \n \n \n \n  mlir::translateModuleToLLVMIR(targetOp.getInnerModule(), context); \n \n \n \n  \n \n \n \n  if (!llvmModule) { \n \n \n \n  return targetOp.emitError(\"Failed to translate executable to LLVM IR\"); \n \n \n \n  } \n \n \n \n \n \n With debug BUILD, it hit an assertion :\n <denchmark-code>iree-translate: ../third_party/llvm-project/llvm/lib/IR/Instructions.cpp:2296: void llvm::InsertValueInst::init(llvm::Value *, llvm::Value *, ArrayRef<unsigned int>, const llvm::Twine &): Assertion `ExtractValueInst::getIndexedType(Agg->getType(), Idxs) == Val->getType() && \"Inserted value must match indexed type!\"' failed.\n </denchmark-code>\n \n To repro:\n patch <denchmark-link:https://github.com/google/iree/pull/3682>#3682</denchmark-link>\n \n iree-translate -iree-mlir-to-vm-bytecode-module -iree-hal-target-backends=llvm-ir ~/x.mlir -o uni\n Narrow down the test case to:\n module {\n   func @main_ex_dispatch_18(%arg0: tensor<1x10xf32>, %arg1: tensor<1x10xf32>, %arg2: tensor<5x1x1xf32>, %arg3: tensor<i32>) -> tensor<1x10xf32> {\n     %cst = constant dense<1.000000e+01> : tensor<1x10xf32>\n     %cst_0 = constant dense<-1.000000e+01> : tensor<1x10xf32>\n     %cst_1 = constant dense<1.000000e+00> : tensor<1x10xf32>\n     %cst_2 = constant dense<0.000000e+00> : tensor<1x10xf32>\n     %cst_3 = constant dense<5.000000e-01> : tensor<1x10xf32>\n     %0 = \"mhlo.torch_index_select\"(%arg2, %arg3) {batch_dims = 0 : i64, dim = 0 : i64} : (tensor<5x1x1xf32>, tensor<i32>) -> tensor<1x1xf32>\n     %1 = \"mhlo.reshape\"(%0) : (tensor<1x1xf32>) -> tensor<1xf32>\n     %2 = \"mhlo.broadcast_in_dim\"(%1) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<1xf32>) -> tensor<1x10xf32>\n     %3 = mhlo.multiply %2, %cst_1 : tensor<1x10xf32>\n     %4 = \"mhlo.compare\"(%3, %cst_2) {comparison_direction = \"GT\"} : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xi1>\n     %5 = mhlo.multiply %arg1, %cst_3 : tensor<1x10xf32>\n     %6 = \"mhlo.tanh\"(%5) : (tensor<1x10xf32>) -> tensor<1x10xf32>\n     %7 = mhlo.multiply %6, %cst_3 : tensor<1x10xf32>\n     %8 = mhlo.add %7, %cst_3 : tensor<1x10xf32>\n     %9 = mhlo.multiply %8, %arg0 : tensor<1x10xf32>\n     %10 = \"mhlo.select\"(%4, %arg0, %9) : (tensor<1x10xi1>, tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>\n     return %10 : tensor<1x10xf32>\n   }\n }\n The fusion on tensors looks good to me:\n module {\n   func @main_ex_dispatch_18_ex_dispatch_0__num_workgroups__(!shapex.ranked_shape<[1,10]>, !shapex.ranked_shape<[1,10]>, !shapex.ranked_shape<[5,1,1]>, !shapex.ranked_shape<[]>, !shapex.ranked_shape<[1,10]>) -> (index, index, index) attributes {sym_visibility = \"private\"}\n   func @main_ex_dispatch_18_ex_dispatch_0() attributes {hal.num_workgroups_fn = @main_ex_dispatch_18_ex_dispatch_0__num_workgroups__} {\n     %c0 = constant 0 : index\n     %cst = constant 1.000000e+00 : f32\n     %cst_0 = constant 0.000000e+00 : f32\n     %cst_1 = constant 5.000000e-01 : f32\n     %0 = hal.interface.load.tensor @legacy_io::@arg2, offset = %c0 {operand_result_index = 2 : i32} : tensor<5x1x1xf32>\n     %1 = hal.interface.load.tensor @legacy_io::@arg3, offset = %c0 {operand_result_index = 3 : i32} : tensor<i32>\n     %2 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 {operand_result_index = 1 : i32} : tensor<10xf32>\n     %3 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor<10xf32>\n     %4 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor<10xf32>\n     %5 = linalg.indexed_generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%1, %4, %2, %3 : tensor<i32>, tensor<10xf32>, tensor<10xf32>, tensor<10xf32>) {\n     ^bb0(%arg0: index, %arg1: i32, %arg2: f32, %arg3: f32, %arg4: f32):  // no predecessors\n       %6 = mulf %arg3, %cst_1 : f32\n       %7 = tanh %6 : f32\n       %8 = mulf %7, %cst_1 : f32\n       %9 = addf %8, %cst_1 : f32\n       %10 = mulf %9, %arg4 : f32\n       %11 = index_cast %arg1 : i32 to index\n       %12 = extract_element %0[%11, %c0, %c0] : tensor<5x1x1xf32>\n       %13 = mulf %12, %cst : f32\n       %14 = cmpf \"ogt\", %13, %cst_0 : f32\n       %15 = select %14, %arg2, %10 : f32\n       linalg.yield %15 : f32\n     } -> tensor<10xf32>\n     hal.interface.store.tensor %5, @legacy_io::@ret0, offset = %c0 {operand_result_index = 4 : i32} : tensor<10xf32>\n     return\n   }\n Lowering to buffer's world also looks good to me.\n func @main_ex_dispatch_18_ex_dispatch_0() attributes {hal.num_workgroups_fn = @main_ex_dispatch_18_ex_dispatch_0__num_workgroups__} {\n   %0 = iree.placeholder for \"interface buffer\" {binding = @legacy_io::@ret0, operand_result_index = 4 : i32} : memref<10xf32>\n   %c0 = constant 0 : index\n   %cst = constant 1.000000e+00 : f32\n   %cst_0 = constant 0.000000e+00 : f32\n   %cst_1 = constant 5.000000e-01 : f32\n   %1 = iree.placeholder for \"interface buffer\" {binding = @legacy_io::@arg2, operand_result_index = 2 : i32} : memref<5x1x1xf32>\n   %2 = iree.placeholder for \"interface buffer\" {binding = @legacy_io::@arg3, operand_result_index = 3 : i32} : memref<i32>\n   %3 = iree.placeholder for \"interface buffer\" {binding = @legacy_io::@arg1, operand_result_index = 1 : i32} : memref<10xf32>\n   %4 = iree.placeholder for \"interface buffer\" {binding = @legacy_io::@arg0, operand_result_index = 0 : i32} : memref<10xf32>\n   %5 = iree.placeholder for \"interface buffer\" {binding = @legacy_io::@arg0, operand_result_index = 0 : i32} : memref<10xf32>\n   linalg.indexed_generic {indexing_maps = [affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = [\"parallel\"]} ins(%2, %5, %3, %4 : memref<i32>, memref<10xf32>, memref<10xf32>, memref<10xf32>) outs(%0 : memref<10xf32>) {\n   ^bb0(%arg0: index, %arg1: i32, %arg2: f32, %arg3: f32, %arg4: f32, %arg5: f32):  // no predecessors\n     %6 = mulf %arg3, %cst_1 : f32\n     %7 = tanh %6 : f32\n     %8 = mulf %7, %cst_1 : f32\n     %9 = addf %8, %cst_1 : f32\n     %10 = mulf %9, %arg4 : f32\n     %11 = index_cast %arg1 : i32 to index\n     %12 = load %1[%11, %c0, %c0] : memref<5x1x1xf32>\n     %13 = mulf %12, %cst : f32\n     %14 = cmpf \"ogt\", %13, %cst_0 : f32\n     %15 = select %14, %arg2, %10 : f32\n     linalg.yield %15 : f32\n   }\n   return\n }\n <denchmark-link:https://github.com/asaadaldien>@asaadaldien</denchmark-link>\n  Could you help on this?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hanhanW", "commentT": "2020-11-03T20:45:53Z", "comment_text": "\n \t\tThis is the only test (unidirectional_lstm.mlir test) that blocks the PR, btw.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hanhanW", "commentT": "2020-11-04T16:46:06Z", "comment_text": "\n \t\tLooking into this.....\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hanhanW", "commentT": "2020-11-04T22:13:27Z", "comment_text": "\n \t\tShould be fixed by running CSE  after tensor fusion <denchmark-link:https://github.com/google/iree/pull/3722>#3722</denchmark-link>\n   so we load once, resulting a hal interface on buffers that is indexable and don't violate the ABI contract.\n <denchmark-code>   %0 = hal.interface.load.tensor @legacy_io::@arg2, offset = %c0 {operand_result_index = 2 : i32} : tensor<5x1x1xf32>\n     %1 = hal.interface.load.tensor @legacy_io::@arg3, offset = %c0 {operand_result_index = 3 : i32} : tensor<i32>\n     %2 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 {operand_result_index = 1 : i32} : tensor<10xf32>\n     %3 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor<10xf32>\n     %4 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor<10xf32\n </denchmark-code>\n \n ->\n <denchmark-code>   %0 = hal.interface.load.tensor @legacy_io::@arg2, offset = %c0 {operand_result_index = 2 : i32} : tensor<5x1x1xf32>\n     %1 = hal.interface.load.tensor @legacy_io::@arg3, offset = %c0 {operand_result_index = 3 : i32} : tensor<i32>\n     %2 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 {operand_result_index = 1 : i32} : tensor<10xf32>\n     %3 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor<10xf32>\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hanhanW", "commentT": "2020-11-05T07:46:52Z", "comment_text": "\n \t\tThanks for the quick fix, this does fix the issue!\n \t\t"}}}, "commit": {"commit_id": "5251652e920a14a9c26236cf8e1a05a7141e4745", "commit_author": "Ahmed S. Taei", "commitT": "2020-11-04 15:18:41-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "iree\\compiler\\Conversion\\HLOToLinalg\\Passes.cpp", "file_new_name": "iree\\compiler\\Conversion\\HLOToLinalg\\Passes.cpp", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "30", "deleted_lines": null, "method_info": {"method_name": "mlir::iree_compiler::addHLOToLinalgOnBuffersPasses", "method_params": "pm", "method_startline": "25", "method_endline": "32"}}}}}}}