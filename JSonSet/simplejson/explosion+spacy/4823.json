{"BR": {"BR_id": "4823", "BR_author": "kormilitzin", "BRopenT": "2019-12-19T17:04:25Z", "BRcloseT": "2019-12-21T12:07:26Z", "BR_text": {"BRsummary": "CLI spacy train fails with large amount of data", "BRdescription": "\n I am training NER model with 7 categories and the data set contains 200K examples (texts) with average 60K annotated spans per category. However spacy train fails if I use all data. When I randomly subsample, then it works normally. The error I receive when use all data:\n $ python -m spacy train en ....\n \n Training pipeline: ['ner']\n Starting with blank model 'en'\n Counting training words (limit=0)\n Traceback (most recent call last):\n File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n \"main\", mod_spec)\n File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n exec(code, run_globals)\n File \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/main.py\", line 33, in \n plac.call(commands[command], sys.argv[1:])\n File \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 367, in call\n cmd, result = parser.consume(arglist)\n File \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/plac_core.py\", line 232, in consume\n return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n File \"/mnt/sdf/andrey_work/spacy/lib/python3.6/site-packages/spacy/cli/train.py\", line 230, in train\n corpus = GoldCorpus(train_path, dev_path, limit=n_examples)\n File \"gold.pyx\", line 224, in spacy.gold.GoldCorpus.init\n File \"gold.pyx\", line 235, in spacy.gold.GoldCorpus.write_msgpack\n File \"gold.pyx\", line 280, in read_tuples\n File \"gold.pyx\", line 545, in read_json_file\n File \"gold.pyx\", line 592, in _json_iterate\n OverflowError: value too large to convert to int\n \n Is there any way to overcome this problem? Thanks.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kormilitzin", "commentT": "2019-12-20T08:53:56Z", "comment_text": "\n \t\tThat does look like spaCy is crashing on the large training file. Could you provide a little more information to help us look into this:\n \n the exact command you ran\n which spaCy version you're using (from source or installed with pip / conda ? which version number ?)\n how large (in MB or GB) your training file is on disk\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kormilitzin", "commentT": "2019-12-21T08:40:16Z", "comment_text": "\n \t\tThis is a duplicate of <denchmark-link:https://github.com/explosion/spaCy/issues/4703>#4703</denchmark-link>\n . I guess we should add a useful warning and there's really no reason not to change it to .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kormilitzin", "commentT": "2019-12-21T12:07:26Z", "comment_text": "\n \t\tMerging this with <denchmark-link:https://github.com/explosion/spaCy/issues/4703>#4703</denchmark-link>\n !\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "kormilitzin", "commentT": "2020-01-24T12:01:43Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "732142bf2825be61824d453009cc0cea130c3b4b", "commit_author": "Sofie Van Landeghem", "commitT": "2019-12-21 21:12:19+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\errors.py", "file_new_name": "spacy\\errors.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "108,109,110,111", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\gold.pyx", "file_new_name": "spacy\\gold.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "16,560,561,562,563,569,577", "deleted_lines": "16,565,573"}}}}}}