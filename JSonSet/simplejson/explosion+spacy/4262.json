{"BR": {"BR_id": "4262", "BR_author": "lautel", "BRopenT": "2019-09-09T10:44:02Z", "BRcloseT": "2019-09-13T14:37:36Z", "BR_text": {"BRsummary": "Confussing behaviour in Phrase Matcher for Japanese model", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n <denchmark-h:h3>0. Load libraries</denchmark-h>\n \n from spacy.matcher import PhraseMatcher\n from spacy.lang.ja import Japanese\n <denchmark-h:h3>1. Define a Phrase Matcher to find custom entities in Japanese text:</denchmark-h>\n \n def phrase_matcher_test(text):\n \n     nlp = Japanese()\n \n     matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n     patterns = [\"nakagawa jun\", \"\u4e2d\u5ddd \u6f64\", \"nakagawa@xxxx.jp\", \"japan\"]\n     patterns_doc = list(nlp.pipe(patterns))\n     matcher.add(\"ENTITY\", None, *patterns_doc)\n \n     doc = nlp(text)\n \n     matches = matcher(doc)\n     print(f'\\n{len(matches)} matches found!')\n \n     for match_id, start, end in matches:\n         print(doc.vocab.strings[match_id]+': ', doc[start:end].text)\n <denchmark-h:h3>3. Call the function</denchmark-h>\n \n input_text = \"Nakagawa Jun (\u4e2d\u5ddd\u6f64) \u306e\uff92\uff70\uff99\u306fnakagawa@xxxx.jp\u3067\u3059. \u5f7c\u306fJapan\u3067\u50cd\u3044\u3066\u3044\u307e\u3059\"\n phrase_matcher_test(input_text )\n <denchmark-h:h3>4. Result</denchmark-h>\n \n Output:\n 3 matches found!\n ENTITY:  \u4e2d\u5ddd\u6f64\n ENTITY:  <denchmark-link:mailto:nakagawa@xxxx.jp>nakagawa@xxxx.jp</denchmark-link>\n \n ENTITY:  Japan\n Expected output:\n 4 matches found!\n ENTITY: Nakagawa Jun\n ENTITY:  \u4e2d\u5ddd\u6f64\n ENTITY:  <denchmark-link:mailto:nakagawa@xxxx.jp>nakagawa@xxxx.jp</denchmark-link>\n \n ENTITY:  Japan\n <denchmark-h:h3>5. Additional info</denchmark-h>\n \n If\n input_text = \"nakagawa jun (\u4e2d\u5ddd\u6f64) \u306e\uff92\uff70\uff99\u306fnakagawa@xxxx.jp\u3067\u3059. \u5f7c\u306fJapan\u3067\u50cd\u3044\u3066\u3044\u307e\u3059\"\n it almost works as expected (note lowercase in 'nakagawa jun', but not in 'Japan'). By almost I mean that it matches the name but deletes the blank space between name and surname. See the following output.\n Output:\n ENTITY:  nakagawajun\n ENTITY:  \u4e2d\u5ddd\u6f64\n ENTITY:  <denchmark-link:mailto:nakagawa@xxxx.jp>nakagawa@xxxx.jp</denchmark-link>\n \n ENTITY:  Japan\n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n spaCy version: 2.1.8\n Platform: Windows-10-10.0.17134-SP0\n Python version: 3.7.3\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lautel", "commentT": "2019-09-11T13:03:30Z", "comment_text": "\n \t\tHi,\n We've kept doing some test and the example given is working fine in SpaCy version 2.1.4. Hope you find this helpful.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lautel", "commentT": "2019-09-12T07:29:46Z", "comment_text": "\n \t\tThanks for the report! I can only replicate the missing space, not the missing match, but I definitely think there is something going on because the Japanese tokenization tests are failing, too.\n With a clean virtual environment with spacy 2.1.8 and mecab-python3, I get the output:\n <denchmark-code>4 matches found!\n ENTITY:  NakagawaJun\n ENTITY:  \u4e2d\u5ddd\u6f64\n ENTITY:  nakagawa@xxxx.jp\n ENTITY:  Japan\n </denchmark-code>\n \n My enviroment:\n <denchmark-code>blis==0.2.4\n certifi==2019.9.11\n chardet==3.0.4\n cymem==2.0.2\n idna==2.8\n mecab-python3==0.996.2\n murmurhash==1.0.2\n numpy==1.17.2\n pkg-resources==0.0.0\n plac==0.9.6\n preshed==2.0.1\n requests==2.22.0\n spacy==2.1.8\n srsly==0.1.0\n thinc==7.0.8\n tqdm==4.35.0\n urllib3==1.25.3\n wasabi==0.2.2\n </denchmark-code>\n \n <denchmark-link:https://github.com/polm>@polm</denchmark-link>\n , do you have any ideas?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lautel", "commentT": "2019-09-12T07:43:54Z", "comment_text": "\n \t\tCan you try it with mecab-python3==0.7? I think that should be the version in optional requires, newer versions have bugs or dictionary issues.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lautel", "commentT": "2019-09-12T08:38:45Z", "comment_text": "\n \t\tHi, thanks for your quick response!\n So, working with mecab-python3==0.996.2 I have same output as <denchmark-link:https://github.com/adrianeboyd>@adrianeboyd</denchmark-link>\n , both in Windows and Linux environment. However, previously I had mecab-python3==0.7 installed and that's why the first match (NakagawaJun) was missing...\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lautel", "commentT": "2019-09-12T12:04:42Z", "comment_text": "\n \t\tSorry, I was just going by the error message (I guess the error message should be more specific?):\n \n \n \n spaCy/spacy/lang/ja/__init__.py\n \n \n         Lines 27 to 29\n       in\n       4d4b3b0\n \n \n \n \n \n \n  raise ImportError( \n \n \n \n  \"Japanese support requires MeCab: \" \n \n \n \n  \"https://github.com/SamuraiT/mecab-python3\" \n \n \n \n \n \n However, I get the same results with 4 matches and no space with mecab-python3==0.7 and nearly all the tests in tests/lang/ja fail. A few examples are tokenized correctly, some aren't, and I think all the POS and lemma tests fail.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "lautel", "commentT": "2019-09-13T12:15:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/adrianeboyd>@adrianeboyd</denchmark-link>\n  Are you sure you have Unidic installed? If you're using ipadic that would cause the tests to fail. To check, show the output of  and .\n <denchmark-link:https://github.com/lautel>@lautel</denchmark-link>\n  With a clean install of spaCy 2.1.8 and mecab-python3 0.7 on Linux I get this output:\n <denchmark-code>4 matches found!\n ENTITY:  NakagawaJun\n ENTITY:  \u4e2d\u5ddd\u6f64\n ENTITY:  nakagawa@xxxx.jp\n ENTITY:  Japan\n </denchmark-code>\n \n So the space issue is there, but I don't have the other mismatch issue. I'm not sure why you aren't getting Nakagawa Jun as an entity, can you figure out what POS tags it's getting?\n As to why the space is missing, the way Mecab handles half-width spaces is weird. I thought I'd handled it correctly but it's possible I missed something, so I'll look over that code in more detail.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "lautel", "commentT": "2019-09-13T12:53:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/polm>@polm</denchmark-link>\n , I've re-run the example to double-check the output and yes, I'm missing the first match. Anyway, this seems to be okay in mecab-python3==0.996.2\n In case you find it useful, see below further information regarding my environment (Linux):\n python -m spacy info --markdown\n \n spaCy version: 2.1.8\n Platform: Linux-4.9.184-linuxkit-x86_64-with-debian-9.9\n Python version: 3.7.4\n \n echo \"\u56f3\u66f8\u9928\" | mecab\n <denchmark-code>\u56f3\u66f8\u9928  \u540d\u8a5e,\u4e00\u822c,*,*,*,*,\u56f3\u66f8\u9928,\u30c8\u30b7\u30e7\u30ab\u30f3,\u30c8\u30b7\u30e7\u30ab\u30f3\n EOS\n </denchmark-code>\n \n mecab -D\n <denchmark-code>filename:       /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/sys.dic\n version:        102\n charset:        UTF8\n type:   0\n size:   4587310\n left size:      1316\n right size:     1316\n </denchmark-code>\n \n pip list\n <denchmark-code>Package       Version\n ------------- ---------\n blis          0.2.4\n certifi       2019.9.11\n chardet       3.0.4\n cymem         2.0.2\n idna          2.8\n mecab-python3 0.7\n mojimoji      0.0.9\n murmurhash    1.0.2\n numpy         1.17.2\n pip           19.1.1\n plac          0.9.6\n preshed       2.0.1\n requests      2.22.0\n setuptools    41.0.1\n spacy         2.1.8\n srsly         0.1.0\n thinc         7.0.8\n tqdm          4.35.0\n urllib3       1.25.3\n wasabi        0.2.2\n wheel         0.33.4\n </denchmark-code>\n \n Thank you both!\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "lautel", "commentT": "2019-09-13T13:17:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lautel>@lautel</denchmark-link>\n  OK, you're using an IPADic based Neologd dictionary, which won't work. spaCy uses Universal Dependencies, which is based on / only supports Unidic. Please install Unidic and configure Mecab to use it. I would also suggest you avoid using Neologd. This is a reminder that I should probably write a dictionary sniffer to check that Unidic is being used...\n The missing spaces was a real issue; I pushed a fix in <denchmark-link:https://github.com/explosion/spaCy/pull/4284>#4284</denchmark-link>\n . Thanks for finding it!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "lautel", "commentT": "2019-10-13T14:42:53Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "29a9e636eb8b0f22f91eee85b4a10b8cdade4ed2", "commit_author": "Paul OLeary McCann", "commitT": "2019-09-13 16:28:12+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "spacy\\lang\\ja\\__init__.py", "file_new_name": "spacy\\lang\\ja\\__init__.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "96", "deleted_lines": null, "method_info": {"method_name": "__call__", "method_params": "self,text", "method_startline": "95", "method_endline": "105"}}, "hunk_1": {"Ismethod": 1, "added_lines": "62,73,74,75,76,77,78,79,80,81,82,83,84,86", "deleted_lines": "68,78,80", "method_info": {"method_name": "detailed_tokens", "method_params": "tokenizer,text", "method_startline": "57", "method_endline": "86"}}, "hunk_2": {"Ismethod": 1, "added_lines": "40,41,42,43,44", "deleted_lines": null, "method_info": {"method_name": "resolve_pos", "method_params": "token", "method_startline": "33", "method_endline": "54"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\lang\\ja\\tag_map.py", "file_new_name": "spacy\\lang\\ja\\tag_map.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "5,24,25", "deleted_lines": "5"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\tests\\lang\\ja\\test_tokenizer.py", "file_new_name": "spacy\\tests\\lang\\ja\\test_tokenizer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "51,52,53,54,55", "deleted_lines": null, "method_info": {"method_name": "test_extra_spaces", "method_params": "ja_tokenizer", "method_startline": "51", "method_endline": "55"}}}}}}}