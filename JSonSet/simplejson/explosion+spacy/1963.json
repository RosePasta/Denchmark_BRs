{"BR": {"BR_id": "1963", "BR_author": "SandeepNaidu", "BRopenT": "2018-02-09T19:35:52Z", "BRcloseT": "2018-12-30T14:18:19Z", "BR_text": {"BRsummary": "doc.merge() doesn't resize doc.tensor", "BRdescription": "\n I have been getting the following error after I added new components to the pipeline. There is a new entity also added. Any help is much appreciated!\n Also I tried running the example in the github clone and it worked well. The model I am using is a model trained(using prodigy) in the new label on top of en_core_web_lg.\n doc = nlp(text)\n File \"/home/sandeep/anaconda2/lib/python2.7/site-packages/spacy/language.py\", line 333, in call\n doc = proc(doc)\n File \"nn_parser.pyx\", line 331, in spacy.syntax.nn_parser.Parser.call\n File \"nn_parser.pyx\", line 762, in spacy.syntax.nn_parser.Parser.set_annotations\n File \"doc.pyx\", line 846, in spacy.tokens.doc.Doc.extend_tensor\n File \"/home/sandeep/anaconda2/lib/python2.7/site-packages/numpy/core/shape_base.py\", line 288, in hstack\n <denchmark-code>return _nx.concatenate(arrs, 1)\n </denchmark-code>\n \n ValueError: all the input array dimensions except for the concatenation axis must match exactly\n My component class is as follows. I have one component class for each of the 4 labels (one of the labels is new one).\n <denchmark-code>class GazzeteerComponent(object):\n     def __init__(self,nlp,entityname):\n         self.vocab = nlp.vocab\n         self.entityname = entityname\n         self.label = nlp.vocab.strings[entityname]\n         self.__name__ = entityname+'GazzeteerComponent'\n         self.matcher = PhraseMatcher(nlp.vocab)\n         self.nlp = nlp\n         patterns = self.getpatterns(entityname)\n         exceptioncount=0\n         for pattern in patterns:\n             try:\n                 self.matcher.add(entityname,None,pattern)\n             except ValueError as e:\n                 exceptioncount+=1\n                 # LOGGER_Generic.error(\"Exception {} for pattern {} for entity {}\".format(e,pattern,entityname))\n \n     def getpatterns(self,entityname):\n         fname = 'patterns_{}.list'.format(entityname)\n         with open(fname) as entityf:\n             alldict = [ensureUtf(text.strip()) for text in entityf.readlines()]\n \n         patterns = [self.nlp.make_doc(pattern) for pattern in alldict]\n         return patterns\n \n     def __call__(self,doc):\n         matches = self.matcher(doc)\n         spans = []\n         try:\n             for _, start, end in matches:\n                 entity = Span(doc,start, end, label=self.label)\n                 spans.append(entity)\n                 doc.ents = list(doc.ents) + [entity]\n             for span in spans:\n                 span.merge()\n         except Exception as e:\n             LOGGER_Generic.info(\"Exception in GazzeteerComponent {}\".format(e))\n \n         return doc\n </denchmark-code>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n <denchmark-h:h2>Info about spaCy</denchmark-h>\n \n \n \n Python version: 2.7.13\n \n \n Platform: Linux-4.4.0-104-generic-x86_64-with-debian-jessie-sid\n \n \n spaCy version: 2.0.2\n \n \n Models: en_core_web_lg, en\n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "SandeepNaidu", "commentT": "2018-02-10T04:42:49Z", "comment_text": "\n \t\tFound that this is happening if ner component is after the PhraseMatcher component. If I put PhraseMatchers after NER component it works fine. Is this a bug that needs a fix?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "SandeepNaidu", "commentT": "2018-02-10T12:04:17Z", "comment_text": "\n \t\tThanks.\n Yes, this is a bug: during the pipeline there's a doc.tensor attribute that has one row per token. The .merge() method should be modifying this tensor, but isn't. Subsequent pipeline components then can't extend the tensor, because the number of tokens is wrong.\n To work around the bug, you could run your matcher component first in the pipeline, or last. You could also have your component set doc.tensor = None, to avoid the problem.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "SandeepNaidu", "commentT": "2018-02-12T07:35:02Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n . Workaround works. But how do I overwrite an entity assigned by previous component? In the example code it is about adding to the existing entities. I am using phrasematcher to boost confidence to reduce false positives in ner component or previous phrasematchers.\n # Overwrite doc.ents and add entity \u2013 be careful not to replace!\n doc.ents = list(doc.ents) + [entity]\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "SandeepNaidu", "commentT": "2018-02-12T09:35:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SandeepNaidu>@SandeepNaidu</denchmark-link>\n  If you actually  replace the entities assigned by previous components, you can also just write to  and replace it with your own list. The main reason we've added the \"be careful not to replace!\" comment is that in most cases, users don't actually want that \u2013 so setting  is a common mistake that can easily lead to confusing results.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "SandeepNaidu", "commentT": "2018-02-13T05:39:49Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/ines>@ines</denchmark-link>\n . That assumes that we find total list of entities in the current component. However I might have ent1(label1) and ent2(label1) detected in the previous components. And ent2(label2) is detected in the current component for a different label. I want to retain ent1(label1) and overwrite ent2(label1) with ent2(label2). With the above method I think that does not happen. I wrote some code and it does not seem to work that way. I am still debugging but I am thinking this is what is happening.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "SandeepNaidu", "commentT": "2018-02-13T06:59:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SandeepNaidu>@SandeepNaidu</denchmark-link>\n  The existing entities in  are also regular s, so you could filter them within your custom component? For example, you could keep all entities whose text  equal to the text of the entity your new component just detected, and add your new entity instead:\n entity = Span(doc,start, end, label=self.label)  # the new entity detected by your custom component\n new_ents = [ent for ent in doc.ents if ent.text != entity.text] + [entity]\n doc.ents = new_ents\n You could also check the ent.label_ and compare that to the new label \u2013 for example, to only overwrite entities which your custom component assigned a different label to. (I hope I understood your question correctly btw!)\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "SandeepNaidu", "commentT": "2018-02-13T09:53:56Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ines>@ines</denchmark-link>\n  Got it. Thanks for the answer. I am using below code to add a filtering component which excludes unwanted labels, gives more control on priority assignment. Pasting it here.\n <denchmark-code>class FilterEntsComponent(object):\n     def __call__(self,doc):\n         allents = list()\n         entitytaglist = ['ORG','PRODUCT','SKILL', 'PERSON']\n         excludelist = ['WORK_OF_ART']\n         for ent in doc.ents:\n             if ent.label_ in excludelist:\n                 continue\n          \n             if ent.label_ not in entitytaglist:\n                 entitytaglist.append(ent.label_)\n             allents.append((ent.start, ent.end,ent.label_,entitytaglist.index(ent.label_),ent))\n \n         allents = pd.DataFrame(allents)\n         allents.sort_values([0,3],inplace=True)\n         allents.drop_duplicates(0,inplace=True)\n         filteredents = allents[4].tolist()\n         doc.ents = filteredents\n         return doc\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "SandeepNaidu", "commentT": "2018-03-26T15:09:10Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n  and <denchmark-link:https://github.com/ines>@ines</denchmark-link>\n , thank you very much for your great, really appreciated work!\n I'm facing the very same problem discussed here. However, I cannot put my component first or last in the pipeline. I'm trying to put it right after the , but before the . In fact, my component needs tag and pos annotations to perform some checks before constructing the dependency tree.\n Can you suggest me a workaround if the problem is not solved yet?\n Thank you!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "SandeepNaidu", "commentT": "2018-03-27T11:21:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alanramponi>@alanramponi</denchmark-link>\n  Thanks! What exactly does your custom component do? Are you also matching spans and merging tokens? The error mentioned above is caused by the tensor not being modified correctly by the  method. Does the following work for you?\n \n You could also have your component set doc.tensor = None, to avoid the problem.\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "SandeepNaidu", "commentT": "2018-03-27T11:41:56Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/ines>@ines</denchmark-link>\n , my custom component performs merging of tokens using the  on some linguistic features including POS tags. It is a kind of chunker based on all the available annotations before parsing and ner modules. This choice helps a lot for my use case in building better dependency parses.\n If I set doc.tensor = None, I get the following error:\n <denchmark-code>File \"/[MY_LIB_PATH]/python3.6/site-packages/spacy/language.py\", line 341, in __call__ doc = proc(doc)\n File \"nn_parser.pyx\", line 338, in spacy.syntax.nn_parser.Parser.__call__\n File \"nn_parser.pyx\", line 786, in spacy.syntax.nn_parser.Parser.set_annotations\n File \"doc.pyx\", line 875, in spacy.tokens.doc.Doc.extend_tensor\n AttributeError: 'NoneType' object has no attribute 'size'\n </denchmark-code>\n \n However, I found a temporary workaround, i.e., setting doc.tensor = numpy.zeros((0,), dtype='float32') instead of doc.tensor = None.\n Hope it can help!\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "SandeepNaidu", "commentT": "2018-03-27T11:48:28Z", "comment_text": "\n \t\tAh, yes, this makes sense \u2013 thanks a lot for sharing your workaround!\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "SandeepNaidu", "commentT": "2018-12-20T15:10:52Z", "comment_text": "\n \t\tThe code for the retokenization lives in spacy/tokens/_retokenize.pyx. It should be pretty easy to collapse the tensor rows for the merged regions. We probably want to just use the last row in the merged region as the row for the whole region.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "SandeepNaidu", "commentT": "2018-12-30T14:18:19Z", "comment_text": "\n \t\tFixed!\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "SandeepNaidu", "commentT": "2019-01-29T14:53:24Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "72e4d3782a32b6b8fac12b74315b85f71aadcfe8", "commit_author": "Matthew Honnibal", "commitT": "2018-12-30 15:17:17+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\tests\\regression\\test_issue1501-2000.py", "file_new_name": "spacy\\tests\\regression\\test_issue1501-2000.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "250,251,252,253,254,255,256,257", "deleted_lines": null, "method_info": {"method_name": "test_issue1963", "method_params": "en_tokenizer", "method_startline": "250", "method_endline": "257"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\_retokenize.pyx", "file_new_name": "spacy\\tokens\\_retokenize.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "10,12,88,89,90,91,92,192,193,194,195,196,197,293,294,295,296,297,298,299,300,301", "deleted_lines": "185,279"}}}}}}