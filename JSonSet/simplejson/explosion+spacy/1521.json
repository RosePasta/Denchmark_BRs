{"BR": {"BR_id": "1521", "BR_author": "davidgengenbach", "BRopenT": "2017-11-09T00:59:19Z", "BRcloseT": "2017-11-09T01:29:25Z", "BR_text": {"BRsummary": "Doc.to_disk() exception", "BRdescription": "\n First of all, thanks for your library!\n I have a problem serializing docs. Here is a minimal example of what I want to do:\n import spacy\n nlp = spacy.load('en')\n texts = ['The brown fox jumped over the fence.', 'The fence was not that tall anyways.']\n docs = nlp.pipe(texts)\n # Here I want to save the docs to the disk\n list(docs)[0].to_disk('some_file')\n but I get the following exception:\n <denchmark-code>---------------------------------------------------------------------------\n AttributeError                            Traceback (most recent call last)\n <ipython-input-24-449d2fe86bdf> in <module>()\n       4 texts = ['The brown fox jumped over the fence.', 'The fence was not that tall anyways.']\n       5 docs = nlp.pipe(texts)\n ----> 6 list(docs)[0].to_disk('some_file')\n \n doc.pyx in spacy.tokens.doc.Doc.to_disk()\n \n AttributeError: 'str' object has no attribute 'open'\n </denchmark-code>\n \n What is the correct way to save multiple documents when using nlp.pipe(texts)? I also tried doing it like this:\n import spacy\n import pickle\n \n nlp = spacy.load('en')\n texts = ['The brown fox jumped over the fence.', 'The fence was not that tall anyways.']\n docs = nlp.pipe(texts)\n docs_b = [doc.to_bytes() for doc in docs]\n with open('some_file', 'wb') as f:\n     pickle.dump(docs_b, f)\n But this produces quite large files (I assume it also saves the model with it?).\n Again, thanks for the library, your work and maybe answering this question! \ud83d\udc4d\n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n spaCy version: 2.0.2\n Platform: Darwin-17.0.0-x86_64-i386-64bit (macOS 10.13)\n Python version: 3.6.2\n Models: en\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "davidgengenbach", "commentT": "2017-11-09T01:06:27Z", "comment_text": "\n \t\tAh, I am sorry. I just saw that just pickling the Docs now works with spaCy 2.0.\n I am leaving this issue nevertheless - maybe somebody also runs into this problem when migrating to spaCy 2.0.\n import spacy\n import pickle\n \n nlp = spacy.load('en')\n texts = ['The brown fox jumped over the fence.', 'The fence was not that tall anyways.']\n docs = nlp.pipe(texts)\n with open('some_file', 'wb') as f:\n     pickle.dump(list(docs), f)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "davidgengenbach", "commentT": "2017-11-09T01:23:25Z", "comment_text": "\n \t\tThanks! It looks like there's a bug in doc.to_disk(): it's expecting a pathlib.Path() object, when it should also accept a string.\n In general I think you'll find the doc.to_bytes() method to be the best way to serialize Doc objects. It lets you do the i/o, and it should be faster and smaller than pickle. Another option if you're storing large datasets is to use doc.to_array(), picking only the attributes you need.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "davidgengenbach", "commentT": "2017-11-09T11:28:22Z", "comment_text": "\n \t\tThanks for answering so quickly and for the quick bug fix!\n I still have a little problem with the file size of the serialized docs - here is a graph of what I mean:\n <denchmark-link:https://user-images.githubusercontent.com/2621491/32603526-543c9acc-c54a-11e7-9921-8bf1f8654661.png></denchmark-link>\n \n Both the doc.to_bytes() and the plain saving with pickle create quite big files! For just 500 documents, the size of the serialized docs is above 400mb! Is there a way to serialize the documents all at once without such big file sizes?\n EDIT: I oversaw your other comment about the doc.to_array() way. Thanks a lot for answering!\n (<denchmark-link:https://gist.github.com/davidgengenbach/6550d1bf61ee2e400d941f0e496891bf>Here</denchmark-link>\n  is the somewhat verbose code to plot this graph)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "davidgengenbach", "commentT": "2017-11-11T04:55:30Z", "comment_text": "\n \t\tIf I use the doc.to_bytes() to export the doc, do I still have to export the Vocab object separately from the Doc or is it included?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "davidgengenbach", "commentT": "2017-11-19T20:46:01Z", "comment_text": "\n \t\tfwiw I'm also running into some of the issues described above. Previously, users could write multiple Docs to a single file with\n <denchmark-code>with open(filepath, mode='wb') as f:\n     for doc in docs:\n         f.write(doc.to_bytes())\n </denchmark-code>\n \n then read those docs back in via Doc.read_bytes(), which took care of segmenting the file's bytes into individual Docs:\n <denchmark-code>docs = []\n with open(filepath, mode='rb') as f:\n     for bytes_string in Doc.read_bytes(f):\n         docs.append(Doc(Vocab).from_bytes(bytes_string))\n </denchmark-code>\n \n But this no longer works in spacy v2, since Doc.read_bytes() has been removed, and the individual docs' byte strings contain newlines, so this can't be done easily in the style of JSONL files.\n Is there a recommended way to save multiple Docs together in the same file? As far as I can tell, the documentation only covers the single Doc case.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "davidgengenbach", "commentT": "2017-11-27T03:53:27Z", "comment_text": "\n \t\tAfter looking into the Doc.to_bytes() and Doc.from_bytes() source code, I don't see any decent way to write+read multiple Docs as bytes from the same file. (Full disclosure: I'm not experienced with msgpack, and a couple hours with the docs and examples of its usage in the wild didn't prove particularly insightful.)\n As I wrote above, it's straightforward to write multiple docs-as-bytes out to the same file, but the old way of reading the data back in no longer works, and the way the methods are written (with corresponding util.to_bytes(), util.from_bytes() functions) seems to force a one-Doc-per-file use case.\n For example, this too-easy option doesn't work:\n <denchmark-code>docs = []\n with io.open(fname, mode='rb') as f:\n     for line in f:\n         docs.append(Doc(Vocab()).from_bytes(line))\n </denchmark-code>\n \n Although you can use msgpack.Unpacker to iterate over the stored objects one-by-one, these objects are unpacked dicts incompatible with the expected inputs for the existing Doc methods:\n <denchmark-code>with io.open(fname, mode='rb') as f:\n     unpacker = msgpack.Unpacker(f)\n     for obj in unpacker:\n         print(obj.keys())\n         break\n </denchmark-code>\n \n Here,  => . I believe this is equivalent to  in <denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/tokens/doc.pyx#L803>this line</denchmark-link>\n  midway through , since the deserializers are all null-op lambdas and the docs don't have any user_data. But to initialize new docs from these dicts, I'd have to effectively duplicate much of the code elsewhere in , which seems like a bad idea.\n I looked to  <denchmark-link:https://pandas.pydata.org/pandas-docs/stable/io.html#msgpack>for inspiration</denchmark-link>\n , but <denchmark-link:https://github.com/pandas-dev/pandas/tree/master/pandas/io/msgpack>the code</denchmark-link>\n  was sufficiently complicated that I didn't get very far.\n I should note that pickling does work, which is great, but you have to write all the docs out at once as a list (load them all into memory...), and the file size is surprisingly large, as somebody pointed out above.\n This issue is my last blocker to making textacy compatible with v2, and I haven't found any good options. Maybe (hopefully) I've missed something in the docs, or github, or gitter chat.\n <denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n  , do you have any suggestions? \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "davidgengenbach", "commentT": "2018-05-08T06:55:19Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "1c218397f69beeb895f357662d718d627842a251", "commit_author": "ines", "commitT": "2017-11-09 02:29:03+01:00", "changed_files": {"file_0": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "spacy\\tests\\serialize\\test_serialize_doc.py"}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\doc.pyx", "file_new_name": "spacy\\tokens\\doc.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "740,752", "deleted_lines": null}}}}}}