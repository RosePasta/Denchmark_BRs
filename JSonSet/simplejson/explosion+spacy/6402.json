{"BR": {"BR_id": "6402", "BR_author": "alvaroabascar", "BRopenT": "2020-11-18T09:03:46Z", "BRcloseT": "2020-12-29T10:55:10Z", "BR_text": {"BRsummary": "doc.user_token_hooks are removed when docs are produced with nlp.pipe() using n_process &gt; 1", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n I do not know whether this is a known behavior or not, but I found the following: when customizing doc.user_token_hooks, in my case by doc.user_token_hooks['vector'] = user_token_hook_vector, this hook is lost when the docs are produced by calling nlp.pipe(texts, n_process=n) where n > 1. If n == 1, the hooks are preserved. Checking the implementation of Language.pipe and Language._multiprocessing_pipe I see that this is due to the serializing/deserializing of docs into bytes (see the second snippet).\n <denchmark-h:h4>First snippet: user_token_hooks lost after processing with .pipe(n_process=2)</denchmark-h>\n \n import spacy\n \n def user_token_hook_vector(token):\n     print('Hook called!', flush=True)\n     vocab = token.doc.vocab\n     return vocab.get_vector(token.text)\n \n def pipe_add_hook(doc):\n     doc.user_token_hooks['vector'] = user_token_hook_vector\n     return doc\n \n nlp = spacy.load('en_core_web_md')\n nlp.add_pipe(pipe_add_hook, first=True)\n \n doc = nlp('hi there')\n doc[0].vector\n # prints \"Hook called!\"\n print(doc.user_token_hooks)\n # prints {'vector': <function user_token_hook_vector at 0x7...>}\n \n docs = list(nlp.pipe(['hi there']))\n docs[0][0].vector\n # prints \"Hook called!\"\n print(docs[0].user_token_hooks)\n # prints {'vector': <function user_token_hook_vector at 0x7...>}\n \n docs = list(nlp.pipe(['hi there!'], n_process=2)) #os.cpu_count()))\n docs[0][0].vector\n # does not print\n print(docs[0].user_token_hooks)\n # prints {}\n <denchmark-h:h4>Second snippet: hook is lost when serializing/deserializing doc as bytes.</denchmark-h>\n \n import spacy\n from spacy.tokens import Doc\n \n def user_token_hook_vector(token):\n     print('Hook called!', flush=True)\n     vocab = token.doc.vocab\n     return vocab.get_vector(token.text)\n \n def pipe_add_hook(doc):\n     doc.user_token_hooks['vector'] = user_token_hook_vector\n     return doc\n \n nlp = spacy.load('en_core_web_md')\n nlp.add_pipe(pipe_add_hook, first=True)\n \n doc = nlp('hi there')\n doc[0].vector\n # prints \"Hook called!\"\n print(doc.user_token_hooks)\n # prints '{vector': <function user_token_hook_vector at 0x7...>}\n \n doc_bytes = doc.to_bytes()\n doc2 = Doc(doc.vocab).from_bytes(doc_bytes)\n doc2.vector\n # no print\n print(doc2.user_token_hooks)\n # prints {}\n <denchmark-h:h2>Environment</denchmark-h>\n \n \n spaCy version: 2.3.2\n Platform: Linux-4.15.0-123-generic-x86_64-with-Ubuntu-18.04-bionic\n Python version: 3.7.5\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "alvaroabascar", "commentT": "2020-11-19T19:36:22Z", "comment_text": "\n \t\tHmm, this is a bug, but the problem is that there's no good way to serialize the user hooks for multiprocessing here. They are serialized if you pickle a doc, but we only want to use to_bytes here to avoid the security problems related to pickle.\n We should see if it's possible add a warning or an error when user hooks are set so that you know that you need to handle the multiprocessing at a higher level rather than with nlp.pipe.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "alvaroabascar", "commentT": "2020-11-20T11:19:29Z", "comment_text": "\n \t\tMaybe the hooks could be sent along with the bytes somehow?\n In my case, by now I'm resetting the hooks after processing:\n docs = nlp.pipe(texts, n_process=2)\n for doc in docs:\n     doc.user_token_hook[\"vector\"] = my_hook\n However this does not solve the problem if the hooks are required by some of the components in the pipeline.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "alvaroabascar", "commentT": "2020-11-20T12:03:13Z", "comment_text": "\n \t\tHmm, maybe it's possible to restructure things so you have a custom pipeline component at the beginning of the pipeline that sets the user hooks?\n There's not going to be a way to include it with doc.to_bytes, though.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "alvaroabascar", "commentT": "2020-11-23T14:25:04Z", "comment_text": "\n \t\tIf you take a look at the snippet I provided, there is already a custom pipe which sets the user hook, but still they are lost. Now I'm wondering whether they are lost at some point during the execution of the pipeline, or when the pipeline has finished and the docs are being returned from the child processes to the main one. I guess it's the latter.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "alvaroabascar", "commentT": "2020-11-23T14:26:09Z", "comment_text": "\n \t\tAlso, even if it is not included in doc.to_bytes, maybe this sending of the hooks along with the bytes could be implemented just for Language.pipe?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "alvaroabascar", "commentT": "2020-11-24T06:58:57Z", "comment_text": "\n \t\tAh, you're right. I was thinking about whether subsequent pipeline components have access to the hooks when you add it with an initial component while multiprocessing, not whether the doc is returned with the hook intact. If you add the hooks like you do above, they should be available to later components in the pipeline. Here's an example:\n import spacy\n \n def user_token_hook_vector(token):\n     print('Hook called!', token.text, flush=True)\n     return token.doc.vocab.get_vector(token.text)\n \n def pipe_add_hook(doc):\n     print(\"add hook\")\n     doc.user_token_hooks['vector'] = user_token_hook_vector\n     return doc\n \n def pipe_access_hook(doc):\n     print(\"access hook\")\n     for token in doc:\n         v = token.vector\n     return doc\n \n nlp = spacy.load('en_core_web_md')\n nlp.add_pipe(pipe_add_hook, first=True)\n nlp.add_pipe(pipe_access_hook)\n \n print(\"With call:\\n\")\n \n doc = nlp('hi there')\n \n print(\"\\nWith pipe with multiprocessing:\\n\")\n \n docs = list(nlp.pipe(['hi there'], n_process=2))\n Output:\n <denchmark-code>With call:\n \n add hook\n access hook\n Hook called! hi\n Hook called! there\n \n With pipe with multiprocessing:\n \n add hook\n access hook\n Hook called! hi\n Hook called! there\n </denchmark-code>\n \n I do think the best workaround is to add the hook to the doc again after calling nlp.pipe.\n I can also imagine that you could implement an alternate way of defining hooks with registered functions that make them easier to serialize, but this would be a major change from how things currently work.\n There's just really no acceptable way to serialize a function with to_bytes. If you want to, you can save your Doc objects with user hooks with pickle, but we don't want to use pickle in the serialization here. (As a warning, along with being insecure pickle is going to be really inefficient. It would be smaller and faster to use DocBin and re-add the hooks after extracting the docs, similar to how you can add them after nlp.pipe.)\n I do still think there should be a warning or error here so this behavior isn't a surprise, but I don't think we can fix it given the way user hooks are implemented right now.\n (Or are you expecting your user hook vectors to be accessed when the tagger and other pretrained components are run? That's not going to change with or without multiprocessing and it would require a completely different explanation about what's going on behind the scenes with the vectors data.)\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "alvaroabascar", "commentT": "2020-11-30T11:31:48Z", "comment_text": "\n \t\tSo so far we have seen that:\n \n Hooks are preserved within the pipeline. That's great! It means that pipes depending on a hook will not fail when using multiprocessing.\n Hooks are lost in the returned doc, but we can set them again after nlp.pipe.\n \n Given this, it would be ideal if nlp.pipe could somehow re-add the hooks before returning, so that the process would be transparent to the user and she wouldn't have to add the hooks again. But I'm guessing this is not possible.\n Then again, maybe we should add a warning in the documentation. Would it be possible to also detect these cases in running code to raise a warning?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "alvaroabascar", "commentT": "2020-12-29T10:55:10Z", "comment_text": "\n \t\tRuntime warning added by PR <denchmark-link:https://github.com/explosion/spaCy/pull/6595>#6595</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "5ca57d822140a84bb790496f3b797e8374573b95", "commit_author": "Adriane Boyd", "commitT": "2020-12-29 11:54:32+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\errors.py", "file_new_name": "spacy\\errors.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "126,127", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "spacy\\tests\\doc\\test_doc_api.py", "file_new_name": "spacy\\tests\\doc\\test_doc_api.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "152,153", "deleted_lines": null, "method_info": {"method_name": "test_doc_api_serialize.inner_func", "method_params": "d1,d2", "method_startline": "152", "method_endline": "153"}}, "hunk_1": {"Ismethod": 1, "added_lines": "152,153,154,155,156,157,158,159,160,161", "deleted_lines": null, "method_info": {"method_name": "test_doc_api_serialize", "method_params": "en_tokenizer,text", "method_startline": "122", "method_endline": "161"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\tokens\\doc.pyx", "file_new_name": "spacy\\tokens\\doc.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1276,1277,1654", "deleted_lines": "1652"}}}}}}