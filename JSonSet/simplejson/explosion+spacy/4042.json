{"BR": {"BR_id": "4042", "BR_author": "psychosis448", "BRopenT": "2019-07-29T17:57:52Z", "BRcloseT": "2019-09-27T18:57:14Z", "BR_text": {"BRsummary": "Adding EntityRuler before ner and saving model to disk crashes loading the model", "BRdescription": "\n <denchmark-h:h2>How to reproduce the behaviour</denchmark-h>\n \n I got a strange problem when loading a modified model, adding an EnityRuler to the pipeline with the before='ner' flag.\n So I create my patterns, add them to my entity_ruler and the ruler to my pipe:\n nlp = spacy.load('de_core_news_sm')\n ruler = EntityRuler(nlp)\n ruler.add_patterns(patterns)\n nlp.add_pipe(ruler, before='ner')\n Then I check for the dir, create it if necessary and save the model to disk:\n output_dir = Path('custom_model')\n if not output_dir.exists():\n   output_dir.mkdir()\n nlp.to_disk(output_dir)\n When I now load the model with\n <denchmark-code>spacy.load(Path('custom_model'))\n </denchmark-code>\n \n It throws the following error:\n ValueError: [E109] Model for component 'ner' not initialized.\n Did you forget to load a model, or forget to call begin_training()?\n There is no problem doing this without before='ner'...\n Extended Error:\n Traceback (most recent call last):\n   File \"/spacy/__init__.py\", line 27, in load\n     return util.load_model(name, **overrides)\n   File \"/spacy/util.py\", line 135, in load_model\n     return load_model_from_path(name, **overrides)\n   File \"/spacy/util.py\", line 173, in load_model_from_path\n     return nlp.from_disk(model_path)\n   File \"/spacy/language.py\", line 791, in from_disk\n     util.from_disk(path, deserializers, exclude)\n   File \"/spacy/util.py\", line 630, in from_disk\n     reader(path / key)\n   File \"/spacy/language.py\", line 787, in <lambda>\n     deserializers[name] = lambda p, proc=proc: proc.from_disk(p, exclude=[\"vocab\"])\n   File \"/spacy/pipeline/entityruler.py\", line 183, in from_disk\n     self.add_patterns(patterns)\n   File \"/spacy/pipeline/entityruler.py\", line 138, in add_patterns\n     self.phrase_patterns[label].append(self.nlp(pattern))\n   File \"/spacy/language.py\", line 390, in __call__\n     doc = proc(doc, **component_cfg.get(name, {}))\n   File \"nn_parser.pyx\", line 205, in spacy.syntax.nn_parser.Parser.__call__\n   File \"nn_parser.pyx\", line 238, in spacy.syntax.nn_parser.Parser.predict\n   File \"nn_parser.pyx\", line 235, in spacy.syntax.nn_parser.Parser.require_model\n ValueError: [E109] Model for component 'ner' not initialized.\n Did you forget to load a model, or forget to call begin_training()?\n <denchmark-h:h2>Your Environment</denchmark-h>\n \n \n spaCy version: 2.1.6\n Platform: Darwin-18.6.0-x86_64-i386-64bit\n Python version: 3.7.3\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "psychosis448", "commentT": "2019-07-29T18:05:51Z", "comment_text": "\n \t\tThanks for opening the issue  Copying over my suspected explanation from <denchmark-link:https://stackoverflow.com/questions/57099240/adding-entityruler-before-ner-and-saving-model-to-disk-crashes-loading-the-model?noredirect=1#comment100963749_57099240>Stack Overflow</denchmark-link>\n :\n \n This looks like it might be a bug: when your entity ruler is loaded back and the patterns are put together spaCy uses the existing nlp object to recreate the phrase patterns. That nlp object contains the still uninitialized ner component (because that's only initialized in the next step, after the entity ruler).\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "psychosis448", "commentT": "2019-08-02T15:07:27Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ines>@ines</denchmark-link>\n  : I tried addressing this by moving some functionality from  to , so that each component would be created, added and deserialized one by one. Cf. PR <denchmark-link:https://github.com/explosion/spaCy/pull/4072>#4072</denchmark-link>\n \n It's still a little ugly, but unfortunately also not working. Weirdly enough, now it doesn't crash (anymore) on deserializing the EntityRuler, but it does crash when trying to deserialize the ner's model, with a thinc complaint about array dimensions:\n \n test_issue4042.py:38:\n \n ...._init_.py:27: in load\n return util.load_model(name, **overrides)\n ....\\util.py:138: in load_model\n return load_model_from_path(name, **overrides)\n ....\\util.py:171: in load_model_from_path\n pipeline=meta.get(\"pipeline\", []),\n ....\\language.py:847: in from_disk\n util.from_disk(path, proc_deserializers, exclude=exclude)\n ....\\util.py:629: in from_disk\n reader(path / key)\n ....\\language.py:845: in \n p, exclude=[\"vocab\"]\n nn_parser.pyx:635: in spacy.syntax.nn_parser.Parser.from_disk\n ???\n C:\\Users\\Sofie\\Anaconda3\\envs\\coref\\lib\\site-packages\\thinc\\neural_classes\\model.py:376: in from_bytes\n copy_array(dest, param[b\"value\"])\n \n dst = array([[-0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,\n -0.,  0.,  0., -0., -0.,  0.,  0., -0...0.,  0., -0., -0., -0.,  0.,\n -0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.,  0., -0.]],\n dtype=float32)\n src = array([[ 0., -0.,  0., -0., -0., -0.,  0., -0.,  0.,  0., -0.,  0., -0.,\n 0., -0., -0., -0.,  0., -0., -0.,  0...0.,  0.,  0.,  0.,  0.,  0.,\n 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n dtype=float32)\n casting = 'same_kind', where = None\n def copy_array(dst, src, casting=\"same_kind\", where=None):\n if isinstance(dst, numpy.ndarray) and isinstance(src, numpy.ndarray):\n dst[:] = src\n ValueError: could not broadcast input array from shape (9,64) into shape (5,64)\n C:\\Users\\Sofie\\Anaconda3\\envs\\coref\\lib\\site-packages\\thinc\\neural\\util.py:124: ValueError\n \n Not sure I'm on the right track to fixing the original issue, but also not sure how to fix the new issue :|\n If you change the unit test to nlp.add_pipe(ruler, after=\"ner\")  it works fine both in the original code base as well as in this PR.\n I'm thinking there's some complex deserialization logic that I'm still not quite grasping... if you have any pointers or ideas I'd be happy to look into this further :-)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "psychosis448", "commentT": "2019-08-02T17:52:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/svlandeg>@svlandeg</denchmark-link>\n  Thanks for looking into this \u2013 I'll take a look!\n Another potential (and probably easier) fix for this particular issue might be to just disable all pipeline components that are supposed to run after the entity ruler in the pipeline. I think the EntityRuler has access to the whole nlp object, so it could check the nlp.pipe_names and then add the phrase patterns within a with nlp.disable_pipes block. But there might still be some edge cases that I'm not considering here \u2013 I'm not 100% sure \ud83e\udd14\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "psychosis448", "commentT": "2019-08-02T21:21:45Z", "comment_text": "\n \t\tCould definitely do that. But then every component that ever accesses the nlp object during initialization will have to make sure to follow that procedure. I was hoping to provide a more generic fix but it does make the deserialization code a little uglier...\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "psychosis448", "commentT": "2019-08-03T08:50:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/svlandeg>@svlandeg</denchmark-link>\n  Yeah, good point. I guess in a way, actually   within a pipeline component is sort of an antipattern. We just have to do it in the case of the entity ruler, because we can't make any assumptions about the user's pipeline and what they're doing to their s that might be relevant to create the phrase patterns.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "psychosis448", "commentT": "2019-08-03T11:18:29Z", "comment_text": "\n \t\tOh weird stuff <denchmark-link:https://github.com/ines>@ines</denchmark-link>\n . Now I implemented your solution which is indeed a much easier fix: PR <denchmark-link:https://github.com/explosion/spaCy/pull/4075>#4075</denchmark-link>\n . Unfortunately, the  deserialization is throwing the exact same error again, .\n So, I'm thinking there were two bugs to begin with, and the first one got fixed in both PRs ... so now off to hunt the second one. Something in the EntityRuler (de)serialization is influencing the ner's (de)serialization.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "psychosis448", "commentT": "2019-08-03T14:19:32Z", "comment_text": "\n \t\tOk, I'm getting closer. The second bug is not really an issue with (de)serialization, but revolves around the internal states of the ner model. In the unit test that I set up, I just do:\n <denchmark-code>ner = nlp.create_pipe(\"ner\")\n ner.add_label(\"SOME_LABEL\")\n nlp.add_pipe(ner)\n </denchmark-code>\n \n and then a bit later:\n <denchmark-code>nlp.add_pipe(ruler, before=\"ner\")\n </denchmark-code>\n \n It turns out that the EntityRuler is influencing the internal states of the BiluoPushDown TransitionSystem. More specifically, after constructing the pipeline, all is fine, and ner.moves.labels is :\n \n {0: Counter(), 1: Counter({'SOME_LABEL': -1}), 2: Counter({'SOME_LABEL': -1}), 3: Counter({'SOME_LABEL': -1}), 4: Counter({'SOME_LABEL': -1}), 5: Counter({'': 1})}\n \n Then when you apply ,  and the NER Parser calls , eventually this <denchmark-link:https://github.com/explosion/spaCy/blob/master/spacy/syntax/ner.pyx#L237>line</denchmark-link>\n :\n \n self.add_action(BEGIN, st._sent[i].ent_type)\n \n sees the entity from the entity ruler (I'm guessing), and changes its internal state self.labels to\n \n {0: Counter(), 1: Counter({'SOME_LABEL': -1, 17374225351233417627: -2}), 2: Counter({'SOME_LABEL': -1}), 3: Counter({'SOME_LABEL': -1}), 4: Counter({'SOME_LABEL': -1}), 5: Counter({'': 1})}\n \n The next few lines will keep adding to this field until eventually, self.labels equals\n \n {0: Counter(), 1: Counter({'SOME_LABEL': -1, 17374225351233417627: -2}), 2: Counter({'SOME_LABEL': -1, 17374225351233417627: -2}), 3: Counter({'SOME_LABEL': -1, 17374225351233417627: -2}), 4: Counter({'SOME_LABEL': -1, 17374225351233417627: -2}), 5: Counter({'': 1})}\n \n This will end up with the ner trying to parse out 9 classes, instead of 5, when reading its data back in from file.\n I'm pretty unfamiliar with the inner workings of the  model code though, so kind of hesitant to start poking around in hope of fixing this... Any suggestions <denchmark-link:https://github.com/honnibal>@honnibal</denchmark-link>\n  or <denchmark-link:https://github.com/ines>@ines</denchmark-link>\n  ?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "psychosis448", "commentT": "2019-08-05T22:09:07Z", "comment_text": "\n \t\tAh this is indeed a somewhat nasty interaction.\n The situation is, the NER's state machine handles preset labels by declaring that all actions that would stop that entity being assigned are invalid. The model produces scores over the actions as normal, but it will be forced to follow a move that preserves the entity. This keeps the details of this \"preset entity\" stuff inside the transition system, so the parser code can stay general across the dependency and NER tasks.\n So before we start parsing, we have to look for any labels on the Doc objects we might be missing, and register them. If we don't do this, we'll get one of those dreaded \"no valid actions\" errors. We do want to support the possibility of adding new entities to a pretrained model anyway. So, the idea of extending a model with another entity doesn't seem so bad.\n It sounds to me like the case we're hitting here is that we're deserialising weights that are a different size from what we expected. If we had registered the labels before deserialising that would be fine, or if we had loaded the model before registering the labels, that would be fine also. But it sounds like we're in an ordering that's not working.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "psychosis448", "commentT": "2019-08-05T22:09:48Z", "comment_text": "\n \t\tShould we be having a look at whether it's possible to avoid the call to nlp()? Is there another way to do the serialisation so that that's not necessary?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "psychosis448", "commentT": "2019-08-06T06:54:28Z", "comment_text": "\n \t\tI think the second bug won't be solved by avoiding the call to self.nlp() in EntityRuler.add_patterns.\n Take <denchmark-link:https://github.com/explosion/spaCy/pull/4072/files#diff-77c14f4ea85d677a9e11fe0b13112af6>this unit test</denchmark-link>\n . The second bug  when I change the last bit to only serialize & deserialize the  component (instead of the whole  object). To me this feels like the NER is in some inherent inconsistent state, that manifests itself only during IO.\n Before IO, it's this code creating that potentially inconsistent state:\n <denchmark-code>nlp.add_pipe(ner)\n nlp.add_pipe(ruler, before=\"ner\")\n doc1 = nlp(\"What do you think about Apple ?\")\n </denchmark-code>\n \n When creating doc1, the ner sees the \"Apple\" entity from the EntityRuler and then changes its internal states as discussed. But then somehow when serializing the NER after that, things go wrong (independently, then, of the EntityRuler).\n Could it be an obscure bug somewhere in the registering of the new labels the NER found?\n In the example above: why is the NER still trying to parse out 5 labels, instead of 9, after it updated its internal state?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "psychosis448", "commentT": "2019-08-06T07:12:20Z", "comment_text": "\n \t\t\n In the example above: why is the NER still trying to parse out 5 labels, instead of 9, after it updated its internal state?\n \n Because it updated its internal state, changed the self.labels and set self.n_moves to 9 instead of 5, but meanwhile the NER had already been initialized and trained on the original dimension of 5?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "psychosis448", "commentT": "2019-09-17T06:44:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/psychosis448>@psychosis448</denchmark-link>\n   Do you use Phrase Matcher for your entityrule?\n I've used it and got same error. And I use token matcher for my entityrule.jsonl. It solved for me.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "psychosis448", "commentT": "2019-10-27T19:44:00Z", "comment_text": "\n \t\tThis thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.\n \t\t"}}}, "commit": {"commit_id": "22b9e1215932f9111c49d4ec5e1ccd25bab1fc3c", "commit_author": "Sofie Van Landeghem", "commitT": "2019-09-27 20:57:13+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "spacy\\pipeline\\entityruler.py", "file_new_name": "spacy\\pipeline\\entityruler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204", "deleted_lines": "183,184,185,186,187,188,189,190,191,192,193,194,195,196,197", "method_info": {"method_name": "add_patterns", "method_params": "self,patterns", "method_startline": "173", "method_endline": "204"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\syntax\\nn_parser.pyx", "file_new_name": "spacy\\syntax\\nn_parser.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "166,167,168,169,170,172,174,175,246,247,248,262,282,456", "deleted_lines": "166,168,240,254,274,448,449"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "spacy\\syntax\\transition_system.pyx", "file_new_name": "spacy\\syntax\\transition_system.pyx", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "66,67,68,69,70,71,72", "deleted_lines": null}}}, "file_3": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "spacy\\tests\\regression\\test_issue4042.py"}, "file_4": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "spacy\\tests\\regression\\test_issue4313.py"}}}}