{"BR": {"BR_id": "349", "BR_author": "eric-haibin-lin", "BRopenT": "2018-09-26T05:51:23Z", "BRcloseT": "2018-09-28T13:05:14Z", "BR_text": {"BRsummary": "Transformer script doesn't run with py3", "BRdescription": "\n <denchmark-code>MXNET_GPU_MEM_POOL_TYPE=Round python3 train_transformer.py --dataset WMT2014BPE                        -$\n src_lang en --tgt_lang de --batch_size 2700                        --optimizer adam --num_accumulated 16 --lr 2.0 --warmup_steps 4000                        --save_dir transformer_$\n n_de_u512 --epochs 30 --gpus 0,1,2,3,4,5,6,7 --scaled                        --average_start 5 --num_buckets 20 --bucket_scheme exp --bleu 13a --log_interval 10 2>&1 | tee base.log\n </denchmark-code>\n \n <denchmark-code>2018-09-26 05:38:28,761 - root - Namespace(average_checkpoint=False, average_start=5, batch_size=2700, beam_size=4, bleu='13a', bucket_ratio=0.0, bucket_scheme='exp', dataset='WMT2$\n 14BPE', dropout=0.1, epochs=30, epsilon=0.1, full=False, gpus='0,1,2,3,4,5,6,7', hidden_size=2048, log_interval=10, lp_alpha=0.6, lp_k=5, lr=2.0, magnitude=3.0, num_accumulated=16,\n num_averages=5, num_buckets=20, num_heads=8, num_layers=6, num_units=512, optimizer='adam', save_dir='transformer_en_de_u512', scaled=True, src_lang='en', src_max_len=-1, test_batc$\n _size=256, tgt_lang='de', tgt_max_len=-1, warmup_steps=4000.0)\n All Logs will be saved to transformer_en_de_u512/train_transformer.log\n Load cached data from /home/ubuntu/transformer/scripts/nmt/cached/WMT2014BPE_en_de_-1_-1_train.npz\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py\", line 650, in read_array\n     array = pickle.load(fp, **pickle_kwargs)\n UnicodeDecodeError: 'ascii' codec can't decode byte 0xfa in position 8: ordinal not in range(128)\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"train_transformer.py\", line 304, in <module>\n     = load_translation_data(dataset=args.dataset, src_lang=args.src_lang, tgt_lang=args.tgt_lang)\n   File \"train_transformer.py\", line 260, in load_translation_data\n     data_train_processed = load_cached_dataset(common_prefix + '_train')\n   File \"train_transformer.py\", line 167, in load_cached_dataset\n     return ArrayDataset(np.array(dat['src_data']), np.array(dat['tgt_data']))\n   File \"/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\", line 235, in __getitem__\n     pickle_kwargs=self.pickle_kwargs)\n   File \"/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py\", line 656, in read_array\n     \"to numpy.load\" % (err,))\n UnicodeError: Unpickling a python object failed: UnicodeDecodeError('ascii', b'\\x07 \\x00\\x00ls\\x00\\x00\\xfab\\x00\\x00k]\\x00\\x00`p\\x00\\x00fi\\x00\\x00\\x03\\x00\\x00\\x00', 8, 9, 'ordinal n$\n t in range(128)')\n You may need to pass the encoding= option to numpy.load\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "eric-haibin-lin", "commentT": "2018-09-26T06:31:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/eric-haibin-lin>@eric-haibin-lin</denchmark-link>\n  What is your default system encoding format? You can use the following code:\n import sys \n sys.defaultencoding()\n Also, did you use py2 to create the cached processed dataset?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "eric-haibin-lin", "commentT": "2018-09-26T17:20:48Z", "comment_text": "\n \t\tYes I used py2 to create the cached processed dataset and realized that I don't have mxnet installed for py2. That's why I switched.\n I can run the code with py2 for a while after I installed mxnet, and then I encountered:\n <denchmark-code>2018-09-26 06:36:20,327 - root - [Epoch 0 Batch 5920/7679] loss=7.3885, ppl=1617.2688, throughput=114.00K wps, wc=5780.56K\n 2018-09-26 06:37:11,299 - root - [Epoch 0 Batch 6080/7679] loss=7.3100, ppl=1495.1115, throughput=113.51K wps, wc=5785.66K\n 2018-09-26 06:38:01,756 - root - [Epoch 0 Batch 6240/7679] loss=7.2699, ppl=1436.3710, throughput=114.49K wps, wc=5776.66K\n 2018-09-26 06:38:52,638 - root - [Epoch 0 Batch 6400/7679] loss=7.2225, ppl=1369.8938, throughput=113.76K wps, wc=5788.13K\n 2018-09-26 06:39:43,664 - root - [Epoch 0 Batch 6560/7679] loss=7.2048, ppl=1345.8282, throughput=113.12K wps, wc=5772.16K\n 2018-09-26 06:40:33,210 - root - [Epoch 0 Batch 6720/7679] loss=7.1519, ppl=1276.5605, throughput=116.81K wps, wc=5787.53K\n 2018-09-26 06:41:25,057 - root - [Epoch 0 Batch 6880/7679] loss=7.1224, ppl=1239.4275, throughput=111.62K wps, wc=5787.31K\n 2018-09-26 06:42:14,961 - root - [Epoch 0 Batch 7040/7679] loss=7.1001, ppl=1212.0946, throughput=115.85K wps, wc=5781.56K\n 2018-09-26 06:43:04,671 - root - [Epoch 0 Batch 7200/7679] loss=7.0639, ppl=1168.9431, throughput=116.21K wps, wc=5777.01K\n 2018-09-26 06:43:56,381 - root - [Epoch 0 Batch 7360/7679] loss=7.0212, ppl=1120.1717, throughput=111.99K wps, wc=5791.21K\n 2018-09-26 06:44:47,374 - root - [Epoch 0 Batch 7520/7679] loss=7.0003, ppl=1096.9319, throughput=113.43K wps, wc=5783.88K\n Traceback (most recent call last):\n   File \"train_transformer.py\", line 632, in <module>\n     train()\n   File \"train_transformer.py\", line 577, in train\n     bpe=bpe)\n   File \"/home/ubuntu/transformer/scripts/nmt/bleu.py\", line 209, in compute_bleu\n     'references and translation should have format of list(list(str)) ' \\\n AssertionError: references and translation should have format of list(list(str)) and list(str), respectively, when toknized is False.\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "eric-haibin-lin", "commentT": "2018-09-26T18:03:46Z", "comment_text": "\n \t\tThe UnicodeDecodeError problem seems due to the incompatibility of np.load between py2 and py3. If you use py3 to create the processed dataset, I think the error would disappear.\n For the second problem, can you print out the type format of references and translations?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "eric-haibin-lin", "commentT": "2018-09-26T18:06:36Z", "comment_text": "\n \t\tShould we alert the py2 users in code?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "eric-haibin-lin", "commentT": "2018-09-26T21:39:36Z", "comment_text": "\n \t\t<denchmark-code>Traceback (most recent call last):\n   File \"train_transformer.py\", line 632, in <module>\n     train()\n   File \"train_transformer.py\", line 577, in train\n     bpe=bpe)\n   File \"/home/ubuntu/transformer/scripts/nmt/bleu.py\", line 210, in compute_bleu\n     'and list(str), respectively, when toknized is False.', reference_corpus_list[0][0], translation_corpus[0])\n AssertionError: ('references and translation should have format of list(list(str)) and list(str), respectively, when toknized is False.', u'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', u'Die Kommission ist die Union.')\n </denchmark-code>\n \n I guess it's just another encoding error. I haven't applied sys.defaultencoding() yet\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "eric-haibin-lin", "commentT": "2018-09-27T03:39:04Z", "comment_text": "\n \t\tYes, the second problem is also due to the encoding error. In py3, the bytes class isn't considered a string anymore.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "eric-haibin-lin", "commentT": "2018-09-27T17:52:10Z", "comment_text": "\n \t\tHow about consistently using UTF-8 by default? I believe all our Datasets default to UTF-8 encoding, even when read with Py2, so this would be consistent with the rest of the library?\n \t\t"}}}, "commit": {"commit_id": "bb82889e5cc39ef7a9e7fefdfb7b51fe8c605cf9", "commit_author": "Shuai Zheng", "commitT": "2018-09-28 21:05:13+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "scripts\\nmt\\bleu.py", "file_new_name": "scripts\\nmt\\bleu.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "99", "deleted_lines": "99", "method_info": {"method_name": "_tokenize_mteval_13a", "method_params": "segment", "method_startline": "74", "method_endline": "108"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "scripts\\nmt\\train_transformer.py", "file_new_name": "scripts\\nmt\\train_transformer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "166", "deleted_lines": "166", "method_info": {"method_name": "load_cached_dataset", "method_params": "prefix", "method_startline": "162", "method_endline": "169"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "scripts\\tests\\test_bleu.py", "file_new_name": "scripts\\tests\\test_bleu.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "123,126", "deleted_lines": "122,125", "method_info": {"method_name": "test_detok_bleu", "method_params": "", "method_startline": "119", "method_endline": "134"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "scripts\\tests\\test_scripts.py", "file_new_name": "scripts\\tests\\test_scripts.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "90,91,92,93,94,95,96,97,98", "deleted_lines": null, "method_info": {"method_name": "test_transformer", "method_params": "", "method_startline": "80", "method_endline": "98"}}}}}}}