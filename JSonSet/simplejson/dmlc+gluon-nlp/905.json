{"BR": {"BR_id": "905", "BR_author": "liusy182", "BRopenT": "2019-08-28T06:47:59Z", "BRcloseT": "2019-09-11T20:52:51Z", "BR_text": {"BRsummary": "KeyError: '&lt;pad&gt;' if run /scripts/word_embeddings/evaluate_pretrained.py with flag `analog-max-vocab-size`", "BRdescription": "\n <denchmark-h:h2>Description</denchmark-h>\n \n run /scripts/word_embeddings/evaluate_pretrained.py with flag analog-max-vocab-size throws exception.\n <denchmark-h:h3>Error Message</denchmark-h>\n \n <denchmark-code>Traceback (most recent call last):\n   File \"evaluate_pretrained.py\", line 216, in <module>\n     vocab.set_embedding(token_embedding_)\n   File \"/Users/siyuanl/private/gluon-nlp/src/gluonnlp/vocab/vocab.py\", line 412, in set_embedding\n     new_idx_to_vec[1:, col_start:col_end] = embs[self._idx_to_token[1:]]\n   File \"/Users/siyuanl/private/gluon-nlp/src/gluonnlp/embedding/token_embedding.py\", line 637, in __getitem__\n     indices = [self._token_to_idx[token] for token in tokens]\n   File \"/Users/siyuanl/private/gluon-nlp/src/gluonnlp/embedding/token_embedding.py\", line 637, in <listcomp>\n     indices = [self._token_to_idx[token] for token in tokens]\n KeyError: '<pad>'\n </denchmark-code>\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n <denchmark-code>cd scripts/word_embeddings\n \n python evaluate_pretrained.py --gpu 0  --embedding-name glove --embedding-source glove.42B.300d --logdir results --analogy-max-vocab-size 300000 --analogy-datasets GoogleAnalogyTestSet BiggerAnalogyTestSet\n </denchmark-code>\n \n <denchmark-h:h2>What have you tried to solve it?</denchmark-h>\n \n It looks like the reason is because <denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/word_embeddings/evaluate_pretrained.py#L208>enforce_max_size</denchmark-link>\n  trims off certain tokens such as  which results to the error above.\n What should be the correct way to run this script?\n <denchmark-h:h2>Environment</denchmark-h>\n \n We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:\n <denchmark-code>curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python\n \n # paste outputs here\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "liusy182", "commentT": "2019-08-28T11:29:01Z", "comment_text": "\n \t\tThanks for reporting this issue liusy182. The reason was that  function incorrectly overwrote an internal TokenEmbedding datastructure, breaking the use of 's embedding for . When this script was written, modifying TokenEmbedding's internals was required. Based on <denchmark-link:https://github.com/dmlc/gluon-nlp/pull/750>#750</denchmark-link>\n  we can now use proper API to make the required changes.\n In general however, vocab = nlp.Vocab(nlp.data.count_tokens(tokens)) can be replaced with vocab = nlp.Vocab(nlp.data.count_tokens(tokens), unknown_token=token_embedding_.unknown_token, padding_token=None, bos_token=None, eos_token=None). I'm opening a PR for these two changes.\n Unfortunately this error slipped through the tests. I'm also extending the testcase.\n \t\t"}}}, "commit": {"commit_id": "b810d10386fad58bded6b39b233f5009406f6da3", "commit_author": "Leonard Lausen", "commitT": "2019-09-11 13:52:50-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "scripts\\tests\\test_scripts.py", "file_new_name": "scripts\\tests\\test_scripts.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "72,73,83,84", "deleted_lines": "72", "method_info": {"method_name": "test_embedding_evaluate_pretrained", "method_params": "fasttextloadngrams", "method_startline": "72", "method_endline": "84"}}, "hunk_1": {"Ismethod": 1, "added_lines": "104,108", "deleted_lines": "101,105", "method_info": {"method_name": "test_embedding_evaluate_from_path", "method_params": "evaluateanalogies,maxvocabsize", "method_startline": "96", "method_endline": "112"}}, "hunk_2": {"Ismethod": 1, "added_lines": "73,83,84", "deleted_lines": null, "method_info": {"method_name": "test_embedding_evaluate_pretrained", "method_params": "fasttextloadngrams,maxvocabsize", "method_startline": "73", "method_endline": "87"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "scripts\\word_embeddings\\evaluate_pretrained.py", "file_new_name": "scripts\\word_embeddings\\evaluate_pretrained.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "148,149,150,151,152,153,154,157,158,159,160,161", "deleted_lines": "143,144,145,151,152,153,154,155,156,159", "method_info": {"method_name": "load_embedding_from_path", "method_params": "args", "method_startline": "134", "method_endline": "165"}}, "hunk_1": {"Ismethod": 1, "added_lines": "185,186,187,188,189,190", "deleted_lines": "183,184,185,186,187,188", "method_info": {"method_name": "enforce_max_size", "method_params": "token_embedding,size", "method_startline": "180", "method_endline": "190"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\gluonnlp\\vocab\\vocab.py", "file_new_name": "src\\gluonnlp\\vocab\\vocab.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "403,404", "deleted_lines": null, "method_info": {"method_name": "set_embedding", "method_params": "self,embeddings", "method_startline": "373", "method_endline": "421"}}}}}}}