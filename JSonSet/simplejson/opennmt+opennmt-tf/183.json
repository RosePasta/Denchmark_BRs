{"BR": {"BR_id": "183", "BR_author": "lockder", "BRopenT": "2018-07-26T07:54:39Z", "BRcloseT": "2018-08-06T12:13:24Z", "BR_text": {"BRsummary": "average_last_checkpoints crashes in distributed training", "BRdescription": "\n 1.5.0 (2018-06-08)\n New features\n Training option average_last_checkpoints to automatically average checkpoints at the end of the training\n Reading the new features I found on 1.5.0 an amazing new parameter to automatically do the avg of the checkpoints  but when I try to use I get this message.\n Traceback (most recent call last):\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\n return fn(*args)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\n options, feed_dict, fetch_list, target_list, run_metadata)\n File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n run_metadata)\n tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/ubuntu/NaturalLanguageRecognition/NaturalLanguageManager/src/Intent_Classification/model/model.ckpt-369\n [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT64, DT_FLOAT, DT_INT64], _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](_recv_save/Const_0_S1, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n [[Node: save/restore_all/NoOp_1_S8 = _Recv<denchmark-link:>client_terminated=false, recv_device=\"/job:chief/replica:0/task:0/device:GPU:0\", send_device=\"/job:ps/replica:0/task:0/device:CPU:0\", send_device_incarnation=8143571150034301869, tensor_name=\"edge_247_save/restore_all/NoOp_1\", tensor_type=DT_FLOAT, _device=\"/job:chief/replica:0/task:0/device:GPU:0\"</denchmark-link>\n ]]\n I went to the console and did the ls inside the folder\n ubuntu@mltraining:~/NaturalLanguageRecognition/NaturalLanguageManager/src/Intent_Classification/model$ ls\n checkpoint  events.out.tfevents.1532590583.mltraining  graph.pbtxt        model.ckpt-369.meta    projector_config.pbtxt  src_word_vocab.txt\n eval        events.out.tfevents.1532591278.mltraining  model.ckpt-0.meta  model_description.pkl  src_char_vocab.txt\n Then I went to the checkpoint file and took a look:\n model_checkpoint_path: \"model.ckpt-369\"\n all_model_checkpoint_paths: \"model.ckpt-0\"\n all_model_checkpoint_paths: \"model.ckpt-369\"\n looks like its trying to find the name inside the checkpoint but its saved with the extension .meta\n The weird thing its the restore training works very well its just when doing the average of the checkpoints\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lockder", "commentT": "2018-07-26T08:20:24Z", "comment_text": "\n \t\tAre you using distributed training? I'm not sure checkpoint averaging is correctly supported in this scenario. Thanks for reporting.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lockder", "commentT": "2018-07-26T08:37:16Z", "comment_text": "\n \t\tyes I'm using distributed training , looking to the code, its checking if its a chief machine before doing the avg. Also I tried to run the script manually and its happening the same thing\n Right now I have 3 machines. 1 chief(gpu) 1 worker(gpu) and 1 ps server (cpu)\n I'm not running the script from the same folder as the model. I'm just sending the full path from another path because I have a central python script running all the scripts from there\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lockder", "commentT": "2018-07-26T09:16:02Z", "comment_text": "\n \t\tThanks. Not sure how to handle correctly this case yet so I need to investigate. Feel free to share any insights.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lockder", "commentT": "2018-07-26T09:20:21Z", "comment_text": "\n \t\tI will try to train it without distributed training and see what happens. if I find the fix I will send it to you\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lockder", "commentT": "2018-07-26T11:25:51Z", "comment_text": "\n \t\ton the first look. Looks like the files are splitted between chief and ps server ( not sure if I have more than one ps server)\n When doing non distributed training the model folder contains files .index and 0000-of-0002\n and the meta files with the checkpoint file\n but with distributed training\n the chief contains .meta files and checkpoint\n the ps server contains .index and 0000-of-0002\n I will keep investigating\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "lockder", "commentT": "2018-07-26T12:33:51Z", "comment_text": "\n \t\tnon distributed training:\n checkpoints_path = tf.train.get_checkpoint_state(model_dir).all_model_checkpoint_paths\n is returning a list of strings with the path of the file\n but also there is only 1 checkpoint instead all of them. I had 4 saved checkpoints inside the folder\n Instead on distributed:\n is returning an object. RepeatedScalarContainer with the path of the meta files\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "lockder", "commentT": "2018-07-27T07:29:56Z", "comment_text": "\n \t\tWell Its not a fix but if you setup the chief and ps machine the same. then it will work since it will have all the files on the same machine and the other pc can be setup as workers.\n But I find it weird because its saying Averaging 1 checkpoints... and I had 5 checkpoints to be saved on the config file\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "lockder", "commentT": "2018-07-27T07:45:30Z", "comment_text": "\n \t\t\n Well Its not a fix but if you setup the chief and ps machine the same. then it will work since it will have all the files on the same machine and the other pc can be setup as workers.\n \n I think that is the expected behavior. I also read that one should consider using a shared filesystem to properly load a checkpoint produced by a distributed training.\n With all this information, I don't think there is anything to change in OpenNMT-tf.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "lockder", "commentT": "2018-07-27T08:05:10Z", "comment_text": "\n \t\tWell it will only work if you have on the same machine all files :) but it will not work if you setup a different ps machine to free up gpu memory so you can have a bigger model or bigger batch size. I tried to setup the ps on a cpu machine but it was more slow.\n and I'm not sure if you have more than 1 ps server what will happen jejeje\n The other issue is the code its finding only the last checkpoint not sure why even if there are more than one checkpoint , so its getting the last one, and not doing and avg\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "lockder", "commentT": "2018-07-27T08:11:20Z", "comment_text": "\n \t\tHDFS could be used to share a filesystem across separate machines. Not sure if it requires code change or not.\n \n The other issue is the code its finding only the last checkpoint not sure why even if there are more than one checkpoint , so its getting the last one, and not doing and avg\n \n What value did you set to average_last_checkpoints?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "lockder", "commentT": "2018-07-27T08:50:45Z", "comment_text": "\n \t\tI see, for the readme I was adding true instead a number :P my mistake! thanks\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "lockder", "commentT": "2018-08-06T12:13:24Z", "comment_text": "\n \t\tAccording to me, the only action required by this issue is to better support remote/shared filesystems like HDFS or S3. The PR above improves the support of such configurations for the model directory and data files.\n Closing for now.\n \t\t"}}}, "commit": {"commit_id": "410eb86e17bf6b4e18b37beab028f5ffae7e2bb9", "commit_author": "Guillaume Klein", "commitT": "2018-08-06 14:10:08+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "23", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "opennmt\\bin\\main.py", "file_new_name": "opennmt\\bin\\main.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "125,126,128,130,131,132,133,134", "deleted_lines": "125,127,129", "method_info": {"method_name": "main", "method_params": "", "method_startline": "44", "method_endline": "175"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "opennmt\\config.py", "file_new_name": "opennmt\\config.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "45,46,47,59,86,87,88,89", "deleted_lines": "45,83,84,85,89", "method_info": {"method_name": "load_model", "method_params": "model_dir,model_file,model_name", "method_startline": "45", "method_endline": "92"}}, "hunk_1": {"Ismethod": 1, "added_lines": "44,45,46,47", "deleted_lines": "45", "method_info": {"method_name": "load_model", "method_params": "model_dir,model_file,model_name,serialize_model", "method_startline": "44", "method_endline": "47"}}, "hunk_2": {"Ismethod": 1, "added_lines": "112", "deleted_lines": "108", "method_info": {"method_name": "load_config", "method_params": "config_paths,config", "method_startline": "98", "method_endline": "125"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "opennmt\\inputters\\text_inputter.py", "file_new_name": "opennmt\\inputters\\text_inputter.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "40,43,54,55", "deleted_lines": "42,45,56,57", "method_info": {"method_name": "visualize_embeddings", "method_params": "log_dir,embedding_var,vocabulary_file,num_oov_buckets", "method_startline": "23", "method_endline": "73"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "opennmt\\tokenizers\\tokenizer.py", "file_new_name": "opennmt\\tokenizers\\tokenizer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "50", "deleted_lines": "51", "method_info": {"method_name": "initialize", "method_params": "self,metadata", "method_startline": "35", "method_endline": "51"}}, "hunk_1": {"Ismethod": 1, "added_lines": "29", "deleted_lines": "30", "method_info": {"method_name": "__init__", "method_params": "self,configuration_file_or_key", "method_startline": "19", "method_endline": "33"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "opennmt\\utils\\misc.py", "file_new_name": "opennmt\\utils\\misc.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "56", "deleted_lines": "56", "method_info": {"method_name": "count_lines", "method_params": "filename", "method_startline": "54", "method_endline": "60"}}}}}}}