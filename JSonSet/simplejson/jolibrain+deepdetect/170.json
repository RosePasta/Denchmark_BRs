{"BR": {"BR_id": "170", "BR_author": "EBazarov", "BRopenT": "2016-09-05T20:53:36Z", "BRcloseT": "2016-09-06T10:47:59Z", "BR_text": {"BRsummary": "Accuracy score from DeDe and computed using sklearn", "BRdescription": "\n I have a difficulty of understanding metrics from DeDe. When I'm providing Train and Test data set, DeDe compute metrics during training, and at the end provide final results. But when I predict same test set, using \"predict API\" and then compute accuracy metric, I had results that are not the same at all. I'm using SVM connector, and for testing purpose decided to use public dataset agnews.\n You can find my code and results in attached html report witch extracted from jupyter notebook.\n Maybe it's not problem of metrics, but problem of wrong using DeDe API, I'm confusing, because  difference is huge.\n <denchmark-link:https://github.com/beniz/deepdetect/files/455607/DeDe_SVM_agnews_test_acc.zip>DeDe_SVM_agnews_test_acc.zip</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "EBazarov", "commentT": "2016-09-06T05:09:21Z", "comment_text": "\n \t\tI believe your python code reproduced below is not doing what I suppose you want it to do:\n def read_dede_results(dic_results):\n     pred_1_prob = []\n     for pred in dic_results['body']['predictions']:\n         pred_1_prob.append(pred['classes'][1]['prob'])\n     y_pred_proba = np.array(pred_1_prob).astype(np.float)\n     y_pred = np.around(np.array(pred_1_prob))\n     return y_pred_proba, y_pred\n \n the top category and probability should be acquired from pred['classes'][0], not sure why you are using the second value instead;\n the label should be acquired from pred['classes'][0]['cat'], instead you are rounding all probabilities to 0\n \n In the end my understanding is that you are computing the accuracy of a classifier that returns label 0 everywhere.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "EBazarov", "commentT": "2016-09-06T05:23:16Z", "comment_text": "\n \t\tI think I get you, you mean that predicted classes sorted in results, that's mean you have at first place best label.\n Sorry my mistake, I will rerun the code. But the problem that before I have used API with 'best':1, and extracted predictions in a little bit more intelligent way, but also had differents.\n Thanks, I will do one m\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "EBazarov", "commentT": "2016-09-06T05:23:53Z", "comment_text": "\n \t\tI will do test one more time ASAP\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "EBazarov", "commentT": "2016-09-06T07:02:41Z", "comment_text": "\n \t\tOk, this is a new results. I have extracted prediction using pred['classes'][0]['cat'], so now I think it should be rigth.\n Still have difference between accuracy from DeDe and computed using sklearn.\n <denchmark-link:https://github.com/beniz/deepdetect/files/456096/DeDe_SVM_agnews_test_acc_bug_fix.zip>DeDe_SVM_agnews_test_acc_bug_fix.zip</denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "EBazarov", "commentT": "2016-09-06T08:00:57Z", "comment_text": "\n \t\tAGnews has 4 balanced classes, you are only declaring two to dd, with nclasses=2. You can easily observe this since the predictions are then only 0 or 1. The probability that a 4 balanced classes classifier with 90% accuracy outputs only 0 and 1 is null. From there you can immediately infer that you have a class specification problem...\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "EBazarov", "commentT": "2016-09-06T08:03:48Z", "comment_text": "\n \t\tAt top of a notebook file I have declared that I will use only \"Sport\" and \"Business\" data for simplicity.\n So that's why I am using nclasses = 2\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "EBazarov", "commentT": "2016-09-06T08:06:49Z", "comment_text": "\n \t\tIt is even worse then, it is impossible to have a binary classifier do 38% accuracy, since reversing the classes would yield 62% accuracy... Just saw it is 53%, which is again suspicious because random.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "EBazarov", "commentT": "2016-09-06T09:28:02Z", "comment_text": "\n \t\tOK, now I can reproduce through another mean, thanks.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "EBazarov", "commentT": "2016-09-06T09:58:02Z", "comment_text": "\n \t\tI am able to reproduce this behavior with the following code\n accuracy calculated by DD: ~90%\n accuracy calculated from DD probabilities: ~64%\n EDIT: Thanks to <denchmark-link:https://github.com/beniz>@beniz</denchmark-link>\n , adding 'measure': ['f1'] during prediction phase is enough to reproduce the bug. I modified the code accordingly\n \n Prediction on test set:\n {'measure': {'accp': 0.6539, 'precision': 0.653899999999346, 'recall': 0.7724241736769586, 'f1': 0.7082375472400635}, 'time': 4605.0}\n Metrics during training phase:\n {'precision': 0.8962499999991037, 'mcll': 0.2626906841772865, 'iteration': 900.0, 'accp': 0.89625, 'auc': 0.95724583, 'recall': 0.8964958313795168, 'train_loss': 0.2949938476085663, 'acc': 0.89625, 'f1': 0.8963728988344203}\n \n import time\n import numpy as np\n from dd_client import DD\n \n sname = \"AccuracyComparison\"\n description = \"Reproduce accuracy measure\"\n \n # change to your model path\n repository = \"/path/to/model\"\n \n # path to data (binary classification data)\n input_data = [\"X_train.svm\", \"X_test.svm\"]\n \n \n # Service creation\n dd = DD('localhost')\n dd.set_return_format(dd.RETURN_PYTHON)\n model = {'templates': '../templates/caffe', 'repository': repository}\n mllib = 'caffe'\n \n service_parameters_input = {'connector': 'svm'}\n service_parameters_mllib = {\"db\": True, \"template\": \"mlp\", \"nclasses\": 2, \"layers\": [128,128], \"activation\": 'relu'}\n service_parameters_output = {}\n \n json_dump = dd.put_service(sname, model, description, mllib, service_parameters_input,\n                                 service_parameters_mllib, service_parameters_output)\n \n \n time.sleep(1)\n \n \n # Training model\n train_parameters_input = {'db': True},\n train_parameters_output = {\"measure\": ['mcll','auc','accp']},\n train_parameters_mllib = {'gpu': True,\n                                'solver': {'iterations': 1000, 'test_interval': 100, 'base_lr': 0.01,\n                                           'solver_type': 'adam'},\n                                'net': {'batch_size': 256}}\n \n dd.post_train(sname, input_data,\n               train_parameters_input,\n               train_parameters_mllib,\n               train_parameters_output, async=True)\n \n train_status = ''\n while True:\n     train_status = dd.get_train(sname, job=1, timeout=2)\n     if train_status['head']['status'] == 'running':\n         print(train_status['body']['measure'])\n     else:\n         print(train_status)\n         break\n \n # Predicting test set\n predict_parameters_input = {},\n predict_parameters_output = {\"best\": 2, 'measure': ['f1']},\n predict_parameters_mllib = {'gpu': True, 'gpuid': 0}\n \n json_dump = dd.post_predict(sname, [input_data[1]], predict_parameters_input, predict_parameters_mllib,\n                             predict_parameters_output)\n print(json_dump['body'])\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "EBazarov", "commentT": "2016-09-06T10:02:49Z", "comment_text": "\n \t\tThanks, you can actually reproduce by simply adding measure:['f1'] to the predict call in the output object.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "EBazarov", "commentT": "2016-09-06T10:47:59Z", "comment_text": "\n \t\tFixed. Some would appreciate the hellish effect of a misplaced -1... So using /predict along with measure allows you to get the accuracy on test set whenever that is needed outside of the training loop.\n \t\t"}}}, "commit": {"commit_id": "73c54d4a6275218634639bd61559ae8a5e399265", "commit_author": "Emmanuel Benazera", "commitT": "2016-09-06 12:47:17+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\svminputfileconn.cc", "file_new_name": "src\\svminputfileconn.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "212", "deleted_lines": "212", "method_info": {"method_name": "dd::SVMInputFileConn::deserialize_vocab", "method_params": "required", "method_startline": "196", "method_endline": "219"}}}}}}}