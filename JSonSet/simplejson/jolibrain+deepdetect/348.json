{"BR": {"BR_id": "348", "BR_author": "YaYaB", "BRopenT": "2017-09-05T14:53:11Z", "BRcloseT": "2017-09-08T12:59:01Z", "BR_text": {"BRsummary": "Cannot indicate a Train and a Test folder using txt connector", "BRdescription": "\n I am trying to train a network based on the template  using the dataset you indicated in <denchmark-link:https://github.com/jolibrain/deepdetect/pull/340>#340</denchmark-link>\n  but I can not specify my own split of train and test.\n <denchmark-h:h4>Configuration</denchmark-h>\n \n \n Version of DeepDetect:\n \n Locally compiled on Ubuntu 16.04 LTS\n \n \n Commit (shown by the server when starting):\n \n Branch Master, 62ea2f4\n \n \n GPUS:\n \n 1 x Nvidia GTX Titan X (Maxwell)\n \n \n \n <denchmark-h:h4>Your question / the problem you're facing:</denchmark-h>\n \n I want to train using an already split dataset in train and test. However it does not seem possible with the txt container. If you put two folders or files as you would do using the svm connector it just concatenate them in our case.\n Here is a small example.\n \n First download the dataset: https://ufile.io/tibzi\n Extract the dataset:\n \n <denchmark-code>tar -xvf agnews.tar.gz\n </denchmark-code>\n \n PATH_DATA will represent the path to the agnews extracted folder.\n \n Usual training\n \n Then we will just create  a usual training with the split managed by DD.\n \n Service Creation\n INPUT\n \n <denchmark-code>curl -X PUT 'http://localhost:8100/services/agnews' -d '{\"mllib\":\"caffe\",\"description\":\"agnews classifier\",\"type\":\"supervised\",\"parameters\":{\"input\":{\"connector\":\"txt\",\"characters\":true,\"embedding\":true,\"sequence\":1014},\"mllib\":{\"nclasses\":4,\"template\":\"vdcnn_9\"}},\"model\":{\"templates\":\"../../templates/caffe/\",\"repository\":\"/home/test\"}}'\n </denchmark-code>\n \n OUTPUT\n <denchmark-code>DeepDetect [ commit 62ea2f4c84fe2f10ea9f1ed465139e7a1fc0ef8b ]\n \n INFO - 14:40:39 - Running DeepDetect HTTP server on localhost:8100\n WARNING: Logging before InitGoogleLogging() is written to STDERR\n I0905 14:40:43.922724  2989 caffelib.cc:131] instantiating model template vdcnn_9\n I0905 14:40:43.922771  2989 caffelib.cc:135] source=../../templates/caffe//vdcnn_9/\n I0905 14:40:43.922777  2989 caffelib.cc:136] dest=/home/test/vdcnn_9.prototxt\n \n INFO - 14:40:43 - Tue Sep  5 14:40:43 2017 UTC - 127.0.0.1 \"PUT /services/agnews\" 201 1001\n \n </denchmark-code>\n \n \n Train Launch\n INPUT\n \n <denchmark-code>curl -X POST 'http://localhost:8100/train' -d '{\"service\":\"agnews\",\"parameters\":{\"input\":{\"connector\":\"txt\",\"embedding\":true,\"sentences\":true,\"db\":true,\"test_split\":0.06,\"shuffle\":true},\"mllib\":{\"gpu\":true,\"net\":{\"batch_size\":128,\"test_batch_size\":128},\"solver\":{\"test_interval\":1000,\"snpashot\":1000,\"base_lr\":0.01,\"solver_type\":\"ADAM\",\"iterations\":25000}},\"output\":{\"measure\":[\"mcll\",\"f1\",\"cmdiag\",\"cmfull\"]}},\"data\":[\"${PATH_DATA}/agnews/full\"]}'\n \n </denchmark-code>\n \n OUTPUT\n <denchmark-code>INFO - 14:42:15 - Tue Sep  5 14:42:15 2017 UTC - 127.0.0.1 \"POST /train\" 201 0\n I0905 14:42:15.974807  3013 txtinputfileconn.cc:74] txtinputfileconn: list subdirs size=4\n loaded text samples=127600I0905 14:42:18.429330  3013 txtinputfileconn.cc:192] vocabulary size=0\n data split test size=7656 / remaining data size=119944\n vocab size=0\n \n INFO - 14:42:18 - db_batchsize=119944 / db_testbatchsize=7656\n \n INFO - 14:42:18 - Opened lmdb /home/test/train.lmdb\n ...\n INFO - 14:42:31 - Opened lmdb /home/test/test.lmdb\n INFO - 14:42:31 - Processed 1000 text entries\n INFO - 14:42:31 - Processed 2000 text entries\n INFO - 14:42:31 - Processed 3000 text entries\n INFO - 14:42:31 - Processed 4000 text entries\n INFO - 14:42:31 - Processed 5000 text entries\n INFO - 14:42:31 - Processed 6000 text entries\n INFO - 14:42:31 - Processed 7000 text entries\n \n </denchmark-code>\n \n As you can see the train set contains 120k examples and the test contains 7.6k examples. Two LMDB are created train.lmdb and test.lmdb.\n The train is launched and it goes well.\n \n Training with own train and test split\n \n Then we will just create a usual training with the split managed by DD.\n \n Service Creation\n INPUT\n \n <denchmark-code>curl -X PUT 'http://localhost:8100/services/agnews' -d '{\"mllib\":\"caffe\",\"description\":\"agnews classifier\",\"type\":\"supervised\",\"parameters\":{\"input\":{\"connector\":\"txt\",\"characters\":true,\"embedding\":true,\"sequence\":1014},\"mllib\":{\"nclasses\":4,\"template\":\"vdcnn_9\"}},\"model\":{\"templates\":\"../../templates/caffe/\",\"repository\":\"/home/test\"}}'\n \n </denchmark-code>\n \n OUTPUT\n <denchmark-code>./dede --port 8100                                                                        \n DeepDetect [ commit 62ea2f4c84fe2f10ea9f1ed465139e7a1fc0ef8b ]\n \n INFO - 14:29:03 - Running DeepDetect HTTP server on localhost:8100\n WARNING: Logging before InitGoogleLogging() is written to STDERR\n I0905 14:29:26.320225  2632 caffelib.cc:131] instantiating model template vdcnn_9\n I0905 14:29:26.320309  2632 caffelib.cc:135] source=../../templates/caffe//vdcnn_9/\n I0905 14:29:26.320319  2632 caffelib.cc:136] dest=/home/test/vdcnn_9.prototxt\n \n INFO - 14:29:26 - Tue Sep  5 14:29:26 2017 UTC - 127.0.0.1 \"PUT /services/agnews\" 201 1506\n \n </denchmark-code>\n \n \n Train Launch\n The test_split is by default set to 0 and two folders are given to the parameters data.\n INPUT\n \n <denchmark-code>curl -X POST 'http://localhost:8100/train' -d '{\"service\":\"agnews\",\"parameters\":{\"input\":{\"connector\":\"txt\",\"embedding\":true,\"sentences\":true,\"db\":true,\"shuffle\":true},\"mllib\":{\"gpu\":true,\"net\":{\"batch_size\":128,\"test_batch_size\":128},\"solver\":{\"test_interval\":1000,\"snpashot\":1000,\"base_lr\":0.01,\"solver_type\":\"ADAM\",\"iterations\":25000}},\"output\":{\"measure\":[\"mcll\",\"f1\",\"cmdiag\",\"cmfull\"]}},\"data\":[\"${PATH_DATA}/agnews/split/train\",\"${PATH_DATA}/agnews/split/test\"]}'\n </denchmark-code>\n \n OUTPUT\n <denchmark-code>INFO - 14:32:55 - Tue Sep  5 14:32:55 2017 UTC - 127.0.0.1 \"POST /train\" 201 0\n I0905 14:32:55.195322  2723 txtinputfileconn.cc:74] txtinputfileconn: list subdirs size=4\n loaded text samples=120000\n I0905 14:32:57.468423  2723 txtinputfileconn.cc:192] vocabulary size=0\n I0905 14:32:57.468482  2723 txtinputfileconn.cc:74] txtinputfileconn: list subdirs size=4\n loaded text samples=127600\n I0905 14:32:57.586179  2723 txtinputfileconn.cc:192] vocabulary size=0\n \n INFO - 14:32:57 - db_batchsize=127600 / db_testbatchsize=0\n \n INFO - 14:32:57 - Opened lmdb /home/test/train.lmdb\n etc.\n </denchmark-code>\n \n As you can see the train set contains now 127.6k examples and no test set was created. All the data was concatenated to one big dataset: train.lmdb\n If we take a closer look at the code behind the connectors, we clearly see that for the svm connector the train and test sets are indicated in the code (see line 138 of <denchmark-link:https://github.com/beniz/deepdetect/blob/master/src/svminputfileconn.h>https://github.com/beniz/deepdetect/blob/master/src/svminputfileconn.h</denchmark-link>\n ).\n However for the txt connector there is no such thing. The  function and the  only indicate that we concatenate the data and nothing more (see lines 266 and 271 of <denchmark-link:https://github.com/beniz/deepdetect/blob/802f316d9c04c9651e2d18a6f1ddf05d7608e195/src/txtinputfileconn.h>https://github.com/beniz/deepdetect/blob/802f316d9c04c9651e2d18a6f1ddf05d7608e195/src/txtinputfileconn.h</denchmark-link>\n )\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "YaYaB", "commentT": "2017-09-06T07:43:58Z", "comment_text": "\n \t\tThanks for catching this one! Could you verify that PR <denchmark-link:https://github.com/jolibrain/deepdetect/pull/350>#350</denchmark-link>\n  does fix this issue ? I've tested it on the above dataset and it does work for me with the PR. Thanks.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "YaYaB", "commentT": "2017-09-06T13:49:03Z", "comment_text": "\n \t\tThanks for the quickness of the PR.\n Thanks to i,  it's possible to train by giving a train and a test set. It does create a train.lmdb and a test.lmdb accordingly.\n However there are several issues.\n When building those datasets, it dumps a file called corresp.txt. This file is the map between our labels and the labels in the lmdb.\n \n Different mapping\n I have noted previously that this file is created when reading the subdirectories of the directory containing the datas. There is no order defined on how to read those subdirectories. That means that we can read folders 1,2,3,4 or 2,3,1,4. It mainly depends on the function readdir (line 85 in https://github.com/beniz/deepdetect/blob/5067658a69deeda46e06f14b0527180f59d018a1/src/utils/fileops.hpp) that does not order the files.\n \n In our case train subdirectories and test subdirectories are not read in the same order. That means the mapping of our labels and the ones in train.lmdb is different than the one between our labels and the test.lmdb. As we train on train.lmdb, we basically define train.lmdb labels as ground truth and when we test on test.lmdb that uses a different mapping we will not get representative results.\n Thus the metrics during training phase are not correct (except for the one on the train set as train_loss, etc.)\n \n Disappearance of the correct corresp file\n In fact DD dumps a corresp.txt each time it reads a directory (line 225 in https://github.com/beniz/deepdetect/blob/a977aedf579e9c2d482c0fc8def92000c24644e9/src/caffeinputconns.cc). However _correspname is fixed (line 352 in https://github.com/beniz/deepdetect/blob/5067658a69deeda46e06f14b0527180f59d018a1/src/txtinputfileconn.h)\n \n In our case we give a train, then a test set. DD will then dump first the corresp.txt associated to the train data then it will replace this file by the one for the test data. That means that we lost the mapping that our model will use. If we want to make a prediction we need that mapping\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "YaYaB", "commentT": "2017-09-06T14:03:03Z", "comment_text": "\n \t\tThe easiest solution, in my opinion, would be to only dump the corresp.txt file associated to the train file and force the mapping of the test set to be similar to the one of the train set.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "YaYaB", "commentT": "2017-09-07T14:18:31Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/YaYaB>@YaYaB</denchmark-link>\n  thanks for the update. Can you test the PR again, the two bullet points above have been dealt with and appear to work for us. I don't think the reading order of repositories can change, but the mapping has been enforced as it already was in the image connector. Thanks.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "YaYaB", "commentT": "2017-09-07T15:54:22Z", "comment_text": "\n \t\tThanks for the update of the PR.\n Let me show you what I obtain on my side with the corresp.txt.\n If I launch a training using only the train folder my corresp will contain the following:\n <denchmark-code>3 2\n 2 1\n 1 3\n 0 4\n </denchmark-code>\n \n However if now I use only the test folder during training I obtain the following inside the corresp file:\n <denchmark-code>3 3\n 2 4\n 1 2\n 0 1\n </denchmark-code>\n \n According to this (<denchmark-link:http://man7.org/linux/man-pages/man3/readdir.3.html>http://man7.org/linux/man-pages/man3/readdir.3.html</denchmark-link>\n ):\n <denchmark-code>The order in which filenames are read by successive calls to\n        readdir() depends on the filesystem implementation; it is unlikely\n        that the names will be sorted in any fashion.\n </denchmark-code>\n \n Then I am not surprised to see that I obtain different corresp files with different folders.\n I am testing your new PR now, I will come back to you when it's over\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "YaYaB", "commentT": "2017-09-08T07:50:11Z", "comment_text": "\n \t\tIt works well for me!\n FYI, I tested the test_split argument. Basically it still works, the train is split accordingly to the value of the test_split and it is then appended to the test set. Is it something that we want? Or is it better to force test_split to 0 when a test set is indicated?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "YaYaB", "commentT": "2017-09-08T12:12:29Z", "comment_text": "\n \t\tYes, rational is that first directory is train, the remaining directories are all test.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "YaYaB", "commentT": "2017-09-08T12:59:01Z", "comment_text": "\n \t\tThanks for the thorough report. Fix has been merged.\n \t\t"}}}, "commit": {"commit_id": "dbc6eea8d6cd2a28a94ea93595b15e652e955061", "commit_author": "Emmanuel Benazera", "commitT": "2017-09-07 13:20:37+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\txtinputfileconn.cc", "file_new_name": "src\\txtinputfileconn.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "77,83,84,92,93,94,95,96,129,134,144,157,187", "deleted_lines": "89,90,123,128,138,151,181", "method_info": {"method_name": "dd::DDTxt::read_dir", "method_params": "dir", "method_startline": "65", "method_endline": "202"}}}}}}}