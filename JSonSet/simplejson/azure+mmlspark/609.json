{"BR": {"BR_id": "609", "BR_author": "emnajaoua", "BRopenT": "2019-07-08T13:21:36Z", "BRcloseT": "2019-10-09T03:27:09Z", "BR_text": {"BRsummary": "java.net.ConnectException: Connection refused (Connection refused) with LightGBMClassifier in Databricks", "BRdescription": "\n I am trying to run this example with my own dataset on databricks.\n <denchmark-link:url>https://github.com/microsoft/recommenders/blob/master/notebooks/02_model/mmlspark_lightgbm_criteo.ipynb</denchmark-link>\n \n My cluster configuration is from 2 until 10 worker nodes. Worker Type is 28.GB Memory, 8 cores.\n In the beginning of my notebook I set the following properties\n \n \n but it seems that it doesn't effect the notebook environment.\n I am using for the LightGBMClassifier , the library Azure:mmlspark:0.16.\n My dataset has 1.502.306 rows and 9 columns. It is a spark dataframe, result of 3 joins between 3 SQL Tables (transformed to spark dataframes with the command spark.sql())\n I apply feature_processor step to encode the categorical columns. Then after setting the LightGBMClassifier parameter, I train the model.\n My LightGBMClassifier parameters are :\n `NUM_LEAVES = 8\n NUM_ITERATIONS = 20\n LEARNING_RATE = 0.1\n FEATURE_FRACTION = 0.8\n EARLY_STOPPING_ROUND = 5\n <denchmark-h:h1>Model name</denchmark-h>\n \n MODEL_NAME = 'lightgbm_criteo.mml'\n lgbm = LightGBMClassifier(\n labelCol=\"kategorie1\",\n featuresCol=\"features\",\n objective=\"multiclass\",\n isUnbalance=True,\n boostingType=\"gbdt\",\n boostFromAverage=True,\n baggingSeed=3, #fr\u00fcher 42\n numLeaves=NUM_LEAVES,\n numIterations=NUM_ITERATIONS,\n learningRate=LEARNING_RATE,\n featureFraction=FEATURE_FRACTION,\n earlyStoppingRound=EARLY_STOPPING_ROUND,\n timeout=1200.0\n #parallelism='data_parallel'\n )I applied the repartition trick as well before training the modeltrain = train.repartition(50)\n train.rdd.getNumPartitions()Then when I runmodel = lgbm.fit(train)then I get the following error Py4JJavaError: An error occurred while calling o1125.fit.\n : org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 36.0 failed 4 times, most recent failure: Lost task 10.3 in stage 36.0 (TID 3493, 10.139.64.10, executor 7): java.net.ConnectException: Connection refused (Connection refused)\n at java.net.PlainSocketImpl.socketConnect(Native Method)\n at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n at java.net.Socket.connect(Socket.java:589)\n at java.net.Socket.connect(Socket.java:538)\n at java.net.Socket.(Socket.java:434)\n at java.net.Socket.(Socket.java:211)\n at com.microsoft.ml.spark.TrainUtils$.getNodes(TrainUtils.scala:178)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$5.apply(TrainUtils.scala:211)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$5.apply(TrainUtils.scala:205)\n at com.microsoft.ml.spark.StreamUtilities$.using(StreamUtilities.scala:29)\n at com.microsoft.ml.spark.TrainUtils$.trainLightGBM(TrainUtils.scala:204)\n at com.microsoft.ml.spark.LightGBMClassifier$$anonfun$3.apply(LightGBMClassifier.scala:83)\n at com.microsoft.ml.spark.LightGBMClassifier$$anonfun$3.apply(LightGBMClassifier.scala:83)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:200)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:197)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:852)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:852)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n at org.apache.spark.scheduler.Task.run(Task.scala:112)\n at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1481)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)\n Driver stacktrace:\n at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2355)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2343)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2342)\n at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2342)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)\n at scala.Option.foreach(Option.scala:257)\n at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1096)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2574)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2510)\n at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:893)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:2240)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:2338)\n at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1051)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n at org.apache.spark.rdd.RDD.withScope(RDD.scala:379)\n at org.apache.spark.rdd.RDD.reduce(RDD.scala:1033)\n at org.apache.spark.sql.Dataset$$anonfun$reduce$1.apply(Dataset.scala:1650)\n at org.apache.spark.sql.Dataset$$anonfun$withNewRDDExecutionId$1.apply(Dataset.scala:3409)\n at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:99)\n at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:228)\n at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:85)\n at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:158)\n at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3405)\n at org.apache.spark.sql.Dataset.reduce(Dataset.scala:1649)\n at com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:85)\n at com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:27)\n at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n at py4j.Gateway.invoke(Gateway.java:295)\n at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n at py4j.commands.CallCommand.execute(CallCommand.java:79)\n at py4j.GatewayConnection.run(GatewayConnection.java:251)\n at java.lang.Thread.run(Thread.java:748)\n Caused by: java.net.ConnectException: Connection refused (Connection refused)\n at java.net.PlainSocketImpl.socketConnect(Native Method)\n at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n at java.net.Socket.connect(Socket.java:589)\n at java.net.Socket.connect(Socket.java:538)\n at java.net.Socket.(Socket.java:434)\n at java.net.Socket.(Socket.java:211)\n at com.microsoft.ml.spark.TrainUtils$.getNodes(TrainUtils.scala:178)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$5.apply(TrainUtils.scala:211)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$5.apply(TrainUtils.scala:205)\n at com.microsoft.ml.spark.StreamUtilities$.using(StreamUtilities.scala:29)\n at com.microsoft.ml.spark.TrainUtils$.trainLightGBM(TrainUtils.scala:204)\n at com.microsoft.ml.spark.LightGBMClassifier$$anonfun$3.apply(LightGBMClassifier.scala:83)\n at com.microsoft.ml.spark.LightGBMClassifier$$anonfun$3.apply(LightGBMClassifier.scala:83)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:200)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:197)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:852)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:852)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n at org.apache.spark.scheduler.Task.run(Task.scala:112)\n at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1481)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n ... 1 more\n `\n I really want to understand the reason behind this error and try the suggestions that you offer. I have been stuck on this problem since 2 weeks. I have read many similar errors, implemented some suggestions like increasing the cluster memory, configuring spark.executor.memory, repartitioning the data but still I cannot train the LightGBMClassifier with my input data.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "emnajaoua", "commentT": "2019-07-08T13:39:30Z", "comment_text": "\n \t\tCouple questions, are you running this on Databricks or a managed cluster service? The reason I ask is you will need to make sure the auto-scaling options are disabled for this algorithm to work properly.\n Also, have you tried MMLSpark version 0.17? There were improvements to the efficiency of LightGBM that might help.\n <denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  any other ideas for debugging?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "emnajaoua", "commentT": "2019-07-08T14:24:48Z", "comment_text": "\n \t\tHi, thank you for your reply. I am running this on Databricks.\n Concerning the auto-scaling option I didn't find as an option here in the lightgbm classifier <denchmark-link:url>https://mmlspark.azureedge.net/docs/pyspark/LightGBMClassifier.html</denchmark-link>\n \n I will try again with MMLSpark version 0.17\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "emnajaoua", "commentT": "2019-07-08T14:27:32Z", "comment_text": "\n \t\thi <denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  , sorry about the trouble you are having,\n <denchmark-code>from 2 until 10 worker nodes\n </denchmark-code>\n \n please make sure to disable dynamic allocation, as that is not supported yet\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "emnajaoua", "commentT": "2019-07-08T14:35:31Z", "comment_text": "\n \t\tSorry for the confusion, I meant auto-scaling as an option on the Databricks cluster. See info here: <denchmark-link:https://docs.databricks.com/user-guide/clusters/sizing.html>https://docs.databricks.com/user-guide/clusters/sizing.html</denchmark-link>\n \n Make sure Enable Auto-Scaling is disabled.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "emnajaoua", "commentT": "2019-07-08T15:36:43Z", "comment_text": "\n \t\tI did exactly as you recommended. Auto-scaling is disabled. Now I am working with 10 worker nodes and I have updated also MMLSpark to version 0.17.\n the model is training since 45 minutes and I am still waiting for the result:\n <denchmark-link:https://user-images.githubusercontent.com/26082645/60822931-b253f300-a1a6-11e9-90c3-cf0318a52e39.png></denchmark-link>\n \n It is stuck in this step. My question is my repartioning correct ?\n the train data had previously 200 partitions and then I decreased to 40 partitions.\n Also, is 10 worker nodes with 28 GB memory for each is enough in my case ?\n Thank you in advance :)\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "emnajaoua", "commentT": "2019-07-08T15:42:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  you have only 10 executors or tasks?  It looks like you have 40 tasks.  How many rows/columns do you have?  What is the current lightgbm debug output in the log4j logs - has it gotten past the network init stage?  If it hasn't gotten past network init then it may be stuck and time out (the driver might be waiting to get all of the workers and there may be fewer workers than it is expecting).\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "emnajaoua", "commentT": "2019-07-08T15:44:33Z", "comment_text": "\n \t\tLightgbm repartitions the data to the number of possible workers/tasks on the cluster, so if you have 200 partitions it will repartition the data before doing training.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "emnajaoua", "commentT": "2019-07-08T15:58:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  since I have 10 worker nodes I assume that I have 10 executors. or is not the case ?\n I have  (9 features and the column feature according to the feature_process) and   in my train data.\n Concerning the driver logs, you will find attached the log file, these are the last logs I am getting\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1096 (name: number of output rows)\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1061 (name: number of files read)\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1370 (name: internal.metrics.output.bytesWritten)\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1100 (name: collision rate (min, med, max))\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1308 (name: internal.metrics.input.sampledBytesRead)\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1063 (name: dynamic partition pruning time total (min, med, max))\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1032 (name: spill write time total (min, med, max))\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1079 (name: number of output rows)\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1271 (name: internal.metrics.shuffle.read.recordsRead)\n 19/07/08 15:32:29 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.139.64.4:36295 in memory (size: 46.3 KB, free: 9.4 GB)\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned shuffle 38\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1003 (name: dynamic partition pruning time total (min, med, max))\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1176 (name: internal.metrics.shuffle.read.localBlocksFetched)\n 19/07/08 15:32:29 INFO ContextCleaner: Cleaned accumulator 1141 (name: internal.metrics.peakExecutionMemory)\n 19/07/08 15:32:29 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.139.64.5:46621 in memory (size: 46.3 KB, free: 9.4 GB)\n 19/07/08 15:32:29 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.139.64.7:34257 in memory (size: 46.3 KB, free: 9.4 GB)\n 19/07/08 15:32:29 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.139.64.15:36649 in memory (size: 46.3 KB, free: 9.4 GB)\n 19/07/08 15:32:29 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.139.64.9:38729 in memory (size: 46.3 KB, free: 9.4 GB)\n 19/07/08 15:37:26 INFO HiveMetaStore: 2: get_database: default\n 19/07/08 15:37:26 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\n 19/07/08 15:37:26 INFO DriverCorral: Metastore health check ok\n 19/07/08 15:37:26 INFO DriverCorral: DBFS health check ok\n 19/07/08 15:42:26 INFO HiveMetaStore: 2: get_database: default\n 19/07/08 15:42:26 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\n 19/07/08 15:42:26 INFO DriverCorral: Metastore health check ok\n 19/07/08 15:42:26 INFO DriverCorral: DBFS health check ok\n 19/07/08 15:47:26 INFO HiveMetaStore: 2: get_database: default\n 19/07/08 15:47:26 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\n 19/07/08 15:47:26 INFO DriverCorral: Metastore health check ok\n 19/07/08 15:47:26 INFO DriverCorral: DBFS health check ok\n 19/07/08 15:52:26 INFO HiveMetaStore: 2: get_database: default\n 19/07/08 15:52:26 INFO audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\n 19/07/08 15:52:26 INFO DriverCorral: Metastore health check ok\n 19/07/08 15:52:26 INFO DriverCorral: DBFS health check ok\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:00:39Z", "comment_text": "\n \t\tI apologize I don't know exactly how to attach the log file that I got from driver logs but I think that it is not stuck at the init process.\n Concerning the repartitioning thing, do you think then that it is useless to repartition the train data before training the model ?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:07:39Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  how many CPUs do you have per worker?  Can you make sure it is set to 1?  That was a bug fixed on latest master (to allow multiple CPUs per task).  Are you running binary or multiclass classification and how many classes?  Does each partition have at least one of each label?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:10:30Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  you can also try running the build from latest master which has several fixes:\n --packages\n com.microsoft.ml.spark:mmlspark_2.11:0.17.dev27\n and --repositories\n <denchmark-link:https://mmlspark.azureedge.net/maven>https://mmlspark.azureedge.net/maven</denchmark-link>\n \n You can also try using barrier execution mode as I recommended here:\n <denchmark-link:https://github.com/Azure/mmlspark/issues/600>#600</denchmark-link>\n \n <denchmark-code>you can set .setUseBarrierExecutionMode(true) in scala or useBarrierExecutionMode=True in python.\n </denchmark-code>\n \n see doc for more info on barrier execution mode:\n <denchmark-link:https://github.com/Azure/mmlspark/blob/master/docs/lightgbm.md#barrier-execution-mode>https://github.com/Azure/mmlspark/blob/master/docs/lightgbm.md#barrier-execution-mode</denchmark-link>\n \n Which was introduced in spark 2.4.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:17:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n   Thank you for your help.\n I don't know exactly how many CPUs per worker but I would like to know. Here is an image of my cluster configuration\n <denchmark-link:https://user-images.githubusercontent.com/26082645/60825521-25ac3380-a1ac-11e9-996b-7f165b705b88.png></denchmark-link>\n \n I am not sure how to set 1 CPU per worker. is it like spark.conf.set property ?\n I am running a multiclass classification: 13 classes. To be honest I am not sure that each partition have at least one label/class but I would like to know how to do it or if you have any example for that.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:21:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n   it looks like you are using databricks, which by default has 1 CPU per task, so I think you are good unless you explicitly set\n <denchmark-code>spark.task.cpus 2\n </denchmark-code>\n \n eg:\n <denchmark-link:https://user-images.githubusercontent.com/24683184/60825994-f4673f80-a17a-11e9-9adc-d26b76a7dd77.png></denchmark-link>\n \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:23:10Z", "comment_text": "\n \t\t\n @emnajaoua you can also try running the build from latest master which has several fixes:\n --packages\n com.microsoft.ml.spark:mmlspark_2.11:0.17.dev27\n and --repositories\n https://mmlspark.azureedge.net/maven\n You can also try using barrier execution mode as I recommended here:\n #600\n you can set .setUseBarrierExecutionMode(true) in scala or useBarrierExecutionMode=True in python.\n \n see doc for more info on barrier execution mode:\n https://github.com/Azure/mmlspark/blob/master/docs/lightgbm.md#barrier-execution-mode\n Which was introduced in spark 2.4.\n \n I will try again with the useBarrierExecutionMode=True and the build from the last master. I will let you know tomorrow how did it go :)\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:24:26Z", "comment_text": "\n \t\t\n @emnajaoua it looks like you are using databricks, which by default has 1 CPU per task, so I think you are good unless you explicitly set\n spark.task.cpus 2\n \n eg:\n \n \n in my Spark config there is no specification for spark.task.cpus so I guess it is one CPU per task.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "emnajaoua", "commentT": "2019-07-08T16:24:54Z", "comment_text": "\n \t\tcan you also do a count of the distinct classes:\n <denchmark-link:https://stackoverflow.com/questions/30218140/spark-how-to-translate-countdistinctvalue-in-dataframe-apis>https://stackoverflow.com/questions/30218140/spark-how-to-translate-countdistinctvalue-in-dataframe-apis</denchmark-link>\n \n you can use countDistinct as one way to find out how many of each class you have, but not per partition\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "emnajaoua", "commentT": "2019-07-09T08:01:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  I have implemented the countDistinct on the raw data (before feature_processor) to know the number of classes as you recommended. as I mentioned before here are the 13 classes\n <denchmark-link:https://user-images.githubusercontent.com/26082645/60870147-91d07b00-a230-11e9-92bc-5b3da593b2b7.png></denchmark-link>\n \n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "emnajaoua", "commentT": "2019-07-09T08:21:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  as you recommended to me, I installed the latest repo for mmlspark and I set useBarrierExecutionMode=True.\n The model is still training (it has been 3 hours on this step):\n <denchmark-link:https://user-images.githubusercontent.com/26082645/60871543-4370ab80-a233-11e9-9a20-e276716d7b6d.png></denchmark-link>\n \n according to the logs (stdout) for one worker, it is still training\n <denchmark-link:https://user-images.githubusercontent.com/26082645/60871765-b0844100-a233-11e9-8103-b0c09a2747f3.png></denchmark-link>\n \n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "emnajaoua", "commentT": "2019-07-09T11:08:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  if you have gotten to that point then you must be training and you have gotten past the network initialization stage, which is where you were getting the connection refused error.  When you look at the workers, did any of them have status failure or success?  Sometimes a worker may exit earlier, I've seen that happen when a partition only has one label, but I think that is unlikely in your scenario. Also, with 13 classes training might take longer than usual, but based on your dataset size I don't think it should take 3 hours.  Are you seeing iterations progress in the debug?  I think it might be easier to just discuss over skype or teams, I was able to resolve an issue like that yesterday.  I can send you a teams link.\n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "emnajaoua", "commentT": "2019-07-09T11:17:16Z", "comment_text": "\n \t\tI wonder if the multiclass classifier is having issues with different numbers of labels on different partitions, since some of the labels have very few instances (eg 1 or 2).  Maybe I can add a test and see if I can reproduce this issue.\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "emnajaoua", "commentT": "2019-07-09T11:43:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  concerning whether the workers have failure or success status, I stopped the training now but if it is okey with you we can schedule today a skype meeting and before that I will run my notebook again so we can check the worker statues and depending on that, we will adjust :)\n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "emnajaoua", "commentT": "2019-07-09T15:27:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n \n here is a teams link if that works for you:\n <denchmark-link:https://teams.microsoft.com/l/meetup-join/19%3ameeting_NmU1NjgyODEtYjBkNS00YTg2LTk2MDYtNzdkNTc2NWUxZDU2%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%227ac33778-88d2-407b-bfe1-d64366fff0e4%22%7d>https://teams.microsoft.com/l/meetup-join/19%3ameeting_NmU1NjgyODEtYjBkNS00YTg2LTk2MDYtNzdkNTc2NWUxZDU2%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%227ac33778-88d2-407b-bfe1-d64366fff0e4%22%7d</denchmark-link>\n \n <denchmark-h:hr></denchmark-h>\n \n Join Microsoft Teams Meeting\n +1 347-991-7781   United States, New York City (Toll)\n (866) 641-7188   (Toll-free)\n Conference ID: 703 977 44#\n Local numbers | Reset PIN | Learn more about Teams | Meeting options\n <denchmark-h:hr></denchmark-h>\n \n when would you prefer to discuss?\n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "emnajaoua", "commentT": "2019-07-09T15:38:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  I trained the algorithm again without using repartitioning for the training data.\n this is what I got as an error:\n org.apache.spark.scheduler.BarrierJobUnsupportedRDDChainException: [SPARK-24820][SPARK-24821]: Barrier execution mode does not allow the following pattern of RDD chain within a barrier stage:\n <denchmark-h:hr></denchmark-h>\n \n Py4JJavaError                             Traceback (most recent call last)\n  in ()\n ----> 1 model = lgbm.fit(train)\n /databricks/spark/python/pyspark/ml/base.py in fit(self, dataset, params)\n 130                 return self.copy(params)._fit(dataset)\n 131             else:\n --> 132                 return self._fit(dataset)\n 133         else:\n 134             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n /databricks/spark/python/pyspark/ml/wrapper.py in _fit(self, dataset)\n 293\n 294     def _fit(self, dataset):\n --> 295         java_model = self._fit_java(dataset)\n 296         model = self._create_model(java_model)\n 297         return self._copyValues(model)\n /databricks/spark/python/pyspark/ml/wrapper.py in _fit_java(self, dataset)\n 290         \"\"\"\n 291         self._transfer_params_to_java()\n --> 292         return self._java_obj.fit(dataset._jdf)\n 293\n 294     def _fit(self, dataset):\n /databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in call(self, *args)\n 1255         answer = self.gateway_client.send_command(command)\n 1256         return_value = get_return_value(\n -> 1257             answer, self.gateway_client, self.target_id, self.name)\n 1258\n 1259         for temp_arg in temp_args:\n /databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\n 61     def deco(*a, **kw):\n 62         try:\n ---> 63             return f(*a, **kw)\n 64         except py4j.protocol.Py4JJavaError as e:\n 65             s = e.java_exception.toString()\n /databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n 326                 raise Py4JJavaError(\n 327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n --> 328                     format(target_id, \".\", name), value)\n 329             else:\n 330                 raise Py4JError(\n Py4JJavaError: An error occurred while calling o433.fit.\n : org.apache.spark.scheduler.BarrierJobUnsupportedRDDChainException: [SPARK-24820][SPARK-24821]: Barrier execution mode does not allow the following pattern of RDD chain within a barrier stage:\n \n Ancestor RDDs that have different number of partitions from the resulting RDD (eg. union()/coalesce()/first()/take()/PartitionPruningRDD). A workaround for first()/take() can be barrierRdd.collect().head (scala) or barrierRdd.collect()[0] (python).\n An RDD that depends on multiple barrier RDDs (eg. barrierRdd1.zip(barrierRdd2)).\n at org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithRDDChainPattern(DAGScheduler.scala:510)\n at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:585)\n at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1132)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2531)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2510)\n at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:893)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:2240)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:2338)\n at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1051)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n at org.apache.spark.rdd.RDD.withScope(RDD.scala:379)\n at org.apache.spark.rdd.RDD.reduce(RDD.scala:1033)\n at com.microsoft.ml.spark.LightGBMBase$class.innerTrain(LightGBMBase.scala:87)\n at com.microsoft.ml.spark.LightGBMClassifier.innerTrain(LightGBMClassifier.scala:24)\n at com.microsoft.ml.spark.LightGBMBase$class.train(LightGBMBase.scala:37)\n at com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:24)\n at com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:24)\n at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n at py4j.Gateway.invoke(Gateway.java:295)\n at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n at py4j.commands.CallCommand.execute(CallCommand.java:79)\n at py4j.GatewayConnection.run(GatewayConnection.java:251)\n at java.lang.Thread.run(Thread.java:748)\n \n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "emnajaoua", "commentT": "2019-07-09T15:44:13Z", "comment_text": "\n \t\t\n @emnajaoua\n here is a teams link if that works for you:\n https://teams.microsoft.com/l/meetup-join/19%3ameeting_NmU1NjgyODEtYjBkNS00YTg2LTk2MDYtNzdkNTc2NWUxZDU2%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%227ac33778-88d2-407b-bfe1-d64366fff0e4%22%7d\n Join Microsoft Teams Meeting\n +1 347-991-7781 United States, New York City (Toll)\n (866) 641-7188 (Toll-free)\n Conference ID: 703 977 44#\n Local numbers | Reset PIN | Learn more about Teams | Meeting options\n when would you prefer to discuss?\n \n I am available now if you like :)\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "emnajaoua", "commentT": "2019-07-09T15:44:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n   sure we can meet right now\n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "emnajaoua", "commentT": "2019-07-09T16:49:18Z", "comment_text": "\n \t\tdebugged on call, this issue looks similar to <denchmark-link:https://github.com/Azure/mmlspark/issues/569>#569</denchmark-link>\n  , the workers seem to be getting out of sync.  I wonder if it is due to unbalanced classes, since for some classes there are only 1 or 2 instances.\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "emnajaoua", "commentT": "2019-07-10T16:30:30Z", "comment_text": "\n \t\tas disussed yesterday, I will try the first solution which is removing classes that have less instances.\n This is the list of classes that I have\n <denchmark-link:https://user-images.githubusercontent.com/26082645/60986872-b9af0400-a340-11e9-9b30-e5768a5b6683.png></denchmark-link>\n \n they have more or less instances comparing to the previous dataframe.\n So the classes now have more instances than before. I have used the same configuration as discussed yesterday in the meeting but I received the connection refused error again.\n Here are screenshots from the worker stderr:\n <denchmark-link:https://user-images.githubusercontent.com/26082645/60986316-81f38c80-a33f-11e9-8f53-014d66adda0d.png></denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/26082645/60986413-b5361b80-a33f-11e9-9a68-f5d526d4cf67.png></denchmark-link>\n \n I doubt that this is due to another notebook in databricks using the same resources or this cannot be the reason ? I will test again tomorrow and will update you here.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "emnajaoua", "commentT": "2019-07-10T16:37:38Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n \n \"due to another notebook in databricks using the same resources or this cannot be the reason\"\n I don't think that would be the reason.\n The iterations are in order here though, which looks much better than when we looked at it.  How many iterations did you set, and how long did you run it for?  Also, did you shuffle the dataset beforehand (I believe the classes should be distributed across all partitions such that on each partition at least one instance of each class should appear, but I could be wrong)?\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "emnajaoua", "commentT": "2019-07-10T16:54:17Z", "comment_text": "\n \t\talso if you remove classes 12, 3, 5, 7 - do you still see the connection refused error?  What if you just have classes 2 and 4 and use binary classification?  Also, are you using string indexer on the label column - the labels need to start from 0 to n.  I'm also wondering if you could try using multiclassova objective, although it would be much slower, it's worth trying it out: <denchmark-link:https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#objective>https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst#objective</denchmark-link>\n \n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "emnajaoua", "commentT": "2019-07-11T02:41:42Z", "comment_text": "\n \t\thi <denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  I was able to reproduce the issue for multiclass classifier, I've put the test case in a branch:\n <denchmark-link:https://github.com/imatiach-msft/mmlspark/commit/c2568b11ab6e4f74a7f349d84cb8358437eed2b2>imatiach-msft@c2568b1</denchmark-link>\n \n I've confirmed the following:\n 1.) If the labels on a partition skip a value, eg [0 to j] inclusive, and [j+2 to k], lightgbm multiclass classifier gets stuck\n 2.) if parititions have different labels but some have fewer than others, eg one has 0 to k and another has 0 to k+1, lightgbm multiclass classifier finishes\n My recommendation is to ensure that all partitions have all labels from 0 to total number of labels.\n In your image above you have labels 12, 1, 3, 5, 4, 8, 7, 2 - but you are missing label 0, 6, 9, 10, 11.\n Can you try running StringIndexer or ValueIndexer, as I do in the multiclass lightgbm tests:\n <denchmark-code>import com.microsoft.ml.spark.featurize.ValueIndexer\n val labelizer = new ValueIndexer().setInputCol(labelColumnName).setOutputCol(labelColumnName).fit(tmpTrainData)\n     val labelizedData = labelizer.transform(tmpTrainData)\n </denchmark-code>\n \n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "emnajaoua", "commentT": "2019-07-13T17:39:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  I did some testing of my StatifiedRepartition transformer and I've found several surprising things that I misunderstood:\n 1.) sampleByKey and sampleByKeyExact only samples values within a partition by key\n 2.) The ratios samples for each key, so they don't need to add up to 1.  The ratios can also be larger than 1.\n I still don't see a way to do a shuffle that absolutely guarantees that the labels will be \"stratified\" across all partitions equally.  That's the part that I am still trying to figure out.\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "emnajaoua", "commentT": "2019-07-14T04:04:14Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  great news, I was able to:\n 1.) create a transformer that you can use to do the stratified repartitioning of the data\n 2.) throw an early termination error within lightgbm multiclass classifier if any labels are skipped - I also added the test that I mentioned above and I was able to validate that lightgbm no longer gets stuck and terminates early\n I created a PR here:\n <denchmark-link:https://github.com/Azure/mmlspark/pull/618>#618</denchmark-link>\n \n will let you know when a build is ready so you can try it out\n \t\t"}, "comments_32": {"comment_id": 33, "comment_author": "emnajaoua", "commentT": "2019-07-15T07:13:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n   Thank you so much for your support so far. I am still working on balancing the dataset. I think by that time your PR will be merged :)\n \t\t"}, "comments_33": {"comment_id": 34, "comment_author": "emnajaoua", "commentT": "2019-07-15T11:14:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  I've got a build for my PR here:\n --packages\n com.microsoft.ml.spark:mmlspark:0.17+79-2d5b4dda\n and --repositories\n <denchmark-link:https://mmlspark.azureedge.net/maven>https://mmlspark.azureedge.net/maven</denchmark-link>\n \n would you be able to try it out?  Specifically, you will need to use ValueIndexer followed by StratifiedRepartition, which should do the balancing for you (but it will make the dataset much larger).  It will make all labels have the same number of instances.  I may need to test it a bit more, and maybe update the interface, but it should work for you unless you go out of memory due to the larger number of instances on your cluster.\n I also added an exception to LightGBM, so it would terminate much faster with a good error message, instead of getting stuck when some labels are missing on some partitions.\n \t\t"}, "comments_34": {"comment_id": 35, "comment_author": "emnajaoua", "commentT": "2019-07-15T11:18:03Z", "comment_text": "\n \t\tyou will just need to call in pyspark:\n from mmlspark.stages import StratifiedRepartition\n stratifiedTrainData = StratifiedRepartition(labelCol=\"my_label\").transform(trainData)\n the pyspark bindings for the PR should be automatically generated\n \t\t"}, "comments_35": {"comment_id": 36, "comment_author": "emnajaoua", "commentT": "2019-07-15T11:23:34Z", "comment_text": "\n \t\talso, if it does go out of memory, you can just call it with \"original\" instead of \"equal\" fraction parameter, but you will need to make sure the number of instances for each label is at least as many as the number of partitions (and hopefully several times more).\n \t\t"}, "comments_36": {"comment_id": 37, "comment_author": "emnajaoua", "commentT": "2019-07-15T11:29:18Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  I am kind of stuck in the process of balancing the dataset. Before when I tried the ValueIndexer and StratifiedSampler it didn't work with this those labels\n <denchmark-link:https://user-images.githubusercontent.com/26082645/61213157-81267600-a704-11e9-973f-95f3b8f30f15.png></denchmark-link>\n \n when I worked before with only those classes [11, 0, 2, 4, 3, 7, 6, 1], the training was completed.\n \t\t"}, "comments_37": {"comment_id": 38, "comment_author": "emnajaoua", "commentT": "2019-07-15T11:30:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  the StratifiedRepartition transformer I added will balance the labels by default, so you will have the same count for each label\n \t\t"}, "comments_38": {"comment_id": 39, "comment_author": "emnajaoua", "commentT": "2019-07-15T12:15:12Z", "comment_text": "\n \t\t\n @emnajaoua the StratifiedRepartition transformer I added will balance the labels by default, so you will have the same count for each label\n \n This is the labels screenshot after using the stratifiedTransformer\n <denchmark-link:https://user-images.githubusercontent.com/26082645/61215327-d9f90d00-a70a-11e9-875e-edb83d192f50.png></denchmark-link>\n \n As sais previsouly , this is the code I am using for stratified sampling\n fractions = labelizedData.select(\"kategorie1\").distinct().withColumn(\"fraction\", lit(0.3)).rdd.collectAsMap()\n sampled_df = labelizedData.stat.sampleBy(\"kategorie1\", fractions, seed=12)\n I will remove label 10 and merge label 8 and 9 together.\n \t\t"}, "comments_39": {"comment_id": 40, "comment_author": "emnajaoua", "commentT": "2019-07-15T13:20:23Z", "comment_text": "\n \t\tI have an idea to use weightCol parameter in the LightGBMClassifier, may be it will help balancing the data.\n <denchmark-link:https://mmlspark.azureedge.net/docs/pyspark/LightGBMClassifier.html>https://mmlspark.azureedge.net/docs/pyspark/LightGBMClassifier.html</denchmark-link>\n \n \t\t"}, "comments_40": {"comment_id": 41, "comment_author": "emnajaoua", "commentT": "2019-07-15T13:42:33Z", "comment_text": "\n \t\tSorry, what does the code look like with stratified repartition?  Can you make sure to not use any sampling logic like above and get the count after stratified repartition?  Also, maybe we can meet over teams, as I'm a bit confused that after running the stratified repartition you are not getting the same counts for all labels.\n \n Get Outlook for Android<<denchmark-link:https://aka.ms/ghei36>https://aka.ms/ghei36</denchmark-link>\n >\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n ________________________________\n From: emnajaoua <notifications@github.com>\n Sent: Monday, July 15, 2019 9:20:29 AM\n To: Azure/mmlspark <mmlspark@noreply.github.com>\n Cc: Ilya Matiach <ilmat@microsoft.com>; Mention <mention@noreply.github.com>\n Subject: Re: [Azure/mmlspark] java.net.ConnectException: Connection refused (Connection refused) with LightGBMClassifier in Databricks (#609)\n \n \n I have an idea to use weightCol parameter in the LightGBMClassifier, may be it will help balancing the data.\n https://mmlspark.azureedge.net/docs/pyspark/LightGBMClassifier.html<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fmmlspark.azureedge.net%2Fdocs%2Fpyspark%2FLightGBMClassifier.html&data=02%7C01%7Cilmat%40microsoft.com%7C09857e6343c14d24c95b08d70927355a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636987936316068436&sdata=AZW7brGm4IID6AnOIQDRqndk3dfKIPHczDeM4Anc0ZU%3D&reserved=0>\n \n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2Fmmlspark%2Fissues%2F609%3Femail_source%3Dnotifications%26email_token%3DAF4KFMFKTR7AOZF7EP36GMLP7R2R3A5CNFSM4H63URZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ5VHRI%23issuecomment-511398853&data=02%7C01%7Cilmat%40microsoft.com%7C09857e6343c14d24c95b08d70927355a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636987936316078441&sdata=90d0ZYM%2FKKrAx6pPGlulVjUvB8sMkIIIMdbT5Mlyxa8%3D&reserved=0>, or mute the thread<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAF4KFMFGODAN3SGV3DE5L6LP7R2R3ANCNFSM4H63URZQ&data=02%7C01%7Cilmat%40microsoft.com%7C09857e6343c14d24c95b08d70927355a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636987936316078441&sdata=f%2B668NwDK7lnyhG%2BNODegOxpi3v0MNBbBa9rVhn1Tiw%3D&reserved=0>.\n \n \t\t"}, "comments_41": {"comment_id": 42, "comment_author": "emnajaoua", "commentT": "2019-07-15T15:34:30Z", "comment_text": "\n \t\t\n , may\n \n I also don't know why it is not getting the same counts for all labels after using straftified repartition. The only thing that I am using before stratified partition is the Value Indexer.\n So basically this is my data processing code before training the model\n raw_train, raw_test = spark_random_split(projectYFeaturesTransformed_df.filter(col(\"kategorie1\") != 10), ratio=0.8, seed=42)\n columns = [c for c in projectYFeaturesTransformed_df.columns if c != 'kategorie1']\n feature_processor = FeatureHasher(inputCols=columns, outputCol='features')\n train = feature_processor.transform(raw_train)\n test = feature_processor.transform(raw_test)\n #ensure that all partitions have all labels\n labelizer = ValueIndexer(inputCol=\"kategorie1\", outputCol=\"kategorie1\").fit(train)\n labelizedData = labelizer.transform(train)\n sampled_df = labelizedData.stat.sampleBy('kategorie1', {0: .60, 1: .50, 2: .90, 3: .60, 4: 1, 5: 1, 6: 1, 7: .90, 8: 1, 9: 1, 10: 1, 11: 1})\n if you have time, we can discuss over Teams as this issue is taking so much time\n \t\t"}, "comments_42": {"comment_id": 43, "comment_author": "emnajaoua", "commentT": "2019-07-15T15:40:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  that code won't work because you are sampling each label, the ratios actually need to be >1 for some of your labels, and none should be less than 1.  You can use the StratifiedRepartition transformer I wrote in my PR by using the build I send you and it should handle this.  Specifically, this code won't work as you are expecting:\n <denchmark-code>sampled_df = labelizedData.stat.sampleBy('kategorie1', {0: .60, 1: .50, 2: .90, 3: .60, 4: 1, 5: 1, 6: 1, 7: .90, 8: 1, 9: 1, 10: 1, 11: 1})\n </denchmark-code>\n \n I think I was confused myself before and I may have confused you as well.  The ratios are for sampling by each label.  For example, if you have ratio .26 and 100 instances of label k, this will sample label k to 26 instances.\n \t\t"}, "comments_43": {"comment_id": 44, "comment_author": "emnajaoua", "commentT": "2019-07-15T15:41:12Z", "comment_text": "\n \t\there is a teams link if that works for you:\n <denchmark-link:https://teams.microsoft.com/l/meetup-join/19%3ameeting_NmU1NjgyODEtYjBkNS00YTg2LTk2MDYtNzdkNTc2NWUxZDU2%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%227ac33778-88d2-407b-bfe1-d64366fff0e4%22%7d>https://teams.microsoft.com/l/meetup-join/19%3ameeting_NmU1NjgyODEtYjBkNS00YTg2LTk2MDYtNzdkNTc2NWUxZDU2%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%227ac33778-88d2-407b-bfe1-d64366fff0e4%22%7d</denchmark-link>\n \n Join Microsoft Teams Meeting\n +1 347-991-7781 United States, New York City (Toll)\n (866) 641-7188 (Toll-free)\n Conference ID: 703 977 44#\n Local numbers | Reset PIN | Learn more about Teams | Meeting options\n when would you prefer to discuss?\n \t\t"}, "comments_44": {"comment_id": 45, "comment_author": "emnajaoua", "commentT": "2019-07-15T15:45:11Z", "comment_text": "\n \t\t\n @emnajaoua that code won't work because you are sampling each label, the ratios actually need to be >1 for some of your labels, and none should be less than 1. You can use the StratifiedRepartition transformer I wrote in my PR by using the build I send you and it should handle this. Specifically, this code won't work as you are expecting:\n sampled_df = labelizedData.stat.sampleBy('kategorie1', {0: .60, 1: .50, 2: .90, 3: .60, 4: 1, 5: 1, 6: 1, 7: .90, 8: 1, 9: 1, 10: 1, 11: 1})\n \n I think I was confused myself before and I may have confused you as well. The ratios are for sampling by each label. For example, if you have ratio .26 and 100 instances of label k, this will sample label k to 26 instances.\n \n If I change the ratios to more than 1, I get this error\n IllegalArgumentException: 'requirement failed: Fractions must be in [0, 1], but got Map(0 -> 1.5, 5 -> 1.0, 10 -> 1.0, 1 -> 1.5, 6 -> 1.0, 9 -> 1.0, 2 -> 1.9, 7 -> 1.9, 3 -> 1.6, 11 -> 1.0, 8 -> 1.0, 4 -> 1.0).'\n I will use your build then tomorrow and we will discuss over Teams. Do you have time tomorrow at 15:00 (German Time)\n \t\t"}, "comments_45": {"comment_id": 46, "comment_author": "emnajaoua", "commentT": "2019-07-15T15:54:24Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  you are using the wrong function, you should use the StratifiedRepartition transformer I wrote in my PR instead of calling samplyBy in pyspark, it looks sampleBy is different from the method I am using and doesn't have this restriction, see this implementation of sampleBy which you are using that throws the error:\n <denchmark-link:https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala#L420>https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala#L420</denchmark-link>\n \n there is no such restriction on the method I am using:\n <denchmark-link:https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L287>https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L287</denchmark-link>\n \n \t\t"}, "comments_46": {"comment_id": 47, "comment_author": "emnajaoua", "commentT": "2019-07-15T16:03:13Z", "comment_text": "\n \t\t\"Do you have time tomorrow at 15:00 (German Time)\"\n <denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  that is 9 am EST, I'm usually at work 10 am - 6/7 pm.  Does 16:00 (German Time) work for you?  Thanks!\n \t\t"}, "comments_47": {"comment_id": 48, "comment_author": "emnajaoua", "commentT": "2019-07-15T21:48:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  actually, I think I may be able to meet at 9 am to debug this with you\n \t\t"}, "comments_48": {"comment_id": 49, "comment_author": "emnajaoua", "commentT": "2019-07-16T06:59:53Z", "comment_text": "\n \t\t\n @emnajaoua actually, I think I may be able to meet at 9 am to debug this with you\n \n So we will discuss the issue 9 a.m means 15.00 (german time)\n I am afraid to tell you that I couldn't install the latest build that you sent me on databricks.\n <denchmark-link:https://user-images.githubusercontent.com/26082645/61273478-ee401700-a7a9-11e9-98e6-fbc030d34090.png></denchmark-link>\n \n \t\t"}, "comments_49": {"comment_id": 50, "comment_author": "emnajaoua", "commentT": "2019-07-16T13:02:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  I am online, waiting for you\n \t\t"}, "comments_50": {"comment_id": 51, "comment_author": "emnajaoua", "commentT": "2019-07-16T13:13:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  sorry joined a bit late, online right now\n \t\t"}, "comments_51": {"comment_id": 52, "comment_author": "emnajaoua", "commentT": "2019-07-19T03:57:49Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  I've updated the PR such that, if you set generateMissingLabels=True, lightgbm will automatically generate any missing labels during train, thereby guaranteeing that it won't get stuck or fail:\n <denchmark-link:https://github.com/Azure/mmlspark/pull/618>#618</denchmark-link>\n \n In that scenario, you won't need StratifiedRepartition.  I also added another mode (renamed the fraction param to mode) to StratifiedRepartition, mode=\"mixed\", so that the majority class would be downsampled and the minority oversampled for all classes.\n Please try out the new build with:\n 1.) Running lightgbm without StratifiedRepartition and with generateMissingLabels param\n 2.) Using StratifiedRepartition with new mixed mode\n \t\t"}, "comments_52": {"comment_id": 53, "comment_author": "emnajaoua", "commentT": "2019-07-19T04:15:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/emnajaoua>@emnajaoua</denchmark-link>\n  the new build should be:\n --packages\n com.microsoft.ml.spark:mmlspark_2.11:0.17+83-11237da2\n and --repositories\n <denchmark-link:https://mmlspark.azureedge.net/maven>https://mmlspark.azureedge.net/maven</denchmark-link>\n \n \t\t"}, "comments_53": {"comment_id": 54, "comment_author": "emnajaoua", "commentT": "2019-08-05T06:39:46Z", "comment_text": "\n \t\tUser class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 6.0 failed 4 times, most recent failure: Lost task 23.3 in stage 6.0 (TID 684, zjy-hadoop-prc-st19.bj, executor 18): java.net.ConnectException: Connection refused (Connection refused)\n at java.net.PlainSocketImpl.socketConnect(Native Method)\n at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n at java.net.Socket.connect(Socket.java:589)\n at java.net.Socket.connect(Socket.java:538)\n at java.net.Socket.(Socket.java:434)\n at java.net.Socket.(Socket.java:211)\n at com.microsoft.ml.spark.TrainUtils$.getNetworkInitNodes(TrainUtils.scala:240)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$7.apply(TrainUtils.scala:303)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$7.apply(TrainUtils.scala:298)\n at com.microsoft.ml.spark.StreamUtilities$.using(StreamUtilities.scala:29)\n at com.microsoft.ml.spark.TrainUtils$.trainLightGBM(TrainUtils.scala:297)\n at com.microsoft.ml.spark.LightGBMBase$$anonfun$4.apply(LightGBMBase.scala:49)\n at com.microsoft.ml.spark.LightGBMBase$$anonfun$4.apply(LightGBMBase.scala:49)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:186)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:183)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:844)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:844)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n at org.apache.spark.scheduler.Task.run(Task.scala:100)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)\n Driver stacktrace:\n at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1515)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1503)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1502)\n at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1502)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:816)\n at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:816)\n at scala.Option.foreach(Option.scala:257)\n at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:816)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1740)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1695)\n at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1684)\n at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:641)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:1957)\n at org.apache.spark.SparkContext.runJob(SparkContext.scala:2020)\n at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1059)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n at org.apache.spark.rdd.RDD.reduce(RDD.scala:1041)\n at org.apache.spark.sql.Dataset.reduce(Dataset.scala:1426)\n at com.microsoft.ml.spark.LightGBMBase$class.train(LightGBMBase.scala:51)\n at com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:23)\n at com.microsoft.ml.spark.LightGBMClassifier.train(LightGBMClassifier.scala:23)\n at org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n at org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n at com.microsoft.ml.spark.TrainClassifier.fit(TrainClassifier.scala:171)\n at com.xiaomi.yuanjie.push.ModelWZ$.main(ModelWZ.scala:51)\n at com.xiaomi.yuanjie.push.ModelWZ.main(ModelWZ.scala)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:682)\n Caused by: java.net.ConnectException: Connection refused (Connection refused)\n at java.net.PlainSocketImpl.socketConnect(Native Method)\n at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n at java.net.Socket.connect(Socket.java:589)\n at java.net.Socket.connect(Socket.java:538)\n at java.net.Socket.(Socket.java:434)\n at java.net.Socket.(Socket.java:211)\n at com.microsoft.ml.spark.TrainUtils$.getNetworkInitNodes(TrainUtils.scala:240)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$7.apply(TrainUtils.scala:303)\n at com.microsoft.ml.spark.TrainUtils$$anonfun$7.apply(TrainUtils.scala:298)\n at com.microsoft.ml.spark.StreamUtilities$.using(StreamUtilities.scala:29)\n at com.microsoft.ml.spark.TrainUtils$.trainLightGBM(TrainUtils.scala:297)\n at com.microsoft.ml.spark.LightGBMBase$$anonfun$4.apply(LightGBMBase.scala:49)\n at com.microsoft.ml.spark.LightGBMBase$$anonfun$4.apply(LightGBMBase.scala:49)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:186)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$6.apply(objects.scala:183)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:844)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:844)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n at org.apache.spark.scheduler.Task.run(Task.scala:100)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)\n \t\t"}, "comments_54": {"comment_id": 55, "comment_author": "emnajaoua", "commentT": "2019-08-26T00:31:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/imatiach-msft>@imatiach-msft</denchmark-link>\n  - I have a large number of classes (>100) which are highly imbalanced. I followed the discussion above and tried:\n \n Removing classes with less than 500 samples.\n Using ValueIndexer followed by StratifiedRepartition (both using mode=\"original\" and mode=\"mixed which generates a HUGE dataset).\n Using ValueIndexer by itself.\n I also tried using pyspark StringIndexer and creating an increasing ID column myself.\n \n     value_indexer = ValueIndexer(inputCol=\"CLASS\", outputCol=\"label\").fit(train)\n     train = value_indexer.transform(train)\n     test = value_indexer.transform(test)\n     stratifiedTrainData = StratifiedRepartition(labelCol=\"label\", mode=\"original\").transform(train_limited)\n     model = LightGBMClassifier(objective='multiclass', labelCol =\"label\", featuresCol=\"features\").fit(stratifiedTrainData)\n error is the same:\n <denchmark-code>org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 131.0 failed 4 \n times, most recent failure: Lost task 20.3 in stage 131.0 (TID 6318, 10.179.68.6, executor 3): \n java.net.ConnectException: Connection refused (Connection refused)\n </denchmark-code>\n \n Features is a very large vector featurised as per PySpark requirements (it contains one hot encoded ngrams + other features).\n Cluster is: 5.5 (includes Apache Spark 2.4.3, Scala 2.11)\n 3 Workers + 1 Master\n (does not auto scale)\n MMLSpark: com.microsoft.ml.spark:mmlspark_2.11:0.18.1\n Also tried with: com.microsoft.ml.spark:mmlspark_2.11:0.17+83-11237da2\n Any ideas here?\n edit:\n relevant error seems to be:\n \n 19/08/26 01:06:53 INFO TaskSetManager: Starting task 17.1 in stage 38.0 (TID 960, 10.179.68.4, executor 2, partition 17, NODE_LOCAL, 5528 bytes)\n 19/08/26 01:06:53 WARN TaskSetManager: Lost task 19.0 in stage 38.0 (TID 940, 10.179.68.4, executor 2): java.lang.Exception: For classification, label values must start from 0 and increase by 1 to n for each partition.  Missing label 0, unique labels 31,32,35,55,73,86,99,113,114,116,133,166\n \n Edit --> Working when re-partitioned. I'm assuming re-partitioning occurs randomly for the labels? How can I re-partition while still benefiting from parallelism?\n \t\t"}, "comments_55": {"comment_id": 56, "comment_author": "emnajaoua", "commentT": "2019-09-14T19:09:40Z", "comment_text": "\n \t\t\n Lightgbm repartitions the data to the number of possible workers/tasks on the cluster, so if you have 200 partitions it will repartition the data before doing training.\n This is good in the case when you have only one model training at a time. If I am training multiple models, this makes it a bit slower because of the increase in the total number of tasks including all different training datas. Was this considered to make lightGBM efficient? And any tips on this will be helpful.\n \n \t\t"}, "comments_56": {"comment_id": 57, "comment_author": "emnajaoua", "commentT": "2019-10-09T03:27:09Z", "comment_text": "\n \t\tthe unbalanced data issue causing lightgbm to be stuck should be fixed with the latest update to native lightgbm 2.3.1:\n <denchmark-link:https://github.com/Azure/mmlspark/pull/705>#705</denchmark-link>\n \n the error message has now been removed, lightgbm can now work on data with missing labels in both multiclass and binary case\n \t\t"}}}, "commit": {"commit_id": "d518b8aa3aae7ace6608742271f7873decb76b84", "commit_author": "Ilya Matiach", "commitT": "2019-08-20 00:34:58-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMClassifier.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMClassifier.scala", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "55,61", "deleted_lines": "48,54", "method_info": {"method_name": "getTrainParams", "method_params": "Int", "method_startline": "48", "method_endline": "64"}}, "hunk_1": {"Ismethod": 1, "added_lines": "41", "deleted_lines": null, "method_info": {"method_name": "setIsUnbalance", "method_params": "Boolean", "method_startline": "39", "method_endline": "41"}}, "hunk_2": {"Ismethod": 1, "added_lines": "46,47", "deleted_lines": "48", "method_info": {"method_name": "setGenerateMissingLabels", "method_params": "Boolean", "method_startline": "46", "method_endline": "48"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMConstants.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMConstants.scala", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,23,24", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\TrainParams.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\TrainParams.scala", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "55", "deleted_lines": "55"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\TrainUtils.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\TrainUtils.scala", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "259,262,263", "deleted_lines": null, "method_info": {"method_name": "translate", "method_params": "String,String,Logger,TrainParams,StructType", "method_startline": "249", "method_endline": "284"}}, "hunk_1": {"Ismethod": 1, "added_lines": "27,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58", "deleted_lines": "26", "method_info": {"method_name": "generateDataset", "method_params": "String,String,StructType,Logger", "method_startline": "23", "method_endline": "67"}}, "hunk_2": {"Ismethod": 1, "added_lines": "27,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58", "deleted_lines": "26", "method_info": {"method_name": "generateDataset", "method_params": "String,String,StructType,Logger,TrainParams", "method_startline": "24", "method_endline": "97"}}}}, "file_4": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\stages\\StratifiedRepartition.scala"}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "src\\test\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\split1\\VerifyLightGBMClassifier.scala", "file_new_name": "src\\test\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\split1\\VerifyLightGBMClassifier.scala", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "14,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456", "deleted_lines": "14"}}}, "file_6": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "src\\test\\scala\\com\\microsoft\\ml\\spark\\stages\\StratifiedRepartitionSuite.scala"}}}}