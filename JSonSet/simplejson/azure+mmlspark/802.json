{"BR": {"BR_id": "802", "BR_author": "13pranav5", "BRopenT": "2020-02-15T01:24:15Z", "BRcloseT": "2020-02-28T15:34:59Z", "BR_text": {"BRsummary": "Error - LightGBMClassifier during train with initScoreCol for multi-label classification:  java.lang.ClassCastException: org.apache.spark.ml.linalg.SparseVector cannot be cast to java.lang.Double", "BRdescription": "\n I get the below error during training of the model. I am using AWS Databricks and the latest snapshot release of MMLSpark package.\n This happens only when i provide initScoreCol parameter. What should be the data type for the inirScoreCol values?\n 20/02/15 01:20:22 INFO LightGBMClassifier: driver closing all sockets and server socket\n 20/02/15 01:20:44 WARN TaskSetManager: Lost task 15.0 in stage 26.0 (TID 4100, 10.96.150.62, executor 6): java.lang.ClassCastException: org.apache.spark.ml.linalg.SparseVector cannot be cast to java.lang.Double\n at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)\n at org.apache.spark.sql.Row$class.getDouble(Row.scala:259)\n at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$generateDataset$2$$anonfun$7.apply(TrainUtils.scala:63)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$generateDataset$2$$anonfun$7.apply(TrainUtils.scala:63)\n at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$generateDataset$2.apply(TrainUtils.scala:63)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$generateDataset$2.apply(TrainUtils.scala:62)\n at scala.Option.foreach(Option.scala:257)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.generateDataset(TrainUtils.scala:62)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.translate(TrainUtils.scala:233)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.trainLightGBM(TrainUtils.scala:385)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:145)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:145)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:200)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:197)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:865)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:865)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n at org.apache.spark.scheduler.Task.run(Task.scala:113)\n at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)\n 20/02/15 01:20:44 INFO TaskSetManager: Starting task 15.1 in stage 26.0 (TID 4325, 10.96.145.58, executor 8, partition 227, PROCESS_LOCAL, 5633 bytes)\n 20/02/15 01:20:44 WARN TaskSetManager: Lost task 15.1 in stage 26.0 (TID 4325, 10.96.145.58, executor 8): java.net.ConnectException: Connection refused (Connection refused)\n at java.net.PlainSocketImpl.socketConnect(Native Method)\n at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n at java.net.Socket.connect(Socket.java:589)\n at java.net.Socket.connect(Socket.java:538)\n at java.net.Socket.(Socket.java:434)\n at java.net.Socket.(Socket.java:211)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.getNetworkInitNodes(TrainUtils.scala:299)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$12.apply(TrainUtils.scala:372)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$$anonfun$12.apply(TrainUtils.scala:367)\n at com.microsoft.ml.spark.core.env.StreamUtilities$.using(StreamUtilities.scala:28)\n at com.microsoft.ml.spark.lightgbm.TrainUtils$.trainLightGBM(TrainUtils.scala:366)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:145)\n at com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(LightGBMBase.scala:145)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:200)\n at org.apache.spark.sql.execution.MapPartitionsExec$$anonfun$5.apply(objects.scala:197)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:865)\n at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:865)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n at org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n at org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n at org.apache.spark.scheduler.Task.run(Task.scala:113)\n at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "13pranav5", "commentT": "2020-02-26T05:36:16Z", "comment_text": "\n \t\tThank you for finding this issue, I've sent the fix here:\n <denchmark-link:https://github.com/Azure/mmlspark/pull/805>#805</denchmark-link>\n \n I\u2019ve based the fix on the lightgbm python code here:\n <denchmark-link:https://github.com/Microsoft/LightGBM/blob/master/python-package/lightgbm/basic.py#L840>https://github.com/Microsoft/LightGBM/blob/master/python-package/lightgbm/basic.py#L840</denchmark-link>\n \n And here:\n <denchmark-link:https://github.com/microsoft/LightGBM/blob/master/src/io/metadata.cpp#L405>https://github.com/microsoft/LightGBM/blob/master/src/io/metadata.cpp#L405</denchmark-link>\n \n Also see related issue:\n <denchmark-link:https://github.com/microsoft/LightGBM/issues/2595>microsoft/LightGBM#2595</denchmark-link>\n \n LightGBM actually only takes a single column, which caused some confusion for me, but for multiclass case the column of vectors need to be reshaped to a single column where all values for class 0 are followed by all values for class 1, etc.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "13pranav5", "commentT": "2020-02-28T15:34:59Z", "comment_text": "\n \t\tclosing as this issue has been fixed and merged into master with PR <denchmark-link:https://github.com/Azure/mmlspark/pull/805>#805</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "df0244c7b4f48e6c86bd8a5478d4b674a9a554ce", "commit_author": "Ilya Matiach", "commitT": "2020-02-27 22:08:50-05:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMBase.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMBase.scala", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "112", "deleted_lines": "112", "method_info": {"method_name": "getTrainingCols", "method_params": "", "method_startline": "105", "method_endline": "120"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMDataset.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\LightGBMDataset.scala", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "58", "deleted_lines": "58", "method_info": {"method_name": "addDoubleField", "method_params": "String,Int", "method_startline": "54", "method_endline": "72"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\TrainUtils.scala", "file_new_name": "src\\main\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\TrainUtils.scala", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103", "deleted_lines": null, "method_info": {"method_name": "addInitScoreColumn", "method_params": "Int,StructType", "method_startline": "81", "method_endline": "104"}}, "hunk_1": {"Ismethod": 1, "added_lines": "61,63", "deleted_lines": "61,62,63,64", "method_info": {"method_name": "generateDataset", "method_params": "String,String,StructType,Logger,TrainParams", "method_startline": "24", "method_endline": "67"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "src\\test\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\split1\\VerifyLightGBMClassifier.scala", "file_new_name": "src\\test\\scala\\com\\microsoft\\ml\\spark\\lightgbm\\split1\\VerifyLightGBMClassifier.scala", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "244", "deleted_lines": null, "method_info": {"method_name": "assertBinaryImprovement", "method_params": "DataFrame,DataFrame", "method_startline": "240", "method_endline": "244"}}, "hunk_1": {"Ismethod": 1, "added_lines": "244,245,246,247", "deleted_lines": null, "method_info": {"method_name": "assertMulticlassImprovement", "method_params": "DataFrame,DataFrame", "method_startline": "244", "method_endline": "248"}}}}}}}