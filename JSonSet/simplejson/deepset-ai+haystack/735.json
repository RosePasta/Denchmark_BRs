{"BR": {"BR_id": "735", "BR_author": "brandenchan", "BRopenT": "2021-01-14T10:51:37Z", "BRcloseT": "2021-01-22T09:06:38Z", "BR_text": {"BRsummary": "Ram Issue in Tutorial 9 (DPR Training) Colab", "BRdescription": "\n When running the DPR Training tutorial in Colab, the download of the training dataset seems to run fine, but the program crashes before starting to download the dev file due to running out of RAM.\n # Download original DPR data\n # WARNING: the train set is 7.4GB and the dev set is 800MB\n doc_dir = \"data/dpr_training/\"\n s3_url_train = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\"\n s3_url_dev = \"https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz\"\n fetch_archive_from_http(s3_url_train, output_dir=doc_dir + \"train/\")\n fetch_archive_from_http(s3_url_dev, output_dir=doc_dir + \"dev/\")\n We need to find some way to reduce this RAM consumption. It likely has something to do with the way the files are being uncompressed\n def fetch_archive_from_http(url: str, output_dir: str, proxies: Optional[dict] = None):\n             ...\n             elif url[-3:] == \".gz\":\n                 json_bytes = gzip.open(temp_file.name).read()\n                 filename = url.split(\"/\")[-1].replace(\".gz\", \"\")\n                 output_filename = Path(output_dir) / filename\n                 output = open(output_filename, \"wb\")\n                 output.write(json_bytes)\n             else:\n                 ...\n         return True\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "brandenchan", "commentT": "2021-01-14T11:04:57Z", "comment_text": "\n \t\tHow about our tutorial just uses small files so people can quickly go through the code + execution, and we have the links for the large datafiles for interested users as comments?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "brandenchan", "commentT": "2021-01-14T14:32:42Z", "comment_text": "\n \t\tActually following line read whole file in memory also decompress it in memory as well.\n <denchmark-code>json_bytes = gzip.open(temp_file.name).read()\n </denchmark-code>\n \n Python 3 support buffered reading automatically so if change it to as follows memory utilization will improve -\n <denchmark-code>            elif url[-3:] == \".gz\":\n                 filename = url.split(\"/\")[-1].replace(\".gz\", \"\")\n                 output_filename = Path(output_dir) / filename\n                 with gzip.open(temp_file.name) as f, open(output_filename, \"wb\") as output:\n                         for line in f:\n                                output.write(line)\n </denchmark-code>\n \n I have not tested above snippet, I will do tonight. If this not work fine there is another solution to read file in <denchmark-link:https://stackoverflow.com/questions/34632521/downloading-a-large-file-in-chunks-with-gzip-encoding-python-3-4>chunks</denchmark-link>\n . But I don't think the would be necessary as gzip already support buffered IO.\n One more point we can directly uncompress file from url instead of download to temp file and uncompressing it. Python compression libs have streaming support. Refer this <denchmark-link:https://github.com/deepset-ai/haystack/issues/709#issuecomment-755654431>#709 (comment)</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "brandenchan", "commentT": "2021-01-14T14:51:31Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/lalitpagaria>@lalitpagaria</denchmark-link>\n  thanks so much for the suggestion! I actually tested it and it solved the problem. I haven't removed the tempfile code but I did integrate your snippet in <denchmark-link:https://github.com/deepset-ai/haystack/pull/737>#737</denchmark-link>\n .\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "brandenchan", "commentT": "2021-01-14T15:09:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/brandenchan>@brandenchan</denchmark-link>\n  glad to know it. Thanks for testing it. \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "brandenchan", "commentT": "2021-01-21T19:43:28Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/tholor>@tholor</denchmark-link>\n  This can be closed now\n \t\t"}}}, "commit": {"commit_id": "725c03220f74f4ab6a9411840a630e5690b3bd29", "commit_author": "Branden Chan", "commitT": "2021-01-21 09:57:55+01:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "haystack\\preprocessor\\utils.py", "file_new_name": "haystack\\preprocessor\\utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "369,370,371", "deleted_lines": "367,370,371", "method_info": {"method_name": "fetch_archive_from_http", "method_params": "str,str,None", "method_startline": "328", "method_endline": "376"}}}}}}}