{"BR": {"BR_id": "2484", "BR_author": "bkong999", "BRopenT": "2021-01-04T03:31:19Z", "BRcloseT": "2021-01-06T09:18:37Z", "BR_text": {"BRsummary": "dgl.prop_nodes_topo Message Passing Error", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Clearly, there is something wrong with the message passing in the following example, either because source nodes' features are passed to the wrong dst nodes or the final updated features are written to different nodes.\n Basically, I use this function to aggregate the non-negative predictions (or sum the upstreaming features including the current node's feature) from the root to the leaves in a tree. As the graph is tree-structured and the predictions are non-negative, the aggregated value should monotonically increase from the root to the leaves. The following shows the result of an 8-path tree using DGL v0.4.3. x-axis means the indices in each path. y-axis is the aggregated value.\n <denchmark-link:https://user-images.githubusercontent.com/61814189/103498209-ca86eb80-4df8-11eb-8a4b-5cae38ac4860.png></denchmark-link>\n \n However, when I switched from v0.4.3 to v0.5.3, the aggregated values are not monotonically increasing anymore. The following shows the result using DGL v0.5.3. Apparently, there is something wrong with this function. Tried both the built-in message passing functions and writing by myself, the results are inconsistent for these versions.\n <denchmark-link:https://user-images.githubusercontent.com/61814189/103498273-002bd480-4df9-11eb-8573-a8ad60098dd6.png></denchmark-link>\n \n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Create a large tree-structured graph (the nodes have either one or zero in degrees).\n Write non-negative feature into the node data.\n Propagate the messages among the graph as follows:\n \n <denchmark-code>dgl.prop_nodes_topo(\n       graph,\n       message_func=dgl.function.u_add_v('data', 'data', 'data'),\n       reduce_func=dgl.function.sum('data', 'data'))\n </denchmark-code>\n \n The synthetic data and script to reproduce this issue can be found in the attached zip file.\n <denchmark-link:https://github.com/dmlc/dgl/files/5762879/reproduce.zip>reproduce.zip</denchmark-link>\n \n <denchmark-h:h2>Expected behavior</denchmark-h>\n \n the aggregated values should be monotonically increasing from the root to the leaves.\n <denchmark-h:h2>Environment</denchmark-h>\n \n \n DGL Version: 0.5.3\n Backend Library & Version: PyTorch 1.7.0\n OS (Linux): Linux\n How you installed DGL (conda): conda install -c dglteam dgl-cuda10.1\n Python version: 3.7.4\n CUDA/cuDNN version (if applicable): 10.1\n GPU models and configuration (e.g. V100): Tesla P40\n Any other relevant information: The synthetic data and script to reproduce this issue can be found in the zip file.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "bkong999", "commentT": "2021-01-06T07:15:18Z", "comment_text": "\n \t\tI guess you need a large enough tree. I have already attached the data and\n the script to reproduce it in the issue:\n <denchmark-link:https://github.com/dmlc/dgl/files/5762879/reproduce.zip>https://github.com/dmlc/dgl/files/5762879/reproduce.zip</denchmark-link>\n \n \n Best\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Tue, Jan 5, 2021 at 11:07 PM Quan (Andy) Gan ***@***.***> wrote:\n  Seems to work for me. Although I'm not sure if it matches your case:\n \n  import dglimport torchg = dgl.graph(([0,0,1,1,2,2,3,3,4,4,5,5,6,6],[1,2,3,4,5,6,7,8,9,10,11,12,13,14]))x = torch.rand(15)g.ndata['x'] = xdgl.prop_nodes_topo(g, dgl.function.u_add_v('x', 'x', 'm'), dgl.function.sum('m', 'x'))print(g.ndata['x'])for i in range(1, 15):\n      assert g.ndata['x'][i] > g.ndata['x'][(i - 1) // 2]\n \n  Could you give an example code so that I can reproduce your problem?\n  Thanks.\n \n  \u2014\n  You are receiving this because you authored the thread.\n  Reply to this email directly, view it on GitHub\n  <#2484 (comment)>, or\n  unsubscribe\n  <https://github.com/notifications/unsubscribe-auth/AOXTLLMP3JXG53A2TU5AANTSYQD4LANCNFSM4VSNTHJA>\n  .\n \n \n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "bkong999", "commentT": "2021-01-06T07:26:32Z", "comment_text": "\n \t\tYeah I noticed that.  Bug confirmed.  Seems that there is a mistake in message passing graph generation.\n EDIT: a minimal example reproducing the same issue with pull:\n g = dgl.graph(([0,1,2],[1,2,3]))\n x = torch.rand(4)\n x\n # tensor([0.4403, 0.7392, 0.1357, 0.9197])\n \n g.ndata['x'] = x\n g.pull([1, 2], dgl.function.u_add_v('x', 'x', 'm'), dgl.function.sum('m', 'x'))\n g.ndata['x']\n # tensor([0.4403, 1.1795, 0.8749, 0.9197])   <-- right\n \n g.ndata['x'] = x\n g.pull([2, 1], dgl.function.u_add_v('x', 'x', 'm'), dgl.function.sum('m', 'x'))\n g.ndata['x']\n # tensor([0.4403, 0.8749, 1.1795, 0.9197])   <-- wrong\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "bkong999", "commentT": "2021-01-06T09:19:40Z", "comment_text": "\n \t\tShould be fixed and ready in the next nightly build.  Please feel free to reopen if it does not work for you.\n \t\t"}}}, "commit": {"commit_id": "2caac086c2cf19ab06c5f707e84de030ccb176e6", "commit_author": "Quan (Andy) Gan", "commitT": "2021-01-06 17:18:37+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\dgl\\heterograph.py", "file_new_name": "python\\dgl\\heterograph.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "6064", "deleted_lines": "6063", "method_info": {"method_name": "_create_compute_graph", "method_params": "graph,u,v,eid,recv_nodes", "method_startline": "6002", "method_endline": "6064"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\compute\\test_basics.py", "file_new_name": "tests\\compute\\test_basics.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "662,663,664,665,666,667,668,669,670,671,672,673,674", "deleted_lines": null, "method_info": {"method_name": "test_issue_2484", "method_params": "idtype", "method_startline": "662", "method_endline": "674"}}}}}}}