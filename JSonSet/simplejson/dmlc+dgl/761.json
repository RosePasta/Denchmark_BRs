{"BR": {"BR_id": "761", "BR_author": "VoVAllen", "BRopenT": "2019-08-13T15:09:47Z", "BRcloseT": "2020-03-23T09:19:13Z", "BR_text": {"BRsummary": "[Bug] Reduce with max built-in having different behavior comparing to UDF", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Node with no in-edge will be initialized as -inf, which is inconsistent with UDF.\n Sometimes also fails CI test.\n <denchmark-h:h2>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n \n \n \n \n <denchmark-h:h2>Expected behavior</denchmark-h>\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n \n DGL Version (e.g., 1.0):\n Backend Library & Version (e.g., PyTorch 0.4.1, MXNet/Gluon 1.3):\n OS (e.g., Linux):\n How you installed DGL (conda, pip, source):\n Build command you used (if compiling from source):\n Python version:\n CUDA/cuDNN version (if applicable):\n GPU models and configuration (e.g. V100):\n Any other relevant information:\n \n <denchmark-h:h2>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "VoVAllen", "commentT": "2019-08-15T13:57:42Z", "comment_text": "\n \t\tPlease note this is also the case for batched_graph.max_nodes.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "VoVAllen", "commentT": "2019-08-16T03:33:41Z", "comment_text": "\n \t\tCould you paste the CI failure message here?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "VoVAllen", "commentT": "2019-08-16T06:25:11Z", "comment_text": "\n \t\t<denchmark-link:http://ci.dgl.ai/blue/organizations/jenkins/DGL/detail/PR-750/3/pipeline>http://ci.dgl.ai/blue/organizations/jenkins/DGL/detail/PR-750/3/pipeline</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "VoVAllen", "commentT": "2019-08-17T07:55:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jermainewang>@jermainewang</denchmark-link>\n  I think it's not appropriate to return  for zero degree nodes/graphs. If user use the returned  feature to do classification, they would get a gradient with all .\n >>> import torch\n >>> import torch.nn.functional as F\n >>> W = torch.rand(3, 4, requires_grad=True)\n >>> W\n tensor([[0.3388, 0.3281, 0.8115, 0.8196],\n         [0.0738, 0.7006, 0.4559, 0.7633],\n         [0.0359, 0.5973, 0.2692, 0.8890]], requires_grad=True)\n >>> x = torch.zeros(1, 3).fill_(-float('inf'))\n >>> y = x @ W\n >>> y\n tensor([[-inf, -inf, -inf, -inf]], grad_fn=<MmBackward>)\n >>> loss = F.cross_entropy(y, torch.LongTensor([1]))\n >>> loss\n tensor(nan, grad_fn=<NllLossBackward>)\n >>> loss.backward(torch.ones(1))\n >>> W.grad\n tensor([[nan, nan, nan, nan],\n         [nan, nan, nan, nan],\n         [nan, nan, nan, nan]])\n \t\t"}}}, "commit": {"commit_id": "ad15947f0ea9b34e15157dfad65b25f3a98e9ac8", "commit_author": "Zihao Ye", "commitT": "2019-08-28 04:24:00+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 9, "file_old_name": "tests\\compute\\test_kernel.py", "file_new_name": "tests\\compute\\test_kernel.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "115,116", "deleted_lines": "158", "method_info": {"method_name": "test_copy_edge_reduce", "method_params": "", "method_startline": "112", "method_endline": "158"}}, "hunk_1": {"Ismethod": 1, "added_lines": "63,64,65", "deleted_lines": null, "method_info": {"method_name": "test_copy_src_reduce._test", "method_params": "red,partial", "method_startline": "61", "method_endline": "100"}}, "hunk_2": {"Ismethod": 1, "added_lines": "115,116", "deleted_lines": null, "method_info": {"method_name": "test_copy_edge_reduce._test", "method_params": "red,partial", "method_startline": "113", "method_endline": "151"}}, "hunk_3": {"Ismethod": 1, "added_lines": "42,43,44,46,47,48,50,51,52,54,55,56", "deleted_lines": "43,44,45,47,48,49,51,52,53,55,56,57", "method_info": {"method_name": "generate_feature", "method_params": "g,broadcast", "method_startline": "35", "method_endline": "57"}}, "hunk_4": {"Ismethod": 1, "added_lines": "63,64,65", "deleted_lines": null, "method_info": {"method_name": "test_copy_src_reduce", "method_params": "", "method_startline": "60", "method_endline": "107"}}, "hunk_5": {"Ismethod": 1, "added_lines": "229,230,231,232,233,234,235,236,237", "deleted_lines": null, "method_info": {"method_name": "test_all_binary_builtins.test_all_binary_builtins._test.mfunc", "method_params": "edges", "method_startline": "227", "method_endline": "237"}}, "hunk_6": {"Ismethod": 1, "added_lines": "162,163,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,229,230,231,232,233,234,235,236,237", "deleted_lines": "158,203,204,205", "method_info": {"method_name": "test_all_binary_builtins._test", "method_params": "g,lhs,rhs,binary_op,reducer,paritial,nid,broadcast", "method_startline": "158", "method_endline": "248"}}, "hunk_7": {"Ismethod": 1, "added_lines": "162,163,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,229,230,231,232,233,234,235,236,237,284,285,304,305", "deleted_lines": "203,204,205,270", "method_info": {"method_name": "test_all_binary_builtins", "method_params": "", "method_startline": "161", "method_endline": "305"}}, "hunk_8": {"Ismethod": 1, "added_lines": "162,163,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,229,230,231,232,233,234,235,236,237", "deleted_lines": "203,204,205,270", "method_info": {"method_name": "test_all_binary_builtins._test", "method_params": "g,lhs,rhs,binary_op,reducer,partial,nid,broadcast", "method_startline": "162", "method_endline": "280"}}}}}}}