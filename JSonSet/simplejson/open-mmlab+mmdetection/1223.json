{"BR": {"BR_id": "1223", "BR_author": "FilipLangr", "BRopenT": "2019-08-20T13:29:36Z", "BRcloseT": "2019-09-25T07:35:44Z", "BR_text": {"BRsummary": "assert len(indices) == self.total_size error during multiple GPU training", "BRdescription": "\n I am trying to train my dataset on 8 GPU's. However, after calling ./dist_train.sh this error assertion appeares:\n Traceback (most recent call last):\n File \"./tools/train.py\", line 113, in \n main()\n File \"./tools/train.py\", line 109, in main\n logger=logger)\n File \"/mmdetection/mmdet/apis/train.py\", line 58, in train_detector\n _dist_train(model, dataset, cfg, validate=validate)\n File \"/mmdetection/mmdet/apis/train.py\", line 186, in _dist_train\n runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n File \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py\", line 358, in run\n epoch_runner(data_loaders[i], **kwargs)\n File \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py\", line 260, in train\n for i, data_batch in enumerate(data_loader):\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 193, in iter    return _DataLoaderIter(self)\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 493, in init\n self._put_indices()\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 591, in _put_indices\n indices = next(self.sample_iter, None)\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/sampler.py\", line 172, in iter\n for idx in self.sampler:\n File \"/mmdetection/mmdet/datasets/loader/sampler.py\", line 138, in iter\n assert len(indices) == self.total_size\n ...\n in the config I tried various values for imgs_per_gpu and workers_per_gpu, currently it is:\n imgs_per_gpu=2, workers_per_gpu=2,\n no settings was working though. Single-GPU training works well.\n What is the meaning of this assert?\n Thanks!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "FilipLangr", "commentT": "2019-08-20T15:02:02Z", "comment_text": "\n \t\tPlease follow the Error report issue template.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "FilipLangr", "commentT": "2019-08-20T15:13:51Z", "comment_text": "\n \t\t\n Please follow the Error report issue template.\n \n Here it is, thanks for any help!\n Checklist\n \n I have searched related issues but cannot get the expected help.\n yes\n The bug has not been fixed in the latest version.\n yup\n \n Describe the bug\n I am trying to train my custom dataset on 8 GPU's. However, after calling ./dist_train.sh the error showed below appeares. In the config I tried more values for imgs_per_gpu and workers_per_gpu (e.g. imgs_per_gpu=2, workers_per_gpu=2), no settings was working though.\n Single-GPU training works well.\n What is the meaning of the assert in the Traceback? What does not fit? Thanks!\n Reproduction\n \n What command or script did you run?\n \n <denchmark-code>./tools/dist_train.sh MY_CONFIG 8 --validate\n </denchmark-code>\n \n \n Did you make any modifications on the code or config? Did you understand what you have modified?\n I modified number of classes, workers_per_gpu, imgs_per_gpu, dataset type and paths to the datasets. No changes in code.\n What dataset did you use?\n My own dataset of 8 classes converted to COCO format.\n \n Environment\n \n OS: Ubuntu 18.04\n GCC 5.4.0\n PyTorch version 1.1.0\n I built the docker I use on the official pytorch+cuda docker\n GPU model: 8xV100\n CUDA version: 10.0, CUDNN version: 7.5\n \n Error traceback\n <denchmark-code>Traceback (most recent call last):\n File \"./tools/train.py\", line 113, in \n main()\n File \"./tools/train.py\", line 109, in main\n logger=logger)\n File \"/mmdetection/mmdet/apis/train.py\", line 58, in train_detector\n _dist_train(model, dataset, cfg, validate=validate)\n File \"/mmdetection/mmdet/apis/train.py\", line 186, in _dist_train\n runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n File \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py\", line 358, in run\n epoch_runner(data_loaders[i], **kwargs)\n File \"/opt/conda/lib/python3.6/site-packages/mmcv/runner/runner.py\", line 260, in train\n for i, data_batch in enumerate(data_loader):\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 193, in iter return _DataLoaderIter(self)\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 493, in init\n self._put_indices()\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 591, in _put_indices\n indices = next(self.sample_iter, None)\n File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/sampler.py\", line 172, in iter\n for idx in self.sampler:\n File \"/mmdetection/mmdet/datasets/loader/sampler.py\", line 138, in iter\n assert len(indices) == self.total_size\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "FilipLangr", "commentT": "2019-08-20T15:20:17Z", "comment_text": "\n \t\tOne more detail, when I print len(indices) and self.total_size right before the critical assert, it's 9308 and 9312. The size of my training dataset is 9306..\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "FilipLangr", "commentT": "2019-08-22T10:51:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hellock>@hellock</denchmark-link>\n  Any ideas? Recently I found out that when I set the config file to train with 2 GPUs (2 img/gpu), the training (called with ) initiates well. However, training with any more GPUs results in the mentioned assert error. Seems like a bug to me.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "FilipLangr", "commentT": "2019-08-22T11:32:27Z", "comment_text": "\n \t\tYou may count the images with aspect ratio >1 and <1. I suspect that there are only 2 images for one of the two groups.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "FilipLangr", "commentT": "2019-08-22T11:58:27Z", "comment_text": "\n \t\t\n You may count the images with aspect ratio >1 and <1. I suspect that there are only 2 images for one of the two groups.\n \n you are right, there are exactly 2 images with height > width\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "FilipLangr", "commentT": "2019-08-22T12:08:35Z", "comment_text": "\n \t\tThe problem lies in <denchmark-link:https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/loader/sampler.py#L135>this line</denchmark-link>\n  where .\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "FilipLangr", "commentT": "2019-08-23T03:11:45Z", "comment_text": "\n \t\tI meet the same issue, how to fix it?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "FilipLangr", "commentT": "2019-09-02T15:57:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ZhexuanZhou>@ZhexuanZhou</denchmark-link>\n \n Before bug fixed, simplest way is make your img_per_gpu as power of 2.\n e.g. 2, 4, 8, 16 ...\n Also, make your gpu number as power of 2.\n This works for me.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "FilipLangr", "commentT": "2019-09-02T16:15:22Z", "comment_text": "\n \t\t\n @ZhexuanZhou\n Before bug fixed, simplest way is make your img_per_gpu as power of 2.\n e.g. 2, 4, 8, 16 ...\n Also, make your gpu number as power of 2.\n This works for me.\n \n This fix doesn't work for me, as is already mentioned in the bug description\n \n Describe the bug\n I am trying to train my custom dataset on 8 GPU's. However, after calling ./dist_train.sh the error showed below appeares. In the config I tried more values for imgs_per_gpu and workers_per_gpu (e.g. imgs_per_gpu=2, workers_per_gpu=2), no settings was working though.\n \n The easiest work-around for me was to comment out the two asserts in the sampler.py :)\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "FilipLangr", "commentT": "2019-09-02T19:45:34Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/FilipLangr>@FilipLangr</denchmark-link>\n \n Yeah, that might cause some samples lost but not that harmful.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "FilipLangr", "commentT": "2019-09-13T07:48:20Z", "comment_text": "\n \t\tHello!\n I have situation, when 6 pics, where w>h.\n 33994 pics, when h>w.\n And 2 pics, where h==w.\n I have deleted h==w pics and have AssertionError: assert len(indices) == self.total_size anyway :(\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "FilipLangr", "commentT": "2019-09-13T08:03:31Z", "comment_text": "\n \t\tThan I deleted w>h pics and get another error: TypeError: 'NoneType' object is not subscriptable\n \t\t"}}}, "commit": {"commit_id": "3dc9ddb73315f016d782fdeb824ec1f5ef2e9f81", "commit_author": "kkkio", "commitT": "2019-09-25 15:35:42+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "mmdet\\datasets\\loader\\sampler.py", "file_new_name": "mmdet\\datasets\\loader\\sampler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "135,136,137,138,139,140", "deleted_lines": "135,136", "method_info": {"method_name": "__iter__", "method_params": "self", "method_startline": "119", "method_endline": "157"}}}}}}}