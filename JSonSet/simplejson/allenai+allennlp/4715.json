{"BR": {"BR_id": "4715", "BR_author": "vikigenius", "BRopenT": "2020-10-07T17:23:47Z", "BRcloseT": "2020-10-08T05:22:20Z", "BR_text": {"BRsummary": "Rouge metric incorrectly computed with distributed training", "BRdescription": "\n <denchmark-h:h2>Checklist</denchmark-h>\n \n \n  I have verified that the issue exists against the master branch of AllenNLP.\n  I have read the relevant section in the contribution guide on reporting bugs.\n  I have checked the issues list for similar or identical bug reports.\n  I have checked the pull requests list for existing proposed fixes.\n  I have checked the CHANGELOG and the commit log to find out if the bug was already fixed in the master branch.\n  I have included in the \"Description\" section below a traceback from any exceptions related to this bug.\n  I have included in the \"Related issues or possible duplicates\" section beloew all related issues and possible duplicate issues (If there are none, check this box anyway).\n  I have included in the \"Environment\" section below the name of the operating system and Python version that I was using when I discovered this bug.\n  I have included in the \"Environment\" section below the output of pip freeze.\n  I have included in the \"Steps to reproduce\" section below a minimally reproducible example.\n \n <denchmark-h:h2>Description</denchmark-h>\n \n Training the same model with and without distributed training produces drastically different metrics and incorrect metrics with distributed.\n In fact distributed training on 8 GPUs gives me scores > 1 which is obviously incorrect.\n \n Multi GPU metrics:\n \n   \"validation_ROUGE-1_R\": 3.7029976844787598,\n   \"validation_ROUGE-2_R\": 1.9398850351572037,\n   \"validation_ROUGE-1_P\": 4.3116472363471985,\n   \"validation_ROUGE-2_P\": 2.3042131861050925,\n   \"validation_ROUGE-1_F1\": 3.819778541723887,\n   \"validation_ROUGE-2_F1\": 2.039711813131968,\n   \"validation_ROUGE-L\": 3.579911470413208,\n \n \n \n \n Single GPU metrics:\n \n   \"validation_ROUGE-1_R\": 0.49840065422148055,\n   \"validation_ROUGE-2_R\": 0.28016004520356264,\n   \"validation_ROUGE-1_P\": 0.5445739355913551,\n   \"validation_ROUGE-2_P\": 0.3068941746277846,\n   \"validation_ROUGE-1_F1\": 0.5003892512410818,\n   \"validation_ROUGE-2_F1\": 0.28298027227105127,\n   \"validation_ROUGE-L\": 0.4661126545409945,\n \n \n \n <denchmark-h:h2>Related issues or possible duplicates</denchmark-h>\n \n \n #4050 is a possibly related issue\n \n <denchmark-h:h2>Environment</denchmark-h>\n \n OS: GNU/Linux\n Python version: 3.8.5\n \n Output of pip freeze:\n \n alabaster==0.7.12\n alembic==1.4.3\n -e git+https://github.com/allenai/allennlp.git@edcb6d3466d2c4263f1e6a5731c6ace5358f47e8#egg=allennlp\n -e git+https://github.com/allenai/allennlp-models.git@a330876f95cfec99f0ab724fbc0237f7d1f3288c#egg=allennlp_models\n apache-airflow==1.10.12\n apispec==1.3.3\n argcomplete==1.12.0\n argon2-cffi==20.1.0\n astor==0.8.1\n async-generator==1.10\n attrs==19.3.0\n Babel==2.8.0\n backcall==0.2.0\n bandit==1.6.2\n bert-serving-client==1.10.0\n bleach==3.2.1\n blis==0.4.1\n boto3==1.15.4\n botocore==1.18.4\n cached-property==1.5.2\n catalogue==1.0.0\n cattrs==1.0.0\n certifi==2020.4.5.2\n cffi==1.14.3\n chardet==3.0.4\n click==7.1.2\n colorama==0.4.3\n colorlog==4.0.2\n configparser==3.5.3\n conllu==4.2\n coverage==5.1\n croniter==0.3.34\n cymem==2.0.3\n darglint==1.4.1\n datasets==1.0.1\n decorator==4.4.2\n defusedxml==0.6.0\n dictdiffer==0.8.1\n dill==0.3.2\n dnspython==2.0.0\n doc8==0.8.1\n docutils==0.16\n dparse==0.5.1\n email-validator==1.1.1\n entrypoints==0.3\n eradicate==1.0\n filelock==3.0.12\n flake8==3.8.3\n flake8-bandit==2.1.2\n flake8-broken-line==0.2.0\n flake8-bugbear==19.8.0\n flake8-commas==2.0.0\n flake8-comprehensions==3.2.3\n flake8-debugger==3.2.1\n flake8-docstrings==1.5.0\n flake8-eradicate==0.3.0\n flake8-isort==3.0.1\n flake8-plugin-utils==1.3.1\n flake8-polyfill==1.0.2\n flake8-pytest-style==1.3.0\n flake8-quotes==2.1.2\n flake8-rst-docstrings==0.0.12\n flake8-string-format==0.2.3\n Flask==1.1.2\n Flask-Admin==1.5.4\n Flask-AppBuilder==2.3.0\n Flask-Babel==1.0.0\n Flask-Caching==1.3.3\n Flask-JWT-Extended==3.24.1\n Flask-Login==0.4.1\n Flask-OpenID==1.2.5\n Flask-SQLAlchemy==2.4.4\n flask-swagger==0.2.14\n Flask-WTF==0.14.3\n ftfy==5.8\n funcsigs==1.0.2\n future==0.18.2\n gitdb==4.0.5\n GitPython==3.1.3\n graphviz==0.14.1\n gunicorn==20.0.4\n h5py==2.10.0\n idna==2.9\n imagesize==1.2.0\n importlib-metadata==1.6.1\n ipykernel==5.3.4\n ipython==7.16.1\n ipython-genutils==0.2.0\n ipywidgets==7.5.1\n iso8601==0.1.13\n isort==4.3.21\n itsdangerous==1.1.0\n jedi==0.17.2\n Jinja2==2.11.2\n jmespath==0.10.0\n joblib==0.16.0\n json-merge-patch==0.2\n jsonnet==0.16.0\n jsonpickle==1.4.1\n jsonschema==3.2.0\n jupyter==1.0.0\n jupyter-client==6.1.7\n jupyter-console==6.2.0\n jupyter-core==4.6.3\n jupyterlab-pygments==0.1.1\n lazy-object-proxy==1.5.1\n lockfile==0.12.2\n m2r==0.2.1\n Mako==1.1.3\n Markdown==2.6.11\n MarkupSafe==1.1.1\n marshmallow==3.6.1\n marshmallow-enum==1.5.1\n marshmallow-polyfield==5.9\n marshmallow-sqlalchemy==0.23.1\n mccabe==0.6.1\n mistune==0.8.4\n more-itertools==8.4.0\n murmurhash==1.0.2\n mypy==0.782\n mypy-extensions==0.4.3\n natsort==7.0.1\n nbclient==0.5.0\n nbconvert==6.0.3\n nbformat==5.0.7\n nest-asyncio==1.4.0\n nitpick==0.22.2\n nltk==3.5\n notebook==6.1.4\n numpy==1.19.2\n overrides==3.1.0\n packaging==20.4\n pandas==0.25.3\n pandocfilters==1.4.2\n parso==0.7.1\n pbr==5.4.5\n pendulum==1.4.4\n pep8-naming==0.9.1\n pexpect==4.8.0\n pickleshare==0.7.5\n plac==1.1.3\n pluggy==0.13.1\n preshed==3.0.2\n prison==0.1.3\n prometheus-client==0.8.0\n prompt-toolkit==3.0.3\n protobuf==3.13.0\n psutil==5.7.2\n ptyprocess==0.6.0\n py==1.8.1\n py-rouge==1.1\n py4j==0.10.9\n pyarrow==1.0.1\n pycodestyle==2.6.0\n pycparser==2.20\n pydocstyle==5.0.2\n pyflakes==2.2.0\n Pygments==2.6.1\n PyJWT==1.7.1\n pyparsing==2.4.7\n pyrsistent==0.17.3\n pyspark==3.0.1\n pytest==5.4.3\n pytest-cov==2.10.1\n pytest-randomly==3.4.1\n python-daemon==2.2.4\n python-dateutil==2.8.1\n python-editor==1.0.4\n python-nvd3==0.15.0\n python-openid==2.2.5\n python-slugify==4.0.0\n pytz==2020.1\n pytzdata==2020.1\n PyYAML==5.3.1\n pyzmq==19.0.2\n qtconsole==4.7.7\n QtPy==1.9.0\n regex==2020.9.27\n requests==2.23.0\n restructuredtext-lint==1.3.1\n ruamel.yaml==0.16.10\n ruamel.yaml.clib==0.2.0\n s3transfer==0.3.3\n sacremoses==0.0.43\n safety==1.9.0\n scikit-learn==0.23.2\n scipy==1.5.2\n Send2Trash==1.5.0\n sentencepiece==0.1.91\n setproctitle==1.1.10\n six==1.15.0\n smmap==3.0.4\n snowballstemmer==2.0.0\n sortedcontainers==2.2.2\n spacy==2.3.2\n Sphinx==2.4.4\n sphinx-autodoc-typehints==1.10.3\n sphinxcontrib-applehelp==1.0.2\n sphinxcontrib-devhelp==1.0.2\n sphinxcontrib-htmlhelp==1.0.3\n sphinxcontrib-jsmath==1.0.1\n sphinxcontrib-qthelp==1.0.3\n sphinxcontrib-serializinghtml==1.1.4\n SQLAlchemy==1.3.19\n SQLAlchemy-JSONField==0.9.0\n SQLAlchemy-Utils==0.36.8\n srsly==1.0.2\n stevedore==2.0.0\n tabulate==0.8.7\n tenacity==4.12.0\n tensorboardX==2.1\n terminado==0.8.3\n testfixtures==6.14.1\n testpath==0.4.4\n text-unidecode==1.3\n thinc==7.4.1\n threadpoolctl==2.1.0\n thrift==0.13.0\n tokenizers==0.8.1rc2\n toml==0.10.0\n tomlkit==0.7.0\n torch==1.6.0\n tornado==6.0.4\n tqdm==4.49.0\n traitlets==4.3.3\n transformers==3.3.1\n typed-ast==1.4.1\n typing-extensions==3.7.4.2\n tzlocal==1.5.1\n unicodecsv==0.14.1\n urllib3==1.25.9\n wasabi==0.8.0\n wcwidth==0.2.4\n webencodings==0.5.1\n wemake-python-styleguide==0.14.1\n Werkzeug==0.16.1\n widgetsnbextension==3.5.1\n word2number==1.1\n WTForms==2.3.3\n xxhash==2.0.0\n zipp==3.1.0\n zope.deprecation==4.4.0\n \n \n \n <denchmark-h:h2>Steps to reproduce</denchmark-h>\n \n Train BART twice, once with distributed training on multiple GPUs and the other on a single GPU without distributed training and compare the metrics.json file.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "vikigenius", "commentT": "2020-10-07T20:37:30Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/vikigenius>@vikigenius</denchmark-link>\n , I believe the issue is that  is not being aggregated across GPUs. Would you be interested in making a PR to fix it? If not, one of us will probably be able to get to it within the next few days.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "vikigenius", "commentT": "2020-10-07T21:05:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/epwalsh>@epwalsh</denchmark-link>\n  Yeah, I will look into creating a PR. I don't have a lot of experience with distributed training yet. But I am excited to learn more.\n \t\t"}}}, "commit": {"commit_id": "bc6f15accc2392c42d97e2bdee7bcb47a664f597", "commit_author": "Vikash", "commitT": "2020-10-08 05:22:19+00:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "73", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "allennlp\\training\\metrics\\rouge.py", "file_new_name": "allennlp\\training\\metrics\\rouge.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "209,210,211,212,213,214,215", "deleted_lines": "209"}}}}}}