{"BR": {"BR_id": "12883", "BR_author": "richardliaw", "BRopenT": "2020-12-15T17:12:41Z", "BRcloseT": "2020-12-17T17:41:49Z", "BR_text": {"BRsummary": "[k8s] printenv occasionally fails", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n I'm running a kubernetes cluster with my docker image (richardliaw/horovod) and I get this:\n <denchmark-code>  Updating cluster configuration. [hash=0194a452ebd82e0ab6eade7b4dd3a4f4f775d5de]\n   New status: syncing-files\n   [2/7] Processing file mounts\n 2020-12-15 09:03:24,116\tINFO command_runner.py:169 -- NodeUpdater: ray-head-m5nvd: Running kubectl -n ray exec -it ray-head-m5nvd -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (mkdir -p ~)'\n Error from server: error dialing backend: EOF\n   New status: update-failed\n   !!!\n   Setup command `kubectl -n ray exec -it ray-head-m5nvd -- printenv HOME` failed with exit code 1. stderr:\n   !!!\n \n Exception in thread Thread-1:\n Traceback (most recent call last):\n   File \"/Users/rliaw/miniconda3/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n     self.run()\n   File \"/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py\", line 124, in run\n     self.do_update()\n   File \"/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py\", line 312, in do_update\n     self.rsync_up, step_numbers=(1, NUM_SETUP_STEPS))\n   File \"/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py\", line 210, in sync_file_mounts\n     do_sync(remote_path, local_path)\n   File \"/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py\", line 198, in do_sync\n     local_path, remote_path, docker_mount_if_possible=True)\n   File \"/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/updater.py\", line 446, in rsync_up\n     self.cmd_runner.run_rsync_up(source, target, options=options)\n   File \"/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/command_runner.py\", line 197, in run_rsync_up\n     target = self._home + target[1:]\n   File \"/Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/autoscaler/_private/command_runner.py\", line 258, in _home\n     raw_out = self.process_runner.check_output(joined_cmd, shell=True)\n   File \"/Users/rliaw/miniconda3/lib/python3.7/subprocess.py\", line 395, in check_output\n     **kwargs).stdout\n   File \"/Users/rliaw/miniconda3/lib/python3.7/subprocess.py\", line 487, in run\n     output=stdout, stderr=stderr)\n subprocess.CalledProcessError: Command 'kubectl -n ray exec -it ray-head-m5nvd -- printenv HOME' returned non-zero exit status 1.\n \n   Failed to setup head node.\n </denchmark-code>\n \n during node startup.\n cc @Gekho457  <denchmark-link:https://github.com/ijrsvt>@ijrsvt</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "richardliaw", "commentT": "2020-12-16T05:12:37Z", "comment_text": "\n \t\tWeird. What happens if you directly launch a pod with the same spec as the one in your head node config and then try the same kubectl exec command with that pod?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "richardliaw", "commentT": "2020-12-16T05:16:03Z", "comment_text": "\n \t\tIt's not easily reproducible; by kubectl exec'ing after this failure, you'll get a exit status 0 (successful execution)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "richardliaw", "commentT": "2020-12-16T05:43:48Z", "comment_text": "\n \t\tIs it correct that you\u2019re running ray up with a running cluster?\n Any other useful context you can think of?\n I wonder if just having KubernetesCommandRunner retry that command a couple times before giving up would be a passable temporary bandaid on this.\n It would be great if we could get more detailed info on what happened than exit status 1 \u2014 need to look into the logging logic.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "richardliaw", "commentT": "2020-12-16T08:10:52Z", "comment_text": "\n \t\tHmm, looks like this is a GKE issue: <denchmark-link:https://gitlab.com/gitlab-org/gitlab-runner/-/issues/3247>https://gitlab.com/gitlab-org/gitlab-runner/-/issues/3247</denchmark-link>\n \n Maybe a retry makes sense?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "richardliaw", "commentT": "2020-12-16T13:36:50Z", "comment_text": "\n \t\tyep, will add retries\n Looks like it might be the same problem that arose in the autoscaler here: <denchmark-link:https://github.com/ray-project/ray/issues/12255>#12255</denchmark-link>\n \n except now it\u2019s impacting the cluster launcher. I\u2019m curious just how long these random API server outages are.\n \t\t"}}}, "commit": {"commit_id": "82f9c7014e2d0acd3e3869066f5dc3142ec9e7a7", "commit_author": "Gekho457", "commitT": "2020-12-17 09:41:48-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\autoscaler\\_private\\command_runner.py", "file_new_name": "python\\ray\\autoscaler\\_private\\command_runner.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "253,254,255,256,257,258,259,260,261,262,263,264,265,266", "deleted_lines": "253,254,255,256,257,258,259,260", "method_info": {"method_name": "_home", "method_params": "self", "method_startline": "252", "method_endline": "266"}}, "hunk_1": {"Ismethod": 1, "added_lines": "268,271,272,273,274,275,276,277", "deleted_lines": null, "method_info": {"method_name": "_try_to_get_home", "method_params": "self", "method_startline": "268", "method_endline": "277"}}}}}}}