{"BR": {"BR_id": "12521", "BR_author": "richardliaw", "BRopenT": "2020-12-01T04:13:26Z", "BRcloseT": "2020-12-02T00:47:04Z", "BR_text": {"BRsummary": "[tune] `with_parameters` doubly serializes parameters", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Ray version and other system information (Python version, TensorFlow version, OS): master\n with_parameters does not actually remove the parameters after serializing them through the object store. In fact, it actually captures them within the scope of the function.\n The cause:\n     prefix = f\"{str(fn)}_\"\n     for k, v in kwargs.items():\n         parameter_registry.put(prefix + k, v)\n \n     use_checkpoint = detect_checkpoint_function(fn)\n \n     def inner(config, checkpoint_dir=None):\n         fn_kwargs = {}\n         if use_checkpoint:\n             default = checkpoint_dir\n             sig = inspect.signature(fn)\n             if \"checkpoint_dir\" in sig.parameters:\n                 default = sig.parameters[\"checkpoint_dir\"].default \\\n                           or default\n             fn_kwargs[\"checkpoint_dir\"] = default\n \n         for k in kwargs:\n             fn_kwargs[k] = parameter_registry.get(prefix + k)\n         fn(config, **fn_kwargs)\n ^ this causes kwargs to be captured within inner, nulling the effect of with_parameters.\n See the below reproduction script.\n The fix for this\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n import os\n import sys\n \n import numpy as np\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n import torchvision\n from numpy.random import normal\n import ray\n from ray import tune\n from ray.tune.schedulers import ASHAScheduler\n import random\n from torchvision.transforms import transforms\n \n random.seed(100)\n \n transform = transforms.Compose([transforms.ToTensor(),\n                                 transforms.Normalize((0.5,), (0.5,)),\n                                 ])\n \n xy_trainPT = torchvision.datasets.MNIST(\n     root=\"./\",\n     train=True,\n     download=True\n )\n \n trainset = torchvision.datasets.MNIST(root=\"./\",\n                                       train=True,\n                                       download=True,\n                                       transform=transform)\n \n originalSet = torchvision.datasets.MNIST(root=\"./\",\n                                          train=True,\n                                          download=True,\n                                          transform=transform)\n \n # noisyArr = np.zeros(shape=(60000, [28, 28]))\n # originalArr = np.zeros(shape=(60000, [28, 28]))\n \n noisyArr = []\n originalArr = []\n \n for index, shape in enumerate(trainset):\n     # shape = (imageTensor, Label)\n     # print(shape[0].squeeze(dim=0).numpy().shape)\n     # noisyArr[index] = shape[0].squeeze(dim=0).numpy()\n     # originalArr[index] = originalSet[0][0].squeeze(dim=0).numpy()\n     noisyArr.append(shape[0].squeeze(dim=0).numpy())\n     originalArr.append(originalSet[0][0].squeeze(dim=0).numpy())\n     if index == 30000:\n         break\n \n noisyArr = np.array(noisyArr)\n originalArr = np.array(originalArr)\n print('done loading data')\n \n original = originalArr / 255\n \n X_2 = noisyArr / 255\n \n for i in range(len(X_2)):\n     norm = abs(np.random.normal(0, 0.3, size=(28, 28)))\n     X_2[i] = X_2[i] + norm\n \n pixels = int(784)\n \n \n class autoencoder(nn.Module):\n     def __init__(self, config):\n         super(autoencoder, self).__init__()\n \n         size = 28\n         kernel = config['convK']\n         # print(f\"kernal: {kernel}\")\n         stride = config['convS']\n         # print(f\"stride: {stride}\")\n         padding = config['convP']\n         #  print(f\"padding: {padding}\")\n         poolK = config['poolK']\n         poolS = config['poolS']\n         poolP = config['poolP']\n         finalOutput = config['actMap']\n         self.conv1 = torch.nn.Conv2d(1, finalOutput, kernel_size=kernel, stride=stride, padding=padding)\n         self.bn1 = torch.nn.BatchNorm2d(finalOutput)\n         self.pool1 = torch.nn.MaxPool2d(stride=poolS, kernel_size=poolK, padding=poolP)\n \n         def poolAdjust(originalSize, kernel=poolK, stride=poolS, dilation=1, padding=poolP):\n             return ((originalSize + (2 * padding) - (dilation * (kernel - 1)) - 1) // stride) + 1\n \n         def conv2d_size_out(size, kernel_size=kernel, stride=stride, padding=padding):\n             return ((size + (padding * 2) - (kernel_size - 1) - 1) // stride) + 1\n \n         #         convw = conv2d_size_out(size)\n         #         convh = conv2d_size_out(size)\n         convw = poolAdjust(conv2d_size_out(size))\n         convh = poolAdjust(conv2d_size_out(size))\n         #  print('Convulution adjust:::', conv2d_size_out(size))\n         #  print('Pooling Adjust: ', convw)\n         self.linear_input_size = convw * convh * finalOutput\n         # print('linear_Input: ', linear_input_size)\n         self.head = torch.nn.Linear(self.linear_input_size, pixels)\n         self.flatten = torch.nn.Linear(self.linear_input_size, self.linear_input_size)\n         self.func = torch.nn.Hardtanh()\n         self.softMax2d = torch.nn.Softmax2d()\n \n     def forward(self, x):\n         x = self.bn1(self.conv1(x))\n         # print(x.size())\n         x = self.pool1(x)\n         # print(x.size())\n         x = torch.nn.functional.relu(x)\n         # print(self.linear_input_size)\n         return self.head(x.view(x.size(0), -1))\n \n \n def train(config, checkpoint_dir=None, data=None):\n     # data = (X_2, original)\n     loss_fn = torch.nn.MSELoss()\n     model = autoencoder(config)\n     optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n     maxIter = 20000\n     batchAmount = config['batchSize']\n \n     if checkpoint_dir:\n         checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n         model_state, optimizer_state = torch.load(checkpoint)\n         model.load_state_dict(model_state)\n         optimizer.load_state_dict(optimizer_state)\n \n     for t in range(maxIter):\n         epoch_loss = 0\n \n         optimizer.zero_grad()\n         # idx = np.random.randint(data[0].shape[0], size=batchAmount)  # bootstrapping a subset of the total samples\n         tempIDX = 1\n         X_scaled = torch.unsqueeze(torch.from_numpy(data[0][tempIDX, :]).float(),\n                                    dim=1)  # creating tensor for convultion\n \n         testValues = torch.from_numpy(\n             np.reshape(data[1][tempIDX, :],\n                        (batchAmount, -1))\n         ).float()  # creating a flattened array for testing\n \n         y_pred = model(X_scaled)  # predict on the subset\n \n         loss = loss_fn(testValues, y_pred)  # get loss on subset\n         epoch_loss += loss.item()\n \n         if not t == 0:\n             if t % (maxIter / 10) == 0:\n                 # print(t, loss.item())\n                 tune.report(score=epoch_loss)\n                 with tune.checkpoint_dir(step=t) as checkpoint_dir:\n                     path = os.path.join(checkpoint_dir, \"checkpoint\")\n                     torch.save(\n                         (model.state_dict(), optimizer.state_dict()), path)\n \n         loss.backward()  # get gradient stuff\n         optimizer.step()  # optimize\n \n     tune.report(score=epoch_loss)\n \n \n def tunerTrain():\n     # ray.init(_memory=1000000000, object_store_memory=8000000000,  _redis_max_memory=1000000000, num_cpus=4)\n     ray.init(_redis_max_memory=1000000000, object_store_memory=1000000000, num_cpus=4)\n     # searchSpace = {\n     #     'lr': 0.025,\n     #     'actMap': tune.grid_search([2, 3, 4]),\n     #     'convK': tune.choice([3, 5, 7, 9]),\n     #     'convS': tune.grid_search([1, 2]),\n     #     'convP': tune.choice([0, 1, 2, 3]),\n     #     'poolK': tune.choice([3, 5, 7, 9]),\n     #     'poolS': tune.grid_search([1, 2]),\n     #     'poolP': tune.grid_search([0,1,2,3]),\n     #     'batchSize': 64,\n     # }\n     searchSpace = {\n         'lr': 0.0351993, 'actMap': 2, 'convK': 3, 'convS': 1, 'convP': 1, 'poolK': 3, 'poolP': 1,\n         'poolS': tune.grid_search([1, 2]),\n         'batchSize': 16,\n     }\n \n     analysis = tune.run(tune.with_parameters(train, data=[X_2, original]), num_samples=10, metric='score', mode='min',\n                         resources_per_trial={\"cpu\": 4},\n                         scheduler=ASHAScheduler(),\n                         config=searchSpace)\n     dfs = analysis.trial_dataframes\n     print(f\"Best Config: {analysis.get_best_config('score', mode='min')}\")\n     df = analysis.results_df\n     logdir = analysis.get_best_logdir(\"score\", mode=\"min\")\n     print(f\"dir of best: {logdir}\")\n     print(analysis.best_result)\n     print(f\"Best trial final score: {analysis.get_best_trial('score', mode='min')}\")\n \n \n tunerTrain()\n \n \n # import ray.cloudpickle as pk\n #\n # object = pk.dumps(train)\n # print(sys.getsizeof(object))\n \n #\n #\n # import inspect;\n # closure = inspect.getclosurevars(train)\n # print(closure)\n \n \n # train(config={\n #     'lr': 0.0351993,\n #     'actMap': 2,\n #     'convK': 3,\n #     'convS': 1,\n #     'convP': 1,\n #     'poolK': 3,\n #     'poolS': 1,\n #     'poolP': 1,\n #     'batchSize': 16,\n # })\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "richardliaw", "commentT": "2020-12-01T04:13:40Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/krfricke>@krfricke</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "4dc16730a7cd97c7dec3484331f508dc5d7a79cd", "commit_author": "Richard Liaw", "commitT": "2020-12-01 16:47:03-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\tune\\function_runner.py", "file_new_name": "python\\ray\\tune\\function_runner.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "630,642", "deleted_lines": "641", "method_info": {"method_name": "with_parameters", "method_params": "fn,kwargs", "method_startline": "587", "method_endline": "654"}}, "hunk_1": {"Ismethod": 1, "added_lines": "642", "deleted_lines": "641", "method_info": {"method_name": "with_parameters.inner", "method_params": "config,checkpoint_dir", "method_startline": "632", "method_endline": "644"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\tune\\registry.py", "file_new_name": "python\\ray\\tune\\registry.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "172", "deleted_lines": null, "method_info": {"method_name": "flush", "method_params": "self", "method_startline": "169", "method_endline": "172"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "python\\ray\\tune\\tests\\test_function_api.py", "file_new_name": "python\\ray\\tune\\tests\\test_function_api.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "475,476,477", "deleted_lines": null, "method_info": {"method_name": "testWithParameters2.__init__", "method_params": "self", "method_startline": "475", "method_endline": "477"}}, "hunk_1": {"Ismethod": 1, "added_lines": "473,474,475,476,477,478,479,480,481,482,483,484", "deleted_lines": null, "method_info": {"method_name": "testWithParameters2", "method_params": "self", "method_startline": "473", "method_endline": "484"}}, "hunk_2": {"Ismethod": 1, "added_lines": "479,480", "deleted_lines": null, "method_info": {"method_name": "testWithParameters2.train", "method_params": "config,data", "method_startline": "479", "method_endline": "480"}}}}}}}