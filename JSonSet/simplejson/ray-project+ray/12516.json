{"BR": {"BR_id": "12516", "BR_author": "hybug", "BRopenT": "2020-12-01T03:18:30Z", "BRcloseT": "2020-12-09T13:20:15Z", "BR_text": {"BRsummary": "[rllib] Trainer.compute_action Error with Dict type observation inputs", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Python: 3.8.5,\n TensorFlow: tensorflow-gpu 2.0.0\n Ray: ray 1.0.1 & ray 0.8.6\n I want to reproduce the code of this blog, but I got an error.  <denchmark-link:https://towardsdatascience.com/action-masking-with-rllib-5e4bec5e7505>Action Masking with RLlib</denchmark-link>\n \n <denchmark-h:h3>Training Script</denchmark-h>\n \n Here is the code script. use pip install or_gym first\n <denchmark-code>from or_gym.utils import create_env\n from gym import spaces\n from ray.rllib.utils import try_import_tf\n from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n from ray.rllib.models import ModelCatalog\n from ray import tune\n from ray.rllib import agents\n import ray\n import or_gym\n import numpy as np\n env = or_gym.make('Knapsack-v0')\n \n print(\"Max weight capacity:\\t{}kg\".format(env.max_weight))\n print(\"Number of items:\\t{}\".format(env.N))\n \n env_config = {'N': 5,\n               'max_weight': 15,\n               'item_weights': np.array([1, 12, 2, 1, 4]),\n               'item_values': np.array([2, 4, 2, 1, 10]),\n               'mask': True}\n env = or_gym.make('Knapsack-v0', env_config=env_config)\n print(\"Max weight capacity:\\t{}kg\".format(env.max_weight))\n print(\"Number of items:\\t{}\".format(env.N))\n \n tf = try_import_tf()\n # tf.compat.v1.disable_eager_execution()\n \n \n class KP0ActionMaskModel(TFModelV2):\n \n     def __init__(self, obs_space, action_space, num_outputs,\n                  model_config, name, true_obs_shape=(11,),\n                  action_embed_size=5, *args, **kwargs):\n \n         super(KP0ActionMaskModel, self).__init__(obs_space,\n                                                  action_space, num_outputs, model_config, name,\n                                                  *args, **kwargs)\n \n         self.action_embed_model = FullyConnectedNetwork(\n             spaces.Box(0, 1, shape=true_obs_shape),\n             action_space, action_embed_size,\n             model_config, name + \"_action_embedding\")\n         self.register_variables(self.action_embed_model.variables())\n \n     def forward(self, input_dict, state, seq_lens):\n         avail_actions = input_dict[\"obs\"][\"avail_actions\"]\n         action_mask = input_dict[\"obs\"][\"action_mask\"]\n         action_embedding, _ = self.action_embed_model({\n             \"obs\": input_dict[\"obs\"][\"state\"]})\n         intent_vector = tf.expand_dims(action_embedding, 1)\n         action_logits = tf.math.reduce_sum(avail_actions * intent_vector,\n                                            axis=1)\n         inf_mask = tf.math.maximum(tf.math.log(action_mask), tf.float32.min)\n         return action_logits + inf_mask, state\n \n     def value_function(self):\n         return self.action_embed_model.value_function()\n \n \n ModelCatalog.register_custom_model('kp_mask', KP0ActionMaskModel)\n \n \n def register_env(env_name, env_config={}):\n     env = create_env(env_name)\n     tune.register_env(env_name, lambda env_name: env(\n         env_name, env_config=env_config))\n \n \n register_env('Knapsack-v0', env_config=env_config)\n \n \n ray.init(ignore_reinit_error=True)\n trainer_config = {\n     \"model\": {\n         \"custom_model\": \"kp_mask\"\n         },\n     \"env_config\": env_config\n      }\n trainer = agents.ppo.PPOTrainer(env='Knapsack-v0', config=trainer_config)\n \n env = trainer.env_creator('Knapsack-v0')\n state = env.state\n state['action_mask'][0] = 0\n \n \n actions = np.array([trainer.compute_action(state) for i in range(10)])\n \n print(actions)\n </denchmark-code>\n \n This script works fine in Ray0.8.7, but in Ray1.0.1 rasie Error. Because trainer.compute_action() can\u2019t deal with dict type input\n <denchmark-h:h3>Error</denchmark-h>\n \n <denchmark-code>Traceback (most recent call last):\n   File \"/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 60, in check_shape\n     if not self._obs_space.contains(observation):\n   File \"/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/gym/spaces/box.py\", line 128, in contains\n     return x.shape == self.shape and np.all(x >= self.low) and np.all(x <= self.high)\n AttributeError: 'dict' object has no attribute 'shape'\n \n During handling of the above exception, another exception occurred:\n \n Traceback (most recent call last):\n   File \"/notebooks/projects/hanyu/ReferProject/MahjongFastPK/test.py\", line 96, in <module>\n     actions = np.array([trainer.compute_action(state) for i in range(10)])\n   File \"/notebooks/projects/hanyu/ReferProject/MahjongFastPK/test.py\", line 96, in <listcomp>\n     actions = np.array([trainer.compute_action(state) for i in range(10)])\n   File \"/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\", line 819, in compute_action\n     preprocessed = self.workers.local_worker().preprocessors[\n   File \"/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 166, in transform\n     self.check_shape(observation)\n   File \"/data2/huangcq/miniconda3/envs/majenv/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\", line 66, in check_shape\n     raise ValueError(\n ValueError: ('Observation for a Box/MultiBinary/MultiDiscrete space should be an np.array, not a Python list.', {'action_mask': array([0, 1, 1, 1, 1]), 'avail_actions': array([1., 1., 1., 1., 1.]), 'state': array([ 1, 12,  2,  1,  4,  2,  4,  2,  1, 10,  0])})\n </denchmark-code>\n \n <denchmark-h:h3>Question</denchmark-h>\n \n The problem is code works fine in Ray 0.8.6, bug in Ray 1.0.1 raise the ValueError.\n So, what should i do to use compute_action() dealing with Dict type input in Ray 1.0.1?\n Thanks for any help!\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hybug", "commentT": "2020-12-01T03:26:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n  Any suggestion will be helpful\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hybug", "commentT": "2020-12-02T08:04:33Z", "comment_text": "\n \t\tsven1977 ask me to assign this issue to him in discuss.ray.io, but i dont know how to assing issues. Could you help me assign this issues? <denchmark-link:https://github.com/ericl>@ericl</denchmark-link>\n   Thanks!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hybug", "commentT": "2020-12-04T08:39:46Z", "comment_text": "\n \t\tUpdate: Ray 1.0.1 Raise Error, And Ray 1.0.0 works well <denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n \n P.S.\n if use Ray 1.0.0 change tf = try_import_tf() to tf1, tf, tfv = try_import_tf() to aviod tf.expand_dims() Error\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hybug", "commentT": "2020-12-09T08:16:23Z", "comment_text": "\n \t\tTaking a look right now ...\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "hybug", "commentT": "2020-12-09T08:16:32Z", "comment_text": "\n \t\tThanks for filing this <denchmark-link:https://github.com/hybug>@hybug</denchmark-link>\n  !\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "hybug", "commentT": "2020-12-09T10:03:24Z", "comment_text": "\n \t\tOk, here is the fix PR: <denchmark-link:https://github.com/ray-project/ray/pull/12706>#12706</denchmark-link>\n \n As a workaround, could you change the following in your rllib/evaluation/worker_set.py (look for the  if-block)?\n <denchmark-code>            # If num_workers > 0, get the action_spaces and observation_spaces\n             # to not be forced to create an Env on the driver.\n             if self._remote_workers:\n                 remote_spaces = ray.get(self.remote_workers(\n                 )[0].foreach_policy.remote(\n                     lambda p, pid: (pid, p.observation_space, p.action_space)))\n                 spaces = {\n                     e[0]: (getattr(e[1], \"original_space\", e[1]), e[2])\n                     for e in remote_spaces\n                 }\n             ...\n </denchmark-code>\n \n The problem was that the local-worker (driver) was using the already preprocessed space (got it from the remote-worker) to build its own policy/preprocessot stack. That's why the necessary DictFlatteningPreprocessor was never built and your Trainer did not do any preprocessing (on your input dict's observation) prior to sending the data to the Policy.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "hybug", "commentT": "2020-12-09T13:20:15Z", "comment_text": "\n \t\tWonderful solution! Thanks to your selfless contribution!\n \t\t"}}}, "commit": {"commit_id": "ea25482f6a4467e8cc3aa6543d83da47543b44b6", "commit_author": "Sven Mika", "commitT": "2020-12-09 11:49:21-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\agents\\trainer_template.py", "file_new_name": "rllib\\agents\\trainer_template.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "68", "deleted_lines": "68"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "rllib\\evaluation\\worker_set.py", "file_new_name": "rllib\\evaluation\\worker_set.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "82,83,84,85", "deleted_lines": "82"}}}}}}