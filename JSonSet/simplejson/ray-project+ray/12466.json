{"BR": {"BR_id": "12466", "BR_author": "liside", "BRopenT": "2020-11-28T04:06:29Z", "BRcloseT": "2020-12-20T05:46:34Z", "BR_text": {"BRsummary": "[SGD] Inconsistent timeout units used in SGD", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Inconsistent timeout units used in SGD, making it unable to time out properly.\n TorchTrainer in RaySGD takes a parameter timeout_s:\n <denchmark-code>timeout_s (float): Seconds before the torch process group times out. Useful when machines are unreliable.\n </denchmark-code>\n \n In the remote mode, TorchTrainer wraps timedelta around the given timeout in seconds to initialize workers. As per Python doc, timedelta turns . If one gives 10.0 to TorchTrainer, timedelta will turn it into 10 days, causing it nearly impossible to time out. See: <denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L178>https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L178</denchmark-link>\n \n Ray version and other system information (Python version, TensorFlow version, OS):\n Ray v1.0.0, PyTorch v1.7.0, Ubuntu v18.04\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n Any tutorial code using remote workers would reproduce this bug.\n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "liside", "commentT": "2020-11-28T18:11:17Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/amogkam>@amogkam</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "liside", "commentT": "2020-11-29T05:11:55Z", "comment_text": "\n \t\tThis is a good catch <denchmark-link:https://github.com/liside>@liside</denchmark-link>\n ! Should be fixed in <denchmark-link:https://github.com/ray-project/ray/pull/12477>#12477</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "liside", "commentT": "2020-11-29T05:52:20Z", "comment_text": "\n \t\tYou might want to change this line as well: <denchmark-link:https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L470>https://github.com/ray-project/ray/blob/master/python/ray/util/sgd/torch/worker_group.py#L470</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "liside", "commentT": "2020-11-29T06:12:11Z", "comment_text": "\n \t\tOh right another good catch ;)\n \t\t"}}}, "commit": {"commit_id": "51139ed37c5a64a87c916e6a6675385f9b700535", "commit_author": "Amog Kamsetty", "commitT": "2020-12-19 21:46:33-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\util\\sgd\\torch\\constants.py", "file_new_name": "python\\ray\\util\\sgd\\torch\\constants.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "9", "deleted_lines": "9"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\util\\sgd\\torch\\torch_trainer.py", "file_new_name": "python\\ray\\util\\sgd\\torch\\torch_trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "148", "deleted_lines": "146", "method_info": {"method_name": "__init__", "method_params": "self,training_operator_cls,initialization_hook,config,num_workers,num_cpus_per_worker,use_gpu,backend,wrap_ddp,timeout_s,use_fp16,use_tqdm,add_dist_sampler,scheduler_step_freq,use_local,num_replicas,batch_size,model_creator,data_creator,optimizer_creator,scheduler_creator,loss_creator,serialize_data_creation,data_loader_args,apex_args", "method_startline": "137", "method_endline": "164"}}, "hunk_1": {"Ismethod": 1, "added_lines": "148", "deleted_lines": "146", "method_info": {"method_name": "__init__", "method_params": "self,training_operator_cls,initialization_hook,config,num_workers,num_cpus_per_worker,use_gpu,backend,wrap_ddp,timeout_s,use_fp16,use_tqdm,add_dist_sampler,scheduler_step_freq,use_local,num_replicas,batch_size,model_creator,data_creator,optimizer_creator,scheduler_creator,loss_creator,serialize_data_creation,data_loader_args,apex_args", "method_startline": "135", "method_endline": "162"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\util\\sgd\\torch\\worker_group.py", "file_new_name": "python\\ray\\util\\sgd\\torch\\worker_group.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "178", "deleted_lines": "178", "method_info": {"method_name": "_setup_process_group", "method_params": "self,address,world_size,starting_rank", "method_startline": "153", "method_endline": "181"}}, "hunk_1": {"Ismethod": 1, "added_lines": "470", "deleted_lines": "470", "method_info": {"method_name": "start_workers", "method_params": "self,num_workers", "method_startline": "438", "method_endline": "481"}}}}}}}