{"BR": {"BR_id": "13554", "BR_author": "dmatch01", "BRopenT": "2021-01-19T14:11:48Z", "BRcloseT": "2021-01-20T19:41:55Z", "BR_text": {"BRsummary": "Failure creating cluster using Kubernetes Operator", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n reference: <denchmark-link:https://docs.ray.io/en/master/cluster/k8s-operator.html#k8s-operator>https://docs.ray.io/en/master/cluster/k8s-operator.html#k8s-operator</denchmark-link>\n \n Followed the instructions referenced in the k8s operator setup.  After successfully creating the operator pod attempted to launch a Ray cluster(kubectl -n ray apply -f ray/python/ray/autoscaler/kubernetes/operator_configs/example_cluster.yaml) but launch failed.  See logs from Ray Operator pod:\n <denchmark-code>$ oc logs ray-operator-pod \n example-cluster:2021-01-19 05:46:11,736\tDEBUG config.py:83 -- Updating the resources of node type head-node to include {'CPU': 1, 'GPU': 0}.\n example-cluster:2021-01-19 05:46:11,737\tDEBUG config.py:83 -- Updating the resources of node type worker-nodes to include {'CPU': 1, 'GPU': 0}.\n example-cluster:2021-01-19 05:46:11,773\tWARNING config.py:164 -- KubernetesNodeProvider: not checking if namespace 'ray' exists\n example-cluster:2021-01-19 05:46:11,773\tINFO config.py:184 -- KubernetesNodeProvider: no autoscaler_service_account config provided, must already exist\n example-cluster:2021-01-19 05:46:11,773\tINFO config.py:210 -- KubernetesNodeProvider: no autoscaler_role config provided, must already exist\n example-cluster:2021-01-19 05:46:11,774\tINFO config.py:236 -- KubernetesNodeProvider: no autoscaler_role_binding config provided, must already exist\n example-cluster:2021-01-19 05:46:11,774\tINFO config.py:269 -- KubernetesNodeProvider: no services config provided, must already exist\n example-cluster:2021-01-19 05:46:11,809\tINFO node_provider.py:114 -- KubernetesNodeProvider: calling create_namespaced_pod (count=1).\n 2021-01-19 05:46:11,687\tINFO commands.py:221 -- Cluster: example-cluster\n 2021-01-19 05:46:11,735\tINFO commands.py:283 -- Checking Kubernetes environment settings\n 2021-01-19 05:46:11,808\tINFO commands.py:533 -- No head node found. Launching a new cluster. Confirm [y/N]: y [automatic, due to --yes]\n 2021-01-19 05:46:11,808\tINFO commands.py:578 -- Acquiring an up-to-date head node\n Process example-cluster:\n Traceback (most recent call last):\n   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n     self.run()\n   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n     self._target(*self._args, **self._kwargs)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/operator/operator.py\", line 48, in _create_or_update\n     self.start_head()\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/operator/operator.py\", line 60, in start_head\n     no_config_cache=True)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/commands.py\", line 228, in create_or_update_cluster\n     override_cluster_name)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/commands.py\", line 598, in get_or_create_head_node\n     provider.create_node(head_node_config, head_node_tags, 1)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/autoscaler/_private/kubernetes/node_provider.py\", line 117, in create_node\n     pod = core_api().create_namespaced_pod(self.namespace, pod_spec)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 7320, in create_namespaced_pod\n     return self.create_namespaced_pod_with_http_info(namespace, body, **kwargs)  # noqa: E501\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 7429, in create_namespaced_pod_with_http_info\n     collection_formats=collection_formats)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 353, in call_api\n     _preload_content, _request_timeout, _host)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 184, in __call_api\n     _request_timeout=_request_timeout)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 397, in request\n     body=body)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 280, in POST\n     body=body)\n   File \"/home/ray/anaconda3/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 233, in request\n     raise ApiException(http_resp=r)\n kubernetes.client.exceptions.ApiException: (403)\n Reason: Forbidden\n HTTP response headers: HTTPHeaderDict({'Audit-Id': '562ee453-9aa8-4190-8450-3fb975bb0a7a', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Tue, 19 Jan 2021 13:46:11 GMT', 'Content-Length': '352'})\n HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"pods \\\"example-cluster-ray-head-pmfw4\\\" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on: , \\u003cnil\\u003e\",\"reason\":\"Forbidden\",\"details\":{\"name\":\"example-cluster-ray-head-pmfw4\",\"kind\":\"pods\"},\"code\":403}\n </denchmark-code>\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a short code snippet (less than 50 lines if possible) that can be copy-pasted to reproduce the issue. The snippet should have no external library dependencies (i.e., use fake or mock data / environments):\n Environment Details:\n Openshift v4.5.8\n <denchmark-code>$ kubectl version\n Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.0\", GitCommit:\"0ed33881dc4355495f623c6f22e7dd0b7632b7c0\", GitTreeState:\"clean\", BuildDate:\"2018-09-27T17:05:32Z\", GoVersion:\"go1.10.4\", Compiler:\"gc\", Platform:\"darwin/amd64\"}\n Server Version: version.Info{Major:\"1\", Minor:\"18+\", GitVersion:\"v1.18.3+fa69cae\", GitCommit:\"fa69cae\", GitTreeState:\"clean\", BuildDate:\"2020-12-14T23:03:06Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n </denchmark-code>\n \n Operator Successfully Running:\n <denchmark-code>$ kubectl get pods\n NAME               READY   STATUS    RESTARTS   AGE\n ray-operator-pod   1/1     Running   1          24m\n </denchmark-code>\n \n Ray Image:\n <denchmark-code>$ kubectl describe pod ray-operator-pod  | grep \"Image ID:\"\n     Image ID:      docker.io/rayproject/ray@sha256:b6273b691dff8d980128dad0a6fe70ceadc755ea24490da413d07710ee04d88b\n </denchmark-code>\n \n YAML output of running operator pod:\n <denchmark-code>$ kubectl get pod ray-operator-pod -o yaml\n apiVersion: v1\n kind: Pod\n metadata:\n   annotations:\n     cni.projectcalico.org/podIP: 172.30.100.133/32\n     cni.projectcalico.org/podIPs: 172.30.100.133/32\n     k8s.v1.cni.cncf.io/network-status: |-\n       [{\n           \"name\": \"k8s-pod-network\",\n           \"ips\": [\n               \"172.30.100.133\"\n           ],\n           \"default\": true,\n           \"dns\": {}\n       }]\n     k8s.v1.cni.cncf.io/networks-status: |-\n       [{\n           \"name\": \"k8s-pod-network\",\n           \"ips\": [\n               \"172.30.100.133\"\n           ],\n           \"default\": true,\n           \"dns\": {}\n       }]\n     kubectl.kubernetes.io/last-applied-configuration: |\n       {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"ray-operator-pod\",\"namespace\":\"ray\"},\"spec\":{\"containers\":[{\"command\":[\"ray-operator\"],\"env\":[{\"name\":\"RAY_OPERATOR_POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"rayproject/ray:nightly\",\"imagePullPolicy\":\"Always\",\"name\":\"ray\",\"resources\":{\"limits\":{\"memory\":\"2Gi\"},\"requests\":{\"cpu\":1,\"memory\":\"1Gi\"}}}],\"serviceAccountName\":\"ray-operator-serviceaccount\"}}\n     openshift.io/scc: anyuid\n   creationTimestamp: 2021-01-19T13:42:53Z\n   managedFields:\n   - apiVersion: v1\n     fieldsType: FieldsV1\n     fieldsV1:\n       f:metadata:\n         f:annotations:\n           .: {}\n           f:kubectl.kubernetes.io/last-applied-configuration: {}\n       f:spec:\n         f:containers:\n           k:{\"name\":\"ray\"}:\n             .: {}\n             f:command: {}\n             f:env:\n               .: {}\n               k:{\"name\":\"RAY_OPERATOR_POD_NAMESPACE\"}:\n                 .: {}\n                 f:name: {}\n                 f:valueFrom:\n                   .: {}\n                   f:fieldRef:\n                     .: {}\n                     f:apiVersion: {}\n                     f:fieldPath: {}\n             f:image: {}\n             f:imagePullPolicy: {}\n             f:name: {}\n             f:resources:\n               .: {}\n               f:limits:\n                 .: {}\n                 f:memory: {}\n               f:requests:\n                 .: {}\n                 f:cpu: {}\n                 f:memory: {}\n             f:terminationMessagePath: {}\n             f:terminationMessagePolicy: {}\n         f:dnsPolicy: {}\n         f:enableServiceLinks: {}\n         f:restartPolicy: {}\n         f:schedulerName: {}\n         f:securityContext: {}\n         f:serviceAccount: {}\n         f:serviceAccountName: {}\n         f:terminationGracePeriodSeconds: {}\n     manager: kubectl\n     operation: Update\n     time: 2021-01-19T13:42:53Z\n   - apiVersion: v1\n     fieldsType: FieldsV1\n     fieldsV1:\n       f:metadata:\n         f:annotations:\n           f:cni.projectcalico.org/podIP: {}\n           f:cni.projectcalico.org/podIPs: {}\n     manager: calico\n     operation: Update\n     time: 2021-01-19T13:42:54Z\n   - apiVersion: v1\n     fieldsType: FieldsV1\n     fieldsV1:\n       f:metadata:\n         f:annotations:\n           f:k8s.v1.cni.cncf.io/network-status: {}\n           f:k8s.v1.cni.cncf.io/networks-status: {}\n     manager: multus\n     operation: Update\n     time: 2021-01-19T13:42:54Z\n   - apiVersion: v1\n     fieldsType: FieldsV1\n     fieldsV1:\n       f:status:\n         f:conditions:\n           k:{\"type\":\"ContainersReady\"}:\n             .: {}\n             f:lastProbeTime: {}\n             f:lastTransitionTime: {}\n             f:status: {}\n             f:type: {}\n           k:{\"type\":\"Initialized\"}:\n             .: {}\n             f:lastProbeTime: {}\n             f:lastTransitionTime: {}\n             f:status: {}\n             f:type: {}\n           k:{\"type\":\"Ready\"}:\n             .: {}\n             f:lastProbeTime: {}\n             f:lastTransitionTime: {}\n             f:status: {}\n             f:type: {}\n         f:containerStatuses: {}\n         f:hostIP: {}\n         f:phase: {}\n         f:podIP: {}\n         f:podIPs:\n           .: {}\n           k:{\"ip\":\"172.30.100.133\"}:\n             .: {}\n             f:ip: {}\n         f:startTime: {}\n     manager: kubelet\n     operation: Update\n     time: 2021-01-19T14:01:52Z\n   name: ray-operator-pod\n   namespace: ray\n   resourceVersion: \"25632355\"\n   selfLink: /api/v1/namespaces/ray/pods/ray-operator-pod\n   uid: a7163014-e459-4479-9665-732ee18eff16\n spec:\n   containers:\n   - command:\n     - ray-operator\n     env:\n     - name: RAY_OPERATOR_POD_NAMESPACE\n       valueFrom:\n         fieldRef:\n           apiVersion: v1\n           fieldPath: metadata.namespace\n     image: rayproject/ray:nightly\n     imagePullPolicy: Always\n     name: ray\n     resources:\n       limits:\n         memory: 2Gi\n       requests:\n         cpu: \"1\"\n         memory: 1Gi\n     securityContext:\n       capabilities:\n         drop:\n         - MKNOD\n     terminationMessagePath: /dev/termination-log\n     terminationMessagePolicy: File\n     volumeMounts:\n     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n       name: ray-operator-serviceaccount-token-lhfnd\n       readOnly: true\n   dnsPolicy: ClusterFirst\n   enableServiceLinks: true\n   imagePullSecrets:\n   - name: ray-operator-serviceaccount-dockercfg-bcrj5\n   nodeName: 10.95.102.76\n   priority: 0\n   restartPolicy: Always\n   schedulerName: default-scheduler\n   securityContext:\n     seLinuxOptions:\n       level: s0:c25,c15\n   serviceAccount: ray-operator-serviceaccount\n   serviceAccountName: ray-operator-serviceaccount\n   terminationGracePeriodSeconds: 30\n   tolerations:\n   - effect: NoExecute\n     key: node.kubernetes.io/not-ready\n     operator: Exists\n     tolerationSeconds: 300\n   - effect: NoExecute\n     key: node.kubernetes.io/unreachable\n     operator: Exists\n     tolerationSeconds: 300\n   - effect: NoSchedule\n     key: node.kubernetes.io/memory-pressure\n     operator: Exists\n   volumes:\n   - name: ray-operator-serviceaccount-token-lhfnd\n     secret:\n       defaultMode: 420\n       secretName: ray-operator-serviceaccount-token-lhfnd\n status:\n   conditions:\n   - lastProbeTime: null\n     lastTransitionTime: 2021-01-19T13:42:53Z\n     status: \"True\"\n     type: Initialized\n   - lastProbeTime: null\n     lastTransitionTime: 2021-01-19T14:01:52Z\n     status: \"True\"\n     type: Ready\n   - lastProbeTime: null\n     lastTransitionTime: 2021-01-19T14:01:52Z\n     status: \"True\"\n     type: ContainersReady\n   - lastProbeTime: null\n     lastTransitionTime: 2021-01-19T13:42:53Z\n     status: \"True\"\n     type: PodScheduled\n   containerStatuses:\n   - containerID: cri-o://a9670e6ca1e06ed70db76af3fe35e79198b91cbb3f75bc3d4d1d777f5aae7dae\n     image: docker.io/rayproject/ray:nightly\n     imageID: docker.io/rayproject/ray@sha256:8a09fc4eff3c142ae9c0174b7beb8311a479afd53d85010aa092307479d59eb5\n     lastState:\n       terminated:\n         containerID: cri-o://08c907a4ff9c2854fe34edd158432cd87724bbcbf4d0ba346690593c8824bcef\n         exitCode: 1\n         finishedAt: 2021-01-19T14:01:49Z\n         reason: Error\n         startedAt: 2021-01-19T13:43:42Z\n     name: ray\n     ready: true\n     restartCount: 1\n     started: true\n     state:\n       running:\n         startedAt: 2021-01-19T14:01:51Z\n   hostIP: 10.95.102.76\n   phase: Running\n   podIP: 172.30.100.133\n   podIPs:\n   - ip: 172.30.100.133\n   qosClass: Burstable\n   startTime: 2021-01-19T13:42:53Z\n </denchmark-code>\n \n If the code snippet cannot be run by itself, the issue will be closed with \"needs-repro-script\".\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "dmatch01", "commentT": "2021-01-19T14:42:19Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/DmitriGekhtman>@DmitriGekhtman</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "dmatch01", "commentT": "2021-01-19T18:15:01Z", "comment_text": "\n \t\tThanks for posting this issue!\n I'm looking into it.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "dmatch01", "commentT": "2021-01-19T18:26:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/DmitriGekhtman>@DmitriGekhtman</denchmark-link>\n  Thanks!  I have a PR for a fix but since this is my first PR to Ray, trying to meet contributing requirement which I'm fumbling through, :).  Basically here is the fix:\n <denchmark-link:https://github.com/dmatch01/ray/tree/k8s-operator-add-finalizer>https://github.com/dmatch01/ray/tree/k8s-operator-add-finalizer</denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/12192012/105077041-ee356d00-5a59-11eb-9222-6b54d13771ce.png></denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "dmatch01", "commentT": "2021-01-19T18:30:14Z", "comment_text": "\n \t\tAwesome, thanks!\n Gathered from a few searches that other OpenShift users had exactly the same problem with other operators, with exactly the same solution.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "dmatch01", "commentT": "2021-01-19T18:46:21Z", "comment_text": "\n \t\t\n @DmitriGekhtman Thanks! I have a PR for a fix but since this is my first PR to Ray, trying to meet contributing requirement which I'm fumbling through, :).\n \n <denchmark-link:https://github.com/dmatch01>@dmatch01</denchmark-link>\n \n I think it'd be fine to just open the PR. Let me know if you have any trouble with that.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "dmatch01", "commentT": "2021-01-19T20:59:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/DmitriGekhtman>@DmitriGekhtman</denchmark-link>\n  Per your suggestion I've submitted PR: <denchmark-link:https://github.com/ray-project/ray/pull/13567>#13567</denchmark-link>\n .  Please review at your convenience.\n \t\t"}}}, "commit": {"commit_id": "fd6882176a1f14188c37cd4dc201055c10ca8556", "commit_author": "dmatch01", "commitT": "2021-01-20 13:02:02-06:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\autoscaler\\kubernetes\\operator_configs\\operator.yaml", "file_new_name": "python\\ray\\autoscaler\\kubernetes\\operator_configs\\operator.yaml", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13,14", "deleted_lines": "13,14"}}}}}}