{"BR": {"BR_id": "11071", "BR_author": "lasagnaphil", "BRopenT": "2020-09-28T12:10:20Z", "BRcloseT": "2020-10-06T05:07:59Z", "BR_text": {"BRsummary": "[rllib] Torch checkpoint taken on GPU fails to deserialize on CPU", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n While trying to load a checkpoint in RLlib using Torch backend and GPU, I immediately get the following error:\n <denchmark-code>2020-09-28 20:49:56,066 ERROR worker.py:1032 -- Possible unhandled error from worker: ray::RolloutWorker.restore() (pid=2205302, ip=147.46.240.48)\n   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 438, in ray._raylet.execute_task.function_executor\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in restore\n     objs = pickle.loads(objs)\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/storage.py\", line 142, in _load_from_bytes\n     return torch.load(io.BytesIO(b))\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py\", line 585, in load\n     return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py\", line 765, in _legacy_load\n     result = unpickler.load()\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py\", line 721, in persistent_load\n     deserialized_objects[root_key] = restore_location(obj, location)\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py\", line 174, in default_restore_location\n     result = fn(storage, location)\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py\", line 150, in _cuda_deserialize\n     device = validate_cuda_device(location)\n   File \"/home/lasagnaphil/envs/MASS/lib/python3.8/site-packages/torch/serialization.py\", line 134, in validate_cuda_device\n     raise RuntimeError('Attempting to deserialize object on a CUDA '\n RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n </denchmark-code>\n \n Although the training continues to run despite the error message, the training seems to start from scratch and not from where I left off, which seems to me that the model is not loaded properly.\n <denchmark-link:https://user-images.githubusercontent.com/11910667/94430234-87e8be00-01ce-11eb-8c17-c285d6cb2755.png></denchmark-link>\n \n This happens only when using Torch and GPU for the training (num_gpus=1). But checkpoints work fine when only using CPU (num_gpus=0). Are there any potential workarounds or bugfixes for this problem?\n Ray version and other system information (Python version, TensorFlow version, OS):\n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n Please provide a script that can be run to reproduce the issue. The script should have no external library dependencies (i.e., use fake or mock data / environments):\n ray_test.py:\n import ray\n from ray import tune\n from ray.rllib.agents.ppo import PPOTrainer\n \n import argparse\n import os\n from pathlib import Path\n \n parser = argparse.ArgumentParser()\n parser.add_argument(\"--checkpoint\", type=str)\n \n args = parser.parse_args()\n \n ray.init(num_cpus=32, num_gpus=1)\n \n # Using config from tuned_examples folder\n config = {\n     \"framework\": \"torch\",\n     \"env\": \"CartPole-v0\",\n     \"num_gpus\": 1,\n     \"num_workers\": 1,\n \n     \"gamma\": 0.99,\n     \"lr\": 0.0003,\n     \"observation_filter\": \"MeanStdFilter\",\n     \"num_sgd_iter\": 6,\n     \"vf_share_layers\": True,\n     \"vf_loss_coeff\": 0.01,\n     \"model\": {\"fcnet_hiddens\": [32], \"fcnet_activation\": \"linear\"}\n }\n \n if args.checkpoint:\n     tune.run(PPOTrainer, config=config, \n         checkpoint_freq=10, \n         local_dir=os.environ[\"PWD\"] + \"/ray_results\",\n         restore=args.checkpoint)\n else:\n     tune.run(PPOTrainer, config=config,\n         checkpoint_freq=10,\n         local_dir=os.environ[\"PWD\"] + \"/ray_results\")\n Starting the training: python ray_test.py\n Loading from a checkpoint (for example): python ray_test.py --checkpoint ray_results/PPO/PPO_CartPole-v0_77d6c_00000_0_2020-09-28_20-48-04/checkpoint_20/checkpoint-20\n If we cannot run your script, we cannot fix your issue.\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lasagnaphil", "commentT": "2020-09-28T18:08:08Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sven1977>@sven1977</denchmark-link>\n  I think this is since we serializing tensors instead of numpy arrays for torch--- can you look into it? I remember there was another issue reported around too but can't find it.\n Fix is just to always call .numpy() when-ever saving/checkpointing a model state.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lasagnaphil", "commentT": "2020-10-05T04:48:51Z", "comment_text": "\n \t\tI examined the code for the TorchPolicy class, and the culprit was that the optimizer states weren't serializing properly (The optimizer weights needed calls for  and  just as the model weights did.) I fixed this and created a pull request which solves the problem (at least for my desktop GPU setup): <denchmark-link:https://github.com/ray-project/ray/pull/11208>#11208</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lasagnaphil", "commentT": "2020-10-06T05:07:59Z", "comment_text": "\n \t\tClosing this issue since the PR is merged. Thanks!\n \t\t"}}}, "commit": {"commit_id": "2b26d2ca1b7ad4f400dec0d97feddcd2f0bf6026", "commit_author": "Philsik Chang", "commitT": "2020-10-05 22:01:55-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\policy\\torch_policy.py", "file_new_name": "rllib\\policy\\torch_policy.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "496,497,498", "deleted_lines": "495", "method_info": {"method_name": "set_state", "method_params": "self,object", "method_startline": "489", "method_endline": "500"}}, "hunk_1": {"Ismethod": 1, "added_lines": "483,484", "deleted_lines": "483", "method_info": {"method_name": "get_state", "method_params": "self", "method_startline": "479", "method_endline": "485"}}}}}}}