{"BR": {"BR_id": "9036", "BR_author": "Cryptiex", "BRopenT": "2020-06-19T11:36:27Z", "BRcloseT": "2020-07-18T07:58:16Z", "BR_text": {"BRsummary": "[tune] Population-based training: broken when using keep_checkpoint_num", "BRdescription": "\n When using population-based training TUNE stops after some times throwing the following error:\n There are paused trials, but no more pending trials with sufficient resources.\n This is caused by not finding the latest checkpoint:\n <denchmark-code>Failure # 1 (occurred at 2020-06-19_11-26-36)\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 294, in start_trial\n     self._start_trial(trial, checkpoint, train=train)\n   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 235, in _start_trial\n     self.restore(trial, checkpoint)\n   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 673, in restore\n     data_dict = TrainableUtil.pickle_checkpoint(value)\n   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py\", line 62, in pickle_checkpoint\n     checkpoint_dir = TrainableUtil.find_checkpoint_dir(checkpoint_path)\n   File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py\", line 87, in find_checkpoint_dir\n     raise FileNotFoundError(\"Path does not exist\", checkpoint_path)\n FileNotFoundError: [Errno Path does not exist] /content/TRASH_TUNE_PBT_oversampling_mimic_densenet121/TUNE_Model_0_2020-06-19_11-24-215xncry9c/checkpoint_6/\n </denchmark-code>\n \n The error appears to be somewhat random since it only appears after quite some iterations\n The error can be reproduced in this <denchmark-link:https://colab.research.google.com/drive/1-o896bEUm7DTvS24Do0btlqbSHre49MH?usp=sharing>colab notebook</denchmark-link>\n . \n <denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  Is this related to <denchmark-link:https://github.com/ray-project/ray/issues/8772>#8772</denchmark-link>\n  ?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Cryptiex", "commentT": "2020-06-19T19:37:07Z", "comment_text": "\n \t\tYeah this seems like an issue. We'll take a look into this.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Cryptiex", "commentT": "2020-06-19T21:05:32Z", "comment_text": "\n \t\tThis is the issue (just for reference):\n <denchmark-code>from ray.tune.schedulers import  PopulationBasedTraining\n \n scheduler = PopulationBasedTraining(\n             time_attr='training_iteration',\n             metric='mean_accuracy',\n             mode='max',\n             perturbation_interval=1,\n             log_config=True,\n             hyperparam_mutations={      \n                 \"lr\": lambda: 10 ** np.random.uniform(-2, -5)\n             })\n \n train_config = {'lr': 0.01}\n \n \n analysis = tune.run(\n         TUNE_Model,         # Reference to the model class. \n         config=train_config, \n         fail_fast=True,         # Stop after first error\n         num_samples=4,          # Num of different hyperparameter configurations\n         search_alg=None,     # Search algotihm like Hyperopt, Nevergrad...\n         resources_per_trial={\"gpu\": 1}, # Requested resources per TRIAL. If set to fraction, total_GPUS/res_p_trial can be run in parallel\n         global_checkpoint_period=np.inf,   # Do not save checkpoints based on time interval\n         checkpoint_freq = 1,        # Save checkpoint every time the checkpoint_score_attr improves\n         checkpoint_at_end = True,   \n         keep_checkpoints_num = 2,   # Keep only the best checkpoint\n         checkpoint_score_attr = 'mean_accuracy', # Metric used to compare checkpoints\n         verbose=1,\n         scheduler=scheduler,             # Stop bad trials early\n         name=\"TRASH_TUNE_PBT_oversampling_mimic_densenet121\",\n         local_dir=\"/content\",\n         stop={                          # Stop a single trial if one of the conditions are met\n             \"mean_accuracy\": 0.85,\n             \"training_iteration\": 15})\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 4, "comment_author": "Cryptiex", "commentT": "2020-06-20T01:09:07Z", "comment_text": "\n \t\tI think the temporary fix actually is to comment this out:\n <denchmark-code>        keep_checkpoints_num = 2,   # Keep only the best checkpoint\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 5, "comment_author": "Cryptiex", "commentT": "2020-06-20T21:15:00Z", "comment_text": "\n \t\tThanks for the fast reply! Commenting out keep_checkpoints_num worked for me as a temporary fix. Any idea where this is coming from? Does the PBT scheduler reference an already deleted checkpoint?\n \t\t"}, "comments_4": {"comment_id": 6, "comment_author": "Cryptiex", "commentT": "2020-06-20T21:47:54Z", "comment_text": "\n \t\tYeah, I think it's something about not properly bookkeeping the checkpoints on disk.\n \t\t"}, "comments_5": {"comment_id": 7, "comment_author": "Cryptiex", "commentT": "2020-06-30T10:20:31Z", "comment_text": "\n \t\tAny updates on this? The temporary fix seems not to work as indicated by <denchmark-link:https://github.com/ray-project/ray/issues/8441>#8441</denchmark-link>\n  and my own experiences\n \t\t"}, "comments_6": {"comment_id": 8, "comment_author": "Cryptiex", "commentT": "2020-12-04T04:40:23Z", "comment_text": "\n \t\tHi,\n I think this problem still persists in another form, mainly in the pickling function which apparently is not using protocol=4, resulting in an error when objects larger than 4GB have to pickled (e.g. during checkpointing). I am using PBT with keep_checkpoints_num=2.\n <denchmark-code>Failure # 1 (occurred at 2020-12-03_20-27-31)\n Traceback (most recent call last):\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 348, in start_trial\n     self._start_trial(trial, checkpoint, train=train)\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 289, in _start_trial\n     self.restore(trial, checkpoint)\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 733, in restore\n     obj = TrainableUtil.checkpoint_to_object(value)\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/trainable.py\", line 86, in checkpoint_to_object\n     data_dict = TrainableUtil.pickle_checkpoint(checkpoint_path)\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/tune/trainable.py\", line 80, in pickle_checkpoint\n     \"data\": data,\n OverflowError: cannot serialize a bytes object larger than 4 GiB\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 9, "comment_author": "Cryptiex", "commentT": "2020-12-04T05:08:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/FarzanT>@FarzanT</denchmark-link>\n  thanks for opening this; I think your issue is unrelated to the original post. Can you create a new issue + try it out on the latest Ray wheels?\n \t\t"}, "comments_8": {"comment_id": 10, "comment_author": "Cryptiex", "commentT": "2020-12-04T05:57:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  Thank you for yout quick response, but before I open another issue I have to say that installing the nightlies resulted in the following problems:\n \n tensorboardx became a requirement all of a sudden (or didn't install with the package)\n I got lots of other errors:\n \n <denchmark-code>(pid=raylet) E1204 00:52:43.674732050   27963 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1607061163.674715559\",\"description\":\"SO_REUSEPORT unavailable on compiling system\",\"file\":\"src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":192}\n 2020-12-04 00:52:44,454 WARNING worker.py:1011 -- The agent on node ucn107-74.hpc.oicr.on.ca failed with the following error:\n Traceback (most recent call last):\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 300, in <module>\n     loop.run_until_complete(agent.run())\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/asyncio/base_events.py\", line 584, in run_until_complete\n     return future.result()\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 174, in run\n     agent_ip_address=self.ip))\n   File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/grpc/experimental/aio/_call.py\", line 286, in __await__\n     self._cython_call._status)\n grpc.experimental.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\n         status = StatusCode.UNAVAILABLE\n         details = \"failed to connect to all addresses\"\n         debug_error_string = \"{\"created\":\"@1607061164.441617149\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":4090,\"referenced_errors\":[{\"created\":\"@1607061164.441609893\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\"\n >\n </denchmark-code>\n \n With the normal pip install, the PBT would train all trials to the iteration where the perturbation was supposed to happen, then throw that pickle error. Now it's throwing the above error right from the beginning.\n Any help is appreciated!\n \t\t"}, "comments_9": {"comment_id": 11, "comment_author": "Cryptiex", "commentT": "2020-12-04T09:52:11Z", "comment_text": "\n \t\tOther errors look weird... well for tensorboardx, you should run also \u2018pip\n install \u201cray[tune]\u201d\u2019.\n \n Can you create an issue for other errors?\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Thu, Dec 3, 2020 at 9:57 PM Farzan Taj ***@***.***> wrote:\n  @richardliaw <https://github.com/richardliaw> Thank you for yout quick\n  response, but before I open another issue I have to say that installing the\n  nightlies resulted in the following problems:\n \n     1. tensorboardx became a requirement all of a sudden (or didn't\n     install with the package)\n     2. I got lots of other errors:\n \n  (pid=raylet) E1204 00:52:43.674732050   27963 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: ***@***.***\",\"description\":\"SO_REUSEPORT unavailable on compiling system\",\"file\":\"src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":192}\n  2020-12-04 00:52:44,454 WARNING worker.py:1011 -- The agent on node ucn107-74.hpc.oicr.on.ca failed with the following error:\n  Traceback (most recent call last):\n    File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 300, in <module>\n      loop.run_until_complete(agent.run())\n    File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/asyncio/base_events.py\", line 584, in run_until_complete\n      return future.result()\n    File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/ray/new_dashboard/agent.py\", line 174, in run\n      agent_ip_address=self.ip))\n    File \"/u/ftaj/anaconda3/envs/drp/lib/python3.7/site-packages/grpc/experimental/aio/_call.py\", line 286, in __await__\n      self._cython_call._status)\n  grpc.experimental.aio._call.AioRpcError: <AioRpcError of RPC that terminated with:\n          status = StatusCode.UNAVAILABLE\n          details = \"failed to connect to all addresses\"\n          debug_error_string = ***@***.***\",\"description\":\"Failed to pick ***@***.***\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\"\n  >\n \n  With the normal pip install, the PBT would train all trials to the\n  iteration where the perturbation was supposed to happen, then throw that\n  pickle error. Now it's throwing the above error right from the beginning.\n \n  Any help is appreciated!\n \n  \u2014\n  You are receiving this because you were mentioned.\n \n \n  Reply to this email directly, view it on GitHub\n  <#9036 (comment)>,\n  or unsubscribe\n  <https://github.com/notifications/unsubscribe-auth/ABCRZZLQLBLFL3CNVAUTTPDSTB25BANCNFSM4OCUXNBA>\n  .\n \n \n \n \t\t"}}}, "commit": {"commit_id": "ad0219b80db0b22c621fa6004095140e89ada75a", "commit_author": "krfricke", "commitT": "2020-07-18 00:58:16-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "python\\ray\\tune\\BUILD", "file_new_name": "python\\ray\\tune\\BUILD", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "183,184,185,186,187,188,189,190", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\tune\\checkpoint_manager.py", "file_new_name": "python\\ray\\tune\\checkpoint_manager.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "112,113,114,115", "deleted_lines": null, "method_info": {"method_name": "on_checkpoint", "method_params": "self,checkpoint", "method_startline": "97", "method_endline": "141"}}}}, "file_2": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "python\\ray\\tune\\tests\\test_trial_scheduler_pbt.py"}}}}