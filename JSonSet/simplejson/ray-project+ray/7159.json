{"BR": {"BR_id": "7159", "BR_author": "justinglibert", "BRopenT": "2020-02-13T22:22:55Z", "BRcloseT": "2020-02-17T18:26:59Z", "BR_text": {"BRsummary": "RLLib: PyTorch Recurrent Model: Hidden_state is not a Tensor, it's a Numpy Array", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n Latest ray installed from this wheel: <denchmark-link:https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl>https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl</denchmark-link>\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n PyTorch models get back a hidden state that has been casted to a Numpy array.\n That breaks torch.nn and will break backprop too.\n I took this file as an example of a RNN in PyTorch:\n <denchmark-link:https://github.com/ray-project/ray/blob/master/rllib/agents/qmix/model.py>https://github.com/ray-project/ray/blob/master/rllib/agents/qmix/model.py</denchmark-link>\n \n import ray\n from ray import tune\n from ray.rllib.agents import ppo\n from ray.rllib.models import ModelCatalog\n from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n from ray.rllib.utils.annotations import override\n from ray.tune.registry import register_env\n import gym\n from gym.spaces import Discrete, Box\n import numpy as np\n \n import torch\n from torch import nn\n \n \n class SimpleCorridor(gym.Env):\n     \"\"\"Example of a custom env in which you have to walk down a corridor.\n     You can configure the length of the corridor via the env config.\"\"\"\n \n     def __init__(self, config):\n         self.end_pos = config[\"corridor_length\"]\n         self.cur_pos = 0\n         self.action_space = Discrete(2)\n         self.observation_space = Box(\n             0.0, self.end_pos, shape=(1, ), dtype=np.float32)\n \n     def reset(self):\n         self.cur_pos = 0\n         return [self.cur_pos]\n \n     def step(self, action):\n         assert action in [0, 1], action\n         if action == 0 and self.cur_pos > 0:\n             self.cur_pos -= 1\n         elif action == 1:\n             self.cur_pos += 1\n         done = self.cur_pos >= self.end_pos\n         return [self.cur_pos], 1 if done else 0, done, {}\n \n \n \n class TestNet(TorchModelV2, nn.Module):\n     def init_hidden(self, hidden_size):\n         h0 = self.policy_fc.weight.new(1, hidden_size).zero_()\n         c0 = self.policy_fc.weight.new(1, hidden_size).zero_()\n         return (h0, c0)\n \n     def __init__(self, obs_space, action_space, num_outputs, config, name):\n         TorchModelV2.__init__(\n             self, obs_space, action_space, num_outputs, config, name\n         )\n         nn.Module.__init__(self)\n         # Value function\n         self._value_branch = nn.Sequential(\n             nn.Linear(1, 4),\n             nn.LeakyReLU(),\n             nn.Linear(4, 1)\n         )\n         # Policy\n         self.policy_fc = nn.Linear(64, 2)\n         self.lstm = nn.LSTM(1, 64, batch_first=True)        \n         self._cur_value = None\n \n     @override(TorchModelV2)\n     def get_initial_state(self):\n         # make hidden states on same device as model\n         lstm_h, lstm_c = self.init_hidden(64)\n     \n         initial_state = [\n             lstm_h,\n             lstm_c,\n         ]\n         return initial_state\n \n     @override(TorchModelV2)\n     def value_function(self):\n         assert self._cur_value is not None, \"must call forward() first\"\n         return self._cur_value\n \n     def forward(self, input_dict, hidden_state, seq_lens):\n         obs = input_dict['obs_flat']\n         batch_size, features = obs.size()\n \n         # Unpack the hidden_state\n         lstm_h = hidden_state[0]\n         # THIS IS NOT A TENSOR\n         print(type(lstm_h))\n         lstm_c = hidden_state[1]\n \n         # Build the tuples\n         lstm_hidden = (lstm_h.reshape(-1, 1, 64), lstm_c.reshape(-1, 1, 64))\n \n         out, lstm_hidden = self.lstm(\n             obs.view(-1, 1, 1), lstm_hidden\n         )\n         new_hidden_state = [\n             lstm_hidden[0],\n             lstm_hidden[1],\n         ]\n         print(new_hidden_state)\n         # Value function\n         self._cur_value = self._value_branch(obs)\n         self.logits = self.policy_fc(out)\n         return logits, new_hidden_state\n \n \n \n \n ModelCatalog.register_custom_model(\"test\", TestNet)\n register_env(\"test\", lambda config: SimpleCorridor(config))\n \n ray.init()\n tune.run(\n     ppo.PPOTrainer,\n     config={\n         \"env\": \"test\",\n         \"use_pytorch\": True,\n         \"env_config\": {\n             \"corridor_length\": 10\n         },\n         \"model\": {\n             \"custom_model\": \"test\",\n             \"custom_options\": {},\n         },\n     },\n )\n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "justinglibert", "commentT": "2020-02-13T23:57:55Z", "comment_text": "\n \t\tHm so I think it's expected it will be numpy during inference (since we never backprop during that). However during learning, it should be a Tensor.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "justinglibert", "commentT": "2020-02-14T00:06:59Z", "comment_text": "\n \t\tTry out <denchmark-link:https://github.com/ray-project/ray/pull/7162>#7162</denchmark-link>\n  ? I couldn't get your example to run completely but it seems to have gotten further (had to also add ).\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "justinglibert", "commentT": "2020-02-14T00:49:58Z", "comment_text": "\n \t\tThanks for the fast reply!\n \"I think it's expected it will be numpy during inference (since we never backprop during that). However during learning, it should be a Tensor.\"\n Do you mean that it is incorrectly doing some eval? Or is the default behaviour of the trainer to first eval before doing any training?\n I'll try the PR tomorrow!\n Also: If the hidden state is a numpy array during eval, what's the recommended way of making sure it's always a Tensor? Should I check the type of the variable and cast it to a Tensor when it's not (during eval I guess)?\n \t\t"}}}, "commit": {"commit_id": "42aea966ff376fff015cabdcf5d059adcc967ad3", "commit_author": "Eric Liang", "commitT": "2020-02-17 10:26:58-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "rllib\\agents\\trainer.py", "file_new_name": "rllib\\agents\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "544,545,546,547,548,549,550", "deleted_lines": "544", "method_info": {"method_name": "_setup.normalize", "method_params": "env", "method_startline": "544", "method_endline": "550"}}, "hunk_1": {"Ismethod": 1, "added_lines": "543,544,545,546,547,548,549,550,551,552", "deleted_lines": "543,544", "method_info": {"method_name": "_setup", "method_params": "self,config", "method_startline": "521", "method_endline": "588"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "rllib\\policy\\torch_policy.py", "file_new_name": "rllib\\policy\\torch_policy.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "217", "deleted_lines": "215,216,217,218", "method_info": {"method_name": "_lazy_tensor_dict", "method_params": "self,postprocessed_batch", "method_startline": "215", "method_endline": "218"}}, "hunk_1": {"Ismethod": 1, "added_lines": "220,221,222,223,224,225,226", "deleted_lines": "220,221,222,223,224", "method_info": {"method_name": "_convert_to_tensor", "method_params": "self,arr", "method_startline": "220", "method_endline": "226"}}, "hunk_2": {"Ismethod": 1, "added_lines": "217,220,221,222", "deleted_lines": "216,217,218,219,220,221,222", "method_info": {"method_name": "_lazy_tensor_dict.convert", "method_params": "arr", "method_startline": "216", "method_endline": "222"}}}}}}}