{"BR": {"BR_id": "12531", "BR_author": "ZumbiAzul", "BRopenT": "2020-12-01T12:58:36Z", "BRcloseT": "2020-12-18T09:31:41Z", "BR_text": {"BRsummary": "[tune] Fails to reproduce a basic tune tutorial example", "BRdescription": "\n Hello, I am trying out RayTune for the first time, and currently I am going through the  <denchmark-link:https://docs.ray.io/en/latest/tune/tutorials/tune-tutorial.html#tune-tutorial>(basic Tune tutorial webpage)</denchmark-link>\n \n The code I am trying to run is the following:\n <denchmark-code>import numpy as np\n import torch\n import torch.optim as optim\n import torch.nn as nn\n from torchvision import datasets, transforms\n from torch.utils.data import DataLoader\n import torch.nn.functional as F\n \n from ray import tune\n from ray.tune.suggest.hyperopt import HyperOptSearch\n from ray.tune.schedulers import ASHAScheduler\n \n from hyperopt import hp\n \n import os\n \n #%% ConvNet model\n class ConvNet(nn.Module):\n     def __init__(self):\n         super(ConvNet, self).__init__()\n         # In this example, we don't change the model architecture\n         # due to simplicity.\n         self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n         self.fc = nn.Linear(192, 10)\n \n     def forward(self, x):\n         x = F.relu(F.max_pool2d(self.conv1(x), 3))\n         x = x.view(-1, 192)\n         x = self.fc(x)\n         return F.log_softmax(x, dim=1)\n     \n #%% Train and test functions\n EPOCH_SIZE = 512\n TEST_SIZE = 256\n \n def train(model, optimizer, train_loader):\n     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n     model.train()\n     for batch_idx, (data, target) in enumerate(train_loader):\n         # We set this just for the example to run quickly.\n         if batch_idx * len(data) > EPOCH_SIZE:\n             return\n         data, target = data.to(device), target.to(device)\n         optimizer.zero_grad()\n         output = model(data)\n         loss = F.nll_loss(output, target)\n         loss.backward()\n         optimizer.step()\n \n \n def test(model, data_loader):\n     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n     model.eval()\n     correct = 0\n     total = 0\n     with torch.no_grad():\n         for batch_idx, (data, target) in enumerate(data_loader):\n             # We set this just for the example to run quickly.\n             if batch_idx * len(data) > TEST_SIZE:\n                 break\n             data, target = data.to(device), target.to(device)\n             outputs = model(data)\n             _, predicted = torch.max(outputs.data, 1)\n             total += target.size(0)\n             correct += (predicted == target).sum().item()\n \n     return correct / total\n \n #%% Train MNIST function\n \n def train_mnist(config):\n     # Data Setup\n     mnist_transforms = transforms.Compose(\n         [transforms.ToTensor(),\n          transforms.Normalize((0.1307, ), (0.3081, ))])\n \n     train_loader = DataLoader(\n         datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n         batch_size=64,\n         shuffle=True)\n     test_loader = DataLoader(\n         datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n         batch_size=64,\n         shuffle=True)\n \n     model = ConvNet()\n     optimizer = optim.SGD(\n         model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n     for i in range(10):\n         train(model, optimizer, train_loader)\n         acc = test(model, test_loader)\n \n         # Send the current training result back to Tune\n         tune.report(mean_accuracy=acc)\n \n         if i % 5 == 0:\n             # This saves the model to the trial directory\n             torch.save(model.state_dict(), \"./model.pth\")\n \n #%% Defining a search space\n \n # Let\u2019s run 1 trial by calling tune.run and randomly sample from a uniform distribution for learning rate and momentum.\n search_space = {\n     \"lr\": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n     \"momentum\": tune.uniform(0.1, 0.9)\n }\n \n # Uncomment this to enable distributed execution\n # `ray.init(address=\"auto\")`\n \n # Download the dataset first\n datasets.MNIST(\"~/data\", train=True, download=True)\n \n analysis = tune.run(train_mnist, config=search_space, resources_per_trial={'gpu': 1})\n \n # tune.run returns an Analysis object. You can use this to plot the performance of this trial.\n dfs = analysis.trial_dataframes\n [d.mean_accuracy.plot() for d in dfs.values()]\n \n #%% Evaluate your model\n \n df = analysis.results_df\n logdir = analysis.get_best_logdir(\"mean_accuracy\", mode=\"max\")\n state_dict = torch.load(os.path.join(logdir, \"model.pth\"))\n \n model = ConvNet()\n model.load_state_dict(state_dict)\n </denchmark-code>\n \n The error message is the following:\n <denchmark-code><IPython.core.display.HTML object>\n 2020-12-01 13:52:06,559\tERROR trial_runner.py:793 -- Trial train_mnist_0303d_00000: Error processing event.\n Traceback (most recent call last):\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 726, in _process_trial\n     result = self.trial_executor.fetch_result(trial)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 489, in fetch_result\n     result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\worker.py\", line 1452, in get\n     raise value.as_instanceof_cause()\n ray.exceptions.RayTaskError(TuneError): ray::ImplicitFunc.train() (pid=16872, ip=130.235.28.9)\n   File \"python\\ray\\_raylet.pyx\", line 482, in ray._raylet.execute_task\n   File \"python\\ray\\_raylet.pyx\", line 436, in ray._raylet.execute_task.function_executor\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\function_manager.py\", line 553, in actor_method_executor\n     return method(actor, *args, **kwargs)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\trainable.py\", line 336, in train\n     result = self.step()\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\function_runner.py\", line 366, in step\n     self._report_thread_runner_error(block=True)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\function_runner.py\", line 512, in _report_thread_runner_error\n     raise TuneError((\"Trial raised an exception. Traceback:\\n{}\"\n ray.tune.error.TuneError: Trial raised an exception. Traceback:\n ray::ImplicitFunc.train() (pid=16872, ip=130.235.28.9)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\function_runner.py\", line 248, in run\n     self._entrypoint()\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\function_runner.py\", line 315, in entrypoint\n     return self._trainable_func(self.config, self._status_reporter,\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\function_runner.py\", line 575, in _trainable_func\n     output = fn()\n   File \"D:\\Jupiter_playground\\raytune_playground.py\", line 90, in train_mnist\n     train(model, optimizer, train_loader)\n   File \"D:\\Jupiter_playground\\raytune_playground.py\", line 45, in train\n     output = model(data)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n     result = self.forward(*input, **kwargs)\n   File \"D:\\Jupiter_playground\\raytune_playground.py\", line 27, in forward\n     x = F.relu(F.max_pool2d(self.conv1(x), 3))\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n     result = self.forward(*input, **kwargs)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 419, in forward\n     return self._conv_forward(input, self.weight)\n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 415, in _conv_forward\n     return F.conv2d(input, weight, self.bias, self.stride,\n RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n <IPython.core.display.HTML object>\n Traceback (most recent call last):\n \n   File \"D:\\Jupiter_playground\\raytune_playground.py\", line 114, in <module>\n     analysis = tune.run(train_mnist, config=search_space, resources_per_trial={'gpu': 1})\n \n   File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\ray\\tune\\tune.py\", line 434, in run\n     raise TuneError(\"Trials did not complete\", incomplete_trials)\n \n TuneError: ('Trials did not complete', [train_mnist_0303d_00000])\n </denchmark-code>\n \n I made sure that all necessary packages are installed into the Conda environment. Could someone help me figure out how to resolve the issue?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ZumbiAzul", "commentT": "2020-12-13T09:43:41Z", "comment_text": "\n \t\tLooks like you need to call model.cuda()\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ZumbiAzul", "commentT": "2020-12-13T09:44:00Z", "comment_text": "\n \t\tOr rather; model.to(device)\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ZumbiAzul", "commentT": "2020-12-14T15:38:21Z", "comment_text": "\n \t\tThanks, <denchmark-link:https://github.com/richardliaw>@richardliaw</denchmark-link>\n  That was indeed the solution. Could you please tell, how you figured it out, because I am not sure how obvious it is mentioned in the error?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ZumbiAzul", "commentT": "2020-12-16T10:39:44Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/ZumbiAzul>@ZumbiAzul</denchmark-link>\n , this can be seen here:\n <denchmark-code>RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n </denchmark-code>\n \n which indicates that the input is a cuda tensor and the model is not (otherwise it would also be a torch.cuda.FloatTensor). By moving the model to the GPU the model weights will become torch.cuda.FloatTensor, too.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ZumbiAzul", "commentT": "2020-12-16T10:56:07Z", "comment_text": "\n \t\tWe're updating the docs in <denchmark-link:https://github.com/ray-project/ray/pull/12914>#12914</denchmark-link>\n  so this doesn't happen in the future. Thanks for pointing this issue out to us!\n \t\t"}}}, "commit": {"commit_id": "426f8a8d15449c5e78efc4604e2b0fd4a8577fb8", "commit_author": "Kai Fricke", "commitT": "2020-12-18 01:31:40-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\tune\\tests\\tutorial.py", "file_new_name": "python\\ray\\tune\\tests\\tutorial.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "96,97,99,100", "deleted_lines": null, "method_info": {"method_name": "train_mnist", "method_params": "config", "method_startline": "81", "method_endline": "112"}}}}}}}