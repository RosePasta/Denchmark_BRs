{"BR": {"BR_id": "8838", "BR_author": "hartikainen", "BRopenT": "2020-06-08T17:25:39Z", "BRcloseT": "2020-06-30T07:55:10Z", "BR_text": {"BRsummary": "[tune] gpus unused in local mode (`CUDA_VISIBLE_DEVICES == ''`)", "BRdescription": "\n <denchmark-h:h3>What is the problem?</denchmark-h>\n \n When running Tune in local mode with ray==0.8.5, the CUDA_VISIBLE_DEVICES are always set to empty string, thus making the gpus unused. This behavior is different from 0.8.4, in which the CUDA_VISIBLE_DEVICES are the same both with local_mode=True and local_mode=False. Same thing happens with the latest wheels.\n Ray version and other system information (Python version, TensorFlow version, OS):\n <denchmark-code>$ pip freeze | grep \"tensorflow\\|tf\\|ray\"\n ray==0.8.5\n tensorflow==2.1.0\n tensorflow-estimator==2.1.0\n tensorflow-probability==0.9.0\n </denchmark-code>\n \n <denchmark-h:h3>Reproduction (REQUIRED)</denchmark-h>\n \n import os\n \n import ray\n from ray import tune\n import sys\n \n expected = os.getenv('CUDA_VISIBLE_DEVICES')\n \n \n class DebugRunner(tune.Trainable):\n     def _setup(self, config):\n         pass\n \n     def _train(self):\n         import os\n         import tensorflow as tf\n         cuda_visible_devices = os.getenv('CUDA_VISIBLE_DEVICES')\n         gpus = tf.config.experimental.list_physical_devices('GPU')\n         print(gpus)\n         assert cuda_visible_devices == expected, (\n             expected, cuda_visible_devices)\n         return {\n             'done': True,\n             'cuda_visible_devices': cuda_visible_devices,\n             'gpus': gpus,\n         }\n \n \n ray.init(local_mode=bool(int(sys.argv[1])))\n tune.run(DebugRunner, resources_per_trial={'gpu': 1})\n Here's what the outputs look like for different ray version and command-line args passed to the snippet (Some lines omitted for clarity):\n <denchmark-h:h2>With ray==0.8.5, local_mode=False</denchmark-h>\n \n <denchmark-code>$ CUDA_VISIBLE_DEVICES=1 python ./scripts/ray_cuda_issue.py 0  # 0.8.5\n ...\n == Status ==\n Memory usage on this node: 307.0/503.8 GiB\n Using FIFO scheduling algorithm.\n Resources requested: 1/40 CPUs, 1/1 GPUs, 0.0/130.32 GiB heap, 0.0/41.26 GiB objects\n Result logdir: /users/krinen/ray_results/DebugRunner\n Number of trials: 1 (1 RUNNING)\n +-------------------+----------+-------+\n | Trial name        | status   | loc   |\n |-------------------+----------+-------|\n | DebugRunner_00000 | RUNNING  |       |\n +-------------------+----------+-------+\n \n ...\n (pid=11144) 2020-06-08 18:11:23.425322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\n (pid=11144) pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n (pid=11144) coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n ...\n (pid=11144) [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n ...\n (pid=11144) 2020-06-08 18:11:23.447472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n ...\n Result for DebugRunner_00000:\n   cuda_visible_devices: '1'\n   date: 2020-06-08_18-11-23\n   done: true\n   experiment_id: 710f0e270d954de9b430b6ff6866bdfa\n   experiment_tag: '0'\n   gpus:\n   - - /physical_device:GPU:0\n     - GPU\n   hostname: gandalf\n   iterations_since_restore: 1\n   node_ip: 163.1.88.121\n   pid: 11144\n   time_since_restore: 5.4869067668914795\n   time_this_iter_s: 5.4869067668914795\n   time_total_s: 5.4869067668914795\n   timestamp: 1591636283\n   timesteps_since_restore: 0\n   training_iteration: 1\n   trial_id: '00000'\n \n == Status ==\n Memory usage on this node: 307.9/503.8 GiB\n Using FIFO scheduling algorithm.\n Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/130.32 GiB heap, 0.0/41.26 GiB objects\n Result logdir: /users/krinen/ray_results/DebugRunner\n Number of trials: 1 (1 TERMINATED)\n +-------------------+------------+-------+--------+------------------+\n | Trial name        | status     | loc   |   iter |   total time (s) |\n |-------------------+------------+-------+--------+------------------|\n | DebugRunner_00000 | TERMINATED |       |      1 |          5.48691 |\n +-------------------+------------+-------+--------+------------------+\n </denchmark-code>\n \n <denchmark-h:h2>With ray==0.8.5, local_mode=True:</denchmark-h>\n \n <denchmark-code>$ CUDA_VISIBLE_DEVICES=1 python ./scripts/ray_cuda_issue.py 1  # 0.8.5\n ...\n 2020-06-08 18:11:57.675648: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n ...\n []\n \n ...\n 2020-06-08 18:11:57,694 ERROR trial_runner.py:519 -- Trial DebugRunner_00000: Error processing event.\n Traceback (most recent call last):\n   File \"/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 467, in _process_trial\n     result = self.trial_executor.fetch_result(trial)\n   File \"/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 431, in fetch_result\n     result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n   File \"/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/worker.py\", line 1515, in get\n     raise value.as_instanceof_cause()\n ray.exceptions.RayTaskError(AssertionError): ray::DebugRunner.train() (pid=11658, ip=163.1.88.121)\n   File \"python/ray/_raylet.pyx\", line 463, in ray._raylet.execute_task\n   File \"python/ray/_raylet.pyx\", line 417, in ray._raylet.execute_task.function_executor\n   File \"/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\n     result = self._train()\n   File \"./scripts/ray_cuda_issue.py\", line 21, in _train\n AssertionError: ('1', '')\n == Status ==\n Memory usage on this node: 307.8/503.8 GiB\n Using FIFO scheduling algorithm.\n Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/130.32 GiB heap, 0.0/41.26 GiB objects\n Result logdir: /users/krinen/ray_results/DebugRunner\n Number of trials: 1 (1 ERROR)\n +-------------------+----------+-------+\n | Trial name        | status   | loc   |\n |-------------------+----------+-------|\n | DebugRunner_00000 | ERROR    |       |\n +-------------------+----------+-------+\n Number of errored trials: 1\n +-------------------+--------------+-------------------------------------------------------------------------------------------+\n | Trial name        |   # failures | error file                                                                                |\n |-------------------+--------------+-------------------------------------------------------------------------------------------|\n | DebugRunner_00000 |            1 | /users/krinen/ray_results/DebugRunner/DebugRunner_0_2020-06-08_18-11-54uby7h2dg/error.txt |\n +-------------------+--------------+-------------------------------------------------------------------------------------------+\n \n Traceback (most recent call last):\n   File \"./scripts/ray_cuda_issue.py\", line 30, in <module>\n     tune.run(DebugRunner, resources_per_trial={'gpu': 1})\n   File \"/users/krinen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/ray/tune/tune.py\", line 347, in run\n     raise TuneError(\"Trials did not complete\", incomplete_trials)\n ray.tune.error.TuneError: ('Trials did not complete', [DebugRunner_00000])\n </denchmark-code>\n \n <denchmark-h:h2>With ray==0.8.4, local_mode=True:</denchmark-h>\n \n <denchmark-code>$ CUDA_VISIBLE_DEVICES=1 python ./scripts/ray_cuda_issue.py 1  # 0.8.4\n ...\n pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\n ...\n 2020-06-08 18:12:53.499990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n == Status ==\n Memory usage on this node: 306.3/503.8 GiB\n Using FIFO scheduling algorithm.\n Resources requested: 1/40 CPUs, 1/1 GPUs, 0.0/130.27 GiB heap, 0.0/41.26 GiB objects\n Result logdir: /users/krinen/ray_results/DebugRunner\n Number of trials: 1 (1 RUNNING)\n +-------------------+----------+-------+\n | Trial name        | status   | loc   |\n |-------------------+----------+-------|\n | DebugRunner_00000 | RUNNING  |       |\n +-------------------+----------+-------+\n \n ...\n Result for DebugRunner_00000:\n   cuda_visible_devices: '1'\n   date: 2020-06-08_18-12-53\n   done: true\n   experiment_id: 119a71b9fb1940ef96834e7ec858d336\n   experiment_tag: '0'\n   gpus:\n   - - /physical_device:GPU:0\n     - GPU\n   hostname: gandalf\n   iterations_since_restore: 1\n   node_ip: 163.1.88.121\n   pid: 12268\n   time_since_restore: 4.360386848449707\n   time_this_iter_s: 4.360386848449707\n   time_total_s: 4.360386848449707\n   timestamp: 1591636373\n   timesteps_since_restore: 0\n   training_iteration: 1\n   trial_id: '00000'\n \n == Status ==\n Memory usage on this node: 306.3/503.8 GiB\n Using FIFO scheduling algorithm.\n Resources requested: 0/40 CPUs, 0/1 GPUs, 0.0/130.22 GiB heap, 0.0/41.26 GiB objects\n Result logdir: /users/krinen/ray_results/DebugRunner\n Number of trials: 1 (1 TERMINATED)\n +-------------------+------------+-------+--------+------------------+\n | Trial name        | status     | loc   |   iter |   total time (s) |\n |-------------------+------------+-------+--------+------------------|\n | DebugRunner_00000 | TERMINATED |       |      1 |          4.36039 |\n +-------------------+------------+-------+--------+------------------+\n </denchmark-code>\n \n \n  I have verified my script runs in a clean environment and reproduces the issue.\n  I have verified the issue also occurs with the latest wheels.\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hartikainen", "commentT": "2020-06-08T17:30:27Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/ijrsvt>@ijrsvt</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "fb074da7c374a69f04a1add9e62b96ec471d32c3", "commit_author": "Ian Rodney", "commitT": "2020-06-30 00:55:09-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "python\\ray\\tests\\test_advanced_2.py", "file_new_name": "python\\ray\\tests\\test_advanced_2.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "675,676,677,678,679", "deleted_lines": null, "method_info": {"method_name": "test_local_mode_gpus.f", "method_params": "", "method_startline": "675", "method_endline": "679"}}, "hunk_1": {"Ismethod": 1, "added_lines": "664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681", "deleted_lines": null, "method_info": {"method_name": "test_local_mode_gpus", "method_params": "save_gpu_ids_shutdown_only", "method_startline": "664", "method_endline": "681"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "python\\ray\\worker.py", "file_new_name": "python\\ray\\worker.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "398,399,400,401", "deleted_lines": null, "method_info": {"method_name": "get_gpu_ids", "method_params": "", "method_startline": "374", "method_endline": "403"}}}}}}}