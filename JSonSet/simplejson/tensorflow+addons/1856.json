{"BR": {"BR_id": "1856", "BR_author": "w4nderlust", "BRopenT": "2020-05-17T21:51:10Z", "BRcloseT": "2020-05-20T03:22:39Z", "BR_text": {"BRsummary": "AttentionWrapper with non LSTM cells returns ValueError", "BRdescription": "\n System information\n \n OS Platform and Distribution: Ubuntu 20.04\n TensorFlow version and how it was installed (source or binary): 2.2.0\n TensorFlow-Addons version and how it was installed (source or binary): 0.10.0\n Python version: 3.7.7\n Is GPU used? (yes/no): yes\n \n Describe the bug\n Working with <denchmark-link:https://github.com/jimthompson5802>@jimthompson5802</denchmark-link>\n  on this PR <denchmark-link:https://github.com/ludwig-ai/ludwig/pull/699>ludwig-ai/ludwig#699</denchmark-link>\n  we discovered an issue with seq2seq attention.\n In seq2seq decoding with attention, one has to wrap the recurrent cell with an AttentionWrapper.\n Right now if you do so with any cell type other than LSTMCell (so with SimpleRNNCell and GRUCell), you get a:\n <denchmark-code>ValueError: The two structures don't have the same nested structure.\n \n First structure: type=AttentionWrapperState str=AttentionWrapperState(cell_state=<tf.Tensor: shape=(128, 256), dtype=float32, dtype=float32)>, attention=<tf.Tensor: shape=(128, 256), dtype=float32, dtype=float32)>, alignments=<tf.Tensor: shape=(128, 256), dtype=float32, dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: shape=(128, 256), dtype=float32, dtype=float32)>)\n \n Second structure: type=AttentionWrapperState str=AttentionWrapperState(cell_state=[<tf.Tensor: shape=(128, 256), dtype=float32, dtype=float32)>], attention=<tf.Tensor: shape=(128, 256), dtype=float32, dtype=float32)>, alignments=<tf.Tensor: shape=(128, 128), dtype=float32, dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: shape=(128, 128), dtype=float32, dtype=float32)>)\n \n More specifically: Substructure \"type=list str=[<tf.Tensor: shape=(128, 256), dtype=float32, dtype=float32)>]\" is a sequence, while substructure \"type=EagerTensor str=tf.Tensor(\n [[0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  ...\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]\n  [0. 0. 0. ... 0. 0. 0.]], shape=(128, 256), dtype=float32)\" is not\n Entire first structure:\n AttentionWrapperState(cell_state=., attention=., alignments=., alignment_history=(), attention_state=.)\n Entire second structure:\n AttentionWrapperState(cell_state=[.], attention=., alignments=., alignment_history=(), attention_state=.)\n </denchmark-code>\n \n This suggests that the AttentionWrapper works well with LSTMCell because it returns a state that's a list of two tensors, but it doesn't work with SimpleRNN and GRU because the state they return is just a Tensor, not a list of length 1 containing one tensor.\n I believe there are 2 solutions: one is making all cells return states as a list of tensors. This will be good and it will guarantee homogeneity. The other option is to correct the AttentionWrapper to work both with cells that retunr lists of tensors as states like LSTM and cells that return just a tensor as state like SimpleRNN and GRU.\n For more context about the use of this in the original PR, plese refer at the comments starting from  <denchmark-link:https://github.com/uber/ludwig/pull/699#issuecomment-629736171>this one</denchmark-link>\n .\n Code to reproduce the issue\n Here is a script to reproduce the issue.\n <denchmark-code>import numpy as np\n import tensorflow as tf\n from tensorflow import keras\n import tensorflow_addons as tfa\n \n # ValueError exception raised for both SimpleRNNCell and GRUCell\n \n RNN_CELL_CLASS = keras.layers.SimpleRNNCell  # Raises ValueError Exception\n # RNN_CELL_CLASS = keras.layers.GRUCell      # Raises ValueError Exception\n # RNN_CELL_CLASS = keras.layers.LSTMCell     # Does not raise an error\n \n VOCAB_SIZE = 100\n EMBED_SIZE = 10\n RNN_UNITS = 256\n INPUT_SEQUENCE_SIZE = 10\n OUTPUT_SEQUENCE_SIZE = 15\n BATCH_SIZE = 128\n \n targets = tf.convert_to_tensor(\n     np.random.randint(VOCAB_SIZE, size=OUTPUT_SEQUENCE_SIZE * BATCH_SIZE)\n     .reshape(BATCH_SIZE, OUTPUT_SEQUENCE_SIZE),\n     dtype=tf.float32\n )\n encoder_outputs = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)\n if RNN_CELL_CLASS._keras_api_names[0] == 'keras.layers.LSTMCell':\n     encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32),\n                      tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]\n else:\n     encoder_state = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)\n     # note this returns error also if:\n     # encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]\n \n embeddings_dec = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE)\n decoder_cell = RNN_CELL_CLASS(RNN_UNITS)\n output_layer = keras.layers.Dense(VOCAB_SIZE)\n \n sampler = tfa.seq2seq.sampler.TrainingSampler()\n attention_mechanism = tfa.seq2seq.LuongAttention(units=RNN_UNITS)\n #attention_mechanism = tfa.seq2seq.BahdanauAttention(units=RNN_UNITS)  # gives same error\n decoder_cell = tfa.seq2seq.AttentionWrapper(\n     decoder_cell,\n     attention_mechanism,\n     attention_layer_size=RNN_UNITS\n )\n \n decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n     decoder_cell,\n     sampler,\n     output_layer=output_layer\n )\n \n # setup for Attention\n attention_mechanism.setup_memory(\n     encoder_outputs,\n     memory_sequence_length=tf.ones(\n         [BATCH_SIZE], dtype=tf.int32) * INPUT_SEQUENCE_SIZE\n )\n \n decoder_initial_state = decoder_cell.get_initial_state(\n     batch_size=BATCH_SIZE,\n     dtype=tf.float32\n )\n decoder_initial_state = decoder_initial_state.clone(\n     cell_state=encoder_state\n )\n \n decoder_emb_inp = embeddings_dec(targets)\n \n # Following statement generates exception\n # ValueError: The two structures don't have the same nested structure\n # for SimpleRNNCell and GRUCell\n (\n     final_outputs,\n     final_state,\n     final_sequence_lengths\n ) = decoder(\n     decoder_emb_inp,\n     initial_state=decoder_initial_state,\n     sequence_length=tf.ones([BATCH_SIZE],\n                             dtype=tf.int32) * OUTPUT_SEQUENCE_SIZE\n )\n \n </denchmark-code>\n \n Note that changing the value of RNN_CELL changes the outcome:\n <denchmark-code>RNN_CELL_CLASS = keras.layers.SimpleRNNCell  # Raises ValueError Exception\n # RNN_CELL_CLASS = keras.layers.GRUCell      # Raises ValueError Exception\n # RNN_CELL_CLASS = keras.layers.LSTMCell     # Does not raise an error\n </denchmark-code>\n \n Moreover, assigning `encoder_state both as a tensor or as a list does not work:\n <denchmark-code>encoder_state = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)\n # encoder_state = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)\n </denchmark-code>\n \n Finally, also note that this is independnent of the type of attention used, the same error is return for both LuongAttention and BahdanauAttention.\n Also here's a version of the same script where attention is not used, it works for all different kinds of cells:\n <denchmark-code>import numpy as np\n import tensorflow as tf\n from tensorflow import keras\n import tensorflow_addons as tfa\n \n RNN_CELL_CLASS = keras.layers.SimpleRNNCell    # Does not raise an error\n # RNN_CELL_CLASS = keras.layers.GRUCell        # Does not raise an error\n # RNN_CELL_CLASS = keras.layers.LSTMCell       # Does not raise an error\n \n VOCAB_SIZE = 100\n EMBED_SIZE = 10\n RNN_UNITS = 256\n INPUT_SEQUENCE_SIZE = 10\n OUTPUT_SEQUENCE_SIZE = 15\n BATCH_SIZE = 128\n \n targets = tf.convert_to_tensor(\n     np.random.randint(VOCAB_SIZE, size=OUTPUT_SEQUENCE_SIZE * BATCH_SIZE)\n     .reshape(BATCH_SIZE, OUTPUT_SEQUENCE_SIZE),\n     dtype=tf.float32\n )\n encoder_outputs = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)\n if RNN_CELL_CLASS._keras_api_names[0] == 'keras.layers.LSTMCell':\n     encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32),\n                      tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]\n else:\n     # encoder_state = tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)\n     encoder_state = [tf.zeros([BATCH_SIZE, RNN_UNITS], dtype=tf.float32)]\n \n embeddings_dec = keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE)\n decoder_cell = RNN_CELL_CLASS(RNN_UNITS)\n output_layer = keras.layers.Dense(VOCAB_SIZE)\n \n sampler = tfa.seq2seq.sampler.TrainingSampler()\n \n decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n     decoder_cell,\n     sampler,\n     output_layer=output_layer\n )\n \n decoder_initial_state = encoder_state\n \n decoder_emb_inp = embeddings_dec(targets)\n \n (\n     final_outputs,\n     final_state,\n     final_sequence_lengths\n ) = decoder(\n     decoder_emb_inp,\n     initial_state=decoder_initial_state,\n     sequence_length=tf.ones([BATCH_SIZE],\n                             dtype=tf.int32) * OUTPUT_SEQUENCE_SIZE\n )\n \n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "w4nderlust", "commentT": "2020-05-19T08:40:29Z", "comment_text": "\n \t\t\n I believe there are 2 solutions: one is making all cells return states as a list of tensors. This will be good and it will guarantee homogeneity. The other option is to correct the AttentionWrapper to work both with cells that retunr lists of tensors as states like LSTM and cells that return just a tensor as state like SimpleRNN and GRU.\n \n I believe TensorFlow should be more consistent here and always return the same state structure. But to unblock the situation I added a workaround in the PR referenced above: we ensure that the cell state structure always matches the initial structure (as returned by cell.get_initial_state).\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "w4nderlust", "commentT": "2020-05-19T17:23:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/guillaumekln>@guillaumekln</denchmark-link>\n  thanks, I agree with you that the interfaces should all be the same. Your PR solves the problem though, without impacting the main TF code, which makes it a good candidate for a solution, thank you very much.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "w4nderlust", "commentT": "2020-05-20T04:54:45Z", "comment_text": "\n \t\tThanks to everyone involved in this fix\n \t\t"}}}, "commit": {"commit_id": "f3d0416fbe3f4c607f4c1ebc2b7554a941ff02a2", "commit_author": "Guillaume Klein", "commitT": "2020-05-19 23:22:38-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "file_new_name": "tensorflow_addons\\seq2seq\\attention_wrapper.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "2049,2050,2051", "deleted_lines": null, "method_info": {"method_name": "call", "method_params": "self,inputs,state,kwargs", "method_startline": "2001", "method_endline": "2112"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_addons\\seq2seq\\tests\\attention_wrapper_test.py", "file_new_name": "tensorflow_addons\\seq2seq\\tests\\attention_wrapper_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "925,926,927,928,929,930,931,932,933,934", "deleted_lines": null, "method_info": {"method_name": "test_attention_wrapper_with_gru_cell", "method_params": "", "method_startline": "925", "method_endline": "934"}}}}}}}