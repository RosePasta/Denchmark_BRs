{"BR": {"BR_id": "2664", "BR_author": "hkvision", "BRopenT": "2020-08-03T08:37:32Z", "BRcloseT": "2020-09-08T05:29:59Z", "BR_text": {"BRsummary": "py4j.protocol.Py4JJavaError: An error occurred while calling o61.csv. : java.lang.IllegalArgumentException: Illegal pattern component: XXX", "BRdescription": "\n <denchmark-link:https://github.com/intel-analytics/analytics-zoo/pull/2628>#2628</denchmark-link>\n \n This PR breaks spark.read.csv/json, with the following error:\n <denchmark-code>py4j.protocol.Py4JJavaError: An error occurred while calling o61.csv.\n : java.lang.IllegalArgumentException: Illegal pattern component: XXX\n \tat org.apache.commons.lang3.time.FastDatePrinter.parsePattern(FastDatePrinter.java:282)\n \tat org.apache.commons.lang3.time.FastDatePrinter.init(FastDatePrinter.java:149)\n \tat org.apache.commons.lang3.time.FastDatePrinter.<init>(FastDatePrinter.java:142)\n \tat org.apache.commons.lang3.time.FastDateFormat.<init>(FastDateFormat.java:384)\n \tat org.apache.commons.lang3.time.FastDateFormat.<init>(FastDateFormat.java:369)\n \tat org.apache.commons.lang3.time.FastDateFormat$1.createInstance(FastDateFormat.java:91)\n \tat org.apache.commons.lang3.time.FastDateFormat$1.createInstance(FastDateFormat.java:88)\n \tat org.apache.commons.lang3.time.FormatCache.getInstance(FormatCache.java:82)\n \tat org.apache.commons.lang3.time.FastDateFormat.getInstance(FastDateFormat.java:165)\n \tat org.apache.spark.sql.execution.datasources.csv.CSVOptions.<init>(CSVOptions.scala:139)\n \tat org.apache.spark.sql.execution.datasources.csv.CSVOptions.<init>(CSVOptions.scala:41)\n \tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:58)\n \tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n \tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n \tat scala.Option.orElse(Option.scala:289)\n \tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n \tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n \tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n \tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n \tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:615)\n \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n \tat java.lang.reflect.Method.invoke(Method.java:498)\n \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n \tat py4j.Gateway.invoke(Gateway.java:282)\n \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n \tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n \tat java.lang.Thread.run(Thread.java:748)\n </denchmark-code>\n \n Either revert this PR if the conflict can be resolved or refer to here: <denchmark-link:https://stackoverflow.com/questions/46429616/spark-2-2-illegal-pattern-component-xxx-java-lang-illegalargumentexception-ill>https://stackoverflow.com/questions/46429616/spark-2-2-illegal-pattern-component-xxx-java-lang-illegalargumentexception-ill</denchmark-link>\n \n Error log: <denchmark-link:http://10.239.47.210:18888/job/ZOO-NB-Pip-ExampleTests-py36/84/console>http://10.239.47.210:18888/job/ZOO-NB-Pip-ExampleTests-py36/84/console</denchmark-link>\n \n Related issue: <denchmark-link:https://github.com/intel-analytics/analytics-zoo/issues/2522>#2522</denchmark-link>\n \n cc <denchmark-link:https://github.com/jenniew>@jenniew</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hkvision", "commentT": "2020-08-03T08:38:16Z", "comment_text": "\n \t\tAlso, for Spark backend, if we are running on a hadoop cluster, we still need to add  for local file prefixes? <denchmark-link:https://github.com/jenniew>@jenniew</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hkvision", "commentT": "2020-08-03T17:14:13Z", "comment_text": "\n \t\tNo need to add file:// in yarn.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hkvision", "commentT": "2020-08-04T01:36:16Z", "comment_text": "\n \t\tBut it raises error:\n <denchmark-code>>>> data_shard = zoo.orca.data.pandas.read_csv(\"/opt/work/client/kai/nyc_taxi.csv\")\n \n Traceback (most recent call last):\n   File \"<stdin>\", line 1, in <module>\n   File \"/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/zoo/orca/data/pandas/preprocessing.py\", line 33, in read_csv\n     return read_file_spark(file_path, \"csv\", **kwargs)\n   File \"/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/zoo/orca/data/pandas/preprocessing.py\", line 146, in read_file_spark\n     df = spark.read.csv(file_path, **kwargs)\n   File \"/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/pyspark/sql/readwriter.py\", line 472, in csv\n     return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n   File \"/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1257, in __call__\n     answer, self.gateway_client, self.target_id, self.name)\n   File \"/opt/work/client/anaconda3/envs/tianchi/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 69, in deco\n     raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n pyspark.sql.utils.AnalysisException: 'Path does not exist: hdfs://Almaren-Node-003:9000/opt/work/client/kai/nyc_taxi.csv;'\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hkvision", "commentT": "2020-08-04T07:26:27Z", "comment_text": "\n \t\tspark.read.csv/json defaults to use hadoop fs.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "hkvision", "commentT": "2020-08-11T04:28:46Z", "comment_text": "\n \t\tThis error also happens on YARN cluster. Need to fix it asap.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "hkvision", "commentT": "2020-08-11T15:36:47Z", "comment_text": "\n \t\tThis is because bigdl and analytics-zoo pip using different spark versions to build?\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "hkvision", "commentT": "2020-08-11T19:04:07Z", "comment_text": "\n \t\twhat error? \"Path does not exist: hdfs://Almaren-Node-003:9000/opt/work/client/kai/nyc_taxi.csv\" is because we use spark to read csv and spark uses hadoop fs to read file. I think we don't need to fix it.\n For  java.lang.IllegalArgumentException: Illegal pattern component: XXX, the error is inconsistent commons-lang3 version between spark and hadoop.  We could either put our bigdl/analytics-zoo jars before hadoop jars, or add option(\"timestampFormat\", \"yyyy/MM/dd HH:mm:ss ZZ\") to fix it. My previous fix is putting our jars at the front of driver and executor classpath, but <denchmark-link:https://github.com/intel-analytics/analytics-zoo/pull/2628>#2628</denchmark-link>\n  changes that order of classpath. I think now we may need to add this option(\"timestampFormat\", \"yyyy/MM/dd HH:mm:ss ZZ\") to fix it.\n \t\t"}}}, "commit": {"commit_id": "98e0205fa8275e7617cb357517c61801c99376da", "commit_author": "Yang Wang", "commitT": "2020-09-08 13:29:58+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pyzoo\\zoo\\util\\engine.py", "file_new_name": "pyzoo\\zoo\\util\\engine.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "76", "deleted_lines": "76", "method_info": {"method_name": "__prepare_analytics_zoo_env.append_path", "method_params": "env_var_name,path", "method_startline": "72", "method_endline": "78"}}, "hunk_1": {"Ismethod": 1, "added_lines": "76", "deleted_lines": "76", "method_info": {"method_name": "__prepare_analytics_zoo_env", "method_params": "", "method_startline": "66", "method_endline": "102"}}}}}}}