{"BR": {"BR_id": "124", "BR_author": "rk37", "BRopenT": "2020-07-25T10:07:07Z", "BRcloseT": "2020-07-25T20:35:49Z", "BR_text": {"BRsummary": "AttributeError occurred when use \"bias=False\" linear layer in custom FeaturesExtractor", "BRdescription": "\n Describe the bug\n When use custom  FeaturesExtractor for common.policies.ActorCriticPolicy, AttributeError occurred : 'NoneType' object has no attribute 'data'.\n test codes\n import sys\n import numpy as np\n import gym\n import torch\n import torch.nn as nn\n \n import stable_baselines3\n from stable_baselines3 import PPO\n from stable_baselines3.ppo import MlpPolicy\n from stable_baselines3.common.cmd_util import make_vec_env\n \n # Parallel environments\n env = make_vec_env('CartPole-v1', n_envs=4)\n \n from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n \n class CustomFeaturesExtractor(BaseFeaturesExtractor):\n         def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n             super(CustomFeaturesExtractor, self).__init__(observation_space, features_dim)\n             self.linear = nn.Sequential(torch.nn.Flatten(),\n                                         # disable bias\n                                         torch.nn.Linear(np.prod(observation_space.shape), features_dim, bias=False),\n                                         torch.nn.ReLU())\n             pass\n         \n         def forward(self, observations: torch.Tensor) -> torch.Tensor:\n             return self.linear(observations)\n             pass\n \n from stable_baselines3.common.policies import ActorCriticPolicy\n \n class CustomPolicy(ActorCriticPolicy):\n     def __init__(self, *args, **kwargs):\n         super(CustomPolicy, self).__init__(*args, **kwargs, features_extractor_class = CustomFeaturesExtractor)\n         pass\n \n model = PPO(CustomPolicy, env, verbose=1)\n model.learn(total_timesteps=25000)\n stack traces\n Using cuda device\n ---------------------------------------------------------------------------\n AttributeError                            Traceback (most recent call last)\n <ipython-input-9-3bcc7e758863> in <module>\n ----> 1 model = PPO(CustomPolicy, env, verbose=1)\n       2 model.learn(total_timesteps=25000)\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py in __init__(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\n     118 \n     119         if _init_setup_model:\n --> 120             self._setup_model()\n     121 \n     122     def _setup_model(self) -> None:\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py in _setup_model(self)\n     121 \n     122     def _setup_model(self) -> None:\n --> 123         super(PPO, self)._setup_model()\n     124 \n     125         # Initialize schedules for policy/value clipping\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py in _setup_model(self)\n     112             n_envs=self.n_envs,\n     113         )\n --> 114         self.policy = self.policy_class(\n     115             self.observation_space,\n     116             self.action_space,\n \n <ipython-input-8-d7d248355231> in __init__(self, *args, **kwargs)\n       3 class CustomPolicy(ActorCriticPolicy):\n       4     def __init__(self, *args, **kwargs):\n ----> 5         super(CustomPolicy, self).__init__(*args, **kwargs, features_extractor_class = CustomFeaturesExtractor)\n       6         pass\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py in __init__(self, observation_space, action_space, lr_schedule, net_arch, device, activation_fn, ortho_init, use_sde, log_std_init, full_std, sde_net_arch, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, normalize_images, optimizer_class, optimizer_kwargs)\n     399         self.action_dist = make_proba_distribution(action_space, use_sde=use_sde, dist_kwargs=dist_kwargs)\n     400 \n --> 401         self._build(lr_schedule)\n     402 \n     403     def _get_data(self) -> Dict[str, Any]:\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py in _build(self, lr_schedule)\n     490             }\n     491             for module, gain in module_gains.items():\n --> 492                 module.apply(partial(self.init_weights, gain=gain))\n     493 \n     494         # Setup optimizer with initial learning rate\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/torch/nn/modules/module.py in apply(self, fn)\n     287         \"\"\"\n     288         for module in self.children():\n --> 289             module.apply(fn)\n     290         fn(self)\n     291         return self\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/torch/nn/modules/module.py in apply(self, fn)\n     287         \"\"\"\n     288         for module in self.children():\n --> 289             module.apply(fn)\n     290         fn(self)\n     291         return self\n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/torch/nn/modules/module.py in apply(self, fn)\n     288         for module in self.children():\n     289             module.apply(fn)\n --> 290         fn(self)\n     291         return self\n     292 \n \n ~/anaconda3/envs/tf2rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py in init_weights(module, gain)\n     189         if isinstance(module, (nn.Linear, nn.Conv2d)):\n     190             nn.init.orthogonal_(module.weight, gain=gain)\n --> 191             module.bias.data.fill_(0.0)\n     192 \n     193     @abstractmethod\n \n AttributeError: 'NoneType' object has no attribute 'data'\n \n System Info\n \n GPU nvidia gtx 1070\n Python 3.8.3\n PyTorch 1.5.1\n Gym 0.17.2\n Stable-Baselines 3 0.8.0a5\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "rk37", "commentT": "2020-07-25T10:11:39Z", "comment_text": "\n \t\tHello,\n It seems that a check is missing during initialization of the weights. We would appreciate a PR that solves this issue ;)\n In the meatime, you can deactivate orthogonal initialization by passing ortho_init=False to the policy.\n \t\t"}}}, "commit": {"commit_id": "bd2aae0c27d238a6a2c2f3a4c54066dfb30bd49b", "commit_author": "rk37", "commitT": "2020-07-25 22:35:48+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\misc\\changelog.rst", "file_new_name": "docs\\misc\\changelog.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "34,360", "deleted_lines": "359"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "stable_baselines3\\common\\policies.py", "file_new_name": "stable_baselines3\\common\\policies.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "191,192", "deleted_lines": "191", "method_info": {"method_name": "init_weights", "method_params": "Module,float", "method_startline": "185", "method_endline": "192"}}}}}}}