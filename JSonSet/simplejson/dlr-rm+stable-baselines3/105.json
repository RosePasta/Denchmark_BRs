{"BR": {"BR_id": "105", "BR_author": "AndyShih12", "BRopenT": "2020-07-15T20:39:12Z", "BRcloseT": "2020-08-03T20:22:52Z", "BR_text": {"BRsummary": "[bug] on-policy rollout collects current \"dones\" instead of last \"dones\"", "BRdescription": "\n Took me a really long time to debug this, so hopefully this helps others out.\n \n The on-policy rollout collects last_obs, current reward, current dones. See <denchmark-link:https://github.com/DLR-RM/stable-baselines3/blob/3cf6e9714b816ab7f1352d6aa439059becff707b/stable_baselines3/common/on_policy_algorithm.py#L156>here</denchmark-link>\n \n In stable-baselines, the rollout collects last_obs, current reward, and last dones. See <denchmark-link:https://github.com/hill-a/stable-baselines/blob/c54dc5dc47be2be796f42bf76a3d2e74fa3db7d8/stable_baselines/ppo2/ppo2.py#L471>here</denchmark-link>\n \n This messes up the returns and advantage calculations.\n I fixed this locally, and PPO improved dramatically on my custom environment (red is before fix, green is after fix).\n <denchmark-link:https://camo.githubusercontent.com/1a79888256e1e9b59d77f04ce71b4658f5396c3ef2fc1ed240f12f0bd3385adb/68747470733a2f2f692e696d6775722e636f6d2f776b564c33676f2e706e67></denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "AndyShih12", "commentT": "2020-07-15T21:10:50Z", "comment_text": "\n \t\tThank you a ton for this! I will implement and test this out on Atari envs, which have also reached lower performance compared to stable-baselines results. My apologies that you had to debug for such a nasty mistake!\n I will double-check if the sb3 code is handling dones as in sb. If I include this in an update, would it be because that I include your name in the contributors? Debugging this error out counts as a good contribution, in my eyes :)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "AndyShih12", "commentT": "2020-07-15T21:26:33Z", "comment_text": "\n \t\tSure, that would be appreciated. Thanks for the quick response, hope it helps on the Atari envs too!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "AndyShih12", "commentT": "2020-07-16T08:51:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AndyShih12>@AndyShih12</denchmark-link>\n \n Could you confirm that this is type of fix you implemented? It initializes and stores the dones from previous iteration. I ran this on some Atari games but it did not improve results, but it is uncertain if this fix would help there (I was hoping it would ^^'), and I want to make double-sure I patched it as you did before moving on.\n --- a/stable_baselines3/common/base_class.py\n +++ b/stable_baselines3/common/base_class.py\n @@ -117,6 +117,7 @@ class BaseAlgorithm(ABC):\n          self.tensorboard_log = tensorboard_log\n          self.lr_schedule = None  # type: Optional[Callable]\n          self._last_obs = None  # type: Optional[np.ndarray]\n +        self._last_dones = None  # type: Optional[np.ndarray]\n          # When using VecNormalize:\n          self._last_original_obs = None  # type: Optional[np.ndarray]\n          self._episode_num = 0\n @@ -444,6 +445,7 @@ class BaseAlgorithm(ABC):\n          # Avoid resetting the environment when calling ``.learn()`` consecutive times\n          if reset_num_timesteps or self._last_obs is None:\n              self._last_obs = self.env.reset()\n +            self._last_dones = np.zeros((self._last_obs.shape[0],), dtype=np.bool)\n              # Retrieve unnormalized observation for saving into the buffer\n              if self._vec_normalize_env is not None:\n                  self._last_original_obs = self._vec_normalize_env.get_original_obs()\n diff --git a/stable_baselines3/common/on_policy_algorithm.py b/stable_baselines3/common/on_policy_algorithm.py\n index 2937b77..52d8573 100644\n --- a/stable_baselines3/common/on_policy_algorithm.py\n +++ b/stable_baselines3/common/on_policy_algorithm.py\n @@ -153,8 +153,9 @@ class OnPolicyAlgorithm(BaseAlgorithm):\n              if isinstance(self.action_space, gym.spaces.Discrete):\n                  # Reshape in case of discrete action\n                  actions = actions.reshape(-1, 1)\n -            rollout_buffer.add(self._last_obs, actions, rewards, dones, values, log_probs)\n +            rollout_buffer.add(self._last_obs, actions, rewards, self._last_dones, values, log_probs)\n              self._last_obs = new_obs\n +            self._last_dones = dones\n \n          rollout_buffer.compute_returns_and_advantage(values, dones=dones)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "AndyShih12", "commentT": "2020-07-16T09:50:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/AndyShih12>@AndyShih12</denchmark-link>\n  thank you for pointing out that issue.\n \n I fixed this locally, and PPO improved dramatically on my custom environment\n \n Could you tell us from about your custom env and the hyperparameters you used for PPO?\n I would expect an environment with either sparse reward or short episodes (~100 timesteps), as the effect should be hard to see on long episodes.\n \n Could you confirm that this is type of fix you implemented?\n \n <denchmark-link:https://github.com/Miffyli>@Miffyli</denchmark-link>\n  I was about to ask you to push a draft PR. I think we can fix directly the GAE computation without having to add extra variable, no?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "AndyShih12", "commentT": "2020-07-16T10:29:04Z", "comment_text": "\n \t\tI concur this effect is more pronounced in short episodes or reward-at-terminal games, but just to make sure I tested it on Atari.\n \n I was about to ask you to push a draft PR. I think we can fix directly the GAE computation without having to add extra variable, no?\n \n Ok, I will begin a draft on matching PPO/A2C discrete-policy performance with stable-baselines (with Atari games). We can fix it that way in GAE as well, but for that I want to make sure other code does not expect the sb2-style-dones.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "AndyShih12", "commentT": "2020-07-16T17:28:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Miffyli>@Miffyli</denchmark-link>\n  Yes your patch is the same as what I did locally.\n <denchmark-link:https://github.com/araffin>@araffin</denchmark-link>\n  Indeed, my environment was sparse rewards and (very) short episodes. The reward only came at the last timestep of each episode, the previous code didn't receive any signal at all.\n Sorry to hear that it didn't close the gap for Atari games.\n \t\t"}}}, "commit": {"commit_id": "2cd6a4f93bee93e216def73703861139e325fd88", "commit_author": "Anssi", "commitT": "2020-08-03 22:22:51+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\misc\\changelog.rst", "file_new_name": "docs\\misc\\changelog.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "37,38,54", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\modules\\a2c.rst", "file_new_name": "docs\\modules\\a2c.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "13,14,15,16,17,18,19,20", "deleted_lines": null}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "stable_baselines3\\a2c\\a2c.py", "file_new_name": "stable_baselines3\\a2c\\a2c.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "119", "deleted_lines": null, "method_info": {"method_name": "train", "method_params": "self", "method_startline": "111", "method_endline": "168"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "stable_baselines3\\common\\base_class.py", "file_new_name": "stable_baselines3\\common\\base_class.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "126,478", "deleted_lines": null}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "stable_baselines3\\common\\on_policy_algorithm.py", "file_new_name": "stable_baselines3\\common\\on_policy_algorithm.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "176,178", "deleted_lines": "176"}}}, "file_5": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "stable_baselines3\\common\\sb2_compat\\__init__.py", "file_new_name": "stable_baselines3\\common\\sb2_compat\\__init__.py"}, "file_6": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "stable_baselines3\\common\\sb2_compat\\rmsprop_tf_like.py"}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "stable_baselines3\\common\\torch_layers.py", "file_new_name": "stable_baselines3\\common\\torch_layers.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "77", "deleted_lines": "77", "method_info": {"method_name": "__init__", "method_params": "self,Box,int", "method_startline": "62", "method_endline": "86"}}}}, "file_8": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\test_custom_policy.py", "file_new_name": "tests\\test_custom_policy.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "38,39,40", "deleted_lines": null, "method_info": {"method_name": "test_tf_like_rmsprop_optimizer", "method_params": "", "method_startline": "38", "method_endline": "40"}}}}}}}