{"BR": {"BR_id": "332", "BR_author": "pi-null-mezon", "BRopenT": "2016-10-07T08:41:59Z", "BRcloseT": "2016-11-20T14:42:33Z", "BR_text": {"BRsummary": "Why network that have worked in version 0.1.1 does not work in 1.0.0?", "BRdescription": "\n When I migrate to the version 1.0.0 I have found strange behavior. My network that i previously trained by version 0.1.1 of tiny-dnn (I have implement SegNet-like architecture for Chest X-RAY lungs segmentation and it works surprisingly well) do not produce similar results with tiny-dnn version 1.0.0. In v.0.1.1 my network trains well (I can see that at the each new epoch of training the loss function value becomes lower and lower and the activation in the last layer becomes more and more accurate), but in v.1.0.0 training process on the same data seems to diverges all the time (loss is not changed considerably between epoches and the last layer activation is not converges to lungs-like projection) . So why it is could happen? Also, I should tell, that I have not use built-in serialization method and instead load weights by my own (and when I rewrite code for the v.1.0.0 support one thing I have done was replacement of get_weights() method to weights() and call init_weights() after network creation and weights upload). Sorry for such long description\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "pi-null-mezon", "commentT": "2016-10-08T12:13:39Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pi-null-mezon>@pi-null-mezon</denchmark-link>\n \n Since tiny-dnn have changed it's parallelism strategy, the result could be differ. What happens if you change the learning rate a bit smaller (x0.8 or x0.5) in your optimizer?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "pi-null-mezon", "commentT": "2016-10-10T09:25:29Z", "comment_text": "\n \t\tI have tried x0.8, x0.5, x0.1 with no success.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "pi-null-mezon", "commentT": "2016-10-20T12:21:21Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/nyanp>@nyanp</denchmark-link>\n  Code is at <denchmark-link:https://github.com/pi-null-mezon/OpenIST/blob/master/CMDUtils/SegNet/cnnsegmentnet.cpp>https://github.com/pi-null-mezon/OpenIST/blob/master/CMDUtils/SegNet/cnnsegmentnet.cpp</denchmark-link>\n  but I don't think that the dataset is available to reproduce exactly the same experiment.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "pi-null-mezon", "commentT": "2016-10-20T12:24:36Z", "comment_text": "\n \t\tI will just add some images, that explain problem.\n So this is the raw image:\n <denchmark-link:https://cloud.githubusercontent.com/assets/5991537/19559381/4215330a-96d8-11e6-90ee-a4b84c77edb5.png></denchmark-link>\n ;\n I want to segment lungs. Using SegNet architecture and tiny-cnn v0.1.1 I have get following result:\n <denchmark-link:https://cloud.githubusercontent.com/assets/5991537/19559431/7af39cc0-96d8-11e6-8140-0e2a622ef29c.png></denchmark-link>\n ;\n All seems to be fine. But after iny-dnn update to the version 1.0.0 same network starts to produce bad results:\n <denchmark-link:https://cloud.githubusercontent.com/assets/5991537/19559489/cc807c16-96d8-11e6-9bfe-b41338081c78.png></denchmark-link>\n \n Ok, I have said, let's retrain network and perform a new experiment. Unsuccessful. And if in v.0.1.1 convergence has been seen after few epoches in v.1.0.0 training procedure does not converge and even after 30-50 epoches produce random pictures like this:\n <denchmark-link:https://cloud.githubusercontent.com/assets/5991537/19559607/4a8862f4-96d9-11e6-903f-df74dcec665b.png></denchmark-link>\n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "pi-null-mezon", "commentT": "2016-10-20T12:26:21Z", "comment_text": "\n \t\tDataset is available and resides here: <denchmark-link:https://github.com/pi-null-mezon/OpenIST/tree/master/Datasets>https://github.com/pi-null-mezon/OpenIST/tree/master/Datasets</denchmark-link>\n \n see in subfolder Chest_x-ray\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "pi-null-mezon", "commentT": "2016-10-20T12:35:58Z", "comment_text": "\n \t\tThank you now we have all the info to reproduce your experiment and check if there is any regression.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "pi-null-mezon", "commentT": "2016-10-20T13:05:00Z", "comment_text": "\n \t\tI should note few little things. I have trained network with input size 128 x 128 pixels with 4 samples per mini batch - it is not very interesting, just to clarify. But what is very important that in success experiments float_t type was defined as double. When I made experiments with float_t as float  network \"blows up\" (NaN weights) after 5-7 epoches.\n By the way, interesting, that network that has been trained and saved with double precision, then could be load and works pretty well in float precision build. So training in double precision seems to have advantage in this particular case.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "pi-null-mezon", "commentT": "2016-10-20T16:27:09Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pi-null-mezon>@pi-null-mezon</denchmark-link>\n \n Thank you very much for providing more info, I'll check the difference later. I've also opened a PR <denchmark-link:https://github.com/tiny-dnn/tiny-dnn/pull/353>#353</denchmark-link>\n  to providing double-precision support in v1.0.0. Hope this would help you.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "pi-null-mezon", "commentT": "2016-11-11T13:49:02Z", "comment_text": "\n \t\tHello! Is there any news?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "pi-null-mezon", "commentT": "2016-11-14T22:45:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pi-null-mezon>@pi-null-mezon</denchmark-link>\n \n hi, I couldn't try to reproduce your problems, but now we have a new v1.0.0a2 release. This includes many bug fixes with double precision support. You can retry this :)\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "pi-null-mezon", "commentT": "2016-11-18T06:41:46Z", "comment_text": "\n \t\tThanks, I will try.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "pi-null-mezon", "commentT": "2016-11-18T08:12:39Z", "comment_text": "\n \t\tUnfortunately new version did not solve the issue. So, I'll ask another question: whether this particular task is the only example of bad performance on new library version?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "pi-null-mezon", "commentT": "2016-11-20T13:28:03Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pi-null-mezon>@pi-null-mezon</denchmark-link>\n \n I've confirmed that your code doesn't work on v1.0.0a2. I've noticed that this is the bug around convolutional layer with  mode. I'll fix it.\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "pi-null-mezon", "commentT": "2016-11-20T14:47:58Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/pi-null-mezon>@pi-null-mezon</denchmark-link>\n \n I've confirmed that <denchmark-link:https://github.com/tiny-dnn/tiny-dnn/commit/391d26fc1af8235cf8127de7c71c6066cf02bf02>391d26f</denchmark-link>\n  solved the issue with half learning rate and double precision :)\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "pi-null-mezon", "commentT": "2016-11-21T12:10:43Z", "comment_text": "\n \t\tI,ve checked branch \"fix/padding_same\" and my segment network realy works as expected! Thanks for the support!\n \t\t"}}}, "commit": {"commit_id": "391d26fc1af8235cf8127de7c71c6066cf02bf02", "commit_author": "nyanp", "commitT": "2016-11-20 06:42:33-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "test\\test_convolutional_layer.h", "file_new_name": "test\\test_convolutional_layer.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "705,706,707,708,709,710,711,712,713,714,715,716,717", "deleted_lines": null, "method_info": {"method_name": "tiny_dnn::TEST", "method_params": "convolutional,gradient_check12_pad_same", "method_startline": "705", "method_endline": "717"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tiny_dnn\\layers\\convolutional_layer.h", "file_new_name": "tiny_dnn\\layers\\convolutional_layer.h", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "275", "deleted_lines": "275", "method_info": {"method_name": "tiny_dnn::convolutional_layer::back_propagation", "method_params": "in_data,out_data,out_grad,in_grad", "method_startline": "254", "method_endline": "288"}}}}}}}