{"BR": {"BR_id": "1290", "BR_author": "AmitMY", "BRopenT": "2020-03-30T11:08:50Z", "BRcloseT": "2020-04-24T14:29:25Z", "BR_text": {"BRsummary": "bug(logger): wandb fails on sweep", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When using wandb sweeps for hyperparameters search, I get this error:\n \n wandb: ERROR Attempted to change value of key \"dropout_std\" from 0.030424838979365657 to 0.030424838979365654\n \n The reason is I ran:\n wandb_logger.log_hyperparams(params)\n Which I guess has some problem with floating-point numbers in high accuracy?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "AmitMY", "commentT": "2020-03-30T18:04:40Z", "comment_text": "\n \t\t+1, I faced the same issue when using pytorch lightning with wandb sweeps. To summarize, wandb automatically logs hyperparams when we run the wandb sweep agent on a machine. Later, pytorch lightning again tries to log same hyperparams but due to precision error between lightning and wandb already logged hyperparams, wandb throws this error. Just a guess: wandb sweep agent might be using double format to generate new hyperparams and when lightning receives those args from command line, it converts them to float and tries to log it. I haven't digged in detail where these hyperparams get altered, it could be on wandb side or lightning side.\n I reported this issue to wandb and got the following response:\n \n It is preferred to either pass your config parameters all at once to: wandb.init(config=config_dict_that_could_have_params_set_by_sweep)\n or:\n experiment = wandb.init()\n experiment.config.setdefaults(config_dict_that_could_have_params_set_by_sweep)\n The advantage of doing this is that it will ignore setting any key that has already been set by the sweep.\n We will look into the torch lightning integration and see if we can make this safer for those using sweeps.\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "AmitMY", "commentT": "2020-03-30T18:05:52Z", "comment_text": "\n \t\tcc: <denchmark-link:https://github.com/neggert>@neggert</denchmark-link>\n , <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n , <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "AmitMY", "commentT": "2020-03-30T18:24:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  <denchmark-link:https://github.com/calclavia>@calclavia</denchmark-link>\n  pls ^^\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "AmitMY", "commentT": "2020-03-30T18:37:27Z", "comment_text": "\n \t\tThe problem is that it tries to log this value twice and is probably called before automatically by pytorch-lightning.\n The callback will automatically log every parameter which is in pl.LightningModule.params (where you probably already have the dropout).\n See an example of using pytorch-lightning with wandb (including sweeps) here: <denchmark-link:https://github.com/borisdayma/lightning-kitti>https://github.com/borisdayma/lightning-kitti</denchmark-link>\n \n I'll be adding it to the pytorch-lightning repo later but still need to push a PR related to the watch method for it to work properly.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "AmitMY", "commentT": "2020-04-16T22:26:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  is it fixed now?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "AmitMY", "commentT": "2020-04-16T23:45:41Z", "comment_text": "\n \t\tI added a fix.\n <denchmark-link:https://github.com/AmitMY>@AmitMY</denchmark-link>\n  <denchmark-link:https://github.com/amoudgl>@amoudgl</denchmark-link>\n  Feel free to test it with your sweeps and let me know if there's still an error.\n \t\t"}}}, "commit": {"commit_id": "f3d139e90f9212813c4f5e6de777bdef9dfe7635", "commit_author": "Boris Dayma", "commitT": "2020-04-24 10:29:24-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "69,70", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\wandb.py", "file_new_name": "pytorch_lightning\\loggers\\wandb.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "117", "deleted_lines": "117", "method_info": {"method_name": "log_hyperparams", "method_params": "self,str", "method_startline": "115", "method_endline": "117"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\loggers\\test_wandb.py", "file_new_name": "tests\\loggers\\test_wandb.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "26", "deleted_lines": "26", "method_info": {"method_name": "test_wandb_logger", "method_params": "wandb", "method_startline": "11", "method_endline": "32"}}}}}}}