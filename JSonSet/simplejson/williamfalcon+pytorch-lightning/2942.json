{"BR": {"BR_id": "2942", "BR_author": "ananthsub", "BRopenT": "2020-08-13T04:47:43Z", "BRcloseT": "2020-08-14T09:37:22Z", "BR_text": {"BRsummary": "ddp_backend in 0.9.0rc12 fails if no CUDA_VISIBLE_DEVICES found", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n In ddp_backend, training immediately fails if the environment variable CUDA_VISIBLE_DEVICES isn't set. This line should handle the None case gracefully: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/accelerators/ddp_backend.py#L90>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/accelerators/ddp_backend.py#L90</denchmark-link>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Start a run using ddp on CPU. This was discovered using torchelastic to launch\n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n This shouldn't crash if the environment variable isn't set. We could default to num_gpus = 0 in this case.\n Replacing the line above with something like this could work:\n num_gpus = os.environ.get('CUDA_VISIBLE_DEVICES', []).split(',').__len__() \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "ananthsub", "commentT": "2020-08-13T05:52:19Z", "comment_text": "\n \t\tThis is a bug in our usage of distributed backend, not an issue with lightning\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "ananthsub", "commentT": "2020-08-13T06:26:22Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ananthsub>@ananthsub</denchmark-link>\n  good catch, mind send a PR?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "ananthsub", "commentT": "2020-08-13T17:30:00Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n   reading more about the accelerator changes, I am confused about the split between ddp and ddp_cpu backends. For example, if the user wants to launch training with torchelastic or a non-slurm launcher, then we use the ddp backend. This will fail if the training is on CPUs. The docs aren't updated with the latest changes either: <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-modes>https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-modes</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "ananthsub", "commentT": "2020-08-13T21:04:16Z", "comment_text": "\n \t\tthe ddp is GPU based compare to ddp_cpu is CPU only...\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "ananthsub", "commentT": "2020-08-13T23:21:35Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  in our test case, we launch with these trainer parameters:\n \n and this is the stacktrace we run into:\n <denchmark-code>    trainer.fit(self)\n   File \"pytorch_lightning/trainer/states.py\", line 34, in wrapped_fn\n     result = fn(self, *args, **kwargs)\n   File \"pytorch_lightning/trainer/trainer.py\", line 1030, in fit\n     results = self.accelerator_backend.spawn_ddp_children(model)\n   File \"pytorch_lightning/accelerators/ddp_backend.py\", line 90, in spawn_ddp_children\n     num_gpus = os.environ['CUDA_VISIBLE_DEVICES'].split(',').__len__()\n   File \"/usr/lib/python3.7/os.py\", line 679, in __getitem__\n     raise KeyError(key) from None\n KeyError: 'CUDA_VISIBLE_DEVICES'\n </denchmark-code>\n \n Should this be detected by the config validator earlier?\n \t\t"}}}, "commit": {"commit_id": "0c264689cb566582ac47333d8b7192d656e19440", "commit_author": "William Falcon", "commitT": "2020-08-13 21:54:57-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\debugging.rst", "file_new_name": "docs\\source\\debugging.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "4,5", "deleted_lines": "4,5"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\accelerators\\ddp_backend.py", "file_new_name": "pytorch_lightning\\accelerators\\ddp_backend.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "90", "deleted_lines": "90", "method_info": {"method_name": "spawn_ddp_children", "method_params": "self,model", "method_startline": "58", "method_endline": "121"}}}}}}}