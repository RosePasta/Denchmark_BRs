{"BR": {"BR_id": "4319", "BR_author": "SeanNaren", "BRopenT": "2020-10-23T10:13:33Z", "BRcloseT": "2020-10-24T20:55:50Z", "BR_text": {"BRsummary": "After DDP train processes have different best val paths", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Tied to <denchmark-link:https://github.com/huggingface/transformers/pull/7852>huggingface/transformers#7852</denchmark-link>\n \n There is no synchronisation/communication to ensure the model has finished saving before loading. If you look at ddp_spawn/ddp_cpu there is communication to ensure that each process has the same best_val_path stored in the model after save.\n Run below on multi-gpu:\n # Copyright The PyTorch Lightning team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n # You may obtain a copy of the License at\n #\n #     http://www.apache.org/licenses/LICENSE-2.0\n #\n # Unless required by applicable law or agreed to in writing, software\n # distributed under the License is distributed on an \"AS IS\" BASIS,\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n # --------------------------------------------\n # --------------------------------------------\n # --------------------------------------------\n # USE THIS MODEL TO REPRODUCE A BUG YOU REPORT\n # --------------------------------------------\n # --------------------------------------------\n # --------------------------------------------\n import glob\n import os\n from tempfile import TemporaryDirectory\n \n import torch\n from torch.utils.data import Dataset\n \n from pytorch_lightning import Trainer, LightningModule\n from pytorch_lightning.callbacks import ModelCheckpoint\n \n \n class RandomDataset(Dataset):\n     def __init__(self, size, length):\n         self.len = length\n         self.data = torch.randn(length, size)\n \n     def __getitem__(self, index):\n         return self.data[index]\n \n     def __len__(self):\n         return self.len\n \n \n class BoringModel(LightningModule):\n \n     def __init__(self):\n         \"\"\"\n         Testing PL Module\n \n         Use as follows:\n         - subclass\n         - modify the behavior for what you want\n \n         class TestModel(BaseTestModel):\n             def training_step(...):\n                 # do your own thing\n \n         or:\n \n         model = BaseTestModel()\n         model.training_epoch_end = None\n \n         \"\"\"\n         super().__init__()\n         self.layer = torch.nn.Linear(32, 2)\n \n     def forward(self, x):\n         return self.layer(x)\n \n     def loss(self, batch, prediction):\n         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\n         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\n \n     def step(self, x):\n         x = self.layer(x)\n         out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\n         return out\n \n     def training_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log('loss', loss)\n         return {\"loss\": loss}\n \n     def training_step_end(self, training_step_outputs):\n         return training_step_outputs\n \n     def validation_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log('x', loss)\n \n     def test_step(self, batch, batch_idx):\n         output = self.layer(batch)\n         loss = self.loss(batch, output)\n         self.log('y', loss)\n \n     def configure_optimizers(self):\n         optimizer = torch.optim.AdamW(self.layer.parameters(), lr=0.1)\n         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n         return [optimizer], [lr_scheduler]\n \n \n def run_test():\n     class TestModel(BoringModel):\n         def validation_step(self, batch, batch_idx):\n             output = self.layer(batch)\n             loss = self.loss(batch, output)\n             self.log('x', loss)\n \n     # fake data\n     train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\n     val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))\n \n     # model\n     model = TestModel()\n     tmp_dir = 'temp/'\n     if os.path.exists(tmp_dir):\n         os.rmdir(tmp_dir)\n \n     trainer = Trainer(\n         default_root_dir=os.getcwd(),\n         max_epochs=2,\n         accelerator='ddp',\n         gpus=2,\n         checkpoint_callback=ModelCheckpoint(\n             dirpath=tmp_dir,\n             monitor='x',\n             mode='min',\n             save_top_k=1\n         )\n     )\n     trainer.fit(model, train_data, val_data)\n     checkpoints = list(sorted(glob.glob(os.path.join(tmp_dir, \"*.ckpt\"), recursive=True)))\n     print(\"checkpoints\", checkpoints)\n     print(trainer.checkpoint_callback.best_model_path)\n     assert os.path.exists(\n         trainer.checkpoint_callback.best_model_path), f'Could not find checkpoint at rank {trainer.global_rank}'\n \n \n if __name__ == '__main__':\n     run_test()\n Output:\n <denchmark-code>Traceback (most recent call last):\n   File \"/home/jovyan/transformers/reproduce.py\", line 139, in <module>\n     run_test()\n   File \"/home/jovyan/transformers/reproduce.py\", line 135, in run_test\n     trainer.checkpoint_callback.best_model_path), f'Could not find checkpoint at rank {trainer.global_rank}'\n AssertionError: Could not find checkpoint at rank 1\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Assertion does not fail\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "SeanNaren", "commentT": "2020-10-23T10:14:08Z", "comment_text": "\n \t\tcc <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "SeanNaren", "commentT": "2020-10-23T12:25:12Z", "comment_text": "\n \t\tYes, this makes sense.\n Only the process 0 is saving checkpoints, so one solution is to send the best model path of rank 0 to all other processes.\n The checkpoint directory based on logger name and version is one example where we do this already:\n \n \n \n pytorch-lightning/pytorch_lightning/callbacks/model_checkpoint.py\n \n \n          Line 445\n       in\n       3abfec8\n \n \n \n \n \n \n  version, name = trainer.accelerator_backend.broadcast((version, trainer.logger.name)) \n \n \n \n \n \n so it could be done in a similar way.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "SeanNaren", "commentT": "2020-10-23T13:03:13Z", "comment_text": "\n \t\tA workaround for you until this issue is fix is to check the rank before you access the path, i.e.\n if trainer.global_rank == 0:\n     print(trainer.checkpoint_callback.best_model_path)  # do whatever with path\n \t\t"}}}, "commit": {"commit_id": "5641b266d56d2324e1d9cb3ba12a60b47ab10558", "commit_author": "Sean Naren", "commitT": "2020-10-24 16:55:49-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "34", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\accelerators\\accelerator.py", "file_new_name": "pytorch_lightning\\accelerators\\accelerator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "55,56", "deleted_lines": "55", "method_info": {"method_name": "teardown", "method_params": "self", "method_startline": "54", "method_endline": "56"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\accelerators\\dp_accelerator.py", "file_new_name": "pytorch_lightning\\accelerators\\dp_accelerator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "104", "deleted_lines": null, "method_info": {"method_name": "teardown", "method_params": "self", "method_startline": "101", "method_endline": "104"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\accelerators\\horovod_accelerator.py", "file_new_name": "pytorch_lightning\\accelerators\\horovod_accelerator.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "110,111", "method_info": {"method_name": "teardown", "method_params": "self", "method_startline": "110", "method_endline": "111"}}}}}}}