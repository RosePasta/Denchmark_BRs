{"BR": {"BR_id": "2936", "BR_author": "import-antigravity", "BRopenT": "2020-08-12T18:38:40Z", "BRcloseT": "2020-10-06T03:15:52Z", "BR_text": {"BRsummary": "Trainer \"optimizers\" attribute is None when saving checkpoint and callbacks list is not empty", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n I'm training a GAN and I'm running a few custom callbacks as well. When the model attempts to save at the end of the first epoch, it crashes. Here's the very strange thing: I have the exact same code in a Jupyter notebook and the error doesn't occur.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n The bug does not occur when the callbacks list passed into the trainer is empty. None of the callbacks I'm using have anything to do with saving checkpoints, they're all for logging certain things about the model. Enabling any one of them causes the error. Running the exact same code in Jupyter results in no crashes.\n Stack trace:\n <denchmark-code>Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588-| 98.33% [590/600 00:05<00:00 loss: -0.558, v_num: 1, d_loss: -1.120, g_loss: -0.016]\n   File \"mnist-dense-gan-convergence.py\", line 55, in <module>\n     main(args)\n   File \"mnist-dense-gan-convergence.py\", line 45, in main\n     trainer.fit(gan)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1044, in fit\n     results = self.run_pretrain_routine(model)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1213, in run_pretrain_routine\n     self.train()\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 370, in train\n     self.run_training_epoch()\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 502, in run_training_epoch\n     self.check_checkpoint_callback(should_check_val)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 513, in check_checkpoint_callback\n     [c.on_validation_end(self, self.get_model()) for c in checkpoint_callbacks]\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py\", line 513, in <listcomp>\n     [c.on_validation_end(self, self.get_model()) for c in checkpoint_callbacks]\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py\", line 12, in wrapped_fn\n     return fn(*args, **kwargs)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 309, in on_validation_end\n     self._do_check_save(filepath, current, epoch)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 346, in _do_check_save\n     self._save_model(filepath)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 168, in _save_model\n     self.save_function(filepath, self.save_weights_only)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py\", line 268, in save_checkpoint\n     checkpoint = self.dump_checkpoint(weights_only)\n   File \"/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py\", line 350, in dump_checkpoint\n     for i, optimizer in enumerate(self.optimizers):\n TypeError: 'NoneType' object is not iterable\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n Here is the relevant part of my setup code:\n inception_callback = GANInceptionScorer(classifier, logits=True, sample_size=1000, input_shape=(-1, 1, 28, 28))\n \n log_dir = os.path.abspath('../logs/mnist-dense-gan-convergence')\n \n params = ParameterMatrixCallback()\n \n callbacks = [\n     GANProgressBar(),\n     GANTensorboardImageView(),\n     params,\n     inception_callback\n ]\n \n trainer_args = {\n         'max_epochs': 100,\n         'default_root_dir': log_dir,\n         'callbacks': callbacks,\n         'progress_bar_refresh_rate': 0\n     }\n \n     print(log_dir)\n     try:\n         trainer = Trainer(gpus=1, **trainer_args)\n     except MisconfigurationException:\n         trainer = Trainer(**trainer_args)\n \n     trainer.fit(gan)\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n <denchmark-h:h3>Environment</denchmark-h>\n \n Please copy and paste the output from our\n <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>\n \n (or fill out the checklist below manually).\n You can get the script and run it with:\n <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py\n # For security purposes, please check the contents of collect_env_details.py before running it.\n python collect_env_details.py\n </denchmark-code>\n \n and the same code in Jupyter:\n inception_callback = GANInceptionScorer(classifier, logits=True, sample_size=1000, input_shape=(-1, 1, 28, 28))\n \n log_dir = os.path.abspath('../logs/mnist-gan-dense')\n \n params = ParameterMatrixCallback()\n \n trainer_args = {\n     'max_epochs': 200, \n     'callbacks': [GANProgressBar(), GANTensorboardImageView(n=4), params, inception_callback],\n     'progress_bar_refresh_rate': 0, \n     'default_root_dir': log_dir\n }\n \n t = Trainer(**trainer_args)\n \n PyTorch Version (e.g., 1.0): 1.3.1\n OS (e.g., Linux): macOS\n How you installed PyTorch (conda, pip, source): conda\n Python version: 3.7\n Any other relevant information: pytorch-lightning 0.8.5\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "import-antigravity", "commentT": "2020-08-12T18:41:55Z", "comment_text": "\n \t\tAdditional info, here are the relevant methods in my GAN class:\n class GAN(LightningModule, ABC):\n    ...\n \n     @abstractmethod\n     def g_optimizer(self) -> Optimizer:\n         pass\n \n     @abstractmethod\n     def d_optimizer(self) -> Optimizer:\n         pass\n \n     def configure_optimizers(self):\n         return self.g_optimizer(), self.d_optimizer()\n \n class MnistGanDense(GAN):\n     ...\n \n     def g_optimizer(self) -> Optimizer:\n         return optim.RMSprop(self.G.parameters(), self.hparams['learning_rate'])\n \n     def d_optimizer(self) -> Optimizer:\n         return optim.RMSprop(self.D.parameters(), self.hparams['learning_rate'])\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "import-antigravity", "commentT": "2020-08-14T01:51:49Z", "comment_text": "\n \t\tcould you try 0.9.0rc12?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "import-antigravity", "commentT": "2020-08-14T02:34:34Z", "comment_text": "\n \t\tIs there a way to do that with conda?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "import-antigravity", "commentT": "2020-08-14T06:44:05Z", "comment_text": "\n \t\tinside your Conda environment you could also install it with pip\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "import-antigravity", "commentT": "2020-08-15T11:47:42Z", "comment_text": "\n \t\tInside conda you can always install with pip:\n <denchmark-code>pip install pytorch-lightning==0.9.0rc13\n </denchmark-code>\n \n If this is still an issue, happy to reopen\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "import-antigravity", "commentT": "2020-09-01T21:41:55Z", "comment_text": "\n \t\tThis is still a problem for me. I updated to 0.9.1rc1 and still get this error. Here is my trace.\n <denchmark-code>Traceback (most recent call last):\n   File \"train_unet.py\", line 270, in <module>\n     trainer.save_checkpoint(args.save_checkpoint_path)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py\", line 275, in\n  save_checkpointe\n     checkpoint = self.dump_checkpoint(weights_only)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py\", line 360, in\n  dump_checkpoint\n     for i, optimizer in enumerate(self.optimizers):\n TypeError: 'NoneType' object is not iterable\n </denchmark-code>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "import-antigravity", "commentT": "2020-09-06T20:04:11Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  could you open this again? I'm still getting the error as well\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "import-antigravity", "commentT": "2020-09-30T06:35:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>\n  didn't we recently make optimizers init to an empty list instead of None? I think this should solve the problem. Could you check?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "import-antigravity", "commentT": "2020-09-30T06:44:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n  yes its an empty list now. But the code for lightning model defined above has optimizers defined, so am not sure yet what's the issue there.\n <denchmark-link:https://github.com/import-antigravity>@import-antigravity</denchmark-link>\n  mind check this on master?\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "import-antigravity", "commentT": "2020-10-02T19:11:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/deekshadangwal>@deekshadangwal</denchmark-link>\n  mind share full sample code so we can reproduce your issue?\n \t\t"}}}, "commit": {"commit_id": "cb2a3265e5eb329a48fb44df6ab8fd74df62b85a", "commit_author": "William Falcon", "commitT": "2020-10-05 23:15:52-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 6, "file_old_name": "tests\\trainer\\test_optimizers.py", "file_new_name": "tests\\trainer\\test_optimizers.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348", "deleted_lines": null, "method_info": {"method_name": "test_multiple_optimizers_callbacks", "method_params": "tmpdir", "method_startline": "304", "method_endline": "348"}}, "hunk_1": {"Ismethod": 1, "added_lines": "313,314", "deleted_lines": null, "method_info": {"method_name": "test_multiple_optimizers_callbacks.on_train_epoch_start", "method_params": "self,trainer,pl_module", "method_startline": "313", "method_endline": "314"}}, "hunk_2": {"Ismethod": 1, "added_lines": "310,311", "deleted_lines": null, "method_info": {"method_name": "test_multiple_optimizers_callbacks.on_train_batch_end", "method_params": "self,trainer,pl_module,batch,batch_idx,dataloader_idx", "method_startline": "310", "method_endline": "311"}}, "hunk_3": {"Ismethod": 1, "added_lines": "322,323,324,325,326,327,328,329,330,331", "deleted_lines": null, "method_info": {"method_name": "test_multiple_optimizers_callbacks.training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "322", "method_endline": "331"}}, "hunk_4": {"Ismethod": 1, "added_lines": "333,334,335,336", "deleted_lines": null, "method_info": {"method_name": "test_multiple_optimizers_callbacks.configure_optimizers", "method_params": "self", "method_startline": "333", "method_endline": "336"}}, "hunk_5": {"Ismethod": 1, "added_lines": "317,318,319,320", "deleted_lines": null, "method_info": {"method_name": "test_multiple_optimizers_callbacks.__init__", "method_params": "self", "method_startline": "317", "method_endline": "320"}}}}}}}