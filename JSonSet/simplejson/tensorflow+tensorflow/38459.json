{"BR": {"BR_id": "38459", "BR_author": "siavash-khodadadeh", "BRopenT": "2020-04-11T16:59:53Z", "BRcloseT": "2020-06-08T18:45:06Z", "BR_text": {"BRsummary": "Both 'mean' and 'variance' must be None when is_training is True and exponential_avg_factor == 1.0", "BRdescription": "\n System information\n \n Have I written custom code (as opposed to using a stock\n example script provided in TensorFlow): Yes\n OS Platform and Distribution (e.g.,\n Linux Ubuntu 16.04): Ubuntu 18.04\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\n the issue happens on mobile device:  No\n TensorFlow installed from (source or\n binary): - TensorFlow version (use command below): 2.2.0-dev20200411\n Python version: 3.6.3\n Bazel\n version (if compiling from source):\n GCC/Compiler version (if compiling from\n source):\n CUDA/cuDNN version: 10.1- GPU model and memory:\n \n You can collect some of this information using our environment capture\n <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>\n \n You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: \n Describe the current behavior\n When instantiating a batch norm layer like this:\n tf.keras.layers.BatchNormalization(momentum=0.0, center=True, scale=False, name='bn1')\n I get the error:\n Both 'mean' and 'variance' must be None when is_training is True and exponential_avg_factor == 1.0\n Describe the expected behavior\n It is not always the expected behavior. Consider meta-learning for example. We are going to see just one batch of training data and we want to adapt all means and variances to this batch, this means the momentum should be zero.\n Then after applying a few training iterations, we evaluate on the same batch norm layer with training=False and that also should work fine.\n Standalone code to reproduce the issue\n Provide a reproducible test case that is the bare minimum necessary to generate\n the problem. If possible, please share a link to Colab/Jupyter/any notebook.\n <denchmark-code>import tensorflow as tf\n import numpy as np\n \n inp = tf.keras.layers.Input(shape=(84, 84, 3))\n dense = tf.keras.layers.Conv2D(10, 3, activation=None)(inp)\n bn = tf.keras.layers.BatchNormalization(momentum=0.0, center=True, scale=False, name='bn1')(dense)\n rel = tf.keras.layers.ReLU()(bn)\n flat = tf.keras.layers.Flatten()(rel)\n out = tf.keras.layers.Dense(1, )(flat)\n model = tf.keras.models.Model(inputs=inp, outputs=out)\n \n model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n model.fit(x=np.random.uniform(size=(4, 84, 84, 3)), y=np.random.uniform(size=(4, 1)), epochs=1)\n model.evaluate(x=np.random.uniform(size=(3, 84, 84, 3)), y=np.random.uniform(size=(3, 1)))\n model.predict(x=np.random.uniform(size=(1, 84, 84, 3)))\n </denchmark-code>\n \n Other info / logs Include any logs or source code that would be helpful to\n diagnose the problem. If including tracebacks, please include the full\n traceback. Large logs and files should be attached.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-12T12:15:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>\n , I have tried to reproduce issue. But I have faced no issue in this code. For you reference link of gist is <denchmark-link:https://gist.github.com/khimraj/25f451b7abaade36dd5390fdabb5a935>here</denchmark-link>\n .\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-12T16:21:07Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>\n , I guess you have to update to 2.2.0-dev20200412 (tf-nightly) and then this happens.\n In previous versions, this does not cause an error, and that is why I think it might be a bug.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-12T16:34:01Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>\n , I have used tensorflow version 2.2.0-rc2.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-12T16:49:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>\n  Sorry, I am not very familiar with versioning. Is rc2 newer than nightly build?\n I mean if this is not resolved, will this be the behavior in TF 2.2.1 or TF 2.3.x?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-13T12:34:11Z", "comment_text": "\n \t\tWas able to run the code without any issues on <denchmark-link:https://colab.research.google.com/gist/amahendrakar/2ad5bccc80083753db0f6657c83ef1ab/38459.ipynb>TF v2.2.0-rc2</denchmark-link>\n .\n Facing an error stating , while running on <denchmark-link:https://colab.research.google.com/gist/amahendrakar/e85b2e37339e4c478b666b53edab264f/38459-tf-nightly.ipynb#scrollTo=aDTL7bbKf82y>TF-nightly</denchmark-link>\n . Please find the attached gist. Thanks!\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-13T15:00:19Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>\n , <denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>\n   , I think  tf-nightly-2.2.0-dev20200412 is the latest build(12/04/2020) than tf-2.2.0-rc2(28/03/2020) . Generally it should not throw an error since if is_training is true  that implies whitening is done on the same input batch rather than over the moving average statistic of the set.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-13T15:14:22Z", "comment_text": "\n \t\tHello <denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>\n , Sorry, I did not get what you mean by whitening. I checked and when training is False, there is no issue. The only problem is when is_training is True and if I understand you correctly, this is a bug and should be fixed.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-13T15:33:22Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>\n  ,by whitening I mean using the mean as 0 and sd/variance as 1 before passing an input vector xi {1,2...m} through an activation unit .I think according to the implementation there is mention of a batch normalised vector value X = x-E[x]/sqrt(var(x) + epsilon) where E[x] is the expectation and epsilon is just added to prevent division by 0. There are other parameters gamma and beta which govern the gradient step (scale and shift rule).During training as false there is a moving average over the input vector of previous layers which is \"whitened\" by applying the above metric and then passed into the activation units  - relu sigmoid etc of the current layer. Yes you are correct this should not be occurring if is_training is true because in that case there is no need of such moving statistic over previous layers,input  batch of current state is whitened and used.If mean and variance are none in this case(training=true) then it is not possible to determine the normalised value which is to be passed into the activation unit.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-13T15:45:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>\n  Thank you for your explanation.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-13T15:57:12Z", "comment_text": "\n \t\tSure <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>\n  , glad I could help.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-16T13:49:46Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>\n  , <denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>\n  ,<denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>\n  ,<denchmark-link:https://github.com/amahendrakar>@amahendrakar</denchmark-link>\n  in the latest nightly release- tf-nightly 2.2.0.dev20200416 the issue is present. I found that according to this commit :<denchmark-link:https://github.com/tensorflow/tensorflow/commit/84f2ec1d60b5bb14a59ccef8f8fa7eb5a1096e8f#diff-ef8609a43751227afcaacc838670a96f>84f2ec1#diff-ef8609a43751227afcaacc838670a96f</denchmark-link>\n     on  <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py</denchmark-link>\n   ,there is mention that if exponential_avg_factor==1.0and is _training ==true ,then the mean and variance should be none. By comparing commits , I found that since is_training  considers current batch inputs,by setting the exponential average to 1 would imply effective mean =0 and variance=0 (mean=(1-exponential_average_factor)exponential_average_factor) and same for variance). However, I think this requires further analysis since  momentum can be 0 on a current input batch and this should not raise exception on mean and variance values since it only belongs to the current batch.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-16T14:12:32Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>\n \n I wanted to ask a question. When we set the momentum to zero during training=True, we expect the moving mean and moving variance to be updated based on just the latest batch the model sees. Is that correct?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-16T14:53:55Z", "comment_text": "\n \t\tYes <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>\n  , that is correct .\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "siavash-khodadadeh", "commentT": "2020-04-16T15:12:43Z", "comment_text": "\n \t\tThank you for your response, <denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>\n . Just wanted to make sure that I understood it accurately.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "siavash-khodadadeh", "commentT": "2020-06-08T18:45:07Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38459>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38459>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "3cfba9571bcc4be237bfdfa3498c66073ae59280", "commit_author": "Scott Zhu", "commitT": "2020-06-08 11:36:54-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow\\python\\ops\\nn_impl.py", "file_new_name": "tensorflow\\python\\ops\\nn_impl.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "1618,1619,1620,1621,1622", "deleted_lines": "1618,1619,1620,1621,1622,1623,1624,1625,1626,1627"}}}}}}