{"BR": {"BR_id": "29989", "BR_author": "whhu", "BRopenT": "2019-06-20T02:46:50Z", "BRcloseT": "2019-07-17T03:38:32Z", "BR_text": {"BRsummary": "Segmentation fault when saving checkpoints with saveable Dataset Iterator", "BRdescription": "\n System information\n \n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n \n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6.1810\n \n \n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\n \n \n TensorFlow installed from (source or binary): Binary\n \n \n TensorFlow version (use command below): 1.13.1\n \n \n Python version: 3.6.8\n \n \n Bazel version (if compiling from source): None\n \n \n GCC/Compiler version (if compiling from source): None\n \n \n CUDA/cuDNN version: None\n \n \n GPU model and memory: None\n \n \n tf.version: 'v1.13.1-0-g6612da8951' 1.13.1\n \n \n Describe the current behavior\n Segmentation fault in saving an initializable dataset iterator when entering the tf.train.MonitoredSession context manager.\n Describe the expected behavior\n The initializable iterator is saved and restored properly, behaving the same with the one shot iterator.\n Code to reproduce the issue\n \"\"\"Illustrate saveable dataset iterator\n \"\"\"\n import tensorflow as tf\n \n DATASET_SIZE = 4\n SAVE_STEPS = 2\n TRAIN_STEP = 3\n CHECKPOINT_DIR = '/tmp/tf_dataset_saveable'\n \n def test_saveable():\n     \"\"\"test saveable\"\"\"\n     graph = tf.Graph()\n     with graph.as_default():\n         dataset = tf.data.Dataset.range(DATASET_SIZE).repeat()\n #        dataset_iterator = dataset.make_one_shot_iterator()\n         dataset_iterator = dataset.make_initializable_iterator()\n         dataset_init = dataset_iterator.initializer\n         data = dataset_iterator.get_next()\n \n         saveable = tf.contrib.data.make_saveable_from_iterator(dataset_iterator)\n         tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\n \n         global_step = tf.train.get_or_create_global_step()\n         inc_global_step = tf.assign_add(global_step, 1)  # critical\n \n         saver = tf.train.Saver()\n         checkpoint_dir = CHECKPOINT_DIR\n         scaffold = tf.train.Scaffold(saver=saver)\n         checkpoint_hook = tf.train.CheckpointSaverHook(\n             checkpoint_dir=checkpoint_dir,\n             save_steps=SAVE_STEPS, scaffold=scaffold)\n \n         hooks = [checkpoint_hook]\n         session_creator = tf.train.ChiefSessionCreator(\n             scaffold=scaffold, checkpoint_dir=checkpoint_dir)\n         with tf.train.MonitoredSession(\n                 session_creator=session_creator, hooks=hooks) as mon_sess:\n             gstep = mon_sess.run(global_step)\n             if not gstep:\n                 mon_sess.run(dataset_init)\n             for _ in range(TRAIN_STEP):\n                 print(mon_sess.run([global_step, data]))\n                 mon_sess.run(inc_global_step)\n \n if __name__ == '__main__':\n     test_saveable()\n Other info / logs\n (tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_dataset_saveable.py \n WARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n Instructions for updating:\n Colocations handled automatically by placer.\n \n WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n For more information, please see:\n   * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n   * https://github.com/tensorflow/addons\n If you depend on functionality not listed there, please file an issue.\n \n WARNING:tensorflow:From tf_dataset_saveable.py:20: make_saveable_from_iterator (from tensorflow.contrib.data.python.ops.iterator_ops) is deprecated and will be removed in a future version.\n Instructions for updating:\n Use `tf.data.experimental.make_saveable_from_iterator(...)`.\n 2019-06-20 10:43:20.947675: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n 2019-06-20 10:43:20.951984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\n 2019-06-20 10:43:20.952497: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3f10ca0 executing computations on platform Host. Devices:\n 2019-06-20 10:43:20.952539: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n Segmentation fault (core dumped)\n (tf-1.13-py3) [huwh1@huwh1-centos worksync]$ \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "whhu", "commentT": "2019-06-21T09:45:11Z", "comment_text": "\n \t\tI tried on colab with Tensorflow 1.13.1. I am able to reproduce the issue.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "whhu", "commentT": "2019-07-03T17:15:49Z", "comment_text": "\n \t\tallenl@ -- not sure if these APIs are expected to work all together. Can you advise as to whether there's a better way?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "whhu", "commentT": "2019-07-03T17:18:33Z", "comment_text": "\n \t\tOops. Actually tagging <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "whhu", "commentT": "2019-07-03T17:36:23Z", "comment_text": "\n \t\tIs the issue that the iterator isn't initialized when the first checkpoint is written? You may need to get MonitoredTrainingSession to do the initialization so it happens before saving. Otherwise the APIs should go fine together AFAIK.\n Either way, it probably shouldn't segfault. <denchmark-link:https://github.com/saxenasaurabh>@saxenasaurabh</denchmark-link>\n  may be more familiar with the serialization op itself.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "whhu", "commentT": "2019-07-15T21:52:19Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>\n  is correct, you need to run the  before you can save the iterator.\n I am going to create a change that will produce an informative error message for this case (as opposed to segfaulting).\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "whhu", "commentT": "2019-07-16T00:30:26Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29989>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29989>No</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "whhu", "commentT": "2019-07-16T07:25:51Z", "comment_text": "\n \t\tMany thanks to the fruitful discussions. Adding dataset initializer to the collection TABLE_INITIALIZERS fixes the segfaulting. Nevertheless, the iterator starts from the very beginning of the dataset every time after recovering from the checkpoint.\n Is there any solution to the problem at present? Maybe something like init_fn in tf.train.Scaffold ...\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "whhu", "commentT": "2019-07-16T17:44:35Z", "comment_text": "\n \t\tThat should not be the case. The whole point of saving the iterator is that you can checkpoint its state (and we have tests that verify that this functionality works).\n Please create a new issue with instructions on how to reproduce your issue so that someone can investigate why is your program resetting the iterator state.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "whhu", "commentT": "2019-07-16T18:10:07Z", "comment_text": "\n \t\tI think this is a missing feature. We added <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/data/CheckpointInputPipelineHook>CheckpointInputPipelineHook</denchmark-link>\n  to fix exactly this <denchmark-link:https://github.com/tensorflow/tensorflow/blob/5e2a91f65cfb23f996136b9201d9312c9c36b941/tensorflow/python/data/experimental/ops/iterator_ops.py#L194>problem</denchmark-link>\n . However, it requires an estimator as an arg. It doesn't necessarily have to. We just did it this way for simplicity.\n It should be fairly straightforward to extend  CheckpointInputPipelineHook to support non-estimator use-cases e.g. by explicitly passing the required args i.e. num_worker_replicas, task_type, task_id, model_dir , save_checkpoints_secs, save_checkpoints_steps. I would be happy to review if you want to send in a PR.\n As a really hacky workaround you could just build a mock Estimator object that implements the expected fields. That may be prone to breakages though.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "whhu", "commentT": "2019-07-17T03:38:32Z", "comment_text": "\n \t\tThank you very much. It helps greatly! :-)\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "whhu", "commentT": "2019-07-17T03:38:33Z", "comment_text": "\n \t\tAre you satisfied with the resolution of your issue?\n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29989>Yes</denchmark-link>\n \n <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29989>No</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "51d1f486fcbe5bc8857586250fd5b10ca110d631", "commit_author": "Jiri Simsa", "commitT": "2019-07-15 17:28:29-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\data\\iterator_ops.cc", "file_new_name": "tensorflow\\core\\kernels\\data\\iterator_ops.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "110", "deleted_lines": "110", "method_info": {"method_name": "tensorflow::data::IteratorResource::Save", "method_params": "ctx,writer", "method_startline": "104", "method_endline": "118"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\data\\experimental\\kernel_tests\\serialization\\serialization_integration_test.py", "file_new_name": "tensorflow\\python\\data\\experimental\\kernel_tests\\serialization\\serialization_integration_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "84,85,86,87,88,89,90,91", "deleted_lines": null, "method_info": {"method_name": "testUninitializedIterator", "method_params": "self", "method_startline": "84", "method_endline": "91"}}}}}}}