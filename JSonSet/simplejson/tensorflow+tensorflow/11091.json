{"BR": {"BR_id": "11091", "BR_author": "anishathalye", "BRopenT": "2017-06-27T20:57:44Z", "BRcloseT": "2017-07-17T16:04:54Z", "BR_text": {"BRsummary": "tf.nn.elu: incorrect second derivative", "BRdescription": "\n <denchmark-h:h3>System information</denchmark-h>\n \n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\n TensorFlow installed from (source or binary): binary\n TensorFlow version (use command below): 1.2.0\n Bazel version (if compiling from source): N/A\n CUDA/cuDNN version: CUDA 8.0 / cuDNN 6.0\n GPU model and memory: GTX 1080 Ti 11GB\n Exact command to reproduce: see below\n \n tf.nn.elu gives incorrect second derivatives:\n Consider the graph y = 2 * elu(-x).\n x = tf.placeholder(tf.float32, ())\n y = 2 * tf.nn.elu(-x)\n We'll be evaluating at x=1:\n x_ = 1\n We can evaluate first derivatives with automatic differentiation:\n dy, = tf.gradients(y, x)\n dy.eval({x: x_})\n => -0.7357589\n This lines up with the analytic answer: y' = -2e^(-x)\n However, for the second derivative:\n ddy, = tf.gradients(dy, x)\n ddy.eval({x: x_})\n => 0.36787945\n Whoops, this doesn't look right! Analytically, the derivative is y'' = 2e^(-x). Evaluated at x=1, this is 0.7357588!\n <denchmark-h:h3>Workaround</denchmark-h>\n \n Just in case anyone else needs to work around this until it's fixed:\n def elu(x):\n     return tf.where(x >= 0.0, x, tf.exp(x) - 1)\n Looks like second derivatives work with that.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "anishathalye", "commentT": "2017-06-27T20:59:19Z", "comment_text": "\n \t\tPossibly related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/7403>#7403</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "anishathalye", "commentT": "2017-06-29T18:31:13Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alextp>@alextp</denchmark-link>\n  do you know why this might be?\n <denchmark-link:https://github.com/anishathalye>@anishathalye</denchmark-link>\n  is it possible to make a simpler repro script that you can share?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "anishathalye", "commentT": "2017-06-29T18:37:31Z", "comment_text": "\n \t\tCan you use the tensorflow gradient checker to test the gradient of your particular graph? See <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradient_checker.py>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradient_checker.py</denchmark-link>\n  and usages of it in the tensorflow tests.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "anishathalye", "commentT": "2017-07-05T22:15:30Z", "comment_text": "\n \t\tOkay, so I tried the following:\n I have a network y(x), where y contains no tf.gradient ops, and I checked:\n \n \n tf.check_numerics(tf.gradients(y, x)[0]) -- result is ok\n \n \n tf.test.compute_gradient_error(x, ..., y, ...) -- result is ~ 983 (is that okay? in any case, for training, first derivatives seem to work)\n \n \n tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0]) -- result is ok\n \n \n tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...) - result is ~ 1522423936\n \n \n Finite differences probably isn't producing great results because y is a fairly big graph. But still, having a maximum error of 1e9 seems kind of large.\n What do you suggest looking into next?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "anishathalye", "commentT": "2017-07-05T22:30:59Z", "comment_text": "\n \t\tThis is really hard to debug without having access to the full graph.\n \n What I'd do if I did have access  to the full graph would be bisect it;\n removing chunks of graph at a time until the error in second derivative\n goes down to see if there's some part of the code which is less numerically\n stable than it should be.\n <denchmark-link:#>\u2026</denchmark-link>\n \n \n On Wed, Jul 5, 2017 at 3:17 PM, Anish Athalye ***@***.***> wrote:\n  Okay, so I tried the following:\n \n  I have a network y(x), where y contains no tf.gradient ops, and I checked:\n \n     1.\n \n     tf.check_numerics(tf.gradients(y, x)[0]) -- result is ok\n     2.\n \n     tf.test.compute_gradient_error(x, ..., y, ...) -- result is ~ 983 (is\n     that okay? in any case, for training, first derivatives seem to work)\n     3.\n \n     tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0]) -- result\n     is ok\n     4.\n \n     tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...) -\n     result is ~ 1522423936\n \n  Finite differences probably isn't producing great results because y is a\n  fairly big graph. But still, having a maximum error of 1e9 seems kind of\n  large.\n \n  What do you suggest looking into next?\n \n  \u2014\n  You are receiving this because you were mentioned.\n  Reply to this email directly, view it on GitHub\n  <#11091 (comment)>,\n  or mute the thread\n  <https://github.com/notifications/unsubscribe-auth/AAATxdLiF38xxI13fIJsHsvl7rooT44Cks5sLAuTgaJpZM4OHLlJ>\n  .\n \n \n -- \n  - Alex\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "anishathalye", "commentT": "2017-07-06T23:42:52Z", "comment_text": "\n \t\tOk, I have some more evidence indicating that there is in fact a bug. Also, I can share the full graph with you.\n I don't want to post it publicly, so I've sent you an email with this additional information.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "anishathalye", "commentT": "2017-07-07T21:57:16Z", "comment_text": "\n \t\tI found the bug: I updated the original post.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "anishathalye", "commentT": "2017-07-17T16:05:06Z", "comment_text": "\n \t\tThe commit from last week should have fixed this.\n \t\t"}}}, "commit": {"commit_id": "e121535a7d04cfc7c7dbb09d8694c01eb29da26f", "commit_author": "Alexandre Passos", "commitT": "2017-07-10 12:52:34-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tensorflow\\python\\kernel_tests\\relu_op_test.py", "file_new_name": "tensorflow\\python\\kernel_tests\\relu_op_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "276,277,278,279,280,281,282,283,284,285", "deleted_lines": null, "method_info": {"method_name": "testGradGrad", "method_params": "self", "method_startline": "276", "method_endline": "285"}}, "hunk_1": {"Ismethod": 1, "added_lines": "36,37,38,39", "deleted_lines": null, "method_info": {"method_name": "_elu_grad_grad", "method_params": "activation", "method_startline": "36", "method_endline": "39"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\python\\ops\\nn_grad.py", "file_new_name": "tensorflow\\python\\ops\\nn_grad.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "330,332,333,334,335", "deleted_lines": "330,332,333,334", "method_info": {"method_name": "_EluGradGrad", "method_params": "op,grad", "method_startline": "329", "method_endline": "335"}}}}}}}