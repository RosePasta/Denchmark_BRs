{"BR": {"BR_id": "23924", "BR_author": "w4-sjcho", "BRopenT": "2018-11-22T14:45:32Z", "BRcloseT": "2018-11-26T19:11:44Z", "BR_text": {"BRsummary": "Memory leak when using tf.contrib.data.unbatch()", "BRdescription": "\n System information\n \n Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\n Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n TensorFlow installed from (source or binary): binary\n TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\n Python version: Python 3.6.7 :: Anaconda, Inc.\n Bazel version (if compiling from source):\n GCC/Compiler version (if compiling from source):\n CUDA/cuDNN version: 9.0\n GPU model and memory: 1080ti\n \n Describe the current behavior\n Memory usage continuously increase when using tf.contrib.data.unbatch().\n Describe the expected behavior\n Memory usage should not increase.\n Code to reproduce the issue\n <denchmark-code>from absl import app\n from absl import flags\n from absl import logging\n import tensorflow as tf\n \n \n FLAGS = flags.FLAGS\n flags.DEFINE_integer('epochs', 1000, '')\n flags.DEFINE_boolean('use_unbatch', False, '')\n \n \n def create_dataset(input_holder):\n     dataset = tf.data.Dataset.from_tensor_slices((input_holder,))\n \n     def generate_random_tensor(size):\n         return tf.random_uniform([5, size, size], dtype=tf.float32)\n \n     dataset = dataset.map(generate_random_tensor)\n     if FLAGS.use_unbatch:\n         dataset = dataset.apply(tf.contrib.data.unbatch())\n     else:\n         dataset = dataset.flat_map(\n             lambda x: tf.data.Dataset.from_tensor_slices((x,)))\n         # The output of the dataset becomes a single-element tuple w/o this.\n         dataset = dataset.map(lambda x: x)\n     return dataset\n \n \n def main(_):\n     with tf.Session() as sess:\n         size_holder = tf.placeholder(tf.int32, shape=[None])\n         dataset = create_dataset(size_holder)\n \n         iterator = dataset.make_initializable_iterator()\n         get_next = iterator.get_next()\n \n         for i in range(FLAGS.epochs):\n             logging.info('Epoch #%d', i)\n             sess.run(iterator.initializer, feed_dict={\n                 size_holder: [1000 + (i % 100)],\n             })\n             try:\n                 while True:\n                     array = sess.run(get_next)\n                     logging.info('  Generated: %s', array.shape)\n             except tf.errors.OutOfRangeError:\n                 pass\n \n \n if __name__ == '__main__':\n     app.run(main)\n </denchmark-code>\n \n Memory usage will increase with --use_unbatch, while with --nouse_unbatch, memory usage does not increase.\n Other info / logs\n It seems like  call is missing in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/unbatch_dataset_op.cc#L42>UnbatchDatasetOp</denchmark-link>\n .\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "w4-sjcho", "commentT": "2018-11-22T16:13:45Z", "comment_text": "\n \t\tPossibly related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/23904>#23904</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "w4-sjcho", "commentT": "2018-11-26T18:43:15Z", "comment_text": "\n \t\tThanks for tracking down the issue: indeed, the missing input_->Unref() seems to be the culprit.\n <denchmark-link:https://github.com/artsobolev>@artsobolev</denchmark-link>\n  I think the root cause for this bug is different. I'll comment on the other thread.\n \t\t"}}}, "commit": {"commit_id": "bb425754adacc784c2ad50ed94307eaa03626b41", "commit_author": "Derek Murray", "commitT": "2018-11-26 11:08:59-08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow\\core\\kernels\\data\\unbatch_dataset_op.cc", "file_new_name": "tensorflow\\core\\kernels\\data\\unbatch_dataset_op.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "57", "deleted_lines": null, "method_info": {"method_name": "tensorflow::data::UnbatchDatasetOp::Dataset::~Dataset", "method_params": "", "method_startline": "57", "method_endline": "57"}}}}}}}