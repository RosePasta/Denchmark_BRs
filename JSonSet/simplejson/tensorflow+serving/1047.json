{"BR": {"BR_id": "1047", "BR_author": "tobegit3hub", "BRopenT": "2018-08-10T10:40:31Z", "BRcloseT": "2018-08-21T22:03:06Z", "BR_text": {"BRsummary": "RESTful server checks consistency of batch size which breaks the model with arbitrary input or output shape", "BRdescription": "\n <denchmark-h:h2>Bug Report</denchmark-h>\n \n <denchmark-h:h3>System information</denchmark-h>\n \n \n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\n TensorFlow Serving installed from (source or binary): binary\n TensorFlow Serving version: 1.8.0\n \n <denchmark-h:h3>Describe the problem</denchmark-h>\n \n The RESTful server will check the consistency of batch size which may requires all the input and output tensor to support \"batch size\". In fact, our models may output the tensor which does not have batch size. These models work like a charm with TensorFlow Serving gRPC APIs but not TensorFlow Serving RESTful APIs.\n <denchmark-h:h3>Exact Steps to Reproduce</denchmark-h>\n \n This is easy to re-produce and we can export the example SavedModel with these script. The code to break RESTful server is constant_op = tf.constant([1.0, 2.0]) which is a tensor with shape [2].\n <denchmark-code>import os\n import tensorflow as tf\n from tensorflow.python.saved_model import builder as saved_model_builder\n from tensorflow.python.saved_model import (\n     signature_constants, signature_def_utils, tag_constants, utils)\n from tensorflow.python.util import compat\n \n model_path = \"model\"\n model_version = 1\n \n keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1], name=\"keys\")\n keys_identity = tf.identity(keys_placeholder, name=\"inference_keys\")\n \n constant_op = tf.constant([1.0, 2.0])\n \n sess = tf.Session()\n sess.run(tf.global_variables_initializer())\n \n model_signature = signature_def_utils.build_signature_def(\n     inputs={\n         \"keys\": utils.build_tensor_info(keys_placeholder),\n     },\n     outputs={\n         \"keys\": utils.build_tensor_info(keys_identity),\n         \"constant\": utils.build_tensor_info(constant_op)\n     },\n     method_name=signature_constants.PREDICT_METHOD_NAME)\n \n export_path = os.path.join(\n     compat.as_bytes(model_path), compat.as_bytes(str(model_version)))\n \n builder = saved_model_builder.SavedModelBuilder(export_path)\n builder.add_meta_graph_and_variables(\n     sess, [tag_constants.SERVING],\n     clear_devices=True,\n     signature_def_map={\n         signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: model_signature,\n     })\n \n builder.save()\n </denchmark-code>\n \n Then load the model with TensorFlow Serving while exposing both gRPC and RESTful APIs. Remember to change the absolute path of your model.\n <denchmark-code>tensorflow_model_server --port=8501 --rest_api_port=8502 --model_base_path=\"/foo/bar/model/\"\n </denchmark-code>\n \n Then request the RESTful APIs with simple curl command.\n <denchmark-code>curl -H \"Content-Type: application/json\" -X POST -d '{\"instances\": [{\"keys\": [[1]]}]}' http://127.0.0.1:8502/v1/models/default:predict\n </denchmark-code>\n \n This will get the error message of { \"error\": \"Tensor name: keys has inconsistent batch size: 1 expecting: 2\" }. If we change to code of model signature with constant_op = tf.constant([1.0]), this will work because our request just have the batch size of [1].\n And we have implement the gRPC client to make sure the model is normal. Here is the code of the gRPC client for this mode.\n <denchmark-code>import time\n import numpy\n import tensorflow as tf\n from grpc.beta import implementations\n from tensorflow_serving.apis import predict_pb2, prediction_service_pb2\n \n def main():\n   host = \"0.0.0.0\"\n   port = 8501\n   model_name = \"default\"\n   model_version = -1\n   signature_name = \"\"\n   request_timeout = 10.0\n \n   # Generate inference data\n   keys = numpy.asarray([[1]])\n   keys_tensor_proto = tf.contrib.util.make_tensor_proto(keys, dtype=tf.int32)\n \n   # Create gRPC client\n   channel = implementations.insecure_channel(host, port)\n   stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n   request = predict_pb2.PredictRequest()\n   request.model_spec.name = model_name\n   if model_version > 0:\n     request.model_spec.version.value = model_version\n   if signature_name != \"\":\n     request.model_spec.signature_name = signature_name\n   request.inputs[\"keys\"].CopyFrom(keys_tensor_proto)\n \n   # Send request\n   result = stub.Predict(request, request_timeout)\n   print(result)\n \n if __name__ == \"__main__\":\n   main()\n </denchmark-code>\n \n And this gRPC client will work and output the expected result.\n <denchmark-code>outputs {\n   key: \"constant\"\n   value {\n     dtype: DT_FLOAT\n     tensor_shape {\n       dim {\n         size: 2\n       }\n     }\n     float_val: 1.0\n     float_val: 2.0\n   }\n }\n outputs {\n   key: \"keys\"\n   value {\n     dtype: DT_INT32\n     tensor_shape {\n       dim {\n         size: 1\n       }\n       dim {\n         size: 1\n       }\n     }\n     int_val: 1\n   }\n }\n model_spec {\n   name: \"default\"\n   version {\n     value: 1\n   }\n   signature_name: \"serving_default\"\n }\n </denchmark-code>\n \n <denchmark-h:h3>Source code / logs</denchmark-h>\n \n Above.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tobegit3hub", "commentT": "2018-08-10T10:56:27Z", "comment_text": "\n \t\tThe root  cause is in <denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/util/json_tensor.cc#L819>https://github.com/tensorflow/serving/blob/master/tensorflow_serving/util/json_tensor.cc#L819</denchmark-link>\n  . It requires all the named tensors to have the same first dimension.\n It is quite unreasonable and inconvenient. In one of our scenarios, we generated the vocabulary(hash table) to map label index and string names and export it as part of the model. This vocabulary obviously does not have the same first dimension as the request data.\n This is not the consistent interfaces of the original TensorFlow Serving gRPC APIs and hope this can change.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tobegit3hub", "commentT": "2018-08-10T23:01:44Z", "comment_text": "\n \t\tThanks for the detailed report. The intent of the code was to keep the input\n and output symmetric. IOW input tensors for each feature, all need to have\n the same batch size. Clearly this does not apply for output.\n If you can prepare a PR with the fix i'd be happy to review!\n Thanks again.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tobegit3hub", "commentT": "2018-08-13T03:09:45Z", "comment_text": "\n \t\tThanks for your response <denchmark-link:https://github.com/netfs>@netfs</denchmark-link>\n  . I can submit the PR to check input tensor only.\n But is it possible for users to specify different input shapes? For example, some developers may want to restrict the number of top-N prediction and add [tf.int32] placeholder in SavedModel which has different shape of other input tensors. I have test with the following code to construct model with different-shape inputs and it is supported in TensorFlow Serving.\n <denchmark-code>keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1], name=\"keys\")\n another_keys_placeholder = tf.placeholder(tf.int32, shape=[3], name=\"another_keys\")\n keys_identity = tf.identity(keys_placeholder, name=\"inference_keys\")\n constant_op = tf.constant([1.0, 2.0])\n \n sess = tf.Session()\n sess.run(tf.global_variables_initializer())\n \n model_signature = signature_def_utils.build_signature_def(\n     inputs={\n         \"keys\": utils.build_tensor_info(keys_placeholder),\n         \"another_keys\": utils.build_tensor_info(another_keys_placeholder),\n     },\n     outputs={\n         \"keys\": utils.build_tensor_info(keys_identity),\n         \"constant\": utils.build_tensor_info(constant_op)\n     },\n     method_name=signature_constants.PREDICT_METHOD_NAME)\n </denchmark-code>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "tobegit3hub", "commentT": "2018-08-14T18:03:01Z", "comment_text": "\n \t\ti see what you mean, and its seems plausible that models requiring inputs\n each with a different batch size. though this presents a problem expressing\n such values in JSON requests.\n the JSON request is list of rows of individual inputs, that we internally columnize\n (and where the present checks are tripping your usage). we expect each row\n of the input to  have all named inputs. so a request typically looks as follows\n (with a batch_size of 2, for all inputs):\n <denchmark-code>{\n   \"instances\": [ \n     {\n        \"tag\": [\"foo\"]\n        \"signal\": [1, 2, 3, 4, 5]\n        \"sensor\": [[1, 2], [3, 4]]\n     },\n     {\n        \"tag\": [\"bar\"]\n        \"signal\": [3, 4, 1, 2, 5]]\n        \"sensor\": [[4, 5], [6, 8]]\n     },\n  ] \n }\n </denchmark-code>\n \n assuming we had another input say \"location_id\", that has batch_size=1, how\n would such a request look? one way would be:\n <denchmark-code>{\n   \"instances\": [ \n     {\n        \"tag\": [\"foo\"]\n        \"signal\": [1, 2, 3, 4, 5]\n        \"location_id\": \"baz\"\n     },\n     {\n        \"tag\": [\"bar\"]\n        \"signal\": [3, 4, 1, 2, 5]]\n         // missing: \"location_id\"\n     },\n  ] \n }\n </denchmark-code>\n \n (note missing \"location_id\" in the second element of instances list)\n this can get confusing (or maybe not :-) seeing missing named input\n in some rows. we could choose to ignore such missing values, and\n let the model processing complain if the batch size is not correct\n (it would). i think this should be ok. let me think about this and get\n back.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "tobegit3hub", "commentT": "2018-08-15T07:34:01Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/netfs>@netfs</denchmark-link>\n  for the detailed explaination.\n Actually we know about this limitation after reading the format of JSON requests. Since we separate batch data into items in instances array, it requires each item of instances has the same shape. Otherwise, passing optional parameters in different items is really confusing.\n Because both the inputs and outputs of TensorFlow models are , have you considerated using the following format? It is much more compact which may improves performance and more similar with . We have another RESTful serving called <denchmark-link:https://github.com/tobegit3hub/simple_tensorflow_serving>simple_tensorflow_serving</denchmark-link>\n  which uses this format and be compatible with all TensorFlow SavedModels.\n <denchmark-code>{\n   \"instances\": {\n     \"tag\": [\"foo\", \"bar\"],\n     \"signal\": [[1, 2, 3, 4, 5], [3, 4, 1, 2, 5]],\n     \"sensor\": [[[1, 2], [3, 4]], [[4, 5], [6, 8]]]\n   }\n }\n </denchmark-code>\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "tobegit3hub", "commentT": "2018-08-15T20:01:45Z", "comment_text": "\n \t\tyes columnar format is what i am presently leaning towards (maybe under a different key called \"inputs\").\n additionally we need to make sure if batching is enabled:\n <denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/serving_advanced.md#batching>https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/serving_advanced.md#batching</denchmark-link>\n \n it degrades gracefully when we have inputs with differing 0-th dimension (i.e. batch sizes).\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "tobegit3hub", "commentT": "2018-08-15T23:18:42Z", "comment_text": "\n \t\tMy current proposal is as follows:\n \n \n Continue to support the current row-format of instances and predictions (as the input/output) with the restriction that 0-th dimension of all inputs/outputs should be same (read: consistent batch size). This format is easier to read for common use-cases.\n \n \n In addition to above row-format, we allow a new column-format style (similar to your proposal) with inputs key in request and outputs key in response. These keys would hold values in column-format. This would be very similar to the gRPC predict API. This would make your request look as follows:\n \n \n <denchmark-code>{\n   \"inputs\": {\n     \"tag\": [\"foo\", \"bar\"],\n     \"signal\": [[1, 2, 3, 4, 5], [3, 4, 1, 2, 5]],\n     \"sensor\": [[[1, 2], [3, 4]], [[4, 5], [6, 8]]]\n   }\n }\n </denchmark-code>\n \n and response will be:\n <denchmark-code>{\n   \"outputs\": {\n     \"constant\": ...,\n     \"keys\": ...,\n   }\n }\n </denchmark-code>\n \n \n We do not allow mixing these two formats in the same request. And response format will match with that of input. So you get predictions for instances (row-format) and outputs for inputs (column-format).\n \n This will allow users to choose suitable format to express their input/output for their models.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "tobegit3hub", "commentT": "2018-08-20T08:24:05Z", "comment_text": "\n \t\tGreat and looking forward to the new column-format \ud83d\udc4d\n Thanks <denchmark-link:https://github.com/netfs>@netfs</denchmark-link>\n  . This issue may be closed if we have supported the  for arbitrary input or output shapes.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "tobegit3hub", "commentT": "2018-08-20T21:52:35Z", "comment_text": "\n \t\tthanks for taking a look. i will compose a change to add support for column format and update this thread once the change is ready. will be great if you can help test it, once its ready.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "tobegit3hub", "commentT": "2018-08-24T08:31:12Z", "comment_text": "\n \t\tGreat! The implementation is nice and we are glad to test it.\n There is one problem I found in that commit <denchmark-link:https://github.com/tensorflow/serving/commit/a5b7cfd0e57dcf8af9b6210ef25006f2eb647aad>a5b7cfd</denchmark-link>\n  . The request json data for columnar inputs should be something like  instead of , right?  <denchmark-link:https://github.com/netfs>@netfs</denchmark-link>\n \n <denchmark-code>def testPredictColumnarREST(self):\n     \"\"\"Test Predict implementation over REST API with columnar inputs.\"\"\"\n     model_path = self._GetSavedModelBundlePath()\n     host, port = TensorflowModelServerTest.RunServer('default',\n                                                      model_path)[2].split(':')\n      # Prepare request\n     url = 'http://{}:{}/v1/models/default:predict'.format(host, port)\n     json_req = {'inputs': [2.0, 3.0, 4.0]}\n      # Send request\n     resp_data = None\n     try:\n       resp_data = CallREST('Predict', url, json_req)\n     except Exception as e:  # pylint: disable=broad-except\n       self.fail('Request failed with error: {}'.format(e))\n      # Verify response\n     self.assertEquals(json.loads(resp_data), {'outputs': [3.0, 3.5, 4.0]})\n </denchmark-code>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "tobegit3hub", "commentT": "2018-08-24T18:57:10Z", "comment_text": "\n \t\tif 'constant' is the only named input, then you can skip specifying that,\n if there are more inputs then you need to use the object notation and\n specify each.\n see the unit tests <denchmark-link:https://github.com/tensorflow/serving/commit/a5b7cfd0e57dcf8af9b6210ef25006f2eb647aad#diff-a8a252f963e32d57426b166ad91b1aea>a5b7cfd#diff-a8a252f963e32d57426b166ad91b1aea</denchmark-link>\n  for details.\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "tobegit3hub", "commentT": "2018-08-27T01:45:26Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/netfs>@netfs</denchmark-link>\n  !\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "tobegit3hub", "commentT": "2019-03-13T05:26:14Z", "comment_text": "\n \t\t\n Thanks @netfs for the detailed explaination.\n Actually we know about this limitation after reading the format of JSON requests. Since we separate batch data into items in instances array, it requires each item of instances has the same shape. Otherwise, passing optional parameters in different items is really confusing.\n Because both the inputs and outputs of TensorFlow models are Tensor, have you considerated using the following format? It is much more compact which may improves performance and more similar with TensorFlow Serving gRPC APIs. We have another RESTful serving called simple_tensorflow_serving which uses this format and be compatible with all TensorFlow SavedModels.\n {\n   \"instances\": {\n     \"tag\": [\"foo\", \"bar\"],\n     \"signal\": [[1, 2, 3, 4, 5], [3, 4, 1, 2, 5]],\n     \"sensor\": [[[1, 2], [3, 4]], [[4, 5], [6, 8]]]\n   }\n }\n \n \n When I use columnar format to infer using TF Serving, I get the following error,\n <denchmark-code>{\n     \"error\": \"seq_lens input must be 1-dim, not 2\\n\\t [[{{node model/att_seq2seq/encode/bidi_rnn_encoder/bidirectional_rnn/bw/ReverseSequence}}]]\"\n }\n </denchmark-code>\n \n The model is based on <denchmark-link:https://github.com/google/seq2seq/>seq2seq</denchmark-link>\n  library.\n Does the error mean the model does not support batching?\n Also, the different format of input gives different error. Kindly see <denchmark-link:https://github.com/tensorflow/serving/issues/1269#issuecomment-471994048>here</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "a5b7cfd0e57dcf8af9b6210ef25006f2eb647aad", "commit_author": "awk", "commitT": "2018-08-21 15:02:59-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow_serving\\g3doc\\api_rest.md", "file_new_name": "tensorflow_serving\\g3doc\\api_rest.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "137,141,142,143,144,148,149,150,151,152,153,155,156,157,158,162,184,185,189,190,192,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,236,237,238,239,243,250,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273", "deleted_lines": "137,141,142,146,147,149,150,156,158,159,160,179,180,184,185,187,192,193,197,198,202,209,211,212,213"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_serving\\model_servers\\http_rest_prediction_handler.cc", "file_new_name": "tensorflow_serving\\model_servers\\http_rest_prediction_handler.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "161,168,173", "deleted_lines": "167,172", "method_info": {"method_name": "tensorflow::serving::HttpRestPredictionHandler::ProcessPredictRequest", "method_params": "model_name,model_version,request_body,output", "method_startline": "151", "method_endline": "175"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_serving\\model_servers\\http_rest_prediction_handler_test.cc", "file_new_name": "tensorflow_serving\\model_servers\\http_rest_prediction_handler_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "310,311,312,313,314,315,316,317,318,319,320", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TEST_F", "method_params": "HttpRestPredictionHandlerTest,Predict", "method_startline": "277", "method_endline": "321"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tensorflow_serving\\model_servers\\tensorflow_model_server_test.py", "file_new_name": "tensorflow_serving\\model_servers\\tensorflow_model_server_test.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612", "deleted_lines": null, "method_info": {"method_name": "testPredictColumnarREST", "method_params": "self", "method_startline": "594", "method_endline": "612"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 9, "file_old_name": "tensorflow_serving\\util\\json_tensor.cc", "file_new_name": "tensorflow_serving\\util\\json_tensor.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::FillPredictRequestFromJson", "method_params": "json,get_tensorinfo_map,request,format", "method_startline": "546", "method_endline": "584"}}, "hunk_1": {"Ismethod": 1, "added_lines": "146,147", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::FormatError", "method_params": "val", "method_startline": "143", "method_endline": "148"}}, "hunk_2": {"Ismethod": 1, "added_lines": "883,884,916", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::MakeRowFormatJsonFromTensors", "method_params": "tensor_map,json", "method_startline": "883", "method_endline": "938"}}, "hunk_3": {"Ismethod": 1, "added_lines": "940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::MakeColumnarFormatJsonFromTensors", "method_params": "tensor_map,json", "method_startline": "940", "method_endline": "960"}}, "hunk_4": {"Ismethod": 1, "added_lines": "412,413,414,415,498", "deleted_lines": "412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,451", "method_info": {"method_name": "tensorflow::serving::FillTensorMapFromInstancesList", "method_params": "itr,tensorinfo_map,tensor_map", "method_startline": "412", "method_endline": "500"}}, "hunk_5": {"Ismethod": 1, "added_lines": "502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::FillTensorMapFromInputsMap", "method_params": "itr,tensorinfo_map,tensor_map", "method_startline": "502", "method_endline": "542"}}, "hunk_6": {"Ismethod": 1, "added_lines": "964,965,966,967,968,969,970,971,972,973,974,975,976,977,978", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::MakeJsonFromTensors", "method_params": "tensor_map,format,json", "method_startline": "964", "method_endline": "978"}}, "hunk_7": {"Ismethod": 1, "added_lines": "412,413,414,415,498,502,503,504,505,506,507,508,509,510,511,512", "deleted_lines": "404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,451", "method_info": {"method_name": "tensorflow::serving::FillPredictRequestFromJson", "method_params": "json,get_tensorinfo_map,request", "method_startline": "404", "method_endline": "512"}}, "hunk_8": {"Ismethod": 1, "added_lines": null, "deleted_lines": "813,814,815,816,817,818,850", "method_info": {"method_name": "tensorflow::serving::MakeJsonFromTensors", "method_params": "tensor_map,json", "method_startline": "813", "method_endline": "872"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "tensorflow_serving\\util\\json_tensor.h", "file_new_name": "tensorflow_serving\\util\\json_tensor.h", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "33,34,35,36,37,38,39,40,58,62,63,64,66,67,68,69,70,71,94,107,108,112,113,115,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,150,202,203,205,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,261", "deleted_lines": "50,54,55,56,58,81,94,95,99,100,102,118,170,171,172,173,175,176,177,178,179,180,181,182,183,184,185,192,193,196"}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 20, "file_old_name": "tensorflow_serving\\util\\json_tensor_test.cc", "file_new_name": "tensorflow_serving\\util\\json_tensor_test.cc", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1177", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TYPED_TEST", "method_params": "ClassifyRegressRequestTest,JsonErrors", "method_startline": "1149", "method_endline": "1244"}}, "hunk_1": {"Ismethod": 1, "added_lines": "693,694", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonSingleBytesTensor", "method_startline": "677", "method_endline": "700"}}, "hunk_2": {"Ismethod": 1, "added_lines": "759,760,773,774,783,784", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonSingleTensorErrors", "method_startline": "754", "method_endline": "787"}}, "hunk_3": {"Ismethod": 1, "added_lines": "90,96,100", "deleted_lines": "93", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,SingleUnnamedTensorWithSignature", "method_startline": "84", "method_endline": "110"}}, "hunk_4": {"Ismethod": 1, "added_lines": "861,862,865,866,867,868,869,870,871,872,873,874,875,876,877,878,880,881,882,883,884,885,886,887,888,889,890,891,892", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonMultipleNamedTensors", "method_startline": "789", "method_endline": "893"}}, "hunk_5": {"Ismethod": 1, "added_lines": "745,746", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonSingleFloatTensorNonFinite", "method_startline": "729", "method_endline": "752"}}, "hunk_6": {"Ismethod": 1, "added_lines": "360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411", "deleted_lines": "373,392", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,MultipleNamedTensorColumnarFormat", "method_startline": "360", "method_endline": "411"}}, "hunk_7": {"Ismethod": 1, "added_lines": "495,511,530,551,571", "deleted_lines": "499,521,541,567", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,MultipleNamedTensorErrors", "method_startline": "485", "method_endline": "575"}}, "hunk_8": {"Ismethod": 1, "added_lines": "174,179,182", "deleted_lines": "170", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,SingleUnnamedTensorBase64Lists", "method_startline": "168", "method_endline": "192"}}, "hunk_9": {"Ismethod": 1, "added_lines": "60,65,68", "deleted_lines": "64", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,SingleUnnamedTensor", "method_startline": "54", "method_endline": "82"}}, "hunk_10": {"Ismethod": 1, "added_lines": "149,154,157", "deleted_lines": "147", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,SingleUnnamedTensorBase64Scalars", "method_startline": "143", "method_endline": "166"}}, "hunk_11": {"Ismethod": 1, "added_lines": "419,421,430,434,435,436,437,438,439,440,441,442,447,455,463,471,480", "deleted_lines": "413,433", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,SingleUnnamedTensorErrors", "method_startline": "413", "method_endline": "483"}}, "hunk_12": {"Ismethod": 1, "added_lines": "237,253,257", "deleted_lines": "240,293", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,MultipleNamedTensor", "method_startline": "225", "method_endline": "298"}}, "hunk_13": {"Ismethod": 1, "added_lines": "300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358", "deleted_lines": "302,310,318,326,334,343", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,SingleUnnamedTensorColumnarFormat", "method_startline": "300", "method_endline": "358"}}, "hunk_14": {"Ismethod": 1, "added_lines": "667,668,670,671,672,673,674", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonSingleScalarTensor", "method_startline": "651", "method_endline": "675"}}, "hunk_15": {"Ismethod": 1, "added_lines": "925,926", "deleted_lines": null, "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonMultipleNamedTensorsErrors", "method_startline": "895", "method_endline": "929"}}, "hunk_16": {"Ismethod": 1, "added_lines": "720,721", "deleted_lines": "706,707,708,709,710,711,712,713,714,715,716,717,718,719", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonSingleFloatTensorSixDigitPrecision", "method_startline": "704", "method_endline": "727"}}, "hunk_17": {"Ismethod": 1, "added_lines": "637,638,642,643,644,645,646,647,648", "deleted_lines": "617,626", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,FromJsonSingleTensor", "method_startline": "612", "method_endline": "649"}}, "hunk_18": {"Ismethod": 1, "added_lines": "120,124,127", "deleted_lines": "119", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,TensorFromNonNullTerminatedBuffer", "method_startline": "112", "method_endline": "141"}}, "hunk_19": {"Ismethod": 1, "added_lines": "200,209,212", "deleted_lines": "198", "method_info": {"method_name": "tensorflow::serving::TEST", "method_params": "JsontensorTest,SingleNamedTensorBase64", "method_startline": "194", "method_endline": "223"}}}}}}}