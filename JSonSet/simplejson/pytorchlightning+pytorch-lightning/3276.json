{"BR": {"BR_id": "3276", "BR_author": "s-rog", "BRopenT": "2020-08-31T01:33:53Z", "BRcloseT": "2020-10-13T10:42:12Z", "BR_text": {"BRsummary": "Logging non-tensor scalar with result breaks subsequent epoch aggregation", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n Logging non-tensor scalar with result breaks subsequent epoch/tbptt aggregation\n (on both 0.9 and master)\n <denchmark-code>-- Process 1 terminated with the following error:\n Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 20, in _wrap\n     fn(i, *args)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py\", line 165, in ddp_train\n     results = self.trainer.run_pretrain_routine(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1237, in run_pretrain_routine\n     self.train()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 396, in train\n     self.run_training_epoch()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 543, in run_training_epoch\n     self.run_training_epoch_end(epoch_output, checkpoint_accumulator, early_stopping_accumulator, num_optimizers)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 672, in run_training_epoch_end\n     epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 696, in __auto_reduce_results_on_epoch_end\n     tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py\", line 392, in reduce_across_time\n     result[k] = tbptt_reduce_fx(value)\n TypeError: mean(): argument 'input' (position 1) must be Tensor, not list\n </denchmark-code>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n <denchmark-code>    def training_step(self, batch, batch_idx):\n         x, y = batch[0], batch[1]\n         x = self.forward(x)\n         loss = self.loss(x, y)\n         result = pl.TrainResult(loss)\n         result.log(\"non tensor scalar\", 1.0)\n         result.log(\"loss\", loss, on_step=False, on_epoch=True)\n </denchmark-code>\n \n <denchmark-h:h3>To Fix</denchmark-h>\n \n <denchmark-code>result.log(\"non tensor scalar\", torch.tensor(1.0))\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n In log() of result objects, value should accept non tensor values as value: Any and not cause issues with other metrics to be logged\n <denchmark-h:h3>Additional context</denchmark-h>\n \n log() can be changed to only accept tensors, or have a built-in conversion, will update as I investigate further\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "s-rog", "commentT": "2020-09-01T14:51:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "s-rog", "commentT": "2020-09-15T16:52:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n  is it still there? <denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n  mind check if it is still on master? or better add test for such case...\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "s-rog", "commentT": "2020-09-16T01:26:44Z", "comment_text": "\n \t\tI'll take a look again when I get a chance, haven't probed much due to the refactors... Are they mostly done?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "s-rog", "commentT": "2020-09-16T02:02:51Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  the bug is still here, the following template tests for this issue as well as <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3278>#3278</denchmark-link>\n \n For this problem the easiest fix would be to force type to tensors. Though that's probably just a bandaid solution, thoughts?\n \n Test template for reference\n #!/opt/conda/bin/python\n \"\"\"\n Runs a model on a single node across multiple gpus.\n \"\"\"\n import os\n from argparse import ArgumentParser\n \n import torch.nn.functional as F\n \n import pytorch_lightning as pl\n from pl_examples.models.lightning_template import LightningTemplateModel\n from pytorch_lightning import Trainer, seed_everything\n \n seed_everything(234)\n \n class custom_template(LightningTemplateModel):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n     \n     def on_epoch_start(self):\n         print(\"on_epoch_start\")\n \n     def on_fit_start(self):\n         print(\"on_fit_start\")\n         \n     def training_step(self, batch, batch_idx):\n         \"\"\"\n         Lightning calls this inside the training loop with the data from the training dataloader\n         passed in as `batch`.\n         \"\"\"\n         # forward pass\n         x, y = batch\n         y_hat = self(x)\n         loss = F.cross_entropy(y_hat, y)\n         result = pl.TrainResult(loss)\n         result.log(\"non tensor scalar\", 1.0)\n         result.log(\"loss\", loss, on_step=False, on_epoch=True)\n         return result\n         \n \n def main(args):\n     \"\"\" Main training routine specific for this project. \"\"\"\n     # ------------------------\n     # 1 INIT LIGHTNING MODEL\n     # ------------------------\n     model = custom_template(**vars(args))\n \n     # ------------------------\n     # 2 INIT TRAINER\n     # ------------------------\n     trainer = Trainer.from_argparse_args(args)\n \n     # ------------------------\n     # 3 START TRAINING\n     # ------------------------\n     trainer.fit(model)\n \n \n def run_cli():\n     # ------------------------\n     # TRAINING ARGUMENTS\n     # ------------------------\n     # these are project-wide arguments\n     root_dir = os.path.dirname(os.path.realpath(__file__))\n     parent_parser = ArgumentParser(add_help=False)\n \n     # each LightningModule defines arguments relevant to it\n     parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)\n     parser = Trainer.add_argparse_args(parser)\n     parser.set_defaults(gpus=1, distributed_backend=None)\n     args = parser.parse_args()\n \n     # ---------------------\n     # RUN TRAINING\n     # ---------------------\n     main(args)\n \n \n if __name__ == '__main__':\n     run_cli()\n \n \n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "s-rog", "commentT": "2020-09-17T01:01:44Z", "comment_text": "\n \t\tI looked into it a bit and reduce_across_time is getting called on all metrics if one metric in results is logged with on_epoch=True which makes on_epoch=True only compatible with tensor scalars since the default tbptt fn is torch.mean\n This is probably not intended behavior?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "s-rog", "commentT": "2020-09-17T15:24:59Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>\n  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "s-rog", "commentT": "2020-09-17T15:57:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>\n  this is not related to metrics.\n <denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n  I guess a simple type check that converts scalars to scalars tensor should do the trick? If so, could you open a PR with this fix?\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "s-rog", "commentT": "2020-10-05T07:45:50Z", "comment_text": "\n \t\tfixed by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3855>#3855</denchmark-link>\n \n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "s-rog", "commentT": "2020-10-13T08:16:31Z", "comment_text": "\n \t\t<denchmark-code>Traceback (most recent call last):\n   File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n     fn(i, *args)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_accelerator.py\", line 152, in ddp_train\n     results = self.train_or_test()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 53, in train_or_test\n     results = self.trainer.train()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 483, in train\n     self.train_loop.run_training_epoch()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 598, in run_training_epoch\n     self.num_optimizers\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 339, in log_train_epoch_end_metrics\n     epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py\", line 449, in __auto_reduce_results_on_epoch_end\n     tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py\", line 483, in reduce_across_time\n     result[k] = tbptt_reduce_fx(value.float())\n AttributeError: 'list' object has no attribute 'float'\n </denchmark-code>\n \n <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  I don't think the issue was fixed completely, this is on rc5 (using self.log)\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "s-rog", "commentT": "2020-10-13T09:49:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>\n  mind add a test for this case?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "s-rog", "commentT": "2020-10-13T10:43:22Z", "comment_text": "\n \t\twilliam beat me to it :]\n \t\t"}}}, "commit": {"commit_id": "2d5a7f5e7dc686cfc8172101a81505bf421468af", "commit_author": "William Falcon", "commitT": "2020-10-13 06:42:11-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\step_result.py", "file_new_name": "pytorch_lightning\\core\\step_result.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "479,480,481", "deleted_lines": null, "method_info": {"method_name": "reduce_across_time", "method_params": "cls,time_outputs", "method_startline": "457", "method_endline": "489"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\trainer\\logging\\test_eval_loop_logging_1_0.py", "file_new_name": "tests\\trainer\\logging\\test_eval_loop_logging_1_0.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "24,25,55,71", "deleted_lines": "68", "method_info": {"method_name": "test__validation_step__log", "method_params": "tmpdir", "method_startline": "13", "method_endline": "72"}}, "hunk_1": {"Ismethod": 1, "added_lines": "24,25", "deleted_lines": null, "method_info": {"method_name": "test__validation_step__log.training_step", "method_params": "self,batch,batch_idx", "method_startline": "20", "method_endline": "27"}}}}}}}