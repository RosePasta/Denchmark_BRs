{"BR": {"BR_id": "2286", "BR_author": "hjalmarlucius", "BRopenT": "2020-06-19T21:38:11Z", "BRcloseT": "2020-07-05T11:17:23Z", "BR_text": {"BRsummary": "example_input_array dtype", "BRdescription": "\n Currently assumed that example_input_array dtype to be equal to model dtype. This is not necessarily correct - e.g. if input is a vector of INT.\n \n \n \n pytorch-lightning/pytorch_lightning/core/memory.py\n \n \n          Line 192\n       in\n       7dc58bd\n \n \n \n \n \n \n  input_ = apply_to_collection(input_, torch.Tensor, lambda x: x.type(model.dtype)) \n \n \n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hjalmarlucius", "commentT": "2020-06-23T17:10:20Z", "comment_text": "\n \t\tHi, I don't understand. Does it throw an error or does it display nothing? Could you clarify?\n I don't think we can very accurately define the \"input shape\" for anything other than tensors.\n For this reason we exclude things like dicts from the overview, because it is not very practical to visualize this in a table.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hjalmarlucius", "commentT": "2020-06-24T02:11:09Z", "comment_text": "\n \t\tHi, currently the model is run with input_ as an input. If the model expects a tensor of INTs then it will crash if floats come. I encountered this issue when pretraining an ALBERT-like model. This receives word embeddings as inputs, which have to be integers as they're going into a nn.Embedding\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hjalmarlucius", "commentT": "2020-06-24T13:38:31Z", "comment_text": "\n \t\tokay I see, so we should not change the dtype as given by example_input_array.\n I can't recall why I added this conversion, maybe  it was because of amp and the half conversions. I'll have a closer look, thanks for bringing it up.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "hjalmarlucius", "commentT": "2020-06-24T13:39:20Z", "comment_text": "\n \t\tas a workaround until it is fixed, cast your input to int before feeding to the embedding layer, or don't use the example_input_array.\n \t\t"}}}, "commit": {"commit_id": "6bfcfa8671c4bf54b34290171f191db65fa27d8c", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-07-05 07:17:22-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "26", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\memory.py", "file_new_name": "pytorch_lightning\\core\\memory.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "211", "method_info": {"method_name": "_forward_example_input", "method_params": "self", "method_startline": "204", "method_endline": "227"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 5, "file_old_name": "tests\\core\\test_memory.py", "file_new_name": "tests\\core\\test_memory.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "104,105,106,107,108,109,110,111,112,113,114,115", "deleted_lines": null, "method_info": {"method_name": "test_mixed_dtype_model_summary", "method_params": "", "method_startline": "104", "method_endline": "115"}}, "hunk_1": {"Ismethod": 1, "added_lines": "81,83", "deleted_lines": "88", "method_info": {"method_name": "test_linear_model_summary_shapes", "method_params": "device,mode", "method_startline": "81", "method_endline": "101"}}, "hunk_2": {"Ismethod": 1, "added_lines": "54,55", "deleted_lines": null, "method_info": {"method_name": "forward", "method_params": "self,x", "method_startline": "54", "method_endline": "55"}}, "hunk_3": {"Ismethod": 1, "added_lines": "75,76,77,78,81,83", "deleted_lines": "68,70,88", "method_info": {"method_name": "test_linear_model_summary_shapes", "method_params": "device,dtype,mode", "method_startline": "68", "method_endline": "89"}}, "hunk_4": {"Ismethod": 1, "added_lines": "48,49,50,51,52", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self", "method_startline": "48", "method_endline": "52"}}}}}}}