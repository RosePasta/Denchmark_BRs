{"BR": {"BR_id": "1223", "BR_author": "lobantseff", "BRopenT": "2020-03-24T14:24:01Z", "BRcloseT": "2020-05-31T12:31:22Z", "BR_text": {"BRsummary": "gan.py multi-gpu running problems", "BRdescription": "\n Running <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/domain_templates/gan.py>gan.py</denchmark-link>\n  example with Trainer(ngpus=2) causes two types of error:\n \n if Trainer(ngpus=2, distributed_backend='dp')\n \n <denchmark-code>Exception has occurred: AttributeError\n 'NoneType' object has no attribute 'detach'\n   File \"/home/user/gan.py\", line 146, in training_step\n     self.discriminator(self.generated_imgs.detach()), fake)\n </denchmark-code>\n \n \n if Trainer(ngpus=2, distributed_backend='ddp')\n \n \n in ./lightling_logs one run creates two folders: version_0 and version_1\n Exception caused:\n File \"/opt/miniconda3/envs/ctln-gan/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 122, in _del_model\n os.remove(filepath)\n FileNotFoundError: [Errno 2] No such file or directory: '/home/user/pyproj/DCGAN/lightning_logs/version_1/checkpoints/epoch=0.ckpt'\n \n it seems that each subprocess tries to create its own checkpoints and delete not ctrated one.\n <denchmark-h:h4>Environment version:</denchmark-h>\n \n python 3.7.5\n pytorch 1.4.0\n pytorch-lightning 0.7.1\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lobantseff", "commentT": "2020-03-24T14:24:41Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "lobantseff", "commentT": "2020-03-24T16:49:10Z", "comment_text": "\n \t\tThe problem is that gan.py example suppose to use buffered values self.generated_images and self.last_img, however during replicating and gathering in \n \n \n pytorch-lightning/pytorch_lightning/overrides/data_parallel.py\n \n \n          Line 64\n       in\n       22a7264\n \n \n \n \n \n \n  replicas = self.replicate(self.module, self.device_ids[:len(inputs)]) \n \n \n \n \n \n buffered values are not replicated and not gathered to main LightningModule model\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "lobantseff", "commentT": "2020-03-27T12:01:55Z", "comment_text": "\n \t\t@armavox good catch, mind draft a PR? \ud83e\udd16\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "lobantseff", "commentT": "2020-03-29T22:14:14Z", "comment_text": "\n \t\tyep, I'll try\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "lobantseff", "commentT": "2020-04-14T17:29:01Z", "comment_text": "\n \t\t@armavox how is it going?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "lobantseff", "commentT": "2020-04-22T09:43:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n   I assume to fix it by May\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "lobantseff", "commentT": "2020-05-28T14:00:50Z", "comment_text": "\n \t\t@armavox Any updates on this? Having the same issue...\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "lobantseff", "commentT": "2020-05-30T15:50:34Z", "comment_text": "\n \t\tMade some updates. Sorry for waiting.\n There is an official l warning about the use of local (buffered here) variables during the distributed training: <denchmark-link:https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel>https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel</denchmark-link>\n \n So I didn't try to create detours in the Lightning code and fixed only the example to work with dp and ddp.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "lobantseff", "commentT": "2020-05-30T15:55:50Z", "comment_text": "\n \t\tThe problem from point 2 in the heading post seems to be fixed by someone. But the unused folder for parallel experiment still created during ddp training. The problem is in \n \n \n pytorch-lightning/pytorch_lightning/trainer/callback_config.py\n \n \n          Line 66\n       in\n       fdbbe96\n \n \n \n \n \n \n  os.makedirs(ckpt_path, exist_ok=True) \n \n \n \n \n , which doesn't use rank_zero_only decorator or something else.\n I would propose some good fixes, but don't know how to do this elegant.\n Thanks for your work!\n Best regards, Artem.\n \t\t"}}}, "commit": {"commit_id": "55fdfe384537e4d43e7397306ba001ffc3474322", "commit_author": "Artem Lobantsev", "commitT": "2020-05-31 08:31:21-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "pl_examples\\domain_templates\\generative_adversarial_net.py", "file_new_name": "pl_examples\\domain_templates\\generative_adversarial_net.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "186,190,195,196", "deleted_lines": "188,192", "method_info": {"method_name": "main", "method_params": "args", "method_startline": "186", "method_endline": "202"}}, "hunk_1": {"Ismethod": 1, "added_lines": "178", "deleted_lines": "179,180", "method_info": {"method_name": "on_epoch_end", "method_params": "self", "method_startline": "177", "method_endline": "183"}}, "hunk_2": {"Ismethod": 1, "added_lines": "190,195,196", "deleted_lines": "188,192", "method_info": {"method_name": "main", "method_params": "hparams", "method_startline": "188", "method_endline": "202"}}, "hunk_3": {"Ismethod": 1, "added_lines": "104,105,106,107,116,117,118,126,147,150", "deleted_lines": "105,109,110,111,117,118,119,127,148,151", "method_info": {"method_name": "training_step", "method_params": "self,batch,batch_idx,optimizer_idx", "method_startline": "102", "method_endline": "160"}}}}}}}