{"BR": {"BR_id": "2532", "BR_author": "lucmos", "BRopenT": "2020-07-06T18:01:39Z", "BRcloseT": "2020-10-05T03:25:03Z", "BR_text": {"BRsummary": "TypeError with multiple validation loaders and overfit_batches", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n A TypeError when using multiple validation datasets and  overfit_batches != 0\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Use multiple val_dataloaders\n Use overfit_batches != 0, e.g. overfit_batches=0.5\n \n <denchmark-h:h3>Code sample</denchmark-h>\n \n <denchmark-link:https://colab.research.google.com/drive/1BtQBCoP5fK-aZm_2uLMOUbf2c9cu-yFb?usp=sharing>https://colab.research.google.com/drive/1BtQBCoP5fK-aZm_2uLMOUbf2c9cu-yFb?usp=sharing</denchmark-link>\n \n <denchmark-h:h3>Traceback</denchmark-h>\n \n <denchmark-code>TypeError                                 Traceback (most recent call last)\n \n <ipython-input-5-c33b987ae54f> in <module>()\n       1 trainer = pl.Trainer(overfit_batches=0.5)\n ----> 2 trainer.fit(model)\n \n 3 frames\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)\n    1018             self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)\n    1019 \n -> 1020             self.run_pretrain_routine(model)\n    1021 \n    1022         # callbacks\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)\n    1137                                           self.val_dataloaders,\n    1138                                           max_batches,\n -> 1139                                           False)\n    1140 \n    1141             # allow no returns from eval\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in _evaluate(self, model, dataloaders, max_batches, test_mode)\n     291                         output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\n     292                 else:\n --> 293                     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)\n     294 \n     295                 # on dp / ddp2 might still want to do something with the batch parts\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in evaluation_forward(self, model, batch, batch_idx, dataloader_idx, test_mode)\n     483             output = model.test_step(*args)\n     484         else:\n --> 485             output = model.validation_step(*args)\n     486 \n     487         return output\n \n TypeError: validation_step() missing 1 required positional argument: 'dataloader_idx'\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n If the codebase is working with multiple validation loaders, it should continue to work even when using overfit_batches != 0\n <denchmark-h:h3>Possible solution</denchmark-h>\n \n \n Check if there were multiple val dataloaders, in case call validation_step with dataloader_idx=0\n Repeat the train loader to match the number of val dataloaders\n Add the possibility to overfit on train but validate and test normally. It is already possible with limit_train_batches, so it would be only a doc change \"If there are multiple val_dataloaders, use limit_train_batches instead of overfit_batches\"\n \n <denchmark-h:h3>Reason</denchmark-h>\n \n When using multiple validation loaders, validation_step takes a dataloader_idx.\n However if later on we set the overfit_batches to something that is not 0, line 268 is executed to use the train loader instead than the validation loaders:\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/data_loading.py\n \n \n         Lines 266 to 270\n       in\n       a91b06e\n \n \n \n \n \n \n  # use the training loader as val and test when overfitting \n \n \n \n  if self.overfit_batches > 0: \n \n \n \n  dataloaders = self.request_dataloader(getattr(model, 'train_dataloader')) \n \n \n \n  else: \n \n \n \n  dataloaders = self.request_dataloader(getattr(model, f'{mode}_dataloader')) \n \n \n \n \n \n Now there is only one validation loader, thus the validation_step function that had a dataloader_idx parameter breaks.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n \n GPU:\n available:         False\n version:           10.1\n \n \n Packages:\n \n numpy:             1.18.5\n pyTorch_debug:     False\n pyTorch_version:   1.5.1+cu101\n pytorch-lightning: 0.8.4\n tensorboard:       2.2.2\n tqdm:              4.41.1\n \n \n System:\n \n OS:                Linux\n architecture:\n \n 64bit\n \n \n \n processor:         x86_64\n python:            3.6.9\n version:           1 SMP Wed Feb 19 05:26:34 PST 2020\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "lucmos", "commentT": "2020-08-03T16:35:56Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  is this issue fixed on master?\n \t\t"}}}, "commit": {"commit_id": "d787208e768085b608198f3e0313e2be28d4cbfe", "commit_author": "William Falcon", "commitT": "2020-10-04 23:25:02-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\data_loading.py", "file_new_name": "pytorch_lightning\\trainer\\data_loading.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "30,237,241,242,243,244,245,246,247", "deleted_lines": "236,238,239,240,241"}}}, "file_1": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\trainer\\flags\\test_overfit_batches.py"}}}}