{"BR": {"BR_id": "566", "BR_author": "iamsimha", "BRopenT": "2019-12-02T11:24:36Z", "BRcloseT": "2019-12-04T12:04:59Z", "BR_text": {"BRsummary": "Using print_nan_grads in the Trainer results in an error", "BRdescription": "\n Describe the bug\n When using\n <denchmark-code>print_nan_grads=True\n </denchmark-code>\n \n in the Trainer, I am getting the error below.\n trainer.fit(lstm_model)\n File \"/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 364, in fit\n self.run_pretrain_routine(model)\n File \"/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 471, in run_pretrain_routine\n self.train()\n File \"/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\", line 60, in train\n self.run_training_epoch()\n File \"/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\", line 99, in run_training_epoch\n output = self.run_training_batch(batch, batch_nb)\n File \"/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\", line 219, in run_training_batch\n self.print_nan_gradients()\n File \"/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/training_tricks_mixin.py\", line 16, in print_nan_gradients\n if torch.isnan(param.grad.float()).any():\n AttributeError: 'NoneType' object has no attribute 'float'\n To Reproduce\n Steps to reproduce the behavior:\n If some param object, does not have .grad, then that object should not be checked for nans\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "iamsimha", "commentT": "2019-12-03T06:40:52Z", "comment_text": "\n \t\tsounds like an easy fix: iterate only over params where grad is not None. Would that solve the issue?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "iamsimha", "commentT": "2019-12-03T12:57:28Z", "comment_text": "\n \t\texactly. Anyone want to submit the PR?\n \t\t"}}}, "commit": {"commit_id": "d4571d1d6f524b0b9284e84ea8f95bb8eb656c86", "commit_author": "Ir1dXD", "commitT": "2019-12-04 07:04:58-05:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_tricks_mixin.py", "file_new_name": "pytorch_lightning\\trainer\\training_tricks_mixin.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "16", "deleted_lines": "16", "method_info": {"method_name": "print_nan_gradients", "method_params": "self", "method_startline": "13", "method_endline": "17"}}}}}}}