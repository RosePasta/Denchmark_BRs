{"BR": {"BR_id": "1156", "BR_author": "LucFrachon", "BRopenT": "2020-03-15T15:03:34Z", "BRcloseT": "2020-06-05T10:28:30Z", "BR_text": {"BRsummary": "ReduceLROnPlateau does not recognise val_loss despite progress_bar dict", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When training my model, I get the following message:\n <denchmark-code>  File \"C:\\Users\\Luc\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\", line 371, in train\n     raise MisconfigurationException(m)\n pytorch_lightning.utilities.debugging.MisconfigurationException: ReduceLROnPlateau conditioned on metric val_loss which is not available. Available metrics are: loss\n </denchmark-code>\n \n Ihis is similar to #321for instance, but I definitely return a progress_bar dict with a val_loss key in it (see code below).\n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>  def training_step(self, batch, batch_idx):\n        z, y_true = batch\n        y_pred = self.forward(z)\n        loss_val = self.loss_function(y_pred, y_true)\n        return {'loss': loss_val.sqrt()}\n \n    def validation_step(self, batch, batch_idx):\n        z, y_true = batch\n        lr = torch.tensor(self.optim.param_groups[0]['lr'])\n        y_pred = self.forward(z)\n        loss_val = self.loss_function(y_pred, y_true)\n        return {'val_loss': loss_val.sqrt(), 'lr': lr}\n \n    def validation_epoch_end(self, outputs):\n        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\n        lr = outputs[-1]['lr']\n        logs = {'val_loss': val_loss_mean, 'lr': lr}\n        return {'val_loss': val_loss_mean, 'progress_bar': logs, 'log': logs}\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n The val_loss value should be picked up by the progress bar.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version (e.g., 1.0): 1.4.0\n OS (e.g., Linux): Windows 10\n How you installed PyTorch (conda, pip, source): pip\n Python version: 3.6.10\n CUDA/cuDNN version: 10\n GPU models and configuration: 1070Ti x 1\n Any other relevant information:\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "LucFrachon", "commentT": "2020-03-16T14:07:38Z", "comment_text": "\n \t\tActually, ReduceLROnPlateau lr schedulers are conditioned on callback_metrics, see here:\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 717 to 726\n       in\n       774d9be\n \n \n \n \n \n \n  if lr_scheduler['reduce_on_plateau']: \n \n \n \n  monitor_key = lr_scheduler['monitor'] \n \n \n \n  monitor_val = self.callback_metrics.get(monitor_key) \n \n \n \n  if monitor_val is None: \n \n \n \n  avail_metrics = ','.join(list(self.callback_metrics.keys())) \n \n \n \n  m = f'ReduceLROnPlateau conditioned on metric {monitor_key} ' \\ \n \n \n \n  f'which is not available. Available metrics are: {avail_metrics}. ' \\ \n \n \n \n  'Condition can be set using `monitor` key in lr scheduler dict' \n \n \n \n  raise MisconfigurationException(m) \n \n \n \n  lr_scheduler['scheduler'].step(monitor_val) \n \n \n \n \n \n and these values are everything not defined in 'progress_bar', 'log' and 'hidden', see here:\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/logging.py\n \n \n         Lines 106 to 109\n       in\n       774d9be\n \n \n \n \n \n \n  callback_metrics = {} \n \n \n \n  for k, v in output.items(): \n \n \n \n  if k not in ['progress_bar', 'log', 'hiddens']: \n \n \n \n  callback_metrics[k] = v \n \n \n \n \n \n since you are returning 'val_loss' as a separate key this should however not be the problem.\n Since only 'loss' is available as a callback_metrics it seems that your lr scheduler is called before your validation data is processed. What does your configure_optimizers() look like?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "LucFrachon", "commentT": "2020-03-16T15:24:11Z", "comment_text": "\n \t\tThanks for your answer. If I understand correctly, ReduceLROnPlateau is the only LR scheduler that does not use values from progress_bar?\n Here's configure_optimizers():\n <denchmark-code>    def configure_optimizers(self):\n         self.optim = torch.optim.AdamW(self.parameters(), lr=self.hp.lr_ini, eps=1.e-4)\n         self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optim, 'max', factor=self.hp.lr_reduce_factor,\n                                                                     patience=self.hp.lr_reduce_patience,\n                                                                     min_lr=self.hp.lr_min)\n         return [self.optim], [self.scheduler]\n </denchmark-code>\n \n And here is the Trainer definition:\n <denchmark-code>trainer = pl.Trainer(gpus=-1, early_stop_callback=None, max_epochs=stgs.PRED_HPARAMS['max_epochs'])\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "LucFrachon", "commentT": "2020-03-17T14:04:20Z", "comment_text": "\n \t\tOnly ReduceLROnPlateau schedulers are conditioned on some value. All other LR schedulers have nothing to do with callback_metrics or progress_bar. Hope this answers your question.\n I don't see any obvious mistakes in your code. As a said in my earlier comment, my best guess is that your ReduceLROnlr scheduler is called before val_loss` is actually calculated. You probably need to check this.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "LucFrachon", "commentT": "2020-03-17T18:32:55Z", "comment_text": "\n \t\tI have the same issue. That's how I deal with it:\n     def training_step(self, batch, batch_idx):\n         loss, lm_loss, mc_loss = self.forward(batch)\n         lr = self.trainer.optimizers[0].param_groups[0]['lr']\n         lr = torch.tensor(lr).unsqueeze(0).to(loss.device) if len(loss.size()) else lr\n         log = {\n             'MC-Loss/train': mc_loss,\n             'LM-Loss/train': lm_loss,\n             'Learning-Rate': lr\n         }\n         # Set up placeholders for valid metrics.\n         if self.trainer.global_step == 0:\n             log.update({'LM-Loss/valid': np.inf})\n \n         return {'loss': loss, 'log': log}\n here I check if self.trainer.global_step == 0 and if yes, I set up a placeholder.\n <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>\n  maybe you can advice a better way\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "LucFrachon", "commentT": "2020-03-17T22:04:57Z", "comment_text": "\n \t\t\n Only ReduceLROnPlateau schedulers are conditioned on some value. All other LR schedulers have nothing to do with callback_metrics or progress_bar. Hope this answers your question.\n I don't see any obvious mistakes in your code. As a said in my earlier comment, my best guess is that your ReduceLROnlr scheduler is called before val_loss` is actually calculated. You probably need to check this.\n \n <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>\n  Not sure I follow you. Maybe I misunderstood, but doesn't PL automatically handle calls to the scheduler? Doesn't it defeat (some of) the purpose of PL if I have to call it myself?\n <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>\n  Thanks, I'll try that, but maybe there is a less hacky way...?\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "LucFrachon", "commentT": "2020-03-19T15:05:06Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/LucFrachon>@LucFrachon</denchmark-link>\n  sorry, I must have misinterpret what you asked about. Yes, PL automatically calls the scheduler you define. Sorry for the confusion.\n <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>\n  I think that indicates that there is some bug here, since you need to setup a placeholder for the first step. I will see if I can come up with a more permanent solution.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "LucFrachon", "commentT": "2020-03-22T12:43:02Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>\n  , btw, the trick:\n         if self.trainer.global_step == 0:\n             log.update({'Loss/valid': np.inf})\n doesn't help in case of warm start from checkpoint, because in warm start, the global_step is never equal to 0 :)\n For now I did it like this:\n         # Set up placeholders for valid metrics.\n         if not self._valid_metrics_patched:\n             log.update({'Loss/valid': np.inf})\n             self._valid_metrics_patched = True\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "LucFrachon", "commentT": "2020-03-23T14:31:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>\n  I have a hard time reproducing this bug, it seems to be a very corner case. Do you have a simple model that I can reproduce?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "LucFrachon", "commentT": "2020-03-23T19:36:37Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>\n \n Yes, sure:\n <denchmark-link:https://gist.github.com/alexeykarnachev/efbd40f0ff0cfcd1e324de044a802c25>https://gist.github.com/alexeykarnachev/efbd40f0ff0cfcd1e324de044a802c25</denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "LucFrachon", "commentT": "2020-03-25T15:40:46Z", "comment_text": "\n \t\tOkay, after looking at your code <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>\n , this does not seems to be a bug. When you set  you are calling the  method for  after each batch and it therefore makes complete sense that no  is calculated yet. If you really want to do something like this, you need to set  in the  construction to a number lower than  in the scheduler construction. In this way  will be calculated before  is called.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "LucFrachon", "commentT": "2020-03-26T10:09:51Z", "comment_text": "\n \t\tThx, now it's more clear.\n Does it mean, that I can't use a schedulers, which are depend on some valid metric with interval=\"step\"?\n For instance, I have a custom reduce on plateau scheduler, which just ignores nan metric values, that's why I need to set a placeholder for valid metric (like in my first example).\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "LucFrachon", "commentT": "2020-03-30T12:23:25Z", "comment_text": "\n \t\tI do not think it is possible just out of the box. However, if you configure your scheduler correctly, then it should be possible. For example, if I initialize my Trainer as\n trainer = Trainer(val_check_interval=50)\n and initialize my scheduler as\n <denchmark-code>scheduler = {\n     'schduler': ReduceLROnPlateau(optimizer, mode, factor, patience),\n     'interval': 'step',\n     'frequency': 100\n }\n </denchmark-code>\n \n it should work (not tested), since val_loss will be created every 50 steps but the scheduler will first be called after 100 steps.\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "LucFrachon", "commentT": "2020-03-30T13:32:50Z", "comment_text": "\n \t\toh, right, I missed the frequency argument. Totally forgot about it. I'll try it. Thank you!\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "LucFrachon", "commentT": "2020-05-29T14:54:40Z", "comment_text": "\n \t\tThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "LucFrachon", "commentT": "2020-06-05T10:05:23Z", "comment_text": "\n \t\tshall be fixed with <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1989>#1989</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "711892a0a293f7c7f951eba0907e1c0ccd2b37d8", "commit_author": "authman", "commitT": "2020-03-19 09:22:29-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\optimizers.rst", "file_new_name": "docs\\source\\optimizers.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "22,23,24,25,26,27,28,29,30,31,32,33,34,35", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\core\\lightning.py", "file_new_name": "pytorch_lightning\\core\\lightning.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "945,947,949,950,951,953,955,957,958,959,960,961,962,963,964,965", "deleted_lines": "945,946,948,949,951,952,953,954,955,956,957,959,960,962,963,965,966"}}}}}}