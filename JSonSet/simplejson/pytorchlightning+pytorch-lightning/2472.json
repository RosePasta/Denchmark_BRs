{"BR": {"BR_id": "2472", "BR_author": "kl0211", "BRopenT": "2020-07-02T16:27:39Z", "BRcloseT": "2020-07-03T17:23:31Z", "BR_text": {"BRsummary": "DDP breaks when `python` does not refer to the correct interpreter", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n If using the DDP distributed_backend, the program breaks if python refers to python2 or does not exist.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Make sure the python command does not link to python3, such as on Ubuntu 18.04.\n Run Trainer().fit with distributed_backed='ddp'\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n The problem lies at \n \n \n pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py\n \n \n          Line 422\n       in\n       0697dd3\n \n \n \n \n \n \n  command = ['python'] + command \n \n \n \n \n  The python command is hardcoded here. On many systems, python is a symlink to python2, or does not exist.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "kl0211", "commentT": "2020-07-02T16:28:37Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "kl0211", "commentT": "2020-07-03T03:33:13Z", "comment_text": "\n \t\tSame question!!!!!\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "kl0211", "commentT": "2020-07-03T08:08:57Z", "comment_text": "\n \t\tit is based on your default python... <denchmark-link:https://github.com/kl0211>@kl0211</denchmark-link>\n  <denchmark-link:https://github.com/RitchieAlpha>@RitchieAlpha</denchmark-link>\n  mind send a Pr which updating the actual python interpreter?\n cc: <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "kl0211", "commentT": "2020-07-03T08:28:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  I just saw <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2482>#2482</denchmark-link>\n . Using  is exactly what I was going to suggest. According to <denchmark-link:https://www.python.org/dev/peps/pep-0394/#for-python-script-publishers>https://www.python.org/dev/peps/pep-0394/#for-python-script-publishers</denchmark-link>\n , this should ensure the same interpreter is used to spawn the child processes.\n Thanks for the quick change!\n \t\t"}}}, "commit": {"commit_id": "fc61c200c085f78fa2af4850aa8dc8e832fb80d0", "commit_author": "Jirka Borovec", "commitT": "2020-07-03 13:23:30-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "24,25", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_data_parallel.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "422,423", "deleted_lines": "422", "method_info": {"method_name": "spawn_ddp_children", "method_params": "self,model", "method_startline": "394", "method_endline": "450"}}}}}}}