{"BR": {"BR_id": "1161", "BR_author": "sneiman", "BRopenT": "2020-03-16T18:09:55Z", "BRcloseT": "2020-03-30T16:13:35Z", "BR_text": {"BRsummary": "multi-gpu ddp calls validation and testing loops too many times", "BRdescription": "\n When using ddp with multiple gpus, each validation and test loop is called with the entire validation dataset for each gpu.\n Expected behavior is that the dataset is divided appropriately across the gpus.\n I am using current master (cloned Mar 14), Ubuntu 19.10, Cuda 10.1, python 3.7.5, pytorch 1.4, venv environment.\n The problem appears to be in auto_add_sampler() in data_loading.py. It does not create a DistributedSampler for validation or test datasets.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "sneiman", "commentT": "2020-03-16T21:24:51Z", "comment_text": "\n \t\tLatest pull - 1 hour ago, no longer this behavior. Closing.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "sneiman", "commentT": "2020-03-17T00:22:10Z", "comment_text": "\n \t\tSorry - this issue still exists in some configurations. My proposed fix is not the total picture. Still investigating - will provide reproducible example.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "sneiman", "commentT": "2020-03-17T03:29:44Z", "comment_text": "\n \t\tTesting underway. Will make PR tomorrow.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "sneiman", "commentT": "2020-03-17T23:18:43Z", "comment_text": "\n \t\tDont want to clutter up PR world if no one is interested in this. Let me know ...\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "sneiman", "commentT": "2020-03-18T21:54:08Z", "comment_text": "\n \t\tthat sounds a good contribution to me... mind send a PR?\n Any suggestion <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>\n ?\n in a technical note when you refer some master state pls use coit hash as there can be multiple commits each day...\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "sneiman", "commentT": "2020-03-18T22:05:28Z", "comment_text": "\n \t\twill do on both pr, and hash ref\n \t\t"}}}, "commit": {"commit_id": "6dfe9951e132bdc9896926557138e7a21d4cd000", "commit_author": "sneiman", "commitT": "2020-03-30 12:13:34-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\data_loading.py", "file_new_name": "pytorch_lightning\\trainer\\data_loading.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "94,95,99", "deleted_lines": "94,95,96,97,98", "method_info": {"method_name": "auto_add_sampler", "method_params": "self,DataLoader,bool", "method_startline": "72", "method_endline": "100"}}}}}}}