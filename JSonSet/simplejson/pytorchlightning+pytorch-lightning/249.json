{"BR": {"BR_id": "249", "BR_author": "awaelchli", "BRopenT": "2019-09-25T13:11:17Z", "BRcloseT": "2019-09-26T17:20:56Z", "BR_text": {"BRsummary": "UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars", "BRdescription": "\n Describe the bug\n Not sure if this is a bug. It shows me this warning at the beginning of training:\n /home/adrian/research/envs/research/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n To Reproduce\n The minimal MNIST example from the docs has this problem when trained on multiple GPUs. Attached the Python script:\n <denchmark-code>import os\n import torch\n from torch.nn import functional as F\n from torch.utils.data import DataLoader\n from torchvision.datasets import MNIST\n import torchvision.transforms as transforms\n \n import pytorch_lightning as pl\n \n \n class CoolModel(pl.LightningModule):\n \n     def __init__(self):\n         super(CoolModel, self).__init__()\n         # not the best model...\n         self.l1 = torch.nn.Linear(28 * 28, 10)\n \n     def forward(self, x):\n         return torch.relu(self.l1(x.view(x.size(0), -1)))\n \n     def training_step(self, batch, batch_nb):\n         # REQUIRED\n         x, y = batch\n         y_hat = self.forward(x)\n         return {'loss': F.cross_entropy(y_hat, y)}\n \n     def validation_step(self, batch, batch_nb):\n         # OPTIONAL\n         x, y = batch\n         y_hat = self.forward(x)\n         return {'val_loss': F.cross_entropy(y_hat, y)}\n \n     def validation_end(self, outputs):\n         # OPTIONAL\n         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n         return {'avg_val_loss': avg_loss}\n \n     def test_step(self, batch, batch_nb):\n         # OPTIONAL\n         x, y = batch\n         y_hat = self.forward(x)\n         return {'test_loss': F.cross_entropy(y_hat, y)}\n \n     def test_end(self, outputs):\n         # OPTIONAL\n         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n         return {'avg_test_loss': avg_loss}\n \n     def configure_optimizers(self):\n         # REQUIRED\n         return [torch.optim.Adam(self.parameters(), lr=0.02)]\n \n     @pl.data_loader\n     def tng_dataloader(self):\n         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n \n     @pl.data_loader\n     def val_dataloader(self):\n         # OPTIONAL\n         # can also return a list of val dataloaders\n         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n \n     @pl.data_loader\n     def test_dataloader(self):\n         # OPTIONAL\n         # can also return a list of test dataloaders\n         return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)\n \n \n if __name__ == '__main__':\n     from pytorch_lightning import Trainer\n \n     trainer = Trainer(\n         gpus=[2, 3],\n         distributed_backend='dp',\n     )\n \n     model = CoolModel()\n     trainer.fit(model)\n \n \n </denchmark-code>\n \n Expected behavior\n There are no scalars involved in the forward pass, so the warning does not make sense and should not be shown.\n Desktop (please complete the following information):\n \n OS: Ubuntu 18.04\n Version: the latest pip install\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "awaelchli", "commentT": "2019-10-14T10:30:30Z", "comment_text": "\n \t\tI'm using the newest pip version 0.5.2.1 and I'm still getting this warning (0.5.1.3 also), do I have to toggle it somehow?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "awaelchli", "commentT": "2020-05-02T13:16:12Z", "comment_text": "\n \t\tIt happens also to me.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "awaelchli", "commentT": "2020-05-02T13:17:26Z", "comment_text": "\n \t\tupdate  to 0.7.5\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "awaelchli", "commentT": "2020-05-02T13:22:38Z", "comment_text": "\n \t\tI'm currently on cutting edge.\n When manually adding. Unsqueeze to the loss it vanishes of course\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "awaelchli", "commentT": "2020-05-02T13:29:05Z", "comment_text": "\n \t\tweird, can you post a colab with a minimal example? we added auto squeezing in dp for this. (i assume this is dp).\n but on a much greater note, use DDP instead of DP as DP use is discouraged in general (by pytorch and us)\n \t\t"}}}, "commit": {"commit_id": "8b2a2aeda3066fe30cc496a58368a523ef90ad9b", "commit_author": "William Falcon", "commitT": "2019-09-26 13:20:54-04:00", "changed_files": {"file_0": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "pytorch_lightning\\trainer\\ignored_warnings.py"}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "25", "deleted_lines": null}}}}}}