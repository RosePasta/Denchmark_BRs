{"BR": {"BR_id": "3303", "BR_author": "NumesSanguis", "BRopenT": "2020-09-01T09:37:48Z", "BRcloseT": "2020-09-21T09:46:50Z", "BR_text": {"BRsummary": "AUROC metric should throw an error when used for multi-class problems", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n AUROC accepts multi-class input without throwing an error. Instead, it gives a random value, which gives the illusion that it is working.\n Background: <denchmark-link:https://forums.pytorchlightning.ai/t/pytorch-lightning-auroc-value-for-multi-class-seems-to-be-completely-off-compared-to-sklearn-using-it-wrong/61/7>https://forums.pytorchlightning.ai/t/pytorch-lightning-auroc-value-for-multi-class-seems-to-be-completely-off-compared-to-sklearn-using-it-wrong/61/7</denchmark-link>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Manually create some multi-class arrays\n Use PyTorch Lightning's AUROC() metric\n Use sklearn's AUROC metric\n Observe values not matching\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n import torch\n import sklearn.metrics\n import pytorch_lightning as pl\n from pytorch_lightning.metrics.classification import AUROC\n \n pl.seed_everything(0)\n auroc = AUROC()\n \n def test_auroc_sk_multiclass():\n     for i in range(100):\n         target = torch.randint(0, 3, size=(10,))  # 2 --> 3\n         pred = torch.rand(10, 3).softmax(dim=1)  # torch.randint(0, 2, size=(10, ))\n         score_sk = sklearn.metrics.roc_auc_score(target.numpy(), pred.numpy(), multi_class='ovo', labels=[0, 1, 2])\n         score_pl = auroc(pred, target)\n         print(score_sk, score_pl)\n         assert torch.allclose(torch.tensor(score_pl).float(), torch.tensor(score_sk).float())\n \n test_auroc_sk_multiclass()\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n \n Throw error that multi-class AUROC has not been implemented (yet).\n Note in documentation that the AUROC metric does not support multi-class yet ()\n \n <denchmark-h:h3>Actual behavior</denchmark-h>\n \n Giving a random value, giving a false sense that it is working.\n <denchmark-h:h3>Environment</denchmark-h>\n \n <denchmark-code>* CUDA:\n \t- GPU:\n \t\t- GeForce GTX 1080 Ti\n \t- available:         True\n \t- version:           10.2\n * Packages:\n \t- numpy:             1.19.1\n \t- pyTorch_debug:     False\n \t- pyTorch_version:   1.6.0\n \t- pytorch-lightning: 0.9.0\n \t- tensorboard:       2.2.0\n \t- tqdm:              4.48.2\n * System:\n \t- OS:                Linux\n \t- architecture:\n \t\t- 64bit\n \t\t- \n \t- processor:         x86_64\n \t- python:            3.7.7\n \t- version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020\n </denchmark-code>\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n The output value is nonsense in the multi-class classification, instead of an error. That's why I thought it was appropriate to file it as a bug instead of a feature request / documentation improvement. I'm aware that the AUROC implementation is not intended to be for multi-class after the discussion on the PyTorch Lightning forum.\n I used the AUROC value and noticed it was wrong after training a few models, but it will take many people off-guard in it's current form.\n Feature request for MulticlassAUROC: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3304>#3304</denchmark-link>\n \n \t"}, "comments": {}}, "commit": {"commit_id": "b1347c956af4752560b53b891d352c48c6050305", "commit_author": "Nicki Skafte", "commitT": "2020-09-21 11:46:48+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "20", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\metrics\\classification.py", "file_new_name": "pytorch_lightning\\metrics\\classification.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "377,380", "deleted_lines": "377,380"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\metrics\\functional\\classification.py", "file_new_name": "pytorch_lightning\\metrics\\functional\\classification.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "587,590,592,685,688,690,861,863,865,866,867,868", "deleted_lines": "587,590,592,685,688,690,861,863"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "tests\\metrics\\functional\\test_classification.py", "file_new_name": "tests\\metrics\\functional\\test_classification.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "48,49,50,51,52,53,55,56,59,60,61", "deleted_lines": "48,49,52,53,59,61", "method_info": {"method_name": "test_against_sklearn", "method_params": "sklearn_metric,torch_metric", "method_startline": "48", "method_endline": "61"}}, "hunk_1": {"Ismethod": 1, "added_lines": "406,407,408,409,410,411", "deleted_lines": null, "method_info": {"method_name": "test_error_on_multiclass_input", "method_params": "metric", "method_startline": "406", "method_endline": "411"}}, "hunk_2": {"Ismethod": 1, "added_lines": "55,56,59,60,61,62,63,64,71,72,73,74,75,76,77,78,79", "deleted_lines": "59,61", "method_info": {"method_name": "test_against_sklearn", "method_params": "sklearn_metric,torch_metric,only_binary", "method_startline": "55", "method_endline": "79"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\metrics\\test_classification.py", "file_new_name": "tests\\metrics\\test_classification.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "121", "deleted_lines": "121", "method_info": {"method_name": "test_auroc", "method_params": "pos_label", "method_startline": "117", "method_endline": "123"}}}}}}}