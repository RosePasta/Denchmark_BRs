{"BR": {"BR_id": "1116", "BR_author": "bkkaggle", "BRopenT": "2020-03-11T02:47:18Z", "BRcloseT": "2020-03-30T22:45:07Z", "BR_text": {"BRsummary": "Wandb logger doesn't upload saved model checkpoint for final epoch", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n When training a model on the TPU and using the wandb logger, the checkpoint for the last epoch trained doesn't get uploaded to wandb.\n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Colab notebook: <denchmark-link:https://colab.research.google.com/drive/1oPaRWGZcz6YEol012xFADN42LV-jowtT>https://colab.research.google.com/drive/1oPaRWGZcz6YEol012xFADN42LV-jowtT</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "bkkaggle", "commentT": "2020-03-11T13:40:37Z", "comment_text": "\n \t\tHello, could you be mo specific what is not working and ho are you using it?\n I see in the notebook for example if __name__ == \"__main__\": parser = argparse.ArgumentParser() which is not correct\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "bkkaggle", "commentT": "2020-03-11T23:42:53Z", "comment_text": "\n \t\tI'm finetuning distilgpt2 on 8 cores of a colab TPU and using the wandb logger. The main problem seems to be that at the end of training, each tpu process and the wandb logger shut down before the built in ModelCheckpoint callback finishes saving the checkpoint for the epoch to disk.\n Relevant part of stack trace:\n <denchmark-code>Epoch 1: 100%|\u2588\u2588\u2588\u2588| 327/327 [01:39<00:00,  1.65it/s, loss=3.468, v_num=2211mbhk]\n Epoch 2:  91%|\u2588\u2588\u2588\u258c| 296/327 [01:15<01:51,  3.59s/it, loss=3.529, v_num=2211mbhk]\n Validating:   0%|                                        | 0/31 [00:00<?, ?it/s]\n Epoch 2:  91%|\u2588\u2588\u2588\u258b| 297/327 [01:24<02:31,  5.05s/it, loss=3.529, v_num=2211mbhk]\n Epoch 2:  92%|\u2588\u2588\u2588\u258b| 301/327 [01:24<01:32,  3.54s/it, loss=3.529, v_num=2211mbhk]\n Epoch 2:  94%|\u2588\u2588\u2588\u258a| 307/327 [01:24<00:49,  2.48s/it, loss=3.529, v_num=2211mbhk]\n Epoch 2:  96%|\u2588\u2588\u2588\u258a| 314/327 [01:24<00:22,  1.74s/it, loss=3.529, v_num=2211mbhk]\n Epoch 2:  98%|\u2588\u2588\u2588\u2589| 321/327 [01:24<00:07,  1.23s/it, loss=3.529, v_num=2211mbhk]\n wandb: Waiting for W&B process to finish, PID 2784\n wandb: Program ended successfully.\n \n wandb: Waiting for W&B process to finish, PID 2784\n \n wandb: Waiting for W&B process to finish, PID 2784\n \n wandb: Waiting for W&B process to finish, PID 2784\n Epoch 2: 100%|\u2588\u2588\u2588\u2588| 327/327 [01:24<00:00,  1.23s/it, loss=3.529, v_num=2211mbhk]\n                                                                                 \n wandb: Waiting for W&B process to finish, PID 2784\n \n wandb: Waiting for W&B process to finish, PID 2784\n \n wandb: Waiting for W&B process to finish, PID 2784\n wandb: Run summary:\n wandb:        global_step 591\n wandb:           _runtime 242.3862087726593\n wandb:      learning_rate 3.7664783427495294e-07\n wandb:         train_loss 3.7114081382751465\n wandb:              _step 61\n wandb:         _timestamp 1583893453.517989\n wandb:           val_loss 3.4930191040039062\n wandb:            val_ppl 32.88510513305664\n wandb:   adjusted_val_ppl 55.14464569091797\n wandb: Syncing files in wandb/run-20200311_022014-2211mbhk:\n wandb:   code/finetune.py\n wandb:   epoch=0.ckpt\n wandb:   epoch=1.ckpt.part\n wandb: plus 7 W&B file(s) and 2 media file(s)\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "bkkaggle", "commentT": "2020-03-14T02:05:16Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  <denchmark-link:https://github.com/calclavia>@calclavia</denchmark-link>\n  pls ^^\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "bkkaggle", "commentT": "2020-03-14T03:16:48Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bkkaggle>@bkkaggle</denchmark-link>\n  does it happen also if you use a jupyter notebook or is it specific to collab?\n wandb has also a way to let you run experiments offline and sync afterwards.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "bkkaggle", "commentT": "2020-03-14T23:13:43Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bkkaggle>@bkkaggle</denchmark-link>\n  I just took a look at this, couple things to note.\n \n Never put your api key in a notebook.  Instead use:\n \n import wandb\n wandb.login()\n And it will prompt you for your api key if it's not in the notebook env yet.\n \n Could this be related to the early stopping callback?  I'm not familiar with the mechanics of lighting and TPU's but this seems to be caused by triggering the finalize action of the WandbLogger before the model has actually been finalized.\n \n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "bkkaggle", "commentT": "2020-03-15T16:41:49Z", "comment_text": "\n \t\tI've set the early stopping callback to have a patience of 10 epochs with\n early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n so I don't think it should be the problem. I also updated the code to use wandb.login() so the API key isn't visible anymore.\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "bkkaggle", "commentT": "2020-03-15T18:07:33Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n \n I overwrote the collab by removing the finalize method and just tracking calls:\n class WandbLogger(WandbLogger):\n     @rank_zero_only\n     def finalize(self, status: str = 'success') -> None:\n         print('Calling finalize')\n         '''try:\n             exit_code = 0 if status == 'success' else 1\n             wandb.join(exit_code)\n         except TypeError:\n             wandb.join()'''\n         print('Called finalize')\n I get following error stack:\n wandb: Tracking run with wandb version 0.8.29\n wandb: Run data is saved locally in wandb/run-20200315_175429-1ig7n4xi\n wandb: Syncing run smooth-leaf-2\n wandb: \u2b50\ufe0f View project at https://app.wandb.ai/borisd13/lm-finetuning\n wandb: \ud83d\ude80 View run at https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi\n wandb: Run `wandb off` to turn off syncing.\n \n /usr/local/lib/python3.6/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:82: UserWarning: Checkpoint directory /content/wandb/run-20200315_175429-1ig7n4xi exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n   f\"Checkpoint directory {filepath} exists and is not empty with save_top_k != 0.\"\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:09<00:00,  9.95s/it]\n Dataset created in 10 seconds\n Dataset length: 9444\n Num tokens: 2417664 | Num original tokens: 2088674\n 2020-03-15 17:54:45.061374: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n 2020-03-15 17:54:45.080550: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n 2020-03-15 17:54:45.099582: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n 2020-03-15 17:54:45.119609: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n 2020-03-15 17:54:45.139724: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n 2020-03-15 17:54:45.159427: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n 2020-03-15 17:54:45.179451: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n E0315 17:54:52.681799302    8753 server_chttp2.cc:40]        {\"created\":\"@1584294892.681777447\",\"description\":\"Only 1 addresses added out of total 2 resolved\",\"file\":\"external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":404,\"referenced_errors\":[{\"created\":\"@1584294892.681774375\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::1]:50549\"}]}\n 2020-03-15 17:54:53.917624: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549\n 2020-03-15 17:54:53.918498: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549\n 2020-03-15 17:54:54.150331: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549\n 2020-03-15 17:54:54.281601: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549\n 2020-03-15 17:54:54.655386: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549\n 2020-03-15 17:54:54.830783: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549\n 2020-03-15 17:54:55.214524: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549\n 2020-03-15 17:54:59.626295: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549\n 0it [00:00, ?it/s]\n   0%|                                                     | 0/1 [00:00<?, ?it/s]\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.40it/s]\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.40it/s]\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.40it/s]\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.40it/s]\n Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.41it/s]Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.40it/s]\n Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.40it/s]\n Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.36it/s]\n Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.35it/s]\n Dataset created in 0 seconds\n Dataset length: 976\n Num tokens: 249856 | Num original tokens: 217646\n Epoch 1:   0%|                                          | 0/327 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:\n \tadd_(Number alpha, Tensor other)\n Consider using one of the following signatures instead:\n \tadd_(Tensor other, Number alpha)\n Epoch 1:  91%|\u2588\u2588\u2588\u258c| 296/327 [01:13<00:21,  1.47it/s, loss=3.468, v_num=1ig7n4xi]\n Validating:   0%|                                        | 0/31 [00:00<?, ?it/s]\n Epoch 1:  91%|\u2588\u2588\u2588\u258b| 297/327 [01:21<01:24,  2.82s/it, loss=3.468, v_num=1ig7n4xi]\n Epoch 1:  91%|\u2588\u2588\u2588\u258b| 298/327 [01:23<01:11,  2.47s/it, loss=3.468, v_num=1ig7n4xi]\n Epoch 1:  93%|\u2588\u2588\u2588\u258b| 305/327 [01:23<00:38,  1.73s/it, loss=3.468, v_num=1ig7n4xi]\n Epoch 1:  95%|\u2588\u2588\u2588\u258a| 312/327 [01:23<00:18,  1.22s/it, loss=3.468, v_num=1ig7n4xi]\n Epoch 1:  98%|\u2588\u2588\u2588\u2589| 319/327 [01:23<00:06,  1.17it/s, loss=3.468, v_num=1ig7n4xi]\n Epoch 1: 100%|\u2588\u2588\u2588\u2588| 327/327 [01:24<00:00,  1.65it/s, loss=3.468, v_num=1ig7n4xi]\n                                                                                 /usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n   warnings.warn(SAVE_STATE_WARNING, UserWarning)\n tcmalloc: large alloc 1704353792 bytes == 0x151728000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x4e307c 0x4e3120 0x4e3024 0x4e34a4 0x4e307c 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9\n Epoch 1: 100%|\u2588\u2588\u2588\u2588| 327/327 [01:41<00:00,  1.65it/s, loss=3.468, v_num=1ig7n4xi]tcmalloc: large alloc 2568052736 bytes == 0x1d72fe000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x5ec012 0x4e1683 0x4e2c5b 0x4e3120 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509d48\n Epoch 2:  91%|\u2588\u2588\u2588\u258c| 296/327 [01:13<01:47,  3.47s/it, loss=3.529, v_num=1ig7n4xi]\n Validating:   0%|                                        | 0/31 [00:00<?, ?it/s]\n Epoch 2:  91%|\u2588\u2588\u2588\u258b| 297/327 [01:21<02:26,  4.88s/it, loss=3.529, v_num=1ig7n4xi]\n Epoch 2:  92%|\u2588\u2588\u2588\u258b| 300/327 [01:21<01:32,  3.43s/it, loss=3.529, v_num=1ig7n4xi]\n Epoch 2:  94%|\u2588\u2588\u2588\u258a| 307/327 [01:21<00:48,  2.40s/it, loss=3.529, v_num=1ig7n4xi]\n Epoch 2:  95%|\u2588\u2588\u2588\u258a| 312/327 [01:21<00:25,  1.69s/it, loss=3.529, v_num=1ig7n4xi]\n Epoch 2:  98%|\u2588\u2588\u2588\u2589| 319/327 [01:21<00:09,  1.19s/it, loss=3.529, v_num=1ig7n4xi]\n Epoch 2: 100%|\u2588\u2588\u2588\u2589| 326/327 [01:21<00:00,  1.20it/s, loss=3.529, v_num=1ig7n4xi]Calling finalize\n Called finalize\n Calling finalize\n Called finalize\n Calling finalize\n Called finalize\n Calling finalize\n Called finalize\n Calling finalize\n Called finalize\n Calling finalize\n Called finalize\n Calling finalize\n Called finalize\n Epoch 2: 100%|\u2588\u2588\u2588\u2588| 327/327 [01:21<00:00,  1.20it/s, loss=3.529, v_num=1ig7n4xi]\n                                                                                 tcmalloc: large alloc 1704116224 bytes == 0x151728000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x4e307c 0x4e3120 0x4e3024 0x4e34a4 0x4e307c 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9\n tcmalloc: large alloc 2567823360 bytes == 0x1e993e000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x5ec012 0x4e1683 0x4e2c5b 0x4e3120 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509d48\n Epoch 2: 100%|\u2588\u2588\u2588\u2588| 327/327 [01:50<00:00,  2.95it/s, loss=3.529, v_num=1ig7n4xi]\n Calling finalize\n Called finalize\n tcmalloc: large alloc 2567823360 bytes == 0x1e993e000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x5ec012 0x4e1683 0x4e2c5b 0x4e3120 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509d48\n Traceback (most recent call last):\n   File \"finetune.py\", line 387, in <module>\n     trainer.fit(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\", line 615, in fit\n     self.load_spawn_weights(model)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_data_parallel.py\", line 366, in load_spawn_weights\n     loaded_model = original_model.__class__.load_from_checkpoint(path)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/lightning.py\", line 1389, in load_from_checkpoint\n     model = cls._load_model_state(checkpoint)\n   File \"/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/lightning.py\", line 1419, in _load_model_state\n     model = cls(*model_args)\n TypeError: __init__() missing 1 required positional argument: 'args'\n \n wandb: Waiting for W&B process to finish, PID 8665\n wandb: Program failed with code 1. Press ctrl-c to abort syncing.\n wandb: Run summary:\n wandb:         train_loss 3.7114081382751465\n wandb:      learning_rate 3.7664783427495294e-07\n wandb:         _timestamp 1584295103.1529658\n wandb:        global_step 591\n wandb:              _step 61\n wandb:           _runtime 236.35386848449707\n wandb:            val_ppl 32.88510513305664\n wandb:           val_loss 3.4930191040039062\n wandb:   adjusted_val_ppl 55.14464569091797\n wandb: Syncing files in wandb/run-20200315_175429-1ig7n4xi:\n wandb:   code/finetune.py\n wandb:   epoch=0.ckpt\n wandb:   epoch=1.ckpt\n wandb: plus 7 W&B file(s) and 2 media file(s)\n wandb:                                                                                \n wandb: Synced smooth-leaf-2: https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi\n We can see 2 issues (I believe unrelated to wandb):\n \n WandbLogger.finalize method is called before \"Epoch 2\" is finished -> could be related to reaching the end of dataloaders?\n Another error appears with load_spawn_weights\n \n Note that the checkpoints get synchronized correctly with W&B now that we ignore the  call: <denchmark-link:https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi>https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi</denchmark-link>\n \n You are probably more familiar with the internals of pytorch_lightning if it's related to it but let me know if I can be of any further help.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "bkkaggle", "commentT": "2020-03-15T21:11:53Z", "comment_text": "\n \t\tThanks for the fix.\n The reason why the error message TypeError: __init__() missing 1 required positional argument: 'args' is probably happening is because I pass a custom parameter to my LightningModel\n <denchmark-code>class LM(pl.LightningModule):\n     def __init__(self, args):\n         super(LM, self).__init__()\n \n         self.args = args\n </denchmark-code>\n \n I'll change my code to follow pytorch-lightning's recommended way of setting hyperparameters (<denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html>https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html</denchmark-link>\n ) so this error message should go away.\n Should I close the issue now or wait until a fix is pushed up?\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "bkkaggle", "commentT": "2020-03-15T22:06:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  thx for your help and tracing back the issues. \n about the  I think that we have been fixing it in the last release\n the  is explicitly called in the cleaning phase after killing the training\n \n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "bkkaggle", "commentT": "2020-03-15T22:47:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>\n  What is strange in this instance is that  seems to be called while Epoch 2 is still running (as we get few more prints after).\n However it may be related to some issues with the implementation of this specific example.\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "bkkaggle", "commentT": "2020-03-16T22:55:53Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>\n  <denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>\n  any thought?\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "bkkaggle", "commentT": "2020-03-17T01:16:52Z", "comment_text": "\n \t\tshould we only set self.logger.finalize(\"success\") on the main process? (and after all other nodes are finished)\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "bkkaggle", "commentT": "2020-03-18T22:39:55Z", "comment_text": "\n \t\tHi, i've updated the colab notebook with <denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>\n  's override of 's  method and updated the  to use take command line args as so it now runs without errors.\n WandbLogger already has the @rank_zero_only decorator applied to its finalize method, so shouldn't it already only be called on the main process?\n     @rank_zero_only\n     def finalize(self, status: str = 'success') -> None:\n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "bkkaggle", "commentT": "2020-03-18T23:06:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/bkkaggle>@bkkaggle</denchmark-link>\n  mind sending a PR?\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "bkkaggle", "commentT": "2020-03-19T01:21:33Z", "comment_text": "\n \t\tSure\n \t\t"}}}, "commit": {"commit_id": "a707d4bea1a78a98265fd1ea5b7a7a6cadc37fb9", "commit_author": "Bilal Khan", "commitT": "2020-03-30 18:45:06-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "35", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\loggers\\wandb.py", "file_new_name": "pytorch_lightning\\loggers\\wandb.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "105,106,107,108,109,110", "method_info": {"method_name": "finalize", "method_params": "self,str", "method_startline": "105", "method_endline": "110"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\loggers\\test_wandb.py", "file_new_name": "tests\\loggers\\test_wandb.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "33,34,35,36,37,38,39,40,41,42,43,44,45,46", "method_info": {"method_name": "test_wandb_logger", "method_params": "wandb", "method_startline": "13", "method_endline": "48"}}}}}}}