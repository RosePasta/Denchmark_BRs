{"BR": {"BR_id": "2188", "BR_author": "xiadingZ", "BRopenT": "2020-06-15T03:10:15Z", "BRcloseT": "2020-08-03T18:44:12Z", "BR_text": {"BRsummary": "[hparams] save_hyperparameters doesn't save kwargs", "BRdescription": "\n <denchmark-h:h2>\u2753 Questions and Help</denchmark-h>\n \n when I use hyperparemeters like docs:\n <denchmark-code>class LitMNIST(LightningModule):\n \n     def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):\n         super().__init__()\n         # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n         self.save_hyperparameters()\n \n </denchmark-code>\n \n model checkpoint doesn't save args in kwargs. But kwargs is important. Args such as num_frames, img_size, img_std ... must be used in creating dataloader, but it will be tedious if writes them in __init__ explicitly .  it can make code clean if hides them in kwargs.\n Before I use hparams, it's ok. But now it's not recommended to use hparams,  is there any good idea to  deal with this problem?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "xiadingZ", "commentT": "2020-06-15T03:24:08Z", "comment_text": "\n \t\tif don't use hparams, it will put all args of model, dataset, dataloader... in a LightnModule' s __init__ method, and save_hyperparameters  doesn't save args in kwargs. It this really a good idea?\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "xiadingZ", "commentT": "2020-06-15T17:21:33Z", "comment_text": "\n \t\tCould you please share your model example?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "xiadingZ", "commentT": "2020-06-16T02:50:51Z", "comment_text": "\n \t\t\n Could you please share your model example?\n \n <denchmark-code>class LitMNIST(LightningModule):\n \n     def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):\n         super().__init__()\n         # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n         self.save_hyperparameters()\n         self.kwargs = kwargs\n         ...\n     \n     def train_dataloader(self):\n         img_size = self.kwargs['img_size']\n         ...\n </denchmark-code>\n \n I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "xiadingZ", "commentT": "2020-06-16T14:34:35Z", "comment_text": "\n \t\t\n I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size\n \n I see, we need to ignore  from the model hparams saving...\n <denchmark-link:https://github.com/xiadingZ>@xiadingZ</denchmark-link>\n  mind adding PR with a test for this case and I ll finish it with a patch?\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "xiadingZ", "commentT": "2020-06-19T00:48:13Z", "comment_text": "\n \t\t<denchmark-code>Traceback (most recent call last):\n   File \"./training.py\", line 101, in <module>\n     main(hparam_trial)\n   File \"./training.py\", line 86, in main\n     model = module(hparams, fold_train, fold_val, data_dir+img_dir)\n   File \"../main/module.py\", line 18, in __init__\n     self.hparams = hparams\n   File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 638, in __setattr__\n     object.__setattr__(self, name, value)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1695, in hparams\n     self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1662, in save_hyperparameters\n     cand_names = [k for k, v in init_args.items() if v == hp]\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1662, in <listcomp>\n     cand_names = [k for k, v in init_args.items() if v == hp]\n   File \"/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\", line 1479, in __nonzero__\n     f\"The truth value of a {type(self).__name__} is ambiguous. \"\n ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n </denchmark-code>\n \n I'm guessing this is why 0.8.0 causes this error, it's trying to save all args (including dataframes in my case) outside of hparams?\n Edit: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2250>#2250</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "6ae9a97b09fd8e3239219c2882c6f3cc31a2ccf8", "commit_author": "William Falcon", "commitT": "2020-06-18 23:08:25-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\core\\lightning.py", "file_new_name": "pytorch_lightning\\core\\lightning.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1696,1697,1698", "deleted_lines": "1695", "method_info": {"method_name": "hparams", "method_params": "self,dict,Namespace", "method_startline": "1695", "method_endline": "1698"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712", "deleted_lines": null, "method_info": {"method_name": "__get_hparams_assignment_variable", "method_params": "self", "method_startline": "1700", "method_endline": "1712"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\saving.py", "file_new_name": "pytorch_lightning\\core\\saving.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "179,184,187,190", "deleted_lines": null, "method_info": {"method_name": "_load_model_state", "method_params": "cls,str,args,kwargs", "method_startline": "175", "method_endline": "207"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\models\\test_hparams.py", "file_new_name": "tests\\models\\test_hparams.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "42", "deleted_lines": "42", "method_info": {"method_name": "_run_standard_hparams_test", "method_params": "tmpdir,model,cls,try_overwrite", "method_startline": "33", "method_endline": "62"}}}}}}}