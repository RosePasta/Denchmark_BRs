{"BR": {"BR_id": "3993", "BR_author": "hbredin", "BRopenT": "2020-10-08T15:07:54Z", "BRcloseT": "2020-10-09T03:02:24Z", "BR_text": {"BRsummary": "Mismatch between docstring and code regarding when `on_load_checkpoint` hook is called", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n The docstring of on_load_checkpoint\u00a0hook says that it is called before trying to load_state_dict:\n \n \n \n pytorch-lightning/pytorch_lightning/core/saving.py\n \n \n         Lines 203 to 206\n       in\n       cea5f1f\n \n \n \n \n \n \n  def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None: \n \n \n \n  \"\"\" \n \n \n \n          Do something with the checkpoint. \n \n \n \n          Gives model a chance to load something before ``state_dict`` is restored. \n \n \n \n \n \n However, in LightningModule.load_from_checkpoint, it is called after load_state_dict:\n \n \n \n pytorch-lightning/pytorch_lightning/core/saving.py\n \n \n         Lines 195 to 199\n       in\n       cea5f1f\n \n \n \n \n \n \n  # load the state_dict on the model automatically \n \n \n \n  model.load_state_dict(checkpoint['state_dict'], strict=strict) \n \n \n \n  \n \n \n \n  # give model a chance to load something \n \n \n \n  model.on_load_checkpoint(checkpoint) \n \n \n \n \n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n Related discussion on Slack: <denchmark-link:https://pytorch-lightning.slack.com/archives/CQXV8BRH9/p1602168345184000>https://pytorch-lightning.slack.com/archives/CQXV8BRH9/p1602168345184000</denchmark-link>\n \n I think the docstring is correct and the call to on_load_checkpoint\u00a0should be moved right before load_state_dict\u00a0to give the model a chance to call setup.\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "hbredin", "commentT": "2020-10-08T15:46:44Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/hbredin>@hbredin</denchmark-link>\n  mind sending a PR to fix it... \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "hbredin", "commentT": "2020-10-08T15:47:39Z", "comment_text": "\n \t\tI can do that. Should I fix the docstring or the code?\n I'd go with the code.\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "hbredin", "commentT": "2020-10-09T03:03:33Z", "comment_text": "\n \t\tI need this code change as well! I'm doing transfer learning and I want to support both loading the original model with the original weights, and modify it for a new task.\n on_load_checkpoint would allow me to redo the modifications I've done for transfer learning, so the state_dict of the transferred model can be properly restored.\n At present I need to add non-network code to the model to handle this logic, which is ugly and prone to bugs.\n This would allow to have the same model to redo the modifications I've made for transfer learning,\n \t\t"}}}, "commit": {"commit_id": "a8573b005224bde87eb7a81ccdf5f428620c121f", "commit_author": "Herv\u00e9 BREDIN", "commitT": "2020-10-08 23:02:23-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\saving.py", "file_new_name": "pytorch_lightning\\core\\saving.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "198,199,200", "deleted_lines": "195,196,197", "method_info": {"method_name": "_load_model_state", "method_params": "cls,str,bool,cls_kwargs_new", "method_startline": "157", "method_endline": "201"}}}}}}}