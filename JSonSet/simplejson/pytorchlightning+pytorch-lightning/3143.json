{"BR": {"BR_id": "3143", "BR_author": "24hours", "BRopenT": "2020-08-25T03:25:50Z", "BRcloseT": "2020-09-10T21:01:21Z", "BR_text": {"BRsummary": "Trainer crashed when optimizer frequency is defined.", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n Run the following code:\n <denchmark-link:https://gist.github.com/24hours/ec67de5384bb05e28544d580ae424639>https://gist.github.com/24hours/ec67de5384bb05e28544d580ae424639</denchmark-link>\n \n <denchmark-code>Traceback (most recent call last):\n   File \"pl_bug.py\", line 40, in <module>\n     trainer.fit(mnist_model, train_loader) \n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py\", line 48, in wrapped_fn\n     result = fn(self, *args, **kwargs)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1073, in fit\n     results = self.accelerator_backend.train(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_backend.py\", line 51, in train\n     results = self.trainer.run_pretrain_routine(model)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1239, in run_pretrain_routine\n     self.train()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 394, in train\n     self.run_training_epoch()\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 491, in run_training_epoch\n     batch_output = self.run_training_batch(batch, batch_idx)\n   File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 883, in run_training_batch\n     batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end)\n </denchmark-code>\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n     def configure_optimizers(self):\n         optimizer_G = torch.optim.Adam(self.parameters(), lr=0.1, weight_decay=1e-5)\n         optimizer_D = torch.optim.Adam(self.parameters(), lr=0.1, weight_decay=1e-5)\n \n         return [\n                 {'optimizer': optimizer_D, 'frequency': 5},\n                 {'optimizer': optimizer_G, 'frequency': 1}\n             ]\n the culprit is 'frequency' : 5, removing the line will allow trainer to run smoothly.\n <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html?highlight=optimizers#configure-optimizers>https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html?highlight=optimizers#configure-optimizers</denchmark-link>\n \n The definition is correct according to this documentation.\n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Model should train without crash.\n The code work in 0.8.5 environment.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n \n GPU:\n \n GeForce GTX TITAN X\n \n \n available:         True\n version:           11.0\n \n \n Packages:\n \n numpy:             1.18.5\n pyTorch_debug:     False\n pyTorch_version:   1.6.0a0+9907a3e\n pytorch-lightning: 0.9.0\n tensorboard:       2.2.0\n tqdm:              4.48.2\n \n \n System:\n \n OS:                Linux\n architecture:\n \n 64bit\n \n \n \n processor:         x86_64\n python:            3.6.10\n version:           #110-Ubuntu SMP Tue Jun 23 02:39:32 UTC 2020\n \n \n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "24hours", "commentT": "2020-08-25T03:26:32Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "24hours", "commentT": "2020-08-25T07:08:55Z", "comment_text": "\n \t\tthis is the case with multiple optimizers, you need to spec them.. so you would prefer having default 1 if freq is not specified?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "24hours", "commentT": "2020-08-25T10:24:40Z", "comment_text": "\n \t\tthe exception occur because trainer_loop.py incorrect count number of optimizer if frequency is defined. Since this configuration work in version 0.8.5, this look like regression error.\n Unless of course if the configuration is unsupported in version 0.9.0\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "24hours", "commentT": "2020-08-25T22:23:14Z", "comment_text": "\n \t\t\n so you would prefer having default 1 if freq is not specified?\n \n That is a totally different case I think. If the frequency is not specified we run train_step for both optimizers but if it is specified to 1 for both then in such case it will run 1st batch for opt_1, 2nd for opt_2, 3rd for opt_1, 4th for opt_2...\n A simple fix here can be:\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n          Line 872\n       in\n       a7705c8\n \n \n \n \n \n \n  batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end) \n \n \n \n \n \n if len(batch_outputs) == 1:  # when frequencies are defined\n     batch_outputs[0].append(opt_closure_result.training_step_output_for_epoch_end)\n else:  # no frequencies\n     batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end)\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "24hours", "commentT": "2020-08-27T14:12:39Z", "comment_text": "\n \t\tok, can someone write a test and submit a PR for this? show the test failing on master first.\n <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>\n   or <denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>\n  ?\n \t\t"}}}, "commit": {"commit_id": "a1ea681c47004599ee5a47a05ddd1b4ea12e60d4", "commit_author": "Rohit Gupta", "commitT": "2020-09-10 23:01:20+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "40,41", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "docs\\source\\converting.rst", "file_new_name": "docs\\source\\converting.rst", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19,21,39,41,51,53,81,83", "deleted_lines": "19,21,39,41,51,53,81,83"}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "503,504", "deleted_lines": "503", "method_info": {"method_name": "run_training_batch", "method_params": "self,batch,batch_idx,dataloader_idx", "method_startline": "431", "method_endline": "553"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\base\\model_optimizers.py", "file_new_name": "tests\\base\\model_optimizers.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "37,38,39,40,41,42,43", "deleted_lines": null, "method_info": {"method_name": "configure_optimizers__multiple_optimizers_frequency", "method_params": "self", "method_startline": "37", "method_endline": "43"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\trainer\\test_optimizers.py", "file_new_name": "tests\\trainer\\test_optimizers.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "245,246,247,248,249,250,251,252,253,254,255,256,257", "deleted_lines": null, "method_info": {"method_name": "test_configure_optimizers_with_frequency", "method_params": "tmpdir", "method_startline": "245", "method_endline": "257"}}}}}}}