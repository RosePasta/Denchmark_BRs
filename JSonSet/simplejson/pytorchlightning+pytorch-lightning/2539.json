{"BR": {"BR_id": "2539", "BR_author": "YuxianMeng", "BRopenT": "2020-07-07T11:30:39Z", "BRcloseT": "2020-07-10T01:28:12Z", "BR_text": {"BRsummary": "TPU fp16 requires apex installed", "BRdescription": "\n When I tried to use precision=16 on TPU, pytorch-lightning is trying to find amp, which is unnecessary.\n The backtrace is\n <denchmark-code>GPU available: False, used: False\n TPU available: True, using: 8 TPU cores\n Traceback (most recent call last):\n   File \"bert_ner/light/fp16_debug.py\", line 16, in <module>\n     trainer = pl.Trainer(tpu_cores=8, precision=16)\n   File \"/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 607, in __init__\n     self.init_amp()\n   File \"/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/auto_mix_precision.py\", line 27, in init_amp\n     \"You set `use_amp=True` but do not have apex installed.\"\n ModuleNotFoundError: You set `use_amp=True` but do not have apex installed.Install apex first using this guide and rerun with use_amp=True:https://github.com/NVIDIA/apex#linux his run will NOT use 16 bit precision\n </denchmark-code>\n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n build a whatever Trainer in TPU and use fp16\n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>import pytorch_lightning as pl\n \n trainer = pl.Trainer(tpu_cores=8, precision=16)\n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Should have nothing error.\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n PyTorch Version (e.g., 1.5.0):\n OS (e.g., Linux): Linux\n How you installed PyTorch (conda, pip, source): conda\n Build command you used (if compiling from source):\n Python version:\n CUDA/cuDNN version:\n GPU models and configuration:\n Any other relevant information: actually I directly use pytorch-xla-1.5 docker on Google Cloud\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "YuxianMeng", "commentT": "2020-07-07T11:31:35Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "YuxianMeng", "commentT": "2020-07-07T12:26:33Z", "comment_text": "\n \t\tIf you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.\n I could get this working using nightly version of pytorch:\n <denchmark-code>pl.Trainer(precision=16, tpu_cores=8)\n >>>GPU available: False, used: False\n >>>TPU available: True, using: 8 TPU cores\n >>>Using native 16bit precision.\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "YuxianMeng", "commentT": "2020-07-08T03:23:51Z", "comment_text": "\n \t\t\n If you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.\n I could get this working using nightly version of pytorch:\n pl.Trainer(precision=16, tpu_cores=8)\n >>>GPU available: False, used: False\n >>>TPU available: True, using: 8 TPU cores\n >>>Using native 16bit precision.\n \n \n Thanks for the quick reply. But <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/apex.html>the document</denchmark-link>\n  does not point out that I must have nightly version of pytorch installed or have apex installed when training on TPU with fp16. Maybe it's better to revise that part of document?\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "YuxianMeng", "commentT": "2020-07-08T09:56:58Z", "comment_text": "\n \t\tYes, I agree that from the documentation it would look like it is only a requirement for gpu training. I guess that the specific requirement for TPU is to have pytorch version 1.6 or higher.\n \t\t"}}}, "commit": {"commit_id": "e068af9ea8c86df8ed5eb20e57a36fbb38c70462", "commit_author": "William Falcon", "commitT": "2020-07-09 21:28:11-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\core\\memory.py", "file_new_name": "pytorch_lightning\\core\\memory.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "212", "deleted_lines": "212", "method_info": {"method_name": "_forward_example_input", "method_params": "self", "method_startline": "204", "method_endline": "226"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\distrib_parts.py", "file_new_name": "pytorch_lightning\\trainer\\distrib_parts.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "243,250", "deleted_lines": "243,250", "method_info": {"method_name": "dp_train", "method_params": "self,model", "method_startline": "229", "method_endline": "273"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "file_new_name": "pytorch_lightning\\trainer\\evaluation_loop.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "289", "deleted_lines": "289"}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "pytorch_lightning\\trainer\\trainer.py", "file_new_name": "pytorch_lightning\\trainer\\trainer.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "1303,1304,1305,1306,1307", "deleted_lines": null, "method_info": {"method_name": "__test_using_best_weights", "method_params": "self,ckpt_path,test_dataloaders", "method_startline": "1287", "method_endline": "1330"}}, "hunk_1": {"Ismethod": 1, "added_lines": "1121", "deleted_lines": "1121", "method_info": {"method_name": "run_pretrain_routine", "method_params": "self,LightningModule", "method_startline": "1104", "method_endline": "1213"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_io.py", "file_new_name": "pytorch_lightning\\trainer\\training_io.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "361", "deleted_lines": "361", "method_info": {"method_name": "dump_checkpoint", "method_params": "self,bool", "method_startline": "316", "method_endline": "381"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "705", "deleted_lines": "705", "method_info": {"method_name": "run_batch_backward_pass", "method_params": "self,split_batch,batch_idx,opt_idx,optimizer", "method_startline": "690", "method_endline": "714"}}, "hunk_1": {"Ismethod": 1, "added_lines": "770,820", "deleted_lines": "770,820", "method_info": {"method_name": "optimizer_closure", "method_params": "self,split_batch,batch_idx,opt_idx,optimizer,hiddens", "method_startline": "762", "method_endline": "847"}}, "hunk_2": {"Ismethod": 1, "added_lines": "753", "deleted_lines": "753", "method_info": {"method_name": "call_optimizer_step", "method_params": "self,optimizer,opt_idx,batch_idx,split_batch", "method_startline": "716", "method_endline": "760"}}}}}}}