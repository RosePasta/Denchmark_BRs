{"BR": {"BR_id": "3487", "BR_author": "Tim-Chard", "BRopenT": "2020-09-13T11:27:42Z", "BRcloseT": "2020-09-15T16:41:28Z", "BR_text": {"BRsummary": "Gradient norms are not logged unless row_log_interval==1", "BRdescription": "\n <denchmark-h:h2>\ud83d\udc1b Bug</denchmark-h>\n \n In version 0.9 the guards to calculate the gradient norms and then log the metrics can't be satisfied in the same batch unless the row_log_interval  is 1. In most places the guard seems to be (batch_idx + 1) % self.row_log_interval == 0 such as here:\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 749 to 757\n       in\n       b40de54\n \n \n \n \n \n \n  def save_train_loop_metrics_to_loggers(self, batch_idx, batch_output): \n \n \n \n  # when metrics should be logged \n \n \n \n  should_log_metrics = (batch_idx + 1) % self.row_log_interval == 0 or self.should_stop \n \n \n \n  if should_log_metrics or self.fast_dev_run: \n \n \n \n  # logs user requested information to logger \n \n \n \n  metrics = batch_output.batch_log_metrics \n \n \n \n  grad_norm_dic = batch_output.grad_norm_dic \n \n \n \n  if len(metrics) > 0 or len(grad_norm_dic) > 0: \n \n \n \n  self.log_metrics(metrics, grad_norm_dic) \n \n \n \n \n \n However in run_batch_backward_pass it is batch_idx % self.row_log_interval == 0\n \n \n \n pytorch-lightning/pytorch_lightning/trainer/training_loop.py\n \n \n         Lines 929 to 939\n       in\n       b40de54\n \n \n \n \n \n \n  def run_batch_backward_pass(self, split_batch, batch_idx, opt_idx, optimizer): \n \n \n \n  # ------------------ \n \n \n \n  # GRAD NORMS \n \n \n \n  # ------------------ \n \n \n \n  # track gradient norms when requested \n \n \n \n  grad_norm_dic = {} \n \n \n \n  if batch_idx % self.row_log_interval == 0: \n \n \n \n  if float(self.track_grad_norm) > 0: \n \n \n \n  model = self.get_model() \n \n \n \n  grad_norm_dic = model.grad_norm( \n \n \n \n  self.track_grad_norm) \n \n \n \n \n \n <denchmark-h:h3>To Reproduce</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Run the code sample below (taken from #1527 ).\n Confirm that gradients are not being logged in tensorboard.\n Change row_log_interval to 1 and rerun the code.\n 4 Confirm that gradients are now being logged.\n \n <denchmark-h:h4>Code sample</denchmark-h>\n \n <denchmark-code>import pytorch_lightning as pl\n from torch.utils.data import TensorDataset, DataLoader\n from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n from torch.optim import SGD\n import torch.nn as nn\n import torch\n \n \n class MWENet(pl.LightningModule):\n     def __init__(self):\n         super(MWENet, self).__init__()\n \n         self.first = nn.Conv2d(1, 1, 3)\n         self.second = nn.Conv2d(1, 1, 3)\n         self.loss = nn.L1Loss()\n \n     def train_dataloader(self):\n         xs, ys = torch.zeros(16, 1, 10, 10), torch.ones(16, 1, 6, 6)\n         ds = TensorDataset(xs, ys)\n         return DataLoader(ds)\n \n     def forward(self, xs):\n         out = self.first(xs)\n         out = self.second(out)\n         return out\n \n     def configure_optimizers(self):\n         first = SGD(self.first.parameters(), lr=0.01)\n         second = SGD(self.second.parameters(), lr=0.01)\n         return [second, first]\n \n     def training_step(self, batch, batch_idx, optimizer_idx):\n         xs, ys = batch\n         out = self.forward(xs)\n         return {'loss': self.loss(out, ys)}\n \n \n net = MWENet()\n logger = TensorBoardLogger('tb_logs', name='testing')\n trainer = pl.Trainer(\n     track_grad_norm=2,\n     row_log_interval=2,\n     max_epochs=50,\n     logger=logger)\n trainer.fit(net)\n \n </denchmark-code>\n \n <denchmark-h:h3>Expected behavior</denchmark-h>\n \n Gradients should be logged if track_grad_norm is True\n <denchmark-h:h3>Environment</denchmark-h>\n \n \n CUDA:\n - GPU:\n - GeForce GTX 1080 Ti\n - available:         True\n - version:           10.2\n Packages:\n - numpy:             1.19.1\n - pyTorch_debug:     False\n - pyTorch_version:   1.6.0\n - pytorch-lightning: 0.9.0\n - tensorboard:       2.2.1\n - tqdm:              4.48.2\n System:\n - OS:                Windows\n - architecture:\n - 64bit\n - WindowsPE\n - processor:         AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\n - python:            3.7.9\n - version:           10.0.19041\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "Tim-Chard", "commentT": "2020-09-13T11:28:24Z", "comment_text": "\n \t\tHi! thanks for your contribution!, great first issue!\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "Tim-Chard", "commentT": "2020-09-13T11:44:15Z", "comment_text": "\n \t\tyeah, it's a bug, mind send a PR?\n \t\t"}}}, "commit": {"commit_id": "4ed96b2eb471124184144f96d259055e49ac97e7", "commit_author": "Adrian W\u00e4lchli", "commitT": "2020-09-15 18:41:27+02:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "CHANGELOG.md", "file_new_name": "CHANGELOG.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "61,62", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "pytorch_lightning\\trainer\\training_loop.py", "file_new_name": "pytorch_lightning\\trainer\\training_loop.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "379,380,383,384", "deleted_lines": "379,380,383,384", "method_info": {"method_name": "_track_gradient_norm", "method_params": "self,batch_idx", "method_startline": "378", "method_endline": "384"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\models\\test_grad_norm.py", "file_new_name": "tests\\models\\test_grad_norm.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101", "deleted_lines": null, "method_info": {"method_name": "test_grad_tracking_interval", "method_params": "tmpdir,row_log_interval", "method_startline": "80", "method_endline": "101"}}}}}}}