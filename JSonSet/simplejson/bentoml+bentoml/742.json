{"BR": {"BR_id": "742", "BR_author": "palver7", "BRopenT": "2020-05-30T02:01:27Z", "BRcloseT": "2020-07-20T16:09:18Z", "BR_text": {"BRsummary": "got internal server error when trying to invoke sagemaker endpoint", "BRdescription": "\n \n I am following the tutorial described in <denchmark-link:https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html>https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html</denchmark-link>\n . I got the expected results until I get to this part\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83316687-6e039280-a251-11ea-835f-d0c459f250f8.png></denchmark-link>\n \n Instead of the output with the red characters, I got this message :\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83316763-10237a80-a252-11ea-9409-6ae6d666d6dc.png></denchmark-link>\n \n I did as the message said and checked the associated CloudWatch log and this are the error messages\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83316799-5082f880-a252-11ea-8b3d-a45b331273ba.png></denchmark-link>\n \n To Reproduce\n Steps to reproduce the behavior:\n \n follow the tutorial described in  https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html . (The only thing different is the region, I set it to ap-southeast-1)\n \n Expected behavior\n the output should be like the first screenshot above.\n Environment:\n \n OS: [Linux Mint 19.3 Tricia Cinnamon]\n Python/BentoML Version [e.g. Python 3.7.4, BentoML 0.7.8]\n \n Additional context\n What I tried  to solve this error:\n The first message about unpickling svm created with scikit-learn 0.21.3 with scikit-learn 0.23.1 may causes errors made me upgrade my local scikit-learn package from 0.21.3 to 0.23.1 but it did nothing. I assume that the scikit-learn package referred here is the scikit-learn inside the docker image for aws sagemaker ?\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "palver7", "commentT": "2020-06-01T10:55:57Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yubozhao>@yubozhao</denchmark-link>\n  update: I tried another tutorial, this time the tensorflow Fashion-MNIST classifier (<denchmark-link:https://github.com/bentoml/gallery/blob/master/tensorflow/fashion-mnist/tensorflow_2_fashion_mnist.ipynb>https://github.com/bentoml/gallery/blob/master/tensorflow/fashion-mnist/tensorflow_2_fashion_mnist.ipynb</denchmark-link>\n ) and tried the to deploy the bentoML service to AWS sagemaker. I ran the tutorial with JupyterLab on my computer.  When I get to the invoke endpoint step, a similar error occurred :\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83401771-c55f5980-a42f-11ea-8172-c863ca0d3a22.png></denchmark-link>\n \n and the CloudWatch logs show these error messages :\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83402263-9695b300-a430-11ea-9a01-6f1bb4acbe86.png></denchmark-link>\n \n It seems that the bug is not specific to iris classifier tutorial then.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "palver7", "commentT": "2020-06-01T20:23:44Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n \n It looks like this is an issue of decoding  for the webserver. I was unable to reproduce it with the fashion mnist example notebook with the latest master branch with the latest commit of <denchmark-link:https://github.com/bentoml/BentoML/commit/5d23859bb34f7e1942db31435273f9255929efc0>5d23859</denchmark-link>\n  and with the latest release version \n \n \n Can you make sure the request is properly encoded with utf-8?\n \n \n Can you share the bentoml.yml file that is generated in your BentoService archive?  It should be located in the BentoML repository directory(~/bentoml/repository/BENTO_SERVICE_NAME/VERSION).\n \n \n Can you build and run a docker container and then try to make a request to see if it works properly? It is in the Build realtime prediction service in docker with BentoService section of the fashion mnist notebook.\n \n To make a curl request:\n echo '{\\\"instances\\\":[{\\\"b64\\\":\\\"'$(base64 test.png)'\\\"}]}' > test.json\n curl -X POST \"http://127.0.0.1:5000/predict\" -H \"Content-Type: application/json\" --data-binary @test.json\n \n \n \n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "palver7", "commentT": "2020-06-02T04:23:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yubozhao>@yubozhao</denchmark-link>\n   Here is the bentoml.yml file\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83476487-1700f600-a4bb-11ea-8453-42f756123bf5.png></denchmark-link>\n \n Regarding point 1, how to check the encoding of the request ?\n About point 3. I ran the docker and ran the curl request but I get a 400 bad request error\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83479587-3734b300-a4c3-11ea-8a78-a90805b6b003.png></denchmark-link>\n \n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "palver7", "commentT": "2020-06-02T04:29:10Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n  for bullet point 1.  Can you try either  and/or  in your terminal and see what is the result is?\n For my MacOS, the value is . Yours should be something similar.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "palver7", "commentT": "2020-06-02T05:09:20Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/yubozhao>@yubozhao</denchmark-link>\n   typing echo $LC_CTYPE prints empty line but typing echo $LANG prints en_US.UTF-8\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "palver7", "commentT": "2020-06-02T06:17:12Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n  could you share what's the output of  after the  line?\n Could you also try sending the prediction request from the docker container running the API server? You will need to first get the ID of the docker container running the BentoML model server, it can be found in the docker ps list after running the container. You can access the running container with the follow command:\n docker exec -it 3afe2f815c3b bash\n And then within the container, try sending the request to localhost:\n apt-get update\n apt-get install curl\n echo \"...\" > test.json\n curl ...\n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "palver7", "commentT": "2020-06-02T07:50:54Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/parano>@parano</denchmark-link>\n  the output of file -i test.json: test.json:\n text/plain; charset=us-ascii\n sending the request to localhost inside the container still give the same error message as before :\n <denchmark-link:https://user-images.githubusercontent.com/42964197/83494415-6f96ba00-a4e0-11ea-90b8-8c7c9be6362a.png></denchmark-link>\n \n However, if I edit the test.json file inside the container so that the double quotes inside the braces are not preceded by backslashes then the prediction 'ankleboot' is printed out.\n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "palver7", "commentT": "2020-06-24T04:30:41Z", "comment_text": "\n \t\tHi <denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n , sorry for the delayed reply on this one,  could you add  option to your curl command so we can see the full request header? thanks!\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "palver7", "commentT": "2020-06-24T16:17:13Z", "comment_text": "\n \t\t\n Hi @palver7, sorry for the delayed reply on this one, could you add -v option to your curl command so we can see the full request header? thanks!\n \n <denchmark-link:https://github.com/parano>@parano</denchmark-link>\n , here is the output with the test.json created as directed\n <denchmark-link:https://user-images.githubusercontent.com/42964197/85592892-c1aaa580-b670-11ea-98cf-48c5c34e8c57.png></denchmark-link>\n \n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "palver7", "commentT": "2020-06-24T16:31:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n  did you specify content-type header in this CURL command? could you include the curl command used in the screenshot?\n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "palver7", "commentT": "2020-06-24T16:35:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/parano>@parano</denchmark-link>\n  here is the command that produces the output:\n curl -X POST \"<denchmark-link:http://127.0.0.1:5000/predict>http://127.0.0.1:5000/predict</denchmark-link>\n \" -H \"Content-Type: application/json\" --data-binary @test.json -v\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "palver7", "commentT": "2020-06-24T16:41:40Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n   I see, I think it should not be  in this case, can you try changing it from  to  instead?\n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "palver7", "commentT": "2020-06-24T16:48:26Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/parano>@parano</denchmark-link>\n  with this command : curl -X POST \"<denchmark-link:http://127.0.0.1:5000/predict>http://127.0.0.1:5000/predict</denchmark-link>\n \" -H \"Content-Type: application/json\" -d @test.json -v\n the error message is still the same, ending with\n The browser (or proxy) sent a request that this server could not understand.\n \n * Closing connection 0\n       \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "palver7", "commentT": "2020-06-26T13:20:25Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/parano>@parano</denchmark-link>\n  I have four  cases in trying to deploy prediction on my local machine:\n \n if I change the content of test.json file from\n {\"instances\":[{\"b64\":\"...\"}]}\n to\n {\"instances\":[{\"b64\":\"...\"}]}\n and run the the curl as above then I get \"Ankle boot\" with no error messages.\n \n 2. if I changed the test.json file as above but implementing my custom tensorflow handler class in the service bundle file as per your suggestion I got the error.\n <denchmark-link:https://user-images.githubusercontent.com/42964197/85861580-cb581880-b7ea-11ea-8f94-05fac4141553.png></denchmark-link>\n \n As you can see, I got the 500 internal server error message instead of bad request.\n \n \n If I do as 2. but creating the custom tensor handler class by copy pasting the content of the tensorflow tensor handler.py into the service bundle python file then I get the prediction \"Ankle boot\".\n \n \n Finally, whether I created the custom tensor handler class or not, if I left the content of the test.json file as generated by the notebook tutorial :\n {\"instances\":[{\"b64\":\"...\"}]}\n I would get the bad request and the proxy sent a request that the server could not understand like the previous post.\n \n \n Edit: The error in <denchmark-link:https://github.com/bentoml/BentoML/pull/2>#2</denchmark-link>\n  is not because of your suggestion but because I was incomplete in defining my custom tensor handler class.  I need to  import several functions needed in the custom handle_request method.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "palver7", "commentT": "2020-07-12T03:07:22Z", "comment_text": "\n \t\tHey <denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n  - are you still experiencing this issue with the latest release of BentoML?  It might be helpful for us to do a quick call and have you show me how you reproduce the issue and we can do an interactive debug session.  Feel free to ping me in the BentoML community slack channel if you'd like to do that.\n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "palver7", "commentT": "2020-07-14T12:25:16Z", "comment_text": "\n \t\t\n Hey @palver7 - are you still experiencing this issue with the latest release of BentoML? It might be helpful for us to do a quick call and have you show me how you reproduce the issue and we can do an interactive debug session. Feel free to ping me in the BentoML community slack channel if you'd like to do that.\n \n <denchmark-link:https://github.com/parano>@parano</denchmark-link>\n   What time are you available for the interactive debug session ?\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "palver7", "commentT": "2020-07-14T16:53:41Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n  could you email me or DM me in BentoML slack?\n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "palver7", "commentT": "2020-07-20T16:09:18Z", "comment_text": "\n \t\tDiscussed with <denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n  - turns out this is an issue with our gallery notebook:\n <denchmark-link:https://github.com/bentoml/gallery/blob/master/tensorflow/fashion-mnist/tensorflow_2_fashion_mnist.ipynb>https://github.com/bentoml/gallery/blob/master/tensorflow/fashion-mnist/tensorflow_2_fashion_mnist.ipynb</denchmark-link>\n \n Closing this issue for now since the descriptions and discussions here are no longer relevant to this issue. We will investigate the issue with the tensorflow_2_fashion_mnist notebook and open a new issue if necessary.\n Thanks <denchmark-link:https://github.com/palver7>@palver7</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "150bc656f53b7c2d2b0981d3a5589443e78b394c", "commit_author": "Chaoyu", "commitT": "2020-06-23 22:22:19-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "bentoml\\adapters\\dataframe_input.py", "file_new_name": "bentoml\\adapters\\dataframe_input.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "149,155", "deleted_lines": "149,155", "method_info": {"method_name": "handle_request", "method_params": "self,Request,func", "method_startline": "147", "method_endline": "170"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "bentoml\\adapters\\json_input.py", "file_new_name": "bentoml\\adapters\\json_input.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "41", "deleted_lines": "41", "method_info": {"method_name": "handle_request", "method_params": "self,Request,func", "method_startline": "39", "method_endline": "49"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "bentoml\\artifact\\json_artifact.py", "file_new_name": "bentoml\\artifact\\json_artifact.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "21,22", "deleted_lines": "21", "method_info": {"method_name": "__init__", "method_params": "self,name,file_extension,encoding,json_module", "method_startline": "21", "method_endline": "22"}}, "hunk_1": {"Ismethod": 1, "added_lines": "21,22,23", "deleted_lines": "21", "method_info": {"method_name": "__init__", "method_params": "self,name,file_extension,encoding,json_module", "method_startline": "21", "method_endline": "30"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "bentoml\\artifact\\keras_model_artifact.py", "file_new_name": "bentoml\\artifact\\keras_model_artifact.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "242,243,244", "deleted_lines": "239", "method_info": {"method_name": "save", "method_params": "self,dst", "method_startline": "239", "method_endline": "259"}}, "hunk_1": {"Ismethod": 1, "added_lines": "192", "deleted_lines": "189", "method_info": {"method_name": "load", "method_params": "self,path", "method_startline": "189", "method_endline": "225"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "bentoml\\artifact\\text_file_artifact.py", "file_new_name": "bentoml\\artifact\\text_file_artifact.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33", "deleted_lines": "33", "method_info": {"method_name": "__init__", "method_params": "self,name,file_extension,encoding", "method_startline": "33", "method_endline": "36"}}, "hunk_1": {"Ismethod": 1, "added_lines": "33", "deleted_lines": "33", "method_info": {"method_name": "__init__", "method_params": "self,name,file_extension,encoding", "method_startline": "33", "method_endline": "36"}}}}, "file_5": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "bentoml\\cli\\config.py", "file_new_name": "bentoml\\cli\\config.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "87", "deleted_lines": "83", "method_info": {"method_name": "get_configuration_sub_command.set_command", "method_params": "updates", "method_startline": "83", "method_endline": "107"}}, "hunk_1": {"Ismethod": 1, "added_lines": "64", "deleted_lines": null, "method_info": {"method_name": "get_configuration_sub_command.view", "method_params": "", "method_startline": "61", "method_endline": "66"}}, "hunk_2": {"Ismethod": 1, "added_lines": "64,87,118", "deleted_lines": "60,83,114", "method_info": {"method_name": "get_configuration_sub_command", "method_params": "", "method_startline": "51", "method_endline": "149"}}, "hunk_3": {"Ismethod": 1, "added_lines": "118", "deleted_lines": "114", "method_info": {"method_name": "get_configuration_sub_command.unset", "method_params": "updates", "method_startline": "114", "method_endline": "137"}}}}, "file_6": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "bentoml\\configuration\\__init__.py", "file_new_name": "bentoml\\configuration\\__init__.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "104,114,115,116", "deleted_lines": "102,112", "method_info": {"method_name": "load_config", "method_params": "", "method_startline": "91", "method_endline": "120"}}}}, "file_7": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\handlers\\test_dataframe_handler.py", "file_new_name": "tests\\handlers\\test_dataframe_handler.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "118", "deleted_lines": "118", "method_info": {"method_name": "test_dataframe_handle_request_csv", "method_params": "", "method_startline": "113", "method_endline": "125"}}}}}}}