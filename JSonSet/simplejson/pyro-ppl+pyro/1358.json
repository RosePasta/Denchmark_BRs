{"BR": {"BR_id": "1358", "BR_author": "neerajprad", "BRopenT": "2018-09-04T02:35:26Z", "BRcloseT": "2018-09-19T18:40:09Z", "BR_text": {"BRsummary": "JitTrace*ELBO does not work with irregular batch sizes", "BRdescription": "\n With <denchmark-link:https://github.com/pytorch/pytorch/pull/10952>pytorch/pytorch#10952</denchmark-link>\n , our VAE example becomes runnable, except that it fails on the last data batch in the epoch which has size  instead of the usual  (runs fine with regular batch sizes).\n A minimal example that fails is below.\n import torch\n from torch.distributions import constraints\n \n import pyro\n import pyro.distributions as dist\n from pyro.infer import SVI, JitTrace_ELBO\n from pyro.optim import Adam\n \n \n def model(data):\n     loc = pyro.param(\"loc\", torch.tensor(0.0))\n     scale = pyro.param(\"scale\", torch.tensor(1.0), constraint=constraints.positive)\n     with pyro.iarange(\"data\", data.shape[0], dim=-2):\n         pyro.sample(\"x\",\n                     dist.Normal(loc, scale).expand([data.shape[0]]),\n                     obs=data)\n \n \n def guide(data):\n     pass\n \n \n pyro.clear_param_store()\n elbo = JitTrace_ELBO()\n inference = SVI(model, guide, Adam({\"lr\": 1e-6}), elbo)\n inference.step(torch.ones(10))\n inference.step(torch.ones(3))  # fails; see error below\n which results in the same error:\n  Stack Trace \n clang: error: unsupported option '-fopenmp'\n clang: error: unsupported option '-fopenmp'\n warning: pytorch jit fuser failed to compile with openmp, trying without it...\n Traceback (most recent call last):\n   File \"examples/expand.py\", line 61, in <module>\n     svi()\n   File \"examples/expand.py\", line 58, in svi\n     inference.step(torch.ones(3))\n   File \"/Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/svi.py\", line 96, in step\n     loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)\n   File \"/Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py\", line 202, in loss_and_grads\n     loss, surrogate_loss = self._loss_and_surrogate_loss(*args)\n   File \"/Users/npradhan/workspace/pyro_dev/pyro/pyro/ops/jit.py\", line 59, in __call__\n     ret = self.compiled[argc](*params_and_args)\n   File \"/Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/jit/__init__.py\", line 736, in forward\n     return self._get_method('forward')(*args, **kwargs)\n RuntimeError:\n The expanded size of the tensor (10) must match the existing size (3) at non-singleton dimension 1 (inferExpandGeometry at /Users/npradhan/workspace/pyro_dev/pytorch/pytorch/aten/src/ATen/ExpandUtils.cpp:71)\n frame #0: at::native::expand(at::Tensor const&, at::ArrayRef<long long>, bool) + 2785 (0x11c6a6ae1 in libcaffe2.dylib)\n frame #1: at::Type::expand(at::Tensor const&, at::ArrayRef<long long>, bool) const + 100 (0x11c957aa4 in libcaffe2.dylib)\n frame #2: torch::autograd::VariableType::expand(at::Tensor const&, at::ArrayRef<long long>, bool) const + 1701 (0x11ea2cc65 in libtorch.dylib)\n frame #3: at::Tensor::expand(at::ArrayRef<long long>, bool) const + 137 (0x11e7a1109 in libtorch.dylib)\n frame #4: torch::jit::(anonymous namespace)::$_283::operator()(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) const + 373 (0x11ee722f5 in libtorch.dylib)\n frame #5: int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_283&, std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&>(torch::jit::(anonymous namespace)::$_283&&&, std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&&&) + 77 (0x11ee7216d in libtorch.dylib)\n frame #6: std::__1::__function::__func<torch::jit::(anonymous namespace)::$_283, std::__1::allocator<torch::jit::(anonymous namespace)::$_283>, int (std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&)>::operator()(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 68 (0x11ee72064 in libtorch.dylib)\n frame #7: std::__1::function<int (std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&)>::operator()(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) const + 142 (0x11b628c4e in _C.cpython-36m-darwin.so)\n frame #8: torch::jit::InterpreterStateImpl::runOneStage(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 315 (0x11f14e5eb in libtorch.dylib)\n frame #9: torch::jit::InterpreterState::runOneStage(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 40 (0x11f14e4a8 in libtorch.dylib)\n frame #10: torch::jit::GraphExecutorImpl::runFallback(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 62 (0x11f0a251e in libtorch.dylib)\n frame #11: torch::jit::GraphExecutorImpl::run(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 2110 (0x11f09e4de in libtorch.dylib)\n frame #12: torch::jit::GraphExecutor::run(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 40 (0x11f09dc98 in libtorch.dylib)\n frame #13: torch::jit::script::Method::run(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 1144 (0x11b76b198 in _C.cpython-36m-darwin.so)\n frame #14: torch::jit::invokeScriptMethodFromPython(torch::jit::script::Method&, pybind11::args, pybind11::kwargs) + 175 (0x11b74d9df in _C.cpython-36m-darwin.so)\n frame #15: pybind11::object pybind11::detail::argument_loader<torch::jit::script::Method&, pybind11::args, pybind11::kwargs>::call_impl<pybind11::object, pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), 0ul, 1ul, 2ul, pybind11::detail::void_type>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::detail::index_sequence<0ul, 1ul, 2ul>, pybind11::detail::void_type&&) + 276 (0x11b7b7624 in _C.cpython-36m-darwin.so)\n frame #16: std::__1::enable_if<!(std::is_void<pybind11::object>::value), pybind11::object>::type pybind11::detail::argument_loader<torch::jit::script::Method&, pybind11::args, pybind11::kwargs>::call<pybind11::object, pybind11::detail::void_type, pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs)>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs)) + 56 (0x11b7b6e68 in _C.cpython-36m-darwin.so)\n frame #17: void pybind11::cpp_function::initialize<pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object, torch::jit::script::Method&, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object (*)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 225 (0x11b7b6d21 in _C.cpython-36m-darwin.so)\n frame #18: void pybind11::cpp_function::initialize<pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object, torch::jit::script::Method&, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object (*)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 24 (0x11b7b6c28 in _C.cpython-36m-darwin.so)\n frame #19: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 6919 (0x11b0152b7 in _C.cpython-36m-darwin.so)\n <omitting python frames>\n :\n operation failed in interpreter:\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/distributions/torch_distribution.py(350): log_prob\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/poutine/trace_struct.py(245): compute_log_prob\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/enum.py(49): get_importance_trace\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py(52): _get_trace\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/elbo.py(111): _get_traces\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py(168): loss_and_surrogate_loss\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/poutine/messenger.py(27): _wraps\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/ops/jit.py(49): compiled\n /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/jit/__init__.py(290): wrapper\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/ops/jit.py(39): __call__\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py(202): loss_and_grads\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/svi.py(96): step\n examples/expand.py(57): svi\n examples/expand.py(61): <module>\n \n \n   [pytorch-master] npradhan@npradhan:~/workspace/pyro_dev/pyro (pytorch-0.4.1)\n   $ python examples/expand.py\n clang: error: unsupported option '-fopenmp'\n clang: error: unsupported option '-fopenmp'\n warning: pytorch jit fuser failed to compile with openmp, trying without it...\n Traceback (most recent call last):\n   File \"examples/expand.py\", line 27, in <module>\n     inference.step(torch.ones(3))\n   File \"/Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/svi.py\", line 96, in step\n     loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)\n   File \"/Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py\", line 202, in loss_and_grads\n     loss, surrogate_loss = self._loss_and_surrogate_loss(*args)\n   File \"/Users/npradhan/workspace/pyro_dev/pyro/pyro/ops/jit.py\", line 59, in __call__\n     ret = self.compiled[argc](*params_and_args)\n   File \"/Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call__\n     result = self.forward(*input, **kwargs)\n   File \"/Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/jit/__init__.py\", line 736, in forward\n     return self._get_method('forward')(*args, **kwargs)\n RuntimeError:\n The expanded size of the tensor (10) must match the existing size (3) at non-singleton dimension 1 (inferExpandGeometry at /Users/npradhan/workspace/pyro_dev/pytorch/pytorch/aten/src/ATen/ExpandUtils.cpp:71)\n frame #0: at::native::expand(at::Tensor const&, at::ArrayRef<long long>, bool) + 2785 (0x112601ae1 in libcaffe2.dylib)\n frame #1: at::Type::expand(at::Tensor const&, at::ArrayRef<long long>, bool) const + 100 (0x1128b2aa4 in libcaffe2.dylib)\n frame #2: torch::autograd::VariableType::expand(at::Tensor const&, at::ArrayRef<long long>, bool) const + 1701 (0x114987c65 in libtorch.dylib)\n frame #3: at::Tensor::expand(at::ArrayRef<long long>, bool) const + 137 (0x1146fc109 in libtorch.dylib)\n frame #4: torch::jit::(anonymous namespace)::$_283::operator()(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) const + 373 (0x114dcd2f5 in libtorch.dylib)\n frame #5: int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_283&, std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&>(torch::jit::(anonymous namespace)::$_283&&&, std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&&&) + 77 (0x114dcd16d in libtorch.dylib)\n frame #6: std::__1::__function::__func<torch::jit::(anonymous namespace)::$_283, std::__1::allocator<torch::jit::(anonymous namespace)::$_283>, int (std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&)>::operator()(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 68 (0x114dcd064 in libtorch.dylib)\n frame #7: std::__1::function<int (std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&)>::operator()(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) const + 142 (0x111583c4e in _C.cpython-36m-darwin.so)\n frame #8: torch::jit::InterpreterStateImpl::runOneStage(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 315 (0x1150a95eb in libtorch.dylib)\n frame #9: torch::jit::InterpreterState::runOneStage(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 40 (0x1150a94a8 in libtorch.dylib)\n frame #10: torch::jit::GraphExecutorImpl::runFallback(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 62 (0x114ffd51e in libtorch.dylib)\n frame #11: torch::jit::GraphExecutorImpl::run(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 2110 (0x114ff94de in libtorch.dylib)\n frame #12: torch::jit::GraphExecutor::run(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 40 (0x114ff8c98 in libtorch.dylib)\n frame #13: torch::jit::script::Method::run(std::__1::vector<torch::jit::IValue, std::__1::allocator<torch::jit::IValue> >&) + 1144 (0x1116c6198 in _C.cpython-36m-darwin.so)\n frame #14: torch::jit::invokeScriptMethodFromPython(torch::jit::script::Method&, pybind11::args, pybind11::kwargs) + 175 (0x1116a89df in _C.cpython-36m-darwin.so)\n frame #15: pybind11::object pybind11::detail::argument_loader<torch::jit::script::Method&, pybind11::args, pybind11::kwargs>::call_impl<pybind11::object, pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), 0ul, 1ul, 2ul, pybind11::detail::void_type>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::detail::index_sequence<0ul, 1ul, 2ul>, pybind11::detail::void_type&&) + 276 (0x111712624 in _C.cpython-36m-darwin.so)\n frame #16: std::__1::enable_if<!(std::is_void<pybind11::object>::value), pybind11::object>::type pybind11::detail::argument_loader<torch::jit::script::Method&, pybind11::args, pybind11::kwargs>::call<pybind11::object, pybind11::detail::void_type, pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs)>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs)) + 56 (0x111711e68 in _C.cpython-36m-darwin.so)\n frame #17: void pybind11::cpp_function::initialize<pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object, torch::jit::script::Method&, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object (*)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 225 (0x111711d21 in _C.cpython-36m-darwin.so)\n frame #18: void pybind11::cpp_function::initialize<pybind11::object (*&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object, torch::jit::script::Method&, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::object (*&&&)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::object (*)(torch::jit::script::Method&, pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 24 (0x111711c28 in _C.cpython-36m-darwin.so)\n frame #19: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 6919 (0x110f702b7 in _C.cpython-36m-darwin.so)\n <omitting python frames>\n :\n operation failed in interpreter:\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/distributions/torch_distribution.py(350): log_prob\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/poutine/trace_struct.py(245): compute_log_prob\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/enum.py(49): get_importance_trace\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py(52): _get_trace\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/elbo.py(111): _get_traces\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py(168): loss_and_surrogate_loss\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/poutine/messenger.py(27): _wraps\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/ops/jit.py(49): compiled\n /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/jit/__init__.py(290): wrapper\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/ops/jit.py(39): __call__\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/trace_elbo.py(202): loss_and_grads\n /Users/npradhan/workspace/pyro_dev/pyro/pyro/infer/svi.py(96): step\n examples/expand.py(26): <module>\n \n \n \n PyTorch is able to generalize tensor expansion beyond the initial tensor sizes provided as arguments to torch.jit.trace, so I think it might just be a simple fix on our end (maybe something like providing *args in the call to torch.jit).\n cc <denchmark-link:https://github.com/eb8680>@eb8680</denchmark-link>\n , <denchmark-link:https://github.com/fritzo>@fritzo</denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "neerajprad", "commentT": "2018-09-04T20:27:44Z", "comment_text": "\n \t\tI believe the issue is that somewhere we are computing the batch size through pure python, and dependency is being hidden from the JIT.\n An easy workaround is to ensure each batch size is compiled separately. This can be done by passing in batch size as a kwarg (our jit wrapper assumes all args are tensors and all kwargs are non-tensors; each kwargs dict triggers a separate compilation.\n def model(data, batch_size):\n     ...body ignores batch_size kwarg...\n def guide(data, batch_size):\n     ...body ignores batch_size kwarg...\n ...\n inference.step(torch.ones(10), batch_size=10)\n inference.step(torch.ones(3), batch_size=3)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "neerajprad", "commentT": "2018-09-19T18:40:09Z", "comment_text": "\n \t\tFixed in <denchmark-link:https://github.com/pyro-ppl/pyro/pull/1392>#1392</denchmark-link>\n .\n \t\t"}}}, "commit": {"commit_id": "ae9c41f130f7b42cf21da645b6c23def7f3dd278", "commit_author": "Neeraj Pradhan", "commitT": "2018-09-11 21:22:39-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 4, "file_old_name": "pyro\\poutine\\indep_messenger.py", "file_new_name": "pyro\\poutine\\indep_messenger.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "22,23", "deleted_lines": null, "method_info": {"method_name": "__ne__", "method_params": "self,other", "method_startline": "22", "method_endline": "23"}}, "hunk_1": {"Ismethod": 1, "added_lines": "25,26", "deleted_lines": null, "method_info": {"method_name": "__hash__", "method_params": "self", "method_startline": "25", "method_endline": "26"}}, "hunk_2": {"Ismethod": 1, "added_lines": "15,16,17", "deleted_lines": null, "method_info": {"method_name": "_key", "method_params": "self", "method_startline": "15", "method_endline": "17"}}, "hunk_3": {"Ismethod": 1, "added_lines": "19,20", "deleted_lines": null, "method_info": {"method_name": "__eq__", "method_params": "self,other", "method_startline": "19", "method_endline": "20"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\infer\\test_jit.py", "file_new_name": "tests\\infer\\test_jit.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "243,244,245,246", "deleted_lines": null, "method_info": {"method_name": "test_cond_indep_equality", "method_params": "x,y", "method_startline": "243", "method_endline": "246"}}}}}}}