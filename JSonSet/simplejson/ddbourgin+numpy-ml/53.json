{"BR": {"BR_id": "53", "BR_author": "eromoe", "BRopenT": "2020-07-13T09:54:31Z", "BRcloseT": "2020-07-27T16:03:25Z", "BR_text": {"BRsummary": "[Question] Why `gamma * beta` stand for ` L2 in  LogisticRegression._NLL_grad", "BRdescription": "\n Hello,  this is a great project , I am learning how to implement model without sklearn/tensorflow , it really help me a lot .\n I have a question on\n \n \n \n numpy-ml/numpy_ml/linear_models/lm.py\n \n \n          Line 252\n       in\n       4f37707\n \n \n \n \n \n \n  d_penalty = gamma * beta if p == \"l2\" else gamma * l1norm(beta) * np.sign(beta) \n \n \n \n \n \n Since P-norm is defined as\n <denchmark-link:https://user-images.githubusercontent.com/3938751/87290451-9957ed80-c530-11ea-98ae-0de91eed40ac.png></denchmark-link>\n \n l1norms(self.beta) means the sum of all absulote value of each element in self.beta .  I don't quite understand why the simple  gamma * beta stand for `L2  ?\n PS: May I ask what IDE and code document plugin you are using ? I see some annotation don't beyond to latex ,  it would be nice to see beautiful math symbols than raw latex :)\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "eromoe", "commentT": "2020-07-17T22:09:01Z", "comment_text": "\n \t\tFor linear regression, the l2-regularization term is gamma * np.sqrt(beta @ beta)\n The gradient of l2 penalty wrt beta is then simply gamma * beta\n Keep in mind that d_penality is the gradient of the penalty term wrt the coefficients, not the penalty itself :)\n I don't use a special IDE, unfortunately. the equations are formatted for display as Sphinx <denchmark-link:https://www.sphinx-doc.org/en/master/usage/restructuredtext/index.html>reStructuredText</denchmark-link>\n . You can see the rendered equations in the online <denchmark-link:https://numpy-ml.readthedocs.io/en/latest/>documentation</denchmark-link>\n , or build it yourself from the source in the  directory. There may also be IDE plugins that will try to render them, but I am not aware of any :)\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "eromoe", "commentT": "2020-07-18T02:31:50Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ddbourgin>@ddbourgin</denchmark-link>\n  Thank you for  reply .\n From <denchmark-link:https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261>https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261</denchmark-link>\n \n <denchmark-link:https://user-images.githubusercontent.com/3938751/87842748-68393d80-c8e1-11ea-94fb-af4ebfde9faa.png></denchmark-link>\n \n l1-regularization term is gamma * np.absolute(beta)\n l2-regularization term is gamma * np.power(np.sqrt(beta @ beta), 2)  (I think you miswrote in previous comment )\n The gradient of l1 penalty wrt beta is then gamma  * np.sign(beta)\n The gradient of l2 penalty wrt beta is then gamma * 2beta  proportional to gamma * beta .\n Actually  I thought  l2-regularization term was gamma * np.sqrt(beta @ beta)  , so the gradient of l2 term is +- 1 too .Because sometimes I thought L2 norm was beta^2 , sometimes it was np.sqrt(beta^2) in my brain  ,  l2 norm and  l2-regularization term` are so likely and mess up ,  now I have figure  it clear .\n But there is a left problem : why you multiply l1norm(beta) in L1 case ?  since the gradient of l1 penalty is gamma * np.sign(beta) , this confused me .\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "eromoe", "commentT": "2020-07-25T03:42:01Z", "comment_text": "\n \t\tWhoops, yup, that's what I get for being hasty! The regularization penalty is <denchmark-link:https://github.com/ddbourgin/numpy-ml/blob/4f37707c6c7c390645dec5a503c12a48e624b249/numpy_ml/linear_models/lm.py#L244>(gamma / 2) * np.sqrt(beta @ beta) ** 2</denchmark-link>\n , which gives a gradient of .\n In the L1 case, I'd recommend explicitly writing down the L1 penalty (not just the l1 norm) and then trying to derive the gradient wrt beta. It should quickly become clear why there is an l1norm term in the calc :)\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "eromoe", "commentT": "2020-07-27T07:28:12Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/ddbourgin>@ddbourgin</denchmark-link>\n  Sorry but I don't quite understand why penalty in L1 case need square as L2 does\n <denchmark-code>penalty = 0.5 * self.gamma * np.linalg.norm(self.beta, ord=order) ** 2   #  remaid square under l1 case\n </denchmark-code>\n \n All ariticles I saw was using a L1 term (penalty) like\n <denchmark-link:https://user-images.githubusercontent.com/3938751/88514973-8871a680-d01d-11ea-81c8-bd3128468f9d.png></denchmark-link>\n \n And the derivative is +-\\lambda  .\n Now I am very confusing .\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "eromoe", "commentT": "2020-07-27T15:38:15Z", "comment_text": "\n \t\tOh! I see what you're saying. You're right, the square of the L1 norm is not what we want. The proper L1 penalty is\n gamma * np.abs(beta).sum()\n which gives a gradient of\n gamma * np.sign(beta)\n I'll make a PR to fix this. Thank you very much for pointing this out :)\n \t\t"}}}, "commit": {"commit_id": "b537fac970f85d4919fe8c491499480a3a71bf8c", "commit_author": "David Bourgin", "commitT": "2020-07-27 12:03:19-04:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "numpy_ml\\linear_models\\lm.py", "file_new_name": "numpy_ml\\linear_models\\lm.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "250,254,256,257,259", "deleted_lines": "244,252", "method_info": {"method_name": "_NLL", "method_params": "self,X,y,y_pred", "method_startline": "240", "method_endline": "260"}}, "hunk_1": {"Ismethod": 1, "added_lines": "267", "deleted_lines": null, "method_info": {"method_name": "_NLL_grad", "method_params": "self,X,y,y_pred", "method_startline": "262", "method_endline": "268"}}, "hunk_2": {"Ismethod": 1, "added_lines": "169,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,192", "deleted_lines": "169,171,172,173,180", "method_info": {"method_name": "__init__", "method_params": "self,penalty,gamma,fit_intercept", "method_startline": "153", "method_endline": "207"}}}}}}}