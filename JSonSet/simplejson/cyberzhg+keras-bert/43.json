{"BR": {"BR_id": "43", "BR_author": "yewang01", "BRopenT": "2019-04-09T02:41:09Z", "BRcloseT": "2019-04-16T05:28:38Z", "BR_text": {"BRsummary": "Inconsistency definition of the training param of load_trained_model_from_checkpoint function", "BRdescription": "\n Inconsistency definition of the training param of load_trained_model_from_checkpoint function\n I loaded the pre-trained BERT model from an official tf checkpoint, using load_trained_model_from_checkpoint with param training=False.\n I don't want to train the BERT model from scratch, i.e. by MLM or NSP, however, I do want my downstream data will somehow update params inside BERT model. As shown in the fig below, the bert is trainable as a keras model, however all weights inside the model are non-trainable weights.\n I'm confused is there anything wrong with my code or anything wrong with the training param?\n <denchmark-link:https://user-images.githubusercontent.com/30275007/55769564-d9216100-5ab3-11e9-8b3d-0828620fd28f.png></denchmark-link>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "yewang01", "commentT": "2019-04-09T03:59:33Z", "comment_text": "\n \t\tI try to separate training param into 'training' and 'training_bert' in loader.py and bert.py.\n now, everything seems good to me. But still not sure if I did the right thing. Can anyone give me any hints?\n <denchmark-code>def get_model(token_num,\n               pos_num=512,\n               seq_len=512,\n               embed_dim=768,\n               transformer_num=12,\n               head_num=12,\n               feed_forward_dim=3072,\n               dropout_rate=0.1,\n               attention_activation=None,\n               feed_forward_activation=gelu,\n               custom_layers=None,\n               training=True,  ### here what i change\n               training_bert=False,   ### here what i change\n               lr=1e-4):\n     \"\"\"Get BERT model.\n \n     See: https://arxiv.org/pdf/1810.04805.pdf\n \n     :param token_num: Number of tokens.\n     :param pos_num: Maximum position.\n     :param seq_len: Maximum length of the input sequence or None.\n     :param embed_dim: Dimensions of embeddings.\n     :param transformer_num: Number of transformers.\n     :param head_num: Number of heads in multi-head attention in each transformer.\n     :param feed_forward_dim: Dimension of the feed forward layer in each transformer.\n     :param dropout_rate: Dropout rate.\n     :param attention_activation: Activation for attention layers.\n     :param feed_forward_activation: Activation for feed-forward layers.\n     :param custom_layers: A function that takes the embedding tensor and returns the tensor after feature extraction.\n                           Arguments such as `transformer_num` and `head_num` will be ignored if `custom_layer` is not\n                           `None`.\n     :param training: The built model will be returned if it is `True`, otherwise the input layers and the last feature\n                      extraction layer will be returned.\n     :param lr: Learning rate.\n     :return: The compiled model.\n     \"\"\"\n     inputs = get_inputs(seq_len=seq_len)\n     embed_layer, embed_weights = get_embedding(\n         inputs,\n         token_num=token_num,\n         embed_dim=embed_dim,\n         pos_num=pos_num,\n         dropout_rate=dropout_rate,\n         trainable=training,\n     )\n     transformed = embed_layer\n     if custom_layers is not None:\n         kwargs = {}\n         if keras.utils.generic_utils.has_arg(custom_layers, 'trainable'):\n             kwargs['trainable'] = training\n         transformed = custom_layers(transformed, **kwargs)\n     else:\n         transformed = get_encoders(\n             encoder_num=transformer_num,\n             input_layer=transformed,\n             head_num=head_num,\n             hidden_dim=feed_forward_dim,\n             attention_activation=attention_activation,\n             feed_forward_activation=feed_forward_activation,\n             dropout_rate=dropout_rate,\n             trainable=training,\n         )\n     if not training_bert:  ### here what i change\n         return inputs[:2], transformed\n     mlm_dense_layer = keras.layers.Dense(\n         units=embed_dim,\n         activation=feed_forward_activation,\n         name='MLM-Dense',\n     )(transformed)\n     mlm_norm_layer = LayerNormalization(name='MLM-Norm')(mlm_dense_layer)\n     mlm_pred_layer = EmbeddingSimilarity(name='MLM-Sim')([mlm_norm_layer, embed_weights])\n     masked_layer = Masked(name='MLM')([mlm_pred_layer, inputs[-1]])\n     extract_layer = Extract(index=0, name='Extract')(transformed)\n     nsp_dense_layer = keras.layers.Dense(\n         units=embed_dim,\n         activation='tanh',\n         name='NSP-Dense',\n     )(extract_layer)\n     nsp_pred_layer = keras.layers.Dense(\n         units=2,\n         activation='softmax',\n         name='NSP',\n     )(nsp_dense_layer)\n     model = keras.models.Model(inputs=inputs, outputs=[masked_layer, nsp_pred_layer])\n     model.compile(\n         optimizer=keras.optimizers.Adam(lr=lr),\n         loss=keras.losses.sparse_categorical_crossentropy,\n     )\n     return model\n </denchmark-code>\n \n <denchmark-code>def load_trained_model_from_checkpoint(config_file,\n                                        checkpoint_file,\n                                        training=False,  ### here what i change\n                                        training_bert=False,  ### here what i change\n                                        seq_len=None):\n     \"\"\"Load trained official model from checkpoint.\n \n     :param config_file: The path to the JSON configuration file.\n     :param checkpoint_file: The path to the checkpoint files, should end with '.ckpt'.\n     :param training: If training, the whole model will be returned.\n                      Otherwise, the MLM and NSP parts will be ignored.\n     :param seq_len: If it is not None and it is shorter than the value in the config file, the weights in\n                     position embeddings will be sliced to fit the new length.\n     :return:\n     \"\"\"\n     with open(config_file, 'r') as reader:\n         config = json.loads(reader.read())\n     if seq_len is None:\n         seq_len = config['max_position_embeddings']\n     else:\n         seq_len = min(seq_len, config['max_position_embeddings'])\n     loader = checkpoint_loader(checkpoint_file)\n     model = get_model(\n         token_num=config['vocab_size'],\n         pos_num=seq_len,\n         seq_len=seq_len,\n         embed_dim=config['hidden_size'],\n         transformer_num=config['num_hidden_layers'],\n         head_num=config['num_attention_heads'],\n         feed_forward_dim=config['intermediate_size'],\n         training=training,\n     )\n     if not training_bert:  ### here what i change\n         inputs, outputs = model\n         model = keras.models.Model(inputs=inputs, outputs=outputs)\n         model.compile(\n             optimizer=keras.optimizers.Adam(),\n             loss=keras.losses.sparse_categorical_crossentropy,\n         )\n     model.get_layer(name='Embedding-Token').set_weights([\n         loader('bert/embeddings/word_embeddings'),\n     ])\n     model.get_layer(name='Embedding-Position').set_weights([\n         loader('bert/embeddings/position_embeddings')[:seq_len, :],\n     ])\n     model.get_layer(name='Embedding-Segment').set_weights([\n         loader('bert/embeddings/token_type_embeddings'),\n     ])\n     model.get_layer(name='Embedding-Norm').set_weights([\n         loader('bert/embeddings/LayerNorm/gamma'),\n         loader('bert/embeddings/LayerNorm/beta'),\n     ])\n     for i in range(config['num_hidden_layers']):\n         model.get_layer(name='Encoder-%d-MultiHeadSelfAttention' % (i + 1)).set_weights([\n             loader('bert/encoder/layer_%d/attention/self/query/kernel' % i),\n             loader('bert/encoder/layer_%d/attention/self/query/bias' % i),\n             loader('bert/encoder/layer_%d/attention/self/key/kernel' % i),\n             loader('bert/encoder/layer_%d/attention/self/key/bias' % i),\n             loader('bert/encoder/layer_%d/attention/self/value/kernel' % i),\n             loader('bert/encoder/layer_%d/attention/self/value/bias' % i),\n             loader('bert/encoder/layer_%d/attention/output/dense/kernel' % i),\n             loader('bert/encoder/layer_%d/attention/output/dense/bias' % i),\n         ])\n         model.get_layer(name='Encoder-%d-MultiHeadSelfAttention-Norm' % (i + 1)).set_weights([\n             loader('bert/encoder/layer_%d/attention/output/LayerNorm/gamma' % i),\n             loader('bert/encoder/layer_%d/attention/output/LayerNorm/beta' % i),\n         ])\n         model.get_layer(name='Encoder-%d-MultiHeadSelfAttention-Norm' % (i + 1)).set_weights([\n             loader('bert/encoder/layer_%d/attention/output/LayerNorm/gamma' % i),\n             loader('bert/encoder/layer_%d/attention/output/LayerNorm/beta' % i),\n         ])\n         model.get_layer(name='Encoder-%d-FeedForward' % (i + 1)).set_weights([\n             loader('bert/encoder/layer_%d/intermediate/dense/kernel' % i),\n             loader('bert/encoder/layer_%d/intermediate/dense/bias' % i),\n             loader('bert/encoder/layer_%d/output/dense/kernel' % i),\n             loader('bert/encoder/layer_%d/output/dense/bias' % i),\n         ])\n         model.get_layer(name='Encoder-%d-FeedForward-Norm' % (i + 1)).set_weights([\n             loader('bert/encoder/layer_%d/output/LayerNorm/gamma' % i),\n             loader('bert/encoder/layer_%d/output/LayerNorm/beta' % i),\n         ])\n     if training_bert:  ### here what i change\n         model.get_layer(name='MLM-Dense').set_weights([\n             loader('cls/predictions/transform/dense/kernel'),\n             loader('cls/predictions/transform/dense/bias'),\n         ])\n         model.get_layer(name='MLM-Norm').set_weights([\n             loader('cls/predictions/transform/LayerNorm/gamma'),\n             loader('cls/predictions/transform/LayerNorm/beta'),\n         ])\n         model.get_layer(name='MLM-Sim').set_weights([\n             loader('cls/predictions/output_bias'),\n         ])\n         model.get_layer(name='NSP-Dense').set_weights([\n             loader('bert/pooler/dense/kernel'),\n             loader('bert/pooler/dense/bias'),\n         ])\n         model.get_layer(name='NSP').set_weights([\n             np.transpose(loader('cls/seq_relationship/output_weights')),\n             loader('cls/seq_relationship/output_bias'),\n         ])\n     return model\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "yewang01", "commentT": "2019-04-09T04:07:37Z", "comment_text": "\n \t\tI've added an argument trainable:\n load_trained_model_from_checkpoint(\n     ...\n     ...\n     training=False,\n     trainable=True,\n )\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "yewang01", "commentT": "2019-04-14T04:40:10Z", "comment_text": "\n \t\tThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \t\t"}}}, "commit": {"commit_id": "43b24a0c7ae4c82a2587f87d3e38b2a8e7ff96bd", "commit_author": "CyberZHG", "commitT": "2019-04-09 12:02:24+08:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "README.md", "file_new_name": "README.md", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "19,119,120", "deleted_lines": "19,119"}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "keras_bert\\bert.py", "file_new_name": "keras_bert\\bert.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "41", "deleted_lines": null, "method_info": {"method_name": "get_model", "method_params": "token_num,pos_num,seq_len,embed_dim,transformer_num,head_num,feed_forward_dim,dropout_rate,attention_activation,feed_forward_activation,custom_layers,training,trainable,lr", "method_startline": "29", "method_endline": "42"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "keras_bert\\loader.py", "file_new_name": "keras_bert\\loader.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "142", "deleted_lines": null, "method_info": {"method_name": "load_trained_model_from_checkpoint", "method_params": "config_file,checkpoint_file,training,trainable,seq_len", "method_startline": "139", "method_endline": "143"}}, "hunk_1": {"Ismethod": 1, "added_lines": "23", "deleted_lines": null, "method_info": {"method_name": "build_model_from_config", "method_params": "config_file,training,trainable,seq_len", "method_startline": "21", "method_endline": "24"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "setup.py", "file_new_name": "setup.py", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "14", "deleted_lines": "14"}}}}}}