{"BR": {"BR_id": "1038", "BR_author": "tanmayb123", "BRopenT": "2020-07-03T17:44:12Z", "BRcloseT": "2020-07-22T17:13:08Z", "BR_text": {"BRsummary": "sigmoidCrossEntropy is not differentiable", "BRdescription": "\n I'm working on an example of Swift for TensorFlow and, for some reason, sigmoidCrossEntropy doesn't seem to be differentiable.\n Code:\n <denchmark-code>@differentiable\n func unetWorkerLoss(model: UNet, input: Tensor<Float>, output: Tensor<Float>) -> Tensor<Float> {\n     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n }\n </denchmark-code>\n \n Error:\n <denchmark-code>/home/tanmay/swift-models/UNet/main.swift:77:5: error: expression is not differentiable\n     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n     ^\n /home/tanmay/swift-models/UNet/main.swift:77:5: note: cannot differentiate functions that have not been marked '@differentiable' and that are defined in other files\n     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n </denchmark-code>\n \n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "tanmayb123", "commentT": "2020-07-03T17:46:11Z", "comment_text": "\n \t\tHowever, when I do copy in the function directly from swift-apis with no modifications, and change the function names:\n <denchmark-code>@differentiable(wrt: logits)\n public func tbSigmoidCrossEntropy<Scalar: TensorFlowFloatingPoint>(\n   logits: Tensor<Scalar>,\n   labels: Tensor<Scalar>,\n   reduction: @differentiable (Tensor<Scalar>) -> Tensor<Scalar> = _mean\n ) -> Tensor<Scalar> {\n   let device = logits.device\n   // This numerically stable implementation is based on the TensorFlow Python API.\n   let maxLogitsWithZero = max(logits, Tensor(0, on: device))\n   let negAbsLogits = max(logits, -logits)  // Custom `abs` to compute gradients at `0`.\n   return reduction(maxLogitsWithZero - logits * labels + log1p(exp(-negAbsLogits)))\n }\n \n @differentiable\n func unetWorkerLoss(model: UNet, input: Tensor<Float>, output: Tensor<Float>) -> Tensor<Float> {\n     tbSigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n }\n </denchmark-code>\n \n It works.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "tanmayb123", "commentT": "2020-07-15T00:38:17Z", "comment_text": "\n \t\tI don't think the original report is a bug though the error message could be improved to say that sigmoidCrossEntropy is only differentiable with respect to logits.   unetWorkerLoss attempts to differentiate with respect to labels also.  Changing @differentiable to @differentiable(wrt: input) in the code above removes the error.  Full working example:\n <denchmark-code>import TensorFlow\n \n struct UNet: Layer {\n   var x = Tensor<Float>(1)\n   \n   func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {\n      x + input\n   }\n }\n \n @differentiable(wrt: input)\n func unetWorkerLoss(model: UNet, input: Tensor<Float>, output: Tensor<Float>) -> Tensor<Float> {\n   sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n }\n </denchmark-code>\n \n It is unexpected (and maybe inefficient) that the workaround of rewriting sigmoidCrossEntropy works but it appears @differentiable is not needed (and functions are always differentiable with respect to all inputs) for differentiation within the same file.  For example, this code compiles and executes without error:\n <denchmark-code>import TensorFlow\n \n //@differentiable(wrt: x) // Code still works if this is uncommented.\n func addXY(x: Tensor<Float>, y: Tensor<Float>) -> Tensor<Float> {\n    x + y\n }\n \n // No need for @differentiable here either.\n func wrapAddXY(x: Tensor<Float>, y: Tensor<Float>) -> Tensor<Float> {\n    return addXY(x: x, y: y)\n }\n \n let grad = gradient(at: Tensor<Float>(1), Tensor<Float>(2), in: wrapAddXY)\n print(grad) // (1.0, 1.0)\n </denchmark-code>\n \n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "tanmayb123", "commentT": "2020-07-15T16:55:57Z", "comment_text": "\n \t\t\n I don't think the original report is a bug though the error message could be improved to say that sigmoidCrossEntropy is only differentiable with respect to logits.\n \n Yes, this is accurate: the current diagnostic messages are misleading. Instead, they should read something like:\n <denchmark-code>/home/tanmay/swift-models/UNet/main.swift:77:5: error: expression is not differentiable\n     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n     ^\n /home/tanmay/swift-models/UNet/main.swift:77:5: note: 'foo' is marked '@differentiable' only with respect to a smaller subset of arguments: '@differentiable(wrt: logits)'\n     sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n </denchmark-code>\n \n <denchmark-link:https://bugs.swift.org/browse/SR-13224>SR-13224</denchmark-link>\n  tracks improving this diagnostic, we'll look into it soon!\n <denchmark-h:hr></denchmark-h>\n \n As <denchmark-link:https://github.com/mikowals>@mikowals</denchmark-link>\n  noted, changing  to be explicitly  (not with respect to ) fixes the error:\n <denchmark-code>@differentiable(wrt: (model, input))\n func unetWorkerLoss(model: UNet, input: Tensor<Float>, output: Tensor<Float>) -> Tensor<Float> {\n   sigmoidCrossEntropy(logits: model(input).reshaped(to: [-1, 256 * 256]), labels: output.reshaped(to: [-1, 256 * 256]))\n }\n </denchmark-code>\n \n This has the advantage of avoiding work to compute derivatives for the output parameter.\n Besides improving the diagnostic, we can also add @differentiable(wrt: (logits, labels)) attributes to sigmoidCrossEntropy and other loss functions to fix the issue.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "tanmayb123", "commentT": "2020-07-22T17:13:08Z", "comment_text": "\n \t\tClosing because  is sufficiently differentiable, and I made a comment in <denchmark-link:https://github.com/tensorflow/swift-models/pull/623>tensorflow/swift-models#623</denchmark-link>\n  explaining how to make it work.\n \t\t"}}}, "commit": {"commit_id": "bf942f2634862596a01f8fe5a5a049ecd3b6f2a4", "commit_author": "Dan Zheng", "commitT": "2020-07-15 17:30:43-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "Sources\\TensorFlow\\BackwardsCompatibility.swift", "file_new_name": "Sources\\TensorFlow\\BackwardsCompatibility.swift", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "27,41,55,69,83,98,112,127,153", "deleted_lines": null}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "Sources\\TensorFlow\\Loss.swift", "file_new_name": "Sources\\TensorFlow\\Loss.swift", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "25,42,58,73,91,108,125,144,163,184,204,221,315,343", "deleted_lines": null}}}}}}