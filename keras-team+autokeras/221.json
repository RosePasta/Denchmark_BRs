{"BR": {"BR_id": "221", "BR_author": "monikatomar92", "BRopenT": "2018-09-25T23:36:36Z", "BRcloseT": "2018-10-22T13:37:22Z", "BR_text": {"BRsummary": "Dataloader load from disk by batches", "BRdescription": "\n <denchmark-h:h3>Bug Description</denchmark-h>\n \n After the first model training, in the second model, I am getting the Memory error. I have also added the latest release as in another issue it was mentioned that memory error bug has been fixed in the latest issue. The error looks as follows:\n Traceback (most recent call last):\n File \"/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 885, in del\n self.close()\n File \"/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 1090, in close\n self._decr_instances(self)\n File \"/home/monika/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 454, in _decr_instances\n cls.monitor.exit()\n File \"/home/monika/.local/lib/python3.6/site-packages/tqdm/_monitor.py\", line 52, in exit\n self.join()\n File \"/usr/lib/python3.6/threading.py\", line 1053, in join\n raise RuntimeError(\"cannot join current thread\")\n RuntimeError: cannot join current thread\n /usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n len(cache))\n Traceback (most recent call last):\n File \"mnist.py\", line 21, in \n clf.fit(x_train, y_train, time_limit=12 * 60 * 60)\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/image_supervised.py\", line 239, in fit\n run_searcher_once(train_data, test_data, self.path, int(time_remain))\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/image_supervised.py\", line 40, in run_searcher_once\n searcher.search(train_data, test_data, timeout)\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/search.py\", line 190, in search\n timeout)\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/bayesian.py\", line 257, in optimize_acq\n for temp_graph in transform(elem.graph):\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/net_transformer.py\", line 83, in transform\n temp_graph = to_deeper_graph(deepcopy(graph))\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/net_transformer.py\", line 63, in to_deeper_graph\n graph.to_conv_deeper_model(layer_id, 3)\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/graph.py\", line 313, in to_conv_deeper_model\n new_layers = deeper_conv_block(target, kernel_size, self.weighted)\n File \"/home/monika/.local/lib/python3.6/site-packages/autokeras/layer_transformer.py\", line 11, in deeper_conv_block\n weight = np.zeros((n_filters, n_filters) + filter_shape)\n MemoryError\n If I run the code with the mnist classification example, I get the following error after the 8th model \ud83d\udc4d\n Process SpawnPoolWorker-9:\n Traceback (most recent call last):\n File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n put((job, i, result))\n File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n self._writer.send_bytes(obj)\n File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n self._send_bytes(m[offset:offset + size])\n File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 397, in _send_bytes\n self._send(header)\n File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n n = write(self._handle, buf)\n BrokenPipeError: [Errno 32] Broken pipe\n During handling of the above exception, another exception occurred:\n Traceback (most recent call last):\n File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n self.run()\n File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n self._target(*self._args, **self._kwargs)\n File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 130, in worker\n put((job, i, (False, wrapped)))\n File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n self._writer.send_bytes(obj)\n File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n self._send_bytes(m[offset:offset + size])\n File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n self._send(header + buf)\n File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n n = write(self._handle, buf)\n BrokenPipeError: [Errno 32] Broken pipe\n /usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\n len(cache))\n /usr/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 4 leaked semaphores to clean up at shutdown\n len(cache))\n <denchmark-h:h3>Reproducing Steps</denchmark-h>\n \n Steps to reproduce the behavior:\n \n Step 1: ... use the mnist.py from examples.\n Step 2: ... run the script with either the custom dataset for classification or the default example\n \n <denchmark-h:h3>Expected Behavior</denchmark-h>\n \n <denchmark-h:h3>Setup Details</denchmark-h>\n \n Include the details about the versions of:\n \n OS type and version: Ubuntu 18.04\n Python: 3.6.5\n autokeras: 0.2.14\n scikit-learn:0.19.1\n numpy:1.15.2\n keras:2.2.2\n scipy:1.1.0\n tensorflow:1.10.1\n pytorch:0.4.1\n \n <denchmark-h:h3>Additional context</denchmark-h>\n \n My data is of the following dimension :\n Training Data : (42532, 128,128), training label :  ( 42532,24)\n Testing Data : ( 1325,128,128) , testing label : (1325,24)\n Running the code on GTX 1080TI 11GB and 32GB RAM\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "monikatomar92", "commentT": "2018-09-27T12:27:55Z", "comment_text": "\n \t\tI met the same error\n <denchmark-code>Traceback (most recent call last):\n   File \"ak_aoi_img.py\", line 23, in <module>\n     clf.fit(x_train, y_train, time_limit=2 * 60 * 60)\n   File \"/home/adt/wen/ak_aoi/autokeras/image_supervised.py\", line 239, in fit\n     run_searcher_once(train_data, test_data, self.path, int(time_remain))\n   File \"/home/adt/wen/ak_aoi/autokeras/image_supervised.py\", line 40, in run_searcher_once\n     searcher.search(train_data, test_data, timeout)\n   File \"/home/adt/wen/ak_aoi/autokeras/search.py\", line 193, in search\n     remaining_time)\n   File \"/home/adt/wen/ak_aoi/autokeras/bayesian.py\", line 256, in optimize_acq\n     for temp_graph in transform(elem.graph):\n   File \"/home/adt/wen/ak_aoi/autokeras/net_transformer.py\", line 83, in transform\n     temp_graph = to_deeper_graph(deepcopy(graph))\n   File \"/home/adt/wen/ak_aoi/autokeras/net_transformer.py\", line 63, in to_deeper_graph\n     graph.to_conv_deeper_model(layer_id, 3)\n   File \"/home/adt/wen/ak_aoi/autokeras/graph.py\", line 313, in to_conv_deeper_model\n     new_layers = deeper_conv_block(target, kernel_size, self.weighted)\n   File \"/home/adt/wen/ak_aoi/autokeras/layer_transformer.py\", line 11, in deeper_conv_block\n     weight = np.zeros((n_filters, n_filters) + filter_shape)\n MemoryError\n </denchmark-code>\n \n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "monikatomar92", "commentT": "2018-09-29T20:51:31Z", "comment_text": "\n \t\tThe solution would be to load the data from disk batch by batch, which can be done by pytorch dataloader.\n \t\t"}}}, "commit": {"commit_id": "e36d9378305235df61fe4ccd8f33322e315dd8be", "commit_author": "Haifeng Jin", "commitT": "2018-10-22 08:37:21-05:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "autokeras\\image\\image_supervised.py", "file_new_name": "autokeras\\image\\image_supervised.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "33", "deleted_lines": "21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39", "method_info": {"method_name": "read_csv_file", "method_params": "csv_file_path", "method_startline": "21", "method_endline": "39"}}, "hunk_1": {"Ismethod": 1, "added_lines": "33", "deleted_lines": "20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39", "method_info": {"method_name": "read_images", "method_params": "img_file_names,images_dir_path", "method_startline": "20", "method_endline": "41"}}}}, "file_1": {"file_change_type": "MODIFY", "file_Nmethod": 3, "file_old_name": "autokeras\\preprocessor.py", "file_new_name": "autokeras\\preprocessor.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "173,174", "deleted_lines": null, "method_info": {"method_name": "__len__", "method_params": "self", "method_startline": "173", "method_endline": "174"}}, "hunk_1": {"Ismethod": 1, "added_lines": "153,154,155,156,157,158,159,160,161,162", "deleted_lines": null, "method_info": {"method_name": "__init__", "method_params": "self,csv_file_path,image_path,has_target", "method_startline": "153", "method_endline": "162"}}, "hunk_2": {"Ismethod": 1, "added_lines": "164,165,166,167,168,169,170,171", "deleted_lines": null, "method_info": {"method_name": "__getitem__", "method_params": "self,index", "method_startline": "164", "method_endline": "171"}}}}, "file_2": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "autokeras\\utils.py", "file_new_name": "autokeras\\utils.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194", "deleted_lines": null, "method_info": {"method_name": "read_csv_file", "method_params": "csv_file_path", "method_startline": "176", "method_endline": "194"}}, "hunk_1": {"Ismethod": 1, "added_lines": "197,198,199", "deleted_lines": null, "method_info": {"method_name": "read_image", "method_params": "img_path", "method_startline": "197", "method_endline": "199"}}}}, "file_3": {"file_change_type": "MODIFY", "file_Nmethod": 1, "file_old_name": "tests\\common.py", "file_new_name": "tests\\common.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": "267,268,269", "deleted_lines": null, "method_info": {"method_name": "mock_train", "method_params": "kwargs", "method_startline": "267", "method_endline": "269"}}}}, "file_4": {"file_change_type": "MODIFY", "file_Nmethod": 2, "file_old_name": "tests\\image\\test_image_supervised.py", "file_new_name": "tests\\image\\test_image_supervised.py", "hunks": {"hunk_0": {"Ismethod": 1, "added_lines": null, "deleted_lines": "9,10,11", "method_info": {"method_name": "mock_train", "method_params": "kwargs", "method_startline": "9", "method_endline": "11"}}, "hunk_1": {"Ismethod": 1, "added_lines": null, "deleted_lines": "179,180,181", "method_info": {"method_name": "test_init_image_classifier_with_none_path", "method_params": "_", "method_startline": "179", "method_endline": "181"}}}}, "file_5": {"file_change_type": "ADD", "file_Nmethod": 0, "file_old_name": "None", "file_new_name": "tests\\test_preprocessor.py"}}}}