{"BR": {"BR_id": "3905", "BR_author": "jlewi", "BRopenT": "2019-08-14T20:58:57Z", "BRcloseT": "2019-11-02T02:01:30Z", "BR_text": {"BRsummary": "auto-deployed clusters from master not accessible via IAP - IAP broken on master", "BRdescription": "\n Tried accessing the following\n <denchmark-link:https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog>https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog</denchmark-link>\n \n Get\n ERR_CONNECTION_CLOSED\n \t"}, "comments": {"comments_0": {"comment_id": 1, "comment_author": "jlewi", "commentT": "2019-08-14T20:59:00Z", "comment_text": "\n \t\tIssue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.90. Please mark this comment with \ud83d\udc4d or \ud83d\udc4e to give our bot feedback!\n Links: <denchmark-link:https://github.com/marketplace/issue-label-bot>app homepage</denchmark-link>\n , <denchmark-link:https://mlbot.net/data/kubeflow/website>dashboard</denchmark-link>\n  and <denchmark-link:https://github.com/hamelsmu/MLapp>code</denchmark-link>\n  for this bot.\n \t\t"}, "comments_1": {"comment_id": 2, "comment_author": "jlewi", "commentT": "2019-08-14T21:25:57Z", "comment_text": "\n \t\tThanks <denchmark-link:https://github.com/jlewi>@jlewi</denchmark-link>\n ! Should this issue be in  or is there also a doc update required?\n \t\t"}, "comments_2": {"comment_id": 3, "comment_author": "jlewi", "commentT": "2019-08-14T21:54:05Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/sarahmaddox>@sarahmaddox</denchmark-link>\n  sorry Filed this in the wrong repo will move it.\n \t\t"}, "comments_3": {"comment_id": 4, "comment_author": "jlewi", "commentT": "2019-08-14T21:54:15Z", "comment_text": "\n \t\tIssue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.90. Please mark this comment with \ud83d\udc4d or \ud83d\udc4e to give our bot feedback!\n Links: <denchmark-link:https://github.com/marketplace/issue-label-bot>app homepage</denchmark-link>\n , <denchmark-link:https://mlbot.net/data/kubeflow/kubeflow>dashboard</denchmark-link>\n  and <denchmark-link:https://github.com/hamelsmu/MLapp>code</denchmark-link>\n  for this bot.\n \t\t"}, "comments_4": {"comment_id": 5, "comment_author": "jlewi", "commentT": "2019-08-14T22:37:15Z", "comment_text": "\n \t\t<denchmark-code>kubectl -n istio-system get ingress\n NAME            HOSTS                                                        ADDRESS          PORTS   AGE\n envoy-ingress   kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog   35.244.168.123   80      22h\n </denchmark-code>\n \n so looks like ingress never spun up ssh\n <denchmark-code>kubectl -n istio-system describe ingress\n Name:             envoy-ingress\n Namespace:        istio-system\n Address:          35.244.168.123\n Default backend:  default-http-backend:80 (10.20.0.8:8080)\n Rules:\n   Host                                                        Path  Backends\n   ----                                                        ----  --------\n   kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog  \n                                                               /*   istio-ingressgateway:80 (<none>)\n Annotations:\n   certmanager.k8s.io/issuer:                    letsencrypt-prod\n   ingress.kubernetes.io/backends:               {\"k8s-be-30656--46808f4d5af9eb5c\":\"HEALTHY\",\"k8s-be-31380--46808f4d5af9eb5c\":\"UNHEALTHY\"}\n   ingress.kubernetes.io/forwarding-rule:        k8s-fw-istio-system-envoy-ingress--46808f4d5af9eb5c\n   ingress.kubernetes.io/target-proxy:           k8s-tp-istio-system-envoy-ingress--46808f4d5af9eb5c\n   networking.gke.io/managed-certificates:       gke-certificate\n   ingress.kubernetes.io/ssl-redirect:           true\n   ingress.kubernetes.io/url-map:                k8s-um-istio-system-envoy-ingress--46808f4d5af9eb5c\n   kubernetes.io/ingress.global-static-ip-name:  kf-vmaster-n01-ip\n   kubernetes.io/tls-acme:                       true\n Events:                                         <none>\n </denchmark-code>\n \n Looks like its using managed certificates.\n \t\t"}, "comments_5": {"comment_id": 6, "comment_author": "jlewi", "commentT": "2019-08-14T22:43:15Z", "comment_text": "\n \t\t<denchmark-code>nslookup kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog\n Address: 34.95.64.223\n </denchmark-code>\n \n So address defined.\n It looks like the backend for the ingress which is the ingress gateway is reported as unhealthy\n <denchmark-code>  - name: http2\n     nodePort: 31380\n     port: 80\n     protocol: TCP\n     targetPort: 80\n   - name: https\n     nodePort: 31390\n     port: 443\n     protocol: TCP\n     targetPort: 443\n </denchmark-code>\n \n \t\t"}, "comments_6": {"comment_id": 7, "comment_author": "jlewi", "commentT": "2019-08-14T22:46:50Z", "comment_text": "\n \t\tLets try running a health check over port-forrward\n <denchmark-code>kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80\n \n curl http://localhost:8080\n Origin authentication failed.\n </denchmark-code>\n \n \t\t"}, "comments_7": {"comment_id": 8, "comment_author": "jlewi", "commentT": "2019-08-14T23:14:44Z", "comment_text": "\n \t\tIt looks like the GCP health check path isn't correct.\n The JWT policy check excludes the path \"/healthz\"\n <denchmark-code>kubectl -n istio-system get policy -o yaml ingress-jwt\n apiVersion: authentication.istio.io/v1alpha1\n kind: Policy\n metadata:\n   creationTimestamp: 2019-08-14T00:23:57Z\n   generation: 1\n   labels:\n     kustomize.component: iap-ingress\n   name: ingress-jwt\n   namespace: istio-system\n   resourceVersion: \"4534\"\n   selfLink: /apis/authentication.istio.io/v1alpha1/namespaces/istio-system/policies/ingress-jwt\n   uid: ce7f2c8b-be29-11e9-a0a4-42010a8e009f\n spec:\n   origins:\n   - jwt:\n       audiences:\n       - TO_BE_PATCHED\n       issuer: https://cloud.google.com/iap\n       jwksUri: https://www.gstatic.com/iap/verify/public_key-jwk\n       jwtHeaders:\n       - x-goog-iap-jwt-assertion\n       trigger_rules:\n       - excluded_paths:\n         - exact: /healthz\n         - prefix: /.well-known/acme-challenge\n   principalBinding: USE_ORIGIN\n   targets:\n   - name: istio-ingressgateway\n     ports:\n </denchmark-code>\n \n I updated the health check manually to change the path and now the backend is healthy.\n \t\t"}, "comments_8": {"comment_id": 9, "comment_author": "jlewi", "commentT": "2019-08-14T23:15:37Z", "comment_text": "\n \t\tI think the problem may be that the auto-deploy job might not be using the updated config.\n \t\t"}, "comments_9": {"comment_id": 10, "comment_author": "jlewi", "commentT": "2019-08-14T23:19:16Z", "comment_text": "\n \t\tAuto deploy infra\n <denchmark-link:https://github.com/kubeflow/testing/tree/master/test-infra/auto-deploy>https://github.com/kubeflow/testing/tree/master/test-infra/auto-deploy</denchmark-link>\n \n Not using --config\n <denchmark-link:https://github.com/kubeflow/testing/blob/8cbd626f678d2c830edc38d5990bc2e1c6e1eb70/py/kubeflow/testing/create_kf_instance.py#L65>https://github.com/kubeflow/testing/blob/8cbd626f678d2c830edc38d5990bc2e1c6e1eb70/py/kubeflow/testing/create_kf_instance.py#L65</denchmark-link>\n \n \t\t"}, "comments_10": {"comment_id": 11, "comment_author": "jlewi", "commentT": "2019-08-15T06:21:42Z", "comment_text": "\n \t\tSo I updated the auto-deploy <denchmark-link:https://github.com/kubeflow/kubeflow/pull/441>#441</denchmark-link>\n \n The ingressgateway Deployment has a readiness probe\n <denchmark-code>apiVersion: extensions/v1beta1\n kind: Deployment\n metadata:\n   annotations:\n     deployment.kubernetes.io/revision: \"1\"\n   creationTimestamp: 2019-08-15T05:47:00Z\n   generation: 3\n   labels:\n     app: istio-ingressgateway\n     chart: gateways\n     heritage: Tiller\n     istio: ingressgateway\n     release: istio\n   name: istio-ingressgateway\n   namespace: istio-system\n   resourceVersion: \"3602\"\n   selfLink: /apis/extensions/v1beta1/namespaces/istio-system/deployments/istio-ingressgateway\n   uid: 19cacbaa-bf20-11e9-aedd-42010a8e006f\n spec:\n   progressDeadlineSeconds: 2147483647\n   replicas: 5\n   revisionHistoryLimit: 10\n   selector:\n     matchLabels:\n       app: istio-ingressgateway\n       chart: gateways\n       heritage: Tiller\n       istio: ingressgateway\n       release: istio\n   strategy:\n     rollingUpdate:\n       maxSurge: 1\n       maxUnavailable: 1\n     type: RollingUpdate\n   template:\n     metadata:\n       annotations:\n         sidecar.istio.io/inject: \"false\"\n       creationTimestamp: null\n       labels:\n         app: istio-ingressgateway\n         chart: gateways\n         heritage: Tiller\n         istio: ingressgateway\n         release: istio\n     spec:\n       affinity:\n         nodeAffinity:\n           preferredDuringSchedulingIgnoredDuringExecution:\n           - preference:\n               matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - amd64\n             weight: 2\n           - preference:\n               matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - ppc64le\n             weight: 2\n           - preference:\n               matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - s390x\n             weight: 2\n           requiredDuringSchedulingIgnoredDuringExecution:\n             nodeSelectorTerms:\n             - matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - amd64\n                 - ppc64le\n                 - s390x\n       containers:\n       - args:\n         - proxy\n         - router\n         - --domain\n         - $(POD_NAMESPACE).svc.cluster.local\n         - --log_output_level=default:info\n         - --drainDuration\n         - 45s\n         - --parentShutdownDuration\n         - 1m0s\n         - --connectTimeout\n         - 10s\n         - --serviceCluster\n         - istio-ingressgateway\n         - --zipkinAddress\n         - zipkin:9411\n         - --proxyAdminPort\n         - \"15000\"\n         - --statusPort\n         - \"15020\"\n         - --controlPlaneAuthPolicy\n         - NONE\n         - --discoveryAddress\n         - istio-pilot:15010\n         env:\n         - name: POD_NAME\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.name\n         - name: POD_NAMESPACE\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.namespace\n         - name: INSTANCE_IP\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: status.podIP\n         - name: HOST_IP\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: status.hostIP\n         - name: ISTIO_META_POD_NAME\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.name\n         - name: ISTIO_META_CONFIG_NAMESPACE\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.namespace\n         - name: ISTIO_META_ROUTER_MODE\n           value: sni-dnat\n         image: docker.io/istio/proxyv2:1.1.6\n         imagePullPolicy: IfNotPresent\n         name: istio-proxy\n         ports:\n         - containerPort: 15020\n           protocol: TCP\n         - containerPort: 80\n           protocol: TCP\n         - containerPort: 443\n           protocol: TCP\n         - containerPort: 31400\n           protocol: TCP\n         - containerPort: 15029\n           protocol: TCP\n         - containerPort: 15030\n           protocol: TCP\n         - containerPort: 15031\n           protocol: TCP\n         - containerPort: 15032\n           protocol: TCP\n         - containerPort: 15443\n           protocol: TCP\n         - containerPort: 15090\n           name: http-envoy-prom\n           protocol: TCP\n         readinessProbe:\n           failureThreshold: 30\n           httpGet:\n             path: /healthz/ready\n             port: 15020\n             scheme: HTTP\n           initialDelaySeconds: 1\n           periodSeconds: 2\n           successThreshold: 1\n           timeoutSeconds: 1\n         resources:\n           limits:\n             cpu: 100m\n             memory: 128Mi\n           requests:\n             cpu: 10m\n             memory: 40Mi\n         terminationMessagePath: /dev/termination-log\n         terminationMessagePolicy: File\n         volumeMounts:\n         - mountPath: /etc/certs\n           name: istio-certs\n           readOnly: true\n         - mountPath: /etc/istio/ingressgateway-certs\n           name: ingressgateway-certs\n           readOnly: true\n         - mountPath: /etc/istio/ingressgateway-ca-certs\n           name: ingressgateway-ca-certs\n           readOnly: true\n       dnsPolicy: ClusterFirst\n       restartPolicy: Always\n       schedulerName: default-scheduler\n       securityContext: {}\n       serviceAccount: istio-ingressgateway-service-account\n       serviceAccountName: istio-ingressgateway-service-account\n       terminationGracePeriodSeconds: 30\n       volumes:\n       - name: istio-certs\n         secret:\n           defaultMode: 420\n           optional: true\n           secretName: istio.istio-ingressgateway-service-account\n       - name: ingressgateway-certs\n         secret:\n           defaultMode: 420\n           optional: true\n           secretName: istio-ingressgateway-certs\n       - name: ingressgateway-ca-certs\n         secret:\n           defaultMode: 420\n           optional: true\n           secretName: istio-ingressgateway-ca-certs\n status:\n   availableReplicas: 5\n   conditions:\n   - lastTransitionTime: 2019-08-15T05:49:44Z\n     lastUpdateTime: 2019-08-15T05:49:44Z\n     message: Deployment has minimum availability.\n     reason: MinimumReplicasAvailable\n     status: \"True\"\n     type: Available\n   observedGeneration: 3\n   readyReplicas: 5\n   replicas: 5\n   updatedReplicas: 5\n \n </denchmark-code>\n \n But the health check still isn't picking it up; the GCLB set the health check to \"/\".\n I tried changing to \"/healthz/ready\" that didn't work.\n I also tried changing to \"/healthz\"\n \t\t"}, "comments_11": {"comment_id": 12, "comment_author": "jlewi", "commentT": "2019-08-15T06:26:42Z", "comment_text": "\n \t\tThe URL\n <denchmark-link:https://kf-v0-6-n04.endpoints.kubeflow-ci-deployment.cloud.goog/>https://kf-v0-6-n04.endpoints.kubeflow-ci-deployment.cloud.goog/</denchmark-link>\n \n works\n Here's the corresponding ingressgateway deployment\n <denchmark-code>apiVersion: extensions/v1beta1\n kind: Deployment\n metadata:\n   annotations:\n     deployment.kubernetes.io/revision: \"1\"\n   creationTimestamp: 2019-08-15T00:22:15Z\n   generation: 6\n   labels:\n     app: istio-ingressgateway\n     chart: gateways\n     heritage: Tiller\n     istio: ingressgateway\n     release: istio\n   name: istio-ingressgateway\n   namespace: istio-system\n   resourceVersion: \"78853\"\n   selfLink: /apis/extensions/v1beta1/namespaces/istio-system/deployments/istio-ingressgateway\n   uid: bc09ac76-bef2-11e9-a983-42010a8e0066\n spec:\n   progressDeadlineSeconds: 2147483647\n   replicas: 4\n   revisionHistoryLimit: 10\n   selector:\n     matchLabels:\n       app: istio-ingressgateway\n       chart: gateways\n       heritage: Tiller\n       istio: ingressgateway\n       release: istio\n   strategy:\n     rollingUpdate:\n       maxSurge: 1\n       maxUnavailable: 1\n     type: RollingUpdate\n   template:\n     metadata:\n       annotations:\n         sidecar.istio.io/inject: \"false\"\n       creationTimestamp: null\n       labels:\n         app: istio-ingressgateway\n         chart: gateways\n         heritage: Tiller\n         istio: ingressgateway\n         release: istio\n     spec:\n       affinity:\n         nodeAffinity:\n           preferredDuringSchedulingIgnoredDuringExecution:\n           - preference:\n               matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - amd64\n             weight: 2\n           - preference:\n               matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - ppc64le\n             weight: 2\n           - preference:\n               matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - s390x\n             weight: 2\n           requiredDuringSchedulingIgnoredDuringExecution:\n             nodeSelectorTerms:\n             - matchExpressions:\n               - key: beta.kubernetes.io/arch\n                 operator: In\n                 values:\n                 - amd64\n                 - ppc64le\n                 - s390x\n       containers:\n       - args:\n         - proxy\n         - router\n         - --domain\n         - $(POD_NAMESPACE).svc.cluster.local\n         - --log_output_level=default:info\n         - --drainDuration\n         - 45s\n         - --parentShutdownDuration\n         - 1m0s\n         - --connectTimeout\n         - 10s\n         - --serviceCluster\n         - istio-ingressgateway\n         - --zipkinAddress\n         - zipkin:9411\n         - --proxyAdminPort\n         - \"15000\"\n         - --statusPort\n         - \"15020\"\n         - --controlPlaneAuthPolicy\n         - NONE\n         - --discoveryAddress\n         - istio-pilot:15010\n         env:\n         - name: POD_NAME\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.name\n         - name: POD_NAMESPACE\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.namespace\n         - name: INSTANCE_IP\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: status.podIP\n         - name: HOST_IP\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: status.hostIP\n         - name: ISTIO_META_POD_NAME\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.name\n         - name: ISTIO_META_CONFIG_NAMESPACE\n           valueFrom:\n             fieldRef:\n               apiVersion: v1\n               fieldPath: metadata.namespace\n         - name: ISTIO_META_ROUTER_MODE\n           value: sni-dnat\n         image: docker.io/istio/proxyv2:1.1.6\n         imagePullPolicy: IfNotPresent\n         name: istio-proxy\n         ports:\n         - containerPort: 15020\n           protocol: TCP\n         - containerPort: 80\n           protocol: TCP\n         - containerPort: 443\n           protocol: TCP\n         - containerPort: 31400\n           protocol: TCP\n         - containerPort: 15029\n           protocol: TCP\n         - containerPort: 15030\n           protocol: TCP\n         - containerPort: 15031\n           protocol: TCP\n         - containerPort: 15032\n           protocol: TCP\n         - containerPort: 15443\n           protocol: TCP\n         - containerPort: 15090\n           name: http-envoy-prom\n           protocol: TCP\n         readinessProbe:\n           failureThreshold: 30\n           httpGet:\n             path: /healthz/ready\n             port: 15020\n             scheme: HTTP\n           initialDelaySeconds: 1\n           periodSeconds: 2\n           successThreshold: 1\n           timeoutSeconds: 1\n         resources:\n           limits:\n             cpu: 100m\n             memory: 128Mi\n           requests:\n             cpu: 10m\n             memory: 40Mi\n         terminationMessagePath: /dev/termination-log\n         terminationMessagePolicy: File\n         volumeMounts:\n         - mountPath: /etc/certs\n           name: istio-certs\n           readOnly: true\n         - mountPath: /etc/istio/ingressgateway-certs\n           name: ingressgateway-certs\n           readOnly: true\n         - mountPath: /etc/istio/ingressgateway-ca-certs\n           name: ingressgateway-ca-certs\n           readOnly: true\n       dnsPolicy: ClusterFirst\n       restartPolicy: Always\n       schedulerName: default-scheduler\n       securityContext: {}\n       serviceAccount: istio-ingressgateway-service-account\n       serviceAccountName: istio-ingressgateway-service-account\n       terminationGracePeriodSeconds: 30\n       volumes:\n       - name: istio-certs\n         secret:\n           defaultMode: 420\n           optional: true\n           secretName: istio.istio-ingressgateway-service-account\n       - name: ingressgateway-certs\n         secret:\n           defaultMode: 420\n           optional: true\n           secretName: istio-ingressgateway-certs\n       - name: ingressgateway-ca-certs\n         secret:\n           defaultMode: 420\n           optional: true\n           secretName: istio-ingressgateway-ca-certs\n status:\n   availableReplicas: 4\n   conditions:\n   - lastTransitionTime: 2019-08-15T00:22:15Z\n     lastUpdateTime: 2019-08-15T00:22:15Z\n     message: Deployment has minimum availability.\n     reason: MinimumReplicasAvailable\n     status: \"True\"\n     type: Available\n   observedGeneration: 6\n   readyReplicas: 4\n   replicas: 4\n   updatedReplicas: 4\n \n </denchmark-code>\n \n \t\t"}, "comments_12": {"comment_id": 13, "comment_author": "jlewi", "commentT": "2019-08-15T13:28:54Z", "comment_text": "\n \t\tNext step would be to deploy from master and see if that works.\n An E2E test was added for IAP for postsubmits.\n <denchmark-link:https://github.com/kubeflow/kubeflow/pull/3482/files>https://github.com/kubeflow/kubeflow/pull/3482/files</denchmark-link>\n \n I think there may be a bug in the ksonnet because it doesn't look the endpoint-is-ready-test is actually included as part of the workflow\n <denchmark-link:https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/kubeflow_kubeflow/kubeflow-postsubmit/1161877435558400003>https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/kubeflow_kubeflow/kubeflow-postsubmit/1161877435558400003</denchmark-link>\n \n \t\t"}, "comments_13": {"comment_id": 14, "comment_author": "jlewi", "commentT": "2019-08-15T18:43:09Z", "comment_text": "\n \t\tHere's a subsequent post-submit run.\n <denchmark-link:https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-periodic-master>https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-periodic-master</denchmark-link>\n \n The endpoint test ran but it failed.\n So it looks like the problem is on master as well.\n \t\t"}, "comments_14": {"comment_id": 15, "comment_author": "jlewi", "commentT": "2019-08-15T20:20:14Z", "comment_text": "\n \t\tIt looks like backend-updater pod is stuck\n <denchmark-code>Waiting for the backend-services resource PROJECT=jlewi-dev NODEPORT=31380 SERVICE=istio-ingressgateway...\n ERROR: (gcloud.compute.backend-services.list) Some requests did not succeed:\n  - Insufficient Permission: Request had insufficient authentication scopes.\n </denchmark-code>\n \n \t\t"}, "comments_15": {"comment_id": 16, "comment_author": "jlewi", "commentT": "2019-08-15T20:22:56Z", "comment_text": "\n \t\tLooks like that script is what's updating the backend health check to /healthz\n \n \n \n kubeflow/kubeflow/gcp/update_backend.sh\n \n \n          Line 44\n       in\n       1a6b85e\n \n \n \n \n \n \n  # Since we create the envoy-ingress ingress object before creating the envoy \n \n \n \n \n \n So I suspect an issue with workload identity.\n \t\t"}, "comments_16": {"comment_id": 17, "comment_author": "jlewi", "commentT": "2019-08-15T20:28:55Z", "comment_text": "\n \t\tkf-admin service account is missing  GSA annotation\n <denchmark-code> kubectl -n istio-system get serviceaccount -o yaml kf-admin\n \n apiVersion: v1\n kind: ServiceAccount\n metadata:\n   creationTimestamp: 2019-08-15T19:03:16Z\n   labels:\n     kustomize.component: iap-ingress\n   name: kf-admin\n   namespace: istio-system\n   resourceVersion: \"4045\"\n   selfLink: /api/v1/namespaces/istio-system/serviceaccounts/kf-admin\n   uid: 56bb5809-bf8f-11e9-a20b-42010a8e00b9\n secrets:\n - name: kf-admin-token-g657p\n </denchmark-code>\n \n \t\t"}, "comments_17": {"comment_id": 18, "comment_author": "jlewi", "commentT": "2019-08-15T21:49:23Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lluunn>@lluunn</denchmark-link>\n  is investigating; on master we shouldn't be using workload identity yet; so it should be applying the overlay to add gcp-credentials which adds secrets but looks like the overlay isn't getting added.\n \t\t"}, "comments_18": {"comment_id": 19, "comment_author": "jlewi", "commentT": "2019-08-16T06:38:46Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lluunn>@lluunn</denchmark-link>\n  Still looks like the tests are failing\n <denchmark-link:https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit>https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit</denchmark-link>\n \n \t\t"}, "comments_19": {"comment_id": 20, "comment_author": "jlewi", "commentT": "2019-08-16T16:00:56Z", "comment_text": "\n \t\tack, looking\n \t\t"}, "comments_20": {"comment_id": 21, "comment_author": "jlewi", "commentT": "2019-08-19T15:20:16Z", "comment_text": "\n \t\tv0.6-branch auto-deployments are still picking up kfctl_gcp_iap.yaml from master.\n Looks like I need to update the docker images used by the deployment jobs as that contains auto_deploy.sh which needs to pass on the kfctl_config file.\n Not sure if that's the same problem affecting the master auto-deployments.\n /cc <denchmark-link:https://github.com/lluunn>@lluunn</denchmark-link>\n \n \t\t"}, "comments_21": {"comment_id": 22, "comment_author": "jlewi", "commentT": "2019-08-19T18:42:32Z", "comment_text": "\n \t\tlatest post-submit endpoint is ready is green now: <denchmark-link:https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit>https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit</denchmark-link>\n \n \t\t"}, "comments_22": {"comment_id": 23, "comment_author": "jlewi", "commentT": "2019-08-19T18:51:39Z", "comment_text": "\n \t\t<denchmark-link:https://kf-vmaster-n04.endpoints.kubeflow-ci-deployment.cloud.goog/>https://kf-vmaster-n04.endpoints.kubeflow-ci-deployment.cloud.goog/</denchmark-link>\n  still not accessible.\n unexpectedly closed the connection.\n describe ingress:\n Error 404: The resource 'projects/kubeflow-ci-deployment/global/sslCertificates/mcrt-92b2c917-da8f-42ca-b337-c2a1def72f8c' was not found, notFound\n Seems like problem with managed cert.\n describe certificate:\n managed-certificate-controller  googleapi: Error 403: Quota 'SSL_CERTIFICATES' exceeded. Limit: 40.0 globally., quotaExceeded\n \t\t"}, "comments_23": {"comment_id": 24, "comment_author": "jlewi", "commentT": "2019-08-19T18:55:50Z", "comment_text": "\n \t\tNot sure if when we delete cluster, the managed cert will be deleted as well?\n Or there is a leak..\n \t\t"}, "comments_24": {"comment_id": 25, "comment_author": "jlewi", "commentT": "2019-08-19T18:57:04Z", "comment_text": "\n \t\tThe last 8 are from the new managed cert\n <denchmark-code>$ gcloud compute ssl-certificates list --project=kubeflow-ci-deployment\n NAME                                                         CREATION_TIMESTAMP\n k8s-ssl-0ff442a6e235686c-460d278ff9b07cb9--4ae776f4f553570c  2019-04-29T15:31:24.406-07:00\n k8s-ssl-2992686342891c62-ca49d77fbf46b99d--26806559b254c2b2  2019-05-06T19:57:47.575-07:00\n k8s-ssl-29b1b58aa9185254-125c25bdeff101f7--bee044758e7e549c  2019-04-30T11:44:20.210-07:00\n k8s-ssl-2f5695bb078b34f3-49e805e5ccf66148--5527702567ce5341  2019-06-15T05:27:55.399-07:00\n k8s-ssl-332d6b14043444d6-f89770934f999d31--a502582361df8372  2019-05-07T11:09:46.804-07:00\n k8s-ssl-367c8c934bc73a1b-5c53f38c50dc3e89--7c9f358a54e3b240  2019-04-30T14:18:29.075-07:00\n k8s-ssl-489a5fc259effe4e-567cf32b1e03aafb--bb9a3658e2516006  2019-04-30T14:37:34.315-07:00\n k8s-ssl-4d5bec85b4a399e0-20395b3addde4973--4da7e63dde472f4c  2019-06-04T05:07:02.391-07:00\n k8s-ssl-519fc4f59435298c-2497a3287a14ef2f--9eb1a983acd997f9  2019-04-29T13:06:11.855-07:00\n k8s-ssl-5a59f83a48f10be7-ccb41ba826909393--424e4ee951a55fda  2019-05-06T20:03:34.554-07:00\n k8s-ssl-5f4fb336806a00c0-285498d1484e381c--20f5463addbc5c1f  2019-05-10T17:14:10.875-07:00\n k8s-ssl-60b2a0301c05f739-74701b1f13e856a3--d1375bf3cda1f61d  2019-05-06T19:58:01.186-07:00\n k8s-ssl-6f00577a44f9db44-2f36293cfcb4f8a7--bb3d0cf69abf1b2c  2019-04-30T15:36:28.300-07:00\n k8s-ssl-70f5481e587f657b-7e921ae4a9b9f882--8239117d0e5ba932  2019-05-13T13:50:52.121-07:00\n k8s-ssl-76f099d80d536c0e-3199ff8e324385c7--ae8bace9ef692851  2019-06-27T17:46:38.271-07:00\n k8s-ssl-8171a698662bc0cc-d44151628081e1a5--7474eca042ec1d1b  2019-08-16T17:09:16.256-07:00\n k8s-ssl-929ae769c54a6995-bdc66256d45b13b1--0cce0ca7c3884348  2019-05-14T01:06:43.168-07:00\n k8s-ssl-9351026770de791a-9b68b792f6a78126--278488d592b4b170  2019-02-20T11:27:31.344-08:00\n k8s-ssl-a080df3fe2b6e319-61efcf233e31032e--79d069c18c20d150  2019-05-07T07:02:21.272-07:00\n k8s-ssl-a345445688e91809-51b8faf754799f23--58db1229215ae1be  2019-04-30T02:29:05.802-07:00\n k8s-ssl-a7a61826c2166f35-594bace88a2ba28a--2e710c3c79412b27  2019-04-26T17:49:40.870-07:00\n k8s-ssl-a933842bee7a9359-5950cd35524a14b4--b9282641e95fc479  2019-03-28T11:55:28.972-07:00\n k8s-ssl-aad965ee0bdc11bc-cb865b3743c620cb--1e80e464eaa362ef  2019-04-30T12:47:52.968-07:00\n k8s-ssl-ad38bbbf5a50d081-0b169cf51ff1fada--c75ceb069223d40b  2019-07-01T07:37:59.053-07:00\n k8s-ssl-ad5932ae000b187f-2a4f324b7443bb37--f82cb094c628fbdc  2019-05-01T16:47:15.836-07:00\n k8s-ssl-c5ee34edab6471d8-be8221ff0051f1c4--41c3bce925563039  2019-07-09T06:11:50.800-07:00\n k8s-ssl-d610bead7d63a72f-13071c6dcad318dd--bca72a4567bb6370  2019-03-28T12:11:35.890-07:00\n k8s-ssl-dad84f65cad94c31-ca81b421956ea4f1--a71c868e26dada4e  2019-04-30T14:08:47.780-07:00\n k8s-ssl-db8fd3936e021334-914706fd254f8e27--5481b7cb9e85e70c  2019-05-07T20:41:01.407-07:00\n k8s-ssl-e58cbc4a1db0fa39-aedd3061534edd0a--1dff0ec7b5676067  2019-08-15T17:11:52.118-07:00\n k8s-ssl-ee4e98c1940507d6-47bc9c7b8121a910--11ae48544d7b6644  2019-05-13T20:56:46.429-07:00\n k8s-ssl-ffffc95b05b976aa-22b80b4b6f0b0134--d5384c3095008610  2019-05-06T20:03:20.788-07:00\n mcrt-085dca4c-16b6-4e89-9229-9ed5a0bb850b                    2019-08-19T10:24:40.391-07:00\n mcrt-29cc7462-0365-401e-9dc2-287fa7d9ed75                    2019-08-14T16:21:32.606-07:00\n mcrt-33c37277-2e44-4912-b7ff-a828186f92f9                    2019-08-16T05:08:07.584-07:00\n mcrt-4cd67bb9-7e15-47b2-82ad-39e26f27b96c                    2019-08-14T17:25:19.634-07:00\n mcrt-68a35467-0eed-479f-ba99-ba0d2d4c39d0                    2019-08-14T16:20:34.196-07:00\n mcrt-a31c4815-24e5-4acd-8260-8ee79fbae5ad                    2019-08-15T05:36:41.817-07:00\n mcrt-cf9e6264-3f1c-4b52-8c85-44c5a3ec9aef                    2019-08-14T23:06:13.339-07:00\n mcrt-fe13e1ac-ecd2-40c7-9e4a-e72a28b7268d                    2019-08-14T17:30:08.173-07:00\n </denchmark-code>\n \n \t\t"}, "comments_25": {"comment_id": 26, "comment_author": "jlewi", "commentT": "2019-08-19T18:59:35Z", "comment_text": "\n \t\tI am increasing quota to 60 for now, and we can see if managed cert is leaking\n \t\t"}, "comments_26": {"comment_id": 27, "comment_author": "jlewi", "commentT": "2019-08-19T20:02:45Z", "comment_text": "\n \t\t<denchmark-link:https://github.com/lluunn>@lluunn</denchmark-link>\n  I think the problem is that with the auto-deployed clusters are using a self-signed certificate to get around lets encrypt limitations.  I suspect that interferes with managed certificates.\n I think we can probably remove that now. In the past we were recycling hostnames because of IAP but with universal redirect we probably no longer need to recycle hostnames so we could give unique hostnames to each deployment.\n I will look into fixing this.\n \t\t"}, "comments_27": {"comment_id": 28, "comment_author": "jlewi", "commentT": "2019-08-19T23:03:02Z", "comment_text": "\n \t\tThe endpoint ended up coming up if I didn't set a self-signed certificate\n <denchmark-link:https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog/>https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog/</denchmark-link>\n \n Opened up kubeflow/testin#444 about note reusing hostnames.\n We''ll have to see whether we run into lets encrypt quota errors if we recycle certificates.\n \t\t"}, "comments_28": {"comment_id": 29, "comment_author": "jlewi", "commentT": "2019-08-19T23:39:20Z", "comment_text": "\n \t\tOn a subsequent trial; it looks like GKE managed certificate is giving me\n <denchmark-code> Warning  BackendError  19s (x14 over 8m58s)  managed-certificate-controller  (combined from similar events): operation operation-1566257853630-59080d2fe0a09-8990e3c1-13c8bb50 failed: FORBIDDEN\n \n </denchmark-code>\n \n \t\t"}, "comments_29": {"comment_id": 30, "comment_author": "jlewi", "commentT": "2019-08-19T23:45:06Z", "comment_text": "\n \t\tLooks like it could be an SSL quota certificate issue; we have quota for 60 SSL certificates and looks like we are using all of them.\n \t\t"}, "comments_30": {"comment_id": 31, "comment_author": "jlewi", "commentT": "2019-09-27T19:57:37Z", "comment_text": "\n \t\tdupe <denchmark-link:https://github.com/kubeflow/kubeflow/issues/3834>#3834</denchmark-link>\n \n I've been  seeing this when  on CLI deploy with GPU pools.\n  Warning  FailedMount  ... Unable to mount volumes for pod \"backend-updater-0_istio-system...\": timeout expired waiting for volumes to attach or mount for pod \"istio-system\"/\"backend-updater-0\". list of unmounted volumes=[sa-key]. list of unattached volumes=[config-volume sa-key envoy-token-4lz4m]\n and here\n  MountVolume.SetUp failed for volume \"sa-key\" : secrets \"admin-gcp-sa\" not found\n but I have the admin-gcp-sa is there , might it be a naming mismatch ? I can look further if this is related. namespace mismatch.\n In my case, this stalls backend-updater and the reset as described above.\n this worked for me  as a temporary fix\n kubectl get secret admin-gcp-sa --namespace ${NAMESPACE}  --export -o yaml |\\ \n kubectl apply --namespace=istio-system -f -\n \t\t"}, "comments_31": {"comment_id": 32, "comment_author": "jlewi", "commentT": "2019-11-02T02:01:30Z", "comment_text": "\n \t\tThis has been fixed. I verified the following endpoint was accessible.\n <denchmark-link:https://kf-vmaster-1102-87a.endpoints.kubeflow-ci-deployment.cloud.goog/>https://kf-vmaster-1102-87a.endpoints.kubeflow-ci-deployment.cloud.goog/</denchmark-link>\n \n \t\t"}}}, "commit": {"commit_id": "1a6b85e3904b732c76d4104480011f6a702cf525", "commit_author": "Jeremy Lewi", "commitT": "2019-08-15 09:06:35-07:00", "changed_files": {"file_0": {"file_change_type": "MODIFY", "file_Nmethod": 0, "file_old_name": "prow_config.yaml", "file_new_name": "prow_config.yaml", "hunks": {"hunk_0": {"Ismethod": 0, "added_lines": "47", "deleted_lines": "47"}}}}}}