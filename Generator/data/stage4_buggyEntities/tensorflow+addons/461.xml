<commit id='6633c43afb6bdd66b43933a2221847b4d273bc87' author='Guillaume Klein' date='2019-09-09 21:02:53-04:00'>
	<dmm_unit complexity='1.0' interfacing='0.9782608695652174' size='0.2391304347826087'></dmm_unit>
	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper.py' new_name='tensorflow_addons\seq2seq\attention_wrapper.py'>
		<file_info nloc='1088' complexity='137' token_count='6371'></file_info>
		<method name='state_size' parameters='self'>
				<method_info nloc='12' complexity='5' token_count='93' nesting_level='1' start_line='1802' end_line='1819'></method_info>
			<added_lines>1812</added_lines>
			<deleted_lines></deleted_lines>
		</method>
		<method name='_batch_size_checks' parameters='self,batch_size,error_message'>
				<method_info nloc='9' complexity='2' token_count='41' nesting_level='1' start_line='1744' end_line='1752'></method_info>
			<added_lines>1745</added_lines>
			<deleted_lines></deleted_lines>
		</method>
		<method name='_attention_mechanisms_checks' parameters='self'>
				<method_info nloc='8' complexity='3' token_count='27' nesting_level='1' start_line='1735' end_line='1742'></method_info>
			<added_lines>1735,1736,1737,1738,1739,1740,1741,1742</added_lines>
			<deleted_lines></deleted_lines>
		</method>
		<method name='get_initial_state' parameters='self,inputs,batch_size,dtype'>
				<method_info nloc='44' complexity='7' token_count='267' nesting_level='1' start_line='1821' end_line='1884'></method_info>
			<added_lines>1871</added_lines>
			<deleted_lines>1844</deleted_lines>
		</method>
		<method name='output_size' parameters='self'>
				<method_info nloc='5' complexity='2' token_count='24' nesting_level='1' start_line='1795' end_line='1799'></method_info>
			<added_lines>1797</added_lines>
			<deleted_lines></deleted_lines>
		</method>
		<method name='_get_attention_layer_size' parameters='self'>
				<method_info nloc='16' complexity='5' token_count='102' nesting_level='1' start_line='1754' end_line='1772'></method_info>
			<added_lines>1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772</added_lines>
			<deleted_lines>1770</deleted_lines>
		</method>
		<method name='memory_initialized' parameters='self'>
				<method_info nloc='2' complexity='1' token_count='10' nesting_level='1' start_line='142' end_line='146'></method_info>
			<added_lines>142,143,144,145,146</added_lines>
			<deleted_lines></deleted_lines>
		</method>
		<modified_lines>
			<added_lines>141,147,1705,1743,1773</added_lines>
			<deleted_lines>1683,1693,1694,1695,1696,1697,1698,1699,1702,1703,1704,1785</deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='MODIFY' old_name='tensorflow_addons\seq2seq\attention_wrapper_test.py' new_name='tensorflow_addons\seq2seq\attention_wrapper_test.py'>
		<file_info nloc='733' complexity='31' token_count='5996'></file_info>
		<method name='testCustomAttentionLayer' parameters='self'>
				<method_info nloc='19' complexity='1' token_count='199' nesting_level='1' start_line='251' end_line='271'></method_info>
			<added_lines>251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271</added_lines>
			<deleted_lines></deleted_lines>
		</method>
		<modified_lines>
			<added_lines>272</added_lines>
			<deleted_lines></deleted_lines>
		</modified_lines>
	</modification>
</commit>
