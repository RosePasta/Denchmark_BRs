<commit id='89d18af12792442f8ce5df86027aaa3f908240ec' author='Myle Ott' date='2020-05-04 07:16:30-07:00'>
	<dmm_unit complexity='0.0' interfacing='0.0' size='1.0'></dmm_unit>
	<modification change_type='MODIFY' old_name='fairseq\models\transformer.py' new_name='fairseq\models\transformer.py'>
		<file_info nloc='741' complexity='100' token_count='4925'></file_info>
		<method name='transformer_align' parameters='args'>
				<method_info nloc='5' complexity='1' token_count='45' nesting_level='0' start_line='1089' end_line='1093'></method_info>
			<added_lines></added_lines>
			<deleted_lines>1089,1090,1091,1092,1093</deleted_lines>
		</method>
		<method name='forward' parameters='self,src_tokens,src_lengths,prev_output_tokens'>
				<method_info nloc='3' complexity='1' token_count='30' nesting_level='1' start_line='334' end_line='336'></method_info>
			<added_lines></added_lines>
			<deleted_lines>334,335,336</deleted_lines>
		</method>
		<method name='transformer_wmt_en_de_big_align' parameters='args'>
				<method_info nloc='4' complexity='1' token_count='33' nesting_level='0' start_line='1097' end_line='1100'></method_info>
			<added_lines></added_lines>
			<deleted_lines>1097,1098,1099,1100</deleted_lines>
		</method>
		<method name='__init__' parameters='self,args,dictionary,embed_tokens,no_encoder_attn'>
				<method_info nloc='89' complexity='16' token_count='573' nesting_level='1' start_line='530' end_line='633'></method_info>
			<added_lines>579,580,581,582,583,584,585,586,587</added_lines>
			<deleted_lines>566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581</deleted_lines>
		</method>
		<method name='add_args' parameters='parser'>
				<method_info nloc='74' complexity='1' token_count='617' nesting_level='1' start_line='94' end_line='172'></method_info>
			<added_lines></added_lines>
			<deleted_lines>155,156</deleted_lines>
		</method>
		<method name='__init__' parameters='self,encoder,decoder,args'>
				<method_info nloc='5' complexity='1' token_count='44' nesting_level='1' start_line='306' end_line='310'></method_info>
			<added_lines></added_lines>
			<deleted_lines>306,307,308,309,310</deleted_lines>
		</method>
		<method name='buffered_future_mask' parameters='self,tensor'>
				<method_info nloc='15' complexity='5' token_count='115' nesting_level='1' start_line='566' end_line='580'></method_info>
			<added_lines>579,580</added_lines>
			<deleted_lines>566,567,568,569,570,571,572,573,574,575,576,577,578,579,580</deleted_lines>
		</method>
		<method name='build_model' parameters='cls,args,task'>
				<method_info nloc='6' complexity='1' token_count='36' nesting_level='1' start_line='325' end_line='332'></method_info>
			<added_lines></added_lines>
			<deleted_lines>325,326,327,328,329,330,331,332</deleted_lines>
		</method>
		<method name='__init__' parameters='self,args,dictionary,embed_tokens'>
				<method_info nloc='45' complexity='9' token_count='282' nesting_level='1' start_line='309' end_line='361'></method_info>
			<added_lines>344,345,346,347,348,349,350,351</added_lines>
			<deleted_lines>309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361</deleted_lines>
		</method>
		<method name='base_architecture' parameters='args'>
				<method_info nloc='40' complexity='1' token_count='391' nesting_level='0' start_line='990' end_line='1031'></method_info>
			<added_lines></added_lines>
			<deleted_lines>1023</deleted_lines>
		</method>
		<method name='forward_decoder' parameters='self,prev_output_tokens,encoder_out,incremental_state,features_only,extra_args'>
				<method_info nloc='7' complexity='1' token_count='21' nesting_level='1' start_line='338' end_line='344'></method_info>
			<added_lines>344</added_lines>
			<deleted_lines>338,339,340,341,342,343,344</deleted_lines>
		</method>
		<modified_lines>
			<added_lines>23,418,419,420,421,754,755,756,757,758,759,760,761,762,763,764,765,766</added_lines>
			<deleted_lines>299,300,301,302,303,304,305,362,363,364,365,412,413,414,415,416,417,472,473,474,487,488,489,490,491,492,493,497,498,669,670,671,672,673,674,675,676,677,839,840,841,842,843,844,845,846,847,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,1086,1087,1088,1094,1095,1096</deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='ADD' old_name='None' new_name='fairseq\models\transformer_align.py'>
		<file_info nloc='71' complexity='7' token_count='420'></file_info>
	</modification>
	<modification change_type='MODIFY' old_name='fairseq\modules\__init__.py' new_name='fairseq\modules\__init__.py'>
		<file_info nloc='63' complexity='0' token_count='220'></file_info>
		<modified_lines>
			<added_lines>20,53</added_lines>
			<deleted_lines></deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='ADD' old_name='None' new_name='fairseq\modules\layer_drop.py'>
		<file_info nloc='35' complexity='5' token_count='99'></file_info>
	</modification>
	<modification change_type='MODIFY' old_name='tests\test_binaries.py' new_name='tests\test_binaries.py'>
		<file_info nloc='1047' complexity='93' token_count='5585'></file_info>
		<method name='test_transformer_cross_self_attention' parameters='self'>
				<method_info nloc='16' complexity='1' token_count='81' nesting_level='1' start_line='238' end_line='253'></method_info>
			<added_lines></added_lines>
			<deleted_lines>251</deleted_lines>
		</method>
		<modified_lines>
			<added_lines></added_lines>
			<deleted_lines></deleted_lines>
		</modified_lines>
	</modification>
</commit>
