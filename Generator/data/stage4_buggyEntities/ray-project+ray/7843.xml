<commit id='bb6c67523171f807c631461b97c00475b21300ef' author='Sven Mika' date='2020-04-03 10:44:58-07:00'>
	<dmm_unit complexity='0.96' interfacing='0.96' size='0.04'></dmm_unit>
	<modification change_type='MODIFY' old_name='rllib\BUILD' new_name='rllib\BUILD'>
		<file_info nloc='None' complexity='None' token_count='None'></file_info>
		<modified_lines>
			<added_lines>113</added_lines>
			<deleted_lines>113</deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='MODIFY' old_name='rllib\agents\ppo\ppo.py' new_name='rllib\agents\ppo\ppo.py'>
		<file_info nloc='132' complexity='27' token_count='752'></file_info>
		<method name='choose_policy_optimizer' parameters='workers,config'>
				<method_info nloc='19' complexity='2' token_count='112' nesting_level='0' start_line='80' end_line='99'></method_info>
			<added_lines>98,99</added_lines>
			<deleted_lines>95</deleted_lines>
		</method>
		<modified_lines>
			<added_lines>70,71,72</added_lines>
			<deleted_lines></deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='MODIFY' old_name='rllib\agents\ppo\tests\test_ppo.py' new_name='rllib\agents\ppo\tests\test_ppo.py'>
		<file_info nloc='184' complexity='22' token_count='1688'></file_info>
		<method name='test_ppo_fake_multi_gpu_learning' parameters='self'>
				<method_info nloc='22' complexity='3' token_count='142' nesting_level='1' start_line='49' end_line='74'></method_info>
			<added_lines>49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74</added_lines>
			<deleted_lines></deleted_lines>
		</method>
		<modified_lines>
			<added_lines>75</added_lines>
			<deleted_lines></deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='MODIFY' old_name='rllib\optimizers\multi_gpu_optimizer.py' new_name='rllib\optimizers\multi_gpu_optimizer.py'>
		<file_info nloc='187' complexity='23' token_count='1276'></file_info>
		<method name='__init__' parameters='self,workers,sgd_batch_size,num_sgd_iter,rollout_fragment_length,num_envs_per_worker,train_batch_size,num_gpus,standardize_fields,shuffle_sequences'>
				<method_info nloc='10' complexity='1' token_count='40' nesting_level='1' start_line='40' end_line='49'></method_info>
			<added_lines></added_lines>
			<deleted_lines>49</deleted_lines>
		</method>
		<method name='__init__' parameters='self,workers,sgd_batch_size,num_sgd_iter,rollout_fragment_length,num_envs_per_worker,train_batch_size,num_gpus,standardize_fields,shuffle_sequences,_fake_gpus'>
				<method_info nloc='11' complexity='1' token_count='44' nesting_level='1' start_line='41' end_line='51'></method_info>
			<added_lines>50,51</added_lines>
			<deleted_lines>49</deleted_lines>
		</method>
		<modified_lines>
			<added_lines>32,33,34,67,68,69,79,80,82,83,84,85,86,87,88</added_lines>
			<deleted_lines>32,33,75,76,77,78,79</deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='MODIFY' old_name='rllib\policy\dynamic_tf_policy.py' new_name='rllib\policy\dynamic_tf_policy.py'>
		<file_info nloc='314' complexity='36' token_count='2047'></file_info>
		<method name='copy' parameters='self,existing_inputs'>
				<method_info nloc='36' complexity='10' token_count='327' nesting_level='1' start_line='262' end_line='304'></method_info>
			<added_lines>285,286,287</added_lines>
			<deleted_lines>284,285</deleted_lines>
		</method>
		<modified_lines>
			<added_lines>115,116,128,129,130</added_lines>
			<deleted_lines>126,127,178,179</deleted_lines>
		</modified_lines>
	</modification>
	<modification change_type='ADD' old_name='None' new_name='rllib\tuned_examples\regression_tests\cartpole-ppo-tf-multi-gpu.yaml'>
		<file_info nloc='None' complexity='None' token_count='None'></file_info>
	</modification>
</commit>
