<bug id='214' author='alan-krumholz' open_date='2019-03-14T13:07:31Z' closed_time='2019-03-19T01:39:48Z'>
	<summary>output text generator failing at prediction time</summary>
	<description>
Describe the bug
Predictions are not produced for trained model
To Reproduce
Steps to reproduce the behavior:

run this code to reproduce:

&lt;denchmark-code&gt;import ludwig
import pandas as pd

model_definition = {
    'input_features':[{
        'name': 'l1',
        'type': 'text',
        'encoder': 'rnn',
        'cell_type': 'lstm',
        'reduce_output': 'None'}],
    'output_features':[{
        'name': 'l2',
        'type': 'text',
        'decoder': 'generator',
        'cell_type': 'lstm',
        'attention': 'bahdanau',
        'loss':{
            'type': 'sampled_softmax_cross_entropy'}}]}

model = ludwig.LudwigModel(model_definition=model_definition)

data_df = pd.DataFrame({
    'l1':['aa ab ac', 'ad ae af'],
    'l2':['za zb zc', 'zd ze zf']})

model_stats = model.train(data_df=data_df)

preds = model.predict(data_df=data_df)
&lt;/denchmark-code&gt;


See error:


KeyError                                  Traceback (most recent call last)
 in 
26 model_stats = model.train(data_df=data_df)
27
---&gt; 28 preds = model.predict(data_df=data_df)
~/.local/lib/python3.5/site-packages/ludwig/api.py in predict(self, data_df, data_csv, data_dict, return_type, batch_size, gpus, gpu_fraction, logging_level)
929             gpus=gpus,
930             gpu_fraction=gpu_fraction,
--&gt; 931             logging_level=logging_level,
932         )
933
~/.local/lib/python3.5/site-packages/ludwig/api.py in _predict(self, data_df, data_csv, data_dict, return_type, batch_size, gpus, gpu_fraction, only_predictions, logging_level)
799             self.model_definition['preprocessing']
800         )
--&gt; 801         replace_text_feature_level(self.model_definition, [preprocessed_data])
802         dataset = Dataset(
803             preprocessed_data,
~/.local/lib/python3.5/site-packages/ludwig/data/preprocessing.py in replace_text_feature_level(model_definition, datasets)
706                         '{}_{}'.format(
707                             feature['name'],
--&gt; 708                             feature['level']
709                         )
710                     ]
KeyError: 'l2_word'

Expected behavior
Predictions are produced
Environment (please complete the following information):

OS: Debian GNU/Linux
Version: 9
Python version: 3.5.3
Ludwig version: 0.1.0

	</description>
	<comments>
		<comment id='1' author='alan-krumholz' date='2019-03-14T17:36:50Z'>
		&lt;denchmark-link:https://github.com/alan-krumholz&gt;@alan-krumholz&lt;/denchmark-link&gt;
 thanks for providing a reproducible example. I was indeed able to reproduce it, will take a closer look at it,
		</comment>
		<comment id='2' author='alan-krumholz' date='2019-03-14T18:47:13Z'>
		Pushed a commit that should solve the issue, &lt;denchmark-link:https://github.com/alan-krumholz&gt;@alan-krumholz&lt;/denchmark-link&gt;
 can you please confirm?
		</comment>
		<comment id='3' author='alan-krumholz' date='2019-03-14T20:50:30Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 the error is not there anymore (for the toy example). Now I'll see if I can make it run with the real thing :)
Thanks!
		</comment>
		<comment id='4' author='alan-krumholz' date='2019-03-14T20:54:12Z'>
		Great! Closing for now, feel free to reopen if other problems arise.
		</comment>
		<comment id='5' author='alan-krumholz' date='2019-03-15T01:34:51Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;

When I print preds from the code above I get:
&lt;denchmark-link:https://user-images.githubusercontent.com/21314745/54401980-c2901180-468f-11e9-8fa8-a5cfa2da1889.png&gt;&lt;/denchmark-link&gt;

I tried the same training with 100, 300, 1,000, and 3,000 real sentences and I always get the same predictions (even when using phrases that were used for training):
&lt;denchmark-link:https://user-images.githubusercontent.com/21314745/54402055-20245e00-4690-11e9-8445-d0fe60931a39.png&gt;&lt;/denchmark-link&gt;

Could something else be wrong? Why would it always learn to predict sequences of UNKNOWNS?
		</comment>
		<comment id='6' author='alan-krumholz' date='2019-03-15T03:54:21Z'>
		That really depends on your data. those amounts or sentences are probably not providing a strong enough signal (usually hundreds of thousands or millions are required to get decent results), plus it really depends on how much predictive power the input has with respect to the output. One thing i would suggest you to do if the number of tokens in your output is small enough is to try to use regular softmax (you may have to decrease the batch size to fir things in memory) and see if it makes a difference, but without knowing about the data and the task, it's just speculation.
		</comment>
		<comment id='7' author='alan-krumholz' date='2019-03-16T15:24:22Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;

Here is another toy exercise/dataset (reverse phrases) that works fine in &lt;denchmark-link:https://google.github.io/seq2seq/nmt/#alternative-generate-toy-data&gt;other seq2seq tutorials&lt;/denchmark-link&gt;
 but gives me the same problem in ludwig that I was describing in my last message:
Code:
&lt;denchmark-code&gt;import ludwig
import pandas as pd
import numpy as np

VOCABULARY = [
    'a','about','all','also','and','as','at','be',
    'because','but','by','can','come','could','day',
    'do','even','find','first','for','from','get','give',
    'go','have','he','her','here','him','his','how','I',
    'if','in','into','it','its','just','know','like','look',
    'make','man','many','me','more','my','new','no','not','now',
    'of','on','one','only','or','other','our','out','people','say',
    'see','she','so','some','take','tell','than','that','the',
    'their','them','then','there','these','they','thing','think',
    'this','those','time','to','two','up','use','very','want','way',
    'we','well','what','when','which','who','will','with','would',
    'year','you','your']

def make_reverse(num_examples, min_len, max_len):
  """
  Generates a dataset where the target is equal to the source reversed.
  Sequence lengths are chosen randomly from [min_len, max_len].
  """
  for _ in range(num_examples):
    turn_length = np.random.choice(np.arange(min_len, max_len + 1))
    source_tokens = np.random.choice(
        list(VOCABULARY), size=turn_length, replace=True)
    target_tokens = source_tokens[::-1]
    yield " ".join(source_tokens), " ".join(target_tokens)

model_definition = {
    'input_features':[{
        'name': 'l1',
        'type': 'text',
        'encoder': 'rnn',
        'cell_type': 'lstm',
        'reduce_output': 'None'}],
    'output_features':[{
        'name': 'l2',
        'type': 'text',
        'decoder': 'generator',
        'cell_type': 'lstm',
        'attention': 'bahdanau',
        'loss':{
            'type': 'sampled_softmax_cross_entropy'}}]}

model = ludwig.LudwigModel(model_definition=model_definition)

data = [x for x in make_reverse(10000, 5, 40)]

data_df = pd.DataFrame({
    'l1':[x[0] for x in data],
    'l2':[x[1] for x in data]})

model_stats = model.train(data_df=data_df)

preds = model.predict(data_df=data_df)
&lt;/denchmark-code&gt;

Results:
&lt;denchmark-link:https://user-images.githubusercontent.com/21314745/54477420-43a0f300-47cd-11e9-933b-22fce6bb2a82.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='alan-krumholz' date='2019-03-19T01:39:48Z'>
		The fix i just pushed it solves the issue, thanks for helping with a perfect reproducible example!
		</comment>
		<comment id='9' author='alan-krumholz' date='2019-03-19T12:33:41Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;

Now I'm getting a new error when I try to run the code above:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
error                                     Traceback (most recent call last)
&lt;ipython-input-1-92b645081c92&gt; in &lt;module&gt;
     53     'l2':[x[1] for x in data]})
     54 
---&gt; 55 model_stats = model.train(data_df=data_df)
     56 
     57 preds = model.predict(data_df=data_df)

~/.local/lib/python3.5/site-packages/ludwig/api.py in train(self, data_df, data_train_df, data_validation_df, data_test_df, data_csv, data_train_csv, data_validation_csv, data_test_csv, data_hdf5, data_train_hdf5, data_validation_hdf5, data_test_hdf5, train_set_metadata_json, dataset_type, model_name, model_load_path, model_resume_path, skip_save_model, skip_save_progress, skip_save_log, skip_save_processed_input, output_directory, gpus, gpu_fraction, random_seed, logging_level, debug, **kwargs)
    462                 preprocessing_params=
    463                 self.model_definition['preprocessing'],
--&gt; 464                 random_seed=random_seed)
    465         else:
    466             (

~/.local/lib/python3.5/site-packages/ludwig/data/preprocessing.py in preprocess_for_training(model_definition, dataset_type, data_df, data_train_df, data_validation_df, data_test_df, data_csv, data_train_csv, data_validation_csv, data_test_csv, data_hdf5, data_train_hdf5, data_validation_hdf5, data_test_hdf5, train_set_metadata_json, skip_save_processed_input, preprocessing_params, random_seed)
    390             features,
    391             preprocessing_params,
--&gt; 392             random_seed=random_seed
    393         )
    394         if not skip_save_processed_input:

~/.local/lib/python3.5/site-packages/ludwig/data/preprocessing.py in build_dataset_df(dataset_df, features, global_preprocessing_parameters, train_set_metadata, random_seed, **kwargs)
     79             dataset_df,
     80             features,
---&gt; 81             global_preprocessing_parameters
     82         )
     83 

~/.local/lib/python3.5/site-packages/ludwig/data/preprocessing.py in build_metadata(dataset_df, features, global_preprocessing_parameters)
    120         train_set_metadata[feature['name']] = get_feature_meta(
    121             dataset_df[feature['name']].astype(str),
--&gt; 122             preprocessing_parameters
    123         )
    124     return train_set_metadata

~/.local/lib/python3.5/site-packages/ludwig/features/text_feature.py in get_feature_meta(column, preprocessing_parameters)
     97             preprocessing_parameters['char_most_common'],
     98             preprocessing_parameters['word_most_common'],
---&gt; 99             preprocessing_parameters['lowercase']
    100         )
    101         (

~/.local/lib/python3.5/site-packages/ludwig/features/text_feature.py in feature_meta(column, most_common_characters, most_common_words, lowercase)
     78             'english_tokenize',
     79             num_most_frequent=most_common_words,
---&gt; 80             lowercase=lowercase
     81         )
     82         return (

~/.local/lib/python3.5/site-packages/ludwig/utils/strings_utils.py in create_vocabulary(data, format, custom_vocabulary, add_unknown, add_padding, lowercase, num_most_frequent)
     89         for line in data:
     90             processed_line = format_function(
---&gt; 91                 line.lower() if lowercase else line)
     92             unit_counts.update(processed_line)
     93             max_line_length = max(max_line_length, len(processed_line))

~/.local/lib/python3.5/site-packages/ludwig/utils/strings_utils.py in english_tokenize(text)
    210 
    211 def english_tokenize(text):
--&gt; 212     return process_text(text, load_nlp_pipeline())
    213 
    214 

~/.local/lib/python3.5/site-packages/ludwig/utils/nlp_utils.py in load_nlp_pipeline()
     40                           'Make sure to download it with: '
     41                           'python -m spacy download en')
---&gt; 42         nlp_pipeline = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])
     43     return nlp_pipeline
     44 

~/.local/lib/python3.5/site-packages/en_core_web_sm/__init__.py in load(**overrides)
     10 
     11 def load(**overrides):
---&gt; 12     return load_model_from_init_py(__file__, **overrides)

~/.local/lib/python3.5/site-packages/spacy/util.py in load_model_from_init_py(init_file, **overrides)
    188     if not model_path.exists():
    189         raise IOError(Errors.E052.format(path=path2str(data_path)))
--&gt; 190     return load_model_from_path(data_path, meta, **overrides)
    191 
    192 

~/.local/lib/python3.5/site-packages/spacy/util.py in load_model_from_path(model_path, meta, **overrides)
    171             component = nlp.create_pipe(name, config=config)
    172             nlp.add_pipe(component, name=name)
--&gt; 173     return nlp.from_disk(model_path)
    174 
    175 

~/.local/lib/python3.5/site-packages/spacy/language.py in from_disk(self, path, exclude, disable)
    784             # Convert to list here in case exclude is (default) tuple
    785             exclude = list(exclude) + ["vocab"]
--&gt; 786         util.from_disk(path, deserializers, exclude)
    787         self._path = path
    788         return self

~/.local/lib/python3.5/site-packages/spacy/util.py in from_disk(path, readers, exclude)
    609         # Split to support file names like meta.json
    610         if key.split(".")[0] not in exclude:
--&gt; 611             reader(path / key)
    612     return path
    613 

~/.local/lib/python3.5/site-packages/spacy/language.py in &lt;lambda&gt;(p)
    774         deserializers["meta.json"] = lambda p: self.meta.update(srsly.read_json(p))
    775         deserializers["vocab"] = lambda p: self.vocab.from_disk(p) and _fix_pretrained_vectors_name(self)
--&gt; 776         deserializers["tokenizer"] = lambda p: self.tokenizer.from_disk(p, exclude=["vocab"])
    777         for name, proc in self.pipeline:
    778             if name in exclude:

tokenizer.pyx in spacy.tokenizer.Tokenizer.from_disk()

tokenizer.pyx in spacy.tokenizer.Tokenizer.from_bytes()

/usr/lib/python3.5/re.py in compile(pattern, flags)
    222 def compile(pattern, flags=0):
    223     "Compile a regular expression pattern, returning a pattern object."
--&gt; 224     return _compile(pattern, flags)
    225 
    226 def purge():

/usr/lib/python3.5/re.py in _compile(pattern, flags)
    291     if not sre_compile.isstring(pattern):
    292         raise TypeError("first argument must be string or compiled pattern")
--&gt; 293     p = sre_compile.compile(pattern, flags)
    294     if not (flags &amp; DEBUG):
    295         if len(_cache) &gt;= _MAXCACHE:

/usr/lib/python3.5/sre_compile.py in compile(p, flags)
    538         pattern = None
    539 
--&gt; 540     code = _code(p, flags)
    541 
    542     # print(code)

/usr/lib/python3.5/sre_compile.py in _code(p, flags)
    523 
    524     # compile the pattern
--&gt; 525     _compile(code, p.data, flags)
    526 
    527     code.append(SUCCESS)

/usr/lib/python3.5/sre_compile.py in _compile(code, pattern, flags)
    183                 skip = _len(code); emit(0)
    184                 # _compile_info(code, av, flags)
--&gt; 185                 _compile(code, av, flags)
    186                 emit(JUMP)
    187                 tailappend(_len(code)); emit(0)

/usr/lib/python3.5/sre_compile.py in _compile(code, pattern, flags)
    156                 lo, hi = av[1].getwidth()
    157                 if lo != hi:
--&gt; 158                     raise error("look-behind requires fixed-width pattern")
    159                 emit(lo) # look behind
    160             _compile(code, av[1], flags)

error: look-behind requires fixed-width pattern
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='alan-krumholz' date='2019-03-19T17:41:48Z'>
		I cannot reproduce it. Did you change the format function to be english_tokenize? Anyway it looks like a problem with your installation of spacy, make sure you run python -m spacy download en and that the version of spacy you have is the supported one.
		</comment>
		<comment id='11' author='alan-krumholz' date='2019-03-20T13:15:14Z'>
		"python3 -m spacy download en"  fixed the issue.
I didn't do this initially I did "pip3 install &lt;denchmark-link:https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz&gt;https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz&lt;/denchmark-link&gt;
"  based on requirements.txt instead
Thanks for your help!
		</comment>
		<comment id='12' author='alan-krumholz' date='2019-03-20T18:16:11Z'>
		That should have worked already... that's weird.
Anyway, glad that solved your issue!
If you play around with those datasets, feel free to do a pull request to add more examples to the documentation, it would be much appreciated.
		</comment>
		<comment id='13' author='alan-krumholz' date='2019-04-09T21:20:47Z'>
		CODE:
import ludwig
import pandas as pd
model_definition = {
'input_features':[{
'name': 'l1',
'type': 'text',
'encoder': 'rnn',
'cell_type': 'lstm',
'reduce_output': 'None',
'level':'word'}],
'output_features':[{
'name': 'l2',
'type': 'text',
'decoder': 'generator',
'cell_type': 'lstm',
'attention': 'bahdanau',
'level':'word'}]}
model = ludwig.LudwigModel(model_definition=model_definition)
data_df = pd.DataFrame({
'l1':['aa ab ac', 'ad ae af'],
'l2':['za zb zc', 'zd ze zf']})
model_stats = model.train(data_df=data_df)
preds = model.predict(data_df=data_df, logging_level='DEBUG')
ISSUE:
Hello,
I am not able to execute the toy example that was mentioned below, here is the same reproducible code as above, I have spacy==2.1.3, can you please look at the code snippet and the error message and help me. Thank you in advance.
&lt;denchmark-h:h2&gt;Error:
DEBUG:root:Preprocessing 2 datapoints&lt;/denchmark-h&gt;

KeyError                                  Traceback (most recent call last)
 in 
26 model_stats = model.train(data_df=data_df)
27
---&gt; 28 preds = model.predict(data_df=data_df, logging_level='DEBUG')
~/anaconda3/lib/python3.7/site-packages/ludwig/api.py in predict(self, data_df, data_csv, data_dict, return_type, batch_size, gpus, gpu_fraction, logging_level)
904             gpus=gpus,
905             gpu_fraction=gpu_fraction,
--&gt; 906             logging_level=logging_level,
907         )
908
~/anaconda3/lib/python3.7/site-packages/ludwig/api.py in _predict(self, data_df, data_csv, data_dict, return_type, batch_size, gpus, gpu_fraction, only_predictions, logging_level)
774             self.model_definition['preprocessing']
775         )
--&gt; 776         replace_text_feature_level(self.model_definition, [preprocessed_data])
777         dataset = Dataset(
778             preprocessed_data,
~/anaconda3/lib/python3.7/site-packages/ludwig/data/preprocessing.py in replace_text_feature_level(model_definition, datasets)
768                     '{}_{}'.format(
769                         feature['name'],
--&gt; 770                         feature['level']
771                     )
772                 ]
KeyError: 'l2_word'
		</comment>
		<comment id='14' author='alan-krumholz' date='2019-04-09T22:00:41Z'>
		Can't reproduce the error, works fine for me, make sure you have the latest version of ludwig installed
		</comment>
		<comment id='15' author='alan-krumholz' date='2019-04-10T18:50:59Z'>
		Thank you! &lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 I was able to resolve the above issue after upgrading Ludwig to 0.1.1
		</comment>
	</comments>
</bug>