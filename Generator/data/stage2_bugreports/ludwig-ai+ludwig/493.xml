<bug id='493' author='BeWe11' open_date='2019-08-14T17:47:52Z' closed_time='2019-08-26T04:03:20Z'>
	<summary>Example for BERT encoder uses wrong type and crashes</summary>
	<description>
The documentation for the BERT encoder gives this example of how to set up an encoder using BERT:
&lt;denchmark-code&gt;name: sequence_csv_column_name
type: sequence
encoder: bert
config_path: &lt;path_to_bert_config.json&gt;
checkpoint_path: &lt;path_to_bert_model.ckpt&gt;
do_lower_case: True
preprocessing:
    tokenizer: bert
    vocab_file: &lt;path_to_bert_vocab.txt&gt;
    padding_symbol: '[PAD]'
    unknown_symbol: '[UNK]'
reduce_output: True
&lt;/denchmark-code&gt;

However, by setting , training crashes because sequence encoders do not pass the  parameter when creating a vocabulary:
&lt;denchmark-link:https://github.com/uber/ludwig/blob/71ca2189bcee7a2667c428aeb1bf738697cbe83d/ludwig/features/sequence_feature.py#L76-L80&gt;https://github.com/uber/ludwig/blob/71ca2189bcee7a2667c428aeb1bf738697cbe83d/ludwig/features/sequence_feature.py#L76-L80&lt;/denchmark-link&gt;

Compare this to the creation of vocabularies for text encoders:
&lt;denchmark-link:https://github.com/uber/ludwig/blob/71ca2189bcee7a2667c428aeb1bf738697cbe83d/ludwig/features/text_feature.py#L81-L89&gt;https://github.com/uber/ludwig/blob/71ca2189bcee7a2667c428aeb1bf738697cbe83d/ludwig/features/text_feature.py#L81-L89&lt;/denchmark-link&gt;

So, either the example should be updated to use type: text, or the sequence encoder class should be updated to pass the vocab_file parameter to the vocabulary creation.
	</description>
	<comments>
		<comment id='1' author='BeWe11' date='2019-08-14T19:06:46Z'>
		Will look into it. The fact that the sequence feature doesn't pass the vocabulary file is a regression most likely.
		</comment>
		<comment id='2' author='BeWe11' date='2019-08-18T20:26:57Z'>
		Is this on master or when using the package?
I don't see any issues with using the following config:
python -m ludwig.experiment --data_csv test_dataset.csv --model_definition "{input_features: [{name: sequence_1, type: sequence, encoder: bert, config_path: /Users/dudin/Downloads/uncased_L-12_H-768_A-12/bert_config.json, checkpoint_path: /Users/dudin/Downloads/uncased_L-12_H-768_A-12/bert_model.ckpt}], output_features: [{name: category_1, type: category}], training: {epochs: 3, learning_rate: 0.001}, preprocessing: {tokenizer: bert, vocab_file: /Users/dudin/Downloads/uncased_L-12_H-768_A-12/vocab.txt}}"
		</comment>
		<comment id='3' author='BeWe11' date='2019-08-19T13:44:26Z'>
		I've used the pip package &lt;denchmark-link:https://github.com/ydudin3&gt;@ydudin3&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='BeWe11' date='2019-08-19T16:41:17Z'>
		Would you mind giving it a shot using current master, I believe it should work?
		</comment>
		<comment id='5' author='BeWe11' date='2019-08-26T04:04:08Z'>
		&lt;denchmark-link:https://github.com/BeWe11&gt;@BeWe11&lt;/denchmark-link&gt;
 I pushed a commit that should have solved the issue, can you please confirm?
		</comment>
		<comment id='6' author='BeWe11' date='2019-09-03T09:40:47Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 Sorry for the delay, I can confirm that the issue has been resolved.
		</comment>
	</comments>
</bug>