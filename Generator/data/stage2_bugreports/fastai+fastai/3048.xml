<bug id='3048' author='marii-moe' open_date='2020-12-02T09:30:20Z' closed_time='2020-12-02T14:35:20Z'>
	<summary>Gradient Accumulation + Mixed Precision shows artificially high training loss</summary>
	<description>
Example of issue on forum: &lt;denchmark-link:https://forums.fast.ai/t/what-is-the-v2-equivalent-of-accumulatescheduler/66409/13&gt;https://forums.fast.ai/t/what-is-the-v2-equivalent-of-accumulatescheduler/66409/13&lt;/denchmark-link&gt;

Please confirm you have the latest versions of fastai, fastcore, fastscript, and nbdev prior to reporting a bug (delete one): YES
Describe the bug
The bug occurs when Gradient Accumulation and the MixedPrecision Callback are both used. Gradient Accumulation run before Mixed Precision and causes the after_backwards to not be run, meaning that the loss is not unscaled before it is logged. This means that very large losses such as 6000000+ to be logged.
To Reproduce
Steps to reproduce the behavior:
Run this code:
&lt;denchmark-code&gt;seed=random.randint(0,2**32-1)

with no_random(seed): 
    db=synth_dbunch(bs=8,n_train=1,n_valid=1,cuda=True)
    learn = synth_learner(data=db)
    learn.fit(1, lr=0.01)
#start without gradient overflow
max_loss_scale=2048.0
with no_random(seed): 
    db=synth_dbunch(bs=1,n_train=8,n_valid=8,cuda=True)
    learn = synth_learner(data=db,cbs=[GradientAccumulation(n_acc=8)])
    learn.to_fp16(max_loss_scale=max_loss_scale)
    learn.fit(1, lr=0.01)
&lt;/denchmark-code&gt;

The training loss will be very high, 5000+ for fp16. fp32 will be reasonable.
Expected behavior
Similar training loss between fp32 and fp16 versions. &lt;2 difference in loss.
Additional context
I have already gotten a fix, bill be submitting it in pull request soon.
	</description>
	<comments>
	</comments>
</bug>