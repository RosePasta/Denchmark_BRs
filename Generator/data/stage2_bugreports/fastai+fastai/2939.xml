<bug id='2939' author='michherren' open_date='2020-11-04T00:38:39Z' closed_time='2020-11-04T22:16:46Z'>
	<summary>unet_learner segmentation fails</summary>
	<description>
Please confirm you have the latest versions of fastai, fastcore, fastscript, and nbdev prior to reporting a bug (delete one): YES

The camvid notebook unet-segmentation fails (&lt;denchmark-link:https://colab.research.google.com/github/fastai/fastai/blob/master/nbs/23_tutorial.vision.ipynb&gt;https://colab.research.google.com/github/fastai/fastai/blob/master/nbs/23_tutorial.vision.ipynb&lt;/denchmark-link&gt;
)
To Reproduce
Steps to reproduce the behavior:

Open https://colab.research.google.com/github/fastai/fastai/blob/master/nbs/23_tutorial.vision.ipynb
Run the "Segmentation"-Block
Fails on learn.fine_tune(8) =&gt; TypeError: new() missing 1 required positional argument: 'x'

Expected behavior
A trained unet
Error with full stack trace
&lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
&lt;ipython-input-13-f067e6d6d342&gt; in &lt;module&gt;()
      1 learn = unet_learner(dls, resnet34)
----&gt; 2 learn.fine_tune(8)

31 frames
/usr/local/lib/python3.6/dist-packages/fastcore/logargs.py in _f(*args, **kwargs)
     54         init_args.update(log)
     55         setattr(inst, 'init_args', init_args)
---&gt; 56         return inst if to_return else f(*args, **kwargs)
     57     return _f

/usr/local/lib/python3.6/dist-packages/fastai/callback/schedule.py in fine_tune(self, epochs, base_lr, freeze_epochs, lr_mult, pct_start, div, **kwargs)
    159     "Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR"
    160     self.freeze()
--&gt; 161     self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)
    162     base_lr /= 2
    163     self.unfreeze()

/usr/local/lib/python3.6/dist-packages/fastcore/logargs.py in _f(*args, **kwargs)
     54         init_args.update(log)
     55         setattr(inst, 'init_args', init_args)
---&gt; 56         return inst if to_return else f(*args, **kwargs)
     57     return _f

/usr/local/lib/python3.6/dist-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)
    111     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),
    112               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}
--&gt; 113     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)
    114 
    115 # Cell

/usr/local/lib/python3.6/dist-packages/fastcore/logargs.py in _f(*args, **kwargs)
     54         init_args.update(log)
     55         setattr(inst, 'init_args', init_args)
---&gt; 56         return inst if to_return else f(*args, **kwargs)
     57     return _f

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)
    205             self.opt.set_hypers(lr=self.lr if lr is None else lr)
    206             self.n_epoch = n_epoch
--&gt; 207             self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
    208 
    209     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_fit(self)
    195         for epoch in range(self.n_epoch):
    196             self.epoch=epoch
--&gt; 197             self._with_events(self._do_epoch, 'epoch', CancelEpochException)
    198 
    199     @log_args(but='cbs')

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_epoch(self)
    189 
    190     def _do_epoch(self):
--&gt; 191         self._do_epoch_train()
    192         self._do_epoch_validate()
    193 

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_epoch_train(self)
    181     def _do_epoch_train(self):
    182         self.dl = self.dls.train
--&gt; 183         self._with_events(self.all_batches, 'train', CancelTrainException)
    184 
    185     def _do_epoch_validate(self, ds_idx=1, dl=None):

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in all_batches(self)
    159     def all_batches(self):
    160         self.n_iter = len(self.dl)
--&gt; 161         for o in enumerate(self.dl): self.one_batch(*o)
    162 
    163     def _do_one_batch(self):

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in one_batch(self, i, b)
    177         self.iter = i
    178         self._split(b)
--&gt; 179         self._with_events(self._do_one_batch, 'batch', CancelBatchException)
    180 
    181     def _do_epoch_train(self):

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_one_batch(self)
    162 
    163     def _do_one_batch(self):
--&gt; 164         self.pred = self.model(*self.xb)
    165         self('after_pred')
    166         if len(self.yb): self.loss = self.loss_func(self.pred, *self.yb)

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

/usr/local/lib/python3.6/dist-packages/fastai/layers.py in forward(self, x)
    395         for l in self.layers:
    396             res.orig = x
--&gt; 397             nres = l(res)
    398             # We have to remove res.orig to avoid hanging refs and therefore memory leaks
    399             res.orig = None

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py in forward(self, input)
    115     def forward(self, input):
    116         for module in self:
--&gt; 117             input = module(input)
    118         return input
    119 

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)
    421 
    422     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 423         return self._conv_forward(input, self.weight)
    424 
    425 class Conv3d(_ConvNd):

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)
    418                             _pair(0), self.dilation, self.groups)
    419         return F.conv2d(input, weight, self.bias, self.stride,
--&gt; 420                         self.padding, self.dilation, self.groups)
    421 
    422     def forward(self, input: Tensor) -&gt; Tensor:

/usr/local/lib/python3.6/dist-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs)
    315     def __torch_function__(self, func, types, args=(), kwargs=None):
    316         with torch._C.DisableTorchFunction(): ret = _convert(func(*args, **(kwargs or {})), self.__class__)
--&gt; 317         if isinstance(ret, TensorBase): ret.set_meta(self, as_copy=True)
    318         return ret
    319 

/usr/local/lib/python3.6/dist-packages/fastai/torch_core.py in set_meta(self, x, as_copy)
    278     "Set all metadata in `__dict__`"
    279     if not hasattr(x,'__dict__'): return
--&gt; 280     self.__dict__ = deepcopy(x.__dict__) if as_copy else x.__dict__
    281 
    282 # Cell

/usr/lib/python3.6/copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--&gt; 150         y = copier(x, memo)
    151     else:
    152         try:

/usr/lib/python3.6/copy.py in _deepcopy_dict(x, memo, deepcopy)
    238     memo[id(x)] = y
    239     for key, value in x.items():
--&gt; 240         y[deepcopy(key, memo)] = deepcopy(value, memo)
    241     return y
    242 d[dict] = _deepcopy_dict

/usr/lib/python3.6/copy.py in deepcopy(x, memo, _nil)
    159             copier = getattr(x, "__deepcopy__", None)
    160             if copier:
--&gt; 161                 y = copier(memo)
    162             else:
    163                 reductor = dispatch_table.get(cls)

/usr/local/lib/python3.6/dist-packages/torch/tensor.py in __deepcopy__(self, memo)
     43         relevant_args = (self,)
     44         if type(self) is not Tensor and has_torch_function(relevant_args):
---&gt; 45             return handle_torch_function(Tensor.__deepcopy__, relevant_args, self, memo)
     46         if not self.is_leaf:
     47             raise RuntimeError("Only Tensors created explicitly by the user "

/usr/local/lib/python3.6/dist-packages/torch/overrides.py in handle_torch_function(public_api, relevant_args, *args, **kwargs)
   1061         # Use `public_api` instead of `implementation` so __torch_function__
   1062         # implementations can do equality/identity comparisons.
-&gt; 1063         result = overloaded_arg.__torch_function__(public_api, types, args, kwargs)
   1064 
   1065         if result is not NotImplemented:

/usr/local/lib/python3.6/dist-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs)
    314 
    315     def __torch_function__(self, func, types, args=(), kwargs=None):
--&gt; 316         with torch._C.DisableTorchFunction(): ret = _convert(func(*args, **(kwargs or {})), self.__class__)
    317         if isinstance(ret, TensorBase): ret.set_meta(self, as_copy=True)
    318         return ret

/usr/local/lib/python3.6/dist-packages/torch/tensor.py in __deepcopy__(self, memo)
     75                         self._backward_hooks)
     76                 else:
---&gt; 77                     new_tensor = self.new()
     78                     new_tensor.set_(new_storage, self.storage_offset(), self.size(), self.stride())
     79                     new_tensor.requires_grad = self.requires_grad

TypeError: new() missing 1 required positional argument: 'x'
&lt;/denchmark-code&gt;


This issue is already mentioned here:
&lt;denchmark-link:https://github.com/fastai/fastai/issues/2919#issuecomment-721185065&gt;#2919 (comment)&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='michherren' date='2020-11-04T01:28:40Z'>
		&lt;denchmark-link:https://github.com/jph00&gt;@jph00&lt;/denchmark-link&gt;
 I can reproduce this issue. It looks to stem from the ConvLayer inside of SequentialEx
(minimal reproducer):
path = untar_data(URLs.CAMVID_TINY)
codes = np.loadtxt(path/'codes.txt', dtype=str)
fnames = get_image_files(path/"images")
def label_func(fn): return path/"labels"/f"{fn.stem}_P{fn.suffix}"
dls = SegmentationDataLoaders.from_label_func(
    path, bs=8, fnames = fnames, label_func = label_func, codes = codes
)
learn = unet_learner(dls, resnet34)
learn.fine_tune(8)
		</comment>
		<comment id='2' author='michherren' date='2020-11-04T15:44:51Z'>
		The exact same issue occurs for me.
fastai=2.1.3
fastcore=1.3.1
		</comment>
		<comment id='3' author='michherren' date='2020-11-04T22:10:09Z'>
		Many thanks for the report. This is due to a problem in PyTorch 1.7 that's already fixed in their master. I'm adding a workaround now.
		</comment>
		<comment id='4' author='michherren' date='2020-11-04T22:16:46Z'>
		Closed by &lt;denchmark-link:https://github.com/fastai/fastai/commit/92df0eb216384e831240a0f83aa50c07b67fb6f7&gt;92df0eb&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>