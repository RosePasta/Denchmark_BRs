<bug id='3027' author='MariiaS' open_date='2020-11-23T14:44:28Z' closed_time='2020-11-23T17:31:07Z'>
	<summary>unsupported operand type(s) for +=: 'TensorCategory' and 'TensorText' when using AWD_LSTM for text classification</summary>
	<description>
Please confirm you have the latest versions of fastai, fastcore, fastscript, and nbdev prior to reporting a bug (delete one): YES / NO

I'm following the 01_intro.ipynb from fastai course on the Google Colab environment, copy of the official fastai notebook: &lt;denchmark-link:https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb&gt;https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb&lt;/denchmark-link&gt;
. I don't make any changes to the code, just executing the code line by line.
On the line with llearn.fine_tune(4, 1e-2) for the block of code
&lt;denchmark-code&gt;dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')
learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)
learn.fine_tune(4, 1e-2)
&lt;/denchmark-code&gt;

the error unsupported operand type(s) for +=: 'TensorCategory' and 'TensorText' appears.
To Reproduce
Steps to reproduce the behavior:

Copy https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb in Google Colab
Execute the notebook

Expected behavior
Expected that the learner for text classification is working as expected, no error appears.
Error with full stack trace
&lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
&lt;ipython-input-9-5ab79cd5e866&gt; in &lt;module&gt;()
      3 dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')
      4 learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)
----&gt; 5 learn.fine_tune(4, 1e-2)

20 frames
/usr/local/lib/python3.6/dist-packages/fastai/callback/schedule.py in fine_tune(self, epochs, base_lr, freeze_epochs, lr_mult, pct_start, div, **kwargs)
    155     "Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR"
    156     self.freeze()
--&gt; 157     self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)
    158     base_lr /= 2
    159     self.unfreeze()

/usr/local/lib/python3.6/dist-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)
    110     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),
    111               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}
--&gt; 112     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)
    113 
    114 # Cell

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)
    203             self.opt.set_hypers(lr=self.lr if lr is None else lr)
    204             self.n_epoch = n_epoch
--&gt; 205             self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
    206 
    207     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    152 
    153     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 154         try:       self(f'before_{event_type}')       ;f()
    155         except ex: self(f'after_cancel_{event_type}')
    156         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_fit(self)
    194         for epoch in range(self.n_epoch):
    195             self.epoch=epoch
--&gt; 196             self._with_events(self._do_epoch, 'epoch', CancelEpochException)
    197 
    198     def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    152 
    153     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 154         try:       self(f'before_{event_type}')       ;f()
    155         except ex: self(f'after_cancel_{event_type}')
    156         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_epoch(self)
    188 
    189     def _do_epoch(self):
--&gt; 190         self._do_epoch_train()
    191         self._do_epoch_validate()
    192 

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_epoch_train(self)
    180     def _do_epoch_train(self):
    181         self.dl = self.dls.train
--&gt; 182         self._with_events(self.all_batches, 'train', CancelTrainException)
    183 
    184     def _do_epoch_validate(self, ds_idx=1, dl=None):

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    152 
    153     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 154         try:       self(f'before_{event_type}')       ;f()
    155         except ex: self(f'after_cancel_{event_type}')
    156         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in all_batches(self)
    158     def all_batches(self):
    159         self.n_iter = len(self.dl)
--&gt; 160         for o in enumerate(self.dl): self.one_batch(*o)
    161 
    162     def _do_one_batch(self):

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in one_batch(self, i, b)
    176         self.iter = i
    177         self._split(b)
--&gt; 178         self._with_events(self._do_one_batch, 'batch', CancelBatchException)
    179 
    180     def _do_epoch_train(self):

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    152 
    153     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 154         try:       self(f'before_{event_type}')       ;f()
    155         except ex: self(f'after_cancel_{event_type}')
    156         finally:   self(f'after_{event_type}')        ;final()

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _do_one_batch(self)
    164         self('after_pred')
    165         if len(self.yb): self.loss = self.loss_func(self.pred, *self.yb)
--&gt; 166         self('after_loss')
    167         if not self.training or not len(self.yb): return
    168         self('before_backward')

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in __call__(self, event_name)
    130     def ordered_cbs(self, event): return [cb for cb in sort_by_run(self.cbs) if hasattr(cb, event)]
    131 
--&gt; 132     def __call__(self, event_name): L(event_name).map(self._call_one)
    133 
    134     def _call_one(self, event_name):

/usr/local/lib/python3.6/dist-packages/fastcore/foundation.py in map(self, f, gen, *args, **kwargs)
    177     def range(cls, a, b=None, step=None): return cls(range_of(a, b=b, step=step))
    178 
--&gt; 179     def map(self, f, *args, gen=False, **kwargs): return self._new(map_ex(self, f, *args, gen=gen, **kwargs))
    180     def argwhere(self, f, negate=False, **kwargs): return self._new(argwhere(self, f, negate, **kwargs))
    181     def filter(self, f=noop, negate=False, gen=False, **kwargs):

/usr/local/lib/python3.6/dist-packages/fastcore/basics.py in map_ex(iterable, f, gen, *args, **kwargs)
    604     res = map(g, iterable)
    605     if gen: return res
--&gt; 606     return list(res)
    607 
    608 # Cell

/usr/local/lib/python3.6/dist-packages/fastcore/basics.py in __call__(self, *args, **kwargs)
    594             if isinstance(v,_Arg): kwargs[k] = args.pop(v.i)
    595         fargs = [args[x.i] if isinstance(x, _Arg) else x for x in self.pargs] + args[self.maxi+1:]
--&gt; 596         return self.func(*fargs, **kwargs)
    597 
    598 # Cell

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in _call_one(self, event_name)
    134     def _call_one(self, event_name):
    135         assert hasattr(event, event_name), event_name
--&gt; 136         [cb(event_name) for cb in sort_by_run(self.cbs)]
    137 
    138     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)

/usr/local/lib/python3.6/dist-packages/fastai/learner.py in &lt;listcomp&gt;(.0)
    134     def _call_one(self, event_name):
    135         assert hasattr(event, event_name), event_name
--&gt; 136         [cb(event_name) for cb in sort_by_run(self.cbs)]
    137 
    138     def _bn_bias_state(self, with_bias): return norm_bias_params(self.model, with_bias).map(self.opt.state)

/usr/local/lib/python3.6/dist-packages/fastai/callback/core.py in __call__(self, event_name)
     42                (self.run_valid and not getattr(self, 'training', False)))
     43         res = None
---&gt; 44         if self.run and _run: res = getattr(self, event_name, noop)()
     45         if event_name=='after_fit': self.run=True #Reset self.run to True at each end of fit
     46         return res

/usr/local/lib/python3.6/dist-packages/fastai/callback/rnn.py in after_loss(self)
     31     def after_loss(self):
     32         if not self.training: return
---&gt; 33         if self.alpha != 0.:  self.learn.loss += self.alpha * self.out.float().pow(2).mean()
     34         if self.beta != 0.:
     35             h = self.raw_out

TypeError: unsupported operand type(s) for +=: 'TensorCategory' and 'TensorText'
&lt;/denchmark-code&gt;


I saw the same problem faced by others on the forum, but there was no solution. The error appeared just recently, as just a week ago I was able to execute the same notebook. &lt;denchmark-link:https://forums.fast.ai/t/tensorcategory-and-tensortext-when-using-awd-lstm-for-text-classification/82428/4&gt;https://forums.fast.ai/t/tensorcategory-and-tensortext-when-using-awd-lstm-for-text-classification/82428/4&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='MariiaS' date='2020-11-23T16:47:16Z'>
		Looks to be related to &lt;denchmark-link:https://github.com/fastai/fastai/issues/3022&gt;#3022&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='MariiaS' date='2020-11-23T17:17:31Z'>
		I have an additional example of this bug at the top of my post on the forums: &lt;denchmark-link:https://forums.fast.ai/t/tensorcategory-and-tensortext-when-using-awd-lstm-for-text-classification/82428&gt;https://forums.fast.ai/t/tensorcategory-and-tensortext-when-using-awd-lstm-for-text-classification/82428&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>