<bug id='3066' author='dreamflasher' open_date='2020-12-09T16:15:55Z' closed_time='2021-01-07T19:53:55Z'>
	<summary>wandb integration failing with latest wandb library</summary>
	<description>
Describe the bug
wandb introduced a breaking change with their library, when calling wandb.log() – previously this would not increase the step, but now it does. Currently the integration increases this counter on the first call, and consecutive calls fail because the provided step (not increased) does not match with the increased step counter.
To Reproduce
Use the WandbCallback
Error with full stack trace
Place between these lines with triple backticks:
&lt;denchmark-code&gt;wandb: WARNING Step must only increase in log calls.  Step 6422 &lt; 6423; dropping {'epoch': 20}.
&lt;/denchmark-code&gt;

Additional context
I recommend to remove the internal step counter and let wandb take care of that. I'll propose a PR asap.
	</description>
	<comments>
		<comment id='1' author='dreamflasher' date='2020-12-10T18:45:51Z'>
		Hi &lt;denchmark-link:https://github.com/dreamflasher&gt;@dreamflasher&lt;/denchmark-link&gt;
 ,
Do you have any reproducible code you could share so that we can see the issue?
Providing the step explicitly ensures we associate validation and training data to the correct step.
With the proposed PR, wandb.step would increase after the end of each training batch. It means that validation metrics would be associated with the next step.
If you log custom data, the recommended way is to create wandb_process with the new @typedispatch as shown in the notebook (maybe we can better document it).
Otherwise you would just use commit=False in your own calls to wandb.log.
Let me know if it works for you.
		</comment>
		<comment id='2' author='dreamflasher' date='2020-12-10T20:45:09Z'>
		Thanks a lot! This is happening in a bigger integrated project, I'll try to distill a simpler piece of code for reproduction. In my code I always call wandb.log with commit=False.
		</comment>
		<comment id='3' author='dreamflasher' date='2020-12-17T12:31:08Z'>
		&lt;denchmark-link:https://github.com/borisdayma&gt;@borisdayma&lt;/denchmark-link&gt;
 I wasn't able to create a concise example, but I believe I found the root cause – actually what I already wrote in the initial message – that you changed the wandb.log api (always increase), but at four places in the code it shouldn't be increased.
Here's the PR: &lt;denchmark-link:https://github.com/fastai/fastai/pull/3080&gt;#3080&lt;/denchmark-link&gt;

Does this make sense to you and is this the expected behaviour?
On my end this fixes the error.
		</comment>
		<comment id='4' author='dreamflasher' date='2020-12-17T17:05:20Z'>
		I'm still surprised it would solve your problem:

commit=True submits metrics as well as previously uncommitted metrics (for example when using commit=False or step), then increments the wandb step. When False it just adds the metrics to dict of uncommitted values for future commit.
step gives direct access to wandb step and prevents from logging at a previous step. When logging at a future step, it will commit all previous steps, and not commit only the current step.

I'm thinking that there is a mismatch between the logging from the callback and your custom logging.
You could:

Option 1 (recommended unless impossible): log data mainly using wandb_process and let the callback handle the proper step, taking advantage of type_dispatch
Option 2: use commit=False and don't provide any step in your custom logging, this will automatically match the last step logged by the callback
Option 3: if you have to provide a step (maybe you log something before the callback), you can access it through wandb.run.step. You need to be careful here because WandbCallback keeps its own internal counter self._wandb_step which it increases at after_batch, right before logging.

		</comment>
		<comment id='5' author='dreamflasher' date='2020-12-17T17:18:06Z'>
		Thanks a lot for your feedback. I removed all custom logging and still getting the error described above. So the root cause is very likely the WandbCallback (I ran all our tests and with the proposed changes it's working). Did you  have a look at the PR? You increase the step count twice at the end of the epoch, is that intentional? At the end of an epoch first after_batch and then after_epoch are being called. You don't increase the callback counter, but you do increase (implicitly) the wandb counter by calling wandb.log without commit, which defaults to True, and thus increases the count, right?
It looks to me like the auto increment with wandb.log() is detrimental, because now it's not possible anymore to submit but not to increase the count?
		</comment>
		<comment id='6' author='dreamflasher' date='2020-12-17T17:37:21Z'>
		I did have a look at the PR. I also performed a few small tests to check commit and step worked as expected.
Basically you're not supposed to provide both step and commit.
When step is provided, typically commit is at its default which is None.
Setting it to False will actually call the exact same functions.
commit is forced to True only when step is None (also default value)
That is why I don't understand why it would change the behavior in your code.
Relevant code section is &lt;denchmark-link:https://github.com/wandb/client/blob/52f31e00a83e887d94cd46a08b2fe2652670f5c9/wandb/sdk/wandb_run.py#L794-L813&gt;here&lt;/denchmark-link&gt;
.
after_batch increases the step to get it ready for current logging (it is initially set at -1).
I'm not sure I understand where we increase the counter at after_epoch as the step is manually given.
Here are other interesting links:

my test checking internal behavior of wandb depending on commit: colab
tests in fastai library, 2 training loops are performed and there is no output warning: see end of notebook

Is the issue happening at your first fit loop or are you having additional training loops within a same run?
For debugging, you could try to print wandb.run.step and WandbCallback._wandb_step.
		</comment>
		<comment id='7' author='dreamflasher' date='2020-12-17T17:53:42Z'>
		It happens after the first epoch, no previous runs before, only one init call:
&lt;denchmark-code&gt;epoch     train_loss  valid_loss  accuracy  brier_score  ece       nll       err  time
0         7.291822    6.423790    0.891155  0.022018     0.056384  6.423790  0.122854           05:20     wandb: WARNING Step must only increase in log calls.  Step 232 &lt; 233; dropping {'epoch': 1}.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='dreamflasher' date='2020-12-17T17:59:15Z'>
		I could take a closer look if you can share any code.
Can you share your W&amp;B run? When you look at each of the metrics logged on your panel, is there any that is not logged through fastai callback?
Otherwise for debugging, you could try to print wandb.run.step and WandbCallback._wandb_step.
		</comment>
		<comment id='9' author='dreamflasher' date='2020-12-18T11:45:15Z'>
		Sometimes it happens after the first epoch, no previous runs before, only one init call:
&lt;denchmark-code&gt;epoch     train_loss  valid_loss  accuracy  brier_score  ece       nll       err  time
0         7.291822    6.423790    0.891155  0.022018     0.056384  6.423790  0.122854           05:20     wandb: WARNING Step must only increase in log calls.  Step 232 &lt; 233; dropping {'epoch': 1}.
&lt;/denchmark-code&gt;

I restored the original fastai code, removed all our custom wandb.log calls, and now the problem occurs after the first run.
The 5th epoch finishes and then I get wandb: WARNING Step must only increase in log calls.  Step 1164 &lt; 1165; dropping {'epoch': 5}..
Also in this run we have multiple consecutive wandb.init calls – but it already happens in the first run (I can turn the multiple runs off, if that helps debugging).
Thank you so much for your continued help! Yes, the wandb run is: imachines/mitl-regression-tests/90xkcy38.
The code should be uploaded by default right?
		</comment>
		<comment id='10' author='dreamflasher' date='2020-12-20T02:06:30Z'>
		I would guess it happens when logging your validation data. If you have a call to wandb.log, make sure to use commit=False.
Feel free to share your code privately if you want me to take a closer look.
It's not always uploaded depending on your settings.
		</comment>
		<comment id='11' author='dreamflasher' date='2020-12-20T03:23:39Z'>
		I don't have a single wandb.log call in there as previously mentioned.
		</comment>
		<comment id='12' author='dreamflasher' date='2021-01-04T19:14:43Z'>
		&lt;denchmark-link:https://github.com/jph00&gt;@jph00&lt;/denchmark-link&gt;
 Please reopen
&lt;denchmark-link:https://github.com/borisdayma&gt;@borisdayma&lt;/denchmark-link&gt;
 Here's a reproducible example of what's failing:
&lt;denchmark-code&gt;import wandb
from fastai.callback.wandb import WandbCallback
from fastai.vision.all import *


def label_func(f):
    return f[0].isupper()


path = untar_data(URLs.PETS)
files = get_image_files(path / "images")
dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(224))
wandb.init()
learn = cnn_learner(dls, resnet34, metrics=error_rate, cbs=[WandbCallback(log_model=False)])

learn.fit_one_cycle(3, slice(0.001, 0.03))

learn.validate()
&lt;/denchmark-code&gt;

The problem is with learn.validate() – took me a while to extract this.
		</comment>
		<comment id='13' author='dreamflasher' date='2021-01-04T20:48:53Z'>
		Thanks a lot for this &lt;denchmark-link:https://github.com/dreamflasher&gt;@dreamflasher&lt;/denchmark-link&gt;

I actually never used this function.
I'll need to take a look to understand the difference between validate and validation from the fit loop.
I imagine you would expect to log validation metrics, though it's tricky to know what should be done here as you could have the same metrics already associated to the last step from your fit loop.
		</comment>
		<comment id='14' author='dreamflasher' date='2021-01-05T13:43:30Z'>
		We use learn.validate() for two reasons

To get the results of the best run and log this to wandb (wandb shows the last result, but as we are keeping the best model for future use, we want to see the best numbers in wandb (according to validation metric)).
We do multiple training runs and want to average metrics across runs (monte-carlo cross-validation), and at the end report those average metrics (wandb doesn't support two levels of grouping so far, we need one for MCCV runs and the other for multiple active learning runs)

In the example above we don't do any logging so far, and the error already occurs. But yes, likely the next issue will be to do the logging without screwing up wandb. But let's start with fixing learn.validate() – it looks to me that during this no logging to wandb should be the default?
Currently
&lt;denchmark-code&gt;def after_batch(self):
        "Log hyper-parameters and training loss"
        if self.training:
&lt;/denchmark-code&gt;

Maybe do the if self.training: guard also in def after_epoch(self):?
		</comment>
		<comment id='15' author='dreamflasher' date='2021-01-05T22:09:15Z'>
		
Maybe do the if self.training: guard also in def after_epoch(self):?

I don't think it would work because we want to log validation metrics there, for which self.training=False.
I believe the solution may be to remove &lt;denchmark-link:https://github.com/fastai/fastai/blob/870409f78df41e04a2be378197e41f8c2e1ab371/fastai/callback/wandb.py#L123&gt;this line&lt;/denchmark-link&gt;
.
Its purpose is to force commit the last data at the end of training (happens automatically in a script but not necessarily in notebooks), which also auto-increment wandb internal step.
I think we would want to log the validation data (though it may overwrite previous validation data logged for this step, which should be the same unless the data changed).
		</comment>
		<comment id='16' author='dreamflasher' date='2021-01-06T21:43:24Z'>
		It looks like this solves the issue on our end, did you run it through your test suite? Shall I create a PR?
		</comment>
		<comment id='17' author='dreamflasher' date='2021-01-07T13:56:26Z'>
		Just pulled the last fastai version and now I am getting this error with the above example:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "wandbtest.py", line 16, in &lt;module&gt;
    learn.fit_one_cycle(3, slice(0.001, 0.03))
  File "/home/ubuntu/git/fastai/fastai/callback/schedule.py", line 112, in fit_one_cycle
    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 211, in fit
    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 160, in _with_events
    try: self(f'before_{event_type}');  f()
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 202, in _do_fit
    self._with_events(self._do_epoch, 'epoch', CancelEpochException)
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 160, in _with_events
    try: self(f'before_{event_type}');  f()
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 196, in _do_epoch
    self._do_epoch_train()
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 188, in _do_epoch_train
    self._with_events(self.all_batches, 'train', CancelTrainException)
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 160, in _with_events
    try: self(f'before_{event_type}');  f()
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 166, in all_batches
    for o in enumerate(self.dl): self.one_batch(*o)
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 184, in one_batch
    self._with_events(self._do_one_batch, 'batch', CancelBatchException)
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 162, in _with_events
    self(f'after_{event_type}');  final()
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 141, in __call__
    def __call__(self, event_name): L(event_name).map(self._call_one)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/foundation.py", line 154, in map
    def map(self, f, *args, gen=False, **kwargs): return self._new(map_ex(self, f, *args, gen=gen, **kwargs))
  File "/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py", line 641, in map_ex
    return list(res)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py", line 631, in __call__
    return self.func(*fargs, **kwargs)
  File "/home/ubuntu/git/fastai/fastai/learner.py", line 145, in _call_one
    for cb in self.cbs.sorted('order'): cb(event_name)
  File "/home/ubuntu/git/fastai/fastai/callback/core.py", line 44, in __call__
    if self.run and _run: res = getattr(self, event_name, noop)()
  File "/home/ubuntu/git/fastai/fastai/callback/wandb.py", line 92, in after_batch
    wandb.log({'epoch': self._wandb_epoch, 'train_loss': to_detach(self.smooth_loss.clone()), 'raw_loss': to_detach(self.loss.clone()), **hypers}, step=s
elf._wandb_step)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py", line 378, in __getattr__
    if attr is not None: return getattr(attr,k)
  File "/home/ubuntu/.local/lib/python3.8/site-packages/fastcore/basics.py", line 378, in __getattr__
    if attr is not None: return getattr(attr,k)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 778, in __getattr__
    raise ModuleAttributeError("'{}' object has no attribute '{}'".format(
torch.nn.modules.module.ModuleAttributeError: 'Sequential' object has no attribute 'smooth_loss'
&lt;/denchmark-code&gt;

cc &lt;denchmark-link:https://github.com/jph00&gt;@jph00&lt;/denchmark-link&gt;
 was there a recent change that could explain this new error?
		</comment>
		<comment id='18' author='dreamflasher' date='2021-01-07T14:28:16Z'>
		&lt;denchmark-link:https://github.com/borisdayma&gt;@borisdayma&lt;/denchmark-link&gt;
 How about this: &lt;denchmark-link:https://github.com/fastai/fastai/pull/3129&gt;#3129&lt;/denchmark-link&gt;
 – we can do the last empty log, but keeping the step_count – this solves it on our side too, and shouldn't change anything in your tests? I also included a fix for the above error. Do you approve?
		</comment>
		<comment id='19' author='dreamflasher' date='2021-01-07T17:33:41Z'>
		Sorry just saw this - I fixed that this morning.
		</comment>
	</comments>
</bug>