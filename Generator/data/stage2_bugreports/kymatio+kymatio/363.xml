<bug id='363' author='Jonas1312' open_date='2019-03-03T09:14:07Z' closed_time='2019-07-22T15:11:05Z'>
	<summary>Scattering2D doesn't work with 2**J == image_size</summary>
	<description>
I'm not sure if &lt;denchmark-link:https://github.com/kymatio/kymatio/pull/346&gt;#346&lt;/denchmark-link&gt;
 fixes &lt;denchmark-link:https://github.com/kymatio/kymatio/issues/284&gt;#284&lt;/denchmark-link&gt;
 :
&lt;denchmark-code&gt;import torch
from kymatio import Scattering2D

scattering = Scattering2D(J=5, shape=(32, 32))
x = torch.randn(1, 1, 32, 32)
Sx = scattering(x)
print(Sx.size())
&lt;/denchmark-code&gt;

gives:
&lt;denchmark-code&gt;C:\Python36\python.exe D:/Cours/3A/PFE/Python/kymatio_examples/scattering2D_test.py
Traceback (most recent call last):
  File "D:/Cours/3A/PFE/Python/kymatio_examples/scattering2D_test.py", line 6, in &lt;module&gt;
    Sx = scattering(x)
  File "C:\Python36\lib\site-packages\kymatio-0.2.0.dev0-py3.6.egg\kymatio\scattering2d\scattering2d.py", line 235, in __call__
  File "C:\Python36\lib\site-packages\kymatio-0.2.0.dev0-py3.6.egg\kymatio\scattering2d\scattering2d.py", line 188, in forward
  File "C:\Python36\lib\site-packages\kymatio-0.2.0.dev0-py3.6.egg\kymatio\scattering2d\backend\backend_torch.py", line 40, in __call__
  File "C:\Python36\lib\site-packages\torch\nn\modules\module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "C:\Python36\lib\site-packages\torch\nn\modules\padding.py", line 172, in forward
    return F.pad(input, self.padding, 'reflect')
  File "C:\Python36\lib\site-packages\torch\nn\functional.py", line 2685, in pad
    ret = torch._C._nn.reflection_pad2d(input, pad)
RuntimeError: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (32, 32) at dimension 3 of input [1, 1, 32, 32]

Process finished with exit code 1
&lt;/denchmark-code&gt;

Can you reproduce the error ?
The goal is to get a (1, K, 1, 1) tensor.
Thanks
	</description>
	<comments>
		<comment id='1' author='Jonas1312' date='2019-03-03T10:45:07Z'>
		Currently, the padding works by finding the smallest size larger than ceil(M+2^J) but divisible by 2^J. There are indeed two issues:

pytorch padding doesn't handle settings for which one of the padding top/down+right/left is larger than the image size. This could be easily fixed, yet I think this is not critical because...
the settings in which J=(log(N)/log(2)) corresponds typically to settings where one does try to compute the expected scattering of a signal. For natural images, there are quite lot of boundary effects, but this is not the typical setting in which one does compute the e.s.. Thus, I simply padded by M meaning that the final size will be 2^(J+1). Alson I think this is consistant with the fact that there: *phi=\int ..

		</comment>
		<comment id='2' author='Jonas1312' date='2019-03-03T10:55:53Z'>
		Got it, thank you üëç
		</comment>
		<comment id='3' author='Jonas1312' date='2019-04-18T18:42:52Z'>
		Hi,
Is there a work-around to this issue right now where we can get an output tensor of size (1, K, 1, 1) ?
		</comment>
		<comment id='4' author='Jonas1312' date='2019-04-19T07:40:50Z'>
		
Hi,
Is there a work-around to this issue right now where we can get an output tensor of size (1, K, 1, 1) ?

For now, the simplest is to pad your image on your side and to extract the center pixel. Sorry about that, pytorch does allow a padding by N-1 of a signal of size N, but not a padding of N.. that needs extra coarse changes that can lead to hard-to-track bugs if not correctly handled..
		</comment>
		<comment id='5' author='Jonas1312' date='2019-04-19T14:19:59Z'>
		Thanks for the quick response &lt;denchmark-link:https://github.com/edouardoyallon&gt;@edouardoyallon&lt;/denchmark-link&gt;

Just to confirm the input size of the image, let's say we have an image from MNIST dataset whose size is 28x28. So, if I have to extract the center pixel for J=3, I would need to crop it to the size 24x24.
Whereas if I have an image from US Postal service handwritten dataset, whose size is 16 x16. For the same result, I would need to pad it to the size of 24x24.
I am trying to follow the original research paper by Dr. Stephane Mallat and Joan Bruna as closely as possible for my analysis. Request you to confirm if the above-written approach is correct.
		</comment>
		<comment id='6' author='Jonas1312' date='2019-07-18T23:55:47Z'>
		Sorry to reopen this, and feel free to close if I'm wrong, but I believe this still doesn't work on master as of now.
from kymatio import Scattering2D
import torch
J = 5
N = 32
scattering = Scattering2D(J, shape=(N, N))
x = torch.zeros(3, 3, N, N)
print(scattering(x).shape)
which prints  but I believe the correct result should be . Whatever the intended result, the test for this is actually disabled: the scattering is never called in
&lt;denchmark-link:https://github.com/kymatio/kymatio/blob/master/kymatio/scattering2d/tests/test_scattering2d.py#L368&gt;https://github.com/kymatio/kymatio/blob/master/kymatio/scattering2d/tests/test_scattering2d.py#L368&lt;/denchmark-link&gt;
, and even if called the output shape should be tested in my opionion (this is also valid for the case  below).
I'm not sure I'm doing the right thing, but I fixed it by making the following changes:

remove the special cases for M == 2 ** J and N == 2** J in compute_padding (https://github.com/kymatio/kymatio/blob/master/kymatio/scattering2d/utils.py#L23) : without the special case, M_padded = 3 * M and it is now M_padded = 2 * M. I believe the correct behaviour is to pad by M on each size, thus yielding a final size of M_padded = 3 * M,
make a copy of self.pad_size in Pad (https://github.com/kymatio/kymatio/blob/master/kymatio/scattering2d/backend/backend_torch.py#L40), because the following lines modify in place self.pad_size, which cause the condition self.pad_size == self.input_size to fail in self.__call__.

I can make a PR if this is indeed the correct behavior.
		</comment>
		<comment id='7' author='Jonas1312' date='2019-07-19T08:29:52Z'>
		Please propose a PR - that should have been fixed in the former PR
		</comment>
	</comments>
</bug>