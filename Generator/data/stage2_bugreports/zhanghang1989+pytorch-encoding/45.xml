<bug id='45' author='nieyan' open_date='2018-05-04T11:13:39Z' closed_time='2018-05-15T18:38:44Z'>
	<summary>syn-batchnorm : error during validation, train seems good.</summary>
	<description>
File "~/.local/lib/python3.6/site-packages/encoding/nn/syncbn.py", line 41, in forward
std = (self.running_var.clamp(self.eps)).sqrt()
TypeError: clamp received an invalid combination of arguments - got (float), but expected one of:

(float min)
didn't match because some of the arguments have invalid types: (float)
(float max)
didn't match because some of the arguments have invalid types: (float)
(float min, float max)

My env: centos 7, python3.6, pytorch 0.3.1, cuda-8.0
Thanks in advance.
	</description>
	<comments>
		<comment id='1' author='nieyan' date='2018-05-04T15:08:18Z'>
		I will make the code pytorch 0.3.1 compatible soon.
		</comment>
		<comment id='2' author='nieyan' date='2018-05-06T04:43:40Z'>
		Thank you for your quick reply!  By the way, which pytorch version has supportedï¼Ÿ I can switch to that version.
		</comment>
		<comment id='3' author='nieyan' date='2018-05-07T13:57:08Z'>
		IMHO, this has nothing to do with pytorch 0.3.1 compatibility, but rather the fact that you use the function clamp in an unusual way. clamp takes two arguments, min and max. You can omit one of them if you use a keyworded argument instead (like .clamp(min=self.eps)). However, .clamp(self.eps), as currently used, is ambiguous. Is the argument now the lower bound or the upper bound? Not clear from the function call. Maybe pytorch 0.4 somehow resolves this automatically to one of the two possibilities,  which it shouldn't according to documentation, and that's then a regression bug in pytorch 0.4 that should be forwarded to the pytorch developers.
However, I am quite astonished that in line 50, where the same function call is used in training mode, the issues does not arise. This should be further investigated. Maybe there is an exception thrown, but this is somehow silently ignored (which means the whole training is corrupted then)?
P.S. Solution
Replace .clamp(self.eps) by .clamp(min=self.eps) in syncbn.py (two times!). I'm pretty sure self.eps is meant to be a lower bound.
		</comment>
		<comment id='4' author='nieyan' date='2018-05-07T14:27:06Z'>
		Thank you. It's so kind of you. Actually, I have tried  ( .clamp(min=self.eps) ) few days ago, there are  another error like type mismatch (self.running_mean and std should be torch.autograd.Variable ) . After I directly changed the type, more deeper error during train come out. So I raised this issue look for help.  I am confusing line 50 what you mentioned too. And I think self.eps is number to prevent zero div.
		</comment>
		<comment id='5' author='nieyan' date='2018-05-08T15:28:56Z'>
		BTW, regarding your other error, I also stumbled across this and together with my fix &lt;denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/issues/47&gt;here&lt;/denchmark-link&gt;
, I got it to run in train and val mode! Not sure whether the results are actually as expected, need to finish training before I can tell. But just that you know it is doable.
		</comment>
		<comment id='6' author='nieyan' date='2018-05-09T08:20:04Z'>
		Cool ! ~  That's work. Thanks a lot. I am training in 4 GPUs and may get higher validation precision later.
		</comment>
		<comment id='7' author='nieyan' date='2018-05-09T12:26:25Z'>
		...except that in val mode, the results are garbage for me (like always predicting the same class for all samples). The only "fix" is to not use the saved running mean and variance during evaluation, instead estimate the statistics based on the val batches. Then it works fine and gives good performance. Running mean and variance must be somehow corrupted.
Did it work for you?
		</comment>
		<comment id='8' author='nieyan' date='2018-05-09T16:08:44Z'>
		Evaluation mode does not work well in my case too,  and someone seems to have reported the issue before in &lt;denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/issues/35&gt;#35&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='nieyan' date='2018-05-10T03:17:20Z'>
		Bad news. Although it can train and val normally after fix the error, model can't converge to baseline.
		</comment>
		<comment id='10' author='nieyan' date='2018-05-10T05:25:39Z'>
		I've tried it in object detection and it seems that during eval() mode, the detector itself tends to predict all region candidates as background with very high confidence. I wonder if this is my problem or the code itself.
		</comment>
		<comment id='11' author='nieyan' date='2018-05-10T05:27:47Z'>
		&lt;denchmark-link:https://github.com/zhanghang1989&gt;@zhanghang1989&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='nieyan' date='2018-05-10T18:50:56Z'>
		The code is currently broken. I will fix it during my spare time.
		</comment>
	</comments>
</bug>