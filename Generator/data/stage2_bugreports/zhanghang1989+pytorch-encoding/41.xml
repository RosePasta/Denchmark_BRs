<bug id='41' author='StevenGrove' open_date='2018-04-24T16:03:18Z' closed_time='2018-05-16T16:20:33Z'>
	<summary>Cuda memory leak in forward process</summary>
	<description>
In the forward process, the Cuda memory increase by about 10MB (one syncbn-2d) for each batch. I have tried the different pytorch version (0.3.0, 0.3.1, 0.4.0), but the issue still occurred.
import torch
from torch.autograd import Variable
from torch.nn.parallel.data_parallel import DataParallel
from encoding.nn import BatchNorm2d
net = BatchNorm2d(128).cuda()
net = DataParallel(net, device_ids=list(range(8)))
while True:
x = Variable(torch.randn(32, 128, 16, 16), requires_grad=True)
net(x)
loss = x.sum()
loss.backward()
	</description>
	<comments>
		<comment id='1' author='StevenGrove' date='2018-04-26T12:50:18Z'>
		I do appreciate your great workï¼Œand I am willing to help improve stability and compatibility. Did you meet the same issue when running a sample SyncBN program? &lt;denchmark-link:https://github.com/zhanghang1989&gt;@zhanghang1989&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='StevenGrove' date='2018-04-26T14:40:54Z'>
		I will take a look into it. Thx
		</comment>
		<comment id='3' author='StevenGrove' date='2018-05-15T18:42:19Z'>
		Please checkout the newest version and see if you still have the trouble. See details in PR &lt;denchmark-link:https://github.com/zhanghang1989/PyTorch-Encoding/pull/51&gt;#51&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='StevenGrove' date='2018-05-16T15:59:21Z'>
		It works and solve the trouble! Thanks.
		</comment>
	</comments>
</bug>