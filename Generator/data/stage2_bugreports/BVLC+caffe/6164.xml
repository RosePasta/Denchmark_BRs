<bug id='6164' author='MrMYHuang' open_date='2018-01-11T08:34:29Z' closed_time='2018-07-01T02:24:06Z'>
	<summary>BatchReindexLayer fails GPU gradient tests under CUDA v9.1</summary>
	<description>
&lt;denchmark-h:h3&gt;Your system configuration&lt;/denchmark-h&gt;

Operating system: CentOS 7.4.1708
Compiler:  x86_64-conda_cos6-linux-gnu-g++, gcc version 7.2.0 (crosstool-NG)
Graphics card: nVIDIA GeForce GTX 1070
CUDA version (if applicable): 9.1
CUDNN version (if applicable): 7.0.5
BLAS: openblas  0.2.20
Python or MATLAB version (for pycaffe and matcaffe respectively):
Anaconda 3 5.0.1 64-bit Python 3.6.4
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

As shown in this issue &lt;denchmark-link:https://github.com/ContinuumIO/anaconda-issues/issues/8025#issuecomment-355612498&gt;crosstool-NG compiled libraries compatibility problem&lt;/denchmark-link&gt;
, so I use Anaconda's gxx_linux-64 7.2.0 compiler to compile caffe (commit &lt;denchmark-link:https://github.com/BVLC/caffe/commit/e93b5e20356689654995e0bf2ce9cbb285a250f3&gt;e93b5e2&lt;/denchmark-link&gt;
) on CentOS 7 with this
&lt;denchmark-link:https://gist.github.com/MrMYHuang/b173435687d06e20040ce93b35869db8&gt;Makefile.config&lt;/denchmark-link&gt;
 and these &lt;denchmark-link:https://gist.github.com/MrMYHuang/4068c24350a08600005f5028a9b19f83&gt;Anaconda packages&lt;/denchmark-link&gt;
 (including libopenblas, leveldb, lmdb, opecv, protobuf, glog, gflags, py-boost, libboost, ...) and the following commands:
&lt;denchmark-code&gt;PATH=/cad/anaconda3/bin:/usr/bin make -j8
PATH=/cad/anaconda3/bin:/usr/bin make -j8 test
LD_LIBRARY_PATH=/usr/local/cuda/lib64 make runtest
&lt;/denchmark-code&gt;

However, make runtest failed at ./build/test/test_batch_reindex_layer.testbin with these error messages:
&lt;denchmark-code&gt;[----------] 2 tests from BatchReindexLayerTest/3, where TypeParam = caffe::GPUDevice&lt;double&gt;
[ RUN      ] BatchReindexLayerTest/3.TestForward
[       OK ] BatchReindexLayerTest/3.TestForward (3 ms)
[ RUN      ] BatchReindexLayerTest/3.TestGradient
./include/caffe/test/test_gradient_check_util.hpp:175: Failure
The difference between computed_gradient and estimated_gradient is 0.68212591193169037, which exceeds threshold_ * scale, where
computed_gradient evaluates to -0.68212591193169037,
estimated_gradient evaluates to 0, and
threshold_ * scale evaluates to 0.01.
debug: (top_id, top_data_id, blob_id, feat_id)=0,0,0,0; feat = -0.18315754813835761; objective+ = 3.4152213335738013; objective- = 3.4152213335738013
...
&lt;/denchmark-code&gt;

After countless caffe compilations and tests, finally, I find a workaround to this problem: I add a line NVCCFLAGS += -G to Makefile and it changes from
&lt;denchmark-code&gt;...
# Debugging
ifeq ($(DEBUG), 1)
        COMMON_FLAGS += -DDEBUG -g -O0
        NVCCFLAGS += -G
else
        COMMON_FLAGS += -DNDEBUG -O2
endif
...
&lt;/denchmark-code&gt;

to
&lt;denchmark-code&gt;...
# Debugging
ifeq ($(DEBUG), 1)
        COMMON_FLAGS += -DDEBUG -g -O0
        NVCCFLAGS += -G
else
        COMMON_FLAGS += -DNDEBUG -O2
        NVCCFLAGS += -G
endif
...
&lt;/denchmark-code&gt;

Then, compiling caffe again... and make runtest passes without failure!
I think the problem could be unrelated to Python and x86_64-conda_cos6-linux-gnu-g++ compiler, but related to nvcc (CUDA 9.1). So, this problem might be reproduced with other g++ compiler!? Actually, I see someone has the &lt;denchmark-link:http://blog.csdn.net/pzh16789/article/details/78913107&gt;same problem&lt;/denchmark-link&gt;
 with Ubuntu 16.04 + CUDA 9.1. I hope someone can fix this problem.
	</description>
	<comments>
		<comment id='1' author='MrMYHuang' date='2018-01-11T16:52:48Z'>
		Confirmed on a standard Ubuntu 16.04 build both by myself (with GCC 5.4.0 and NVCC 9.1.85) and others: first in &lt;denchmark-link:https://github.com/BVLC/caffe/issues/6140&gt;#6140&lt;/denchmark-link&gt;
, but also on caffe-users (&lt;denchmark-link:https://groups.google.com/forum/#!topic/caffe-users/7jQ8McFTtv8&gt;thread1&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://groups.google.com/forum/#!topic/caffe-users/FX5BoCTp-88&gt;thread2&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://groups.google.com/forum/#!topic/caffe-users/52xp37IdLkU&gt;thread3&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://groups.google.com/forum/#!topic/caffe-users/8LcTi0izJ6U&gt;thread 4&lt;/denchmark-link&gt;
).
Your workaround is to add a  flag to NVCC even for the standard build, correct? This flag causes generation of debug information for GPU code and disables all optimizations [&lt;denchmark-link:http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-altering-compiler-linker-behavior&gt;ref&lt;/denchmark-link&gt;
] - the latter effect  more relevant.
		</comment>
		<comment id='2' author='MrMYHuang' date='2018-01-12T04:05:46Z'>
		Hi Noiredd,

Your workaround is to add a -G flag to NVCC even for the standard build, correct?

Yes.
Additionally, I find another runtest failure which is unrelated to NVCCFLAGS += -G but related to opencv 3.3.1:  if I enable opencv (by commenting USE_OPENCV := 0), the runtest will fail at
./build/test/test_net.testbin with these error messages:
&lt;denchmark-code&gt;Cuda number of devices: 1
Current device id: 0
Current device name: GeForce GTX 1070
[==========] Running 124 tests from 5 test cases.
[----------] Global test environment set-up.
[----------] 26 tests from NetTest/0, where TypeParam = caffe::CPUDevice&lt;float&gt;
[ RUN      ] NetTest/0.TestHasBlob
[       OK ] NetTest/0.TestHasBlob (593 ms)
[ RUN      ] NetTest/0.TestGetBlob
[       OK ] NetTest/0.TestGetBlob (2 ms)
...
[ RUN      ] NetTest/0.TestSharedWeightsResume
[       OK ] NetTest/0.TestSharedWeightsResume (0 ms)
[ RUN      ] NetTest/0.TestParamPropagateDown
[       OK ] NetTest/0.TestParamPropagateDown (1 ms)
[ RUN      ] NetTest/0.TestFromTo
src/caffe/test/test_net.cpp:1446: Failure
Value of: *loss_ptr
  Actual: 6.95498
Expected: loss
Which is: 6.94028
src/caffe/test/test_net.cpp:1446: Failure
Value of: *loss_ptr
  Actual: 6.95498
Expected: loss
Which is: 6.94028
[  FAILED  ] NetTest/0.TestFromTo, where TypeParam = caffe::CPUDevice&lt;float&gt; (3 ms)
[ RUN      ] NetTest/0.TestReshape
...
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] NetTest/0.TestFromTo, where TypeParam = caffe::CPUDevice&lt;float&gt;
[  FAILED  ] NetTest/1.TestFromTo, where TypeParam = caffe::CPUDevice&lt;double&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='MrMYHuang' date='2018-01-14T06:42:07Z'>
		Confirmed, the following tests fails on CUDA 9.1 and CUDNN 7.
&lt;denchmark-code&gt;**[  FAILED  ] 2 tests, listed below:
[  FAILED  ] BatchReindexLayerTest/2.TestGradient, where TypeParam = N5caffe9GPUDeviceIfEE
[  FAILED  ] BatchReindexLayerTest/3.TestGradient, where TypeParam = N5caffe9GPUDeviceIdEE**
&lt;/denchmark-code&gt;

I was able to pass the tests by following &lt;denchmark-link:https://github.com/MrMYHuang&gt;@MrMYHuang&lt;/denchmark-link&gt;
 's suggestion to add NVCCFLAG.
		</comment>
		<comment id='4' author='MrMYHuang' date='2018-01-18T20:37:38Z'>
		&lt;denchmark-link:https://github.com/MrMYHuang&gt;@MrMYHuang&lt;/denchmark-link&gt;
's suggestion worked. You have to add  to Makefile and do
&lt;denchmark-code&gt;$ make clean &amp; make all &amp; make test &amp; make runtest
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='MrMYHuang' date='2018-01-19T05:36:15Z'>
		I submitted a bug report to &lt;denchmark-link:https://developer.nvidia.com&gt;nVIDIA&lt;/denchmark-link&gt;
. An nVIDIA staff replied me that the (CUDA) development team has identified this (CUDA 9.1) issue and is planning to fix it in the next release. At this time, it is suggested to use CUDA 9.0.
		</comment>
		<comment id='6' author='MrMYHuang' date='2018-01-22T12:37:54Z'>
		I also had this problem.
CUDA 9.1 + cuDNN 7.0.5 + caffe     [  FAILED  ] 1 tests.    mnist OK. my net failed
CUDA 9.0 + cuDNN 7.0.5 + caffe     [  FAILED  ] 2 tests.    mnist OK. my net failed
CUDA 8.0 + cuDNN 7.0.5 + caffe     [  PASSED  ] 2123 tests. mnist OK. my net failed
CUDA 8.0 + cuDNN 6.0.21+ caffe     [  PASSED  ] 2123 tests. mnist OK. my net failed
CUDA 8.0 + cuDNN 5.0.5 + caffe-rc5 [  PASSED  ] 2123 tests. mnist OK. my net OK.
CUDA 8.0 + cuDNN 6.0.21+ caffe-1.0 [  PASSED  ] 2123 tests. mnist OK. my net OK.
GTX 1080, Ubuntu 16.04.3, Driver Version: 387.34, i7 980X 3.3GHz, P6T-SE, RAM 6GB
CUDA 8.0 + cuDNN 7.0.5 passed "make runtest", and passed Caffe's mnist training.
But my net training failed. Its accuracy got 100% around 300 training iterations.
Using CPU training was OK. I gave up latest Caffe. Finally,
CUDA 8.0 + cuDNN 6.0.21+ caffe-1.0 was OK.
My net is for computer Go. It predicts next move.
12 conv layers, 128 channels, kernel_size is 3, without batch normalization.
		</comment>
		<comment id='7' author='MrMYHuang' date='2018-01-22T13:50:39Z'>
		Confirmed, the following tests fails on ubuntu 17.10, CUDA 9.1 and CUDNN 7.

I was able to pass the tests by following &lt;denchmark-link:https://github.com/MrMYHuang&gt;@MrMYHuang&lt;/denchmark-link&gt;
 's suggestion to add NVCCFLAG.
		</comment>
		<comment id='8' author='MrMYHuang' date='2018-02-04T04:41:09Z'>
		I am also using CUDA 9.1 and CUDNN v7.0.5 and can confirm this failure.  I actually came here to post another test failure I had but when I was about to I noticed that disabling multi-gpu's fixed that test failure and presented this one I will post that in a seperate issue though.
edit: actually after unsetting the "CUDA_VISIBLE_DEVICES" variable the other issue I am referring to is no longer occurring oddly.  I guess I won't post an issue for that right now until I can get the log to be generated again.  I might not have re-enabled multi-gpu support properly.
		</comment>
		<comment id='9' author='MrMYHuang' date='2018-03-03T10:15:25Z'>
		Unfortunately with latest nvcc patch 2 released problem with
BatchReindexLayerTest/3.TestGradient, where TypeParam = caffe::GPUDevice
still persist.
		</comment>
		<comment id='10' author='MrMYHuang' date='2018-04-04T03:18:30Z'>
		Issue still exists, with phyical machine + single Pascal + CentOS 7 + nvcc 9.1.85 + cudnn 7.0.5
		</comment>
		<comment id='11' author='MrMYHuang' date='2018-04-08T20:04:41Z'>
		Issue exists for me too:
Physical Machine
Ubuntu 16.04
Nvidia drivers: 390.48
CUDA: 9.1.85 + Patch 1,2,3
cuDNN: cuDNN v7.1.2
Got:
&lt;denchmark-code&gt;[  FAILED  ] 1 test, listed below:
[  FAILED  ] BatchReindexLayerTest/3.TestGradient, where TypeParam = caffe::GPUDevice&lt;double&gt;
&lt;/denchmark-code&gt;

After adding "NVCCFLAGS += -G" as OP suggested, no error and all passed.
But what does this mean for us when the flag is added?
i.e. (Are the optimizations only disabled during make or completely?)
		</comment>
		<comment id='12' author='MrMYHuang' date='2018-04-20T12:49:15Z'>
		I too had this error with &lt;denchmark-link:https://github.com/evilmtv&gt;@evilmtv&lt;/denchmark-link&gt;
's setup (except Ubuntu 14.04). I wanted to try following &lt;denchmark-link:https://github.com/Noiredd&gt;@Noiredd&lt;/denchmark-link&gt;
's suggestion and see whether this problem could be fixed by only changing the optimization level with the  flag (rather than the  flag).
Short answer: no. The -G is the needed work around.
After changing Makefile.config so that NVCCFLAGS += --optimize 0 (or NVCCFLAGS += -O0) and removing the -O2 entry from COMMON_FLAGS in the Makefile (line 322) so as to avoid an error caused by repeating the flag, the same tests failed.
		</comment>
		<comment id='13' author='MrMYHuang' date='2018-04-20T16:15:43Z'>
		Same problem occured when compiling under Gentoo Linux
gcc - 6.4.0
cuda 9.1.85
glibc 2.26-r6
and caffe compiled without python support
NVCCFLAGS += -G fixed it.
		</comment>
		<comment id='14' author='MrMYHuang' date='2018-05-03T18:28:51Z'>
		GPU: Nvidia GT 1030
Ubuntu 16.04, kernel 4.10.0-28-generic
Driver: 387.34
caffe: commit &lt;denchmark-link:https://github.com/BVLC/caffe/commit/864520713a4c5ffae7382ced5d34e4cadc608473&gt;8645207&lt;/denchmark-link&gt;

CUDNN: 7.0.5.15-1+cuda9.1
CuBLAS: 9.1.85.3-1
Cuda-NVCC: 9.1.85.2-1
4 trsts failed:
[==========] 2199 tests from 285 test cases ran. (343419 ms total)
[  PASSED  ] 2195 tests.
[  FAILED  ] 4 tests, listed below:
[  FAILED  ] NetTest/0.TestFromTo, where TypeParam = caffe::CPUDevice
[  FAILED  ] NetTest/1.TestFromTo, where TypeParam = caffe::CPUDevice
[  FAILED  ] BatchReindexLayerTest/2.TestGradient, where TypeParam = caffe::GPUDevice
[  FAILED  ] BatchReindexLayerTest/3.TestGradient, where TypeParam = caffe::GPUDevice
Try add option "-G" to Makefile, does not fix... in any case 3 test failed..
I try train ResNet 34, and ResNet 18, network, (for make it faster on 6 images) and next try to run it on CPU. It is not working after training... But mnist, normal working, and simplified bottleneck ResNet50 too working. Don't know depended it this unit tests or not....
		</comment>
		<comment id='15' author='MrMYHuang' date='2018-05-04T09:52:13Z'>
		I rebuild with CUDA 8.0 + cuDNN 6.0.21, with disabled OpenCV, and all tests passed.
But, before i use OpenCV 2.4 from ubuntu repo, not 3.3.1.
Without OpenCV ofcose i fave not ImageData layer, it use imread, and cv::mat to load image files, and it is not good for me.
Ok, i solve all tests, and now training/run all network work fine. In CPU and in GPU same.
[ FAILED ] NetTest/0.TestFromTo, where TypeParam = caffe::CPUDevice
[ FAILED ] NetTest/1.TestFromTo, where TypeParam = caffe::CPUDevice
This 2 falue depended to last version MKL 2018.2.199. When i replace it to Atlas, it fine working, and this 2 test passed.
CUDA 8.0 + cuDNN 6.0.21 + Atlas 3.10.2-9, work for me....
		</comment>
		<comment id='16' author='MrMYHuang' date='2018-05-27T05:35:24Z'>
		Confirmed.
Debian SId,
GCC-6/CUDA 9.1/Nvidia 390.48
		</comment>
		<comment id='17' author='MrMYHuang' date='2018-05-27T05:39:43Z'>
		Has anyone tested CUDA 9.2?
		</comment>
		<comment id='18' author='MrMYHuang' date='2018-05-27T06:31:14Z'>
		&lt;denchmark-link:https://github.com/cdluminate&gt;@cdluminate&lt;/denchmark-link&gt;
 All tests passed with latest commit + CUDA 9.2 + gcc 7.3.1
		</comment>
		<comment id='19' author='MrMYHuang' date='2018-05-27T06:42:02Z'>
		&lt;denchmark-link:https://github.com/xkszltl&gt;@xkszltl&lt;/denchmark-link&gt;
 Thanks. That means I can remove the temporary fix from Debian/Ubuntu's pre-built binary package as long as CUDA 9.2 is available. With  enabled for nvcc, the performance drop looks significant ...
		</comment>
		<comment id='20' author='MrMYHuang' date='2018-05-28T09:41:34Z'>
		&lt;denchmark-link:https://github.com/cdluminate&gt;@cdluminate&lt;/denchmark-link&gt;

Don't....simply trust me....
Experience may vary by system and...luck...๑乛◡乛๑
BTW I'm on CentOS
		</comment>
		<comment id='21' author='MrMYHuang' date='2018-06-15T03:45:07Z'>
		Not working here with latest commit + libcudnn7 (7.1.4.18-1+cuda9.2) + cuda 9.2 + gcc (5.4)
=/
		</comment>
		<comment id='22' author='MrMYHuang' date='2018-07-01T02:24:06Z'>
		All tests passed with commit &lt;denchmark-link:https://github.com/BVLC/caffe/commit/864520713a4c5ffae7382ced5d34e4cadc608473&gt;8645207&lt;/denchmark-link&gt;
 + CentOS 7.5.1804 + CUDA 9.2 + CUDNN 7.1 + gcc 4.8.5!
		</comment>
		<comment id='23' author='MrMYHuang' date='2019-01-29T21:27:02Z'>
		Not working for me. Details of problem in following link :
&lt;denchmark-link:https://github.com/BVLC/caffe/issues/6686&gt;#6686&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='MrMYHuang' date='2019-01-30T04:14:41Z'>
		&lt;denchmark-link:https://github.com/meriem87&gt;@meriem87&lt;/denchmark-link&gt;
  You issue looks unrelated to this.
		</comment>
		<comment id='25' author='MrMYHuang' date='2020-02-05T02:39:39Z'>
		
$ make clean &amp; make all &amp; make test &amp; make runtest


Are you root?
		</comment>
		<comment id='26' author='MrMYHuang' date='2020-02-05T02:50:17Z'>
		No, you aren't.
You are a normal user with GPU access.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Feb 4, 2020, 23:39 Swjtu-only ***@***.***&gt; wrote:
 $ make clean &amp; make all &amp; make test &amp; make runtest

 Are you root?

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub
 &lt;#6164?email_source=notifications&amp;email_token=ABSIJETW2FBOCDZVSMXDWT3RBIRG3A5CNFSM4ELJ53X2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKZ6ECQ#issuecomment-582214154&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ABSIJESGNZL4GQUBPH5NKRLRBIRG3ANCNFSM4ELJ53XQ&gt;
 .



		</comment>
		<comment id='27' author='MrMYHuang' date='2020-02-05T02:57:50Z'>
		
No, you aren't. You are a normal user with GPU access.
…
On Tue, Feb 4, 2020, 23:39 Swjtu-only @.***&gt; wrote: $ make clean &amp; make all &amp; make test &amp; make runtest Are you root? — You are receiving this because you commented. Reply to this email directly, view it on GitHub &lt;#6164?email_source=notifications&amp;email_token=ABSIJETW2FBOCDZVSMXDWT3RBIRG3A5CNFSM4ELJ53X2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKZ6ECQ#issuecomment-582214154&gt;, or unsubscribe https://github.com/notifications/unsubscribe-auth/ABSIJESGNZL4GQUBPH5NKRLRBIRG3ANCNFSM4ELJ53XQ .

Thanks,if i am a normal GPU access,i will meet permissions issue.
So,i decided run one by one  and lucky everything is ok for me.
		</comment>
	</comments>
</bug>