<bug id='2682' author='ctensmeyer' open_date='2015-07-02T20:59:08Z' closed_time='2017-04-14T02:02:59Z'>
	<summary>cuDNN errors when input &amp;lt; kernel &amp;lt;= padded input?</summary>
	<description>
This is a follow up on &lt;denchmark-link:https://github.com/BVLC/caffe/issues/2235&gt;#2235&lt;/denchmark-link&gt;
, where a CUDNN_STATUS_BAD_PARAM error code is returned when attempting to do convolution with a kernel bigger than the input, .
My example of failure is a 4x4 input with a 5x5 kernel with padding=1.  This causes the error when using Engine=CUDNN, but not with Engine=CAFFE.  Using a kernel size of 4x4 on a 4x4 input with no padding works just fine with CUDNN.
Here is the prototxt I am using to trigger the failure:
&lt;denchmark-code&gt;name: "Padding_Issue_Test"
layer {
  name: "data"
  type: "Data"
  top: "data"
  transform_param {
    crop_size: 4
  }
  data_param {
    source: "..."
    batch_size: 1
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 1
    kernel_size: 5
    stride: 1
    pad: 1
    engine: CUDNN
  }
}
&lt;/denchmark-code&gt;

This may be a problem with CUDNN v2 itself, but I don't know enough to tell.
	</description>
	<comments>
		<comment id='1' author='ctensmeyer' date='2015-07-13T07:51:04Z'>
		&lt;denchmark-link:https://github.com/slayton58&gt;@slayton58&lt;/denchmark-link&gt;
, are you aware of how cuDNN handles this case? Is this an issue with cuDNN or a parameter setting that cuDNN does not intend to support?
		</comment>
		<comment id='2' author='ctensmeyer' date='2015-07-14T03:37:56Z'>
		I've spent a little bit of time trying to replicate this, and so far I haven't been able to, either in our own tests or in a modified version of the lenet MNIST solver (crop the input to 4x4, run a single conv layer with a 5x5 filter, single pooling layer with 1x1 window and stride of 1).
@waldol1 If you can give me a self-contained repro case that exhibits the problem then I'd be happy to take a look
		</comment>
		<comment id='3' author='ctensmeyer' date='2015-07-15T11:37:11Z'>
		I have the same problem so I've attached a configuration that will replicate the issue:
&lt;denchmark-link:https://drive.google.com/folderview?id=0B4gLjNLmGOjBfl90WGF4RXo3VElxYzlCLVhxS0p3d0d0Q1BVTnhjTXAwcGRPR2g1dHhLVlE&amp;usp=sharing&gt;case&lt;/denchmark-link&gt;

It is based on a modified version of the cpp_classification example. You should use the following command line to run it:
./classification.bin test.prototxt test.caffemodel test-mean.protobin labels.txt img1.bmp
If you compile without CUDNN it works fine but with CUDNN it generates the following error:
F0715 11:25:25.748229 29878 cudnn_conv_layer.cu:53] Check failed: status == CUDN
N_STATUS_SUCCESS (3 vs. 0)  CUDNN_STATUS_BAD_PARAM
*** Check failure stack trace: ***
@     0x7f3b66673daa  (unknown)
@     0x7f3b66673ce4  (unknown)
@     0x7f3b666736e6  (unknown)
@     0x7f3b66676687  (unknown)
@     0x7f3b66afdea9  caffe::CuDNNConvolutionLayer&lt;&gt;::Forward_gpu()
@     0x7f3b66ac4569  caffe::Net&lt;&gt;::ForwardFromTo()
@     0x7f3b66ac49b5  caffe::Net&lt;&gt;::ForwardPrefilled()
@           0x40541e  Classifier::Predict()
@           0x4055c5  Classifier::Classify()
@           0x403e0b  main
@     0x7f3b65293ec5  (unknown)
@           0x4041ce  (unknown)
@              (nil)  (unknown)
		</comment>
		<comment id='4' author='ctensmeyer' date='2015-07-15T14:10:11Z'>
		&lt;denchmark-link:https://github.com/slayton58&gt;@slayton58&lt;/denchmark-link&gt;
, my reproduction case is what you said you've been trying (and what I posted before) without pooling after convolution, though that shouldn't make a difference.  Seems strange, but perhaps we can figure out the discrepancy.
This is the model file I'm using: &lt;denchmark-link:https://drive.google.com/open?id=0B5ufps6XAsvOSjVBM1FBanZkU0k&gt;https://drive.google.com/open?id=0B5ufps6XAsvOSjVBM1FBanZkU0k&lt;/denchmark-link&gt;

This is the solver file I'm using: &lt;denchmark-link:https://drive.google.com/open?id=0B5ufps6XAsvOa3ctSE9zUXpVeEE&gt;https://drive.google.com/open?id=0B5ufps6XAsvOa3ctSE9zUXpVeEE&lt;/denchmark-link&gt;

Both train and time cmds result in the error:
./build/tools/caffe train --log_dir=logs -solver test_solver.prototxt --gpu 0
yields the following log: &lt;denchmark-link:https://drive.google.com/open?id=0B5ufps6XAsvOaW5FaXU0NlNNd1U&gt;https://drive.google.com/open?id=0B5ufps6XAsvOaW5FaXU0NlNNd1U&lt;/denchmark-link&gt;

./build/tools/caffe time -model test.prototxt --log_dir=logs --iterations 1 --gpu 0
yields this log: &lt;denchmark-link:https://drive.google.com/open?id=0B5ufps6XAsvOaTlPYXVhenRZMlk&gt;https://drive.google.com/open?id=0B5ufps6XAsvOaTlPYXVhenRZMlk&lt;/denchmark-link&gt;

In the model definition file, changing "engine: CUDNN" to "engine: CAFFE" results in both cmds succeeding without error.  I ran these on the master branch of a fresh clone of the repo today.  The only change I made to the default make configurations is compile with CUDNN.  All tests were passing.  As well, I doubled checked that the built libcaffe.so is linked against libcudnn.so.6.5.48, which I understand to be the CUDNN v2 release.  I'm running on a K40c, with Nvidia Driver 346.46 on Ubuntu 14.04.2 LTS.
If you are seeing different results, I'm very curious.
		</comment>
		<comment id='5' author='ctensmeyer' date='2015-07-15T15:37:05Z'>
		Thanks @waldol1, &lt;denchmark-link:https://github.com/zashani&gt;@zashani&lt;/denchmark-link&gt;
 those test cases were very helpful!
I agree that cuDNN is not handling this case as of v2 - I think my LD_LIBRARY_PATH was pulling in a more recent version of the library as opposed to v2, causing my tests to originally work. Starting from scratch I can confirm I see the same issue as both of you.
I can also confirm that this is fixed in the upcoming cuDNN v3 release.
		</comment>
		<comment id='6' author='ctensmeyer' date='2015-07-15T15:57:05Z'>
		Any idea when this will be available?
On Jul 15, 2015 6:38 PM, slayton58 &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:
Thanks @waldol1&lt;denchmark-link:https://github.com/waldol1&gt;https://github.com/waldol1&lt;/denchmark-link&gt;
, @zashanihttps://github.com/zashani those test cases were very helpful!
I agree that cuDNN is not handling this case as of v2 - I think my LD_LIBRARY_PATH was pulling in a more recent version of the library as opposed to v2, causing my tests to originally work. Starting from scratch I can confirm I see the same issue as both of you.
I can also confirm that this is fixed in the upcoming cuDNN v3 release.
&lt;denchmark-h:h2&gt;&lt;/denchmark-h&gt;

Reply to this email directly or view it on GitHubhttps://github.com/&lt;denchmark-link:https://github.com/BVLC/caffe/issues/2682&gt;/issues/2682&lt;/denchmark-link&gt;
#issuecomment-121654580.
		</comment>
		<comment id='7' author='ctensmeyer' date='2015-07-16T03:20:22Z'>
		Thanks all. The current version of Caffe should therefore select CAFFE convolution when set to DEFAULT in this case. Anyone is welcome to make a PR for that, although we'll probably just let this fix itself in the next release of cuDNN.
		</comment>
		<comment id='8' author='ctensmeyer' date='2015-07-17T12:12:27Z'>
		&lt;denchmark-link:https://github.com/zashani&gt;@zashani&lt;/denchmark-link&gt;
 We're targeting the beginning of August for a public RC
		</comment>
		<comment id='9' author='ctensmeyer' date='2015-07-18T06:51:24Z'>
		Thanks for the update
On Jul 17, 2015 3:13 PM, slayton58 &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:
@zashanihttps://github.com/zashani We're targeting the beginning of August for a public RC
&lt;denchmark-h:h2&gt;&lt;/denchmark-h&gt;

Reply to this email directly or view it on GitHubhttps://github.com/&lt;denchmark-link:https://github.com/BVLC/caffe/issues/2682&gt;/issues/2682&lt;/denchmark-link&gt;
#issuecomment-122257991.
		</comment>
		<comment id='10' author='ctensmeyer' date='2015-07-19T07:14:53Z'>
		Does anyone know of a workaround until the patch comes out ? For example, if there is a way to gracefully raise an exception, one could dynamically switch to CPU mode when GPU mode crashes. Currently, we can't since the program crashes and we don't have a way to handle the error.
Appreciate if anyone has any pointers (For example, if anyone know of the top level call where I could exit the program upon error, that would help as well). I don't know the code well enough to figure this yet.
		</comment>
		<comment id='11' author='ctensmeyer' date='2015-07-20T13:42:48Z'>
		&lt;denchmark-link:https://github.com/slayton58&gt;@slayton58&lt;/denchmark-link&gt;
 - I though it might just be a bug in CuDNN.  Thanks for verifying what was going on.  I'm more than happy to wait for CuDNN v3.
&lt;denchmark-link:https://github.com/karthikkrishnanv&gt;@karthikkrishnanv&lt;/denchmark-link&gt;
 - For now probably the best thing to do is manually set the engine to CAFFE in the prototxt for your network for any convolution layers (perhaps pooling layers see the same problem), that might run into this problem.
		</comment>
		<comment id='12' author='ctensmeyer' date='2017-04-14T02:02:59Z'>
		Fixed in later cuDNN.
		</comment>
	</comments>
</bug>