<bug id='2926' author='raingo' open_date='2015-08-14T18:48:50Z' closed_time='2017-04-14T02:12:43Z'>
	<summary>runtest crash with TEST_GPUID != 0</summary>
	<description>
to duplicate this,
TEST_GPUID := 2 in Makefile.config
make runtest
the solver tests will crash.
no problem with TEST_GPUID := 0 though
	</description>
	<comments>
		<comment id='1' author='raingo' date='2015-08-14T20:04:54Z'>
		My experience is that it crashes when TEST_GPUID &gt;= 2 (however, TEST_GPUID = 1 still works). There should be a bug in the test code.
		</comment>
		<comment id='2' author='raingo' date='2015-08-14T20:07:41Z'>
		This was originally discussed in the multi-gpu work.  The problem is how the tests initialize up front before calls to the solver are made.  This causes an alloction/device mismatch that we now catch.
Setting CUDA_VISIBLE_DEVICES to direct testing does work.
		</comment>
		<comment id='3' author='raingo' date='2015-08-14T20:29:37Z'>
		so maybe just a documentation issue. where can I find info for CUDA_VISIBLE_DEVICES?
		</comment>
		<comment id='4' author='raingo' date='2015-08-14T20:39:50Z'>
		It is an environment variable you can set to control visibility of the GPUs to your app.
		</comment>
		<comment id='5' author='raingo' date='2015-08-14T20:45:27Z'>
		I can see the problem here. It seems that we shouldn't have initialized the test solvers beforehand and feed them to P2PSync afterwards. (but I am still a bit confused why TEST_GPUID = 1 passed for me...)
		</comment>
		<comment id='6' author='raingo' date='2015-08-14T21:10:39Z'>
		I see, it's from CUDA SDK. I think we can just remove the TEST_GPUID flag, because CUDA_VISIBLE_DEVICES can do everything.
		</comment>
		<comment id='7' author='raingo' date='2015-08-14T21:18:17Z'>
		We have options here.  Fix the test on initialization order and/or swap the Makefile setup to set/use CUDA_VISIBLE_DEVICES instead of trying to override in the code.  I think we should try to fix the test to delay initialization or actually initialize the right device(s).
		</comment>
		<comment id='8' author='raingo' date='2015-08-14T21:20:49Z'>
		&lt;denchmark-link:https://github.com/raingo&gt;@raingo&lt;/denchmark-link&gt;
 - We probably want to leave this bug open until we have a proper fix/workaround in place.
		</comment>
		<comment id='9' author='raingo' date='2015-08-14T22:15:02Z'>
		FYI, on my machine it also crashes with TEST_GPUID := 0 (see below). The workaround of doing "setenv CUDA_VISIBLE_DEVICES 0" before doing "make runtest" prevents the crash.
&lt;denchmark-code&gt;[----------] 9 tests from AdaGradSolverTest/2, where TypeParam = caffe::GPUDevice&lt;float&gt;
[ RUN      ] AdaGradSolverTest/2.TestAdaGradLeastSquaresUpdate
F0814 18:10:39.053434  4412 math_functions.cpp:91] Check failed: error == cudaSuccess (11 vs. 0)  invalid argument
*** Check failure stack trace: ***
    @     0x7f0a0add80dd  google::LogMessage::Fail()
    @     0x7f0a0add9eac  google::LogMessage::SendToLog()
    @     0x7f0a0add7ccc  google::LogMessage::Flush()
    @     0x7f0a0adda7be  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f0a07ed0f51  caffe::caffe_copy&lt;&gt;()
    @     0x7f0a07fb16a7  caffe::apply_buffers&lt;&gt;()
    @     0x7f0a07fb47d2  caffe::GPUParams&lt;&gt;::GPUParams()
    @     0x7f0a07fb50da  caffe::P2PSync&lt;&gt;::P2PSync()
    @     0x7f0a07fb5d59  caffe::P2PSync&lt;&gt;::run()
    @           0x810504  caffe::GradientBasedSolverTest&lt;&gt;::RunLeastSquaresSolver()
    @           0x81e99f  caffe::GradientBasedSolverTest&lt;&gt;::TestLeastSquaresUpdate()
    @           0x85a4da  testing::internal::HandleExceptionsInMethodIfSupported&lt;&gt;()
    @           0x84fdd9  testing::Test::Run()
    @           0x84feb7  testing::TestInfo::Run()
    @           0x84fff5  testing::TestCase::Run()
    @           0x850365  testing::internal::UnitTestImpl::RunAllTests()
    @           0x85a05a  testing::internal::HandleExceptionsInMethodIfSupported&lt;&gt;()
    @           0x84f641  testing::UnitTest::Run()
    @           0x4e12c2  main
    @       0x35a521ed5d  (unknown)
    @           0x4e1099  (unknown)
make: *** [runtest] Aborted (core dumped)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='raingo' date='2015-08-14T22:17:30Z'>
		&lt;denchmark-link:https://github.com/xavigibert&gt;@xavigibert&lt;/denchmark-link&gt;
 - can you post the output of nvidia-smi?  Do you have different board types?
		</comment>
		<comment id='11' author='raingo' date='2015-08-14T22:18:39Z'>
		Sure. Here it is.
&lt;denchmark-code&gt;Fri Aug 14 18:18:01 2015       
+------------------------------------------------------+                       
| NVIDIA-SMI 340.32     Driver Version: 340.32         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 690     Off  | 0000:03:00.0     N/A |                  N/A |
| 35%   49C    P8    N/A /  N/A |      8MiB /  2047MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 690     Off  | 0000:04:00.0     N/A |                  N/A |
| 30%   37C    P8    N/A /  N/A |      8MiB /  2047MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
|   2  Quadro NVS 290      Off  | 0000:05:00.0     N/A |                  N/A |
|100%   67C    P0    N/A /  N/A |    194MiB /   255MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Compute processes:                                               GPU Memory |
|  GPU       PID  Process name                                     Usage      |
|=============================================================================|
|    0            Not Supported                                               |
|    1            Not Supported                                               |
|    2            Not Supported                                               |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='raingo' date='2015-08-14T22:22:00Z'>
		Okay, so the issue here is we are attempting to use 2 wildely different devices at the same time. It seems.  Hrmm, not sure how to defend against that as that is going to break generally.
I wonder if we should support asymetric configs or detect the issue when walking devices to build the device tree.  In practice, the user specifies the devices they really want to train with, but the tests walk available devices.
		</comment>
		<comment id='13' author='raingo' date='2015-08-14T22:30:51Z'>
		A side note. The documentation should be improved around multi-gpu. For example, what's the best practice? what's the hardware/sdk requirements? For a multi-gpu example, what's the gain expected (for testing purpose).
		</comment>
		<comment id='14' author='raingo' date='2015-08-14T22:53:54Z'>
		You mean reading the code isn't sufficient for the average user.  ;-)
Yes, we need to clean that up.  Can you open an issue so that git will hound us?
We more or less assume the same class devices, if not identical devices, are being used and you have a reasonable PCIe topology.  For example a "gemini" board (2 GPUs on one board) or a motherboard with fully connected slots.  Most "gaming" motherboards have 2 GPUs on the same root complex.  Specific motherboards, like that in the DIGITS DevBox, have a special PCIe switch to enable 4 boards to talk together with RDMA.
Perf is MASSIVELY network dependent and can end up depending on how good your IO subsystem is to load the data.  AlexNet using compressed JPEGs should get ~1.8X scaling on 2 devices for most people unless you hit IO or PCIe topology issues.  If you are running cuDNNv3, the scaling will drop as the communication becomes more limiting.  But if you increase the batch size, you can change the compute and communication ratio (don't forget to adjust learning rate...).
		</comment>
		<comment id='15' author='raingo' date='2015-08-15T16:44:59Z'>
		Partially fixed in &lt;denchmark-link:https://github.com/BVLC/caffe/pull/2931&gt;#2931&lt;/denchmark-link&gt;
. Solution should work for the original issue but not &lt;denchmark-link:https://github.com/xavigibert&gt;@xavigibert&lt;/denchmark-link&gt;
's.
I'd like to keep this issue open and discuss how to set CUDA_VISIBLE_DEVICES, probably adding it to Makefile.config.example?
		</comment>
		<comment id='16' author='raingo' date='2017-04-14T02:12:43Z'>
		Fixed by switch to new parallelism in &lt;denchmark-link:https://github.com/BVLC/caffe/pull/4563&gt;#4563&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>