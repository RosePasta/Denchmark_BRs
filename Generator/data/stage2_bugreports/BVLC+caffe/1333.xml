<bug id='1333' author='tleyden' open_date='2014-10-20T15:56:45Z' closed_time='2014-11-30T22:54:01Z'>
	<summary>Error in caffe: double free or corruption</summary>
	<description>
I'm using commit &lt;denchmark-link:https://github.com/BVLC/caffe/commit/51be3523b8fe883d86600fbf3531b6ef7a6290fe&gt;51be352&lt;/denchmark-link&gt;
 and getting the following error when trying to do training:
&lt;denchmark-code&gt;...
I1020 15:24:48.951339  3162 caffe.cpp:121] Optimization Done.
*** Error in `/opt/caffe/build/tools/caffe': double free or corruption (out): 0x0000000001db6160 ***
Aborted (core dumped)
&lt;/denchmark-code&gt;

To reproduce the error:

$ git clone https://github.com/tleyden/caffe.git
$ git checkout feature/alpha_example
$ cd caffe/data/alphabet_classification &amp;&amp; ./get_alpha.sh
$ cd caffe/examples/alphabet_classification &amp;&amp; ./train_alpha.sh

Here are the caffe configuration files and training data I'm using:

alpha_solver.prototxt
alpha_train_text.prototxt
alpha_training_images.txt
training data zipfile ~100k

The image set consists of 28x28 greyscale images of 0-9A-Z (36 classes in all), generated with a few different fonts.
I suppose there is something wrong in my configuration or training data (I'm a caffe Noob), but the reason I'm filing a bug is that:

I wouldn't expect caffe to crash.
If caffe is going to crash, it should at least give me a better and more actionable error message.

Would be very interested in hearing what I need to change in my configuration / training set to avoid this error, but also think it's an opportunity to make caffe more robust.
	</description>
	<comments>
		<comment id='1' author='tleyden' date='2014-10-20T16:45:59Z'>
		Thanks for the clear report and instructions to reproduce the issue. The cause for the crash isn't immediately obvious -- but you're certainly right it should not crash -- but with these details one should be able to debug it.
As a sanity check, did the LeNet example work properly for you?
		</comment>
		<comment id='2' author='tleyden' date='2014-10-20T17:04:34Z'>
		You need to change the number of outputs in your train_val.prototxt from 10
to 36 -- as it is anything labeled &gt;=10 is causing an invalid read and/or
write, which probably leads to the weird error you see. I think a CHECK was
recently added to softmax loss layer that the label is in range (but don't
care to check from my phone).
On Oct 20, 2014 8:56 AM, "Traun Leyden" &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

I'm using commit 51be352
51be352
and getting the following error when trying to do training:
...
I1020 15:24:48.951339  3162 caffe.cpp:121] Optimization Done.
*** Error in `/opt/caffe/build/tools/caffe': double free or corruption (out): 0x0000000001db6160 ***
Aborted (core dumped)
To reproduce the error:

$ git clone https://github.com/tleyden/caffe.git
$ git checkout feature/alpha_example
$ cd caffe/data/alphabet_classification &amp;&amp; ./get_alpha.sh
$ cd caffe/examples/alphabet_classification &amp;&amp; ./train_alpha.sh

Here are the caffe configuration files and training data I'm using:

alpha_solver.prototxt
https://github.com/tleyden/caffe/blob/feature/alpha_example/examples/alphabet_classification/alpha_solver.prototxt
alpha_train_text.prototxt
https://github.com/tleyden/caffe/blob/feature/alpha_example/examples/alphabet_classification/alpha_train_text.prototxt
training data zipfile ~100k
http://cl.ly/473w163M1g2D/download/alphabet_training_data.zip

The image set consists of 28x28 greyscale images of 0-9A-Z (36 classes in
all), generated with a few different fonts.
I suppose there is something wrong in my configuration or training data
(I'm a caffe Noob), but the reason I'm filing a bug is that:

I wouldn't expect caffe to crash.
If caffe is going to crash, it should at least give me a better and
more actionable error message.

Would be very interested in hearing what I need to change in my
configuration / training set to avoid this error, but also think it's an
opportunity to make caffe more robust.
—
Reply to this email directly or view it on GitHub
#1333.

		</comment>
		<comment id='3' author='tleyden' date='2014-10-20T18:31:08Z'>
		&lt;denchmark-link:https://github.com/jeffdonahue&gt;@jeffdonahue&lt;/denchmark-link&gt;
 is right about the  and the softmax range check is in dev at &lt;denchmark-link:https://github.com/BVLC/caffe/commit/802ee6cbd2925a6db55f9c56d939a233403bd556&gt;802ee6c&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='tleyden' date='2014-10-20T18:59:59Z'>
		
As a sanity check, did the LeNet example work properly for you?

Yep, that worked.

You need to change the number of outputs in your train_val.prototxt from 10
to 36 -- as it is anything labeled &gt;=10 is causing an invalid read and/or
write, which probably leads to the weird error you see.

Thanks, I'll give that a shot and report back.
		</comment>
		<comment id='5' author='tleyden' date='2014-10-20T20:07:32Z'>
		&lt;denchmark-link:https://github.com/jeffdonahue&gt;@jeffdonahue&lt;/denchmark-link&gt;
 I tried setting the num_output to 36, but got the same error:
&lt;denchmark-code&gt;I1020 18:46:32.180645  1816 caffe.cpp:121] Optimization Done.
*** Error in `/opt/caffe/build/tools/caffe': double free or corruption (out): 0x00000000029d1390 ***
Aborted (core dumped)
&lt;/denchmark-code&gt;

Can you check that I changed the right parameter?
Here's the updated config: &lt;denchmark-link:https://github.com/tleyden/caffe/blob/feature/alpha_example/examples/alphabet_classification/alpha_train_text.prototxt#L117&gt;https://github.com/tleyden/caffe/blob/feature/alpha_example/examples/alphabet_classification/alpha_train_text.prototxt#L117&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='tleyden' date='2014-10-20T20:22:21Z'>
		Looks right -- at this point I'm not sure what else could be wrong. I'd try
running the training in valgrind (just do valgrind caffe &lt;args&gt; instead
of caffe &lt;args&gt;) and look for any memory errors it reports.
On Mon, Oct 20, 2014 at 1:07 PM, Traun Leyden &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

@jeffdonahue https://github.com/jeffdonahue I tried setting the
num_output to 36, but got the same error:
I1020 18:46:32.180645  1816 caffe.cpp:121] Optimization Done.
*** Error in `/opt/caffe/build/tools/caffe': double free or corruption (out): 0x00000000029d1390 ***
Aborted (core dumped)
Can you check that I changed the right parameter?
Here's the updated config:
https://github.com/tleyden/caffe/blob/feature/alpha_example/examples/alphabet_classification/alpha_train_text.prototxt#L117
—
Reply to this email directly or view it on GitHub
#1333 (comment).

		</comment>
		<comment id='7' author='tleyden' date='2014-10-20T21:51:04Z'>
		&lt;denchmark-link:https://github.com/jeffdonahue&gt;@jeffdonahue&lt;/denchmark-link&gt;
 I re-ran with valgrind, and it didn't seem to core dump this time .. here's the full output:
&lt;denchmark-link:https://gist.github.com/tleyden/415bcc58b6a1c42a4999&gt;https://gist.github.com/tleyden/415bcc58b6a1c42a4999&lt;/denchmark-link&gt;

anything else I can do to help get to the bottom of this?
		</comment>
		<comment id='8' author='tleyden' date='2014-10-20T21:52:56Z'>
		Here's an "invalid read": &lt;denchmark-link:https://gist.github.com/tleyden/415bcc58b6a1c42a4999#file-gistfile1-txt-L450-L481&gt;https://gist.github.com/tleyden/415bcc58b6a1c42a4999#file-gistfile1-txt-L450-L481&lt;/denchmark-link&gt;

Also, I take it that  (&lt;denchmark-link:https://gist.github.com/tleyden/415bcc58b6a1c42a4999#file-gistfile1-txt-L482&gt;https://gist.github.com/tleyden/415bcc58b6a1c42a4999#file-gistfile1-txt-L482&lt;/denchmark-link&gt;
) isn't a good sign?
		</comment>
		<comment id='9' author='tleyden' date='2014-10-20T22:06:59Z'>
		The invalid read/write near the beginning of training are probably where to
look.  If you recompile with debugging enabled (set DEBUG := 1 in
Makefile.config), valgrind should tell you on exactly which line in
pooling_layer.cpp the invalid read/write occurs (I'd be interested to know)
-- you can probably just stop training once you see the valgrind invalid
read/write errors.
On Mon, Oct 20, 2014 at 2:52 PM, Traun Leyden &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

Here's an "invalid read":
https://gist.github.com/tleyden/415bcc58b6a1c42a4999#file-gistfile1-txt-L450-L481
Also, I take it that Iteration 100, loss = -nan (
https://gist.github.com/tleyden/415bcc58b6a1c42a4999#file-gistfile1-txt-L482)
isn't a good sign?
—
Reply to this email directly or view it on GitHub
#1333 (comment).

		</comment>
		<comment id='10' author='tleyden' date='2014-10-20T22:23:39Z'>
		OK here's the updated output with DEBUG := 1:
&lt;denchmark-link:https://gist.github.com/tleyden/65fead16ced7466e2752&gt;https://gist.github.com/tleyden/65fead16ced7466e2752&lt;/denchmark-link&gt;

Here are the invalid reads/writes:
&lt;denchmark-link:https://gist.github.com/tleyden/65fead16ced7466e2752#file-gistfile1-txt-L486-L530&gt;https://gist.github.com/tleyden/65fead16ced7466e2752#file-gistfile1-txt-L486-L530&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='tleyden' date='2014-10-20T22:34:22Z'>
		Is this based on current master? i.e., is this line 240 of
pooling_layer.cpp for you:
&lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L240&gt;https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L240&lt;/denchmark-link&gt;

On Mon, Oct 20, 2014 at 3:23 PM, Traun Leyden &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

OK here's the updated output with DEBUG := 1:
https://gist.github.com/tleyden/65fead16ced7466e2752
Here are the invalid reads/writes:
https://gist.github.com/tleyden/65fead16ced7466e2752#file-gistfile1-txt-L486-L530
—
Reply to this email directly or view it on GitHub
#1333 (comment).

		</comment>
		<comment id='12' author='tleyden' date='2014-10-20T22:38:13Z'>
		Yup, I just opened that file and verified the lines match up.
My fork is based on master commit &lt;denchmark-link:https://github.com/BVLC/caffe/commit/51be3523b8fe883d86600fbf3531b6ef7a6290fe&gt;51be352&lt;/denchmark-link&gt;
 and I haven't changed any of the core code.
		</comment>
		<comment id='13' author='tleyden' date='2014-10-20T23:34:08Z'>
		&lt;denchmark-link:https://github.com/tleyden&gt;@tleyden&lt;/denchmark-link&gt;
 thanks a lot for reporting this, posting a thorough description of the problem, and following the initial debugging steps I've suggested.  All I can say is that it seems likely this is unfortunately an actual bug in Caffe's  CPU implementation (do you have a CUDA-capable GPU to try running this on as well?), as opposed to anything you've done wrong -- I've labeled it as such.
At this point I'd begin a more ad-hoc debugging process that would be impractical to communicate step-by-step in this forum.  It would seem that both bottom_index and top_index in that line are out of bounds (since there's both an invalid read and invalid write error displayed); so I'd probably print those values before the line that's causing the error and go from there. EDIT: actually it could be that only bottom_index is out of bounds since that value is both read and written, per the +=.
If you (or anyone else) does manage to figure out what's going wrong, a PR to fix the bug would of course be welcome.  Feel free to also continue following up with information about the debugging process here -- I'll check back if/whenever there's new information on this issue and offer ideas if I have any.
		</comment>
		<comment id='14' author='tleyden' date='2014-10-21T00:30:50Z'>
		
do you have a CUDA-capable GPU to try running this on as well?

I'll have to do some work to get it running on a GPU, will loop back on that.

At this point I'd begin a more ad-hoc debugging process that would be impractical to communicate step-by-step in this forum

Do you mean because of noise or just the tediousness nature of the back-and-forth?
Is there an IRC chat room or some other realtime chat?  (I just stumbled across &lt;denchmark-link:https://gitter.im/&gt;https://gitter.im/&lt;/denchmark-link&gt;
 .. don't know if you guys support that)
If it's a noise issue, I could also add an issue to my fork, which would avoid generating noise for everyone.

actually it could be that only bottom_index is out of bounds since that value is both read and written, per the +=.

I added some debug output, and here's what I'm seeing:
&lt;denchmark-code&gt;I1020 23:53:47.134955  2599 pooling_layer.cpp:240] bottom_index: 546 index: 141
I1020 23:53:47.135129  2599 pooling_layer.cpp:240] bottom_index: 548 index: 142
I1020 23:53:47.136464  2599 pooling_layer.cpp:240] bottom_index: 550 index: 143
I1020 23:53:47.509040  2599 pooling_layer.cpp:240] bottom_index: -1 index: 0
==2599== Invalid read of size 4
... 
==2599==  Address 0x1b6d0d5c is 4 bytes before a block of size 64,000 alloc'd
.. etc..
&lt;/denchmark-code&gt;


If you (or anyone else) does manage to figure out what's going wrong, a PR to fix the bug would of course be welcome. Feel free to also continue following up with information about the debugging process here -- I'll check back if/whenever there's new information on this issue and offer ideas if I have any.

I feel like the next step should be to isolate it down to a test case.  I can try, but it's an unfamiliar codebase and C++ isn't my best language.  It would probably go a lot faster with some interactive help.
		</comment>
		<comment id='15' author='tleyden' date='2014-10-21T00:45:50Z'>
		Ah, well given the  from your debug printout the memory access error makes sense.  But the bug must be further upstream then.  -1 is the default value filled into the mask (which should contain the 'argmax' indices of the input after maxpooling Forward) by  (see &lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L123&gt;https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L123&lt;/denchmark-link&gt;
).  So for some reason, some of the mask is not getting filled in during  -- I'd add some print statements in Forward to try to get more info.
Isolating the bug in a unit test would indeed be great, but can be difficult.
		</comment>
		<comment id='16' author='tleyden' date='2014-10-21T06:59:21Z'>
		Just taking a glance, it seems like this is exactly what I'd expect to happen after things diverge badly. (Note that &lt;denchmark-link:https://github.com/tleyden&gt;@tleyden&lt;/denchmark-link&gt;
 is getting NaN losses (&lt;denchmark-link:https://github.com/BVLC/caffe/issues/1333#issuecomment-59846030&gt;#1333 (comment)&lt;/denchmark-link&gt;
)). Once NaNs have propagated to the bottom blob, NaN is never greater than , so the mask never gets filled in.
Perhaps we should initialize the top a different way, or maybe we should just fail whenever loss is NaN (though I suppose one could end up with NaNs a different way...). Or we could add a check and fail if the mask is never filled in, which means we saw all NaNs or all -FLT_MAXs or all negative infinities (shouldn't we be using negative infinity?)
Added: Another (kind of ugly) alternative which prevents the crash (but doesn't fail fast) is to use !(a &lt;= b) in place of (a &gt; b).
		</comment>
		<comment id='17' author='tleyden' date='2014-10-21T07:12:38Z'>
		And yes, the CUDA version seems to have the same problem. And perhaps yet another way to prevent the crash would be to initialize the mask with something valid in the first place (but then that prevents failure, which is probably a bad idea).
		</comment>
		<comment id='18' author='tleyden' date='2014-10-22T15:17:54Z'>
		I was able to reproduce the issue on the mnist data set.
Unless I did something wrong in the extraction of the mnist images, then that isolates the issue to a problem in the IMAGE_DATA layer.
Here's how to reproduce:

$ git clone https://github.com/tleyden/caffe.git
$ git checkout  feature/mnist_image_data_layer
$ cd $CAFFE_ROOT &amp;&amp; make all
Edit examples/mnist/create_mnist.sh to change: BACKEND="files"
$ ./data/mnist/get_mnist.sh
$ ./examples/mnist/create_mnist.sh
$ ./examples/mnist/train_lenet_files.sh

Here are the &lt;denchmark-link:https://gist.github.com/tleyden/1aad925b50db3bffcbd2&gt;full logs&lt;/denchmark-link&gt;
 and the &lt;denchmark-link:https://gist.github.com/tleyden/1aad925b50db3bffcbd2#file-gistfile1-txt-L1051-L1052&gt;core dump&lt;/denchmark-link&gt;
.  Also note the &lt;denchmark-link:https://gist.github.com/tleyden/1aad925b50db3bffcbd2#file-gistfile1-txt-L482&gt;nan log messages&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='19' author='tleyden' date='2014-10-22T15:21:54Z'>
		Does this &lt;denchmark-link:https://gist.github.com/tleyden/1aad925b50db3bffcbd2#file-gistfile1-txt-L371&gt;log output&lt;/denchmark-link&gt;
 look right?
&lt;denchmark-code&gt;I1022 03:32:02.258487  7240 net.cpp:103] Top shape: 64 3 28 28 (150528)
&lt;/denchmark-code&gt;

Shouldn't that be Top shape: 64 1 28 28 instead?
It looks like it's treating the images as 3 channel RGB images, when they're actually 1 channel greyscale images.  Not sure if that's the root problem but I doubt it would help things.
		</comment>
		<comment id='20' author='tleyden' date='2014-10-22T15:26:25Z'>
		I tried training the default mnist dataset (lmdb + DATA layer), and verified that the top shape is indeed treating the data as single channel greyscale:
&lt;denchmark-code&gt;I1022 15:23:02.468415 25052 net.cpp:103] Top shape: 64 1 28 28 (50176)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://gist.github.com/tleyden/1f0342d615b8124a5429&gt;Partial logs&lt;/denchmark-link&gt;
 from that test run.
		</comment>
		<comment id='21' author='tleyden' date='2014-10-22T15:31:39Z'>
		&lt;denchmark-link:https://github.com/tleyden&gt;@tleyden&lt;/denchmark-link&gt;
 if images are grayscale add  to the 
		</comment>
		<comment id='22' author='tleyden' date='2014-10-22T15:32:59Z'>
		I noticed something odd when I was looking over the code .. figured I'd throw it out there just in case.
&lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/src/caffe/util/io.cpp#L69&gt;ReadImageToDatum&lt;/denchmark-link&gt;
 takes 6 parameters, but at this &lt;denchmark-link:https://github.com/BVLC/caffe/blob/737ea5e936821b5c69f9c3952d72693ae5843370/src/caffe/layers/image_data_layer.cpp#L58&gt;call site in image_data_layer.cpp&lt;/denchmark-link&gt;
, it's only passing 5 params, and I'm not seeing any default values set for the params.
		</comment>
		<comment id='23' author='tleyden' date='2014-10-22T15:36:01Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
 thanks for the tip, are you sure it will look at that parameter?  It's not documented in the &lt;denchmark-link:http://caffe.berkeleyvision.org/tutorial/layers.html&gt;layer catalog&lt;/denchmark-link&gt;
 in the IMAGE_DATA layer docs.
		</comment>
		<comment id='24' author='tleyden' date='2014-10-22T15:36:23Z'>
		When argument is_color is omitted then is assumed to be true.
If you look at &lt;denchmark-link:https://github.com/BVLC/caffe/blob/dev/src/caffe/layers/image_data_layer.cpp#L118&gt;https://github.com/BVLC/caffe/blob/dev/src/caffe/layers/image_data_layer.cpp#L118&lt;/denchmark-link&gt;
 that is fixed
		</comment>
		<comment id='25' author='tleyden' date='2014-10-22T15:38:29Z'>
		&lt;denchmark-link:https://github.com/tleyden&gt;@tleyden&lt;/denchmark-link&gt;
 when you want to know latest arguments for a layer I would recommend you to look at caffe.proto or the code of the layer.
		</comment>
		<comment id='26' author='tleyden' date='2014-10-22T15:38:53Z'>
		&lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
 that didn't work for me, I got this error:
&lt;denchmark-code&gt;[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 10:13: Message type "caffe.ImageDataParameter" has no field named "is_color".
&lt;/denchmark-code&gt;

		</comment>
		<comment id='27' author='tleyden' date='2014-10-22T15:41:27Z'>
		Are you using the latest dev?
&lt;denchmark-link:https://github.com/BVLC/caffe/blob/dev/src/caffe/proto/caffe.proto#L535&gt;https://github.com/BVLC/caffe/blob/dev/src/caffe/proto/caffe.proto#L535&lt;/denchmark-link&gt;

		</comment>
		<comment id='28' author='tleyden' date='2014-10-22T15:42:22Z'>
		Nope, I'm on the master branch.  Should I be on dev?
		</comment>
		<comment id='29' author='tleyden' date='2014-10-22T16:44:09Z'>
		I rebased &lt;denchmark-link:https://github.com/tleyden/caffe/tree/feature/dev_mnist_data_layer&gt;my branch&lt;/denchmark-link&gt;
 from the master branch to the dev branch, added , and still get the same errors.
So it looks like the error is not related to the number of channels in the image, but there's still an error somewhere in the IMAGE_DATA layer, since it can't train the mnist data (or, I've done something wrong in extracting the mnist data into images or in the layer params).
&lt;denchmark-link:https://gist.github.com/tleyden/235d5007102a2afa1583&gt;Partial logs w/ core dump&lt;/denchmark-link&gt;

&lt;denchmark-link:https://gist.github.com/tleyden/fe27cb1c4f968ee9a3d9&gt;lenet_train_test_files.prototxt&lt;/denchmark-link&gt;
 that I used.
		</comment>
		<comment id='30' author='tleyden' date='2014-10-22T18:48:13Z'>
		Um... was my comment &lt;denchmark-link:https://github.com/BVLC/caffe/issues/1333#issuecomment-59886967&gt;#1333 (comment)&lt;/denchmark-link&gt;
 not clear?
There is a bug in the pooling layer that I expect to cause exactly this behavior: after divergence, NaNs appear in the bottom blob, the argmax never gets filled in during forward, and so backward uses the default index (-1) to write to the diff, which is of course invalid, but because the diff is on the heap it probably just screws up the structure of the heap a little bit, which doesn't get noticed until it can no longer be freed at the end.
Did you try changing &lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L128&gt;https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L128&lt;/denchmark-link&gt;
 to
caffe_set(top_count, 0, mask)
or changing &lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L146&gt;https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp#L146&lt;/denchmark-link&gt;
 to
if (!(bottom_data[index] &lt;= top_data[pool_index])) {
?
The real issue from a practical standpoint, of course, is why you're getting a divergence on LeNet in the first place; my guess is it's because you're missing the 1/255 scaling (i.e., the transform_param at &lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt#L13&gt;https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt#L13&lt;/denchmark-link&gt;
).
		</comment>
		<comment id='31' author='tleyden' date='2014-10-22T19:07:36Z'>
		&lt;denchmark-link:https://github.com/longjon&gt;@longjon&lt;/denchmark-link&gt;
 actually it wasn't -- I took your comment regarding divergence to mean that my dataset was flawed, and so I thought it would shed more light to try a known working dataset like mnist.
Also, if it's really a bug in the pooler, why don't I see the issue running mnist using the DATA layer (as opposed to IMAGE_DATA layer)?

my guess is it's because you're missing the 1/255 scaling

I was looking for a way to add a scaling parameter, since it's being used in &lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt&gt;lenet_train_test.prototxt&lt;/denchmark-link&gt;
, but I couldn't find anything in the docs -- I was looking in &lt;denchmark-link:http://caffe.berkeleyvision.org/tutorial/layers.html&gt;http://caffe.berkeleyvision.org/tutorial/layers.html&lt;/denchmark-link&gt;
 under the IMAGE_DATA section, and there is no mention of a scaling parameter.  I guess I'll use &lt;denchmark-link:https://github.com/sguada&gt;@sguada&lt;/denchmark-link&gt;
's advice and dig into the .proto definitions.
Are there more recent docs I should be looking at?
		</comment>
		<comment id='32' author='tleyden' date='2014-10-22T19:10:44Z'>
		
Also, if it's really a bug in the pooler, why don't I see the issue running mnist using the DATA layer (as opposed to IMAGE_DATA layer)?

I take that question back ... I think I get it now.
Since IMAGE_DATA layer is missing the 1/255 scaling parameter, it's causing divergence, which causes the bug in the pooler to manifest itself.  Since using the DATA layer w/ scaling parameter doesn't cause divergence, the bug in the pooler stays hidden.
		</comment>
		<comment id='33' author='tleyden' date='2014-10-22T19:19:12Z'>
		Yeah, exactly... sorry if I am a bit terse sometimes.
I've reproduced a post-optimization crash in master using the LeNet example with  layer, just by omitting  and running in CPU mode. (You can add  to the  layer just as it's done at &lt;denchmark-link:https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt#L12&gt;https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt#L12&lt;/denchmark-link&gt;
; that's a fairly recent change, so I'm not sure it's well-documented yet.)
		</comment>
		<comment id='34' author='tleyden' date='2014-10-22T19:29:32Z'>
		...and, divergence without a crash with the change above. I'll make a separate issue about the NaN handling itself.
		</comment>
		<comment id='35' author='tleyden' date='2014-10-22T19:33:10Z'>
		&lt;denchmark-link:https://github.com/longjon&gt;@longjon&lt;/denchmark-link&gt;
 re: &lt;denchmark-link:https://github.com/BVLC/caffe/issues/1333#issuecomment-59886967&gt;#1333 (comment)&lt;/denchmark-link&gt;
 it seems reasonable to deliberately exit on divergence. The NaN isn't the fault of the pooling and is never a reasonable value to back-propagate so why not check at the end of Forward and err?
		</comment>
		<comment id='36' author='tleyden' date='2014-10-22T19:45:29Z'>
		&lt;denchmark-link:https://github.com/shelhamer&gt;@shelhamer&lt;/denchmark-link&gt;
 yep yep, I just made &lt;denchmark-link:https://github.com/BVLC/caffe/issues/1349&gt;#1349&lt;/denchmark-link&gt;
 to keep track of that thought. However, that doesn't necessarily prevent the crash, since NaNs could appear at pooling layer bottoms for other reasons before they reach the loss...
		</comment>
		<comment id='37' author='tleyden' date='2014-10-22T20:32:16Z'>
		
(You can add transform_param to the IMAGE_DATA layer just as it's done at https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt#L12; )

Awesome, I will give that a shot.

that's a fairly recent change, so I'm not sure it's well-documented yet.

I'll try to send a PR with an update to those docs.
		</comment>
		<comment id='38' author='tleyden' date='2014-10-22T21:20:30Z'>
		After adding transform_param and is_color: false, training on lenet with IMAGE_DATA layer is now completing without errors.  (this is using my &lt;denchmark-link:https://github.com/tleyden/caffe/tree/feature/dev_mnist_data_layer&gt;branch&lt;/denchmark-link&gt;
 which is based off of dev).
The full .prototext file is here: &lt;denchmark-link:https://github.com/tleyden/caffe/blob/feature/dev_mnist_data_layer/examples/mnist/lenet_train_test_files.prototxt&gt;https://github.com/tleyden/caffe/blob/feature/dev_mnist_data_layer/examples/mnist/lenet_train_test_files.prototxt&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>