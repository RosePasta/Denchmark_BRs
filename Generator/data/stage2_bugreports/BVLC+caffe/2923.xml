<bug id='2923' author='philipp-fischer' open_date='2015-08-14T14:01:43Z' closed_time='2015-09-26T18:20:47Z'>
	<summary>PythonLayer and Multi-GPU random segfaults</summary>
	<description>
Hi, unfortunately I cannot give a very clear bug description, because I get different errors every time.
However:
Using the simplest imaginable PythonLayer and multi-gpu with GPUs 0,1 selected, segfaults in boost Python library pop up. Sometimes I get errors from the garbage collector Fatal Python error: GC object already tracked
This seems to be because of parallelizing the Python code in multiple threads.
When I set share_in_parallel: true to serialize Python execution, everything works fine.
The Python layer I use:
&lt;denchmark-code&gt;import caffe
import numpy as np

class PythonLayer(caffe.Layer):

    def setup(self, bottom, top):
        if len(bottom) != 1:
            raise Exception("Need one input.")

    def reshape(self, bottom, top):
        top[0].reshape(bottom[0].num, bottom[0].channels, bottom[0].height, bottom[0].width)

    def forward(self, bottom, top):
        top[0].data[...] = bottom[0].data

    def backward(self, top, propagate_down, bottom):
        if propagate_down[0]:
            bottom[0].diff[...] = top[0].diff
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='philipp-fischer' date='2015-08-15T19:51:24Z'>
		seems to be similar to &lt;denchmark-link:http://stackoverflow.com/questions/8888124/embedded-python-segfaults&gt;http://stackoverflow.com/questions/8888124/embedded-python-segfaults&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='philipp-fischer' date='2015-08-16T08:36:27Z'>
		&lt;denchmark-link:https://github.com/philipp-fischer&gt;@philipp-fischer&lt;/denchmark-link&gt;
 Note

When I set share_in_parallel: true to serialize Python execution, everything works fine.

This is not true in general. Currently  (introduced in &lt;denchmark-link:https://github.com/BVLC/caffe/pull/2903&gt;#2903&lt;/denchmark-link&gt;
) only applies to forward via a mutex lock, but not backward, as I hope one only do so when PythonLayer is used as a data layer and does no backward. However, I didn't expect this bug in &lt;denchmark-link:https://github.com/BVLC/caffe/pull/2903&gt;#2903&lt;/denchmark-link&gt;
. It was my fault.
(however, things are really complicated here as simply adding a backward lock in &lt;denchmark-link:https://github.com/BVLC/caffe/pull/2903&gt;#2903&lt;/denchmark-link&gt;
 won't ensure correct results, when solver2 does forward between solver1's forward and backward and PythonLayer has internal state such as the maxid in PoolingLayer)
		</comment>
		<comment id='3' author='philipp-fischer' date='2015-09-26T18:20:47Z'>
		Close as is partially resolved by &lt;denchmark-link:https://github.com/BVLC/caffe/pull/3032&gt;#3032&lt;/denchmark-link&gt;
 as a short-term workaround before we have better solutions. Further discussions on this issue should follow in &lt;denchmark-link:https://github.com/BVLC/caffe/issues/2936&gt;#2936&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>