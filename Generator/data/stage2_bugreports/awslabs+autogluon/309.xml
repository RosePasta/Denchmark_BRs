<bug id='309' author='austinmw' open_date='2020-02-18T20:54:16Z' closed_time='2020-02-22T01:34:38Z'>
	<summary>predictor.predict hanging</summary>
	<description>
I'm trying to integrate autogluon with SageMaker training/deployment.
With the test script I'm currently using, sometimes fit/deploy/predict work fine, but about half of the time (without any modification of docker image or train/serve code, seemingly randomly) when I get to the point where I'm trying to call predictor.predict(data), I can see in my endpoint logs that it's just hanging there and timing out. I'm using the tabular predictor on a single datapoint, so it shouldn't be timing out due to model complexity.
What could be going wrong here, and how can I go about debugging this?
	</description>
	<comments>
		<comment id='1' author='austinmw' date='2020-02-18T21:15:14Z'>
		Looks like question on Tabular Predition. &lt;denchmark-link:https://github.com/Innixma&gt;@Innixma&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jwmueller&gt;@jwmueller&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='austinmw' date='2020-02-19T01:20:58Z'>
		Removing the SageMaker component of this, does your code fail on predict in the local setting?
Can you also provide a code sample of the failure? Ensure that the single data point you are passing into .predict is a DataFrame and not a Series.
		</comment>
		<comment id='3' author='austinmw' date='2020-02-19T18:09:57Z'>
		Hi &lt;denchmark-link:https://github.com/Innixma&gt;@Innixma&lt;/denchmark-link&gt;
 , looks like it does not fail in the local setting, so must be a sagemaker/environment issue. Having a bit of difficulty in teasing the difference between the two environments. Happen to know of a way to get more to the bottom of why predictor.predict() times out in SM? Dataframe input and model look to be the same. Going to get more environment info and minimum reproducible examples. Thanks.
		</comment>
		<comment id='4' author='austinmw' date='2020-02-19T18:20:18Z'>
		&lt;denchmark-link:https://github.com/zhanghang1989&gt;@zhanghang1989&lt;/denchmark-link&gt;
 would be the most knowledgable regarding SM with AutoGluon, but right now its largely an untested domain. Insights into this from your testing would be greatly appreciated!
		</comment>
		<comment id='5' author='austinmw' date='2020-02-21T02:37:28Z'>
		Is this your first time using SM? Perhaps you can also try using a different python model (eg. scikit-learn) as well as autogluon, just to rule out that it is a pure SM issue?
		</comment>
		<comment id='6' author='austinmw' date='2020-02-21T02:57:59Z'>
		It is possible the issue stems from the path issue solved in &lt;denchmark-link:https://github.com/awslabs/autogluon/pull/319&gt;#319&lt;/denchmark-link&gt;
. I'd recommend trying again after building from latest source once this PR is merged.
		</comment>
		<comment id='7' author='austinmw' date='2020-02-21T14:42:12Z'>
		&lt;denchmark-link:https://github.com/jwmueller&gt;@jwmueller&lt;/denchmark-link&gt;
 been using SM for quite a while now. Will look into path issue, thanks! Still working on reproducible examples.
		</comment>
		<comment id='8' author='austinmw' date='2020-02-21T23:41:46Z'>
		&lt;denchmark-link:https://github.com/awslabs/autogluon/pull/319&gt;#319&lt;/denchmark-link&gt;
 has been merged, please try to see if it works with the latest build. Note that it requires retraining of the model, the model from the old version won't be fixed.
Best,
Nick
		</comment>
		<comment id='9' author='austinmw' date='2020-02-22T01:34:38Z'>
		Worked!
		</comment>
		<comment id='10' author='austinmw' date='2020-02-22T02:06:31Z'>
		Awesome!
		</comment>
	</comments>
</bug>