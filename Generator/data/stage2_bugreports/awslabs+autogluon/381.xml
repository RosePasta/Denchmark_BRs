<bug id='381' author='austinmw' open_date='2020-03-25T20:04:32Z' closed_time='2020-03-30T21:33:08Z'>
	<summary>Issue with f1 eval_metric and net.evaluate_predictions</summary>
	<description>
On a tabular binary classification problem ("yes"/"no" labels rather than 0/1). I'm able to run net.evaluate_predictions successfully with eval_metric='accuracy', but when I switch to eval_metric='f1', and rerun evaluate_predictions, I get an sklearn error:
pos_label=1 is not a valid label: array(['no', 'yes'], dtype='&lt;U3')
Not sure if this needs to be fixed since I could probably just map no/yes to 0/1, but figured I'd mention it.
	</description>
	<comments>
		<comment id='1' author='austinmw' date='2020-03-25T20:10:36Z'>
		This is likely because evaluate_predictions takes in the prediction values after they were converted to the external user representation, but does not convert them to the internal representation before doing evaluations. This should be fixed by converting the inputs to internal representation through self._learner.label_cleaner inside evaluate_predictions()
&lt;denchmark-link:https://github.com/jwmueller&gt;@jwmueller&lt;/denchmark-link&gt;
 Can you take a look and see if you can fix?
		</comment>
		<comment id='2' author='austinmw' date='2020-03-28T00:12:04Z'>
		Thanks for pointing out this issue.  It should be fixed in this PR:  &lt;denchmark-link:https://github.com/awslabs/autogluon/pull/387&gt;#387&lt;/denchmark-link&gt;

Note that in the meantime before this PR is merged, you can just do the following as long as the label column is still present in your test_data Dataframe:
&lt;denchmark-code&gt;predictor = task.fit(..., eval_metric = 'f1')
f1 = predictor.evaluate(test_data)
&lt;/denchmark-code&gt;

Please reopen this issue if you still have problems after the PR has been merged.
		</comment>
		<comment id='3' author='austinmw' date='2020-03-30T21:33:08Z'>
		PR has been merged.
		</comment>
	</comments>
</bug>