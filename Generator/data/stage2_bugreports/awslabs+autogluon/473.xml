<bug id='473' author='nitinmnsn' open_date='2020-05-18T18:02:17Z' closed_time='2020-06-15T05:24:31Z'>
	<summary>/home/nitin/anaconda3/envs/automlgluon/lib/python3.6/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log   loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)</summary>
	<description>
I am using autogluon to help me a binary classification problem. It is an unbalanced dataset (90:10) and the neural network breaks down in training and never recovers. I guess a reinitialization once this has been encountered would help?
&lt;denchmark-code&gt;/home/nitin/anaconda3/envs/automlgluon/lib/python3.6/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log
  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)
/home/nitin/anaconda3/envs/automlgluon/lib/python3.6/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log
  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)
/home/nitin/anaconda3/envs/automlgluon/lib/python3.6/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log
  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)
/home/nitin/anaconda3/envs/automlgluon/lib/python3.6/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log
  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)
/home/nitin/anaconda3/envs/automlgluon/lib/python3.6/site-packages/sklearn/metrics/classification.py:2174: RuntimeWarning: divide by zero encountered in log
&lt;/denchmark-code&gt;

The task is fitting all the other models just fine and hitting an 'auc' of 0.74+ but neural network has issues (dead relus, inf encountered and now wont recover? just guesses) but i guess reinitialization once the weights have been bastardized irrevocably would help
	</description>
	<comments>
		<comment id='1' author='nitinmnsn' date='2020-05-18T20:02:17Z'>
		Thanks for submitting an issue!
We are aware that the NN can at times break down as you say. We have a PR in progress that hopes to deal with at least part of the problem: &lt;denchmark-link:https://github.com/awslabs/autogluon/pull/339&gt;#339&lt;/denchmark-link&gt;
 .
&lt;denchmark-link:https://github.com/jwmueller&gt;@jwmueller&lt;/denchmark-link&gt;
 Can you take ownership of this issue and see to resolving this problem in the next release? I think it affects quite a few datasets.
		</comment>
		<comment id='2' author='nitinmnsn' date='2020-05-19T01:57:41Z'>
		Yep I already have a much better fix than the one in the PR but it's currently entangled with a bunch of other code. Will try to update the PR shortly
		</comment>
		<comment id='3' author='nitinmnsn' date='2020-05-19T09:22:03Z'>
		I have seen this issue in writing my own neural networks based pipeline (it was couple of years back though). I did a few things in trying to solve it and they helped (all in tensorflow) :

Gradient clipping
Batchnorm as the first layer
Weights normalization (You have batchnorm with dropout. I have read that they dont work well together). If anyway you have batchnorm then keeping gamma frozen at 1 and beta frozen at 0 for first few epochs might help
Reinitializing the network altogether when nan was encounter in loss
Leaky ReLU for first few epoch and then ReLU. Also, SELU was great at not letting the network bastardize to even 6-7 layers of depths.
Deep feed forward netowrks with dense connections (densenet style) with SELU (since SELU can handle depth) never got the issue. But, you are going for speed as well and SELU is going to need at least 100 neurons in hidden layers for fixed point theorem to kick in plus huge network .. so.. I dont know .. just putting down what i had tried

Also, with me it did happen that the network broke down at times but i didnt see it break down with every initialization but that is happening here. I dropped around 100 features using null importance and still the neural network is breaking on every initialization.
		</comment>
		<comment id='4' author='nitinmnsn' date='2020-05-20T01:23:06Z'>
		The issue is much simpler, when you specify eval_metric='roc_auc', the NN is actually internally checking the per-epoch validation log-loss metric for early stopping. Near initialization, the NN often predicts class-probabilities == 0 by pure chance which causes 'divide by zero'.  For now, if you want to use NN alone with 'roc_auc' metric, then:  simply change your eval_metric to accuracy. This may produce better 'roc_auc' from the NN as it'll avoid this issue by early stopping based on per-epoch validation accuracy rather than log-loss (despite no change to its architecture/initialization/training updates).
		</comment>
		<comment id='5' author='nitinmnsn' date='2020-05-22T19:16:05Z'>
		Alternatively, you can also set stopping_metric='roc_auc' to fix it.
		</comment>
		<comment id='6' author='nitinmnsn' date='2020-05-24T11:35:04Z'>
		&lt;denchmark-link:https://github.com/jwmueller&gt;@jwmueller&lt;/denchmark-link&gt;
  - "Near initialization, the NN often predicts class-probabilities == 0 by pure chance".. Maybe you wanted to say 'hard classes' and not 'class-probabilities'. Outoput of softmax wont be zero.

I do not think this is a case of pure chance but a case of imbalanced data. NN is getting so many samples with 0 as the label that it initially learns to predict all 0s. Plus it is happening on every initialization. Also, for accuracy you would have to set a threshold which even if you would be setting equal to proportion of 1s., the network would have to be very unlucky to predict all probabilities below threshold to have all labels are 0. For these reasons I guess the network is just learning to predict very low probabilities for each sample
How this translates to a 'division by zero' beats me. Can you please shed some light on it?
This imbalanced classes is the sole reason i dont want to use accuracy as 'eval_metric'. My rule based class_predicton = 0 has 90% accuracy to begin with.

		</comment>
		<comment id='7' author='nitinmnsn' date='2020-06-15T05:24:31Z'>
		Fixed in: &lt;denchmark-link:https://github.com/awslabs/autogluon/pull/481&gt;#481&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>