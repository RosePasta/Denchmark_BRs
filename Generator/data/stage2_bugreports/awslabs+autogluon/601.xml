<bug id='601' author='mli' open_date='2020-08-01T00:17:10Z' closed_time='2020-09-04T02:13:17Z'>
	<summary>Predict for tabular: Cannot convert non-finite values (NA or inf) to integer</summary>
	<description>
Got an error when  on a popular &lt;denchmark-link:https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data&gt;kaggle datasets&lt;/denchmark-link&gt;
 (may due to this dataset has a bunch of NAs?)
&lt;denchmark-code&gt;File delimiter for ../input/house-prices-advanced-regression-techniques/test.csv inferred as ',' (comma). If this is incorrect, please manually load the data as a pandas DataFrame.
Loaded data from: ../input/house-prices-advanced-regression-techniques/test.csv | Columns = 80 / 80 | Rows = 1459 -&gt; 1459
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-48-91455d7b9940&gt; in &lt;module&gt;
      1 test_data = task.Dataset(file_path=os.path.join(in_dir, job, 'test.csv'))
      2 
----&gt; 3 preds = model.predict(test_data)

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/autogluon/task/tabular_prediction/predictor.py in predict(self, dataset, model, as_pandas, use_pred_cache, add_to_pred_cache)
    130         """
    131         dataset = self.__get_dataset(dataset)
--&gt; 132         return self._learner.predict(X=dataset, model=model, as_pandas=as_pandas, use_pred_cache=use_pred_cache, add_to_pred_cache=add_to_pred_cache)
    133 
    134     def predict_proba(self, dataset, model=None, as_pandas=False, as_multiclass=False):

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/autogluon/utils/tabular/ml/learner/abstract_learner.py in predict(self, X, model, as_pandas, use_pred_cache, add_to_pred_cache)
    127 
    128         if len(X_cache_miss) &gt; 0:
--&gt; 129             y_pred_proba = self.predict_proba(X=X_cache_miss, model=model, inverse_transform=False)
    130             problem_type = self.trainer_problem_type or self.problem_type
    131             y_pred = get_pred_from_proba(y_pred_proba=y_pred_proba, problem_type=problem_type)

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/autogluon/utils/tabular/ml/learner/abstract_learner.py in predict_proba(self, X, model, as_pandas, as_multiclass, inverse_transform)
     92     # TODO: Add pred_proba_cache functionality as in predict()
     93     def predict_proba(self, X: DataFrame, model=None, as_pandas=False, as_multiclass=False, inverse_transform=True):
---&gt; 94         X = self.transform_features(X)
     95         y_pred_proba = self.load_trainer().predict_proba(X, model=model)
     96         if inverse_transform:

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/autogluon/utils/tabular/ml/learner/abstract_learner.py in transform_features(self, X)
    216     def transform_features(self, X):
    217         for feature_generator in self.feature_generators:
--&gt; 218             X = feature_generator.transform(X)
    219         return X
    220 

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/autogluon/utils/tabular/utils/decorators.py in inner1(*args, **kwargs)
     12         begin = time.time()
     13 
---&gt; 14         output = func(*args, **kwargs)
     15 
     16         # storing time after function execution

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/autogluon/utils/tabular/features/abstract_feature_generator.py in transform(self, X)
    183             raise ValueError(f'Required columns are missing from the provided dataset. Missing columns: {missing_cols}')
    184 
--&gt; 185         X = X.astype(self.features_init_types)
    186         X.reset_index(drop=True, inplace=True)
    187         X_features = self.generate_features(X)

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors, **kwargs)
   5863                     results.append(
   5864                         col.astype(
-&gt; 5865                             dtype=dtype[col_name], copy=copy, errors=errors, **kwargs
   5866                         )
   5867                     )

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/pandas/core/generic.py in astype(self, dtype, copy, errors, **kwargs)
   5880             # else, only a single dtype is given
   5881             new_data = self._data.astype(
-&gt; 5882                 dtype=dtype, copy=copy, errors=errors, **kwargs
   5883             )
   5884             return self._constructor(new_data).__finalize__(self)

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/pandas/core/internals/managers.py in astype(self, dtype, **kwargs)
    579 
    580     def astype(self, dtype, **kwargs):
--&gt; 581         return self.apply("astype", dtype=dtype, **kwargs)
    582 
    583     def convert(self, **kwargs):

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
    436                     kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy)
    437 
--&gt; 438             applied = getattr(b, f)(**kwargs)
    439             result_blocks = _extend_blocks(applied, result_blocks)
    440 

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors, values, **kwargs)
    557 
    558     def astype(self, dtype, copy=False, errors="raise", values=None, **kwargs):
--&gt; 559         return self._astype(dtype, copy=copy, errors=errors, values=values, **kwargs)
    560 
    561     def _astype(self, dtype, copy=False, errors="raise", values=None, **kwargs):

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/pandas/core/internals/blocks.py in _astype(self, dtype, copy, errors, values, **kwargs)
    641                     # _astype_nansafe works fine with 1-d only
    642                     vals1d = values.ravel()
--&gt; 643                     values = astype_nansafe(vals1d, dtype, copy=True, **kwargs)
    644 
    645                 # TODO(extension)

~/miniconda3/envs/autogluon/lib/python3.7/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna)
    698         if not np.isfinite(arr).all():
    699             raise ValueError(
--&gt; 700                 "Cannot convert non-finite values (NA or inf) to " "integer"
    701             )
    702 

ValueError: Cannot convert non-finite values (NA or inf) to integer
&lt;/denchmark-code&gt;

Here is the code
import os
import glob

def run_cmd(cmd):
    assert os.system(cmd) == 0, cmd
    
job = 'house-prices-advanced-regression-techniques'
in_dir = os.path.join('..', 'input')    

run_cmd(f'kaggle c download -c {job} -p {in_dir}')
run_cmd(f'cd {in_dir}; unzip -n {job}.zip -d {job}')
glob.glob(os.path.join(in_dir, job, '*'))

from autogluon import TabularPrediction as task

train_data = task.Dataset(file_path=os.path.join(in_dir, job, 'train.csv'))
model = task.fit(train_data=train_data.drop(columns='Id'), label='SalePrice', verbosity=4) 
test_data = task.Dataset(file_path=os.path.join(in_dir, job, 'test.csv'))
preds = model.predict(test_data.drop(columns='Id'))
	</description>
	<comments>
		<comment id='1' author='mli' date='2020-08-01T02:45:35Z'>
		&lt;denchmark-link:https://github.com/Innixma&gt;@Innixma&lt;/denchmark-link&gt;
  Seems like a new bug perhaps introduced in the feature-generator refactor?
I've run this dataset multiple times in the past without issue.
		</comment>
		<comment id='2' author='mli' date='2020-08-04T00:05:19Z'>
		Fixed in &lt;denchmark-link:https://github.com/awslabs/autogluon/pull/584&gt;#584&lt;/denchmark-link&gt;
 (&lt;denchmark-link:https://github.com/awslabs/autogluon/commit/11b4f5abf9e29f773be074ddc0ccfe685a0b6c51&gt;11b4f5a&lt;/denchmark-link&gt;
)
When the functionality to convert dtypes to the original train dtypes at inference time was added, this bug was created, because integer types can't have NaN. So during training, if an int type has no NaN's, then training succeeds, but if the same column has NaN's at test time, pandas actually loads the column as float. When asked to convert to int by our code, it crashes with the error described. It appears that none of our benchmark datasets have this property unfortunately, so it wasn't caught.
For now, I've patched this by filling NaN's to 0 for such columns, although this may not be ideal in the long term.
Another way to avoid the crash (Without needing the fix) is to pass test_data as tuning_data=test_data.drop(columns='Id') in task.fit, since when concatenating train and test dataframes for feature generation, the int columns of train are converted to float if test type is float. This guarantees test_data will not crash in predict, but not new data test_data_2 for example.
		</comment>
		<comment id='3' author='mli' date='2020-08-06T02:12:45Z'>
		&lt;denchmark-link:https://github.com/Innixma&gt;@Innixma&lt;/denchmark-link&gt;

Whenever we cast columns into new types, what if we just to try-except and keep the original-type if the column can't be casted?
I'd imagine all models will still work if trained on int column but then tested with float column (not 100% sure about this though).
		</comment>
		<comment id='4' author='mli' date='2020-08-06T02:48:41Z'>
		
@Innixma
Whenever we cast columns into new types, what if we just to try-except and keep the original-type if the column can't be casted?
I'd imagine all models will still work if trained on int column but then tested with float column (not 100% sure about this though).

That will open the gates to many defects down the road I think. This will also mean anyone who makes models will have to know that the input data could be of a different type during inference, which is really confusing.
		</comment>
		<comment id='5' author='mli' date='2020-08-07T17:49:55Z'>
		I agree having unexpected types during inference could be weird if people are using models that can't handle the type-change (I'm not sure what models those are though). For now we can just do this on case-by-case basis, although I guess then we need to wait for bug notifications first. For these ints specifically, we should probably be imputing via something like: int(nanmean(column_vals). Introducing 0s at test-time may be bad if say the training ints are all &gt; 1000.
		</comment>
		<comment id='6' author='mli' date='2020-08-07T19:23:48Z'>
		Imputation method should likely be a parameter to the generator. The main reason I am converting to 0 at present is because that is how our downstream models handle it generally (RF/KNN), so making it something other than 0 would also mean we probably want to change it for RF/KNN as well.
		</comment>
	</comments>
</bug>