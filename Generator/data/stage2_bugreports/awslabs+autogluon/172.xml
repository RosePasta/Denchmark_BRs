<bug id='172' author='gradientsky' open_date='2019-12-22T21:34:12Z' closed_time='2020-01-09T22:47:09Z'>
	<summary>holdout_frac heuristics is too aggressive for multi-class classification</summary>
	<description>
The heuristics below is too aggressive for multi-class classification and can cause errors like these on large datasets: it cuts number of samples to 2500 and it might not work correctly when stratified with 38 classes:
&lt;denchmark-code&gt;ValueError: y_true and y_pred contain different number of classes 36, 37. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
 25 26 27 28 29 30 31 32 33 34 35 36]
&lt;/denchmark-code&gt;

autogluon/task/tabular_prediction/tabular_prediction.py:249
&lt;denchmark-code&gt;holdout_frac = max(0.01, min(0.1, 2500.0 / num_train_rows))
&lt;/denchmark-code&gt;

Workaround: explicitly specify holdout_frac in task.fit() call args.
	</description>
	<comments>
		<comment id='1' author='gradientsky' date='2019-12-23T21:25:48Z'>
		Agreed this seems too aggressive.
&lt;denchmark-link:https://github.com/Innixma&gt;@Innixma&lt;/denchmark-link&gt;
 what's the rationale for using such a limited holdout set when dataset size is large?
I still prefer 0.1 as a hard lower bound for holdout_frac.  As dataset size increases, we should really get improving validation estimates as well.  If we feel there is too much data being held-out for the given base models, then I think we should just return FULL versions of these models at the end.
Also, the most important factor to determine holdout_frac is IMO not the number of training examples, but rather things like: which ensembling strategy (in the absence of bagging) over how many base models will be utilized (need more holdout data to combat overfitting during ensembling), whether or not the base models will be retrained as FULL models at the end, whether label distribution is heavy-tailed (in regression or multiclass settings), etc.
		</comment>
		<comment id='2' author='gradientsky' date='2019-12-30T00:32:30Z'>
		&lt;denchmark-link:https://github.com/jwmueller&gt;@jwmueller&lt;/denchmark-link&gt;
 Just to clarify, the issue is not inherently because holdout is too small, it's due to a bug where threshold is not adjusted according to holdout_frac. This problem gets complex because if we drop too many classes due to high threshold, then we are at a net loss of training data compared to a higher holdout_frac percentage. This can be fixed by altering the train_val split code to force low frequency classes to have at least 1 row in the val. I will look to implementing this.
Regarding the question of if 2500 is enough in general, this is the code:
&lt;denchmark-code&gt;if holdout_frac is None:
    # Between row count 5,000 and 25,000 keep 0.1 holdout_frac, as we want to grow validation set to a stable 2500 examples
    if num_train_rows &lt; 5000:
        holdout_frac = max(0.1, min(0.2, 500.0 / num_train_rows))
    else:
        holdout_frac = max(0.01, min(0.1, 2500.0 / num_train_rows))
    if hyperparameter_tune:
        holdout_frac = min(0.2, holdout_frac*2)  # We want to allocate more validation data for HPO to avoid overfitting
&lt;/denchmark-code&gt;

As can be seen, we double the size of the validation data for HPO. I believe 5000 rows of validation data should be sufficient for most problems, but I'd be happy to be shown a counter-example. Additionally, for datasets &gt; 250,000 rows, we continue to grow the validation set. 500,000 rows of data would give 10,000 rows of validation.
The points you mention on other factors to consider are valid, but it's hard to know to what extent these should impact the holdout_frac value.
FULL models definitely change things. The current code does not train FULL models, and if we were to do that we would likely need to re-evaluate how much holdout data we need, and possibly even introduce a test set for scoring of the ensemble if we had multiple ensemble strategies.
The problem I see is these improvements likely provide very little benefit. For large datasets, _FULL doesn't matter as much because we are only using a small amount of data for validation relative to the training, and the time would be better spent on feature pruning / HPO. For small-medium datasets, the user is probably better off using bagging / stack ensembling as runtime will likely not be much of an issue.
		</comment>
		<comment id='3' author='gradientsky' date='2020-01-06T18:58:19Z'>
		Perhaps the most critical thing to do for now is as you say just ensure things always work whenever holdout_frac &amp; threshold are left unspecified by the user.
I agree considering FULL is low priority for now.  Re whether 5000 validation rows suffices, my point is more that it depends what adaptive decisions are being made using these rows.  Eg. if it's solely HPO, it is probably ok, but if it is the weighted ensembling with 100-1000s of base models (after HPO without bagging), then the resulting predictor would likely overfit.
		</comment>
		<comment id='4' author='gradientsky' date='2020-01-08T04:15:12Z'>
		This defect is fixed in &lt;denchmark-link:https://github.com/awslabs/autogluon/pull/177&gt;#177&lt;/denchmark-link&gt;
 , the code will no longer crash because of this regardless of how many kfolds / how low of holdout_frac is specified.
This commit fixes:
&lt;denchmark-link:https://github.com/awslabs/autogluon/commit/ba80c70e453e2a137f630a25df008c83b110ad82&gt;ba80c70&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='gradientsky' date='2020-01-09T22:47:06Z'>
		&lt;denchmark-link:https://github.com/awslabs/autogluon/pull/177&gt;#177&lt;/denchmark-link&gt;
 is merged. This issue is now fixed.
		</comment>
	</comments>
</bug>