<bug id='10' author='jwmueller' open_date='2019-09-12T18:38:20Z' closed_time='2019-12-17T22:26:54Z'>
	<summary>searcher get_config() may get stuck in infinite while loop</summary>
	<description>
Listing issue here for the record since I removed these comments from my skopt_searcher code to clean it up.
Issue:
get_config() may become stuck in infinite while loop. There is no termination condition to handle the case where all possible configs have already been tried
(should be inherited from BaseSearcher or Scheduler should automatically terminate).
In general, perhaps the BaseSearcher should by default revert to grid search whenever the search-space is sufficiently small.
	</description>
	<comments>
		<comment id='1' author='jwmueller' date='2019-09-12T18:41:17Z'>
		Thanks for adding this issue tracker. I will fix this in later PR :)
		</comment>
		<comment id='2' author='jwmueller' date='2019-11-20T21:37:51Z'>
		Potential fix mentioned in my PR:
Here is one potential fix below, but I am not sure it will be thread/parallel-safe so I didn't put it in yet.
&lt;denchmark-link:https://github.com/zhanghang1989&gt;@zhanghang1989&lt;/denchmark-link&gt;
 can you verify whether this would be safe or some kind of lock is needed?
In the base Searcher object, we keep instance variable num_possible_configs
and when the search-space is passed to scheduler, we update this space, eg. via:
&lt;denchmark-code&gt;if any of the hyperparameters are Real:
    self.num_possible_configs = np.inf
else:
   self.num_possible_configs = 0
   for each hyperparameter in search space:
           self.num_possible_configs += number of possible values of hyperparameter (assuming Categorical or bounded Integer, may be np.inf as well)
&lt;/denchmark-code&gt;

Then for each subclassed Searcher, we have:
&lt;denchmark-code&gt;def get_config():
   if self.num_possible_configs &lt;= 0:
      terminate search process
   ...
   new_config = ...
   self.num_possible_configs -= 1
   return new_config
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='jwmueller' date='2019-11-21T08:53:35Z'>
		Hello,
I suppose this is a problem if the search space is finite, and somehow all configs have been processed already?
Our searchers usually have loops of re-trials, at least when they sample at random, and if they fail after some number of re-trials, they throw an exception. I think this is a good behaviour, since the search process should stop. We also store training_history regularly to file, so we still have results if an exception is thrown.
I'd not deal with this in the BaseSearcher, to be honest.
And at least for the random searcher, I don't see why one should not allow it to sample config's several times, and then there is no infinite loop, right? The problem only comes in if you do not allow to sample a config &gt;1 times.
		</comment>
		<comment id='4' author='jwmueller' date='2019-11-21T08:55:10Z'>
		&lt;denchmark-link:https://github.com/jwmueller&gt;@jwmueller&lt;/denchmark-link&gt;
 Why would there be an infinite loop? There should in general be a stopping condition, which depends either on num_trials, or on wall clock time. The random searcher would happily sample configs &gt;1 times if the search space is small, until the budget is spent.
		</comment>
		<comment id='5' author='jwmueller' date='2019-11-22T18:29:46Z'>
		
@jwmueller Why would there be an infinite loop? There should in general be a stopping condition, which depends either on num_trials, or on wall clock time. The random searcher would happily sample configs &gt;1 times if the search space is small, until the budget is spent.

I agree there is implicit stopping condition based on num_trials or wall-clock time, so it's not strictly infinite.  But consider the case where the user has specified no search space at all, should we really run all trials up to num_trials in this case?
To see the issue in current RandomSearcher code, note that the while loop here: &lt;denchmark-link:https://github.com/awslabs/autogluon/blob/master/autogluon/searcher/searcher.py#L161&gt;https://github.com/awslabs/autogluon/blob/master/autogluon/searcher/searcher.py#L161&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;while pickle.dumps(new_config) in self._results.keys()
    new_config = self.configspace.sample_configuration().get_dictionary()
&lt;/denchmark-code&gt;

will just keep wasting computation indefinitely until stopping condition is met. It won't even start any additional trials due to trying to enforce that the new_config was not already previously seen and stored in self._results.
I agree it's not necessarily a bad idea to run a new trial for a previously already evaluated configuration to ensure it is actually robust (if there is lots of remaining computation).  However, I don't think the current searcher is really built to support this, due to: the infinite while loop above, and even if this were fixed, the current searcher would presumably just overwrite the previously-seen performance of this config with its performance in the new trial (Rather than say averaging them, or considering the conditional distribution, or considering which of the two trials of same config produced the superior model and keeping that one).
For something like deep learning which is affected by many sources of randomness (initialization, composition of the mini-batches, GPU-randomness in floating pt, etc), perhaps it's not bad to just store the best performance achieved under a config (and save the model corresponding to this config)?  However, average config-performance is more appropriate to store, if we retrain the model after HPO on the merged train+validation data.  Regardless of what is best, I don't think these issues are considered at all by our current base / random searcher.
I think your strategy sounds good, but which number do you store for config-performance in the case where you've tried a config multiple times? (and if you store all performance vals from repeated trials, how do you select at the end which config to use finally?)
		</comment>
		<comment id='6' author='jwmueller' date='2019-11-25T07:54:07Z'>
		Hi Jonas,
since I am still working on the AutoGluon package, I may not see more recent searchers. For the random searcher, allowing repeated sampling of a config makes sense, and that one does not maintain any data anyway, or at least does not make use of it.
The GP-based searchers we implement do not allow repeated eval of the same config, they maintain a blacklist internally. For those, you'd get an exception.
I still feel this should not be dealt with in the base class. You should only deal with things in the base class that are never going to be different in subclasses. Sometimes, I like Java for not allowing any code in interfaces!
		</comment>
		<comment id='7' author='jwmueller' date='2019-12-17T22:26:54Z'>
		This should be fixed in &lt;denchmark-link:https://github.com/mseeger&gt;@mseeger&lt;/denchmark-link&gt;
 's PR
		</comment>
	</comments>
</bug>