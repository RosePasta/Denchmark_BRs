<bug id='1592' author='rcurtin' open_date='2018-12-09T18:39:43Z' closed_time='2018-12-17T14:00:16Z'>
	<summary>Occasional failures for OneStepSarsaTest and OneStepQLearningTest</summary>
	<description>
I found that very occasionally, the tests AsyncLearningTest/OneStepSarsaTest and AsyncLearningTest/OneStepQLearningTest can fail.  Here are some examples (you will need to log into ci.mlpack.org before these links work):
&lt;denchmark-link:http://ci.mlpack.org/job/docker%20mlpack%20weekly%20build/armadillo_version=armadillo-8.300.3,boost_version=boost_1_49_0,buildmode=debug,compiler_version=gcc-6.3.0,label=docker/lastCompletedBuild/testReport/mlpackTest.AsyncLearningTest._home_jenkins_workspace_docker.mlpack.weekly/build_armadillo_version_armadillo-8/OneStepSarsaTest/&gt;http://ci.mlpack.org/job/docker%20mlpack%20weekly%20build/armadillo_version=armadillo-8.300.3,boost_version=boost_1_49_0,buildmode=debug,compiler_version=gcc-6.3.0,label=docker/lastCompletedBuild/testReport/mlpackTest.AsyncLearningTest._home_jenkins_workspace_docker.mlpack.weekly/build_armadillo_version_armadillo-8/OneStepSarsaTest/&lt;/denchmark-link&gt;

&lt;denchmark-link:http://ci.mlpack.org/job/docker%20mlpack%20weekly%20build/armadillo_version=armadillo-8.300.3,boost_version=boost_1_49_0,buildmode=debug,compiler_version=gcc-6.3.0,label=docker/lastCompletedBuild/testReport/mlpackTest.AsyncLearningTest._home_jenkins_workspace_docker.mlpack.weekly/build_armadillo_version_armadillo-8/OneStepQLearningTest/&gt;http://ci.mlpack.org/job/docker%20mlpack%20weekly%20build/armadillo_version=armadillo-8.300.3,boost_version=boost_1_49_0,buildmode=debug,compiler_version=gcc-6.3.0,label=docker/lastCompletedBuild/testReport/mlpackTest.AsyncLearningTest._home_jenkins_workspace_docker.mlpack.weekly/build_armadillo_version_armadillo-8/OneStepQLearningTest/&lt;/denchmark-link&gt;

At first I thought this was a memory error, but valgrind doesn't report anything wrong.  So I kept digging further and I realized that the problem is in this bit:
&lt;denchmark-code&gt;  auto measure = [&amp;rewards, &amp;pos, &amp;testEpisodes](double reward)
  { 
    size_t maxEpisode = 10000;
    if (testEpisodes &gt; maxEpisode)
      BOOST_REQUIRE(false);
    testEpisodes++;
    rewards[pos++] = reward;
    pos %= rewards.n_elem;
    // Maybe underestimated.
    double avgReward = arma::mean(rewards);
    Log::Debug &lt;&lt; "Average return: " &lt;&lt; avgReward
        &lt;&lt; " Episode return: " &lt;&lt; reward &lt;&lt; std::endl;
    if (avgReward &gt; 60)
      return true;
    return false;
  };
&lt;/denchmark-code&gt;

The failure comes because testEpisodes &gt; maxEpisode.  When I found a random seed that would fail, I found that even if I ran for a million or more episodes, it would never converge.  In fact, avgReward stayed around 8.5 and the reward of each episode was generally less than 10, for OneStepSarsaTest.  Typically, OneStepSarsaTest will converge to avgReward &gt; 60 in roughly 2000 episodes (based on the few trials I looked at).  It's about the same for OneStepQLearningTest, but convergence seems to happen in more like 1000 episodes.
So, this leads to a few questions, since I am not familiar with this part of the code:

Should we rerun OneStepSarsaTest or OneStepQLearningTest if they fail to converge?  Basically if it takes too many iterations, we just reset and try again, and then, e.g., require that it converges at least once in three trials.
Since these tests typically converge in ~2k iterations, should we reduce maxEpisode to 10k, not 100k?
Could something else be wrong here, or is it reasonable that OneStepSarsaTest or OneStepQLearningTest might fail once in a while?

&lt;denchmark-link:https://github.com/ShangtongZhang&gt;@ShangtongZhang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zoq&gt;@zoq&lt;/denchmark-link&gt;
: I think both of you know this code far better than me.  What do you think of the questions above?  I'm happy to write a patch to change the behavior of the tests (it's easy).
	</description>
	<comments>
		<comment id='1' author='rcurtin' date='2018-12-09T18:55:01Z'>
		
I think it is a good strategy to require it converges at least once in three trials
If we decide to require one convergence in three trials, I think it is reasonable to change maxEpsidoe to 10K
There is no guarantee that OneStepSarsa and  OneStepQLearning can converge. As you mentioned at the very beginning, this failure happens very occasionally, so I think it is very likely that we just unlucky to have a bad seed.

		</comment>
		<comment id='2' author='rcurtin' date='2018-12-11T17:08:09Z'>
		Thanks for the response!  I'll open a PR shortly that takes your advice.  I am testing the changes now; it seems much better.  My hope is to get the rate of failures below 1 in 1000. :)
		</comment>
		<comment id='3' author='rcurtin' date='2018-12-17T14:00:15Z'>
		&lt;denchmark-link:https://github.com/mlpack/mlpack/pull/1595&gt;#1595&lt;/denchmark-link&gt;
 is merged so I'll close this now.
		</comment>
	</comments>
</bug>