<bug id='2731' author='zsogitbe' open_date='2020-11-23T10:12:55Z' closed_time='2020-12-01T15:55:40Z'>
	<summary>ParallelSGD does not work as a drop-in replacement for SGD and it is not compatible with the rest of the code (does not compile)</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue description&lt;/denchmark-h&gt;

ParallelSGD does not work as a drop-in replacement for SGD and it is not compatible with the rest of the code (it does not compile). SGD uses only 1 thread and therefore it is slow. It would be interesting to have a faster optimization if ParallelSGD would work. It seems to me that ParallelSGD is not worked out well. What is the reasons for this? Is it not a good algorithm? If the ParallelSGD algorithm is not good, would it be possible to speed up the SGD optimizer with parallel processing in some way?
&lt;denchmark-h:h4&gt;Steps to reproduce&lt;/denchmark-h&gt;

Use for example a simple neural network example with SGD and replace the SGD optimizer with ParallelSGD (needs a modified function...)
&lt;denchmark-h:h4&gt;Expected behavior&lt;/denchmark-h&gt;

The code should compile well and the optimization should be much faster.
&lt;denchmark-h:h4&gt;Actual behavior&lt;/denchmark-h&gt;

The code does not compile (several errors with mentioning the wrong number of parameters on several places (e.g. the Evaluate function).
	</description>
	<comments>
		<comment id='1' author='zsogitbe' date='2020-12-01T13:42:45Z'>
		This looks like an issue that should be opened in the ensmallen repository.  Nonetheless, the &lt;denchmark-link:http://ensmallen.org/docs.html#hogwild-parallel-sgd&gt;documentation for ParallelSGD&lt;/denchmark-link&gt;
 points out that the API required for a function being optimized with  is slightly different than for regular separable differentiable functions, and suggests changes that you can make to your function's implementation that should allow the usage of .
The Hogwild! algorithm is most performant when the gradients of the objective function are sparse.  That may not be the case for a neural network in general, but it can work well for, e.g., a high-dimensional sparse logistic regression problem.  Check out the paper on the algorithm for more details.
Please include a minimum reproducible example with bug reports (so that we can compile that directly), as well as the exact errors that are being encountered.  Although you provided some directions about how to reproduce the issue, it would take a while to write the code and debug it, and there would also be no guarantee that we would even see the same issue you are reporting. üëç
		</comment>
		<comment id='2' author='zsogitbe' date='2020-12-01T14:12:15Z'>
		Thank you for your answer Ryan!
Would you recommend ParallelSGD for recurrent neural networks?
Please find an example project in attachment. This is a slightly modified former version of the RNN electricity consumption example. I have dropped in ParallelSGD instead of SGD. Things I have removed are signed with '//@-psgd' and things I have added are signed with '//@+psgd'. There are only a very few things changed. It would be interesting to see if this works if it will be able to compile.
&lt;denchmark-link:https://github.com/mlpack/mlpack/files/5623086/LSTMTimeSeriesUnivariatePSGD.zip&gt;LSTMTimeSeriesUnivariatePSGD.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='zsogitbe' date='2020-12-01T15:08:02Z'>
		I wouldn't---I don't expect an RNN (unless very specifically constructed) to have sparse gradients.  There may be other parallel SGD variants that could work for dense data and gradients but honestly I think that the best level of single-node parallelism for neural networks is not at the optimizer level but at the linear algebra level.  So if you are using OpenBLAS already then the large linear algebra operations should already be using multiple cores.
Let me try out the example code you sent...
		</comment>
		<comment id='4' author='zsogitbe' date='2020-12-01T15:33:54Z'>
		Ok, I see what's going on here.  The issue isn't actually anything with ParallelSGD; it's that the RNN class does not implement the sparse separable differentiable function API required by ParallelSGD.  As I mentioned earlier, it's not likely that an RNN will be able to make effective use of the Hogwild algorithm because the gradients will, in general, be fully dense.
In order to fix this issue, the RNN::Evaluate() and RNN::Gradient() methods would need to be adapted such that they could take an arbitrary matrix type as input.  However, that's quite an undertaking and given the reasons above I don't think it's worthwhile to do that right now.
At the same time, it's worth pointing out that &lt;denchmark-link:https://github.com/mlpack/mlpack/issues/290&gt;#290&lt;/denchmark-link&gt;
 is an issue that has been open for a long tine with the intention of fully templatizing mlpack's algorithms to work with any matrix type.  If/when that is done, then RNNs will work with Hogwild, but like I mentioned I don't think it would yield noticeable speedup even if it did work. 
		</comment>
		<comment id='5' author='zsogitbe' date='2020-12-01T15:55:40Z'>
		OK! I understand. I will close this issue.
		</comment>
	</comments>
</bug>