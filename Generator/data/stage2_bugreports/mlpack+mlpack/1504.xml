<bug id='1504' author='zsogitbe' open_date='2018-09-04T18:46:20Z' closed_time='2018-10-03T23:59:51Z'>
	<summary>BUG: The BatchNorm layer is not deserialized well.</summary>
	<description>
I was saving and then loading a trained ann network while testing and I have noticed that the network is serialized well but when deserialized the BatchNorm layer is not loaded well. After loading, the beta and gamma vector sizes were still the default (10) and the values were not loaded.
	</description>
	<comments>
		<comment id='1' author='zsogitbe' date='2018-09-04T19:25:45Z'>
		&lt;denchmark-link:https://github.com/zsogitbe&gt;@zsogitbe&lt;/denchmark-link&gt;
 Thanks for pointing this out. I suspect the reason might be because we're currently we're trying to  the aliases ( and  in this case) in place of the , as we generally do &lt;denchmark-link:https://github.com/mlpack/mlpack/blob/master/src/mlpack/methods/ann/layer/convolution_impl.hpp#L331-L332&gt;here&lt;/denchmark-link&gt;
. Can you please try that out and let us know if it works for you?
		</comment>
		<comment id='2' author='zsogitbe' date='2018-09-04T19:38:12Z'>
		I think that you will most probably need to do 'if (Archive::is_loading::value)' and 'set_size' for 'beta' and 'gamma' in the BatchNorm layer serialization! They are not deserialized/loaded well because the size is still the default... (the serialization/save is OK!). I am not sure how to get the right size at that place in the code because I am still new to mlpack but I am sure that more experienced users will know it immediately. I hope this helps!
		</comment>
		<comment id='3' author='zsogitbe' date='2018-09-05T06:29:29Z'>
		This pull request does not work either. I would like to politely suggest that you always test the code before committing a pull request in order to prevent the accidental degradation of the library.
I have also noticed that you have removed the serialization of beta and gamma entirely. Are you sure that you know what you are doing?
I do not have time to help you further with this issue because I am new to the library and I am just testing it.
		</comment>
		<comment id='4' author='zsogitbe' date='2018-09-05T07:41:19Z'>
		Hmm, weird. &lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
 Can you think of a reason why the serialization might not be working?
		</comment>
		<comment id='5' author='zsogitbe' date='2018-09-05T12:40:32Z'>
		
Are you sure that you know what you are doing?

I'm sure that &lt;denchmark-link:https://github.com/ShikharJ&gt;@ShikharJ&lt;/denchmark-link&gt;
 knows what he is doing---he has made substantial contributions to the library in the past year.
In this situation Shikhar is right: the gamma and bias matrices are simply aliases on top of the weights matrix.  The ANN code is complex so it's not necessarily easy to read, and the way that the network interacts with each layer is also complex.  For the sake of serialization, it turns out that the network itself holds all the weights in each layer as a contiguous matrix and that will be serialized, so we don't need to do that in the layer itself.
However it seems like maybe there are some other elements that aren't being serialized: size, eps, and stats jump out at me as possibilities.  Another possibility is that stats is being reset in Reset(), but for the batch normalization layer we should not be resetting that information when we serialize/deserialize the matrix.  Instead I think that should only happen at initialization time.
&lt;denchmark-link:https://github.com/zsogitbe&gt;@zsogitbe&lt;/denchmark-link&gt;
: you're right that the PR that Shikhar proposed is not tested.  (If it was, you would not have encountered the bug at all.  But perfect testing coverage is not necessarily possible with such complex software...)  I think that a nice thing to do might be to work in your failing code as a test case, if you can supply simple code to build the network and then the code after serialization that fails.
		</comment>
		<comment id='6' author='zsogitbe' date='2018-09-05T13:17:38Z'>
		Thank you for your answer and clarification Ryan! I am just starting to test mlpack. The BatchNorm layer does not improve my test cases but makes them worse, therefore I will not use it further for the moment. I just wanted to let you know that there is something wrong, I think that you are right about the eps or stats reset. I advice you (the one who will try to solve the problem) in this case to save the xml model and look at the output, this is already a first easy test. I am sorry that I can not help you more in this at this time. I do not know the software yet well. If I will decide that I will use it (after some tests) then I will definitely help later!
I understand your intentions and I also find it very nice that you protect Shikhar! I however think that you should have more discipline while developing the software to always make at least one test before committing it. This will also reduce the final bugs in the software and it is easier to test a small part than the whole complex thing! But I of course understand that there can be bugs in any software, so consider my remarks just as help.
		</comment>
		<comment id='7' author='zsogitbe' date='2018-09-05T13:24:05Z'>
		Right, thanks for the extra details. If you are able to provide the C++ code you used to build the network this can help us figure out the right test case.
But to be sure no code in mlpack is committed without testing.  The PR Shikhar opened is not merged, and would not be approved by others unless it were fully tested and known to work.  But the first step of testing that was chosen here was to check with the user if the new code works, which I think is a reasonable approach, and then we can figure out how to make that a part of our test suite to ensure that later changes do not break the fix here. :)
		</comment>
		<comment id='8' author='zsogitbe' date='2018-09-05T13:38:20Z'>
		You can use a very simple example you probably have already for the testing. Just build a simple nn, train it, save it, load it and try to predict. You could also make an automatic test case from this which could for example compare the prediction results before and after loading the model. One BatchNorm layer would be sufficient for the test:
FFN &lt;NegativeLogLikelihood&lt;&gt;, RandomInitialization&gt; model;
model.Add&lt;Linear&lt;&gt; &gt;(trainX.n_rows, H1);
model.Add&lt;BatchNorm&lt;&gt; &gt;();
model.Add&lt;ReLULayer&lt;&gt; &gt;();
model.Add&lt;Linear&lt;&gt; &gt;(H1, H2);
model.Add&lt;ReLULayer&lt;&gt; &gt;();
model.Add&lt;Linear&lt;&gt; &gt;(H2, nClasses);
model.Add&lt;LogSoftMax&lt;&gt; &gt;();
SGD  optimizer(...
train the model CYCLES times
save the model with: mlpack::data::Save(...
load the model with: mlpack::data::Load(...
predict
		</comment>
		<comment id='9' author='zsogitbe' date='2018-10-03T23:59:51Z'>
		Ok, I think that we can mark this as resolved since &lt;denchmark-link:https://github.com/mlpack/mlpack/pull/1508&gt;#1508&lt;/denchmark-link&gt;
 is merged now.  Thanks for the bug report!  
		</comment>
	</comments>
</bug>