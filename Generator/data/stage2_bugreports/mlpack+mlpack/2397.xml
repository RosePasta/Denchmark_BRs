<bug id='2397' author='menkaur' open_date='2020-05-08T00:36:32Z' closed_time='2020-06-14T12:58:10Z'>
	<summary>How to gracefully exit agent training?</summary>
	<description>
I've been playing with the CartPole example. Here's the modified code:
&lt;denchmark-code&gt;std::ofstream myfile;
myfile.open("history.txt");

//mlpack::ann::
// Set up the network.
FFN&lt;MeanSquaredError&lt;&gt;, GaussianInitialization&gt; model(MeanSquaredError&lt;&gt;(), GaussianInitialization(0, 0.01));
model.Add&lt;Linear&lt;&gt;&gt;(4, 128);
model.Add&lt;LeakyReLU&lt;&gt;&gt;();
model.Add&lt;Linear&lt;&gt;&gt;(128, 128);
model.Add&lt;LeakyReLU&lt;&gt;&gt;();
model.Add&lt;Linear&lt;&gt;&gt;(128, 128);
model.Add&lt;LeakyReLU&lt;&gt;&gt;();
model.Add&lt;Linear&lt;&gt;&gt;(128, 2);

AggregatedPolicy&lt;GreedyPolicy&lt;CartPole&gt;&gt; policy({ GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.1),
                                                 GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.01),
                                                 GreedyPolicy&lt;CartPole&gt;(0.7, 5000, 0.5) },
    arma::colvec("0.4 0.3 0.3"));

const int stepLimit = 300;

TrainingConfig config;
config.StepSize() = 0.01;
config.Discount() = 0.9;
config.TargetNetworkSyncInterval() = 100;
config.ExplorationSteps() = stepLimit;
config.DoubleQLearning() = true;
config.StepLimit() = stepLimit;

OneStepQLearning&lt;CartPole, decltype(model), ens::VanillaUpdate, decltype(policy)&gt;
    agent(std::move(config), std::move(model), std::move(policy));

arma::vec returns(100, arma::fill::zeros);
size_t position = 0;
size_t episode = 0;
int executionPosition = -1;

auto measure = [&amp;](double episodeReturn)
{
    returns[position++] = episodeReturn;
    position = position % returns.n_elem;
    episode++;

    auto mean = arma::mean(returns);
	
    if (mean&gt;stepLimit*.95&amp;&amp;episode&gt; returns.n_elem)
    {
        std::cout &lt;&lt; "returning because mean=" &lt;&lt; mean &lt;&lt; std::endl;
        return true;
    }

    std::cout &lt;&lt; "["&lt;&lt; executionPosition &lt;&lt;"," &lt;&lt; episode&lt;&lt;"]"
        &lt;&lt; "; Episode Return: " &lt;&lt; episodeReturn
        &lt;&lt; "; Average Return: " &lt;&lt; mean 
    	&lt;&lt;std::endl;

    myfile &lt;&lt; episodeReturn &lt;&lt; std::endl;
    myfile.flush();
    return false;
};

agent.Train(measure);
myfile.close();`
&lt;/denchmark-code&gt;

The idea behind it was running the training until majority of items in the sample will balance for max steps. It turns out, when running training in this manner, something else is happening: for around the first 6000 iterations, the model keeps gradually improving, and than improvement stops and it oscillates at low values (can be seen by studying the content of history.txt).
My question is - is this expected behaviour?  What can be done to make the model keep gradually improving? Is there a better way to stop training the agent?
Update:
I'm having this issue with mlpack installed with vcpkg on Windows.
When I'm running latest version of the software on Linux, it doesn't seem to have the same issue.
I still am looking for a suggestion about what is a graceful way to exit the training
	</description>
	<comments>
		<comment id='1' author='menkaur' date='2020-05-08T11:28:40Z'>
		We will have to look into the windows issue, about the exit strategy, what about checking the reward? See 


mlpack/src/mlpack/tests/async_learning_test.cpp


         Line 81
      in
      6a64926






 auto measure = [&amp;rewards, &amp;pos, &amp;testEpisodes](double reward) 




 for an example?
		</comment>
		<comment id='2' author='menkaur' date='2020-05-08T11:52:47Z'>
		Yes, this is approximately what I'm doing
		</comment>
		<comment id='3' author='menkaur' date='2020-06-07T11:59:04Z'>
		This issue has been automatically marked as stale because it has not had any recent activity.  It will be closed in 7 days if no further activity occurs.  Thank you for your contributions! üëç
		</comment>
	</comments>
</bug>