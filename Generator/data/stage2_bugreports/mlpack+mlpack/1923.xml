<bug id='1923' author='saksham189' open_date='2019-06-14T13:46:15Z' closed_time='2019-06-20T03:08:26Z'>
	<summary>Backward() implemented incorrectly in BatchNorm Layer.</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue description&lt;/denchmark-h&gt;

Many of the layers use the numerical gradient test to verify the implementation of the Gradient function. However, the test does not verify the implementation of Backward function because there is no layer before the layer that is being tested.
For example the model used in GradientBatchNormTest is:
&lt;denchmark-code&gt;model = new FFN&lt;NegativeLogLikelihood&lt;&gt;, NguyenWidrowInitialization&gt;();
model-&gt;Predictors() = input;
model-&gt;Responses() = target;
model-&gt;Add&lt;IdentityLayer&lt;&gt; &gt;();
model-&gt;Add&lt;BatchNorm&lt;&gt; &gt;(10);
model-&gt;Add&lt;Linear&lt;&gt; &gt;(10, 2);
model-&gt;Add&lt;LogSoftMax&lt;&gt; &gt;();
&lt;/denchmark-code&gt;

The test passes with this model. However if the model is changed by adding a linear layer before the BatchNorm layer, the test will start failing:
&lt;denchmark-code&gt;model = new FFN&lt;NegativeLogLikelihood&lt;&gt;, NguyenWidrowInitialization&gt;();
model-&gt;Predictors() = input;
model-&gt;Responses() = target;
model-&gt;Add&lt;IdentityLayer&lt;&gt; &gt;();
model-&gt;Add&lt;Linear&lt;&gt; &gt;(10, 10);
model-&gt;Add&lt;BatchNorm&lt;&gt; &gt;(10);
model-&gt;Add&lt;Linear&lt;&gt; &gt;(10, 2);
model-&gt;Add&lt;LogSoftMax&lt;&gt; &gt;();
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Steps to reproduce&lt;/denchmark-h&gt;

Change the model by adding a linear layer before BatchNorm and run the GradientBatchNormTest test.
&lt;denchmark-h:h4&gt;Expected behavior&lt;/denchmark-h&gt;

The test should not fail if the Backward function had been implemented correctly.
&lt;denchmark-h:h4&gt;Actual behavior&lt;/denchmark-h&gt;

The numerical gradient test fails and the Backward function is implemented incorrectly.
Most of the numerical gradient tests do not have a parametric layer being the layer being tested. So the implementation of the Backward function is never really tested.
	</description>
	<comments>
		<comment id='1' author='saksham189' date='2019-06-14T15:00:08Z'>
		In the following line in Backward for BatchNorm layer:
const arma::mat inputMean = input.each_col() - mean;
Here input parameter of the function is actually the output of the layer. We should be using the actual input to the layer to fix the implementation.
		</comment>
	</comments>
</bug>