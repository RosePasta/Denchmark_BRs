<bug id='2069' author='zsogitbe' open_date='2019-10-31T15:55:35Z' closed_time='2019-12-17T22:20:14Z'>
	<summary>Recurrent Network Test</summary>
	<description>
I am testing the recurrent neural network code and there are some things which are not clear in the included testing code:


In RNNSineTest (regression) it is stated at the end that "Adjust the vectors for comparison, as the prediction is one step ahead.". Could you please tell me why the prediction is one step ahead, what causes this? I do not see immediately that this is caused by the way how the data is defined. I do not see lagging defined in the data. Is this caused by the algorithm?


In SequenceClassificationBRNNTest (classification) there is no mention about the prediction being one step ahead! Why?


In SequenceClassificationBRNNTest (classification) the prediction of the last slice is taken: "prediction.slice(rho - 1).col(i)". Is this all right with all data points? More precisely, is this OK for the first 'rho' number of data points?


Thank you very much for your explanation.
	</description>
	<comments>
		<comment id='1' author='zsogitbe' date='2019-11-01T04:20:49Z'>
		Hey &lt;denchmark-link:https://github.com/zsogitbe&gt;@zsogitbe&lt;/denchmark-link&gt;
, I think I can answer the questions (but if someone else has a better answer they should correct me :)).

In RNNSineTest (regression) it is stated at the end that "Adjust the vectors for comparison, as the prediction is one step ahead.". Could you please tell me why the prediction is one step ahead, what causes this?

I think that the meaning here has to do with what's being predicted.  For the noisy sine input, the task is to predict the next input.  If you take a look at GenerateNoisySinRNN(), you can see that the output is the sine's value rho time steps ahead.  (So, actually, it is rho steps, not 1.)
So, the key here is that in the sine test, the goal is to predict a future input, meaning that the labels are actually just a time-shifted version of the data.  It would have been possible to use labels, too, but that isn't how the code is written.  (Either works.)

In SequenceClassificationBRNNTest (classification) there is no mention about the prediction being one step ahead! Why?

In this case, the generated labels from GenerateNoisySines are used, because the labels (which are 0 or 1) can't easily be constructed from the data, and are easier to get from GenerateNoisySines.

In SequenceClassificationBRNNTest (classification) the prediction of the last slice is taken: "prediction.slice(rho - 1).col(i)". Is this all right with all data points? More precisely, is this OK for the first 'rho' number of data points?

So, in these cases, we are using the RNN to predict the class only after all the elements in the sequence are seen.  When we look at prediction.slice(j).col(i), that is going to tell us what the label prediction is after j + 1 steps of the input sequence have been seen.  Since we want to look only at the prediction after the entire input sequence is seen, then we can simply look at prediction.slice(rho - 1).col(i).
		</comment>
		<comment id='2' author='zsogitbe' date='2019-11-01T08:04:45Z'>
		Thank you Ryan! I will try to summarize what you have said. Please correct me if I am wrong. There are also a few more questions and suggestions ;).


The code
labels.subcube(0, i, 0, outputSteps - 1, i, 0) = y.rows(i * rho + rho, i * rho + rho + outputSteps - 1);
should be changed to
labels.subcube(0, i, 0, outputSteps - 1, i, 0) = y.rows(i * rho + 1, i * rho + rho + outputSteps - 1);
in order to be one step ahead and not rho steps ahead!? I guess that it does not have any sense to go rho steps ahead.


It is not entirely clear what outputSteps does. If it is &gt;1 then there will be more labels used per step. Does this mean that we can get outputSteps number of future predictions in one shot?


My second question is not yet entirely clear:

In SequenceClassificationBRNNTest (classification) there is no mention about the prediction being  one step ahead! Why?

You generate two separate signals and you label them separately and put them after each other per sequence. Then in the prediction result you look at the last slice. Is this rho steps ahead? You compare that with the target. Is the target then rho steps back compared to the prediction!?
(note: I think that what would make more sense is that there is a sequence of values which 'cause' a specific label in the next time step. prediction one step ahead.)


The tests with these generate functions are confusing. I think that it would be better to use hard coded data in the source code or read in some simple data.


So it does not have any sense to look at values before the last slice (rho) because not all elements in the sequence are seen yet. We always have to look at the last slice with index rho-1 when taking the results. This means also that we need always minimum rho number of records before we can do anything with an RNN and the first rho-1 number of records can not be predicted.


I have similar remarks and questions about the TimeSeries tutorials in the 'model' section of the mlpack distribution (see below). Note: the TimeSeries tutorials need to be cleaned up because there are a lot of comments and instructions which are copied from somewhere else and which are wrong.


“TimeSeries-Univariate”: the labels in CreateTimeSeriesData are made with:
y.subcube(span(), span(i), span()) = dataset.submat(span(), span(i+1, i+rho));. This selects all features as label. Should this not only select the energy consumption as label?


“TimeSeries-Univariate”: If I run a prediction after training the model (with the above correction and using 10 hidden units instead of 4) I get a matrix (predOut) with ‘rho’ columns and ‘nsamples-rho’ rows. I obviously want to predict the next value of the label (energy consumption). What is the next value? The last value in the last slice? This is the aim of this tutorial, why is this not shown?


“TimeSeries-Univariate”: The prediction output matrix is always rho number of records shorter than the input. How to synchronize this with the input (target)!? And how the lagging of the labels influences the synchronization (takes the label of the next record instead of the current record)?


Similar remarks about the "TimeSeries-Multivariate" tutorial as above. The aim of the tutorial is not solved. Where is the predicted next value?


Sorry for the many questions! I hope that you find valuable that I am thoroughly testing the code.
		</comment>
		<comment id='3' author='zsogitbe' date='2019-11-10T20:49:03Z'>
		Sorry, I think that I misunderstood the code in GenerateNoisySinRNN() a little bit.  To be more specific, the data we are generating works like this:

We generate rho time steps of the noisy sine sequence.
The output is the result after rho time steps.

So, essentially, for this problem, we have rho time steps of data, followed by a single output, which is the data at time step rho + 1.
The outputSize parameter controls how many sequences of length rho we generate; each of these has a corresponding output label in labels.
That should clarify the first two questions.


&lt;...&gt; Is the target then rho steps back compared to the prediction!?


No, the target is the time step following all of the input data.  This is the same way RNNs are generally used to predict with sequences.  The code is a little bit convoluted, but to me it does appear correct.


The tests with these generate functions are confusing. I think that it would be better to use hard coded data in the source code or read in some simple data.


I don't necessarily disagree, but one disadvantage is that it makes the repository larger (for this data, the effect is probably quite small).  If you wanted to make this change and open a PR, I don't have a huge issue with it.  But we should at least leave the code that generated the data in the repository, even if it's commented out.


So it does not have any sense to look at values before the last slice (rho) because not all elements in the sequence are seen yet. We always have to look at the last slice with index rho-1 when taking the results. This means also that we need always minimum rho number of records before we can do anything with an RNN and the first rho-1 number of records can not be predicted.


Well, an RNN does give outputs at every time step.  But when we are training, we have the option to use the RNN in one of two ways: (1) we read in the whole sequence, and then expect an output; once the output is compared with the true value, we backpropagate error from that last time step backwards; (2) as we read in each time step of data, we have an associated true value, and we compare the RNN's output with that true value and backpropagate error at every time step.  Control between these two modes of training is accomplished via the single parameter to the RNN class.  In the tests here, we are using the first mode.
Now, when we are predicting with the RNN, we will get an output at every time step.  But we as users need to be aware of how we trained the model; if we trained using strategy (1), then we aren't interested in any time step's output except the last.  If we trained using strategy (2), then we are probably interested in every time step's output.


... I have similar remarks and questions about the TimeSeries tutorials in the 'model' section of the mlpack distribution (see below). Note: the TimeSeries tutorials need to be cleaned up because there are a lot of comments and instructions which are copied from somewhere else and which are wrong.


Yeah, I agree here, I think that the time series tutorials in the models/ repo should be cleaned up.  We could open an issue in that repository if you like, or, alternately, if you'd like to take a pass on them feel free.


“TimeSeries-Univariate”: the labels in CreateTimeSeriesData are made with:
y.subcube(span(), span(i), span()) = dataset.submat(span(), span(i+1, i+rho));. This selects all features as label. Should this not only select the energy consumption as label?


I think I already answered this in an email.  dataset only has one row anyway, so this should be selecting only one row.  If that's not the case, there's a bug in the tutorial, I think.


... What is the next value? The last value in the last slice? This is the aim of this tutorial, why is this not shown?


Right, exactly, the last value in the last slice should be the prediction.  It was probably overlooked and I agree that should be highlighted.


“TimeSeries-Univariate”: The prediction output matrix is always rho number of records shorter than the input. How to synchronize this with the input (target)!? And how the lagging of the labels influences the synchronization (takes the label of the next record instead of the current record)?


This one I am not sure on; I have only been tangentially involved with the RNN code so I am not sure exactly how it will behave in these situations.  I think that, if you worked on a simple reproducible example where you pass in a sequence of length t and you are getting back far fewer responses, this is an issue that we should open.  I think that ideally, slice i of the responses should correspond to the output at time step i for the given input.


Similar remarks about the "TimeSeries-Multivariate" tutorial as above. The aim of the tutorial is not solved. Where is the predicted next value?


Good question; if you want to take the time to review or update those tutorials so they are more sensible, it would be greatly appreciated.

Sorry for the many questions! I hope that you find valuable that I am thoroughly testing the code.

Oh, no worries!  It is definitely valuable, so long as you are okay with the fact that I may not always have time to fix everything. :)  You should definitely feel empowered to contribute fixes too. 👍
		</comment>
		<comment id='4' author='zsogitbe' date='2019-12-10T21:20:11Z'>
		This issue has been automatically marked as stale because it has not had any recent activity.  It will be closed in 7 days if no further activity occurs.  Thank you for your contributions! 👍
		</comment>
	</comments>
</bug>