<bug id='2713' author='zsogitbe' open_date='2020-11-13T22:06:56Z' closed_time='2021-01-19T22:13:00Z'>
	<summary>Possible Bug in the Recurrent Neural Network (RNN) implementation</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue description&lt;/denchmark-h&gt;

While training a RNN like this
&lt;denchmark-code&gt;for (int i = 0; i &lt;= CYCLES; i++)
{
   model.Train(...
   model.Predict(...
}
&lt;/denchmark-code&gt;

the prediction step increases the size of the final model. If you use the training data for prediction then the size of the final model may become several times bigger just because of the Prediction step. For example, with a small test dataset in the prediction step you get a final model size of 2MB and if you use the full training dataset you get 20MB. It is not logical to me why the prediction step increase the size of the trained model.
&lt;denchmark-h:h4&gt;Your environment&lt;/denchmark-h&gt;


version of mlpack: last
operating system: Windows 10
compiler: VS2017
version of dependencies (Boost/Armadillo): last
any other environment information you think is relevant: -

&lt;denchmark-h:h4&gt;Steps to reproduce&lt;/denchmark-h&gt;

see above
&lt;denchmark-h:h4&gt;Expected behavior&lt;/denchmark-h&gt;

The Prediction step should not influence the size of the model.
&lt;denchmark-h:h4&gt;Actual behavior&lt;/denchmark-h&gt;

see above
	</description>
	<comments>
		<comment id='1' author='zsogitbe' date='2020-11-14T19:37:48Z'>
		&lt;denchmark-link:https://github.com/zsogitbe&gt;@zsogitbe&lt;/denchmark-link&gt;
 thanks for the report.  I think this may be very simple to fix.  &lt;denchmark-link:https://github.com/zoq&gt;@zoq&lt;/denchmark-link&gt;
 should we clear  and  and other internal storage of the FFN at the end of ?  (and RNN too I assume)
		</comment>
		<comment id='2' author='zsogitbe' date='2020-11-18T13:37:58Z'>
		Yes, that sounds like a good approach.
		</comment>
		<comment id='3' author='zsogitbe' date='2020-11-28T20:01:11Z'>
		&lt;denchmark-link:https://github.com/zsogitbe&gt;@zsogitbe&lt;/denchmark-link&gt;
 can you tell us about the network structure you are using?  I actually think my initial diagnosis was not correct here---I think some layers may be (incorrectly) caching data, but I want to make sure I actually catch the situation you're reporting.  If you can paste some code that, when run, reproduces the issue, that would be best. 
		</comment>
		<comment id='4' author='zsogitbe' date='2020-11-29T12:55:48Z'>
		Hi Ryan, you can use any of the RNN examples (stock or electricity for this). You will need to use the 'old' way of training per cycle with which you can have a prediction step after each cycle. We need this prediction step in order to see the evolution of the MSQ (regression) or accuracy (classification) and not only the 'loss'... Case 1: train the model without a prediction step and save the model. Case 2: train the model with a prediction step (test) in each cycle and by using the whole training dataset. You will see that the model size in Case 1 will be very small and in Case 2 it will be several MB's larger. If you save the model in text form instead of binary then you will see that the prediction steps add the multiple of weights to the model. The prediction step must not modify the model in any way. I hope that this helps to track down the problem.
		</comment>
		<comment id='5' author='zsogitbe' date='2020-12-13T20:43:49Z'>
		Thanks for the report, I'll take a look into the issue.
		</comment>
		<comment id='6' author='zsogitbe' date='2021-01-12T21:12:55Z'>
		This issue has been automatically marked as stale because it has not had any recent activity.  It will be closed in 7 days if no further activity occurs.  Thank you for your contributions! üëç
		</comment>
	</comments>
</bug>