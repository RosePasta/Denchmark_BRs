<bug id='2226' author='shrit' open_date='2020-02-19T16:15:40Z' closed_time='2020-03-17T13:21:36Z'>
	<summary>Verify that ann is reading the entire dataset at least once</summary>
	<description>
Hello,
In the actual implementation of the Artificial Neural Network, the users are using the Train() function to train a model. We pass a dataset into the train function,  but we are not sure that in one epoch we are passing over the entire dataset.
It would be very appropriate to print a warning telling the user that the epoch did not pass over the entire dataset, asking to modify the maxIteration() to be at the same size as the numbers of columns of the dataset matrix.
	</description>
	<comments>
		<comment id='1' author='shrit' date='2020-02-20T13:44:58Z'>
		Hi, &lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 I would like to move forward with this issue. Any hint from your side on how to move forward with it would be very helpful. Thanks.
		</comment>
		<comment id='2' author='shrit' date='2020-02-20T13:55:30Z'>
		&lt;denchmark-link:https://github.com/gaurav-singh1998&gt;@gaurav-singh1998&lt;/denchmark-link&gt;
 You can choose any dataset that can be trained with neural networks, then you reduce the  parameter, and monitor how the training process using  callback. If you choose a small  much smaller than the dataset size, you will notice that the training will stop before completing the epoch.
		</comment>
		<comment id='3' author='shrit' date='2020-02-22T04:27:33Z'>
		Can I take up this problem? Can I work on this problem?
		</comment>
		<comment id='4' author='shrit' date='2020-02-22T04:29:50Z'>
		Hey, &lt;denchmark-link:https://github.com/gauthampkrishnan&gt;@gauthampkrishnan&lt;/denchmark-link&gt;
 I am currently working on this. Can you take another issue? Thanks.
		</comment>
		<comment id='5' author='shrit' date='2020-02-22T04:31:21Z'>
		
Hey, @gauthampkrishnan I am currently working on this. Can you take another issue? Thanks.

&lt;denchmark-link:https://github.com/gaurav-singh1998&gt;@gaurav-singh1998&lt;/denchmark-link&gt;
 yes ofcourse all the best
		</comment>
		<comment id='6' author='shrit' date='2020-02-22T04:32:49Z'>
		Interested in a collaboration? &lt;denchmark-link:https://github.com/Gaura&gt;@Gaura&lt;/denchmark-link&gt;



Hey, @gauthampkrishnan I am currently working on this. Can you take another issue? Thanks.

@gaurav-singh1998 yes ofcourse all the best

interested in collaboration?
		</comment>
		<comment id='7' author='shrit' date='2020-02-22T14:50:16Z'>
		Hi, &lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 sorry for the late reply, got busy because of college mid-sem exams. I have been trying to replicate the issue you mentioned. Code regarding the same can be found &lt;denchmark-link:https://ideone.com/bZH5Xx&gt;here&lt;/denchmark-link&gt;
. The dataset used was generated with the help of python interpreter and the code regarding it can be found &lt;denchmark-link:https://ideone.com/a3wbW0&gt;here&lt;/denchmark-link&gt;
. I wanted to know if I am on the correct path with this and if this is correct then how to use the Progressbar() callback in 'Train()' method of Perceptron object?
		</comment>
		<comment id='8' author='shrit' date='2020-02-22T14:58:12Z'>
		I am not sure that perceptrons accept callbacks. You can use the code I have used in this issue &lt;denchmark-link:https://github.com/mlpack/mlpack/issues/2073#issuecomment-549080270&gt;#2073 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='shrit' date='2020-02-23T13:41:39Z'>
		Hey, &lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 I was going through the issue you mentioned and was able to understand the issue present. So, according to me, there has to be a warning in &lt;denchmark-link:https://github.com/mlpack/mlpack/blob/master/src/mlpack/methods/ann/ffn.hpp&gt;ffn.hpp&lt;/denchmark-link&gt;
 telling the user to pass  value the same as the number of columns in the dataset? Please correct me if I am wrong
		</comment>
		<comment id='10' author='shrit' date='2020-02-23T13:49:56Z'>
		&lt;denchmark-link:https://github.com/gaurav-singh1998&gt;@gaurav-singh1998&lt;/denchmark-link&gt;
 You understood it right, this is exaclty what has to be done.
		</comment>
		<comment id='11' author='shrit' date='2020-02-23T13:54:39Z'>
		Thanks, &lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 for the clarification will open a PR by tomorrow :)
		</comment>
		<comment id='12' author='shrit' date='2020-02-23T17:04:00Z'>
		Hey, &lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 one final query. Can you tell what should the warning say and should the same thing to be done with rnn_impl.hpp?
		</comment>
		<comment id='13' author='shrit' date='2020-02-23T20:25:09Z'>
		&lt;denchmark-link:https://github.com/gaurav-singh1998&gt;@gaurav-singh1998&lt;/denchmark-link&gt;
 You can add something as follows: 
You can modify this warning as you like, but it has to tell the user about these points.
		</comment>
		<comment id='14' author='shrit' date='2020-02-24T12:22:09Z'>
		Hi, &lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zoq&gt;@zoq&lt;/denchmark-link&gt;
 I have incorporated the warning message in ,  and  but if we call the  function inside a for loop the warning gets printed multiple times. Is there a way to tackle it?
		</comment>
		<comment id='15' author='shrit' date='2020-02-24T13:43:58Z'>
		&lt;denchmark-link:https://github.com/gaurav-singh1998&gt;@gaurav-singh1998&lt;/denchmark-link&gt;
 The  function itself have a loop inside (in the optimizer part), why we would have to put it inside a loop?
		</comment>
		<comment id='16' author='shrit' date='2020-02-24T13:46:45Z'>
		&lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 like in the example you used in the comment you did,
for(int i=0; i&lt;300; i++)
{
model.Train(trainX, trainY, optimizer, ens::PrintLoss(),
ens::ProgressBar(),
ens::StoreBestCoordinatesarma::mat());
&lt;denchmark-code&gt;optimizer.ResetPolicy() = false;
&lt;/denchmark-code&gt;

}
Because of this the warn message is getting multiple times. Can you review &lt;denchmark-link:https://github.com/mlpack/mlpack/pull/2233&gt;#2233&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='17' author='shrit' date='2020-02-24T14:15:29Z'>
		&lt;denchmark-link:https://github.com/gaurav-singh1998&gt;@gaurav-singh1998&lt;/denchmark-link&gt;
  Exactly. I agree with you.
When I posted this issue, it was back in November. Callbacks where just begin merged in mlpack.
Later on, I understood we do not have to use outside  loop. In fact, it is totally possible to achieve the objective of the optimization without outside outer loop.
Before callbacks, we had to put iterations since we did not observe in real-time either the training loss or the validation loss, so you had to test on a validation set after each return of , and write by ourself an Early Stopping locally based on the .
&lt;denchmark-link:https://github.com/zoq&gt;@zoq&lt;/denchmark-link&gt;
 What do you think? Do we have to put an outside loop on the train function nowadays?
		</comment>
		<comment id='18' author='shrit' date='2020-02-25T22:34:53Z'>
		
When I posted this issue, it was back in November. Callbacks where just begin merged in mlpack.
Later on, I understood we do not have to use outside for loop. In fact, it is totally possible to achieve the objective of the optimization without outside outer loop.
Before callbacks, we had to put iterations since we did not observe in real-time either the training loss or the validation loss, so you had to test on a validation set after each return of Train(), and write by ourself an Early Stopping locally based on the Evaluate().
@zoq What do you think? Do we have to put an outside loop on the train function nowadays?

You are absolutely correct, the callbacks should make the for loop obsolete.
		</comment>
		<comment id='19' author='shrit' date='2020-03-17T12:59:32Z'>
		Hi, &lt;denchmark-link:https://github.com/shrit&gt;@shrit&lt;/denchmark-link&gt;
 since this issue has been solved. I think we should go ahead and close it. Thanks.
		</comment>
	</comments>
</bug>