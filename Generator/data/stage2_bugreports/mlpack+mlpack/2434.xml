<bug id='2434' author='ibalampanis' open_date='2020-05-29T13:28:23Z' closed_time='2020-08-18T07:35:38Z'>
	<summary>Problem with Embedding layer definition in C++</summary>
	<description>
I face the following problem. I want to build an LSTM model to make Sentiment Analysis. In C++, I use this wonderful library. Firstly, to understand the architecture I approached the problem via python, and Keras advised from some tutorials. This &lt;denchmark-link:https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456&gt;tutorial&lt;/denchmark-link&gt;
 had this code:
&lt;denchmark-code&gt;    from keras.models import Sequential
    from keras.layers import Dense, Embedding, LSTM, GRU
    from keras.layers.embeddings import Embedding

    EMBEDDING_DIM = 100
   
    print('Build model...')

    model = Sequential()
    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length())
    model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
&lt;/denchmark-code&gt;

In my code, in C++, I have written this block:
&lt;denchmark-code&gt;    model.Add&lt;IdentityLayer&lt;&gt; &gt;();
    model.Add&lt;Embedding&lt;&gt; &gt;(vocabSize, embedSize);
    model.Add&lt;LSTM&lt;&gt; &gt;(inputSize, lstmCells, maxRho);
    model.Add&lt;Dropout&lt;&gt; &gt;(0.5);
    model.Add&lt;ReLULayer&lt;&gt; &gt;();
    model.Add&lt;Linear&lt;&gt; &gt;(lstmCells, outputSize);
&lt;/denchmark-code&gt;

The output console gives me this error:
&lt;denchmark-code&gt;error: conv_to(): given object can't be interpreted as a vector
terminate called after throwing an instance of 'std::logic_error'
what():  conv_to(): given object can't be interpreted as a vector
&lt;/denchmark-code&gt;

I cannot understand where is the error after several hours of debugging ...
Thank you
	</description>
	<comments>
		<comment id='1' author='ibalampanis' date='2020-06-15T21:31:47Z'>
		Hey &lt;denchmark-link:https://github.com/ibalampanis&gt;@ibalampanis&lt;/denchmark-link&gt;
, sorry for the slow response.  Can you share the full code?  Is there a use of  anywhere in your code?  Or, perhaps, are you using the  layer somewhere?  (That's the only layer I see that uses  internally.)
		</comment>
		<comment id='2' author='ibalampanis' date='2020-06-15T21:35:34Z'>
		My full code is here: &lt;denchmark-link:https://www.github.com/ibalampanis/distributed-training-of-recurrent-neural-networks-by-fgm-protocol/tree/master/cpp%2Fmodels%2Frnn.cc&gt;https://www.github.com/ibalampanis/distributed-training-of-recurrent-neural-networks-by-fgm-protocol/tree/master/cpp%2Fmodels%2Frnn.cc&lt;/denchmark-link&gt;
 . Thanks.
		</comment>
		<comment id='3' author='ibalampanis' date='2020-06-15T21:43:58Z'>
		Thanks!  I don't see anything immediately wrong with it.  It looks like it would be hard for me to reproduce the issue since that's the entire project; is there any chance that you can provide a small self-contained example that I could build with g++?
Also, if you can provide a backtrace from gdb or similar, I might be able to take a guess at what the issue is. üëç
		</comment>
		<comment id='4' author='ibalampanis' date='2020-06-15T21:46:38Z'>
		Yes, tomorrowI will provide you my gdb log. Thank you.
		</comment>
		<comment id='5' author='ibalampanis' date='2020-06-15T21:47:28Z'>
		Thanks!  Be sure to compile with -g so that we get line numbers too. :)
		</comment>
		<comment id='6' author='ibalampanis' date='2020-06-16T08:02:54Z'>
		Hello, this is my full &lt;denchmark-link:https://github.com/mlpack/mlpack/files/4785254/bt.log&gt;bt.log&lt;/denchmark-link&gt;

Thanks a lot.
		</comment>
		<comment id='7' author='ibalampanis' date='2020-06-16T18:04:53Z'>
		I think I can see a very quick workaround, which is to use a batch size of 1.  Can you try that and see if it works?  I'm digging into a better fix than that though; hope to have something put together soon. üëç
(Edit: I'm not totally sure if I understood the code right.  The issue might be something else.  Hopefully I will learn more soon.)
		</comment>
		<comment id='8' author='ibalampanis' date='2020-06-16T20:35:50Z'>
		You are absolutely correct, the layer only supports batchSize = 1 right now, will open an issue.
		</comment>
		<comment id='9' author='ibalampanis' date='2020-06-16T21:27:36Z'>
		Actually, I am not sure if my thought was correct.  &lt;denchmark-link:https://github.com/ibalampanis&gt;@ibalampanis&lt;/denchmark-link&gt;
 can you say a little about how you are encoding your input data when  ? &lt;denchmark-link:https://github.com/ibalampanis/distributed-training-of-recurrent-neural-networks-by-fgm-protocol/blob/master/cpp/models/rnn.cc#L77&gt;https://github.com/ibalampanis/distributed-training-of-recurrent-neural-networks-by-fgm-protocol/blob/master/cpp/models/rnn.cc#L77&lt;/denchmark-link&gt;

My question is, what is the shape of the input?  I think that what you are doing is passing in a dictionary encoding of each word at each time step.  For the Embedding layer, this should actually not be one-hot encoded---it should just be an int representing the index of the dictionary encoding.  So, e.g., word 3 should be encoded just as [1] and not [0 0 0 1 0 0 ...].  If you are using a one-hot encoded representation, try not doing that, and I think that it will work.
In whichever case solves your problem, I'll definitely add some better error checking so that other users won't be similarly stumped. :)
		</comment>
		<comment id='10' author='ibalampanis' date='2020-06-16T21:35:58Z'>
		
Actually, I am not sure if my thought was correct. @ibalampanis can you say a little about how you are encoding your input data when datasetType == nlp ? https://github.com/ibalampanis/distributed-training-of-recurrent-neural-networks-by-fgm-protocol/blob/master/cpp/models/rnn.cc#L77
My question is, what is the shape of the input? I think that what you are doing is passing in a dictionary encoding of each word at each time step. For the Embedding layer, this should actually not be one-hot encoded---it should just be an int representing the index of the dictionary encoding. So, e.g., word 3 should be encoded just as [1] and not [0 0 0 1 0 0 ...]. If you are using a one-hot encoded representation, try not doing that, and I think that it will work.
In whichever case solves your problem, I'll definitely add some better error checking so that other users won't be similarly stumped. :)

I make the preprocess &lt;denchmark-link:https://github.com/ibalampanis/distributed-training-of-recurrent-neural-networks-by-fgm-protocol/blob/master/python/amazon-reviews-preproc.py&gt;here&lt;/denchmark-link&gt;
 .
It is not one-hot, but encoded with word2vec algorithm.
Thanks all of you for your replies!
		</comment>
		<comment id='11' author='ibalampanis' date='2020-06-16T21:46:54Z'>
		If you've already done word2vec on the data, it seems maybe there would be no need for an Embedding layer then?
		</comment>
		<comment id='12' author='ibalampanis' date='2020-06-16T22:38:12Z'>
		Yes, sure. I found this solution, but it is not via mlpack.
		</comment>
		<comment id='13' author='ibalampanis' date='2020-06-16T23:10:39Z'>
		Perhaps the Embedding layer works differently in other toolkits.  Try removing it and let us know if that solves the issue?
		</comment>
		<comment id='14' author='ibalampanis' date='2020-07-12T06:35:58Z'>
		I have tried to fix the issue in PR &lt;denchmark-link:https://github.com/mlpack/mlpack/pull/2398&gt;#2398&lt;/denchmark-link&gt;
. &lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
 Let me know your thoughts.
		</comment>
		<comment id='15' author='ibalampanis' date='2020-08-11T07:07:13Z'>
		This issue has been automatically marked as stale because it has not had any recent activity.  It will be closed in 7 days if no further activity occurs.  Thank you for your contributions! üëç
		</comment>
	</comments>
</bug>