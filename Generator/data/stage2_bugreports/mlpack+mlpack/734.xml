<bug id='734' author='rcurtin' open_date='2016-07-22T18:01:52Z' closed_time='2019-02-25T20:26:15Z'>
	<summary>RAModel test fails for different types of trees.</summary>
	<description>
This may be related to &lt;denchmark-link:https://github.com/mlpack/mlpack/issues/338&gt;#338&lt;/denchmark-link&gt;
.
I've committed a temporary workaround in &lt;denchmark-link:https://github.com/mlpack/mlpack/commit/356dacfb91a419ad98ac861ef07235cc9a3e1549&gt;356dacf&lt;/denchmark-link&gt;
 for this issue, but it should hopefully be temporary!
Many times, when RAModelTest in src/mlpack/tests/krann_search_test.cpp is run, there will be a failure indicating that too many queries have failed the rank approximation guarantee.  The output looks a bit like this:
&lt;denchmark-code&gt;/home/travis/build/mlpack/mlpack/src/mlpack/tests/krann_search_test.cpp(704): fatal error in "RAModelTest": critical check numQueriesFail &lt; maxNumQueriesFail failed [12 &gt;= 12]
&lt;/denchmark-code&gt;

But even with the value of 12 there, it should be 5.  We should figure out for which tree types this test is failing, and then figure out why it is failing.  So a good first step might be to simply print the tree type that is being tested, and see how often it fails with different random seeds; then we can go from there.
	</description>
	<comments>
		<comment id='1' author='rcurtin' date='2016-07-23T12:03:45Z'>
		I tried a number of random seeds and figured out that the algorithm with the Hilbert R tree failed in ~45% cases. Moreover, the single-tree algorithm works fine. The problem occurs in the dual-tree algorithm.
		</comment>
		<comment id='2' author='rcurtin' date='2017-03-09T13:54:47Z'>
		Hi &lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
 , I found that HILBERT_R_TREE, R_PLUS_TREE, UB_TREE fails the most(but the failing percentage vary on different threshold, HILBERT_R_TREE, R_PLUS_TREE fails most on threshold = 5, and UB_TREE, HILBERT_R_TREE fails most on threshold = 12, and the randomBasis para differs). Nearly all failure is caused by dual tree. I think the rann algorithm is independent of the tree type, so if all worked correctly they should fail on nearly same amount of test? And what is the effect of randomBasis(I find it multiply the data point by a transform matrix, how can this affect the result?). Thank you!
		</comment>
		<comment id='3' author='rcurtin' date='2017-03-14T13:58:29Z'>
		Thanks for running these experiments.
The random basis parameter is based on this paper: &lt;denchmark-link:http://drops.dagstuhl.de/opus/volltexte/2012/3847/pdf/7.pdf&gt;http://drops.dagstuhl.de/opus/volltexte/2012/3847/pdf/7.pdf&lt;/denchmark-link&gt;

We ran some experiments many years ago and found that randomly orienting the dataset can actually produce a runtime acceleration, at least for kd-trees (which is what I tried this with).  The acceleration was only usually a few percentage points though, based on the experimental notes I have.  I am not sure if the acceleration would also be seen with other types of trees, but it was an easy enough option to add.
I looked through the paper for RANN again: &lt;denchmark-link:http://www.learninger.com/docs/nips09_ranknn.pdf&gt;http://www.learninger.com/docs/nips09_ranknn.pdf&lt;/denchmark-link&gt;

But I see that while there is Theorem 3.3, which is an error bound for the single-tree algorithm, there is no such bound for the dual-tree algorithm a few paragraphs later.  This may have been because there was no space for it in the NIPS submission.  I happen to be having lunch with Pari today, I'll ask him there and let you know what I find.
I think, if there is no guarantee for the dual-tree algorithm, we can simply stop testing the dual-tree algorithm in that way, since it's kind of a vacuous test if so.  You say that nearly all failures are caused by dual-tree, but are there some cases where the failure is for naive or single-tree mode?  (i.e. when j == 0 or j == 1?)
Also related here may be this issue: &lt;denchmark-link:https://github.com/mlpack/mlpack/issues/338&gt;#338&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='rcurtin' date='2017-03-15T16:57:02Z'>
		This is my test results.
&lt;denchmark-link:https://gist.github.com/ThyrixYang/2fa3008cc2b959a9bf37dd3fb3a9d033&gt;https://gist.github.com/ThyrixYang/2fa3008cc2b959a9bf37dd3fb3a9d033&lt;/denchmark-link&gt;

As the results showed, naive model never fail(it has a good probability bound), and single tree model rarely fail, some dual tree failed obviously more. A confusing fact is that it tend to fail more often with randbasis = true.
The deterministic method must have been tested strictly. And many single tree model ran well.
So I have some conjectures here:

Some tree model is not implemented correctly. I think it's not the case(but have a possibility, as some model failed more often on all tests)
Wrong with dual tree traverser. As many dual tree ran well, not this case.
The theoretical probability bound can't be applied to tree search directly. The bound in the paper is based on simple analyze of random sampling(I've done it by hand). Maybe there is no such bound on dual tree algorithm. But as some dual tree worked very well, I think it must be differed on different tree model. Because the tree structure can be biased with different split strategy, which cause a "not purely randomized subdataset in subtree. I think we can do more experiment to show this, if it's true. The failure of some single tree model can be also explained by this way(because they have a not so randomized structure).

&lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
 , all are my tentative thinking, please correct me if any wrong. :)
		</comment>
		<comment id='5' author='rcurtin' date='2017-03-16T14:25:50Z'>
		I talked to Pari yesterday, he agreed that the bound in the paper is kind of "cheap", it's just based on random sampling.  (Now that I review it more closely, this is indeed the case.)  So the tree algorithms should both be able to satisfy this bound, since the tree algorithms only prune away a node when it is known that the points in that node cannot possibly be a better nearest neighbor candidate than any of the already-seen points.  This means that there's no need to randomly sample from that node, and in essence the tree algorithm is simply the random sampling approach with some pruning on top of it that doesn't affect the results.
I also noticed a couple of observations in your results that suggest we might be looking at multiple issues:


When randomBasis = true, dual-tree search almost always fails for all types of trees, except the cover tree and the octree.  So does taking a random transformation cause the proof to no longer apply?  Intuitively that does not make sense.  I think this will be the easier of the two issues to work out: either the proof still applies when you do a random transformation and there is some minor bug in the code, or otherwise if the proof does not apply we simply remove the randomBasis option.


The Hilbert R tree and the R+ tree are the only ones that fail when randomBasis = false.  This probably has something to do with the different structure of the tree, such as whether or not it has overlapping nodes and other properties like this.  I remember that the R+ tree algorithm intrinsically has a problem that I think causes it to have overlapping nodes in some cases, but I'll have to review my notes.  I'll see if I can find additional time to look into this, but this one will be tricky: the paper derives RANN for kd-trees, but the generalization that I did with Pari's implementation to support any tree type could possibly make some of the bounds invalid.


Thank you so much for looking into this, it is really nice to have some help digging to the bottom of complex issues like this one. :)
		</comment>
		<comment id='6' author='rcurtin' date='2017-03-16T16:07:53Z'>
		I don't have any idea about the randomBasis now, because naive approach worked well on this situation.(maybe there is a bug only affect tree search? I don't have any entry point of this problem)
I have not look into Hilbert R tree and the R+ tree in detail now, but I'd like to read about these algorithm latter.
About the case I said above, I think I didn't explain my thought clearly:
I think a tree model is just like split many items into bins, so we can work on these bins separately. If we split these items totally in random, the proportion of different item will remain the same as original dataset regardless of the size of bins. And we can get a good approximate by sampling in these bins according to the size of bins(as suggested in the paper).
But if the split is biased, say, if we have 100 items, and 30 of them is what we need(the top k neighbor). And we split them into two bins, both of size 50. Somehow, we have 20 needed items in first bin, and 10 in second. If we can't prune away both bins(child node), we will sample the same amount in both bin, but on different distribution. This is not what we want, and if it happened, the bound will fail.
The tree structure is biased, because almost any tree split point according to some spacial criterion, so some points are more closer and some are more away in spacial distribution in different node. And different tree will behave differently in the rann model.
&lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
 thank you for kind explanation, but maybe I'm still not clearly get the point. I'm looking forward to have a better comprehension on this subject.
		</comment>
		<comment id='7' author='rcurtin' date='2017-03-16T16:23:07Z'>
		I can't say whether this will cause a big difference as the test show, if it's true.
		</comment>
		<comment id='8' author='rcurtin' date='2017-03-17T13:40:38Z'>
		I have done a few experiments today, but did not find any evidence that support my conjecture.
I tried to sort the data in advance and split them into bins(simulate the space split structure), then perform sampling in bins. I tried to sort according to x value, y value and the distance from (0, 0). The naive approach did not fail in these case(may because my approach is too simple compare to space trees). I agree with you that we should focus on randomBasis and the two tree types.
A more test result that may help, printed the average failure number of every model:
&lt;denchmark-link:https://gist.github.com/ThyrixYang/0e89d7f048fd4299cd266befa7aae6f6&gt;https://gist.github.com/ThyrixYang/0e89d7f048fd4299cd266befa7aae6f6&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='rcurtin' date='2017-07-02T12:49:30Z'>
		&lt;denchmark-link:https://github.com/ThyrixYang&gt;@ThyrixYang&lt;/denchmark-link&gt;
: thanks again for your investigations here.  Unfortunately for me my free time ran out for now and I wasn't able to pursue this further for now.
For the time being since this test causes so much confusion I have increased the tolerance to 50 with &lt;denchmark-link:https://github.com/mlpack/mlpack/commit/62100ddca45880a57e7abb0432df72d285e5728b&gt;62100dd&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='10' author='rcurtin' date='2019-02-18T19:58:30Z'>
		This issue has been automatically marked as stale because it has not had any recent activity.  It will be closed in 7 days if no further activity occurs.  Thank you for your contributions! 👍
		</comment>
	</comments>
</bug>