<bug id='2565' author='hl-xue' open_date='2020-08-12T14:26:29Z' closed_time='2020-09-01T03:00:05Z'>
	<summary>k-fold cv returns -nan</summary>
	<description>
Hello to all.
I am using mlpack-3.3.2. When doing k-fold cross-validation using f1 score for Naive Bayes Classifier, I found for some input, .Evaluate() method returns -nan as the result. According to what I understood to the f1 score formula, this happens when in the test set TP=FP=FN=0. I tried to shuffle the k-fold data multiple times (with the method .Shuffle()), but this occurs always.
For information, I have only one feature with multiple observations. The observations are labelled in binary as true or false.
I would like verify with you please if this -nan returned by .Evaluate() method is normal ? If yes, when would this happen ?
Thanks in advance !
Best wishes.
	</description>
	<comments>
		<comment id='1' author='hl-xue' date='2020-08-17T23:48:26Z'>
		Hi &lt;denchmark-link:https://github.com/hl-xue&gt;@hl-xue&lt;/denchmark-link&gt;
, I guess, a thing to check would be the performance on the full training set of the Naive Bayes Classifier.  Can you try training an NBC model on the whole dataset, and then see what the accuracy / TP / FP / FN rates are?  If those are problematic, it might be easier to debug what is going on.
For instance, I think that Naive Bayes might have problems if you have a dimension that takes only one value, so, like, a column of all zeros or similar.  This would be because the variance for a particular column is 0, and that can cause problems during the prediction process.
Let us know what you find out, and we can find some way to get the code working right. :)
		</comment>
		<comment id='2' author='hl-xue' date='2020-08-20T14:03:39Z'>
		Hi &lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
,
Thanks for your reply.
I had some tests. I trained NBC model on the whole dataset, classified on the same dataset, and then looked the confusion matrix and F1 score. Things works fine.
But when I re-add the k-fold cross-validation, the -nan reoccurs. For example, I made a simple data as below:

Observations = [7, 6, 5, 4, 3, 2, 1]
Labels = [0, 0, 0, 1, 1, 1, 1]
with 2-fold cross-validation and f1 score, it gives -nan as the score.
but in this case, if I shuffle the cross-validation data and re-estimate the score, -nan can disappear.

		</comment>
		<comment id='3' author='hl-xue' date='2020-08-20T14:07:36Z'>
		Interesting, do you think this is because one of the folds will sometimes end up only having point with label 0 and no points with label 1?
I'll dig in to see if I can reproduce it, but I suppose, if you did LOOCV (7-fold), the F1 score shouldn't be -nan, at least on the training data.
		</comment>
		<comment id='4' author='hl-xue' date='2020-08-20T14:19:57Z'>
		
Interesting, do you think this is because one of the folds will sometimes end up only having point with label 0 and no points with label 1?

I guess so.

I'll dig in to see if I can reproduce it, but I suppose, if you did LOOCV (7-fold), the F1 score shouldn't be -nan, at least on the training data.

Thanks, I will try LOOCV.
		</comment>
		<comment id='5' author='hl-xue' date='2020-08-20T14:27:04Z'>
		Another question, I guess, is how large is your dataset?  This situation of only getting one class in a subset would be more likely for small data.
(It won't be until tonight that I can look into this more deeply, at the earliest.)
		</comment>
		<comment id='6' author='hl-xue' date='2020-08-20T14:32:55Z'>
		I have 100+ samples.
Besides, it seems to me that when number of fold increases, it would be more likely to have one class in a subset. Imagine we have 50 pos and 50 neg, and we use 10-fold cv, then it could be more possible that there some fold where the 10 samples are all associated with a same label then the case of 2-fold.
OK, no problem ;-)
		</comment>
		<comment id='7' author='hl-xue' date='2020-08-25T02:26:49Z'>
		Yes, I think what's going on here is that there are too many folds being used, and some of the folds have only one class in the validation fold, causing an F1 score of -nan.  Here is a test program I wrote:
#include &lt;mlpack/core.hpp&gt;
#include &lt;mlpack/core/cv/k_fold_cv.hpp&gt;
#include &lt;mlpack/core/cv/metrics/f1.hpp&gt;
#include &lt;mlpack/methods/naive_bayes/naive_bayes_classifier.hpp&gt;

using namespace mlpack;
using namespace mlpack::naive_bayes;
using namespace mlpack::cv;

int main()
{
  mlpack::math::RandomSeed(std::time(NULL));
  std::cout &lt;&lt; "2 classes, 1 dimension, 7 points, 2 folds:\n";
  for (size_t i = 0; i &lt; 20; ++i)
  {
    arma::mat data = arma::randu&lt;arma::mat&gt;(1, 7);
    arma::Row&lt;size_t&gt; labels = arma::randi&lt;arma::Row&lt;size_t&gt;&gt;(7,
        arma::distr_param(0, 1));

    KFoldCV&lt;NaiveBayesClassifier&lt;&gt;, F1&lt;Binary&gt;&gt; cv(2, data, labels, (size_t) 2);
    double f1 = cv.Evaluate();

    std::cout &lt;&lt; "Computed F1 score: " &lt;&lt; f1 &lt;&lt; "\n";
  }

  std::cout &lt;&lt; "\n2 classes, 1 dimension, 100 points, 10 folds:\n";
  for (size_t i = 0; i &lt; 20; ++i)
  {
    arma::mat data = arma::randu&lt;arma::mat&gt;(6, 100);
    arma::Row&lt;size_t&gt; labels = arma::randi&lt;arma::Row&lt;size_t&gt;&gt;(100,
        arma::distr_param(0, 1));

    KFoldCV&lt;NaiveBayesClassifier&lt;&gt;, F1&lt;Binary&gt;&gt; cv(10, data, labels, (size_t) 2);
    double f1 = cv.Evaluate();

    std::cout &lt;&lt; "Computed F1 score: " &lt;&lt; f1 &lt;&lt; "\n";
  }
}
And when I run this, I get:
&lt;denchmark-code&gt;$ ./nbc_test 
2 classes, 1 dimension, 7 points, 2 folds:
Computed F1 score: -nan
Computed F1 score: -nan
Computed F1 score: 0.733333
Computed F1 score: -nan
Computed F1 score: -nan
Computed F1 score: 0.928571
Computed F1 score: -nan
Computed F1 score: 0.583333
Computed F1 score: 0.666667
Computed F1 score: -nan
Computed F1 score: -nan
Computed F1 score: 0.828571
Computed F1 score: -nan
Computed F1 score: -nan
Computed F1 score: -nan
Computed F1 score: -nan
Computed F1 score: -nan
Computed F1 score: 0.828571
Computed F1 score: -nan
Computed F1 score: -nan

2 classes, 1 dimension, 100 points, 10 folds:
Computed F1 score: 0.577908
Computed F1 score: 0.579621
Computed F1 score: 0.376178
Computed F1 score: 0.515405
Computed F1 score: 0.57883
Computed F1 score: 0.557687
Computed F1 score: 0.45131
Computed F1 score: 0.522008
Computed F1 score: 0.592907
Computed F1 score: 0.374891
Computed F1 score: 0.468586
Computed F1 score: 0.557859
Computed F1 score: 0.523672
Computed F1 score: 0.225253
Computed F1 score: 0.665776
Computed F1 score: 0.574669
Computed F1 score: -nan
Computed F1 score: 0.419733
Computed F1 score: 0.535509
Computed F1 score: -nan
&lt;/denchmark-code&gt;

If I run again with a different random seed, the results are roughly the same.  As the number of points per fold goes up, the probability of a -nan score decreases.  So, what I would recommend here is just that you reduce the number of folds, or try to increase the number of observations that you have.
Another thing you could do---which admittedly does seem a little pointless---is to use the KFoldCV::Model() function in order to recover the last trained model with k-fold cross-validation, and then compute the F1 score on a separate test set.  (But this kind of defeats the purpose of cross-validation.)
Now, the Evaluate() function in KFoldCV returns the mean of the given Metric (here F1) across all folds.  Certainly, the idea of the mean doesn't make much sense when any of the elements is -nan.  But I am not sure if we can expect to do better, or if it is reasonable to use any other strategy for averaging metrics across the test set here.
This does mean that a third potential option would be to implement a "smoothed" F1 score, where the score is either 0 or 1 (your choice depending on your modeling needs) when TP=FP=FN=0.  You could do this by following this tutorial: &lt;denchmark-link:https://www.mlpack.org/doc/mlpack-3.3.2/doxygen/cv.html#cvbasic_metrics&gt;https://www.mlpack.org/doc/mlpack-3.3.2/doxygen/cv.html#cvbasic_metrics&lt;/denchmark-link&gt;
 and you could simply adapt the  class implementation accordingly.
However, I am not sure how good any of those three options are for your use case.
&lt;denchmark-link:https://github.com/micyril&gt;@micyril&lt;/denchmark-link&gt;
 I know it has been a long time, but if you like, I'd be interested in your opinion on this: to me, this seems like expected functionality, but do you have any thoughts on whether it could be improved, or if a -nan would be expected here in the case of k-fold cross-validation with the F1 score where there is only one label in the test set?
		</comment>
		<comment id='8' author='hl-xue' date='2020-08-25T06:02:59Z'>
		Hi Ryan,

I think everything you mentioned is reasonable. I can think of few other options.
1. We can shuffle data in the way that reduces the probability of presenting just one label in a validation fold. For example, we can shuffle separately positive and negative data points, and then mix them in the way that there is roughly the same distribution of labels in any window of size (number of data points) / k.
2. Optionally exclude from averaging nan metric values.

Best regards,

Kirill Mishchenko
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 On 25 Aug 2020, at 05:27, Ryan Curtin ***@***.***&gt; wrote:


 Yes, I think what's going on here is that there are too many folds being used, and some of the folds have only one class in the validation fold, causing an F1 score of -nan. Here is a test program I wrote:

 #include &lt;mlpack/core.hpp&gt;
 #include &lt;mlpack/core/cv/k_fold_cv.hpp&gt;
 #include &lt;mlpack/core/cv/metrics/f1.hpp&gt;
 #include &lt;mlpack/methods/naive_bayes/naive_bayes_classifier.hpp&gt;

 using namespace mlpack;
 using namespace mlpack::naive_bayes;
 using namespace mlpack::cv;

 int main()
 {
   mlpack::math::RandomSeed(std::time(NULL));
   std::cout &lt;&lt; "2 classes, 1 dimension, 7 points, 2 folds:\n";
   for (size_t i = 0; i &lt; 20; ++i)
   {
     arma::mat data = arma::randu&lt;arma::mat&gt;(1, 7);
     arma::Row&lt;size_t&gt; labels = arma::randi&lt;arma::Row&lt;size_t&gt;&gt;(7,
         arma::distr_param(0, 1));

     KFoldCV&lt;NaiveBayesClassifier&lt;&gt;, F1&lt;Binary&gt;&gt; cv(2, data, labels, (size_t) 2);
     double f1 = cv.Evaluate();

     std::cout &lt;&lt; "Computed F1 score: " &lt;&lt; f1 &lt;&lt; "\n";
   }

   std::cout &lt;&lt; "\n2 classes, 1 dimension, 100 points, 10 folds:\n";
   for (size_t i = 0; i &lt; 20; ++i)
   {
     arma::mat data = arma::randu&lt;arma::mat&gt;(6, 100);
     arma::Row&lt;size_t&gt; labels = arma::randi&lt;arma::Row&lt;size_t&gt;&gt;(100,
         arma::distr_param(0, 1));

     KFoldCV&lt;NaiveBayesClassifier&lt;&gt;, F1&lt;Binary&gt;&gt; cv(10, data, labels, (size_t) 2);
     double f1 = cv.Evaluate();

     std::cout &lt;&lt; "Computed F1 score: " &lt;&lt; f1 &lt;&lt; "\n";
   }
 }
 And when I run this, I get:

 $ ./nbc_test
 2 classes, 1 dimension, 7 points, 2 folds:
 Computed F1 score: -nan
 Computed F1 score: -nan
 Computed F1 score: 0.733333
 Computed F1 score: -nan
 Computed F1 score: -nan
 Computed F1 score: 0.928571
 Computed F1 score: -nan
 Computed F1 score: 0.583333
 Computed F1 score: 0.666667
 Computed F1 score: -nan
 Computed F1 score: -nan
 Computed F1 score: 0.828571
 Computed F1 score: -nan
 Computed F1 score: -nan
 Computed F1 score: -nan
 Computed F1 score: -nan
 Computed F1 score: -nan
 Computed F1 score: 0.828571
 Computed F1 score: -nan
 Computed F1 score: -nan

 2 classes, 1 dimension, 100 points, 10 folds:
 Computed F1 score: 0.577908
 Computed F1 score: 0.579621
 Computed F1 score: 0.376178
 Computed F1 score: 0.515405
 Computed F1 score: 0.57883
 Computed F1 score: 0.557687
 Computed F1 score: 0.45131
 Computed F1 score: 0.522008
 Computed F1 score: 0.592907
 Computed F1 score: 0.374891
 Computed F1 score: 0.468586
 Computed F1 score: 0.557859
 Computed F1 score: 0.523672
 Computed F1 score: 0.225253
 Computed F1 score: 0.665776
 Computed F1 score: 0.574669
 Computed F1 score: -nan
 Computed F1 score: 0.419733
 Computed F1 score: 0.535509
 Computed F1 score: -nan
 If I run again with a different random seed, the results are roughly the same. As the number of points per fold goes up, the probability of a -nan score decreases. So, what I would recommend here is just that you reduce the number of folds, or try to increase the number of observations that you have.

 Another thing you could do---which admittedly does seem a little pointless---is to use the KFoldCV::Model() function in order to recover the last trained model with k-fold cross-validation, and then compute the F1 score on a separate test set. (But this kind of defeats the purpose of cross-validation.)

 Now, the Evaluate() function in KFoldCV returns the mean of the given Metric (here F1) across all folds. Certainly, the idea of the mean doesn't make much sense when any of the elements is -nan. But I am not sure if we can expect to do better, or if it is reasonable to use any other strategy for averaging metrics across the test set here.

 This does mean that a third potential option would be to implement a "smoothed" F1 score, where the score is either 0 or 1 (your choice depending on your modeling needs) when TP=FP=FN=0. You could do this by following this tutorial: https://www.mlpack.org/doc/mlpack-3.3.2/doxygen/cv.html#cvbasic_metrics &lt;https://www.mlpack.org/doc/mlpack-3.3.2/doxygen/cv.html#cvbasic_metrics&gt; and you could simply adapt the F1 class implementation accordingly.

 However, I am not sure how good any of those three options are for your use case.

 @micyril &lt;https://github.com/micyril&gt; I know it has been a long time, but if you like, I'd be interested in your opinion on this: to me, this seems like expected functionality, but do you have any thoughts on whether it could be improved, or if a -nan would be expected here in the case of k-fold cross-validation with the F1 score where there is only one label in the test set?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub &lt;#2565 (comment)&gt;, or unsubscribe &lt;https://github.com/notifications/unsubscribe-auth/AAOFZPFHKD4LAHYWY3UIYNDSCMOPPANCNFSM4P4Y3G5Q&gt;.



		</comment>
		<comment id='9' author='hl-xue' date='2020-08-25T15:02:04Z'>
		Hi &lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
,
OK, thanks a lot for your detailed reply and suggestions ! I will take the first suggestion for now.
By the way, for information, I tried the LOOCV (7-fold) on the simple test data [7, 6, 5, 4, 3, 2, 1], but I still got -nan.
Best wishes.
		</comment>
		<comment id='10' author='hl-xue' date='2020-08-27T02:42:33Z'>
		&lt;denchmark-link:https://github.com/micyril&gt;@micyril&lt;/denchmark-link&gt;
 thanks for the thoughts!   Actually I think the best way forward here would be option (2): exclude any NaN values from averaging, and print something to  indicating to the user that a fold had a NaN measure, and that this can happen when folds don't have points for all classes, or something like this.  I'll prepare a PR shortly. :)
&lt;denchmark-link:https://github.com/hl-xue&gt;@hl-xue&lt;/denchmark-link&gt;
 great to hear the ideas will work for you---hopefully in the future once the PR I suggested above is merged, the output will be a little bit better for a situation like this. 
		</comment>
		<comment id='11' author='hl-xue' date='2020-08-27T08:34:39Z'>
		Hi &lt;denchmark-link:https://github.com/rcurtin&gt;@rcurtin&lt;/denchmark-link&gt;
, I've just met another interesting case: I was trying to evaluate a "bad" feature without cross-validation and using f1 score. I trained and tested both on the same dataset of 374 observations (240 negatives and 134 positives). Since it's a "bad" feature, the naive Bayes classifier predicts all observations as negative. So I have the confusion matrix below:
&lt;denchmark-code&gt;Confusion matrix:
   2.4000e+02   1.3400e+02
            0            0
&lt;/denchmark-code&gt;

And for the f1 score, it output again -nan but this time without cross-validation.
I am not sure if this is related with the -nan returned by cross-validation. But, I imagine if the algorithm gives -nan when the predictions contains no positive class (TP = FN = 0), this could explain that if a fold in cross-validation has only positive prediction (no matter if it contains actually positive or negative lables), the algorithm gives -nan as the result.
I didn't verify this, but I remember that I have met some -nan score systematically appears for some features, even though I shuffled (using KFoldCV.Shuffle() method) up to 1000 times. It seems to me that the two issues might be related, so I would like to let you know about this.
Best wishes.
		</comment>
		<comment id='12' author='hl-xue' date='2020-08-30T00:43:01Z'>
		Hi &lt;denchmark-link:https://github.com/hl-xue&gt;@hl-xue&lt;/denchmark-link&gt;
, the F1 score in the two-class case will return  if there are zero positive class predictions, or if there are zero positive class instances.  This comes from the expression of F1 score as , and the fact that  is a division by 0 if there are no positive class predictions, and  is a division by 0 if there are no positive class samples.
I opened &lt;denchmark-link:https://github.com/mlpack/mlpack/pull/2595&gt;#2595&lt;/denchmark-link&gt;
, which causes  to issue a warning when a metric returns  or , and then this particular measurement is ignored when computing the average to return with .  Hopefully this will help clarify the situation if it happens in the future. 
		</comment>
		<comment id='13' author='hl-xue' date='2020-08-30T08:11:42Z'>
		Ah, yes, you're right. I was thinking the formula f1 = TP/(TP + 0.5(FP + FN)), but this formula is deducible only if (TP+FP)(TP+FN) is not equal to 0.
Thanks !
		</comment>
		<comment id='14' author='hl-xue' date='2020-09-01T03:00:05Z'>
		&lt;denchmark-link:https://github.com/mlpack/mlpack/pull/2595&gt;#2595&lt;/denchmark-link&gt;
 is merged now, so I think this issue is fixed (or at least improved).  Please feel free to open more issues in the future, or, if your problem is actually not solved and I misunderstood, we can reopen this one. :)
		</comment>
	</comments>
</bug>