<bug id='88' author='HighPerformance01' open_date='2019-11-05T07:32:57Z' closed_time='2019-12-20T00:39:39Z'>
	<summary>BatchNorm problem with latest TCN</summary>
	<description>
Hi,
After using the TCN code update 22 days back, all my code is not working.
Please check the attached text files:
same_code.py is the same code used in old and new TCN
old_model_summary.txt is the model summary before the update
new_model_summary.txt is the model summary after the update
tcn283_old.py is the last tcn which was working
Would you please give any clue to what is going on.
Thank you
&lt;denchmark-link:https://github.com/philipperemy/keras-tcn/files/3807703/new_model_summary.txt&gt;new_model_summary.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/philipperemy/keras-tcn/files/3807704/old_model_summary.txt&gt;old_model_summary.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/philipperemy/keras-tcn/files/3807707/same_code.txt&gt;same_code.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/philipperemy/keras-tcn/files/3807708/tcn283_old.txt&gt;tcn283_old.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='HighPerformance01' date='2019-11-05T08:32:00Z'>
		&lt;denchmark-link:https://github.com/HighPerformance01&gt;@HighPerformance01&lt;/denchmark-link&gt;
 is it a compilation issue or convergence issue?
		</comment>
		<comment id='2' author='HighPerformance01' date='2019-11-05T10:00:31Z'>
		It is a convergence issue. New TCN can not go more 0.3 accuracy while the old one can go to more than 0.97 accuracy
Thank you for the quick response
		</comment>
		<comment id='3' author='HighPerformance01' date='2019-11-06T02:23:42Z'>
		&lt;denchmark-link:https://github.com/HighPerformance01&gt;@HighPerformance01&lt;/denchmark-link&gt;
 that's concerning. Did you try to run it several times?
		</comment>
		<comment id='4' author='HighPerformance01' date='2019-11-06T08:32:15Z'>
		Yes, I tried with different machines even.
The problem can be noticed easily from the model summary files
using the same code the old model generates a different CNN model than the current one.
New model Total params: 524
Old model Total params: 14,810,956
Were you able to check the text files attached? They use the same code and generate totally different networks.
Thank you for following up
		</comment>
		<comment id='5' author='HighPerformance01' date='2019-11-06T08:34:06Z'>
		Oh yeah the new model seems pretty empty. It's pretty weird. Will have a look this weekend!
		</comment>
		<comment id='6' author='HighPerformance01' date='2019-11-06T08:46:29Z'>
		Thank you
		</comment>
		<comment id='7' author='HighPerformance01' date='2019-11-07T09:55:46Z'>
		&lt;denchmark-link:https://github.com/psomers3&gt;@psomers3&lt;/denchmark-link&gt;
 maybe linked to the recent changes.
		</comment>
		<comment id='8' author='HighPerformance01' date='2019-11-07T10:12:24Z'>
		Oh no... will try and take a look this afternoon
		</comment>
		<comment id='9' author='HighPerformance01' date='2019-11-07T10:20:01Z'>
		Thank you so much!
		</comment>
		<comment id='10' author='HighPerformance01' date='2019-11-07T11:44:52Z'>
		&lt;denchmark-link:https://github.com/HighPerformance01&gt;@HighPerformance01&lt;/denchmark-link&gt;
 Are you using tensorflow as the backend or something else?
		</comment>
		<comment id='11' author='HighPerformance01' date='2019-11-07T14:34:19Z'>
		Yes I am using TensorFlow as backend
I put now everything including the data so you can reproduce the issue on the following google drive shared folder:
&lt;denchmark-link:https://drive.google.com/drive/folders/1JYp7G8d34IOJhHWUyfIe_bUqsoRGhogt?usp=sharing&gt;https://drive.google.com/drive/folders/1JYp7G8d34IOJhHWUyfIe_bUqsoRGhogt?usp=sharing&lt;/denchmark-link&gt;

thank you for the quick response
		</comment>
		<comment id='12' author='HighPerformance01' date='2019-11-07T14:49:58Z'>
		I cannot access google drive right now for proxy reasons, but I am able to build your model just fine and have the same number of parameters as your old model. What version of Keras and tensorflow are you using?
		</comment>
		<comment id='13' author='HighPerformance01' date='2019-11-07T15:06:16Z'>
		I just run it on colab leaving it to defaults and still I noticed the issue:
keras version: 2.2.5
tf version: 1.15.0
		</comment>
		<comment id='14' author='HighPerformance01' date='2019-11-08T07:51:01Z'>
		It definitely seems to just be an issue on google colab. Unfortunately, I am completely new to that service and the jupyter notebook style of scripting, so it could take me awhile to find the issue.
I did find it will work on there with:
tensorflow 1.15 and 2.0
keras 2.3.1
So maybe try just updating keras. I think that should fix it for you.
		</comment>
		<comment id='15' author='HighPerformance01' date='2019-11-08T11:54:16Z'>
		Yes after upgrade to keras 2.3.1 it has the same number of parameters but still has the convergence issue. See below a comparison of old and new tcn for val_accuracy
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Old TCN:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Total params: 14,810,956
Trainable params: 14,803,786
Non-trainable params: 7,170
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Epoch 1/3000
100/100 [==============================] - 181s 2s/step - loss: 1.2263 - accuracy: 0.7991 - val_loss: 7.1074 - val_accuracy: 0.5445
Epoch 00001: val_loss improved from inf to 7.10735, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 2/3000
100/100 [==============================] - 154s 2s/step - loss: 0.3821 - accuracy: 0.8993 - val_loss: 1.4262 - val_accuracy: 0.8534
Epoch 00002: val_loss improved from 7.10735 to 1.42625, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 3/3000
100/100 [==============================] - 154s 2s/step - loss: 0.2848 - accuracy: 0.9201 - val_loss: 4.8484 - val_accuracy: 0.6977
Epoch 00003: val_loss did not improve from 1.42625
Epoch 4/3000
100/100 [==============================] - 154s 2s/step - loss: 0.2012 - accuracy: 0.9396 - val_loss: 0.5044 - val_accuracy: 0.9443
Epoch 00004: val_loss improved from 1.42625 to 0.50444, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 5/3000
100/100 [==============================] - 154s 2s/step - loss: 0.1407 - accuracy: 0.9539 - val_loss: 0.6171 - val_accuracy: 0.9385
Epoch 00005: val_loss did not improve from 0.50444
Epoch 6/3000
100/100 [==============================] - 154s 2s/step - loss: 0.1199 - accuracy: 0.9585 - val_loss: 0.0913 - val_accuracy: 0.9611
Epoch 00006: val_loss improved from 0.50444 to 0.09128, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 7/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0989 - accuracy: 0.9648 - val_loss: 0.1525 - val_accuracy: 0.9581
Epoch 00007: val_loss did not improve from 0.09128
Epoch 8/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0854 - accuracy: 0.9697 - val_loss: 0.1131 - val_accuracy: 0.9637
Epoch 00008: val_loss did not improve from 0.09128
Epoch 9/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0768 - accuracy: 0.9716 - val_loss: 0.1820 - val_accuracy: 0.9677
Epoch 00009: val_loss did not improve from 0.09128
Epoch 10/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0682 - accuracy: 0.9744 - val_loss: 0.1400 - val_accuracy: 0.9539
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

New TCN:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Total params: 14,810,956
Trainable params: 14,803,786
Non-trainable params: 7,170
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Epoch 1/3000
100/100 [==============================] - 181s 2s/step - loss: 0.9029 - accuracy: 0.8485 - val_loss: 13.7754 - val_accuracy: 0.1399
Epoch 00001: val_loss improved from inf to 13.77543, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 2/3000
100/100 [==============================] - 154s 2s/step - loss: 0.2103 - accuracy: 0.9338 - val_loss: 13.8976 - val_accuracy: 0.1410
Epoch 00002: val_loss did not improve from 13.77543
Epoch 3/3000
100/100 [==============================] - 154s 2s/step - loss: 0.1569 - accuracy: 0.9476 - val_loss: 14.6791 - val_accuracy: 0.0985
Epoch 00003: val_loss did not improve from 13.77543
Epoch 4/3000
100/100 [==============================] - 154s 2s/step - loss: 0.1176 - accuracy: 0.9588 - val_loss: 14.5196 - val_accuracy: 0.0961
Epoch 00004: val_loss did not improve from 13.77543
Epoch 5/3000
100/100 [==============================] - 154s 2s/step - loss: 0.1163 - accuracy: 0.9594 - val_loss: 13.9379 - val_accuracy: 0.1567
Epoch 00005: val_loss did not improve from 13.77543
Epoch 6/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0932 - accuracy: 0.9665 - val_loss: 14.2479 - val_accuracy: 0.1177
Epoch 00006: val_loss did not improve from 13.77543
Epoch 7/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0792 - accuracy: 0.9709 - val_loss: 14.3323 - val_accuracy: 0.1167
Epoch 00007: val_loss did not improve from 13.77543
Epoch 8/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0775 - accuracy: 0.9712 - val_loss: 13.2020 - val_accuracy: 0.1875
Epoch 00008: val_loss improved from 13.77543 to 13.20198, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 9/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0699 - accuracy: 0.9742 - val_loss: 14.6189 - val_accuracy: 0.1015
Epoch 00009: val_loss did not improve from 13.20198
Epoch 10/3000
100/100 [==============================] - 154s 2s/step - loss: 0.0667 - accuracy: 0.9752 - val_loss: 14.4819 - val_accuracy: 0.0966
		</comment>
		<comment id='16' author='HighPerformance01' date='2019-11-08T13:59:35Z'>
		I will take a look this weekend when I can access your training data.
		</comment>
		<comment id='17' author='HighPerformance01' date='2019-11-08T14:14:25Z'>
		If I had to take a guess right now, the issue lies with the batch normalization because it is only the validation data that doesn’t converge.
		</comment>
		<comment id='18' author='HighPerformance01' date='2019-11-08T14:22:15Z'>
		Thank you for that, the dataset is at:
&lt;denchmark-link:https://drive.google.com/drive/folders/1JYp7G8d34IOJhHWUyfIe_bUqsoRGhogt?usp=sharing&gt;https://drive.google.com/drive/folders/1JYp7G8d34IOJhHWUyfIe_bUqsoRGhogt?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='HighPerformance01' date='2019-11-11T05:30:10Z'>
		I'm really at a loss as to why the validation is having issues. I took a look this weekend and found that we should be passing the training keyword to the normalization layer, but that didn't seem to fix it. It is tedious to debug this on colab. &lt;denchmark-link:https://github.com/HighPerformance01&gt;@HighPerformance01&lt;/denchmark-link&gt;
 Do you have a smaller network that is having the same problem so I may debug locally?
		</comment>
		<comment id='20' author='HighPerformance01' date='2019-11-11T06:38:30Z'>
		To have smaller network, we just change the parameters at the top of the file.
New Parameters:
n_filters=32
kernel_size=32
dilations=3
n_stacks=3
Also, this smaller network produce the same issue:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Old TCN:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Layer (type)                    Output Shape         Param #     Connected to&lt;/denchmark-h&gt;

input_1 (InputLayer)            (None, 1000, 1)      0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_1 (BatchNor (None, 1000, 1)      4           input_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_1 (Conv1D)               (None, 1000, 32)     64          batch_normalization_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_2 (Conv1D)               (None, 1000, 32)     32800       conv1d_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_2 (BatchNor (None, 1000, 32)     128         conv1d_2[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_1 (Activation)       (None, 1000, 32)     0           batch_normalization_2[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_1 (SpatialDro (None, 1000, 32)     0           activation_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_3 (Conv1D)               (None, 1000, 32)     32800       spatial_dropout1d_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_3 (BatchNor (None, 1000, 32)     128         conv1d_3[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_2 (Activation)       (None, 1000, 32)     0           batch_normalization_3[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_2 (SpatialDro (None, 1000, 32)     0           activation_2[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_4 (Conv1D)               (None, 1000, 32)     1056        conv1d_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_1 (Add)                     (None, 1000, 32)     0           conv1d_4[0][0]
spatial_dropout1d_2[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_3 (Activation)       (None, 1000, 32)     0           add_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_5 (Conv1D)               (None, 1000, 32)     32800       activation_3[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_4 (BatchNor (None, 1000, 32)     128         conv1d_5[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_4 (Activation)       (None, 1000, 32)     0           batch_normalization_4[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_3 (SpatialDro (None, 1000, 32)     0           activation_4[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_6 (Conv1D)               (None, 1000, 32)     32800       spatial_dropout1d_3[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_5 (BatchNor (None, 1000, 32)     128         conv1d_6[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_5 (Activation)       (None, 1000, 32)     0           batch_normalization_5[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_4 (SpatialDro (None, 1000, 32)     0           activation_5[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_7 (Conv1D)               (None, 1000, 32)     1056        activation_3[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_2 (Add)                     (None, 1000, 32)     0           conv1d_7[0][0]
spatial_dropout1d_4[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_6 (Activation)       (None, 1000, 32)     0           add_2[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_8 (Conv1D)               (None, 1000, 32)     32800       activation_6[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_6 (BatchNor (None, 1000, 32)     128         conv1d_8[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_7 (Activation)       (None, 1000, 32)     0           batch_normalization_6[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_5 (SpatialDro (None, 1000, 32)     0           activation_7[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_9 (Conv1D)               (None, 1000, 32)     32800       spatial_dropout1d_5[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_7 (BatchNor (None, 1000, 32)     128         conv1d_9[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_8 (Activation)       (None, 1000, 32)     0           batch_normalization_7[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_6 (SpatialDro (None, 1000, 32)     0           activation_8[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_10 (Conv1D)              (None, 1000, 32)     1056        activation_6[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_3 (Add)                     (None, 1000, 32)     0           conv1d_10[0][0]
spatial_dropout1d_6[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_9 (Activation)       (None, 1000, 32)     0           add_3[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_11 (Conv1D)              (None, 1000, 32)     32800       activation_9[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_8 (BatchNor (None, 1000, 32)     128         conv1d_11[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_10 (Activation)      (None, 1000, 32)     0           batch_normalization_8[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_7 (SpatialDro (None, 1000, 32)     0           activation_10[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_12 (Conv1D)              (None, 1000, 32)     32800       spatial_dropout1d_7[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_9 (BatchNor (None, 1000, 32)     128         conv1d_12[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_11 (Activation)      (None, 1000, 32)     0           batch_normalization_9[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_8 (SpatialDro (None, 1000, 32)     0           activation_11[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_13 (Conv1D)              (None, 1000, 32)     1056        activation_9[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_4 (Add)                     (None, 1000, 32)     0           conv1d_13[0][0]
spatial_dropout1d_8[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_12 (Activation)      (None, 1000, 32)     0           add_4[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_14 (Conv1D)              (None, 1000, 32)     32800       activation_12[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_10 (BatchNo (None, 1000, 32)     128         conv1d_14[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_13 (Activation)      (None, 1000, 32)     0           batch_normalization_10[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_9 (SpatialDro (None, 1000, 32)     0           activation_13[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_15 (Conv1D)              (None, 1000, 32)     32800       spatial_dropout1d_9[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_11 (BatchNo (None, 1000, 32)     128         conv1d_15[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_14 (Activation)      (None, 1000, 32)     0           batch_normalization_11[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_10 (SpatialDr (None, 1000, 32)     0           activation_14[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_16 (Conv1D)              (None, 1000, 32)     1056        activation_12[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_5 (Add)                     (None, 1000, 32)     0           conv1d_16[0][0]
spatial_dropout1d_10[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_15 (Activation)      (None, 1000, 32)     0           add_5[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_17 (Conv1D)              (None, 1000, 32)     32800       activation_15[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_12 (BatchNo (None, 1000, 32)     128         conv1d_17[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_16 (Activation)      (None, 1000, 32)     0           batch_normalization_12[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_11 (SpatialDr (None, 1000, 32)     0           activation_16[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_18 (Conv1D)              (None, 1000, 32)     32800       spatial_dropout1d_11[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_13 (BatchNo (None, 1000, 32)     128         conv1d_18[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_17 (Activation)      (None, 1000, 32)     0           batch_normalization_13[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_12 (SpatialDr (None, 1000, 32)     0           activation_17[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_19 (Conv1D)              (None, 1000, 32)     1056        activation_15[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_6 (Add)                     (None, 1000, 32)     0           conv1d_19[0][0]
spatial_dropout1d_12[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_18 (Activation)      (None, 1000, 32)     0           add_6[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_20 (Conv1D)              (None, 1000, 32)     32800       activation_18[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_14 (BatchNo (None, 1000, 32)     128         conv1d_20[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_19 (Activation)      (None, 1000, 32)     0           batch_normalization_14[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_13 (SpatialDr (None, 1000, 32)     0           activation_19[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_21 (Conv1D)              (None, 1000, 32)     32800       spatial_dropout1d_13[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_15 (BatchNo (None, 1000, 32)     128         conv1d_21[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_20 (Activation)      (None, 1000, 32)     0           batch_normalization_15[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_14 (SpatialDr (None, 1000, 32)     0           activation_20[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_22 (Conv1D)              (None, 1000, 32)     1056        activation_18[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_7 (Add)                     (None, 1000, 32)     0           conv1d_22[0][0]
spatial_dropout1d_14[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_21 (Activation)      (None, 1000, 32)     0           add_7[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_23 (Conv1D)              (None, 1000, 32)     32800       activation_21[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_16 (BatchNo (None, 1000, 32)     128         conv1d_23[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_22 (Activation)      (None, 1000, 32)     0           batch_normalization_16[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_15 (SpatialDr (None, 1000, 32)     0           activation_22[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_24 (Conv1D)              (None, 1000, 32)     32800       spatial_dropout1d_15[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_17 (BatchNo (None, 1000, 32)     128         conv1d_24[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_23 (Activation)      (None, 1000, 32)     0           batch_normalization_17[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_16 (SpatialDr (None, 1000, 32)     0           activation_23[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_25 (Conv1D)              (None, 1000, 32)     1056        activation_21[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_8 (Add)                     (None, 1000, 32)     0           conv1d_25[0][0]
spatial_dropout1d_16[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_24 (Activation)      (None, 1000, 32)     0           add_8[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_26 (Conv1D)              (None, 1000, 32)     32800       activation_24[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_18 (BatchNo (None, 1000, 32)     128         conv1d_26[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_25 (Activation)      (None, 1000, 32)     0           batch_normalization_18[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_17 (SpatialDr (None, 1000, 32)     0           activation_25[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

conv1d_27 (Conv1D)              (None, 1000, 32)     32800       spatial_dropout1d_17[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_19 (BatchNo (None, 1000, 32)     128         conv1d_27[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

activation_26 (Activation)      (None, 1000, 32)     0           batch_normalization_19[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

spatial_dropout1d_18 (SpatialDr (None, 1000, 32)     0           activation_26[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

add_10 (Add)                    (None, 1000, 32)     0           spatial_dropout1d_2[0][0]
spatial_dropout1d_4[0][0]
spatial_dropout1d_6[0][0]
spatial_dropout1d_8[0][0]
spatial_dropout1d_10[0][0]
spatial_dropout1d_12[0][0]
spatial_dropout1d_14[0][0]
spatial_dropout1d_16[0][0]
spatial_dropout1d_18[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dropout_1 (Dropout)             (None, 1000, 32)     0           add_10[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dense_1 (Dense)                 (None, 1000, 8)      264         dropout_1[0][0]
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;activation_28 (Activation)      (None, 1000, 8)      0           dense_1[0][0]&lt;/denchmark-h&gt;

Total params: 601,484
Trainable params: 600,330
Non-trainable params: 1,154
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
Epoch 1/3000
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.
100/100 [==============================] - 45s 454ms/step - loss: 0.9593 - accuracy: 0.6933 - val_loss: 0.4456 - val_accuracy: 0.8612
Epoch 00001: val_loss improved from inf to 0.44556, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 2/3000
100/100 [==============================] - 30s 304ms/step - loss: 0.3273 - accuracy: 0.8857 - val_loss: 0.2667 - val_accuracy: 0.9170
Epoch 00002: val_loss improved from 0.44556 to 0.26668, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 3/3000
100/100 [==============================] - 30s 303ms/step - loss: 0.2480 - accuracy: 0.9141 - val_loss: 0.2447 - val_accuracy: 0.9374
Epoch 00003: val_loss improved from 0.26668 to 0.24470, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 4/3000
100/100 [==============================] - 30s 302ms/step - loss: 0.2153 - accuracy: 0.9242 - val_loss: 0.2763 - val_accuracy: 0.9266
Epoch 00004: val_loss did not improve from 0.24470
Epoch 5/3000
100/100 [==============================] - 30s 302ms/step - loss: 0.1867 - accuracy: 0.9343 - val_loss: 0.3029 - val_accuracy: 0.9099
Epoch 00005: val_loss did not improve from 0.24470
Epoch 6/3000
100/100 [==============================] - 30s 303ms/step - loss: 0.1704 - accuracy: 0.9406 - val_loss: 0.2116 - val_accuracy: 0.9363
Epoch 00006: val_loss improved from 0.24470 to 0.21161, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 7/3000
100/100 [==============================] - 30s 303ms/step - loss: 0.1500 - accuracy: 0.9471 - val_loss: 0.1758 - val_accuracy: 0.9476
Epoch 00007: val_loss improved from 0.21161 to 0.17579, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 8/3000
100/100 [==============================] - 30s 304ms/step - loss: 0.1461 - accuracy: 0.9486 - val_loss: 0.2517 - val_accuracy: 0.9350
Epoch 00008: val_loss did not improve from 0.17579
Epoch 9/3000
100/100 [==============================] - 30s 303ms/step - loss: 0.1435 - accuracy: 0.9496 - val_loss: 0.4883 - val_accuracy: 0.8707
Epoch 00009: val_loss did not improve from 0.17579
Epoch 10/3000
100/100 [==============================] - 30s 303ms/step - loss: 0.1338 - accuracy: 0.9526 - val_loss: 0.2854 - val_accuracy: 0.9140
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

New TCN:
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Layer (type)                 Output Shape              Param #&lt;/denchmark-h&gt;

input_1 (InputLayer)         (None, 1000, 1)           0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

batch_normalization_1 (Batch (None, 1000, 1)           4
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

tcn_1 (TCN)                  (None, 1000, 32)          601216
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dropout_1 (Dropout)          (None, 1000, 32)          0
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

dense_1 (Dense)              (None, 1000, 8)           264
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;activation_28 (Activation)   (None, 1000, 8)           0&lt;/denchmark-h&gt;

Total params: 601,484
Trainable params: 600,330
Non-trainable params: 1,154
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.
Epoch 1/3000
100/100 [==============================] - 37s 369ms/step - loss: 1.3282 - accuracy: 0.6292 - val_loss: 12.0828 - val_accuracy: 0.2600
Epoch 00001: val_loss improved from inf to 12.08278, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 2/3000
100/100 [==============================] - 30s 297ms/step - loss: 0.4080 - accuracy: 0.8558 - val_loss: 13.1702 - val_accuracy: 0.1695
Epoch 00002: val_loss did not improve from 12.08278
Epoch 3/3000
100/100 [==============================] - 29s 295ms/step - loss: 0.2878 - accuracy: 0.8990 - val_loss: 13.9210 - val_accuracy: 0.1374
Epoch 00003: val_loss did not improve from 12.08278
Epoch 4/3000
100/100 [==============================] - 30s 297ms/step - loss: 0.2414 - accuracy: 0.9152 - val_loss: 12.5346 - val_accuracy: 0.2269
Epoch 00004: val_loss did not improve from 12.08278
Epoch 5/3000
100/100 [==============================] - 30s 296ms/step - loss: 0.2010 - accuracy: 0.9302 - val_loss: 12.6691 - val_accuracy: 0.2147
Epoch 00005: val_loss did not improve from 12.08278
Epoch 6/3000
100/100 [==============================] - 29s 295ms/step - loss: 0.1889 - accuracy: 0.9342 - val_loss: 12.4054 - val_accuracy: 0.2432
Epoch 00006: val_loss did not improve from 12.08278
Epoch 7/3000
100/100 [==============================] - 30s 296ms/step - loss: 0.1700 - accuracy: 0.9400 - val_loss: 11.9768 - val_accuracy: 0.2491
Epoch 00007: val_loss improved from 12.08278 to 11.97675, saving model to /content/drive/My Drive/colab01/wlogs06/weights.h5
Epoch 8/3000
100/100 [==============================] - 30s 295ms/step - loss: 0.1534 - accuracy: 0.9459 - val_loss: 12.3316 - val_accuracy: 0.2452
Epoch 00008: val_loss did not improve from 11.97675
Epoch 9/3000
100/100 [==============================] - 30s 295ms/step - loss: 0.1392 - accuracy: 0.9503 - val_loss: 14.3819 - val_accuracy: 0.1122
Epoch 00009: val_loss did not improve from 11.97675
Epoch 10/3000
100/100 [==============================] - 30s 296ms/step - loss: 0.1357 - accuracy: 0.9521 - val_loss: 14.1681 - val_accuracy: 0.1122
		</comment>
		<comment id='21' author='HighPerformance01' date='2019-11-13T07:03:51Z'>
		hava a same problem, the val_accuracy cannot imporve with the accuracy, is there any suggestion?
		</comment>
		<comment id='22' author='HighPerformance01' date='2019-11-13T07:13:53Z'>
		Unfortunately, I have not had much of a chance to look at this again lately. And probably won‘t until later this week. :(
&lt;denchmark-link:https://github.com/wildaurora&gt;@wildaurora&lt;/denchmark-link&gt;
 Can you post your TCN layer configuration? (ie parameters you built the layer with)
		</comment>
		<comment id='23' author='HighPerformance01' date='2019-11-14T06:03:05Z'>
		I use TCN as language model , and config as follow:
def build_model(vocab_size): model = Sequential() model.add(Embedding(vocab_size, input_embedding_size)) model.add(TCN(nb_stacks=4, kernel_size=3, dropout_rate=0.45, return_sequences=True,dilations=(1, 2, 4, 8), use_batch_norm=True, activation='relu')) model.add(Dropout(0.2)) model.add(Dense(vocab_size, activation='softmax')) return model
		</comment>
		<comment id='24' author='HighPerformance01' date='2019-11-14T10:12:25Z'>
		I found where the problem is..Caused by use_batch_norm=True, while set use_batch_norm=False, the val_acc will improve with acc, maybe its due to keras batch_normalization_layer trainable param. However, even if set model.trainable=True, the problem won't be fixed. So I just set use_batch_norm=False.
		</comment>
		<comment id='25' author='HighPerformance01' date='2019-11-14T10:49:57Z'>
		this is what I was playing with on my forked copy. We should figure this out. the layer should be set as trainable, but even when I passed „training“ to the call to put the layer in inference mode (note there is a significant difference between „trainable“ and „training“), it still didn‘t work. From looking online, it seems like the batch_norm layer can be a pain. I just don‘t know why it worked before and not now. That‘s mostly what concerns me.
		</comment>
		<comment id='26' author='HighPerformance01' date='2019-11-20T12:01:14Z'>
		&lt;denchmark-link:https://github.com/psomers3&gt;@psomers3&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wildaurora&gt;@wildaurora&lt;/denchmark-link&gt;
 Hum this batch norm is a tricky thing. Weight norm was released for tensorflow so maybe we should move to &lt;denchmark-link:https://github.com/philipperemy/keras-tcn/issues/91&gt;#91&lt;/denchmark-link&gt;

		</comment>
		<comment id='27' author='HighPerformance01' date='2019-11-20T12:02:18Z'>
		Btw I could reproduce the convergence issue on my GPU server:
With pip install keras-tcn==2.8.3
&lt;denchmark-link:https://pastebin.com/P4A7PFj7&gt;https://pastebin.com/P4A7PFj7&lt;/denchmark-link&gt;

&lt;denchmark-link:https://pastebin.com/w0AcW5zr&gt;https://pastebin.com/w0AcW5zr&lt;/denchmark-link&gt;

With master
&lt;denchmark-link:https://pastebin.com/YaiqtZ7Q&gt;https://pastebin.com/YaiqtZ7Q&lt;/denchmark-link&gt;

&lt;denchmark-link:https://pastebin.com/hiy99ZtY&gt;https://pastebin.com/hiy99ZtY&lt;/denchmark-link&gt;

=&gt; Convergence problem.
		</comment>
		<comment id='28' author='HighPerformance01' date='2019-12-17T12:59:37Z'>
		I just checked with the mnist example with tf.version = '2.0.0', current github version:
# no dropout, batchnorm=True after 10 epochs: 97s 2ms/sample - loss: 0.0495 - accuracy: 0.9842 - val_loss: 0.0470 - val_accuracy: 0.9842
# 0.05 dropout, batchnorm=True after 10 epochs: 100s 2ms/sample - loss: 0.0512 - accuracy: 0.9834 - val_loss: 0.0483 - val_accuracy: 0.9853
I see no problem.
Btw.: Nice package, great work!
		</comment>
		<comment id='29' author='HighPerformance01' date='2019-12-17T14:02:51Z'>
		Yes for me too now.
Nice package, great work!
		</comment>
		<comment id='30' author='HighPerformance01' date='2019-12-20T00:39:39Z'>
		Thank you everyone! We can close this issue now!
		</comment>
	</comments>
</bug>