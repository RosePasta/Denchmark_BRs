<bug id='61' author='beasteers' open_date='2019-07-10T14:29:54Z' closed_time='2019-11-29T05:21:46Z'>
	<summary>Name parameter not used</summary>
	<description>
The name parameter is currently not being applied to the layer. As a related issue, the current layer implementation dumps all of the model layers into a single flattened list and it's hard to see where one TCN block ends and another begins. I propose that, to solve both of these issues, the TCN layer should inherit from keras.layers.Layer
I patched the layer to implement the desired interface here, but all of this can be factored into the original TCN layer.
from keras.layers import Layer
class TCN2(Layer):
    def __init__(self, *a, name=None, **kw):
        self.model = TCN(*a, **kw)
        Layer.__init__(self, name=name)
        
    def call(self, x):
        return self.model(x)
    
    def compute_output_shape(self, input_shape):
        return input_shape[:-1] + (self.model.nb_filters,)
Here's a sketch of the changes translated to the original class:
from keras.layers import Layer
class TCN(Layer):
    # remove name from args here so it is passed in **kw
    def __init__(self, *a, ..., **kw): 
        ...
        super().__init__(*a, **kw)
        
    call = __call__ # just change the method name
    
    def compute_output_shape(self, input_shape):
        return input_shape[:-1] + (self.nb_filters,)
Now when I do this:
batch_size, timesteps, input_ch = None, fs*dur, 1

i = Input(batch_shape=(batch_size, timesteps, input_ch), name='tcn_in')
o = TCN2(16, return_sequences=True, name='tcn1')(i)
o = TCN2(16, return_sequences=True, name='tcn2')(o)
o = TCN2(16, return_sequences=True, name='tcn3')(o)

m = Model(inputs=[i], outputs=[o])
m.compile(optimizer='adam', loss='mse')
[l.name for l in m.layers]
I get:
&lt;denchmark-code&gt;['tcn_in', 'tcn1', 'tcn2', 'tcn3']
&lt;/denchmark-code&gt;

Instead of:
&lt;denchmark-code&gt;['tcn_in',
 'conv1d_927',
 'conv1d_928',
 'activation_871',
 'spatial_dropout1d_581',
 'conv1d_929',
 'activation_872',
 'spatial_dropout1d_582',
 'conv1d_930',
 'add_334',
 'activation_873',
 'conv1d_931',
 'activation_874',
 'spatial_dropout1d_583',
 'conv1d_932',
 'activation_875',
 'spatial_dropout1d_584',
 'conv1d_933',
 'add_335',
 'activation_876',
 'conv1d_934',
 'activation_877',
 'spatial_dropout1d_585',
 'conv1d_935',
 'activation_878',
 'spatial_dropout1d_586',
 'conv1d_936',
 'add_336',
 'activation_879',
 'conv1d_937',
 'activation_880',
 'spatial_dropout1d_587',
 'conv1d_938',
 'activation_881',
 'spatial_dropout1d_588',
 'conv1d_939',
 'add_337',
 'activation_882',
 'conv1d_940',
 'activation_883',
 'spatial_dropout1d_589',
 'conv1d_941',
 'activation_884',
 'spatial_dropout1d_590',
 'conv1d_942',
 'add_338',
 'activation_885',
 'conv1d_943',
 'activation_886',
 'spatial_dropout1d_591',
 'conv1d_944',
 'activation_887',
 'spatial_dropout1d_592',
 'add_340',
 'conv1d_946',
 'conv1d_947',
 'activation_889',
 'spatial_dropout1d_593',
 'conv1d_948',
 'activation_890',
 'spatial_dropout1d_594',
 'conv1d_949',
 'add_341',
 'activation_891',
 'conv1d_950',
 'activation_892',
 'spatial_dropout1d_595',
 'conv1d_951',
 'activation_893',
 'spatial_dropout1d_596',
 'conv1d_952',
 'add_342',
 'activation_894',
 'conv1d_953',
 'activation_895',
 'spatial_dropout1d_597',
 'conv1d_954',
 'activation_896',
 'spatial_dropout1d_598',
 'conv1d_955',
 'add_343',
 'activation_897',
 'conv1d_956',
 'activation_898',
 'spatial_dropout1d_599',
 'conv1d_957',
 'activation_899',
 'spatial_dropout1d_600',
 'conv1d_958',
 'add_344',
 'activation_900',
 'conv1d_959',
 'activation_901',
 'spatial_dropout1d_601',
 'conv1d_960',
 'activation_902',
 'spatial_dropout1d_602',
 'conv1d_961',
 'add_345',
 'activation_903',
 'conv1d_962',
 'activation_904',
 'spatial_dropout1d_603',
 'conv1d_963',
 'activation_905',
 'spatial_dropout1d_604',
 'add_347',
 'conv1d_965',
 'conv1d_966',
 'activation_907',
 'spatial_dropout1d_605',
 'conv1d_967',
 'activation_908',
 'spatial_dropout1d_606',
 'conv1d_968',
 'add_348',
 'activation_909',
 'conv1d_969',
 'activation_910',
 'spatial_dropout1d_607',
 'conv1d_970',
 'activation_911',
 'spatial_dropout1d_608',
 'conv1d_971',
 'add_349',
 'activation_912',
 'conv1d_972',
 'activation_913',
 'spatial_dropout1d_609',
 'conv1d_973',
 'activation_914',
 'spatial_dropout1d_610',
 'conv1d_974',
 'add_350',
 'activation_915',
 'conv1d_975',
 'activation_916',
 'spatial_dropout1d_611',
 'conv1d_976',
 'activation_917',
 'spatial_dropout1d_612',
 'conv1d_977',
 'add_351',
 'activation_918',
 'conv1d_978',
 'activation_919',
 'spatial_dropout1d_613',
 'conv1d_979',
 'activation_920',
 'spatial_dropout1d_614',
 'conv1d_980',
 'add_352',
 'activation_921',
 'conv1d_981',
 'activation_922',
 'spatial_dropout1d_615',
 'conv1d_982',
 'activation_923',
 'spatial_dropout1d_616',
 'add_354']
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='beasteers' date='2019-07-10T15:06:17Z'>
		The one thing that I'm not sure is when you do this, I don't know how to access the child layers.
		</comment>
		<comment id='2' author='beasteers' date='2019-07-10T15:17:45Z'>
		Update:
according to notes in &lt;denchmark-link:https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#layers_are_recursively_composable&gt;this&lt;/denchmark-link&gt;
, it seems that in order for the layer to track trainable weights in child layers, you need to assign the layers as a direct attribute to the TCN layer which makes things,,, a bit more complicated.
		</comment>
		<comment id='3' author='beasteers' date='2019-07-12T03:12:22Z'>
		&lt;denchmark-link:https://github.com/beasteers&gt;@beasteers&lt;/denchmark-link&gt;
 thanks for reporting! Going to have a look over the weekend :)
		</comment>
		<comment id='4' author='beasteers' date='2019-07-12T17:57:38Z'>
		Just a thought, maybe we can create it as a Sequential object instead of a layer? Personally, I prefer the functional API, but this would allow us to automatically capture the trainable weights of the child layers by adding them to self.layers (and it also allows for easy layer access).
Per the &lt;denchmark-link:https://keras.io/layers/containers/&gt;docs&lt;/denchmark-link&gt;
 here it says:

The Sequential container is a linear stack of layers. Apart from the add methods and the layers constructor argument, the API is identical to that of the Layer class.

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Edit:
I did a bit of digging and it appears that there's a discrepancy between how keras and tensorflow.keras handle nested layers. The note stating that Keras tracks layers assigned as attributes only applies to the tensorflow version of Keras and not the original.
:
&lt;denchmark-link:https://github.com/keras-team/keras/blob/29f27f333efb427830e62bec62fd79b444df4011/keras/engine/base_layer.py#L192&gt;https://github.com/keras-team/keras/blob/29f27f333efb427830e62bec62fd79b444df4011/keras/engine/base_layer.py#L192&lt;/denchmark-link&gt;

:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/2bb4f008ff48630dd516a637232ade00ffd018cf/tensorflow/python/keras/engine/base_layer.py#L812&gt;https://github.com/tensorflow/tensorflow/blob/2bb4f008ff48630dd516a637232ade00ffd018cf/tensorflow/python/keras/engine/base_layer.py#L812&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/2bb4f008ff48630dd516a637232ade00ffd018cf/tensorflow/python/keras/engine/base_layer.py#L2190&gt;https://github.com/tensorflow/tensorflow/blob/2bb4f008ff48630dd516a637232ade00ffd018cf/tensorflow/python/keras/engine/base_layer.py#L2190&lt;/denchmark-link&gt;

tensorflow.keras tracks nested layers using the _layers attribute. It appears that o.g. Keras does a very similar thing inside its Network class (which is the underlying logic for Model, without the training stuff).
:
&lt;denchmark-link:https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/keras/engine/network.py#L469&gt;https://github.com/keras-team/keras/blob/ed07472bc5fc985982db355135d37059a1f887a9/keras/engine/network.py#L469&lt;/denchmark-link&gt;

So depending on which implementation of Keras you want to roll with (tf or o.g.), it looks like we can either build off of the tensorflow keras Layer class, or the keras Network class.
In both cases, we just need to add all layers to the _layers attribute in order for their trainable_weights to be tracked.
		</comment>
		<comment id='5' author='beasteers' date='2019-09-28T23:11:24Z'>
		I didn't realize that this was already being discussed. I have done this in my pending pull request.
		</comment>
		<comment id='6' author='beasteers' date='2019-10-07T06:28:12Z'>
		&lt;denchmark-link:https://github.com/psomers3&gt;@psomers3&lt;/denchmark-link&gt;
 excellent! I am going to check all that :)
		</comment>
		<comment id='7' author='beasteers' date='2019-11-28T00:17:38Z'>
		Hi,
I'm also having issues trying to load in a pre-trained model that was trained with all the child layers into TF 2.0 using tf.keras. Building the model in the exact same manner in TF 2.0 with tf.keras only shows the parent layers, making me think that is where the loading of weights is going wrong.
Any update on how I could fix this?
Thanks in advance!
		</comment>
		<comment id='8' author='beasteers' date='2019-11-28T07:45:02Z'>
		&lt;denchmark-link:https://github.com/asyms&gt;@asyms&lt;/denchmark-link&gt;
 Can you provide a bit more detail? What do you mean by showing only the parent layers? And does the loading completely fail? Or does the model just not work properly?
		</comment>
		<comment id='9' author='beasteers' date='2019-11-28T09:54:29Z'>
		&lt;denchmark-link:https://github.com/psomers3&gt;@psomers3&lt;/denchmark-link&gt;
 Yes of course, sorry for not doing so earlier.
I have two setups: new one with TF 2.0 using tf.keras, old one with TF 1.14 using Keras.
On both setups I build a TCN model using the functional API style:
On the old one the imports use keras directly:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

from keras import Input, Model
from keras import layers
from keras.layers import Activation, Dense, Flatten
from keras.optimizers import Adam
from tcn import TCN

def build_model(batchSize, numTimeSteps=98, numTargets=12, numFeats=10):
  DropoutProb = 0.2  # Dropout probability [%]
  FIRST_LAYER_NEURONS=16
  NEXT_LAYERS_NEURONS=48
  L2_lambda = 0.0008

  # Training parameters
  adamLearningRate = 0.001
  adamBeta1 = 0.9
  adamBeta2 = 0.999
  adamEpsilon = 1e-08
  adamDecay = 0.0

  # Make Network
  i = Input(batch_shape=(batchSize, numTimeSteps, numFeats))


  o = TCN(nb_filters=FIRST_LAYER_NEURONS,
          kernel_size=3,
          nb_stacks=1,
          dilations=[1],
          padding='causal',
          use_skip_connections=False,
          dropout_rate=DropoutProb,
          activation='linear',
          return_sequences=True,
          use_batch_norm=True,
          name='tcn_0')(i)

  o = TCN(nb_filters=NEXT_LAYERS_NEURONS,
          kernel_size=3,
          nb_stacks=1,
          dilations=[2,4,8],
          padding='causal',
          use_skip_connections=True,
          dropout_rate=DropoutProb,
          activation='relu',
          return_sequences=True,
          use_batch_norm=True,
          name='tcn_1')(o)


  o = layers.AveragePooling1D(pool_size=(4))(o)


  o = Flatten()(o)
  o = Dense(numTargets)(o)
  o = Activation('softmax', name='softmax')((o))
  
  model = Model(inputs=[i], outputs= [o])


     
  # Define optimizer
  adam = Adam(lr=adamLearningRate, beta_1=adamBeta1, beta_2=adamBeta2, epsilon=adamEpsilon, decay=adamDecay)
  # Compile model
  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
  print(model.count_params())
  print(model.summary())
&lt;/denchmark-code&gt;

While one the new one the imports are as follows (and the build_model function is identical):
&lt;denchmark-code&gt;from tensorflow.keras import Sequential, Input, Model
from tensorflow.keras import layers
from tensorflow.keras.layers import Activation, Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tcn import TCN
&lt;/denchmark-code&gt;

Calling this build_model for a batchSize=1, numTimeSteps=61, numTargets=12, numFeats=10 gives this result on the old setup:
&lt;denchmark-code&gt;__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (1, 61, 10)          0                                            
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (1, 61, 16)          176         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (1, 61, 16)          784         conv1d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (1, 61, 16)          64          conv1d_2[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (1, 61, 16)          0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
spatial_dropout1d_1 (SpatialDro (1, 61, 16)          0           activation_1[0][0]               
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (1, 61, 16)          784         spatial_dropout1d_1[0][0]        
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (1, 61, 16)          64          conv1d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (1, 61, 16)          0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (1, 61, 16)          272         conv1d_1[0][0]                   
__________________________________________________________________________________________________
spatial_dropout1d_2 (SpatialDro (1, 61, 16)          0           activation_2[0][0]               
__________________________________________________________________________________________________
add_1 (Add)                     (1, 61, 16)          0           conv1d_4[0][0]                   
                                                                 spatial_dropout1d_2[0][0]        
__________________________________________________________________________________________________
activation_3 (Activation)       (1, 61, 16)          0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (1, 61, 48)          816         activation_3[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (1, 61, 48)          6960        conv1d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (1, 61, 48)          192         conv1d_6[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (1, 61, 48)          0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
spatial_dropout1d_3 (SpatialDro (1, 61, 48)          0           activation_4[0][0]               
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (1, 61, 48)          6960        spatial_dropout1d_3[0][0]        
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (1, 61, 48)          192         conv1d_7[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (1, 61, 48)          0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
spatial_dropout1d_4 (SpatialDro (1, 61, 48)          0           activation_5[0][0]               
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (1, 61, 48)          2352        conv1d_5[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (1, 61, 48)          0           conv1d_8[0][0]                   
                                                                 spatial_dropout1d_4[0][0]        
__________________________________________________________________________________________________
activation_6 (Activation)       (1, 61, 48)          0           add_2[0][0]                      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (1, 61, 48)          6960        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (1, 61, 48)          192         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (1, 61, 48)          0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
spatial_dropout1d_5 (SpatialDro (1, 61, 48)          0           activation_7[0][0]               
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (1, 61, 48)          6960        spatial_dropout1d_5[0][0]        
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (1, 61, 48)          192         conv1d_10[0][0]                  
__________________________________________________________________________________________________
activation_8 (Activation)       (1, 61, 48)          0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
spatial_dropout1d_6 (SpatialDro (1, 61, 48)          0           activation_8[0][0]               
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (1, 61, 48)          2352        activation_6[0][0]               
__________________________________________________________________________________________________
add_3 (Add)                     (1, 61, 48)          0           conv1d_11[0][0]                  
                                                                 spatial_dropout1d_6[0][0]        
__________________________________________________________________________________________________
activation_9 (Activation)       (1, 61, 48)          0           add_3[0][0]                      
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (1, 61, 48)          6960        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (1, 61, 48)          192         conv1d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (1, 61, 48)          0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
spatial_dropout1d_7 (SpatialDro (1, 61, 48)          0           activation_10[0][0]              
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (1, 61, 48)          6960        spatial_dropout1d_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (1, 61, 48)          192         conv1d_13[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (1, 61, 48)          0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
spatial_dropout1d_8 (SpatialDro (1, 61, 48)          0           activation_11[0][0]              
__________________________________________________________________________________________________
add_5 (Add)                     (1, 61, 48)          0           spatial_dropout1d_4[0][0]        
                                                                 spatial_dropout1d_6[0][0]        
                                                                 spatial_dropout1d_8[0][0]        
__________________________________________________________________________________________________
average_pooling1d_1 (AveragePoo (1, 15, 48)          0           add_5[0][0]                      
__________________________________________________________________________________________________
flatten_1 (Flatten)             (1, 720)             0           average_pooling1d_1[0][0]        
__________________________________________________________________________________________________
dense_1 (Dense)                 (1, 12)              8652        flatten_1[0][0]                  
__________________________________________________________________________________________________
softmax (Activation)            (1, 12)              0           dense_1[0][0]                    
==================================================================================================
&lt;/denchmark-code&gt;

While giving following result on the new setup:
&lt;denchmark-code&gt;_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(1, 61, 10)]             0         
_________________________________________________________________
tcn_0 (TCN)                  (1, 61, 16)               2144      
_________________________________________________________________
tcn_1 (TCN)                  (1, 61, 48)               48432     
_________________________________________________________________
average_pooling1d (AveragePo (1, 15, 48)               0         
_________________________________________________________________
flatten (Flatten)            (1, 720)                  0         
_________________________________________________________________
dense (Dense)                (1, 12)                   8652      
_________________________________________________________________
softmax (Activation)         (1, 12)                   0         
=================================================================
&lt;/denchmark-code&gt;

Now, I have models that were trained using the old setup saved. Loading these with the command
model.load_weights('weights/model.hdf5', by_name=True) works fine for the old setup, while loading that model in the new setup gives unexpected results. It doesn't give an error when loading, but upon inference the accuracy on the new setup is always 0.
Kind regards
		</comment>
		<comment id='10' author='beasteers' date='2019-11-29T05:21:46Z'>
		&lt;denchmark-link:https://github.com/asyms&gt;@asyms&lt;/denchmark-link&gt;
 can you update your Keras TCN library and try again? If it still does not work, please open a new issue!
		</comment>
	</comments>
</bug>