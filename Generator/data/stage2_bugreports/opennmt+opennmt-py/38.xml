<bug id='38' author='Chung-I' open_date='2017-04-25T05:40:39Z' closed_time='2017-08-27T23:28:13Z'>
	<summary>optim.optimizer.state_dict.state not found when loading from checkpoint</summary>
	<description>
I encountered an error when using Adam optimizer and resuming training from checkpoint, which states that state in Adam optimizer is not found. I found that the line optim.set_parameters(model.parameters()) wipe out states in
optim.optimizer.state_dict.state.
&lt;denchmark-code&gt;if not opt.train_from_state_dict and not opt.train_from:
        for p in model.parameters():
            p.data.uniform_(-opt.param_init, opt.param_init)
        encoder.load_pretrained_vectors(opt)
        decoder.load_pretrained_vectors(opt)
        optim = onmt.Optim(
            opt.optim, opt.learning_rate, opt.max_grad_norm,
            lr_decay=opt.learning_rate_decay,
            start_decay_at=opt.start_decay_at
        )
    else:
        print('Loading optimizer from checkpoint:')
        optim = checkpoint['optim']
        print(optim)

    optim.set_parameters(model.parameters())
&lt;/denchmark-code&gt;

after I move the line optim.set_parameters(model.parameters()) into the block under if statement, the code works fine:
&lt;denchmark-code&gt;if not opt.train_from_state_dict and not opt.train_from:
        for p in model.parameters():
            p.data.uniform_(-opt.param_init, opt.param_init)
        encoder.load_pretrained_vectors(opt)
        decoder.load_pretrained_vectors(opt)
        optim = onmt.Optim(
            opt.optim, opt.learning_rate, opt.max_grad_norm,
            lr_decay=opt.learning_rate_decay,
            start_decay_at=opt.start_decay_at
        )
        optim.set_parameters(model.parameters())
    else:
        print('Loading optimizer from checkpoint:')
        optim = checkpoint['optim']
        print(optim)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Chung-I' date='2017-05-23T05:27:48Z'>
		Thanks for your very helpful tip!
		</comment>
		<comment id='2' author='Chung-I' date='2017-05-24T01:01:05Z'>
		&lt;denchmark-link:https://github.com/ylhsieh&gt;@ylhsieh&lt;/denchmark-link&gt;
  But I later found that the code above is just silencing the error rather than fixing it ...... Adam stop updating parameters when reading from a checkpoint.
		</comment>
		<comment id='3' author='Chung-I' date='2017-05-24T03:25:24Z'>
		Thanks again! That is exactly what I was going to post just now. Wonder if anyone has a fix...
		</comment>
		<comment id='4' author='Chung-I' date='2017-08-27T23:28:13Z'>
		(Superceded by more recent issue)
		</comment>
	</comments>
</bug>