<bug id='283' author='aryamccarthy' open_date='2017-09-20T18:16:32Z' closed_time='2018-02-19T20:17:57Z'>
	<summary>Massive memory use because entire file is slurped in IO.py</summary>
	<description>
Is there a strong reason for slurping the entire source file in translate.py? On a source file of 2M single-character-per-token sentences, memory usage exceeds 256 GB. I figure that since evaluation is sentence-by-sentence (or batch-by-batch), this can be avoided.
Lab-mates and I have written an &lt;denchmark-link:https://docs.python.org/3/library/itertools.html#itertools.islice&gt;itertools.islice&lt;/denchmark-link&gt;
-based workaround. I'm happy to convert it into a PR if I have more information about the design goals and decisions, so I don't step on toes.
	</description>
	<comments>
		<comment id='1' author='aryamccarthy' date='2017-09-20T21:14:34Z'>
		Huh, not sure why it is doing this. It reads in the whole file, but only constructs the matrices for a batch at a time. Not sure why it is taking up so much memory. We will look into it.
		</comment>
		<comment id='2' author='aryamccarthy' date='2017-09-20T21:16:25Z'>
		Thanks. I'm happy to work on it, Sasha. Is it on OpenNMT's side or PyTorch's?
		</comment>
		<comment id='3' author='aryamccarthy' date='2017-09-20T21:24:48Z'>
		Yeah, we would love to have more contributions. That would be great.
It's likely on our side. This line &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/master/translate.py#L84&gt;https://github.com/OpenNMT/OpenNMT-py/blob/master/translate.py#L84&lt;/denchmark-link&gt;
 does read in all the examples as text. But I can't imagine that would take 256 GB. Can you check if that line alone does it?
		</comment>
		<comment id='4' author='aryamccarthy' date='2017-09-20T21:39:31Z'>
		My sysadmin wouldn't like it if I tried the code again. My guess is that it happens inside that initializer, on &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/b08b56966d4c495c904a1bddc4319d0bfeecbf47/onmt/IO.py#L203&gt;this line&lt;/denchmark-link&gt;
 that constructs a list. I'll investigate what would happen if it were replaced with a generator.
		</comment>
		<comment id='5' author='aryamccarthy' date='2017-09-21T20:40:25Z'>
		Sounds like this is your issue &lt;denchmark-link:https://github.com/vince62s&gt;@vince62s&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='aryamccarthy' date='2017-09-21T20:44:18Z'>
		same issue ? mine was in preprocess.py, similar code ?
		</comment>
		<comment id='7' author='aryamccarthy' date='2017-09-21T21:01:11Z'>
		Yeah, it's the same code. Think someone changed something recently. Investigating.
		</comment>
		<comment id='8' author='aryamccarthy' date='2017-09-21T21:52:09Z'>
		Think this should help &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/289&gt;#289&lt;/denchmark-link&gt;
 . Working on the file saving part.
		</comment>
		<comment id='9' author='aryamccarthy' date='2017-09-24T19:43:30Z'>
		Using a lazy iterator for the examples, as in &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/289&gt;#289&lt;/denchmark-link&gt;
, will only work if  is , and at the moment that's only the case if  is also . See the constructor of 's &lt;denchmark-link:https://github.com/pytorch/text/blob/master/torchtext/data/dataset.py#L36&gt;parent class&lt;/denchmark-link&gt;
. Whatever benefit you gain from the generator, you lose the moment it's loaded into the list.
		</comment>
		<comment id='10' author='aryamccarthy' date='2017-09-25T20:16:10Z'>
		I don't think that is true. The code before was creating multiple objects of the size of the data. The constant factor matters a lot here.
		</comment>
		<comment id='11' author='aryamccarthy' date='2017-09-25T20:27:56Z'>
		You're right, saying that all the benefits are lost is incorrect. I was thinking based on the assumption that the whole-file slurping was the problem, but you're right, the problem was actually all the auxiliary data structures that we got rid of.
		</comment>
		<comment id='12' author='aryamccarthy' date='2017-09-25T20:32:33Z'>
		Actually, not really sure what the problem is. Possibly that the data structure of example is memory inefficient? Maybe ever having them all in memory is bad.
		</comment>
		<comment id='13' author='aryamccarthy' date='2017-10-26T10:10:33Z'>
		I encounter the same problem ( namely: I use preprocess.py to handle a corpus with 10M sentences and memory consumptions goes up to 30 GB before I kill it).
It seems that the problem is caused in 


OpenNMT-py/onmt/IO.py


        Lines 156 to 157
      in
      9f4b4d7






 examples = (join_dicts(src, tgt) 



 for src, tgt in zip(src_examples, tgt_examples)) 





When I start the program it runs up to that point, never continues but keeps allocating memory
edit: Apparently this could be a difference of 'zip' for generators  between python 2 and 3 ( see &lt;denchmark-link:https://stackoverflow.com/a/20910242&gt;https://stackoverflow.com/a/20910242&lt;/denchmark-link&gt;
 ). Changing that line to
 and adding () helps. Still the problem of allocating to much memory occurs in a later state
		</comment>
		<comment id='14' author='aryamccarthy' date='2017-10-26T10:25:24Z'>
		I concur about zip. Are you using 2 or 3?
		</comment>
		<comment id='15' author='aryamccarthy' date='2017-10-26T10:30:43Z'>
		I used python2 so far. I am just about to try python3. Is there a recommendation for OpenNMT-py?
		</comment>
		<comment id='16' author='aryamccarthy' date='2017-10-26T10:56:21Z'>
		We're trying to keep it functional with both. The problem is that there are a bunch of cases where the semantics of built-ins changed between 2 and 3, and lots of stuff that used to return a list returns an iterator now. These are hard to catch because the output is the same, it's just the memory use that's different. You found one of these, and I think your  solution is correct. Feel free to submit a pull request, although it probably won't get merged right away something weird is going on with our testing and I haven't heard from &lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 in a while.
As for the continuing memory problems, it's possible there are more cases analogous to this. Switching to python3 might help with some of these. However, if I recall there's also some potentially troublesome code in torchtext, which we're using for the IO. I'll look in to it when I get the chance.
		</comment>
		<comment id='17' author='aryamccarthy' date='2017-10-26T12:59:00Z'>
		I can do that however just replacing the zip is not enough in my case.
Next I encounter the same memory problems in



OpenNMT-py/onmt/IO.py


        Lines 196 to 200
      in
      9f4b4d7






 super(ONMTDataset, self).__init__( 



 construct_final(chain([ex], examples)), 



 fields, 



 filter_pred if opt is not None 



 else None) 





which leads to the execution of the lines in Torch: site-packages/torchtext/data/dataset.py:
&lt;denchmark-code&gt;class Dataset(torch.utils.data.Dataset): 
    ... 
    def __init__(self, examples, fields, filter_pred=None):
       ...
       if filter_pred is not None:
            examples = list(filter(filter_pred, examples))
&lt;/denchmark-code&gt;

setting the filters to None fixes the problem for me (Test still running) but of course changes the behaviour.
		</comment>
		<comment id='18' author='aryamccarthy' date='2017-10-26T14:11:51Z'>
		ok. To summarize this: I can use &lt;denchmark-link:https://pythonhosted.org/six/#module-six.moves&gt;six&lt;/denchmark-link&gt;
  and add  to remove problems with the python2/3 storage difference in zip.
Then I still have the problem that if filter_pred is not None I run into memory issues. If I resolve this (by deleting filter_pred) I cant save my model (using python 3.5.2) because of:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/XYZ/OpenNMT-py/preprocess.py", line 78, in &lt;module&gt;
    main()
  File "/XYZ/OpenNMT-py/preprocess.py", line 73, in main
    torch.save(train, open(opt.save_data + '.train.pt', 'wb'))
  File "ABC/lib/python3.5/site-packages/torch/serialization.py", line 120, in save
    return _save(obj, f, pickle_module, pickle_protocol)
  File "/ABC/lib/python3.5/site-packages/torch/serialization.py", line 186, in _save
    pickler.dump(obj)
TypeError: can't pickle generator objects
&lt;/denchmark-code&gt;

I can put this in to a pull request but as I end up with a version that is not working I am not sure if I should do this.
		</comment>
		<comment id='19' author='aryamccarthy' date='2017-11-09T03:11:00Z'>
		I had preprossed the data of WMT14-EN-FR(1.7G and 2.0G individually) and resulted in train.pt with 17G, and when I load it into memory for training, it consumes 238G in total. Does It seem like some types of memory leak? Here is my script of preprocessing:
python preprocess.py -train_src /home/lgy/data/wmt/wmt14-en-fr/training-en.tok -train_tgt /home/lgy/data/wmt/wmt14-en-fr/training-fr.tok -valid_src /home/lgy/data/wmt/wmt14-en-fr/val-en.tok -valid_tgt /home/lgy/data/wmt/wmt14-en-fr/val-fr.tok -save_data /home/lgy/data/wmt/wmt14-en-fr/pywmt14 -src_vocab_size 30000 -tgt_vocab_size 30000 -src_seq_length 1000 -tgt_seq_length 1000 -dynamic_dict
		</comment>
		<comment id='20' author='aryamccarthy' date='2017-12-06T22:43:21Z'>
		Yeah I have the same problem. When I loaded the model for training, it consumed far larger memory that the file size. Any suggestions?
		</comment>
		<comment id='21' author='aryamccarthy' date='2017-12-19T07:37:36Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 Have this problem been solved?
I face the same problem.
		</comment>
		<comment id='22' author='aryamccarthy' date='2017-12-20T22:47:20Z'>
		Yeah, sorry this is our number one issue right now. &lt;denchmark-link:https://github.com/JianyuZhan&gt;@JianyuZhan&lt;/denchmark-link&gt;
 is on it. There is currently a pull request to support sharding. It should be available soon.
		</comment>
		<comment id='23' author='aryamccarthy' date='2017-12-21T02:28:33Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/JianyuZhan&gt;@JianyuZhan&lt;/denchmark-link&gt;
 Thank you for your effort.
I thought that you can refer to the works below:
&lt;denchmark-link:https://github.com/tensorflow/nmt&gt;https://github.com/tensorflow/nmt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/facebookresearch/fairseq-py/&gt;https://github.com/facebookresearch/fairseq-py/&lt;/denchmark-link&gt;

I have tried this two work, and it works and very fast.
		</comment>
		<comment id='24' author='aryamccarthy' date='2018-02-19T20:17:57Z'>
		This is fixed now.
		</comment>
		<comment id='25' author='aryamccarthy' date='2018-02-21T08:55:11Z'>
		May I know which pull request solving this?
		</comment>
		<comment id='26' author='aryamccarthy' date='2018-02-21T14:33:47Z'>
		We added sharding to the preprocess.py script. Use -max_shard_size to split the file into shard each of which are loaded one at a time.
		</comment>
		<comment id='27' author='aryamccarthy' date='2018-02-21T15:13:35Z'>
		Great, thanks for this option.
Probably this can be added to the documentation page:
&lt;denchmark-link:http://opennmt.net/OpenNMT-py/options/preprocess.html&gt;http://opennmt.net/OpenNMT-py/options/preprocess.html&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>