<bug id='306' author='mzeidhassan' open_date='2017-09-27T15:40:38Z' closed_time='2017-12-27T19:26:36Z'>
	<summary>pickle.dump memory error</summary>
	<description>
I am getting a memory error while ONMT is saving the demo.train.pt file.
Here is the output of the terminal:
Prepared 11062622 sentences (788855 ignored due to length == 0 or src len &gt; 50 or tgt len &gt; 50) Preparing validation ... Processing data/src-val.txt &amp; data/tgt-val.txt ... ... shuffling sentences ... sorting sentences by size Prepared 5300 sentences (17 ignored due to length == 0 or src len &gt; 50 or tgt len &gt; 50) Saving source vocabulary to 'data/demo.src.dict'... Saving target vocabulary to 'data/demo.tgt.dict'... Saving data to 'data/demo.train.pt'... Traceback (most recent call last): File "preprocess.py", line 296, in &lt;module&gt; main() File "preprocess.py", line 292, in main torch.save(save_data, opt.save_data + '.train.pt') File "C:\Users\Anaconda3\envs\python35\lib\site-packages\torch\serialization.py", line 120, in save return _save(obj, f, pickle_module, pickle_protocol) File "C:\Users\Anaconda3\envs\python35\lib\site-packages\torch\serialization.py", line 186, in _save pickler.dump(obj) MemoryError
Is there a workaround for this issue?
I came across this issue and it could be helpful.
&lt;denchmark-link:https://stackoverflow.com/questions/17513036/pickle-dump-huge-file-without-memory-error&gt;https://stackoverflow.com/questions/17513036/pickle-dump-huge-file-without-memory-error&lt;/denchmark-link&gt;

Thanks,
mzeid
	</description>
	<comments>
		<comment id='1' author='mzeidhassan' date='2017-10-05T18:02:42Z'>
		Yes, we are aware this is an issue and are looking into it.
		</comment>
		<comment id='2' author='mzeidhassan' date='2017-10-26T01:09:52Z'>
		I also have this issue and up to now have no fundamental solutions. A temporary work around is to divide your training set into several pieces, and use multi-fold training like cross-validation. Or you have to add more RAM to your machine.
		</comment>
		<comment id='3' author='mzeidhassan' date='2017-12-21T02:59:53Z'>
		Same issue with large training data
		</comment>
		<comment id='4' author='mzeidhassan' date='2017-12-27T19:26:33Z'>
		Okay, so we now have a way to shard the training preprocessing. Let us know if this satisfies your issues.
		</comment>
	</comments>
</bug>