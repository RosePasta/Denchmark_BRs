<bug id='142' author='pltrdy' open_date='2017-07-24T15:20:44Z' closed_time='2017-07-26T15:20:07Z'>
	<summary>Translate not using the GPU.</summary>
	<description>
Hey,
I'm facing performance issues with translation. I'm working on summarization on CNN/DM dataset (limited to -src_seq_length 400 -tgt_seq_length 100).
&lt;denchmark-h:h2&gt;Training&lt;/denchmark-h&gt;

python train.py -data $data/data.train.pt -save_model $root/model -gpu "$gpu" -copy_attn -coverage_attn -batch _size 32
&lt;denchmark-h:h2&gt;Translation (summarization in fact)&lt;/denchmark-h&gt;

python translate.py -model "$root/$best_model" -src "$test_src" -gpu "$gpu" -verbose -output pred.txt -batch_size 32

No issue with checkpoint: I'm selecting last model checkpoint file best_model=$(ls -lsrt "$root" | grep ".pt" | tail -n 1 | awk '{print $NF}')
No issue with datafile, I checked the path &amp; read rights.
Both training and translation are done on same machine, even same GPU.

&lt;denchmark-h:h2&gt;Problem&lt;/denchmark-h&gt;


No GPU usage (but the GPU is found, memory is even allocated on it)

&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |
|  0%   36C    P8     8W / 210W |    163MiB /  8108MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 1080    Off  | 0000:02:00.0     Off |                  N/A |
| 49%   51C    P2    54W / 210W |   2858MiB /  8114MiB |      6%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1218    G   /usr/lib/xorg/Xorg                             115MiB |
|    0      2551    G   compiz                                          43MiB |
|    1     19020    C   python                                        2855MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;


single CPU usage: 4 process are listed, only 1 is 100% CPU, other 7 cores are 0.0% load (no other task)

Useless to say it is taking ages to output the test target. Longer than training in fact.
Is it some known issue? Not sure where to look to fix it.
	</description>
	<comments>
		<comment id='1' author='pltrdy' date='2017-07-24T16:30:56Z'>
		Huh, clearly not using GPU at all. Will check.
		</comment>
		<comment id='2' author='pltrdy' date='2017-07-26T15:15:53Z'>
		The problem only exists with the -copy_attn.
More precisely, I found that commenting &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Translator.py#L188&gt;Translator.py:188&lt;/denchmark-link&gt;
 to line 192 (included) allow 100% GPU load.
The bottleneck is here.
		</comment>
		<comment id='3' author='pltrdy' date='2017-07-26T15:20:07Z'>
		Okay thanks. Note this code is still alpha and will likely change. We will certainly be rewriting this part in the coming weeks.
		</comment>
		<comment id='4' author='pltrdy' date='2017-07-26T15:21:46Z'>
		(note: the issue isn't probably solved, as I just virtually erased some computation. Performances are back but results may not be as wanted. I'm not sure to understand what these line are for..)
		</comment>
		<comment id='5' author='pltrdy' date='2017-07-26T15:29:10Z'>
		we're rewriting this part completely.
		</comment>
		<comment id='6' author='pltrdy' date='2018-03-12T04:31:58Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 what's the status now? can we use GPU now for translation?
		</comment>
		<comment id='7' author='pltrdy' date='2018-03-14T01:49:05Z'>
		Oh yeah. This is long solved.
		</comment>
	</comments>
</bug>