<bug id='116' author='pltrdy' open_date='2017-07-10T16:05:09Z' closed_time='2017-08-29T16:38:58Z'>
	<summary>Training with `-train_from` crashes with Adam</summary>
	<description>
Hi,
Using the last onmt-py code (pulled last week and just now) I am facing errors when trying to continue training.
My process looks like:
&lt;denchmark-h:h2&gt;Preprocessing:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;  python preprocess.py \
      -train_src $data/train.src.txt \
      -train_tgt $data/train.tgt.txt \
      -valid_src $data/valid.src.txt \
      -valid_tgt $data/valid.tgt.txt \
      -src_seq_length 400 \
      -tgt_seq_length 100 \
      -save_data $root/data
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Training:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;python train.py -data $data/data.train.pt -save_model $root/model -gpu "$gpu"
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Continue training&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;# finding best (last in fact) model
best_model=$(ls -lsrt "$root" | grep ".pt" | tail -n 1 | awk '{print $NF}')

# re-training from here
python train.py -data $data/data.train.pt -save_model $root/model -train_from "$root/$best_model" -gpu "$gpu"
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I get the following output:
&lt;denchmark-code&gt;Namespace(attention_type='dotprod', batch_size=64, brnn=False, brnn_merge='concat', context_gate=None, copy_attn=False, coverage_attn=False, curriculum=False, data='./cnndm/data.train.pt', decay_method='', decoder_layer='rnn', dropout=0.3, encoder_layer='rnn', encoder_type='text', epochs=13, experiment_name='', extra_shuffle=False, feature_vec_size=100, gpus=[], input_feed=1, lambda_coverage=1, layers=2, learning_rate=1.0, learning_rate_decay=0.5, log_interval=50, log_server='', max_generator_batches=32, max_grad_norm=5, optim='sgd', param_init=0.1, position_encoding=False, pre_word_vecs_dec=None, pre_word_vecs_enc=None, rnn_size=500, rnn_type='LSTM', save_model='exp_1/model', seed=-1, share_decoder_embeddings=False, start_checkpoint_at=0, start_decay_at=8, start_epoch=1, train_from='exp_1/coverage_acc_30.74_ppl_74.52_e13.pt', train_from_state_dict='', truncated_decoder=0, warmup_steps=4000, word_vec_size=500)
Loading data from './cnndm/data.train.pt'
Loading dicts from checkpoint at exp_1/coverage_acc_30.74_ppl_74.52_e13.pt
 * vocabulary size. source = 50004; target = 50004
 * number of training sentences. 39331
 * maximum batch size. 64
Building model...
Loading model from checkpoint at exp_1/coverage_acc_30.74_ppl_74.52_e13.pt
Traceback (most recent call last):
  File "train.py", line 442, in &lt;module&gt;
    main()
  File "train.py", line 381, in main
    generator_state_dict = chk_model.generator.state_dict()
AttributeError: 'dict' object has no attribute 'generator'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='pltrdy' date='2017-07-10T16:28:04Z'>
		Hey, this a known issue and similar to issue &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/111&gt;#111&lt;/denchmark-link&gt;
. We are currently trying to add some backwards compatibility in &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/104&gt;#104&lt;/denchmark-link&gt;
. For a quick workaround now, can you use the same code you trained originally with?
		</comment>
		<comment id='2' author='pltrdy' date='2017-07-10T16:34:52Z'>
		Oh. Right. I'll look about it.
The thing is, this issue is exactly what my experiment is on i.e. training first "default" model then try other attentions (e.g. -copy, -coverage).
		</comment>
		<comment id='3' author='pltrdy' date='2017-07-12T13:49:45Z'>
		Let's call this a feature request. Currently -copy uses a different final layer, so we don't support switching in mid trainining.
		</comment>
		<comment id='4' author='pltrdy' date='2017-07-21T12:12:05Z'>
		I just noticed that even if initial training had -copy/-coverage I can't continue training.
i.e. I train using both coverage and copy, then stop, then use the exact same cmd + -train_from xxx.pt and get the same issue.
Your point about how what I was initally reporting is a feature request make sense. But now it looks like a bug.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

It turns out, reading at the code I had to use -train_from_state_dict, it seems to be working, but I don't understand those options.
		</comment>
		<comment id='5' author='pltrdy' date='2017-07-22T04:05:28Z'>
		&lt;denchmark-link:https://github.com/pltrdy&gt;@pltrdy&lt;/denchmark-link&gt;
 for  and , i would recommend using my fork &lt;denchmark-link:https://github.com/falcondai/OpenNMT-py/tree/copy&gt;https://github.com/falcondai/OpenNMT-py/tree/copy&lt;/denchmark-link&gt;
 for now. I am still training it on CNN/Dailymail (in settings similar to See et al's paper) to verify its correctness. Will merge it soon after that.
you should use  going forward,  i believe expects a saved model from older versions. &lt;denchmark-link:https://github.com/sebastianGehrmann&gt;@sebastianGehrmann&lt;/denchmark-link&gt;
 can explain this better.
		</comment>
		<comment id='6' author='pltrdy' date='2017-07-22T17:26:47Z'>
		&lt;denchmark-link:https://github.com/falcondai&gt;@falcondai&lt;/denchmark-link&gt;
 let's get all this working with beam search before we take on users. there is no hurry for now.
		</comment>
		<comment id='7' author='pltrdy' date='2017-08-20T12:11:35Z'>
		Also when using , removing &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/master/train.py#L447&gt;this&lt;/denchmark-link&gt;
 part of the code prevents crash.
code to remove:
    if opt.train_from or opt.train_from_state_dict:
        optim.optimizer.load_state_dict(
            checkpoint['optim'].optimizer.state_dict())
crash it prevents:
Traceback (most recent call last):
  File "train.py", line 470, in &lt;module&gt;
    main()
  File "train.py", line 466, in main
    trainModel(model, trainData, validData, dataset, optim)
  File "train.py", line 283, in trainModel
    train_stats = trainEpoch(epoch)
  File "train.py", line 262, in trainEpoch
    optim.step()
  File "/fe870606-9f26-4198-8ead-daba18b70cf5/dnc/OpenNMT-py/onmt/Optim.py", line 54, in step
    self.optimizer.step()
  File "/usr/lib/python3.6/site-packages/torch/optim/adam.py", line 46, in step
    state = self.state[p]
KeyError: Parameter containing:
 4.3261e-02 -2.6617e-02  8.5256e-02  ...   6.7876e-02  2.5414e-02  8.2912e-02
-4.1675e-02 -1.9028e-01  2.6831e-02  ...   2.8189e-01 -4.0940e-01 -2.7037e-01
-3.8753e-02 -3.0040e-02  4.1044e-02  ...   2.6281e-02  7.4824e-02  3.2906e-02
                ...                   â‹±                   ...                
-1.6777e-02  1.4038e-01  1.3111e-01  ...   1.9887e-02  3.7055e-02 -1.7800e-02
-8.1981e-02 -4.5128e-02 -7.6631e-02  ...   5.1998e-02 -1.1770e-01 -6.9789e-02
 1.6157e-02 -1.7590e-02 -3.6672e-02  ...  -1.6998e-02 -6.6259e-02  1.3109e-01
[torch.cuda.FloatTensor of size 50004x500 (GPU 1)]
From &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/37&gt;#37&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='pltrdy' date='2017-08-29T16:38:58Z'>
		Should work now.
		</comment>
	</comments>
</bug>