<bug id='950' author='murthyrudra' open_date='2018-09-06T16:15:20Z' closed_time='2019-01-27T07:49:39Z'>
	<summary>"-fix_word_vecs_enc" triggers an error when reloading an existing model</summary>
	<description>
Hi, I was training the Transformer model with the options as specified in &lt;denchmark-link:link&gt;http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model&lt;/denchmark-link&gt;
. The training works fine, but the problem arises when i stop the training and resume the training using the option . I get the following error:
&lt;denchmark-code&gt;    Traceback (most recent call last):
      File "train.py", line 40, in &lt;module&gt;
        main(opt)
      File "train.py", line 27, in main
        single_main(opt)
      File "/home/rudra/OpenNMT-py/onmt/train_single.py", line 113, in main
        optim = build_optim(model, opt, checkpoint)
      File "/home/rudra/OpenNMT-py/onmt/utils/optimizers.py", line 62, in build_optim
        optim.optimizer.load_state_dict(saved_optimizer_state_dict)
      File "/home/rudra/miniconda3/envs/py36/lib/python3.6/site-packages/torch/optim/optimizer.py", line         107, in load_state_dict
        raise ValueError("loaded state dict contains a parameter group "
    ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
&lt;/denchmark-code&gt;

The error occurs when the model tries to load the optimizer state in the File utils/optimizers.py line 62. Could anyone please help me in debugging this error?
	</description>
	<comments>
		<comment id='1' author='murthyrudra' date='2018-09-06T16:45:53Z'>
		Give your command lines, both the first initial training and the train_from
		</comment>
		<comment id='2' author='murthyrudra' date='2018-09-06T16:57:10Z'>
		Hi, the initial command I ran was
&lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES=1 python train.py -data data/demo -save_model model-transformer-baseline 
-pre_word_vecs_enc data/embeddings.enc.pt -word_vec_size 300 -gpuid 1 -encoder_type transformer 
-decoder_type transformer -layers 5 -rnn_size 300 -transformer_ff 500 -heads 5 -fix_word_vecs_enc 
-max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens -normalization tokens 
-accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 
-learning_rate 1 -max_grad_norm 0 -param_init 0  -param_init_glorot -label_smoothing 0.1 
-valid_steps 5000 -save_checkpoint_steps 5000
&lt;/denchmark-code&gt;

I stopped the training after 5000 steps and ran the following command
&lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES=1 python train.py -data data/demo -save_model model-transformer-baseline 
-pre_word_vecs_enc data/embeddings.enc.pt -word_vec_size 300 -gpuid 1 -encoder_type transformer 
-decoder_type transformer -layers 5 -rnn_size 300 -transformer_ff 500 -heads 5 -fix_word_vecs_enc 
-max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens -normalization tokens 
-accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 
-learning_rate 1 -max_grad_norm 0 -param_init 0  -param_init_glorot -label_smoothing 0.1 
-valid_steps 5000 -save_checkpoint_steps 5000 -train_from model-transformer-baseline_step_5000.pt
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='murthyrudra' date='2018-09-06T18:05:13Z'>
		it might not be theproblem but you are not on master. line 62 of optimizer.py is not the same.
git pull and repost the error.
		</comment>
		<comment id='4' author='murthyrudra' date='2018-09-07T03:34:59Z'>
		Hi, I cloned the master branch and ran the model using the same command as earlier. I am getting the same error. I am using PyTorch 0.4.0 and Cuda 9.0
		</comment>
		<comment id='5' author='murthyrudra' date='2018-09-07T05:33:48Z'>
		I need the error trace to see the lines number
		</comment>
		<comment id='6' author='murthyrudra' date='2018-09-07T05:36:11Z'>
		Hi, the error occurs when the optimizer tries to load the saved state, specifically in the code optim/optimizer.py in &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py&gt;https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py&lt;/denchmark-link&gt;
 line 107. I tried printing the contents of  and  and the  has length of 218 whereas  has length of 217 and the individual contents are in the attached file
&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/files/2359639/error.txt&gt;error.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='murthyrudra' date='2018-09-07T05:40:39Z'>
		Hi, Please find the error traceback:
&lt;denchmark-code&gt;traceback (most recent call last):
  File "train.py", line 40, in &lt;module&gt;
    main(opt)
  File "train.py", line 27, in main
    single_main(opt)
  File "/home/rudra/NMT/OpenNMT/OpenNMT-py/onmt/train_single.py", line 112, in main
    optim = build_optim(model, opt, checkpoint)
  File "/home/rudra/NMT/OpenNMT/OpenNMT-py/onmt/utils/optimizers.py", line 51, in build_optim
    optim.optimizer.load_state_dict(saved_optimizer_state_dict)
  File "/home/rudra/miniconda3/envs/py36/lib/python3.6/site-packages/torch/optim/optimizer.py", line 107, in load_state_dict
raise ValueError("loaded state dict contains a parameter group "
ValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='murthyrudra' date='2018-09-07T06:34:05Z'>
		can  you remove this -pre_word_vecs_enc data/embeddings.enc.pt (and maybe the -fix_word_vecs_enc) from the second command line ?
		</comment>
		<comment id='9' author='murthyrudra' date='2018-09-07T06:54:43Z'>
		Hi, I am getting the same error. The error occurs when it tries to load the previously stored Optimizer's state. I did try bypassing the loading of Optimizer's state, but that results in perplexity being Nan values.
		</comment>
		<comment id='10' author='murthyrudra' date='2018-09-07T07:05:31Z'>
		Just as a sanity check can you please retrain from scratch without -pre_word_vecs_enc -fix_word_vecs_enc
save after afew steps, and then do a train_from ?
thanks
		</comment>
		<comment id='11' author='murthyrudra' date='2018-09-07T08:26:20Z'>
		Hi, If I train without specifying pre-trained word embeddings using pre_word_vecs_enc and the -fix_word_vecs_enc option and retraining using -train_from, the model works well. There is no error and the perplexity values seems good.
Now, I tried specifying only pre-trained word embeddings using pre_word_vecs_enc and retraining after some steps, the model works well, with no error. The problem arises only when I specify the argument fix_word_vecs_enc.
Thanks
		</comment>
		<comment id='12' author='murthyrudra' date='2019-01-27T03:19:34Z'>
		The problem has beed solved by pull request &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/1211&gt;#1211&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>