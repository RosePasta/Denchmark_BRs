<bug id='394' author='pltrdy' open_date='2017-11-28T10:44:53Z' closed_time='2018-04-11T12:21:28Z'>
	<summary>Continuing training corrupts options</summary>
	<description>
Hi,
&lt;denchmark-h:h1&gt;Error&lt;/denchmark-h&gt;

When you continue training using -train_from the checkpoint that are dropped does not contain the right options.
I reproduced it on master.
&lt;denchmark-h:h1&gt;Example&lt;/denchmark-h&gt;

I used:
run.sh:
&lt;denchmark-code&gt;preprocess(){
  mkdir -p $root
  python preprocess.py \
      -train_src $data/train.src.txt \
      -train_tgt $data/train.tgt.txt \
      -valid_src $data/valid.src.txt \
      -valid_tgt $data/valid.tgt.txt \
      -src_seq_length 100 \
      -tgt_seq_length 100 \
      -save_data $root/data \
      -src_vocab_size 20 \
      -dynamic_dict
}

train(){
  python train.py -save_model $root/model \
        -batch_size 64 \
        -layers 2 \ 
        -rnn_size 200 \
        -word_vec_size 100 \
        -rnn_type "LSTM" \
        -seed 10000 \
        -epochs 100 \
        -encoder_type "brnn" \
        -optim sgd \
        -learning_rate 1 \ 
        -gpuid 0 \ 
        -data $root/data
}

translate(){
  best_model=$(ls -lsrt $root/model*.pt | tail -n 1 | awk '{print $NF}')
  echo "Loading: $best_model"
  python translate.py -model "$best_model" \
                      -src $data/test.src.txt \
                      -gpu "$gpu" \
                      -batch_size 1 \
                      -verbose \
                      -beam_size 1

  mv pred.txt "$root/pred.txt"
  echo "Translated to $root/pred.txt"

}

retrain(){
  best_model=$(ls -lsrt $root/model*.pt | tail -n 1 | awk '{print $NF}')
  echo "Loading: $best_model"
  python train.py -train_from "$best_model" \
                  -data $root/data \
                  -save_model $root/model \
                  -batch_size $bs \
                  -gpu "$gpu"
}

for action in "$@"; do
  printf "\n****\nRunning command '$action'\n\n"
  eval "$action"
&lt;/denchmark-code&gt;

Then I train for &gt;= 1 epoch to dump a checkpoint, (on dummy dataset):
&lt;denchmark-code&gt;./run.sh train
&lt;/denchmark-code&gt;

I stop it, then retrain for &gt;= 1 epoch (to dump a checkpoint):
&lt;denchmark-code&gt;./run.sh retrain
&lt;/denchmark-code&gt;

then translation returns hyper parameters (= opt) errors
&lt;denchmark-code&gt;./run.sh translate

[...]
Loading model parameters.
While copying the parameter named encoder.embeddings.make_embedding.emb_luts.0.weight, whose dimensions in the model are torch.Size([22, 500]) and whose dimensions in the checkpoint are torch.Size([22, 100]), ...
Traceback (most recent call last):
  File "translate.py", line 133, in &lt;module&gt;
    main()
  File "translate.py", line 55, in main
    translator = onmt.Translator(opt, dummy_opt.__dict__)
  File "/home/pltrdy/pytorchwork/OpenNMT-py/onmt/Translator.py", line 29, in __init__
    model_opt, self.fields, use_gpu(opt), checkpoint)
  File "/home/pltrdy/pytorchwork/OpenNMT-py/onmt/ModelConstructor.py", line 166, in make_base_model
    model.load_state_dict(checkpoint['model'])
  File "/home/pltrdy/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 360, in load_state_dict
    own_state[name].copy_(param)
RuntimeError: inconsistent tensor size, expected tensor [22 x 500] and src [22 x 100] to have the same number of elements, but got 11000 and 2200 elements respectively at /home/pltrdy/pytorch/torch/lib/TH/generic/THTensorCopy.c:86
&lt;/denchmark-code&gt;

&lt;denchmark-h:h1&gt;work around&lt;/denchmark-h&gt;

I manually edit the checkpoint in order to fix the parameter, in my case I used something like:
&lt;denchmark-code&gt;def fix_opt(opt):
    opt.brnn = True
    opt.encoder_type = "brnn"
    opt.enc_layers = 1
    opt.dec_layers = 1
    opt.src_word_vec_size = 100
    opt.tgt_word_vec_size = 100
    opt.optim = "adam"
    opt.optim = 0.001
    opt.input_feed = 0
    opt.rnn_size = 400
     return opt

import torch
m = torch.load(checkpoint_file,  map_location=lambda storage, loc: storage)
m['opt'] = fix_opt(m['opt'])
torch.save(m, "%s.fixed" % checkpoint_file)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='pltrdy' date='2017-11-30T01:43:44Z'>
		Sorry, I didn't look into the detail. What option do you think cause the problem?   This issue has same problem at the , not sure if it is related to &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/388&gt;#388&lt;/denchmark-link&gt;
.  Will investigate this when I have time.
		</comment>
		<comment id='2' author='pltrdy' date='2017-11-30T10:00:30Z'>
		I submitted a PR. This is probably not related to &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/388&gt;#388&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>