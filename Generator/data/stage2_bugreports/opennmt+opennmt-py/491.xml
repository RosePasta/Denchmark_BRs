<bug id='491' author='KelleyYin' open_date='2017-12-28T13:44:01Z' closed_time='2018-01-08T20:21:03Z'>
	<summary>Transformer bug in pytorch 0.3</summary>
	<description>
Here's hyperparameters as follows , which are close to Tensor2Tensor .  Therefore , I want to test its performance .
python  train.py -gpuid 0  -layers 6 -rnn_size 512 -word_vec_size 512 -epochs 30 -max_grad_norm 0 -optim adam -encoder_type transformer -decoder_type transformer -position_encoding -dropout 0.1 -param_init 0 -warmup_steps 12000 -learning_rate 0.2 -decay_method noam -label_smoothing 0.1 -adam_beta2 0.98 -batch_size 80 -start_decay_at 31
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 348, in &lt;module&gt;
    main()
  File "train.py", line 344, in main
    train_model(model, train_dataset, valid_dataset, fields, optim, model_opt)
  File "train.py", line 168, in train_model
    train_stats = trainer.train(epoch, report_func)
  File "/home/mmyin/experience/OpenNMT-1228/onmt/Trainer.py", line 151, in train
    self.model(src, tgt, src_lengths, dec_state)
  File "/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/mmyin/experience/OpenNMT-1228/onmt/Models.py", line 548, in forward
    enc_hidden, context = self.encoder(src, lengths)
  File "/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/mmyin/experience/OpenNMT-1228/onmt/modules/Transformer.py", line 132, in forward
    out = self.transformer[i](out, mask)
  File "/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/mmyin/experience/OpenNMT-1228/onmt/modules/Transformer.py", line 68, in forward
    mid, _ = self.self_attn(input_norm, input_norm, input_norm, mask=mask)
  File "/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/mmyin/experience/OpenNMT-1228/onmt/modules/MultiHeadedAttn.py", line 135, in forward
    scaled = scaled.masked_fill(Variable(mask), -float('inf')) \
  File "/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py", line 430, in masked_fill
    return self.clone().masked_fill_(mask, value)
RuntimeError: value cannot be converted to type float without overflow: -inf
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='KelleyYin' date='2017-12-28T14:00:59Z'>
		I encounted the same error using the hyperparameters as follows .  What's wrong with the parameters ?
python train.py -gpuid 3 -data data/demo -layers 4 -rnn_size 1024 -word_vec_size 1024 -epochs 50 -max_grad_norm 0 -optim adam -encoder_type transformer -decoder_type transformer -position_encoding -dropout 0.2 -param_init 0 -warmup_steps 2000 -learning_rate 0.05 -decay_method noam
		</comment>
		<comment id='2' author='KelleyYin' date='2017-12-28T15:35:37Z'>
		Sounds like a bug, we'll look into it.
Did this happen immediately upon launch? What data are you using?
		</comment>
		<comment id='3' author='KelleyYin' date='2017-12-28T16:19:31Z'>
		(5): TransformerDecoderLayer(
(self_attn): MultiHeadedAttention(
(linear_keys): BottleLinear(in_features=512, out_features=512)
(linear_values): BottleLinear(in_features=512, out_features=512)
(linear_query): BottleLinear(in_features=512, out_features=512)
(sm): BottleSoftmax()
(activation): ReLU()
(dropout): Dropout(p=0.1)
(res_dropout): Dropout(p=0.1)
)
(context_attn): MultiHeadedAttention(
(linear_keys): BottleLinear(in_features=512, out_features=512)
(linear_values): BottleLinear(in_features=512, out_features=512)
(linear_query): BottleLinear(in_features=512, out_features=512)
(sm): BottleSoftmax()
(activation): ReLU()
(dropout): Dropout(p=0.1)
(res_dropout): Dropout(p=0.1)
)
(feed_forward): PositionwiseFeedForward(
(w_1): BottleLinear(in_features=512, out_features=2048)
(w_2): BottleLinear(in_features=2048, out_features=512)
(layer_norm): BottleLayerNorm(
)
(dropout_1): Dropout(p=0.1)
(dropout_2): Dropout(p=0.1)
(relu): ReLU()
)
(layer_norm_1): BottleLayerNorm(
)
(layer_norm_2): BottleLayerNorm(
)
)
)
(layer_norm): BottleLayerNorm(
)
)
(generator): Sequential(
(0): Linear(in_features=512, out_features=30004)
(1): LogSoftmax()
)
)

number of parameters: 85500212
('encoder: ', 32691200)
('decoder: ', 52809012)

Traceback (most recent call last):
&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='KelleyYin' date='2017-12-28T16:24:28Z'>
		It's not immediately and has constructed the model .  I am using  English&amp;Chinese corpus.
&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='KelleyYin' date='2017-12-28T16:57:19Z'>
		Oh I see, well it is immediate in that it is the first run of the system. Which version of pytorch are you using? This is definitely a bug on our side.
		</comment>
		<comment id='6' author='KelleyYin' date='2017-12-28T17:26:09Z'>
		import torch
print(torch.version)
0.3.0.post4
&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='KelleyYin' date='2017-12-28T17:35:38Z'>
		I encountered the same issue.
		</comment>
		<comment id='8' author='KelleyYin' date='2017-12-28T17:56:05Z'>
		ah, okay. we are trying to upgrade to 0.3 support shortly. things should work in 0.2
		</comment>
		<comment id='9' author='KelleyYin' date='2017-12-30T18:43:35Z'>
		(I don't think we fixed this yet)
		</comment>
		<comment id='10' author='KelleyYin' date='2018-01-02T23:00:26Z'>
		maybe can use -1e18 in place of float('-inf')?
&lt;denchmark-code&gt;In [1]: import torch

In [2]: from torch.autograd import Variable

In [3]: a = torch.rand(3,2)

In [4]: a_v = Variable(a)

In [5]: a_v.fill_(float('-inf'))
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-fd96f1912f7e&gt; in &lt;module&gt;()
----&gt; 1 a_v.fill_(float('-inf'))

RuntimeError: value cannot be converted to type float without overflow: -inf

In [8]: a_v.fill_(float('-1e18'))
Out[8]:
Variable containing:
-1.0000e+18 -1.0000e+18
-1.0000e+18 -1.0000e+18
-1.0000e+18 -1.0000e+18
[torch.FloatTensor of size 3x2]

In [9]: a_v.fill_(float('-1e18')).exp()
Out[9]:
Variable containing:
 0  0
 0  0
 0  0
[torch.FloatTensor of size 3x2]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='KelleyYin' date='2018-01-03T16:01:22Z'>
		Oh sure, guess that is easy enough for now. Let me know if that fixed it.
		</comment>
		<comment id='12' author='KelleyYin' date='2018-01-08T01:06:22Z'>
		This bug has been fixed by replacing '-inf' to '-1e18', I think we can close it now. &lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/KelleyYin&gt;@KelleyYin&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='KelleyYin' date='2018-01-08T02:35:54Z'>
		Looks like it's working ok for us too.
		</comment>
	</comments>
</bug>