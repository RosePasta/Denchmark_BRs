<bug id='432' author='helson73' open_date='2017-12-11T05:28:55Z' closed_time='2017-12-27T19:23:37Z'>
	<summary>Update transformer based on opennmt-tf</summary>
	<description>

forward() in  Transformer is not compatible with Translator.py (line 153), which requires context_lengths.
Transformer actually doesn't converge using default options. (PPL is just ridiculous)
Too many parameters are reported.

Some of them are already reported in other threads.
It could be hardly used in current state.
	</description>
	<comments>
		<comment id='1' author='helson73' date='2017-12-11T06:19:46Z'>
		
This is a bug due to a recent check-in:19dc63b , will fix !

2 &amp; 3.  Could you post your test result (including your parameter settings)  for us to reproduce and investigate further?  If you have any clue, also welcome feedback!
Thanks.
		</comment>
		<comment id='2' author='helson73' date='2017-12-11T06:45:52Z'>
		Parameter Setting: n_layers: 4, dimensions for embedings &amp; hiddens : 512, others are the same as default opt.py

number of parameters: 72370484
('encoder: ', 26913792)
('decoder: ', 45456692)
... ...
... ...
Epoch 13, 14500/14740; acc:   7.41; ppl: 710.32; 7383 src tok/s; 7064 tgt tok/s;   3246 s elapsed
Epoch 13, 14550/14740; acc:   7.76; ppl: 694.27; 7450 src tok/s; 7244 tgt tok/s;   3257 s elapsed
Epoch 13, 14600/14740; acc:   7.54; ppl: 702.28; 7286 src tok/s; 6979 tgt tok/s;   3269 s elapsed
Epoch 13, 14650/14740; acc:   7.82; ppl: 686.46; 7804 src tok/s; 7536 tgt tok/s;   3280 s elapsed
Epoch 13, 14700/14740; acc:   7.80; ppl: 683.17; 7389 src tok/s; 7154 tgt tok/s;   3291 s elapsed
Train perplexity: 693.605
Train accuracy: 7.69626
Validation perplexity: 1367.47
Validation accuracy: 6.81118
Decaying learning rate to 0.000244141

Due to it's default setting, learning is based on vanilla SGD, according to original paper, ADAM would be a better option.
We have a transformer networks with only 4 layers and 30K vocab size, but 72M params seem like unreasonable according to the original paper. (In the paper, 6 layers with  50K vocab size has only 65M params)
		</comment>
		<comment id='3' author='helson73' date='2017-12-11T06:49:26Z'>
		If I have an option, I will use official release (Tensor2Tensor), but for some reasons, I have to use pytorch.
I will continue to do further hack on this, if I got further progress, I will post on this thread.
		</comment>
		<comment id='4' author='helson73' date='2017-12-11T06:51:55Z'>
		In the original paper, target and output embeddings are shared, and mine is not. Maybe this made a difference.
		</comment>
		<comment id='5' author='helson73' date='2017-12-11T07:16:17Z'>
		Thanks, will fix  problem 1 today. And will investigate later.
		</comment>
		<comment id='6' author='helson73' date='2017-12-11T21:17:41Z'>
		&lt;denchmark-link:https://github.com/helson73&gt;@helson73&lt;/denchmark-link&gt;
 same problem here! Perplexity is ridiculously high. I tried adding  (I think this is the option you were referring to), but nothing changed. Here's the command I'm running:
&lt;denchmark-code&gt;python train.py -data mydata/data -save_model mydata/model -gpuid 0 -epochs 60 \
    -learning_rate_decay 0.98 -batch_size 100 -warmup_steps 32000 \
    -encoder_type transformer -decoder_type transformer \
    -word_vec_size 512 -rnn_size 512 -layers 2 -share_decoder_embeddings
&lt;/denchmark-code&gt;

I'm running also a classic rnn network for reference:
&lt;denchmark-code&gt;python train.py -data mydata/data -save_model mydata/model -gpuid 0 -epochs 50 \
    -learning_rate_decay 0.9 -batch_size 128 -warmup_steps 16000 \
    -encoder_type rnn  -decoder_type rnn \
    -word_vec_size 512 -rnn_size 512 -layers 2
&lt;/denchmark-code&gt;

After 12 epochs reached Validation perplexity: 3.53089 while the transformer one is stuck at Validation perplexity: 745.375
		</comment>
		<comment id='7' author='helson73' date='2017-12-11T21:24:04Z'>
		The transformer module is not fully documented yet, but it should work with the correct set of parameters. Try the following (in addition to batch_size, gpuid, data etc.) :
-layers  4 -rnn_size 1024  -word_vec_size 1024 -epochs 50  -max_grad_norm 0 -optim adam -encoder_type transformer -decoder_type transformer -position_encoding -dropout 0.2 -param_init 0 -warmup_steps 2000 -learning_rate 0.05 -decay_method noam
You might have to play around with the learning rate and number of warmup steps a bit, but it should converge within 40 epochs (for me, typically around epoch 37 on a small data set)
		</comment>
		<comment id='8' author='helson73' date='2017-12-11T21:59:49Z'>
		The recommended parameters should be as close as possible to the &lt;denchmark-link:https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py#L791&gt;transformer_base&lt;/denchmark-link&gt;
 configuration used in Tensor2Tensor. It's likely that these hyperparameters were validated at Google scale.
It should also be pointed out that this "base" configuration slightly differs from the paper. In particular, the "Add &amp; Norm" layer is split and layer normalization is moved to the block input. Also dropout is applied after the ReLU activation. It might be a good idea to apply these changes here as well.
		</comment>
		<comment id='9' author='helson73' date='2017-12-12T06:59:05Z'>
		&lt;denchmark-link:https://github.com/sebastianGehrmann&gt;@sebastianGehrmann&lt;/denchmark-link&gt;
 much better now! At epoch 13 the engine reached  and it is decreasing.
Anyway, do you confirm that the number of parameters is too high respect to the same topology with RNN? I have ~180M parameters, almost the same amount of the RNN network.
		</comment>
		<comment id='10' author='helson73' date='2017-12-12T20:32:17Z'>
		thanks &lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 . Were you able to replicate results in opennmt-tf? I will try to make our model as close as possible to your version, and then publish the exact code.
		</comment>
		<comment id='11' author='helson73' date='2017-12-12T21:37:02Z'>
		The implementation over at OpenNMT-tf is very close to the one implemented in Tensor2Tensor and so gets competitive results. However, batch size is important when using the Noam decay and they use a different semantic for it: the number of tokens. This is also something to consider and explore.
Here are the direct links to the implementation if you want to look it up:

https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/utils/transformer.py
https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/encoders/self_attention_encoder.py
https://github.com/OpenNMT/OpenNMT-tf/blob/master/opennmt/decoders/self_attention_decoder.py

		</comment>
		<comment id='12' author='helson73' date='2017-12-19T10:45:17Z'>
		Other issues about gaps between transformer-py and T2T:

Label smoothing. label smoothing of T2T
Shared embeddings for decoder: When embeddings are shared between target input and output, input embeddings should be multiplied by sqrt(d-model), where d-model is dimension of embeddings.
embeddings of T2T

&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='helson73' date='2017-12-19T10:59:56Z'>
		&lt;denchmark-link:https://github.com/davidecaroselli&gt;@davidecaroselli&lt;/denchmark-link&gt;
 In the paper, #parameters per hyperparameters are reported in detail.
As far as I found, if input and output embeddings of decoder are shared, number of parameters is quite close to the one reported.
		</comment>
		<comment id='14' author='helson73' date='2017-12-19T12:02:46Z'>
		Seems like embedding weight tying in Transformer is a little bit more complicated than I expected.
In &lt;denchmark-link:https://arxiv.org/pdf/1706.03762.pdf&gt;Attention is All You Need&lt;/denchmark-link&gt;
 :

We also use the usual learned linear transformation
and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to 29.

I have to check the sourcecode further, but it sounds like they applied Three-Way-Weight-Tying (shared embeddings for source, target input, target output).
		</comment>
		<comment id='15' author='helson73' date='2017-12-20T09:58:47Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/JianyuZhan&gt;@JianyuZhan&lt;/denchmark-link&gt;

I applied following changes as &lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 suggested,

Move norm layer to the block input,
Apply dropout after ReLU in point-wise feed forward block,

and trained on small dataset (WAT 2016 Japanese-to-English, randomly selected 100K sentence pairs) for 20 epochs,
got following result.
Before:


Train perplexity: 32.4537
Train accuracy: 38.7784
Validation perplexity: 56.2081
Validation accuracy: 33.0021
After:


Train perplexity: 29.8781
Train accuracy: 40.5064
Validation perplexity: 52.0337
Validation accuracy: 34.0583
		</comment>
		<comment id='16' author='helson73' date='2017-12-20T10:07:44Z'>
		I forgot to mention that layer normalization is also applied on the encoder and decoder final output.
Also, they did not always use weight sharing. For example, the big ENFR model was trained without it.
		</comment>
		<comment id='17' author='helson73' date='2017-12-20T10:14:11Z'>
		&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 I also applied that, actually I did those changes based on opennmt-tf sourcecode.
About weight sharing, once it was switched on, PPL became extremely high, I am trying to figure out what causes this.
		</comment>
		<comment id='18' author='helson73' date='2017-12-20T22:52:03Z'>
		

We should have embedding weight sharing implemented. share_embeddings and share_decoder_embeddings. Do you see issues with those?


Label smoothing shouldn't be so bad. I think we could just replace this line
https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Loss.py#L111


		</comment>
		<comment id='19' author='helson73' date='2017-12-20T23:13:48Z'>
		&lt;denchmark-link:https://github.com/helson73&gt;@helson73&lt;/denchmark-link&gt;
 thanks for the PR.
Do you want to try to do label smoothing as well? Here's roughly how I would do it:
class LabelSmooth:
  def __init__(self, confidence, vocab_size):
     vocab_size = float(vocab_size)
     self.crit = nn.KLDivLoss()
     self.low_confidence = (1.0 - confidence) / (vocab_size - 1)
     self.confidence = confidence
     entropy = \
        confidence * torch.log(confidence) + (vocab_size - 1) *
        low_confidence * torch.log(low_confidence + 1e-20)

  def forward(self, log_probs, targets):    
    soft_targets.fill_(low_confidence)
    soft_targets.scatter_(1, targets, confidence)
    return self.crit(log_probs, soft_targets) - entropy
(probably want to check that this gives the exact same answer as the standard loss with confidence=1
		</comment>
		<comment id='20' author='helson73' date='2017-12-21T04:51:11Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

Thank you for merging PR.
I implemented as follows, similar with yours, but entropy term is omitted due to it has no influence to learning.
class NMTLossCompute(LossComputeBase):

    def __init__(self, generator, tgt_vocab, label_smoothing=0.0):
        super(NMTLossCompute, self).__init__(generator, tgt_vocab)

        weight = torch.ones(len(tgt_vocab))
        weight[self.padding_idx] = 0
        if label_smoothing &gt; 0:
            self.criterion = nn.KLDivLoss(size_average=False)
            self.p_hot = 1 - label_smoothing * (1 - 1 / len(tgt_vocab))
            self.one_hot = torch.randn(1, len(tgt_vocab))
            self.one_hot.fill_(label_smoothing / len(tgt_vocab))
            self.register_buffer('one_hot_vec', self.one_hot)
        else:
            self.criterion = nn.NLLLoss(weight, size_average=False)
        self.label_smoothing = label_smoothing

    def make_shard_state(self, batch, output, range_, attns=None):
        """ See base class for args description. """
        return {
            "output": output,
            "target": batch.tgt[range_[0] + 1: range_[1]],
        }

    def compute_loss(self, batch, output, target):
        """ See base class for args description. """
        scores = self.generator(self.bottle(output))

        target_feed = target.view(-1)
        if self.label_smoothing &gt; 0:
            target_ = Variable(self.one_hot.repeat(target_feed.size(0), 1),
                               requires_grad=False)
            target_.scatter_(1, target_feed.unsqueeze(1), self.p_hot)
            target_feed = target_

        loss = self.criterion(scores, target_feed)
        loss_data = loss.data.clone()

        stats = self.stats(loss_data, scores.data, target.view(-1).data)

        return loss, stats

    def cuda(self):
        super(NMTLossCompute, self).cuda()
        if self.label_smoothing &gt; 0:
            self.one_hot = self.one_hot.cuda()

    def cpu(self):
        super(NMTLossCompute, self).cpu()
        if self.label_smoothing &gt; 0:
            self.one_hot = self.one_hot.cpu()
Accuracy is slightly improved on my small dataset, need further explore. BTW, #class should be (vocab_size-1) due to the PAD symbol, but since vocab_size is huge, I just ignore that part. I split label smoothing and normal NLL loss, because KLDivLoss usually has higher computing cost. When I applied KLDivLoss on GTX 1080 with batch size 64, processing speed dropped from 6800 words/s to 6600 words/s.
		</comment>
		<comment id='21' author='helson73' date='2017-12-21T05:07:58Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

About weight sharing embeddings, I didn't see any issue from code.
But it's quite weird that once I turn weight sharing option on, PPL became extremely high. (only with Transformer, not sure whether same happens to RNN models)
		</comment>
		<comment id='22' author='helson73' date='2017-12-21T06:12:27Z'>
		&lt;denchmark-link:https://github.com/helson73&gt;@helson73&lt;/denchmark-link&gt;
, you don't need to write your  and  implementation, since  is subclass of , the  will get to gpu if the whole object do so.
		</comment>
		<comment id='23' author='helson73' date='2017-12-21T06:43:57Z'>
		&lt;denchmark-link:https://github.com/JianyuZhan&gt;@JianyuZhan&lt;/denchmark-link&gt;
 Yeah, I see it, actually that's a dirty patch during debugging.
New code:
class NMTLossCompute(LossComputeBase):
    """
    Standard NMT Loss Computation.
    """
    def __init__(self, generator, tgt_vocab, label_smoothing=0.0):
        super(NMTLossCompute, self).__init__(generator, tgt_vocab)

        weight = torch.ones(len(tgt_vocab))
        weight[self.padding_idx] = 0
        if label_smoothing &gt; 0:
            self.criterion = nn.KLDivLoss(size_average=False)
            self.p_hot = 1 - label_smoothing * (1 - 1 / len(tgt_vocab))
            one_hot = torch.randn(1, len(tgt_vocab))
            one_hot.fill_(label_smoothing / len(tgt_vocab))
            self.register_buffer('one_hot', one_hot)
        else:
            self.criterion = nn.NLLLoss(weight, size_average=False)
        self.label_smoothing = label_smoothing

    def make_shard_state(self, batch, output, range_, attns=None):
        """ See base class for args description. """
        return {
            "output": output,
            "target": batch.tgt[range_[0] + 1: range_[1]],
        }

    def compute_loss(self, batch, output, target):
        """ See base class for args description. """
        scores = self.generator(self.bottle(output))

        target_feed = target.view(-1)
        if self.label_smoothing &gt; 0:
            target_ = Variable(self.one_hot.repeat(target_feed.size(0), 1),
                               requires_grad=False)
            target_.scatter_(1, target_feed.unsqueeze(1), self.p_hot)
            target_feed = target_

        loss = self.criterion(scores, target_feed)
        loss_data = loss.data.clone()

        stats = self.stats(loss_data, scores.data, target.view(-1).data)

        return loss, stats
		</comment>
		<comment id='24' author='helson73' date='2017-12-21T07:17:10Z'>
		Okay, so by default the label_smoothing behavior is off?  We would provide a train option for this or not? Besides, it would be better to document in code for this, at least reference is ok.
		</comment>
		<comment id='25' author='helson73' date='2017-12-21T17:18:03Z'>
		&lt;denchmark-link:https://github.com/JianyuZhan&gt;@JianyuZhan&lt;/denchmark-link&gt;

Yes, default is off.
I created a new PR.
Here is the reference paper. Please see chapter 7.
&lt;denchmark-link:https://arxiv.org/abs/1512.00567&gt;https://arxiv.org/abs/1512.00567&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='helson73' date='2017-12-22T18:59:29Z'>
		About hyperparameters, I use followings, which is close to the base setting in Tensor2Tensor.
python -u train.py -gpuid 0 -save_model models/ja-en-1M/ja-en -data database/ja-en -layers 6 -rnn_size 512 -word_vec_size 512 -epochs 30 -max_grad_norm 0 -optim adam -encoder_type transformer -decoder_type transformer -position_encoding -dropout 0.1 -param_init 0 -warmup_steps 12000 -learning_rate 0.2 -decay_method noam -label_smoothing 0.1 -adam_beta2 0.98 -batch_size 80 -start_decay_at 31
start_decay_at option is enabled in default, which should be turned off otherwise learning rate might be unnecessarily decreased. This should be fixed in the future, make that disabled when noam decay is enabled.
On the other hand, warmup_steps option is varied case by case, which depends on batch size and training data size. If it is set properly, model will converge fast, and vice versa. One may want to check at which step learning rate should reach the peek.
		</comment>
		<comment id='27' author='helson73' date='2017-12-27T19:23:37Z'>
		Going to close this one as it is basically all done. Huge props to &lt;denchmark-link:https://github.com/helson73&gt;@helson73&lt;/denchmark-link&gt;
 . I will add you to the README.
		</comment>
	</comments>
</bug>