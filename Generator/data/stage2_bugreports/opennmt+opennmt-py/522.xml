<bug id='522' author='vince62s' open_date='2018-01-18T14:44:43Z' closed_time='2018-03-05T13:39:06Z'>
	<summary>[question] Shards sorting / shuffling</summary>
	<description>
&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/JianyuZhan&gt;@JianyuZhan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/helson73&gt;@helson73&lt;/denchmark-link&gt;

Guys,
I have a question for both the sharding preprocessing and the batching at training time.
I observed that during an epoch the PPL starts quite high, keeps decreasing and after it's finished it starts again very high.
I think it is due to the fact that the dataset may be sorted in some ways both during preprocess sharding, and also before batching in buckets.
I think buckets of the same length must be shuffled between them so that we do not see this phenomenon of ppl starting high at the beg of each epoch.
	</description>
	<comments>
		<comment id='1' author='vince62s' date='2018-01-18T14:49:39Z'>
		Yes, I agree. We should also be much more explicit about lengths in the debug output as well.
		</comment>
		<comment id='2' author='vince62s' date='2018-01-20T19:45:51Z'>
		I am not familiar with data loading process.
But how it comes to consume so huge memory(dozens of GBs) while the original data size is quite small(~3GB)?
		</comment>
		<comment id='3' author='vince62s' date='2018-01-24T06:38:35Z'>
		&lt;denchmark-link:https://github.com/helson73&gt;@helson73&lt;/denchmark-link&gt;
 the generated  files are actually , which contains , which consists of the original data, among other things, like loaded library, etc.
		</comment>
		<comment id='4' author='vince62s' date='2018-01-24T21:27:04Z'>
		Hmm, having a bit of trouble understanding this issue. Within a shard we batch with sentences of the same length, but the order of these batches are randomly shuffled. i.e. batches will alternate between long and short sentences.
I guess we don't shuffle before sharding, so if the original data is sorted we don't mix that up. Is that the phenomenon you are describing?
		</comment>
		<comment id='5' author='vince62s' date='2018-01-25T09:21:44Z'>
		Yes I think this is it.
My Dataset is not sorted but it is the concat of 3 datasets (for WMT14) so they may be each in a similar domain (especially europarl)
So shuffling before sharding should do the job.
		</comment>
		<comment id='6' author='vince62s' date='2018-03-03T16:41:30Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

I have a question regarding the way we fill the token batches.
In T2T/opennmt-tf we first use this bucket_width to sort the data and when we fill the batch we make sure that all sentences are about the same length (at bucket_width max difference).
If we don't do so, maybe the token batch is made of various length sentences and if there is too m uch padding it does take too much memory.
not sure I am clear.
		</comment>
		<comment id='7' author='vince62s' date='2018-03-05T13:39:06Z'>
		closing this for now:
Sharding is done.
Shuffling needs to be done offline before preprocessing.
Bucketing is now fixed and no more memory issue.
		</comment>
		<comment id='8' author='vince62s' date='2018-06-21T20:27:12Z'>
		I fell into the same rabbit hole when trying to interpret the weird training curve observed during training.
My question is: are the data re-shuffled anywhere for each epoch? From my understanding of the code, neither shard loading nor data iteration within shards perform any shuffling. Not sure if I'm correct though.
		</comment>
	</comments>
</bug>