<bug id='1024' author='zhujie02' open_date='2018-10-31T16:28:26Z' closed_time='2019-08-22T03:14:52Z'>
	<summary>all_gather_list reports error with multi-gpu and truncated_decoder option</summary>
	<description>
&lt;denchmark-h:h1&gt;steps to reproduce the error&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;env setting&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;pytorch v0.4.1
torchtext v0.3.0
OpenNMT-py v0.5.0
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;obtain the data&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;https://github.com/tensorflow/nmt/blob/master/nmt/scripts/download_iwslt15.sh
&lt;/denchmark-code&gt;

after downloading, delete special words in the data, like &lt;unk&gt; &lt;s&gt; &lt;/s&gt;
&lt;denchmark-h:h2&gt;preprocessing&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;python preprocess.py \
    -data_type text \
    -train_src data/iwslt15/train.vi \
    -train_tgt data/iwslt15/train.en \
    -valid_src data/iwslt15/tst2012.vi \
    -valid_tgt data/iwslt15/tst2012.en \
    -save_data data/iwslt15/iwslt15 \
    -src_words_min_frequency 0 \
    -tgt_words_min_frequency 0 \
    -src_seq_length 800 \
    -tgt_seq_length 800 \
    -src_seq_length_trunc 400 \
    -tgt_seq_length_trunc 400 \
    -dynamic_dict \
    -shard_size 200000 \
    -src_vocab data/iwslt15/vocab.vi \
    -tgt_vocab data/iwslt15/vocab.en \
    -src_vocab_size 8000 \
    -tgt_vocab_size 18000 \
    -seed 1234 \
#    -shuffle 1
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;training&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;CUDA_VISIBLE_DEVICES="1, 2, 3, 4, 5, 6, 7" \
    python train.py \
    -model_type text \
    -save_model models/iwslt15 \
    -data data/iwslt15/iwslt15 \
    -global_attention mlp \
    -word_vec_size 32 \
    -rnn_size 32 \
    -layers 1 \
    -encoder_type rnn \
    -decoder_type rnn \
    -train_steps 800000 \
    -report_every 5 \
    -max_grad_norm 2 \
    -dropout 0. \
    -batch_size 4 \
    -optim adagrad \
    -learning_rate 0.05 \
    -adagrad_accumulator_init 0.1 \
    -copy_loss_by_seqlength \
    -bridge -seed 777 \
    -world_size 7 \
    -gpu_ranks 0 1 2 3 4 5 6 \
    -gpu_verbose_level 0 \
    -tensorboard \
    -tensorboard_log_dir runs/iwslt15_test2 \
    -bridge \
    -truncated_decoder 30
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;error message&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/OpenNMT-py-0.5.0/train.py", line 63, in run
    single_main(opt, device_id)
  File "/home/OpenNMT-py-0.5.0/onmt/train_single.py", line 141, in main
    opt.valid_steps)
  File "/home/OpenNMT-py-0.5.0/onmt/trainer.py", line 182, in train
    report_stats)
  File "/home/OpenNMT-py-0.5.0/onmt/trainer.py", line 354, in _maybe_report_training
    multigpu=self.n_gpu &gt; 1)
  File "/home/OpenNMT-py-0.5.0/onmt/utils/report_manager.py", line 69, in report_training
    report_stats = onmt.utils.Statistics.all_gather_stats(report_stats)
  File "/home/OpenNMT-py-0.5.0/onmt/utils/statistics.py", line 42, in all_gather_stats
    stats = Statistics.all_gather_stats_list([stat], max_size=max_size)
  File "/home/OpenNMT-py-0.5.0/onmt/utils/statistics.py", line 59, in all_gather_stats_list
    all_stats = all_gather_list(stat_list, max_size=max_size)
  File "/home/OpenNMT-py-0.5.0/onmt/utils/distributed.py", line 120, in all_gather_list
    result = pickle.loads(bytes_list)
_pickle.UnpicklingError: invalid load key, '\xe5'.
&lt;/denchmark-code&gt;

after removing the truncated_decoder option, everything works well.
how to train the model with both multi-gpu and truncated bptt? is there anything I missed?
	</description>
	<comments>
		<comment id='1' author='zhujie02' date='2018-10-31T16:42:31Z'>
		before we dig, can you please confirm that the exact same command line without truncated_decoder works fine ?
		</comment>
		<comment id='2' author='zhujie02' date='2018-10-31T16:56:39Z'>
		thank you for your quick reply. &lt;denchmark-link:https://github.com/vince62s&gt;@vince62s&lt;/denchmark-link&gt;

here is the training output of the program without truncated_decoder :
&lt;denchmark-code&gt;[2018-10-31 16:49:57,923 INFO] Loading train dataset from data/iwslt15/iwslt15.train.0.pt, number of examples: 133166
[2018-10-31 16:49:57,955 INFO]  * vocabulary size. source = 7708; target = 17192
[2018-10-31 16:49:57,955 INFO] Building model...
[2018-10-31 16:50:01,441 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(7708, 32, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(32, 32)
    (bridge): ModuleList(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
    )
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(17192, 32, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.0)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.0)
      (layers): ModuleList(
        (0): LSTMCell(64, 32)
      )
    )
    (attn): GlobalAttention(
      (linear_context): Linear(in_features=32, out_features=32, bias=False)
      (linear_query): Linear(in_features=32, out_features=32, bias=True)
      (v): Linear(in_features=32, out_features=1, bias=False)
      (linear_out): Linear(in_features=64, out_features=32, bias=True)
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=32, out_features=17192, bias=True)
    (1): LogSoftmax()
  )
)
[2018-10-31 16:50:01,441 INFO] encoder: 257216
[2018-10-31 16:50:01,441 INFO] decoder: 1134216
[2018-10-31 16:50:01,441 INFO] * number of parameters: 1391432
[2018-10-31 16:50:01,595 INFO] Start training...
[2018-10-31 16:50:17,529 INFO] Loading train dataset from data/iwslt15/iwslt15.train.0.pt, number of examples: 133166
[2018-10-31 16:50:19,337 INFO] Step  5/800000; acc:   4.02; ppl: 15851.24; xent: 9.67; lr: 0.05000; 6569/5444 tok/s;      2 sec
[2018-10-31 16:50:20,035 INFO] Step 10/800000; acc:   4.13; ppl: 13644.56; xent: 9.52; lr: 0.05000; 4824/4397 tok/s;      3 sec
[2018-10-31 16:50:20,617 INFO] Step 15/800000; acc:   4.04; ppl: 8416.51; xent: 9.04; lr: 0.05000; 6640/5826 tok/s;      3 sec
[2018-10-31 16:50:21,452 INFO] Step 20/800000; acc:   4.34; ppl: 3067.27; xent: 8.03; lr: 0.05000; 6974/5652 tok/s;      4 sec
[2018-10-31 16:50:22,039 INFO] Step 25/800000; acc:   5.71; ppl: 1682.36; xent: 7.43; lr: 0.05000; 5716/4985 tok/s;      5 sec
[2018-10-31 16:50:22,562 INFO] Step 30/800000; acc:   4.69; ppl: 1741.09; xent: 7.46; lr: 0.05000; 6500/5381 tok/s;      5 sec
[2018-10-31 16:50:23,221 INFO] Step 35/800000; acc:   6.50; ppl: 1204.36; xent: 7.09; lr: 0.05000; 4361/3909 tok/s;      6 sec
&lt;/denchmark-code&gt;

of course, others are just toy params, not carefully tuned.
		</comment>
		<comment id='3' author='zhujie02' date='2019-04-02T09:54:18Z'>
		&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/UNMT-dev/train.py", line 59, in run
    single_multi_main(opt, device_id)
  File "/home/UNMT-dev/onmt/train_single.py", line 208, in multi_main
    valid_steps=opt.valid_steps)
  File "/home/UNMT-dev/onmt/trainer.py", line 677, in train
    normalization = sum(onmt.utils.distributed.all_gather_list(normalization))
  File "/home/UNMT-dev/onmt/utils/distributed.py", line 120, in all_gather_list
    result = pickle.loads(bytes_list)
EOFError: Ran out of input
&lt;/denchmark-code&gt;

find this error, but I change corpus, it fixed. Is there any invalid character not permit?
		</comment>
		<comment id='4' author='zhujie02' date='2019-08-22T03:14:52Z'>
		actually, it's a problem of distributed training, due to different data length of different batches.
already fixed in current master branch~
		</comment>
	</comments>
</bug>