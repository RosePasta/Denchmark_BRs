<bug id='241' author='SyndicateYe' open_date='2017-09-08T02:40:06Z' closed_time='2017-09-08T13:37:36Z'>
	<summary>Transformer translation problem</summary>
	<description>
1.When I tried to use transformer to encoder and decoder, I changed the layers to 2. Then there is error said
&lt;denchmark-code&gt;THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
Traceback (most recent call last):
 File "train.py", line 310, in &lt;module&gt;
   main()
 File "train.py", line 306, in main
   train_model(model, train, valid, fields, optim)
 File "train.py", line 169, in train_model
   train_stats = train_epoch(epoch)
 File "train.py", line 149, in train_epoch
   optim.step()
 File "/home/xiaodong/OpenNMT-py-master/onmt/Optim.py", line 54, in step
   self.optimizer.step()
 File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py", line 68, in step
   denom = exp_avg_sq.sqrt().add_(group['eps'])
RuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66
&lt;/denchmark-code&gt;

However, I can train the model with the -layers 2
This is my train options.
&lt;denchmark-code&gt;python train.py -data split_data/preprocessed -save_model split_model/cls2mdn -layers 2 -rnn_size 1024 -word_vec_size 1024 -batch_size 32 -epochs 40 -gpuid 0 -report_every 100  -max_grad_norm 0 -optim adam -encoder_type transformer -decoder_type transformer -position_encoding -dropout 0.2 -param_init 0 -warmup_steps 4000 -learning_rate 1 -start_checkpoint_at 20 -decay_method noam
&lt;/denchmark-code&gt;

2.When I tried to translate the test docs using the trained model, there sometimes was a problem.
&lt;denchmark-code&gt;Loading model
Traceback (most recent call last):
  File "translate.py", line 151, in &lt;module&gt;
    main()
  File "translate.py", line 94, in main
    = translator.translate(batch, data)
  File "/home/xiaodong/OpenNMT-py-master/onmt/Translator.py", line 190, in translate
    pred, predScore, attn, goldScore = self.translateBatch(batch, data)
  File "/home/xiaodong/OpenNMT-py-master/onmt/Translator.py", line 139, in translateBatch
    self.model.decoder(inp, context, decStates)
  File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/xiaodong/OpenNMT-py-master/onmt/Models.py", line 638, in forward
    aeq(input_batch, contxt_batch, src_batch, tgt_batch)
  File "/home/xiaodong/OpenNMT-py-master/onmt/modules/Util.py", line 10, in aeq
    + ") doesn't equals to each of " + str(rest)
AssertionError: base(150) doesn't equals to each of (150, 30, 150)
&lt;/denchmark-code&gt;

Whatever I use transformer or brnn.
	</description>
	<comments>
		<comment id='1' author='SyndicateYe' date='2017-09-08T03:22:57Z'>
		looking into this issue.
		</comment>
		<comment id='2' author='SyndicateYe' date='2017-09-08T03:57:39Z'>
		&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/242&gt;#242&lt;/denchmark-link&gt;
 Should fix it. Please confirm.
		</comment>
		<comment id='3' author='SyndicateYe' date='2017-09-08T11:40:46Z'>
		thx. It works well.
		</comment>
	</comments>
</bug>