<bug id='277' author='jingxil' open_date='2017-09-18T17:08:31Z' closed_time='2017-09-20T21:25:06Z'>
	<summary>hidden state shapes returned by SRU and LSTM are not consistent</summary>
	<description>
The hidden state returned by LSTM has the shape [layers*directions x batch x dim], while the one returned by SRU has the shape [layers, batch, dim*directions]. Current version seems to ignore this inconsistency. I think possible solution could be A) modify the shape of hidden state returned by SRU  or  B) make a special case for SRU in init_decoder_state function.
	</description>
	<comments>
		<comment id='1' author='jingxil' date='2017-09-19T06:12:46Z'>
		I suggest to override init_decoder_state function in SRURNNDecoder which is inherited from StdRNNDecoder. If this solution sounds all right, I would like to propose a PR.
		</comment>
		<comment id='2' author='jingxil' date='2017-09-19T06:26:54Z'>
		This approach is better, please refer to TransformerDecoderState and CNNDecoderState as well,  to make their interfaces consistent.
		</comment>
	</comments>
</bug>