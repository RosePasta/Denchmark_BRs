<bug id='699' author='comja' open_date='2018-04-19T05:59:29Z' closed_time='2018-08-02T20:48:43Z'>
	<summary>Total Batch value is logged incorrectly.</summary>
	<description>
I wonder when it stops training a model
For example, below is log row that I got
Epoch  1, 25300/ 9477; acc:  46.97; ppl:  16.42; xent:   2.80; 149 src tok/s; 146 tgt tok/s; 179660 s elapsed
So here, epoch is 1, 9477 is the number of batches and 25300 is the current batch. How it's possible that the current batch is bigger than the number of batches.
	</description>
	<comments>
		<comment id='1' author='comja' date='2018-04-21T23:30:11Z'>
		Can you send the preprocessing command that you used?
		</comment>
	</comments>
</bug>