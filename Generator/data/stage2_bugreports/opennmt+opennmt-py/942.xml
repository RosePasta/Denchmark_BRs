<bug id='942' author='ruidongtd' open_date='2018-09-04T06:40:18Z' closed_time='2018-09-05T10:16:13Z'>
	<summary>valid_batch_size</summary>
	<description>
In the train_single.py file , lines 120 to 124
&lt;denchmark-code&gt;def train_iter_fct(): return build_dataset_iter(
        lazily_load_dataset("train", opt), fields, opt)

    def valid_iter_fct(): return build_dataset_iter(
        lazily_load_dataset("valid", opt), fields)
&lt;/denchmark-code&gt;

should be changed
&lt;denchmark-code&gt;def train_iter_fct(): return build_dataset_iter(
        lazily_load_dataset("train", opt), fields, opt)

    def valid_iter_fct(): return build_dataset_iter(
        lazily_load_dataset("valid", opt), fields, opt, is_train=False)
&lt;/denchmark-code&gt;

If it doesn't, it will not use valid_batch_size.
	</description>
	<comments>
		<comment id='1' author='ruidongtd' date='2018-09-04T06:47:58Z'>
		Good point, wondering why it never popped before..., can you please PR ?
		</comment>
		<comment id='2' author='ruidongtd' date='2018-09-04T07:06:59Z'>
		First, if it doesn't, it will not use valid_batch_size, and it will use opt.batch_size.
In the inputter.py file. lines 466 to 472.
&lt;denchmark-code&gt;def build_dataset_iter(datasets, fields, opt, is_train=True):
    """
    This returns user-defined train/validate data iterator for the trainer
    to iterate over. We implement simple ordered iterator strategy here,
    but more sophisticated strategy like curriculum learning is ok too.
    """
    batch_size = opt.batch_size if is_train else opt.valid_batch_size
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ruidongtd' date='2018-09-04T07:22:03Z'>
		I know I get it, just asking you if you can send a PR .... thanks
		</comment>
		<comment id='4' author='ruidongtd' date='2018-09-05T10:40:38Z'>
		ok
		</comment>
	</comments>
</bug>