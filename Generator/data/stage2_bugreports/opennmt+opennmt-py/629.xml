<bug id='629' author='nikhilweee' open_date='2018-03-22T04:53:47Z' closed_time='2018-05-31T09:32:45Z'>
	<summary>-attn_debug does nothing</summary>
	<description>
A quick find through the code (and &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/285#issue-142350789&gt;#285 (comment)&lt;/denchmark-link&gt;
) tells me that the following functionality (which was once present) is missing.


I'm willing to fix this, but this conflicts with &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/623&gt;#623&lt;/denchmark-link&gt;
. What is the suggested way to go?
	</description>
	<comments>
		<comment id='1' author='nikhilweee' date='2018-03-23T14:24:59Z'>
		Yes good question. We originally printed out the tensors for attention but it was quite hard to read. Maybe you could print it out in a prettier way?
		</comment>
		<comment id='2' author='nikhilweee' date='2018-03-27T05:06:59Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
: How about something along the following lines?
&lt;denchmark-code&gt;SENT 111: ('SELECT', "'Official", 'registration', "notes'", 'FROM', "'TABLE'", 'WHERE', "'Website'", '=', "'usek.edu.lb'")
PRED 111: what is the official uk value for the verb genre
               SELECT    'Offici    registr     notes'       FROM    'TABLE'      WHERE    'Websit          =    'usek.e
      what  0.0846188 *0.2082578  0.0883837  0.0923672  0.1498164  0.1597425  0.0853733  0.0134627  0.0499635  0.0680141
        is  0.0332641 *0.4930810  0.1704664  0.0593620  0.0790212  0.0782350  0.0315087  0.0231864  0.0168549  0.0150202
       the  0.0338663 *0.6660646  0.0918875  0.0269391  0.0483395  0.0558411  0.0244910  0.0224096  0.0161286  0.0140326
  official  0.0000866 *0.9795582  0.0178013  0.0011653  0.0002517  0.0003294  0.0001192  0.0005375  0.0000487  0.0001022
        uk  0.0016796  0.0402414 *0.7090231  0.2094505  0.0095488  0.0072347  0.0029233  0.0078895  0.0064761  0.0055329
     value  0.0043845  0.0143043  0.4274417 *0.4393372  0.0299011  0.0217524  0.0107041  0.0190628  0.0236078  0.0095042
       for  0.0823848  0.0201787  0.0532638  0.1637556 *0.1758792  0.1559338  0.1184693  0.0452287  0.1010770  0.0838291
       the  0.0153939  0.0145299  0.0078321  0.0285574  0.0536341  0.0571012  0.0518566 *0.4666554  0.1214725  0.1829669
      verb  0.0020748  0.0473116  0.0087327  0.0139740  0.0071218  0.0100148  0.0118727 *0.7860709  0.0449136  0.0679133
     genre  0.0014737  0.0000718  0.0208369  0.1750923  0.0085761  0.0052418  0.0062615 *0.5167642  0.2342641  0.0314177
      &lt;/s&gt;  0.1570428  0.0022152  0.0067411  0.0316489  0.1121768  0.1155129 *0.1765236  0.1540518  0.1292526  0.1148344
PRED SCORE: -4.6967
&lt;/denchmark-code&gt;

Salient features:

No additional imports used
Max attention score for each row is preceeded with a *

I have a question though, the attention vectors always have a length which exceeds the number of output tokens by one. I assume that the last row corresponds to an &lt;/s&gt; token. Please correct me if I'm wrong.
		</comment>
	</comments>
</bug>