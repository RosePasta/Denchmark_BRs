<bug id='891' author='aswinsuresh' open_date='2018-08-09T15:30:20Z' closed_time='2018-08-28T15:20:31Z'>
	<summary>Zero gradients within truncated BPTT</summary>
	<description>



OpenNMT-py/onmt/trainer.py


         Line 279
      in
      6b93582






 if self.grad_accum_count == 1: 





The gradients are being set to zero within the inner loop for truncated BPTT. Shouldn't this be done in the outer loop instead ?
	</description>
	<comments>
		<comment id='1' author='aswinsuresh' date='2018-08-19T18:30:03Z'>
		no it's fine but we should make it clearer that BPTT and accum_count &gt; 1 are not compatible.
		</comment>
		<comment id='2' author='aswinsuresh' date='2018-08-19T19:01:04Z'>
		Do you mean truncated BPTT and accum_count == 1 ? Right now, if accum_count == 1 and there is truncated BPTT, the gradients are being set to zero before forward propagating for each trunc. This does not seem right to me. I think this should be done before processing each batch. Kindly correct me if I am wrong.
		</comment>
		<comment id='3' author='aswinsuresh' date='2018-08-25T12:00:56Z'>
		There is a bug, it has been introduced by my refactoring from June.
I'll try to fix it.
It is good to zero the gradients before each trunc, but we need to update the parameters also.
I mixed up when implementing the multi-gpu part.
		</comment>
		<comment id='4' author='aswinsuresh' date='2018-08-27T12:16:08Z'>
		&lt;denchmark-link:https://github.com/aswinsuresh&gt;@aswinsuresh&lt;/denchmark-link&gt;
 Can you test &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/926&gt;#926&lt;/denchmark-link&gt;
 and let me know if it works for you ?
		</comment>
	</comments>
</bug>