<bug id='256' author='SyndicateYe' open_date='2017-09-12T15:28:52Z' closed_time='2017-09-20T21:25:33Z'>
	<summary>AttributeError: 'Variable' object has no attribute 'copy_'</summary>
	<description>
When I tried to translate the test.txt after I trained the model, there was an error.
Here is the command.
python translate.py -gpu 0 -model split_model/cls2mdn_e40.pt -src split_data/cls_split_test.txt -tgt split_data/mdn_split_test.txt -output pred.txt -replace_unk -verbose
&lt;denchmark-code&gt;Loading model
While copying the parameter named encoder.embeddings.make_embedding.pe.pe, whose dimensions in the model are torch.Size([5000, 1, 1024]) and whose dimensions in the checkpoint are torch.Size([5000, 1, 1024]), ...
Traceback (most recent call last):
  File "translate.py", line 153, in &lt;module&gt;
    main()
  File "translate.py", line 77, in main
    translator = onmt.Translator(opt, dummy_opt.__dict__)
  File "/home/xiaodong/OpenNMT-py-master9.12/onmt/Translator.py", line 28, in __init__
    opt, model_opt, self.fields, checkpoint)
  File "/home/xiaodong/OpenNMT-py-master9.12/onmt/ModelConstructor.py", line 169, in make_base_model
    model.load_state_dict(checkpoint['model'])
  File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 360, in load_state_dict
    own_state[name].copy_(param)
  File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py", line 65, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'Variable' object has no attribute 'copy_'`
&lt;/denchmark-code&gt;

The model was trained with transformer.
python train.py -data split_data/preprocessed -save_model split_model/cls2mdn -layers 4 -rnn_size 1024 -word_vec_size 1024 -batch_size 32 -epochs 40 -gpuid 0 -report_every 100 -max_grad_norm 0 -optim adam -encoder_type transformer -decoder_type transformer -position_encoding -dropout 0.2 -param_init 0 -warmup_steps 4000 -learning_rate 1 -start_checkpoint_at 20 -decay_method noam 
	</description>
	<comments>
		<comment id='1' author='SyndicateYe' date='2017-09-13T05:29:51Z'>
		Now state_dict() stores dictionary of Tensor, while in your case you get Variable from the checkpoint's state_dict(). The checkpoint` is stale, with respect to the pytorch version you are using now?
		</comment>
		<comment id='2' author='SyndicateYe' date='2017-09-13T11:21:25Z'>
		I think it is the latest version 0.2.0.post3.
		</comment>
		<comment id='3' author='SyndicateYe' date='2017-09-13T11:29:00Z'>
		Have you tried recreating a new checkpoint and reload it, and see if this issue pops up again?
		</comment>
		<comment id='4' author='SyndicateYe' date='2017-09-14T01:50:08Z'>
		Yes. I have changed the checkpoint and another computer. But the issue shows again.
		</comment>
		<comment id='5' author='SyndicateYe' date='2017-09-15T02:18:03Z'>
		I've looked at the pytorch code,  is just a thin wrapper around , so it should have had the  method attribute, but it complained no such attribute. I figure this is might be the same problem with this one:&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/254&gt;#254&lt;/denchmark-link&gt;

Would you mind updating pytorch and having an another try?
		</comment>
		<comment id='6' author='SyndicateYe' date='2017-09-15T05:59:40Z'>
		I have already checked my pytorch version. However, my pytorch is the latest one. I only find this problem when I use the transformer encode and decode type. So, I wonder whether there is a problem about transformer system.
		</comment>
		<comment id='7' author='SyndicateYe' date='2017-09-16T21:57:26Z'>
		&lt;denchmark-link:https://github.com/JianyuZhan&gt;@JianyuZhan&lt;/denchmark-link&gt;
 do you understand what is happening here? Is this a problem with a buffer vs. non-buffer?
		</comment>
		<comment id='8' author='SyndicateYe' date='2017-09-17T08:15:14Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 ,  I doubt so.
I chased down the code, and found that in the Trackback,  the   in  is . So I suspected the type of  might had been changed from  to . Then I chased the git log and found in this commit &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/commit/368717e0dde1bd35b9830c25b0a83a283880d238&gt;368717e&lt;/denchmark-link&gt;
 the  is changed from  to (compare the old  and  class in that commit).  So I changed it back as the patch below, and tested it and found the problem solved.
&lt;denchmark-link:https://github.com/SyndicateYe&gt;@SyndicateYe&lt;/denchmark-link&gt;
, would you please help test the patch below and confirm if it fixes your problem? Just copy it to a file called  in the project root directory,  and  to patch it. It is based on the latest master branch. Please update master branch first. You need to retrain and then translate to see if it is solved. Thanks.
&lt;denchmark-code&gt;diff --git a/onmt/modules/Embeddings.py b/onmt/modules/Embeddings.py
index f7ca1f4..3254a7b 100644
--- a/onmt/modules/Embeddings.py
+++ b/onmt/modules/Embeddings.py
@@ -14,13 +14,14 @@ class PositionalEncoding(nn.Module):
         pe = pe * div_term.expand_as(pe)
         pe[:, 0::2] = torch.sin(pe[:, 0::2])
         pe[:, 1::2] = torch.cos(pe[:, 1::2])
-        pe = Variable(pe.unsqueeze(1))
+        pe = pe.unsqueeze(1)
         super(PositionalEncoding, self).__init__()
         self.register_buffer('pe', pe)
         self.dropout = nn.Dropout(p=dropout)
 
     def forward(self, emb):
-        emb = emb + self.pe[:emb.size(0), :1, :emb.size(2)].expand_as(emb)
+        emb = emb.data + self.pe[:emb.size(0), :1, :emb.size(2)].expand_as(emb)
+        emb = Variable(emb)
         emb = self.dropout(emb)
         return emb

&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='SyndicateYe' date='2017-09-17T14:02:57Z'>
		I'm sorry to say this is not working.
This is the log.
`Loading data from 'split_data/preprocessed'

vocabulary size. source = 2822; target = 2816
number of training sentences. 10982
maximum batch size. 32
Building model...
NMTModel (
(encoder): TransformerEncoder (
(embeddings): Embeddings (
(make_embedding): Sequential (
(emb_luts): Elementwise (
(0): Embedding(2822, 1024, padding_idx=1)
)
(pe): PositionalEncoding (
(dropout): Dropout (p = 0.2)
)
)
)
(transformer): ModuleList (
(0): TransformerEncoderLayer (
(self_attn): MultiHeadedAttention (
(linear_keys): BottleLinear (1000 -&gt; 1000)
(linear_values): BottleLinear (1000 -&gt; 1000)
(linear_query): BottleLinear (1000 -&gt; 1000)
(sm): BottleSoftmax ()
(activation): ReLU ()
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(res_dropout): Dropout (p = 0.2)
)
(feed_forward): PositionwiseFeedForward (
(w_1): BottleLinear (1000 -&gt; 2048)
(w_2): BottleLinear (2048 -&gt; 1000)
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(relu): ReLU ()
)
)
(1): TransformerEncoderLayer (
(self_attn): MultiHeadedAttention (
(linear_keys): BottleLinear (1000 -&gt; 1000)
(linear_values): BottleLinear (1000 -&gt; 1000)
(linear_query): BottleLinear (1000 -&gt; 1000)
(sm): BottleSoftmax ()
(activation): ReLU ()
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(res_dropout): Dropout (p = 0.2)
)
(feed_forward): PositionwiseFeedForward (
(w_1): BottleLinear (1000 -&gt; 2048)
(w_2): BottleLinear (2048 -&gt; 1000)
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(relu): ReLU ()
)
)
)
)
(decoder): TransformerDecoder (
(embeddings): Embeddings (
(make_embedding): Sequential (
(emb_luts): Elementwise (
(0): Embedding(2816, 1024, padding_idx=1)
)
(pe): PositionalEncoding (
(dropout): Dropout (p = 0.2)
)
)
)
(transformer_layers): ModuleList (
(0): TransformerDecoderLayer (
(self_attn): MultiHeadedAttention (
(linear_keys): BottleLinear (1000 -&gt; 1000)
(linear_values): BottleLinear (1000 -&gt; 1000)
(linear_query): BottleLinear (1000 -&gt; 1000)
(sm): BottleSoftmax ()
(activation): ReLU ()
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(res_dropout): Dropout (p = 0.2)
)
(context_attn): MultiHeadedAttention (
(linear_keys): BottleLinear (1000 -&gt; 1000)
(linear_values): BottleLinear (1000 -&gt; 1000)
(linear_query): BottleLinear (1000 -&gt; 1000)
(sm): BottleSoftmax ()
(activation): ReLU ()
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(res_dropout): Dropout (p = 0.2)
)
(feed_forward): PositionwiseFeedForward (
(w_1): BottleLinear (1000 -&gt; 2048)
(w_2): BottleLinear (2048 -&gt; 1000)
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(relu): ReLU ()
)
)
(1): TransformerDecoderLayer (
(self_attn): MultiHeadedAttention (
(linear_keys): BottleLinear (1000 -&gt; 1000)
(linear_values): BottleLinear (1000 -&gt; 1000)
(linear_query): BottleLinear (1000 -&gt; 1000)
(sm): BottleSoftmax ()
(activation): ReLU ()
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(res_dropout): Dropout (p = 0.2)
)
(context_attn): MultiHeadedAttention (
(linear_keys): BottleLinear (1000 -&gt; 1000)
(linear_values): BottleLinear (1000 -&gt; 1000)
(linear_query): BottleLinear (1000 -&gt; 1000)
(sm): BottleSoftmax ()
(activation): ReLU ()
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(res_dropout): Dropout (p = 0.2)
)
(feed_forward): PositionwiseFeedForward (
(w_1): BottleLinear (1000 -&gt; 2048)
(w_2): BottleLinear (2048 -&gt; 1000)
(layer_norm): BottleLayerNorm (
)
(dropout): Dropout (p = 0.2)
(relu): ReLU ()
)
)
)
)
(generator): Sequential (
(0): Linear (1000 -&gt; 2816)
(1): LogSoftmax ()
)
)
number of parameters: 43008320
encoder:  17095824
decoder:  25912496

Traceback (most recent call last):
File "train.py", line 318, in 
main()
File "train.py", line 314, in main
train_model(model, train, valid, fields, optim)
File "train.py", line 175, in train_model
train_stats = train_epoch(epoch)
File "train.py", line 136, in train_epoch
model(src, tgt, src_lengths, dec_state)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 259, in call
result = self.forward(*input, **kwargs)
File "/home/xiaodong/OpenNMT-py-master9.17/onmt/Models.py", line 417, in forward
enc_hidden, context = self.encoder(src, lengths)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 259, in call
result = self.forward(*input, **kwargs)
File "/home/xiaodong/OpenNMT-py-master9.17/onmt/modules/Transformer.py", line 105, in forward
out = self.transformer[i](out, mask)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 259, in call
result = self.forward(*input, **kwargs)
File "/home/xiaodong/OpenNMT-py-master9.17/onmt/modules/Transformer.py", line 63, in forward
mid, _ = self.self_attn(input, input, input, mask=mask)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 259, in call
result = self.forward(*input, **kwargs)
File "/home/xiaodong/OpenNMT-py-master9.17/onmt/modules/MultiHeadedAttn.py", line 75, in forward
key_up = shape_projection(self.linear_keys(key))
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 259, in call
result = self.forward(*input, **kwargs)
File "/home/xiaodong/OpenNMT-py-master9.17/onmt/modules/UtilClass.py", line 10, in forward
out = super(Bottle, self).forward(input.view(size[0]*size[1], -1))
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 55, in forward
return F.linear(input, self.weight, self.bias)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py", line 575, in linear
output = input.matmul(weight.t())
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py", line 571, in matmul
return torch.matmul(self, other)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/functional.py", line 173, in matmul
return torch.mm(tensor1, tensor2)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py", line 590, in mm
return Addmm.apply(output, self, matrix, 0, 1, True)
File "/home/xiaodong/anaconda3/lib/python3.6/site-packages/torch/autograd/_functions/blas.py", line 36, in forward
matrix1, matrix2, out=output)
RuntimeError: size mismatch at /home/xiaodong/pytorch/torch/lib/THC/generic/THCTensorMathBlas.cu:243`
		</comment>
		<comment id='10' author='SyndicateYe' date='2017-09-17T14:35:19Z'>
		This seems another problem of parameters size mismatch. What are your train parameters like?
		</comment>
		<comment id='11' author='SyndicateYe' date='2017-09-20T21:25:33Z'>
		Think this is fixed now.
		</comment>
	</comments>
</bug>