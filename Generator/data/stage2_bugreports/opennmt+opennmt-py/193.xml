<bug id='193' author='XinyuHua' open_date='2017-08-26T03:58:58Z' closed_time='2017-08-26T19:46:14Z'>
	<summary>Vocab size incorrect when use features for encoder</summary>
	<description>
I encountered this problem when I'm trying to label each src token with 1 or 0, e.g. "The|1 word|0". But this causes the preprocessed .pt file to have only 2 words in vocabulary, &lt;unk&gt; and &lt;blank&gt;. Also for tgt the vocabulary contains &lt;unk&gt;, &lt;blank&gt;, &lt;s&gt;, &lt;/s&gt; only as well.  (I've tried the same document without adding features and it worked fine.)
	</description>
	<comments>
		<comment id='1' author='XinyuHua' date='2017-08-26T16:18:48Z'>
		Just to confirm. Are you using the latest code? How many features does it say you have when you run train.py
		</comment>
		<comment id='2' author='XinyuHua' date='2017-08-26T19:01:51Z'>
		Thanks for the reply. I cloned the repo just now so I'm assuming it's up-to-date. I included one bit as feature. If I run train.py it says:

vocabulary size. source = 2; target = 4
src feature 0 size = 2
number of training sentences. 0
maximum batch size. 32

If I add print('numFeatures:', numFeatures) at the end of extractFeatures method in onmt/IO.py it prints this:
Preparing training ...
('numFeatures:', 1)
Building Training...
('numFeatures:', 1)
('numFeatures:', 0)
		</comment>
		<comment id='3' author='XinyuHua' date='2017-08-26T19:46:14Z'>
		I found the problem, the input sequence was too long so it's removed...sorry about the trouble
		</comment>
		<comment id='4' author='XinyuHua' date='2019-05-02T06:34:14Z'>
		I'm using one feature (source and target). The values are {'N', 'U', 'C', 'L'}.
Then, why the vocab size are different?
&lt;denchmark-code&gt;[2019-05-02 03:28:49,162 INFO]  * src vocab size = 154622
[2019-05-02 03:28:49,163 INFO]  * src_feat_0 vocab size = 2
[2019-05-02 03:28:49,163 INFO]  * tgt vocab size = 131749
[2019-05-02 03:28:49,163 INFO]  * tgt_feat_0 vocab size = 4
[2019-05-02 03:28:49,163 INFO] Building model...
[2019-05-02 03:28:54,630 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(154622, 600, padding_idx=1)
          (1): Embedding(2, 2, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(602, 400, num_layers=2, dropout=0.2, bidirectional=True)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(131749, 600, padding_idx=1)
          (1): Embedding(4, 4, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.2)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.2)
      (layers): ModuleList(
        (0): LSTMCell(1404, 800)
        (1): LSTMCell(800, 800)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=800, out_features=800, bias=False)
      (linear_out): Linear(in_features=1600, out_features=800, bias=False)
      (linear_cover): Linear(in_features=1, out_features=800, bias=False)
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=800, out_features=131749, bias=True)
    (1): LogSoftmax()
  )
)

&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>