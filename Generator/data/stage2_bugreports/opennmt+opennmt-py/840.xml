<bug id='840' author='goncalomcorreia' open_date='2018-07-17T13:02:09Z' closed_time='2018-08-31T14:42:33Z'>
	<summary>Option -gpuid not working as it should</summary>
	<description>
When I use -gpuid option in current master of OpenNMT, regardless of what gpu I choose, the training script always chooses gpu 0.
If I use multigpu, for example, -gpuid 2 3, the training goes to gpus 0 and 1.
Is this a known issue?
	</description>
	<comments>
		<comment id='1' author='goncalomcorreia' date='2018-07-17T13:37:55Z'>
		For anyone who's having this problem, if you want to train on gpus that are not gpu 0, you can change the visible devices with CUDA_VISIBLE_DEVICES. For example, doing export CUDA_VISIBLE_DEVICES=1,2 will make your experiments go to gpu 1 and then gpu 2 in the current master.
		</comment>
		<comment id='2' author='goncalomcorreia' date='2018-07-17T15:58:49Z'>
		AFAIK, this has always been the behavior of OpenNMT-py.
		</comment>
		<comment id='3' author='goncalomcorreia' date='2018-07-17T16:09:28Z'>
		As far as I understand, this is really a pytorch+CUDA "inconsistency"
		</comment>
		<comment id='4' author='goncalomcorreia' date='2018-07-18T08:54:28Z'>
		&lt;denchmark-link:https://github.com/vince62s&gt;@vince62s&lt;/denchmark-link&gt;
, I'm pretty sure this was not the behavior of OpenNMT-py as I used that option all the time to train on gpus other than 0, without changing the CUDA_VISIBLE_DEVICES
		</comment>
		<comment id='5' author='goncalomcorreia' date='2018-07-18T12:27:02Z'>
		well you know that multi gpu was not working before the recent changes, right ?
But I'll look into it.
		</comment>
		<comment id='6' author='goncalomcorreia' date='2018-07-18T12:38:41Z'>
		ok here is the thing.
BEFORE I introduced multi-gpu, the behavior was to take the GPU id of the command line.
But it worked only with single gpu.
WHEN I introduced the multi-gpu, I replicated the behavior of Fairseq, which clearly says we need to specify the CUDA_VISIBLE_DEVICES env var.
The main reason is that, it is ultimately capable of multi node training, and it is way easier to handle it this way.
Nevertheless, I take the point into account, but a PR would be welcome.
Cheers.
		</comment>
		<comment id='7' author='goncalomcorreia' date='2018-07-18T13:47:08Z'>
		Yes, I know multigpu wasn't working before and I was very excited to know that it was working now - it was actually the main reason I pulled the latest changes.
I think that since, as you said, it may be hard to have the previous behavior with multi gpu, the -gpuid option may be a bit misleading in that regard. Some possible solutions:


Not sure if this is possible, but we could change the CUDA_VISIBLE_DEVICES after parsing the gpuid option (not a fan of this one since it changes an env var without the user knowing)


Another possibility is to have something like -n_gpus instead, where you choose the number of gpus you want to use and then specify in the documentation that you need to change your CUDA_VISIBLE_DEVICES with the gpu ids that you are able to use in that moment


Or OpenNMT could just replicate Fairseq in that regard and just use all the gpus available as default and, as in the option above, specifying in the documentation that the env var CUDA_VISIBLE_DEVICES needs to be changed with the gpus the user wants to train on


Not sure which of these options is the better one. Personally, I would go for number 2.
I would be happy to do a pull request with the preferred behavior for this.
		</comment>
		<comment id='8' author='goncalomcorreia' date='2018-07-18T14:12:29Z'>
		
would work in the short term, BUT not if we want to support multi node, it may require a bit more work (see gpu_rank option)

		</comment>
		<comment id='9' author='goncalomcorreia' date='2018-07-18T14:15:53Z'>
		Maybe just a heads up in the README then (in step 2's description)?
		</comment>
		<comment id='10' author='goncalomcorreia' date='2018-07-31T14:29:18Z'>
		although I have assigned the gpuid by using the CUDA_VISIBLE_DEVICES , the gpu 0 still be automatically used as a priority....
		</comment>
		<comment id='11' author='goncalomcorreia' date='2018-08-17T21:07:59Z'>
		&lt;denchmark-link:https://github.com/goncalomcorreia&gt;@goncalomcorreia&lt;/denchmark-link&gt;
 What is the problem of Solution number 1. we can do it by . When you change the , you could put that into the log that you are changing the environment variable, and also the env change is only going to effect in that python run. According to me it's the easiest way to solve this issue.
&lt;denchmark-link:https://github.com/vince62s&gt;@vince62s&lt;/denchmark-link&gt;

Regarding the current solution, I also think it's not that so eyecatching but a clever way. But this is not properly documented in the documentation. Currently, it is written as,
-gpuid [] Use CUDA on the listed devices.
I think this is not that so clear at the first sight.
		</comment>
		<comment id='12' author='goncalomcorreia' date='2018-08-31T14:42:33Z'>
		guys,
a PR would be welcome, in the meantime I updated the FAQ to be clearer.
&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/FAQ.md&gt;https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/FAQ.md&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='goncalomcorreia' date='2018-09-18T10:44:32Z'>
		can you guys have a look at &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/962&gt;#962&lt;/denchmark-link&gt;
 and tell me if you're comfortable with these new options ?
		</comment>
		<comment id='14' author='goncalomcorreia' date='2018-09-18T13:27:41Z'>
		Just to be sure I got it right, does the model run in all gpus available by default so you can then choose which gpus you want to use with CUDA_VISIBLE_DEVICES?
		</comment>
		<comment id='15' author='goncalomcorreia' date='2018-09-18T13:42:07Z'>
		Not exactly.
First you need to set which GPU you want to use with CUDA_VISIBLE_DEVICES
Then case (1) on a single node, you need to set:
world_size = 3 (if you want to run an 3 GPU, given you need to have made 3 visible at least)
gpu_ranks = 0 1 2
I know, this looks a little cumbersome because you need to set 2 parameters.
But you will get it when looking at case (2), on 2 nodes:
Node 1:
master_ip=ip_address_node1
master_port=portnumber_node1
world_size = 7
gpu_ranks = 0 1 2
Node2:
masterip=ipaddress_node1
master_port=portnumber_node1
world_size = 7
gpuranks = 3 4 5 6
for node 1 you need to have set at least 3 cuda visible devices
for node 2 at least 4
		</comment>
		<comment id='16' author='goncalomcorreia' date='2018-11-08T13:42:27Z'>
		I would recommend adding this to the opts section since it's not super clear the first time around what to do
		</comment>
		<comment id='17' author='goncalomcorreia' date='2018-11-08T14:00:32Z'>
		For me, it is not working on a single node, when trying to use nodes other than the first ones: my node has 8 GPUs and I want to use GPUs 2 and 3. I am setting --world_size 8 --gpu_ranks 2 3 and it gets stuck indefinitely. Setting --world_size 2 is of no use either. CUDA_VISIBLE_DEVICES is properly set to 2,3.
This happens with the current master: &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/commit/ee810e509579d0a7d6f46ecefb2c9c7a646956c8&gt;ee810e5&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='goncalomcorreia' date='2018-11-08T15:15:20Z'>
		Single node you need
gpu_ranks 0 1
world_size 2
And set cuda visible device as you need
		</comment>
		<comment id='19' author='goncalomcorreia' date='2018-11-08T17:21:42Z'>
		&lt;denchmark-link:https://github.com/Henry-E&gt;@Henry-E&lt;/denchmark-link&gt;
 can you PR a doc / opts amendment ?
		</comment>
		<comment id='20' author='goncalomcorreia' date='2018-11-08T17:39:56Z'>
		Sure
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Nov 8, 2018, 6:21 PM Vincent Nguyen ***@***.***&gt; wrote:
 @Henry-E &lt;https://github.com/Henry-E&gt; can you PR a doc / opts amendment ?

 —
 You are receiving this because you were mentioned.


 Reply to this email directly, view it on GitHub
 &lt;#840 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AMB2GCCp0vAaYUVR_o2PZRDFNE6iQc60ks5utGgtgaJpZM4VSzel&gt;
 .



		</comment>
		<comment id='21' author='goncalomcorreia' date='2018-11-09T07:21:33Z'>
		Setting arguments as &lt;denchmark-link:https://github.com/vince62s&gt;@vince62s&lt;/denchmark-link&gt;
 suggested (, with ) gets me this error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "../../OpenNMT-py/train.py", line 118, in &lt;module&gt;
    main(opt)
  File "../../OpenNMT-py/train.py", line 48, in main
    p.join()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/usr/lib/python3.6/multiprocessing/popen_fork.py", line 50, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/usr/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "../../OpenNMT-py/train.py", line 105, in signal_handler
    raise Exception(msg)
Exception:

-- Tracebacks above this line can probably
                 be ignored --

Traceback (most recent call last):
  File "/home/usuaris/veu/noe.casas/experiments2/eneu_lemma/OpenNMT-py/train.py", line 59, in run
    gpu_rank = onmt.utils.distributed.multi_init(opt, device_id)
  File "/home/usuaris/veu/noe.casas/experiments2/eneu_lemma/OpenNMT-py/onmt/utils/distributed.py", line 27, in multi_init
    world_size=dist_world_size, rank=opt.gpu_ranks[device_id])
  File "/home/usuaris/veu/noe.casas/experiments2/eneu_lemma/env/lib/python3.6/site-packages/torch/distributed/__init__.py", line 94, in init_process_group
    group_name, rank)
RuntimeError: Address already in use at /pytorch/torch/lib/THD/process_group/General.cpp:17
&lt;/denchmark-code&gt;

At that time, GPUs 0 and 1 were taken by other processes.
		</comment>
		<comment id='22' author='goncalomcorreia' date='2018-11-09T08:00:24Z'>
		"At that time, GPUs 0 and 1 were taken by other processes."
by a another onmt-py run or something else ?
monitor what you're doing with watch nvidia-smi
		</comment>
		<comment id='23' author='goncalomcorreia' date='2018-11-09T08:16:56Z'>
		GPUs 0 and 1 were taken by other non-onmt-py processes owned by other users of the machine. Target GPUs were 2 and 3, which were free, yet OpenNMT-py said "Address already in use". Consistently reproducible in my environment.
		</comment>
		<comment id='24' author='goncalomcorreia' date='2018-11-09T08:23:53Z'>
		As I said, please open a terminal and run "watch nvidia-smi"
run it again and see if your python processes take GPU 2 and 3
		</comment>
		<comment id='25' author='goncalomcorreia' date='2018-11-09T09:09:37Z'>
		The process never appears in nvidia-smi . The exception happens just after starting OpenNMT-py, so I ran OpenNMT-py several times (in a loop) while I was watching nvidia-smi and OpenNMT-py never appeared in the output.
However, I was mistaken: the problem is  100% reproducible. In 2 different nodes I can reproduce it systematically but in a third node I can not reproduce it, so there are other factors going on here. I am running in a &lt;denchmark-link:https://slurm.schedmd.com/&gt;slurm&lt;/denchmark-link&gt;
 cluster; in theory it should not affect how stuff works, but I am no expert in slurm, so not totally sure.
		</comment>
		<comment id='26' author='goncalomcorreia' date='2018-12-11T11:24:26Z'>
		
The process never appears in nvidia-smi . The exception happens just after starting OpenNMT-py, so I ran OpenNMT-py several times (in a loop) while I was watching nvidia-smi and OpenNMT-py never appeared in the output.
However, I was mistaken: the problem is not 100% reproducible. In 2 different nodes I can reproduce it systematically but in a third node I can not reproduce it, so there are other factors going on here. I am running in a slurm cluster; in theory it should not affect how stuff works, but I am no expert in slurm, so not totally sure.

Yes, I encounter the same problem, have you solve it at last?
		</comment>
		<comment id='27' author='goncalomcorreia' date='2018-12-11T11:34:10Z'>
		Nope. I still think this is a problem related to OpenNMT-py, as I have never got anything alike with tensor2tensor or fairseq in the same environment, but I can't provide further info to properly diagnose it.
		</comment>
	</comments>
</bug>