<bug id='115' author='gladuo' open_date='2017-07-10T12:41:35Z' closed_time='2017-07-13T23:14:00Z'>
	<summary>size mismatch error</summary>
	<description>
When I run
python train.py -data xxx -word_vec_size 300 -feature_vec_size 300  -brnn -context_gate both -batch_size 256 -dropout 0.2 -position_encoding -gpus 0 1 2 3 
I got
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 447, in &lt;module&gt;
    main()
  File "train.py", line 443, in main
    trainModel(model, trainData, validData, dataset, optim)
  File "train.py", line 281, in trainModel
    train_stats = trainEpoch(epoch)
  File "train.py", line 253, in trainEpoch
    dec_state)
  File "/data01/home/i-chaiduo/project/OpenNMT-py/VENV/lib/python2.7/site-packages/torch/nn/modules/module.py", line 206, in __call__
    result = self.forward(*input, **kwargs)
  File "/data01/home/i-chaiduo/project/OpenNMT-py/VENV/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py", line 61, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/data01/home/i-chaiduo/project/OpenNMT-py/VENV/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py", line 71, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs)
  File "/data01/home/i-chaiduo/project/OpenNMT-py/VENV/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py", line 46, in parallel_apply
    raise output
RuntimeError: size mismatch at /b/wheel/pytorch-src/torch/lib/THC/generic/THCTensorMathBlas.cu:243

&lt;/denchmark-code&gt;

I'm not sure if it is because of any of features which is still not avaliable ?
Thanks for any warm help.
	</description>
	<comments>
		<comment id='1' author='gladuo' date='2017-07-12T13:45:40Z'>
		Yeah, this is a bug. Interesting mix of different features.
		</comment>
		<comment id='2' author='gladuo' date='2017-07-12T13:46:49Z'>
		Would you mind trying on single GPU? Curious if the issue is due to dataparallel.
		</comment>
		<comment id='3' author='gladuo' date='2017-07-13T09:52:24Z'>
		I suffer from the same problem with single GPU, when I run
&lt;denchmark-code&gt;python3 train.py -data data/demo.train.pt -word_vec_size 300 -feature_vec_size 300 -brnn -context_gate both -batch_size 256 -dropout 0.2 -position_encoding -gpus 0
&lt;/denchmark-code&gt;

and get the error :
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 440, in &lt;module&gt;
    main()
  File "train.py", line 436, in main
    trainModel(model, trainData, validData, dataset, optim)
  File "train.py", line 274, in trainModel
    train_stats = trainEpoch(epoch)
  File "train.py", line 246, in trainEpoch
    dec_state)
  File "/home/eternal/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 206, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/eternal/john/pytorch_OpenNMT/OpenNMT-py-master/onmt/Models.py", line 381, in forward
    else dec_state)
  File "/home/eternal/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 206, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/eternal/john/pytorch_OpenNMT/OpenNMT-py-master/onmt/Models.py", line 309, in forward
    emb_t, rnn_output, attn_output
  File "/home/eternal/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 206, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/eternal/john/pytorch_OpenNMT/OpenNMT-py-master/onmt/modules/Gate.py", line 89, in forward
    z, source, target = self.context_gate(prev_emb, dec_state, attn_state)
  File "/home/eternal/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 206, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/eternal/john/pytorch_OpenNMT/OpenNMT-py-master/onmt/modules/Gate.py", line 40, in forward
    z = self.sig(self.gate(input_tensor))
  File "/home/eternal/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 206, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/eternal/.local/lib/python3.5/site-packages/torch/nn/modules/linear.py", line 54, in forward
    return self._backend.Linear()(input, self.weight, self.bias)
  File "/home/eternal/.local/lib/python3.5/site-packages/torch/nn/_functions/linear.py", line 10, in forward
    output.addmm_(0, 1, input, weight.t())
RuntimeError: size mismatch at /b/wheel/pytorch-src/torch/lib/THC/generic/THCTensorMathBlas.cu:243

&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='gladuo' date='2017-07-13T10:54:19Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 seems because of -context_gate
		</comment>
		<comment id='5' author='gladuo' date='2017-07-13T23:14:00Z'>
		It should work now with &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/pull/128&gt;#128&lt;/denchmark-link&gt;
. Please re-open if you see this error again.
		</comment>
		<comment id='6' author='gladuo' date='2017-07-30T11:38:38Z'>
		As of July 30th, the size mismatch is still bugging.
		</comment>
		<comment id='7' author='gladuo' date='2017-12-16T10:42:37Z'>
		The size mismatch problem is encountered when I attempted to run summarization-related task:
Preprocess command:
&lt;denchmark-code&gt;python preprocess.py -train_src ../dataset/efcamdat/sample_train_src.txt \                                                                                                1 ↵
-train_tgt ../dataset/efcamdat/sample_train_tgt.txt \
-valid_src ../dataset/jfleg/dev/dev.src \
-valid_tgt ../dataset/jfleg/dev/dev.ref0 \
-save_data ./data/sample \
-dynamic_dict -share_vocab
&lt;/denchmark-code&gt;

Train command:
&lt;denchmark-code&gt;python train.py -data data/sample \                                                                                                                                       1 ↵
-save_model sample-baseline-model \
-gpuid 0 \
-seed 1 \
-batch_size 64 \
-max_generator_batches 32 \
-epochs 5 \
-optim 'adam' \
-learning_rate 0.001 \
-max_grad_norm 2 \
-word_vec_size 128 \
-encoder_type 'brnn' \
-decoder_type 'rnn' \
-layers 2 \
-rnn_size 256 \
-rnn_type 'LSTM' \
-input_feed 0 \
-global_attention 'general' \
-share_decoder_embeddings -share_embeddings
&lt;/denchmark-code&gt;

Logs:
&lt;denchmark-code&gt;Loading train and validate data from 'data/sample'
 * number of training sentences: 99490
 * maximum batch size: 64
 * vocabulary size. source = 43625; target = 43625
Building model...
Intializing model parameters.
NMTModel (
  (encoder): RNNEncoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(43625, 128, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(128, 128, num_layers=2, dropout=0.3, bidirectional=True)
  )
  (decoder): StdRNNDecoder (
    (embeddings): Embeddings (
      (make_embedding): Sequential (
        (emb_luts): Elementwise (
          (0): Embedding(43625, 128, padding_idx=1)
        )
      )
    )
    (dropout): Dropout (p = 0.3)
    (rnn): LSTM(128, 256, num_layers=2, dropout=0.3)
    (attn): GlobalAttention (
      (linear_in): Linear (256 -&gt; 256)
      (linear_out): Linear (512 -&gt; 256)
      (sm): Softmax ()
      (tanh): Tanh ()
    )
  )
  (generator): Sequential (
    (0): Linear (256 -&gt; 43625)
    (1): LogSoftmax ()
  )
)
* number of parameters: 7405289
encoder:  6243456
decoder:  1161833

Traceback (most recent call last):
  File "train.py", line 306, in &lt;module&gt;
    main()
  File "train.py", line 302, in main
    train_model(model, train, valid, fields, optim)
  File "train.py", line 159, in train_model
    train_stats = trainer.train(epoch, report_func)
  File "/home/howard/research/OpenNMT-py/onmt/Trainer.py", line 125, in train
    trunc_size, self.shard_size)
  File "/home/howard/research/OpenNMT-py/onmt/Loss.py", line 73, in sharded_compute_loss
    loss, stats = self.compute_loss(batch, **shard)
  File "/home/howard/research/OpenNMT-py/onmt/Loss.py", line 121, in compute_loss
    scores = self.generator(self.bottle(output))
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/nn/modules/container.py", line 67, in forward
    input = module(input)
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/nn/modules/module.py", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 53, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/nn/functional.py", line 553, in linear
    return torch.addmm(bias, input, weight.t())
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/autograd/variable.py", line 924, in addmm
    return cls._blas(Addmm, args, False)
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/autograd/variable.py", line 920, in _blas
    return cls.apply(*(tensors + (alpha, beta, inplace)))
  File "/home/howard/.conda/envs/cedl2017/lib/python3.6/site-packages/torch/autograd/_functions/blas.py", line 26, in forward
    matrix1, matrix2, out=output)
RuntimeError: size mismatch at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCTensorMathBlas.cu:243
&lt;/denchmark-code&gt;

Please check this, thanks.
		</comment>
		<comment id='8' author='gladuo' date='2017-12-16T11:15:04Z'>
		OK, I found the problem:
Since I enabled -share_decoder_embeddings, this makes the input feature size in the weight matrix of the linear layer in the generator be equal to the tgt_word_vec_size, and causes the matrix size mismatch to the decoder's outputs' rnn_size.
To put it briefly, if I did not enable -share_decoder_embeddings, the data flow will be:
decoder's outputs (sent_length, batch_size, rnn_size) --input to generator to project the hidden state to word ids--&gt;  (sent_length, batch_size, rnn_size) x linear layer in generator (in_features, out_features). And here, in_features should be the same as rnn_size when performing batch matrix-matrix product. But if we enable -share_decoder_embeddings, the in_features will become tgt_word_vec_size, and if tgt_word_vec_size != rnn_size, this will cause size mismatch error!
So I'm wondering:

Is this a bug?
In what scenario case would we enable -share_decoder_embeddings?
If this is not a bug, and what is the correct way to use?

I post this issue to the new issue &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/issues/444&gt;#444&lt;/denchmark-link&gt;
 .
		</comment>
	</comments>
</bug>