<bug id='258' author='ghost(ghost)' open_date='2017-09-13T10:20:06Z' closed_time='2017-10-10T13:47:21Z'>
	<summary>Copy Attention Error</summary>
	<description>
When I set -copy_attn:
Traceback (most recent call last):
File "train.py", line 307, in 
main()
File "train.py", line 303, in main
trainModel(model, train, valid, fields, optim)
File "train.py", line 166, in trainModel
train_stats = trainEpoch(epoch)
File "train.py", line 141, in trainEpoch
**shard)
File "/home/haixu/Desktop/OpenNMT9.4/onmt/Loss.py", line 175, in computeLoss
unbottle(scores_data), batch, self.tgt_vocab)
File "/home/haixu/Desktop/OpenNMT9.4/onmt/IO.py", line 226, in collapse_copy_scores
scores[:, b, ti] += scores[:, b, offset + i]
IndexError: index 3458 is out of range for dimension 1 (of size 3458)
So, how to set  # Genenerator and loss options?  What is copy attention layer? What is coverage attention layer? Are there any papers?
	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2017-09-21T13:44:01Z'>
		Sorry, this is a bug. We've been using copy attention in a different branch. I'll fix asap.
		</comment>
		<comment id='2' author='ghost(ghost)' date='2017-10-09T19:25:03Z'>
		I've encountered same issue with the index error. Hope a fix would be published soon. Thanks!!
		</comment>
		<comment id='3' author='ghost(ghost)' date='2017-10-10T13:47:21Z'>
		Oh sorry, this should be fixed now in master. Are you still seeing this issue?
		</comment>
		<comment id='4' author='ghost(ghost)' date='2017-10-10T15:48:30Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 So far so good (and looks like copy_attn works better than without it)! Thanks so much for the prompt fix!!
		</comment>
		<comment id='5' author='ghost(ghost)' date='2017-10-10T16:18:28Z'>
		sounds good. please let us know if there are more issues. This is an active area of code development, but we are trying to keep it stable and keep up with the research.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2018-01-08T06:26:25Z'>
		Hi, I encountered a pretty much same problem here when I use -copy_attn in train.py:
/root/OpenNMT-py/onmt/modules/GlobalAttention.py:177: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  align_vectors = self.sm(align.view(batch*targetL, sourceL))
/root/OpenNMT-py/onmt/modules/CopyGenerator.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  prob = F.softmax(logits)
Traceback (most recent call last):
  File "/root/OpenNMT-py/train.py", line 352, in &lt;module&gt;
    main()
  File "/root/OpenNMT-py/train.py", line 348, in main
    train_model(model, train_dataset, valid_dataset, fields, optim, model_opt)
  File "/root/OpenNMT-py/train.py", line 172, in train_model
    train_stats = trainer.train(epoch, report_func)
  File "/root/OpenNMT-py/onmt/Trainer.py", line 156, in train
    trunc_size, self.shard_size)
  File "/root/OpenNMT-py/onmt/Loss.py", line 120, in sharded_compute_loss
    loss, stats = self._compute_loss(batch, **shard)
  File "/root/OpenNMT-py/onmt/modules/CopyGenerator.py", line 181, in _compute_loss
    batch, self.tgt_vocab)
  File "/root/OpenNMT-py/onmt/io/DatasetBase.py", line 63, in collapse_copy_scores
    scores[:, b, ti] += scores[:, b, offset + i]
IndexError: index 3098 is out of range for dimension 1 (of size 3098)
some additional info:
&lt;denchmark-code&gt; * number of train sentences: 1205993
 * number of valid sentences: 4996
 * maximum batch size: 64
 * vocabulary size. source = 2965; target = 3085
&lt;/denchmark-code&gt;

And I run preprocess.py with these options defined:
&lt;denchmark-code&gt;-src_vocab_size 10000 -tgt_vocab_size 10000 -dynamic_dict -max_shard_size 409600
&lt;/denchmark-code&gt;

PyTorch version: 0.3.0.post4
OpenNMT-py: latest &lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/commit/af24c9b58417ce154c86496d37d46e5872b52179&gt;master branch&lt;/denchmark-link&gt;

I've no clue how to fix this. Did the new changes bring this bug back again or is there something wrong with my settings?
Thanks very much.
		</comment>
		<comment id='7' author='ghost(ghost)' date='2018-01-08T19:45:23Z'>
		There is another report of a bug with sharding and copying. I am taking a look.
		</comment>
	</comments>
</bug>