<bug id='602' author='xforceco' open_date='2018-03-15T08:55:02Z' closed_time='2018-08-02T18:32:25Z'>
	<summary>Question: About second training stage with model loading</summary>
	<description>
Let's say I have a model file trained after 15 epochs, trained with learning_rate 1 and start_decay_at 8.
Now, I want to loading this model and train another 5 epochs with learning_rate 1 and no decay, what should I do?
Using train_from and set start_decay_at 999 and epochs 20 does not work in the second training stage. (learning rate is not 1 in epoch 16)
	</description>
	<comments>
		<comment id='1' author='xforceco' date='2018-03-15T14:09:15Z'>
		Is the problem that it starts from the earlier learning rate?
		</comment>
		<comment id='2' author='xforceco' date='2018-03-15T14:12:29Z'>
		Oh I see. Short term fix is to add a False to the top of this if statement. We default to continue using the same optimizer. We'll add an option to use a new optimizer.
&lt;denchmark-link:https://github.com/OpenNMT/OpenNMT-py/blob/master/train.py#L367&gt;https://github.com/OpenNMT/OpenNMT-py/blob/master/train.py#L367&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xforceco' date='2018-03-15T14:25:29Z'>
		Thank you for adding that.
Currently I am hard coding the self.lr in update_learning_rate in Optim.py
		</comment>
		<comment id='4' author='xforceco' date='2018-03-15T14:33:44Z'>
		That works too :)
		</comment>
	</comments>
</bug>