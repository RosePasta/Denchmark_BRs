<bug id='3' author='nasimrahaman' open_date='2017-07-06T22:23:33Z' closed_time='2017-08-22T11:02:22Z'>
	<summary>Graph model fails to replicate on multiple devices.</summary>
	<description>
As of &lt;denchmark-link:https://github.com/nasimrahaman/inferno/tree/f87058b033797807e2decfbac12f25496a726e62&gt;this commit&lt;/denchmark-link&gt;
, the problem can be reproduced as follows:
import torch
from torch.autograd import Variable
import torch.nn as nn
from torch.nn.parallel.data_parallel import data_parallel
from inferno.extensions.containers.graph import Graph

input_shape = [8, 1, 3, 128, 128]
model = Graph()\
    .add_input_node('input')\
    .add_node('conv0', nn.Conv3d(1, 10, 3, padding=1), previous='input')\
    .add_node('conv1', nn.Conv3d(10, 1, 3, padding=1), previous='conv0')\
    .add_output_node('output', previous='conv1')

model.cuda()
input = Variable(torch.rand(*input_shape).cuda())
output = data_parallel(model, input, device_ids=[0, 1, 2, 3])
This raises:
&lt;denchmark-code&gt;RuntimeError: tensors are on different GPUs
&lt;/denchmark-code&gt;

Could this be due to &lt;denchmark-link:https://github.com/nasimrahaman/inferno/blob/f87058b033797807e2decfbac12f25496a726e62/inferno/extensions/containers/graph.py#L205&gt;this add_module&lt;/denchmark-link&gt;
?
	</description>
	<comments>
		<comment id='1' author='nasimrahaman' date='2017-08-17T18:33:45Z'>
		Tested with Pytorch V0.2, still does not work. ðŸ˜ž
		</comment>
		<comment id='2' author='nasimrahaman' date='2017-08-17T21:44:06Z'>
		The bug was due to model._graph containing copies of the module, fixed in &lt;denchmark-link:https://github.com/inferno-pytorch/inferno/commit/3e3613f1cb4e16d0bbc27b05ae4f62d0580757f9&gt;3e3613f&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='nasimrahaman' date='2017-08-22T07:45:00Z'>
		The bug is a more subtle threading issue. Due to &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/replicate.py#L29&gt;this line in torch.nn.parallel.replicate&lt;/denchmark-link&gt;
, the &lt;denchmark-link:https://github.com/nasimrahaman/inferno/blob/master/inferno/extensions/containers/graph.py#L70&gt;Graph._graph&lt;/denchmark-link&gt;
 object is not replicated. Replicas from &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/parallel_apply.py#L50&gt;multiple threads&lt;/denchmark-link&gt;
 (i.e. multiple devices) therefore end up reading from and writing to the same networkx graph object, which is bad.
One solution would be to annotate the &lt;denchmark-link:https://github.com/nasimrahaman/inferno/blob/master/inferno/extensions/containers/graph.py#L336&gt;payload&lt;/denchmark-link&gt;
 with thread-id. Using a threading lock (to concurrently access ) isn't completely trivial because they're not pickle-able, but that can be solved by overloading 's  and  methods.
The other solution would be to use properties to automagically copy the Graph._graph if it's being accessed from some thread other than the creator thread.
		</comment>
		<comment id='4' author='nasimrahaman' date='2017-08-22T11:02:22Z'>
		Fixed with &lt;denchmark-link:https://github.com/inferno-pytorch/inferno/pull/35&gt;#35&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>