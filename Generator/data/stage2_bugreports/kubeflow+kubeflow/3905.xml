<bug id='3905' author='jlewi' open_date='2019-08-14T20:58:57Z' closed_time='2019-11-02T02:01:30Z'>
	<summary>auto-deployed clusters from master not accessible via IAP - IAP broken on master</summary>
	<description>
Tried accessing the following
&lt;denchmark-link:https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog&gt;https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog&lt;/denchmark-link&gt;

Get
ERR_CONNECTION_CLOSED
	</description>
	<comments>
		<comment id='1' author='jlewi' date='2019-08-14T20:59:00Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.90. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/website&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='jlewi' date='2019-08-14T21:25:57Z'>
		Thanks &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
! Should this issue be in  or is there also a doc update required?
		</comment>
		<comment id='3' author='jlewi' date='2019-08-14T21:54:05Z'>
		&lt;denchmark-link:https://github.com/sarahmaddox&gt;@sarahmaddox&lt;/denchmark-link&gt;
 sorry Filed this in the wrong repo will move it.
		</comment>
		<comment id='4' author='jlewi' date='2019-08-14T21:54:15Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.90. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='5' author='jlewi' date='2019-08-14T22:37:15Z'>
		&lt;denchmark-code&gt;kubectl -n istio-system get ingress
NAME            HOSTS                                                        ADDRESS          PORTS   AGE
envoy-ingress   kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog   35.244.168.123   80      22h
&lt;/denchmark-code&gt;

so looks like ingress never spun up ssh
&lt;denchmark-code&gt;kubectl -n istio-system describe ingress
Name:             envoy-ingress
Namespace:        istio-system
Address:          35.244.168.123
Default backend:  default-http-backend:80 (10.20.0.8:8080)
Rules:
  Host                                                        Path  Backends
  ----                                                        ----  --------
  kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog  
                                                              /*   istio-ingressgateway:80 (&lt;none&gt;)
Annotations:
  certmanager.k8s.io/issuer:                    letsencrypt-prod
  ingress.kubernetes.io/backends:               {"k8s-be-30656--46808f4d5af9eb5c":"HEALTHY","k8s-be-31380--46808f4d5af9eb5c":"UNHEALTHY"}
  ingress.kubernetes.io/forwarding-rule:        k8s-fw-istio-system-envoy-ingress--46808f4d5af9eb5c
  ingress.kubernetes.io/target-proxy:           k8s-tp-istio-system-envoy-ingress--46808f4d5af9eb5c
  networking.gke.io/managed-certificates:       gke-certificate
  ingress.kubernetes.io/ssl-redirect:           true
  ingress.kubernetes.io/url-map:                k8s-um-istio-system-envoy-ingress--46808f4d5af9eb5c
  kubernetes.io/ingress.global-static-ip-name:  kf-vmaster-n01-ip
  kubernetes.io/tls-acme:                       true
Events:                                         &lt;none&gt;
&lt;/denchmark-code&gt;

Looks like its using managed certificates.
		</comment>
		<comment id='6' author='jlewi' date='2019-08-14T22:43:15Z'>
		&lt;denchmark-code&gt;nslookup kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog
Address: 34.95.64.223
&lt;/denchmark-code&gt;

So address defined.
It looks like the backend for the ingress which is the ingress gateway is reported as unhealthy
&lt;denchmark-code&gt;  - name: http2
    nodePort: 31380
    port: 80
    protocol: TCP
    targetPort: 80
  - name: https
    nodePort: 31390
    port: 443
    protocol: TCP
    targetPort: 443
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='jlewi' date='2019-08-14T22:46:50Z'>
		Lets try running a health check over port-forrward
&lt;denchmark-code&gt;kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80

curl http://localhost:8080
Origin authentication failed.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='jlewi' date='2019-08-14T23:14:44Z'>
		It looks like the GCP health check path isn't correct.
The JWT policy check excludes the path "/healthz"
&lt;denchmark-code&gt;kubectl -n istio-system get policy -o yaml ingress-jwt
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  creationTimestamp: 2019-08-14T00:23:57Z
  generation: 1
  labels:
    kustomize.component: iap-ingress
  name: ingress-jwt
  namespace: istio-system
  resourceVersion: "4534"
  selfLink: /apis/authentication.istio.io/v1alpha1/namespaces/istio-system/policies/ingress-jwt
  uid: ce7f2c8b-be29-11e9-a0a4-42010a8e009f
spec:
  origins:
  - jwt:
      audiences:
      - TO_BE_PATCHED
      issuer: https://cloud.google.com/iap
      jwksUri: https://www.gstatic.com/iap/verify/public_key-jwk
      jwtHeaders:
      - x-goog-iap-jwt-assertion
      trigger_rules:
      - excluded_paths:
        - exact: /healthz
        - prefix: /.well-known/acme-challenge
  principalBinding: USE_ORIGIN
  targets:
  - name: istio-ingressgateway
    ports:
&lt;/denchmark-code&gt;

I updated the health check manually to change the path and now the backend is healthy.
		</comment>
		<comment id='9' author='jlewi' date='2019-08-14T23:15:37Z'>
		I think the problem may be that the auto-deploy job might not be using the updated config.
		</comment>
		<comment id='10' author='jlewi' date='2019-08-14T23:19:16Z'>
		Auto deploy infra
&lt;denchmark-link:https://github.com/kubeflow/testing/tree/master/test-infra/auto-deploy&gt;https://github.com/kubeflow/testing/tree/master/test-infra/auto-deploy&lt;/denchmark-link&gt;

Not using --config
&lt;denchmark-link:https://github.com/kubeflow/testing/blob/8cbd626f678d2c830edc38d5990bc2e1c6e1eb70/py/kubeflow/testing/create_kf_instance.py#L65&gt;https://github.com/kubeflow/testing/blob/8cbd626f678d2c830edc38d5990bc2e1c6e1eb70/py/kubeflow/testing/create_kf_instance.py#L65&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='jlewi' date='2019-08-15T06:21:42Z'>
		So I updated the auto-deploy &lt;denchmark-link:https://github.com/kubeflow/kubeflow/pull/441&gt;#441&lt;/denchmark-link&gt;

The ingressgateway Deployment has a readiness probe
&lt;denchmark-code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: 2019-08-15T05:47:00Z
  generation: 3
  labels:
    app: istio-ingressgateway
    chart: gateways
    heritage: Tiller
    istio: ingressgateway
    release: istio
  name: istio-ingressgateway
  namespace: istio-system
  resourceVersion: "3602"
  selfLink: /apis/extensions/v1beta1/namespaces/istio-system/deployments/istio-ingressgateway
  uid: 19cacbaa-bf20-11e9-aedd-42010a8e006f
spec:
  progressDeadlineSeconds: 2147483647
  replicas: 5
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: istio-ingressgateway
      chart: gateways
      heritage: Tiller
      istio: ingressgateway
      release: istio
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      creationTimestamp: null
      labels:
        app: istio-ingressgateway
        chart: gateways
        heritage: Tiller
        istio: ingressgateway
        release: istio
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - ppc64le
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - s390x
            weight: 2
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
                - ppc64le
                - s390x
      containers:
      - args:
        - proxy
        - router
        - --domain
        - $(POD_NAMESPACE).svc.cluster.local
        - --log_output_level=default:info
        - --drainDuration
        - 45s
        - --parentShutdownDuration
        - 1m0s
        - --connectTimeout
        - 10s
        - --serviceCluster
        - istio-ingressgateway
        - --zipkinAddress
        - zipkin:9411
        - --proxyAdminPort
        - "15000"
        - --statusPort
        - "15020"
        - --controlPlaneAuthPolicy
        - NONE
        - --discoveryAddress
        - istio-pilot:15010
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: INSTANCE_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: ISTIO_META_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: ISTIO_META_CONFIG_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: ISTIO_META_ROUTER_MODE
          value: sni-dnat
        image: docker.io/istio/proxyv2:1.1.6
        imagePullPolicy: IfNotPresent
        name: istio-proxy
        ports:
        - containerPort: 15020
          protocol: TCP
        - containerPort: 80
          protocol: TCP
        - containerPort: 443
          protocol: TCP
        - containerPort: 31400
          protocol: TCP
        - containerPort: 15029
          protocol: TCP
        - containerPort: 15030
          protocol: TCP
        - containerPort: 15031
          protocol: TCP
        - containerPort: 15032
          protocol: TCP
        - containerPort: 15443
          protocol: TCP
        - containerPort: 15090
          name: http-envoy-prom
          protocol: TCP
        readinessProbe:
          failureThreshold: 30
          httpGet:
            path: /healthz/ready
            port: 15020
            scheme: HTTP
          initialDelaySeconds: 1
          periodSeconds: 2
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 40Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/certs
          name: istio-certs
          readOnly: true
        - mountPath: /etc/istio/ingressgateway-certs
          name: ingressgateway-certs
          readOnly: true
        - mountPath: /etc/istio/ingressgateway-ca-certs
          name: ingressgateway-ca-certs
          readOnly: true
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: istio-ingressgateway-service-account
      serviceAccountName: istio-ingressgateway-service-account
      terminationGracePeriodSeconds: 30
      volumes:
      - name: istio-certs
        secret:
          defaultMode: 420
          optional: true
          secretName: istio.istio-ingressgateway-service-account
      - name: ingressgateway-certs
        secret:
          defaultMode: 420
          optional: true
          secretName: istio-ingressgateway-certs
      - name: ingressgateway-ca-certs
        secret:
          defaultMode: 420
          optional: true
          secretName: istio-ingressgateway-ca-certs
status:
  availableReplicas: 5
  conditions:
  - lastTransitionTime: 2019-08-15T05:49:44Z
    lastUpdateTime: 2019-08-15T05:49:44Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 3
  readyReplicas: 5
  replicas: 5
  updatedReplicas: 5

&lt;/denchmark-code&gt;

But the health check still isn't picking it up; the GCLB set the health check to "/".
I tried changing to "/healthz/ready" that didn't work.
I also tried changing to "/healthz"
		</comment>
		<comment id='12' author='jlewi' date='2019-08-15T06:26:42Z'>
		The URL
&lt;denchmark-link:https://kf-v0-6-n04.endpoints.kubeflow-ci-deployment.cloud.goog/&gt;https://kf-v0-6-n04.endpoints.kubeflow-ci-deployment.cloud.goog/&lt;/denchmark-link&gt;

works
Here's the corresponding ingressgateway deployment
&lt;denchmark-code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: 2019-08-15T00:22:15Z
  generation: 6
  labels:
    app: istio-ingressgateway
    chart: gateways
    heritage: Tiller
    istio: ingressgateway
    release: istio
  name: istio-ingressgateway
  namespace: istio-system
  resourceVersion: "78853"
  selfLink: /apis/extensions/v1beta1/namespaces/istio-system/deployments/istio-ingressgateway
  uid: bc09ac76-bef2-11e9-a983-42010a8e0066
spec:
  progressDeadlineSeconds: 2147483647
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: istio-ingressgateway
      chart: gateways
      heritage: Tiller
      istio: ingressgateway
      release: istio
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      creationTimestamp: null
      labels:
        app: istio-ingressgateway
        chart: gateways
        heritage: Tiller
        istio: ingressgateway
        release: istio
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - ppc64le
            weight: 2
          - preference:
              matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - s390x
            weight: 2
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/arch
                operator: In
                values:
                - amd64
                - ppc64le
                - s390x
      containers:
      - args:
        - proxy
        - router
        - --domain
        - $(POD_NAMESPACE).svc.cluster.local
        - --log_output_level=default:info
        - --drainDuration
        - 45s
        - --parentShutdownDuration
        - 1m0s
        - --connectTimeout
        - 10s
        - --serviceCluster
        - istio-ingressgateway
        - --zipkinAddress
        - zipkin:9411
        - --proxyAdminPort
        - "15000"
        - --statusPort
        - "15020"
        - --controlPlaneAuthPolicy
        - NONE
        - --discoveryAddress
        - istio-pilot:15010
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: INSTANCE_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.hostIP
        - name: ISTIO_META_POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        - name: ISTIO_META_CONFIG_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: ISTIO_META_ROUTER_MODE
          value: sni-dnat
        image: docker.io/istio/proxyv2:1.1.6
        imagePullPolicy: IfNotPresent
        name: istio-proxy
        ports:
        - containerPort: 15020
          protocol: TCP
        - containerPort: 80
          protocol: TCP
        - containerPort: 443
          protocol: TCP
        - containerPort: 31400
          protocol: TCP
        - containerPort: 15029
          protocol: TCP
        - containerPort: 15030
          protocol: TCP
        - containerPort: 15031
          protocol: TCP
        - containerPort: 15032
          protocol: TCP
        - containerPort: 15443
          protocol: TCP
        - containerPort: 15090
          name: http-envoy-prom
          protocol: TCP
        readinessProbe:
          failureThreshold: 30
          httpGet:
            path: /healthz/ready
            port: 15020
            scheme: HTTP
          initialDelaySeconds: 1
          periodSeconds: 2
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 40Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/certs
          name: istio-certs
          readOnly: true
        - mountPath: /etc/istio/ingressgateway-certs
          name: ingressgateway-certs
          readOnly: true
        - mountPath: /etc/istio/ingressgateway-ca-certs
          name: ingressgateway-ca-certs
          readOnly: true
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: istio-ingressgateway-service-account
      serviceAccountName: istio-ingressgateway-service-account
      terminationGracePeriodSeconds: 30
      volumes:
      - name: istio-certs
        secret:
          defaultMode: 420
          optional: true
          secretName: istio.istio-ingressgateway-service-account
      - name: ingressgateway-certs
        secret:
          defaultMode: 420
          optional: true
          secretName: istio-ingressgateway-certs
      - name: ingressgateway-ca-certs
        secret:
          defaultMode: 420
          optional: true
          secretName: istio-ingressgateway-ca-certs
status:
  availableReplicas: 4
  conditions:
  - lastTransitionTime: 2019-08-15T00:22:15Z
    lastUpdateTime: 2019-08-15T00:22:15Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  observedGeneration: 6
  readyReplicas: 4
  replicas: 4
  updatedReplicas: 4

&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='jlewi' date='2019-08-15T13:28:54Z'>
		Next step would be to deploy from master and see if that works.
An E2E test was added for IAP for postsubmits.
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/pull/3482/files&gt;https://github.com/kubeflow/kubeflow/pull/3482/files&lt;/denchmark-link&gt;

I think there may be a bug in the ksonnet because it doesn't look the endpoint-is-ready-test is actually included as part of the workflow
&lt;denchmark-link:https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/kubeflow_kubeflow/kubeflow-postsubmit/1161877435558400003&gt;https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/kubeflow_kubeflow/kubeflow-postsubmit/1161877435558400003&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='jlewi' date='2019-08-15T18:43:09Z'>
		Here's a subsequent post-submit run.
&lt;denchmark-link:https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-periodic-master&gt;https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-periodic-master&lt;/denchmark-link&gt;

The endpoint test ran but it failed.
So it looks like the problem is on master as well.
		</comment>
		<comment id='15' author='jlewi' date='2019-08-15T20:20:14Z'>
		It looks like backend-updater pod is stuck
&lt;denchmark-code&gt;Waiting for the backend-services resource PROJECT=jlewi-dev NODEPORT=31380 SERVICE=istio-ingressgateway...
ERROR: (gcloud.compute.backend-services.list) Some requests did not succeed:
 - Insufficient Permission: Request had insufficient authentication scopes.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='jlewi' date='2019-08-15T20:22:56Z'>
		Looks like that script is what's updating the backend health check to /healthz



kubeflow/kubeflow/gcp/update_backend.sh


         Line 44
      in
      1a6b85e






 # Since we create the envoy-ingress ingress object before creating the envoy 





So I suspect an issue with workload identity.
		</comment>
		<comment id='17' author='jlewi' date='2019-08-15T20:28:55Z'>
		kf-admin service account is missing  GSA annotation
&lt;denchmark-code&gt; kubectl -n istio-system get serviceaccount -o yaml kf-admin

apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2019-08-15T19:03:16Z
  labels:
    kustomize.component: iap-ingress
  name: kf-admin
  namespace: istio-system
  resourceVersion: "4045"
  selfLink: /api/v1/namespaces/istio-system/serviceaccounts/kf-admin
  uid: 56bb5809-bf8f-11e9-a20b-42010a8e00b9
secrets:
- name: kf-admin-token-g657p
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='jlewi' date='2019-08-15T21:49:23Z'>
		&lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;
 is investigating; on master we shouldn't be using workload identity yet; so it should be applying the overlay to add gcp-credentials which adds secrets but looks like the overlay isn't getting added.
		</comment>
		<comment id='19' author='jlewi' date='2019-08-16T06:38:46Z'>
		&lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;
 Still looks like the tests are failing
&lt;denchmark-link:https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit&gt;https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='jlewi' date='2019-08-16T16:00:56Z'>
		ack, looking
		</comment>
		<comment id='21' author='jlewi' date='2019-08-19T15:20:16Z'>
		v0.6-branch auto-deployments are still picking up kfctl_gcp_iap.yaml from master.
Looks like I need to update the docker images used by the deployment jobs as that contains auto_deploy.sh which needs to pass on the kfctl_config file.
Not sure if that's the same problem affecting the master auto-deployments.
/cc &lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='jlewi' date='2019-08-19T18:42:32Z'>
		latest post-submit endpoint is ready is green now: &lt;denchmark-link:https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit&gt;https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-postsubmit&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='jlewi' date='2019-08-19T18:51:39Z'>
		&lt;denchmark-link:https://kf-vmaster-n04.endpoints.kubeflow-ci-deployment.cloud.goog/&gt;https://kf-vmaster-n04.endpoints.kubeflow-ci-deployment.cloud.goog/&lt;/denchmark-link&gt;
 still not accessible.
unexpectedly closed the connection.
describe ingress:
Error 404: The resource 'projects/kubeflow-ci-deployment/global/sslCertificates/mcrt-92b2c917-da8f-42ca-b337-c2a1def72f8c' was not found, notFound
Seems like problem with managed cert.
describe certificate:
managed-certificate-controller  googleapi: Error 403: Quota 'SSL_CERTIFICATES' exceeded. Limit: 40.0 globally., quotaExceeded
		</comment>
		<comment id='24' author='jlewi' date='2019-08-19T18:55:50Z'>
		Not sure if when we delete cluster, the managed cert will be deleted as well?
Or there is a leak..
		</comment>
		<comment id='25' author='jlewi' date='2019-08-19T18:57:04Z'>
		The last 8 are from the new managed cert
&lt;denchmark-code&gt;$ gcloud compute ssl-certificates list --project=kubeflow-ci-deployment
NAME                                                         CREATION_TIMESTAMP
k8s-ssl-0ff442a6e235686c-460d278ff9b07cb9--4ae776f4f553570c  2019-04-29T15:31:24.406-07:00
k8s-ssl-2992686342891c62-ca49d77fbf46b99d--26806559b254c2b2  2019-05-06T19:57:47.575-07:00
k8s-ssl-29b1b58aa9185254-125c25bdeff101f7--bee044758e7e549c  2019-04-30T11:44:20.210-07:00
k8s-ssl-2f5695bb078b34f3-49e805e5ccf66148--5527702567ce5341  2019-06-15T05:27:55.399-07:00
k8s-ssl-332d6b14043444d6-f89770934f999d31--a502582361df8372  2019-05-07T11:09:46.804-07:00
k8s-ssl-367c8c934bc73a1b-5c53f38c50dc3e89--7c9f358a54e3b240  2019-04-30T14:18:29.075-07:00
k8s-ssl-489a5fc259effe4e-567cf32b1e03aafb--bb9a3658e2516006  2019-04-30T14:37:34.315-07:00
k8s-ssl-4d5bec85b4a399e0-20395b3addde4973--4da7e63dde472f4c  2019-06-04T05:07:02.391-07:00
k8s-ssl-519fc4f59435298c-2497a3287a14ef2f--9eb1a983acd997f9  2019-04-29T13:06:11.855-07:00
k8s-ssl-5a59f83a48f10be7-ccb41ba826909393--424e4ee951a55fda  2019-05-06T20:03:34.554-07:00
k8s-ssl-5f4fb336806a00c0-285498d1484e381c--20f5463addbc5c1f  2019-05-10T17:14:10.875-07:00
k8s-ssl-60b2a0301c05f739-74701b1f13e856a3--d1375bf3cda1f61d  2019-05-06T19:58:01.186-07:00
k8s-ssl-6f00577a44f9db44-2f36293cfcb4f8a7--bb3d0cf69abf1b2c  2019-04-30T15:36:28.300-07:00
k8s-ssl-70f5481e587f657b-7e921ae4a9b9f882--8239117d0e5ba932  2019-05-13T13:50:52.121-07:00
k8s-ssl-76f099d80d536c0e-3199ff8e324385c7--ae8bace9ef692851  2019-06-27T17:46:38.271-07:00
k8s-ssl-8171a698662bc0cc-d44151628081e1a5--7474eca042ec1d1b  2019-08-16T17:09:16.256-07:00
k8s-ssl-929ae769c54a6995-bdc66256d45b13b1--0cce0ca7c3884348  2019-05-14T01:06:43.168-07:00
k8s-ssl-9351026770de791a-9b68b792f6a78126--278488d592b4b170  2019-02-20T11:27:31.344-08:00
k8s-ssl-a080df3fe2b6e319-61efcf233e31032e--79d069c18c20d150  2019-05-07T07:02:21.272-07:00
k8s-ssl-a345445688e91809-51b8faf754799f23--58db1229215ae1be  2019-04-30T02:29:05.802-07:00
k8s-ssl-a7a61826c2166f35-594bace88a2ba28a--2e710c3c79412b27  2019-04-26T17:49:40.870-07:00
k8s-ssl-a933842bee7a9359-5950cd35524a14b4--b9282641e95fc479  2019-03-28T11:55:28.972-07:00
k8s-ssl-aad965ee0bdc11bc-cb865b3743c620cb--1e80e464eaa362ef  2019-04-30T12:47:52.968-07:00
k8s-ssl-ad38bbbf5a50d081-0b169cf51ff1fada--c75ceb069223d40b  2019-07-01T07:37:59.053-07:00
k8s-ssl-ad5932ae000b187f-2a4f324b7443bb37--f82cb094c628fbdc  2019-05-01T16:47:15.836-07:00
k8s-ssl-c5ee34edab6471d8-be8221ff0051f1c4--41c3bce925563039  2019-07-09T06:11:50.800-07:00
k8s-ssl-d610bead7d63a72f-13071c6dcad318dd--bca72a4567bb6370  2019-03-28T12:11:35.890-07:00
k8s-ssl-dad84f65cad94c31-ca81b421956ea4f1--a71c868e26dada4e  2019-04-30T14:08:47.780-07:00
k8s-ssl-db8fd3936e021334-914706fd254f8e27--5481b7cb9e85e70c  2019-05-07T20:41:01.407-07:00
k8s-ssl-e58cbc4a1db0fa39-aedd3061534edd0a--1dff0ec7b5676067  2019-08-15T17:11:52.118-07:00
k8s-ssl-ee4e98c1940507d6-47bc9c7b8121a910--11ae48544d7b6644  2019-05-13T20:56:46.429-07:00
k8s-ssl-ffffc95b05b976aa-22b80b4b6f0b0134--d5384c3095008610  2019-05-06T20:03:20.788-07:00
mcrt-085dca4c-16b6-4e89-9229-9ed5a0bb850b                    2019-08-19T10:24:40.391-07:00
mcrt-29cc7462-0365-401e-9dc2-287fa7d9ed75                    2019-08-14T16:21:32.606-07:00
mcrt-33c37277-2e44-4912-b7ff-a828186f92f9                    2019-08-16T05:08:07.584-07:00
mcrt-4cd67bb9-7e15-47b2-82ad-39e26f27b96c                    2019-08-14T17:25:19.634-07:00
mcrt-68a35467-0eed-479f-ba99-ba0d2d4c39d0                    2019-08-14T16:20:34.196-07:00
mcrt-a31c4815-24e5-4acd-8260-8ee79fbae5ad                    2019-08-15T05:36:41.817-07:00
mcrt-cf9e6264-3f1c-4b52-8c85-44c5a3ec9aef                    2019-08-14T23:06:13.339-07:00
mcrt-fe13e1ac-ecd2-40c7-9e4a-e72a28b7268d                    2019-08-14T17:30:08.173-07:00
&lt;/denchmark-code&gt;

		</comment>
		<comment id='26' author='jlewi' date='2019-08-19T18:59:35Z'>
		I am increasing quota to 60 for now, and we can see if managed cert is leaking
		</comment>
		<comment id='27' author='jlewi' date='2019-08-19T20:02:45Z'>
		&lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;
 I think the problem is that with the auto-deployed clusters are using a self-signed certificate to get around lets encrypt limitations.  I suspect that interferes with managed certificates.
I think we can probably remove that now. In the past we were recycling hostnames because of IAP but with universal redirect we probably no longer need to recycle hostnames so we could give unique hostnames to each deployment.
I will look into fixing this.
		</comment>
		<comment id='28' author='jlewi' date='2019-08-19T23:03:02Z'>
		The endpoint ended up coming up if I didn't set a self-signed certificate
&lt;denchmark-link:https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog/&gt;https://kf-vmaster-n01.endpoints.kubeflow-ci-deployment.cloud.goog/&lt;/denchmark-link&gt;

Opened up kubeflow/testin#444 about note reusing hostnames.
We''ll have to see whether we run into lets encrypt quota errors if we recycle certificates.
		</comment>
		<comment id='29' author='jlewi' date='2019-08-19T23:39:20Z'>
		On a subsequent trial; it looks like GKE managed certificate is giving me
&lt;denchmark-code&gt; Warning  BackendError  19s (x14 over 8m58s)  managed-certificate-controller  (combined from similar events): operation operation-1566257853630-59080d2fe0a09-8990e3c1-13c8bb50 failed: FORBIDDEN

&lt;/denchmark-code&gt;

		</comment>
		<comment id='30' author='jlewi' date='2019-08-19T23:45:06Z'>
		Looks like it could be an SSL quota certificate issue; we have quota for 60 SSL certificates and looks like we are using all of them.
		</comment>
		<comment id='31' author='jlewi' date='2019-09-27T19:57:37Z'>
		dupe &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/3834&gt;#3834&lt;/denchmark-link&gt;

I've been  seeing this when  on CLI deploy with GPU pools.
 Warning  FailedMount  ... Unable to mount volumes for pod "backend-updater-0_istio-system...": timeout expired waiting for volumes to attach or mount for pod "istio-system"/"backend-updater-0". list of unmounted volumes=[sa-key]. list of unattached volumes=[config-volume sa-key envoy-token-4lz4m]
and here
 MountVolume.SetUp failed for volume "sa-key" : secrets "admin-gcp-sa" not found
but I have the admin-gcp-sa is there , might it be a naming mismatch ? I can look further if this is related. namespace mismatch.
In my case, this stalls backend-updater and the reset as described above.
this worked for me  as a temporary fix
kubectl get secret admin-gcp-sa --namespace ${NAMESPACE}  --export -o yaml |\ 
kubectl apply --namespace=istio-system -f -
		</comment>
		<comment id='32' author='jlewi' date='2019-11-02T02:01:30Z'>
		This has been fixed. I verified the following endpoint was accessible.
&lt;denchmark-link:https://kf-vmaster-1102-87a.endpoints.kubeflow-ci-deployment.cloud.goog/&gt;https://kf-vmaster-1102-87a.endpoints.kubeflow-ci-deployment.cloud.goog/&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>