<bug id='4847' author='lalithvaka' open_date='2020-03-10T22:07:33Z' closed_time='2020-03-31T22:02:18Z'>
	<summary>OIDC-Authservice integration with Ping Federate setup</summary>
	<description>
/kind bug
What steps did you take and what happened:
[A clear and concise description of what the bug is.]
Did the kfctl build and updated the oidc-authservice/base/params.env to include the following as an SSO integration setup with Ping Federate
client_id=kubeflow-oidc-authservice_OIDC_DEV1
oidc_provider=&lt;denchmark-link:https://xxxx.org&gt;https://xxxx.org&lt;/denchmark-link&gt;

oidc_redirect_uri=&lt;denchmark-link:http://K8SAPIServerIP:31380/login/oidc&gt;http://K8SAPIServerIP:31380/login/oidc&lt;/denchmark-link&gt;

oidc_auth_url=&lt;denchmark-link:https://xxxx.org/as/authorization.oauth2&gt;https://xxxx.org/as/authorization.oauth2&lt;/denchmark-link&gt;

application_secret=xxxxx
skip_auth_uri=/dex
userid-header=kubeflow-userid
userid-prefix=
namespace=istio-system
hi &lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
 I have tried integrating with Ping and am running into issues as following. Not sure if am missing anything.
My authservice-0 log showing as follows
➜ kubectl logs authservice-0 -n istio-system
time="2020-03-09T22:08:39Z" level=info msg="Starting readiness probe at 8081"
time="2020-03-09T22:08:39Z" level=info msg="No USERID_TOKEN_HEADER specified, using 'kubeflow-userid-token' as default."
time="2020-03-09T22:08:39Z" level=info msg="No SERVER_HOSTNAME specified, using '' as default."
time="2020-03-09T22:08:39Z" level=info msg="No SERVER_PORT specified, using '8080' as default."
time="2020-03-09T22:08:39Z" level=info msg="No SESSION_MAX_AGE specified, using '86400' as default."
time="2020-03-09T22:08:39Z" level=info msg="Starting web server at :8080"
time="2020-03-09T22:08:40Z" level=error msg="OIDC provider setup failed, retrying in 10 seconds: 404 Not Found: \n\n&lt;title&gt;404 Not Found&lt;/title&gt;\n\nNot Found\n
The requested URL /.well-known/openid-configuration was not found on this server.
\n\n"
Also, not able to access the kubeflow dashboard using portforward and noticed above log
What did you expect to happen:
A redirect to the Ping SSO screen
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:

➜ kubectl get pods -n istio-system
NAME                                                         READY   STATUS      RESTARTS   AGE
authservice-0                                                0/1     Running     0          22h
cluster-local-gateway-9d544d7db-n55ln                        1/1     Running     0          22h
istio-citadel-75f76d94f-j5thr                                1/1     Running     0          22h
istio-galley-c9f4fb7bf-fp76b                                 1/1     Running     0          22h
istio-ingressgateway-68875ff4cd-pzf7h                        1/1     Running     0          22h
istio-nodeagent-bbxrd                                        1/1     Running     0          22h
istio-nodeagent-zbdsq                                        1/1     Running     0          22h
istio-pilot-99597b4f6-kcrrq                                  2/2     Running     0          22h
istio-policy-757f4d7856-48khj                                2/2     Running     0          22h
istio-security-post-install-release-1.3-latest-daily-5hdgg   0/1     Completed   0          22h
istio-sidecar-injector-5f5484c5f5-lvwjh                      1/1     Running     1          22h
istio-telemetry-598cdc58bf-2p722                             2/2     Running     0          22h
prometheus-7dbbf976d8-52lkb                                  1/1     Running     0          22h
(base)

➜ kubectl get pods -n kubeflow
NAME                                                           READY   STATUS             RESTARTS   AGE
admission-webhook-deployment-5767bcf4b7-9pc4d                  1/1     Running            0          25h
application-controller-stateful-set-0                          1/1     Running            0          25h
argo-ui-778676df64-b5wbf                                       1/1     Running            0          25h
centraldashboard-869f55ddbd-spzdf                              1/1     Running            0          25h
jupyter-web-app-deployment-89789fd5-9qdtb                      1/1     Running            0          25h
katib-controller-6b789b6cb5-pn7tf                              1/1     Running            1          25h
katib-db-manager-64f548b47c-jwxtj                              0/1     CrashLoopBackOff   303        25h
katib-mysql-57884cb488-ct2k5                                   0/1     Pending            0          25h
katib-ui-5c5cc6bd77-s8d7c                                      1/1     Running            0          25h
kfserving-controller-manager-0                                 2/2     Running            1          25h
metadata-db-76c9f78f77-4lz4b                                   1/1     Running            1          25h
metadata-deployment-674fdd976b-kgfpv                           1/1     Running            0          25h
metadata-envoy-deployment-5688989bd6-s79bs                     1/1     Running            0          25h
metadata-grpc-deployment-5579bdc87b-7kwhz                      1/1     Running            19         25h
metadata-ui-9b8cd699d-6hw2j                                    1/1     Running            0          25h
minio-755ff748b-4llnd                                          0/1     Pending            0          25h
ml-pipeline-79b4f85cbc-pkbcg                                   0/1     CrashLoopBackOff   289        25h
ml-pipeline-ml-pipeline-visualizationserver-5fdffdc5bf-n2v2n   1/1     Running            0          25h
ml-pipeline-persistenceagent-645cb66874-v29z9                  0/1     CrashLoopBackOff   201        25h
ml-pipeline-scheduledworkflow-6c978b6b85-8z4wr                 1/1     Running            0          25h
ml-pipeline-ui-6995b7bccf-9z4m2                                1/1     Running            0          25h
ml-pipeline-viewer-controller-deployment-8554dc7b9f-qjgt9      1/1     Running            0          25h
mysql-598bc897dc-fsc6t                                         1/1     Running            0          25h
notebook-controller-deployment-7db57b9ccf-f85hk                1/1     Running            0          25h
profiles-deployment-7b89df86c8-87cmb                           2/2     Running            0          25h
pytorch-operator-5fd5f94bdd-qcwtx                              1/1     Running            1          25h
seldon-controller-manager-679fc777cd-j4fh5                     1/1     Running            2          25h
spark-operatorcrd-cleanup-2rh6n                                0/2     Completed          0          25h
spark-operatorsparkoperator-c7b64b87f-tnbhq                    1/1     Running            0          25h
spartakus-volunteer-6b767c8d6-m2x24                            1/1     Running            0          25h
tensorboard-6544748d94-q9c6d                                   1/1     Running            0          25h
tf-job-operator-7d7c8fb8bb-d5dhb                               1/1     Running            1          25h
workflow-controller-945c84565-8drwm                            1/1     Running            0          25h
Set up a K8S cluster on Azure using 3 VMs and created local volumes

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard):
kfctl version: (use kfctl version):kfctl v1.0-0-g94c35cf
Kubernetes platform: (e.g. minikube): 1 console node and 2 worker nodes on Azure
Kubernetes version: (use kubectl version): 1.14
OS (e.g. from /etc/os-release): ubuntu

	</description>
	<comments>
		<comment id='1' author='lalithvaka' date='2020-03-10T22:08:03Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='lalithvaka' date='2020-03-11T17:06:47Z'>
		&lt;denchmark-link:https://github.com/lalithvaka&gt;@lalithvaka&lt;/denchmark-link&gt;
 the first step of the OIDC flow is the Provider discovery.
The oidc-authservice hits  and gets the OIDC Provider's settings.
This step seems to result in a 404.
Can you check if the /.well-known/openid-configuration is correct?
It should look something like this &lt;denchmark-link:https://accounts.google.com/.well-known/openid-configuration&gt;https://accounts.google.com/.well-known/openid-configuration&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='lalithvaka' date='2020-03-11T17:24:24Z'>
		hi &lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
 looks like this would be an issue to start with as am not able to access this outside our enterprise network and my cluster is on Azure VMs. Is there any workaround to this? Also, beyond this issue, kustomize/oidc-authservice/base/params.env is the right place to update the OIDC provider details?
		</comment>
		<comment id='4' author='lalithvaka' date='2020-03-14T00:21:39Z'>
		Hi &lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/krishnadurai&gt;@krishnadurai&lt;/denchmark-link&gt;
  can you please comment on the above configuration if am updating the OIDC parameters in the right file? kustomize/oidc-authservice/base/params.env ? Appreciate your help.
		</comment>
		<comment id='5' author='lalithvaka' date='2020-03-14T13:57:27Z'>
		&lt;denchmark-link:https://github.com/lalithvaka&gt;@lalithvaka&lt;/denchmark-link&gt;
 It would be better to set parameters in the kfdef:
&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_istio_dex.yaml#L82-L95&gt;https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_istio_dex.yaml#L82-L95&lt;/denchmark-link&gt;

Any parameter you set here overrides the parameter set in kustomize/oidc-authservice/base/params.env
		</comment>
		<comment id='6' author='lalithvaka' date='2020-03-14T14:09:45Z'>
		
hi @yanniszark looks like this would be an issue to start with as am not able to access this outside our enterprise network and my cluster is on Azure VMs. Is there any workaround to this?

It must be possible for you to start a shell in the authservice pod (or create one pod in authservice's namespace - istio-system) to check if there's an appropriate response from a curl command.
		</comment>
		<comment id='7' author='lalithvaka' date='2020-03-14T14:16:42Z'>
		&lt;denchmark-link:https://github.com/lalithvaka&gt;@lalithvaka&lt;/denchmark-link&gt;
 the parameters end up as environment variables on the oidc-authservice statefulset, in the istio-system namespace.
You can try editing those for quicker iterations.
		</comment>
		<comment id='8' author='lalithvaka' date='2020-03-17T18:00:22Z'>
		

hi @yanniszark looks like this would be an issue to start with as am not able to access this outside our enterprise network and my cluster is on Azure VMs. Is there any workaround to this?

It must be possible for you to start a shell in the authservice pod (or create one pod in authservice's namespace - istio-system) to check if there's an appropriate response from a curl command.

Hi &lt;denchmark-link:https://github.com/krishnadurai&gt;@krishnadurai&lt;/denchmark-link&gt;
 My pod is not starting with constant msgs in the logs as following. Am not sure if there is an alternate to this unless I tear the entire cluster and redeploy. As you know kfctl delete won't work as I have deployed istio as well for me to redeploy kubeflow alone.
My authservice-0 log showing as follows
➜ kubectl logs authservice-0 -n istio-system
time="2020-03-09T22:08:39Z" level=info msg="Starting readiness probe at 8081"
time="2020-03-09T22:08:39Z" level=info msg="No USERID_TOKEN_HEADER specified, using 'kubeflow-userid-token' as default."
time="2020-03-09T22:08:39Z" level=info msg="No SERVER_HOSTNAME specified, using '' as default."
time="2020-03-09T22:08:39Z" level=info msg="No SERVER_PORT specified, using '8080' as default."
time="2020-03-09T22:08:39Z" level=info msg="No SESSION_MAX_AGE specified, using '86400' as default."
time="2020-03-09T22:08:39Z" level=info msg="Starting web server at :8080"
time="2020-03-09T22:08:40Z" level=error msg="OIDC provider setup failed, retrying in 10 seconds: 404 Not Found: \n\n&lt;title&gt;404 Not Found&lt;/title&gt;\n\nNot Found\n
The requested URL /.well-known/openid-configuration was not found on this server.
\n\n"
		</comment>
		<comment id='9' author='lalithvaka' date='2020-03-17T19:29:21Z'>
		Could you possibly deploy another sleep pod to the istio-system namespace?

You could curl from this pod to get the well-known configuration url's
output.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Mar 17, 2020, 11:30 PM Lalith Vaka ***@***.***&gt; wrote:
 hi @yanniszark &lt;https://github.com/yanniszark&gt; looks like this would be
 an issue to start with as am not able to access this outside our enterprise
 network and my cluster is on Azure VMs. Is there any workaround to this?

 It must be possible for you to start a shell in the authservice pod (or
 create one pod in authservice's namespace - istio-system) to check if
 there's an appropriate response from a curl command.

 Hi @krishnadurai &lt;https://github.com/krishnadurai&gt; My pod is not starting
 with constant msgs in the logs as following. Am not sure if there is an
 alternate to this unless I tear the entire cluster and redeploy. As you
 know kfctl delete won't work as I have deployed istio as well for me to
 redeploy kubeflow alone.

 My authservice-0 log showing as follows
 ➜ kubectl logs authservice-0 -n istio-system
 time="2020-03-09T22:08:39Z" level=info msg="Starting readiness probe at
 8081"
 time="2020-03-09T22:08:39Z" level=info msg="No USERID_TOKEN_HEADER
 specified, using 'kubeflow-userid-token' as default."
 time="2020-03-09T22:08:39Z" level=info msg="No SERVER_HOSTNAME specified,
 using '' as default."
 time="2020-03-09T22:08:39Z" level=info msg="No SERVER_PORT specified,
 using '8080' as default."
 time="2020-03-09T22:08:39Z" level=info msg="No SESSION_MAX_AGE specified,
 using '86400' as default."
 time="2020-03-09T22:08:39Z" level=info msg="Starting web server at :8080"
 time="2020-03-09T22:08:40Z" level=error msg="OIDC provider setup failed,
 retrying in 10 seconds: 404 Not Found: \n\n&lt;title&gt;404 Not
 Found&lt;/title&gt;\n\nNot Found\n
 The requested URL /.well-known/openid-configuration was not found on this
 server.
 \n\n"

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#4847 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ABPZEXMVVEFSTRPXIUA3QLDRH63EZANCNFSM4LFI6Y7Q&gt;
 .



		</comment>
		<comment id='10' author='lalithvaka' date='2020-03-17T23:49:35Z'>
		
@lalithvaka the parameters end up as environment variables on the oidc-authservice statefulset, in the istio-system namespace.
You can try editing those for quicker iterations.

Hi &lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
 I was able to change the configuration and redeploy oidc-authservice kustomizeconfig with out deleting the whole cluster / kubeflow install. But now I am running into "Error opening bolt store: open /var/lib/authservice/data.db: permission denied" error. And the authservice statefulset goes into crashloopback error. Am not sure if need to create a local Persistent Volume?
➜ kubectl logs authservice-0 -n istio-system
time="2020-03-17T22:55:03Z" level=info msg="Starting readiness probe at 8081"
time="2020-03-17T22:55:03Z" level=info msg="No  USERID_TOKEN_HEADER  specified, using 'kubeflow-userid-token' as default."
time="2020-03-17T22:55:03Z" level=info msg="No  SERVER_HOSTNAME  specified, using '' as default."
time="2020-03-17T22:55:03Z" level=info msg="No  SERVER_PORT  specified, using '8080' as default."
time="2020-03-17T22:55:03Z" level=info msg="No  SESSION_MAX_AGE  specified, using '86400' as default."
time="2020-03-17T22:55:03Z" level=info msg="Starting web server at :8080"
time="2020-03-17T22:55:04Z" level=fatal msg="Error opening bolt store: open /var/lib/authservice/data.db: permission denied"
➜ kubectl get pods -n istio-system
NAME                                                         READY   STATUS             RESTARTS   AGE
authservice-0                                                0/1     CrashLoopBackOff   5          4m20s
cluster-local-gateway-9d544d7db-n55ln                        1/1     Running            0          8d
istio-citadel-75f76d94f-j5thr                                1/1     Running            0          8d
istio-galley-c9f4fb7bf-fp76b                                 1/1     Running            0          8d
istio-ingressgateway-68875ff4cd-pzf7h                        1/1     Running            0          8d
istio-nodeagent-bbxrd                                        1/1     Running            0          8d
istio-nodeagent-zbdsq                                        1/1     Running            0          8d
istio-pilot-99597b4f6-kcrrq                                  2/2     Running            0          8d
istio-policy-757f4d7856-48khj                                2/2     Running            0          8d
istio-security-post-install-release-1.3-latest-daily-5hdgg   0/1     Completed          0          8d
istio-sidecar-injector-5f5484c5f5-lvwjh                      1/1     Running            1          8d
istio-telemetry-598cdc58bf-2p722                             2/2     Running            0          8d
prometheus-7dbbf976d8-52lkb                                  1/1     Running            0          8d
(base)
		</comment>
		<comment id='11' author='lalithvaka' date='2020-03-19T21:50:07Z'>
		/area centraldashboard
/priority p1
		</comment>
		<comment id='12' author='lalithvaka' date='2020-03-31T22:02:13Z'>
		Thank you &lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
 for helping resolve this issue. "work around this by chowning the pv's underlying folder to the correct group (see fsgroup value on authservice pod) or to something like 755". Also be able to update the scope in the stateful set yaml to exclude groups. Though we might need groups to do the AuthZ eventually.
		</comment>
	</comments>
</bug>