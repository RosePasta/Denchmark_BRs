<bug id='2852' author='rileyjbauer' open_date='2019-03-28T23:20:06Z' closed_time='2019-10-05T05:06:56Z'>
	<summary>upstream connect error or disconnect/reset before headers</summary>
	<description>
I and others have recently been seeing the "upstream connect error or disconnect/reset before headers" error with some frequency.
It doesn't seem to be deterministic, for example, only one of the below requests failed.
&lt;denchmark-link:https://user-images.githubusercontent.com/34456002/55198731-73101200-5174-11e9-84d3-3a3810b7d37f.png&gt;&lt;/denchmark-link&gt;

and upon refreshing the page, a different one, or more, of those same requests may fail.
The errors seem to dissipate after refreshing the page a few times, and I have not yet encountered this while port-forwarding, as opposed to using the "cluster.endpoints.project.cloud.goog" URL for my deployment.
I wasn't sure if this should be its own issue, or should be added to &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/1710&gt;#1710&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='rileyjbauer' date='2019-03-31T21:49:17Z'>
		I think upstream errors are an issue indicating that Ambassador thinks the backend its forwarding traffic to is unhealthy.
Are there particular backends you are seeing this error with?
		</comment>
		<comment id='2' author='rileyjbauer' date='2019-04-04T04:18:40Z'>
		I have seen the same error. This is happening to me when loading runs for a scheduled pipeline in pipeline UI. &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 Do you think this can be caused by pipeline?
		</comment>
		<comment id='3' author='rileyjbauer' date='2019-04-04T04:20:59Z'>
		FWIW this is happening among a batch of requests. the rest of the requests succeeded indicating the backend should be running.
		</comment>
		<comment id='4' author='rileyjbauer' date='2019-04-08T21:12:12Z'>
		
I think upstream errors are an issue indicating that Ambassador thinks the backend its forwarding traffic to is unhealthy.
Are there particular backends you are seeing this error with?

This is happening with the root Kubeflow UX on Kubeflow deploayments with IAM enabled.
It seems to be happening more and more. Previously it was happening after waiting for several hours. Not it can happen after few minutes.
		</comment>
		<comment id='5' author='rileyjbauer' date='2019-04-16T02:35:45Z'>
		&lt;denchmark-link:https://github.com/Ark-kun&gt;@Ark-kun&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/IronPan&gt;@IronPan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rileyjbauer&gt;@rileyjbauer&lt;/denchmark-link&gt;
 when you observe this error can you take a look and provide your Ambassador pod logs?
I noticed this and when I looked at the logs (see below) I saw errors like the following
&lt;denchmark-code&gt;2019-04-16 00:40:50 kubewatch 0.37.0 ERROR: could not watch for Kubernetes service changes
Traceback (most recent call last):
  File "/usr/lib/python3.6/site-packages/urllib3/connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3.6/site-packages/urllib3/util/connection.py", line 83, in create_connection
    raise err
  File "/usr/lib/python3.6/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.6/site-packages/urllib3/connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "/usr/lib/python3.6/site-packages/urllib3/connectionpool.py", line 346, in _make_request
    self._validate_conn(conn)
  File "/usr/lib/python3.6/site-packages/urllib3/connectionpool.py", line 850, in _validate_conn
    conn.connect()
  File "/usr/lib/python3.6/site-packages/urllib3/connection.py", line 284, in connect
    conn = self._new_conn()
  File "/usr/lib/python3.6/site-packages/urllib3/connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f7ff7bafb00&gt;: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ambassador/kubewatch.py", line 440, in main
    watch_loop(restarter)
  File "/ambassador/kubewatch.py", line 341, in watch_loop
    for evt in watched:
  File "/usr/lib/python3.6/site-packages/kubernetes/watch/watch.py", line 122, in stream
    resp = func(*args, **kwargs)
  File "/usr/lib/python3.6/site-packages/kubernetes/client/apis/core_v1_api.py", line 14358, in list_service_for_all_namespaces
    (data) = self.list_service_for_all_namespaces_with_http_info(**kwargs)
  File "/usr/lib/python3.6/site-packages/kubernetes/client/apis/core_v1_api.py", line 14455, in list_service_for_all_namespaces_with_http_info
    collection_formats=collection_formats)
  File "/usr/lib/python3.6/site-packages/kubernetes/client/api_client.py", line 321, in call_api
    _return_http_data_only, collection_formats, _preload_content, _request_timeout)
  File "/usr/lib/python3.6/site-packages/kubernetes/client/api_client.py", line 155, in __call_api
    _request_timeout=_request_timeout)
  File "/usr/lib/python3.6/site-packages/kubernetes/client/api_client.py", line 342, in request
    headers=headers)
  File "/usr/lib/python3.6/site-packages/kubernetes/client/rest.py", line 231, in GET
    query_params=query_params)
  File "/usr/lib/python3.6/site-packages/kubernetes/client/rest.py", line 205, in request
    headers=headers)
  File "/usr/lib/python3.6/site-packages/urllib3/request.py", line 66, in request
    **urlopen_kw)
  File "/usr/lib/python3.6/site-packages/urllib3/request.py", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File "/usr/lib/python3.6/site-packages/urllib3/poolmanager.py", line 321, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "/usr/lib/python3.6/site-packages/urllib3/connectionpool.py", line 668, in urlopen
    **response_kw)
  File "/usr/lib/python3.6/site-packages/urllib3/connectionpool.py", line 668, in urlopen
    **response_kw)
  File "/usr/lib/python3.6/site-packages/urllib3/connectionpool.py", line 668, in urlopen
    **response_kw)
  File "/usr/lib/python3.6/site-packages/urllib3/connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3.6/site-packages/urllib3/util/retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='10.7.240.1', port=443): Max retries exceeded with url: /api/v1/services?watch=True (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f7ff7bafb00&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))
&lt;/denchmark-code&gt;

If you observe this I might suggest trying to kill all your Ambassador pods.
&lt;denchmark-code&gt;kubectl delete pods -l service=ambassador
&lt;/denchmark-code&gt;

Ambassador tries to setup a K8s watch on the APIServer to be notified about service changes. It looks like it is having a problem establishing a connection to the APIServer.
The problem might be dependent on Ambassador as well as your APIServer; is your APIServer under a lot of load?
We are using
quay.io/datawire/ambassador:0.37.0
It might be worth trying a newer version of Ambassador.
@ellis-bigelow Do you recall what the performance issues with Ambassador you saw were?
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/3082778/ambassador-5cf8cd97d5-pqrsw.pods.txt&gt;ambassador-5cf8cd97d5-pqrsw.pods.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='rileyjbauer' date='2019-04-16T13:24:37Z'>
		I ran into this problem while installing seldon to my cluster. I added it in twice, once as seldon and another time as seldon-core. This might have been the root cause for this issue, as well as argocd not syncing.
		</comment>
		<comment id='7' author='rileyjbauer' date='2019-04-16T20:46:00Z'>
		Thanks for the direction &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

I tried killing the pods and after the new ones were up, but I continued to see the errors, and there didn't seem to be anything notable in the ambassador or API server pod logs
		</comment>
		<comment id='8' author='rileyjbauer' date='2019-04-18T19:17:01Z'>
		Seeing this too from recent master in EC2. Went down to 1 ambassador replica but no joy.
		</comment>
		<comment id='9' author='rileyjbauer' date='2019-05-03T14:27:52Z'>
		Re posting as this thread seems more recent and active.
Envoy upstream had an &lt;denchmark-link:https://github.com/envoyproxy/envoy/issues/6190&gt;issue&lt;/denchmark-link&gt;
 that only recently was fixed in dev but not yet fixed in any stable version where if the service it was proxying to ended the connection with a  envoy would responding with only an  and still leave it in its connection pool and would send the next request to that service using that connection.
The service would receive it, say a get request, and then send a RST as since it had already FIN/ACKed it doens't have a way to reply to the request.
Its a roll of the dice whether your request get loaded to an http connection in the pool that is already dead but envoy doesn't know it or goes to a live one which is why the symptoms of this issue are so intermittent.
May be related to what your seeing, to confirm if you have a way to capture packets on the service side you should see the weird behavoir of the service doing a FIN/ACK but envoy only responding with ACK and then sometime later sending another request on that TCP stream triggering the service to send a RST.
In envoy 1.10 they improved the message you get back so after upstream connect error or disconnect/reset before headers you will get more information, in my case got a message like connection terminated so if you upgrade to the latest envoy you may at least get additional information to confirm this what the source of the problem is even if it isn't this specific envoy issue.
		</comment>
		<comment id='10' author='rileyjbauer' date='2019-06-02T17:09:19Z'>
		Related issue: &lt;denchmark-link:https://github.com/datawire/ambassador/issues/504&gt;datawire/ambassador#504&lt;/denchmark-link&gt;
 There is some specific mention of issues with Seldon.
		</comment>
		<comment id='11' author='rileyjbauer' date='2019-07-28T20:08:01Z'>
		0.6 so now available. If you are encountering this issue with a version early then Kubeflow 0.6 please try upgrading to 0.6.
Per the discussion in this thread we think this might have been due to an issue with envoy and/or Ambassador.
With 0.6 we've switched to ISTIO and are using much newer versions of envoy. So we need to see if this problem still reoccurs and then we can try to track it down.
		</comment>
		<comment id='12' author='rileyjbauer' date='2019-08-08T01:18:39Z'>
		Saw this with 0.6.1 in AWS
		</comment>
		<comment id='13' author='rileyjbauer' date='2019-08-15T22:44:47Z'>
		I saw this with 0.6.1 on GCP, using basic auth IIRC.
&lt;denchmark-link:https://user-images.githubusercontent.com/115093/63131731-8ed55280-bf73-11e9-9f90-5d009439a14a.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='rileyjbauer' date='2019-08-16T03:14:41Z'>
		basic auth is still using Ambassador and probably an older version of envoy.
If your seeing this error please clarify whether the error is persistent or whether a refresh fixes it.
		</comment>
		<comment id='15' author='rileyjbauer' date='2019-08-16T14:07:33Z'>
		In my case a few refreshes fixed it.
		</comment>
		<comment id='16' author='rileyjbauer' date='2019-10-03T06:39:54Z'>
		In my case it is the problem on the resources (memory allocation, cpus, etc).
I didn't follow this one; &lt;denchmark-link:https://www.kubeflow.org/docs/started/k8s/overview/#minimum-system-requirements&gt;https://www.kubeflow.org/docs/started/k8s/overview/#minimum-system-requirements&lt;/denchmark-link&gt;
, that's why it's crashing down
In case you don't wanna look at the link, here are the recommended specs:
&lt;denchmark-code&gt;Minimum system requirements
The Kubernetes cluster must meet the following minimum requirements:

Kubernetes version 1.11 or later.
At least one worker node with a minimum of:
* 4 CPU
* 50 GB storage
* 12 GB memory
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='rileyjbauer' date='2019-10-05T05:06:56Z'>
		Thanks &lt;denchmark-link:https://github.com/arianyambao&gt;@arianyambao&lt;/denchmark-link&gt;

I'm going to close this issue.

upstream connect error or disconnect/reset before headers is a generic error that occurs when envoy is trying to forward traffic to a service that is unavailable.
Threre's lots of reasons this can happen
The migration to ISTIO with 0.6 seems to have reduced the general frequency of these errors.

		</comment>
	</comments>
</bug>