<bug id='3930' author='amygdala' open_date='2019-08-16T21:43:20Z' closed_time='2020-02-14T18:04:31Z'>
	<summary>GKE cluster autoprovisioning settings are problematic</summary>
	<description>
/kind bug
With KF 0.6.1 on GKE
The kfctl autoprovisioning settings seem problematic for the GKE cluster setup.  If you create a new autoscaling node pool with n1-standard-8 nodes and min of 0, it can't actually scale, as that would bump the CPUs to 24, over the configured max.  This happens, e.g., if you follow the node pool creation example in the docs about using preemptible VMs.
The low number of GPUs could be problematic as well.  I think a user won't know/realize about the auto-provisioning config and will not understand why autoscaling is not working.
&lt;denchmark-link:https://user-images.githubusercontent.com/115093/63199740-9eb46b80-c033-11e9-876d-f95b078d0f47.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='amygdala' date='2019-08-16T21:43:23Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.96. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='amygdala' date='2019-08-19T02:32:11Z'>
		&lt;denchmark-link:https://github.com/amygdala&gt;@amygdala&lt;/denchmark-link&gt;
 Do you have recommended default settings?
The settings are configured here



kubeflow/deployment/gke/deployment_manager_configs/cluster-kubeflow.yaml


         Line 70
      in
      169f03d






 autoprovisioning-config: 








kubeflow/deployment/gke/deployment_manager_configs/cluster.jinja


         Line 98
      in
      169f03d






 enableNodeAutoprovisioning: true 





		</comment>
		<comment id='3' author='amygdala' date='2019-08-21T18:13:52Z'>
		This is kind of a tough question.  We'd presumably want the values to be reasonably small. But the problem is that if someone also creates an explicit node pool, configured to use autoscaling, it probably won't scale as intended, and (if the user is not aware of the autoprovisioning constraints) it will be hard for them to figure out why.
I think that we should probably:

bump the CPU to at least 48 and the k80s to at least 16. And memory to ~500?
add some info about this on the 'troubleshooting' page, and include the command to adjust autoprovisioning settings
consider having it be a high-level CLI flag as to whether or not to use autoprovisioning for the cluster ?  If the user had to make an explicit decision about whether to use it, they will understand better if it breaks subsequent autoscaling.

		</comment>
		<comment id='4' author='amygdala' date='2019-09-11T05:30:45Z'>
		&lt;denchmark-link:https://github.com/amygdala&gt;@amygdala&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kunmingg&gt;@kunmingg&lt;/denchmark-link&gt;

we need this for kubecon SD where cluster-kubeflow.yaml needs to include
&lt;denchmark-code&gt;    cpu-pool-min-cpu-platform ‚ÄúIntel Skylake"
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='amygdala' date='2019-09-11T22:31:47Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kkasravi&gt;@kkasravi&lt;/denchmark-link&gt;
  can these settings be surfaced via the gcp plugin spec?
We can surface them in the spec or give the user a method to specify their own cluster_config.
WDYT?
		</comment>
		<comment id='6' author='amygdala' date='2019-09-13T00:00:42Z'>
		If users want to customize the GKE settings the recommended way to do that is to modify the DM configs after they are generated. If we start adding configuration options into GcpPluginSpec my worry is that we will end up exposing a never ending set of options. The DM configs correspond to well defined APIs and resources (i.e. match the GKE REST API).
If we think the default values could be improved then we can fix that.
		</comment>
		<comment id='7' author='amygdala' date='2019-11-07T23:20:05Z'>
		I see the problem now.
The limits sets for auto provisioning are limits for the cluster not per node. So setting a max of 20 CPUs means the cluster as a whole should have a maximum of 20 CPUs not that a node should have a maximum of 20.
It also looks like we can list multiple accelerators
&lt;denchmark-link:https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning&gt;https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#Cluster.ResourceLimit&gt;https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#Cluster.ResourceLimit&lt;/denchmark-link&gt;

Related issue: &lt;denchmark-link:https://github.com/kubeflow/kfctl/issues/82&gt;kubeflow/kfctl#82&lt;/denchmark-link&gt;
 - auto-optimize cluster size
Related issue: &lt;denchmark-link:https://github.com/kubeflow/kfctl/issues/59&gt;kubeflow/kfctl#59&lt;/denchmark-link&gt;
 - auto set GPU type for node auto provisioner
One thing we could do is automatically set the limits based on the quota a user has?
		</comment>
		<comment id='8' author='amygdala' date='2019-11-09T17:43:45Z'>
		yeah, it is per cluster.
I wonder if it's a good idea to use the user's quota as the limit‚Äî at first that sounded good, but now I am thinking that if they have some v. high quota, then potentially they could scale out more than they meant to with some ill-configured job, then get an unexpectedly high bill etc.
Maybe we could just set it reasonably high (I'm assuming that if it's higher than the user's actual quota, things still work), and point to the docs on how to modify it if they want to adjust it.
		</comment>
		<comment id='9' author='amygdala' date='2020-02-07T18:04:23Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>