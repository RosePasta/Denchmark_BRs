<bug id='3767' author='abhi-g' open_date='2019-07-26T22:29:54Z' closed_time='2020-05-07T19:52:01Z'>
	<summary>kfctl delete leaves behind istio resources</summary>
	<description>
When deleting a Kubeflow deployment using kfctl delete, it deletes 'kubeflow' namespace but leaves behind the istio-system namespace. Therefore the istio resources still hang around after deleting Kubeflow deployment.
I can manaully delete the istio-system namespace, but ideally kfctl delete can do the complete cleanup.
	</description>
	<comments>
		<comment id='1' author='abhi-g' date='2019-07-26T22:29:56Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.98. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='abhi-g' date='2019-07-26T22:30:08Z'>
		/cc &lt;denchmark-link:https://github.com/gabrielwen&gt;@gabrielwen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kunmingg&gt;@kunmingg&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='abhi-g' date='2019-07-27T00:38:39Z'>
		I also see this and just wondering it's a by design feature or a bug. Last time I remember someone mentioned there's concern to delete existing istio people deployed?
		</comment>
		<comment id='4' author='abhi-g' date='2019-07-28T15:15:52Z'>
		Istio namespace seems to clean up correctly, but CRDs:
experiments.kubeflow.org
notebooks.kubeflow.org
poddefaults.kubeflow.org
profiles.kubeflow.org
pytorchjobs.kubeflow.org
scheduledworkflows.kubeflow.org
tfjobs.kubeflow.org
trials.kubeflow.org
viewers.kubeflow.org
workflows.argoproj.io
Are left behind
		</comment>
		<comment id='5' author='abhi-g' date='2019-07-28T15:41:14Z'>
		Also I see cluster roles (which are global) not being deleted either
		</comment>
		<comment id='6' author='abhi-g' date='2019-07-28T19:34:39Z'>
		&lt;denchmark-link:https://github.com/kkasravi&gt;@kkasravi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
 Thoughts on what the desired behavior should be?
&lt;denchmark-link:https://github.com/kkasravi&gt;@kkasravi&lt;/denchmark-link&gt;
 Is not deleting the CRDs WAI or is that a bug?
		</comment>
		<comment id='7' author='abhi-g' date='2019-07-29T06:49:36Z'>
		Deleting namespace 'kubeflow' alone doesn't seem right. All global resources and non-kubeflow resources are left behind.  This is a bigger problem when we try KF installation on existing k8s clusters(using kfctl_k8s_istio.yaml). The expectation of Kfctl delete would be restoring the state back to where we were(without Kubeflow).  Why don't we delete the same way as we created each component, instead of namespace delete?.   With partial deletes, it will be difficult to debug and will  lead to many unexpected errors.
Related: &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/3684&gt;#3684&lt;/denchmark-link&gt;

Also, Can we get the fix this in 0.6 itself?
		</comment>
		<comment id='8' author='abhi-g' date='2019-07-29T13:21:38Z'>
		Fully agreed. All CRDs, Cluster Roles and Role binding should be cleaned up
		</comment>
		<comment id='9' author='abhi-g' date='2019-07-31T11:10:32Z'>
		storage is also not removed &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/3646&gt;#3646&lt;/denchmark-link&gt;

This is a fairly recent regression. Earlier behavior prior to the regression was to delete the namespace and then delete all cluster level resources by label.
Probably the better approach is to have the client (kfctl) just delete the applications and have the application-controller delete the resources since it has set each resource's owner to itself. This would include cluster-level resources. We definitely need an e2e test to verify this behavior.
		</comment>
		<comment id='10' author='abhi-g' date='2019-07-31T11:16:58Z'>
		&lt;denchmark-link:https://github.com/kunmingg&gt;@kunmingg&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zabbasi&gt;@zabbasi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gabrielwen&gt;@gabrielwen&lt;/denchmark-link&gt;

One question related to kfctl delete all is what is done with profiles. Are these deleted? Since an individual's profile may also include cloud-provider related resources, are these deleted as well?
See my comment above about having the application-controller do cleanup.
		</comment>
		<comment id='11' author='abhi-g' date='2019-08-01T22:03:35Z'>
		I ran into this on master ( &lt;denchmark-link:https://github.com/kubeflow/kubeflow/commit/df2b68d92cf15374b1706d3d91b8123972b8920c&gt;df2b68d&lt;/denchmark-link&gt;
 ), and the  namespace was still there.
		</comment>
		<comment id='12' author='abhi-g' date='2019-08-01T22:04:45Z'>
		It also left behind the default namespace (for me that was kubeflow-programmerboo).
		</comment>
		<comment id='13' author='abhi-g' date='2019-08-01T22:15:32Z'>
		I had a discussion with &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;
 about this. It seems like istio-system is the namespace created by the istio deployment. If a deployment of Kubeflow was done into a system that already has istio configured, it will have an istio-system namespace. In this case if the kfctl delete removes the istio-namespace it will end up deleting all pre-existing policies.
I don't know what the correct solution is:

Create kubeflow specific istio mesh in a different namespace: This seems complex and I am not sure if it will work correctly. FYI getting Istio configured and working was non-trivial..
or 2. Delete all the kubeflow specific istio policies that were added during deployment. this could get trickly as well and we'll always need to remember to update deletion symmetric to any policy changes.

		</comment>
		<comment id='14' author='abhi-g' date='2019-08-01T23:08:18Z'>
		So from trying to deploy k8s with istio, right now if istio is already deployed it fails to deploy, so I think worrying about stomping on an existing istio namespace is perhaps not an immediate concern.
		</comment>
		<comment id='15' author='abhi-g' date='2019-08-02T02:19:31Z'>
		If user has opted to install istio through k8s, it should be deleted as well. If he has opted not to install istio, he would have commented out already &lt;denchmark-link:https://github.com/kubeflow/kubeflow/blob/master/bootstrap/config/kfctl_k8s_istio.yaml#L19&gt;https://github.com/kubeflow/kubeflow/blob/master/bootstrap/config/kfctl_k8s_istio.yaml#L19&lt;/denchmark-link&gt;
  Can't we use the same config as the source of truth to decide what components to delete?
		</comment>
		<comment id='16' author='abhi-g' date='2019-08-02T19:09:04Z'>
		In the past we've used labels, so I think there may have been a regression.
I'll try and get some cycles to fix by Monday. Assigning myself to this...
		</comment>
		<comment id='17' author='abhi-g' date='2019-08-02T19:09:30Z'>
		/assign &lt;denchmark-link:https://github.com/kkasravi&gt;@kkasravi&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='abhi-g' date='2019-09-02T10:19:09Z'>
		&lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/2415&gt;#2415&lt;/denchmark-link&gt;
 is about cleaning up all resources but was closed. Not sure if fixed and this is a regression or not. These are the resources that are left behind after running :
&lt;denchmark-code&gt;$ kubectl api-resources --verbs=list -o name  | grep -v '^componentstatuses$'| xargs -n 1 kubectl get --all-namespaces --show-kind --ignore-not-found -l app.kubernetes.io/part-of=kubeflow
NAME                                                                                        CREATED AT
customresourcedefinition.apiextensions.k8s.io/notebooks.kubeflow.org                        2019-08-29T16:50:52Z
customresourcedefinition.apiextensions.k8s.io/seldondeployments.machinelearning.seldon.io   2019-08-29T12:09:35Z
customresourcedefinition.apiextensions.k8s.io/tfjobs.kubeflow.org                           2019-08-29T16:52:05Z
NAME                                                                                AGE
clusterrolebinding.rbac.authorization.k8s.io/jupyter-web-app-cluster-role-binding   3d17h
clusterrolebinding.rbac.authorization.k8s.io/notebook-controller-role-binding       3d17h
clusterrolebinding.rbac.authorization.k8s.io/seldon-operator-manager-rolebinding    3d17h
clusterrolebinding.rbac.authorization.k8s.io/tf-job-dashboard                       3d17h
clusterrolebinding.rbac.authorization.k8s.io/tf-job-operator                        3d17h
NAME                                                                 AGE
clusterrole.rbac.authorization.k8s.io/jupyter-web-app-cluster-role   80m
clusterrole.rbac.authorization.k8s.io/notebook-controller-role       3d17h
clusterrole.rbac.authorization.k8s.io/seldon-operator-manager-role   3d17h
clusterrole.rbac.authorization.k8s.io/tf-job-dashboard               77m
clusterrole.rbac.authorization.k8s.io/tf-job-operator                77m
&lt;/denchmark-code&gt;

(For some reason get componentstatuses returns all statuses, no matter what label selector is used. So I've excepted it here)
		</comment>
		<comment id='19' author='abhi-g' date='2019-09-02T10:20:49Z'>
		Btw, while this might seem like a 'nice to have', it's actually crucial to have a clean way to remove the resources since otherwise it's impossible to ever update/upgrade kubeflow given that right now it means re-installing from a clean slate. I've ran into a multitude of problems because of resources that weren't deleted and hard to find 'manually'. So I hope we can get a fix for this soon.
		</comment>
		<comment id='20' author='abhi-g' date='2019-09-02T11:20:23Z'>
		Looks like there are more resources not labeled with app.kubernetes.io/part-of=kubeflow:
&lt;denchmark-code&gt;$ kubectl api-resources --verbs=list -o name  | grep -v '^componentstatuses$'| xargs -n 1 kubectl get --all-namespaces --show-kind --ignore-not-found -l kustomize.component
NAME                                                                                                         CREATED AT
mutatingwebhookconfiguration.admissionregistration.k8s.io/admission-webhook-mutating-webhook-configuration   2019-07-24T13:52:49Z
NAME                                                                                       CREATED AT
customresourcedefinition.apiextensions.k8s.io/compositecontrollers.metacontroller.k8s.io   2019-07-24T13:52:01Z
customresourcedefinition.apiextensions.k8s.io/controllerrevisions.metacontroller.k8s.io    2019-07-24T13:52:01Z
customresourcedefinition.apiextensions.k8s.io/decoratorcontrollers.metacontroller.k8s.io   2019-07-24T13:52:01Z
customresourcedefinition.apiextensions.k8s.io/poddefaults.kubeflow.org                     2019-08-29T16:48:21Z
customresourcedefinition.apiextensions.k8s.io/profiles.kubeflow.org                        2019-08-29T16:54:24Z
NAME                                                                                            AGE
clusterrolebinding.rbac.authorization.k8s.io/admission-webhook-bootstrap-cluster-role-binding   3d16h
clusterrolebinding.rbac.authorization.k8s.io/admission-webhook-cluster-role-binding             3d16h
clusterrolebinding.rbac.authorization.k8s.io/argo                                               3d17h
clusterrolebinding.rbac.authorization.k8s.io/argo-ui                                            3d17h
clusterrolebinding.rbac.authorization.k8s.io/centraldashboard                                   3d17h
clusterrolebinding.rbac.authorization.k8s.io/meta-controller-cluster-role-binding               3d16h
clusterrolebinding.rbac.authorization.k8s.io/profiles-cluster-role-binding                      3d16h
clusterrolebinding.rbac.authorization.k8s.io/pytorch-operator                                   3d17h
clusterrolebinding.rbac.authorization.k8s.io/spartakus                                          3d16h
NAME                                                                             AGE
clusterrole.rbac.authorization.k8s.io/admission-webhook-bootstrap-cluster-role   39d
clusterrole.rbac.authorization.k8s.io/admission-webhook-cluster-role             39d
clusterrole.rbac.authorization.k8s.io/argo                                       95m
clusterrole.rbac.authorization.k8s.io/argo-ui                                    95m
clusterrole.rbac.authorization.k8s.io/centraldashboard                           95m
clusterrole.rbac.authorization.k8s.io/pytorch-operator                           92m
clusterrole.rbac.authorization.k8s.io/spartakus                                  39d
&lt;/denchmark-code&gt;

Unfortunately there are even more resources that have no tag at all but look like they are part of kubeflow like the clusterroles: katib-controller, katib-ui, ml-pipeline-persistenceagent, ml-pipeline-viewer-controller-role, pipeline-runner
And these clusterrolebindings: katib-controller katib-ui ml-pipeline-persistenceagent ml-pipeline-scheduledworkflow ml-pipeline-viewer-crd-role-binding pipeline-runner
		</comment>
		<comment id='21' author='abhi-g' date='2019-09-02T11:24:15Z'>
		And more CRDs that aren't labeled either:
&lt;denchmark-code&gt;kubectl get crds|egrep 'kubeflow|dex'
authcodes.dex.coreos.com                      2019-08-29T16:46:42Z
authrequests.dex.coreos.com                   2019-08-29T16:46:42Z
connectors.dex.coreos.com                     2019-08-29T16:46:42Z
experiments.kubeflow.org                      2019-08-29T16:49:34Z
oauth2clients.dex.coreos.com                  2019-08-29T16:46:42Z
offlinesessionses.dex.coreos.com              2019-08-29T16:46:42Z
passwords.dex.coreos.com                      2019-08-29T16:46:42Z
pytorchjobs.kubeflow.org                      2019-08-29T16:51:08Z
refreshtokens.dex.coreos.com                  2019-08-29T16:46:42Z
scheduledworkflows.kubeflow.org               2019-08-29T16:54:09Z
signingkeies.dex.coreos.com                   2019-08-29T16:46:42Z
trials.kubeflow.org                           2019-08-29T16:49:35Z
viewers.kubeflow.org                          2019-08-29T16:53:53Z
&lt;/denchmark-code&gt;

So if anyone is in a similar situation like me and can't just recreate the k8s cluster on each update, this should clean up the remains (but it's not very safe, given there is no clear ownership label):
&lt;denchmark-code&gt;for labels in app.kubernetes.io/part-of=kubeflow kustomize.component; do
  kubectl api-resources --verbs=list -o name  | grep -v '^componentstatuses$' \
    | xargs -n 1 kubectl delete --all-namespaces --show-kind --ignore-not-found -l app.kubernetes.io/part-of=kubeflow
done

kubectl delete clusterroles        katib-controller katib-ui ml-pipeline-persistenceagent ml-pipeline-viewer-controller-role pipeline-runner
kubectl delete clusterrolebindings katib-controller katib-ui ml-pipeline-persistenceagent ml-pipeline-scheduledworkflow ml-pipeline-viewer-crd-role-binding pipeline-runner

kubectl get crd -o name| egrep 'kubeflow|dex'|xargs kubectl delete
&lt;/denchmark-code&gt;

		</comment>
		<comment id='22' author='abhi-g' date='2019-09-02T14:21:25Z'>
		More leftovers:
&lt;denchmark-code&gt;MutatingWebhookConfiguration admission-webhook-mutating-webhook-configuration
MutatingWebhookConfiguration experiment-mutating-webhook-config
MutatingWebhookConfiguration istio-sidecar-injector
MutatingWebhookConfiguration mutating-webhook-configuration
&lt;/denchmark-code&gt;

		</comment>
		<comment id='23' author='abhi-g' date='2019-09-11T22:17:24Z'>
		I don't believe we've done much to change the default istio install. &lt;denchmark-link:https://github.com/gabrielwen&gt;@gabrielwen&lt;/denchmark-link&gt;
 seems like we should cleanup misleading labels similar to how we removed ksonnet labels. Thoughts?
		</comment>
		<comment id='24' author='abhi-g' date='2019-09-11T22:23:54Z'>
		Helm v3 (now in beta) also does away with Tiller and does all of it's templating/generation client side.
Maybe that would allow for using Helm for installing istio instead of the output of helm template stored side-by-side with the kubeflow installation files. Helm chart versions can be pinned if compatibility is a concern.
I would really like that approach because then I can manage istio similarly to other installations and not have to know two different istio installation patterns, especially with regards to how complex istio already is.
		</comment>
		<comment id='25' author='abhi-g' date='2019-09-15T02:51:25Z'>
		Kubeflow doesn't use helm.
		</comment>
		<comment id='26' author='abhi-g' date='2019-10-04T15:24:53Z'>
		I had some major issues trying to reinstall kubeflow, since I'd accidentally introduced some components from the master manifests. Among others, kf-serving leaves behind some webhooks which prevented me from installing kubeflow again:
&lt;denchmark-code&gt;kubectl api-resources --verbs=list | grep hook
mutatingwebhookconfigurations                               admissionregistration.k8s.io   false        MutatingWebhookConfiguration
validatingwebhookconfigurations                             admissionregistration.k8s.io   false        ValidatingWebhookConfiguration

kubectl get mutatingwebhookconfigurations
NAME                                               CREATED AT
admission-webhook-mutating-webhook-configuration   2019-08-21T13:15:49Z
experiment-mutating-webhook-config                 2019-08-21T13:14:46Z
kfservice.serving.kubeflow.org                     2019-09-26T15:34:39Z
mutating-webhook-configuration                     2019-08-23T13:24:59Z

kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io 
NAME                                   CREATED AT
experiment-validating-webhook-config   2019-08-21T13:14:46Z
kfservice.serving.kubeflow.org         2019-09-14T20:34:59Z
studyjob-validating-webhook-config     2019-06-26T12:07:40Z
validating-webhook-configuration       2019-08-23T13:25:00Z

&lt;/denchmark-code&gt;

I'm not sure if all the hooks are from kubeflow, but at least the kfservice one was driving me insane. Basically I got this error when applying kfctl manifests, in case anyone else runs into this:
&lt;denchmark-code&gt;Error: couldn't apply KfApp:  (kubeflow.error): Code 500 with message: kfApp Apply failed for kustomize:  (kubeflow.error): Code 500 with message: couldn't create resources from katib-controller Error: Internal error occurred: failed calling webhook "kfservice.kfserving-webhook-server.deployment-mutator": Post https://kfserving-webhook-server-service.kubeflow.svc:443/mutate-deployments?timeout=30s: service "kfserving-webhook-server-service" not found
&lt;/denchmark-code&gt;

I didn't check their labels or so, so I couldn't verify if they were managed by kubeflow or similar, but it is indeed a major pain that non-namespaced resources are not cleaned up. I could imagine this problem is worse in on-prem clusters (as is my case) though, where it's harder to just tear down everything and try again with a fresh start.
May I suggest a solution where you can provide an option for kfctl to label everything generated at install to be deleted when uninstalling? This way, it's easier to be aware that you'll remove everything related to kubeflow when uninstalling, so you don't accidentally remove for example your pre-existing istio.
		</comment>
		<comment id='27' author='abhi-g' date='2019-10-14T10:27:32Z'>
		For those looking for an easy hack, I guess following will help
kubectl delete all -n istio-system --all
		</comment>
		<comment id='28' author='abhi-g' date='2019-10-17T15:10:48Z'>
		I haven't been able to install Kubeflow yet -- possibly related to this. Not only Istio is a problem. I'm now getting:
&lt;denchmark-code&gt;couldn't generate KfApp:  (kubeflow.error): Code 500 with message: kfApp Generate failed for kustomize: kustomize generate failed Error:  (kubeflow.error): Code 500 with message: couldn't copy application katib-db
&lt;/denchmark-code&gt;

Maybe you should be prefixing namespace of other tools installed with kubeflow- (e.g. kubeflow-istio-system) and then they can be deleted to clean up. Optionally, then, you could offer the ability to share resources like istio on install. In this case, they wouldn't be deleted on cleanup.
In the meantime, ... how do I get rid of katib-db (etc.)? A full list rather than picking through the rubble would be great.
		</comment>
		<comment id='29' author='abhi-g' date='2019-10-18T08:41:50Z'>
		This command will look through all resource types your cluster has available and list the ones related to your search term; while not a direct solution it's the most efficient witch-hunting method I have for these problems atm:
kubectl api-resources --verbs=list -o name | xargs -I % -n 1 sh -c 'echo "Resource type:" %; kubectl get % -o name | grep &lt;search-term&gt; ;echo'
Although I never saw anything related to katib when removing Kubeflow, as far as I can tell all katib related things are namespaced, did you remove the kubeflow namespace? If you're using v0.6.2 that is
NOTE: Be careful if you decide to pipe this into kubectl delete in some way (just remove the echo Resource type statement for example), as this goes through ALL resource types, including non-namespaced global resources!!
		</comment>
		<comment id='30' author='abhi-g' date='2020-01-16T10:21:52Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='31' author='abhi-g' date='2020-01-22T20:24:37Z'>
		Issue still exists in kfctl v1.0-rc.1
		</comment>
		<comment id='32' author='abhi-g' date='2020-02-06T21:41:05Z'>
		It would be nice to have this issue resolved and kfctl delete actually cleanly removing all resources and CRD's. Does not seem possible in 0.7 or 1.0rc to cleanup kubeflow other than writing our own scripts to remove all resources.
		</comment>
		<comment id='33' author='abhi-g' date='2020-05-06T22:02:19Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='34' author='abhi-g' date='2020-05-07T10:03:54Z'>
		/lifecycle-frozen
		</comment>
		<comment id='35' author='abhi-g' date='2020-05-07T19:51:57Z'>
		This should be fixed in the latest kfctl.
We changed the delete algorithm.
/close
		</comment>
		<comment id='36' author='abhi-g' date='2020-05-07T19:52:01Z'>
		&lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
: Closing this issue.

In response to this:

This should be fixed in the latest kfctl.
We changed the delete algorithm.
/close

Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository.

		</comment>
		<comment id='37' author='abhi-g' date='2020-07-15T01:51:56Z'>
		this still exist on kfctl_1.0.2
		</comment>
		<comment id='38' author='abhi-g' date='2020-07-30T06:51:58Z'>
		&lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;

Even we are facing the same with kfctl v1.0.2-0.
kfctl version
kfctl v1.0.2-0-ga476281
kubectl version --short
Client Version: v1.16.0
Server Version: v1.16.9
		</comment>
		<comment id='39' author='abhi-g' date='2020-07-30T08:37:26Z'>
		Guys its working fine with "kfctl v1.1.0-0"
Just now I tried.
		</comment>
	</comments>
</bug>