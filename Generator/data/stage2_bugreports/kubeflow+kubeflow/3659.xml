<bug id='3659' author='kkasravi' open_date='2019-07-15T14:13:29Z' closed_time='2019-11-04T20:58:59Z'>
	<summary>CrashLoopBackOff in iap-enabler pod</summary>
	<description>
after deploying gcp i'm seeing a crashloopbackoff
kubectl get pods
&lt;denchmark-code&gt;NAME                                      READY   STATUS             RESTARTS   AGE
backend-updater-0                         1/1     Running            0          28m
grafana-7f4d444dd5-wk4kd                  1/1     Running            0          33m
iap-enabler-778889874f-kplrj              0/1     CrashLoopBackOff   9          28m
ingress-bootstrap-2zzhq                   1/1     Running            0          28m
istio-citadel-765998c84c-nwkkn            1/1     Running            0          33m
istio-cleanup-secrets-1.1.6-lr6kp         0/1     Completed          0          32m
istio-egressgateway-75cbcf7ddd-qf92v      1/1     Running            0          33m
istio-egressgateway-75cbcf7ddd-v4f6q      1/1     Running            0          30m
istio-egressgateway-75cbcf7ddd-vqnjz      1/1     Running            0          30m
istio-galley-68c4567df4-2qt8q             1/1     Running            0          32m
istio-grafana-post-install-1.1.6-tpppq    0/1     Completed          0          32m
istio-ingressgateway-5f55c95767-rb9p2     1/1     Running            0          28m
istio-ingressgateway-5f55c95767-vjl7j     1/1     Running            0          32m
istio-ingressgateway-5f55c95767-w2qj5     1/1     Running            0          30m
istio-pilot-7fcfd5fb7c-2khsz              2/2     Running            0          28m
istio-pilot-7fcfd5fb7c-5vc4w              2/2     Running            0          32m
istio-pilot-7fcfd5fb7c-9466m              2/2     Running            0          30m
istio-pilot-7fcfd5fb7c-9z95w              2/2     Running            0          30m
istio-pilot-7fcfd5fb7c-cwxs8              2/2     Running            0          30m
istio-policy-84f55df645-pnmpt             2/2     Running            1          32m
istio-policy-84f55df645-zf9zw             2/2     Running            0          31m
istio-security-post-install-1.1.6-h2lgt   0/1     Completed          0          32m
istio-sidecar-injector-649f4f4dfd-cmkzp   1/1     Running            0          32m
istio-telemetry-cd6777c59-zvlsp           2/2     Running            1          32m
istio-tracing-79db5954f-xrk6p             1/1     Running            0          32m
kiali-68677d47d7-kg5lt                    1/1     Running            0          32m
prometheus-5977597c75-vxr7n               1/1     Running            0          32m
whoami-app-5dff755cf6-9gl5t               1/1     Running            0          28m
&lt;/denchmark-code&gt;

I'm seeing this error
&lt;denchmark-code&gt;$ ‚òû  kubectl logs -f iap-enabler-778889874f-kplrj
Activated service account credentials for: [dls-kf-admin@constant-cubist-173123.iam.gserviceaccount.com]
[component_manager]
disable_update_check = true
[core]
account = dls-kf-admin@constant-cubist-173123.iam.gserviceaccount.com
disable_usage_reporting = true
project = constant-cubist-173123
[metrics]
environment = github_docker_image

Your active configuration is: [default]
node port is 31380
fetching backends info with envoy-ingress: {"k8s-be-31144--006b1afb10dc94eb":"HEALTHY","k8s-be-31380--006b1afb10dc94eb":"HEALTHY"}
backend name is k8s-be-31380--006b1afb10dc94eb
Waiting for backend id PROJECT=constant-cubist-173123 NAMESPACE=istio-system SERVICE=istio-ingressgateway filter=name~k8s-be-31380--006b1afb10dc94eb
BACKEND_ID=8317341060107943013
patch JWT audience: /projects/336335541993/global/backendServices/8317341060107943013
policy.authentication.istio.io/ingress-jwt not patched
Clearing lock on service annotation
service/istio-ingressgateway not patched
Mon Jul 15 14:04:14 UTC 2019 WARN: Backend check failed, restarting container.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kkasravi' date='2019-07-15T14:13:32Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.79. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='kkasravi' date='2019-07-15T14:13:45Z'>
		/area kfctl
		</comment>
		<comment id='3' author='kkasravi' date='2019-07-15T15:12:22Z'>
		Related to &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/3628&gt;#3628&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='kkasravi' date='2019-10-16T13:19:38Z'>
		I seem to have the same problem. Initiated deployment via &lt;denchmark-link:https://deploy.kubeflow.cloud/#/deploy&gt;https://deploy.kubeflow.cloud/#/deploy&lt;/denchmark-link&gt;
. All deployments ok bar one, a  for iap-enabler. I enclose a redacted log.
&lt;denchmark-code&gt;$ kubectl get pods --all-namespaces
NAMESPACE      NAME                                                             READY   STATUS             RESTARTS   AGE
&lt;..&gt;
istio-system   iap-enabler-778889874f-gmnvf                                     0/1     CrashLoopBackOff   12         41m
&lt;..&gt;
$ kubectl logs --namespace istio-system -f iap-enabler-778889874f-gmnvf
Activated service account credentials for: [&lt;..&gt;]
[component_manager]
disable_update_check = true
[core]
account = &lt;..&gt;
disable_usage_reporting = true
project = &lt;..&gt;
[metrics]
environment = github_docker_image

Your active configuration is: [default]
node port is 31380
fetching backends info with envoy-ingress: {"k8s-be-&lt;..&gt;":"HEALTHY","k8s-be-&lt;..&gt;":"HEALTHY"}
backend name is k8s-be-&lt;..&gt;
Waiting for backend id PROJECT=&lt;..&gt; NAMESPACE=istio-system SERVICE=istio-ingressgateway filter=name~k8s-be-&lt;..&gt;
BACKEND_ID=1143116796548264619
patch JWT audience: /projects/&lt;..&gt;/global/backendServices/&lt;..&gt;
policy.authentication.istio.io/ingress-jwt not patched
Clearing lock on service annotation
service/istio-ingressgateway not patched

$ kubectl describe pod --namespace istio-system iap-enabler-778889874f-gmnvf
Name:               iap-enabler-778889874f-gmnvf
Namespace:          istio-system
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               gke-text-class-kf-19-text-class-&lt;..&gt;/10.132.0.5
Start Time:         Wed, 16 Oct 2019 14:25:26 +0200
Labels:             kustomize.component=iap-ingress
                    pod-template-hash=&lt;..&gt;
                    service=iap-enabler
Annotations:        &lt;none&gt;
Status:             Running
IP:                 &lt;..&gt;
Controlled By:      ReplicaSet/iap-enabler-&lt;..&gt;
Containers:
  iap:
    Container ID:  docker://&lt;..&gt;
    Image:         gcr.io/kubeflow-images-public/ingress-setup:latest
    Image ID:      docker-pullable://gcr.io/kubeflow-images-public/ingress-setup@sha256:91db78e53a3b87cf64829c096b25d6b4ab0770b28fec0f7641a0e1707d12c83c
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
      /var/envoy-config/setup_backend.sh
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Wed, 16 Oct 2019 15:05:53 +0200
      Finished:     Wed, 16 Oct 2019 15:06:01 +0200
    Ready:          False
    Restart Count:  12
    Environment:
      NAMESPACE:                       istio-system
      SERVICE:                         istio-ingressgateway
      INGRESS_NAME:                    envoy-ingress
      ENVOY_ADMIN:                     http://localhost:8001
      GOOGLE_APPLICATION_CREDENTIALS:  /var/run/secrets/sa/admin-gcp-sa.json
      USE_ISTIO:                       true
    Mounts:
      /var/envoy-config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from envoy-token-tgcpl (ro)
      /var/run/secrets/sa from sa-key (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      envoy-config
    Optional:  false
  sa-key:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  admin-gcp-sa
    Optional:    false
  envoy-token-tgcpl:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  envoy-token-tgcpl
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                    From                                                          Message
  ----     ------     ----                   ----                                                          -------
  Normal   Scheduled  43m                    default-scheduler                                             Successfully assigned istio-system/iap-&lt;..&gt; to gke-text-class-kf-19-text-class-kf-19-9707658c-xbmp
  Normal   Started    40m (x4 over 41m)      kubelet, gke-text-class-kf-19-&lt;..&gt;                            Started container
  Normal   Pulled     39m (x5 over 41m)      kubelet, gke-text-class-kf-19-&lt;..&gt;                            Successfully pulled image "gcr.io/kubeflow-images-public/ingress-setup:latest"
  Normal   Created    39m (x5 over 41m)      kubelet, gke-text-class-kf-19-&lt;..&gt;                            Created container
  Normal   Pulling    8m21s (x12 over 43m)   kubelet, gke-text-class-kf-19-&lt;..&gt;                            pulling image "gcr.io/kubeflow-images-public/ingress-setup:latest"
  Warning  BackOff    3m14s (x166 over 40m)  kubelet, gke-text-class-kf-19-&lt;..&gt;                            Back-off restarting failed container
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='kkasravi' date='2019-10-16T14:16:06Z'>
		Is anyone aware of a workaround / recipe that does work? I'm hitting the same. Thanks
		</comment>
		<comment id='6' author='kkasravi' date='2019-10-16T19:36:19Z'>
		Same issue here when running both the kubeflow CLI or UI examples on GCP. I haven't been able to fix it either.
		</comment>
		<comment id='7' author='kkasravi' date='2019-10-17T06:43:39Z'>
		I just ran across this issue. I deployed Kubeflow just yesterday and had no issues.
&lt;denchmark-code&gt;NAME                                          READY   STATUS             RESTARTS   AGE
...
pod/iap-enabler-778889874f-9gtrx              0/1     CrashLoopBackOff   6          9m16s
...
pod/istio-grafana-post-install-1.1.6-sgfz8    0/1     Completed          0          30m
...
pod/istio-grafana-post-install-1.1.6-sgfz8    0/1     Completed          0          30m
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;NAME                                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/iap-enabler              1         1         1            0           19m
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/iap-enabler-778889874f              1         1         0       19m
&lt;/denchmark-code&gt;

The endpoint at (project).endpoints.cloud.goog is up, but it simply shows
&lt;denchmark-code&gt;default backend - 404
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='kkasravi' date='2019-10-17T08:17:59Z'>
		Same issue here, please advice.
		</comment>
		<comment id='9' author='kkasravi' date='2019-10-22T03:30:45Z'>
		Duplicate of &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4043&gt;#4043&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/AGKhalil&gt;@AGKhalil&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/wronk&gt;@wronk&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/VishDev12&gt;@VishDev12&lt;/denchmark-link&gt;
 in particular please see the &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4043#issuecomment-539667009&gt;comment&lt;/denchmark-link&gt;

The crash looping behavior is expected and not necessarily indicative of a problem. The comment has instructions for verifying that the script ran correctly.
If your endpoint is working could you please follow the guide
&lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/monitor-iap-setup/&gt;https://www.kubeflow.org/docs/gke/deploy/monitor-iap-setup/&lt;/denchmark-link&gt;

and let us know where things are breaking for you?
		</comment>
		<comment id='10' author='kkasravi' date='2019-10-23T09:12:46Z'>
		Had the same issue.
&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

JWT istio policies and timeout where already set accordingly, still the issue remained. So I thought of just 'restarting' the pod by downscaling to 0 and scale to 1. That solved it for me.
		</comment>
		<comment id='11' author='kkasravi' date='2019-11-04T20:58:59Z'>
		This is fixed in 0.7.0.
		</comment>
	</comments>
</bug>