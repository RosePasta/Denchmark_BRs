<bug id='4783' author='karlschriek' open_date='2020-02-19T16:02:42Z' closed_time='2020-05-29T14:36:36Z'>
	<summary>Kubeflow Pods don't start after Nodes scaled to zero and back up</summary>
	<description>
We have a dev environment where we typically scale the nodegroups in our EKS cluster down to zero at the end of the day. I've never had a problem getting the cluster up and running again the next morning.
However, if I scale the nodegroups down to zero and then scale them back up again soon thereafter (the longest I've waited to test this out was about 30 Minutes) then most or all of the deployments related to Kubeflow just stay on 0/1. Sometimes a handful of pods start up (but this tends to not always be the exact same ones).
The only way I have been able to fix this was to delete kubeflow and roll it out again.
Environment:

Kubeflow version: 0.7):
Kubernetes platform: EKS
Kubernetes version: 1.14

	</description>
	<comments>
		<comment id='1' author='karlschriek' date='2020-02-19T16:02:56Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.90



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='karlschriek' date='2020-02-24T01:24:01Z'>
		&lt;denchmark-link:https://github.com/karlschriek&gt;@karlschriek&lt;/denchmark-link&gt;
 I'm on the same boat. After v0.6 installation was not working anymore, I was forced to install v0.7 (v1.0-rc still does not exist at this point). Now I have done a couple of things with the deployment. But none worked.
Environment:

I have 2 notebooks occcupying 80% of a node's resource
I have 2 AZs
I have 3 Nodes

Activity:
Zero-Scale and Scale-Up Cluster at:

30 minutes
1 hr



Edited kubeflow deployments progressDeadline to a year, edited maxSurge to 1, maxUnavailable to 0. This did not work at all. After a couple of restarts the 0/1 deployment happens again.


Scaled all kubeflow replicas deployments to 2 (this will incur error to pods like mysql as only 1 can exist at the same time, but will still have 1 running pod). This one lasted me 4 days.


I am currently experimenting on scaling up my number of nodes to 4 as this might be resource issue.
I'm hoping someone could explain how does the kubeflow deployments are handled from a zero-scaled environment. (what blocks it from spawning the pods? or does it depend on something like the scheduled workflow perhaps?)
		</comment>
		<comment id='3' author='karlschriek' date='2020-03-02T05:36:21Z'>
		Related to &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4808&gt;#4808&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='karlschriek' date='2020-04-17T21:53:03Z'>
		/area gcp
/priority p1
		</comment>
		<comment id='5' author='karlschriek' date='2020-05-28T02:08:31Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 Hey Jeremy - Any advice on this issue?
		</comment>
		<comment id='6' author='karlschriek' date='2020-05-29T14:36:33Z'>
		 &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4808&gt;#4808&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>