<bug id='4693' author='tjmonsi' open_date='2020-01-27T01:30:10Z' closed_time='2020-01-29T06:30:56Z'>
	<summary>Cannot Deploy Kubeflow using CLI and UI - Problem with metadata servers - GKE bug</summary>
	<description>
/kind bug
What steps did you take and what happened:
[A clear and concise description of what the bug is.]
Deploy using the Kubeflow CLI, following the instructions.
What happens are these:

All services are working except the ingress: envoy-ingress

&lt;denchmark-code&gt;$ kubectl describe -n istio-system ingress

spec":{"rules":[{"host":"[KF_NAME].endpoints.[PROJECT].cloud.goog","http":{"paths":[{"backend":{"serviceName":"istio-ingressgateway","servicePort":80},"path":"/*"}]}}]}}

  kubernetes.io/tls-acme:                       true
  certmanager.k8s.io/issuer:                    letsencrypt-prod
  ingress.kubernetes.io/ssl-redirect:           true
  ingress.kubernetes.io/url-map:                k8s-um-istio-system-envoy-ingress--22b7dfea8184e195
  ingress.gcp.kubernetes.io/pre-shared-cert:    mcrt-6449d713-f72b-4e12-93eb-0bac67e8a83d
  ingress.kubernetes.io/target-proxy:           k8s-tp-istio-system-envoy-ingress--22b7dfea8184e195
  kubernetes.io/ingress.global-static-ip-name:  [KF_NAME]-ip
Events:
  Type    Reason  Age   From                     Message
  ----    ------  ----  ----                     -------
  Normal  ADD     25m   loadbalancer-controller  istio-system/envoy-ingress
  Normal  CREATE  24m   loadbalancer-controller  ip: 34.95.117.30
&lt;/denchmark-code&gt;


Crash loop back off on pods in namespace istio-system

&lt;denchmark-code&gt;NAME                                      READY   STATUS             RESTARTS   AGE
backend-updater-0                         0/1     CrashLoopBackOff   10         28m
iap-enabler-7f87598576-ch2rk              0/1     CrashLoopBackOff   10         28m
istio-policy-7f8bb87857-4x9j9             1/2     CrashLoopBackOff   15         31m
istio-telemetry-8759dc6b7-9vpd6           1/2     CrashLoopBackOff   15         31m
&lt;/denchmark-code&gt;


Logs of the following above

&lt;denchmark-code&gt;$ kubectl logs -n istio-system backend-updater-0 
# also for iap-enabler-7f87598576-ch2rk

+ '[' -z istio-system ']'
+ '[' -z istio-ingressgateway ']'
+ '[' -z envoy-ingress ']'
++ curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/project-id
+ PROJECT=
+ '[' -z ']'
+ echo Error unable to fetch PROJECT from compute metadata
Error unable to fetch PROJECT from compute metadata
+ exit 1
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n istio-system istio-policy-7f8bb87857-4x9j9  -c mixer

2020-01-27T01:42:08.202842Z     info    mcp       [31] istio/config/v1alpha2/legacy/prometheuses
2020-01-27T01:42:08.202854Z     info    mcp       [32] istio/config/v1alpha2/legacy/solarwindses
2020-01-27T01:42:08.202859Z     info    mcp       [33] istio/policy/v1beta1/attributemanifests
2020-01-27T01:42:08.202863Z     info    mcp       [34] istio/config/v1alpha2/legacy/listentries
2020-01-27T01:42:08.202868Z     info    mcp       [35] istio/config/v1alpha2/legacy/memquotas
2020-01-27T01:42:08.202872Z     info    mcp       [36] istio/config/v1alpha2/legacy/circonuses
2020-01-27T01:42:08.202891Z     info    parsed scheme: ""
2020-01-27T01:42:08.202902Z     info    scheme "" not registered, fallback to default scheme
2020-01-27T01:42:08.203023Z     info    Using new MCP client sink stack
2020-01-27T01:42:08.203071Z     info    Awaiting for config store sync...
2020-01-27T01:42:08.203212Z     info    ccResolverWrapper: sending new addresses to cc: [{istio-galley.istio-system.svc:9901 0  &lt;nil&gt;}]
2020-01-27T01:42:08.203226Z     info    ClientConn switching balancer to "pick_first"
2020-01-27T01:42:08.203291Z     info    pickfirstBalancer: HandleSubConnStateChange: 0xc420025160, CONNECTING
2020-01-27T01:42:08.203806Z     info    mcp     (re)trying to establish new MCP sink stream
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n istio-system istio-policy-7f8bb87857-4x9j9  -c proxy

2020-01-27T01:42:08.202842Z     info    mcp       [31] istio/config/v1alpha2/legacy/prometheuses
2020-01-27T01:42:08.202854Z     info    mcp       [32] istio/config/v1alpha2/legacy/solarwindses
2020-01-27T01:42:08.202859Z     info    mcp       [33] istio/policy/v1beta1/attributemanifests
2020-01-27T01:42:08.202863Z     info    mcp       [34] istio/config/v1alpha2/legacy/listentries
2020-01-27T01:42:08.202868Z     info    mcp       [35] istio/config/v1alpha2/legacy/memquotas
2020-01-27T01:42:08.202872Z     info    mcp       [36] istio/config/v1alpha2/legacy/circonuses
2020-01-27T01:42:08.202891Z     info    parsed scheme: ""
2020-01-27T01:42:08.202902Z     info    scheme "" not registered, fallback to default scheme
2020-01-27T01:42:08.203023Z     info    Using new MCP client sink stack
2020-01-27T01:42:08.203071Z     info    Awaiting for config store sync...
2020-01-27T01:42:08.203212Z     info    ccResolverWrapper: sending new addresses to cc: [{istio-galley.istio-system.svc:9901 0  &lt;nil&gt;}]
2020-01-27T01:42:08.203226Z     info    ClientConn switching balancer to "pick_first"
2020-01-27T01:42:08.203291Z     info    pickfirstBalancer: HandleSubConnStateChange: 0xc420025160, CONNECTING
2020-01-27T01:42:08.203806Z     info    mcp     (re)trying to establish new MCP sink stream
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;kubectl logs -n istio-system istio-telemetry-8759dc6b7-9vpd6 -c mixer

2020-01-27T01:47:45.001319Z     info    scheme "" not registered, fallback to default scheme
2020-01-27T01:47:45.001448Z     info    Using new MCP client sink stack
2020-01-27T01:47:45.001495Z     info    Awaiting for config store sync...
2020-01-27T01:47:45.011774Z     info    ccResolverWrapper: sending new addresses to cc: [{istio-galley.istio-system.svc:9901 0  &lt;nil&gt;}]
2020-01-27T01:47:45.011820Z     info    ClientConn switching balancer to "pick_first"
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;kubectl logs -n istio-system istio-telemetry-8759dc6b7-9vpd6 -c istio-proxy

2020-01-27T00:43:47.645657Z     info    PilotSAN []string(nil)
2020-01-27T00:43:47.645849Z     info    Starting proxy agent
2020-01-27T00:43:47.645905Z     info    Received new config, resetting budget
2020-01-27T00:43:47.645917Z     info    Reconciling retry (budget 10)
2020-01-27T00:43:47.645929Z     info    Epoch 0 starting
2020-01-27T00:43:47.645975Z     info    Envoy command: [-c /etc/istio/proxy/envoy.yaml --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster istio-telemetry --service-node sidecar~10.8.1.11~istio-telemetry-8759dc6b7-
9vpd6.istio-system~istio-system.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warning]
2020-01-27T00:43:47.646126Z     warn    watching /etc/certs/cert-chain.pem encounters an error no such file or directory
[2020-01-27 00:43:47.670][16][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Cluster.hosts' from file cds.proto. This configuration will be removed from Envoy soon. Please see https:/
/www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-27 00:43:47.670][16][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Cluster.hosts' from file cds.proto. This configuration will be removed from Envoy soon. Please see https:/
/www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
&lt;/denchmark-code&gt;


Crash loop back off on pods in namespace kubeflow

&lt;denchmark-code&gt;NAME                                                           READY   STATUS             RESTARTS   AGE
cloud-endpoints-controller-5456754fb4-crn4j                    0/1     CrashLoopBackOff   11         35m
metadata-grpc-deployment-79b46d65f5-dmjhz                      0/1     CrashLoopBackOff   10         37m
ml-pipeline-persistenceagent-66f89b56d9-8zrtd                  0/1     CrashLoopBackOff   8          36m
&lt;/denchmark-code&gt;


Logs  for the following are

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow cloud-endpoints-controller-5456754fb4-crn4j

2020/01/27 01:19:32 [INFO] Fetching Project ID from Compute metadata API...
2020/01/27 01:19:32 Error loading config: Get http://169.254.169.254/computeMetadata/v1/project/project-id: dial tcp 169.254.169.254:80: connect: connection refused
&lt;/denchmark-code&gt;

Above is the same I think with the problem encountered at issue &lt;denchmark-link:https://github.com/kubeflow/manifests/issues/736&gt;kubeflow/manifests#736&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow metadata-grpc-deployment-79b46d65f5-dmjhz

2020-01-27 01:23:09.265433: F ml_metadata/metadata_store/metadata_store_server_main.cc:178] Non-OK-status: ml_metadata::CreateMetadataStore( connection_config, server_config.migration_options(), &amp;metadata_store) status: Internal: mysql_real_connect failed: errno: 2005, error: Unknown MySQL server host 'metadata-db.kubeflow' (-3)MetadataStore cannot be created with the given connection config.
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow ml-pipeline-persistenceagent-66f89b56d9-8zrtd

W0127 01:26:07.238733       1 client_config.go:552] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
&lt;/denchmark-code&gt;

What did you expect to happen:
A deployed clean slate of Kubeflow
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
I have tried 0.7 and 0.7.1 as well as the UI version. Nothing seems to work
Additional logs
&lt;denchmark-code&gt;kubectl logs -n istio-system istio-pilot-6dd5b8f74c-6vcmx -c discovery --tail=20
2020-01-27T01:53:17.412368Z     info    Configuration not synced: first push for [istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/config/v1alpha2/httpapispecbindings istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules] not received
2020-01-27T01:53:17.512369Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings] not received
2020-01-27T01:53:17.612382Z     info    Configuration not synced: first push for [istio/config/v1alpha2/httpapispecbindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/rbac/v1alpha1/rbacconfigs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles] not received
2020-01-27T01:53:17.712392Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs] not received
2020-01-27T01:53:17.812545Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs] not received
2020-01-27T01:53:17.912379Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs] not received
2020-01-27T01:53:18.012373Z     info    Configuration not synced: first push for [istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/rbac/v1alpha1/rbacconfigs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/config/v1alpha2/httpapispecbindings] not received
2020-01-27T01:53:18.112364Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs] not received
2020-01-27T01:53:18.212502Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/virtualservices istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/networking/v1alpha3/envoyfilters istio/config/v1alpha2/httpapispecbindings] not received
2020-01-27T01:53:18.217375Z     info    mcp     (re)trying to establish new MCP sink stream
2020-01-27T01:53:18.217460Z     error   mcp     Failed to create a new MCP sink stream: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = "transport: Error while dialing dial tcp: lookup istio-galley.istio-system.svc on 10.11.240.10:53: read udp 10.8.0.9:38525-&gt;10.11.240.10:53: i/o timeout"
2020-01-27T01:53:18.314354Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings] not received
2020-01-27T01:53:18.412488Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs] not received
2020-01-27T01:53:18.521431Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs] not received
2020-01-27T01:53:18.612389Z     info    Configuration not synced: first push for [istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/rbac/v1alpha1/rbacconfigs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/config/v1alpha2/httpapispecbindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings] not received
2020-01-27T01:53:18.712454Z     info    Configuration not synced: first push for [istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/config/v1alpha2/httpapispecbindings] not received
2020-01-27T01:53:18.814354Z     info    Configuration not synced: first push for [istio/config/v1alpha2/httpapispecbindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/rbac/v1alpha1/rbacconfigs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles] not received
2020-01-27T01:53:18.912472Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings] not received
2020-01-27T01:53:19.012556Z     info    Configuration not synced: first push for [istio/config/v1alpha2/httpapispecbindings istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs] not received
2020-01-27T01:53:19.112436Z     info    Configuration not synced: first push for [istio/networking/v1alpha3/serviceentries istio/networking/v1alpha3/sidecars istio/authentication/v1alpha1/meshpolicies istio/rbac/v1alpha1/servicerolebindings istio/rbac/v1alpha1/clusterrbacconfigs istio/networking/v1alpha3/virtualservices istio/networking/v1alpha3/gateways istio/networking/v1alpha3/destinationrules istio/config/v1alpha2/httpapispecs istio/mixer/v1/config/client/quotaspecs istio/networking/v1alpha3/envoyfilters istio/mixer/v1/config/client/quotaspecbindings istio/authentication/v1alpha1/policies istio/rbac/v1alpha1/serviceroles istio/rbac/v1alpha1/rbacconfigs istio/config/v1alpha2/httpapispecbindings] not received
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n istio-system istio-ingressgateway-565b894b5f-wbxwz --tail=20

2020-01-27T01:55:51.687911Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
[2020-01-27 01:55:52.852][45][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:86] gRPC config stream closed: 14, no healthy upstream
[2020-01-27 01:55:52.852][45][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:49] Unable to establish new stream
2020-01-27T01:55:53.687959Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:55:55.687995Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:55:57.688135Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:55:59.688234Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:01.687827Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:03.688066Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:05.688036Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:07.687813Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:09.687587Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
[2020-01-27 01:56:09.939][45][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:86] gRPC config stream closed: 14, no healthy upstream
[2020-01-27 01:56:09.939][45][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:49] Unable to establish new stream
2020-01-27T01:56:11.687838Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:13.687899Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:15.687770Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:17.687692Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:19.687841Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-27T01:56:21.687869Z     info    Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 0 successful, 0 rejected; lds updates: 0 successful, 0 rejected
&lt;/denchmark-code&gt;

Environment:

Kubeflow version: 0.7.1
kfctl version:v1.0-rc.1-0-g963c787
Kubernetes platform: GCP
Kubernetes version:

&lt;denchmark-code&gt;Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.10", GitCommit:"575467a0eaf3ca1f20eb86215b3bde40a5ae617a", GitTreeState:"clean", BuildDate:"2019-12-11T12:41:00Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64
"}
Server Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.10-gke.17", GitCommit:"bdceba0734835c6cb1acbd1c447caf17d8613b44", GitTreeState:"clean", BuildDate:"2020-01-17T23:10:13Z", GoVersion:"go1.12.12b4", Compiler:"gc", Platform:"l
inux/amd64"}
&lt;/denchmark-code&gt;


OS:

&lt;denchmark-code&gt;PRETTY_NAME="Debian GNU/Linux 9 (stretch)"
NAME="Debian GNU/Linux"
VERSION_ID="9"
VERSION="9 (stretch)"
VERSION_CODENAME=stretch
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tjmonsi' date='2020-01-27T01:30:29Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.96



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='tjmonsi' date='2020-01-27T23:33:25Z'>
		Hi, the current default version has metadata connection issue.
One workaround is to deploy an older GKE version:


fork manifests repo, checkout v0.7 branch, change GKE version to "1.14.8": https://github.com/kubeflow/manifests/blob/v0.7-branch/gcp/deployment_manager_configs/cluster-kubeflow.yaml#L34


change uri in kfdef to point to the forked/modified manifests: https://github.com/kubeflow/manifests/blob/v0.7-branch/kfdef/kfctl_gcp_iap.0.7.1.yaml#L399


Deploy using new kfdef.


		</comment>
		<comment id='3' author='tjmonsi' date='2020-01-27T23:41:19Z'>
		Though I haven't tried it yet, I believe another workaround would be to follow the instructions here: &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#alternatively-set-up-your-configuration-for-later-deployment&gt;https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#alternatively-set-up-your-configuration-for-later-deployment&lt;/denchmark-link&gt;
 (essentially, 'kfctl build', then edit the config to modify the GKE version, then 'kfctl apply' using the local config file).
		</comment>
		<comment id='4' author='tjmonsi' date='2020-01-28T00:23:08Z'>
		v0.7.1 should now use GKE 1.14.8 by default:
&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/v0.7-branch/kfdef/kfctl_gcp_iap.0.7.1.yaml&gt;https://github.com/kubeflow/manifests/blob/v0.7-branch/kfdef/kfctl_gcp_iap.0.7.1.yaml&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='tjmonsi' date='2020-01-28T04:45:08Z'>
		Thanks &lt;denchmark-link:https://github.com/kunmingg&gt;@kunmingg&lt;/denchmark-link&gt;
 filed &lt;denchmark-link:https://github.com/kubeflow/manifests/issues/757&gt;kubeflow/manifests#757&lt;/denchmark-link&gt;
 to unpin.
		</comment>
		<comment id='6' author='tjmonsi' date='2020-01-28T21:59:49Z'>
		I think I am seeing a related issue on 1.4.8-gke.17 where backend-updater-0 is unable to set the healthcheck path for the Google LB:
&lt;denchmark-code&gt;++ gcloud --project=my-project compute backend-services list --filter=name~k8s-be-31380--some-number --uri
ERROR: (gcloud.compute.backend-services.list) Some requests did not succeed:
 - Invalid Credentials
&lt;/denchmark-code&gt;

the gke-metadata-server DaemonSet pod on the same node has this log message logged repeatedly (for each request it receives?):
&lt;denchmark-code&gt;I0128 21:58:26.237728       1 serviceaccounts.go:188] KSA istio-system:kf-admin doesn't have MPI annotation, using Identity Namespace "my-project.svc.id.goog"
I0128 21:58:26.237844       1 metadata.go:476] /computeMetadata/v1/instance/service-accounts/my-project.svc.id.goog/token HTTP/200 size=440
I0128 21:58:28.130883       1 serviceaccounts.go:188] KSA istio-system:kf-admin doesn't have MPI annotation, using Identity Namespace "my-project.svc.id.goog"
I0128 21:58:28.130991       1 metadata.go:476] /computeMetadata/v1/instance/service-accounts/my-project.svc.id.goog/token HTTP/200 size=440
&lt;/denchmark-code&gt;

Should &lt;denchmark-link:https://github.com/kubeflow/manifests/blob/master/gcp/iap-ingress/base/service-account.yaml&gt;https://github.com/kubeflow/manifests/blob/master/gcp/iap-ingress/base/service-account.yaml&lt;/denchmark-link&gt;
 have an annotation for which workload identity GSA to use?
		</comment>
		<comment id='7' author='tjmonsi' date='2020-01-29T03:03:40Z'>
		&lt;denchmark-link:https://github.com/mattnworb&gt;@mattnworb&lt;/denchmark-link&gt;
  Could you open a new issue please and please provide the version of KF you are using.
		</comment>
		<comment id='8' author='tjmonsi' date='2020-01-29T06:30:55Z'>
		Closing this issue because it should be fixed by the updates which pin to GKE 1.14.8
		</comment>
		<comment id='9' author='tjmonsi' date='2020-01-29T16:47:20Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 makes sense. Our issue turned out to be that the KSA did not have the  annotation &lt;denchmark-link:https://github.com/kubeflow/kfctl/blob/1e9c8cb963f0701eafa3f54d07d56d919a34ad35/pkg/kfapp/gcp/gcp.go#L1713-L1754&gt;which is added by kfctl&lt;/denchmark-link&gt;
, since we tried to upgrade Kubeflow in a cluster which was already existing and therefore didn't use kfctl for the process.
		</comment>
	</comments>
</bug>