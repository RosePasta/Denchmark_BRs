<bug id='5102' author='DavidSpek' open_date='2020-07-02T20:04:24Z' closed_time='2020-09-30T14:58:24Z'>
	<summary>Tensorflow notebooks kernel stuck in connected state, never reaches idle or runs any command</summary>
	<description>
/kind bug

When launching the default provided notebooks (tensorflow-1.15.2 cpu and gpu, and tensorflow-2.1.0 cpu and gpu) the python 3 kernel never changes to idle status. After the kernel has started, the status changes from  to  after about a minute and stays in this state.
&lt;denchmark-link:https://user-images.githubusercontent.com/28541758/86403276-a9a8d680-bcad-11ea-936c-fe093fad03d6.png&gt;&lt;/denchmark-link&gt;

Executing any command results in no output, even something as simple as 1+1 or print("Hello World"). While this is happening the kernel state is still Kernel Connected.
What did you expect to happen:
When launching the image tensorflow-1.13.1-notebook-cpu:v0.5.0 the kernel initializes much quicker, after which the kernel state goes to Kernel Idle. It is then able to successfully complete 1+1, print("Hello World"), or even the following test code.
&lt;denchmark-code&gt;from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

import tensorflow as tf

x = tf.placeholder(tf.float32, [None, 784])

W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

for _ in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print("Accuracy: ", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/28541758/86404013-18d2fa80-bcaf-11ea-878d-8b047a13f78a.png&gt;&lt;/denchmark-link&gt;

I'm not sure what could be causing this issue, and I assume it isn't very common as it breaks the notebook functionality out-of-the-box completely. Also, given the large amount of recently found security issues it might be advisable to update the images to tensorflow 2.1.1 or 2.3.0rc0 and 1.15.3.
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:

Kubeflow version: 1.0.2
kfctl version: 1.0.2-0-ga476281
Kubernetes platform: RKE
Kubernetes version: 1.7.6
OS: Ubuntu 20.04

	</description>
	<comments>
		<comment id='1' author='DavidSpek' date='2020-07-02T20:04:30Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='DavidSpek' date='2020-07-02T21:01:11Z'>
		I built a custom image using tensorflow/tensorflow:2.2.0rc2-py3-jupyter with the attached dockerfile and the kernel state went to Kernerl Idle as expected and was able to perform 1+1 without issues. I would still be interested to know what could be causing this issue as otherwise it might pop up randomly in the future.
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4866635/Dockerfile.txt&gt;Dockerfile.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='DavidSpek' date='2020-07-06T09:07:14Z'>
		&lt;denchmark-link:https://github.com/DavidSpek&gt;@DavidSpek&lt;/denchmark-link&gt;
 glad you were able to unblock yourself.
It would be helpful to have the logs of the jupyter pod(s) to see if there are any errors.
		</comment>
		<comment id='4' author='DavidSpek' date='2020-07-09T10:22:36Z'>
		I have since moved on to a custom notebook image based on the jupyter datascience-notebook. However, strangely enough when I just tried tensortflow-2.1.0-notebook-cpu:1.0.0 again to get a log for you the kernel went to idle after initializing as it should and it was able to run a simple 1+1 command fine. I will see if I can replicate the issue again once I get my datascience notebook working as it needs to.
		</comment>
		<comment id='5' author='DavidSpek' date='2020-09-30T14:58:24Z'>
		I haven't experienced this issue again so I am closing it for now. If it comes up again in the future I will reopen it.
		</comment>
	</comments>
</bug>