<bug id='4962' author='alanhughes' open_date='2020-04-16T12:03:03Z' closed_time='2020-09-03T01:51:49Z'>
	<summary>Notebook with EBS volume - creation failure</summary>
	<description>
/kind bug
What steps did you take and what happened:
We run an EKS cluster with workers in multiple availability zones. If you try and create a notebook server with extra data volumes, in mode "ReadWriteOnce", there is no guarantee that both EBS volumes will be created in the same AWS availability zone. If they are created in different zones, the notebook fails to start, since no worker node can attach to two volumes in different AZs.
What did you expect to happen:
When multiple volumes are specified, they should get created in the same AWS region
Anything else you would like to add:
N/A
Environment:

Kubeflow version: Using 1.0.1 manifests, bottom left of UI says dev_local
kfctl version: (use kfctl version): kfctl v1.0-0-g94c35cf
Kubernetes platform: (e.g. minikube) EKS
Kubernetes version: (use kubectl version): Client Version: v1.17.4
Server Version: v1.15.10-eks-bac369
OS (e.g. from /etc/os-release): MacOS Catalina 10.15.4

	</description>
	<comments>
		<comment id='1' author='alanhughes' date='2020-04-16T12:03:14Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.99



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='alanhughes' date='2020-04-16T12:57:18Z'>
		workaround at the moment is to pre-create a storage class per zone with:
&lt;denchmark-code&gt;allowedTopologies:
- matchLabelExpressions:
  - key: failure-domain.beta.kubernetes.io/zone
    values:
    - &lt;availability-zone&gt;
&lt;/denchmark-code&gt;

as per &lt;denchmark-link:https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies&gt;https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies&lt;/denchmark-link&gt;
 (with the imporant  annotation set!)
and then pre-create all PVCs that we will require, distributing them as required between StorageClasses, but requiring that any volumes that will be attached to the same notebook are in the same StorageClass
		</comment>
		<comment id='3' author='alanhughes' date='2020-04-16T18:59:14Z'>
		This is a common issue. There's no guarantee PV are always created in the right AZ.
There's some community solution on this.
&lt;denchmark-link:https://kubernetes.io/blog/2018/10/11/topology-aware-volume-provisioning-in-kubernetes/&gt;https://kubernetes.io/blog/2018/10/11/topology-aware-volume-provisioning-in-kubernetes/&lt;/denchmark-link&gt;

I think more elegant solution is create a topolocy-aware storage class and set it as default storage class for Jupyter controller.
This would be a lightweight wrapper of default storage class
		</comment>
		<comment id='4' author='alanhughes' date='2020-04-16T18:59:37Z'>
		/assign &lt;denchmark-link:https://github.com/Jeffwan&gt;@Jeffwan&lt;/denchmark-link&gt;

Can you help verify if this work for you case? I am not sure if we want to handle this for Kubeflow users
		</comment>
		<comment id='5' author='alanhughes' date='2020-04-17T20:37:01Z'>
		/platform
/area jupyter
/priority p1
		</comment>
		<comment id='6' author='alanhughes' date='2020-05-28T01:29:39Z'>
		&lt;denchmark-link:https://github.com/Jeffwan&gt;@Jeffwan&lt;/denchmark-link&gt;
 Hi Jeff, we are getting close to the Kubeflow 1.1 cut date.   This seems like an important fix but I think there is work on AWS that is needed.  I have moved this to a P2.
		</comment>
		<comment id='7' author='alanhughes' date='2020-08-27T00:34:51Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>