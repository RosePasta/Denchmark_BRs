<bug id='4819' author='denisl78' open_date='2020-03-03T14:30:32Z' closed_time='2020-08-19T06:43:25Z'>
	<summary>CentralDashboard failed to start on AWS custom k8s</summary>
	<description>
Hi
I'm running k8s on AWS , not EKS
Run deployment with kfctl_k8s_istio.v1.0.0.yaml
After deployment finished, I see that centraldashboard always restarted with exit 0
&lt;denchmark-code&gt;$ kubectl -n kubeflow get pods
NAME                                                           READY   STATUS      RESTARTS   AGE
admission-webhook-bootstrap-stateful-set-0                     1/1     Running     0          2d2h
admission-webhook-deployment-569558c8b6-zpt4h                  1/1     Running     0          2d2h
application-controller-stateful-set-0                          1/1     Running     0          2d2h
argo-ui-7ffb9b6577-tgdd7                                       1/1     Running     0          2d2h
centraldashboard-66656fb95c-597kl                              0/1     Completed   4          112s
jupyter-web-app-deployment-679d5f5dc4-tchx8                    1/1     Running     0          2d2h
katib-controller-7f58569f7d-nbdtm                              1/1     Running     1          2d2h
katib-db-manager-54b66f9f9d-nchm5                              1/1     Running     0          2d2h
katib-mysql-dcf7dcbd5-kcg9n                                    1/1     Running     0          2d2h
katib-ui-6f97756598-wmh7w                                      1/1     Running     0          2d2h
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl -n kubeflow describe pod centraldashboard-66656fb95c-597kl 
Name:           centraldashboard-66656fb95c-597kl
Namespace:      kubeflow
Priority:       0
Node:           ip-10-110-64-38.ec2.internal/10.110.64.38
Start Time:     Tue, 03 Mar 2020 16:08:42 +0200
Labels:         app=centraldashboard
                app.kubernetes.io/component=centraldashboard
                app.kubernetes.io/instance=centraldashboard-v1.0.0
                app.kubernetes.io/managed-by=kfctl
                app.kubernetes.io/name=centraldashboard
                app.kubernetes.io/part-of=kubeflow
                app.kubernetes.io/version=v1.0.0
                kustomize.component=centraldashboard
                pod-template-hash=66656fb95c
Annotations:    &lt;none&gt;
Status:         Running
IP:             10.110.66.237
Controlled By:  ReplicaSet/centraldashboard-66656fb95c
Containers:
  centraldashboard:
    Container ID:   docker://f5b700520bf8f725a90f145f250f4af4465748532beef9053cdafe4b4d06d3fc
    Image:          gcr.io/kubeflow-images-public/centraldashboard
    Image ID:       docker-pullable://gcr.io/kubeflow-images-public/centraldashboard@sha256:6494ad72a0cff84d5d505c7c69fbb0a2bd0e9431596d8d072e28f6bd63b52816
    Port:           8082/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 03 Mar 2020 16:10:26 +0200
      Finished:     Tue, 03 Mar 2020 16:10:27 +0200
    Ready:          False
    Restart Count:  4
    Environment:
      USERID_HEADER:               
      USERID_PREFIX:               
      PROFILES_KFAM_SERVICE_HOST:  profiles-kfam.kubeflow
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from centraldashboard-token-bbp5f (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  centraldashboard-token-bbp5f:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  centraldashboard-token-bbp5f
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                  From                                   Message
  ----     ------     ----                 ----                                   -------
  Normal   Scheduled  2m46s                default-scheduler                      Successfully assigned kubeflow/centraldashboard-66656fb95c-597kl to ip-10-110-64-38.ec2.internal
  Normal   Pulling    2m45s                kubelet, ip-10-110-64-38.ec2.internal  Pulling image "gcr.io/kubeflow-images-public/centraldashboard"
  Normal   Pulled     2m42s                kubelet, ip-10-110-64-38.ec2.internal  Successfully pulled image "gcr.io/kubeflow-images-public/centraldashboard"
  Normal   Created    62s (x5 over 2m42s)  kubelet, ip-10-110-64-38.ec2.internal  Created container centraldashboard
  Normal   Started    62s (x5 over 2m42s)  kubelet, ip-10-110-64-38.ec2.internal  Started container centraldashboard
  Normal   Pulled     62s (x4 over 2m40s)  kubelet, ip-10-110-64-38.ec2.internal  Container image "gcr.io/kubeflow-images-public/centraldashboard" already present on machine
  Warning  BackOff    49s (x9 over 2m38s)  kubelet, ip-10-110-64-38.ec2.internal  Back-off restarting failed container
&lt;/denchmark-code&gt;

Tryed to investigate where is the problem, I'm not familiar with TS , but found that its exit on
app/server.ts in
import {getMetricsService} from './metrics_service_factory';
with
Segmentation fault (core dumped)
Environment:

Kubeflow version: v1.0.0
kfctl version: (use kfctl version): kfctl v1.0-0-g94c35cf
Kubernetes platform: vanilla k8s
Kubernetes version: (use kubectl version): v1.15.7
OS (e.g. from /etc/os-release): Debian GNU/Linux 9 (stretch)"

	</description>
	<comments>
		<comment id='1' author='denisl78' date='2020-03-03T14:30:58Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.75



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='denisl78' date='2020-03-03T14:32:30Z'>
		To solve this I've bumped node version on Dockerfile from 10 to 12
&lt;denchmark-code&gt; # Step 1: Builds and tests
-FROM node:10-alpine AS build
+FROM node:12-alpine AS build
 
 ARG kubeflowversion
 ARG commit
@@ -39,7 +39,7 @@ RUN npm rebuild &amp;&amp; \
     npm prune --production
 
 # Step 2: Packages assets for serving
-FROM node:10-alpine AS serve
+FROM node:12-alpine AS serve

&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='denisl78' date='2020-03-15T20:51:47Z'>
		Was having the same issue, so thanks for sharing your solution &lt;denchmark-link:https://github.com/denisl78&gt;@denisl78&lt;/denchmark-link&gt;
!  How did you figure it out?
		</comment>
		<comment id='4' author='denisl78' date='2020-03-16T09:23:27Z'>
		Hi
What I did  is insert prints steps to app/server.ts  and see where it fails and ran centraldashboard with debug mode
If someone want to steps for bypass this issue:


Clone this project
git clone https://github.com/kubeflow/kubeflow.git
cd kubeflow


Change base image for centraldashboard from node-10 to node-12
sed -i "s/10-alpine/12-alpine/g" components/centraldashboard/Dockerfile


Build centraldashboard with new changes
docker build -t centraldashboard:node-12 components/centraldashboard/


Tag docker for push to ECR
docker tag centraldashboard:node-12 YOUR_ECR/centraldashboard:node-12


Login to ECR and push
docker login -u AWS YOUR_ECR
docker push YOUR_ECR/centraldashboard:node-12


Chage centraldashboard docker image on your k8s at AWS
kubectl -n kubeflow set image deployment/centraldashboard centraldashboard=YOUR_ECR/centraldashboard:node-12 



Change YOUR_ECR with AWS ECR that k8s working with

		</comment>
		<comment id='5' author='denisl78' date='2020-03-16T09:51:39Z'>
		&lt;denchmark-link:https://github.com/denisl78&gt;@denisl78&lt;/denchmark-link&gt;
  thanks u for your solution, for me, it's worked.
		</comment>
		<comment id='6' author='denisl78' date='2020-03-30T08:23:00Z'>
		In case you need, I already built the docker image with the fix: &lt;denchmark-link:https://hub.docker.com/repository/docker/gsantomaggio/centraldashboard&gt;https://hub.docker.com/repository/docker/gsantomaggio/centraldashboard&lt;/denchmark-link&gt;

I will leave it there until the new release
		</comment>
		<comment id='7' author='denisl78' date='2020-04-16T13:32:46Z'>
		I had the same problem deploying kubeflow into a KOPS cluster into AWS. I edited the  deployment and change the  for the one that &lt;denchmark-link:https://github.com/Gsantomaggio&gt;@Gsantomaggio&lt;/denchmark-link&gt;
 built, and it worked.
The value I left at the deployment is 
The deployment is in READY state again:
&lt;denchmark-code&gt;$ kubectl get deployments -n kubeflow centraldashboard 
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
centraldashboard   1/1     1            1           19m
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='denisl78' date='2020-08-12T03:10:13Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>