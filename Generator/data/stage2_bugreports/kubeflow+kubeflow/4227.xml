<bug id='4227' author='swiftdiaries' open_date='2019-10-04T03:39:51Z' closed_time='2019-10-17T20:49:59Z'>
	<summary>Post-submits seem to pass while endpoint_is_ready test is failing</summary>
	<description>
/kind bug
What steps did you take and what happened:
[A clear and concise description of what the bug is.]
Checked a postsubmit for a successful run.
But the endpoint_is_ready test seems to be failing but is chalked off as a successful run anyway.
Logs:
&lt;denchmark-code&gt;gcp_util.py                 98 INFO     Trying url: https://kfctl-65e7.endpoints.kubeflow-ci-deployment.cloud.goog
gcp_util.py                117 INFO     https://kfctl-65e7.endpoints.kubeflow-ci-deployment.cloud.goog: IAP not ready, exception caught HTTPSConnectionPool(host='
kfctl-65e7.endpoints.kubeflow-ci-deployment.cloud.goog', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation
 of protocol (_ssl.c:645)'),)), request number: 150
xfail
generated xml file: /mnt/test-data-volume/kubeflow-postsubmit-kfctl-go-iap-endpoint-529de7d-7824-4684/output/artifacts/junit_kubeflow-postsubmit-kfctl-go-iap-endp
oint-529de7d-7824-4684/junit_endpoint-is-ready-test-kfctl_gcp_iap.xml
========================= 1 xfailed in 1505.83 seconds =========================
&lt;/denchmark-code&gt;

Workflow: &lt;denchmark-link:http://testing-argo.kubeflow.org/workflows/kubeflow-test-infra/kubeflow-postsubmit-kfctl-go-iap-endpoint-529de7d-7824-4684?tab=workflow&amp;nodeId=kubeflow-postsubmit-kfctl-go-iap-endpoint-529de7d-7824-4684-3259484450&gt;http://testing-argo.kubeflow.org/workflows/kubeflow-test-infra/kubeflow-postsubmit-kfctl-go-iap-endpoint-529de7d-7824-4684?tab=workflow&amp;nodeId=kubeflow-postsubmit-kfctl-go-iap-endpoint-529de7d-7824-4684-3259484450&lt;/denchmark-link&gt;

What did you expect to happen:
All tests to have passed successfully.
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard):
kfctl version: (use kfctl version):
Kubernetes platform: (e.g. minikube)
Kubernetes version: (use kubectl version):
OS (e.g. from /etc/os-release):

	</description>
	<comments>
		<comment id='1' author='swiftdiaries' date='2019-10-04T03:40:05Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='swiftdiaries' date='2019-10-04T03:41:24Z'>
		/cc &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/abhi-g&gt;@abhi-g&lt;/denchmark-link&gt;

This needs to be p0
		</comment>
		<comment id='3' author='swiftdiaries' date='2019-10-04T03:44:45Z'>
		Is the endpoint actually ready? Or is this just a test failure?
		</comment>
		<comment id='4' author='swiftdiaries' date='2019-10-04T03:46:53Z'>
		I'm guessing IAP is taking time to be fully ready and the test finishes before that.
We could try increasing the backoff and retry timeout
		</comment>
		<comment id='5' author='swiftdiaries' date='2019-10-11T23:45:50Z'>
		I increased the timeout to 50 minutes but the endpoint wasn't ready still.
I guess the problem is still about the SSL certificate quota, which is at its limit right now. So I will later try to clean the usage and rerun this test.
		</comment>
		<comment id='6' author='swiftdiaries' date='2019-10-14T21:26:45Z'>
		same issue as in &lt;denchmark-link:https://github.com/kubeflow/kfctl/issues/42&gt;kubeflow/kfctl#42&lt;/denchmark-link&gt;

Just tried the endpoint test when the quota is enough.(For last a few days the quota was full 200/200) The IAP endpoint was ready in ~20 mins and all tests passed.
I think we are can close this issue now.
I checked the cron Job on releasing the resources: it does seem to do its job. Some SSLs were held not to be deleted because of being used by some targetHttpsProxy, which were in turn used by some forwarding rules.
A possible problem is that the deletion of forwardingRule is a long running operation. e.g.
&lt;denchmark-code&gt;INFO|2019-10-11T10:26:09|/src/kubeflow/testing/py/kubeflow/testing/cleanup_ci.py|467| response = {u'status': u'RUNNING', u'kind': u'compute#operation', u'name': u'operation-1570789568790-5949ff243bf5e-90491358-8413ffbb', u'startTime': u'2019-10-11T03:26:09.248-07:00', u'insertTime': u'2019-10-11T03:26:09.227-07:00', u'targetId': u'5379620327702838498', u'targetLink': u'https://www.googleapis.com/compute/v1/projects/kubeflow-ci-deployment/global/forwardingRules/k8s-fws-istio-system-envoy-ingress--49a7244eccc604fa', u'operationType': u'delete', u'progress': 0, u'id': u'2547581075254535726', u'selfLink': u'https://www.googleapis.com/compute/v1/projects/kubeflow-ci-deployment/global/operations/operation-1570789568790-5949ff243bf5e-90491358-8413ffbb', u'user': u'kubeflow-testing@kubeflow-ci.iam.gserviceaccount.com'}
&lt;/denchmark-code&gt;

We don't know if it is completed or not based on the logs. Maybe some operation failed which causes the SSLs were not deleted in the past few days.
		</comment>
		<comment id='7' author='swiftdiaries' date='2019-10-15T23:20:27Z'>
		Considering we seem to have some leakage, should the cleanup script have retries?
		</comment>
		<comment id='8' author='swiftdiaries' date='2019-10-16T12:37:38Z'>
		&lt;denchmark-link:https://github.com/zhenghuiwang&gt;@zhenghuiwang&lt;/denchmark-link&gt;
 Any update on this?
We have a bunch of potential issues affecting IAP

e.g #4266
e.g. #4043

Using the post submits to ensure this is running reliable would be a huge help.
		</comment>
	</comments>
</bug>