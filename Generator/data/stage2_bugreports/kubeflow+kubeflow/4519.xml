<bug id='4519' author='Luke035' open_date='2019-11-22T08:24:37Z' closed_time='2020-09-04T12:01:52Z'>
	<summary>Jupyter Notebook - istio side car readiness probe fail (status code 503)</summary>
	<description>
/kind bug
What steps did you take and what happened:
Launching a jupyter notebook with a simple PVC bounded on the home dir, creates a running pod that fails the readiness probe.
&lt;denchmark-code&gt;  Normal   Created                 35s   kubelet, REDACTED  Created container istio-init
  Normal   Started                 35s   kubelet, REDACTED  Started container istio-init
  Normal   Pulled                  32s   kubelet, REDACTED  Container image "gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0" already present on machine
  Normal   Created                 32s   kubelet, REDACTED  Created container test-nb
  Normal   Started                 32s   kubelet, REDACTED  Started container test-nb
  Normal   Pulled                  32s   kubelet, REDACTED  Container image "docker.io/istio/proxyv2:1.1.6" already present on machine
  Normal   Created                 32s   kubelet, REDACTED  Created container istio-proxy
  Normal   Started                 32s   kubelet, REDACTED  Started container istio-proxy
  Warning  Unhealthy               30s   kubelet, REDACTED  Readiness probe failed: HTTP probe failed with statuscode: 503
&lt;/denchmark-code&gt;

What did you expect to happen:
Readiness probe should be executed correctly
Environment:

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard): 0.7
kfctl version: (use kfctl version): 0.7
Kubernetes platform: (e.g. minikube) EKS
Kubernetes version: (use kubectl version): 1.14.7
OS (e.g. from /etc/os-release): Amazon Linux (Centos)

	</description>
	<comments>
		<comment id='1' author='Luke035' date='2019-11-22T08:24:52Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='Luke035' date='2019-11-25T14:22:28Z'>
		Can you provide the output of kubeftl get pods -o yaml for your notebook ?
Can you also provide the logs for your notebook pod?
		</comment>
		<comment id='3' author='Luke035' date='2019-11-26T09:08:25Z'>
		Sure please find them here
kubectl get pods -o yaml
&lt;denchmark-code&gt;apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/psp: eks.privileged
      sidecar.istio.io/status: '{"version":"5f3ae3613c7945ef767cb9fd594596bc001ff3ab915f12e4379c0cb5648d2729","initContainers":["istio-init"],"containers":["istio-proxy"],"volumes":["istio-envoy","istio-certs"],"imagePullSecrets":null}'
    creationTimestamp: "2019-11-26T09:04:36Z"
    generateName: test-nb-
    labels:
      app: test-nb
      controller-revision-hash: test-nb-6f54946787
      notebook-name: test-nb
      statefulset: test-nb
      statefulset.kubernetes.io/pod-name: test-nb-0
    name: test-nb-0
    namespace: ai-dev
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: test-nb
      uid: c4da8ddc-102b-11ea-ab86-026cbae33139
    resourceVersion: "39969087"
    selfLink: /api/v1/namespaces/ai-dev/pods/test-nb-0
    uid: c4ea8bdd-102b-11ea-ab86-026cbae33139
  spec:
    containers:
    - env:
      - name: NB_PREFIX
        value: /notebook/ai-dev/test-nb
      image: gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0
      imagePullPolicy: IfNotPresent
      name: test-nb
      ports:
      - containerPort: 8888
        name: notebook-port
        protocol: TCP
      resources:
        requests:
          cpu: "1"
          memory: 2Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /home/jovyan
        name: workspace-test-nb
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-editor-token-sxchs
        readOnly: true
      workingDir: /home/jovyan
    - args:
      - proxy
      - sidecar
      - --domain
      - $(POD_NAMESPACE).svc.cluster.local
      - --configPath
      - /etc/istio/proxy
      - --binaryPath
      - /usr/local/bin/envoy
      - --serviceCluster
      - test-nb.$(POD_NAMESPACE)
      - --drainDuration
      - 45s
      - --parentShutdownDuration
      - 1m0s
      - --discoveryAddress
      - istio-pilot.istio-system:15010
      - --zipkinAddress
      - zipkin.istio-system:9411
      - --connectTimeout
      - 10s
      - --proxyAdminPort
      - "15000"
      - --concurrency
      - "2"
      - --controlPlaneAuthPolicy
      - NONE
      - --statusPort
      - "15020"
      - --applicationPorts
      - "8888"
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: INSTANCE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ISTIO_META_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: ISTIO_META_CONFIG_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: ISTIO_META_INTERCEPTION_MODE
        value: REDIRECT
      - name: ISTIO_METAJSON_ANNOTATIONS
        value: |
          {"kubernetes.io/psp":"eks.privileged"}
      - name: ISTIO_METAJSON_LABELS
        value: |
          {"app":"test-nb","controller-revision-hash":"test-nb-6f54946787","notebook-name":"test-nb","statefulset":"test-nb","statefulset.kubernetes.io/pod-name":"test-nb-0"}
      image: docker.io/istio/proxyv2:1.1.6
      imagePullPolicy: IfNotPresent
      name: istio-proxy
      ports:
      - containerPort: 15090
        name: http-envoy-prom
        protocol: TCP
      readinessProbe:
        failureThreshold: 30
        httpGet:
          path: /healthz/ready
          port: 15020
          scheme: HTTP
        initialDelaySeconds: 1
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "2"
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 40Mi
      securityContext:
        readOnlyRootFilesystem: true
        runAsUser: 1337
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/istio/proxy
        name: istio-envoy
      - mountPath: /etc/certs/
        name: istio-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-editor-token-sxchs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: test-nb-0
    initContainers:
    - args:
      - -p
      - "15001"
      - -u
      - "1337"
      - -m
      - REDIRECT
      - -i
      - '*'
      - -x
      - ""
      - -b
      - "8888"
      - -d
      - "15020"
      image: docker.io/istio/proxy_init:1.1.6
      imagePullPolicy: IfNotPresent
      name: istio-init
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 10m
          memory: 10Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
    nodeName: ip-192-168-5-250.ec2.internal
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 100
    serviceAccount: default-editor
    serviceAccountName: default-editor
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: workspace-test-nb
      persistentVolumeClaim:
        claimName: workspace-test-nb
    - emptyDir:
        medium: Memory
      name: dshm
    - name: default-editor-token-sxchs
      secret:
        defaultMode: 420
        secretName: default-editor-token-sxchs
    - emptyDir:
        medium: Memory
      name: istio-envoy
    - name: istio-certs
      secret:
        defaultMode: 420
        optional: true
        secretName: istio.default-editor
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-26T09:04:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-26T09:05:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-26T09:05:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-26T09:04:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://7e4a19cf8ea41dd67587b9038fa5bc9c988bb5b5a12a174c8fe736d910c141c8
      image: istio/proxyv2:1.1.6
      imageID: docker-pullable://istio/proxyv2@sha256:e7ee1ad38bd5b556ad0527ac691a9f647b66835960417b154c5d28b2ed9219cb
      lastState: {}
      name: istio-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-26T09:05:34Z"
    - containerID: docker://a2f008729989f53caa05a8c2b0308d8db271adb26e7faf8aee2654e13829ecd4
      image: gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu:v0.7.0
      imageID: docker-pullable://gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-cpu@sha256:fe174faf7c477bc3dae796b067d98ac3f0d31e8075007a1146f86d13f2c98e13
      lastState: {}
      name: test-nb
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-26T09:05:29Z"
    hostIP: 192.168.5.250
    initContainerStatuses:
    - containerID: docker://67e79e8c6fe47cb54946432ab7787700678d1cfdc67e949f82236cb9fdeda886
      image: istio/proxy_init:1.1.6
      imageID: docker-pullable://istio/proxy_init@sha256:54d89fb2b3b0a2365f2d2b0a8862f1f8320a63ab6a09c637c60f13f6021c4609
      lastState: {}
      name: istio-init
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://67e79e8c6fe47cb54946432ab7787700678d1cfdc67e949f82236cb9fdeda886
          exitCode: 0
          finishedAt: "2019-11-26T09:04:46Z"
          reason: Completed
          startedAt: "2019-11-26T09:04:45Z"
    phase: Running
    podIP: 192.168.4.137
    qosClass: Burstable
    startTime: "2019-11-26T09:04:36Z"
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
&lt;/denchmark-code&gt;

Logs:
Notebook:
&lt;denchmark-code&gt;[W 08:51:36.138 NotebookApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.
[I 08:51:36.163 NotebookApp] JupyterLab extension loaded from /usr/local/lib/python3.6/dist-packages/jupyterlab
[I 08:51:36.163 NotebookApp] JupyterLab application directory is /usr/local/share/jupyter/lab
[I 08:51:36.165 NotebookApp] Serving notebooks from local directory: /home/jovyan
[I 08:51:36.165 NotebookApp] The Jupyter Notebook is running at:
[I 08:51:36.165 NotebookApp] http://(test-nb-0 or 127.0.0.1):8888/notebook/ai-dev/test-nb/
[I 08:51:36.165 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
&lt;/denchmark-code&gt;

ISTIO-PROXY
&lt;denchmark-code&gt;2019-11-25T08:51:37.834651Z	info	Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 1 successful, 0 rejected; lds updates: 0 successful, 0 rejected
[2019-11-25 08:51:38.110][19][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2019-11-25 08:51:38.113][19][warning][filter] [src/envoy/http/authn/http_filter_factory.cc:102] mTLS PERMISSIVE mode is used, connection can be either plaintext or TLS, and client cert can be omitted. Please consider to upgrade to mTLS STRICT mode for more secure configuration that only allows TLS connection with client cert. See https://istio.io/docs/tasks/security/mtls-migration/
[2019-11-25 08:51:38.114][19][warning][filter] [src/envoy/http/authn/http_filter_factory.cc:102] mTLS PERMISSIVE mode is used, connection can be either plaintext or TLS, and client cert can be omitted. Please consider to upgrade to mTLS STRICT mode for more secure configuration that only allows TLS connection with client cert. See https://istio.io/docs/tasks/security/mtls-migration/
2019-11-25T08:51:39.834882Z	info	Envoy proxy is ready
[2019-11-25 08:51:43.110][19][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
2019-11-25T08:51:45.886890Z	info	watchFileEvents: notifying
[2019-11-25 08:52:17.914][19][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2019-11-25 08:52:29.809][19][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2019-11-25 08:52:34.915][19][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
2019-11-25T08:52:42.919672Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2019-11-25T08:52:42.919716Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2019-11-25T08:52:42.919736Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2019-11-25T08:52:42.919781Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Luke035' date='2019-11-26T12:26:04Z'>
		I seem to be experiencing the same issue.
A. Started with &lt;denchmark-link:https://aihub.cloud.google.com/u/0/p/products%2F40e64ddb-156d-4f83-95f0-a9ef640bf19d/v/1&gt;https://aihub.cloud.google.com/u/0/p/products%2F40e64ddb-156d-4f83-95f0-a9ef640bf19d/v/1&lt;/denchmark-link&gt;
. Created a kubeflow cluster via the page, chose user/pw and not iap. Ran nicely. GUI up and working. All pods and services ok.
B. Started a notebook server via the kubeflow gui. The pod fails with a 503. See below for details.
------- details ---------
&lt;denchmark-code&gt;$ kubectl get pods --namespace=kubeflow-nnn
NAME          READY   STATUS    RESTARTS   AGE
191126nb1-0   1/2     Running   0          3m49s
$ kubectl describe pod 191126nb1-0 --namespace=kubeflow-nnn
Name:               191126nb1-0
Namespace:          kubeflow-nnn
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               gke-aihub-kf-191126-aihub-kf-191126-c-95b79f3f-ms7m/10.116.11.23
Start Time:         Tue, 26 Nov 2019 11:25:58 +0100
Labels:             app=191126nb1
                    controller-revision-hash=a443Bnb1-bc46fbb99
                    notebook-name=a443Bnb1
                    statefulset=a443Bnb1
                    statefulset.kubernetes.io/pod-name=a443Bnb1-0
Annotations:        sidecar.istio.io/status:
                      {"version":"5f3ae3613c7945ef767cb9fd5945fd949301ff3ab915f12e4379c0cb5648d2729","initContainers":["istio-init"],"containers":["istio-proxy"]...
Status:             Running
IP:                 10.116.11.24
Controlled By:      StatefulSet/a443Bnb1
Init Containers:
  istio-init:
    Container ID:  docker://0bb1c56080fe0b57e09dfdk944307cad5523473b7bfade47a4e7c31e85835eb1212
    Image:         docker.io/istio/proxy_init:1.1.6
    Image ID:      docker-pullable://istio/proxy_init@sha256:54d89fb2b3b0a2365f2d2b0a8862f1f8320a63ab6a09c637c60f13f6021c4609
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Args:
      -p
      15001
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x
      
      -b
      8888
      -d
      15020
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Tue, 26 Nov 2019 11:26:17 +0100
      Finished:     Tue, 26 Nov 2019 11:26:19 +0100
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:        10m
      memory:     10Mi
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
Containers:
  a443Bnb1:
    Container ID:   docker://f3ecc8adaf951a2074fkjfkdk484893Ad6c756735e4fdbbbcf9f6605f4ecf38fa87879
    Image:          gcr.io/kubeflow-images-public/tensorflow-1.13.1-notebook-cpu:v0.5.0
    Image ID:       docker-pullable://gcr.io/kubeflow-images-public/tensorflow-1.13.1-notebook-cpu@sha256:5aaccf0267f085afd976342a8e943a9c6cefdjf88D8ff5b554df4e15fa7bf15cbd7a3
    Port:           8888/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Tue, 26 Nov 2019 11:28:41 +0100
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     500m
      memory:  1Gi
    Environment:
      NB_PREFIX:  /notebook/kubeflow-nnn/a443Bnb1
    Mounts:
      /dev/shm from dshm (rw)
      /home/jovyan from workspace-a443Bnb1 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-editor-token-fcmqt (ro)
  istio-proxy:
    Container ID:  docker://8e676a1ed8194118a95692a300708e2d211414fe72462c1364251949a4f11a15
    Image:         docker.io/istio/proxyv2:1.1.6
    Image ID:      docker-pullable://istio/proxyv2@sha256:e7ee1ad38bd5b556ad0527ac691a9f647b66835960417b154c5d28b2ed9219cb
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --configPath
      /etc/istio/proxy
      --binaryPath
      /usr/local/bin/envoy
      --serviceCluster
      a443Bnb1.$(POD_NAMESPACE)
      --drainDuration
      45s
      --parentShutdownDuration
      1m0s
      --discoveryAddress
      istio-pilot.istio-system:15010
      --zipkinAddress
      zipkin.istio-system:9411
      --connectTimeout
      10s
      --proxyAdminPort
      15000
      --concurrency
      2
      --controlPlaneAuthPolicy
      NONE
      --statusPort
      15020
      --applicationPorts
      8888
    State:          Running
      Started:      Tue, 26 Nov 2019 11:28:42 +0100
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  128Mi
    Requests:
      cpu:      10m
      memory:   40Mi
    Readiness:  http-get http://:15020/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30
    Environment:
      POD_NAME:                      a443Bnb1-0 (v1:metadata.name)
      POD_NAMESPACE:                 kubeflow-nnn (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      ISTIO_META_POD_NAME:           a443Bnb1-0 (v1:metadata.name)
      ISTIO_META_CONFIG_NAMESPACE:   kubeflow-nnn (v1:metadata.namespace)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_METAJSON_LABELS:         {"app":"a443Bnb1","controller-revision-hash":"a443Bnb1-bc46fbb99","notebook-name":"a443Bnb1","statefulset":"a443Bnb1","statefulset.kubernetes.io/pod-name":"a443Bnb1-0"}
                                     
    Mounts:
      /etc/certs/ from istio-certs (ro)
      /etc/istio/proxy from istio-envoy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-editor-token-fcmqt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  workspace-a443Bnb1:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  workspace-a443Bnb1
    ReadOnly:   false
  dshm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  &lt;unset&gt;
  default-editor-token-fcmqt:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-editor-token-fcmqt
    Optional:    false
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  &lt;unset&gt;
  istio-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio.default-editor
    Optional:    true
QoS Class:       Burstable
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason                  Age                    From                                                          Message
  ----     ------                  ----                   ----                                                          -------
  Warning  FailedScheduling        9m26s (x3 over 9m28s)  default-scheduler                                             pod has unbound immediate PersistentVolumeClaims (repeated 2 times)
  Normal   Scheduled               9m26s                  default-scheduler                                             Successfully assigned kubeflow-nnn/a443Bnb1-0 to gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3
  Normal   SuccessfulAttachVolume  9m21s                  attachdetach-controller                                       AttachVolume.Attach succeeded for volume "pvc-214848365e-1044-99ea-ad9b-42010a84389489"
  Normal   Pulling                 9m8s                   kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  pulling image "docker.io/istio/proxy_init:1.1.6"
  Normal   Pulled                  9m7s                   kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Successfully pulled image "docker.io/istio/proxy_init:1.1.6"
  Normal   Created                 9m7s                   kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Created container
  Normal   Started                 9m6s                   kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Started container
  Normal   Pulling                 9m5s                   kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  pulling image "gcr.io/kubeflow-images-public/tensorflow-1.13.1-notebook-cpu:v0.5.0"
  Normal   Pulled                  6m55s                  kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Successfully pulled image "gcr.io/kubeflow-images-public/tensorflow-1.13.1-notebook-cpu:v0.5.0"
  Normal   Created                 6m43s                  kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Created container
  Normal   Started                 6m43s                  kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Started container
  Normal   Pulled                  6m43s                  kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Container image "docker.io/istio/proxyv2:1.1.6" already present on machine
  Normal   Created                 6m43s                  kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Created container
  Normal   Started                 6m42s                  kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Started container
  Warning  Unhealthy               4m7s (x78 over 6m41s)  kubelet, gke-aihub-kf-0394-aihub-kf-3324-c-95b7fkdb8-m09s3  Readiness probe failed: HTTP probe failed with statuscode: 503
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='Luke035' date='2020-01-08T22:42:19Z'>
		/area jupyter
/priority p2
		</comment>
		<comment id='6' author='Luke035' date='2020-01-17T13:11:56Z'>
		having the same problem. installed kubeflow 0.7 on kubernetes on docker local mac. no error after apply but when creating a notebook getthing   Warning  Unhealthy         6m26s (x8 over 6m40s)  kubelet, docker-desktop  Readiness probe failed: HTTP probe failed with statuscode: 503
&lt;denchmark-code&gt;âžœ  ~ kd pod/test-0 -n hamed2
Name:               test-0
Namespace:          hamed2
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               docker-desktop/192.168.65.3
Start Time:         Fri, 17 Jan 2020 10:58:19 +0000
Labels:             app=test
                    controller-revision-hash=test-7cf785ff47
                    notebook-name=test
                    statefulset=test
                    statefulset.kubernetes.io/pod-name=test-0
Annotations:        sidecar.istio.io/status:
                      {"version":"5f3ae3613c7945ef767cb9fd594596bc001ff3ab915f12e4379c0cb5648d2729","initContainers":["istio-init"],"containers":["istio-proxy"]...
Status:             Running
IP:                 10.1.0.100
Controlled By:      StatefulSet/test
Init Containers:
  istio-init:
    Container ID:  docker://639798eaa36ac8fd74f02e16d256e7479b8737555eb21cf6469ccb5f94b5f41e
    Image:         docker.io/istio/proxy_init:1.1.6
    Image ID:      docker-pullable://istio/proxy_init@sha256:54d89fb2b3b0a2365f2d2b0a8862f1f8320a63ab6a09c637c60f13f6021c4609
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Args:
      -p
      15001
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x

      -b
      8888
      -d
      15020
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 17 Jan 2020 10:58:21 +0000
      Finished:     Fri, 17 Jan 2020 10:58:22 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:        10m
      memory:     10Mi
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
Containers:
  test:
    Container ID:   docker://fbf4b37cdafed64024955b8800788a85d077e32494e63642b4303d6c20406d0c
    Image:          python:3.7
    Image ID:       docker-pullable://python@sha256:d64c9b3d97e57a8df37ec1c4db9be638660efe0d8703f9679cc1254358a9136d
    Port:           8888/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 17 Jan 2020 11:03:58 +0000
      Finished:     Fri, 17 Jan 2020 11:03:58 +0000
    Ready:          False
    Restart Count:  6
    Requests:
      cpu:     200m
      memory:  100Mi
    Environment:
      NB_PREFIX:  /notebook/hamed2/test
    Mounts:
      /dev/shm from dshm (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-editor-token-rrbvs (ro)
  istio-proxy:
    Container ID:  docker://b2a6cd1a2cc3ac00af0dbcbc9626e9918f14dac9710b63fe0cb7655ae35f01ae
    Image:         docker.io/istio/proxyv2:1.1.6
    Image ID:      docker-pullable://istio/proxyv2@sha256:e7ee1ad38bd5b556ad0527ac691a9f647b66835960417b154c5d28b2ed9219cb
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --configPath
      /etc/istio/proxy
      --binaryPath
      /usr/local/bin/envoy
      --serviceCluster
      test.$(POD_NAMESPACE)
      --drainDuration
      45s
      --parentShutdownDuration
      1m0s
      --discoveryAddress
      istio-pilot.istio-system:15010
      --zipkinAddress
      zipkin.istio-system:9411
      --connectTimeout
      10s
      --proxyAdminPort
      15000
      --concurrency
      2
      --controlPlaneAuthPolicy
      NONE
      --statusPort
      15020
      --applicationPorts
      8888
    State:          Running
      Started:      Fri, 17 Jan 2020 10:58:23 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  128Mi
    Requests:
      cpu:      10m
      memory:   40Mi
    Readiness:  http-get http://:15020/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30
    Environment:
      POD_NAME:                      test-0 (v1:metadata.name)
      POD_NAMESPACE:                 hamed2 (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      ISTIO_META_POD_NAME:           test-0 (v1:metadata.name)
      ISTIO_META_CONFIG_NAMESPACE:   hamed2 (v1:metadata.namespace)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_METAJSON_LABELS:         {"app":"test","controller-revision-hash":"test-7cf785ff47","notebook-name":"test","statefulset":"test","statefulset.kubernetes.io/pod-name":"test-0"}

    Mounts:
      /etc/certs/ from istio-certs (ro)
      /etc/istio/proxy from istio-envoy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-editor-token-rrbvs (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  dshm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  &lt;unset&gt;
  default-editor-token-rrbvs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-editor-token-rrbvs
    Optional:    false
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  &lt;unset&gt;
  istio-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio.default-editor
    Optional:    true
QoS Class:       Burstable
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age                    From                     Message
  ----     ------            ----                   ----                     -------
  Normal   Scheduled         6m45s                  default-scheduler        Successfully assigned hamed2/test-0 to docker-desktop
  Normal   Pulled            6m43s                  kubelet, docker-desktop  Container image "docker.io/istio/proxy_init:1.1.6" already present on machine
  Normal   Created           6m43s                  kubelet, docker-desktop  Created container istio-init
  Normal   Started           6m43s                  kubelet, docker-desktop  Started container istio-init
  Normal   Created           6m41s                  kubelet, docker-desktop  Created container istio-proxy
  Normal   Pulled            6m41s                  kubelet, docker-desktop  Container image "docker.io/istio/proxyv2:1.1.6" already present on machine
  Normal   Started           6m41s                  kubelet, docker-desktop  Started container istio-proxy
  Normal   Created           6m27s (x3 over 6m41s)  kubelet, docker-desktop  Created container test
  Normal   Pulled            6m27s (x3 over 6m41s)  kubelet, docker-desktop  Container image "python:3.7" already present on machine
  Normal   Started           6m26s (x3 over 6m41s)  kubelet, docker-desktop  Started container test
  Warning  Unhealthy         6m26s (x8 over 6m40s)  kubelet, docker-desktop  Readiness probe failed: HTTP probe failed with statuscode: 503
  Warning  BackOff           50s (x29 over 6m39s)   kubelet, docker-desktop  Back-off restarting failed container
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='Luke035' date='2020-01-19T23:13:28Z'>
		Exactly the same problem here
		</comment>
		<comment id='8' author='Luke035' date='2020-01-29T01:38:36Z'>
		/cc &lt;denchmark-link:https://github.com/kunmingg&gt;@kunmingg&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/kimwnasptd&gt;@kimwnasptd&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='Luke035' date='2020-01-29T04:11:53Z'>
		&lt;denchmark-link:https://github.com/prcastro&gt;@prcastro&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Luke035&gt;@Luke035&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jtfogarty&gt;@jtfogarty&lt;/denchmark-link&gt;
  it looks like it is the ISTIO side car that is failing the readiness probe; it looks like the Jupyter container doesn't have any probes.
If you wait a couple minutes does the istio side car become ready?
In my case I observed the istio-side car proxy took a couple of minutes to become ready.
Could you provide logs for the istio-sidecar?
&lt;denchmark-code&gt;kubectl -n $NAMESPACE logs $POD -c istio-proxy
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='Luke035' date='2020-01-29T04:13:52Z'>
		Looking at the output in
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4519#issuecomment-558533737&gt;#4519 (comment)&lt;/denchmark-link&gt;

It looks like the container status indicates both containers are in state ready with no restarts.
		</comment>
		<comment id='11' author='Luke035' date='2020-01-29T13:30:00Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 I also noticed that this issue seems to appear for every istio-proxy container in the cluster with ISTIO installed via KUBEFLOW.
Since I'm using AWS EKS can be something related to it? Available for debugging!
		</comment>
		<comment id='12' author='Luke035' date='2020-01-29T18:09:04Z'>
		I seem to be running into same issue.
$ kubectl -n  describe pod/test1-0
&lt;denchmark-code&gt;Name:           test1-0
Namespace:      &lt;NAMESPACE&gt;
Priority:       0
Node:           node1/10.75.38.135
Start Time:     Wed, 29 Jan 2020 11:41:24 -0600
Labels:         app=test1
                controller-revision-hash=test1-555f67bdc6
                notebook-name=test1
                statefulset=test1
                statefulset.kubernetes.io/pod-name=test1-0
Annotations:    sidecar.istio.io/status:
                  {"version":"5f3ae3613c7945ef767cb9fd594596bc001ff3ab915f12e4379c0cb5648d2729","initContainers":["istio-init"],"containers":["istio-proxy"]...
Status:         Running
IP:             10.233.85.30
Controlled By:  StatefulSet/test1
Init Containers:
  istio-init:
    Container ID:  docker://7664ab7a5e4597004a5fbc9c5d5cb32581327cadb1e1adad599e0544e2a2f12d
    Image:         docker.io/istio/proxy_init:1.1.6
    Image ID:      docker-pullable://istio/proxy_init@sha256:54d89fb2b3b0a2365f2d2b0a8862f1f8320a63ab6a09c637c60f13f6021c4609
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Args:
      -p
      15001
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x

      -b
      8888
      -d
      15020
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 29 Jan 2020 11:41:27 -0600
      Finished:     Wed, 29 Jan 2020 11:41:29 -0600
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:        10m
      memory:     10Mi
    Environment:  &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-editor-token-x97bq (ro)
Containers:
  test1:
    Container ID:   docker://ddc45eaf9ce40e90e448e91a6045e4163fb4c3b3f938a5733b419a0b67e7ef4a
    Image:          gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-gpu:v0.7.0
    Image ID:       docker-pullable://gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-gpu@sha256:895aa500ff09c9080e0755044e4fd9a3bd3dc3ae403415e220ebcc1dab09b6ef
    Port:           8888/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 29 Jan 2020 11:41:31 -0600
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:     1
      memory:  1Gi
    Environment:
      NB_PREFIX:  /notebook/&lt;NAMESPACE&gt;/test1
    Mounts:
      /dev/shm from dshm (rw)
      /home/jovyan from workspace-test1 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-editor-token-x97bq (ro)
  istio-proxy:
    Container ID:  docker://73383c973cffbd2c6238fa967aa8906f5f02c32ee35852b4622fe676296522ca
    Image:         docker.io/istio/proxyv2:1.1.6
    Image ID:      docker-pullable://istio/proxyv2@sha256:e7ee1ad38bd5b556ad0527ac691a9f647b66835960417b154c5d28b2ed9219cb
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --configPath
      /etc/istio/proxy
      --binaryPath
      /usr/local/bin/envoy
      --serviceCluster
      test1.$(POD_NAMESPACE)
      --drainDuration
      45s
      --parentShutdownDuration
      1m0s
      --discoveryAddress
      istio-pilot.istio-system:15010
      --zipkinAddress
      zipkin.istio-system:9411
      --connectTimeout
      10s
      --proxyAdminPort
      15000
      --concurrency
      2
      --controlPlaneAuthPolicy
      NONE
      --statusPort
      15020
      --applicationPorts
      8888
    State:          Running
      Started:      Wed, 29 Jan 2020 11:41:32 -0600
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  128Mi
    Requests:
      cpu:      10m
      memory:   40Mi
    Readiness:  http-get http://:15020/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30
    Environment:
      POD_NAME:                      test1-0 (v1:metadata.name)
      POD_NAMESPACE:                 &lt;NAMESPACE&gt; (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      ISTIO_META_POD_NAME:           test1-0 (v1:metadata.name)
      ISTIO_META_CONFIG_NAMESPACE:   &lt;NAMESPACE&gt; (v1:metadata.namespace)
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_METAJSON_LABELS:         {"app":"test1","controller-revision-hash":"test1-555f67bdc6","notebook-name":"test1","statefulset":"test1","statefulset.kubernetes.io/pod-name":"test1-0"}

    Mounts:
      /etc/certs/ from istio-certs (ro)
      /etc/istio/proxy from istio-envoy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-editor-token-x97bq (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  workspace-test1:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  workspace-test1
    ReadOnly:   false
  dshm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  &lt;unset&gt;
  default-editor-token-x97bq:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-editor-token-x97bq
    Optional:    false
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  &lt;unset&gt;
  istio-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  istio.default-editor
    Optional:    true
QoS Class:       Burstable
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                From                                          Message
  ----     ------     ----               ----                                          -------
  Normal   Scheduled  24m                default-scheduler                             Successfully assigned &lt;NAMESPACE&gt;/test1-0 to node1
  Normal   Pulled     24m                kubelet, node1  Container image "docker.io/istio/proxy_init:1.1.6" already present on machine
  Normal   Created    24m                kubelet, node1  Created container istio-init
  Normal   Started    24m                kubelet, node1  Started container istio-init
  Normal   Pulled     24m                kubelet, node1  Container image "gcr.io/kubeflow-images-public/tensorflow-1.14.0-notebook-gpu:v0.7.0" already present on machine
  Normal   Created    24m                kubelet, node1  Created container test1
  Normal   Started    24m                kubelet, node1  Started container test1
  Normal   Pulled     24m                kubelet, node1  Container image "docker.io/istio/proxyv2:1.1.6" already present on machine
  Normal   Created    24m                kubelet, node1  Created container istio-proxy
  Normal   Started    24m                kubelet, node1  Started container istio-proxy
  Warning  Unhealthy  24m (x2 over 24m)  kubelet, node1  Readiness probe failed: HTTP probe failed with statuscode: 503
&lt;/denchmark-code&gt;

$ kubectl -n ${NAMESPACE} logs pod/test1-0 -c test1
&lt;denchmark-code&gt;[I 17:41:32.160 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret
[W 17:41:32.372 NotebookApp] All authentication is disabled.  Anyone who can connect to this server will be able to run code.
[I 17:41:32.398 NotebookApp] JupyterLab extension loaded from /usr/local/lib/python3.6/dist-packages/jupyterlab
[I 17:41:32.399 NotebookApp] JupyterLab application directory is /usr/local/share/jupyter/lab
[I 17:41:32.400 NotebookApp] Serving notebooks from local directory: /home/jovyan
[I 17:41:32.400 NotebookApp] The Jupyter Notebook is running at:
[I 17:41:32.401 NotebookApp] http://test1-0:8888/notebook/kevin-vasko/test1/
[I 17:41:32.401 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
&lt;/denchmark-code&gt;


kubectl -n ${NAMESPACE} logs pod/test1-0 -c istio-proxy

&lt;denchmark-code&gt;2020-01-29T17:41:32.114481Z	info	FLAG: --applicationPorts="[8888]"
2020-01-29T17:41:32.114542Z	info	FLAG: --binaryPath="/usr/local/bin/envoy"
2020-01-29T17:41:32.114550Z	info	FLAG: --concurrency="2"
2020-01-29T17:41:32.114556Z	info	FLAG: --configPath="/etc/istio/proxy"
2020-01-29T17:41:32.114563Z	info	FLAG: --connectTimeout="10s"
2020-01-29T17:41:32.114573Z	info	FLAG: --controlPlaneAuthPolicy="NONE"
2020-01-29T17:41:32.114580Z	info	FLAG: --controlPlaneBootstrap="true"
2020-01-29T17:41:32.114585Z	info	FLAG: --customConfigFile=""
2020-01-29T17:41:32.114590Z	info	FLAG: --datadogAgentAddress=""
2020-01-29T17:41:32.114596Z	info	FLAG: --disableInternalTelemetry="false"
2020-01-29T17:41:32.114602Z	info	FLAG: --discoveryAddress="istio-pilot.istio-system:15010"
2020-01-29T17:41:32.114608Z	info	FLAG: --domain="&lt;NAMESPACE&gt;.svc.cluster.local"
2020-01-29T17:41:32.114614Z	info	FLAG: --drainDuration="45s"
2020-01-29T17:41:32.114624Z	info	FLAG: --envoyMetricsServiceAddress=""
2020-01-29T17:41:32.114629Z	info	FLAG: --help="false"
2020-01-29T17:41:32.114635Z	info	FLAG: --id=""
2020-01-29T17:41:32.114643Z	info	FLAG: --ip=""
2020-01-29T17:41:32.114648Z	info	FLAG: --lightstepAccessToken=""
2020-01-29T17:41:32.114658Z	info	FLAG: --lightstepAddress=""
2020-01-29T17:41:32.114663Z	info	FLAG: --lightstepCacertPath=""
2020-01-29T17:41:32.114668Z	info	FLAG: --lightstepSecure="false"
2020-01-29T17:41:32.114674Z	info	FLAG: --log_as_json="false"
2020-01-29T17:41:32.114679Z	info	FLAG: --log_caller=""
2020-01-29T17:41:32.114684Z	info	FLAG: --log_output_level="default:info"
2020-01-29T17:41:32.114697Z	info	FLAG: --log_rotate=""
2020-01-29T17:41:32.114703Z	info	FLAG: --log_rotate_max_age="30"
2020-01-29T17:41:32.114712Z	info	FLAG: --log_rotate_max_backups="1000"
2020-01-29T17:41:32.114718Z	info	FLAG: --log_rotate_max_size="104857600"
2020-01-29T17:41:32.114727Z	info	FLAG: --log_stacktrace_level="default:none"
2020-01-29T17:41:32.114737Z	info	FLAG: --log_target="[stdout]"
2020-01-29T17:41:32.114743Z	info	FLAG: --parentShutdownDuration="1m0s"
2020-01-29T17:41:32.114752Z	info	FLAG: --proxyAdminPort="15000"
2020-01-29T17:41:32.114757Z	info	FLAG: --proxyLogLevel="warning"
2020-01-29T17:41:32.114763Z	info	FLAG: --serviceCluster="test1.&lt;NAMESPACE&gt;"
2020-01-29T17:41:32.114768Z	info	FLAG: --serviceregistry="Kubernetes"
2020-01-29T17:41:32.114774Z	info	FLAG: --statsdUdpAddress=""
2020-01-29T17:41:32.114779Z	info	FLAG: --statusPort="15020"
2020-01-29T17:41:32.114785Z	info	FLAG: --templateFile=""
2020-01-29T17:41:32.114793Z	info	FLAG: --trust-domain=""
2020-01-29T17:41:32.114798Z	info	FLAG: --zipkinAddress="zipkin.istio-system:9411"
2020-01-29T17:41:32.114818Z	info	Version root@1844d064-72cc-11e9-a0d5-0a580a2c0304-docker.io/istio-1.1.6-04850e14d38a69a38c16c800e237b1108056513e-Clean
2020-01-29T17:41:32.115022Z	info	Obtained private IP [10.233.85.30]
2020-01-29T17:41:32.115068Z	info	Proxy role: &amp;model.Proxy{ClusterID:"", Type:"sidecar", IPAddresses:[]string{"10.233.85.30", "10.233.85.30"}, ID:"test1-0.&lt;NAMESPACE&gt;", Locality:(*core.Locality)(nil), DNSDomain:"&lt;NAMESPACE&gt;.svc.cluster.local", ConfigNamespace:"", TrustDomain:"cluster.local", Metadata:map[string]string{}, SidecarScope:(*model.SidecarScope)(nil), ServiceInstances:[]*model.ServiceInstance(nil), WorkloadLabels:model.LabelsCollection(nil)}
2020-01-29T17:41:32.115082Z	info	PilotSAN []string(nil)
2020-01-29T17:41:32.115641Z	info	Effective config: binaryPath: /usr/local/bin/envoy
concurrency: 2
configPath: /etc/istio/proxy
connectTimeout: 10s
discoveryAddress: istio-pilot.istio-system:15010
drainDuration: 45s
parentShutdownDuration: 60s
proxyAdminPort: 15000
serviceCluster: test1.&lt;NAMESPACE&gt;
statNameLength: 189
tracing:
  zipkin:
    address: zipkin.istio-system:9411

2020-01-29T17:41:32.115658Z	info	Monitored certs: []string{"/etc/certs/cert-chain.pem", "/etc/certs/key.pem", "/etc/certs/root-cert.pem"}
2020-01-29T17:41:32.115679Z	info	PilotSAN []string(nil)
2020-01-29T17:41:32.115729Z	info	Opening status port 15020

2020-01-29T17:41:32.115837Z	info	Starting proxy agent
2020-01-29T17:41:32.116185Z	info	Received new config, resetting budget
2020-01-29T17:41:32.116234Z	info	Reconciling retry (budget 10)
2020-01-29T17:41:32.116290Z	info	Epoch 0 starting
2020-01-29T17:41:32.116321Z	info	watching /etc/certs/cert-chain.pem for changes
2020-01-29T17:41:32.116351Z	info	watching /etc/certs/key.pem for changes
2020-01-29T17:41:32.116365Z	info	watching /etc/certs/root-cert.pem for changes
2020-01-29T17:41:32.121095Z	info	Envoy command: [-c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster test1.&lt;NAMESPACE&gt; --service-node sidecar~10.233.85.30~test1-0.&lt;NAMESPACE&gt;~&lt;NAMESPACE&gt;.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warning --concurrency 2]
[2020-01-29 17:41:32.149][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Cluster.hosts' from file cds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:41:32.149][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Cluster.hosts' from file cds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:41:32.149][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Cluster.hosts' from file cds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:41:32.154][25][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:86] gRPC config stream closed: 14, no healthy upstream
[2020-01-29 17:41:32.154][25][warning][config] [bazel-out/k8-opt/bin/external/envoy/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:49] Unable to establish new stream
2020-01-29T17:41:32.941647Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:41:32.941730Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:41:32.941753Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:41:33.949389Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:41:33.949478Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:41:33.949509Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:41:33.949540Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:41:33.950764Z	info	Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 1 successful, 0 rejected; lds updates: 0 successful, 0 rejected
2020-01-29T17:41:35.950419Z	info	Envoy proxy is NOT ready: config not received from Pilot (is Pilot running?): cds updates: 1 successful, 0 rejected; lds updates: 0 successful, 0 rejected
[2020-01-29 17:41:37.227][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:41:37.232][25][warning][filter] [src/envoy/http/authn/http_filter_factory.cc:102] mTLS PERMISSIVE mode is used, connection can be either plaintext or TLS, and client cert can be omitted. Please consider to upgrade to mTLS STRICT mode for more secure configuration that only allows TLS connection with client cert. See https://istio.io/docs/tasks/security/mtls-migration/
[2020-01-29 17:41:37.234][25][warning][filter] [src/envoy/http/authn/http_filter_factory.cc:102] mTLS PERMISSIVE mode is used, connection can be either plaintext or TLS, and client cert can be omitted. Please consider to upgrade to mTLS STRICT mode for more secure configuration that only allows TLS connection with client cert. See https://istio.io/docs/tasks/security/mtls-migration/
2020-01-29T17:41:37.951160Z	info	Envoy proxy is ready
2020-01-29T17:41:42.941990Z	info	watchFileEvents: notifying
[2020-01-29 17:41:46.520][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
2020-01-29T17:43:03.049961Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:43:03.050167Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:43:03.050208Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:43:13.050448Z	info	watchFileEvents: notifying
[2020-01-29 17:44:00.618][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:44:03.820][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:44:22.119][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:44:26.024][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
2020-01-29T17:44:26.999872Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:44:27.000018Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:44:27.000069Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
[2020-01-29 17:44:33.125][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
[2020-01-29 17:44:35.824][25][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon. Please see https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated for details.
2020-01-29T17:44:37.000193Z	info	watchFileEvents: notifying
2020-01-29T17:45:48.024698Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:45:48.024832Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:45:48.024890Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:45:58.025050Z	info	watchFileEvents: notifying
2020-01-29T17:47:07.023200Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:47:07.023452Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:47:07.023487Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:47:17.023553Z	info	watchFileEvents: notifying
2020-01-29T17:48:34.989075Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:48:34.989199Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:48:34.989225Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:48:44.989328Z	info	watchFileEvents: notifying
2020-01-29T17:49:52.063017Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:49:52.063143Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:49:52.063170Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:50:02.063424Z	info	watchFileEvents: notifying
2020-01-29T17:50:54.029104Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:50:54.029193Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:50:54.029220Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:51:04.029441Z	info	watchFileEvents: notifying
2020-01-29T17:52:15.072027Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:52:15.072167Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:52:15.072196Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:52:25.072333Z	info	watchFileEvents: notifying
2020-01-29T17:53:33.064696Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:53:33.064838Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:53:33.064869Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:53:43.065085Z	info	watchFileEvents: notifying
2020-01-29T17:54:50.084390Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:54:50.084489Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:54:50.084505Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:55:00.084688Z	info	watchFileEvents: notifying
2020-01-29T17:56:10.993036Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:56:10.993191Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:56:10.993234Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:56:20.993395Z	info	watchFileEvents: notifying
2020-01-29T17:57:12.965720Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:57:12.965844Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:57:12.965904Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:57:22.965938Z	info	watchFileEvents: notifying
2020-01-29T17:58:15.043033Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:58:15.043475Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:58:15.043552Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:58:25.043457Z	info	watchFileEvents: notifying
2020-01-29T17:59:42.004203Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T17:59:42.004297Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T17:59:42.004315Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
2020-01-29T17:59:52.004475Z	info	watchFileEvents: notifying
2020-01-29T18:01:09.072571Z	info	watchFileEvents: "/etc/certs/cert-chain.pem": MODIFY|ATTRIB
2020-01-29T18:01:09.072718Z	info	watchFileEvents: "/etc/certs/key.pem": MODIFY|ATTRIB
2020-01-29T18:01:09.072745Z	info	watchFileEvents: "/etc/certs/root-cert.pem": MODIFY|ATTRIB
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='Luke035' date='2020-01-31T03:51:56Z'>
		&lt;denchmark-link:https://github.com/Luke035&gt;@Luke035&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vaskokj&gt;@vaskokj&lt;/denchmark-link&gt;
 is the notebook accessible?
To summarize it looks like the issue so far is that the ISTIO side car is failing its readiness probe. But the container appears to still be running.
Is the readiness probe consistently failing? Or is it just failing at startup because it takes some time for everything to be ready?
		</comment>
		<comment id='14' author='Luke035' date='2020-01-31T13:59:09Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

Yes the notebook for me does become accessible.
It just sits in that state for as long as I can tell in the describe. In the kubectl -n &lt;MYNAMESPACE&gt; get pods it does show a Running status.
		</comment>
		<comment id='15' author='Luke035' date='2020-01-31T14:34:04Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

Notebooks are accessible and work fine.
Readiness probe keeps failing but ISTIO-PROXY, for example, is sending metrics correctly. Since this is common for every POD (not just the notebook), my concern is that this is an ISTIO problem or and EKS problem.
		</comment>
		<comment id='16' author='Luke035' date='2020-02-03T17:27:36Z'>
		Downgrading to P1 because since notebooks and Kubeflow are working this is not release blocking.
&lt;denchmark-link:https://github.com/Luke035&gt;@Luke035&lt;/denchmark-link&gt;
 agree it looks like some sort of problem with ISTIO.  Has anyone checked to see if this some sort of known issue with ISTIO?
		</comment>
		<comment id='17' author='Luke035' date='2020-03-25T07:30:22Z'>
		I am facing same problem.
I deployed centos Deployment and confirmed that Pod's IP is different from probe's target IP(kubeflow v1.0 build on on-premiss k8s cluster v1.15.11).
As &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 said, it looks like some bugs in ISTIO.
&lt;denchmark-code&gt;
$kubectl get ev -n my-workspace |grep probe
20mÂ Â Â Â Â Â Â Â  WarningÂ Â  UnhealthyÂ Â Â Â Â Â Â Â Â Â  pod/centos-7bc6d8c645-96z44Â Â Â  Readiness probe failed: HTTP probe failed with statuscode:Â 503
19mÂ Â Â Â Â Â Â Â  WarningÂ Â  UnhealthyÂ Â Â Â Â Â Â Â Â Â  pod/centos-7bc6d8c645-96z44Â Â Â  Readiness probe failed: Get http://10.244.2.38:15020/healthz/ready: dial tcp 10.244.2.38:15020: connect: connection refused



$kubectl get pod -n my-workspace -o wide
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  READYÂ Â  STATUSÂ Â Â Â Â Â Â Â Â Â Â Â  RESTARTSÂ Â  AGEÂ Â  IPÂ Â Â Â Â Â Â Â Â Â Â  NODEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  NOMINATED NODEÂ Â  READINESS GATES
centos-fbfdd45d8-6sfvd   1/2     CrashLoopBackOff   8          18m   10.244.2.39   node1.example.com    &lt;none&gt;           &lt;none&gt;Â Â Â Â Â 
--



&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='Luke035' date='2020-04-14T22:21:06Z'>
		I just deployed v1.0.1 &lt;denchmark-link:https://github.com/kubeflow/kfctl/releases/download/v1.0.1/kfctl_v1.0.1-0-gf3edb9b_linux.tar.gz&gt;kfctl &lt;/denchmark-link&gt;
with the v1.0.1 &lt;denchmark-link:https://raw.githubusercontent.com/kubeflow/manifests/d6b267e80b71007f95501eb92d4b961b9dab50f8/kfdef/kfctl_k8s_istio.v1.0.1.yaml%22&gt;manifest &lt;/denchmark-link&gt;
 for istio on-prem.
I am running on K8S v1.15.3 deployed via Kubespray.
I am hitting this issue with all of the default/included GCR containers. The containers do not become accessible, I am consisntently getting "no healthy upstream".
If I use my own &lt;denchmark-link:https://github.com/NVIDIA/deepops/blob/master/containers/ngc/tensorflow/Dockerfile&gt;container &lt;/denchmark-link&gt;
 everything works fine.
&lt;denchmark-code&gt;Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  2m41s              default-scheduler  Successfully assigned anonymous/test-0 to gpu02
  Normal   Pulled     2m40s              kubelet, gpu02     Container image "docker.io/istio/proxy_init:1.1.6" already present on machine
  Normal   Created    2m39s              kubelet, gpu02     Created container istio-init
  Normal   Started    2m39s              kubelet, gpu02     Started container istio-init
  Normal   Pulling    2m38s              kubelet, gpu02     Pulling image "gcr.io/kubeflow-images-public/tensorflow-1.15.2-notebook-gpu:1.0.0"
  Normal   Pulled     89s                kubelet, gpu02     Successfully pulled image "gcr.io/kubeflow-images-public/tensorflow-1.15.2-notebook-gpu:1.0.0"
  Normal   Pulled     88s                kubelet, gpu02     Container image "docker.io/istio/proxyv2:1.1.6" already present on machine
  Normal   Started    87s                kubelet, gpu02     Started container istio-proxy
  Normal   Created    87s                kubelet, gpu02     Created container istio-proxy
  Warning  Unhealthy  86s                kubelet, gpu02     Readiness probe failed: HTTP probe failed with statuscode: 503
  Normal   Started    43s (x4 over 88s)  kubelet, gpu02     Started container test
  Normal   Pulled     43s (x3 over 87s)  kubelet, gpu02     Container image "gcr.io/kubeflow-images-public/tensorflow-1.15.2-notebook-gpu:1.0.0" already present on machine
  Normal   Created    43s (x4 over 89s)  kubelet, gpu02     Created container test
  Warning  BackOff    42s (x5 over 86s)  kubelet, gpu02     Back-off restarting failed container

&lt;/denchmark-code&gt;

Edit:
Scratch that, this looks like it might be an issue with the actual container or some other aspect of Kubeflow. I created this using no workspace volume and there seems to be a home directory permissions issue.
During handling of the above exception, another exception occurred:
&lt;denchmark-code&gt;$kubectl logs  -n anonymous test-0 test
...
Traceback (most recent call last):
  File "/usr/local/bin/jupyter-notebook", line 8, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/jupyter_core/application.py", line 268, in launch_instance
    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py", line 663, in launch_instance
    app.initialize(argv)
  File "&lt;/usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-7&gt;", line 2, in initialize
  File "/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py", line 87, in catch_config_error
    return method(app, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/notebook/notebookapp.py", line 1766, in initialize
    self.init_configurables()
  File "/usr/local/lib/python3.6/dist-packages/notebook/notebookapp.py", line 1380, in init_configurables
    connection_dir=self.runtime_dir,
  File "/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py", line 556, in __get__
    return self.get(obj, cls)
  File "/usr/local/lib/python3.6/dist-packages/traitlets/traitlets.py", line 535, in get
    value = self._validate(obj, dynamic_default())
  File "/usr/local/lib/python3.6/dist-packages/jupyter_core/application.py", line 99, in _runtime_dir_default
    ensure_dir_exists(rd, mode=0o700)
  File "/usr/local/lib/python3.6/dist-packages/jupyter_core/utils/__init__.py", line 13, in ensure_dir_exists
    os.makedirs(path, mode=mode)
  File "/usr/lib/python3.6/os.py", line 210, in makedirs
    makedirs(head, mode, exist_ok)
  File "/usr/lib/python3.6/os.py", line 210, in makedirs
    makedirs(head, mode, exist_ok)
  File "/usr/lib/python3.6/os.py", line 210, in makedirs
    makedirs(head, mode, exist_ok)
  File "/usr/lib/python3.6/os.py", line 220, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/home/jovyan/.local'

&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='Luke035' date='2020-05-18T16:19:21Z'>
		I am having the same problem as &lt;denchmark-link:https://github.com/supertetelman&gt;@supertetelman&lt;/denchmark-link&gt;
 - mounted without any volume the home directory for the jovyan user is all owned by root - which prevents launching the notebook since jovyan lacks permissions to alter root user owned files and directories.
Working on migrating my existing notebook container build to meet some of the launch criteria in the interim.
		</comment>
		<comment id='20' author='Luke035' date='2020-05-29T15:02:10Z'>
		&lt;denchmark-link:https://github.com/humantargetjoe&gt;@humantargetjoe&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/supertetelman&gt;@supertetelman&lt;/denchmark-link&gt;
 see &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4863&gt;#4863&lt;/denchmark-link&gt;
 your issue is different from this issue about istio side car readiness.
		</comment>
		<comment id='21' author='Luke035' date='2020-08-28T03:08:35Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>