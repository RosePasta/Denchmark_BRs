<bug id='4864' author='everett-moon' open_date='2020-03-16T07:18:10Z' closed_time='2020-03-27T03:42:05Z'>
	<summary>ERROR 1049 (42000): Unknown database 'katib'</summary>
	<description>
/kind bug
What steps did you take and what happened:
[A clear and concise description of what the bug is.]
I have install kubeflow via kfctl_k8s_istio.v1.0.1.yaml on existed kubernetes cluster v1.14.4.
all pod were running properly except two: katib-mysql and katib-db-manager
[vimer@xushangming-dev ~]$ kubectl  -n kubeflow describe pod  katib-mysql-57884cb488-6gg98
Name:               katib-mysql-57884cb488-6gg98
Namespace:          kubeflow
Priority:           0
PriorityClassName:  
Node:               k8s-slave02/192.168.10.186
Start Time:         Mon, 16 Mar 2020 14:25:31 +0800
Labels:             app=katib
app.kubernetes.io/component=katib
app.kubernetes.io/instance=katib-controller-0.8.0
app.kubernetes.io/managed-by=kfctl
app.kubernetes.io/name=katib-controller
app.kubernetes.io/part-of=kubeflow
app.kubernetes.io/version=0.8.0
component=mysql
pod-template-hash=57884cb488
Annotations:        cni.projectcalico.org/podIP: 10.244.2.123/32
sidecar.istio.io/inject: false
Status:             Running
IP:                 10.244.2.123
Controlled By:      ReplicaSet/katib-mysql-57884cb488
Containers:
katib-mysql:
Container ID:  docker://e2bb6b7bc9baa2402dfac6ee7c14b4bc67f2ef084590a81d8c3c0303128e888f
Image:         mysql:8
Image ID:      docker-pullable://mysql@sha256:4a30434ce03d2fa396d0414f075ad9ca9b0b578f14ea5685e24dcbf789450a2c
Port:          3306/TCP
Host Port:     0/TCP
Args:
--datadir
/var/lib/mysql/datadir
State:          Running
Started:      Mon, 16 Mar 2020 14:25:34 +0800
Ready:          False
Restart Count:  0
Liveness:       exec [/bin/bash -c mysqladmin ping -u root -p${MYSQL_ROOT_PASSWORD}] delay=30s timeout=5s period=10s #success=1 #failure=3
Readiness:      exec [/bin/bash -c mysql -D ${MYSQL_DATABASE} -u root -p${MYSQL_ROOT_PASSWORD} -e 'SELECT 1'] delay=5s timeout=1s period=10s #success=1 #failure=3
Environment:
MYSQL_ROOT_PASSWORD:         &lt;set to the key 'MYSQL_ROOT_PASSWORD' in secret 'katib-mysql-secrets'&gt;  Optional: false
MYSQL_ALLOW_EMPTY_PASSWORD:  true
MYSQL_DATABASE:              katib
Mounts:
/var/lib/mysql from katib-mysql (rw)
/var/run/secrets/kubernetes.io/serviceaccount from default-token-rhqvt (ro)
Conditions:
Type              Status
Initialized       True
Ready             False
ContainersReady   False
PodScheduled      True
Volumes:
katib-mysql:
Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
ClaimName:  katib-mysql
ReadOnly:   false
default-token-rhqvt:
Type:        Secret (a volume populated by a Secret)
SecretName:  default-token-rhqvt
Optional:    false
QoS Class:       BestEffort
Node-Selectors:  
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
node.kubernetes.io/unreachable:NoExecute for 300s
Events:
Type     Reason     Age                    From                  Message
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Normal   Scheduled  37m                    default-scheduler     Successfully assigned kubeflow/katib-mysql-57884cb488-6gg98 to k8s-slave02
Normal   Pulled     37m                    kubelet, k8s-slave02  Container image "mysql:8" already present on machine
Normal   Created    37m                    kubelet, k8s-slave02  Created container katib-mysql
Normal   Started    37m                    kubelet, k8s-slave02  Started container katib-mysql
Warning  Unhealthy  2m19s (x211 over 37m)  kubelet, k8s-slave02  Readiness probe failed: mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)
[vimer@xushangming-dev kubeflow]$ kubectl logs katib-db-manager-64f548b47c-p76vb -n kubeflow
E0316 07:13:33.545869       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:38.559161       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:43.560064       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:48.560665       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:53.561246       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:58.563278       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:03.565287       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:08.566754       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:13.567753       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:18.568420       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:23.569090       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
F0316 07:14:23.569212       1 main.go:83] Failed to open db connection: DB open failed: Timeout waiting for DB conn successfully opened.
goroutine 1 [running]:
github.com/kubeflow/katib/vendor/k8s.io/klog.stacks(0xc000238200, 0xc000250000, 0x89, 0xc0)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:830 +0xb8
github.com/kubeflow/katib/vendor/k8s.io/klog.(*loggingT).output(0xdf1ca0, 0xc000000003, 0xc00024e000, 0xd93a76, 0x7, 0x53, 0x0)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:781 +0x2d0
github.com/kubeflow/katib/vendor/k8s.io/klog.(*loggingT).printf(0xdf1ca0, 0x3, 0x9b448c, 0x20, 0xc0001b5f20, 0x1, 0x1)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:678 +0x14b
github.com/kubeflow/katib/vendor/k8s.io/klog.Fatalf(...)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:1209
main.main()
/go/src/github.com/kubeflow/katib/cmd/db-manager/v1alpha3/main.go:83 +0x165
[vimer@xushangming-dev kubeflow]$
[vimer@xushangming-dev kubeflow]$ kubectl logs -n kubeflow katib-mysql-57884cb488-6gg98
2020-03-16 06:25:34+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-16 06:25:34+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-16 06:25:34+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-16T06:25:34.763839Z 0 [Warning] [MY-011070] [Server] 'Disabling symbolic links using --skip-symbolic-links (or equivalent) is the default. Consider not using this option as it' is deprecated and will be removed in a future release.
2020-03-16T06:25:34.765274Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.19) starting as process 1
2020-03-16T06:25:36.204511Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2020-03-16T06:25:36.239657Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
2020-03-16T06:25:36.283798Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.19'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
2020-03-16T06:25:36.428562Z 0 [ERROR] [MY-011294] [Server] Plugin mysqlx reported: 'Unable to use user mysql.session account when connecting the server for internal plugin requests.'
2020-03-16T06:25:36.428752Z 0 [ERROR] [MY-011301] [Server] Plugin mysqlx reported: 'Unable to switch context to user mysql.session'
[vimer@xushangming-dev kubeflow]$
What did you expect to happen:
[vimer@xushangming-dev kubeflow]$ kubectl logs katib-db-manager-64f548b47c-p76vb -n kubeflow
E0316 07:13:33.545869       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:38.559161       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:43.560064       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:48.560665       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:53.561246       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:13:58.563278       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:03.565287       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:08.566754       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:13.567753       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:18.568420       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
E0316 07:14:23.569090       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.97.244.59:3306: i/o timeout
F0316 07:14:23.569212       1 main.go:83] Failed to open db connection: DB open failed: Timeout waiting for DB conn successfully opened.
goroutine 1 [running]:
github.com/kubeflow/katib/vendor/k8s.io/klog.stacks(0xc000238200, 0xc000250000, 0x89, 0xc0)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:830 +0xb8
github.com/kubeflow/katib/vendor/k8s.io/klog.(*loggingT).output(0xdf1ca0, 0xc000000003, 0xc00024e000, 0xd93a76, 0x7, 0x53, 0x0)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:781 +0x2d0
github.com/kubeflow/katib/vendor/k8s.io/klog.(*loggingT).printf(0xdf1ca0, 0x3, 0x9b448c, 0x20, 0xc0001b5f20, 0x1, 0x1)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:678 +0x14b
github.com/kubeflow/katib/vendor/k8s.io/klog.Fatalf(...)
/go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:1209
main.main()
/go/src/github.com/kubeflow/katib/cmd/db-manager/v1alpha3/main.go:83 +0x165
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard):
kfctl version: (use kfctl version): kfctl v1.0.1-0-gf3edb9b
Kubernetes platform: (e.g. minikube)  docker
Kubernetes version: (use kubectl version): v1.14.4
OS (e.g. from /etc/os-release): CentOS Linux release 7.7.1908 (Core)

	</description>
	<comments>
		<comment id='1' author='everett-moon' date='2020-03-16T07:18:57Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.94



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='everett-moon' date='2020-03-16T07:32:38Z'>
		I have redeployed kubeflow by command kfctl apply -V -f kfctl_k8s_istio.v1.0.1.yaml after getting the full name list of necessary docker images and downloading them manually.
How can I fix this problem?
		</comment>
		<comment id='3' author='everett-moon' date='2020-03-16T09:23:03Z'>
		After I attached into the katib-mysql container, I found that the password of mysql is unset,
then I set it manully with the default value.
"mysql -D ${MYSQL_DATABASE} -u root -p${MYSQL_ROOT_PASSWORD} -e 'SELECT 1'"
value of variable MYSQL_DATABASE is katib which didn't exist in database? All the related PVs claimed by glusterfs storage class seems working properly. This really confused me. Can anyone explain it for me ? Think you.
		</comment>
		<comment id='4' author='everett-moon' date='2020-03-16T12:46:38Z'>
		/cc &lt;denchmark-link:https://github.com/hougangliu&gt;@hougangliu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gaocegege&gt;@gaocegege&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='everett-moon' date='2020-03-16T15:40:14Z'>
		@Lordfourteen For some reason mysql didn't run properly, so katib-db-manager can't make connection to mysql db, create Katib db, etc.
Can you try to change imagePullPolicy: Always in katib-mysql deployment in Kubeflow namespace. It will create mysql pod again. Check mysql pod logs after it.
Also, check if you created secret with Katib DB password while deploying Kubeflow.
		</comment>
		<comment id='6' author='everett-moon' date='2020-03-17T02:34:58Z'>
		&lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
, I have changed  to  already, it's not a imagePullErr,
and the  have been created in the namespace  already.
		</comment>
		<comment id='7' author='everett-moon' date='2020-03-17T06:32:36Z'>
		&lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
 , Hi, I have noticed that there were some extra operations to done while check the logs of mysql pod , thus I entered following comands:
&lt;denchmark-code&gt;  /usr/bin/mysqladmin -u root password 'test'
  /usr/bin/mysqladmin -u root -h mysql-598bc897dc-znrlm password 'test'
&lt;/denchmark-code&gt;

After restart the katib-mysql pod, it complains me that MYSQL_DATABASE katib doesn't exist.
I'm quite a novice for Katib, and I also don't get any instructions in the documents about it,
How can I setup it properly?
Logs:
&lt;denchmark-code&gt;    Liveness:       exec [/bin/bash -c mysqladmin ping -u root -p${MYSQL_ROOT_PASSWORD}] delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:      exec [/bin/bash -c mysql -D ${MYSQL_DATABASE} -u root -p${MYSQL_ROOT_PASSWORD} -e 'SELECT 1'] delay=5s timeout=1s period=10s #success=1 #failure=3

**ERROR 1049 (42000): Unknown database 'katib'**
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='everett-moon' date='2020-03-17T12:06:05Z'>
		@Lordfourteen Katib db manager makes connection to Katib mysql DB and creates Katib db there, etc. You don't need to run any commands manually on mysql DB. Katib db manager should create eveything for you.
Can you show me output of Katib mysql service:
kubectl get svc katib-mysql -n kubeflow
Also describe Katib db manager pod:
kubectl describe pod &lt;katib-db-manager pod name&gt; -n kubeflow
		</comment>
		<comment id='9' author='everett-moon' date='2020-03-17T14:25:50Z'>
		I have been having the same issue. Like @Lordfourteen I edited the root user. I have also tried creating the katib database and add more privileges (host=%) to the root account in the katib-mysql pod. This makes katib-mysql work, but katib-db-manager still fails.
Now, when trying to running mysql commands (e.g.: show databases;) in katib-mysql I get the following error:
ERROR 1449 (HY000): The user specified as a definer ('mysql.infoschema'@'localhost') does not exist.
I have attached my output for the commands mentioned by &lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
 here:
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4343844/kubectl-logs.txt&gt;kubectl-logs.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='everett-moon' date='2020-03-17T16:49:54Z'>
		&lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 I think you have problem with  deployment. While searching, I saw that your error can happen when you downgrading mySQL versions.
Can you try to deploy just katib-mysql fresh deployment with pulling mysql:8 image and make sure that it is running without any errors?
You don't need to deploy katib-db-manager
		</comment>
		<comment id='11' author='everett-moon' date='2020-03-18T01:16:03Z'>
		&lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
 Sure,

This is the details of katib-mysql service:

&lt;denchmark-code&gt;[vimer@xushangming-dev kubeflow]$ kubectl get svc katib-mysql -n kubeflow
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
katib-mysql   ClusterIP   10.97.244.59   &lt;none&gt;        3306/TCP   47h
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"katib","app.kubernetes.io/component":"katib","app.kubernetes.io/instance":"katib-controller-0.8.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"katib-controller","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.8.0","component":"mysql"},"name":"katib-mysql","namespace":"kubeflow"},"spec":{"ports":[{"name":"dbapi","port":3306,"protocol":"TCP"}],"selector":{"app":"katib","app.kubernetes.io/component":"katib","app.kubernetes.io/instance":"katib-controller-0.8.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"katib-controller","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"0.8.0","component":"mysql"},"type":"ClusterIP"}}
  creationTimestamp: "2020-03-16T01:36:35Z"
  labels:
    app: katib
    app.kubernetes.io/component: katib
    app.kubernetes.io/instance: katib-controller-0.8.0
    app.kubernetes.io/managed-by: kfctl
    app.kubernetes.io/name: katib-controller
    app.kubernetes.io/part-of: kubeflow
    app.kubernetes.io/version: 0.8.0
    component: mysql
  name: katib-mysql
  namespace: kubeflow
  resourceVersion: "600988"
  selfLink: /api/v1/namespaces/kubeflow/services/katib-mysql
  uid: 9290878d-6726-11ea-9875-fcaa14938ed4
spec:
  clusterIP: 10.97.244.59
  ports:
  - name: dbapi
    port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    app: katib
    app.kubernetes.io/component: katib
    app.kubernetes.io/instance: katib-controller-0.8.0
    app.kubernetes.io/managed-by: kfctl
    app.kubernetes.io/name: katib-controller
    app.kubernetes.io/part-of: kubeflow
    app.kubernetes.io/version: 0.8.0
    component: mysql
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
&lt;/denchmark-code&gt;


This the outputs of descripe katib-mysql pod:

&lt;denchmark-code&gt;[vimer@xushangming-dev kubeflow]$ kubectl describe pod katib-mysql-57884cb488-c6q5n -n kubeflow
Name:               katib-mysql-57884cb488-c6q5n
Namespace:          kubeflow
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               xushangming-dev/192.168.10.205
Start Time:         Tue, 17 Mar 2020 14:10:46 +0800
Labels:             app=katib
                    app.kubernetes.io/component=katib
                    app.kubernetes.io/instance=katib-controller-0.8.0
                    app.kubernetes.io/managed-by=kfctl
                    app.kubernetes.io/name=katib-controller
                    app.kubernetes.io/part-of=kubeflow
                    app.kubernetes.io/version=0.8.0
                    component=mysql
                    pod-template-hash=57884cb488
Annotations:        cni.projectcalico.org/podIP: 10.244.0.74/32
                    sidecar.istio.io/inject: false
Status:             Running
IP:                 10.244.0.74
Controlled By:      ReplicaSet/katib-mysql-57884cb488
Containers:
  katib-mysql:
    Container ID:  docker://f500ff09ff40b7c96135fcfa105cc53c64fdc645c50dd61816d7c8a12b47d412
    Image:         mysql:8
    Image ID:      docker-pullable://mysql@sha256:4a30434ce03d2fa396d0414f075ad9ca9b0b578f14ea5685e24dcbf789450a2c
    Port:          3306/TCP
    Host Port:     0/TCP
    Args:
      --datadir
      /var/lib/mysql/datadir
    State:          Running
      Started:      Tue, 17 Mar 2020 14:10:50 +0800
    Ready:          False
    Restart Count:  0
    Liveness:       exec [/bin/bash -c mysqladmin ping -u root -p${MYSQL_ROOT_PASSWORD}] delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:      exec [/bin/bash -c mysql -D ${MYSQL_DATABASE} -u root -p${MYSQL_ROOT_PASSWORD} -e 'SELECT 1'] delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:
      MYSQL_ROOT_PASSWORD:         &lt;set to the key 'MYSQL_ROOT_PASSWORD' in secret 'katib-mysql-secrets'&gt;  Optional: false
      MYSQL_ALLOW_EMPTY_PASSWORD:  true
      MYSQL_DATABASE:              katib
    Mounts:
      /var/lib/mysql from katib-mysql (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rhqvt (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  katib-mysql:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  katib-mysql
    ReadOnly:   false
  default-token-rhqvt:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-rhqvt
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                     From                      Message
  ----     ------     ----                    ----                      -------
  Warning  Unhealthy  3m56s (x6780 over 18h)  kubelet, xushangming-dev  Readiness probe failed: mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1049 (42000): Unknown database 'katib'
&lt;/denchmark-code&gt;

Beside, I'm quited confused that there are two pods in the namespaces kubeflow, one is for mysql
using docker image mysql:5.6 while another for katib-mysql  using mysql:8 instead. Why do kubeflow use two different version of mysql?
		</comment>
		<comment id='12' author='everett-moon' date='2020-03-18T02:20:07Z'>
		&lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;


Katib db manager makes connection to Katib mysql DB and creates Katib db there, etc. You don't need to run any commands manually on mysql DB. Katib db manager should create eveything for you.

I'm not quite sure if I can think of it this way.  First of all, when pod of katib-mysql startup, mysql8.0 didn't have original password. Then Katib db manager will connect to mysql to finish this operation. and when Katib db manager successed to connect to mysql, it would create katib db there.
Current problem is that katib db manager couldn't connect to it. Is there anything wrong with katib db manager?
I want to know that which component is responsible to setup the original password of mysql.
		</comment>
		<comment id='13' author='everett-moon' date='2020-03-18T02:25:03Z'>
		&lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 If you have solved this problem, notice to me please.
		</comment>
		<comment id='14' author='everett-moon' date='2020-03-18T09:37:22Z'>
		Just to clarify:

I have not upgraded/downgraded mysql manually
Using NFS for persistent volumes, no problems with other pods (nfs-client-provisioner). Although there were some issues when setting it up intitially
I have deleted all the persistent volumes before redeploying

Given &lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
 suggestion I have tried:

Removing mysql images from docker docker rmi mysql:{tag} -f on all nodes (3 in my case)
Delete / redeploy kubeflow

Next deploy only katib-mysql

kfctl build -V -f {config_uri}
manually created pv &amp; pvc (hostpath)
Deploy katib-secrets: kubectl apply -f kustomize/katib-controller/base/katib-mysql-secret.yaml
Deploy katib-mysql kubectl apply -f kustomize/katib-controller/base/katib-mysql-deployment.yaml

Which works! :)
Trying deploying kubeflow again, same errors (access denied, database doesn't exist).
So reading some time ago that this could be related to slow storage and timeouts and having it work with a non-nfs persistent volume, just for testing purposes I edited katib-mysql-pvc to use a local ssd instead, like this: &lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4348101/katib-mysql-pvc.yaml.txt&gt;katib-mysql-pvc.yaml.txt&lt;/denchmark-link&gt;
. Now every pod runs without issue.
@Lordfourteen as you are using glusterfs, maybe it would be interesting to try using local storage as well, to make sure that we have the same issue and mine is not a nfs specific issue (as there are specific nfs issues like the ones reported here for example: &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4827&gt;#4827&lt;/denchmark-link&gt;
 ).
Thinking it could have something to do with timing and timeouts, I changed the timeoutSeconds in katib-mysql-deployment.yaml and katib-db-manager-deployment.yaml from 1 to 10 seconds. But that has no effect.
Probably unrelated but at first I (misguidedly) tried to  to get a yaml file to deploy katib-mysql with, but this fails because  is not recognized: &lt;denchmark-link:https://github.com/kubernetes-sigs/kustomize/issues/1069&gt;kubernetes-sigs/kustomize#1069&lt;/denchmark-link&gt;
 This is fixed by
&lt;denchmark-code&gt;  envs: 
  - params.env
&lt;/denchmark-code&gt;

I don't know whether that is an actual issue or a mistake on my part for using kustomize the wrong way.
		</comment>
		<comment id='15' author='everett-moon' date='2020-03-19T13:36:30Z'>
		&lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 I am sorry to reply you until now. I will try this way later.
		</comment>
		<comment id='16' author='everett-moon' date='2020-03-19T16:05:14Z'>
		
Beside, I'm quited confused that there are two pods in the namespaces kubeflow, one is for mysql
using docker image mysql:5.6 while another for katib-mysql using mysql:8 instead. Why do kubeflow use two different version of mysql?

In kubeflow deployment  pod with mysql:5.6 is used for another project, not related to Katib.
		</comment>
		<comment id='17' author='everett-moon' date='2020-03-19T16:12:59Z'>
		&lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 If you run only  deployment without , everything is running, right?
PVC  is bounded also?
After that, when you try to deploy katib-db-manager, you can see errors in that pod ?
		</comment>
		<comment id='18' author='everett-moon' date='2020-03-19T16:52:26Z'>
		Concerning your first question. Yes, everything runs when just deploying
katib-mysql. There are no issues as far as I know with pvc neither when
using nfs nor local pvs.

I skipped the step of deploying just Katib-dB-manager and immediately went
for “full kubeflow”. I am going to try just deploying Katib-db-manager
after Katib-MySQL starts later today and get back to you.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, 19 Mar 2020 at 17:13, Andrey Velichkevich ***@***.***&gt; wrote:
 @mvwestendorp &lt;https://github.com/mvwestendorp&gt; If you run only
 katib-mysql deployment without katib-db-manager, everything is running,
 right?
 PVC katib-mysq is bounded also?

 After that, when you try to deploy katib-db-manager, you can see errors
 in that pod ?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#4864 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AAG4L2POSSUHDKFZTN67BNTRIJABVANCNFSM4LL3QVCA&gt;
 .



		</comment>
		<comment id='19' author='everett-moon' date='2020-03-20T01:18:29Z'>
		&lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
  all pvs are bound successfully, and both   and  are not in ready states.
The following content is liveness and readiness of  pod katib-mysql , So I think pod wouldn't work as expect.
&lt;denchmark-code&gt;    Liveness:       exec [/bin/bash -c mysqladmin ping -u root -p${MYSQL_ROOT_PASSWORD}] delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:      exec [/bin/bash -c mysql -D ${MYSQL_DATABASE} -u root -p${MYSQL_ROOT_PASSWORD} -e 'SELECT 1'] delay=5s timeout=1s period=10s #success=1 #failure=3
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;[vimer@xushangming-dev ~]$ kubectl get pv -n kubeflow
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS        REASON   AGE
pvc-820e622f-6726-11ea-9875-fcaa14938ed4   10Gi       RWO            Delete           Bound    kubeflow/metadata-mysql   glusterfs-storage            3d23h
pvc-957d1b85-6726-11ea-9875-fcaa14938ed4   20Gi       RWO            Delete           Bound    kubeflow/minio-pv-claim   glusterfs-storage            3d23h
pvc-95f3c826-6726-11ea-9875-fcaa14938ed4   20Gi       RWO            Delete           Bound    kubeflow/mysql-pv-claim   glusterfs-storage            3d23h
pvc-e1858e09-6801-11ea-9875-fcaa14938ed4   10Gi       RWO            Delete           Bound    kubeflow/katib-mysql      glusterfs-storage            2d21h
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='everett-moon' date='2020-03-20T03:50:35Z'>
		I have checked the entrypoint script docker-entrypoint.sh in the mysql:8, it seems to be stuck while setting up.
		</comment>
		<comment id='21' author='everett-moon' date='2020-03-20T07:27:15Z'>
		&lt;denchmark-link:https://github.com/andreyvelich&gt;@andreyvelich&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 It is a problem of docker image , the entrypoint don't startup  successfully.
I haved tried:

attach the running pod by docker exec, then setup root password with test
after connecting to mysql, I found the database created unsuccessfully, so I created it manually

Then the pod  of katib-mysql is in ready status. If I configured it with correct access permission, I think the pod of katib-db-manager would work as well.
&lt;denchmark-code&gt;katib-mysql-57884cb488-mnrgh                                   1/1     Running            1          5h4m

[vimer@xushangming-dev kubeflow]$ kubectl logs katib-db-manager-64f548b47c-4p6mk -n kubeflow
E0320 07:07:32.068131       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:37.065604       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:42.066101       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:47.065011       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:52.065393       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:57.065777       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:08:02.065504       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
&lt;/denchmark-code&gt;

When I try to run the container manually, I get following error:
&lt;denchmark-code&gt;[vimer@xushangming-dev kubeflow]$ docker run -it mysql:8 -e MYSQL_DATABASE=katib MYSQL_ROOT_PASSWORD=test MYSQL_ALLOW_EMPTY_PASSWORD="true"
2020-03-20 08:33:08+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-20 08:33:08+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-20 08:33:08+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-20 08:33:09+00:00 [ERROR] [Entrypoint]: Database is uninitialized and password option is not specified
        You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD
&lt;/denchmark-code&gt;

This is the actual way how the deployment katib-mysql creates the pod. I have checked twice the script, I don't find any problem.
I have resolved this problem by replacing mysql docker image tags from 8 to 8.0.3.
The metadata-db and katib-mysql use two different tags.
		</comment>
		<comment id='22' author='everett-moon' date='2020-03-20T09:27:56Z'>
		
If you run only katib-mysql deployment without katib-db-manager, everything is running, right?
PVC katib-mysql is bounded also?

Two scenarios

Hostpath: Yes, if I use a hostpath pv on the node itself then it works.
NFS: Bounded pv and pvc if I use a nfs share, but it fails giving the following:

&lt;denchmark-code&gt;kubectl describe pod/katib-mysql-6bb9f765f9-2w8lt
Name:           katib-mysql-6bb9f765f9-2w8lt
Namespace:      default
Priority:       0
Node:           node3/192.168.2.198
Start Time:     Fri, 20 Mar 2020 09:38:52 +0100
Labels:         app=katib
                component=mysql
                pod-template-hash=6bb9f765f9
Annotations:    sidecar.istio.io/inject: false
Status:         Running
IP:             10.233.92.216
Controlled By:  ReplicaSet/katib-mysql-6bb9f765f9
Containers:
  katib-mysql:
    Container ID:  docker://8255dd87c3eea6f6dfc0762864ba2a35fd46158b4399504ee67869ad6ffea0c8
    Image:         mysql:8
    Image ID:      docker-pullable://mysql@sha256:4a30434ce03d2fa396d0414f075ad9ca9b0b578f14ea5685e24dcbf789450a2c
    Port:          3306/TCP
    Host Port:     0/TCP
    Args:
      --datadir
      /var/lib/mysql/datadir
    State:          Running
      Started:      Fri, 20 Mar 2020 09:40:15 +0100
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Fri, 20 Mar 2020 09:38:54 +0100
      Finished:     Fri, 20 Mar 2020 09:40:15 +0100
    Ready:          False
    Restart Count:  1
    Liveness:       exec [/bin/bash -c mysqladmin ping -u root -p${MYSQL_ROOT_PASSWORD}] delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:      exec [/bin/bash -c mysql -D ${MYSQL_DATABASE} -u root -p${MYSQL_ROOT_PASSWORD} -e 'SELECT 1'] delay=30s timeout=10s period=30s #success=1 #failure=3
    Environment:
      MYSQL_ROOT_PASSWORD:         &lt;set to the key 'MYSQL_ROOT_PASSWORD' in secret 'katib-mysql-secrets'&gt;  Optional: false
      MYSQL_ALLOW_EMPTY_PASSWORD:  true
      MYSQL_DATABASE:              katib
    Mounts:
      /var/lib/mysql from katib-mysql (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nxjpr (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  katib-mysql:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  katib-mysql
    ReadOnly:   false
  default-token-nxjpr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-nxjpr
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  &lt;unknown&gt;              default-scheduler  Successfully assigned default/katib-mysql-6bb9f765f9-2w8lt to node3
  Warning  Unhealthy  3m30s (x3 over 3m50s)  kubelet, node3     Liveness probe failed: mysqladmin: [Warning] Using a password on the command line interface can be insecure.
mysqladmin: connect to server at 'localhost' failed
error: 'Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)'
Check that mysqld is running and that the socket: '/var/run/mysqld/mysqld.sock' exists!
  Normal   Killing    3m30s                 kubelet, node3  Container katib-mysql failed liveness probe, will be restarted
  Warning  Unhealthy  3m7s (x2 over 3m37s)  kubelet, node3  Readiness probe failed: mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)
  Normal   Pulled     2m59s (x2 over 4m21s)  kubelet, node3  Container image "mysql:8" already present on machine
  Normal   Created    2m59s (x2 over 4m21s)  kubelet, node3  Created container katib-mysql
  Normal   Started    2m59s (x2 over 4m20s)  kubelet, node3  Started container katib-mysql
  Warning  Unhealthy  7s (x5 over 2m7s)      kubelet, node3  Readiness probe failed: mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;kubectl logs pod/katib-mysql-6bb9f765f9-2w8lt
2020-03-20 08:40:15+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-20 08:40:15+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-20 08:40:15+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-20T08:40:16.481684Z 0 [Warning] [MY-011070] [Server] 'Disabling symbolic links using --skip-symbolic-links (or equivalent) is the default. Consider not using this option as it' is deprecated and will be removed in a future release.
2020-03-20T08:40:16.481776Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.19) starting as process 1

InnoDB: Progress in percents: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 672020-03-20T08:40:21.989960Z 0 [System] [MY-010229] [Server] Starting XA crash recovery...
 682020-03-20T08:40:22.001587Z 0 [System] [MY-010232] [Server] XA crash recovery finished.
 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 1002020-03-20T08:40:22.844095Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2020-03-20T08:40:22.855753Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
2020-03-20T08:40:22.875803Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.19'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server - GPL.
2020-03-20T08:40:22.895526Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: '/var/run/mysqld/mysqlx.sock' bind-address: '::' port: 33060
&lt;/denchmark-code&gt;


After that, when you try to deploy katib-db-manager, you can see errors in that pod ?

I got to it just now. So, after katib-mysql runs using a hostpath pv I do the following:
kubectl apply -f kustomize/katib-controller/base/katib-db-manager-deployment.yaml
This gives me the following in the logs (not much else)
mysql.go:62] Ping to Katib db failed: dial tcp: lookup katib-mysql on 169.254.25.10:53: no such host
and
&lt;denchmark-code&gt; kubectl describe pod/katib-db-manager-7698986bc7-jvnmv
Name:           katib-db-manager-7698986bc7-jvnmv
Namespace:      default
Priority:       0
Node:           node3/192.168.2.198
Start Time:     Fri, 20 Mar 2020 09:52:19 +0100
Labels:         app=katib
                component=db-manager
                pod-template-hash=7698986bc7
Annotations:    sidecar.istio.io/inject: false
Status:         Running
IP:             10.233.92.246
Controlled By:  ReplicaSet/katib-db-manager-7698986bc7
Containers:
  katib-db-manager:
    Container ID:  docker://3eb3fedc58a914ae1518401302c17863a52c93d86cc0f3825e49f51149a43fdd
    Image:         gcr.io/kubeflow-images-public/katib/v1alpha3/katib-db-manager
    Image ID:      docker-pullable://gcr.io/kubeflow-images-public/katib/v1alpha3/katib-db-manager@sha256:d98557eb69045023a0e25300e8b2b573a79e851e7e6fe59a69698b8df13a5e9a
    Port:          6789/TCP
    Host Port:     0/TCP
    Command:
      ./katib-db-manager
    State:          Running
      Started:      Fri, 20 Mar 2020 09:53:21 +0100
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Fri, 20 Mar 2020 09:52:21 +0100
      Finished:     Fri, 20 Mar 2020 09:53:21 +0100
    Ready:          False
    Restart Count:  1
    Liveness:       exec [/bin/grpc_health_probe -addr=:6789] delay=30s timeout=1s period=60s #success=1 #failure=15
    Readiness:      exec [/bin/grpc_health_probe -addr=:6789] delay=30s timeout=10s period=10s #success=1 #failure=3
    Environment:
      DB_NAME:      mysql
      DB_PASSWORD:  &lt;set to the key 'MYSQL_ROOT_PASSWORD' in secret 'katib-mysql-secrets'&gt;  Optional: false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nxjpr (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  default-token-nxjpr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-nxjpr
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  &lt;unknown&gt;           default-scheduler  Successfully assigned default/katib-db-manager-7698986bc7-jvnmv to node3
  Normal   Pulled     45s (x2 over 106s)  kubelet, node3     Container image "gcr.io/kubeflow-images-public/katib/v1alpha3/katib-db-manager" already present on machine
  Normal   Created    45s (x2 over 106s)  kubelet, node3     Created container katib-db-manager
  Normal   Started    45s (x2 over 105s)  kubelet, node3     Started container katib-db-manager
  Warning  Unhealthy  1s (x5 over 71s)    kubelet, node3     Readiness probe failed: timeout: failed to connect service ":6789" within 1s
&lt;/denchmark-code&gt;

But kubectl get all shows no service for katib at this time.
&lt;denchmark-code&gt;pod/katib-mysql-6bb9f765f9-z6drd              1/1     Running   0          3m3s
pod/nfs-client-provisioner-7fd7d45d96-v576f   1/1     Running   1          42h


NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.233.0.1   &lt;none&gt;        443/TCP   7d23h


NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/katib-mysql              1/1     1            1           3m3s
deployment.apps/nfs-client-provisioner   1/1     1            1           2d19h

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/katib-mysql-6bb9f765f9              1         1         1       3m3s
replicaset.apps/nfs-client-provisioner-7fd7d45d96   1         1         1       2d19h
&lt;/denchmark-code&gt;

Manually creating the service: kubectl apply -f kustomize/katib-controller/base/katib-mysql-service.yaml and both run successfully.
&lt;denchmark-code&gt;kubectl get all
NAME                                          READY   STATUS    RESTARTS   AGE
pod/katib-db-manager-7698986bc7-jvnmv         1/1     Running   2          3m19s
pod/katib-mysql-6bb9f765f9-z6drd              1/1     Running   0          7m27s
pod/nfs-client-provisioner-7fd7d45d96-v576f   1/1     Running   1          42h


NAME                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/katib-mysql   ClusterIP   10.233.6.91   &lt;none&gt;        3306/TCP   71s
service/kubernetes    ClusterIP   10.233.0.1    &lt;none&gt;        443/TCP    7d23h


NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/katib-db-manager         1/1     1            1           3m19s
deployment.apps/katib-mysql              1/1     1            1           7m27s
deployment.apps/nfs-client-provisioner   1/1     1            1           2d19h

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/katib-db-manager-7698986bc7         1         1         1       3m19s
replicaset.apps/katib-mysql-6bb9f765f9              1         1         1       7m27s
replicaset.apps/nfs-client-provisioner-7fd7d45d96   1         1         1       2d19h
&lt;/denchmark-code&gt;

Doing a complete kubeflow deployment for testing with katib-mysql-pvc.yaml changed to the following works for me.
&lt;denchmark-code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: katib-mysql
  namespace: kubeflow
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: manual
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
      type: katib-local
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: katib-mysql
  labels:
    type: katib-local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/katib"
&lt;/denchmark-code&gt;

So I think for me it is not an issue with the mysql image but with the underlying storage.  I am unsure if this is related to the problem faced by @Everret-moon  but please do let me know if there is anything I can do to further help narrowing down the cause of these problems.
		</comment>
		<comment id='23' author='everett-moon' date='2020-03-20T10:09:18Z'>
		&lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 I don't think it is a problem with glusterfs. After I update  the image's tag in   deployment, everything works well.  In fact, I have try to add extra log message by rebuilding a new image from ,  and several envirment variables, such as ,  have been setup. but in the function  throws exception. This really confused me a lot.
&lt;denchmark-code&gt;        You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD
[root@k8s-slave02 dockerbuild]# docker run -it mysql:debug -e MYSQL_ROOT_PASSWORD="test" MYSQL_ALLOW_EMPTY_PASSWORD="true" --entrypoint "env"
2020-03-20 09:26:18+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-20 09:26:18+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-20 09:26:18+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
MYSQL_ROOT_PASSWORD:
MYSQL_ALLOW_EMPTY_PASSWORD:
MYSQL_RANDOM_ROOT_PASSWORD:
2020-03-20 09:26:18+00:00 [ERROR] [Entrypoint]: Database is uninitialized and password option is not specified
        You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD

&lt;/denchmark-code&gt;

Luckly, the image mysql:8.0.3 works well for me.
&lt;denchmark-code&gt;[vimer@xushangming-dev ~]$ kubectl get deploy -n kubeflow metadata-db -oyaml | grep image:
        image: mysql:8.0.3
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='everett-moon' date='2020-03-20T13:14:24Z'>
		@Everret-moon Good to hear you got it working!
I have tried 8.0.3 version when using nfs, but in my case it doesn't work:  Warning  Unhealthy  6m46s (x31 over 31m)  kubelet, node3  Liveness probe failed: /bin/bash: mysqladmin: command not found
So it seems that I am having a different (nfs specific?) problem.
		</comment>
		<comment id='25' author='everett-moon' date='2020-03-20T17:55:39Z'>
		&lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 So after you deploy PV in addition to PVC, everything was working? Because if you take a look in the  folder:  &lt;denchmark-link:https://github.com/kubeflow/katib/tree/master/manifests/v1alpha3/pv&gt;https://github.com/kubeflow/katib/tree/master/manifests/v1alpha3/pv&lt;/denchmark-link&gt;
, we also have your PV.
Maybe your storage class glusterfs storage class works in another way.
@Everret-moon This command works for me:
&lt;denchmark-code&gt;docker run -it -e MYSQL_ROOT_PASSWORD=test mysql:8
&lt;/denchmark-code&gt;

		</comment>
		<comment id='26' author='everett-moon' date='2020-03-21T21:15:12Z'>
		I didn’t see that pv. What a coincidence I used the same path.
I don’t use gluster. But when I try to use nfs in a pv with katib-mysql it does not work even though pv and pvc are created and bound. Both when creating a pv manually that uses nfs or through nfs-client-provisioner.
		</comment>
		<comment id='27' author='everett-moon' date='2020-03-23T01:17:42Z'>
		&lt;denchmark-link:https://github.com/mvwestendorp&gt;@mvwestendorp&lt;/denchmark-link&gt;
 Just running   works for me as well, but it have a bit diffrence with the one running in the corresponding pod, which has extra environment variables.
		</comment>
		<comment id='28' author='everett-moon' date='2020-03-23T13:22:55Z'>
		@Everret-moon You can try to specify all env variables and run mysql using docker.
		</comment>
		<comment id='29' author='everett-moon' date='2020-09-24T18:01:09Z'>
		
@andreyvelich @mvwestendorp It is a problem of docker image mysql:8, the entrypoint don't startup successfully.
I haved tried:

attach the running pod by docker exec, then setup root password with test
after connecting to mysql, I found the database created unsuccessfully, so I created it manually

Then the pod of katib-mysql is in ready status. If I configured it with correct access permission, I think the pod of katib-db-manager would work as well.
katib-mysql-57884cb488-mnrgh                                   1/1     Running            1          5h4m

[vimer@xushangming-dev kubeflow]$ kubectl logs katib-db-manager-64f548b47c-4p6mk -n kubeflow
E0320 07:07:32.068131       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:37.065604       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:42.066101       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:47.065011       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:52.065393       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:07:57.065777       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server
E0320 07:08:02.065504       1 mysql.go:62] Ping to Katib db failed: Error 1130: Host '10.244.1.137' is not allowed to connect to this MySQL server

When I try to run the container manually, I get following error:
[vimer@xushangming-dev kubeflow]$ docker run -it mysql:8 -e MYSQL_DATABASE=katib MYSQL_ROOT_PASSWORD=test MYSQL_ALLOW_EMPTY_PASSWORD="true"
2020-03-20 08:33:08+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-20 08:33:08+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-20 08:33:08+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian10 started.
2020-03-20 08:33:09+00:00 [ERROR] [Entrypoint]: Database is uninitialized and password option is not specified
        You need to specify one of MYSQL_ROOT_PASSWORD, MYSQL_ALLOW_EMPTY_PASSWORD and MYSQL_RANDOM_ROOT_PASSWORD

This is the actual way how the deployment katib-mysql creates the pod. I have checked twice the script, I don't find any problem.
I have resolved this problem by replacing mysql docker image tags from 8 to 8.0.3.
The metadata-db and katib-mysql use two different tags.

change the sql image to to 5.6. It will work without any issue
		</comment>
	</comments>
</bug>