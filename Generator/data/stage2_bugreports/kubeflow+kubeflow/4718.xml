<bug id='4718' author='jlewi' open_date='2020-02-02T03:19:09Z' closed_time='2020-02-05T00:17:21Z'>
	<summary>Centraldashboard is crash looping - failed liveness probe</summary>
	<description>
/kind bug
Centraldashboard is crash looping.
This is an auto-deployed cluster from the v1 branch.
cluster:  kf-v1-0131-d72
project: kubeflow-ci-deployment
&lt;denchmark-code&gt;Using Profiles service at http://profiles-kfam.kubeflow:8081/kfam
Server listening on port http://localhost:8082 (in production mode)
Unable to fetch Application information: { kind: 'Status',
  apiVersion: 'v1',
  metadata: {},
  status: 'Failure',
  message:
   'applications.app.k8s.io is forbidden: User "system:serviceaccount:kubeflow:centraldashboard" cannot list resource "applications" in API group "app.k8s.io" in the namespace "kubeflow"',
  reason: 'Forbidden',
  details: { group: 'app.k8s.io', kind: 'applications' },
  code: 403 }
Unable to fetch Nodes { kind: 'Status',
  apiVersion: 'v1',
  metadata: {},
  status: 'Failure',
  message:
   'nodes is forbidden: User "system:serviceaccount:kubeflow:centraldashboard" cannot list resource "nodes" in API group "" at the cluster scope',
  reason: 'Forbidden',
  details: { kind: 'nodes' },
  code: 403 }
Unable to get environment info: Unexpected error getting environment info
Error: connect ECONNREFUSED 10.51.249.29:8081
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)
Unable to get environment info: Unexpected error getting environment info
Error: connect ECONNREFUSED 10.51.249.29:8081
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jlewi' date='2020-02-02T03:19:18Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.99



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='jlewi' date='2020-02-02T03:27:18Z'>
		Here's the rolebinding
&lt;denchmark-code&gt;kubectl -n kubeflow get clusterrolebinding -o yaml centraldashboard
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"app":"centraldashboard","app.kubernetes.io/component":"centraldashboard","app.kubernetes.io/instance":"centraldashboard-v1.0.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"centraldashboard","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"v1.0.0","kustomize.component":"centraldashboard"},"name":"centraldashboard"},"roleRef":{"apiGroup":"rbac.authorization.k8s.io","kind":"ClusterRole","name":"centraldashboard"},"subjects":[{"kind":"ServiceAccount","name":"centraldashboard","namespace":"$(namespace)"}]}
  creationTimestamp: "2020-01-31T09:42:18Z"
  labels:
    app: centraldashboard
    app.kubernetes.io/component: centraldashboard
    app.kubernetes.io/instance: centraldashboard-v1.0.0
    app.kubernetes.io/managed-by: kfctl
    app.kubernetes.io/name: centraldashboard
    app.kubernetes.io/part-of: kubeflow
    app.kubernetes.io/version: v1.0.0
    kustomize.component: centraldashboard
  name: centraldashboard
  resourceVersion: "2725"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/centraldashboard
  uid: f888dd23-440d-11ea-86ff-42010a80001b
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: centraldashboard
subjects:
- kind: ServiceAccount
  name: centraldashboard
  namespace: $(namespace)
&lt;/denchmark-code&gt;

It doesn't look like namespace got substituted correctly
		</comment>
		<comment id='3' author='jlewi' date='2020-02-02T03:28:32Z'>
		I changed that to
&lt;denchmark-code&gt;subjects:
- kind: ServiceAccount
  name: centraldashboard
  namespace: kubeflow
&lt;/denchmark-code&gt;

And it fixed the problem.
		</comment>
		<comment id='4' author='jlewi' date='2020-02-02T03:41:54Z'>
		I'm not sure what if anything changed.
I seem to be able to reproduce this just by running kustomize build inside kubeflow/manifests/common/centraldashboard base.
I'm not sure why master is working.
I think we could just remove $(namespace) and let kustomize set the namespace.
It looks like the only place where the parameter $(namespace) is the now obsolete annotation for Ambassador.
I'm puzzled though as to why this appears to be working on master.
		</comment>
		<comment id='5' author='jlewi' date='2020-02-02T03:55:22Z'>
		The cluster was deployed from
&lt;denchmark-link:https://github.com/kubeflow/manifests/commit/4e550919e10ecf8cbae8643117405345a1bdb758&gt;kubeflow/manifests@4e55091&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='jlewi' date='2020-02-02T04:01:45Z'>
		For cluster which was deployed from
&lt;denchmark-link:https://github.com/kubeflow/manifests/commit/7d36e486210eeec8504d67cb1b1855c1ffc9bd35&gt;kubeflow/manifests@7d36e48&lt;/denchmark-link&gt;

It seems to work. The clusterrolebinding appears to have the same problem
&lt;denchmark-code&gt;kubectl -n kubeflow get clusterrolebinding centraldashboard -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRoleBinding","metadata":{"annotations":{},"labels":{"app":"centraldashboard","app.kubernetes.io/component":"centraldashboard","app.kubernetes.io/instance":"centraldashboard-v0.7.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"centraldashboard","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"v0.7.0","kustomize.component":"centraldashboard"},"name":"centraldashboard"},"roleRef":{"apiGroup":"rbac.authorization.k8s.io","kind":"ClusterRole","name":"centraldashboard"},"subjects":[{"kind":"ServiceAccount","name":"centraldashboard","namespace":"$(namespace)"}]}
  creationTimestamp: "2020-01-31T09:41:43Z"
  labels:
    app: centraldashboard
    app.kubernetes.io/component: centraldashboard
    app.kubernetes.io/instance: centraldashboard-v0.7.0
    app.kubernetes.io/managed-by: kfctl
    app.kubernetes.io/name: centraldashboard
    app.kubernetes.io/part-of: kubeflow
    app.kubernetes.io/version: v0.7.0
    kustomize.component: centraldashboard
  name: centraldashboard
  resourceVersion: "2551"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/centraldashboard
  uid: e3dc890d-440d-11ea-9755-42010a800147
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: centraldashboard
subjects:
- kind: ServiceAccount
  name: centraldashboard
  namespace: $(namespace)
&lt;/denchmark-code&gt;

Here are the centraldashboard logs
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4143525/centraldashboard.logs.txt&gt;centraldashboard.logs.txt&lt;/denchmark-link&gt;

Here's the deployment
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4143527/centraldashboard.yaml.txt&gt;centraldashboard.yaml.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='jlewi' date='2020-02-02T05:01:36Z'>
		I redeployed and this time it worked.
cluster: kf-v1-0201-33e
&lt;denchmark-link:https://github.com/kubeflow/manifests/commit/e9dfeb30abcd12a920ad8f8510f26a167390e5fc&gt;kubeflow/manifests@e9dfeb3&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='jlewi' date='2020-02-02T05:06:49Z'>
		Here are the centraldashboard logs
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4143555/centraldashboard.logs.txt&gt;centraldashboard.logs.txt&lt;/denchmark-link&gt;

I see the same errors as before but its not crashing so they may have been a red herring.
		</comment>
		<comment id='9' author='jlewi' date='2020-02-02T05:09:14Z'>
		Looking back at the original pod; it looks like it might have been failing the liveness probe.
&lt;denchmark-code&gt;kubectl -n kubeflow describe pods centraldashboard-6fbf8f9b89-hghbj
Name:               centraldashboard-6fbf8f9b89-hghbj
Namespace:          kubeflow
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:               gke-kf-v1-0131-d72-kf-v1-0131-d72-cpu-dd7a5c4f-8b7f/10.128.0.121
Start Time:         Fri, 31 Jan 2020 01:42:18 -0800
Labels:             app=centraldashboard
                    app.kubernetes.io/component=centraldashboard
                    app.kubernetes.io/instance=centraldashboard-v1.0.0
                    app.kubernetes.io/managed-by=kfctl
                    app.kubernetes.io/name=centraldashboard
                    app.kubernetes.io/part-of=kubeflow
                    app.kubernetes.io/version=v1.0.0
                    kustomize.component=centraldashboard
                    pod-template-hash=6fbf8f9b89
Annotations:        &lt;none&gt;
Status:             Running
IP:                 10.48.0.18
Controlled By:      ReplicaSet/centraldashboard-6fbf8f9b89
Containers:
  centraldashboard:
    Container ID:   docker://a63bea5d23bd43e85410fe6e141bda13802b60282c690171a84907fd45027afa
    Image:          gcr.io/kubeflow-images-public/centraldashboard:v1.0.0-gb1400446
    Image ID:       docker-pullable://gcr.io/kubeflow-images-public/centraldashboard@sha256:7187dd2ff77e89f9bd1331e92db9dde1fb53317bace5359dd2111e0e14b86229
    Port:           8082/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sat, 01 Feb 2020 21:06:43 -0800
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 01 Feb 2020 20:59:38 -0800
      Finished:     Sat, 01 Feb 2020 21:01:37 -0800
    Ready:          True
    Restart Count:  244
    Liveness:       http-get http://:8082/api/workgroup/env-info delay=30s timeout=1s period=30s #success=1 #failure=3
    Environment:
      USERID_HEADER:               X-Goog-Authenticated-User-Email
      USERID_PREFIX:               accounts.google.com:
      PROFILES_KFAM_SERVICE_HOST:  profiles-kfam.kubeflow
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from centraldashboard-token-rvdxn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  centraldashboard-token-rvdxn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  centraldashboard-token-rvdxn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                     From                                                          Message
  ----     ------     ----                    ----                                                          -------
  Warning  BackOff    5m25s (x2889 over 17h)  kubelet, gke-kf-v1-0131-d72-kf-v1-0131-d72-cpu-dd7a5c4f-8b7f  Back-off restarting failed container
  Warning  Unhealthy  22s (x732 over 43h)     kubelet, gke-kf-v1-0131-d72-kf-v1-0131-d72-cpu-dd7a5c4f-8b7f  Liveness probe failed: Get http://10.48.0.18:8082/api/workgroup/env-info: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='jlewi' date='2020-02-02T05:13:18Z'>
		On the original cluster it looks like it is still crashlooping; i.e. changing the RBAC role didn't fix it. I probably got lucky and just managed to send a request before it got started.
		</comment>
		<comment id='11' author='jlewi' date='2020-02-02T05:20:50Z'>
		Here are the logs from the unhealthy centraldashboard
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4143562/centraldashboard.logs.txt&gt;centraldashboard.logs.txt&lt;/denchmark-link&gt;

I see the following errors
&lt;denchmark-code&gt;Unable to get environment info: Unexpected error getting environment info
Error: connect ECONNREFUSED 10.51.249.29:8081
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)
Unable to get environment info: Unexpected error getting environment info
Error: connect ECONNREFUSED 10.51.249.29:8081
    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1107:14)
&lt;/denchmark-code&gt;

So its having problems contacting the profiles-kfam.
&lt;denchmark-code&gt;profiles-kfam                                  ClusterIP   10.51.249.29    &lt;none&gt;        8081/TCP            43h
&lt;/denchmark-code&gt;

It looks like the profiles deployment is missing.
&lt;denchmark-code&gt;kubectl -n kubeflow get deploy 
NAME                                          READY   UP-TO-DATE   AVAILABLE   AGE
admission-webhook-deployment                  1/1     1            1           43h
argo-ui                                       1/1     1            1           43h
centraldashboard                              1/1     1            1           43h
cloud-endpoints-controller                    1/1     1            1           43h
jupyter-web-app-deployment                    1/1     1            1           43h
katib-controller                              1/1     1            1           43h
katib-db                                      1/1     1            1           43h
katib-manager                                 1/1     1            1           43h
katib-ui                                      1/1     1            1           43h
metadata-db                                   0/1     1            0           43h
metadata-deployment                           0/1     1            0           43h
metadata-envoy-deployment                     1/1     1            1           43h
metadata-grpc-deployment                      0/1     1            0           43h
metadata-ui                                   1/1     1            1           43h
minio                                         1/1     1            1           43h
ml-pipeline                                   1/1     1            1           43h
ml-pipeline-ml-pipeline-visualizationserver   1/1     1            1           43h
ml-pipeline-persistenceagent                  1/1     1            1           43h
ml-pipeline-scheduledworkflow                 1/1     1            1           43h
ml-pipeline-ui                                1/1     1            1           43h
ml-pipeline-viewer-controller-deployment      1/1     1            1           43h
mysql                                         1/1     1            1           43h
notebook-controller-deployment                1/1     1            1           43h
pytorch-operator                              1/1     1            1           43h
spark-operatorsparkoperator                   1/1     1            1           43h
tensorboard                                   1/1     1            1           43h
tf-job-operator                               1/1     1            1           43h
workflow-controller                           1/1     1            1           43h
&lt;/denchmark-code&gt;

I suspect the deployment never fully succeeded and that's why we are missing the profiles controller.
I suspect it might only have partially succeeded due to &lt;denchmark-link:https://github.com/kubeflow/manifests/issues/806&gt;kubeflow/manifests#806&lt;/denchmark-link&gt;

The fact that the liveness probe depends on the profile controller seems like something that should be fixed.
		</comment>
		<comment id='12' author='jlewi' date='2020-02-04T23:54:02Z'>
		Here's the centraldashboard with the new liveness probe on master.
&lt;denchmark-link:https://github.com/kubeflow/manifests/tree/09cc432&gt;https://github.com/kubeflow/manifests/tree/09cc432&lt;/denchmark-link&gt;

Everything looks good; its set to /healthz and everything is working.
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4156750/centraldashboard.yaml.txt&gt;centraldashboard.yaml.txt&lt;/denchmark-link&gt;

We just need to verify this on the 1.0 branch once the CP. lands.
		</comment>
		<comment id='13' author='jlewi' date='2020-02-05T00:17:21Z'>
		For a 1.0 branch cluster
&lt;denchmark-link:https://github.com/kubeflow/manifests/tree/32aba35&gt;https://github.com/kubeflow/manifests/tree/32aba35&lt;/denchmark-link&gt;

Here's the output confirming liveness probe is working with the the new /healthz endpoint.
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/4156803/centraldashboard.describe.txt&gt;centraldashboard.describe.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='jlewi' date='2020-03-16T19:36:55Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 I am getting the same in my 1.0.1 cluster.
Logs from the pod are:
‚ùØ kubectl logs centraldashboard-6944c87dd5-w28ht -n kubeflow --previous

kubeflow-centraldashboard@0.0.2 start /app
npm run serve


kubeflow-centraldashboard@0.0.2 serve /app
node dist/server.js

Any resolution ideas? Help would be much appreciated.
		</comment>
		<comment id='15' author='jlewi' date='2020-07-13T14:44:28Z'>
		mark
		</comment>
		<comment id='16' author='jlewi' date='2020-09-20T16:04:10Z'>
		
I redeployed and this time it worked.
cluster: kf-v1-0201-33e
kubeflow/manifests@e9dfeb3

How did you redploy it? did you redploy the entire manifests? or just the centraldasboard?
		</comment>
		<comment id='17' author='jlewi' date='2020-10-14T00:39:10Z'>
		I got a similar error after I deployed the centraldashboard manifests from &lt;denchmark-link:https://github.com/kubeflow/manifests/tree/v1.1.0/common/centraldashboard&gt;v1.1.0&lt;/denchmark-link&gt;
 tag. Any idea what might get wrong?
kubectl describe pod -n kubeflow centraldashboard-84d468779d-sjfpk
Events:
  Type     Reason     Age                    From                                                      Message
  ----     ------     ----                   ----                                                      -------
  Normal   Scheduled  6m12s                  default-scheduler                                         Successfully assigned kubeflow/centraldashboard-84d468779d-sjfpk to gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1
  Normal   Pulled     6m12s                  kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Container image "docker.io/istio/proxy_init:1.1.6" already present on machine
  Normal   Created    6m11s                  kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Created container istio-init
  Normal   Started    6m11s                  kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Started container istio-init
  Normal   Started    6m10s                  kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Started container istio-proxy
  Normal   Created    6m10s                  kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Created container istio-proxy
  Normal   Pulled     6m10s                  kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Container image "docker.io/istio/proxyv2:1.1.6" already present on machine
  Warning  Unhealthy  6m5s (x3 over 6m9s)    kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Readiness probe failed: HTTP probe failed with statuscode: 503
  Normal   Started    4m37s (x2 over 6m10s)  kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Started container centraldashboard
  Normal   Created    3m7s (x3 over 6m10s)   kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Created container centraldashboard
  Normal   Pulled     3m7s (x3 over 6m10s)   kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Container image "gcr.io/kubeflow-images-public/centraldashboard:v1.1.0-g35d7484a" already present on machine
  Normal   Killing    3m7s (x2 over 4m37s)   kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Container centraldashboard failed liveness probe, will be restarted
  Warning  Unhealthy  67s (x10 over 5m37s)   kubelet, gke-kf-prod-2-kf-prod-cpu-pool-v1-10fee0b0-xch1  Liveness probe failed: Get http://10.205.96.170:8082/healthz: EOF
		</comment>
		<comment id='18' author='jlewi' date='2020-10-14T00:39:17Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/jupyter
0.96



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
	</comments>
</bug>