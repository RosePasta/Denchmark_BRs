<bug id='5103' author='linkvt' open_date='2020-07-03T08:40:34Z' closed_time='2020-07-03T09:15:30Z'>
	<summary>Increase istio replicas to 2+ for better availability and because of PodDisruptionBudgets</summary>
	<description>
/kind bug
Edit wups wanted to create this in the manifests repo, please tell me if I should recreate the issue.
What steps did you take and what happened:
The PodDisruptionBudgets for istio currently require always 1 pod available while the affected istio deployments have only 1 replica. Draining individual nodes is impossible as the PodDisruptionBudgets would be violated.
What did you expect to happen:
There should be 2+ replicas of the Istio pods that have PodDisruptionBudgets set so that individual nodes can be drained in rolling node upgrades - also of course for a higher availability of Istio.
Anything else you would like to add:
These are the deployments (ignore prometheus and citadel):
&lt;denchmark-code&gt; k get deploy
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
cluster-local-gateway    1/1     1            1           8d
istio-citadel            1/1     1            1           8d
istio-galley             1/1     1            1           8d
istio-ingressgateway     1/1     1            1           8d
istio-pilot              1/1     1            1           8d
istio-policy             1/1     1            1           8d
istio-sidecar-injector   1/1     1            1           8d
istio-telemetry          1/1     1            1           8d
prometheus               1/1     1            1           8d
&lt;/denchmark-code&gt;

and the PodDisruptionBudgets:
&lt;denchmark-code&gt;$ k get poddisruptionbudgets.policy 
NAME                     MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
cluster-local-gateway    1               N/A               0                     8d
istio-galley             1               N/A               0                     8d
istio-ingressgateway     1               N/A               0                     8d
istio-pilot              1               N/A               0                     8d
istio-policy             1               N/A               0                     8d
istio-sidecar-injector   1               N/A               0                     8d
istio-telemetry          1               N/A               0                     8d
&lt;/denchmark-code&gt;

Environment:

Kubeflow version: build version dev_local but we use the 1.0.2 manifests
kfctl version: 1.0.2
Kubernetes platform: vanilla with kubespray
Kubernetes version: (use kubectl version): 1.14 eager to upgrade after our migration from kubeflow 0.5 :D
OS (e.g. from /etc/os-release): Ubuntu 18.04

	</description>
	<comments>
		<comment id='1' author='linkvt' date='2020-07-03T08:40:41Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='linkvt' date='2020-07-03T08:43:04Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/feature
0.53



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='3' author='linkvt' date='2020-07-03T08:43:26Z'>
		Oups, I wanted to create this in the manifests repo, please tell me if I should recreate the issue there or if this repo is also used as central kubeflow issue tracker, thanks.
		</comment>
		<comment id='4' author='linkvt' date='2020-07-03T09:15:30Z'>
		Recreated in &lt;denchmark-link:https://github.com/kubeflow/manifests/issues/1343&gt;kubeflow/manifests#1343&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>