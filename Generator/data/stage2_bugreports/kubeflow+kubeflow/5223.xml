<bug id='5223' author='marrrcin' open_date='2020-08-18T14:47:33Z' closed_time='2020-11-09T00:41:24Z'>
	<summary>Installation guide for GCP is incomplete and it's impossible to deploy new Kubeflow cluster</summary>
	<description>
/kind bug

I've carefully followed the GCP Kubeflow deployment guide  (starting from scratch), without success.
&lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/&gt;https://www.kubeflow.org/docs/gke/deploy/&lt;/denchmark-link&gt;

My  from Makefile (the one from &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#fetch-packages-using-kpt&gt;https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#fetch-packages-using-kpt&lt;/denchmark-link&gt;
):
set-values:
	kpt cfg set ./instance gke.private false

	kpt cfg set ./instance mgmt-ctxt kf-mgmnt

	kpt cfg set ./upstream/manifests/gcp name kf
	kpt cfg set ./upstream/manifests/gcp gcloud.core.project XXX
	kpt cfg set ./upstream/manifests/gcp gcloud.compute.zone europe-west4
	kpt cfg set ./upstream/manifests/gcp location europe-west4-b
	kpt cfg set ./upstream/manifests/gcp log-firewalls false

	kpt cfg set ./upstream/manifests/stacks/gcp name kf
	kpt cfg set ./upstream/manifests/stacks/gcp gcloud.core.project XXX

	kpt cfg set ./instance name kf
	kpt cfg set ./instance location europe-west4-b
	kpt cfg set ./instance gcloud.core.project XXX
	kpt cfg set ./instance email X@XX
Final error I get after running  in step &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#deploy-kubeflow&gt;https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#deploy-kubeflow&lt;/denchmark-link&gt;
 is:
kubectl --context=kf-mgmnt wait --for=condition=Ready --timeout=600s  containercluster kf
error: timed out waiting for the condition on containerclusters/kf
What did you expect to happen:
Installation is seamless and completes without errors, installation guide is complete.
Anything else you would like to add:
Current deployment documentation seems to be neglected and there are a lot of missing pieces there that waste people's time on digging through many GitHub issues.
Issues I've had to deal with manually / questions:

When management cluster has long name, like kubeflow-management, the setup https://www.kubeflow.org/docs/gke/deploy/management-setup/ cannot be completed due to violation of some max characters limit for service account names
The guide https://www.kubeflow.org/docs/gke/deploy/ does not mention that you need kustomize IN SPECIFIC VERSION (kubeflow/manifests#1490 (comment)), istioctl IN GOOGLE-PROVIDED version (kubeflow/manifests#1490 (comment))
There is a strong assumption that cluster needs to have the namespace exactly the same as the project? Why is that?
Why Kubeflow needs a separate "management" cluster now?

Environment:

Kubeflow version: n/a
kfctl version: (use kfctl version): not installed
Kubernetes platform: GCP
Kubernetes version:

&lt;denchmark-code&gt;Client Version: version.Info{Major:"1", Minor:"16+", GitVersion:"v1.16.13-dispatcher", GitCommit:"fd22db44e150011eccc8729db223945384460143", GitTreeState:"clean", BuildDate:"2020-07-24T07:48:37Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"16+", GitVersion:"v1.16.13-gke.1", GitCommit:"688c6543aa4b285355723f100302d80431e411cc", GitTreeState:"clean", BuildDate:"2020-07-21T02:37:26Z", GoVersion:"go1.13.9b4", Compiler:"gc", Platform:"linux/amd64"}
&lt;/denchmark-code&gt;


OS: macOS 10.15.1

	</description>
	<comments>
		<comment id='1' author='marrrcin' date='2020-08-18T14:47:42Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




platform/gcp
0.88



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='marrrcin' date='2020-08-18T14:47:43Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




platform/gcp
0.88



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='3' author='marrrcin' date='2020-08-18T21:54:19Z'>
		Regarding the error. I think this has been resolved.
Did you follow with &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#configure-kubeflow&gt;updated docs&lt;/denchmark-link&gt;
?
&lt;denchmark-h:h3&gt;Make the managed project’s namespace default of the context:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;kubectl config set-context --current --namespace ${PROJECT}
&lt;/denchmark-code&gt;

If not it's okay. Just edit the line in the instance Makefile with namespace.
&lt;denchmark-code&gt;kubectl --context=$(MGMTCTXT) wait --for=condition=Ready --timeout=600s  containercluster $(NAME) -n &lt;MANAGED_PROJECT&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='marrrcin' date='2020-08-19T15:46:02Z'>
		First of all, I really appreciate the effort put by the community in the Kubeflow project, it's really awesome.
But &lt;denchmark-link:https://github.com/marrrcin&gt;@marrrcin&lt;/denchmark-link&gt;
 I have the same impression as you after trying to set up the GCP guide: the documentation really seems to be neglected.
When you make a change that's not backward compatible (e.g., introduce a management cluster), please make sure that the new documentation works...
		</comment>
		<comment id='5' author='marrrcin' date='2020-08-19T17:51:40Z'>
		&lt;denchmark-link:https://github.com/marrrcin&gt;@marrrcin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/theophilee&gt;@theophilee&lt;/denchmark-link&gt;
 - I'm an external contributor and reviewer here. Saw your post on Slack and I appreciate your awesome feedback.
Can you please do the following to get this resolved faster:
- Move/duplucate this docs-related issue in www.github.com/kubeflow/website since that is where the docs sit. (EDIT: there're a few docs-labeled Issues in kubeflow/kubeflow, so let's leave this here

Assign the team at GCP (/assign @Bobgy @joeliedtke @jlewi) and they or somewhere else from Google Cloud should get back to you asap.

I will try to help you all too, if I can.
Thanks a million!
		</comment>
		<comment id='6' author='marrrcin' date='2020-08-19T17:51:48Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/docs
0.78



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='7' author='marrrcin' date='2020-08-19T18:07:26Z'>
		/assign &lt;denchmark-link:https://github.com/Bobgy&gt;@Bobgy&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/joeliedtke&gt;@joeliedtke&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='marrrcin' date='2020-08-20T08:33:28Z'>
		
Regarding the error. I think this has been resolved.
Did you follow with updated docs?
Make the managed project’s namespace default of the context:
kubectl config set-context --current --namespace ${PROJECT}

If not it's okay. Just edit the line in the instance Makefile with namespace.
kubectl --context=$(MGMTCTXT) wait --for=condition=Ready --timeout=600s  containercluster $(NAME) -n &lt;MANAGED_PROJECT&gt;



Of course I've used the most recent docs.
Your solution does not help.

		</comment>
		<comment id='9' author='marrrcin' date='2020-08-21T02:29:22Z'>
		I feel like I could contribute a guide for deploying vanilla K8S (dex based) using argo-cd.
Which can easily be used on GKE, and is much better if you already have a GKE cluster you want to use. (Which is what I currently do for my deployments)
		</comment>
		<comment id='10' author='marrrcin' date='2020-08-21T05:48:46Z'>
		Hi all, I feel sorry you are getting the frustration when deploying to GCP.
I have already updated problems 2. in the public docs (before I saw this issue), so I hope there won't be future confusions for others.
Thanks for mentioning 1, and I will add that to the docs.
For &lt;denchmark-link:https://github.com/marrrcin&gt;@marrrcin&lt;/denchmark-link&gt;
's problem when deploying, we could have done better at explaining what it means when we are waiting for containercluster resource. The management cluster runs a &lt;denchmark-link:https://cloud.google.com/config-connector/docs/overview&gt;https://cloud.google.com/config-connector/docs/overview&lt;/denchmark-link&gt;
 instance, which can reconcile kubernetes CRDs into GCP resources (like clusters, service accounts, IAM policy rules).
You are timing out waiting for the containercluster resource to be ready, so you can debug it by
&lt;denchmark-code&gt;kubectl --context ${MGMTCONTEXT} get containercluster -n ${PROJECT}  -o yaml
&lt;/denchmark-code&gt;

And the resource will have a status field like other kubernetes resources, it will have detailed reason for the failure.
Reading your config, I'm not sure if "kpt cfg set ./upstream/manifests/gcp gcloud.compute.zone europe-west4" was a typo or not, you should specify a zone there, but you put a region there instead.
I'll try to explain some of the reasonings we made with the updated blueprint pattern and why we are doing these in &lt;denchmark-link:https://github.com/kubeflow/gcp-blueprints/issues/123&gt;kubeflow/gcp-blueprints#123&lt;/denchmark-link&gt;
 when I get some time (it's been a busy week).
		</comment>
		<comment id='11' author='marrrcin' date='2020-08-21T05:53:28Z'>
		&lt;denchmark-link:https://github.com/thesuperzapper&gt;@thesuperzapper&lt;/denchmark-link&gt;
 I think that's a good idea too!
The current GCP blueprint pattern focuses on making GCP resource provisioning scalable (it works with customizations and upgrades), but if you prefer managing GCP resources yourself, or just want to use GCP resources as minimal as possible and use Kubernetes resources instead. Not using the blueprint pattern also makes sense.
Note that the makefile in blueprint is meant to be easy to understand and customizable, if you don't want those GCP resources, you can skip management cluster, applying GCP resources and only use it to hydrate and apply kubernetes resources.
		</comment>
		<comment id='12' author='marrrcin' date='2020-08-21T19:04:49Z'>
		Some feedback from another user:
&lt;denchmark-link:https://user-images.githubusercontent.com/19637339/90925603-74854e80-e3e9-11ea-861b-1c26d32341e2.png&gt;&lt;/denchmark-link&gt;

cc &lt;denchmark-link:https://github.com/Bobgy&gt;@Bobgy&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/joeliedtke&gt;@joeliedtke&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='marrrcin' date='2020-08-21T19:54:05Z'>
		Hi there,
Hopefully I'm not spamming this thread, but I am getting the same timeout error as OP after running  at the end of the &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#configure-kubeflow&gt;deploy&lt;/denchmark-link&gt;
 portion of the tutorial (I successfully completed the preceding steps listed &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/&gt;here&lt;/denchmark-link&gt;
.
The specific error occurs at the output line kubectl --context=xx wait --for=condition=Ready --timeout=600s  containercluster xx -n XXXX:
&lt;denchmark-code&gt;error: timed out waiting for the condition on containerclusters/xx
make: *** [wait-gcp] Error 1
&lt;/denchmark-code&gt;

The set-values section of the kubeflow/Makefile:
&lt;denchmark-code&gt;set-values:
        kpt cfg set ./instance gke.private false

        kpt cfg set ./instance mgmt-ctxt xx

        kpt cfg set ./upstream/manifests/gcp name xx
        kpt cfg set ./upstream/manifests/gcp gcloud.core.project XXXX
        kpt cfg set ./upstream/manifests/gcp gcloud.compute.zone b
        kpt cfg set ./upstream/manifests/gcp location us-east1-b
        kpt cfg set ./upstream/manifests/gcp log-firewalls false

        kpt cfg set ./upstream/manifests/stacks/gcp name xx
        kpt cfg set ./upstream/manifests/stacks/gcp gcloud.core.project XXXX

        kpt cfg set ./instance name xx
        kpt cfg set ./instance location us-east1-b
        kpt cfg set ./instance gcloud.core.project  XXXX
        kpt cfg set ./instance email xxx@e.mail
&lt;/denchmark-code&gt;

Environment:
&lt;denchmark-code&gt;Kubeflow version: latest version fetched via `kpt pkg get https://github.com/kubeflow/gcp-blueprints.git/kubeflow@v1.1-branch ./${KFDIR}`
kfctl version: (use kfctl version): not installed
Kubernetes platform: GCP
Kubernetes version: not sure how to get this

OS: macOS 10.14.6
&lt;/denchmark-code&gt;

I ran the debug command provided by &lt;denchmark-link:https://github.com/Bobgy&gt;@Bobgy&lt;/denchmark-link&gt;
 above; the  field reads as follows:
&lt;denchmark-code&gt;status:
    conditions:
    - lastTransitionTime: "2020-08-21T18:44:53Z"
      message: reference IAMServiceAccount XXXX/xx-vm is
        not ready
      reason: DependencyNotReady
      status: "False"
      type: Ready
&lt;/denchmark-code&gt;

I created a key for the service account and stored its local path to GOOGLE_APPLICATION_CREDENTIALS in case that was the issue, but it did not resolve.
In case it helps, here's the rest of the output from kubectl --context kf get containercluster -n XXXX  -o yaml (sensitive info removed):
&lt;denchmark-code&gt;apiVersion: v1
items:
- apiVersion: container.cnrm.cloud.google.com/v1beta1
  kind: ContainerCluster
  metadata:
    annotations:
      cnrm.cloud.google.com/management-conflict-prevention-policy: resource
      cnrm.cloud.google.com/project-id: XXXX
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"container.cnrm.cloud.google.com/v1beta1","kind":"ContainerCluster","metadata":{"annotations":{},"clusterName":"XXXX/us-east1-b/xx","labels":{"kf-name":"xx","mesh_id":"XXXX_us-east1-b_xx"},"name":"xx","namespace":"XXXX"},"spec":{"clusterAutoscaling":{"autoProvisioningDefaults":{"oauthScopes":["https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/devstorage.read_only"],"serviceAccountRef":{"name":"xx-vm"}},"enabled":true,"resourceLimits":[{"maximum":128,"resourceType":"cpu"},{"maximum":2000,"resourceType":"memory"},{"maximum":16,"resourceType":"nvidia-tesla-k80"}]},"initialNodeCount":2,"location":"us-east1-b","loggingService":"logging.googleapis.com/kubernetes","monitoringService":"monitoring.googleapis.com/kubernetes","nodeConfig":{"machineType":"n1-standard-8","metadata":{"disable-legacy-endpoints":"true"},"oauthScopes":["https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/devstorage.read_only"],"serviceAccountRef":{"name":"xx-vm"},"workloadMetadataConfig":{"nodeMetadata":"GKE_METADATA_SERVER"}},"releaseChannel":{"channel":"REGULAR"},"workloadIdentityConfig":{"identityNamespace":"XXXX.svc.id.goog"}}}
    creationTimestamp: "2020-08-21T18:44:53Z"
    generation: 3
    labels:
      kf-name: xx
      mesh_id: XXXX_us-east1-b_xx
    name: xx
    namespace: XXXX
    resourceVersion: "9475"
    selfLink: /apis/container.cnrm.cloud.google.com/v1beta1/namespaces/XXXX/containerclusters/xx
    uid: 1e755295-ed20-4513-be68-2229e3b102d1
  spec:
    clusterAutoscaling:
      autoProvisioningDefaults:
        oauthScopes:
        - https://www.googleapis.com/auth/logging.write
        - https://www.googleapis.com/auth/monitoring
        - https://www.googleapis.com/auth/devstorage.read_only
        serviceAccountRef:
          name: xx-vm
      enabled: true
      resourceLimits:
      - maximum: 128
        resourceType: cpu
      - maximum: 2000
        resourceType: memory
      - maximum: 16
        resourceType: nvidia-tesla-k80
    initialNodeCount: 2
    location: us-east1-b
    loggingService: logging.googleapis.com/kubernetes
    monitoringService: monitoring.googleapis.com/kubernetes
    nodeConfig:
      machineType: n1-standard-8
      metadata:
        disable-legacy-endpoints: "true"
      oauthScopes:
      - https://www.googleapis.com/auth/logging.write
      - https://www.googleapis.com/auth/monitoring
      - https://www.googleapis.com/auth/devstorage.read_only
      serviceAccountRef:
        name: xx-vm
      workloadMetadataConfig:
        nodeMetadata: GKE_METADATA_SERVER
    releaseChannel:
      channel: REGULAR
    workloadIdentityConfig:
      identityNamespace: XXXX.svc.id.goog
  status:
    conditions:
    - lastTransitionTime: "2020-08-21T18:44:53Z"
      message: reference IAMServiceAccount XXXX/xx-vm is
        not ready
      reason: DependencyNotReady
      status: "False"
      type: Ready
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
&lt;/denchmark-code&gt;

Let me know if you're able to direct me in the right direction to resolve this issue; hopefully that output helps in debugging / improving documentation on your end as well!
		</comment>
		<comment id='14' author='marrrcin' date='2020-08-24T01:25:08Z'>
		Hi &lt;denchmark-link:https://github.com/cm185255&gt;@cm185255&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;  status:
    conditions:
    - lastTransitionTime: "2020-08-21T18:44:53Z"
      message: reference IAMServiceAccount XXXX/xx-vm is
        not ready
      reason: DependencyNotReady
      status: "False"
      type: Ready
&lt;/denchmark-code&gt;

Here's the important part and it tells another service account this cluster depends on is not ready.
You can debug what went wrong with the service account in the same approach.
&lt;denchmark-code&gt;kubectl --context ${MGMTCONTEXT} describe iamserviceaccount -n ${PROJECT}
# note, you can also use `kubectl get -o yaml`, but describe's output is more readable
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='marrrcin' date='2020-08-24T11:15:16Z'>
		Hi all, I answered most of questions raised here centrally in &lt;denchmark-link:https://github.com/kubeflow/gcp-blueprints/issues/123&gt;kubeflow/gcp-blueprints#123&lt;/denchmark-link&gt;
. Please take a look and feel free to ask me any further questions.
		</comment>
		<comment id='16' author='marrrcin' date='2020-08-24T14:21:55Z'>
		Thanks &lt;denchmark-link:https://github.com/Bobgy&gt;@Bobgy&lt;/denchmark-link&gt;
 very much for the feedback and for providing the commands.
When I run kubectl --context ${MGMTCONTEXT} describe iamserviceaccount -n ${PROJECT}, this is the output:
&lt;denchmark-code&gt;Name:         xx-admin
Namespace:    XXXX
Labels:       kf-name=xx
Annotations:  cnrm.cloud.google.com/management-conflict-prevention-policy: none
              cnrm.cloud.google.com/project-id: XXXX
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"iam.cnrm.cloud.google.com/v1beta1","kind":"IAMServiceAccount","metadata":{"annotations":{},"labels":{"kf-name":"xx"}...
API Version:  iam.cnrm.cloud.google.com/v1beta1
Kind:         IAMServiceAccount
Metadata:
  Creation Timestamp:  2020-08-21T18:45:28Z
  Finalizers:
    cnrm.cloud.google.com/finalizer
    cnrm.cloud.google.com/deletion-defender
  Generation:        15
  Resource Version:  78378
  Self Link:         /apis/iam.cnrm.cloud.google.com/v1beta1/namespaces/XXXX/iamserviceaccounts/xx
  UID:               1c12bab9-d71b-417f-b3b0-88e25c5d569a
Spec:
  Display Name:  kubeflow admin service account
Status:
  Conditions:
    Last Transition Time:  2020-08-21T20:14:47Z
    Message:               The resource is up to date
    Reason:                UpToDate
    Status:                True
    Type:                  Ready
  Email:                   xx-admin@XXXX.iam.gserviceaccount.com
  Name:                    projects/XXXX7/serviceAccounts/xx-admin@XXXX.iam.gserviceaccount.com
  Unique Id:               101226608390048011955
Events:                    &lt;none&gt;

Name:         xx-user
Namespace:    XXXX
Labels:       kf-name=xx
Annotations:  cnrm.cloud.google.com/management-conflict-prevention-policy: none
              cnrm.cloud.google.com/project-id: XXXX
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"iam.cnrm.cloud.google.com/v1beta1","kind":"IAMServiceAccount","metadata":{"annotations":{},"labels":{"kf-name":"xx"}...
API Version:  iam.cnrm.cloud.google.com/v1beta1
Kind:         IAMServiceAccount
Metadata:
  Creation Timestamp:  2020-08-21T18:45:30Z
  Finalizers:
    cnrm.cloud.google.com/finalizer
    cnrm.cloud.google.com/deletion-defender
  Generation:        15
  Resource Version:  78430
  Self Link:         /apis/iam.cnrm.cloud.google.com/v1beta1/namespaces/XXXX/iamserviceaccounts/xx
  UID:               d7e94ed1-1946-458a-a6e7-cf626ccae211
Spec:
  Display Name:  kubeflow user service account
Status:
  Conditions:
    Last Transition Time:  2020-08-21T20:14:55Z
    Message:               The resource is up to date
    Reason:                UpToDate
    Status:                True
    Type:                  Ready
  Email:                   xxf-user@XXXX.iam.gserviceaccount.com
  Name:                    projects/XXXX/serviceAccounts/xx-user@XXXX.iam.gserviceaccount.com
  Unique Id:               104591388018196568815
Events:                    &lt;none&gt;

Name:         xx-vm
Namespace:    XXXX
Labels:       kf-name=xx
Annotations:  cnrm.cloud.google.com/management-conflict-prevention-policy: none
              cnrm.cloud.google.com/project-id: XXXX
              kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"iam.cnrm.cloud.google.com/v1beta1","kind":"IAMServiceAccount","metadata":{"annotations":{},"labels":{"kf-name":"xx"}...
API Version:  iam.cnrm.cloud.google.com/v1beta1
Kind:         IAMServiceAccount
Metadata:
  Creation Timestamp:  2020-08-21T18:45:31Z
  Finalizers:
    cnrm.cloud.google.com/finalizer
    cnrm.cloud.google.com/deletion-defender
  Generation:        18
  Resource Version:  78451
  Self Link:         /apis/iam.cnrm.cloud.google.com/v1beta1/namespaces/kubeflow-explore-287117/iamserviceaccounts/xx-vm
  UID:               0a6d947a-1317-4ed0-9049-4f1e0118910c
Spec:
  Display Name:  kubeflow vm service account
Status:
  Conditions:
    Last Transition Time:  2020-08-21T20:14:59Z
    Message:               The resource is up to date
    Reason:                UpToDate
    Status:                True
    Type:                  Ready
  Email:                   xx-vm@XXXX.iam.gserviceaccount.com
  Name:                    projects/XXXX/serviceAccounts/xx-vm@XXXX.iam.gserviceaccount.com
  Unique Id:               110984778110870330220
Events:                    &lt;none&gt;
&lt;/denchmark-code&gt;

From this output, everything looks fine with the kubeflow vm service account; it now says it is Ready.
However the useful portion of kubectl --context xx get containercluster -n XXXX -o yaml now reveals:
&lt;denchmark-code&gt;status:
    conditions:
    - lastTransitionTime: "2020-08-21T18:44:53Z"
      message: 'Update call failed: the desired mutation for the following field(s)
        is invalid: [nodeConfig.0.ServiceAccount nodeConfig.0.MachineType ipAllocationPolicy.#
        nodeConfig.0.OauthScopes.# nodeConfig.0.OauthScopes.3859019814 initialNodeCount]'
      reason: UpdateFailed
      status: "False"
      type: Ready
&lt;/denchmark-code&gt;

This message also prints as a warning at the bottom of kubectl describe containercluster --context xx -n XXXX xx:
&lt;denchmark-code&gt;Warning  UpdateFailed  52s   containercluster-controller  Update call failed: the desired mutation for the following field(s) is invalid: [nodeConfig.0.ServiceAccount nodeConfig.0.MachineType ipAllocationPolicy.# nodeConfig.0.OauthScopes.# nodeConfig.0.OauthScopes.3859019814 initialNodeCount]
&lt;/denchmark-code&gt;

I don't see anything online about this issue. Could you please let me know if you have any insight about this message, or what may be going on? Thanks!
		</comment>
		<comment id='17' author='marrrcin' date='2020-08-24T15:30:22Z'>
		&lt;denchmark-link:https://github.com/cm185255&gt;@cm185255&lt;/denchmark-link&gt;
 The error indicates your configuration is invalid. Can you attach the output of
&lt;denchmark-code&gt;kubectl --context=${MGMT-CTXT} --namespace ${PROJECT} get containercluster -o yaml
&lt;/denchmark-code&gt;

Does the cluster actually exist? e.g.
&lt;denchmark-code&gt;gcloud --project=${PROJECT} container clusters list
&lt;/denchmark-code&gt;

Try deleting the K8s resource and recreating it
&lt;denchmark-code&gt;kubectl --context=${MGMT-CTXT} --namespace ${PROJECT} delete containercluster ${NAM}
make apply
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='marrrcin' date='2020-08-24T16:48:27Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 Thanks for you feedback.
The full output for kubectl --context=xx --namespace XXXX get containercluster -o yaml (with ${PROJECT} changed to XXXX and ${MGMT-CTXT} changed to xx) is:
&lt;denchmark-code&gt;apiVersion: v1
items:
- apiVersion: container.cnrm.cloud.google.com/v1beta1
  kind: ContainerCluster
  metadata:
    annotations:
      cnrm.cloud.google.com/management-conflict-prevention-policy: resource
      cnrm.cloud.google.com/project-id: XXXX
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"container.cnrm.cloud.google.com/v1beta1","kind":"ContainerCluster","metadata":{"annotations":{},"clusterName":"XXXX/us-east1-b/xx","labels":{"kf-name":"xxf","mesh_id":"XXXX_us-east1-b_xx"},"name":"xx","namespace":"XXXX"},"spec":{"clusterAutoscaling":{"autoProvisioningDefaults":{"oauthScopes":["https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/devstorage.read_only"],"serviceAccountRef":{"name":"xx-vm"}},"enabled":true,"resourceLimits":[{"maximum":128,"resourceType":"cpu"},{"maximum":2000,"resourceType":"memory"},{"maximum":16,"resourceType":"nvidia-tesla-k80"}]},"initialNodeCount":2,"location":"us-east1-b","loggingService":"logging.googleapis.com/kubernetes","monitoringService":"monitoring.googleapis.com/kubernetes","nodeConfig":{"machineType":"n1-standard-8","metadata":{"disable-legacy-endpoints":"true"},"oauthScopes":["https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/devstorage.read_only"],"serviceAccountRef":{"name":"xx-vm"},"workloadMetadataConfig":{"nodeMetadata":"GKE_METADATA_SERVER"}},"releaseChannel":{"channel":"REGULAR"},"workloadIdentityConfig":{"identityNamespace":"XXXX.svc.id.goog"}}}
    creationTimestamp: "2020-08-24T16:00:09Z"
    generation: 20
    labels:
      kf-name: xx
      mesh_id: XXXX_us-east1-b_xx
    name: xx
    namespace: XXXX
    resourceVersion: "1290712"
    selfLink: /apis/container.cnrm.cloud.google.com/v1beta1/namespaces/XXXX/containerclusters/xx
    uid: 0c9868a3-d248-430a-aa42-7543fc2b6b09
  spec:
    clusterAutoscaling:
      autoProvisioningDefaults:
        oauthScopes:
        - https://www.googleapis.com/auth/logging.write
        - https://www.googleapis.com/auth/monitoring
        - https://www.googleapis.com/auth/devstorage.read_only
        serviceAccountRef:
          name: xx-vm
      enabled: true
      resourceLimits:
      - maximum: 128
        resourceType: cpu
      - maximum: 2000
        resourceType: memory
      - maximum: 16
        resourceType: nvidia-tesla-k80
    initialNodeCount: 2
    location: us-east1-b
    loggingService: logging.googleapis.com/kubernetes
    monitoringService: monitoring.googleapis.com/kubernetes
    nodeConfig:
      machineType: n1-standard-8
      metadata:
        disable-legacy-endpoints: "true"
      oauthScopes:
      - https://www.googleapis.com/auth/logging.write
      - https://www.googleapis.com/auth/monitoring
      - https://www.googleapis.com/auth/devstorage.read_only
      serviceAccountRef:
        name: xx-vm
      workloadMetadataConfig:
        nodeMetadata: GKE_METADATA_SERVER
    releaseChannel:
      channel: REGULAR
    workloadIdentityConfig:
      identityNamespace: XXXX.svc.id.goog
  status:
    conditions:
    - lastTransitionTime: "2020-08-24T16:00:12Z"
      message: 'Update call failed: the desired mutation for the following field(s)
        is invalid: [nodeConfig.0.OauthScopes.# ipAllocationPolicy.# initialNodeCount
        nodeConfig.0.OauthScopes.3859019814 nodeConfig.0.MachineType nodeConfig.0.ServiceAccount]'
      reason: UpdateFailed
      status: "False"
      type: Ready
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
&lt;/denchmark-code&gt;

with the same UpdateFailed message printed in the status section.
The cluster does seem to exist from gcloud --project=${PROJECT} container clusters list
&lt;denchmark-code&gt;NAME       LOCATION    MASTER_VERSION  MASTER_IP      MACHINE_TYPE   NODE_VERSION   NUM_NODES  STATUS
xx         us-east1-b  1.16.13-gke.1   xx.xxx.xxx.xx  n1-standard-4  1.16.13-gke.1  2          RUNNING
&lt;/denchmark-code&gt;

when I check the deployment via
&lt;denchmark-code&gt;gcloud container clusters get-credentials ${KF_NAME} --zone ${ZONE} --project ${PROJECT}
kubectl -n kubeflow get all
&lt;/denchmark-code&gt;

from the &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#configure-kubeflow&gt;tutorial&lt;/denchmark-link&gt;
 I get:
&lt;denchmark-code&gt;Fetching cluster endpoint and auth data.
kubeconfig entry generated for xx.

No resources found in kubeflow namespace.
&lt;/denchmark-code&gt;

I tried the final step of deleting the K8s resource and recreating, but was met with the same timeout issue and the same UpdateFailed status and other outputs detailed above
		</comment>
		<comment id='19' author='marrrcin' date='2020-08-24T23:42:36Z'>
		&lt;denchmark-link:https://github.com/cm185255&gt;@cm185255&lt;/denchmark-link&gt;
 can you try this to make sure it's not race condition?
&lt;denchmark-code&gt;kubectl --context=${MGMT-CTXT} --namespace ${PROJECT} delete containercluster ${NAM}
# wait a while
gcloud --project=${PROJECT} container clusters list
# verify the cluster has already been deleted
make apply
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='marrrcin' date='2020-08-24T23:52:13Z'>
		&lt;denchmark-link:https://github.com/cm185255&gt;@cm185255&lt;/denchmark-link&gt;
 I created an issue for you in Cloud Config Connector's repo. We can follow up there if this still doesn't work.
&lt;denchmark-link:https://github.com/GoogleCloudPlatform/k8s-config-connector/issues/259&gt;GoogleCloudPlatform/k8s-config-connector#259&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='marrrcin' date='2020-11-07T19:35:17Z'>
		I am experiencing this issue too when following &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#deploy-kubeflow&gt;https://www.kubeflow.org/docs/gke/deploy/deploy-cli/#deploy-kubeflow&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='marrrcin' date='2020-11-07T20:24:30Z'>
		&lt;denchmark-link:https://github.com/Rustastra&gt;@Rustastra&lt;/denchmark-link&gt;
 Per &lt;denchmark-link:https://github.com/GoogleCloudPlatform/k8s-config-connector/issues/259&gt;GoogleCloudPlatform/k8s-config-connector#259&lt;/denchmark-link&gt;
 it looks like the problem is fixed in newer versions of KCC.
I would suggest following the &lt;denchmark-link:https://cloud.google.com/config-connector/docs/how-to/install-other-kubernetes&gt;Cloud Config Connector Docs&lt;/denchmark-link&gt;
 to install or upgrade KCC on your management cluster.
		</comment>
		<comment id='23' author='marrrcin' date='2020-11-07T20:56:51Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 thank you! after a lot of poking around, I realised what the error was for me after seeing this:
"Just for confirmation, did you try to deploy kubeflow into the management cluster? That's not supported. You need to use a different cluster for kubeflow." &lt;denchmark-link:https://github.com/GoogleCloudPlatform/k8s-config-connector/issues/259#issuecomment-680169631&gt;here &lt;/denchmark-link&gt;

I was setting the same values to KF_NAME and NAME in Makefile, which resulted in an attempt to deploy KF into management cluster.
		</comment>
		<comment id='24' author='marrrcin' date='2020-11-09T00:41:18Z'>
		I think most of the issues mentioned in the original issue has been either fixed or documentation updated to clarify that. Also most of people who replied got there problem solved, but original reporter &lt;denchmark-link:https://github.com/marrrcin&gt;@marrrcin&lt;/denchmark-link&gt;
 didn't reply here.
Closing the issue to clean up backlog, if anyone has any further problems, feel free to create a new issue in &lt;denchmark-link:http://www.github.com/kubeflow/gcp-blueprints&gt;www.github.com/kubeflow/gcp-blueprints&lt;/denchmark-link&gt;
 repo.
I am here to keep supporting Kubeflow GCP installation guide, and I keep making sure it's possible to deploy (as it is today). There might be edge cases that needs everyone's help to report &amp; fix/document, because this is open source.
EDIT:
and add a quick list of helpful resources in this thread:

To troubleshoot the issue's error message (wait containercluster timeout): see instructions in #5223 (comment)
For understanding changes in Kubeflow 1.1 compared to 1.0, see kubeflow/gcp-blueprints#123
A common mistake that can cause above error is using the same name for management cluster and kubeflow cluster, they SHOULD NOT be the same. See kubeflow/gcp-blueprints#142

		</comment>
	</comments>
</bug>