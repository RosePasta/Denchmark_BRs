<bug id='2312' author='jayunit100' open_date='2019-01-22T15:48:08Z' closed_time='2019-05-22T22:45:30Z'>
	<summary>kubeflow vanilla style installation is a little fragile (at least on GKE)</summary>
	<description>
This issue is a combination of questions and some notes from my initial installation.
&lt;denchmark-h:h2&gt;Initial observation on GKE&lt;/denchmark-h&gt;

Although my dashboard came up, there was no indicator that all the services were down, could we have such a thing to early detect / provide feedback ?
&lt;denchmark-link:https://user-images.githubusercontent.com/826111/51546870-19692f80-1e33-11e9-87e2-6920dfb7f359.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Questions&lt;/denchmark-h&gt;


Is there a vanilla installer ?
Do we support it on GKE ?
Is any kube cluster type supported ?

&lt;denchmark-h:h2&gt;Bug ?&lt;/denchmark-h&gt;

In general, this isnt a bug because, we did not follow the GKE instructions, but I think the feedback in here could result in a few UX tweaks that make the first time experience for running a little more predictable.
Feel free to close if this isn't actionable....
suggestion: if we can fail fast when installing on a kube cluster that might not work properly, we should, for example, alot of services were 404'ing for me.
observations: I noticed 3 interesting issues when deploying on GKE, using the Ksonnet, noticed a couple of things which can be improved.


404's on the katib dashboard (and all others), when port forwarding


Ambassador logs seem like they are exiting:


&lt;denchmark-code&gt;[2019-01-22 15:30:27.809][64][info][main] source/server/server.cc:193]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector
[2019-01-22 15:30:27.809][64][info][main] source/server/server.cc:196]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy
[2019-01-22 15:30:27.809][64][info][main] source/server/server.cc:198]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.statsd
[2019-01-22 15:30:27.809][64][info][main] source/server/server.cc:200]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.zipkin
[2019-01-22 15:30:27.809][64][info][main] source/server/server.cc:203]   transport_sockets.downstream: envoy.transport_sockets.capture,raw_buffer,tls
[2019-01-22 15:30:27.809][64][info][main] source/server/server.cc:206]   transport_sockets.upstream: envoy.transport_sockets.capture,raw_buffer,tls
[2019-01-22 15:30:27.825][24][warning][main] source/server/server.cc:449] shutting down admin due to child startup
[2019-01-22 15:30:27.825][24][warning][main] source/server/server.cc:457] terminating parent process
[2019-01-22 15:30:27.828][64][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
[2019-01-22 15:30:27.846][64][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
[2019-01-22 15:30:27.864][64][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
[2019-01-22 15:30:27.864][64][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
[2019-01-22 15:30:27.864][64][info][main] source/server/server.cc:398] starting main dispatch loop
[2019-01-22 15:30:27.876][64][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
[2019-01-22 15:30:27.876][64][info][main] source/server/server.cc:378] all clusters initialized. initializing init manager
[2019-01-22 15:30:27.876][64][info][config] source/server/listener_manager_impl.cc:781] all dependencies initialized. starting workers
[2019-01-22 15:30:27.877][24][info][main] source/server/server.cc:98] closing and draining listeners
[2019-01-22 15:30:37.877][64][info][main] source/server/drain_manager_impl.cc:63] shutting down parent after drain
[2019-01-22 15:30:37.878][24][info][main] source/server/hot_restart_impl.cc:435] shutting down due to child request
[2019-01-22 15:30:37.878][24][warning][main] source/server/server.cc:348] caught SIGTERM
[2019-01-22 15:30:37.878][24][info][main] source/server/server.cc:402] main dispatch loop exited
[2019-01-22 15:30:37.911][24][info][main] source/server/server.cc:437] exiting
got SIGCHLD
PID=24 exited with code=0
&lt;/denchmark-code&gt;


The env.sh  is appened, rather then overwritten, making a very confusing env file .

&lt;denchmark-code&gt;jayunit100-MID-2015-16GB:0.3.5 jayunit100$ cat env.sh 
PLATFORM=none
KUBEFLOW_REPO=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/kubeflow_gke/0.3.5
KUBEFLOW_VERSION=0.3.5
KUBEFLOW_KS_DIR=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/my_kubeflow_gke/0.3.5/ks_app
KUBEFLOW_DOCKER_REGISTRY=
K8S_NAMESPACE=kubeflow
KUBEFLOW_CLOUD=null
PLATFORM=none
KUBEFLOW_REPO=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/kubeflow_gke/0.3.5
KUBEFLOW_VERSION=0.3.5
KUBEFLOW_KS_DIR=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/my_kubeflow_gke/0.3.5/ks_app
KUBEFLOW_DOCKER_REGISTRY=
K8S_NAMESPACE=kubeflow
KUBEFLOW_CLOUD=null
PLATFORM=none
KUBEFLOW_REPO=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/kubeflow_gke/0.3.5
KUBEFLOW_VERSION=0.3.5
KUBEFLOW_KS_DIR=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/my_kubeflow_gke/0.3.5/ks_app
KUBEFLOW_DOCKER_REGISTRY=
K8S_NAMESPACE=kubeflow
KUBEFLOW_CLOUD=null
PLATFORM=none
KUBEFLOW_REPO=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/kubeflow_gke/0.3.5
KUBEFLOW_VERSION=0.3.5
KUBEFLOW_KS_DIR=/Users/jayunit100/work/gram.ai/kubeflow-hacking/install_scripts/my_kubeflow_gke/0.3.5/ks_app
KUBEFLOW_DOCKER_REGISTRY=
K8S_NAMESPACE=kubeflow
KUBEFLOW_CLOUD=null
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jayunit100' date='2019-01-25T01:11:16Z'>
		Thanks this is great feedback and I generally agree with it. Adding additional monitoring centered around Kubernetes Applications is happening but very slowly so we'd love contributions.
Kubeflow now gets deployed with a K8s Application so you can do
&lt;denchmark-code&gt;kubectl describe application ${KFAPP}
&lt;/denchmark-code&gt;

There's a lot more work to do provide relevant information.
We'd like to add agents to monitor status and then report it using K8s events.
No one is actively working on this so if you're interested in helping out that would be awesome.
A PR to fix kfctl.sh would also be extremly welcome. We are currently refocused on re-writing it as a go binary to create a much better user experience.
		</comment>
		<comment id='2' author='jayunit100' date='2019-01-28T19:44:26Z'>
		I've been working on this with &lt;denchmark-link:https://github.com/jayunit100&gt;@jayunit100&lt;/denchmark-link&gt;
 trying to put together a simple installation script (see below) that isn't tied to a specific type of kube cluster. This was created by going through the kubeflow/scripts folder and removing anything that was platform specific.
&lt;denchmark-code&gt;#!/usr/bin/env bash
set -e
set -u
export KUBEFLOW_VERSION=0.4.0
export KUBEFLOW_TAG=v${KUBEFLOW_VERSION}
export KUBEFLOW_SRC="${PWD}/kubeflow/${KUBEFLOW_VERSION}"
export KS_APP="${PWD}/kubeflow_ks/${KUBEFLOW_VERSION}"
export DEPLOYMENT_NAME="ks-app"
export KUBEFLOW_COMPONENTS='"ambassador","centraldashboard","tf-job-operator","argo","katib"'
export KUBEFLOW_PLATFORM="minikube"
export K8S_NAMESPACE="kubeflow"

function preflight() {
  echo "verifying ksonnet is installed..."
  if ! [ -x "$(command -v ks)" ]; then
    echo 'Error: ksonnet is not installed.' &gt;&amp;2
    echo "Put it on your path before running this installer"
    exit 1
  fi
  # In case someone tries to run this script standalone...
  if ! [ -d k8s ]; then
    echo "The companion yml files for installation are missing!"
  fi
}

function create_directories() {
  mkdir -p ${KUBEFLOW_SRC}
  mkdir -p ${KS_APP}
}
function download_kubeflow() {
  pushd ${KUBEFLOW_SRC}
  curl https://raw.githubusercontent.com/kubeflow/kubeflow/v${KUBEFLOW_VERSION}/scripts/download.sh &gt; download.sh
  bash download.sh &amp;&gt;/dev/null
  popd
}
function init_deployment() {
  pushd ${KS_APP}
  eval ks init ${DEPLOYMENT_NAME} --skip-default-registries
  popd
}
function install_kubeflow_components() {
  pushd "${KS_APP}/${DEPLOYMENT_NAME}"
  # Install kubeflow registry for ks
  ks registry add kubeflow "${KUBEFLOW_SRC}/kubeflow"
  # Install all required ks packages
  ks pkg install kubeflow/argo@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/pipeline@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/common@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/examples@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/jupyter@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/katib@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/mpi-job@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/pytorch-job@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/seldon@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/tf-serving@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/openvino@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/tensorboard@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/tf-training@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/metacontroller@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/profiles@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/application@v${KUBEFLOW_VERSION}-branch
  ks pkg install kubeflow/modeldb@v${KUBEFLOW_VERSION}-branch
  # Generate all required components
  ks generate pytorch-operator pytorch-operator
  ks generate ambassador ambassador
  # ks generate openvino openvino
  ks generate jupyter jupyter
  ks generate centraldashboard centraldashboard
  ks generate tf-job-operator tf-job-operator
  # ks generate tensorboard tensorboard
  ks generate metacontroller metacontroller
  # ks generate profiles profiles
  ks generate notebooks notebooks
  ks generate argo argo
  # ks generate pipeline pipeline
  ks generate katib katib
  ks generate application application
  popd
}
configure_ksonnet() {
  pushd "${KS_APP}/${DEPLOYMENT_NAME}"
  ks param set ambassador platform ${KUBEFLOW_PLATFORM}
  ks param set ambassador replicas 1
  popd
}
create_namespace() {
  kubectl create namespace ${K8S_NAMESPACE} || echo "Namespace already exists, moving on..."
}
deploy_application() {
  pushd "${KS_APP}/${DEPLOYMENT_NAME}"
  ks env set default --namespace "${K8S_NAMESPACE}"
  ks param set application components '['$KUBEFLOW_COMPONENTS']'
  ks param set application name ${DEPLOYMENT_NAME}
  ks show default &gt; default.yaml
  kubectl apply --validate=false -f default.yaml
  popd
}
setup_tunnels() {
  sleep 30 # Give services time to bind
  kubectl -n ${K8S_NAMESPACE} port-forward svc/ambassador 8080:80
}
preflight
create_directories
download_kubeflow
init_deployment
install_kubeflow_components
configure_ksonnet
create_namespace
deploy_application
setup_tunnels
&lt;/denchmark-code&gt;

It runs fine on minikube but if I run it on a GKE cluster I get the following error.
&lt;denchmark-code&gt;The CustomResourceDefinition "tfjobs.kubeflow.org" is invalid: spec.version: Invalid value: "v1beta1": must match the first version in spec.versions
&lt;/denchmark-code&gt;

I was surprised the API versions work on minikube but not on GKE. We had a similar experience with a previous installation on GKE when creating tfjobs. If we were running on minikube we had to set the API version to v1beta1 to make it work, but if we were running on GKE we had to set the API version to v1alpha2.
We managed to fix the 404 errors on minikube by adding the setup_tunnels function to our installation but we still get 404 errors when running on GKE and using port-forward to view the central dashboard. On minikube, if we don't add setup_tunnels to the installation we can still navigate to the tfjob dashboard but not the other ones. Do you know why some of the dashboard links require port-forwarding while others don't?
One final issue we ran into was figuring out how the different components depend on each other. For example, we are using Tensorflow so initially didn't create the pytorch operators. However, when we added katib we found that we had to install the pytorch operator to make katib work. Another example is some of the dashboards relying on ambassador while other's don't. Is this clearly documented anywhere?
Our long-term goal is to run kubeflow on our own on-prem cluster, and we are just using minikube and GKE to test out the installation. This seems to align with one of the current CUJs ( &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/2327&gt;#2327&lt;/denchmark-link&gt;
 ) and we'd love to help out. We are also excited to help migrate the kfctl.sh to go. We initially tried using this script to install but struggled to get it working for scenarios not in the getting started section on the website.
		</comment>
		<comment id='3' author='jayunit100' date='2019-05-15T22:43:45Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>