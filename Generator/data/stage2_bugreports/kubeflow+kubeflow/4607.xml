<bug id='4607' author='rky0930' open_date='2019-12-28T18:09:01Z' closed_time='2020-02-05T23:12:14Z'>
	<summary>GKE Kubeflow google cloud SDK "ERROR: timed out"</summary>
	<description>
/kind bug
What steps did you take and what happened:
[A clear and concise description of what the bug is.]

Deploy Kubeflow 0.7 on GKE
Run a Pods(docker image google/cloud-sdk:273.0.0)
start to use google cloud SDK(gsutil)
gsutil and glcoud start to timeout few hours later

The timeout error does not occur initially, but begins to occur after a few hours.
Once it occurs, the timeout error continues to occur.
I did some research and couldn't find a reason. If anyone knows the reason for this, please help.
gsutil timeout message
&lt;denchmark-code&gt;yoon@daangn-gpu2:~$ k exec -it -n kubeflow yoon-preprocess-test-pod /bin/bash
root@yoon-preprocess-test-pod:/app# gsutil ls gs://&lt;GS_URL&gt;
ERROR: (gsutil) timed out
This may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.
&lt;/denchmark-code&gt;

gcloud timeout message
&lt;denchmark-code&gt;yoon@daangn-gpu2:~$ k exec -it -n kubeflow yoon-preprocess-test-pod /bin/bash
root@yoon-preprocess-test-pod:/app# gcloud auth list
ERROR: (gcloud.auth.list) timed out
This may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.
&lt;/denchmark-code&gt;

What did you expect to happen:
No problem using google cloud sdk
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:


Kubeflow version: build version 0.7.0


kfctl version: kfctl v0.7.0


Kubernetes platform: GKE


Kubernetes version:
Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.6", GitCommit:"7015f71e75f670eb9e7ebd4b5749639d42e20079", GitTreeState:"clean", BuildDate:"2019-11-13T11:20:18Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.9-gke.2", GitCommit:"0f206d1d3e361e1bfe7911e1e1c686bc9a1e0aa5", GitTreeState:"clean", BuildDate:"2019-11-25T19:35:58Z", GoVersion:"go1.12.12b4", Compiler:"gc", Platform:"linux/amd64"}


OS (e.g. from /etc/os-release):


Inside Pod -
NAME="Ubuntu"
VERSION="16.04.5 LTS (Xenial Xerus)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 16.04.5 LTS"
VERSION_ID="16.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
VERSION_CODENAME=xenial



&lt;denchmark-link:https://user-images.githubusercontent.com/5013450/71547727-21d4cd80-29e7-11ea-82c2-1aecb8c0e452.png&gt;&lt;/denchmark-link&gt;


&lt;denchmark-link:https://user-images.githubusercontent.com/5013450/71547700-c1de2700-29e6-11ea-967a-2c1a501bd30d.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='rky0930' date='2019-12-28T18:09:14Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='rky0930' date='2019-12-29T20:32:22Z'>
		&lt;denchmark-link:https://github.com/rky0930&gt;@rky0930&lt;/denchmark-link&gt;
 this doesn't look like a Kubeflow issue but a GCP or GKE issue.
Do you observe the same behavior on non-Kubeflow GKE clusters? Can you reproduce the behavior on a GKE cluster at the same version (1.14) with workload identity enabled?
We've observed some problems in the past with workload identity see &lt;denchmark-link:https://github.com/kubeflow/kfctl/issues/48&gt;kubeflow/kfctl#48&lt;/denchmark-link&gt;
.
Can you run the command
&lt;denchmark-code&gt;kubectl -n kube-system get pods
&lt;/denchmark-code&gt;

To verify that the GKE metadata server pods are running and healthy.
		</comment>
		<comment id='3' author='rky0930' date='2019-12-30T02:54:03Z'>
		I got the same issue: &lt;denchmark-link:https://github.com/kubeflow/pipelines/issues/2773&gt;kubeflow/pipelines#2773&lt;/denchmark-link&gt;

I think that is GKE's issue. The workload identity is unstable now
		</comment>
		<comment id='4' author='rky0930' date='2019-12-30T06:54:41Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 Thanks for the answer.
All pods in kube-system is running status now.
&lt;denchmark-code&gt;yoon@daangn-gpu2:~$ kubectl -n kube-system get pods
NAME                                                           READY   STATUS    RESTARTS   AGE
event-exporter-v0.2.5-7df89f4b8f-66w5r                         2/2     Running   0          3d18h
fluentd-gcp-scaler-54ccb89d5-pnl7g                             1/1     Running   0          3d18h
fluentd-gcp-v3.1.1-7rmg9                                       2/2     Running   0          3d18h
fluentd-gcp-v3.1.1-mjc9f                                       2/2     Running   0          3d18h
gke-metadata-server-fv5jt                                      1/1     Running   0          3d18h
gke-metadata-server-mxhs2                                      1/1     Running   0          3d18h
heapster-7d456cb754-nmjv4                                      3/3     Running   0          3d18h
kube-dns-5877696fb4-qmmvp                                      4/4     Running   0          3d18h
kube-dns-5877696fb4-sdlbh                                      4/4     Running   0          3d18h
kube-dns-autoscaler-85f8bdb54-gflsw                            1/1     Running   0          3d18h
kube-proxy-gke-kubeflow2-kubeflow2-cpu-pool-v1-755478a8-937x   1/1     Running   0          3d18h
kube-proxy-gke-kubeflow2-kubeflow2-cpu-pool-v1-755478a8-wszw   1/1     Running   0          3d18h
l7-default-backend-8f479dd9-prz7q                              1/1     Running   0          3d18h
metrics-server-v0.3.1-5c6fbf777-ctlqd                          2/2     Running   0          3d18h
netd-52zl5                                                     1/1     Running   0          3d18h
netd-lfvp4                                                     1/1     Running   0          3d18h
prometheus-to-sd-9f55j                                         2/2     Running   0          3d18h
prometheus-to-sd-hgk27                                         2/2     Running   0          3d18h
stackdriver-metadata-agent-cluster-level-6bf96696bf-st8g4      1/1     Running   0          3d18h
&lt;/denchmark-code&gt;

And I checked liveness of gke-metadata-server, I seem unstatble
&lt;denchmark-code&gt;    Liveness:     http-get http://127.0.0.1:54898/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='rky0930' date='2019-12-30T06:55:08Z'>
		&lt;denchmark-link:https://github.com/bruce3557&gt;@bruce3557&lt;/denchmark-link&gt;
 Thanks for the answer.
Looks like the same issue. Did you find a solution?
Is binding the workload to a pipeline runner the solution?
I think I need to check more of the workload identity side.
		</comment>
		<comment id='6' author='rky0930' date='2019-12-30T07:00:45Z'>
		&lt;denchmark-link:https://github.com/rky0930&gt;@rky0930&lt;/denchmark-link&gt;

Binding workload identity just solved the pipeline issue.
But about timeout issue, we need to wait for GCP fixing that.
		</comment>
		<comment id='7' author='rky0930' date='2019-12-30T15:13:56Z'>
		&lt;denchmark-link:https://github.com/rky0930&gt;@rky0930&lt;/denchmark-link&gt;
 can you provide the logs of the gke-metadata-server pods? You can use this script to fetch the logs.
&lt;denchmark-link:https://github.com/jlewi/kubeflow-dev/blob/master/fetch_stackdriver_pod_logs.sh&gt;https://github.com/jlewi/kubeflow-dev/blob/master/fetch_stackdriver_pod_logs.sh&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='rky0930' date='2019-12-30T21:20:59Z'>
		We just filed a ticket with Google Support about this today. We see this problem on 1.15.4-gke.22 but not on 1.14.9-gke.2 1.14.8-gke.18. We are using workload identity for our pipelines.  The gke-metadata-server daemonset pods report:
&lt;denchmark-code&gt;getServiceAccountMetadata("workload-identity-test",
 "kubeflow-prod-user@myproject.iam.gserviceaccount.com"): 
 surfaceable error (status 500, message "Unable to generate access token"): 
getFederatedToken("myproject.svc.id.goog", pod): Post 
https://securetoken.googleapis.com/v1/identitybindingtoken: context canceled
&lt;/denchmark-code&gt;

We are able to temporarily resolve the issue by cycling the gke-metadata-server daemonset pods.
&lt;denchmark-code&gt;kubectl delete pods -n kube-system --selector=k8s-app=gke-metadata-server
&lt;/denchmark-code&gt;

Related Google Ticket: &lt;denchmark-link:https://issuetracker.google.com/issues/146622472&gt;https://issuetracker.google.com/issues/146622472&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='rky0930' date='2020-01-02T16:20:57Z'>
		&lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 I tried running the script, but there was no output. Maybe something was wrong. After that, I was unable to retry as I reinstalled the cluster. Thanks.
		</comment>
		<comment id='10' author='rky0930' date='2020-01-02T16:27:38Z'>
		&lt;denchmark-link:https://github.com/louisvernon&gt;@louisvernon&lt;/denchmark-link&gt;
 I fixed it by downgrading the cluster to version 1.14.8-gke.18. I will keep checking the tickets you have left and will upgrade the cluster later. Thanks.
		</comment>
		<comment id='11' author='rky0930' date='2020-01-16T22:03:45Z'>
		&lt;denchmark-link:https://github.com/rky0930&gt;@rky0930&lt;/denchmark-link&gt;
 how do you downgrade? Can't seem to find where that is defined.
		</comment>
		<comment id='12' author='rky0930' date='2020-01-17T03:05:39Z'>
		&lt;denchmark-link:https://github.com/yantriks-edi-bice&gt;@yantriks-edi-bice&lt;/denchmark-link&gt;
 Oh sorry. I deleted and reinstalled the GKE cluster to downgrade.  I have not tried downgrading an existing cluster.
		</comment>
		<comment id='13' author='rky0930' date='2020-01-23T03:24:08Z'>
		Despite the downgrade GKE cluster, gcloud SDK "time out error" occurred again.. Finally, we disabled GKE metadata server option in the node pool. And it's working without error.
		</comment>
		<comment id='14' author='rky0930' date='2020-01-24T15:05:23Z'>
		Hey &lt;denchmark-link:https://github.com/rky0930&gt;@rky0930&lt;/denchmark-link&gt;
 , not sure if this is a good idea if one wants to use  as there is no request token can be used.
		</comment>
		<comment id='15' author='rky0930' date='2020-01-25T05:47:17Z'>
		&lt;denchmark-link:https://github.com/Felihong&gt;@Felihong&lt;/denchmark-link&gt;
 I don't think that workload idenetities can be used with errors. We can use &lt;denchmark-link:https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity?_ga=2.131383436.-2082640807.1576128972#alternatives&gt;Alternatives&lt;/denchmark-link&gt;
 now, and later use GKE Metadata server when workload identities are GA and the bug is fixed. The workload identities are in Beta now.
		</comment>
		<comment id='16' author='rky0930' date='2020-02-05T23:12:14Z'>
		see &lt;denchmark-link:https://cloud.google.com/kubernetes-engine/docs/release-notes#january_24_2020&gt;https://cloud.google.com/kubernetes-engine/docs/release-notes#january_24_2020&lt;/denchmark-link&gt;
 for release notes about GKE versions to avoid with bugs in workload identity
see &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4693&gt;#4693&lt;/denchmark-link&gt;

The Kubeflow 0.7.1 and 1.0.0 configs are pinned to known good versions.
		</comment>
		<comment id='17' author='rky0930' date='2020-03-05T06:05:40Z'>
		FYI, for anyone still hitting intermittent permission issues with GKE workload identity.
First, your GKE version should be one of the good versions mentioned in above comment.
Second, your clients should use latest google/cloud-sdk, version &gt;=277.0.0
I verified intermittent timeouts go away with 1.14.8-gke.33 and google/cloud-sdk:279.0.0. Recommend upgrading language specific clients to latest version too.
		</comment>
	</comments>
</bug>