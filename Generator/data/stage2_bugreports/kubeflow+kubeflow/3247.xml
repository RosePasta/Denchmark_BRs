<bug id='3247' author='liusheng' open_date='2019-05-11T03:26:52Z' closed_time='2019-05-14T01:15:50Z'>
	<summary>TFjobs of tensorflow beenchmaks still in running status after benchmarks finished</summary>
	<description>
I have ran  the tf_cnn_benchmarks testing in a kubeflow v0.5.0 + k8s 1.14.0 env, after the benchmarks running finished, I expect  the tfjob status should be Succeeded or Complated status, but it is still in Running status. see:
&lt;denchmark-code&gt;root@ubuntu:~# kubectl -n kf-tfbenchmarks get pods
NAME                         READY   STATUS    RESTARTS   AGE
kf-tfbenchmarks-1-ps-0       1/1     Running   0          172m
kf-tfbenchmarks-1-worker-0   1/1     Running   0          172m
root@ubuntu:~# kubectl -n kf-tfbenchmarks get tfjobs
NAME                STATE     AGE
kf-tfbenchmarks-1   Running   172m
root@ubuntu:~# kubectl -n kf-tfbenchmarks logs kf-tfbenchmarks-1-worker-0
INFO|2019-05-11T00:23:05|/opt/launcher.py|48| Launcher started.
INFO|2019-05-11T00:23:05|/opt/launcher.py|73| Command to run: python tf_cnn_benchmarks.py --batch_size=32 --model=resnet50 --variable_update=parameter_server --flush_stdout=true --num_gpus=
1 --local_parameter_device=cpu --device=cpu --data_format=NHWC --job_name=worker --ps_hosts=kf-tfbenchmarks-1-ps-0.kf-tfbenchmarks.svc:2222 --worker_hosts=kf-tfbenchmarks-1-worker-0.kf-tfbe
nchmarks.svc:2222 --task_index=0
INFO|2019-05-11T00:23:05|/opt/launcher.py|15| Running python tf_cnn_benchmarks.py --batch_size=32 --model=resnet50 --variable_update=parameter_server --flush_stdout=true --num_gpus=1 --loca
l_parameter_device=cpu --device=cpu --data_format=NHWC --job_name=worker --ps_hosts=kf-tfbenchmarks-1-ps-0.kf-tfbenchmarks.svc:2222 --worker_hosts=kf-tfbenchmarks-1-worker-0.kf-tfbenchmarks
.svc:2222 --task_index=0
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| 2019-05-11 00:23:06.901639: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was
 not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| E0511 00:23:06.901877649       6 ev_epoll1_linux.c:1051]     grpc epoll fd: 3
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| 2019-05-11 00:23:06.909395: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; kf-t
fbenchmarks-1-ps-0.kf-tfbenchmarks.svc:2222}
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| 2019-05-11 00:23:06.909442: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt;
localhost:2222}
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| 2019-05-11 00:23:06.910685: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| TensorFlow:  1.5
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Model:       resnet50
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Mode:        training
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| SingleSess:  False
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Batch size:  32 global
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| 32 per device
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Devices:     ['/job:worker/task:0/cpu:0']
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Data format: NHWC
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Optimizer:   sgd
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Variables:   parameter_server
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Sync:        True
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| ==========
INFO|2019-05-11T00:23:06|/opt/launcher.py|27| Generating model
INFO|2019-05-11T00:23:11|/opt/launcher.py|27| 2019-05-11 00:23:11.777894: I tensorflow/core/distributed_runtime/master_session.cc:1008] Start master session 20b392b09a648827 with config: intra_op_parallelism_threads: 1 gpu_options { force_gpu_compatible: true } allow_soft_placement: true
INFO|2019-05-11T00:23:13|/opt/launcher.py|27| Running warm up
INFO|2019-05-11T00:32:14|/opt/launcher.py|27| Done warm up
INFO|2019-05-11T00:32:14|/opt/launcher.py|27| Step      Img/sec loss
INFO|2019-05-11T00:33:08|/opt/launcher.py|27| 1 images/sec: 0.6 +/- 0.0 (jitter = 0.0)  10.224
INFO|2019-05-11T00:41:15|/opt/launcher.py|27| 10        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  8.759
INFO|2019-05-11T00:50:16|/opt/launcher.py|27| 20        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  8.227
INFO|2019-05-11T00:59:17|/opt/launcher.py|27| 30        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  8.199
INFO|2019-05-11T01:08:17|/opt/launcher.py|27| 40        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  9.441
INFO|2019-05-11T01:17:19|/opt/launcher.py|27| 50        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  8.111
INFO|2019-05-11T01:26:19|/opt/launcher.py|27| 60        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  8.722
INFO|2019-05-11T01:35:26|/opt/launcher.py|27| 70        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  7.964
INFO|2019-05-11T01:44:36|/opt/launcher.py|27| 80        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  7.839
INFO|2019-05-11T01:53:45|/opt/launcher.py|27| 90        images/sec: 0.6 +/- 0.0 (jitter = 0.0)  7.953
INFO|2019-05-11T02:02:54|/opt/launcher.py|27| 100       images/sec: 0.6 +/- 0.0 (jitter = 0.0)  8.004
INFO|2019-05-11T02:02:54|/opt/launcher.py|27| ----------------------------------------------------------------
INFO|2019-05-11T02:02:54|/opt/launcher.py|27| total images/sec: 0.59
INFO|2019-05-11T02:02:54|/opt/launcher.py|27| ----------------------------------------------------------------
INFO|2019-05-11T02:02:54|/opt/launcher.py|80| Finished: python tf_cnn_benchmarks.py --batch_size=32 --model=resnet50 --variable_update=parameter_server --flush_stdout=true --num_gpus=1 --local_parameter_device=cpu --device=cpu --data_format=NHWC --job_name=worker --ps_hosts=kf-tfbenchmarks-1-ps-0.kf-tfbenchmarks.svc:2222 --worker_hosts=kf-tfbenchmarks-1-worker-0.kf-tfbenchmarks.svc:2222 --task_index=0
INFO|2019-05-11T02:02:54|/opt/launcher.py|84| Command ran successfully sleep for ever.
INFO|2019-05-11T02:12:54|/opt/launcher.py|84| Command ran successfully sleep for ever.
INFO|2019-05-11T02:22:54|/opt/launcher.py|84| Command ran successfully sleep for ever.
&lt;/denchmark-code&gt;

The TFjob resource YAML:
piVersion: kubeflow.org/v1beta2
kind: TFJob
metadata:
  name: kf-tfbenchmarks-1
  namespace: kf-tfbenchmarks
spec:
  tfReplicaSpecs:
    Ps:
      replicas: 1
      template:
        spec:
          containers:
          - args:
            - python
            - tf_cnn_benchmarks.py
            - --batch_size=32
            - --model=resnet50
            - --variable_update=parameter_server
            - --flush_stdout=true
            - --num_gpus=1
            - --local_parameter_device=cpu
            - --device=cpu
            - --data_format=NHWC
            image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3
            name: tensorflow
            resources:
              limits:
                cpu: 1000m
                memory: 1000Mi
              requests:
                cpu: 1000m
                memory: 1000Mi
            workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
      tfReplicaType: PS
    Worker:
      replicas: 1
      template:
        spec:
          containers:
          - args:
            - python
            - tf_cnn_benchmarks.py
            - --batch_size=32
            - --model=resnet50
            - --variable_update=parameter_server
            - --flush_stdout=true
            - --num_gpus=1
            - --local_parameter_device=cpu
            - --device=cpu
            - --data_format=NHWC
            image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3
            name: tensorflow
            resources:
              limits:
                cpu: 2000m
                memory: 3800Mi
              requests:
                cpu: 2000m
                memory: 3800Mi
            workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks
          restartPolicy: OnFailure
	</description>
	<comments>
		<comment id='1' author='liusheng' date='2019-05-11T03:26:54Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.88. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='liusheng' date='2019-05-12T12:36:05Z'>
		Kubeflow consider a job is Succeeded only when master or worker[0] exit with 0.
In the Tensorflow benchmark, it will call time.sleep to keep the worker alive for user to analyze the logs.
As a result, from Kubeflow perspective, this job is still running.
		</comment>
		<comment id='3' author='liusheng' date='2019-05-14T01:15:50Z'>
		&lt;denchmark-link:https://github.com/ChanYiLin&gt;@ChanYiLin&lt;/denchmark-link&gt;
 thanks for clarification :), so let me built another image for testing.
		</comment>
	</comments>
</bug>