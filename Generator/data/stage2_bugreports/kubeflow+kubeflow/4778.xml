<bug id='4778' author='Jeffwan' open_date='2020-02-18T19:34:41Z' closed_time='2020-02-22T07:14:06Z'>
	<summary>Namespaces can not be terminated after `kfctl delete`</summary>
	<description>
/kind bug
What steps did you take and what happened:
I try to include cert-manager namespace and crds deletion in kfctl. The problem is after kfctl delete  , I notice resource objects have been deleted in all namespaces but these three namespaces are alway in Terminating status.
&lt;denchmark-code&gt;âžœ  ~ kubectl get ns
NAME              STATUS        AGE
anonymous         Active        31m
cert-manager      Terminating   11h
default           Active        12h
istio-system      Terminating   11h
kube-node-lease   Active        12h
kube-public       Active        12h
kube-system       Active        12h
kubeflow          Terminating   11h
&lt;/denchmark-code&gt;

Checking controller-manager logs, I notice there's some cluster level webhook still there.
&lt;denchmark-code&gt;E0218 19:20:26.662720       1 memcache.go:199] couldn't get resource list for webhook.cert-manager.io/v1beta1: the server is currently unable to handle the request
&lt;/denchmark-code&gt;

In order to unblock namespace deletion. We have to delete api service v1beta1.webhook.cert-manager.io.
 kubectl delete apiservices v1beta1.webhook.cert-manager.io will work.
If not, please try kubectl delete ns cert-manager istio-system kubeflow --force --grace-period 0.
In conclusion, if user want to delete cert-manager, please check &lt;denchmark-link:https://github.com/kubeflow/manifests/tree/master/cert-manager&gt;manifest&lt;/denchmark-link&gt;
 and delete all of them. Otherwise, there's no backend for the webhook and cluster won't work as expect.
What did you expect to happen:
Namespace should be deleted successfully.
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard): v1.0
kfctl version: (use kfctl version): master
Kubernetes platform: (e.g. minikube): AWS EKS
Kubernetes version: (use kubectl version): 1.14
OS (e.g. from /etc/os-release):

	</description>
	<comments>
		<comment id='1' author='Jeffwan' date='2020-02-18T19:34:59Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.99



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='Jeffwan' date='2020-02-18T19:37:37Z'>
		This is not in current kfctl version and this issue has been resolve. Just want to surface this incase some one will meet similar problem
		</comment>
		<comment id='3' author='Jeffwan' date='2020-04-02T00:36:42Z'>
		Similarily, for knative-serving
v1beta1.coordination.k8s.io                 Local                               True                      25h
v1beta1.custom.metrics.k8s.io               knative-serving/autoscaler          False (ServiceNotFound)   4h38m
v1beta1.events.k8s.io                       Local                               True                      25h

		</comment>
	</comments>
</bug>