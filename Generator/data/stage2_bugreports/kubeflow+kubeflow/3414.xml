<bug id='3414' author='Ark-kun' open_date='2019-06-05T20:11:53Z' closed_time='2019-10-23T05:25:32Z'>
	<summary>Cannot connect to notebook: Error: "Sorry, /notebook/kubeflow/kfp-notebook is not a valid page"</summary>
	<description>
I've deployed KFP 0.1.20 with IAP using web deployer.
I've created a notebook using the Kubeflow UI (default configuration and image).
When I click "Connect" I'm redirected to the  &lt;denchmark-link:https://foo.endpoints.bar.cloud.goog/notebook/kubeflow/kfp-notebook&gt;https://foo.endpoints.bar.cloud.goog/notebook/kubeflow/kfp-notebook&lt;/denchmark-link&gt;
 page which shows "Sorry, /notebook/kubeflow/kfp-notebook is not a valid page".
&lt;denchmark-link:https://user-images.githubusercontent.com/1829149/58906122-1541fe00-86c0-11e9-9d3a-a9c956fbb720.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Ark-kun' date='2019-06-05T20:11:56Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.69. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='Ark-kun' date='2019-06-06T03:36:07Z'>
		/cc &lt;denchmark-link:https://github.com/zabbasi&gt;@zabbasi&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Ark-kun' date='2019-06-06T03:38:37Z'>
		&lt;denchmark-link:https://github.com/Ark-kun&gt;@Ark-kun&lt;/denchmark-link&gt;
 The URL looks correct. Is your notebook actually running; what does
What is the output of
&lt;denchmark-code&gt;kubectl -n kubeflow get notebooks
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Ark-kun' date='2019-06-06T22:56:00Z'>
		I have the same issue. I have try with different names and also deleting the PVC generated before to retry.
The output of kubectl -n kubeflow get notebooks in my case is showing the list of running notebooks:
&lt;denchmark-code&gt;git:(master) kubectl -n kubeflow get notebooks
NAME      AGE
daniela   23d
edi       5h
hillary   25d
isabel    3d
lino      25d
miguel    22d
&lt;/denchmark-code&gt;

The kubernetes cluster is ok from a resource perspective and there is no pods with error or pending state.
The cluster have almost a month working without problems. I'm experiencing the issue since yesterday.
Kubernetes version 1.12 deployed in GCP.
Kubeflow build version 0.5.0
Any advice?
		</comment>
		<comment id='5' author='Ark-kun' date='2019-06-06T23:33:44Z'>
		Thanks &lt;denchmark-link:https://github.com/Edilmo&gt;@Edilmo&lt;/denchmark-link&gt;
 . Can you please check the services in your cluster and check if you can connect to notebooks using port-forward ()?
		</comment>
		<comment id='6' author='Ark-kun' date='2019-06-07T00:13:49Z'>
		Hi &lt;denchmark-link:https://github.com/zabbasi&gt;@zabbasi&lt;/denchmark-link&gt;
 thx for your quick response.
Doing  the notebook is accessible through this URL: &lt;denchmark-link:http://localhost:8888/notebook/kubeflow/edi/tree&gt;http://localhost:8888/notebook/kubeflow/edi/tree&lt;/denchmark-link&gt;

Any other URLs like the followings are not working:

http://localhost:8888/
http://localhost:8888/tree
http://localhost:8888/tree?
http://localhost:8888/lab
http://localhost:8888/edi/
http://localhost:8888/edi/tree
http://localhost:8888/edi/tree?

I don't know if that is relevant or not.
Any other idea how to debug this?
Which component is in charge of the redirection?
Or is a problem at the ingress level?
FYI, the deployment was done using Istio. So the ingress is going through envoy.
		</comment>
		<comment id='7' author='Ark-kun' date='2019-06-07T14:27:31Z'>
		&lt;denchmark-link:https://github.com/Edilmo&gt;@Edilmo&lt;/denchmark-link&gt;
 have you deployed from master?
/cc &lt;denchmark-link:https://github.com/gabrielwen&gt;@gabrielwen&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/lluunn&gt;@lluunn&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Ark-kun' date='2019-06-07T17:08:50Z'>
		Ambassador is the service used to provide the reverse proxy.
Can you provide the output of
kubectl get service -o yaml edi
We'd like to see whether that has the proper YAML annotation for Ambassador.
Since your able to connect through the central dashboard to other pages like the notebooks spawner page that indicates Ambassador is correctly handling some routes.
What is the status of the Ambassador pods
&lt;denchmark-code&gt;kubectl get pods -l service=ambassador
&lt;/denchmark-code&gt;

Try forcing an ambassador pod restart by deleting the pods
&lt;denchmark-code&gt;kubectl delete pods -l service=ambassador
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/Edilmo&gt;@Edilmo&lt;/denchmark-link&gt;
 Is it just the new notebook that isn't accessible or is it old notebooks that aren't accessible?
		</comment>
		<comment id='9' author='Ark-kun' date='2019-06-07T19:03:35Z'>
		
@Edilmo have you deployed from master?
/cc @gabrielwen @lluunn

I followed the instructions &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli&gt;here - Deploy using CLI&lt;/denchmark-link&gt;
. I'm not aware of what  command does.
		</comment>
		<comment id='10' author='Ark-kun' date='2019-06-07T19:11:59Z'>
		Hi &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

All the previous notebooks are accessible, it's just the new ones.
Here the details:
&lt;denchmark-code&gt;mammut-deploy git:(master) kubectl get service -n kubeflow -o yaml edi 
apiVersion: v1
kind: Service
metadata:
  annotations:
    getambassador.io/config: |-
      ---
      apiVersion: ambassador/v0
      kind:  Mapping
      name: notebook_kubeflow_edi_mapping
      prefix: /notebook/kubeflow/edi
      rewrite: /notebook/kubeflow/edi
      timeout_ms: 300000
      service: edi.kubeflow
      use_websocket: true
  creationTimestamp: 2019-06-06T17:05:21Z
  name: edi
  namespace: kubeflow
  ownerReferences:
  - apiVersion: kubeflow.org/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: Notebook
    name: edi
    uid: 44b0cb90-887d-11e9-baea-42010a8000e3
  resourceVersion: "13092041"
  selfLink: /api/v1/namespaces/kubeflow/services/edi
  uid: 44bab4d3-887d-11e9-baea-42010a8000e3
spec:
  clusterIP: 10.0.11.161
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8888
  selector:
    statefulset: edi
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;mammut-deploy git:(master) kubectl get pods -n kubeflow -l service=ambassador
NAME                          READY     STATUS    RESTARTS   AGE
ambassador-7b8477f667-6pdnp   1/1       Running   0          27d
ambassador-7b8477f667-9vpz7   1/1       Running   0          27d
ambassador-7b8477f667-nk8bs   1/1       Running   0          27d
&lt;/denchmark-code&gt;

About restarting ambassador, how long could take that? (I don't want to affect the others guys working in this cluster)
Thanks in advance!!
		</comment>
		<comment id='11' author='Ark-kun' date='2019-06-08T22:12:50Z'>
		Can you upload the logs for the ambassador pods? Are you running on GCP? It would be good to grab the logs before you restart the pods.
What happens if you try to access the notebook directly and not through the central dashboard e.g.
&lt;denchmark-code&gt;https://${DOMAN}/notebook/kubeflow/ed1
&lt;/denchmark-code&gt;

Since the notebook is accessible through kubectl port-forward, I suspect a problem with Ambassador.
It looks like Ambassador may not be registering routes for the new notebooks. Ambassador works by watching for new services. So if it is having problem contacting the Master that could be the problem.
Can you check what routes are registered in Ambassador.
&lt;denchmark-code&gt;kubectl port-forward &lt;AMBASSADOR POD&gt; 8877:8877
http://localhost:8877/ambassador/v0/diag/
&lt;/denchmark-code&gt;

(ref &lt;denchmark-link:http://localhost:8877/ambassador/v0/diag/&gt;Ambassador docs&lt;/denchmark-link&gt;
)
Do you see a route for the newer notebooks?
&lt;denchmark-link:https://github.com/Edilmo&gt;@Edilmo&lt;/denchmark-link&gt;
 Restarting Ambassador should just take a couple seconds to restart. If you want to avoid disrupting initial workloads you could try deleting the Ambassador pods individually and waiting for each pod to restart before deleting the next one.
Here are the next steps I'd follow


Follow above instructions to access the debug UI for one of the Ambassador pods

Check if there is an entry for the new notebook and if so is it reported as healthy?



Fetch the logs for the associated pods

Upload them to the issue and look for any errors



Delete one of the Ambassador pods and wait for it to be restarted


Repeat the above steps to see if anything is different for the new Ambassador pod.


		</comment>
		<comment id='12' author='Ark-kun' date='2019-06-10T02:54:37Z'>
		I has same issue. Recreate ambassador pods fixed it.
ambassador container logs

I  ACCESS [2019-06-10T02:31:55.031Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "9397d14d-76ea-4c8e-bf24-d6a22be64ade" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:31:55.191Z] "GET /jupyter/new?namespace=kubeflow HTTP/1.1" 200 - 0 15200 37 36 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "9c8a5409-2f29-4232-93fc-445f5182c030" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
E  2019-06-10 02:32:00 kubewatch 0.37.0 INFO: generating config with gencount 28 (1 change)
 
E  2019-06-10 02:32:01 kubewatch 0.37.0 INFO: Scout reports {"latest_version": "0.71.0", "application": "ambassador", "notices": [], "cached": true, "timestamp": 1560133034.133822}
 
E  [2019-06-10 02:32:01.075][3014][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 
E  [2019-06-10 02:32:01.080][3014][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 
E  [2019-06-10 02:32:01.080][3014][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 
E  [2019-06-10 02:32:01.097][3014][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 
E  [2019-06-10 02:32:01.098][3014][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
 
I  got SIGHUP
 
I  forking and execing new child process at epoch 27
 
I  forked new child process with PID=3018
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:183] initializing epoch 27 (hot restart version=10.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363 size=2654312)
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:185] statically linked extensions:
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:187]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:190]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,envoy.filters.http.rbac,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash,extauth
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:193]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:196]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:198]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.statsd
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:200]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.zipkin
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:203]   transport_sockets.downstream: envoy.transport_sockets.capture,raw_buffer,tls
 
E  [2019-06-10 02:32:01.130][3018][info][main] source/server/server.cc:206]   transport_sockets.upstream: envoy.transport_sockets.capture,raw_buffer,tls
 
E  [2019-06-10 02:32:01.149][3000][warning][main] source/server/server.cc:449] shutting down admin due to child startup
 
E  [2019-06-10 02:32:01.149][3000][warning][main] source/server/server.cc:457] terminating parent process
 
E  [2019-06-10 02:32:01.153][3018][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 
E  [2019-06-10 02:32:01.176][3018][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 
E  [2019-06-10 02:32:01.198][3018][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 
E  [2019-06-10 02:32:01.198][3018][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
 
E  [2019-06-10 02:32:01.198][3018][info][main] source/server/server.cc:398] starting main dispatch loop
 
E  [2019-06-10 02:32:01.200][3018][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 
E  [2019-06-10 02:32:01.200][3018][info][main] source/server/server.cc:378] all clusters initialized. initializing init manager
 
E  [2019-06-10 02:32:01.200][3018][info][config] source/server/listener_manager_impl.cc:781] all dependencies initialized. starting workers
 
E  [2019-06-10 02:32:01.200][3000][info][main] source/server/server.cc:98] closing and draining listeners
 
I  ACCESS [2019-06-10T02:31:59.846Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 107 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "2a599706-a860-4c8b-8253-6af0f4bc9faa" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:00.995Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 108 107 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "e0b81dd9-480b-4e17-9601-ad41d65b3df6" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:02.267Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 105 104 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "76894434-1991-491d-8158-40e26684b8db" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:03.419Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 105 104 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "45e0a301-8495-41e3-8118-db304042faa9" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:04.573Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 105 104 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "292152d0-e015-4811-a9dd-17a74f35cfe4" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
E  2019-06-10 02:32:05 kubewatch 0.37.0 INFO: generating config with gencount 24 (1 change)
 
E  2019-06-10 02:32:06 kubewatch 0.37.0 INFO: Scout reports {"latest_version": "0.71.0", "application": "ambassador", "notices": [], "cached": true, "timestamp": 1560133039.08287}
 
E  [2019-06-10 02:32:06.078][2977][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 
E  [2019-06-10 02:32:06.082][2977][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 
E  [2019-06-10 02:32:06.082][2977][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 
E  [2019-06-10 02:32:06.098][2977][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 
E  [2019-06-10 02:32:06.098][2977][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
 
I  got SIGHUP
 
I  forking and execing new child process at epoch 23
 
I  forked new child process with PID=2981
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:183] initializing epoch 23 (hot restart version=10.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363 size=2654312)
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:185] statically linked extensions:
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:187]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:190]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,envoy.filters.http.rbac,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash,extauth
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:193]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:196]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:198]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.statsd
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:200]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.zipkin
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:203]   transport_sockets.downstream: envoy.transport_sockets.capture,raw_buffer,tls
 
E  [2019-06-10 02:32:06.132][2981][info][main] source/server/server.cc:206]   transport_sockets.upstream: envoy.transport_sockets.capture,raw_buffer,tls
 
E  [2019-06-10 02:32:06.150][2963][warning][main] source/server/server.cc:449] shutting down admin due to child startup
 
E  [2019-06-10 02:32:06.150][2963][warning][main] source/server/server.cc:457] terminating parent process
 
E  [2019-06-10 02:32:06.153][2981][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 
E  [2019-06-10 02:32:06.172][2981][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 
E  [2019-06-10 02:32:06.190][2981][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 
E  [2019-06-10 02:32:06.190][2981][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
 
E  [2019-06-10 02:32:06.191][2981][info][main] source/server/server.cc:398] starting main dispatch loop
 
E  [2019-06-10 02:32:06.192][2981][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 
E  [2019-06-10 02:32:06.192][2981][info][main] source/server/server.cc:378] all clusters initialized. initializing init manager
 
E  [2019-06-10 02:32:06.192][2981][info][config] source/server/listener_manager_impl.cc:781] all dependencies initialized. starting workers
 
E  [2019-06-10 02:32:06.193][2963][info][main] source/server/server.cc:98] closing and draining listeners
 
E  [2019-06-10 02:32:11.200][3018][info][main] source/server/drain_manager_impl.cc:63] shutting down parent after drain
 
E  [2019-06-10 02:32:11.200][3000][info][main] source/server/hot_restart_impl.cc:435] shutting down due to child request
 
E  [2019-06-10 02:32:11.200][3000][warning][main] source/server/server.cc:348] caught SIGTERM
 
E  [2019-06-10 02:32:11.200][3000][info][main] source/server/server.cc:402] main dispatch loop exited
 
E  [2019-06-10 02:32:11.205][3000][info][main] source/server/server.cc:437] exiting
 
I  got SIGCHLD
 
I  PID=3000 exited with code=0
 
I  ACCESS [2019-06-10T02:32:06.717Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "9c21d6d6-690a-4985-a3aa-94ffdf0bf857" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:08.912Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "29aa8297-8b78-4ce4-aa5f-d7fe5f1086bd" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:11.052Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 107 106 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "ea96ed62-ef99-4323-a8dd-0fe623e4990f" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
E  [2019-06-10 02:32:16.193][2981][info][main] source/server/drain_manager_impl.cc:63] shutting down parent after drain
 
E  [2019-06-10 02:32:16.193][2963][info][main] source/server/hot_restart_impl.cc:435] shutting down due to child request
 
E  [2019-06-10 02:32:16.194][2963][warning][main] source/server/server.cc:348] caught SIGTERM
 
E  [2019-06-10 02:32:16.194][2963][info][main] source/server/server.cc:402] main dispatch loop exited
 
E  [2019-06-10 02:32:16.198][2963][info][main] source/server/server.cc:437] exiting
 
I  got SIGCHLD
 
I  PID=2963 exited with code=0
 
I  ACCESS [2019-06-10T02:32:15.227Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 105 104 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "9b11c4b2-b1c1-40d8-b163-a58bf576fb5d" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:19.375Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 107 106 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "7dcd55bc-f52a-488d-81ec-89c6624ba5ff" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:23.523Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "118dfede-b6cb-4e11-9c7a-979a08b036ac" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:31.666Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 32 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "d0a3ba1c-c090-47fe-8c5d-3dfab5a5f235" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:33.474Z] "POST /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 355 26 12 11 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "03985d69-e369-4c14-b915-e6b0593c2cb9" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:33.523Z] "GET /jupyter/?namespace=kubeflow HTTP/1.1" 200 - 0 3633 28 27 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "dd974aad-4b65-42ad-9c06-e5471f79099e" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:33.671Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 442 111 109 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "7e495ff5-cd8d-4489-b92a-e45bbad4dae4" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:34.884Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "92037245-67a6-4178-a388-7141c1cf7682" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
E  2019-06-10 02:32:36 kubewatch 0.37.0 INFO: generating config with gencount 25 (1 change)
 
E  2019-06-10 02:32:36 kubewatch 0.37.0 INFO: Scout reports {"latest_version": "0.71.0", "application": "ambassador", "notices": [], "cached": true, "timestamp": 1560133039.08287}
 
E  [2019-06-10 02:32:36.445][2994][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 
E  [2019-06-10 02:32:36.449][2994][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 
E  [2019-06-10 02:32:36.449][2994][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 
E  [2019-06-10 02:32:36.465][2994][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 
E  [2019-06-10 02:32:36.465][2994][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
 
I  got SIGHUP
 
I  forking and execing new child process at epoch 24
 
I  forked new child process with PID=2998
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:183] initializing epoch 24 (hot restart version=10.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363 size=2654312)
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:185] statically linked extensions:
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:187]   access_loggers: envoy.file_access_log,envoy.http_grpc_access_log
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:190]   filters.http: envoy.buffer,envoy.cors,envoy.ext_authz,envoy.fault,envoy.filters.http.header_to_metadata,envoy.filters.http.jwt_authn,envoy.filters.http.rbac,envoy.grpc_http1_bridge,envoy.grpc_json_transcoder,envoy.grpc_web,envoy.gzip,envoy.health_check,envoy.http_dynamo_filter,envoy.ip_tagging,envoy.lua,envoy.rate_limit,envoy.router,envoy.squash,extauth
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:193]   filters.listener: envoy.listener.original_dst,envoy.listener.proxy_protocol,envoy.listener.tls_inspector
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:196]   filters.network: envoy.client_ssl_auth,envoy.echo,envoy.ext_authz,envoy.filters.network.thrift_proxy,envoy.http_connection_manager,envoy.mongo_proxy,envoy.ratelimit,envoy.redis_proxy,envoy.tcp_proxy
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:198]   stat_sinks: envoy.dog_statsd,envoy.metrics_service,envoy.statsd
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:200]   tracers: envoy.dynamic.ot,envoy.lightstep,envoy.zipkin
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:203]   transport_sockets.downstream: envoy.transport_sockets.capture,raw_buffer,tls
 
E  [2019-06-10 02:32:36.498][2998][info][main] source/server/server.cc:206]   transport_sockets.upstream: envoy.transport_sockets.capture,raw_buffer,tls
 
E  [2019-06-10 02:32:36.516][2981][warning][main] source/server/server.cc:449] shutting down admin due to child startup
 
E  [2019-06-10 02:32:36.516][2981][warning][main] source/server/server.cc:457] terminating parent process
 
E  [2019-06-10 02:32:36.519][2998][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 
E  [2019-06-10 02:32:36.540][2998][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 
E  [2019-06-10 02:32:36.558][2998][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 
E  [2019-06-10 02:32:36.559][2998][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration
 
E  [2019-06-10 02:32:36.559][2998][info][main] source/server/server.cc:398] starting main dispatch loop
 
E  [2019-06-10 02:32:40.603][2998][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 
E  [2019-06-10 02:32:40.603][2998][info][main] source/server/server.cc:378] all clusters initialized. initializing init manager
 
E  [2019-06-10 02:32:40.603][2998][info][config] source/server/listener_manager_impl.cc:781] all dependencies initialized. starting workers
 
E  [2019-06-10 02:32:40.604][2981][info][main] source/server/server.cc:98] closing and draining listeners
 
I  ACCESS [2019-06-10T02:32:36.077Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 105 104 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "34e5bbc1-b388-49fb-b688-1baa2cbf9cd8" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:38.096Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "ac5fbe06-bf2b-4976-9859-3aaecbc545d0" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:39.822Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "35117355-6ca5-441a-9a7c-6443326aa979" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:40.097Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 107 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "da807d3f-e846-450d-bf08-2aa3b017eeac" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:40.971Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 107 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "fc22f689-1e74-47e3-91da-1f1e566b9909" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:42.096Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 126 124 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36" "788f135f-4411-4ff1-8fa4-0a063d7b3ea4" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:43.168Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 107 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "f847f2c2-ff93-4a4d-9663-80ed71f3881b" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
I  ACCESS [2019-06-10T02:32:44.315Z] "GET /jupyter/api/namespaces/kubeflow/notebooks HTTP/1.1" 200 - 0 482 106 105 "218.161.0.128, 130.211.33.158" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36" "7eb48187-1798-404a-ad34-2a0a5e67b34e" "tagtoo-ai.endpoints.tagtoo-staging.cloud.goog" "10.31.249.199:80"
 
E  2019-06-10 02:32:46 kubewatch 0.37.0 INFO: generating config with gencount 29 (1 change)
 
E  2019-06-10 02:32:46 kubewatch 0.37.0 INFO: Scout reports {"latest_version": "0.71.0", "application": "ambassador", "notices": [], "cached": true, "timestamp": 1560133034.133822}
 
E  [2019-06-10 02:32:46.316][3031][info][config] source/server/configuration_impl.cc:50] loading 0 static secret(s)
 
E  [2019-06-10 02:32:46.320][3031][info][upstream] source/common/upstream/cluster_manager_impl.cc:132] cm init: all clusters initialized
 
E  [2019-06-10 02:32:46.320][3031][info][config] source/server/configuration_impl.cc:60] loading 1 listener(s)
 
E  [2019-06-10 02:32:46.337][3031][info][config] source/server/configuration_impl.cc:94] loading tracing configuration
 
E  [2019-06-10 02:32:46.337][3031][info][config] source/server/configuration_impl.cc:116] loading stats sink configuration



		</comment>
		<comment id='13' author='Ark-kun' date='2019-06-14T05:08:38Z'>
		&lt;denchmark-link:https://github.com/Edilmo&gt;@Edilmo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Ark-kun&gt;@Ark-kun&lt;/denchmark-link&gt;
 Does recreating the Ambassador pods fix it for you?
		</comment>
		<comment id='14' author='Ark-kun' date='2019-06-16T21:50:24Z'>
		Hi &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

Answering your questions:

The new notebook was not accessible even if I try directly the link in the browser.
Certainly the ambassador was not registering routes for the new notebooks, the "Ambassador Diagnostic Overview" page was not showing the route for the new notebooks.
Recreating the Ambassador fix the problem!!!
yes, I'm using a kubernetes cluster in GKE.

The logs of the 3 ambassador pods:
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/3294669/ambassador-7b8477f667-6pdnp.csv.txt&gt;ambassador-7b8477f667-6pdnp.csv.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/3294670/ambassador-7b8477f667-9vpz7.csv.txt&gt;ambassador-7b8477f667-9vpz7.csv.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/3294671/ambassador-7b8477f667-nk8bs.csv.txt&gt;ambassador-7b8477f667-nk8bs.csv.txt&lt;/denchmark-link&gt;

Sorry for the late response.
Let me know if I can help with something else.
		</comment>
		<comment id='15' author='Ark-kun' date='2019-07-15T13:32:13Z'>
		&lt;denchmark-link:https://github.com/Edilmo&gt;@Edilmo&lt;/denchmark-link&gt;
 thanks. So it looks like the issue is with Envoy or Ambassador. It looks like the temporary work around is to restart the Ambassador pods.
0.6 will include updated versions of Envoy and ISTIO.  So I think the next step would be to wait for 0.6 to become available and then see if this problem reoccurs with ISTIO.
I will leave this issue open until we have a 0.6 release available.
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/3559&gt;#3559&lt;/denchmark-link&gt;
 is tracking 0.6
		</comment>
		<comment id='16' author='Ark-kun' date='2019-07-16T06:58:44Z'>
		Thanks &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 for your help!!!
		</comment>
		<comment id='17' author='Ark-kun' date='2019-07-24T05:21:32Z'>
		0.6 is now available so I'm going to close this bug.
Please try 0.6 and open a new issue if the problem still occurs.
		</comment>
		<comment id='18' author='Ark-kun' date='2019-10-17T10:13:14Z'>
		Hi, I just encountered a similar problem using Kubeflow v0.6.2. It is deployed under this instruction &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/&gt;https://www.kubeflow.org/docs/gke/deploy/deploy-cli/&lt;/denchmark-link&gt;
 on GCP and the default TensorFlow notebooks work fine.
However, With a customised image (build with this Dockerfile: &lt;denchmark-link:https://github.com/kubeflow/kubeflow/blob/master/components/tensorflow-notebook-image/Dockerfile&gt;https://github.com/kubeflow/kubeflow/blob/master/components/tensorflow-notebook-image/Dockerfile&lt;/denchmark-link&gt;
) , there is a 404 error saying the requested page does not exist.
I've updated the Entrypoint and specified the BASE_URL feature
ENTRYPOINT ["tini", "--"]
ENV NB_PREFIX /notebook/kubeflow/test
CMD ["sh","-c", "jupyter notebook --notebook-dir=/home/${NB_USER} --ip=0.0.0.0 --no-browser --allow-root --port=8888 --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.allow_origin='*' --NotebookApp.base_url=${NB_PREFIX}"]
I've also tried port-forwarding, it doesn't work either.
Any thoughts?
		</comment>
		<comment id='19' author='Ark-kun' date='2019-10-23T05:24:52Z'>
		&lt;denchmark-link:https://github.com/Felihong&gt;@Felihong&lt;/denchmark-link&gt;
 looks like an issue with your Jupyter setup. If your port-forwarding to the notebook and jupyter and you are getting 404s then something not involving Kubeflow is going on.
You can compare against our latest Jupyter images which are based on the stock TensorFlow images.
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/blob/master/components/tensorflow-notebook-image/Dockerfile&gt;https://github.com/kubeflow/kubeflow/blob/master/components/tensorflow-notebook-image/Dockerfile&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='Ark-kun' date='2019-10-23T05:25:32Z'>
		I'm closing this because the original issue was filed with KF 0.5 which is no longer supported.
		</comment>
	</comments>
</bug>