<bug id='4827' author='vaskokj' open_date='2020-03-04T15:04:35Z' closed_time='2020-06-21T23:11:30Z'>
	<summary>Several pods not starting due to various errors related to using NFS as dynamic provisioner</summary>
	<description>
/kind bug
What steps did you take and what happened:
I started by creating a dynamic NFS provisioner by running
&lt;denchmark-link:https://github.com/justmeandopensource/kubernetes/blob/master/yamls/nfs-provisioner/rbac.yaml&gt;https://github.com/justmeandopensource/kubernetes/blob/master/yamls/nfs-provisioner/rbac.yaml&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/justmeandopensource/kubernetes/blob/master/yamls/nfs-provisioner/default-sc.yaml&gt;https://github.com/justmeandopensource/kubernetes/blob/master/yamls/nfs-provisioner/default-sc.yaml&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/deploy/deployment.yaml&gt;https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/deploy/deployment.yaml&lt;/denchmark-link&gt;

which is from &lt;denchmark-link:https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client&gt;https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client&lt;/denchmark-link&gt;

I then installed kubeflow with &lt;denchmark-link:https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_istio_dex.v1.0.0.yaml&gt;https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_istio_dex.v1.0.0.yaml&lt;/denchmark-link&gt;

Several of the pods do not seem to be starting up due to various issues.
&lt;denchmark-code&gt;$ kubectl describe -n istio-system pod authservice-0
Name:           authservice-0
Namespace:      istio-system
Priority:       0
Node:           node1.kr.example.com/10.75.38.135
Start Time:     Tue, 03 Mar 2020 15:03:29 -0600
Labels:         app=authservice
                app.kubernetes.io/component=oidc-authservice
                app.kubernetes.io/instance=oidc-authservice-v1.0.0
                app.kubernetes.io/managed-by=kfctl
                app.kubernetes.io/name=oidc-authservice
                app.kubernetes.io/part-of=kubeflow
                app.kubernetes.io/version=v1.0.0
                controller-revision-hash=authservice-5f786759c5
                statefulset.kubernetes.io/pod-name=authservice-0
Annotations:    sidecar.istio.io/inject: false
Status:         Pending
IP:
Controlled By:  StatefulSet/authservice
Containers:
  authservice:
    Container ID:
    Image:          gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef
    Image ID:
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Readiness:      http-get http://:8081/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:
      USERID_HEADER:  kubeflow-userid
      USERID_PREFIX:
      USERID_CLAIM:   email
      OIDC_PROVIDER:  http://dex.auth.svc.cluster.local:5556/dex
      OIDC_AUTH_URL:  /dex/auth
      OIDC_SCOPES:    profile email groups
      REDIRECT_URL:   /login/oidc
      SKIP_AUTH_URI:  /dex
      PORT:           8080
      CLIENT_ID:      kubeflow-oidc-authservice
      CLIENT_SECRET:  pUBnBOY80SnXgjibTYM9ZWNzY2xreNGQok
      STORE_PATH:     /var/lib/authservice/data.db
    Mounts:
      /var/lib/authservice from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6wg9h (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  authservice-pvc
    ReadOnly:   false
  default-token-6wg9h:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-6wg9h
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason       Age                    From                                          Message
  ----     ------       ----                   ----                                          -------
  Warning  FailedMount  34m (x397 over 17h)    kubelet, node1.kr.example.com  Unable to mount volumes for pod "authservice-0_istio-system(10199390-900a-4276-acea-b7aecdf456d7)": timeout expired waiting for volumes to attach or mount for pod "istio-system"/"authservice-0". list of unmounted volumes=[data]. list of unattached volumes=[data default-token-6wg9h]
  Warning  FailedMount  4m39s (x593 over 17h)  kubelet, node1.kr.example.com  (combined from similar events): MountVolume.SetUp failed for volume "pvc-72502148-4c02-4f45-a9aa-4cc19d701503" : mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/10199390-900a-4276-acea-b7aecdf456d7/volumes/kubernetes.io~nfs/pvc-72502148-4c02-4f45-a9aa-4cc19d701503 --scope -- mount -t nfs dell-ds1.example.com:/k8/istio-system-authservice-pvc-pvc-72502148-4c02-4f45-a9aa-4cc19d701503 /var/lib/kubelet/pods/10199390-900a-4276-acea-b7aecdf456d7/volumes/kubernetes.io~nfs/pvc-72502148-4c02-4f45-a9aa-4cc19d701503
Output: Running scope as unit: run-r177080a878e5475f952104755b41a3e9.scope
mount.nfs: Protocol not supported
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow mysql-6bcbfbb6b8-rzlf8
2020-03-04 13:35:11+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.6.47-1debian9 started.
2020-03-04 13:35:11+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-04 13:35:11+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.6.47-1debian9 started.
mkdir: cannot create directory '/var/lib/mysql/': File exists
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow katib-db-manager-54b66f9f9d-d5dch
E0304 13:42:21.619878       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:42:26.611773       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:42:31.635879       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:42:36.627814       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:42:41.619904       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:42:46.611779       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:42:51.635869       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:42:56.627784       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:43:01.619889       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:43:06.611712       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:43:11.635854       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
E0304 13:43:16.627642       1 mysql.go:62] Ping to Katib db failed: dial tcp 10.233.23.201:3306: connect: connection refused
F0304 13:43:16.627719       1 main.go:83] Failed to open db connection: DB open failed: Timeout waiting for DB conn successfully opened.
goroutine 1 [running]:
github.com/kubeflow/katib/vendor/k8s.io/klog.stacks(0xc00024a200, 0xc0002520e0, 0x89, 0xd1)
        /go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:830 +0xb8
github.com/kubeflow/katib/vendor/k8s.io/klog.(*loggingT).output(0xdf1ca0, 0xc000000003, 0xc000278000, 0xd93a76, 0x7, 0x53, 0x0)
        /go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:781 +0x2d0
github.com/kubeflow/katib/vendor/k8s.io/klog.(*loggingT).printf(0xdf1ca0, 0x3, 0x9b448c, 0x20, 0xc0001e1f20, 0x1, 0x1)
        /go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:678 +0x14b
github.com/kubeflow/katib/vendor/k8s.io/klog.Fatalf(...)
        /go/src/github.com/kubeflow/katib/vendor/k8s.io/klog/klog.go:1209
main.main()
        /go/src/github.com/kubeflow/katib/cmd/db-manager/v1alpha3/main.go:83 +0x165
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow katib-mysql-dcf7dcbd5-djx45
2020-03-04 13:46:11+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian9 started.
2020-03-04 13:46:11+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-04 13:46:11+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian9 started.
mkdir: cannot create directory '/var/lib/mysql': Permission denied
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow metadata-db-65fb5b695d-656hw
mkdir: cannot create directory '/var/lib/mysql': Permission denied
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow metadata-grpc-deployment-75f9888cbf-d9q5m
2020-03-04 13:50:04.660297: F ml_metadata/metadata_store/metadata_store_server_main.cc:219] Non-OK-status: status status: Internal: mysql_real_connect failed: errno: 2002, error: Can't connect to MySQL server on 'metadata-db' (115)MetadataStore cannot be created with the given connection config.
&lt;/denchmark-code&gt;

What did you expect to happen:
Kubeflow pods all to be up and application functioning.
Anything else you would like to add:
All of my pv, pvc's are bound. A
&lt;denchmark-code&gt;$ kubectl get pv,pvc -A
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS          REASON   AGE
persistentvolume/pvc-0b7c97b5-a650-4355-91bc-00d5de17c4c3   10Gi       RWO            Delete           Bound    kubeflow/metadata-mysql        managed-nfs-storage            16h
persistentvolume/pvc-72502148-4c02-4f45-a9aa-4cc19d701503   10Gi       RWO            Delete           Bound    istio-system/authservice-pvc   managed-nfs-storage            18h
persistentvolume/pvc-bdfbde9e-b056-4f6f-8415-2e8e18bcff7b   20Gi       RWO            Delete           Bound    kubeflow/mysql-pv-claim        managed-nfs-storage            16h
persistentvolume/pvc-c6791418-ae14-42ca-9193-037ef31688d4   10Gi       RWO            Delete           Bound    kubeflow/katib-mysql           managed-nfs-storage            16h
persistentvolume/pvc-c988ed2d-d121-41fd-9cb5-f22c1906d64b   20Gi       RWO            Delete           Bound    kubeflow/minio-pv-claim        managed-nfs-storage            16h

NAMESPACE      NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
istio-system   persistentvolumeclaim/authservice-pvc   Bound    pvc-72502148-4c02-4f45-a9aa-4cc19d701503   10Gi       RWO            managed-nfs-storage   18h
kubeflow       persistentvolumeclaim/katib-mysql       Bound    pvc-c6791418-ae14-42ca-9193-037ef31688d4   10Gi       RWO            managed-nfs-storage   16h
kubeflow       persistentvolumeclaim/metadata-mysql    Bound    pvc-0b7c97b5-a650-4355-91bc-00d5de17c4c3   10Gi       RWO            managed-nfs-storage   16h
kubeflow       persistentvolumeclaim/minio-pv-claim    Bound    pvc-c988ed2d-d121-41fd-9cb5-f22c1906d64b   20Gi       RWO            managed-nfs-storage   16h
kubeflow       persistentvolumeclaim/mysql-pv-claim    Bound    pvc-bdfbde9e-b056-4f6f-8415-2e8e18bcff7b   20Gi       RWO            managed-nfs-storage   16h
&lt;/denchmark-code&gt;

Environment:

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard):
kfctl version: (use kfctl version): kfctl v1.0-0-g94c35cf
Kubernetes platform: kubespray
Kubernetes version: (use kubectl version):$ kubectl version
Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.3", GitCommit:"2d3c76f9091b6bec110a5e63777c332469e0cba2", GitTreeState:"clean", BuildDate:"2019-08-19T11:05:50Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.3", GitCommit:"2d3c76f9091b6bec110a5e63777c332469e0cba2", GitTreeState:"clean", BuildDate:"2019-08-19T11:05:50Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"}
OS (e.g. from /etc/os-release): Ubuntu 18.04.3

	</description>
	<comments>
		<comment id='1' author='vaskokj' date='2020-03-04T15:05:23Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




kind/bug
0.97



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='vaskokj' date='2020-03-04T15:19:23Z'>
		I did find this related item &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4630&gt;#4630&lt;/denchmark-link&gt;
. It has very similar errors I'm seeing but I don't know where he made his modification to try it to see if it the same problem.
Just for completeness I was able to run from step 5 to validate the NFS dynamic provisioner works. &lt;denchmark-link:https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client&gt;https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client&lt;/denchmark-link&gt;

kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml
and was able to get the "SUCCESS" file on my NFS server.
		</comment>
		<comment id='3' author='vaskokj' date='2020-03-04T21:43:58Z'>
		I went and tried to diagnose this issue myself a little further.
In the kustomize i went and edited the mysql deployment.yaml file.
Because the mysql container was in a restartloop and was never starting I went and added
command: [ "/bin/sh", "-c", "sleep 100000" ]  to the mysql container.
I was then able to get into the container with
kubectl exec -ti mysql-65d9cff4ff-dgxnr bash
the container was successfully mounting my NFS storage
nfs1.example.com:/k8/kubeflow-mysql-pv-claim-pvc-06b4ffb5-2d10-427c-8128-02bed3f30e21  1.0T  1.6G 1023G   1% /var/lib/mysql
$  touch /var/lib/mysql/hello.txt
It creates the file not problem...
&lt;denchmark-code&gt;ls /var/lib/mysql/
hello.txt
&lt;/denchmark-code&gt;

So based off this I don't think there is anything wrong with my NFS provisioner, its functioning properly, mounting in the container without any issues, it can write to the folder no problem.
However, by default the mysql container will not start up and just spits out
&lt;denchmark-code&gt;$ kubectl logs -n kubeflow mysql-6bcbfbb6b8-rzlf8
2020-03-04 13:35:11+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.6.47-1debian9 started.
2020-03-04 13:35:11+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-04 13:35:11+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.6.47-1debian9 started.
mkdir: cannot create directory '/var/lib/mysql/': File exists
&lt;/denchmark-code&gt;

At this point I have no idea why/how this isn't working...
		</comment>
		<comment id='4' author='vaskokj' date='2020-03-05T18:47:37Z'>
		Ok so this is an issue with the mysql container that kubeflow is using it appears. While I don't know a lot of K8, I don't see how this has ever worked with dynamic provisioning (maybe specifically NFS).
I took the code snippets from here:
&lt;denchmark-link:https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/&gt;https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/&lt;/denchmark-link&gt;

Having a Dyanamic NFS provisioner installed...
kubectl apply mysql-pvc.yaml
&lt;denchmark-code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
spec:
  storageClassName: managed-nfs-storage  &lt;--- THIS SHOULD MATCH  YOUR NFS STORAGECLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  selector:
    app: mysql
  clusterIP: None
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        command: [ "/bin/sh", "-c", "sleep 100000" ]
        env:
          # Use secret in real usage
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS          REASON   AGE
persistentvolume/mysql-pv-volume                            20Gi       RWO            Retain           Bound    default/mysql-pv-claim         managed-nfs-storage            5m16s

NAMESPACE      NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
default        persistentvolumeclaim/mysql-pv-claim    Bound    mysql-pv-volume                            20Gi       RWO            managed-nfs-storage   5m27s
&lt;/denchmark-code&gt;

The pv was created automatically by the dynamic provisioner
Same error:
&lt;denchmark-code&gt;$ kubectl logs mysql-7d7fdd478f-l2m8h
2020-03-05 18:26:21+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.6.47-1debian9 started.
2020-03-05 18:26:21+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-05 18:26:21+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.6.47-1debian9 started.
mkdir: cannot create directory '/var/lib/mysql/': File exists
&lt;/denchmark-code&gt;

This error stops the container from starting...
I went and deleted the deployment and added  command: [ "/bin/sh", "-c", "sleep 100000" ] so the container would start...
After getting into the container, the NFS mount is properly mounted and is writable...
&lt;denchmark-code&gt;# df -h | grep mysql
nfs1.example.com:/k8/default-mysql-pv-claim-pvc-0808d1bd-69ca-4ff5-825a-b846b1133e3a  1.0T  1.6G 1023G   1% /var/lib/mysql
&lt;/denchmark-code&gt;

If I create a "local" pv
&lt;denchmark-code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
&lt;/denchmark-code&gt;

and created the mysql deployment, the mysql pod starts up without issue.
So at this point, with dynamic provisioning (potentially just on NFS?) doesn't work. I have no clue how anyone elses NFS is actually functioning unless they didn't setup their NFS Dynamic Provisioning correctly, is new/older version than mine. I'm really curious as to how other people have gotten this to work. But the NFS provisioning seems to be working properly on my end and this seems to be an issue with the mysql container that Kubeflow uses.
I'll go bug the mysql container people and see if they have any suggestions on how to resolve this.
		</comment>
		<comment id='5' author='vaskokj' date='2020-03-05T21:53:44Z'>
		&lt;denchmark-link:https://github.com/vaskokj&gt;@vaskokj&lt;/denchmark-link&gt;

I've been struggling with kubeflow and nfs-client-provisioner myself but recently came right. Unfortunately haven't been able to pin point the change that resulted in the fix. Thought my working configuration might help you.
One thing I noticed upfront was that the Minio pod did NOT have an issue with the nfs storage class. Seems to have only been with the MySQL Pods. Minio executes as root while the MySQL Pods use UID 999.
I thought NFS "vers=3.0" vs "vers=4.0" might have had a baring but tested both successfully.
The MySQL image might have changed - pasted SHA below.
  katib-mysql:
    Container ID:  containerd://f30dec127ea0842f0fc84239c41a82314827abdc698d3b4740e22969e32e89ca
    Image:         mysql:8
    Image ID:      docker.io/library/mysql@sha256:6d0741319b6a2ae22c384a97f4bbee411b01e75f6284af0cce339fee83d7e314
Files permissions as seen from inside nfs-client-provisioner:
$ kubectl exec -it -n kube-nfs-client-provisioner   nfs-client-provisioner-6cddd69dbf-npln4    sh
/ # ls -la /persistentvolumes/kubeflow-mysql-pv-claim-pvc-f9c74e12-7274-45b6-892e-d7f5de985b69
total 133808
drwxrwxrwx    5 root     root           196 Mar  5 20:46 .
drwxrwxrwx   10 uucp     wheel          650 Mar  5 20:39 ..
-rwxrwx---    1 999      ping            56 Mar  5 20:39 auto.cnf
-rwxrwx---    1 999      ping      50331648 Mar  5 20:47 ib_logfile0
-rwxrwx---    1 999      ping      50331648 Mar  5 20:39 ib_logfile1
-rwxrwx---    1 999      ping      12582912 Mar  5 20:47 ibdata1
drwxrwx---    2 999      ping           644 Mar  5 20:46 mlpipeline
drwxrwx---    2 999      ping          2665 Mar  5 20:39 mysql
drwxrwx---    2 999      ping          2602 Mar  5 20:39 performance_schema
Working MySQL Pod logs:
kubeflow $ kubectl logs -n kubeflow                      mysql-b6f8955c9-dwpcb    | more
2020-03-05 20:39:35+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.6.47-1debian9 started.
2020-03-05 20:39:36+00:00 [Note] [Entrypoint]: Initializing database files
2020-03-05 20:39:36 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2020-03-05 20:39:36 0 [Note] Ignoring --secure-file-priv value as server is running with --bootstrap.
2020-03-05 20:39:36 0 [Note] /usr/sbin/mysqld (mysqld 5.6.47) starting as process 27 ...
2020-03-05 20:39:36 27 [Note] InnoDB: Using atomics to ref count buffer pool pages
2020-03-05 20:39:36 27 [Note] InnoDB: The InnoDB memory heap is disabled
2020-03-05 20:39:36 27 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2020-03-05 20:39:36 27 [Note] InnoDB: Memory barrier is not used
2020-03-05 20:39:36 27 [Note] InnoDB: Compressed tables use zlib 1.2.11
2020-03-05 20:39:36 27 [Note] InnoDB: Using Linux native AIO
2020-03-05 20:39:36 27 [Note] InnoDB: Using CPU crc32 instructions
2020-03-05 20:39:36 27 [Note] InnoDB: Initializing buffer pool, size = 128.0M
2020-03-05 20:39:36 27 [Note] InnoDB: Completed initialization of buffer pool
2020-03-05 20:39:36 27 [Note] InnoDB: The first specified data file ./ibdata1 did not exist: a new database to be created!
2020-03-05 20:39:36 27 [Note] InnoDB: Setting file ./ibdata1 size to 12 MB
2020-03-05 20:39:36 27 [Note] InnoDB: Database physically writes the file full: wait...
2020-03-05 20:39:36 27 [Note] InnoDB: Setting log file ./ib_logfile101 size to 48 MB
2020-03-05 20:39:37 27 [Note] InnoDB: Setting log file ./ib_logfile1 size to 48 MB
2020-03-05 20:39:37 27 [Note] InnoDB: Renaming log file ./ib_logfile101 to ./ib_logfile0
2020-03-05 20:39:37 27 [Warning] InnoDB: New log files created, LSN=45781
2020-03-05 20:39:37 27 [Note] InnoDB: Doublewrite buffer not found: creating new
2020-03-05 20:39:37 27 [Note] InnoDB: Doublewrite buffer created
2020-03-05 20:39:37 27 [Note] InnoDB: 128 rollback segment(s) are active.
2020-03-05 20:39:37 27 [Warning] InnoDB: Creating foreign key constraint system tables.
2020-03-05 20:39:37 27 [Note] InnoDB: Foreign key constraint system tables created
2020-03-05 20:39:37 27 [Note] InnoDB: Creating tablespace and datafile system tables.
2020-03-05 20:39:37 27 [Note] InnoDB: Tablespace and datafile system tables created.
2020-03-05 20:39:37 27 [Note] InnoDB: Waiting for purge to start
2020-03-05 20:39:37 27 [Note] InnoDB: 5.6.47 started; log sequence number 0
2020-03-05 20:39:37 27 [Note] RSA private key file not found: /var/lib/mysql//private_key.pem. Some authentication plugins will not work.
2020-03-05 20:39:37 27 [Note] RSA public key file not found: /var/lib/mysql//public_key.pem. Some authentication plugins will not work.
...
2020-03-05 20:39:42+00:00 [Note] [Entrypoint]: Database files initialized
2020-03-05 20:39:42+00:00 [Note] [Entrypoint]: Starting temporary server
2020-03-05 20:39:42+00:00 [Note] [Entrypoint]: Waiting for server startup
2020-03-05 20:39:42 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2020-03-05 20:39:42 0 [Note] mysqld (mysqld 5.6.47) starting as process 75 ...
kubeflow $ kubectl logs -f -n kubeflow                      katib-mysql-74747879d7-c6bt9 
2020-03-05 20:39:17+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian9 started.
2020-03-05 20:39:18+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-05 20:39:18+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian9 started.
2020-03-05 20:39:18+00:00 [Note] [Entrypoint]: Initializing database files
2020-03-05T20:39:18.737718Z 0 [Warning] [MY-011070] [Server] 'Disabling symbolic links using --skip-symbolic-links (or equivalent) is the default. Consider not using this option as it' is deprecated and will be removed in a future release.
2020-03-05T20:39:18.738804Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.19) initializing of server in progress as process 46
2020-03-05T20:39:22.305420Z 5 [Warning] [MY-010453] [Server] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option.
2020-03-05 20:39:25+00:00 [Note] [Entrypoint]: Database files initialized
2020-03-05 20:39:25+00:00 [Note] [Entrypoint]: Starting temporary server
2020-03-05T20:39:26.209792Z 0 [Warning] [MY-011070] [Server] 'Disabling symbolic links using --skip-symbolic-links (or equivalent) is the default. Consider not using this option as it' is deprecated and will be removed in a future release.
2020-03-05T20:39:26.209973Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.19) starting as process 96
2020-03-05T20:39:27.657151Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2020-03-05T20:39:27.665938Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location '/var/run/mysqld' in the path is accessible to all OS users. Consider choosing a different directory.
2020-03-05T20:39:27.724189Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.19'  socket: '/var/run/mysqld/mysqld.sock'  port: 0  MySQL Community Server - GPL.
2020-03-05 20:39:27+00:00 [Note] [Entrypoint]: Temporary server started.
2020-03-05T20:39:27.796806Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Socket: '/var/run/mysqld/mysqlx.sock'
Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it.
Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it.
2020-03-05 20:39:32+00:00 [Note] [Entrypoint]: Creating database katib
Hopefully we can pinpoint the fix. What NFS server are you using?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Kubernetes 1.16.2
containerd/cri v1.3.2
PhotonOS 3R2
nfs-client-provisioner v3.1.0-k8s1.11
nfs server = Isilon
		</comment>
		<comment id='6' author='vaskokj' date='2020-03-06T14:19:28Z'>
		&lt;denchmark-link:https://github.com/MnrGreg&gt;@MnrGreg&lt;/denchmark-link&gt;
 Thanks for the post.
Same for me on the Minio front. It works without issues and starts up but none of the mysql containers start up. I've tried the mysql5.6 and the mysql5.7 containers without success.
I'm using a Dell Unity NFS server (its just SuSe NFS server).
I feel this falls on the mysql container, but not sure why. Its interesting that yours wasn't working and started working.
Containers:
katib-mysql:
Container ID:  docker://51908a58d9bf87d473e95d99d864fb46c22035efc2dc69ac2c7ce82a5b606a17
Image:         mysql:8
Image ID:      docker-pullable://mysql@sha256:f1df505c4c6e8eae599a0482e3bde3e761cd700c00cbc371a8161648a26817c0
All of my containers that seem to deal with mysql all fail with different errors.
&lt;denchmark-code&gt;$ kubectl logs -n kubeflow katib-mysql-dcf7dcbd5-mm4jq
2020-03-06 14:14:01+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian9 started.
2020-03-06 14:14:01+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-06 14:14:01+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.19-1debian9 started.
mkdir: cannot create directory '/var/lib/mysql': Permission denied
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow metadata-db-65fb5b695d-nfv2r
mkdir: cannot create directory '/var/lib/mysql': Permission denied
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow metadata-grpc-deployment-75f9888cbf-fhd8d
2020-03-06 14:15:30.676110: F ml_metadata/metadata_store/metadata_store_server_main.cc:219] Non-OK-status: status status: Internal: mysql_real_connect failed: errno: 2002, error: Can't connect to MySQL server on 'metadata-db' (115)MetadataStore cannot be created with the given connection config.
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ kubectl logs -n kubeflow mysql-7d8496b77c-n5f2m
2020-03-06 14:15:31+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.29-1debian10 started.
2020-03-06 14:15:31+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql'
2020-03-06 14:15:31+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.29-1debian10 started.
mkdir: cannot create directory '/var/lib/mysql/': File exists
&lt;/denchmark-code&gt;

I really do not feel there is a permission issue on the NFS server. The NFS server is wide open. On top of that, its not having any problems creating the container folder...and I can go into the container and write files with no problems.
&lt;denchmark-code&gt;
$ ls -lah /data
total 20K
drwxrwxrwx. 11 root       root      8.0K Mar  6 08:12 .
dr-xr-xr-x. 26 root       root      4.0K Feb 28 12:20 ..
drwxrwxrwx.  2 polkitd    bin        152 Mar  5 12:27 default-mysql-pv-claim-pvc-0808d1bd-69ca-4ff5-825a-b846b1133e3a
drwxrwxrwx.  2 4294967294 nfsnobody  152 Mar  4 13:44 default-pvc-nfs-pv1-pvc-7f0844e2-c874-49da-a016-fdadaf810b14
drwxrwxrwx.  3 root       bin        152 Mar  6 08:12 kubeflow-katib-mysql-pvc-5eeb2001-1a04-4da8-9419-c47fcd9b6aef
drwxrwxrwx.  3 root       bin        152 Mar  6 08:12 kubeflow-metadata-mysql-pvc-fa3e28c7-40fc-461c-b777-c541cb77e455
drwxrwxrwx.  3 root       bin        152 Mar  6 08:12 kubeflow-minio-pv-claim-pvc-f2b357a8-cbc5-4400-ab01-b7e6e597d127
drwxrwxrwx.  2 polkitd    bin        152 Mar  6 08:18 kubeflow-mysql-pv-claim-pvc-2e0baffd-9831-4413-b105-8f064a5cb07d
&lt;/denchmark-code&gt;

Just to add to this, no files are created in the kubeflow-mysql-pv-claim-pvc-2e0baffd-9831-4413-b105-8f064a5cb07d folder on the NFS server. Its empty.
&lt;denchmark-code&gt;$ ls -lah /data/kubeflow-mysql-pv-claim-pvc-3bbddd72-b0fb-4d5e-802e-92d28a35812d/
total 8.0K
drwxrwxrwx.  2 polkitd bin   152 Mar  6 08:34 .
drwxrwxrwx. 11 root    root 8.0K Mar  6 08:33 ..
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='vaskokj' date='2020-03-09T18:52:40Z'>
		&lt;denchmark-link:https://github.com/MnrGreg&gt;@MnrGreg&lt;/denchmark-link&gt;
 you find anything enlightening?  I'm still struggling with this. My post on mysql docker support hasn't gotten a response. The post on stackoverflow has gotten a few suggestions but nothing that has worked so far.
		</comment>
		<comment id='8' author='vaskokj' date='2020-03-09T21:57:35Z'>
		&lt;denchmark-link:https://github.com/vaskokj&gt;@vaskokj&lt;/denchmark-link&gt;
 I see a lot of logs about permission errors.
What I usually like to do to debug things like that is create a terminal into the affected environment and look around.
For example, you can overridde the mysql deployment entrypoint with ["/bin/sh", "-c", "sleep 9999999999"] and then kubectl exec inside the pod to see if you can create folders/files, what the permissions are, what user/group you are given etc.
		</comment>
		<comment id='9' author='vaskokj' date='2020-03-10T12:45:08Z'>
		&lt;denchmark-link:https://github.com/yanniszark&gt;@yanniszark&lt;/denchmark-link&gt;
 Thanks but I already did that. See my post here &lt;denchmark-link:https://github.com/kubeflow/kubeflow/issues/4827#issuecomment-594866984&gt;#4827 (comment)&lt;/denchmark-link&gt;
. It was one of the first things I did.
I can create new files, edit those files, write files and new folders to the folder /var/lib/mysql(NFS Share) etc.
Obviously, the container is trying to do a mkdir /var/lib/mysql/ and it already exists, which it should, it just got mounted and makes sense. mkdir will fail if the folder exists, so I assumed the container was doing a just mkdir instead of mkdir -p but I can't find anywhere in the container entrypoint.sh or the Dockerfile where just mkdir is being called.
I believe mkdir is being called somewhere where mkdir -p should be called but I think its a bug somewhere regardless.
		</comment>
		<comment id='10' author='vaskokj' date='2020-03-11T00:36:10Z'>
		&lt;denchmark-link:https://github.com/vaskokj&gt;@vaskokj&lt;/denchmark-link&gt;
 I see one diff was that I added the runAsUser and fsGroup to the securityContext but not sure that it was this that solved the problem. When I get a gap I'll do a clean re-install and see if I can replicate the issue.
kustomize/mysql/base/
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  strategy:
    type: Recreate
  template:
    spec:
      containers:
      - name: mysql
        securityContext:
          runAsUser: 999
          fsGroup: 999 
		</comment>
		<comment id='11' author='vaskokj' date='2020-03-11T13:39:54Z'>
		&lt;denchmark-link:https://github.com/MnrGreg&gt;@MnrGreg&lt;/denchmark-link&gt;

Thanks, I gave that a shot and I got a slightly different error message this time.
&lt;denchmark-code&gt;$ kubectl logs -n kubeflow mysql-6d459d4976-7hplv
2020-03-11 13:38:25+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 5.7.29-1debian10 started.
2020-03-11 13:38:25+00:00 [Note] [Entrypoint]: Initializing database files
mysqld: Can't create directory '/var/lib/mysql/' (Errcode: 17 - File exists)
2020-03-11T13:38:25.179956Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2020-03-11T13:38:25.183555Z 0 [ERROR] Aborting
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='vaskokj' date='2020-03-13T04:13:55Z'>
		&lt;denchmark-link:https://github.com/vaskokj&gt;@vaskokj&lt;/denchmark-link&gt;
 Just to add- I did a completely new install with the kubeflow v1.0 release without any issues and without have to change anything. This was using PVCs from nfs-client-provisioner.
		</comment>
		<comment id='13' author='vaskokj' date='2020-03-13T16:19:43Z'>
		&lt;denchmark-link:https://github.com/MnrGreg&gt;@MnrGreg&lt;/denchmark-link&gt;
 So I think I figured out the issue with the help from some of the people on the mysql container.
So Docker/Kubernetes, NFS client provisioner is essentially injecting the NFS mount automatically into the container. It fails with the above error, we established that. I could go into the mysql container with command: [ "/bin/sh", "-c", "sleep 100000" ] and could see the mount with df -h, and could read/write from the NFS as needed, there were no problems from that perspective.
However, the kicker in this is how the mount was done by the NFS provisioner. The NFS mount was mounted with mount  /&lt;localpath, which isn't a problem, except that this NFS server has kerberos security enabled. So what appears to be happening is that when mount  /&lt;localpath, was ran in the container, the NFS mount was automatically mounted with mount sec=krb5p (the mount command uses the high security the server and client support). The NFS server allowed the minimum security to be sec=sys, however mounting it with just mount and not an explicit option of mount -o sec=sys, it however defaulted to the highest mutual security between server and client, which was sec=krb5p.
I created a new NFS server and did not enable any security which resulted in the NFS Kerberos security to default to sec=sys. This allowed whatever permissions/mkdir calls to work properly.
You wouldn't happen to know how to explicitly make the NFS Provisioner pass the -o sec=sys parameters would you?
		</comment>
		<comment id='14' author='vaskokj' date='2020-03-16T22:13:20Z'>
		&lt;denchmark-link:https://github.com/vaskokj&gt;@vaskokj&lt;/denchmark-link&gt;
 The NFS backend in our case was an Isilon. At one point we had "Windows Mode" enabled on the Export which might have caused similar issues to what you experienced above.
I'm not 100% sure what the correct route is to add NFS parameters. Some of the old code seems to suggest one can do it within the StorageClass with 
&lt;denchmark-link:https://github.com/kubernetes-incubator/external-storage/pull/835/files&gt;https://github.com/kubernetes-incubator/external-storage/pull/835/files&lt;/denchmark-link&gt;

This Helm chart also suggests one can specific  on the NFS key too:
&lt;denchmark-link:https://github.com/helm/charts/tree/master/stable/nfs-client-provisioner&gt;https://github.com/helm/charts/tree/master/stable/nfs-client-provisioner&lt;/denchmark-link&gt;

Should probably start looking at the CSI driver going forward: &lt;denchmark-link:https://github.com/kubernetes-csi/csi-driver-nfs&gt;https://github.com/kubernetes-csi/csi-driver-nfs&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='vaskokj' date='2020-06-14T22:47:33Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='16' author='vaskokj' date='2020-06-14T22:47:41Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/katib
0.89



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/marketplace/issue-label-botdata/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
	</comments>
</bug>