<bug id='5275' author='jvendegna' open_date='2020-09-03T22:15:56Z' closed_time='2020-09-04T15:12:58Z'>
	<summary>pods not scheduling to GPU nodes, taint tolerations present</summary>
	<description>
/kind bug

Hello all, I am taking a break for a moment because of frustration with applying a taint-toleration using kfp on my ai-platform pipelines cluster.
I have a model training job which requires GPUs, not uncommon.
Default behavior for extras like GPUs in GKE is to add a node taint so pods must explicitly state a toleration to be scheduled to that node. I get that, I like that, I appreciate it.
So, I’ve read the docs &lt;denchmark-link:https://kubeflow-pipelines.readthedocs.io/en/latest/_modules/kfp/gcp.html#add_gpu_toleration&gt;here&lt;/denchmark-link&gt;

I applied that to my code like so:
&lt;denchmark-code&gt;toleration = client.models.v1_toleration.V1Toleration(effect='NoSchedule', key='nvidia.com/gpu', operator='Equal', value='present')
train.apply(gcp.add_gpu_toleration(toleration))
train.apply(gcp.add_gpu_toleration())
&lt;/denchmark-code&gt;

Generate pipeline file, upload it.
Triggers a scale up of the correct node pool with the CPU/RAM/GPU requirements I’ve specified.
kubectl describe node &lt;name&gt; shows only the following taint: nvidia.com/gpu=present:NoSchedule
kubectl describe pod &lt;name&gt; shows the taints I’ve tolerated via kfp.gcp.add_gpu_toleration():
&lt;denchmark-code&gt;Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
                 nvidia.com/gpu=present:NoSchedule
                 nvidia.com/gpu=true:NoSchedule
                 nvidia.com/gpu:NoSchedule
&lt;/denchmark-code&gt;

But here is the problem:
from kubectl describe pod &lt;name&gt; events field:
&lt;denchmark-code&gt;Events:
  Type     Reason            Age                 From                Message
  ----     ------            ----                ----                -------
  Normal   TriggeredScaleUp  15m                 cluster-autoscaler  pod triggered scale-up: [{https://content.googleapis.com/compute/v1/projects/&lt;REDACTED&gt; 1-&gt;2 (max: 10)}]
  Warning  FailedScheduling  13m (x4 over 16m)   default-scheduler   0/4 nodes are available: 3 Insufficient cpu, 3 Insufficient memory, 3 node(s) didn't match node selector, 4 Insufficient nvidia.com/gpu.
  Warning  FailedScheduling  90s (x12 over 13m)  default-scheduler   0/5 nodes are available: 3 Insufficient cpu, 3 Insufficient memory, 3 node(s) didn't match node selector, 5 Insufficient nvidia.com/gpu.
&lt;/denchmark-code&gt;

There is some useful information here. For one, I know my label selector is working. As only 3/5 nodes don’t match that.
So that is a win… Now if I could just start the pod on the node with the GPU, my whole day might turn out to be as nice as it is outside here in Denver today!
Any help is greatly appreciated, any cheat codes will definitely be rewarded.
What did you expect to happen:
Pod scheduled to node with 2 GPUs as defined in the workflow manifest

new pod does trigger scale up event
nvidia-gpu-device-plugin is present on scaled up nodes with attached gpus
node pool is configured correctly but seeing is believing so here's a clip:
&lt;denchmark-link:https://user-images.githubusercontent.com/58440894/92178548-089ae100-ee00-11ea-9be6-5812ca9056c7.png&gt;&lt;/denchmark-link&gt;

Environment:

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard): 1.0.0
kfctl version: (use kfctl version): never used it
Kubernetes platform: (e.g. minikube) GKE via AI-Platform
Kubernetes version: (use kubectl version):
`
Client Version: version.Info{Major:"1", Minor:"16+", GitVersion:"v1.16.6-beta.0", GitCommit:"e7f962ba86f4ce7033828210ca3556393c377bcc", GitTreeState:"clean", BuildDate:"2020-01-15T08:26:26Z", GoVersion:"go1.13.5", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"16+", GitVersion:"v1.16.13-gke.1", GitCommit:"688c6543aa4b285355723f100302d80431e411cc", GitTreeState:"clean", BuildDate:"2020-07-21T02:37:26Z", GoVersion:"go1.13.9b4", Compiler:"gc", Platform:"linux/amd64"}

&lt;denchmark-code&gt;- OS (e.g. from `/etc/os-release`): macos catalina
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jvendegna' date='2020-09-03T22:16:04Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='jvendegna' date='2020-09-03T22:16:04Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='3' author='jvendegna' date='2020-09-04T00:02:40Z'>
		/assign
I don't have experience trying this. let me find Who knows
		</comment>
		<comment id='4' author='jvendegna' date='2020-09-04T06:12:50Z'>
		&lt;denchmark-link:https://github.com/jvendegna&gt;@jvendegna&lt;/denchmark-link&gt;
 ,
Have you tried adding a nodeSelector ?
Something like:
  nodeSelector:
    cloud.google.com/gke-nodepool: my-gpu-enabled-nodepool-name  
		</comment>
		<comment id='5' author='jvendegna' date='2020-09-04T14:20:10Z'>
		
@jvendegna ,
Have you tried adding a nodeSelector ?
Something like:
  nodeSelector:
    cloud.google.com/gke-nodepool: my-gpu-enabled-nodepool-name  

I have, but I appreciate the suggestion, it still results in:  after scale-up event.
I'm going to try to follow the guide here to see if it fixes anything: &lt;denchmark-link:https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/gpu/gpu.ipynb&gt;https://github.com/kubeflow/pipelines/blob/master/samples/tutorials/gpu/gpu.ipynb&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='jvendegna' date='2020-09-04T14:29:08Z'>
		&lt;denchmark-link:https://github.com/jvendegna&gt;@jvendegna&lt;/denchmark-link&gt;
 when you do a describe on the gpu-node does it correctly show the availability of the GPUs that can be allocated on the machine like so:
&lt;denchmark-code&gt;Capacity:
 cpu:                            6
 memory:                         57713784Ki
 nvidia.com/gpu:                 2
Allocatable:
 cpu:                            5916m
 memory:                         51702904Ki
 nvidia.com/gpu:                 2
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='jvendegna' date='2020-09-04T14:33:15Z'>
		For future reference, this did the trick:
kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
The insufficient GPU message was due to no driver being installed on the node, as detailed in the link in my last comment.
		</comment>
		<comment id='8' author='jvendegna' date='2020-09-04T15:12:58Z'>
		&lt;denchmark-link:https://github.com/acesir&gt;@acesir&lt;/denchmark-link&gt;
 That's a great question worthy of exploration. I thought so but wasn't sure. I uninstalled the nvidia-driver-installer from my cluster and triggered another run. After scale up and to my surprise the pod was scheduled to the node anyway. I don't appreciate smoke and mirrors, so I dug deeper and recreated the nodepool. Turns out, no. You're inclinations were correct.
&lt;denchmark-code&gt;Capacity:
 attachable-volumes-gce-pd:  127
 cpu:                        16
 ephemeral-storage:          98868448Ki
 hugepages-2Mi:              0
 memory:                     107260540Ki
 pods:                       110
Allocatable:
 attachable-volumes-gce-pd:  127
 cpu:                        15890m
 ephemeral-storage:          47093746742
 hugepages-2Mi:              0
 memory:                     98936444Ki
 pods:                       110
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='jvendegna' date='2020-09-04T15:17:51Z'>
		
@acesir That's a great question worthy of exploration. I thought so but wasn't sure. I uninstalled the nvidia-driver-installer from my cluster and triggered another run. After scale up and to my surprise the pod was scheduled to the node anyway. I don't appreciate smoke and mirrors, so I dug deeper and recreated the nodepool. Turns out, no. You're inclinations were correct.
Capacity:
 attachable-volumes-gce-pd:  127
 cpu:                        16
 ephemeral-storage:          98868448Ki
 hugepages-2Mi:              0
 memory:                     107260540Ki
 pods:                       110
Allocatable:
 attachable-volumes-gce-pd:  127
 cpu:                        15890m
 ephemeral-storage:          47093746742
 hugepages-2Mi:              0
 memory:                     98936444Ki
 pods:                       110


&lt;denchmark-link:https://github.com/jvendegna&gt;@jvendegna&lt;/denchmark-link&gt;
 Yes the nvidia daemonSet is required in order to expose the GPU resource for scheduling otherwise kubernetes doesn't know about it and will only allow CPU/Memory requests for scheduling. Tainting and labeling the does no good as the GPU resources are not advertised for scheduling.
You can find more information here about the scenario: &lt;denchmark-link:https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/&gt;https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>