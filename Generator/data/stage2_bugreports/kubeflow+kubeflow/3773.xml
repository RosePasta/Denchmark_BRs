<bug id='3773' author='tanguycdls' open_date='2019-07-29T11:55:30Z' closed_time='2020-03-18T10:56:27Z'>
	<summary>Pipelines + tensorboard + S3, blank page</summary>
	<description>
Hi, I'm running Kubeflow (V0.5) on AWS, using Pipelines I want to use the Tensorboard button in the pipelines metadata result.
I saved my pipeline result into a private S3 bucket with the following code:
&lt;denchmark-code&gt;metadata = {
    'outputs' : [{
      'type': 'tensorboard',
      'source': 's3://'+ bucket_name + '/' + destination,
    }]}
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;with open('/mlpipeline-ui-metadata.json', 'w') as f:
        json.dump(metadata, f)`
&lt;/denchmark-code&gt;

But when I try opening the Artifacts results of that operation I only get the circle loading bar and nothing happens. If I look at the Network Request i find a POST REQUEST with the following response :



{"outputs": [{"type": "tensorboard", "source": "s3://BUCKET_NAME/FOLDER1/FOLDER2/1564392073"}]}



I tried using GCS instead to save the file and in that case, the Tensorboard button appears and I can start the TB instance. (It fails because I don't have any GC creds (MountVolume.SetUp failed for volume "gcp-credentials" : secrets "user-gcp-sa" not found).
How should I save my summary writers to be able to start the TB instance from the pipeline artifacts on AWS/ S3?
Thank your for your help,
&lt;denchmark-h:h2&gt;EDIT :&lt;/denchmark-h&gt;

I found that issue &lt;denchmark-link:https://github.com/kubeflow/pipelines/issues/337&gt;kubeflow/pipelines#337&lt;/denchmark-link&gt;
 in the pipeline project of Kubeflow. Can someone confirm that Tensorboard through Pipelines on AWS stored in S3 is currently not supported ?
	</description>
	<comments>
		<comment id='1' author='tanguycdls' date='2019-07-29T11:55:32Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.55. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='tanguycdls' date='2019-07-29T17:47:30Z'>
		&lt;denchmark-link:https://github.com/tanguycdls&gt;@tanguycdls&lt;/denchmark-link&gt;

Yes, it's hardcoded there and may takes some efforts to make it generic. Right now, TB in pipeline artifacts is not supported.
I would suggest you right now not using it. It create a pod and you need to manage its lifecycle. Instead, using one TB in kubeflow and load more model under s3 path.
		</comment>
		<comment id='3' author='tanguycdls' date='2019-07-29T17:47:41Z'>
		/assign &lt;denchmark-link:https://github.com/Jeffwan&gt;@Jeffwan&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='tanguycdls' date='2019-07-29T19:01:08Z'>
		Thank you for your answer  ! In that case we will start them independantly and handle their lifecycle manually on Notebooks ...
I did not have the time yet to try ROC Curve and confusion matrix are they supported with S3 or is it the same for all artifacts ?
Thks !
		</comment>
		<comment id='5' author='tanguycdls' date='2019-07-29T23:32:50Z'>
		&lt;denchmark-link:https://github.com/kubeflow/pipelines/pull/1278&gt;kubeflow/pipelines#1278&lt;/denchmark-link&gt;

ROC curve and Confusion Matrix is a little bit different. It will work.
		</comment>
		<comment id='6' author='tanguycdls' date='2019-09-17T22:54:11Z'>
		/platform aws
		</comment>
		<comment id='7' author='tanguycdls' date='2019-12-19T08:47:25Z'>
		With this PR &lt;denchmark-link:https://github.com/kubeflow/pipelines/pull/1906&gt;kubeflow/pipelines#1906&lt;/denchmark-link&gt;
, we are able to use S3 and Tensorboard.
It need to use AWS_REGION and  AWS_LOG_LEVEL=3 env variables (see &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/21898&gt;tensorflow/tensorflow#21898&lt;/denchmark-link&gt;
)
However

I would suggest you right now not using it. It create a pod and you need to manage its lifecycle. Instead, using one TB in kubeflow and load more model under s3 path.

The viewer controller is now supposed to clean older viewers pods but there is a bug I reported here:
&lt;denchmark-link:https://github.com/kubeflow/pipelines/issues/2757&gt;kubeflow/pipelines#2757&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='tanguycdls' date='2020-03-18T09:13:57Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='9' author='tanguycdls' date='2020-03-18T10:56:27Z'>
		it now works
		</comment>
	</comments>
</bug>