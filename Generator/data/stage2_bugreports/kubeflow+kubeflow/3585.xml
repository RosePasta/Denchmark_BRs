<bug id='3585' author='Snapple49' open_date='2019-07-01T14:31:33Z' closed_time='2020-02-26T19:51:58Z'>
	<summary>Kubeflow default serviceaccount lacking permissions</summary>
	<description>
Hi!
I'm experimenting a bit with Kubeflow on our cluster and I ran into some issues with RBAC permissions in pipelines.
Problem
The kubeflow pipelines experiment gave the following output:
This step is in Error state with this message: failed to save outputs: verify serviceaccount kubeflow:default has necessary privileges
Output of the command kubectl auth can-i view pod --as=kubeflow:serviceaccount:kubeflow:default:
&lt;denchmark-code&gt;Error from server (InternalError): an error on the server ("{\"Code\":{\"Code\":\"ServerError\",\"Status\":500},\"Message\":\"Forbidden 403: clusters.management.cattle.io \\\"local\\\" is forbidden: User \\\"kubeflow:serviceaccount:kubeflow:default\\\" cannot get resource \\\"clusters\\\" in API group \\\"management.cattle.io\\\" at the cluster scope\",\"Cause\":null,\"FieldName\":\"\"}") has prevented the request from succeeding (post selfsubjectaccessreviews.authorization.k8s.io)
&lt;/denchmark-code&gt;

After reading about some similar issues i found that there were no rolebinding/clusterrolebinding for the default serviceaccount in the kubeflow namespace, so I created a clusterrolebinding with role cluster-admin, serviceaccount default in the kubeflow namespace and this solved the issue, but I don't like providing full access and I feel like this should have been in place already?

Kubeflow installed as per &lt;denchmark-link:https://kubeflow.org/docs/started/getting-started-k8s/&gt;on-premise installation instructions&lt;/denchmark-link&gt;
, with 
It's a Rancher 2.2 kubernetes cluster
kubectl version:
&lt;denchmark-code&gt;Client Version: version.Info{Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529", GitTreeState:"clean", BuildDate:"2019-06-19T16:40:16Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.5", GitCommit:"2166946f41b36dea2c4626f90a77706f426cdea2", GitTreeState:"clean", BuildDate:"2019-03-25T15:19:22Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Snapple49' date='2019-07-01T14:31:36Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.63. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='Snapple49' date='2019-07-01T17:28:21Z'>
		&lt;denchmark-link:https://github.com/IronPan&gt;@IronPan&lt;/denchmark-link&gt;
 Is it expected that pipelines run with the default service account in the kubeflow namespace?
		</comment>
		<comment id='3' author='Snapple49' date='2019-07-03T09:55:05Z'>
		Another issue which I'm thinking could be related is that the argo clusterrole seems to have too few permissions, in the pipelines UI I cannot delete experiments because argo cannot clean up pods. I'm just assuming this is relevant since I understand kfpipelines is built on argo? This is the output of kubectl describe clusterrole argo:
&lt;denchmark-code&gt;Name:         argo
Labels:       app=argo
              app.kubernetes.io/deploy-manager=ksonnet
              ksonnet.io/component=argo
Annotations:  ksonnet.io/managed:
                {"pristine":"H4sIAAAAAAAA/4yQsW4yMRCE+/8xprYO0f26NkX6FGkiij3fAg4+r7VeQxTEu0c2QkoUJFKt12N/M5ozKIdX1hIkYYRO5AeqthcNn2RB0nD4X4Ygq+N6YqM1HA4hz...
              kubecfg.ksonnet.io/garbage-collect-tag: gc-tag
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  persistentvolumeclaims  []                 []              [create delete]
  pods/exec               []                 []              [create get list watch update patch]
  pods                    []                 []              [create get list watch update patch]
  workflows.argoproj.io   []                 []              [get list watch update patch]
  configmaps              []                 []              [get watch list]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Snapple49' date='2019-10-01T09:59:54Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='5' author='Snapple49' date='2019-10-01T15:14:54Z'>
		I have install kubeflow 0.6.0 in one namespace and now I am installing kubeflow 0.6.2 using kfctl in custom namespace like: kfctl init mykubeflow --namespace kftest --config "my/config/path".
and i am facing one issue that service account "argo" for second kubeflow installation is not getting added into clusterrolebinding "argo". due to that my workflow-contrller pod is going into CrashLoopBackOff.
Error log:
$ kubectl logs workflow-controller-68c5865896-pn4ds -n kftest
Error: configmaps "workflow-controller-configmap" is forbidden: User "system:serviceaccount:kftest:argo" cannot get resource "configmaps" in API group "" in the namespace "kftest"
configmaps "workflow-controller-configmap" is forbidden: User "system:serviceaccount:kftest:argo" cannot get resource "configmaps" in API group "" in the namespace "kftest"
Any idea why it is not getting added into clusterrolebinding.
		</comment>
		<comment id='6' author='Snapple49' date='2019-11-20T23:35:09Z'>
		&lt;denchmark-link:https://github.com/harshalkwagh&gt;@harshalkwagh&lt;/denchmark-link&gt;
  I am facing a similar issue. Any luck resolving it?
		</comment>
		<comment id='7' author='Snapple49' date='2019-11-21T08:41:14Z'>
		I would take a look at the rolebinding yamls in the  and in argo I would compare the Kubeflow generated version towards Argo's own clusterroles, found &lt;denchmark-link:https://github.com/argoproj/argo-helm/tree/master/charts/argo/templates&gt;here&lt;/denchmark-link&gt;
. I've made a much more custom installation of Argo since I have some quite specific environment quirks, but I think that might be a place to start
		</comment>
		<comment id='8' author='Snapple49' date='2020-02-19T09:17:26Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>