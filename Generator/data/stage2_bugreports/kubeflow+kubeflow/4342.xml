<bug id='4342' author='holdenk' open_date='2019-10-18T16:09:19Z' closed_time='2019-10-31T11:31:29Z'>
	<summary>IAP enabler is broken / gcloud crashes in IAP enabler</summary>
	<description>
/kind bug
What steps did you take and what happened:
Deploy kubeflow on GCP, attempt to access IAP receive error
What did you expect to happen:
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Here's the logs of the istio-enabler pod:


'[' -z istio-system ']'
'[' -z istio-ingressgateway ']'
'[' -z envoy-ingress ']'
++ curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/project-id
PROJECT=boos-demo-projects-are-rad
'[' -z boos-demo-projects-are-rad ']'
++ curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/numeric-project-id
PROJECT_NUM=658583755711
'[' -z 658583755711 ']'
'[' '!' -z '' ']'
gcloud config list
ERROR: gcloud crashed (MetadataServerException): HTTP Error 500: Internal Server Error

If you would like to report this issue, please run the following command:
gcloud feedback
To check gcloud for common problems, please run the following command:
gcloud info --run-diagnostics

gcloud auth list
ERROR: gcloud crashed (MetadataServerException): HTTP Error 500: Internal Server Error

If you would like to report this issue, please run the following command:
gcloud feedback
To check gcloud for common problems, please run the following command:
gcloud info --run-diagnostics
++ kubectl --namespace=istio-system get svc istio-ingressgateway -o 'jsonpath={.spec.ports[?(@.name=="http2")].nodePort}'

NODE_PORT=31380
echo 'node port is 31380'
node port is 31380
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
backend name is
sleep 2
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=


Environment:

Kubeflow version: (version number can be found at the bottom left corner of the Kubeflow dashboard): N/A (did not reach deployment)
kfctl version: (use kfctl version): Built from b624463
Kubernetes platform: (e.g. minikube): GCP
Kubernetes version: (use kubectl version): Server Version: version.Info{Major:"1", Minor:"14+", GitVersion:"v1.14.6-gke.2", GitCommit:"c9de33b5439df6e206d7ba646787c6ace92d737b", GitTreeState:"clean", BuildDate:"2019-09-06T18:30:33Z", GoVersion:"go1.12.9b4", Compiler:"gc", Platform:"linux/amd64"}
OS (e.g. from /etc/os-release):
manifests version: 117b5d2

	</description>
	<comments>
		<comment id='1' author='holdenk' date='2019-10-18T16:09:40Z'>
		Issue Label Bot is not confident enough to auto-label this issue.
See &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 for more details.
		</comment>
		<comment id='2' author='holdenk' date='2019-10-18T22:23:55Z'>
		&lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 Which version of Kubeflow did you use? What was the URL of the config file and what was the URL of the config file you are using?
It is having a problem contacting the metadata server. There was an issue with GKE and workload identity &lt;denchmark-link:https://github.com/kubeflow/kfctl/issues/48&gt;kubeflow/kfctl#48&lt;/denchmark-link&gt;
 ; that should be fixed in the newer configs by pinning to an older version of GKE.
What version is your cluster?
Can you also run the following command
&lt;denchmark-code&gt;kubectl -n kube-system get pods -l  k8s-app=gke-metadata-server
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='holdenk' date='2019-10-18T23:24:55Z'>
		The version hashes are in the initial report. I'm using the IAP kfdef from manifests, but edited with my project, email, and such. I'll try updating kfctl as well tomorrow if it's a split fix (part in manifests part in kfctl)?

holden@miniholden:~/repos/manifests$ kubectl -n kube-system get pods -l  k8s-app=gke-metadata-server
NAME                        READY   STATUS    RESTARTS   AGE
gke-metadata-server-855rk   1/1     Running   0          10h
gke-metadata-server-tftjd   1/1     Running   0          10h

		</comment>
		<comment id='4' author='holdenk' date='2019-10-18T23:35:50Z'>
		Thanks &lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 my apologies I missed that.
This doesn't look like an issue with kfctl or our configs so upgrading is unlikely to help. Its possible that a new GKE cluster would help. The problem is that gcloud is unable to contact the metadata server. This is likely related to workload identity which runs a fake metadata server as daemonset.
Can we look at the logs of the gke metadata server objects
&lt;denchmark-code&gt;kubectl -n kube-system logs -l k8s-app=gke-metadata-server
&lt;/denchmark-code&gt;

Could you also try forcing a restart of the iap-enabler pod by deleting it?
Lets also try running a new gcloud pod to see if the problem is reproducible
&lt;denchmark-code&gt;kubectl run -it \
  --generator=run-pod/v1 \
  --image google/cloud-sdk \
  --serviceaccount kf-admin \
  --namespace istio-system \
  workload-identity-test
&lt;/denchmark-code&gt;

Then inside the pod run gcloud auth list.
		</comment>
		<comment id='5' author='holdenk' date='2019-10-18T23:50:55Z'>
		So the metadata service logs look ok:

holden@miniholden:~/repos/manifests$ kubectl -n kube-system logs -l k8s-app=gke-metadata-server
I1018 22:59:08.083435       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fmonitoring%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fmonitoring.read%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fmonitoring.write HTTP/200 size=202
I1018 22:59:15.183846       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 22:59:27.983795       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 22:59:30.487767       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 23:30:29.981953       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 23:30:30.585211       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 23:31:07.685251       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fmonitoring%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fmonitoring.read%2Chttps%3A%2F%2Fwww.googleapis.com%2Fauth%2Fmonitoring.write HTTP/200 size=202
I1018 23:31:15.392828       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 23:31:27.883804       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 23:33:55.084725       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 16:32:26.090550       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 17:00:25.987249       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 17:58:25.789181       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 18:52:25.711730       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 19:49:26.287235       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 20:19:25.888327       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 20:54:25.889403       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 21:22:26.288205       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 21:55:25.700815       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202
I1018 22:52:25.790198       1 metadata.go:473] /computeMetadata/v1/instance/service-accounts/default/token HTTP/200 size=202

Deleting the IAP pod and seeing it re-launch and it seems to still not have the backend but the gcloud command succeeds:

^C
holden@miniholden:~/gcp07b$ kubectl logs -n istio-system iap-enabler-7f87598576-58c7d

'[' -z istio-system ']'
'[' -z istio-ingressgateway ']'
'[' -z envoy-ingress ']'
++ curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/project-id
PROJECT=boos-demo-projects-are-rad
'[' -z boos-demo-projects-are-rad ']'
++ curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/numeric-project-id
PROJECT_NUM=658583755711
'[' -z 658583755711 ']'
'[' '!' -z '' ']'
gcloud config list
[component_manager]
disable_update_check = true
[core]
account = gcp07b-admin@boos-demo-projects-are-rad.iam.gserviceaccount.com
disable_usage_reporting = true
project = boos-demo-projects-are-rad
[metrics]
environment = github_docker_image

Your active configuration is: [default]

gcloud auth list
Credentialed Accounts
ACTIVE  ACCOUNT



  gcp07b-admin@boos-demo-projects-are-rad.iam.gserviceaccount.com



To set the active account, run:
$ gcloud config set account ACCOUNT
++ kubectl --namespace=istio-system get svc istio-ingressgateway -o 'jsonpath={.spec.ports[?(@.name=="http2")].nodePort}'

NODE_PORT=31380
echo 'node port is 31380'
node port is 31380
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
backend name is
sleep 2
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is


Running the cloud-sdk pod also seems to work:

holden@miniholden:~/repos/manifests$ kubectl run -it \

--generator=run-pod/v1 
--image google/cloud-sdk 
--serviceaccount kf-admin 
--namespace istio-system 
workload-identity-test

If you don't see a command prompt, try pressing enter.
root@workload-identity-test:/#
root@workload-identity-test:/#
root@workload-identity-test:/#
root@workload-identity-test:/# gcloud
ERROR: (gcloud) Command name argument expected.
root@workload-identity-test:/# gcloud config list
[core]
account = gcp07b-admin@boos-demo-projects-are-rad.iam.gserviceaccount.com
disable_usage_reporting = True
project = boos-demo-projects-are-rad
Your active configuration is: [default]
root@workload-identity-test:/#

		</comment>
		<comment id='6' author='holdenk' date='2019-10-18T23:51:48Z'>
		I'll leave it to run overnight if maybe something needs to settle, happy to do more debugging tomorrow. Or if it makes sense I can blow away and start with a new version if we think this is fixed by something.
		</comment>
		<comment id='7' author='holdenk' date='2019-10-19T00:12:56Z'>
		&lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 replied in slack that after bouncing the IAP pod; gcloud was able to contact the metadata server and get the config.
Its unclear whether killing the pod is necessary or if just restarting the container would be sufficient.
If we could fix it just by restarting the container then we could just exit in the event of problems contacting the metadata server.
		</comment>
		<comment id='8' author='holdenk' date='2019-10-19T12:36:25Z'>
		To be clear while it get metadata, the IAP gateway did not come up.
It might make sense to change this script to exit on failure so the pod restarts automatically.
		</comment>
		<comment id='9' author='holdenk' date='2019-10-21T18:18:17Z'>
		&lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 that's a good suggestion.
&lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 Could you elaborate on what you mean by IAP gateway didn't come up? That script is creating the JWT policy. Is the policy in fact created?
&lt;denchmark-code&gt;kubectl -n istio-system get policy -o yaml ingress-jwt
&lt;/denchmark-code&gt;

		</comment>
		<comment id='10' author='holdenk' date='2019-10-21T18:47:52Z'>
		I had a deployment where I was getting origin authentication failure because the backend id used in the jwt policy was wrong.
Changing it to the correct value fixed the issue.
What's puzzling is that the logs output indicates that the backend id should have been correct.
Here are the logs from the iap enabler pod
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/files/3752397/logs.txt&gt;logs.txt&lt;/denchmark-link&gt;

The relevant logs are
&lt;denchmark-code&gt;+ gcloud compute --project=kubeflow-ci-deployment backend-services list --filter=name~k8s-be-31380--e4cbfbe055d11c15 '--format=value(id)'
+ BACKEND_ID=5983735141766762113
&lt;/denchmark-code&gt;

This turns out to be the wrong backend id. Rerunning the command now it looks like the backend is different
&lt;denchmark-code&gt;gcloud compute --project=kubeflow-ci-deployment backend-services list --filter=name~k8s-be-31380--e4cbfbe055d11c15 '--format=value(id)'
1889258933483317877
&lt;/denchmark-code&gt;

This is the correct value. So it looks like the backend changed over time. I didn't know this was possible; but I guess its possible the ingress controller for whatever reason deleted it and recreated it.
So I think we need two fixes

If we don't get the metadata lets exit in the hopes a container restart helps
We should periodically call the set JWT policy function in a loop so that we recover if the backend service changes.

		</comment>
		<comment id='11' author='holdenk' date='2019-10-21T18:51:31Z'>
		It looks like the exit on error is already fixed.
&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/1b139edff8a4a7819003670210a5e0bf2ff407e9/gcp/iap-ingress/base/config-map.yaml#L60&gt;https://github.com/kubeflow/manifests/blob/1b139edff8a4a7819003670210a5e0bf2ff407e9/gcp/iap-ingress/base/config-map.yaml#L60&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='holdenk' date='2019-10-21T22:18:42Z'>
		/assign &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='holdenk' date='2019-10-22T01:44:57Z'>
		I gave it another shot today while working on my Spark PR. However the IAP still is not working. Going with a manifest based on top of &lt;denchmark-link:https://github.com/kubeflow/kubeflow/commit/ca951f4ffe4d9e016aec795b83ac46f8b0d60261&gt;ca951f4&lt;/denchmark-link&gt;
 (namely using ede913bbe2e3d3931c131728775d95e7f913bb98)  w/ kfctl 0.7rc3 looking at the IAP log I get something very similar.


'[' -z istio-system ']'
'[' -z istio-ingressgateway ']'
'[' -z envoy-ingress ']'
++ curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/project-id
PROJECT=boos-demo-projects-are-rad
'[' -z boos-demo-projects-are-rad ']'
++ curl -s -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/numeric-project-id
PROJECT_NUM=658583755711
'[' -z 658583755711 ']'
'[' '!' -z '' ']'
gcloud config list
ERROR: (gcloud.config.list) timed out
This may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.
gcloud auth list
Credentialed Accounts
ACTIVE  ACCOUNT



  gcp07b-admin@boos-demo-projects-are-rad.iam.gserviceaccount.com



To set the active account, run:
$ gcloud config set account ACCOUNT

true
set_jwt_policy
++ kubectl --namespace=istio-system get svc istio-ingressgateway -o 'jsonpath={.spec.ports[?(@.name=="http2")].nodePort}'
node port is 31380
NODE_PORT=31380
echo 'node port is 31380'
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
fetching backends info with envoy-ingress:
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
++ echo
++ grep -o 'k8s-be-31380--[0-9a-z]+'
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ ++ grep echo
-o 'k8s-be-31380--[0-9a-z]+'
BACKEND_NAME=
echo 'backend name is '


		</comment>
		<comment id='14' author='holdenk' date='2019-10-22T01:45:54Z'>
		Also here is the ingress-jwt polcy:

holden@hkdesktop:~/repos/manifests$ kubectl -n istio-system get policy -o yaml ingress-jwt
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
annotations:
kubectl.kubernetes.io/last-applied-configuration: |
{"apiVersion":"authentication.istio.io/v1alpha1","kind":"Policy","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"iap-ingress","app.kubernetes.io/instance":"iap-ingress-v0.7.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"iap-ingress","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"v0.7.0","kustomize.component":"iap-ingress"},"name":"ingress-jwt","namespace":"istio-system"},"spec":{"origins":[{"jwt":{"audiences":["TO_BE_PATCHED"],"issuer":"https://cloud.google.com/iap","jwksUri":"https://www.gstatic.com/iap/verify/public_key-jwk","jwtHeaders":["x-goog-iap-jwt-assertion"],"trigger_rules":[{"excluded_paths":[{"exact":"/healthz/ready"},{"prefix":"/.well-known/acme-challenge"}]}]}}],"principalBinding":"USE_ORIGIN","targets":[{"name":"istio-ingressgateway","ports":[{"number":80}]}]}}
creationTimestamp: "2019-10-22T00:49:16Z"
generation: 1
labels:
app.kubernetes.io/component: iap-ingress
app.kubernetes.io/instance: iap-ingress-v0.7.0
app.kubernetes.io/managed-by: kfctl
app.kubernetes.io/name: iap-ingress
app.kubernetes.io/part-of: kubeflow
app.kubernetes.io/version: v0.7.0
kustomize.component: iap-ingress
name: ingress-jwt
namespace: istio-system
resourceVersion: "3281"
selfLink: /apis/authentication.istio.io/v1alpha1/namespaces/istio-system/policies/ingress-jwt
uid: c61b6ed9-f465-11e9-a67d-42010a8a0110
spec:
origins:

jwt:
audiences:

TO_BE_PATCHED
issuer: https://cloud.google.com/iap
jwksUri: https://www.gstatic.com/iap/verify/public_key-jwk
jwtHeaders:
x-goog-iap-jwt-assertion
trigger_rules:
excluded_paths:

exact: /healthz/ready
prefix: /.well-known/acme-challenge
principalBinding: USE_ORIGIN
targets:




name: istio-ingressgateway
ports:

number: 80




		</comment>
		<comment id='15' author='holdenk' date='2019-10-22T03:22:42Z'>
		&lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 it looks the script was not able to get the backend
&lt;denchmark-code&gt; kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
&lt;/denchmark-code&gt;

Could you please check the following


Run the same kubectl command; do you get any ouput?
kubectl --namespace=istio-system get ingress envoy-ingress -o yaml



If the backend you get when you run the CLI is different from the value in the script could you try deleting the pod and seeing what happens when the pod restarts?


The script is looping here
&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/973fba5a2012e7c9e305092727b2da446a2b6b81/gcp/iap-ingress/base/config-map.yaml#L85&gt;https://github.com/kubeflow/manifests/blob/973fba5a2012e7c9e305092727b2da446a2b6b81/gcp/iap-ingress/base/config-map.yaml#L85&lt;/denchmark-link&gt;

Because it is unable to get a backend. So one of two things is happening
i) Either there was a problem setting up the ingress and no backend was created and so the BACKEND is in fact empty
ii) Something is messed up with pod networking and kubectl is having problems contacting the APIServer.
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/pull/1&gt;#1&lt;/denchmark-link&gt;
 seems more likely but the spec/status of the ingress should make it easy to confirm.
		</comment>
		<comment id='16' author='holdenk' date='2019-10-22T05:29:09Z'>
		FWIW I deployed using the following
&lt;denchmark-link:https://github.com/kubeflow/kubeflow/commit/2fa8ba73&gt;2fa8ba7&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/kubeflow/manifests/commit/5eea1f1&gt;kubeflow/manifests@5eea1f1&lt;/denchmark-link&gt;

Everything worked fine.
		</comment>
		<comment id='17' author='holdenk' date='2019-10-31T11:31:29Z'>
		I deployed again using the latest RC.
kubeflow/manifests - v0.7.0-rc.2-16-gd6dad08
kubeflow/kubeflow - v0.4.0-rc.1-802-gd66f8b97
Everything is working.
&lt;denchmark-code&gt;kubectl -n istio-system get pods
NAME                                      READY   STATUS      RESTARTS   AGE
backend-updater-0                         1/1     Running     0          11h
grafana-67c69bb567-42zb2                  1/1     Running     0          11h
iap-enabler-7f87598576-nkmjh              1/1     Running     0          11h
istio-citadel-67697b6697-7wkh4            1/1     Running     0          11h
istio-cleanup-secrets-1.1.6-cqpmn         0/1     Completed   0          11h
istio-egressgateway-7dbbb87698-p5pxc      1/1     Running     0          11h
istio-egressgateway-7dbbb87698-sbb4r      1/1     Running     0          11h
istio-galley-7bffd57ff4-s2hqr             1/1     Running     0          11h
istio-grafana-post-install-1.1.6-5wnfb    0/1     Completed   0          11h
istio-ingressgateway-565b894b5f-q6qs9     1/1     Running     0          11h
istio-ingressgateway-565b894b5f-qhq84     1/1     Running     0          11h
istio-pilot-6dd5b8f74c-npn44              2/2     Running     0          11h
istio-pilot-6dd5b8f74c-p2pks              2/2     Running     0          11h
istio-policy-7f8bb87857-pkcdw             2/2     Running     1          11h
istio-security-post-install-1.1.6-c5npp   0/1     Completed   0          11h
istio-sidecar-injector-fd5875568-r67c6    1/1     Running     0          11h
istio-telemetry-8759dc6b7-bdtmj           2/2     Running     1          11h
istio-tracing-5d8f57c8ff-mv8rc            1/1     Running     0          11h
kiali-d4d886dd7-q4bzt                     1/1     Running     0          11h
prometheus-d8d46c5b5-x5m9h                1/1     Running     0          11h
whoami-app-cbdf7bf6f-nm2fx                1/1     Running     0          11h
&lt;/denchmark-code&gt;

Our endpoint test for IAP is also green.
&lt;denchmark-link:https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-periodic-master&gt;https://k8s-testgrid.appspot.com/sig-big-data#kubeflow-periodic-master&lt;/denchmark-link&gt;

So I'm going to close this issue as fixed.
		</comment>
	</comments>
</bug>