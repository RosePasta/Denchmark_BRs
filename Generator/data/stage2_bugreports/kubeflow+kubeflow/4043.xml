<bug id='4043' author='amygdala' open_date='2019-08-30T17:54:46Z' closed_time='2019-11-07T22:20:58Z'>
	<summary>in KF0.6.2 IAP GKE install, 'iap-enabler' pod is crash-looping</summary>
	<description>
/kind bug
using KF0.6.2 w/ IAP on GKE.
I see the iap-enabler-* pod continuously crash-looping.  Here's its logs:
&lt;denchmark-code&gt;kubectl logs iap-enabler-778889874f-4vrfd -n istio-system
Activated service account credentials for: [kf062test-admin@xxxx.iam.gserviceaccount.com]
[component_manager]
disable_update_check = true
[core]
account = kf062test-admin@xxxx.iam.gserviceaccount.com
disable_usage_reporting = true
project =xxxx
[metrics]
environment = github_docker_image

Your active configuration is: [default]
node port is 31380
fetching backends info with envoy-ingress: {"k8s-be-31380--5810244e269b1626":"HEALTHY","k8s-be-32253--5810244e269b1626":"HEALTHY"}
backend name is k8s-be-31380--5810244e269b1626
Waiting for backend id PROJECT=xxxx NAMESPACE=istio-system SERVICE=istio-ingressgateway filter=name~k8s-be-31380--5810244e269b1626
BACKEND_ID=6347129120648147983
patch JWT audience: /projects/xxxx/global/backendServices/6347129120648147983
policy.authentication.istio.io/ingress-jwt not patched
Clearing lock on service annotation
service/istio-ingressgateway not patched
Fri Aug 30 17:48:10 UTC 2019 WARN: Backend check failed, restarting container.
&lt;/denchmark-code&gt;

I saw this w/ both kfctl-based install and using the (staging) c2d webapp.
	</description>
	<comments>
		<comment id='1' author='amygdala' date='2019-08-30T17:54:48Z'>
		Issue-Label Bot is automatically applying the label kind/bug to this issue, with a confidence of 0.80. Please mark this comment with üëç or üëé to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://mlbot.net/data/kubeflow/kubeflow&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='amygdala' date='2019-09-09T12:58:52Z'>
		Links to source
&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/deployment.yaml#L44&gt;https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/deployment.yaml#L44&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/config-map.yaml#L127&gt;https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/config-map.yaml#L127&lt;/denchmark-link&gt;

It looks like there may have been a problem patching the services.
If anyone encounters this issue it would be great to grab the following output as well.
&lt;denchmark-code&gt;NAMESPACE=istio-system
SERVICE=istio-ingressgateway

kubectl -n ${NAMESPACE} get policy ingress-jwt -o yaml
&lt;/denchmark-code&gt;

It looks like we clear the annotation on the service here
&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/config-map.yaml#L111&gt;https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/config-map.yaml#L111&lt;/denchmark-link&gt;

We should add the -x option so we can see what the output of the commands are to aid debugging.
		</comment>
		<comment id='3' author='amygdala' date='2019-09-09T13:04:11Z'>
		It looks like checkBackend is checking if the backend timeout is updated to 1 hour.
&lt;denchmark-link:https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/config-map.yaml#L122&gt;https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/config-map.yaml#L122&lt;/denchmark-link&gt;

But setup.sh doesn't actually update the timeout to one hour; that happens in in update_backend.sh.
So maybe we should remove that check?
So hypothesis is that its update_backend.sh that is failing to update the timeout and that's why the iap-enabler pod is crashing.
		</comment>
		<comment id='4' author='amygdala' date='2019-09-09T13:06:04Z'>
		We should aim to do the following

Try to troubleshoot this and come up with docs for explaining how users can figure out what the root cause is and fix it
For 0.7 add -e  to the bash scripts so we echo commands to aid debugging
For 0.7 remove the check in checkBackend that verifies the timeout is set correctly from setup.sh

		</comment>
		<comment id='5' author='amygdala' date='2019-09-11T06:56:34Z'>
		HI, When I was running Kubeflow 0.6.2 in GCP according to the &lt;denchmark-link:https://www.kubeflow.org/docs/gke/deploy/deploy-cli/&gt;CLI deployment tutorial&lt;/denchmark-link&gt;
, I encountered the same problem that iap-enabler-778889874f-vxtx8 was in CrashLoopBackOff.
By the suggested commands
&lt;denchmark-code&gt;NAMESPACE=istio-system
SERVICE=istio-ingressgateway

kubectl -n ${NAMESPACE} get policy ingress-jwt -o yaml
&lt;/denchmark-code&gt;

I have got a yaml file:
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  creationTimestamp: "2019-09-11T03:37:50Z"
  generation: 1
  labels:
    kustomize.component: iap-ingress
  name: ingress-jwt
  namespace: istio-system
  resourceVersion: "8275"
  selfLink: /apis/authentication.istio.io/v1alpha1/namespaces/istio-system/policies/ingress-jwt
  uid: 87b73d46-d445-11e9-ba4d-42010a800030
spec:
  origins:
  - jwt:
      audiences:
      - /projects/26155353191/global/backendServices/4716516933859922628
      issuer: https://cloud.google.com/iap
      jwksUri: https://www.gstatic.com/iap/verify/public_key-jwk
      jwtHeaders:
      - x-goog-iap-jwt-assertion
      trigger_rules:
      - excluded_paths:
        - exact: /healthz
        - prefix: /.well-known/acme-challenge
  principalBinding: USE_ORIGIN
  targets:
  - name: istio-ingressgateway
    ports:
    - number: 80
I hope this will be some help for this problem, thank you.
		</comment>
		<comment id='6' author='amygdala' date='2019-09-16T16:21:29Z'>
		Is there some workaround  for the issue?
		</comment>
		<comment id='7' author='amygdala' date='2019-09-20T09:32:01Z'>
		I have no idea, but Kubeflow v0.6.2 deployment by the UI tool has come to go well.
Now Kubeflow UI is working well although iap-enabler is still in CrashLoopBackOff.
		</comment>
		<comment id='8' author='amygdala' date='2019-09-21T05:15:34Z'>
		I still get iap-enabler in CrashLoopBackOff when using deploy via UI.
		</comment>
		<comment id='9' author='amygdala' date='2019-10-01T18:23:00Z'>
		We are having the same problem. I believe the problem is that &lt;denchmark-link:https://github.com/kubeflow/manifests/blob/e97671c579e176bca9928c8cefa38c1c16432fe8/gcp/iap-ingress/base/config-map.yaml#L122&gt;https://github.com/kubeflow/manifests/blob/e97671c579e176bca9928c8cefa38c1c16432fe8/gcp/iap-ingress/base/config-map.yaml#L122&lt;/denchmark-link&gt;
 requires that the  of the backend service is 3600 seconds, but there is an error when  (invoked by the backend-updater StatefulSet) &lt;denchmark-link:https://github.com/kubeflow/manifests/blob/e97671c579e176bca9928c8cefa38c1c16432fe8/gcp/iap-ingress/base/config-map.yaml#L194-L199&gt;tries to update the timeout to be 3600 seconds&lt;/denchmark-link&gt;
.
From the backend-updater-0 pod's logs:
&lt;denchmark-code&gt;Increasing backend timeout for JupyterHub
ERROR: (gcloud.compute.backend-services.update) unrecognized arguments:
  https://www.googleapis.com/compute/v1/projects/our-project/global/backendServices/k8s-be-31380--912a1fa9425e39fa
  https://www.googleapis.com/compute/v1/projects/our-project/global/backendServices/k8s-be-31380--dc141d6075541ed7
Backend updated successfully. Waiting 1 hour before updating again.
&lt;/denchmark-code&gt;

and when adding more echo commands to debug setup_backend.sh run by iap-enabler:
policy.authentication.istio.io/ingress-jwt not patched
Clearing lock on service annotation
service/istio-ingressgateway not patched
# I added this output ...
BACKEND_ID 4889179730160308585
CURR_BACKEND_ID 4889179730160308585
CURR_BACKEND_TIMEOUT 30
# ... before this command is run:
# [[ "$BACKEND_ID" == "$CURR_BACKEND_ID" &amp;&amp; "${CURR_BACKEND_TIMEOUT}" -eq 3600 ]]
Tue Oct  1 18:24:31 UTC 2019 WARN: Backend check failed, restarting container.
Seems like there are a lot of coupling between the scripts run as the iap-enabler Pod (which is launched by a Deployment) and the scripts run by backend-updater StatefulSet, and that the operations need to be completed in a certain order.
I'm not very familiar with why there are separated like this and the details of exactly what they are updating, but would it be simpler (and less prone to race conditions on out-of-order events) if this was a single script and pod?
		</comment>
		<comment id='10' author='amygdala' date='2019-10-08T19:31:44Z'>
		&lt;denchmark-h:h2&gt;Work around&lt;/denchmark-h&gt;

To work around this issue I would first verify whether the crash looping is actually affecting you
These scripts are doing two things

Setting the JWT on the ISTIO policy
Updating the timeout for the backend

So if both of these are true then you should be good to go.
To check the JWT ISTIO policy
&lt;denchmark-code&gt;kubectl -n istio-system get policy ingress-jwt -o yaml
&lt;/denchmark-code&gt;



If the audience is correctly set e.g. to /projects/${PROJECTNUMBER}/global/backendServices/${BACKENDSERVICE}


Then your policy is correctly set


If not then you could manually update using kubectl


To check the backend timeout
&lt;denchmark-code&gt;gcloud --project=${PROJECT} compute backend-services describe --global ${BACKEND}
&lt;/denchmark-code&gt;


If the timeout isn't set to 1 hour then you could update it using gcloud.

Update 10/21 - A proper fix for the crash looping will be in 0.7
## For a proper fix I think we can try something like

~~ Remove checkBackend from setup_backend.sh~~


setup_backend.sh is configuring the JWT policy based on IAP; its not updating the backend timeout so there's no reason to check that


i.e delete everything after https://github.com/kubeflow/manifests/blob/56e2fb15db286198f7a53723cb1fbfecf3fe83fb/gcp/iap-ingress/base/config-map.yaml#L113


Add a sleep at the end for 1800-3600 seconds




I think by design we want the container to periodically restart. I think this is because we saw weird issues with IAM where we needed to restart the container to pick up IAM changes.
		</comment>
		<comment id='11' author='amygdala' date='2019-10-08T20:25:14Z'>
		&lt;denchmark-link:https://github.com/mattnworb&gt;@mattnworb&lt;/denchmark-link&gt;
 any interest in submitting a PR to try improve the backend scripts; e.g.

Add set e so we get better logging
Remove the backend timeout check from setup_backend.sh

		</comment>
		<comment id='12' author='amygdala' date='2019-10-15T13:15:56Z'>
		I'm running into this on 0.7 RC1.

holden@miniholden:/repos/sparklingml$ NAMESPACE=istio-system
holden@miniholden:/repos/sparklingml$ SERVICE=istio-ingressgateway
holden@miniholden:/repos/sparklingml$
holden@miniholden:/repos/sparklingml$ kubectl -n ${NAMESPACE} get policy ingress-jwt -o yaml
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
annotations:
kubectl.kubernetes.io/last-applied-configuration: |
{"apiVersion":"authentication.istio.io/v1alpha1","kind":"Policy","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"iap-ingress","app.kubernetes.io/instance":"iap-ingress-v0.7.0","app.kubernetes.io/managed-by":"kfctl","app.kubernetes.io/name":"iap-ingress","app.kubernetes.io/part-of":"kubeflow","app.kubernetes.io/version":"v0.7.0","kustomize.component":"iap-ingress"},"name":"ingress-jwt","namespace":"istio-system"},"spec":{"origins":[{"jwt":{"audiences":["TO_BE_PATCHED"],"issuer":"https://cloud.google.com/iap","jwksUri":"https://www.gstatic.com/iap/verify/public_key-jwk","jwtHeaders":["x-goog-iap-jwt-assertion"],"trigger_rules":[{"excluded_paths":[{"exact":"/healthz/ready"},{"prefix":"/.well-known/acme-challenge"}]}]}}],"principalBinding":"USE_ORIGIN","targets":[{"name":"istio-ingressgateway","ports":[{"number":80}]}]}}
creationTimestamp: "2019-10-15T12:14:18Z"
generation: 2
labels:
app.kubernetes.io/component: iap-ingress
app.kubernetes.io/instance: iap-ingress-v0.7.0
app.kubernetes.io/managed-by: kfctl
app.kubernetes.io/name: iap-ingress
app.kubernetes.io/part-of: kubeflow
app.kubernetes.io/version: v0.7.0
kustomize.component: iap-ingress
name: ingress-jwt
namespace: istio-system
resourceVersion: "5925"
selfLink: /apis/authentication.istio.io/v1alpha1/namespaces/istio-system/policies/ingress-jwt
uid: 4fc12317-ef45-11e9-a1ec-42010a8400b2
spec:
origins:

jwt:
audiences:

/projects/658583755711/global/backendServices/4483781589608171228
issuer: https://cloud.google.com/iap
jwksUri: https://www.gstatic.com/iap/verify/public_key-jwk
jwtHeaders:
x-goog-iap-jwt-assertion
trigger_rules:
excluded_paths:

exact: /healthz/ready
prefix: /.well-known/acme-challenge
principalBinding: USE_ORIGIN
targets:




name: istio-ingressgateway
ports:

number: 80




Describing the baclend service:

holden@miniholden:~/repos/sparklingml$ gcloud  compute backend-services describe --global 4483781589608171228
affinityCookieTtlSec: 0
backends:

balancingMode: RATE
capacityScaler: 1.0
group: https://www.googleapis.com/compute/v1/projects/boos-demo-projects-are-rad/zones/europe-west1-b/instanceGroups/k8s-ig--d06d8cfe13f2d356
maxRatePerInstance: 1.0
connectionDraining:
drainingTimeoutSec: 0
creationTimestamp: '2019-10-15T05:14:43.775-07:00'
description: '{"kubernetes.io/service-name":"istio-system/istio-ingressgateway","kubernetes.io/service-port":"80"}'
enableCDN: false
fingerprint: Ae1G-avBKI4=
healthChecks:
https://www.googleapis.com/compute/v1/projects/boos-demo-projects-are-rad/global/healthChecks/k8s-be-31380--d06d8cfe13f2d356
iap:
enabled: true
oauth2ClientId: CLIENTID
oauth2ClientSecretSha256: SECRETSHA256
id: '4483781589608171228'
kind: compute#backendService
loadBalancingScheme: EXTERNAL
name: k8s-be-31380--d06d8cfe13f2d356
port: 31380
portName: port31380
protocol: HTTP
selfLink: https://www.googleapis.com/compute/v1/projects/boos-demo-projects-are-rad/global/backendServices/k8s-be-31380--d06d8cfe13f2d356
sessionAffinity: NONE
timeoutSec: 30


I tried bumping the timeout as suggested by &lt;denchmark-link:https://github.com/jlewi&gt;@jlewi&lt;/denchmark-link&gt;
 but its still failing.
		</comment>
		<comment id='13' author='amygdala' date='2019-10-16T02:31:05Z'>
		&lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 The fix &lt;denchmark-link:https://github.com/kubeflow/manifests#513&gt;https://github.com/kubeflow/manifests#513&lt;/denchmark-link&gt;
 didn't go in until after you filed your bug report.
Also note the fix is in kubeflow/manifests so its not just about the kfctl version you are using but the version of kubeflow/manifests you pick up.
		</comment>
		<comment id='14' author='amygdala' date='2019-10-16T09:36:03Z'>
		So I just redployed. Looking in my manifests now I can see &lt;denchmark-link:https://github.com/kubeflow/manifests/pull/531/files&gt;https://github.com/kubeflow/manifests/pull/531/files&lt;/denchmark-link&gt;
 is included.
The error message is different but the IAP enabler still isn't working:

backend name is

sleep 2
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ echo
++ grep -o 'k8s-be-31380--[0-9a-z]+'
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
fetching backends info with envoy-ingress:
echo 'fetching backends info with envoy-ingress: '
++ ++ echo
grep -o 'k8s-be-31380--[0-9a-z]+'
BACKEND_NAME=
echo 'backend name is '
backend name is
sleep 2
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
fetching backends info with envoy-ingress:
echo 'fetching backends info with envoy-ingress: '
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
backend name is
BACKEND_NAME=
echo 'backend name is '
sleep 2
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
fetching backends info with envoy-ingress:
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
++ echo++ grep -o 'k8s-be-31380--[0-9a-z]+'

backend name is

BACKEND_NAME=
echo 'backend name is '
sleep 2
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ ++ grep echo
-o 'k8s-be-31380--[0-9a-z]+'
backend name is
BACKEND_NAME=
echo 'backend name is '
sleep 2
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
fetching backends info with envoy-ingress:
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
echo 'backend name is '
sleep 2
backend name is
[[ -z '' ]]
++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
echo 'fetching backends info with envoy-ingress: '
fetching backends info with envoy-ingress:
++ grep -o 'k8s-be-31380--[0-9a-z]+'
++ echo
BACKEND_NAME=
backend name is
echo 'backend name is '
sleep 2


		</comment>
		<comment id='15' author='amygdala' date='2019-10-16T12:52:46Z'>
		&lt;denchmark-link:https://github.com/holdenk&gt;@holdenk&lt;/denchmark-link&gt;
 Your problem looks different. Can you provide the output of
&lt;denchmark-code&gt;kubectl -n istio-system ingress envoy-ingress -o yaml
kubectl -n istio-system describe envoy-ingress
&lt;/denchmark-code&gt;

It looks to me like the BACKEND_NAME isn't getting set
&lt;denchmark-code&gt;++ kubectl --namespace=istio-system get ingress envoy-ingress -o 'jsonpath={.metadata.annotations.ingress.kubernetes.io/backends}'
BACKENDS=
&lt;/denchmark-code&gt;

So I suspect your ingress is failing to create the GCP backend for some other reason (insufficient quota)?
It would be good to confirm if that is the problem. If it is we should open up a separate issue to provide a more helpful error message in the logs.
		</comment>
		<comment id='16' author='amygdala' date='2019-11-07T22:20:57Z'>
		Fixed in 0.7.0
		</comment>
	</comments>
</bug>