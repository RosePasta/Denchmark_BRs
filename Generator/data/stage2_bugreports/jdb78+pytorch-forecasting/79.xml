<bug id='79' author='emigre459' open_date='2020-10-06T02:42:32Z' closed_time='2020-11-05T14:09:12Z'>
	<summary>Issues running TDS Stallion Example with W&amp;B logger</summary>
	<description>
I'm seeing issues trying to run the W&amp;B logger when replicating the Towards Data Science example with the Stallion dataset (to be fair, switching to TensorBoard made it fail too, although for a completely different-sounding reason oddly). When I try to train the model, I get a single graphical output (attached) and then it errors out. I'm using:
&lt;denchmark-code&gt;- pytorch=1.4.0
- pytorch-forecasting=0.4.1
- pytorch-lightning=0.9.0
- wandb=0.10.4
&lt;/denchmark-code&gt;

I know the requirements.txt indicates (py)torch &gt;= 1.6, but I can't get conda to find a good solution for that in my dependency tree, and this seems to be a logger issue anyhow. Here's the full traceback:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-16-562ea3edbba3&gt; in &lt;module&gt;
     25     tft,
     26     train_dataloader=train_dataloader,
---&gt; 27     val_dataloaders=val_dataloader
     28 )

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)
     46             if entering is not None:
     47                 self.state = entering
---&gt; 48             result = fn(self, *args, **kwargs)
     49 
     50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
   1082             self.accelerator_backend = CPUBackend(self)
   1083             self.accelerator_backend.setup(model)
-&gt; 1084             results = self.accelerator_backend.train(model)
   1085 
   1086         # on fit end callback

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/accelerators/cpu_backend.py in train(self, model)
     37 
     38     def train(self, model):
---&gt; 39         results = self.trainer.run_pretrain_routine(model)
     40         return results

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
   1222 
   1223         # run a few val batches before training starts
-&gt; 1224         self._run_sanity_check(ref_model, model)
   1225 
   1226         # clear cache before training

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model, model)
   1255             num_loaders = len(self.val_dataloaders)
   1256             max_batches = [self.num_sanity_val_steps] * num_loaders
-&gt; 1257             eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)
   1258 
   1259             # allow no returns from eval

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py in _evaluate(self, model, dataloaders, max_batches, test_mode)
    331                         output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
    332                 else:
--&gt; 333                     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
    334 
    335                 is_result_obj = isinstance(output, Result)

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py in evaluation_forward(self, model, batch, batch_idx, dataloader_idx, test_mode)
    685             output = model.test_step(*args)
    686         else:
--&gt; 687             output = model.validation_step(*args)
    688 
    689         return output

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in validation_step(self, batch, batch_idx)
    138     def validation_step(self, batch, batch_idx):
    139         x, y = batch
--&gt; 140         log, _ = self.step(x, y, batch_idx, label="val")
    141         return log
    142 

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py in step(self, x, y, batch_idx, label)
    595         """
    596         # extract data and run model
--&gt; 597         log, out = super().step(x, y, batch_idx, label=label)
    598         # calculate interpretations etc for latter logging
    599         if self.log_interval(label == "train") &gt; 0:

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in step(self, x, y, batch_idx, label)
    223             log["loss"] = loss
    224         if self.log_interval(label == "train") &gt; 0:
--&gt; 225             self._log_prediction(x, out, batch_idx, label=label)
    226         return log, out
    227 

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in _log_prediction(self, x, out, batch_idx, label)
    281                 else:
    282                     tag += f" of item {idx} in batch {batch_idx}"
--&gt; 283                 self.logger.experiment.add_figure(
    284                     tag,
    285                     fig,

AttributeError: 'Run' object has no attribute 'add_figure'
&lt;/denchmark-code&gt;

It seems as though pytorch_forecasting is assuming that all loggers have the same add_figure() method, but clearly that's not the case in this version of W&amp;B/pytorch-lightning. Any thoughts on way to rectify this? I'd also be game for a workaround to disable the default figure generation during training, although it is very nice to get those super-informative figures so I'd rather get them working if I could!
&lt;denchmark-link:https://user-images.githubusercontent.com/11720378/95152213-884b0100-075a-11eb-971a-d9d617efb540.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='emigre459' date='2020-10-06T07:37:38Z'>
		If you want to switch logging off, just pass log_interval=-1. If you are aware of a unified logging interface, I would be keen to use it. Alternatively, one could

write such a logging interface, e.g. it could be passed as part of the __init__ arguments to the models.
override the methods that implement logging. Most of them are already separate such as _log_prediction()

From a user perspective option 1 would be preferable. Any thoughts?
For tensorboard, keep in mind that you should deinstall  or use the workaround in &lt;denchmark-link:https://github.com/jdb78/pytorch-forecasting/issues/58&gt;#58&lt;/denchmark-link&gt;
. To install pytorch &gt;=1.6 with conda, you have to use the pytorch channel, i.e. .
		</comment>
		<comment id='2' author='emigre459' date='2020-10-06T18:58:03Z'>
		OK, cool. Once I can get my example up and running, I might take a whack at the unified logging interface, as I'm not aware of such a thing right now. When I disable logging as you've said by setting log_interval=-1, I get the same error I saw earlier when trying to log via Tensorboard instead of W&amp;B (I checked and my environment does not have TensorFlow installed, but does have tensorboard=2.2.0). Here's the new traceback:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-31-562ea3edbba3&gt; in &lt;module&gt;
     25     tft,
     26     train_dataloader=train_dataloader,
---&gt; 27     val_dataloaders=val_dataloader
     28 )

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)
     46             if entering is not None:
     47                 self.state = entering
---&gt; 48             result = fn(self, *args, **kwargs)
     49 
     50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
   1082             self.accelerator_backend = CPUBackend(self)
   1083             self.accelerator_backend.setup(model)
-&gt; 1084             results = self.accelerator_backend.train(model)
   1085 
   1086         # on fit end callback

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/accelerators/cpu_backend.py in train(self, model)
     37 
     38     def train(self, model):
---&gt; 39         results = self.trainer.run_pretrain_routine(model)
     40         return results

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
   1237 
   1238         # CORE TRAINING LOOP
-&gt; 1239         self.train()
   1240 
   1241     def _run_sanity_check(self, ref_model, model):

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in train(self)
    392                 # RUN TNG EPOCH
    393                 # -----------------
--&gt; 394                 self.run_training_epoch()
    395 
    396                 if self.max_steps and self.max_steps &lt;= self.global_step:

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)
    477         # run epoch
    478         for batch_idx, (batch, is_last_batch) in self.profiler.profile_iterable(
--&gt; 479                 enumerate(_with_is_last(train_dataloader)), "get_train_batch"
    480         ):
    481             # stop epoch if we limited the number of training batches

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/profiler/profilers.py in profile_iterable(self, iterable, action_name)
     76             try:
     77                 self.start(action_name)
---&gt; 78                 value = next(iterator)
     79                 self.stop(action_name)
     80                 yield value

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py in _with_is_last(iterable)
   1320     See `https://stackoverflow.com/a/1630350 &lt;https://stackoverflow.com/a/1630350&amp;gt;`_"""
   1321     it = iter(iterable)
-&gt; 1322     last = next(it)
   1323     for val in it:
   1324         # yield last and has next

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)
    343 
    344     def __next__(self):
--&gt; 345         data = self._next_data()
    346         self._num_yielded += 1
    347         if self._dataset_kind == _DatasetKind.Iterable and \

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)
    383     def _next_data(self):
    384         index = self._next_index()  # may raise StopIteration
--&gt; 385         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    386         if self._pin_memory:
    387             data = _utils.pin_memory.pin_memory(data)

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in &lt;listcomp&gt;(.0)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---&gt; 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

/opt/conda/envs/DIU_NORAD/lib/python3.7/site-packages/pytorch_forecasting/data/timeseries.py in __getitem__(self, idx)
    965                 x_cont=data_cont,
    966                 encoder_length=encoder_length,
--&gt; 967                 encoder_target=target[:encoder_length],
    968                 encoder_time_idx_start=time[0],
    969                 groups=groups,

TypeError: only integer tensors of a single element can be converted to an index
&lt;/denchmark-code&gt;

It seems to occur right after Lightning does the validation sanity checks, as part of building the first training batch. Not sure why W&amp;B approach seemed able to complete 10-20 batches (and display the first visual from these) before having issues, yet the no-logs/Tensorboard approaches seem to error out immediately, but I'm sure it's all related. Unfortunately, it's hard for me to tell from the code what all goes into setting encoder_length, although it's clearly a combination of the min and max encoder length parameters. Since those are just integers (0 and 24, per the tutorial), not sure what the issue may be.
Also, thanks for the tip on latest-and-greatest pytorch installs! I had forgotten that the pytorch channel wasn't in my current conda config. Sadly, updating to 1.6.0 didn't solve the problem. Maybe &lt;denchmark-link:https://discuss.pytorch.org/t/typeerror-only-integer-tensors-of-a-single-element-can-be-converted-to-an-index/45641/2&gt;this&lt;/denchmark-link&gt;
 explains it at least in part?
		</comment>
		<comment id='3' author='emigre459' date='2020-10-07T06:55:44Z'>
		It looks like encoder_length is a tensor with more than one element or maybe of type float? Can you run with a debugger and confirm that? In case any of the passed length parameters are not integer, this might happen. I will add some validation tests.
		</comment>
		<comment id='4' author='emigre459' date='2020-10-07T16:38:28Z'>
		Just submitted PR &lt;denchmark-link:https://github.com/jdb78/pytorch-forecasting/pull/82&gt;#82&lt;/denchmark-link&gt;
 to address this. Going to try and get W&amp;B logging to work next!
		</comment>
		<comment id='5' author='emigre459' date='2020-10-26T20:03:01Z'>
		&lt;denchmark-link:https://github.com/jdb78&gt;@jdb78&lt;/denchmark-link&gt;
 Just found that &lt;denchmark-link:https://github.com/jdb78/pytorch-forecasting/pull/95&gt;#95&lt;/denchmark-link&gt;
 (which seems to be in release 0.5.0 if I'm not mistaken) doesn't seem to fix it on my end, so I'm re-opening this issue. Will try and work on an updated PR soon.
		</comment>
		<comment id='6' author='emigre459' date='2020-10-27T21:59:54Z'>
		Are you on pytorch 1.6? Could you post the exact error?
		</comment>
		<comment id='7' author='emigre459' date='2020-11-05T14:09:12Z'>
		I think I was having an issue with my dependencies. Re-testing doesn't turn up the error, so should be good after all. My mistake!
		</comment>
	</comments>
</bug>