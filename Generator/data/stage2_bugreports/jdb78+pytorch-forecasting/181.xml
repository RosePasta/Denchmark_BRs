<bug id='181' author='diditforlulz273' open_date='2020-11-30T13:25:35Z' closed_time='2020-12-04T22:39:43Z'>
	<summary>Categorical encodings and missing values</summary>
	<description>
Following the closed issue &lt;denchmark-link:https://github.com/jdb78/pytorch-forecasting/issues/122&gt;#122&lt;/denchmark-link&gt;
, where I wrote a comment but was unable to re-open it:
&lt;denchmark-link:https://github.com/jdb78&gt;@jdb78&lt;/denchmark-link&gt;
 The problem has just transformed into another form. On data where crashes used to occur before the fix in 0.6.1, now this is seen:
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
Number of parameters in network: 628.7k
Epoch 0: 15%|█▌ | 15/99 [00:11&lt;01:04, 1.31it/s, loss=1.410, v_num=12, train_loss_step=1.29]Traceback (most recent call last):
File "/home/seva/PycharmProjects/ECOM_demand/classes/model_tf_transformer.py", line 216, in
model = train(params=None, train_set=ts, valid_sets=vs, verbose_eval=20)
File "/home/seva/PycharmProjects/ECOM_demand/classes/model_tf_transformer.py", line 175, in train
trainer = mdl.fit(train_dataloader=train_dataloader, test_dataloader=val_dataloader)
File "/home/seva/PycharmProjects/ECOM_demand/classes/model_tf_transformer.py", line 151, in fit
trainer.fit(self._mdl, train_dataloader=train_dataloader, val_dataloaders=test_dataloader)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 445, in fit
results = self.accelerator_backend.train()
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_accelerator.py", line 59, in train
results = self.train_or_test()
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 66, in train_or_test
results = self.trainer.train()
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 494, in train
self.train_loop.run_training_epoch()
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 561, in run_training_epoch
batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 728, in run_training_batch
self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 469, in optimizer_step
self.trainer.accelerator_backend.optimizer_step(
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 114, in optimizer_step
model_ref.optimizer_step(
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1380, in optimizer_step
optimizer.step(closure=optimizer_closure, *args, **kwargs)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_forecasting/optim.py", line 131, in step
_ = closure()
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 718, in train_step_and_backward_closure
result = self.training_step_and_backward(
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 823, in training_step_and_backward
self.backward(result, optimizer, opt_idx)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 843, in backward
result.closure_loss = self.trainer.accelerator_backend.backward(
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 95, in backward
model.backward(closure_loss, optimizer, opt_idx, *args, **kwargs)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1258, in backward
loss.backward(*args, **kwargs)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/torch/tensor.py", line 185, in backward
torch.autograd.backward(self, gradient, retain_graph, create_graph)
File "/home/seva/PycharmProjects/ECOM_demand/venv/lib/python3.8/site-packages/torch/autograd/init.py", line 125, in backward
Variable._execution_engine.run_backward(
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
I can not definitely trace it, because it happens on a random step in an epoch(see I have 99 steps here), but always somewhere on the very first epoch. Datasets that worked smoothly before, still run well now, and those which crashed proceeded to crash, but in a different way.
If you need a reproducible example or some traces - just poke me here :)
	</description>
	<comments>
		<comment id='1' author='diditforlulz273' date='2020-11-30T21:44:38Z'>
		Do you have a collab notebook with a reproducible example? I think you should be able make things deterministic by setting a seed.
		</comment>
		<comment id='2' author='diditforlulz273' date='2020-12-01T12:41:28Z'>
		&lt;denchmark-link:https://github.com/jdb78&gt;@jdb78&lt;/denchmark-link&gt;
 no, but I'll create it and share it here in a couple of days!
		</comment>
		<comment id='3' author='diditforlulz273' date='2020-12-04T21:06:29Z'>
		&lt;denchmark-link:https://github.com/jdb78&gt;@jdb78&lt;/denchmark-link&gt;
 Finally got time to make an example, upgraded pytorch-forecasting to 0.7.0, and the problem is gone. Dunno what has been changed, but it yielded the effect :)
		</comment>
		<comment id='4' author='diditforlulz273' date='2020-12-04T22:39:29Z'>
		Good news. Let me know if the bug resurfaces so we can get a regression test in place.
		</comment>
	</comments>
</bug>