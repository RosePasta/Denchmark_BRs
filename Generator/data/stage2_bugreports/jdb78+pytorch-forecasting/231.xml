<bug id='231' author='reumar' open_date='2020-12-31T17:37:33Z' closed_time='2021-01-02T11:40:13Z'>
	<summary>DeepAR with BetaDistributionLoss returns RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</summary>
	<description>

PyTorch-Forecasting version: 0.7.1
PyTorch version: 1.7.1
Python version: 3.8.5
Operating System: Ubuntu 18.04.5 LTS

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I wanted to test DeepAR with Beta Distribution Loss, so I used the example from the TFT tutorial with a few tweaks.
I expected the model to start training, but on the first batch it crashes.
&lt;denchmark-h:h3&gt;Actual behavior&lt;/denchmark-h&gt;

The printed error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-12-6644968460fc&gt; in &lt;module&gt;
     28 )
     29 
---&gt; 30 trainer.fit(
     31     deepar,
     32     train_dataloader=train_dataloader,

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
    468         self.call_hook('on_fit_start')
    469 
--&gt; 470         results = self.accelerator_backend.train()
    471         self.accelerator_backend.teardown()
    472 

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/accelerators/cpu_accelerator.py in train(self)
     60 
     61         # train or test
---&gt; 62         results = self.train_or_test()
     63         return results
     64 

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py in train_or_test(self)
     67             results = self.trainer.run_test()
     68         else:
---&gt; 69             results = self.trainer.train()
     70         return results
     71 

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py in train(self)
    519                 with self.profiler.profile("run_training_epoch"):
    520                     # run train epoch
--&gt; 521                     self.train_loop.run_training_epoch()
    522 
    523                 if self.max_steps and self.max_steps &lt;= self.global_step:

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)
    558             # ------------------------------------
    559             with self.trainer.profiler.profile("run_training_batch"):
--&gt; 560                 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
    561 
    562             # when returning -1 from train_step, we end epoch early

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)
    716 
    717                         # optimizer step
--&gt; 718                         self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
    719 
    720                     else:

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)
    491 
    492         # model hook
--&gt; 493         model_ref.optimizer_step(
    494             self.trainer.current_epoch,
    495             batch_idx,

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py in optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)
   1255             # wraps into LightingOptimizer only for running step
   1256             optimizer = LightningOptimizer.to_lightning_optimizer(optimizer, self.trainer)
-&gt; 1257         optimizer.step(closure=optimizer_closure)
   1258 
   1259     def optimizer_zero_grad(

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py in step(self, closure, make_optimizer_step, *args, **kwargs)
    276 
    277         if make_optimizer_step:
--&gt; 278             self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)
    279         else:
    280             # make sure to call optimizer_closure when accumulating

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py in __optimizer_step(self, closure, profiler_name, *args, **kwargs)
    134         else:
    135             with trainer.profiler.profile(profiler_name):
--&gt; 136                 optimizer.step(closure=closure, *args, **kwargs)
    137 
    138         accelerator_backend = trainer.accelerator_backend

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_forecasting/optim.py in step(self, closure)
    129             closure: A closure that reevaluates the model and returns the loss.
    130         """
--&gt; 131         _ = closure()
    132         loss = None
    133         # note - below is commented out b/c I have other work that passes back

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in train_step_and_backward_closure()
    706 
    707                         def train_step_and_backward_closure():
--&gt; 708                             result = self.training_step_and_backward(
    709                                 split_batch,
    710                                 batch_idx,

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)
    814                 # backward pass
    815                 with self.trainer.profiler.profile("model_backward"):
--&gt; 816                     self.backward(result, optimizer, opt_idx)
    817 
    818                 # hook - call this hook only

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py in backward(self, result, optimizer, opt_idx, *args, **kwargs)
    834             self.trainer.accelerator_backend.backward(result, optimizer, opt_idx, *args, **kwargs)
    835         else:
--&gt; 836             result.closure_loss = self.trainer.accelerator_backend.backward(
    837                 result.closure_loss, optimizer, opt_idx, *args, **kwargs
    838             )

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py in backward(self, closure_loss, optimizer, opt_idx, *args, **kwargs)
    102             # do backward pass
    103             model = self.trainer.get_model()
--&gt; 104             model.backward(closure_loss, optimizer, opt_idx, *args, **kwargs)
    105 
    106             # once backward has been applied, release graph

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py in backward(self, loss, optimizer, optimizer_idx, *args, **kwargs)
   1149         """
   1150         if self.trainer.train_loop.automatic_optimization or self._running_manual_backward:
-&gt; 1151             loss.backward(*args, **kwargs)
   1152 
   1153     def toggle_optimizer(self, optimizer: Optimizer, optimizer_idx: int):

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    219                 retain_graph=retain_graph,
    220                 create_graph=create_graph)
--&gt; 221         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    222 
    223     def register_hook(self, hook):

~/miniconda3/envs/smartsilex-eda/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
    128         retain_graph = create_graph
    129 
--&gt; 130     Variable._execution_engine.run_backward(
    131         tensors, grad_tensors_, retain_graph, create_graph,
    132         allow_unreachable=True)  # allow_unreachable flag

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Code to reproduce the problem&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import os
import warnings

warnings.filterwarnings("ignore")  # avoid printing out absolute paths

import copy
from pathlib import Path
import warnings

import numpy as np
import pandas as pd
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
import torch

from pytorch_forecasting import Baseline, DeepAR, TimeSeriesDataSet
from pytorch_forecasting.data import GroupNormalizer, TorchNormalizer
from pytorch_forecasting.metrics import NormalDistributionLoss, NegativeBinomialDistributionLoss, BetaDistributionLoss

from pytorch_forecasting.data.examples import get_stallion_data

data = get_stallion_data()

# add time index
data["time_idx"] = data["date"].dt.year * 12 + data["date"].dt.month
data["time_idx"] -= data["time_idx"].min()

# add additional features
data["month"] = data.date.dt.month.astype(str).astype("category")  # categories have be strings
data["log_volume"] = np.log(data.volume + 1e-8)
data["avg_volume_by_sku"] = data.groupby(["time_idx", "sku"], observed=True).volume.transform("mean")
data["avg_volume_by_agency"] = data.groupby(["time_idx", "agency"], observed=True).volume.transform("mean")

# we want to encode special days as one variable and thus need to first reverse one-hot encoding
special_days = [
    "easter_day",
    "good_friday",
    "new_year",
    "christmas",
    "labor_day",
    "independence_day",
    "revolution_day_memorial",
    "regional_games",
    "fifa_u_17_world_cup",
    "football_gold_cup",
    "beer_capital",
    "music_fest",
]
data[special_days] = data[special_days].apply(lambda x: x.map({0: "-", 1: x.name})).astype("category")

# Normalize volume so its between 0 and 1
data["volume"] = data.volume / (data.volume.max())

max_prediction_length = 6
max_encoder_length = 24
training_cutoff = data["time_idx"].max() - max_prediction_length

training = TimeSeriesDataSet(
    data[lambda x: x.time_idx &lt;= training_cutoff],
    time_idx="time_idx",
    target="volume",
    group_ids=["agency", "sku"],
    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)
    max_encoder_length=max_encoder_length,
    min_prediction_length=1,
    max_prediction_length=max_prediction_length,
    static_categoricals=["agency", "sku"],
    static_reals=["avg_population_2017", "avg_yearly_household_income_2017"],
    time_varying_known_categoricals=["special_days", "month"],
    variable_groups={"special_days": special_days},  # group of categorical variables can be treated as one variable
    time_varying_known_reals=["time_idx", "price_regular", "discount_in_percent"],
    time_varying_unknown_categoricals=[],
    time_varying_unknown_reals=[
        "volume",
    ],
    target_normalizer=TorchNormalizer(method="standard", transformation="logit", center=True),  # use softplus and normalize by group
    add_relative_time_idx=True,
    add_target_scales=True,
    add_encoder_length=True,
)

# create validation set (predict=True) which means to predict the last max_prediction_length points in time
# for each series
validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)

# create dataloaders for model
batch_size = 128  # set this between 32 to 128
train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)
val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)

# configure network and trainer
pl.seed_everything(42)

early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=1e-4, patience=10, verbose=False, mode="min")
lr_logger = LearningRateMonitor()  # log the learning rate
logger = TensorBoardLogger("lightning_logs")  # logging results to a tensorboard

trainer = pl.Trainer(
    max_epochs=30,
    gpus=0,
    weights_summary="top",
    gradient_clip_val=0.1,
    limit_train_batches=30,  # coment in for training, running valiation every 30 batches
    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs
    callbacks=[lr_logger, early_stop_callback],
    logger=logger,
)

deepar = DeepAR.from_dataset(
    training,
    cell_type = "LSTM",  # "LSTM" or "GRU"
    hidden_size = 40,
    rnn_layers = 2,
    dropout = 0.1, # Dropout between layers
    loss = BetaDistributionLoss(), # Defaults to NormalDistributionLoss. Can also be NegativeBinomialDistributionLoss, LogNormalDistributionLoss, BetaDistributionLoss
    logging_metrics = None # Defaults to nn.ModuleList([SMAPE(), MAE(), RMSE(), MAPE(), MASE()])
)

trainer.fit(
    deepar,
    train_dataloader=train_dataloader,
    val_dataloaders=val_dataloader,
)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='reumar' date='2021-01-01T06:53:09Z'>
		Thanks! Will look into it and set up some regression tests.
		</comment>
	</comments>
</bug>