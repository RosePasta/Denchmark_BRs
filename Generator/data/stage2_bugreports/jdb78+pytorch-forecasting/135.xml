<bug id='135' author='yonghah' open_date='2020-11-01T00:55:49Z' closed_time='2020-11-07T21:58:49Z'>
	<summary>Error passing CUDA tensor to nn.utils.rnn.pack_padded_sequence</summary>
	<description>

PyTorch-Forecasting version: v0.5.3
PyTorch version: 1.7.0
Python version: 3.7.9
Operating System: Ubuntu 20.04.1 LTS

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I executed codes to find optimal learning rate or to fit network and and expected to get result as written in pytorch-forecasting.readthedocs.io. The only difference was gpus=1 in pl.Trainer parameter.
&lt;denchmark-code&gt;# configure network and trainer
pl.seed_everything(42)
trainer = pl.Trainer(
    gpus=1,
    # clipping gradients is a hyperparameter and important to prevent divergance
    # of the gradient for recurrent neural networks
    gradient_clip_val=0.1,
)


tft = TemporalFusionTransformer.from_dataset(
    training,
    # not meaningful for finding the learning rate but otherwise very important
    learning_rate=0.03,
    hidden_size=16,  # most important hyperparameter apart from learning rate
    # number of attention heads. Set to up to 4 for large datasets
    attention_head_size=1,
    dropout=0.1,  # between 0.1 and 0.3 are good values
    hidden_continuous_size=8,  # set to &lt;= hidden_size
    output_size=7,  # 7 quantiles by default
    loss=QuantileLoss(),
    # reduce learning rate if no improvement in validation loss after x epochs
    reduce_on_plateau_patience=4,
)
print(f"Number of parameters in network: {tft.size()/1e3:.1f}k")

# find optimal learning rate
res = trainer.tuner.lr_find(
    tft,
    train_dataloader=train_dataloader,
    val_dataloaders=val_dataloader,
    max_lr=10.0,
    min_lr=1e-6,
)

print(f"suggested learning rate: {res.suggestion()}")
fig = res.plot(show=True, suggest=True)
fig.show()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Actual behavior&lt;/denchmark-h&gt;

However, it gives RuntimeError  like below:
&lt;denchmark-code&gt;RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-11-a92b5627800b&gt; in &lt;module&gt;
      5     val_dataloaders=val_dataloader,
      6     max_lr=10.0,
----&gt; 7     min_lr=1e-6,
      8 )
      9 

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/tuner/tuning.py in lr_find(self, model, train_dataloader, val_dataloaders, min_lr, max_lr, num_training, mode, early_stop_threshold, datamodule)
    128             mode,
    129             early_stop_threshold,
--&gt; 130             datamodule,
    131         )
    132 

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/tuner/lr_finder.py in lr_find(trainer, model, train_dataloader, val_dataloaders, min_lr, max_lr, num_training, mode, early_stop_threshold, datamodule)
    173                 train_dataloader=train_dataloader,
    174                 val_dataloaders=val_dataloaders,
--&gt; 175                 datamodule=datamodule)
    176 
    177     # Prompt if we stopped early

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
    437         self.call_hook('on_fit_start')
    438 
--&gt; 439         results = self.accelerator_backend.train()
    440         self.accelerator_backend.teardown()
    441 

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py in train(self)
     52 
     53         # train or test
---&gt; 54         results = self.train_or_test()
     55         return results
     56 

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in train_or_test(self)
     64             results = self.trainer.run_test()
     65         else:
---&gt; 66             results = self.trainer.train()
     67         return results
     68 

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in train(self)
    459 
    460     def train(self):
--&gt; 461         self.run_sanity_check(self.get_model())
    462 
    463         # enable train mode

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_sanity_check(self, ref_model)
    645 
    646             # run eval step
--&gt; 647             _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)
    648 
    649             # allow no returns from eval

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, test_mode, max_batches)
    565 
    566                 # lightning module methods
--&gt; 567                 output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)
    568                 output = self.evaluation_loop.evaluation_step_end(output)
    569 

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py in evaluation_step(self, test_mode, batch, batch_idx, dataloader_idx)
    169             output = self.trainer.accelerator_backend.test_step(args)
    170         else:
--&gt; 171             output = self.trainer.accelerator_backend.validation_step(args)
    172 
    173         # track batch size for weighted average

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py in validation_step(self, args)
     76                 output = self.__validation_step(args)
     77         else:
---&gt; 78             output = self.__validation_step(args)
     79 
     80         return output

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py in __validation_step(self, args)
     84         batch = self.to_device(batch)
     85         args[0] = batch
---&gt; 86         output = self.trainer.model.validation_step(*args)
     87         return output
     88 

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in validation_step(self, batch, batch_idx)
    138     def validation_step(self, batch, batch_idx):
    139         x, y = batch
--&gt; 140         log, _ = self.step(x, y, batch_idx, label="val")  # log loss
    141         self.log("val_loss", log["loss"], on_step=False, on_epoch=True, prog_bar=True)
    142         return log

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py in step(self, x, y, batch_idx, label)
    566         """
    567         # extract data and run model
--&gt; 568         log, out = super().step(x, y, batch_idx, label=label)
    569         # calculate interpretations etc for latter logging
    570         if self.log_interval(label == "train") &gt; 0:

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_forecasting/models/base_model.py in step(self, x, y, batch_idx, label)
    194             loss = loss * (1 + monotinicity_loss)
    195         else:
--&gt; 196             out = self(x)
    197             out["prediction"] = self.transform_output(out)
    198 

~/repo/emart-promo/env/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/repo/emart-promo/env/lib/python3.7/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py in forward(self, x)
    489         encoder_output, (hidden, cell) = self.lstm_encoder(
    490             rnn.pack_padded_sequence(
--&gt; 491                 embeddings_varying_encoder, lstm_encoder_lengths, enforce_sorted=False, batch_first=True
    492             ),
    493             (input_hidden, input_cell),

~/repo/emart-promo/env/lib/python3.7/site-packages/torch/nn/utils/rnn.py in pack_padded_sequence(input, lengths, batch_first, enforce_sorted)
    242 
    243     data, batch_sizes = \
--&gt; 244         _VF._pack_padded_sequence(input, lengths, batch_first)
    245     return _packed_sequence_init(data, batch_sizes, sorted_indices, None)
    246 

RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor
&lt;/denchmark-code&gt;

Seems related to these issues:

https://discuss.pytorch.org/t/error-with-lengths-in-pack-padded-sequence/35517/9
pytorch/pytorch#16542 (comment)
pytorch/pytorch#43227

	</description>
	<comments>
		<comment id='1' author='yonghah' date='2020-11-01T09:09:05Z'>
		Good point. Thanks for reporting this! The behaviour might have changed with PyTorch 1.7 again. Currently there are no tests on GPU (and it used to work a couple of weeks ago) but I am planning some soonish.
		</comment>
		<comment id='2' author='yonghah' date='2020-11-14T08:36:58Z'>
		Wonderful job!
I have take the lastest version master, why it happen still? How can i solve this, thanks:
pytorch:  1.7.0+cu101
pl :1.0.6
pytorch_forecasting :0.6.0
		</comment>
		<comment id='3' author='yonghah' date='2020-11-16T08:16:10Z'>
		just ï¼šy = rnn.pack_padded_sequence(y, lengths=x["decoder_lengths"].cpu(), batch_first=True, enforce_sorted=False)
		</comment>
		<comment id='4' author='yonghah' date='2020-11-18T07:48:16Z'>
		&lt;denchmark-link:https://github.com/BeHappyForMe&gt;@BeHappyForMe&lt;/denchmark-link&gt;
 I had the same issue and the .cpu() fix did it for me.
		</comment>
		<comment id='5' author='yonghah' date='2020-11-20T17:18:25Z'>
		This error is still persistent.
		</comment>
		<comment id='6' author='yonghah' date='2020-11-21T10:33:28Z'>
		Missed this one. Fixed in &lt;denchmark-link:https://github.com/jdb78/pytorch-forecasting/pull/169&gt;#169&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>