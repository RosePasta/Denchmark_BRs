<bug id='928' author='tkakantousis' open_date='2019-11-12T15:26:04Z' closed_time='2019-11-22T13:32:33Z'>
	<summary>BeamDagRunner, Transform deadlock with Python 3.6</summary>
	<description>
Hi everyone,
Running the taxi_pipeline_beam example fails with
&lt;denchmark-code&gt;  from tensorflow_transform.output_wrapper import TFTransformOutput
  File "/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/tensorflow_transform/output_wrapper.py", line 29, in &lt;module&gt;
    from tensorflow_transform.tf_metadata import metadata_io
  File "&lt;frozen importlib._bootstrap&gt;", line 968, in _find_and_load
  File "&lt;frozen importlib._bootstrap&gt;", line 149, in __enter__
  File "&lt;frozen importlib._bootstrap&gt;", line 94, in acquire
_frozen_importlib._DeadlockError: deadlock detected by _ModuleLock('tensorflow_transform.tf_metadata.metadata_io') at 139642620901192
&lt;/denchmark-code&gt;

while running Transform &lt;denchmark-link:https://github.com/tensorflow/tfx/blob/0.14.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_beam.py#L85&gt;https://github.com/tensorflow/tfx/blob/0.14.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_beam.py#L85&lt;/denchmark-link&gt;

TFX version is 0.14, Beam is 2.15 and running on Flink 1.8.1.
All works well with Python 2.7. Has anyone encountered this issue before?
Thank you,
Theo
	</description>
	<comments>
		<comment id='1' author='tkakantousis' date='2019-11-13T02:49:22Z'>
		Hi, &lt;denchmark-link:https://github.com/tkakantousis&gt;@tkakantousis&lt;/denchmark-link&gt;
. Thanks a lot for reporting this. We have seen an issue &lt;denchmark-link:https://lists.apache.org/thread.html/5581ddfcf6d2ae10d25b834b8a61ebee265ffbcf650c6ec8d1e69408@%3Cdev.beam.apache.org%3E&gt;reported&lt;/denchmark-link&gt;
 on Beam mailing list, that may be similar in nature, but we have not yet rootcaused it.
Do you know if this error is reproducible in your setup on Python 3.7.3 or higher?
		</comment>
		<comment id='2' author='tkakantousis' date='2019-11-13T10:00:40Z'>
		Hi &lt;denchmark-link:https://github.com/tvalentyn&gt;@tvalentyn&lt;/denchmark-link&gt;

Thank you for looking into the issue. I tried twice with Python3.7.3, first time time it failed during TFMA, second time it failed during Transform.
&lt;denchmark-code&gt;File "/srv/hops/anaconda/anaconda/envs/python373/lib/python3.7/site-packages/tensorflow_model_analysis/evaluators/__init__.py", line 20, in &lt;module&gt;
    from tensorflow_model_analysis.evaluators.metrics_and_plots_evaluator import MetricsAndPlotsEvaluator
  File "/srv/hops/anaconda/anaconda/envs/python373/lib/python3.7/site-packages/tensorflow_model_analysis/evaluators/metrics_and_plots_evaluator.py", line 26, in &lt;module&gt;
    from tensorflow_model_analysis.evaluators import aggregate
  File "&lt;frozen importlib._bootstrap&gt;", line 980, in _find_and_load
  File "&lt;frozen importlib._bootstrap&gt;", line 149, in __enter__
  File "&lt;frozen importlib._bootstrap&gt;", line 94, in acquire
_frozen_importlib._DeadlockError: deadlock detected by _ModuleLock('tensorflow_model_analysis.evaluators.aggregate') at 140200765413696
 [while running 'Run[ModelValidator]']
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='tkakantousis' date='2019-11-14T01:47:48Z'>
		Thanks. I am trying to reproduce this locally, just wanted to double check - how do you launch this example?
I've tried so far python tfx/examples/chicago_taxi_pipeline/taxi_pipeline_beam.py which worked for me on Python 3.5.4, 3.7.5rc1, but I believe that does not involve Beam on Flink.
I am working on Beam primarily, and looks like there were a quite a few changes to Chicago taxi example since the last time I played with it.
		</comment>
		<comment id='4' author='tkakantousis' date='2019-11-14T09:23:05Z'>
		Hi,
I copied the example into an iPython notebook and I ran it from Jupyter. As you suggested it most probably is a Beam issue with Python, maybe I should open an issue on Beam or post on the mailing list? Also the output of the pipeline in my case is saved into HDFS, not the local filesystem. The strange thing is that I see Flink tasks completing successfully, the deadlock occurs between finishing the previous task and launching the next.
		</comment>
		<comment id='5' author='tkakantousis' date='2019-11-14T17:43:29Z'>
		Are you sure you are running  example and not ? If you have made changes to the example, and have these changes somewhere on your fork, please share them too. I can open the issue in Beam once I can reproduce it. Also, I've opened  &lt;denchmark-link:https://issues.apache.org/jira/browse/BEAM-8651&gt;https://issues.apache.org/jira/browse/BEAM-8651&lt;/denchmark-link&gt;
  for another issue I mentioned earlier.
		</comment>
		<comment id='6' author='tkakantousis' date='2019-11-14T17:49:16Z'>
		I'm not using the portable one and I haven't done any changes. Only change is that the input/output files are in HDFS, not local. Weird thing is it works with Python2.7.
		</comment>
		<comment id='7' author='tkakantousis' date='2019-11-14T17:52:27Z'>
		Do you observe the issue when input/output files are local? How about when running the example outside of Jupyter,  using python interpreter directly?
		</comment>
		<comment id='8' author='tkakantousis' date='2019-11-14T18:42:07Z'>
		It appears that that taxi_pipeline_beam uses beam orchestrator on Direct runner, so I am not sure how you end up using Flink if you are running  taxi_pipeline_beam...
		</comment>
		<comment id='9' author='tkakantousis' date='2019-11-14T20:09:15Z'>
		I haven't tried with local files and outside jupyter. I end up using Beam/Flink because I set the , similarly to here &lt;denchmark-link:https://github.com/tensorflow/tfx/blob/0.14.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py#L140&gt;https://github.com/tensorflow/tfx/blob/0.14.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py#L140&lt;/denchmark-link&gt;

From what I understand these two examples run the same pipeline, except for the Beam vs Airflow DagRunner and the Direct vs Portable runer. So instead of changing AirflowDagRunner to BeamDagRunner in , I added the beam_pipeline_args in . There's also a comment here &lt;denchmark-link:https://github.com/tensorflow/tfx/blob/0.14.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py#L167&gt;https://github.com/tensorflow/tfx/blob/0.14.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py#L167&lt;/denchmark-link&gt;
 suggesting it is/will be possible to run the PortableRunner with the BeamDAGRunner. My motivation is I do not want to use Airflow for this pipeline.
Am I missing something?
		</comment>
		<comment id='10' author='tkakantousis' date='2019-11-15T00:00:08Z'>
		Thanks, that explains your usecase. As I said, I am just trying to replicate the error on my machine, so it's helpful to know which modifications you did, as I am not intimately familiar with chicago_taxi pipeline. By the way, which tensorflow version are you using?
		</comment>
		<comment id='11' author='tkakantousis' date='2019-11-15T00:52:11Z'>
		Got the example running on  PortableRunner with the BeamDAGRunner, with Flink 1.8.2, TF 1.14.0, Beam 2.16.0,  but was not able to run it on 2.15.0, are you certain you use Beam 2.15.0?
Also, could you provide the beam_pipeline_args that you set?
Did not repro the error so far.
Looks like it's important to clean up ~/tfx directory before each run, to clean any cached computated results.Otherwise pipelines don't start.
		</comment>
		<comment id='12' author='tkakantousis' date='2019-11-15T03:01:26Z'>
		I can  reproduce this issue on TFX 0.14.0 when increasing parallelism level on Flink Cluster and in  Beam Portable runner as follows:
&lt;denchmark-code&gt;      additional_pipeline_args={                                                
          'beam_pipeline_args': [                                               
              # ----- Beam Args -----.                                          
              '--runner=PortableRunner',                                        
              '--job_endpoint=localhost:8099',                                  
              '--environment_type=LOOPBACK',                                       
              '--sdk_worker_parallelism=12',                                       
              '--experiments=use_loopback_process_worker=True',                 
              '--environment_cache_millis=1000000',                             
              '--parallelism=12',                                                  
              '--experiments=worker_threads=100',                               
              '--experiments=pre_optimize=all',                                    
              '--execution_mode_for_batch=BATCH_FORCED',                        
          ],                                                                    
      }, 
&lt;/denchmark-code&gt;

Looking further to understand the rootcause.
		</comment>
		<comment id='13' author='tkakantousis' date='2019-11-15T08:41:48Z'>
		Thank you for looking into it, it's great you managed to reproduce. I'm on TensorFlow 1.14.0.
My beam_pipeline_args are same as yours except I'm using environment_type=PROCESS and --parallelism=1
Does it work for you with Beam 2.16 and parallelism=12?
		</comment>
		<comment id='14' author='tkakantousis' date='2019-11-16T00:08:21Z'>
		No, it does not - it fails 100% of the time. There are at least 5 different failure modes that I've observed:
&lt;denchmark-code&gt;_frozen_importlib._DeadlockError: deadlock detected by _ModuleLock(...)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;ModuleNotFoundError: No module named 'tensorflow.python.util.tf_export.SymbolAlreadyExposedError'; 'tensorflow.python.util.tf_export' is not a package
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol BytesList is already exposed as ('train.BytesList',)
&lt;/denchmark-code&gt;

These happen during unpickling of a payload within Beam code.  On one occasion, the code had to retry 9 times before  call finished without an exception. I plan to further investigate this problem in &lt;denchmark-link:https://issues.apache.org/jira/browse/BEAM-8651&gt;https://issues.apache.org/jira/browse/BEAM-8651&lt;/denchmark-link&gt;
 .
I am also seeing following errors, possibly related:
An error during pickling as opposed to unpickling:
&lt;denchmark-code&gt;... 
 File "/usr/local/google/home/valentyn/tmp/tfx_py37_master/lib/python3.7/site-packages/dill/_dill.py", line 910, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File "/usr/lib/python3.7/pickle.py", line 859, in save_dict
    self._batch_setitems(obj.items())
  File "/usr/lib/python3.7/pickle.py", line 885, in _batch_setitems
    save(v)
  File "/usr/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/google/home/valentyn/tmp/tfx_py37_master/lib/python3.7/site-packages/dill/_dill.py", line 1394, in save_function
    obj.__dict__), obj=obj)
  File "/usr/lib/python3.7/pickle.py", line 638, in save_reduce
    save(args)
  File "/usr/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/lib/python3.7/pickle.py", line 789, in save_tuple
    save(element)
  File "/usr/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/usr/local/google/home/valentyn/tmp/tfx_py37_master/lib/python3.7/site-packages/apache_beam/internal/pickler.py", line 180, in new_save_module_dict
    for m in sys.modules.values():
RuntimeError: dictionary changed size during iteration
 [while running 'Run[Transform]']
&lt;/denchmark-code&gt;

The workaround for pickling errors can be to retry unpickling (&lt;denchmark-link:https://github.com/apache/beam/pull/10133&gt;apache/beam#10133&lt;/denchmark-link&gt;
).
I am also encontering:
&lt;denchmark-code&gt;packages/apache_beam/runners/worker/bundle_processor.py", line 1149, in _create_pardo_operation
    if not dofn_data[-1]:
TypeError: 'NoneType' object is not subscriptable
 [while running 'Run[Transform]']
&lt;/denchmark-code&gt;

which may be a different error, filed: &lt;denchmark-link:https://issues.apache.org/jira/browse/BEAM-8730&gt;https://issues.apache.org/jira/browse/BEAM-8730&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='tkakantousis' date='2019-11-20T21:06:40Z'>
		All errors observed here should be fixed with &lt;denchmark-link:https://github.com/apache/beam/pull/10167&gt;apache/beam#10167&lt;/denchmark-link&gt;
. Feel free to patch this PR. We plan to cherrypick this fix into Beam 2.17.0 that is in the works.
		</comment>
		<comment id='16' author='tkakantousis' date='2019-11-20T22:09:27Z'>
		Thanks again, &lt;denchmark-link:https://github.com/tkakantousis&gt;@tkakantousis&lt;/denchmark-link&gt;
, for opening this issue.
		</comment>
		<comment id='17' author='tkakantousis' date='2019-11-21T09:00:45Z'>
		Great, looking forward to the next release :)
Thank you so much &lt;denchmark-link:https://github.com/tvalentyn&gt;@tvalentyn&lt;/denchmark-link&gt;
 for looking into it.
		</comment>
		<comment id='18' author='tkakantousis' date='2019-11-22T13:32:35Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tfx/issues/928&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tfx/issues/928&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>