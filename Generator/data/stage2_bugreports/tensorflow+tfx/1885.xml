<bug id='1885' author='jason-brian-anderson' open_date='2020-05-31T17:04:51Z' closed_time='2020-06-06T04:23:23Z'>
	<summary>Keras Tutorial produces model that does not integrate with the TF Serving Rest API</summary>
	<description>
Apologies if I am missing something obvious, but i've been struggling with this for a about 5 days and it looks like critical documentation is missing.   I'm building a text classifier based off of the &lt;denchmark-link:https://www.tensorflow.org/tfx/tutorials/tfx/components_keras&gt;Keras TFX tutorial&lt;/denchmark-link&gt;
.  As far as i can tell,  the  signaturedef created by the serve_tf_examples_fn. literally only parses tf.examples.:
&lt;denchmark-code&gt;@tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec). #Causes graph to require tf.example input.  what about raw string input?

    transformed_features = model.tft_layer(parsed_features)
    transformed_features.pop(_transformed_name(_LABEL_KEY))

    return model(transformed_features)

&lt;/denchmark-code&gt;

This makes perfect sense in terms of TFMA input for the Evaluator component, which works beautifully,  but makes no sense in terms of running the model in Serving, as it requires the REST client to import the necessary Tensorflow libraries in order to format and serialize a tf.example for every request as is expected by the line above:
   parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
Surely this is not intended as acceptable overhead expected by clients?
I've successfully built an entire TFX pipeline to classify text as a tf.string and am ready to deploy to production but for this one roadblock. I'd be happy to submit a pull request with a documentation update if any experts could point me in the right direction toward documenting  the appropriate way to parse in raw text vs a serialized TF.example.
Thank you very much.
	</description>
	<comments>
		<comment id='1' author='jason-brian-anderson' date='2020-06-01T21:51:32Z'>
		Maybe a better way of phrasing this question is:  what does a signaturedef look like that would allow a tf transform coupled model to be placed in tf serving so that you can do a simple REST request and not have to create a full serialized tf.example on the client prior to submission?
This source code suggests there is such a thing, but it(transform_raw_features_layer) is nowhere to be found via google search engine!:
&lt;denchmark-code&gt;  For internal use only.  Users should use the `transform_raw_features` or
  `transform_raw_features_layer` method of the TFTrandformOutput class.

&lt;/denchmark-code&gt;

I have a fully functional ML pipeline from 3 weeks with the Keras Tutorial.  Once TFX documents this it is going to be completely awesome!
		</comment>
		<comment id='2' author='jason-brian-anderson' date='2020-06-04T21:46:36Z'>
		Hi &lt;denchmark-link:https://github.com/jason-brian-anderson&gt;@jason-brian-anderson&lt;/denchmark-link&gt;
,
I came across your question. Someone asked a similar question regarding the TFX BERT Pipeline example and therefore, I created an example export tf function to avoid the tf.Example dependency. Maybe the colab example below can be helpful.
Previously, in the Estimator world, we had methods like build_raw_serving_input_receiver_fn to avoid the tf.Example dependency. As far as I understand, we can now define the inputs through the TensorSpecs in the concrete_fuunction() when we export the model. TF is looking for the serve_tf_examples_fn() function but we can remove the tf.Example parsing and add our input handling as shown in the Colab notebook below:
&lt;denchmark-code&gt;    @tf.function
    def serve_tf_examples_fn(text):
        """Returns the output to be used in the serving signature.
        TF is currently looking for a function serve_tf_examples_fn(),
        therefore the function name can't be changed, even though we aren't 
        parsing tf.Example records.
        """
        reshaped_text = tf.reshape(text, [-1, 1])
        transformed_features = model.tft_layer({"text": reshaped_text})
        outputs = model(transformed_features)
        return {'outputs': outputs}
&lt;/denchmark-code&gt;

model.tft_layer expects a dict of the inputs.  Because of our input signature of text has the shape [None], we need to reshape the Tensor to match the Keras input requirements.
Colab version of the BERT Pipeline which exports a model for simple REST requests:
&lt;denchmark-link:https://colab.research.google.com/gist/hanneshapke/f0980b7422d367808dae409536fe9b46/tfx_pipeline_for_bert_preprocessing_wo_tf-example.ipynb&gt;https://colab.research.google.com/gist/hanneshapke/f0980b7422d367808dae409536fe9b46/tfx_pipeline_for_bert_preprocessing_wo_tf-example.ipynb&lt;/denchmark-link&gt;

Please note: I am not working for Google and this answer doesn't represent the reply a Tensorflower. It just shows a way of how I solved the same problem.
&lt;denchmark-link:https://github.com/jason-brian-anderson&gt;@jason-brian-anderson&lt;/denchmark-link&gt;
 Let me know if this solution works for you.
		</comment>
		<comment id='3' author='jason-brian-anderson' date='2020-06-05T18:57:25Z'>
		Hi, &lt;denchmark-link:https://github.com/jason-brian-anderson&gt;@jason-brian-anderson&lt;/denchmark-link&gt;
, you can try change the signature to take tensors instead of serialized tf.example see if that works for tf.serving restful api.
here is an &lt;denchmark-link:https://github.com/tensorflow/tfx/pull/1906&gt;example&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='jason-brian-anderson' date='2020-06-05T19:15:44Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tfx/issues/1108&gt;#1108&lt;/denchmark-link&gt;
 is likely related
		</comment>
		<comment id='5' author='jason-brian-anderson' date='2020-06-06T04:12:01Z'>
		&lt;denchmark-link:https://github.com/hanneshapke&gt;@hanneshapke&lt;/denchmark-link&gt;
  - first thank you for the help, and second - for the great book that you are co-authoring! I only wish i'd found it before starting this project!
As mentioned,  my colleague and I solved the problem in such a way as to 1) allow for the  the &lt;denchmark-link:https://www.tensorflow.org/tfx/guide/evaluator&gt;examplegen component&lt;/denchmark-link&gt;
 example output ()  for model evaluation input to the &lt;denchmark-link:https://www.tensorflow.org/tfx/guide/evaluator&gt;evaluator component&lt;/denchmark-link&gt;
 component, while 2) simultaneously providing a signature specifiable in TF Serving context that would allow of raw text predictions. Keeping TFMA validation to prevent the blessing of unworthy models is a key requirement for us.
The following code was added within the scope of the run_fn function. (note that instead of 'text' as an input, our input is 'request_url':
&lt;denchmark-code&gt;    class MyModule(tf.Module):
        def __init__(self, model, tf_transform_output):
            self.model = model
            self.tf_transform_output = tf_transform_output
            self.model.tft_layer = self.tf_transform_output.transform_features_layer()

        @tf.function(input_signature=[tf.TensorSpec(shape=(None), dtype=tf.string, name='request_url')])
        def tf_serving_raw_input_fn(self, request_url):

            request_url_sp_tensor = tf.sparse.SparseTensor(
                indices=[[0, 0]],
                values=request_url,
                dense_shape=(1, 1)
            )

            parsed_features = {'request_url': request_url_sp_tensor, }
            transformed_features = self.model.tft_layer(parsed_features)
            transformed_features.pop(_transformed_name(_LABEL_KEY))
            return self.model(transformed_features)

        @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')])
        def serve_tf_examples_fn(self, serialized_tf_examples):
            feature_spec = self.tf_transform_output.raw_feature_spec()
            feature_spec.pop(_LABEL_KEY)
            parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
            transformed_features = self.model.tft_layer(parsed_features)
            transformed_features.pop(_transformed_name(_LABEL_KEY))
            return self.model(transformed_features)


    module = MyModule(model, tf_transform_output)

    signatures = {"serving_default": module.serve_tf_examples_fn,
                  "serving_raw_input": module.tf_serving_raw_input_fn,

                  }
    tf.saved_model.save(module,
                        export_dir=fn_args.serving_model_dir,
                        signatures=signatures,
                        options=None,
                        )
                        )
&lt;/denchmark-code&gt;

The first obvious change is subclassing tf.module instead of directly saving the Keras model as is done in the &lt;denchmark-link:https://www.tensorflow.org/tfx/tutorials/tfx/components_keras&gt;TF2 Keras Tutorial&lt;/denchmark-link&gt;
.  This approach was inspired by this &lt;denchmark-link:https://stackoverflow.com/questions/56659949/saving-a-tf2-keras-model-with-custom-signature-defs&gt;stackoverflow&lt;/denchmark-link&gt;
,but it doesn't  explain the difference in terms of this approach vs directly saving the keras model with the  approach.I'm not sure whether using this approach contributed to our success or whether a direct keras save with the above  would have also worked.
The REST API code was as follows:
&lt;denchmark-code&gt;rest_uri = f'http://{server}:8501/v1/models/wrcv3/versions/{ver}:predict'
headers = {"content-type": "application/json"}

data = {
  "signature_name":"serving_raw_input",
  "instances":[
    {
       "request_url":"cgi-bin"
    },
      
  ]
}
data = json.dumps(data)
json_response = requests.post(rest_uri, data=data, headers=headers)
print(json_response.content)
print(json_response.json)
&lt;/denchmark-code&gt;

resulting in:
&lt;denchmark-code&gt;b'{\n    "predictions": [[0.997684]\n    ]\n}'
&lt;bound method Response.json of &lt;Response [200]&gt;&gt;
&lt;/denchmark-code&gt;

One notable drawback to our solution above it that it is not capable of receiving multiple inputs. Although we expect this to be an easy fix.  Not sure if this will affect TF Serving's batching ability. Haven't tested that yet.
I did carefully read both your response and spent time with the referenced colab,  Referring to your cited code comment:
&lt;denchmark-code&gt;    @tf.function
    def serve_tf_examples_fn(text):
        """Returns the output to be used in the serving signature.
        TF is currently looking for a function serve_tf_examples_fn(),
        therefore the function name can't be changed, even though we aren't 
        parsing tf.Example records.
        """
        reshaped_text = tf.reshape(text, [-1, 1])
        transformed_features = model.tft_layer({"text": reshaped_text})
        outputs = model(transformed_features)
        return {'outputs': outputs}
&lt;/denchmark-code&gt;

This presents a problem for us in that Evaluator component  by default uses the serving_default signature which our tf2 keras tutorial -based serve_tf_examples_fn is keyed to in the signatures dict.
To test the assertion made above, i renamed serve_tf_examples_fn to alternatively_named_serve_tf_examples_fn as an experiment.  This did result in a fully successful Airflow DAG along with a BLESSED model,  TFMA validation metrics,  and several render_slicing_metrics output from TFMA.  So i'm wondering what the discrepency is between what you have observed and this?
Regarding the specific content of the  you cite above, using the &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/reshape&gt;tf.reshape&lt;/denchmark-link&gt;
 seems like a great way to handle an incoming dense tensor, but as we stuck very closely with the tutorial's code, we use the  as the first operation in our  function, which expects and parses sparsetensors, and not dense tensors.  We did try it anyway but it didn't work,  the traceback suggesting that tf.reshape was not happy about receiving a sparse tensor.
Perhaps our use of the tf.Module subclassing approach enabled us to evade the constraint you found above? and if so, then perhaps that constraint is limited to the keras.save functionality, or the  code on which (i think) the keras save functionality depends?
We are new to TF2 and TFX so if we've misunderstood or mis-stated anything please feel encouraged to correct us!
Side note,  the &lt;denchmark-link:https://www.tensorflow.org/tfx/serving/signature_defs&gt;signaturedefs documentation&lt;/denchmark-link&gt;
 on the tfx site that lists the predict/classify/regression protobufs is still an absolute mystery to us - in terms of how to use them at least.
		</comment>
		<comment id='6' author='jason-brian-anderson' date='2020-06-06T04:23:24Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tfx/issues/1885&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tfx/issues/1885&gt;No&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='jason-brian-anderson' date='2020-06-08T18:01:19Z'>
		HI, Did you try change the signature to take tensor like &lt;denchmark-link:https://github.com/tensorflow/tfx/pull/1906&gt;this&lt;/denchmark-link&gt;

In this way the signature can take tensors and it works with tfma.
		</comment>
		<comment id='8' author='jason-brian-anderson' date='2020-06-10T03:00:46Z'>
		Hi &lt;denchmark-link:https://github.com/jason-brian-anderson&gt;@jason-brian-anderson&lt;/denchmark-link&gt;
, thanks for your details comment above. I'm having the same issue and I got the following error with the concept that you had. I wonder if you can help me with that. Thanks in advance! Instead of request URL, I have multiple data features as tf.string.
Error:
&lt;denchmark-code&gt;    110           ("Expected a TensorFlow function to generate a signature for, but "
    111            "got {}. Only `tf.functions` with an input signature or "
--&gt; 112            "concrete functions can be used as a signature.").format(function))
    113 
    114     wrapped_functions[original_function] = signature_function = (

ValueError: Expected a TensorFlow function to generate a signature for, but got . Only `tf.functions` with an input signature or concrete functions can be used as a signature.
&lt;/denchmark-code&gt;

Code:
&lt;denchmark-code&gt;def run_fn(fn_args):

    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

    train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 40)
    eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 40)

    model = get_model()

    model.fit(
      train_dataset,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps)
    
    class MyModule(tf.Module):
        def __init__(self, model, tf_transform_output):
            self.model = get_model
            self.tf_transform_output = tf_transform_output
            self.model.tft_layer = self.tf_transform_output.transform_features_layer()
        @tf.function
        def serve_tf_examples_fn(serialized_tf_examples):
            """Returns the output to be used in the serving signature."""
            feature_spec = tf_transform_output.raw_feature_spec()
            feature_spec.pop(_LABEL_KEY_EA)
            parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

            transformed_features = model.tft_layer(parsed_features)
            transformed_features.pop(_transformed_name(_LABEL_KEY_EA))
            return self.model(transformed_features)

        @tf.function(input_signature=[tf.TensorSpec(shape=(None), dtype=tf.string, name='raw_examples')])
        def tf_serving_raw_input_fn(self, raw_features):
            raw_features_sp_tensor = tf.sparse.SparseTensor(
                indices=[[0, 0]],
                values=raw_features,
                dense_shape=(1, 1)
            )
            parsed_features = {
                'raw_features': raw_features_sp_tensor,
            }
            transformed_features = self.model.tft_example_layer(parsed_features)
            transformed_features.pop(_transformed_name(_LABEL_KEY_EA))
            return self.model(transformed_features)

    module = MyModule(model, tf_transform_output)

    signatures = {
      'serving_default': module.serve_tf_examples_fn,
      'serving_raw_input': module.tf_serving_raw_input_fn
    }

    tf.saved_model.save(module, export_dir=fn_args.serving_model_dir, signatures=signatures, options=None,)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>