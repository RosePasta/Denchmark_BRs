<bug id='166' author='Malonl' open_date='2019-05-22T12:32:46Z' closed_time='2019-05-24T05:49:58Z'>
	<summary>ModelValidator does not work with regression model</summary>
	<description>
I have created my own custom pipeline, using the taxi pipeline as basis to branch out from. So this pipeline contains everything from CsvExampleGen to the Pusher.
I am trying to build a DNNLinearCombinedRegressor model. This all works fine until the ModelValidator component. It tries to compare the accuracy of the blessed model with the current model, however since we are running a regressor model, there is no accuracy metric and so the component fails on that point and with that the rest of the pipeline.
This is the error message that pops up here:

[2019-05-22 10:01:52,613] {logging_mixin.py:95} INFO - [2019-05-22 10:01:52,613] {executor.py:51} INFO - TF Logging: current metric: ((), {u'label/mean': {'doubleValue': 5.047114372253418}, u'average_loss': {'doubleValue': 335.8445129394531}, u'post_export_metrics/example_count': {'doubleValue': 43299.0}, u'prediction/mean': {'doubleValue': 5.458231449127197}})
[2019-05-22 10:01:52,613] {models.py:1788} ERROR - 'accuracy'
Traceback (most recent call last):
File "/home/username/taxi_pipeline/local/lib/python2.7/site-packages/airflow/models.py", line 1657, in _run_raw_task
result = task_copy.execute(context=context)
File "/home/username/taxi_pipeline/local/lib/python2.7/site-packages/airflow/operators/python_operator.py", line 103, in execute
return_value = self.execute_callable()
File "/home/username/taxi_pipeline/local/lib/python2.7/site-packages/airflow/operators/python_operator.py", line 108, in execute_callable
return self.python_callable(*self.op_args, **self.op_kwargs)
File "/home/username/taxi_pipeline/local/lib/python2.7/site-packages/tfx/orchestration/airflow/airflow_adapter.py", line 150, in python_exec
executor.Do(self._input_dict, self._output_dict, self._exec_properties)
File "/home/username/taxi_pipeline/local/lib/python2.7/site-packages/tfx/components/model_validator/executor.py", line 164, in Do
blessed_model_dir=blessed_model_dir)
File "/home/username/taxi_pipeline/local/lib/python2.7/site-packages/tfx/components/model_validator/executor.py", line 108, in _generate_blessing_result
blessed_model_eval_result)):
File "/home/username/taxi_pipeline/local/lib/python2.7/site-packages/tfx/components/model_validator/executor.py", line 55, in _compare_eval_result
if (current_metric[1]['accuracy']['doubleValue'] &lt;
KeyError: 'accuracy'

The code where this fails is in tfx/components/model_validator/executor.py in the _compare_eval_result definition:
&lt;denchmark-code&gt;  # TODO(jyzhao): customized validation support.
  def _compare_eval_result(self, current_model_eval_result,
                           blessed_model_eval_result):
    """Compare accuracy of all metrics and return true if current is better or equal."""
    for current_metric, blessed_metric in zip(
        current_model_eval_result.slicing_metrics,
        blessed_model_eval_result.slicing_metrics):
      # slicing_metric is a tuple, index 0 is slice, index 1 is its value.
      tf.logging.info("TF Logging: current metric: " + str(current_metric))
      tf.logging.info("TF Logging: current metric: " + str(blessed_metric))
      if current_metric[0] != blessed_metric[0]:
        raise RuntimeError('EvalResult not match {} vs {}.'.format(
            current_metric[0], blessed_metric[0]))
      if (current_metric[1]['accuracy']['doubleValue'] &lt;
          blessed_metric[1]['accuracy']['doubleValue']):
        tf.logging.info(
            'Current model accuracy is worse than blessed model: {}'.format(
                current_metric[0]))
        return False
    return True
&lt;/denchmark-code&gt;

There seems to be no switch to check for accuracy when using a classification model and something else, like average_loss, when using a regressor model.
I did a workaroundÂ by just replacing accuracy with average_loss, but this is hardcoded in the component code, so not very desirable.
Is there something that I am missing here, and is there actually a switch or a place to add custom configuration for the model validator? Or is this something that is indeed not implemented?
I am running in Python 2.7 and TFX 0.13.0.
Thanks!
	</description>
	<comments>
		<comment id='1' author='Malonl' date='2019-05-22T12:37:34Z'>
		Also experiencing that. I wonder if there is some possible configuration to be passed to the validator. There is a (not so well) documented input called "eval_spec" in a documentation page but this doesn't seem to be present in the actual code.
documentation page: &lt;denchmark-link:https://www.tensorflow.org/tfx/guide/modelval&gt;https://www.tensorflow.org/tfx/guide/modelval&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Malonl' date='2019-05-22T16:11:05Z'>
		Hi, currently it's not supported, will keep this as a feature request to support customized model validation
for now, you can change the _pass_threshold() and _compare_eval_result() to do customization
		</comment>
		<comment id='3' author='Malonl' date='2019-05-23T07:04:36Z'>
		&lt;denchmark-link:https://github.com/1025KB&gt;@1025KB&lt;/denchmark-link&gt;
 just to check: the "eval_spec" argument to ModelValidator is in the documentation anyways (&lt;denchmark-link:https://www.tensorflow.org/tfx/guide/modelval&gt;https://www.tensorflow.org/tfx/guide/modelval&lt;/denchmark-link&gt;
).
Is this documentation wrong (the argument don't exist), or is it outdated (it existed in a previous version but not anymore), or is it referring to some other functionality not related to this issue?
		</comment>
		<comment id='4' author='Malonl' date='2019-05-23T17:22:08Z'>
		&lt;denchmark-link:https://github.com/Efaq&gt;@Efaq&lt;/denchmark-link&gt;
 sorry the documentation was incorrect. I'll work with &lt;denchmark-link:https://github.com/rcrowe-google&gt;@rcrowe-google&lt;/denchmark-link&gt;
 to fix it.
We definitely plan to open up more configurations for model validator in short term future.
		</comment>
		<comment id='5' author='Malonl' date='2019-05-24T05:49:58Z'>
		Ok, thanks for all the answers.
We will keep our workaround for now.
		</comment>
		<comment id='6' author='Malonl' date='2019-05-24T06:59:19Z'>
		&lt;denchmark-link:https://github.com/zhitaoli&gt;@zhitaoli&lt;/denchmark-link&gt;
 Ok, thank you for these updates!
		</comment>
	</comments>
</bug>