<bug id='190' author='mratsim' open_date='2018-03-03T21:22:28Z' closed_time='2018-05-06T22:09:04Z'>
	<summary>Investigate slowness and memory usage compared to Python</summary>
	<description>
Reported by &lt;denchmark-link:https://github.com/narimiran&gt;@narimiran&lt;/denchmark-link&gt;
 on IRC
import math
import arraymancer

const
  dz = 0.01
  z = 100
  spaceSteps = int(z / dz)
  timeSteps = 50000
  dt = 0.12 / timeSteps
  alpha = 2.0
  startingTemp = 30.0
  oscillations = 20.0

var
  Ts = startingTemp * ones[float](timeSteps, spaceSteps)


for j in 0 ..&lt; timeSteps:
  Ts[j, 0] = startingTemp - oscillations * sin(2.0 * PI * j.float / timeSteps)

proc f(T: Tensor[float]): Tensor[float] =
  let d2T_dz2 = (T[_, 0..^3] - 2.0 * T[_, 1..^2] + T[_, 2..^1]) / dz^2
  return alpha * d2T_dz2

proc eulerSolve(Ts: var Tensor[float]) =
  for t in 0 ..&lt; timeSteps-1:
    Ts[t+1, 1..^2] = Ts[t, 1..^2] + dt * f(Ts[t, _])
    Ts[t+1, ^1] = Ts[t+1, ^2]

Ts.eulerSolve()
&lt;denchmark-link:https://user-images.githubusercontent.com/22738317/36939437-596b0a92-1f31-11e8-80e5-512351ac8e14.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='mratsim' date='2018-03-03T21:40:17Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/22738317/36939582-6aaa90be-1f33-11e8-817d-6cd5e9f208c8.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='mratsim' date='2018-03-03T21:57:22Z'>
		Nim is spending 21 seconds on this, there is a big issue on assignments:
import math
import arraymancer

const
  dz = 0.01
  z = 100
  spaceSteps = int(z / dz)
  timeSteps = 50000
  dt = 0.12 / timeSteps
  alpha = 2.0
  startingTemp = 30.0
  oscillations = 20.0

var
  Ts = startingTemp * ones[float](timeSteps, spaceSteps)


for j in 0 ..&lt; timeSteps:
  Ts[j, 0] = startingTemp - oscillations * sin(2.0 * PI * j.float / timeSteps)
		</comment>
		<comment id='3' author='mratsim' date='2018-03-03T22:13:32Z'>
		There might be an allocator bug because it's not even the loop that is slow:
This takes a whooping 17seconds half of it allocating memory
import math
import arraymancer

const
  dz = 0.01
  z = 100
  spaceSteps = int(z / dz)
  timeSteps = 50000
  dt = 0.12 / timeSteps
  alpha = 2.0
  startingTemp = 30.0
  oscillations = 20.0

var
  Ts = startingTemp * ones[float](timeSteps, spaceSteps)
&lt;denchmark-link:https://user-images.githubusercontent.com/22738317/36939843-767b9dde-1f38-11e8-8b1e-d01eb5e3a390.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='mratsim' date='2018-03-03T22:46:36Z'>
		Raised upstream: &lt;denchmark-link:https://github.com/nim-lang/Nim/issues/7295&gt;nim-lang/Nim#7295&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='mratsim' date='2018-03-04T06:58:11Z'>
		&lt;denchmark-link:https://github.com/mratsim&gt;@mratsim&lt;/denchmark-link&gt;
 thank you for raising this and immediately investigating!!
		</comment>
		<comment id='6' author='mratsim' date='2018-03-04T07:44:16Z'>
		For a comparison, here is a version without Arraymancer, using Nim's arrays.
Notice timeSteps = 10000 - I cannot use 50000 with arrays, it raises an error during compilation.
import math

const
  dz = 0.01
  z = 100
  spaceSteps = int(z / dz)
  timeSteps = 10000
  dt = 0.12 / timeSteps
  alpha = 2.0
  startingTemp = 30.0
  oscillations = 20.0

var Ts: array[timeSteps, array[spaceSteps, float]]

for t in 0 ..&lt; timeSteps:
  Ts[t][0] = startingTemp - oscillations * sin(2.0 * PI * t.float / timeSteps) 
  for s in 1 ..&lt; spaceSteps:
    Ts[t][s] = startingTemp


proc f(T: array[spaceSteps, float]): array[spaceSteps-2, float] =
  for i in 0 .. spaceSteps-3:
    result[i] = alpha * (T[i] - 2.0 * T[i+1] + T[i+2]) / dz^2


proc eulerSolve(Ts: var array[timeSteps, array[spaceSteps, float]]) =
  for t in 0 ..&lt; timeSteps-1:
    let
      k = f(Ts[t])
    for i in 1 .. spaceSteps-2:
      Ts[t+1][i] = Ts[t][i] + dt * k[i-1]
    Ts[t+1][spaceSteps-1] = Ts[t+1][spaceSteps-2] 

Ts.eulerSolve()
On my computer, this runs in 0.8 sec (Arraymancer needs 2.6 sec)
		</comment>
		<comment id='7' author='mratsim' date='2018-03-04T09:49:30Z'>
		Python version:
import numpy as np


dz = 0.01
z = 100
space_steps = int(z / dz)

time_steps = 50000
ts = np.linspace(0, 0.12, time_steps)
dt = 0.12 / time_steps
alpha = 2
starting_temp = 30
oscillations = 20

Ts = starting_temp * np.ones((time_steps, space_steps), dtype=np.float64)
Ts[:, 0] = starting_temp - oscillations * np.sin(2 * np.pi * ts / 12)


def f(T):
    d2T_dz2 = (T[:-2] - 2 * T[1:-1] + T[2:]) / dz**2
    return alpha * d2T_dz2


def euler_solve(Ts):
    for t in range(time_steps-1):
        Ts[t+1, 1:-1] = Ts[t, 1:-1] + dt * f(Ts[t])
        Ts[t+1, -1] = Ts[t+1, -2]
    return Ts


euler = euler_solve(Ts)
		</comment>
		<comment id='8' author='mratsim' date='2018-03-04T15:39:02Z'>
		
On my computer, this runs in 0.8 sec (Arraymancer needs 2.6 sec)

With the updated Arraymancer, the timings for a problem with 10000 time steps are as follows:
&lt;denchmark-code&gt;nim+Arraymancer | 1.3 sec
nim with arrays | 0.7 sec
python+numpy    | 1.3 sec
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='mratsim' date='2018-03-14T13:46:21Z'>
		A couple updates on this:

#193 has been merged, previously functions/computations may have been done twice if they were used as template inputs
#164 I thought I could optimize items iteration further by implementing the TRIOT paper mentioned in the issue, (un?)fortunately the current iteration scheme is 20% faster than macros generated nested for loops

&lt;denchmark-h:h1&gt;Next steps:&lt;/denchmark-h&gt;

There shouldn't be any reason why Arraymancer has the same speed as Python here (there is no external libraries like BLAS involved I think).
I don't think there is a way to get as fast as arrays purely on the stack though.
So:

I'll investigate the remaining overhead and squash it (slicing? assignments with unnecessary copies?).

		</comment>
		<comment id='10' author='mratsim' date='2018-03-17T10:20:07Z'>
		If you change this line:
var
  Ts = startingTemp * ones[float](timeSteps, spaceSteps)
to this
  var
    Ts = newTensorWith[float]([timeSteps, spaceSteps], startingTemp)
It cuts the time from 22 to 10 seconds on my machine.
		</comment>
		<comment id='11' author='mratsim' date='2018-03-18T00:42:00Z'>
		So after this wild day of investigation, here are my findings:


When benchmarking, if Python is linked to MKL it might  be surprisingly fast. I was hunting a perf difference that was due to multi-threading.


Slicing is actually cheap because there is no memory copied


The biggest performance improvement (~30%} is by avoiding temporary tensors, which you can do with the apply3_inline and map3_inline templates as showed here. This brings single-threaded Arraymancer within striking distance to MKL multi-threaded Numpy. (I can't compete with stack arrays yet though).


Unfortunately, EulerSolve at the moment is not compatible with OpenMP, computations are slowed quite hard. This is similar to what happened here: #134 and the corresponding fix: #139


The next step would be to implement a loopFusion macro with this syntax as mentionned in &lt;denchmark-link:https://github.com/mratsim/Arraymancer/issues/145&gt;#145&lt;/denchmark-link&gt;
:
proc f(T: Tensor[float]): Tensor[float] =
  loopFusion:
    a_dz2 * (T[_, 0..^3] - 2.0 * T[_, 1..^2] + T[_, 2..^1])
The current apply_inline templates are a bit unwieldly and do not scale beyond 3.
		</comment>
		<comment id='12' author='mratsim' date='2018-05-01T13:19:30Z'>
		&lt;denchmark-link:https://github.com/narimiran&gt;@narimiran&lt;/denchmark-link&gt;
 I've noticed a 3x performance regression in Nim, raised here: &lt;denchmark-link:https://github.com/mratsim/Arraymancer/issues/221&gt;#221&lt;/denchmark-link&gt;
, upstreamed: &lt;denchmark-link:https://github.com/nim-lang/Nim/issues/7743&gt;nim-lang/Nim#7743&lt;/denchmark-link&gt;
 and caused by &lt;denchmark-link:https://github.com/nim-lang/Nim/commit/358709e9cbc4ed3e084dc6d1db313a862b8356e4&gt;nim-lang/Nim@358709e&lt;/denchmark-link&gt;
.
I did not have time to run your benchmark with the previous OK commit but it should improve time significantly.
		</comment>
		<comment id='13' author='mratsim' date='2018-05-06T08:04:32Z'>
		Should be fixed with &lt;denchmark-link:https://github.com/nim-lang/Nim/commit/88cf6573e04bd7ee8762aa336460b9748f0d4644&gt;nim-lang/Nim@88cf657&lt;/denchmark-link&gt;
 but I haven't tested it.
EDIT: Seems like it even increased the performance compared to before the regression. Yay!
		</comment>
		<comment id='14' author='mratsim' date='2018-05-06T22:09:04Z'>
		Yes on my machine single-threaded Arraymancer is as fast as multi-threaded Numpy (via OpenMP and Intel MKL) on the Euler benchmark (~6s both), so fixed for me. It is even 2.7x faster than "normal" NumPy (~16s).
The "optimized" code without any temporary allocation is even faster as it completes in 3.7s.
		</comment>
	</comments>
</bug>