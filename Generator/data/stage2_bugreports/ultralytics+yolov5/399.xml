<bug id='399' author='DennisYoung96' open_date='2020-07-14T10:54:59Z' closed_time='2020-09-03T00:37:15Z'>
	<summary>RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.</summary>
	<description>
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


OS: [Ubuntu20]
GPU [NVIDIA Corporation TU102 [TITAN RTX] ] * 2

Thanks for ur great work!
I worked very well those days in my custom datasets.
but today, i did nothing  but want transfer from A datasets to B datasets,then i run into problems like this:
Output:
&lt;denchmark-code&gt;     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
   59/2999     1.55G    0.1143   0.01843   0.09183    0.2246        26       640: 100%|██████████████████████████████████████████████████████████████████| 87/87 [00:37&lt;00:00,  2.35it/s]
               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████████████████████████████████████████████████| 22/22 [00:07&lt;00:00,  2.88it/s]
                 all         340    1.09e+03     0.00327      0.0208     0.00251    0.000429

     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
  0%|      
Traceback (most recent call last):
  File "my_train.py", line 411, in &lt;module&gt;
    train(hyp)
  File "my_train.py", line 257, in train
    pred = model(imgs)
  File "/home/yz/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/yz/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 475, in forward
    self.reducer.prepare_for_backward([])
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
&lt;/denchmark-code&gt;

it always occur when run into the second epoch. whatever i use the pretrained weights files like best.pt or yolov5s.pt or None , this bug happend as expected.
Do anyone run into the same bugs?  I'm appreciate ur  your answers
#more bugs:
if i use the arg :--evolve ，the process always interrupted ,and raise the Error like this:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "my_train.py", line 453, in &lt;module&gt;
    results = train(hyp.copy())
  File "my_train.py", line 169, in train
    rank=0)  # node rank
  File "/home/yz/anaconda3/envs/torch/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py", line 364, in init_process_group
    raise RuntimeError("trying to initialize the default process group "
RuntimeError: trying to initialize the default process group twice!
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='DennisYoung96' date='2020-07-14T10:55:47Z'>
		Hello &lt;denchmark-link:https://github.com/DennisYoung96&gt;@DennisYoung96&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;Jupyter Notebook&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov5&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart&gt;Google Cloud Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
If this is a custom model or data training question, please note that Ultralytics does not provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:

Cloud-based AI systems operating on hundreds of HD video streams in realtime.
Edge AI integrated into custom iOS and Android apps for realtime 30 FPS video inference.
Custom data training, hyperparameter evolution, and model exportation to any destination.

For more information please visit &lt;denchmark-link:https://www.ultralytics.com&gt;https://www.ultralytics.com&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='DennisYoung96' date='2020-07-15T07:25:51Z'>
		I have the same problem, and I found that the GPU memory is unstable when training yolov5s.
reduced batch sizes is worked for me.
		</comment>
		<comment id='3' author='DennisYoung96' date='2020-07-15T07:36:08Z'>
		&lt;denchmark-link:https://github.com/FinnChou&gt;@FinnChou&lt;/denchmark-link&gt;
 it appears you may have environment problems. We don't try to debug local environments. Instead we offer a variety of verified environments:
&lt;denchmark-h:h2&gt;Environments&lt;/denchmark-h&gt;

YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including &lt;denchmark-link:https://developer.nvidia.com/cuda&gt;CUDA&lt;/denchmark-link&gt;
/&lt;denchmark-link:https://developer.nvidia.com/cudnn&gt;CUDNN&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://www.python.org/&gt;Python&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://pytorch.org/&gt;PyTorch&lt;/denchmark-link&gt;
 preinstalled):

Google Colab Notebook with free GPU: 
Kaggle Notebook with free GPU: https://www.kaggle.com/ultralytics/yolov5
Google Cloud Deep Learning VM. See GCP Quickstart Guide
Docker Image https://hub.docker.com/r/ultralytics/yolov5. See Docker Quickstart Guide 

		</comment>
		<comment id='4' author='DennisYoung96' date='2020-07-16T10:13:45Z'>
		I find that if i train with one GPU ,or change the other datasets it works again!
so , it's the problem about DistributedDataParallel?
		</comment>
		<comment id='5' author='DennisYoung96' date='2020-07-17T03:37:03Z'>
		&lt;denchmark-link:https://github.com/DennisYoung96&gt;@DennisYoung96&lt;/denchmark-link&gt;
 I also meet this problem when I use mutil gpus. I dont know what happen.
		</comment>
		<comment id='6' author='DennisYoung96' date='2020-07-17T03:44:18Z'>
		I have the same problem in mutil gpus.
reduced batch sizes is worked for me.
		</comment>
		<comment id='7' author='DennisYoung96' date='2020-07-17T04:20:33Z'>
		thanks, i will try
		</comment>
		<comment id='8' author='DennisYoung96' date='2020-07-29T09:51:15Z'>
		in the v2 release,the problem seems not to appear again for me.
		</comment>
		<comment id='9' author='DennisYoung96' date='2020-08-29T00:15:55Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>