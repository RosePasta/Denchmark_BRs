<bug id='307' author='Zacchaeus14' open_date='2020-07-06T09:24:58Z' closed_time='2020-07-07T02:57:07Z'>
	<summary>mAP abnormally low after resuming and can be solved by deleting shape.npy under label folder</summary>
	<description>
Before submitting a bug report, please be aware that your issue must be reproducible with all of the following, otherwise it is non-actionable, and we can not help you:

Current repo: run git fetch &amp;&amp; git status -uno to check and git pull to update repo
Common dataset: coco.yaml or coco128.yaml
Common environment: Colab, Google Cloud, or Docker image. See https://github.com/ultralytics/yolov5#reproduce-our-environment

If this is a custom dataset/training question you must include your train*.jpg, test*.jpg and results.png figures, or we can not help you. You can generate these with utils.plot_results().
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

A clear and concise description of what the bug is.
&lt;denchmark-h:h2&gt;To Reproduce (REQUIRED)&lt;/denchmark-h&gt;

Input:
&lt;denchmark-code&gt;import torch

a = torch.tensor([5])
c = a / 0
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/Users/glennjocher/opt/anaconda3/envs/env1/lib/python3.7/site-packages/IPython/core/interactiveshell.py", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "&lt;ipython-input-5-be04c762b799&gt;", line 5, in &lt;module&gt;
    c = a / 0
RuntimeError: ZeroDivisionError
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

A clear and concise description of what you expected to happen.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.

OS: [e.g. Ubuntu]
GPU [e.g. 2080 Ti]

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='Zacchaeus14' date='2020-07-06T17:21:01Z'>
		&lt;denchmark-link:https://github.com/Zacchaeus14&gt;@Zacchaeus14&lt;/denchmark-link&gt;
 can you clarify a bit better the problem you're seeing?
		</comment>
		<comment id='2' author='Zacchaeus14' date='2020-07-06T17:48:15Z'>
		&lt;denchmark-link:https://github.com/Zacchaeus14&gt;@Zacchaeus14&lt;/denchmark-link&gt;
 is this related to &lt;denchmark-link:https://github.com/ultralytics/yolov5/issues/306&gt;#306&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='3' author='Zacchaeus14' date='2020-07-07T02:48:18Z'>
		Yes, the problem I encountered is exactly same as &lt;denchmark-link:https://github.com/ultralytics/yolov5/issues/306&gt;#306&lt;/denchmark-link&gt;
. Specifically, train.py doesn't work when it loads an old .npy file. Thank you for your reply.
		</comment>
		<comment id='4' author='Zacchaeus14' date='2020-07-07T02:51:27Z'>
		btw, I was running on a cloab VM.
		</comment>
		<comment id='5' author='Zacchaeus14' date='2020-07-07T02:57:07Z'>
		&lt;denchmark-link:https://github.com/Zacchaeus14&gt;@Zacchaeus14&lt;/denchmark-link&gt;
 ok got it! The dataloading code affected by this will hopefully undergo a revamp this week to fix this and incorporate a few other improvements.
I'll close this issue as duplicate and focus the conversation then on &lt;denchmark-link:https://github.com/ultralytics/yolov5/issues/306&gt;#306&lt;/denchmark-link&gt;
 since it was opened earlier.
		</comment>
		<comment id='6' author='Zacchaeus14' date='2020-07-10T03:12:07Z'>
		&lt;denchmark-link:https://github.com/Zacchaeus14&gt;@Zacchaeus14&lt;/denchmark-link&gt;
 I just pushed a major update &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/520f5de6f0febf30dd3545793f528c3478c18299&gt;520f5de&lt;/denchmark-link&gt;
 that should resolve all label caching issues. Labels and images are scanned together the first time a dataset is trained, and cached into a labels.cache file now which contains a dictionary of image names, shapes, labels and a unique hash for the cached dataset. Images are scanned for corruption now also as part of the process. All subsequent trainings retrieve the current hash value of the dataset, and if it matches the cached value, then the cached file is used, else the entire dataset is recached and a new file saved. Please try out the changes and let me know if this resolves your issue!
		</comment>
	</comments>
</bug>