<bug id='851' author='bsugerman' open_date='2020-08-26T18:58:32Z' closed_time='2020-11-06T00:32:43Z'>
	<summary>Unable to resume training when using DDP</summary>
	<description>
I'm running training on 2 GPUs without any problems as follows:
&lt;denchmark-code&gt; python -m torch.distributed.launch --nproc_per_node 2 --master_port 1111  train.py --cfg yolov5s.yaml --weights '' --epochs 3 --batch-size 12 --workers 64 --device 0,1 --data data/coco128.yaml
&lt;/denchmark-code&gt;

However, if I have to kill the job (so someone else can use the processors for a bit), I can not restart the training. I've tried
&lt;denchmark-code&gt;python -m torch.distributed.launch --nproc_per_node 2 --master_port 1111  train.py --cfg yolov5s.yaml --weights '' --epochs 3 --batch-size 12 --workers 64 --device 0,1 --data data/coco128.yaml --resume 
&lt;/denchmark-code&gt;

and a number of variants by leaving out different arguments related to multi-processors. The training gets to:
&lt;denchmark-code&gt;Transferred 370/370 items from ./runs/exp1/weights/last.pt
Using DDP
&lt;/denchmark-code&gt;

sits for a few seconds, then issues the second
&lt;denchmark-code&gt;Using DDP
&lt;/denchmark-code&gt;

and then just hangs. On the GPU, 3 processors are started. Two on processor 0, each using 2250Mb, and one on processor 1 using 965Mb.
Any ideas?
	</description>
	<comments>
		<comment id='1' author='bsugerman' date='2020-08-26T19:00:42Z'>
		I should note that resumption works fine when just using 1 gpu.
		</comment>
		<comment id='2' author='bsugerman' date='2020-08-26T19:28:26Z'>
		&lt;denchmark-link:https://github.com/bsugerman&gt;@bsugerman&lt;/denchmark-link&gt;
 I have not tested --resume on multi-gpu, but a problem above I see is that --resume is incapable of modifying original arguments. The only supported use cases are
 or

You might want to try this with your DDP command.
		</comment>
		<comment id='3' author='bsugerman' date='2020-09-12T17:35:09Z'>
		I also meet the problem when I try to resume training with ddp, can you fix the bug? &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='bsugerman' date='2020-09-21T12:31:33Z'>
		I have tried these, and it doesn't work. I have isolated the problem to the line in train.py that converts the model to DDP:
&lt;denchmark-code&gt;    # DDP mode
    if cuda and rank != -1:
         model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)
&lt;/denchmark-code&gt;

This is where the program seems to hang, for me at least.

@bsugerman I have not tested --resume on multi-gpu, but a problem above I see is that --resume is incapable of modifying original arguments. The only supported use cases are
python train.py --resume or
python train.py --resume path/to/last.pt
You might want to try this with your DDP command.

		</comment>
		<comment id='5' author='bsugerman' date='2020-10-01T10:01:10Z'>
		A simple workaround, for now, is to comment the lines with opt replacement in train.py:
&lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/train.py#L422-L423&gt;https://github.com/ultralytics/yolov5/blob/master/train.py#L422-L423&lt;/denchmark-link&gt;

because every time you read this config you get local_rank = 0.
Then run training as follows:

		</comment>
		<comment id='6' author='bsugerman' date='2020-11-01T00:32:16Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='7' author='bsugerman' date='2020-12-24T13:24:34Z'>
		
A simple workaround, for now, is to comment the lines with opt replacement in train.py:
https://github.com/ultralytics/yolov5/blob/master/train.py#L422-L423
because every time you read this config you get local_rank = 0.
Then run training as follows:
python -m torch.distributed.launch --nproc_per_node 8 train.py --%your opt params% --resume

Thanks! But sorry i'm a little confused about your method to solve the problem, which line should i comment ? And how to configure "--%your opt params%" ? Thank you!
		</comment>
		<comment id='8' author='bsugerman' date='2020-12-24T13:53:01Z'>
		&lt;denchmark-link:https://github.com/chongkuiqi&gt;@chongkuiqi&lt;/denchmark-link&gt;

Sorry, I should have provided the link to a specific version since the line positions are unstable.
Here are the lines to comment: 

		</comment>
		<comment id='9' author='bsugerman' date='2020-12-24T13:57:56Z'>
		@ how to configure "--%your opt params%"
Since you will no longer override these params with the cached opt.yaml, just make sure you are using the same params manually.
		</comment>
		<comment id='10' author='bsugerman' date='2020-12-24T14:11:35Z'>
		
@ how to configure "--%your opt params%"
Since you will no longer override these params with the cached opt.yaml, just make sure you are using the same params manually.

Thank you !  It works !  And now i understand what caused the problem. Thanks  for your quick reply !
		</comment>
		<comment id='11' author='bsugerman' date='2020-12-24T19:54:41Z'>
		&lt;denchmark-link:https://github.com/chongkuiqi&gt;@chongkuiqi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tyomj&gt;@tyomj&lt;/denchmark-link&gt;
 hi guys, I'm glad to hear there is a workaround. Is this something that might be codeable into a PR to help future people?
Also might there be a workaround to include opt.yaml? opt.yaml carries all the previous argparser arguments that need to be applied the same way on --resume.
		</comment>
	</comments>
</bug>