<bug id='447' author='123456789mojtaba' open_date='2020-07-19T13:44:44Z' closed_time='2020-09-13T00:39:56Z'>
	<summary>Possible AutoAnchor reversal in v2.0</summary>
	<description>
hey guys.
I have trained yolov5 on visdrone for car and pedestrian. But it detects some cars and pedestrians with 2 boundig box instead of one?
does anyone know the problem?
&lt;denchmark-link:https://user-images.githubusercontent.com/53895147/87876207-b33f7780-c9eb-11ea-88eb-1c6873c10cea.jpg&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='123456789mojtaba' date='2020-07-19T14:18:17Z'>
		I have a similar problem with yolov5s. Not sure why it predicts a small bounding box. Next, I will be training on default anchors instead of calculating during training. I doubt anchors might play a role here because proposed anchors for my datasets are smaller.
		</comment>
		<comment id='2' author='123456789mojtaba' date='2020-07-19T15:49:52Z'>
		Can setting higher iou-threshold help?
		</comment>
		<comment id='3' author='123456789mojtaba' date='2020-07-19T17:34:42Z'>
		&lt;denchmark-link:https://github.com/123456789mojtaba&gt;@123456789mojtaba&lt;/denchmark-link&gt;
 do not use a bug label for training results that you don't understand.
		</comment>
		<comment id='4' author='123456789mojtaba' date='2020-07-19T17:42:41Z'>
		&lt;denchmark-link:https://github.com/123456789mojtaba&gt;@123456789mojtaba&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 first, without looking at your  it is impossible to say whether you have trained properly, so displaying anecdotal evidence of improper training on a custom dataset out of context allows no one to properly help you.
Second, 5s is the naturally the smallest and least accurate model. If your goal is accuracy, 5s should not be your first choice obviously. You can see a comparison in our readme table &lt;denchmark-link:https://github.com/ultralytics/yolov5#pretrained-checkpoints&gt;https://github.com/ultralytics/yolov5#pretrained-checkpoints&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='123456789mojtaba' date='2020-07-22T21:45:37Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 There is no doubt on the dataset and training.  The problem is even with YOLOv5l. As I have predicted, the fault was the calculated anchor boxes, because &lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/train.py#L218&gt;check_anchors()&lt;/denchmark-link&gt;
 function is giving smaller anchor values for mine dataset. I get very good results with default anchors. I will update training results.png and prediction result by Saturday 25.07.2020.
		</comment>
		<comment id='6' author='123456789mojtaba' date='2020-07-23T00:27:33Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 hmm that's strange. check_anchors() is supposed to check your anchors to make sure they are aligned to your stride order. i.e. they should both be large to small or small to large depending on your head.
		</comment>
		<comment id='7' author='123456789mojtaba' date='2020-07-23T00:28:54Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 ah, nevermind, check_anchors() recomputes new anchors if needed based on your dataset BPR. You can disable it with python train.py --noautoanchor
		</comment>
		<comment id='8' author='123456789mojtaba' date='2020-07-26T10:13:30Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  Thank you!!
So following are the results.png and prediction.
&lt;denchmark-h:h3&gt;YOLOv5s with auto anchors&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/43172056/88476439-98638a80-cf38-11ea-98d9-46fa5bb9ce15.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/43172056/88476478-f85a3100-cf38-11ea-804e-d7c5cbbc328f.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;YOLOv5s without auto anchors (i.e. --noautoanchor)&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/43172056/88476501-2a6b9300-cf39-11ea-957a-34a8026c3d91.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/43172056/88476510-35bebe80-cf39-11ea-8130-fcf29f0684cf.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='123456789mojtaba' date='2020-07-27T02:01:57Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 ah interesting. Yes the second is definitely better. Can you report your anchors for both using:

AutoAnchor (actually any anchor evolution using our code) works under the assumption that the objects are spread around a range of sizes relative to the model output strides 8, 16 and 32. In theory if your labels are composed solely of larger or smaller objects, then some output layers may be better of being completely removed or ignored than being assigned anchors far outside their receptive field size. In practice though it is difficult determining actual receptive field dimensions.
		</comment>
		<comment id='10' author='123456789mojtaba' date='2020-07-27T23:13:17Z'>
		Hi &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
, Thank you for explaining. So we have labels.png generated during training which is really cool. Can you explain (or have any link) about how to interpret this image?
I have the following values
&lt;denchmark-h:h3&gt;With autoAnchors&lt;/denchmark-h&gt;

Console output during training was
&lt;denchmark-code&gt;thr=0.25: 0.9990 best possible recall, 4.61 anchors past thr
n=9, img_size=416, metric_all=0.313/0.732-mean/best, past_thr=0.488-mean: 6,6,  12,11,  12,25,  23,16,  37,26,  30,61,  62,40,  94,72,  139,123
thr=0.25: 0.9995 best possible recall, 5.22 anchors past thr
n=9, img_size=416, metric_all=0.345/0.757-mean/best, past_thr=0.493-mean: 5,4,  7,7,  13,10,  8,18,  21,17,  19,43,  36,28,  63,46,  113,88
&lt;/denchmark-code&gt;

I have one question here. are these new calculated anchors? If yes then why it doesn't match with following anchors saved in the model? I think the larger anchors group is divided by 8  and smaller group by 32. Whereas it should be opposite right? correct me if I'm wrong
&lt;denchmark-code&gt;tensor([[[ 4.49609,  3.44922],
     [ 7.89453,  5.73438],
     [14.11719, 11.00000]],

    [[ 0.49658,  1.11914],
     [ 1.30859,  1.06055],
     [ 1.21582,  2.69922]],

    [[ 0.14978,  0.13513],
     [ 0.23328,  0.22156],
     [ 0.41089,  0.31543]]], dtype=torch.float16)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Without autoAnchors&lt;/denchmark-h&gt;

These anchors match with the values in yolov5s.yaml file.
&lt;denchmark-code&gt;tensor([[[ 3.62500,  2.81250],
     [ 4.87500,  6.18750],
     [11.65625, 10.18750]],

    [[ 1.87500,  3.81250],
     [ 3.87500,  2.81250],
     [ 3.68750,  7.43750]],

    [[ 1.25000,  1.62500],
     [ 2.00000,  3.75000],
     [ 4.12500,  2.87500]]], dtype=torch.float16)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='123456789mojtaba' date='2020-07-27T23:58:55Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 anchors displayed using this command are in stride units. You are using a pre v2.0 version of the repo so your anchors are reversed compared to v2.0 anchors, but this is not a problem.
yolov5s.yaml:
# anchors
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32
yolov5s anchors:
print(torch.load('yolov5s.pt')['model'].model[-1].anchors)
tensor([[[ 1.25000,  1.62500],
         [ 2.00000,  3.75000],
         [ 4.12500,  2.87500]],
        [[ 1.87500,  3.81250],
         [ 3.87500,  2.81250],
         [ 3.68750,  7.43750]],
        [[ 3.62500,  2.81250],
         [ 4.87500,  6.18750],
         [11.65625, 10.18750]]], dtype=torch.float16)
You have two anchor computations that both look similar, but they do not correspond to your autoanchor model output. Since your code is out of date, there are likely issues with it that have already been resolved. I would git clone the most recent repo and repeat your experiment, using all default settings (changing nothing except with and without autoanchor). It looks like you only need about 30 training epochs to make a comparison.
		</comment>
		<comment id='12' author='123456789mojtaba' date='2020-07-28T01:09:54Z'>
		Hi &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Yes you are right. Thank you :). The problem is solved with most recent pull. Results are good with latest git pull.
The problem in v2.0 was with the reversed anchors and k means computed anchors were divided with wrong stride value (instead of 8, 16, 32 it was divided with 32, 16, 8). However, I am also able to get the perfect result in v2.0 by changing following line with,



yolov5/utils/utils.py


         Line 99
      in
      7f8471e






 m.anchors[:] = new_anchors.clone().view_as(m.anchors) / m.stride.to(m.anchors.device).view(-1, 1, 1)  # loss 





m.anchors[:] = new_anchors.clone().view_as(m.anchors) / torch.flip(m.stride.to(m.anchors.device).view(-1, 1, 1),[0,1]) # loss
		</comment>
		<comment id='13' author='123456789mojtaba' date='2020-07-28T01:19:58Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 I don't understand. Are you saying that utils.py L99 in &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/7f8471eaebe4b192c5e6ab4e5c821d91e43cb4fe&gt;7f8471e&lt;/denchmark-link&gt;
 (current master) needs changing?
		</comment>
		<comment id='14' author='123456789mojtaba' date='2020-07-28T01:40:37Z'>
		L99 is the line that divides the anchors from pixels to strides. L100 right after it is supposed to check the anchor order and reverse them if necessary. Perhaps this region of the code should be updated to make it more robust to different scenarios. For now it should work fine with the public architectures offered (I'm training several models currently that rely on autoanchor and they are training correctly).
		</comment>
		<comment id='15' author='123456789mojtaba' date='2020-07-28T10:20:29Z'>
		Hi &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
. Sorry for creating misunderstanding. . The problem was when I was using old version and yolov5s.yaml was using the following order of anchors,
&lt;denchmark-code&gt;# anchors
anchors:
  - [116,90, 156,198, 373,326]  # P5/32
  - [30,61, 62,45, 59,119]  # P4/16
  - [10,13, 16,30, 33,23]  # P3/8
&lt;/denchmark-code&gt;

So, L100 in utils.py will correct the order but I guess it should be done before L99 and then divide it with correct stride value (correct me if I'm wrong).
In my old version of repo, L99 had following values for the tensor, where it is necessary to flip either dividing tensor or new anchor tensor.
&lt;denchmark-code&gt;&gt;&gt; m.stride.to(m.anchors.device).view(-1, 1, 1)
&gt;&gt; tensor([[[32.]],

        [[16.]],

        [[ 8.]]])
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;&gt;&gt; new_anchors.clone().view_as(m.anchors)
&gt;&gt; tensor([[[  4.79442,   4.32408],
         [  7.46562,   7.09048],
         [ 13.14909,  10.09316]],

        [[  7.94588,  17.91208],
         [ 20.93719,  16.97507],
         [ 19.46055,  43.18595]],

        [[ 35.97452,  27.59841],
         [ 63.15837,  45.87284],
         [112.93896,  87.99326]]])
&lt;/denchmark-code&gt;

After L99
&lt;denchmark-code&gt;&gt;&gt; tensor([[[ 0.14983,  0.13513],
         [ 0.23330,  0.22158],
         [ 0.41091,  0.31541]],

        [[ 0.49662,  1.11951],
         [ 1.30857,  1.06094],
         [ 1.21628,  2.69912]],

        [[ 4.49682,  3.44980],
         [ 7.89480,  5.73411],
         [14.11737, 10.99916]]])
&lt;/denchmark-code&gt;

After L100
&lt;denchmark-code&gt;tensor([[[ 4.49682,  3.44980],
         [ 7.89480,  5.73411],
         [14.11737, 10.99916]],

        [[ 0.49662,  1.11951],
         [ 1.30857,  1.06094],
         [ 1.21628,  2.69912]],

        [[ 0.14983,  0.13513],
         [ 0.23330,  0.22158],
         [ 0.41091,  0.31541]]])
&lt;/denchmark-code&gt;

So do you see the problem? Anchors were divided by the wrong value and check_anchor_order at L100 only changes its order.
		</comment>
		<comment id='16' author='123456789mojtaba' date='2020-07-28T18:06:47Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 yes I think believe you are correct that we should adjust the order in conjunction with the strides to keep them both synchronized. The evolved anchors are sorted from small to large before being attached to the model and then divided by stride, which in v2.0 model yamls is also always small to large.
But I just finished my training with a v2.0 autoanchor model, and while the training mAPs performed well (better than the official model actually), when I test the saved model I get about half the mAP expected. So it seems something is still not quite right.
		</comment>
		<comment id='17' author='123456789mojtaba' date='2020-07-28T19:12:56Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 I've taken a quick look, and am very confused about what could be wrong. The same EMA gets passed to test.py during training as is saved each epoch, so there should not be any differences. If the EMA performs at x mAP during training then test.py should produce the same results independently.
Just to be clear, were you able to train a v2.0 model using Autoanchor, and observed good training results, and also, separately once training was complete observed good test.py results using best.pt or last.pt?
		</comment>
		<comment id='18' author='123456789mojtaba' date='2020-07-29T07:44:30Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Yes I completed training and I observe that the results are almost similar to yolov5s trained on the previous version without autoanchor. Just little boost on the specific class category which is more frequent than other object categories in my dataset.  Results.png is almost same as the one I have posted earlier in this issue except for minimum objectness for both training and validation is 0.1 instead of 0.05
		</comment>
		<comment id='19' author='123456789mojtaba' date='2020-07-29T17:16:08Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 ok thanks. Maybe the problem is only in my dev branch then.
		</comment>
		<comment id='20' author='123456789mojtaba' date='2020-08-05T19:54:56Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 trying to figure out the status of this issue. Are you still seeing any problems in the current code or would you say the original issue appears resolved now?
		</comment>
		<comment id='21' author='123456789mojtaba' date='2020-08-07T15:04:08Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I don't see any problem now. I even tried altering the order of anchors in  file and it worked as expected.
		</comment>
		<comment id='22' author='123456789mojtaba' date='2020-08-07T17:02:30Z'>
		&lt;denchmark-link:https://github.com/priteshgohil&gt;@priteshgohil&lt;/denchmark-link&gt;
 ok, great, thanks!
		</comment>
		<comment id='23' author='123456789mojtaba' date='2020-09-07T00:38:57Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='24' author='123456789mojtaba' date='2020-11-12T12:35:10Z'>
		TODO removed as original issue appears resolved.
		</comment>
	</comments>
</bug>