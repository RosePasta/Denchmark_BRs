<bug id='141' author='zidanexu' open_date='2020-06-20T02:30:46Z' closed_time='2020-08-08T00:31:07Z'>
	<summary>--resume bug</summary>
	<description>
&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  I training with default command by multi GPU,  When run about 255 epoch, the program is broken,So, I  use --resume option to begin continue.  but I found the difference about MAP is bigger. eg:  at broken point(200 epoch), the MAP(0,5-0,95) is 0.302,  but after resume (256 epoch) the MAP(0.5-0.95) decrease only 0.16 quickly.  I forget some things in use?
&lt;denchmark-link:https://user-images.githubusercontent.com/54395639/85189465-f66dd400-b2e1-11ea-86ce-6e51ff49a268.png&gt;&lt;/denchmark-link&gt;

and I wonder , when I kill the program and resume again , the MAP(0.5-0.95) change normal 0.3 again.
&lt;denchmark-link:https://user-images.githubusercontent.com/54395639/85189436-c32b4500-b2e1-11ea-8db6-9f3655c97f3b.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='zidanexu' date='2020-06-20T18:04:12Z'>
		&lt;denchmark-link:https://github.com/zidanexu&gt;@zidanexu&lt;/denchmark-link&gt;
 --resume has unresolved issues, possibly related to LR continuity. I doubt they will be resolved anytime soon, so I recommend you restart from 0 and train fully.
If you come up with a fix though please let us know!
		</comment>
		<comment id='2' author='zidanexu' date='2020-06-22T11:53:00Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

I try  save the state_dict of schedular  like this
&lt;denchmark-link:https://user-images.githubusercontent.com/54395639/85284410-bd8c5580-b4c1-11ea-8e41-787aaaf6dd52.png&gt;&lt;/denchmark-link&gt;

and load from weight when resume like below
&lt;denchmark-link:https://user-images.githubusercontent.com/54395639/85284439-c8df8100-b4c1-11ea-9c65-e3a283fe1496.png&gt;&lt;/denchmark-link&gt;

the same I disable your option and change the situation like below
&lt;denchmark-link:https://user-images.githubusercontent.com/54395639/85284487-de54ab00-b4c1-11ea-8f59-232e74891731.png&gt;&lt;/denchmark-link&gt;

.  there are some positive effect but not completly fixed .
do you think  EMA function have some contribution about this difference?
		</comment>
		<comment id='3' author='zidanexu' date='2020-06-22T18:01:05Z'>
		&lt;denchmark-link:https://github.com/zidanexu&gt;@zidanexu&lt;/denchmark-link&gt;
 yes, that is a good point. ema will play a part. The ema weights are used for all testing and checkpointing, so both the normal model and the ema model must be saved during all checkpoints for resume to function correctly, in theory. Otherwise the resumed ema will be building off of the saved ema rather than the normal model.
Honestly, resuming is much more headache than it's worth. My advice is to never resume.
		</comment>
		<comment id='4' author='zidanexu' date='2020-06-23T19:10:44Z'>
		&lt;denchmark-link:https://github.com/zidanexu&gt;@zidanexu&lt;/denchmark-link&gt;
 was referencing this issue in a recent PR. I think I will remove all official support for resuming training, as it has never, ever worked correctly, either here or in ultralytics/yolov3 and it is creating much more confusion and headaches than it's worth.
		</comment>
		<comment id='5' author='zidanexu' date='2020-07-02T01:32:01Z'>
		so, has it solved now?
		</comment>
		<comment id='6' author='zidanexu' date='2020-07-02T01:53:30Z'>
		&lt;denchmark-link:https://github.com/iamastar88&gt;@iamastar88&lt;/denchmark-link&gt;
 --resume runs without error, but I would strongly advise against using it, as it does not seamlessly do what it says properly. If possible, you should always train fully from start to finish in one go.
		</comment>
		<comment id='7' author='zidanexu' date='2020-08-01T05:22:18Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='8' author='zidanexu' date='2020-09-10T19:06:21Z'>
		UPDATE: --resume if fully functional now, it has graduated into an officially supported feature. You use it by itself with no arguments, or by pointing to a last.pt to resume from:
python train.py --resume  # resume from most recent last.pt
python train.py --resume runs/exp0/weights/last.pt  # resume from specific weights
		</comment>
	</comments>
</bug>