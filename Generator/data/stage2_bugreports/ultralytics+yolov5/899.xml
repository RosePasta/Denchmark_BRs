<bug id='899' author='jinfagang' open_date='2020-09-02T04:10:05Z' closed_time='2020-10-10T00:44:32Z'>
	<summary>onnx export break</summary>
	<description>
I can export onnx with previous code, but after pulled updates, when export got this error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "models/onnx_export.py", line 63, in &lt;module&gt;
    output_names=['output'])  # output_names=['output']
  File "/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py", line 208, in export
    custom_opsets, enable_onnx_checker, use_external_data_format)
  File "/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py", line 92, in export
    use_external_data_format=use_external_data_format)
  File "/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py", line 530, in _export
    fixed_batch_size=fixed_batch_size)
  File "/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py", line 409, in _model_to_graph
    _export_onnx_opset_version)
RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select

&lt;/denchmark-code&gt;

Any idea?
	</description>
	<comments>
		<comment id='1' author='jinfagang' date='2020-09-02T05:24:07Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 ONNX export shows success in the latest CI tests. Do you have ONNX 1.7 and PyTorch 1.6?
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/91935084-b774db00-eca1-11ea-8374-74faeddc713a.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jinfagang' date='2020-09-02T06:59:52Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  OK,. I rechecked, I am using torch.device('cuda') for exporting while you using cpu default.
The error got because there is an node using cpu.
I can export onnx out using cpu, but problem is the exported onnx model seems not same as using cuda device.
When I call python3 -m onnxsim a.onnx a_sim.onnx I got error:
&lt;denchmark-code&gt;onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Concat node. Name:'Concat_672' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/concat.cc:64 onnxruntime::common::Status onnxruntime::ConcatBase::PrepareForCompute(onnxruntime::OpKernelContext*, const std::vector&lt;const onnxruntime::Tensor*&gt;&amp;, onnxruntime::Prepare&amp;) const inputs_n_rank == inputs_0_rank was false. Ranks of input data are different, cannot concatenate them. expected rank: 5 got: 6

&lt;/denchmark-code&gt;

Have u ever tried this? Well also, I export using
&lt;denchmark-code&gt;return (torch.cat(z, 1), x)
&lt;/denchmark-code&gt;

not return x.
		</comment>
		<comment id='3' author='jinfagang' date='2020-09-02T07:15:41Z'>
		The 'cuda' device problem is weird since I can get pytorch output, which means it can be forward on pytorch side in cuda mode:
&lt;denchmark-code&gt;output shape: torch.Size([1, 50400, 18])

&lt;/denchmark-code&gt;

It got this error right in onnx.export
		</comment>
		<comment id='4' author='jinfagang' date='2020-09-02T15:49:02Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 always export in CPU mode.
		</comment>
		<comment id='5' author='jinfagang' date='2020-09-03T01:28:09Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 question is, cpu exported model not right
		</comment>
		<comment id='6' author='jinfagang' date='2020-10-04T00:44:25Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>