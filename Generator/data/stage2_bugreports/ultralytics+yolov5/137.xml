<bug id='137' author='spinlud' open_date='2020-06-19T11:13:17Z' closed_time='2020-06-25T07:43:54Z'>
	<summary>Couldn't close file exception during training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I've tried to run (on cpu) the default training example with the coco128 dataset, but after some epochs I got an exception libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: Couldn't close file:
&lt;denchmark-link:https://user-images.githubusercontent.com/12783208/85126413-b9e0a080-b22d-11ea-9e0c-b6e5f4522ec5.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;To Reproduce (REQUIRED)&lt;/denchmark-h&gt;

python train.py --img-size 256 --batch 16 --epochs 5 --data ./data/coco128.yaml --cfg ./models/yolov5s.yaml --weights ''
Output:
libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: Couldn't close file
See screenshot above.
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Training should complete
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.

OS: MacOS 10.15.4
GPU No

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I am using conda environment where I have installed the required dependencies for this repo:
&lt;denchmark-link:https://user-images.githubusercontent.com/12783208/85126746-602ca600-b22e-11ea-85a1-1608c8221a01.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='spinlud' date='2020-06-19T11:13:55Z'>
		Hello &lt;denchmark-link:https://github.com/spinlud&gt;@spinlud&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;Jupyter Notebook&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov5&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart&gt;Google Cloud Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
If this is a custom model or data training question, please note that Ultralytics does not provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:

Cloud-based AI systems operating on hundreds of HD video streams in realtime.
Edge AI integrated into custom iOS and Android apps for realtime 30 FPS video inference.
Custom data training, hyperparameter evolution, and model exportation to any destination.

For more information please visit &lt;denchmark-link:https://www.ultralytics.com&gt;https://www.ultralytics.com&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='spinlud' date='2020-06-19T17:09:29Z'>
		&lt;denchmark-link:https://github.com/spinlud&gt;@spinlud&lt;/denchmark-link&gt;
 that's interesting. I sometimes see this locally as well on my macbook pro with conda environment, but I never see it in colab or docker. One of our unit tests is actually to train 3 epochs, test, and detect with both gpu and cpu, and these tests are currently passing in colab.
Perhaps setting the number of workers to 1 in train.py may resolve this, I'm not sure:



yolov5/train.py


        Lines 164 to 166
      in
      80b82e8






 # Dataloader 



 batch_size = min(batch_size, len(dataset)) 



 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8])  # number of workers 





In any case though, you should probably never train on cpu, there's no real use case for it, as a colab instance can train more in 1 hour than your cpu can in 1 day.
		</comment>
		<comment id='3' author='spinlud' date='2020-06-21T00:31:21Z'>
		&lt;denchmark-link:https://github.com/spinlud&gt;@spinlud&lt;/denchmark-link&gt;

run
conda uninstall matplotlib
More details seeÔºö &lt;denchmark-link:https://github.com/gammapy/gammapy/issues/2453&gt;gammapy/gammapy#2453&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='spinlud' date='2020-06-21T21:24:03Z'>
		
@spinlud that's interesting. I sometimes see this locally as well on my macbook pro with conda environment, but I never see it in colab or docker. One of our unit tests is actually to train 3 epochs, test, and detect with both gpu and cpu, and these tests are currently passing in colab.
Perhaps setting the number of workers to 1 in train.py may resolve this, I'm not sure:



yolov5/train.py


        Lines 164 to 166
      in
      80b82e8






 # Dataloader 



 batch_size = min(batch_size, len(dataset)) 



 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8])  # number of workers 





In any case though, you should probably never train on cpu, there's no real use case for it, as a colab instance can train more in 1 hour than your cpu can in 1 day.

Hi, thanks for the reply!
I've set number of workers to 1, but got the same error unfortunately:
batch_size = min(batch_size, len(dataset))
# nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8])  # number of workers
nw = 1  # hack
&lt;denchmark-link:https://user-images.githubusercontent.com/12783208/85235568-42219a00-b416-11ea-8b5b-ac56bf51a9c7.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='spinlud' date='2020-06-22T01:21:26Z'>
		Ok. If it is reproducible then you should probably raise an issue on the pytorch repo, as it seems to originate from the Pytorch Dataloader class.
		</comment>
		<comment id='6' author='spinlud' date='2020-06-24T17:57:05Z'>
		&lt;denchmark-link:https://github.com/spinlud&gt;@spinlud&lt;/denchmark-link&gt;
 I got it work if you set num workers = 0 instead of 1. Obviously not ideal, but seems to be an overall issue with the python multiprocessing library running on Catalina
		</comment>
		<comment id='7' author='spinlud' date='2020-06-24T18:32:06Z'>
		&lt;denchmark-link:https://github.com/mrk230&gt;@mrk230&lt;/denchmark-link&gt;
 see same effect on Catalina also. Agree it's some sort of python multiprocessing problem.
		</comment>
		<comment id='8' author='spinlud' date='2020-06-25T07:43:54Z'>
		Thank you guys!
		</comment>
	</comments>
</bug>