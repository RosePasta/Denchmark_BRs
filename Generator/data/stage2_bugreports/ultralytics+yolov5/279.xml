<bug id='279' author='oyrq' open_date='2020-07-03T08:42:24Z' closed_time='2020-07-04T18:50:35Z'>
	<summary>TypeError: can't pickle torch.distributed.ProcessGroupNCCL objects</summary>
	<description>
Hi，
I meet a problem:
Traceback (most recent call last):
File "train.py", line 394, in 
train(hyp)
File "train.py", line 331, in train
torch.save(ckpt, last)
File "/home/yy/anaconda3/lib/python3.6/site-packages/torch/serialization.py", line 328, in save
_legacy_save(obj, opened_file, pickle_module, pickle_protocol)
File "/home/yy/anaconda3/lib/python3.6/site-packages/torch/serialization.py", line 401, in _legacy_save
pickler.dump(obj)
TypeError: can't pickle torch.distributed.ProcessGroupNCCL objects
Thanks!
environment:
ubuntu 16.04
GPU 2080Ti *4
pytorch 1.4.0
	</description>
	<comments>
		<comment id='1' author='oyrq' date='2020-07-03T08:43:07Z'>
		Hello &lt;denchmark-link:https://github.com/oyrq&gt;@oyrq&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;Jupyter Notebook&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov5&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart&gt;Google Cloud Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
If this is a custom model or data training question, please note that Ultralytics does not provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:

Cloud-based AI systems operating on hundreds of HD video streams in realtime.
Edge AI integrated into custom iOS and Android apps for realtime 30 FPS video inference.
Custom data training, hyperparameter evolution, and model exportation to any destination.

For more information please visit &lt;denchmark-link:https://www.ultralytics.com&gt;https://www.ultralytics.com&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='oyrq' date='2020-07-03T09:12:29Z'>
		Hm, I got this too after pulling from latest release. I thought I broke something in my code, but I guess not.
		</comment>
		<comment id='3' author='oyrq' date='2020-07-03T09:44:34Z'>
		
Hm, I got this too after pulling from latest release. I thought I broke something in my code, but I guess not.

I think your code is broke, there are four GPUs running, but they have the same PID。
&lt;denchmark-link:https://user-images.githubusercontent.com/24582893/86456567-d4019f00-bd54-11ea-99e4-226837352e38.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='oyrq' date='2020-07-03T09:48:17Z'>
		What do you mean? This is running Single process DistributedDataParallel, so it should be the same PID.  I am working on Multi process DDP if that's what you're thinking about.
		</comment>
		<comment id='5' author='oyrq' date='2020-07-03T12:09:07Z'>
		I encountered the same problem here after pulling from the lastest version. Any ideas how to fix it?
		</comment>
		<comment id='6' author='oyrq' date='2020-07-03T13:15:57Z'>
		I would just suggest waiting a while, see if others have the same issue and if &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 would see this. It can be due to the most recent merge &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/e02a189a3a3cb5b3d9cc0b29b1b5de5b21b3d11a&gt;e02a189&lt;/denchmark-link&gt;
 . You can also use an earlier version. The 30th June was fine for me.
EDIT: Found out that this only happens for multiple GPU because of nccl backend. It works fine for single GPU. So you can run it by setting --device 0 or which ever single GPU ID.
		</comment>
		<comment id='7' author='oyrq' date='2020-07-03T14:04:38Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 Thanks for your suggestion. Branch of 30th June works fine for me.
		</comment>
		<comment id='8' author='oyrq' date='2020-07-03T16:42:01Z'>
		&lt;denchmark-link:https://github.com/GWwangshuo&gt;@GWwangshuo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;

i got this error with the new pull too, but my PR work, you guys can check it.
the problem may be in the new update in ema, i will try to find out what's going on with it
		</comment>
		<comment id='9' author='oyrq' date='2020-07-03T17:29:49Z'>
		&lt;denchmark-link:https://github.com/yxNONG&gt;@yxNONG&lt;/denchmark-link&gt;
 , I’m not sure what went wrong. I tried to look through code. The issue is with saving for multi gpu, but the only place that is related is with saving ckpt for ema
The only thing else in commits are update to ONNX
		</comment>
		<comment id='10' author='oyrq' date='2020-07-03T17:51:46Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 Hi guys. Unfortunately I don't have access to a multi-gpu machine to debug. If anyone here finds the problem please submit a PR.
Seems like the current patch is to train single-gpu. Unit tests (run on single GPU) are all passing currently.
python train.py --device 0
		</comment>
		<comment id='11' author='oyrq' date='2020-07-03T18:03:35Z'>
		To add a bit more detail, this issue likely originates in recent pushes to update the EMA code. I'll try to update the EMA handling to isolate it as single-GPU in all cases, as right now both the main model and the EMA are a confusing allowable mix of single GPU and DP.
We swapped test.py multigpu out last month for single-gpu FP16 testing during training, so I suppose this will go well with that change.
		</comment>
		<comment id='12' author='oyrq' date='2020-07-03T18:37:50Z'>
		May I ask if you still have the test.py for multi GPU or can reference it?
So, ema.ema.module means that its distributed right? In which part does it become like that?
I only see that we pass model to Ema, and it creates deep copy called ema.ema . Where does module come from?
		</comment>
		<comment id='13' author='oyrq' date='2020-07-03T18:54:20Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 unit tests are here. We run these in a Colab notebook. You can modify the train device to multi-gpu i.e. 0,1. Be warned this will delete your default yolov5 directory if it exists, so you should run from a subdirectory.
# Unit tests
rm -rf yolov5 &amp;&amp; git clone https://github.com/ultralytics/yolov5 &amp;&amp; cd yolov5
export PYTHONPATH="$PWD" # to run *.py. files in subdirectories
pip install -r requirements.txt onnx
python3 -c "from utils.google_utils import *; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f', 'coco128.zip')" &amp;&amp; mv ./coco128 ../
for x in yolov5s yolov5m yolov5l yolov5x # models
do
  python train.py --weights $x.pt --cfg $x.yaml --epochs 4 --img 320 --device 0,1  # train
  for di in 0 cpu # inference devices
  do
    python detect.py --weights $x.pt --device $di  # detect official
    python detect.py --weights weights/last.pt --device $di  # detect custom
    python test.py --weights $x.pt --device $di # test official
    python test.py --weights weights/last.pt --device $di # test custom
  done
  python models/yolo.py --cfg $x.yaml # inspect
  python models/export.py --weights $x.pt --img 640 --batch 1 # export
done
		</comment>
		<comment id='14' author='oyrq' date='2020-07-03T18:58:54Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 and everyone, I just pushed an EMA update which may or may not resolve this issue. The updates 1) creates and maintains the EMA as a single-device model at all times, and passes it to test.py this way and to checkpoint saving this way, and 2) reverts the EMA to FP16 to reduce device 0 GPU memory usage slightly.
This passes all single-gpu unit tests above, though as I said before someone with a multi-gpu machine should run the tests themselves to verify.
		</comment>
		<comment id='15' author='oyrq' date='2020-07-03T19:47:29Z'>
		Right now, i only have one gpu available, so I will test multiple later, but I got weird error when running it on single.
Calling python train.py --weights yolov5s.pt --epochs 4 --img 320 --device 1 modified from the UnitTest after pip install ..
&lt;denchmark-code&gt;Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
       0/3     1.75G    0.1334    0.1112   0.04518    0.2898       235       320
               Class      Images     Targets           P           R      mAP@.5
Traceback (most recent call last):
  File "train.py", line 394, in &lt;module&gt;
    train(hyp)
  File "train.py", line 299, in train
    dataloader=testloader)
  File "test.py", line 97, in test
    output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres, merge=merge)
  File "utils/utils.py", line 605, in non_max_suppression
    i = torchvision.ops.boxes.nms(boxes, scores, iou_thres)
  File "python3.7/site-packages/torchvision/ops/boxes.py", line 35, in nms
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
RuntimeError: Trying to create tensor with negative dimension -1754807296: [-1754807296]
&lt;/denchmark-code&gt;

Second time I ran it, I got RuntimeError: Trying to create tensor with negative dimension -1754807296: [-1754807296]
Third time, RuntimeError: Trying to create tensor with negative dimension -1754807296: [-1754807296]
EDIT: My device 0 is busy. Could it be related? I also tried using CUDA_VISIBLE_DEVICES=1, same result.
		</comment>
		<comment id='16' author='oyrq' date='2020-07-03T19:58:09Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 I don't know. I can't reproduce on Colab. May be specific to your environment? The tests are intended for Colab or Docker. Anything outside of that I can't speak for.
		</comment>
		<comment id='17' author='oyrq' date='2020-07-03T23:58:41Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 got your same error in the docker container, but not on colab strangely enough. When I reverted EMA to FP32 this removed the docker error.
		</comment>
		<comment id='18' author='oyrq' date='2020-07-04T02:26:45Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , thanks. That commit fixed the Single GPU issue for me. I also tested it on Colab, no issue.
For Multiple GPU, the same error persists unfortunately.
&lt;denchmark-code&gt;              Class      Images     Targets           P           R      mAP@.5
                 all         128         929       0.135       0.633       0.343       0.145
Traceback (most recent call last):
  File "train.py", line 394, in &lt;module&gt;
    train(hyp)
  File "train.py", line 331, in train
    torch.save(ckpt, last)
  File "torch/serialization.py", line 370, in save
    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
  File "torch/serialization.py", line 443, in _legacy_save
    pickler.dump(obj)
TypeError: can't pickle torch.distributed.ProcessGroupNCCL objects
&lt;/denchmark-code&gt;

Single: python train.py --weights yolov5s.pt --epochs 4 --img 320 --device 0
Double: python train.py --weights yolov5s.pt --epochs 4 --img 320 --device 0,3
		</comment>
		<comment id='19' author='oyrq' date='2020-07-04T03:14:49Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 Ok. If you find a solution that works please submit a PR.
		</comment>
		<comment id='20' author='oyrq' date='2020-07-04T03:24:45Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 why don't you try to move the EMA definition up before the DDP init?
		</comment>
		<comment id='21' author='oyrq' date='2020-07-04T03:48:14Z'>
		
@NanoCode012 why don't you try to move the EMA definition up before the DDP init?

Sure, that’s one of the things I looked to do. One other thing is whether I can move the parameters of model like model.nc before the DDP wrapper? I read on pytorch that it can cause unexpected behaviors when modifying a model’s parameters after DDP wrapped.
&lt;denchmark-link:https://github.com/pytorch/pytorch/blob/15864d170384f584e9c8a06118781e9817ef8cc5/torch/nn/parallel/distributed.py#L138&gt;https://github.com/pytorch/pytorch/blob/15864d170384f584e9c8a06118781e9817ef8cc5/torch/nn/parallel/distributed.py#L138&lt;/denchmark-link&gt;

Though, I was hesitant on doing it as I would have to modify all calls on model in the training loop?
		</comment>
		<comment id='22' author='oyrq' date='2020-07-04T03:52:29Z'>
		Ok! parameters in that context means model weights that have gradients. The values atta he’s to the model after DDP are class attributes.
		</comment>
		<comment id='23' author='oyrq' date='2020-07-04T09:00:10Z'>
		I've tried a few things.

Moved EMA above DDP wrapper
Moved DDP wrapper below EMA (had to re-assign attrib)
Changed torch.save(ckpt, last) to torch.save(ema.ema, last)
Setting only device cuda:0 to be allowed to save ckpt

It always errors on line , so I looked into ema. I don't see any major change (please correct me), besides refactor from &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/f02481c73a4f8e3dbc0ae809b50310c0b2d700c9&gt;f02481c&lt;/denchmark-link&gt;
 commit till now.
I did some print for k,v pairs in module.__dict__ and I notice that keys present when running in 1 gpu, differ from 2 or more.
3 keys from single gpu aren't present in multi-gpu. On the other hand, multiple keys from multi-gpu aren't present in single with one being very interesting.
&lt;denchmark-code&gt;# 1 gpu
md with val {'nc': 80, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'anchors': [[11...
save with val [4, 6, 10, 14, 17, 18, 21, 22]
stride with val tensor([32., 16.,  8.])

# Multi gpu
process_group with val &lt;torch.distributed.ProcessGroupNCCL object at 0x7f20ce4c6930&gt; # Should we save this?
....
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='oyrq' date='2020-07-04T17:39:12Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

Thank you very much for your help. Your work is so good that I can use &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/e02a189a3a3cb5b3d9cc0b29b1b5de5b21b3d11a&gt;e02a189&lt;/denchmark-link&gt;
 to work normally. I look forward to your solving this bug.
		</comment>
		<comment id='25' author='oyrq' date='2020-07-04T17:49:25Z'>
		&lt;denchmark-link:https://github.com/oyrq&gt;@oyrq&lt;/denchmark-link&gt;
 , If you must use multi gpu immediately, you can simply clone my branch as &lt;denchmark-link:https://github.com/yxNONG&gt;@yxNONG&lt;/denchmark-link&gt;
  and I have tested it out already.
Or you can also fix it yourself. It’s just one line.
If you encountered a bug, it’d be great to mention it to the PR. Thanks.
		</comment>
		<comment id='26' author='oyrq' date='2020-07-04T18:45:55Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 thanks for running the experiments! This process_group should definitely not be added, it must be the problem. I suppose we could insert a check into the EMA attribute update to prevent it from being added. Can you try this?
    def update_attr(self, model):
        # Update EMA attributes
        for k, v in model.__dict__.items():
            if not k.startswith('_') and k != 'module' and k != 'process_group':
                setattr(self.ema, k, v)
		</comment>
	</comments>
</bug>