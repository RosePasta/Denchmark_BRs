<bug id='1248' author='AWilco' open_date='2020-10-30T12:28:47Z' closed_time='2020-10-30T16:27:07Z'>
	<summary>Cannot load an exported torchscript model using torch.jit.load</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When attempting to load an exported torchscript model of yolov5s.pt using test.py (or my own implementation which is copied from this) the model cannnot be loaded. The python error log is:
Traceback (most recent call last):
File "test.py", line 290, in 
save_conf=opt.save_conf,
File "test.py", line 59, in test
model = attempt_load(weights, map_location=device)  # load FP32 model
File "/content/yolov5/models/experimental.py", line 137, in attempt_load
model.append(torch.load(w, map_location=map_location)['model'].float().fuse().eval())  # load FP32 model
File "/usr/local/lib/python3.6/dist-packages/torch/jit/init.py", line 2106, in getitem
return self.forward_magic_method("getitem", idx)
File "/usr/local/lib/python3.6/dist-packages/torch/jit/init.py", line 2099, in forward_magic_method
raise NotImplementedError()
NotImplementedError
&lt;denchmark-h:h2&gt;To Reproduce (REQUIRED)&lt;/denchmark-h&gt;

See &lt;denchmark-link:https://colab.research.google.com/drive/1xTOgWqoHQ3tauH-YuAkNctLU4YrBH8JI?usp=sharing&gt;https://colab.research.google.com/drive/1xTOgWqoHQ3tauH-YuAkNctLU4YrBH8JI?usp=sharing&lt;/denchmark-link&gt;
 as a colab which generates the error at the "Test Export and Import" section
(this has been tested with a local download of the source code from this morning and also using the google colab result)
Input:
&lt;denchmark-code&gt;python models/export.py --weights yolov5s.pt --img 640 --batch 1

python test.py --data "coco128.yaml" --weights "yolov5s.torchscript.pt"
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test.py", line 290, in &lt;module&gt;
    save_conf=opt.save_conf,
  File "test.py", line 59, in test
    model = attempt_load(weights, map_location=device)  # load FP32 model
  File "/content/yolov5/models/experimental.py", line 137, in attempt_load
    model.append(torch.load(w, map_location=map_location)['model'].float().fuse().eval())  # load FP32 model
  File "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py", line 2106, in __getitem__
    return self.forward_magic_method("__getitem__", idx)
  File "/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py", line 2099, in forward_magic_method
    raise NotImplementedError()
NotImplementedError
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

The model is successfully loaded by torch.jit.load
(Note that torch.load automatically redirects to torch.jit.load when called in test.py)
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.

OS: Ubuntu
GPU 1050

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I have tried with a custom trained model based on cfg/yolov5s.yml also.
	</description>
	<comments>
		<comment id='1' author='AWilco' date='2020-10-30T12:29:31Z'>
		Hello &lt;denchmark-link:https://github.com/AWilco&gt;@AWilco&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;Jupyter Notebook&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov5&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart&gt;Google Cloud Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
If this is a custom model or data training question, please note Ultralytics does not provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:

Cloud-based AI systems operating on hundreds of HD video streams in realtime.
Edge AI integrated into custom iOS and Android apps for realtime 30 FPS video inference.
Custom data training, hyperparameter evolution, and model exportation to any destination.

For more information please visit &lt;denchmark-link:https://www.ultralytics.com&gt;https://www.ultralytics.com&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='AWilco' date='2020-10-30T15:19:08Z'>
		&lt;denchmark-link:https://github.com/AWilco&gt;@AWilco&lt;/denchmark-link&gt;
 YOLOv5 *.pt checkpoints are python dictionaries, with the actual models located in .
import torch

ckpt = torch.load('yolov5s.pt')
print(ckpt.keys())
dict_keys(['epoch', 'best_fitness', 'training_results', 'model', 'optimizer'])
The exported models are directly saved to their respective formats (rather than being assigned values in a checkpoint dictionary), and are not intended to be replaceable with the exported files when running detection or testing.
		</comment>
		<comment id='3' author='AWilco' date='2020-10-30T16:27:07Z'>
		Thanks &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
. Sorry for labelling this as a bug when it is clearly my own confusion. I think I had already used things correctly but managed to confuse myself debugging other errors.
To confirm my understanding:
*.pt checkpoint files are portable across python projects that use pytorch. To use a checkpoint file in another project all you use is:
&lt;denchmark-code&gt;import torch
self.model = torch.load(opt["checkpoint.pt"], map_location=self.device)['model'].float().fuse().eval()
predictions = self.model(img_on_device)
&lt;/denchmark-code&gt;

Torchscript files would be fed into the torchscript compiler to use the model in a C++ project, but if you're staying in python and pytorch there's no reason to use it.
		</comment>
	</comments>
</bug>