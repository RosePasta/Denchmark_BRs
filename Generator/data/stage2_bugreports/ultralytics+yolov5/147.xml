<bug id='147' author='ouening' open_date='2020-06-20T17:24:36Z' closed_time='2020-08-12T00:31:35Z'>
	<summary>TypeError: forward() missing 1 required positional argument: 'x'</summary>
	<description>
Before submitting a bug report, please be aware that your issue must be reproducible with all of the following, otherwise it is non-actionable, and we can not help you:

Current repo: run git fetch &amp;&amp; git status -uno to check and git pull to update repo
Common dataset: coco.yaml or coco128.yaml
Common environment: Colab, Google Cloud, or Docker image. See https://github.com/ultralytics/yolov5#reproduce-our-environment

If this is a custom dataset/training question you must include your train*.jpg, test*.jpg and results.png figures, or we can not help you. You can generate these with utils.plot_results().
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi, I'm training yolov5 using custom dataset with 8 classes. The dataset is in other directory (does it necessary to be next to the /yolov5 directory? ).  Structure of dataset is:
&lt;denchmark-code&gt; |----CustomImages
    ‚îú‚îÄ‚îÄ classes.txt
    ‚îú‚îÄ‚îÄ /images # store *.png
    ‚îú‚îÄ‚îÄ /labels  # store *.txt
    ‚îú‚îÄ‚îÄ test.txt
    ‚îú‚îÄ‚îÄ train.txt
    ‚îú‚îÄ‚îÄ trainval.txt
    ‚îú‚îÄ‚îÄ val.txt
&lt;/denchmark-code&gt;

Content of yml file is:
&lt;denchmark-code&gt;# train and val datasets (image directory or *.txt file with image paths)
train: /path/to/trainval.txt
val: /path/to/test.txt

# number of classes
nc: 8

# class names
names: ['screw_drive',
'blade',
'knife',
'scissors',
'board_marker',
'mobile_phone',
'wireless_mouse',
'water_bottle']
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce (REQUIRED)&lt;/denchmark-h&gt;

Input:
&lt;denchmark-code&gt;python3 train.py --img 640 --batch 8 --epochs 5 --data ./data/terahertz.yml --cfg ./models/yolov5s.yaml --weights ''
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;Starting training for 5 epochs...

     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
       0/4    0.394G    0.1007   0.03779   0.06582    0.2043         3       640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:21&lt;00:00,  4.80it/s]
               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 26/27 [00:05&lt;00:00,  5.09it/s]
Traceback (most recent call last):
  File "train.py", line 403, in &lt;module&gt;
    train(hyp)
  File "train.py", line 308, in train
    fast=epoch &lt; epochs / 2)
  File "/media/gaoya/disk/Documents/zhouwenqing/ÊØï‰∏öËÆæËÆ°/3_models/yolov5/yolov5/test.py", line 103, in test
    inf_out, train_out = model(img, augment=augment)  # inference and training outputs
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/distributed.py", line 449, in forward
    outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/distributed.py", line 474, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/usr/local/lib/python3.6/dist-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
TypeError: Caught TypeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'x'

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

It should train normally, but it failed after the first epoch test, I don't know why? Is it beacuse the dataset is not next to /yolov5?
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.

OS: Ubuntu18.04
GPU: RTX 2080 (8G)

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='ouening' date='2020-06-20T17:25:16Z'>
		Hello &lt;denchmark-link:https://github.com/ouening&gt;@ouening&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;Jupyter Notebook&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb&gt;&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov5&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart&gt;Google Cloud Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
If this is a custom model or data training question, please note that Ultralytics does not provide free personal support. As a leader in vision ML and AI, we do offer professional consulting, from simple expert advice up to delivery of fully customized, end-to-end production solutions for our clients, such as:

Cloud-based AI systems operating on hundreds of HD video streams in realtime.
Edge AI integrated into custom iOS and Android apps for realtime 30 FPS video inference.
Custom data training, hyperparameter evolution, and model exportation to any destination.

For more information please visit &lt;denchmark-link:https://www.ultralytics.com&gt;https://www.ultralytics.com&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='ouening' date='2020-06-20T18:26:56Z'>
		&lt;denchmark-link:https://github.com/ouening&gt;@ouening&lt;/denchmark-link&gt;
 if this occurs on your data and not coco128.yaml, then the cause will be your data. This guide might help: &lt;denchmark-link:https://blog.roboflow.ai/how-to-train-yolov5-on-a-custom-dataset/&gt;https://blog.roboflow.ai/how-to-train-yolov5-on-a-custom-dataset/&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='ouening' date='2020-06-20T19:00:39Z'>
		Thank you! The same dataset can be trained using yolov3 successfully. I'll try later
		</comment>
		<comment id='4' author='ouening' date='2020-06-28T21:56:22Z'>
		I get this error when using multi GPUs. The temporary fix is to use only a single GPU with --device 0
		</comment>
		<comment id='5' author='ouening' date='2020-06-28T22:56:54Z'>
		&lt;denchmark-link:https://github.com/JustinMBrown&gt;@JustinMBrown&lt;/denchmark-link&gt;
 thanks for the feedback. Does this error also appear in the docker container? If so please give us instructions to reproduce this error, ideally on a dataset we can access like coco128.yaml. Thanks!
		</comment>
		<comment id='6' author='ouening' date='2020-06-29T04:58:26Z'>
		I also experienced a similar issue as Justin when training on my custom dataset. However, I got quite peculiar results.
When training on 4 GPU, I got the forward error from the first post. This happened multiple times, so I tried to reduce the GPU count.
When training on 2 GPU, I got significantly worse mAP. 80-90(single)&gt;47(2 GPU). I will try to reproduce this error. As I've chosen to use single GPU after experiencing this.
Code to run for single: CUDA_VISIBLE_DEVICES=0 python train.py --img 640 --batch 16 --epochs 1000 --data ./data/obj.yaml --cfg ./models/yolov5x.yaml --weights yolov5x.pt --nosave --cache
I changed the cfg to follow my class count.
For multiple, I just changed the DEVICES variable .
This code was cloned on the 24th. I am now testing the newest pull on single GPU first, then multiple later.
		</comment>
		<comment id='7' author='ouening' date='2020-06-29T05:30:04Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 thanks. The reduced multi-gpu performance may be due to batch norm layers not synchronizing across gpus. I think there's a syncbatchnorm layer that might be used to fix this. We do all of our training on single GPUs, so it's not something we've explored in depth.
To use a specific cuda device:
python train.py --device 2
To use specific multiple devices:
python train.py --device 0,1,2,3
		</comment>
		<comment id='8' author='ouening' date='2020-06-29T06:40:22Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 .
I just tried to run with 4 GPUs(and 2 GPUs) via :
CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --img 640 --batch 16 --epochs 1000 --data ./data/obj.yaml --cfg ./models/yolov5x.yaml --weights yolov5x.pt --nosave --cache --device 0,1,2,3
python train.py --img 640 --batch 16 --epochs 1000 --data ./data/obj.yaml --cfg ./models/yolov5x.yaml --weights yolov5x.pt --nosave --cache --device 0,1,2,3
CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --img 640 --batch 16 --epochs 1000 --data ./data/obj.yaml --cfg ./models/yolov5x.yaml --weights yolov5x.pt --nosave --cache
A mix of with and without the devices argument.
They still gave the same error forward() missing 1 required.. when calculating mAP of first epoch.
I also noted that
&lt;denchmark-code&gt;UserWarning: Single-Process Multi-GPU is not the recommended mode for DDP. In this mode, each DDP instance operates on multiple devices and creates multiple module replicas within one process. The overhead of scatter/gather and GIL contention in every forward pass can slow down training. Please consider using one DDP instance per device or per module replica by explicitly setting device_ids or CUDA_VISIBLE_DEVICES. NB: There is a known issue in nn.parallel.replicate that prevents a single DDP instance to operate on multiple model replicas.
&lt;/denchmark-code&gt;

occurs no matter if I specify CUDA_VISIBLE_DEVICES or just --device
As of now, I can't run it with more than 1 GPU on the latest code base.
		</comment>
		<comment id='9' author='ouening' date='2020-06-29T11:45:06Z'>
		Hi, &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
!
Also got TypeError: forward() missing 1 required postional argument: 'x'
Custom single class dataset, 4x Tesla V100, batch 2, 4, 8, etc, input size 1024. Used 2, 3 and 4 GPUs. Now trying with single GPU.
		</comment>
		<comment id='10' author='ouening' date='2020-06-29T17:04:57Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/xevolesi&gt;@xevolesi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/JustinMBrown&gt;@JustinMBrown&lt;/denchmark-link&gt;
 thanks everyone. To be honest, the people that are best positioned to debug these multi-gpu bugs are you guys. We do most of our training on single-GPU. I've got 8 VMs running single-GPU experiments at the moment, as this is the most efficient cost structure in terms of FLOPS/$. This means I don't have much multi-GPU experience. The basic validation process for the repo is to run the following unit tests on a 2-T4 VM running our latest docker container, which tests everything in a mix of devices, and currently the unit tests are passing.
# Unit tests
rm -rf yolov5 &amp;&amp; git clone https://github.com/ultralytics/yolov5 &amp;&amp; cd yolov5
python3 -c "from utils.google_utils import *; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f', 'coco128.zip')" &amp;&amp; mv ./coco128 ../
for d in 0, 0,1 cpu # device
do
  for x in yolov5s #yolov5m yolov5l yolov5x # models
  do
    python detect.py --weights $x.pt --device $d
    python test.py --weights $x.pt --device $d
    
    python train.py --weights $x.pt --cfg $x.yaml --epochs 4 --img 320 --device $d
    
    python detect.py --weights weights/last.pt --device 0
    python test.py --weights weights/last.pt --device 0
    python test.py --weights weights/last.pt --device $d
  done
done
If you guys make any headway into these multi-gpu issues, please submit PRs for the benefit of everyone. Thank you!
		</comment>
		<comment id='11' author='ouening' date='2020-06-30T01:23:26Z'>
		
I also experienced a similar issue as Justin when training on my custom dataset. However, I got quite peculiar results.
When training on 4 GPU, I got the forward error from the first post. This happened multiple times, so I tried to reduce the GPU count.
When training on 2 GPU, I got significantly worse mAP. 80-90(single)&gt;47(2 GPU). I will try to reproduce this error. As I've chosen to use single GPU after experiencing this.
Code to run for single: CUDA_VISIBLE_DEVICES=0 python train.py --img 640 --batch 16 --epochs 1000 --data ./data/obj.yaml --cfg ./models/yolov5x.yaml --weights yolov5x.pt --nosave --cache
I changed the cfg to follow my class count.
For multiple, I just changed the DEVICES variable .
This code was cloned on the 24th. I am now testing the newest pull on single GPU first, then multiple later.

Thank you! I trained yolov5 successfully using single gpu.
		</comment>
		<comment id='12' author='ouening' date='2020-06-30T08:25:51Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , Unfortunately I am unable to modify the code directly as I don't have enough knowledge to do it yet.
One thing I noted was that multiple GPUs(8) works with detect.py and test.py but not with train.py, which is quite weird because the fail happens when train.py calls test.test to calculates mAP.
Another point is that I'm running it on a server which blocks other ports, so I'm not sure if it's related to
dist.init_process_group(backend='nccl',  # distributed backend
                                init_method='tcp://127.0.0.1:9999',  #&lt;-- other process may not be able to communicate
                                world_size=1,  # number of nodes
                                rank=0)  # node rank
I do not have a local machine with multiple GPU to test this on.
Also, as I was reading on doing DistributedDataParallel on Pytorch, I saw that it is needed to spawn multiple process, 1 for each GPU and more, but I do not see that code in train.py.
import torch.multiprocessing as mp

mp.spawn(train....) 
Also, setting the rank of the node by GPU isn't done dynamically, as you set it to rank=0 up above, so maybe all process sees itself as rank=0?
I apologize if I made any mistake when coming to these conclusions as I'm still learning.
		</comment>
		<comment id='13' author='ouening' date='2020-06-30T15:55:11Z'>
		Hi everybody, i am newbie but  experimenting with my own dataset i faced this issue. I simply do this for fix the issue: In train.py line 155 i commented the line model = torch.nn.parallel.DistributedDataParallel(model) and add
model=torch.nn.parallel.DataParallel(model) like this
#model = torch.nn.parallel.DistributedDataParallel(model)
model = torch.nn.parallel.DataParallel(model)
For purpose i suppose inefficient use of  my GPU units  on pytorch 1.5  with this change  but i get rid of this problem. I am still learning also
		</comment>
		<comment id='14' author='ouening' date='2020-06-30T16:00:52Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 yes,  I think the current best practices for multi-gpu in pytorch involves mp.spawn. There is a pytorch demo  that shows how to do this here:
&lt;denchmark-link:https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&gt;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&lt;/denchmark-link&gt;

One user benchmarked this implementation and saw much faster multi-gpu performance than the current implementation.
		</comment>
		<comment id='15' author='ouening' date='2020-06-30T16:51:32Z'>
		
Hi everybody, i am newbie but experimenting with my own dataset i faced this issue. I simply do this for fix the issue: In train.py line 155 i commented the line model = torch.nn.parallel.DistributedDataParallel(model) and add
model=torch.nn.parallel.DataParallel(model) like this
#model = torch.nn.parallel.DistributedDataParallel(model)
model = torch.nn.parallel.DataParallel(model)

&lt;denchmark-link:https://github.com/diego0718&gt;@diego0718&lt;/denchmark-link&gt;
 , When reading around, a lot of people suggested to stop using DataParallel and use DistributedDataParallel instead. Can you tell me if it ran faster( by how much) or had better performance?
&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , Thanks. I've been reading a few demos to get a grasp.  I will try and see if I can get it working
		</comment>
		<comment id='16' author='ouening' date='2020-07-01T09:55:02Z'>
		Hi everybody, i have found the problem for this bug.
The number of validation set must be divisible by 2 or 4 when using multi GPUs(2 or 4 GPUs). I get rid of this problem when using even number of valiation set.
		</comment>
		<comment id='17' author='ouening' date='2020-07-01T10:00:31Z'>
		
Hi everybody, i have found the problem for this bug.
The number of validation set must be divisible by 2 or 4 when using multi GPUs(2 or 4 GPUs). I get rid of this problem when using even number of valiation set.

Hi, &lt;denchmark-link:https://github.com/AIpakchoi&gt;@AIpakchoi&lt;/denchmark-link&gt;
, and thank you!
Do you mean the number of images in validation set must be divisible by 2 or 4 wrt to number of GPUs?
		</comment>
		<comment id='18' author='ouening' date='2020-07-01T11:02:45Z'>
		Hi, &lt;denchmark-link:https://github.com/xevolesi&gt;@xevolesi&lt;/denchmark-link&gt;
, you are right. In train.py line 155 i commented the line model = torch.nn.parallel.DistributedDataParallel(model) and add model=torch.nn.parallel.DistributedDataParallel(model,find_unused_parameters=True) like this
#model = torch.nn.parallel.DistributedDataParallel(model)
model = torch.nn.parallel.DistributedDataParallel(model,find_unused_parameters=True)
i have tried several experiments and found that the number of images in validation set must be divisible by 2 or 4 wrt to number of GPUs. This might be the problem of test.py.
		</comment>
		<comment id='19' author='ouening' date='2020-07-01T11:51:30Z'>
		&lt;denchmark-link:https://github.com/AIpakchoi&gt;@AIpakchoi&lt;/denchmark-link&gt;
 .Thanks! I quickly tested with 16 pic for validation and it worked. However, I didn't need to set  though.
Interestingly, I tried with an odd number for training set, and there were no problems.
Lastly, can you tell me if there was a performance drop? I experienced a large drop when using 2GPU compared to 1.
		</comment>
		<comment id='20' author='ouening' date='2020-07-02T03:49:41Z'>
		Hi,&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
, I did not train this code on COCO data set, i trained yolov5 on the KITTY data set for autonomous driving, so i didn't kown the performance on COCO when using 2GPU. But i can share you some tips with you, you can evolve hyp by running:
python train.py --epochs 100 --evolve.
and i have found the confidence of prediction is affected by batch_size, i got satifactory performance  when setting batch_size=8.
		</comment>
		<comment id='21' author='ouening' date='2020-07-02T03:54:11Z'>
		I tried to evolve with 300 epochs on one custom dataset I had. It didn't make a big difference compared to training it normally.
Although it did slightly increase the performance(for Precision) at the early stage compared to not evolving, but it evened out at the middle (my peak/plateau). I trained it for 1000 epochs on small dataset though, so maybe that's why.
I will try your advice on batch_size.
		</comment>
		<comment id='22' author='ouening' date='2020-07-02T04:00:49Z'>
		&lt;denchmark-link:https://github.com/NanoCode012&gt;@NanoCode012&lt;/denchmark-link&gt;
 you might want to see &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/392&gt;ultralytics/yolov3#392&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='ouening' date='2020-07-02T04:05:05Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , thank you for that. I read through it before. I got the below(the best one, others weren't as high fitness) and replaced it into the  hyp variable but it didn't change much. I also tried to change the number of times it evolve 10-&gt;20-&gt;30 till I got memory error.
&lt;denchmark-link:https://user-images.githubusercontent.com/9899957/86315189-bb5d9000-bc53-11ea-88f3-4189d3e232b5.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='ouening' date='2020-07-02T04:17:16Z'>
		Ah great! Your results show that momentum evolved to a much higher value than default üòÆ
		</comment>
		<comment id='25' author='ouening' date='2020-07-03T09:44:16Z'>
		
Hi, @xevolesi, you are right. In train.py line 155 i commented the line model = torch.nn.parallel.DistributedDataParallel(model) and add model=torch.nn.parallel.DistributedDataParallel(model,find_unused_parameters=True) like this
#model = torch.nn.parallel.DistributedDataParallel(model)
model = torch.nn.parallel.DistributedDataParallel(model,find_unused_parameters=True)
i have tried several experiments and found that the number of images in validation set must be divisible by 2 or 4 wrt to number of GPUs. This might be the problem of test.py.

I think you are right, test dataset should be divisible by batch-size, otherwise it will run to 'TypeError: forward() missing 1 required positional argument: 'x'.
		</comment>
		<comment id='26' author='ouening' date='2020-07-03T13:59:41Z'>
		Interestingly.. it doesn't happen anymore. Maybe a commit fixed it. &lt;denchmark-link:https://github.com/zoezhu&gt;@zoezhu&lt;/denchmark-link&gt;
 , are you on latest version?
		</comment>
		<comment id='27' author='ouening' date='2020-07-06T01:24:37Z'>
		
Interestingly.. it doesn't happen anymore. Maybe a commit fixed it. @zoezhu , are you on latest version?

I git clone it three days ago, but I run it with my own dataset, I don't know whether it cause this error.
		</comment>
		<comment id='28' author='ouening' date='2020-08-07T00:31:31Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>