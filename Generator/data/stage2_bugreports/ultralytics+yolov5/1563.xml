<bug id='1563' author='sammilei' open_date='2020-11-30T19:28:33Z' closed_time='2020-12-09T02:15:39Z'>
	<summary>Flag --save-txt returns higher mAP scores</summary>
	<description>
Repo: b6ed110
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;


!python test.py --weights "/content/train_80_20_small/weights/best.pt" --data /content/ms.yaml --img 416 --task test --verbose --save-txt --iou-thres=0.5
and
!python test.py --weights "/content/train_80_20_small/weights/best.pt" --data /content/ms.yaml --img 416 --task test --verbose --iou-thres=0.5 

Yield different mAP score
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

result of command1:
&lt;denchmark-code&gt;Namespace(augment=False, batch_size=32, conf_thres=0.001, data='/content/ms.yaml', device='', exist_ok=False, img_size=416, iou_thres=0.5, name='exp', project='runs/test', save_conf=False, save_json=False, save_txt=True, single_cls=False, task='test', verbose=True, weights=['/content/train_80_20_small/weights/best.pt'])
Using torch 1.7.0+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280MB)

Fusing layers... 
Model Summary: 232 layers, 7254609 parameters, 0 gradients
Scanning '/content/b/labels/val.cache' for images and labels... 1008 found, 0 missing, 1 empty, 0 corrupted: 100% 1008/1008 [00:00&lt;00:00, 6172056.11it/s]
               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 32/32 [00:14&lt;00:00,  2.23it/s]
                 all    1.01e+03    1.01e+03       0.729           1       0.996       0.996
                   cls1    1.01e+03         187       0.704           1       0.995       0.995
                   cls2   1.01e+03         405       0.878           1       0.996       0.996
                   cls3   1.01e+03         206       0.595           1       0.995       0.995
                   cls4.  1.01e+03         209       0.738           1       0.995       0.995
Speed: 1.4/2.8/4.2 ms inference/NMS/total per 416x416 image at batch-size 32
Results saved to runs/test/exp12
1008 labels saved to runs/test/exp12/labels
&lt;/denchmark-code&gt;

result of command2:
&lt;denchmark-code&gt;Namespace(augment=False, batch_size=32, conf_thres=0.001, data='/content/ms.yaml', device='', exist_ok=False, img_size=416, iou_thres=0.5, name='exp', project='runs/test', save_conf=False, save_json=False, save_txt=False, single_cls=False, task='test', verbose=True, weights=['/content/train_80_20_small/weights/best.pt'])
Using torch 1.7.0+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280MB)

Fusing layers... 
Model Summary: 232 layers, 7254609 parameters, 0 gradients
Scanning '/content/b/labels/val.cache' for images and labels... 1008 found, 0 missing, 1 empty, 0 corrupted: 100% 1008/1008 [00:00&lt;00:00, 6118463.72it/s]
               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 32/32 [00:10&lt;00:00,  2.99it/s]
                 all    1.01e+03    1.01e+03       0.729       0.941       0.939       0.581
                cls1      1.01e+03         187       0.708       0.952        0.95       0.577
                cls2     1.01e+03         405       0.875       0.914       0.951       0.582
                cls3     1.01e+03         206       0.595       0.971       0.917       0.592
                cls4     1.01e+03         209       0.737       0.928       0.939       0.574
Speed: 1.4/2.3/3.8 ms inference/NMS/total per 416x416 image at batch-size 32
Results saved to runs/test/exp9
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

YOLOv5 Colab tutorial
	</description>
	<comments>
		<comment id='1' author='sammilei' date='2020-11-30T19:42:32Z'>
		&lt;denchmark-link:https://github.com/sammilei&gt;@sammilei&lt;/denchmark-link&gt;
 thanks for the bug report. Yes I can understand how this could be confusing. We recently made a change to test.py so that --save-txt actually merges predictions with the existing labels to form hybrid autolabels as the new default behavior (if the dataset is unlabelled then the labels would be all predictions).
I should probably consider separating the two behaviors in the future.
For the time being the mAP you are seeing is actually correct, it is the hybrid autolabel mAP.
TODO: separate --save-txt from label merging behavior.
		</comment>
		<comment id='2' author='sammilei' date='2020-11-30T21:17:38Z'>
		I see. Thanks for the quick answer again!
I agree with making two different functions: 1. merge auto-labels with ground truth and prediction 2. a stack of prediction only
		</comment>
		<comment id='3' author='sammilei' date='2020-11-30T21:55:42Z'>
		&lt;denchmark-link:https://github.com/sammilei&gt;@sammilei&lt;/denchmark-link&gt;
 as a quick fix you can simply set lb=[] instead.


		</comment>
		<comment id='4' author='sammilei' date='2020-12-07T12:23:43Z'>
		Hi, I see the same issue here, and still finding it confusing.
One wants to run "test", to "fairly" test the ability of the model to predict the ground truth. Is this option equal to save-txt = False ?
		</comment>
		<comment id='5' author='sammilei' date='2020-12-08T14:58:05Z'>
		&lt;denchmark-link:https://github.com/bonorico&gt;@bonorico&lt;/denchmark-link&gt;
 thanks for your feedback! This item is on our TODO list to update, in the meantime test.py will produce the correct results when not using --save-txt.
		</comment>
		<comment id='6' author='sammilei' date='2020-12-09T02:15:12Z'>
		&lt;denchmark-link:https://github.com/sammilei&gt;@sammilei&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/bonorico&gt;@bonorico&lt;/denchmark-link&gt;
 this issue should be resolved now in PR &lt;denchmark-link:https://github.com/ultralytics/yolov5/pull/1646&gt;#1646&lt;/denchmark-link&gt;
.
This PR introduces hybrid autolabelling support in test.py. The auto-labelling options are now:

python test.py --save-txt: traditional auto-labelling
python test.py --save-hybrid: save hybrid autolabels, combining existing labels with new predictions before NMS (existing predictions given confidence=1.0 before NMS.
python test.py --save-conf: add confidences to any of the above commands

Regardless of any of the above settings, be aware that auto-labelling works best at very high confidence thresholds, i.e. 0.90 confidence, whereas mAP computation relies on very low confidence threshold, i.e. 0.001, to properly evaluate the area under the PR curve. The two activities are thus essentially mutually exclusive, there is no reason I know of to combine the two into a single test run.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/101565136-4a986d80-3981-11eb-8c08-7e384d8ad9e6.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='sammilei' date='2020-12-09T13:45:59Z'>
		Hi Glen, thank you for the answer. So basically, drop --save-txt for standard model-testing purposes only...
		</comment>
		<comment id='8' author='sammilei' date='2020-12-09T14:48:37Z'>
		&lt;denchmark-link:https://github.com/bonorico&gt;@bonorico&lt;/denchmark-link&gt;
 --save-txt will simply save labels, mAP is unaffected now.
--save-hybrid will save hybrid labels, with the correspondingly higher mAP being displayed.
		</comment>
	</comments>
</bug>