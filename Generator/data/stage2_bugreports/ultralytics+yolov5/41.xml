<bug id='41' author='jinfagang' open_date='2020-06-12T06:40:33Z' closed_time='2020-08-10T00:32:29Z'>
	<summary>onnx export RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same?</summary>
	<description>
Run onnx export got error:
&lt;denchmark-code&gt;RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same

&lt;/denchmark-code&gt;

weights trained on GPU, and converted both model and image to cuda device, why this error still happens
	</description>
	<comments>
		<comment id='1' author='jinfagang' date='2020-06-12T07:13:28Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 you should be able to export to onnx like this. Run this command from the /yolov5 directory.
export PYTHONPATH="$PWD" 
python models/onnx_export.py --weights ./weights/yolov5s.pt --img 640 640 --batch 1
		</comment>
		<comment id='2' author='jinfagang' date='2020-06-12T07:15:24Z'>
		Output should look like this. You might want to git pull also, we've made recent changes to onnx export.
...
  %416 = Shape(%285)
  %417 = Constant[value = &lt;Scalar Tensor []&gt;]()
  %418 = Gather[axis = 0](%416, %417)
  %419 = Shape(%285)
  %420 = Constant[value = &lt;Scalar Tensor []&gt;]()
  %421 = Gather[axis = 0](%419, %420)
  %422 = Shape(%285)
  %423 = Constant[value = &lt;Scalar Tensor []&gt;]()
  %424 = Gather[axis = 0](%422, %423)
  %427 = Unsqueeze[axes = [0]](%418)
  %430 = Unsqueeze[axes = [0]](%421)
  %431 = Unsqueeze[axes = [0]](%424)
  %432 = Concat[axis = 0](%427, %439, %440, %430, %431)
  %433 = Reshape(%285, %432)
  %434 = Transpose[perm = [0, 1, 3, 4, 2]](%433)
  return %output, %415, %434
}
Export complete. ONNX model saved to ./weights/yolov5s.onnx
View with https://github.com/lutzroeder/netron
		</comment>
		<comment id='3' author='jinfagang' date='2020-06-12T08:08:59Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 turns out my model is trained on GPU, and the model serialized as cuda device, so the input does not to cuda throw this error.
However, when I force it to cuda, it still got error oppsite, seems some code inside model, still using CPU tensor instead.
is there any special reason for using cpu tensor there?
		</comment>
		<comment id='4' author='jinfagang' date='2020-06-12T08:15:20Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/21303438/84480823-c01fcb80-acc7-11ea-8bc6-0903d3ef6345.png&gt;&lt;/denchmark-link&gt;

the generated onnx default seems enabled augmentation.
How to obtain boxes and scores and class from these outputs?
&lt;denchmark-link:https://user-images.githubusercontent.com/21303438/84480932-e5acd500-acc7-11ea-8b22-2f5c17d53b3b.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='jinfagang' date='2020-06-12T17:27:10Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 onnx export should only be done when the model is on cpu.
The netron image you show is correct. The boxes are part of the v5 architecture, they are not related to image augmentation during training.
At the moment onnx export stops at the output features. This is an example P3 output (smallest boxes) for 3 anchors with a grid size 40x24. The 85 features are xywh, objectness, and 80 class confidences.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/84529882-13c5f100-ac97-11ea-9082-41d0cdaabd22.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='jinfagang' date='2020-06-12T20:56:23Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 I ran into a cuda issue with an onnx export today, and pushed a fix &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/1e2cb6b48014f38be8337a976cb2fcc177edf898&gt;1e2cb6b&lt;/denchmark-link&gt;
 for this. This may or may not solve your original issue.
		</comment>
		<comment id='7' author='jinfagang' date='2020-06-13T07:39:24Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  So the output is same with yolov3 in your previous repo? I wanna access the outputs and accelerate it in tensorrt.
		</comment>
		<comment id='8' author='jinfagang' date='2020-06-15T06:13:02Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  Does anchors decodee process can also exported into onnx? So that it can be more end2end when transfer into other paltforms for inference?
		</comment>
		<comment id='9' author='jinfagang' date='2020-06-15T07:40:06Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 yes, this would be more useful. It is more complicated to implement though, especially if you want a clean onnx graph. We will try to add this in the future.
		</comment>
		<comment id='10' author='jinfagang' date='2020-06-15T08:53:00Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  I had a tiny experiments on this, it ends involved a ScatterND op there, this op is hard to convert to other platforms. If we want eliminate this op, postprocess scripts (Detect layer here) need re-written (only for export mode, in a more complicated way but can export and works perfectly)
		</comment>
		<comment id='11' author='jinfagang' date='2020-06-18T06:34:30Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 I also ran into this issue. Resolved it by converting the model to cuda and then saving the weights. Used those weights to convert to onnx model but I ran into some &lt;denchmark-link:https://forums.developer.nvidia.com/t/tensorrt-cannot-parse-onnx-model/128262/3&gt;issue&lt;/denchmark-link&gt;
 in converting onnx to tensorRT.
If you successfully converted the model to TensorRT please let me know how you did that.
Thanks
		</comment>
		<comment id='12' author='jinfagang' date='2020-06-18T07:13:07Z'>
		&lt;denchmark-link:https://github.com/makaveli10&gt;@makaveli10&lt;/denchmark-link&gt;
  I already converted the model to onnx and inferenced it on TensorRT.
&lt;denchmark-link:https://user-images.githubusercontent.com/21303438/84989172-da4d2400-b175-11ea-846f-8f1ef84c4e21.png&gt;&lt;/denchmark-link&gt;

However, this involved some special operations different than this repo does, and accordingly on TensorRT side needs some special operation to do. Overall, the TensorRT accelerated speed is about: 38ms with a 1280x768 input resolution, the performance is quite well:
&lt;denchmark-link:https://user-images.githubusercontent.com/21303438/84989361-226c4680-b176-11ea-9da9-57aa4f14095d.png&gt;&lt;/denchmark-link&gt;

you can add my wechat: jintianiloveu if you intested in this accelerate tech.
		</comment>
		<comment id='13' author='jinfagang' date='2020-06-18T07:29:52Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 great work! What is the speedup compared to using detect.py? What GPU are you using?
		</comment>
		<comment id='14' author='jinfagang' date='2020-06-18T08:28:30Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  Am using GTX1080Ti, speed tested on this. The speed measured included post process time (from engine forward to nms and copy data back to CPU etc.).  I think the speed is almost same with darknet version yolov4 converted tensorrt. (I previously tested with 800x800 input).
the speed can still be optimized by including all postprocess to cuda kernel and fp16 or int8 quantization
		</comment>
		<comment id='15' author='jinfagang' date='2020-06-18T15:40:28Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 amazing to see you got it running in such a short time. I'm able to convert the pth files to onnx format but I keep gettting this error when I try to convert to tensorrt6:
(Unnamed Layer* 0) [Slice]: slice is out of input range
While parsing node number 9 [Slice]:
3
If you have some pointers for me, I would really appreciate it. Connecting on WeChat is difficult for me cause I don't have an account and don't have a friend who can validate my new account.
		</comment>
		<comment id='16' author='jinfagang' date='2020-06-19T12:10:21Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 I dont have an account on WeChat. Neither I have a friend who can verify a new account. Can you please share your code to inference onnx on TensorRT somehow? I am getting incorrect outputs from the engine that I generated using onnx model.
		</comment>
		<comment id='17' author='jinfagang' date='2020-06-19T20:30:17Z'>
		&lt;denchmark-link:https://github.com/makaveli10&gt;@makaveli10&lt;/denchmark-link&gt;
 mind sharing how you were able to generate an onnx model that worked with TensorRT? Also, which version of TRT did you use?
		</comment>
		<comment id='18' author='jinfagang' date='2020-06-19T20:36:49Z'>
		&lt;denchmark-link:https://github.com/aj-ames&gt;@aj-ames&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/TrojanXu/yolov5-tensorrt&gt;https://github.com/TrojanXu/yolov5-tensorrt&lt;/denchmark-link&gt;

Let me know if you make any sort of progress please!
		</comment>
		<comment id='19' author='jinfagang' date='2020-06-19T21:02:14Z'>
		&lt;denchmark-link:https://github.com/makaveli10&gt;@makaveli10&lt;/denchmark-link&gt;
 thanks. I will update my findings here.
		</comment>
		<comment id='20' author='jinfagang' date='2020-06-20T03:25:19Z'>
		
@aj-ames https://github.com/TrojanXu/yolov5-tensorrt
Let me know if you make any sort of progress please!

I use this project. But I enconter the same error when I try to convert to tensorrt6:
[TensorRT] ERROR: (Unnamed Layer* 0) [Slice]: slice is out of input range
ERROR: Failed to parse the ONNX file.
If you have some pointers for me, I would really appreciate it.
		</comment>
		<comment id='21' author='jinfagang' date='2020-06-20T03:28:12Z'>
		
@glenn-jocher Am using GTX1080Ti, speed tested on this. The speed measured included post process time (from engine forward to nms and copy data back to CPU etc.). I think the speed is almost same with darknet version yolov4 converted tensorrt. (I previously tested with 800x800 input).
the speed can still be optimized by including all postprocess to cuda kernel and fp16 or int8 quantization

When I convert onnx to tensorrt,  I enconter the same error when I try to convert to tensorrt6:
[TensorRT] ERROR: (Unnamed Layer* 0) [Slice]: slice is out of input range
ERROR: Failed to parse the ONNX file.
I use tensorrt 6.0 and onnx 1.5.0 or 1.6.0, they are all not work.
If you have some pointers for me, I would really appreciate it.
		</comment>
		<comment id='22' author='jinfagang' date='2020-08-01T05:25:07Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>