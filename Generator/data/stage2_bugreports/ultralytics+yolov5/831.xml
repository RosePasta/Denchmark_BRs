<bug id='831' author='jinfagang' open_date='2020-08-24T09:09:46Z' closed_time='2020-09-30T00:41:36Z'>
	<summary>RuntimeError: Exporting the operator hardswish to ONNX opset version 11 is not supported. Please open a bug to request ONNX export support for the missing operator.</summary>
	<description>
Hardswish changed into exportable way:
&lt;denchmark-code&gt;class Hardswish(nn.Module):  # alternative to nn.Hardswish() for export
    @staticmethod
    def forward(x):
        # return x * F.hardsigmoid(x)
        return x * F.hardtanh(x + 3, 0., 6.) / 6.
&lt;/denchmark-code&gt;

still occured this error. ( i have changed conv in common.py)
	</description>
	<comments>
		<comment id='1' author='jinfagang' date='2020-08-25T03:07:36Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 hey there. I've been experimenting with export recently. This custom Hardswish() class provides alternatives to the pytorch nn.Hardswish() class.
The first line is best for CoreML export, the second is best for ONNX export. But in both cases you need to replace existing nn.Hardswish() with this custom version with something like this:
    import models
    import utils

    # Update model
    for k, m in model.named_modules():
        m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatability
        if isinstance(m, models.common.Conv):
           m.act = utils.activations.Hardswish()  # assign activation
If you are training a model from scratch however (intended for export), I would recommend simply using nn.LeakyReLU(0.1) here to avoid all this hassle:



yolov5/models/common.py


        Lines 20 to 27
      in
      a8751e5






 class Conv(nn.Module): 



 # Standard convolution 



 def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups 



 super(Conv, self).__init__() 



 self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False) 



 self.bn = nn.BatchNorm2d(c2) 



 self.act = nn.Hardswish() if act else nn.Identity() 



 





		</comment>
		<comment id='2' author='jinfagang' date='2020-08-25T03:19:17Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  I got a more better performance with v3.0 architecture, I not sure it's benifit from new activation or new FPN connection behaviour, I am training a new model with custom Hardswish, can it able to export in this way?
		</comment>
		<comment id='3' author='jinfagang' date='2020-08-25T03:55:59Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 yes, you can definitely export a v3.0 model this way, including all Hardswish activations. This exports correctly to all 3 (torchscript, onnx and coreml). I'm thinking of updating export.py to handle this automatically BTW, I might to that today to fully address this.
I should warn you though some export destinations are not hardswish optimized, since it's very new. PyTorch speed is unaffected, but CoreML speeds are much slower for now, until Apple does the work on their side to optimize these ops.
BTW, export.py has been updated now for automatic model fusion at the beginning, see &lt;denchmark-link:https://github.com/ultralytics/yolov5/issues/827&gt;#827&lt;/denchmark-link&gt;
. Previously I only had this working for ONNX, but now its also applied to torchscript and CoreML. This should result in a significant layer count reduction in exported models.
		</comment>
		<comment id='4' author='jinfagang' date='2020-08-25T03:57:35Z'>
		TODO: Automatic nn.Hardswish() replacement with utils.activations.Hardswish() for v3.0 exports.
		</comment>
		<comment id='5' author='jinfagang' date='2020-08-25T04:53:42Z'>
		Commit &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/4d7f222f73ef9ab06df45ddc5f363d24aff4570f&gt;4d7f222&lt;/denchmark-link&gt;
 updates export.py for full v3.0 hardswish compatibility. Please git pull and try again, and let me know if you run into any more problems.
Current v3.0 export command:
$ export PYTHONPATH="$PWD" &amp;&amp; python models/export.py --weights ./yolov5s.pt --img 640 --batch 1
Output:
Namespace(batch_size=1, img_size=[640, 640], weights='./yolov5s.pt')
Downloading https://github.com/ultralytics/yolov5/releases/download/v3.0/yolov5s.pt to ./yolov5s.pt...

Fusing layers... 
Model Summary: 140 layers, 7.45958e+06 parameters, 6.61683e+06 gradients, 17.5 GFLOPS

Starting TorchScript export with torch 1.6.0...
...
TorchScript export success, saved as ./yolov5s.torchscript.pt

Starting ONNX export with onnx 1.7.0...
ONNX export success, saved as ./yolov5s.onnx

Starting CoreML export with coremltools 4.0b3...
Running MIL optimization passes: 100%|██████████| 16/16 [00:00&lt;00:00, 16.12 passes/s]
...
Translating MIL ==&gt; MLModel Ops: 100%|██████████| 1077/1077 [00:00&lt;00:00, 1092.24 ops/s]
CoreML export success, saved as ./yolov5s.mlmodel

Export complete. Visualize with https://github.com/lutzroeder/netron.

Process finished with exit code 0
		</comment>
		<comment id='6' author='jinfagang' date='2020-08-25T05:00:39Z'>
		Update: &lt;denchmark-link:https://github.com/ultralytics/yolov5/commit/4fb8cb353f7d1945e3e1b270980c883c82297d2f&gt;4fb8cb3&lt;/denchmark-link&gt;
 adds backwards compatibility and robustness for earlier (or custom) models with alternate activation strategies.
		</comment>
		<comment id='7' author='jinfagang' date='2020-09-25T00:41:00Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>