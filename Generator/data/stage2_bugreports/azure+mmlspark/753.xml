<bug id='753' author='chris-smith-zocdoc' open_date='2019-12-04T23:56:27Z' closed_time='2020-05-28T05:34:56Z'>
	<summary>Failure to handle large datasets. Integer Overflow</summary>
	<description>
Describe the bug
mmlspark is unable to handle datasets whose size exceeds IntMax on a single executor. When using datasets this large it causes a silent integer overflow which leads to memory corruption or a jvm crash.
To Reproduce
The easiest way to reproduce is to train a model on a single executor using one partition. This ensures all the data will be loaded into a single array.
The bug is &lt;denchmark-link:https://github.com/Azure/mmlspark/blob/master/src/main/scala/com/microsoft/ml/spark/lightgbm/LightGBMUtils.scala#L192&gt;this line&lt;/denchmark-link&gt;
 when the result is larger than an int.
val data = lightgbmlib.new_doubleArray(numCols * numRows)
Just changing the type there is not enough however, as lightgbmlib.new_doubleArray takes an integer as its argument, not a Long
Expected behavior
To not crash

MMLSpark Version: &lt;denchmark-link:https://github.com/Azure/mmlspark/commit/95b7ef006d5cdb77346beb826130dc31239fa1db&gt;95b7ef0&lt;/denchmark-link&gt;

Spark Version 2.4.3
Spark Platform Databricks
5.5 ML Runtime
** Stacktrace**
If the overflow results in a negative number, it fails to allocate
&lt;denchmark-code&gt;terminate called after throwing an instance of 'std::bad_array_new_length'
  what():  std::bad_array_new_length
&lt;/denchmark-code&gt;

If the overflow results in a positive number it passes the initial allocation and crashes in &lt;denchmark-link:https://github.com/Azure/mmlspark/blob/master/src/main/scala/com/microsoft/ml/spark/lightgbm/LightGBMUtils.scala#L195&gt;lightgbmlib.doubleArray_setitem&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [lib_lightgbm_swig.so+0xa3d3]  Java_com_microsoft_ml_lightgbm_lightgbmlibJNI_doubleArray_1setitem+0x3
J 32655 C2 com.microsoft.ml.spark.lightgbm.LightGBMUtils$$anonfun$generateData$1.apply(Ljava/lang/Object;)Ljava/lang/Object; (12 bytes) @ 0x00007f6f63f96ba8 [0x00007f6f63f96900+0x2a8]
J 11102 C2 scala.collection.mutable.ArrayOps$ofRef.foreach(Lscala/Function1;)V (6 bytes) @ 0x00007f6f61eb2bc0 [0x00007f6f61eb2b20+0xa0]
j  com.microsoft.ml.spark.lightgbm.LightGBMUtils$.generateData(I[[D)Lscala/Tuple2;+76
j  com.microsoft.ml.spark.lightgbm.LightGBMUtils$.generateDenseDataset(I[[DLscala/Option;Lscala/Option;Lcom/microsoft/ml/spark/lightgbm/TrainParams;Lorg/slf4j/Logger;)Lcom/microsoft/ml/spark/lightgbm/LightGBMDataset;+251
j  com.microsoft.ml.spark.lightgbm.TrainUtils$.generateDataset([Lorg/apache/spark/sql/Row;Ljava/lang/String;Ljava/lang/String;Lscala/Option;Lscala/Option;Lscala/Option;Lscala/Option;Lorg/apache/spark/sql/types/StructType;Lorg/slf4j/Logger;Lcom/microsoft/ml/spark/lightgbm/TrainParams;)Lscala/Option;+269
j  com.microsoft.ml.spark.lightgbm.TrainUtils$.translate(Ljava/lang/String;Ljava/lang/String;Lscala/Option;Lscala/Option;Lscala/Option;Lscala/Option;Lorg/slf4j/Logger;Lcom/microsoft/ml/spark/lightgbm/TrainParams;Lorg/apache/spark/sql/types/StructType;Lscala/collection/Iterator;)Lscala/collection/Iterator;+49
j  com.microsoft.ml.spark.lightgbm.TrainUtils$.trainLightGBM(Lcom/microsoft/ml/spark/lightgbm/NetworkParams;Ljava/lang/String;Ljava/lang/String;Lscala/Option;Lscala/Option;Lscala/Option;Lscala/Option;Lorg/slf4j/Logger;Lcom/microsoft/ml/spark/lightgbm/TrainParams;ILorg/apache/spark/sql/types/StructType;Lscala/collection/Iterator;)Lscala/collection/Iterator;+239
j  com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(Lscala/collection/Iterator;)Lscala/collection/Iterator;+99
j  com.microsoft.ml.spark.lightgbm.LightGBMBase$$anonfun$6.apply(Ljava/lang/Object;)Ljava/lang/Object;+5
j  org.apache.spark.rdd.RDDBarrier$$anonfun$mapPartitions$1$$anonfun$apply$1.apply(Lorg/apache/spark/TaskContext;ILscala/collection/Iterator;)Lscala/collection/Iterator;+5
j  org.apache.spark.rdd.RDDBarrier$$anonfun$mapPartitions$1$$anonfun$apply$1.apply(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+13
j  org.apache.spark.rdd.MapPartitionsRDD.compute(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator;+27
J 10071 C1 org.apache.spark.rdd.RDD.computeOrReadCheckpoint(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator; (30 bytes) @ 0x00007f6f61d5c484 [0x00007f6f61d59d40+0x2744]
j  org.apache.spark.rdd.RDD.iterator(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lscala/collection/Iterator;+33
J 29117 C2 org.apache.spark.scheduler.Task.doRunTask(JI)Ljava/lang/Object; (338 bytes) @ 0x00007f6f635185ac [0x00007f6f635169a0+0x1c0c]
J 12512 C1 org.apache.spark.scheduler.Task.run(JILorg/apache/spark/metrics/MetricsSystem;)Ljava/lang/Object; (151 bytes) @ 0x00007f6f623ce55c [0x00007f6f623cd280+0x12dc]
j  org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply()Ljava/lang/Object;+37
J 28798 C2 org.apache.spark.executor.Executor$TaskRunner.run()V (2776 bytes) @ 0x00007f6f622e5a8c [0x00007f6f622e3420+0x266c]
J 27489 C2 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x00007f6f61bcc408 [0x00007f6f61bcc160+0x2a8]
J 24208 C1 java.util.concurrent.ThreadPoolExecutor$Worker.run()V (9 bytes) @ 0x00007f6f6039df04 [0x00007f6f6039de00+0x104]
J 17404 C1 java.lang.Thread.run()V (17 bytes) @ 0x00007f6f62aaa8fc [0x00007f6f62aaa7c0+0x13c]
v  ~StubRoutines::call_stub
V  [libjvm.so+0x6685d0]
V  [libjvm.so+0x665ab4]
V  [libjvm.so+0x666097]
V  [libjvm.so+0x6aa4b4]
V  [libjvm.so+0xa1c777]
V  [libjvm.so+0xa1cb60]
V  [libjvm.so+0x8c3102]
C  [libpthread.so.0+0x76ba]  start_thread+0xca
&lt;/denchmark-code&gt;

If the bug pertains to a specific feature please tag the appropriate &lt;denchmark-link:https://github.com/Azure/mmlspark/blob/master/CODEOWNERS&gt;CODEOWNER&lt;/denchmark-link&gt;
 for better visibility
&lt;denchmark-link:https://github.com/imatiach-msft&gt;@imatiach-msft&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='chris-smith-zocdoc' date='2020-05-28T05:34:56Z'>
		closing as this has been fixed in lightgbm and now, with lightgbm upgrade on latest master, it has been fixed in mmlspark as well
		</comment>
	</comments>
</bug>