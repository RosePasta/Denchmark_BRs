<bug id='703' author='chris-smith-zocdoc' open_date='2019-10-07T22:15:03Z' closed_time='2019-10-11T02:41:36Z'>
	<summary>Incorrect NDCG reported on Validation Data</summary>
	<description>
Describe the bug
I'm seeing incorrect values (unbelievably high near 1.0) of NDCG being reported in the logs while training LightGBMRanker. Computing NDCG with a &lt;denchmark-link:http://lobotomys.blogspot.com/2016/08/normalised-discounted-cumulative-gain.html&gt;udf&lt;/denchmark-link&gt;
 based implementation I see ~.6 on the same dataset.  I believe there are a couple bugs in the implementation that are contributing to this
1. The ranker is being trained on the validation data
The validation data is copied out of the main dataframe here
&lt;denchmark-link:https://github.com/Azure/mmlspark/blob/master/src/main/scala/com/microsoft/ml/spark/lightgbm/LightGBMBase.scala#L138&gt;LightGBMBase.scala#L138&lt;/denchmark-link&gt;
 but afaict  still has the validation data in it when we perform the training.
In the logs below you'll see
the training set

LightGBM worker generating dense dataset with 5000 rows and 80 columns

the validation set

LightGBMRanker: LightGBM worker generating dense dataset with 500 rows and 80 columns

I have 2 workers and 10k rows with 5% as validation data in my example, so this should be 4750 in the training set and 500 in the validation set per worker.
2. The validationData isn't sorted for ranking
When using LightGBMRanker we rely on preprocessData to sort by the group column &lt;denchmark-link:https://github.com/Azure/mmlspark/blob/master/src/main/scala/com/microsoft/ml/spark/lightgbm/LightGBMRanker.scala#L75&gt;LightGBMRanker.scala#L75&lt;/denchmark-link&gt;
 but only the training dataframe ends up being sorted &lt;denchmark-link:https://github.com/Azure/mmlspark/blob/master/src/main/scala/com/microsoft/ml/spark/lightgbm/LightGBMBase.scala#L140&gt;LightGBMBase.scala#L140&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker got nodes for network init: 10.222.225.231:12401,10.222.246.27:12400
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker listening on: 12400
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker generating dense dataset with 5000 rows and 80 columns
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker generateData from double array
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker LGBM_DatasetCreateFromMat
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker Validate generated dataset has the correct number of rows and cols
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker addFloatField
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker addGroupColumn
19/10/07 21:54:44 INFO LightGBMRanker: LightGBM worker done generateDataset
19/10/07 21:54:44 INFO TorrentBroadcast: Started reading broadcast variable 173
19/10/07 21:54:45 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 93.4 KB, free 47.8 GB)
19/10/07 21:54:45 INFO TorrentBroadcast: Reading broadcast variable 173 took 3 ms
19/10/07 21:54:45 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 406.6 KB, free 47.8 GB)
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker generating dense dataset with 500 rows and 80 columns
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker generateData from double array
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker LGBM_DatasetCreateFromMat
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker Validate generated dataset has the correct number of rows and cols
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker addFloatField
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker addGroupColumn
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker done generateDataset
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM parameters: max_position=20  eval_at=1,2,3,4,5 is_pre_partition=True boosting_type=gbdt tree_learner=data_parallel num_iterations=100 learning_rate=0.1 num_leaves=31 max_bin=255 bagging_fraction=1.0 bagging_freq=0 bagging_seed=3 early_stopping_round=5 feature_fraction=1.0 max_depth=-1 min_sum_hessian_in_leaf=0.001 num_machines=2 objective=lambdarank verbosity=1 lambda_l1=0.0 lambda_l2=0.0  metric= 
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM LGBM_BoosterCreate (nolock): com.microsoft.ml.lightgbm.SWIGTYPE_p_void@232f1182
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM LGBM_BoosterAddValidData
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM begin trainCore
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker calling LGBM_BoosterUpdateOneIter
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM running iteration: 0 with result: 0 and is finished: false
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@1=0.9198606271777003
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@2=0.9506376883275274
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@3=0.9610906500000013
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@4=0.9610906500000013
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@5=0.9624385691889719
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM worker calling LGBM_BoosterUpdateOneIter
19/10/07 21:54:45 INFO LightGBMRanker: LightGBM running iteration: 1 with result: 0 and is finished: false
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@1=0.9372822299651568
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@2=0.965860929604282
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@3=0.9693452501617733
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@4=0.9723464805316228
19/10/07 21:54:45 INFO LightGBMRanker: Valid ndcg@5=0.9736943997205932
&lt;/denchmark-code&gt;

To Reproduce
import mmlspark
import pandas as pd 
import numpy as np
import pyspark.sql.functions as fn

from mmlspark.lightgbm import LightGBMRanker
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler
from mmlspark.lightgbm import LightGBMRanker
from scipy.sparse import rand

ranker = LightGBMRanker()
ranker.setGroupCol('search_id')
ranker.setUseBarrierExecutionMode(True)
ranker.setValidationIndicatorCol("is_validation")
ranker.setEarlyStoppingRound(5)


num_cols = 80
num_rows = 1000 * 10

def create_row(i):
  m = np.array(rand(1, num_cols + 2, density=.2, format='csr').todense())[0]
  m[0] = int(i // 10) # search id
  m[1] = int(i % 10 == 0) # label
  return m.tolist()

feat_cols = ['feat_{}'.format(c) for c in range(num_cols)]

df = spark.createDataFrame(pd.DataFrame([create_row(i) for i in range(num_rows)], columns=[ 'search_id', 'label'] + feat_cols ))

df = df.withColumn('label', fn.col('label').cast(IntegerType()))
df = df.withColumn('search_id', fn.col('search_id').cast(IntegerType()))

# This should give us
df = df.withColumn('is_validation', fn.expr('pmod(search_id, 100) &gt;= 95'))

assembler = VectorAssembler().setInputCols(feat_cols).setOutputCol('features')
transformed = assembler.transform(df).withColumn('features', fn.expr('toDense(features)'))

transformed = transformed.repartition(1, 'search_id')

model = ranker.fit(transformed)
Expected behavior
NDCG to be computed correctly
Validation data shouldn't be included in the training set
Info (please complete the following information):

MMLSpark Version: 035fcd9
Spark Version 2.4.3
Spark Platform Databricks

	</description>
	<comments>
		<comment id='1' author='chris-smith-zocdoc' date='2019-10-09T03:23:43Z'>
		&lt;denchmark-link:https://github.com/chris-smith-zocdoc&gt;@chris-smith-zocdoc&lt;/denchmark-link&gt;
 great find, and you were even able to suggest the fixes.  Would you be able to review this PR to fix both of the issues you mentioned above:
&lt;denchmark-link:https://github.com/Azure/mmlspark/pull/706&gt;#706&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='chris-smith-zocdoc' date='2019-10-11T02:41:36Z'>
		closing as PR has been merged
		</comment>
	</comments>
</bug>