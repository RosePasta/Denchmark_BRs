<bug id='814' author='aleph-null84' open_date='2020-03-04T17:54:51Z' closed_time='2020-05-28T05:38:06Z'>
	<summary>Mini-batch inference can improve speed of the inference stage</summary>
	<description>
Hello!
Recently, I have tried the LightGBMClassifier model in our ML tasks. It have shown excellent predictive performance. But I have noticed quite slow inference stage of the model. I think, that You can significantly improve the speed of the inference stage by using mini-batch inference instead of row-wise. Namely, You can select a mini-batch of input records from the Spark dataset, and then apply the native LGBM_BoosterPredictForMat method to whole mini-batch. Similar functionality is implemented in the XGBoost4J and DL4J.
BR
	</description>
	<comments>
		<comment id='1' author='aleph-null84' date='2020-03-04T17:54:53Z'>
		üëã Thanks for opening your first issue here! If you're reporting a üêû bug, please make sure you include steps to reproduce it.
		</comment>
		<comment id='2' author='aleph-null84' date='2020-03-04T18:21:10Z'>
		&lt;denchmark-link:https://github.com/aleph-null84&gt;@aleph-null84&lt;/denchmark-link&gt;
 there was a regression which caused some 3-4x performance improvements during inference to be reverted by accident when they introduced a bug, it has been fixed in latest lightgbm here:
&lt;denchmark-link:https://github.com/microsoft/LightGBM/pull/2799&gt;microsoft/LightGBM#2799&lt;/denchmark-link&gt;

but I need to update mmlspark to latest lightgbm which I am currently working on
		</comment>
		<comment id='3' author='aleph-null84' date='2020-05-28T05:38:06Z'>
		Closing as lightgbm has been updated on master with the fix for the scoring execution time regression.
Also, there are more optimizations being added here for prediction, which will be available in the future:
&lt;denchmark-link:https://github.com/microsoft/LightGBM/pull/2992&gt;microsoft/LightGBM#2992&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>