<bug id='512' author='loretoparisi' open_date='2018-11-27T11:01:25Z' closed_time='2018-12-14T17:00:40Z'>
	<summary>SageMaker fails when using Multi-GPU with keras.utils.multi_gpu_model</summary>
	<description>
Please fill out the form below.
&lt;denchmark-h:h3&gt;System Information&lt;/denchmark-h&gt;


Framework (e.g. TensorFlow) / Algorithm (e.g. KMeans): Keras (backend: Tensorflow)
Framework Version: Keras 2.2.0, Tensorflow 1.12.0
Python Version: 2.7
CPU or GPU: SageMaker CPU / GPU - ml.p3.8xlarge
Python SDK Version: 2.7
Are you using a custom image: Yes

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Running AWS SageMaker with a custom model, the TrainingJob fails with an Algorithm Error when using Keras plus a Tensorflow backend in multi-gpu configuration.
&lt;denchmark-h:h3&gt;Minimal repro / logs&lt;/denchmark-h&gt;

def setup_multi_gpu(model):

  import tensorflow as tf
  from keras.utils.training_utils import multi_gpu_model
  from tensorflow.python.client import device_lib

  # IMPORTANT: Tells tf to not occupy a specific amount of memory
  from keras.backend.tensorflow_backend import set_session

  config = tf.ConfigProto()
  config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU
  sess = tf.Session(config=config)
  set_session(sess)  # set this TensorFlow session as the default session for Keras.

  print('reading gpus avaliable..')
  local_device_protos = device_lib.list_local_devices()
  avail_gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']
  num_gpu = len(avail_gpus)
  print('Amount of GPUs available: %s' % num_gpu)

  multi_model = multi_gpu_model(model, gpus=num_gpu)

  return multi_model 

_model = create_model()
model = setup_multi_gpu(_model)
model.compile(params)
model.train(params)
This parallel model loading will fail. There is no further error or exception from CloudWatch logging. This configuration works properly on local machine with 2x NVIDIA GTX 1080, same Keras Tensorflow backend.
According to SageMaker documentation and &lt;denchmark-link:https://github.com/awslabs/keras-apache-mxnet/wiki/Multi-GPU-Model-Training-with-Keras-MXNet&gt;tutorials&lt;/denchmark-link&gt;
 the  utility will work ok when Keras backend is MXNet, but I did not find any mention when the backend is Tensorflow with the same multi gpu configuration.
In the following some logging before the TrainingJob hangs. This logging repeats twice
&lt;denchmark-code&gt;2018-11-27 10:02:49.878414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3
2018-11-27 10:02:49.878462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-27 10:02:49.878471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 1 2 3
2018-11-27 10:02:49.878477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N Y Y Y
2018-11-27 10:02:49.878481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1: Y N Y Y
2018-11-27 10:02:49.878486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2: Y Y N Y
2018-11-27 10:02:49.878492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3: Y Y Y N
2018-11-27 10:02:49.879340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 14874 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)
2018-11-27 10:02:49.879486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 14874 MB memory) -&gt; physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)
2018-11-27 10:02:49.879694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:2 with 14874 MB memory) -&gt; physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)
2018-11-27 10:02:49.879872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:3 with 14874 MB memory) -&gt; physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
&lt;/denchmark-code&gt;

Before there is some logging info about each GPU, that repeats 4 times
&lt;denchmark-code&gt;2018-11-27 10:02:46.447639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
totalMemory: 15.78GiB freeMemory: 15.37GiB
&lt;/denchmark-code&gt;

According to the logging all the 4 GPUs are visible and loaded in the Tensorflow Keras backend. After that no application logging follows, the TrainingJob status is inProgress for a while, after that it becomes Failed with the same Algorithm Error.
&lt;denchmark-link:https://user-images.githubusercontent.com/163333/49077561-da0bd180-f23b-11e8-8960-48d532866444.png&gt;&lt;/denchmark-link&gt;

Looking at CloudWatch logging I can see some metrics at work. Specifically GPU Memory Utilization, CPU Utilization are ok, while GPU utilization is 0%.
&lt;denchmark-link:https://i.stack.imgur.com/260hL.png&gt;&lt;/denchmark-link&gt;

I have posted the question to the forum &lt;denchmark-link:https://forums.aws.amazon.com/thread.jspa?threadID=294095&amp;tstart=0&gt;here&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='loretoparisi' date='2018-11-27T22:57:27Z'>
		Hi &lt;denchmark-link:https://github.com/loretoparisi&gt;@loretoparisi&lt;/denchmark-link&gt;
 ,
Thanks for reporting this with good details.
First I need to confirm one thing, how do you use SageMaker with the codes above? Especially how do you use tensorflow 1.12.0 with SageMaker? Now the latest SageMaker TensorFlow containers are built with TensorFlow 1.11.0. See README here: &lt;denchmark-link:https://github.com/aws/sagemaker-python-sdk/blob/master/README.rst#tensorflow-sagemaker-estimators&gt;https://github.com/aws/sagemaker-python-sdk/blob/master/README.rst#tensorflow-sagemaker-estimators&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='loretoparisi' date='2018-11-28T09:23:43Z'>
		&lt;denchmark-link:https://github.com/yangaws&gt;@yangaws&lt;/denchmark-link&gt;
 hello, so our code is in a docker container that runs on SageMaker TrainingJob. The docker image is from . So tensorflow version was , gpu version, while Keras is  (not 2.2.0 as mentioned above, my fault).
The main problem here is that we have any logging from the Training Job before it hangs. We have tried different approaches, the first was adapted from this SF question &lt;denchmark-link:https://stackoverflow.com/questions/53488870/sagemaker-fails-when-using-multi-gpu-with-keras-utils-multi-gpu-model&gt;SageMaker fails when using Multi-GPU with keras.utils.multi_gpu_model&lt;/denchmark-link&gt;

The latter approach overrides the  method, since the problem seems to be related to the current implementation within Keras that causes an issue when slicing the data through the GPU devices  - specifically due to this import &lt;denchmark-link:https://github.com/keras-team/keras/commit/d059890d0342955e968fdf97b5a90d19c9d68b4e&gt;keras-team/keras@d059890&lt;/denchmark-link&gt;

See for more details &lt;denchmark-link:https://github.com/keras-team/keras/issues/8123#issuecomment-354857044&gt;keras-team/keras#8123 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='loretoparisi' date='2018-11-29T11:38:25Z'>
		[UPDATE]
To be sure of the exceptions and error, we did an override of excepthook so that we could capture every runtime error
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

def trapUncaughtException(exctype, value, tb):
    print 'My Error Information'
    print 'Type:', exctype
    print 'Value:', value
    print 'Traceback:', tb

def installUncaughtException(handler):
    sys.excepthook = handler
Despited of this, it seems that there is any logging in CloudWatch that could drive us to a possibile solution.
		</comment>
		<comment id='4' author='loretoparisi' date='2018-12-04T21:24:46Z'>
		Could you provide some information about the model? What kind of model is this?
		</comment>
		<comment id='5' author='loretoparisi' date='2018-12-05T11:16:17Z'>
		&lt;denchmark-link:https://github.com/cavdard&gt;@cavdard&lt;/denchmark-link&gt;
 it's basically a variation of this CNN &lt;denchmark-link:https://github.com/keunwoochoi/music-auto_tagging-keras/tree/master/compact_cnn&gt;https://github.com/keunwoochoi/music-auto_tagging-keras/tree/master/compact_cnn&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='loretoparisi' date='2018-12-05T17:48:18Z'>
		&lt;denchmark-link:https://github.com/cavdard&gt;@cavdard&lt;/denchmark-link&gt;
 this may help. I have slightly modified the  code adding some initializers
with tf.Session() as session:
    K.set_session(session)
    session.run(tf.global_variables_initializer())
    session.run(tf.tables_initializer())
and now at least I can see that one GPU (I assume gpu:0) is used from the instance metrics üëç  I will investigate if this may now help to make the multi-gpu to work (since I can see the gpu loaded in tensorflow, I cannot be sure that more than one GPU is in use at this time without detailed logging...)
Hope this help other dev.
		</comment>
		<comment id='7' author='loretoparisi' date='2018-12-13T19:34:05Z'>
		I apologize for the frustrating experience and delayed response.

Which instance type are you using?
How many instances are you using?
Would it be possible to see the Python code that invokes SageMaker on your behalf?
Were there any notebook examples or code you are following for attempting to do multi-gpu with keras on SageMaker?

		</comment>
		<comment id='8' author='loretoparisi' date='2018-12-13T19:58:01Z'>
		For reference to others with the same issue:
&lt;denchmark-link:https://forums.aws.amazon.com/thread.jspa?messageID=881541&gt;https://forums.aws.amazon.com/thread.jspa?messageID=881541&lt;/denchmark-link&gt;

&lt;denchmark-link:https://forums.aws.amazon.com/thread.jspa?messageID=881540&gt;https://forums.aws.amazon.com/thread.jspa?messageID=881540&lt;/denchmark-link&gt;

&lt;denchmark-link:https://stackoverflow.com/questions/53488870/sagemaker-fails-when-using-multi-gpu-with-keras-utils-multi-gpu-model/53754450#53754450&gt;https://stackoverflow.com/questions/53488870/sagemaker-fails-when-using-multi-gpu-with-keras-utils-multi-gpu-model/53754450#53754450&lt;/denchmark-link&gt;

The discussion will continue in the stackoverflow post.
		</comment>
		<comment id='9' author='loretoparisi' date='2018-12-14T17:00:39Z'>
		&lt;denchmark-link:https://github.com/ChoiByungWook&gt;@ChoiByungWook&lt;/denchmark-link&gt;
 Okay I'm then closing the issue here and let's go on on SF.
		</comment>
	</comments>
</bug>