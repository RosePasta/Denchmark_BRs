<bug id='632' author='gediminaszylius' open_date='2019-02-08T12:39:01Z' closed_time='2019-07-24T23:18:45Z'>
	<summary>Sagemaker distributed xgboost error when reading LIBSVM files</summary>
	<description>
&lt;denchmark-h:h3&gt;System Information&lt;/denchmark-h&gt;


Framework: xgboost:
Framework Version: latest on 2019-02-01:
Python Version: 3.6:
CPU or GPU: cpu:
Python SDK Version: sagemaker version 1.17.2 (installed via pip):
Are you using a custom image: no:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I'm getting following xgboost error on sagemaker when i train on LIBSVM formatted data in s3:
ValueError: Error for Training job xgboost-2019-02-01-09-57-04-694: Failed Reason: ClientError: Failed to load libsvm data with exception: [10:07:07] src/data/./row_block.h:177: Check failed: offset.back() == value.size() || value.size() == 0 
But this happens randomly in distributed mode (sometimes i get it sometimes i dont and model trains normally) and i have noticed that it never happens when i use single instance instead of multiple ones (in single node mode, not in distributed) in Sagemaker, even though training and validation datasets are the same.
I have checked xgboost of similar errors and I have found mentioned that some bad values may exist in files, but after training locally on my EC2 server and in Sagemaker on single node xgboost trains normally. So i suspect this may be distributed xgboost backend issue.
&lt;denchmark-h:h3&gt;Minimal code&lt;/denchmark-h&gt;

instance_type='ml.m5.4xlarge'
instance_count=3
disk_for_data_download_size=200
container = get_image_uri(boto3.Session().region_name, 'xgboost',repo_version='latest')
s3_input_train = sagemaker.s3_input(s3_data=train_data, content_type='libsvm')
s3_input_test = sagemaker.s3_input(s3_data=val_data, content_type='libsvm')
sess = sagemaker.Session()
xgb = sagemaker.estimator.Estimator(container,
iam_role,
train_instance_count=instance_count,
train_instance_type=instance_type,
output_path=output_model,
train_volume_size=disk_for_data_download_size,
sagemaker_session=sess)
xgb.set_hyperparameters(max_depth=15,
eta=0.1,
silent=1,
objective='reg:logistic',
booster='gbtree',
min_child_weight=100,
eval_metric='auc',
num_round=300,
early_stopping_rounds=50
)
xgb.fit({'train': s3_input_train,'validation': s3_input_test})
	</description>
	<comments>
		<comment id='1' author='gediminaszylius' date='2019-02-13T21:34:27Z'>
		Hello &lt;denchmark-link:https://github.com/gediminaszylius&gt;@gediminaszylius&lt;/denchmark-link&gt;
,
Apologies for the late response. We have reached out to the SageMaker team that owns the XGBoost  algorithm and they will respond as soon as possible.
Thank you for patience.
		</comment>
		<comment id='2' author='gediminaszylius' date='2019-02-25T23:52:45Z'>
		Hi &lt;denchmark-link:https://github.com/gediminaszylius&gt;@gediminaszylius&lt;/denchmark-link&gt;
,
This is an open issue for xgboost. Mostly it's because of invalid libsvm input data (numeric values, Nan, extra spaces somewhere, etc). The distribution engine would simply do a line base split to the file and send partitions to different workers. It might happen that it created some extra line or space when doing the splitting and making the data invalid. XGBoost is very strict regarding to the libsvm input.
Can you be more specific on what instance type you're using and how many instances you launched?
Thanks.
		</comment>
		<comment id='3' author='gediminaszylius' date='2019-02-26T07:55:12Z'>
		Thanks for the reply, code example is added above (instance types and number, plus disk size).  :
instance_type='ml.m5.4xlarge'
instance_count=3
disk_for_data_download_size=200
With those i get error. However i tried different variations of instance types and counts, increased disk space too.
Also then i see that sagemaker goes to distributed mode, but when i use 1 instance (larger one) i get no error (i assume it uses single mode) which is strange, should it dont matter if i use one or more instances on same data in terms of data formation? Maybe single mode is more robust in some way when reading data and cluster mode isnt?.
Also as i mentioned i initially got the success with model training with multiple instances on sagemaker in the same setting and on same data, but that was only few times. Several other times i retried on same data, it throwed me the same error.
I format data with Spark libsvm and it has 1-based indexing not 0-based as xgboost uses. But it should not matter that first column is then assumed to be always missing right?
		</comment>
		<comment id='4' author='gediminaszylius' date='2019-02-27T01:05:17Z'>
		
Also then i see that sagemaker goes to distributed mode, but when i use 1 instance (larger one) i get no error (i assume it uses single mode) which is strange, should it dont matter if i use one or more instances on same data in terms of data formation? Maybe single mode is more robust in some way when reading data and cluster mode isnt?.

Yes, single mode was more robust compared to distributing setup. The way that XGBoost split files into partitions would be nondeterministic with different number of workers and allocated files in each machine. So even with same data input, change on instance type and S3DataDistributionType will change the way that files being partitioned. If you have sample dataset that can easily reproduce the error, please let use know so that we can look into the issue in depth.

I format data with Spark libsvm and it has 1-based indexing not 0-based as xgboost uses. But it should not matter that first column is then assumed to be always missing right?

Yes, XGBoost will simply treat it as an empty first column.
		</comment>
	</comments>
</bug>