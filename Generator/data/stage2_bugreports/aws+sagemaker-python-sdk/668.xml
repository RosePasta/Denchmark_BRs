<bug id='668' author='andremoeller' open_date='2019-02-26T20:47:43Z' closed_time='2019-09-25T20:29:01Z'>
	<summary>Using "tensorflow-serving" with Batch Transform</summary>
	<description>
Please fill out the form below.
&lt;denchmark-h:h3&gt;System Information&lt;/denchmark-h&gt;


Framework (e.g. TensorFlow) / Algorithm (e.g. KMeans): N/A: TensorFlow Serving
Framework Version: N/A
Python Version: 3.5
CPU or GPU: CPU
Python SDK Version: 1.18.3
Are you using a custom image: No

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Is there a way to use the "tensorflow-serving" endpoint type for Batch (or a reason this can't or shouldn't be done)?
Would implementing this mostly be a matter of adding an endpoint_type parameter to def transform()?
Thanks!
	</description>
	<comments>
		<comment id='1' author='andremoeller' date='2019-04-17T16:56:18Z'>
		Thanks for bringing this to our attention! Yes, I think the solution here would be adding an endpoint_type arg to transformer() to mimic what TensorFlow.deploy() does.
In the meantime, you should still be able to start Batch Transform jobs either with the  class directly or through a &lt;denchmark-link:https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html#tensorflow-serving-model&gt;TensorFlow Serving Model object&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='andremoeller' date='2019-09-25T20:29:01Z'>
		Closing, this was released in 1.32.0. Thanks!
&lt;denchmark-link:https://github.com/aws/sagemaker-python-sdk/blob/91ea028c153fb1b618c73e3e92e7d6cf10712dfb/CHANGELOG.md#v1320-2019-07-02&gt;https://github.com/aws/sagemaker-python-sdk/blob/91ea028c153fb1b618c73e3e92e7d6cf10712dfb/CHANGELOG.md#v1320-2019-07-02&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>