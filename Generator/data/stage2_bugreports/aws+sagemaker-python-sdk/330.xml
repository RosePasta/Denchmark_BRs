<bug id='330' author='chang2394' open_date='2018-08-01T07:43:11Z' closed_time='2018-08-20T22:30:30Z'>
	<summary>Check failed: work_element_count &amp;gt; 0 (0 vs. 0)</summary>
	<description>
&lt;denchmark-h:h3&gt;System Information&lt;/denchmark-h&gt;


Framework (e.g. TensorFlow) / Algorithm (e.g. KMeans): Tensorflow-1.8 (Keras)
Framework Version: 1.7.1
Python Version: 2.7
CPU or GPU: GPU
Python SDK Version:
Are you using a custom image:

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

I am trying to run a stacked LSTM keras model but it fails with the error:
F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count &gt; 0 (0 vs. 0)
Failed Reason: AlgorithmError: Exit Code: 139
&lt;denchmark-code&gt;def rmse_filtered(y_true, y_pred, low, high):
    y_true_reshaped = K.flatten(y_true)
    y_pred_reshaped = K.flatten(y_pred)

    indices1 = array_ops.where(K.greater_equal(y_true_reshaped, low))
    yf1_true = K.gather(y_true_reshaped, indices1)
    yf1_pred = K.gather(y_pred_reshaped, indices1)

    indices2 = array_ops.where(K.less(yf1_true, high))
    yf2_true = K.gather(yf1_true, indices2)
    yf2_pred = K.gather(yf1_pred, indices2)

    return K.sqrt(mse(yf2_true, yf2_pred))

def range_rmse(low, high):
    def loss(y_true, y_pred):
        return rmse_filtered(y_true, y_pred, low, high)
    loss.__name__ = "rmse_{}_{}".format(int(low), int(high))
    return loss

rmse_metrics = [
    range_rmse(1.0, 5.0),
    range_rmse(5.0, 10.0),
    range_rmse(10.0, 20.0),
    range_rmse(20.0, 50.0),
    range_rmse(50.0, 100.0),
    range_rmse(100.0, 200.0),
    range_rmse(200.0, 500.0),
    range_rmse(500.0, 1000.0),
    range_rmse(1000.0, 2000.0),
    range_rmse(2000.0, 5000.0)
]

def keras_model_fn(hyperparameters):
    model = Sequential()
    model.add(Bidirectional(LSTM(100, return_sequences=True), input_shape=(12, 1), name="inputs", merge_mode='ave'))
    model.add(LSTM(100, return_sequences=False))
    model.add(Dense(1))

    model_metrics = ['mae', mape, mape_gz, msle] + rmse_metrics
    model.compile(loss='mean_squared_error',
                  optimizer=Adam(lr=0.001), metrics=model_metrics)
    logger.info(model.summary())
    return model


&lt;/denchmark-code&gt;

The error seems to go away when i switch from p-series instance types to c-series.
&lt;denchmark-h:h3&gt;Minimal repro / logs&lt;/denchmark-h&gt;

2018-07-31 15:09:40,402 INFO - root - running container entrypoint
2018-07-31 15:09:40,403 INFO - root - starting train task
2018-07-31 15:09:40,527 INFO - container_support.training - Training starting
2018-07-31 15:09:43,036 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2
2018-07-31 15:09:43,364 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): sagemaker-poc-test.s3.amazonaws.com
2018-07-31 15:09:43,503 INFO - sagemaker - TF VERSION: 1.8.0
2018-07-31 15:09:43,509 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com
2018-07-31 15:09:43,608 INFO - tf_container - ----------------------TF_CONFIG--------------------------
2018-07-31 15:09:43,608 INFO - tf_container - {"environment": "cloud", "cluster": {"master": ["algo-1:2222"]}, "task": {"index": 0, "type": "master"}}
2018-07-31 15:09:43,608 INFO - tf_container - ---------------------------------------------------------
2018-07-31 15:09:43,608 INFO - tf_container - creating RunConfig:
2018-07-31 15:09:43,609 INFO - tf_container - {'save_checkpoints_secs': 300}
2018-07-31 15:09:43,609 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'master'}}
2018-07-31 15:09:43,609 INFO - tf_container - invoking the user-provided keras_model_fn
2018-07-31 15:09:44,629 INFO - sagemaker - None
2018-07-31 15:09:44,629 INFO - tensorflow - Using the Keras model provided.
2018-07-31 15:09:44,629 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': u'master', '_train_distribute': None, '_is_chief': True, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdf257c3a50&gt;, '_model_dir': u's3://sagemaker-poc-test/artifacts/tf-lstm-test-pipe-mode-temp16-2018-07-31-15-05-21-747/checkpoints', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_service': None, '_global_id_in_cluster': 0, '_save_summary_steps': 100, '_num_ps_replicas': 0}
2018-07-31 15:09:58,402 INFO - tensorflow - Skip starting Tensorflow server as there is only one node in the cluster.
2018-07-31 15:09:58,736 INFO - sagemaker - train input fn: None
2018-07-31 15:09:58,768 INFO - tensorflow - Calling model_fn.
2018-07-31 15:10:02,559 INFO - tensorflow - Done calling model_fn.
2018-07-31 15:10:02,560 INFO - tensorflow - Create CheckpointSaverHook.
2018-07-31 15:10:03,886 INFO - tensorflow - Graph was finalized.
2018-07-31 15:10:04,228 INFO - tensorflow - Restoring parameters from s3://sagemaker-poc-test/artifacts/tf-lstm-test-pipe-mode-temp16-2018-07-31-15-05-21-747/checkpoints/keras_model.ckpt
2018-07-31 15:10:05,005 INFO - tensorflow - Running local_init_op.
2018-07-31 15:10:05,057 INFO - tensorflow - Done running local_init_op.
2018-07-31 15:10:11,480 INFO - tensorflow - Saving checkpoints for 1 into s3://sagemaker-poc-test/artifacts/tf-lstm-test-pipe-mode-temp16-2018-07-31-15-05-21-747/checkpoints/model.ckpt.
2018-07-31 15:10:14,595 INFO - sagemaker - eval input fn: None
2018-07-31 15:10:14,616 INFO - tensorflow - Calling model_fn.
2018-07-31 15:10:15,951 INFO - tensorflow - Done calling model_fn.
2018-07-31 15:10:15,971 INFO - tensorflow - Starting evaluation at 2018-07-31-15:10:15
2018-07-31 15:10:16,096 INFO - tensorflow - Graph was finalized.
2018-07-31 15:10:16,098 INFO - tensorflow - Restoring parameters from s3://sagemaker-poc-test/artifacts/tf-lstm-test-pipe-mode-temp16-2018-07-31-15-05-21-747/checkpoints/model.ckpt-1
2018-07-31 15:10:16,407 INFO - tensorflow - Running local_init_op.
2018-07-31 15:10:16,450 INFO - tensorflow - Done running local_init_op.
2018-07-31 15:10:19.357309: F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count &gt; 0 (0 vs. 0)
Downloading s3://sagemaker-poc-test/customcode/tensorflow_test/tf-lstm-test-pipe-mode-temp16-2018-07-31-15-05-21-747/source/sourcedir.tar.gz to /tmp/script.tar.gz
	</description>
	<comments>
		<comment id='1' author='chang2394' date='2018-08-03T10:11:24Z'>
		This seems to be an issue with tensorflow version 1.8, downgrading to 1.7 resolves this.
		</comment>
		<comment id='2' author='chang2394' date='2018-08-15T01:24:56Z'>
		Hello,
This issue might be related to &lt;denchmark-link:https://github.com/aws/sagemaker-python-sdk/issues/263&gt;#263&lt;/denchmark-link&gt;
, since you mention that the error goes away when you switch from p-series to c-series.
However, your issue was posted after the 'fix' was implemented.
		</comment>
		<comment id='3' author='chang2394' date='2018-08-20T22:30:30Z'>
		I don't think this is a SageMaker specific problem especially according to this &lt;denchmark-link:https://github.com/matterport/Mask_RCNN/issues/521&gt;matterport/Mask_RCNN#521&lt;/denchmark-link&gt;

According to the thread, users report that downgrading to 1.7 or upgrading resolves this problem.
		</comment>
	</comments>
</bug>