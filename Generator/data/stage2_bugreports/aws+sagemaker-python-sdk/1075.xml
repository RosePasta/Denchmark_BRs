<bug id='1075' author='salmanmashayekh' open_date='2019-10-04T18:23:26Z' closed_time='2020-02-19T19:10:40Z'>
	<summary>Batch transfer failing for LDA model</summary>
	<description>
Reference ID: 0412573472
I am training a Sagemaker LDA model on a recordio formatted input data. Here is the training code:
&lt;denchmark-code&gt;session = sagemaker.Session()

# specify general training job information
lda = sagemaker.estimator.Estimator(
    container,
    role,
    output_path = f"s3://{BUCKET}/{OUTPUT_DIR}",
    train_instance_count = INSTANCE_COUNT,
    train_instance_type = INSTANCE_TYPE,
    sagemaker_session = session,
)

# set algorithm-specific hyperparameters
# https://docs.aws.amazon.com/sagemaker/latest/dg/lda_hyperparameters.html
lda.set_hyperparameters(
    num_topics = N_TOPICS,
    feature_dim = VOCAB_SIZE,
    mini_batch_size = DOC_SIZE,
    max_restarts = 50,
)

train_channel = sagemaker.session.s3_input(
    s3_data = f"s3://{BUCKET}/{TRAIN_DIR}/{TRAIN_FILE}",
    content_type = "application/x-recordio-protobuf",
)

lda.fit({'train': train_channel})
&lt;/denchmark-code&gt;

It trains fine and finishes after a few minutes. But the batch transfer fails on the same input data with the error unable to evaluate payload provided. Here is the code:
&lt;denchmark-code&gt;transformer = lda.transformer(
    instance_count = INSTANCE_COUNT, 
    instance_type = INSTANCE_TYPE, 
    output_path = f"s3://{BUCKET}/{PRED_DIR}",
    strategy = "SingleRecord",
    max_payload = 1,
)

transformer.transform(
    data = f"s3://{BUCKET}/{TRAIN_DIR}/{TRAIN_FILE}", 
    data_type = "S3Prefix", 
    content_type = "application/x-recordio-protobuf", 
    split_type = "RecordIO"
)
&lt;/denchmark-code&gt;

I have tried also the  with no luck. The model file can be downloaded from here: &lt;denchmark-link:https://drive.google.com/open?id=1y2UGgRGq7UBY4m88rHVPSLJQitRMa66p&gt;https://drive.google.com/open?id=1y2UGgRGq7UBY4m88rHVPSLJQitRMa66p&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='salmanmashayekh' date='2019-10-08T20:17:53Z'>
		Hello &lt;denchmark-link:https://github.com/salmanmashayekh&gt;@salmanmashayekh&lt;/denchmark-link&gt;
,
Apologies for the late response.
Thank you for bringing this to our attention.
Just wondering, does the model work for you if do a regular deploy instead of transform?
I will attempt to reproduce this ASAP and see what the problem.
Reference ID: 0412573472
Thank you for your patience.
		</comment>
		<comment id='2' author='salmanmashayekh' date='2019-10-09T20:26:29Z'>
		Thanks &lt;denchmark-link:https://github.com/ChoiByungWook&gt;@ChoiByungWook&lt;/denchmark-link&gt;
 !
Yes, a regular  does work fine.
		</comment>
		<comment id='3' author='salmanmashayekh' date='2019-10-11T00:15:28Z'>
		&lt;denchmark-link:https://github.com/salmanmashayekh&gt;@salmanmashayekh&lt;/denchmark-link&gt;
,
Is there any sample data I can use/reference to reproduce the error?
In addition, when you do a regular deploy and making predictions, are you using custom serializers or deserializers?
I believe in batch transform, it doesn't have the additional layer of serialization/deserialization that the RealTimePredictor provides, which might cause those errors.
Also, excuse my negligence, but is it usual for LDA to be able to make predictions against the same data that was used for training?
Thank you and let me know if there is anything I can clarify!
		</comment>
		<comment id='4' author='salmanmashayekh' date='2020-02-19T19:10:40Z'>
		closing due to inactivity. feel free to reopen if necessary
		</comment>
	</comments>
</bug>