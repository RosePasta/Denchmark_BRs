<bug id='1341' author='cfregly' open_date='2020-03-09T23:05:11Z' closed_time='2020-03-11T17:56:16Z'>
	<summary>`KeyError: 'ProcessingOutputConfig'` when calling Calling `ProcessingJob.from_processing_name()` for a ProcessingJob defined without a ProcessingOutput (ie. Spark Processor)</summary>
	<description>
After running this ScriptProcessor:
&lt;denchmark-code&gt;processor.run(code='preprocess-spark.py',
              arguments=['s3_input_data', balanced_train_data_input,
                         's3_output_data', balanced_train_data_tfidf_output,
              ],    
              logs=True,
              wait=False
)
&lt;/denchmark-code&gt;

and running this code:
&lt;denchmark-code&gt;preprocessing_job_description = processor.jobs[-1].describe()
processing_job_name = preprocessing_job_description['ProcessingJobName']

running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,
&lt;/denchmark-code&gt;

I'm seeing this error
&lt;denchmark-code&gt;---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-55-5b16753426ad&gt; in &lt;module&gt;()
      1 running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,
----&gt; 2                                                                             sagemaker_session=sagemaker_session)
      3 running_processor.describe()

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/processing.py in from_processing_name(cls, sagemaker_session, processing_job_name)
    668             outputs=[
    669                 ProcessingOutput(
--&gt; 670                     source=job_desc["ProcessingOutputConfig"]["Outputs"][0]["S3Output"][
    671                         "LocalPath"
    672                     ],

KeyError: 'ProcessingOutputConfig'
&lt;/denchmark-code&gt;

Note:  I'm not using  because I am using a Spark Processor and writing directly to S3.  (See &lt;denchmark-link:https://github.com/aws/amazon-sagemaker-examples/issues/994&gt;aws/amazon-sagemaker-examples#994&lt;/denchmark-link&gt;
 for more info on why I'm doing this.)
	</description>
	<comments>
		<comment id='1' author='cfregly' date='2020-03-09T23:17:39Z'>
		A workaround is to add a dummy ProcessingOutput as follows:
&lt;denchmark-code&gt;processor.run(code='preprocess-spark.py',
              arguments=['s3_input_data', balanced_train_data_input,
                         's3_output_data', balanced_train_data_tfidf_output,
              ],
              # We need this dummy output to allow us to call 
              #    ProcessingJob.from_processing_name() later 
              #    to describe the job and poll for Completed status
              outputs=[
                       ProcessingOutput(s3_upload_mode='EndOfJob',
                                        output_name='dummy-output',
                                        source='/opt/ml/processing/output')
              ],            
              logs=True,
              wait=False
)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='cfregly' date='2020-03-10T19:12:11Z'>
		Oh no!
Thanks for reaching out!
This definitely looks like a bug on our part. I went ahead and created a PR to fix this issue, as well as add a test for it to ensure we don't lose this functionality in the future.
Thanks for letting us know!
		</comment>
		<comment id='3' author='cfregly' date='2020-03-11T17:56:16Z'>
		A fix for this was released to PyPI a few minutes ago =)
Let me know if there's anything else we can do to help!
		</comment>
	</comments>
</bug>