<bug id='356' author='RudyVenguswamy' open_date='2020-11-10T21:38:56Z' closed_time='2021-01-06T07:55:21Z'>
	<summary>High Accuracy without Training on GPU, Lower accuracy after training</summary>
	<description>
Context: We’re a team trying to build a reverse image searcher for satellite data. Currently, we’re using the UC Merced land use dataset (linked in notebook below) to train a self-supervised learner and evaluate it using the labels provided with the dataset.
Issue: We use the embeddings from SimCLR’s encoder and pass it through the projection head, resulting in an embedding of length 128. We search for the nearest neighbor using Google ScaNN’s algorithm and then check if the class of the nearest neighbor returned is the same as the reference image. With no training, our model achieves 25% accuracy on the closest nearest neighbor (rank 1) searching, when we would expect, among 21 classes to get 0.047 accuracy by random chance. This occurs across multiple random seeds. With further training the accuracy goes down.
 &lt;denchmark-link:https://colab.research.google.com/drive/1DTps7BfnUDmMtGugy94QoW02-Xx39UTe?usp=sharing&gt;https://colab.research.google.com/drive/1DTps7BfnUDmMtGugy94QoW02-Xx39UTe?usp=sharing&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='RudyVenguswamy' date='2020-11-11T23:32:22Z'>
		&lt;denchmark-link:https://github.com/RudyVenguswamy&gt;@RudyVenguswamy&lt;/denchmark-link&gt;
 the 128 dimensional representations out of the projection head are supposed to be invariant to a lot of transforms and thus not good for downstream tasks. The recommended representations to user are the ones directly out of the resnet-50 encoder.
Also, I looked at the "UC Merced land use dataset" and the satellite images in the dataset seem to have quite a different distribution as compared to images from the imagenet dataset. So, I wouldn't be entirely sure of SimCLR having a good prediction accuracy on these images.
		</comment>
		<comment id='2' author='RudyVenguswamy' date='2020-11-12T17:22:48Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 it seems that the 128 representation is  at representing the images, without any training. I don't believe the initialization of the weights for SimCLR uses pretrained embeddings - or does it actually use pretrained weights from imagenet, which might explain the 25-40% accuracy with no training needed on a dataset with 21 classes?
		</comment>
		<comment id='3' author='RudyVenguswamy' date='2020-11-12T22:31:38Z'>
		&lt;denchmark-link:https://github.com/RudyVenguswamy&gt;@RudyVenguswamy&lt;/denchmark-link&gt;
 the initialization of SimCLR doesn't use Imagenet weights. Also, like I mentioned the 2048 dim embeddings from the resnet-50 encoder is recommended to be used in any task. Right now the implementation doesn't flatten this (2048 * 2 * 2). The fix will change this to a flattened 2048 dim vector.
Can you try the 2048 dimensional embeddings instead? Also, like we mentioned in the mail, you can use swav as well instead of simclr.
		</comment>
		<comment id='4' author='RudyVenguswamy' date='2020-11-14T21:58:31Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 I redid it with the 2048 embedding. The shape of the embedding is 2048 * 16 * 16 so flattening it took too much memory per datapoint so I took a slice of length 2048 from it. This results in 9% accuracy before training occurs (21 classes, guessing is ~4.7%).
		</comment>
		<comment id='5' author='RudyVenguswamy' date='2020-11-16T06:46:37Z'>
		&lt;denchmark-link:https://github.com/RudyVenguswamy&gt;@RudyVenguswamy&lt;/denchmark-link&gt;
 what is the size of input image to your network? Even in the old setup, the final representations should not be as large as 2048 * 16 * 16. Also, use Avgpool in python to reduce spatial dim to (1, 1) and then flatten.
		</comment>
		<comment id='6' author='RudyVenguswamy' date='2020-11-24T08:28:59Z'>
		&lt;denchmark-link:https://github.com/RudyVenguswamy&gt;@RudyVenguswamy&lt;/denchmark-link&gt;
 is this still a bug?
		</comment>
		<comment id='7' author='RudyVenguswamy' date='2021-01-06T07:55:20Z'>
		&lt;denchmark-link:https://github.com/RudyVenguswamy&gt;@RudyVenguswamy&lt;/denchmark-link&gt;
 feel free to reopen if needed 
		</comment>
	</comments>
</bug>