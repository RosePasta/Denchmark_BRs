<bug id='154' author='bhokaal' open_date='2020-08-10T05:46:50Z' closed_time='2020-11-02T11:50:09Z'>
	<summary>For Pytorch 1.6 the LARS implementation seems incorrect?</summary>
	<description>
I tried using the LARS optimizer for SimCLR but the training was not giving desirable result. Looked into the code and observed in the line below-



pytorch-lightning-bolts/pl_bolts/optimizers/layer_adaptive_scaling.py


         Line 114
      in
      dc3ac7c






 buf.mul_(momentum).add(actual_lr, d_p + weight_decay * p.data) 





we have add() vs. add_()?
Thanks
	</description>
	<comments>
		<comment id='1' author='bhokaal' date='2020-08-10T05:47:30Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='bhokaal' date='2020-08-10T06:48:12Z'>
		&lt;denchmark-link:https://github.com/bhokaal&gt;@bhokaal&lt;/denchmark-link&gt;
 we are adding a new version for this pretty soon. We will be removing this implementation of LARS.
		</comment>
		<comment id='3' author='bhokaal' date='2020-08-13T17:24:57Z'>
		&lt;denchmark-link:https://github.com/bhokaal&gt;@bhokaal&lt;/denchmark-link&gt;
 for now we are experimenting between:

https://github.com/kakaobrain/torchlars
LARC found in apex

you can start using one of those two in you experiments. I am updating the simclr branch as the we get results from different runs
		</comment>
		<comment id='4' author='bhokaal' date='2020-09-14T13:52:27Z'>
		Any news on this?
I see a new &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/optimizers/lars_scheduling.py&gt;implementation&lt;/denchmark-link&gt;
. Has it been tested?
		</comment>
		<comment id='5' author='bhokaal' date='2020-10-15T13:43:27Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 ^^ ?
		</comment>
		<comment id='6' author='bhokaal' date='2020-11-02T11:50:09Z'>
		yes the current LARS implementation works fine in the tests we have run.
		</comment>
	</comments>
</bug>