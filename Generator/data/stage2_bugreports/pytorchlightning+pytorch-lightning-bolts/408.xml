<bug id='408' author='koszpe' open_date='2020-11-26T12:33:10Z' closed_time='2020-12-08T08:25:45Z'>
	<summary>Is cifar10 num_samples correct?</summary>
	<description>



pytorch-lightning-bolts/pl_bolts/datamodules/cifar10_datamodule.py


         Line 98
      in
      ecbb82a






 self.num_samples = 60000 - val_split 





The self.num_samples = 60000 - val_split may be wrong, the dataset indeed contains 60000 examples, but it includes the test set too, which size is 10000.
So I think the correct size would be 50000 - val_split
I checked it with val_split=5000, and the train_length here is 50000:



pytorch-lightning-bolts/pl_bolts/datamodules/cifar10_datamodule.py


         Line 122
      in
      ecbb82a






 train_length = len(dataset) 





the len(dataset_train) is 45000 (not 55000) here:



pytorch-lightning-bolts/pl_bolts/datamodules/cifar10_datamodule.py


         Line 123
      in
      ecbb82a






 dataset_train, _ = random_split( 





It may affects the SimCLR performance, it is used here:



pytorch-lightning-bolts/pl_bolts/models/self_supervised/simclr/simclr_module.py


         Line 400
      in
      ecbb82a






 args.num_samples = dm.num_samples 





after here:



pytorch-lightning-bolts/pl_bolts/models/self_supervised/simclr/simclr_module.py


         Line 136
      in
      ecbb82a






 self.train_iters_per_epoch = self.num_samples // global_batch_size 





So it influences the lr schedule.
	</description>
	<comments>
		<comment id='1' author='koszpe' date='2020-11-26T18:17:23Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 mind have a look? :]
		</comment>
	</comments>
</bug>