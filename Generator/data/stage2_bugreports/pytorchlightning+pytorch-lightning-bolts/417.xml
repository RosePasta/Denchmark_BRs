<bug id='417' author='senarvi' open_date='2020-12-01T11:00:19Z' closed_time='2020-12-01T13:00:23Z'>
	<summary>All data modules don't support providing batch size in the constructor.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Half of the data loaders don't support providing batch size in the constructor, but only in the {train,val,test}_dataloader() methods. This means that if a data module is passed Trainer.fit(), the default batch size is used.
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/cifar10_datamodule.py&gt;Cifar10DataModule&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/cityscapes_datamodule.py&gt;CityscapesDataModule&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/imagenet_datamodule.py&gt;ImagenetDataModule&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/kitti_datamodule.py&gt;KittiDataModule&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/stl10_datamodule.py&gt;STL10DataModule&lt;/denchmark-link&gt;
 take batch size in constructor, which is used by the  methods.
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/mnist_datamodule.py&gt;MNISTDataModule&lt;/denchmark-link&gt;
 takes batch size in constructor, but it's not used.  methods always use the default value 32.
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/binary_mnist_datamodule.py&gt;BinaryMNISTDataModule&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/fashion_mnist_datamodule.py&gt;FashionMNISTDataModule&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/sklearn_datamodule.py&gt;SklearnDataModule&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/ssl_imagenet_datamodule.py&gt;SSLImagenetDataModule&lt;/denchmark-link&gt;
,  and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/datamodules/vocdetection_datamodule.py&gt;VOCDetectionDataModule&lt;/denchmark-link&gt;
 don't take batch size in constructor. These have  methods that take a batch size argument, but a trainer always calls them without arguments.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

To illustrate the problem, the  argument has no effect in the the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/0.2.5/pl_bolts/models/detection/faster_rcnn.py#L126&gt;FasterRCNN&lt;/denchmark-link&gt;
 model.
Steps to reproduce the behavior:

Add print(batch_size) to VOCDetectionDataModule.train_dataloader().
Run python faster_rcnn.py --batch_size 10.
See that 1 is printed instead of 10.

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The batch size argument should be passed in the dataloader calls. This could be fixed by calling
&lt;denchmark-code&gt;trainer.fit(model, datamodule.train_dataloader(batch_size), datamodule.val_dataloader(batch_size))
&lt;/denchmark-code&gt;

instead of
&lt;denchmark-code&gt;trainer.fit(model, datamodule)
&lt;/denchmark-code&gt;

I guess it would be better to fix the data modules so that they take batch size in the constructor, though.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.7.0
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): conda
Python version: 3.8.5

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

The &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html&gt;DataModule documentation&lt;/denchmark-link&gt;
 could be clearer regarding to how to set the batch size. The first MNISTDataModule example takes the batch size in constructor and saves it in the  attribute, which is used by . In the second example, the  methods take the batch size as an argument. However, if  is called with a data module, the only way to set the batch size is in the data module constructor.
	</description>
	<comments>
		<comment id='1' author='senarvi' date='2020-12-01T11:01:03Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='senarvi' date='2020-12-01T12:50:57Z'>
		&lt;denchmark-link:https://github.com/senarvi&gt;@senarvi&lt;/denchmark-link&gt;
 Thank you for reporting the issue! Actually, &lt;denchmark-link:https://github.com/hecoding&gt;@hecoding&lt;/denchmark-link&gt;
 has pointed it out in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/issues/334&gt;#334&lt;/denchmark-link&gt;
 and added  argument to all datamodules in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/pull/344&gt;#344&lt;/denchmark-link&gt;
, and the fixes should be merged into the main branch very soon.

the --batch_size argument has no effect in the the FasterRCNN model.

You're right. We are trying to decouple datamodules from models in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/issues/207&gt;#207&lt;/denchmark-link&gt;
, and it seems we haven't removed those data-related arguments (e.g. ) from  as you pointed out.

In the second example, the {train,val,test}_dataloader() methods take the batch size as an argument.

Which example are you referring to?
		</comment>
		<comment id='3' author='senarvi' date='2020-12-01T13:00:22Z'>
		
@senarvi Thank you for reporting the issue! Actually, @hecoding has pointed it out in #334 and added batch_size argument to all datamodules in #344, and the fixes should be merged into the main branch very soon.

Great, then I'll close this issue!


In the second example, the {train,val,test}_dataloader() methods take the batch size as an argument.

Which example are you referring to?

Never mind, actually the second example ("Here‚Äôs a more realistic, complex DataModule") always uses batch size 32.
		</comment>
	</comments>
</bug>