<bug id='235' author='dandelin' open_date='2020-09-16T17:33:08Z' closed_time='2020-09-16T22:49:07Z'>
	<summary>SiameseArm has no pooler</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/self_supervised/byol/models.py#L22&gt;SiameseArm&lt;/denchmark-link&gt;
 has no pooler for the feature extracted from .
It works fine for small enough images that yield 1x1 spatial resolution features, but self.projector will not be able to accept the .view(#batch, -1) sized features when the resolution gets bigger.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Using higher-resolution datasets such as ImageNet will yield size mismatch error.

&lt;denchmark-code&gt;  | Name           | Type       | Params
----------------------------------------------
0 | online_network | SiameseArm | 35 M
1 | target_network | SiameseArm | 35 M
Traceback (most recent call last):
  File "byol_test.py", line 17, in &lt;module&gt;
    trainer.fit(model, dm)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 324, in fit
    results = self.accelerator_backend.train()
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/accelerators/cpu_backend.py", line 48, in train
    results = self.train_or_test()
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/accelerators/base_backend.py", line 36, in train_or_test
    results = self.trainer.train()
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 343, in train
    self.run_sanity_check(self.get_model())
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 518, in run_sanity_check
    _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 449, in run_evaluation
    output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 145, in evaluation_step
    output = self.trainer.accelerator_backend.validation_step(args)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/pytorch_lightning/accelerators/cpu_backend.py", line 64, in validation_step
    output = self.trainer.model.validation_step(*args)
  File "/data2/dsets/pytorch-lightning-bolts/pl_bolts/models/self_supervised/byol/byol_module.py", line 145, in validation_step
    loss_a, loss_b, total_loss = self.shared_step(batch, batch_idx)
  File "/data2/dsets/pytorch-lightning-bolts/pl_bolts/models/self_supervised/byol/byol_module.py", line 118, in shared_step
    y1, z1, h1 = self.online_network(img_1)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data2/dsets/pytorch-lightning-bolts/pl_bolts/models/self_supervised/byol/models.py", line 38, in forward
    z = self.projector(y)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data2/dsets/pytorch-lightning-bolts/pl_bolts/models/self_supervised/byol/models.py", line 18, in forward
    x = self.model(x)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 91, in forward
    return F.linear(input, self.weight, self.bias)
  File "/data1/users/dandelin/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/torch/nn/functional.py", line 1676, in linear
    output = input.matmul(weight.t())
RuntimeError: size mismatch, m1: [32 x 100352], m2: [2048 x 4096] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;model = BYOL(num_classes=1000, input_height=224)
dm = ImagenetDataModule(...)
trainer = pl.Trainer()
trainer.fit(model, dm)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The feature extracted from self.encoder should be aggregated via some pooler.
For BYOL, we can simply use nn.AdaptiveAvgPool2d.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.6.0
OS (e.g., Linux): Ubuntu 18.04
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.7.7
CUDA/cuDNN version: 10.2
GPU models and configuration: P40
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='dandelin' date='2020-09-16T17:33:51Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
	</comments>
</bug>