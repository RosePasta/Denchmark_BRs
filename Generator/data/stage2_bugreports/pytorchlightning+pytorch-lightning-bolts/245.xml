<bug id='245' author='igormq' open_date='2020-09-22T16:05:36Z' closed_time='2020-12-16T16:41:29Z'>
	<summary>pin_memory should be true only if gpus are specified</summary>
	<description>
Hey folks,
First of all, thank you for this amazing package. It has saved many hours of coding :)
I was using for the first time the pl data modules when I hit the following line
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/c2cdad7c12e4d33ed39db808879706c608c1c7/pl_bolts/datamodules/cifar10_datamodule.py#L132&gt;cifar10_datamodule.py#L132&lt;/denchmark-link&gt;
. The problem is: when I am training/evaluating/testing on CPU, the data module try to pin the data and try to reserve a batch size memory amount in gpu. Nothing goes wrong if I have GPU with enough memory ran, but, when I try to run in CPU mode with all my GPUs in use I get the following error:
&lt;denchmark-code&gt;RuntimeError: Caught RuntimeError in pin memory thread for device 0.
Original Traceback (most recent call last):
  File "/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py", line 31, in _pin_memory_loop
    data = pin_memory(data)
  File "/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py", line 55, in pin_memory
    return [pin_memory(sample) for sample in data]
  File "lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py", line 55, in &lt;listcomp&gt;
    return [pin_memory(sample) for sample in data]
  File "lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py", line 47, in pin_memory
    return data.pin_memory()
RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1595629427478/work/aten/src/THC/THCCachingHostAllocator.cpp:278
&lt;/denchmark-code&gt;

The fix is simple and we can borrow the code from the pl core implementation as in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0045119b3fd4532bf8edf17dcbaeba9bc66ba25b/pytorch_lightning/accelerators/accelerator_connector.py#L90&gt;pytorch_lightning/accelerators/accelerator_connector.py#L90&lt;/denchmark-link&gt;

Bests.
	</description>
	<comments>
		<comment id='1' author='igormq' date='2020-09-22T16:06:20Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='igormq' date='2020-09-22T18:46:48Z'>
		&lt;denchmark-link:https://github.com/igormq&gt;@igormq&lt;/denchmark-link&gt;
 mind sending a PR? if not I can address this.
		</comment>
		<comment id='3' author='igormq' date='2020-09-22T18:47:35Z'>
		I think the main thing here is to just include the pin_memory flag in the datamodules. I think by default it makes sense to set it to True either way? please correct me if I'm wrong ðŸ˜„
		</comment>
		<comment id='4' author='igormq' date='2020-11-08T07:21:51Z'>
		&lt;denchmark-link:https://github.com/nateraw&gt;@nateraw&lt;/denchmark-link&gt;
 I think even beyond the  flag, some other flags should also be included such as the  and  flag. What do you think? I can work on a pull request for updating the flags in the datamodules.
		</comment>
		<comment id='5' author='igormq' date='2020-11-18T03:04:01Z'>
		&lt;denchmark-link:https://github.com/briankosw&gt;@briankosw&lt;/denchmark-link&gt;
 sure! for the  and  flags, lets just make sure we use sensible defaults.   Thanks!!
		</comment>
	</comments>
</bug>