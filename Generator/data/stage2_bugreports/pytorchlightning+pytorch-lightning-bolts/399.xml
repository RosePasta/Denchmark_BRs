<bug id='399' author='sid-sundrani' open_date='2020-11-24T07:15:02Z' closed_time='2020-12-07T00:58:03Z'>
	<summary>Replay buffer not cleared after each epoch in reinforce</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The replay buffer (states, actions, qvals lists) in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/rl/reinforce_model.py&gt;Reinforce&lt;/denchmark-link&gt;
 is not reinitialized after each pseudo epoch (training batch). The   parameter controls the number of episodes to rollout in each training batch. However, since the buffer isn't cleared each training batch has all previous episodes instead of just  . I just wanted to check if this is intended?
From what I understand, the replay buffer is to be cleared after each epoch so that the agent can be trained on trajectory data obtained under the current policy rather than from all policies that came before it. And the &lt;denchmark-link:https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter11/02_cartpole_reinforce.py&gt;script&lt;/denchmark-link&gt;
 cited in reinforce_model.py does accordingly clear the replay buffer as well.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;from pl_bolts.models.rl.reinforce_model import Reinforce
from pytorch_lightning import Trainer

model=Reinforce(env='CartPole-v0', num_batch_episodes=1, total_steps=2, epoch_len=6)

trainer = Trainer(max_epochs=1)
trainer.fit(model)
# one sample from training data 
old_state = model.batch_states[2] 

trainer = Trainer(max_epochs=1)
trainer.fit(model)
# one sample from training data from next epoch 
new_state = model.batch_states[2]
# if old state is equal to new, replay buffer hasnt be reset. Training on trajectory data from old policies 
print((old_state == new_state).all())
&lt;/denchmark-code&gt;

	</description>
	<comments>
	</comments>
</bug>