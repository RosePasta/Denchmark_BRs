<bug id='3' author='842974287' open_date='2019-08-05T21:22:23Z' closed_time='2019-08-13T05:45:51Z'>
	<summary>Balance Module</summary>
	<description>
&lt;denchmark-h:h1&gt;üêû Bug&lt;/denchmark-h&gt;

Balance by time didn't ignore the time for CUDA initialization. I think adding a warm-up run can eliminate the effect of that.
Balance by size actually accumulate the size so that the latter layer always consumes more memory than the previous layers.
&lt;denchmark-h:h2&gt;Code that reproduces&lt;/denchmark-h&gt;

Change the test to the following can reproduce for balance by size.
def test_balance_by_size():
    class Expand(nn.Module):
        def __init__(self, times):
            super().__init__()
            self.times = times

        def forward(self, x):
            for i in range(self.times):
                x = x + torch.rand_like(x, requires_grad=True)
            return x

    model = nn.Sequential(*[Expand(i) for i in [6, 5, 4, 3, 2, 1]])
    sample = torch.rand(10, 100, 100)
    balance = balance_by_size(model, sample, partitions=2, device='cuda')
    assert balance == [2, 4]
	</description>
	<comments>
		<comment id='1' author='842974287' date='2019-08-06T04:12:39Z'>
		Hi, thanks for reporting those bugs.
For the first report, do you mean the initialization time about 5 seconds per device? If you do, balance_by_time doesn't include the time in its profiling result. The initialization is done at sample, device = utils.concentrate_on_device(module, sample, device) line, before the actual profiling.
For the second report, I reproduced the bug at  you reported. The reason I guess is that I misunderstood . I asked about it &lt;denchmark-link:https://discuss.pytorch.org/t/why-max-memory-allocated-after-reset-doesnt-return-0/52626/1&gt;in the forum&lt;/denchmark-link&gt;
. When I get a clear answer, I'll make a patch.
Thank you.
		</comment>
		<comment id='2' author='842974287' date='2019-08-06T16:57:54Z'>
		For the first one, it happens when some layers contain matmul operations (or other similar operations). It's because the the Nvidia fast GPU-accelerated library of standard basic linear algebra subroutines (cuBLAS) loads lazily when it first being used, so that the first operation involves this library will cost much longer time.
For the following model, if print the costs of each layer that calculated in the balancing module, you can observe a significant difference in their time cost.
model = nn.Sequential(nn.Linear(3, 3), nn.Linear(3, 3))
		</comment>
		<comment id='3' author='842974287' date='2019-08-07T05:49:20Z'>
		I reproduced your example:
&gt;&gt;&gt; profile_times(model, sample)
[193178, 170848]
The purpose of torchgpipe_balancing is not the optimal balancer. If it is, GPipe shouldn't take balance as a parameter. Please treat it as a naive but handy tool to make you focus on the model architecture. If you know the certain balance, just hardcode it at the balance parameter.
I think it's better to describe the weakness of torchgpipe_balancing in the documentation, rather than fixing the problem you reported.
		</comment>
		<comment id='4' author='842974287' date='2019-08-07T22:31:38Z'>
		That makes sense, thanks!
		</comment>
		<comment id='5' author='842974287' date='2019-08-13T05:45:51Z'>
		I fixed this issue with &lt;denchmark-link:https://github.com/kakaobrain/torchgpipe/commit/3011ff0613fc91606c637d5df139e44b2d3bfaa5&gt;3011ff0&lt;/denchmark-link&gt;
. If you still have the same issue, please reopen it.
		</comment>
	</comments>
</bug>