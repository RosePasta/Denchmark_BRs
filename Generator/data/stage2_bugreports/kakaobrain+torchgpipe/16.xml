<bug id='16' author='YHRen' open_date='2020-08-18T22:12:31Z' closed_time='2020-09-22T10:56:49Z'>
	<summary>Work with DistributedDataParallel</summary>
	<description>
&lt;denchmark-h:h1&gt;üêû Bug&lt;/denchmark-h&gt;

likely a new feature:
On a 6 GPU node, group the first 3 as one pipeline, and the rest 3 as the other.
Two model replica are communicated using nn.parallel.DistributedDataParallel.
&lt;denchmark-h:h2&gt;Code that reproduces&lt;/denchmark-h&gt;

        from nn.parallel imort DistributedDataParallel as DDP
        model = GPipe(model, balance=[1, 1, 2], devices=devices, chunks=CSZ)
        model = DDP(model)
the full version can be found here:
&lt;denchmark-link:https://github.com/YHRen/gpipe_demo/blob/master/main.py&gt;https://github.com/YHRen/gpipe_demo/blob/master/main.py&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
Traceback (most recent call last):
  File "main_dist.py", line 155, in &lt;module&gt;
  File "main_dist.py", line 155, in &lt;module&gt;
    loss.backward()
    loss.backward()
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/tensor.py", line 166, in backward
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/tensor.py", line 166, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/autograd/function.py", line 77, in apply
    allow_unreachable=True)  # allow_unreachable flag
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/autograd/function.py", line 77, in apply
    return self._forward_cls.backward(self, *args)
    return self._forward_cls.backward(self, *args)
  File "/ccsopen/home/yren/.local/lib/python3.6/site-packages/torchgpipe/checkpoint.py", line 269, in backward
  File "/ccsopen/home/yren/.local/lib/python3.6/site-packages/torchgpipe/checkpoint.py", line 269, in backward
    torch.autograd.backward(tensors, grad_output)
    torch.autograd.backward(tensors, grad_output)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /opt/anaconda/conda-bld/pytorch-base_1594299597148/work/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch. 
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /opt/anaconda/conda-bld/pytorch-base_1594299597148/work/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch. 
Traceback (most recent call last):
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/distributed/launch.py", line 253, in &lt;module&gt;
    main()
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/distributed/launch.py", line 249, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/bin/python', '-u', 'main_dist.py', '--local_rank=1', '-m', 'cnn', '-b', '32', '-c', '4', '-d', '2048', '-w', '128', '-l', '5', '-e', '2', '--dist', '--gpus_per_group', '3', '--group_per_node', '2']' returned non-zero exit status 1.

&lt;/denchmark-code&gt;

If we wrap another way around:
        model = model.cuda() # default cuda id has been handled per rank. 
        model = DDP(model)
        model = GPipe(model, balance=[1, 1, 2], devices=devices, chunks=CSZ)
The error will complain all tensors must be on the same device.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "main_dist.py", line 135, in &lt;module&gt;
    model = DDP(model)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 325, in __init__
    self._ddp_init_helper()
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 343, in _ddp_init_helper
    self._module_copies = replicate(self.module, self.device_ids, detach=True)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/nn/parallel/replicate.py", line 96, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/nn/parallel/replicate.py", line 75, in _broadcast_coalesced_reshape
    return comm.broadcast_coalesced(tensors, devices)
  File "/sw/ascent/ibm-wml-ce/anaconda-base/envs/ibm-wml-ce-1.7.0-0/lib/python3.6/site-packages/torch/cuda/comm.py", line 39, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
RuntimeError: all tensors must be on devices[0]

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;GPU: _CudaDeviceProperties(name='Tesla V100-SXM2-16GB', major=7, minor=0, total_memory=16128MB, multi_processor_count=80)
Number of GPUs: 6
CUDA: 10.2.89
cuDNN: 7605
Python: 3.6.10
PyTorch: 1.3.1
torchgpipe: 0.0.6

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I'm wondering if it is possible to run "DDP" for each split in the model pipeline.
Thank you.
	</description>
	<comments>
		<comment id='1' author='YHRen' date='2020-08-18T22:59:25Z'>
		related issues in pytorch:
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues?q=is%3Aissue+RuntimeError++has_marked_unused_parameters&gt;https://github.com/pytorch/pytorch/issues?q=is%3Aissue+RuntimeError++has_marked_unused_parameters&lt;/denchmark-link&gt;
_
		</comment>
		<comment id='2' author='YHRen' date='2020-08-19T05:37:58Z'>
		Currently we do not have any plan yet to support distributed training yet.
(Also see &lt;denchmark-link:https://github.com/kakaobrain/torchgpipe/issues/7&gt;#7&lt;/denchmark-link&gt;
.)
Despite this, the error seems to arise from that reentrant backward may not be compatible with DDP yet (up to the latest version torch==1.6.0). For example, see &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/24005&gt;this issue&lt;/denchmark-link&gt;
. Since PyTorch's &lt;denchmark-link:https://pytorch.org/docs/stable/checkpoint.html&gt;checkpoint&lt;/denchmark-link&gt;
 and the internal checkpointing of  both involves reentrant backwards, one may encounter the following error (from &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/752f433a2484db25f076b2fc85c40ab191656bd9/torch/csrc/distributed/c10d/reducer.cpp#L482-L511&gt;the source&lt;/denchmark-link&gt;
, v1.6.0):
&lt;denchmark-code&gt;RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the 
following reasons: 1) Use of a module parameter outside the `forward` function. Please make 
sure model parameters are not shared across multiple concurrent forward-backward passes 
2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple 
`checkpoint` functions to wrap the same part of your model, it would result in the same set of 
parameters been used by different reentrant backward passes multiple times, and hence 
marking a variable ready multiple times. DDP does not support such use cases yet. 
3) Incorrect unused parameter detection. The return value of the `forward` function is 
inspected by the distributed data parallel wrapper to figure out if any of the module's 
parameters went unused. For unused parameters, DDP would not expect gradients from then. 
However, if an unused parameter becomes part of the autograd graph at a later point in time 
(e.g., in a reentrant backward when using `checkpoint`), the gradient will show up unexpectedly. 
If all parameters in the model participate in the backward pass, you can disable unused 
parameter detection by passing the keyword argument `find_unused_parameters=False` to 
`torch.nn.parallel.DistributedDataParallel`.
&lt;/denchmark-code&gt;

Further investigation is needed to figure out the exact reason for the error.
One way to avoid this issue is to disable checkpointing by setting checkpoint='never'. In this case GPipe module can be dealt as other model parallel modules and DDP may work as expected.
On the other hand, wrapping other way around does not work since GPipe will try to move each partition to different devices, which conflicts with the DDP module wrapping it.
		</comment>
		<comment id='3' author='YHRen' date='2020-08-19T06:18:00Z'>
		Hi &lt;denchmark-link:https://github.com/chiheonk&gt;@chiheonk&lt;/denchmark-link&gt;

Thank you so much for your detailed explanation!
I see. If I understand correctly, the recompute (checkpoint) for previous chunk will be "destroyed" by the next chunk of microbatch.

One way to avoid this issue is to disable checkpointing by setting checkpoint='never'. In this case GPipe module can be dealt as other model parallel modules and DDP may work as expected.

This looks like a promising way. The effect would be maximized memory footprint, correct?
Could you instruct me how to set checkpoint='never'?
Thank you!
		</comment>
		<comment id='4' author='YHRen' date='2020-08-23T08:13:26Z'>
		Sorry for the late response.

I see. If I understand correctly, the recompute (checkpoint) for previous chunk will be "destroyed" by the next chunk of microbatch.

Yes. Computation graph of a partition will be built and destroyed during backpropagation for several times (due to checkpointing), hence multiple gradient accumulations on the corresponding parameters will occur and it causes the error in DDP.

This looks like a promising way. The effect would be maximized memory footprint, correct?

Yes.

Could you instruct me how to set checkpoint='never'?

Simply change your code as follows:
model = GPipe(model, balance=[1, 1, 2], devices=devices, chunks=CSZ, checkpoint='never')
		</comment>
		<comment id='5' author='YHRen' date='2020-08-25T17:49:53Z'>
		Dear &lt;denchmark-link:https://github.com/sublee&gt;@sublee&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/chiheonk&gt;@chiheonk&lt;/denchmark-link&gt;

I have tested your suggested solution of using checkpoint='never' .
I confirm it works without any errors.
My minimum demo is here: &lt;denchmark-link:https://github.com/YHRen/gpipe_demo&gt;https://github.com/YHRen/gpipe_demo&lt;/denchmark-link&gt;

I really like your work and thank you so much for your responses.
Please feel free to close the issue.
		</comment>
	</comments>
</bug>