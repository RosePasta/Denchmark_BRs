<bug id='474' author='IrisDinge' open_date='2020-12-29T01:36:00Z' closed_time='2021-01-05T19:02:40Z'>
	<summary>when default_engine=TensorFlow, Could not create cudnn handle : CUDNN_STATUS_ALLOC_FAILED</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

when run
..\gradlew run -Dmain=ai.djl.example.inference.ObjectDetectionWithTensorFlowSavedModel -Dai.djl.default_engine=TensorFlow
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

should do inference like default_engine=PyTorch or mxnet
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

2020-12-28 17:47:18.897853: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
2020-12-28 17:47:18.898377: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
Exception in thread "main" org.tensorflow.exceptions.TensorFlowException: 2 root error(s) found.
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;


https://blog.csdn.net/u013421629/article/details/104838307
tensorflow/tensorflow#39989

&lt;denchmark-h:h2&gt;Environment Info&lt;/denchmark-h&gt;

Windows 10
------------------ CUDA -----------------
GPU Count: 2
Default Device: gpu(0)
CUDA: 101
ARCH: 75
GPU(0) memory used: 1706098688 bytes
GPU(1) memory used: 2084134934 bytes
----------------- Engines ---------------
Default Engine: MXNet
[DEBUG] - Using cache dir: C:\Users\TME-DJ.djl.ai\mxnet
MXNet:1.7.0, capabilities: [
CUDA,
CUDNN,
SIGNAL_HANDLER,
LAPACK,
BLAS_OPEN,
CUDA_RTC,
OPENMP,
OPENCV,
MKLDNN,
]
MXNet Library: C:\Users\TME-DJ.djl.ai\mxnet\1.7.0-backport-cu101mkl-win-x86_64\mxnet.dll
--------------- Hardware --------------
Available processors (cores): 16
Byte Order: LITTLE_ENDIAN
Free memory (bytes): 1044731184
Maximum memory (bytes): 17154703360
Total memory available to JVM (bytes): 1073741824
Heap committed: 1073741824
Heap nonCommitted: 32309248
TensorFlow: native-auto, 2.3.1
Q: could u pls show me how to set something like 'allow_growth=True' in DJL?
(write tensorflow 2.X in python have never see this bug before
Thks in adv.
	</description>
	<comments>
		<comment id='1' author='IrisDinge' date='2020-12-30T19:19:19Z'>
		I tried on Windows 2019, I'm not able to reproduce your error.
&lt;denchmark-code&gt;cd examples
..\gradlew run -Dmain=ai.djl.examples.inference.ObjectDetectionWithTensorflowSavedModel -Dai.djl.default_engine=TensorFlow

[INFO ] - Detected objects image has been saved in: build/output/detected-tensorflow-model-dog_bike_car.png
[INFO ] - [
        class: "bicycle", probability: 0.80220, bounds: [x=0.147, y=0.209, width=0.576, height=0.603]
        class: "car", probability: 0.73779, bounds: [x=0.596, y=0.145, width=0.297, height=0.149]
        class: "dog", probability: 0.72259, bounds: [x=0.172, y=0.397, width=0.261, height=0.548]
]
&lt;/denchmark-code&gt;

Would you please run the following command:
&lt;denchmark-code&gt;cd djl
gradlew debugEnv -Dai.djl.default_engine=TensorFlow
&lt;/denchmark-code&gt;

It should print something like:
&lt;denchmark-code&gt;[DEBUG] - Using cache dir: C:\Users\Administrator\.djl.ai\tensorflow
TensorFlow:2.3.1, capabilities: [
        MKL,
        CUDA,
]
TensorFlow Library: C:\Users\Administrator\.djl.ai\tensorflow\2.3.1-cu101-win-x86_64\jnitensorflow.dll
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='IrisDinge' date='2020-12-30T19:33:19Z'>
		&lt;denchmark-link:https://github.com/IrisDinge&gt;@IrisDinge&lt;/denchmark-link&gt;

By the way, you can access all low level TF options: org.tensorflow.proto.framework.GPUOptions
See: &lt;denchmark-link:https://www.javadoc.io/doc/org.tensorflow/proto/1.1.0-rc2/org/tensorflow/framework/GPUOptions.html&gt;https://www.javadoc.io/doc/org.tensorflow/proto/1.1.0-rc2/org/tensorflow/framework/GPUOptions.html&lt;/denchmark-link&gt;

There are twos way to pass ConfigProto to the model:

directly use Model.load(Path modelPath, String prefix, Map&lt;String, ?&gt; options) API

&lt;denchmark-code&gt;Model model = Model.newInstance("MyModel");
Map&lt;String, Object&gt; options = new HashMap&lt;&gt;();
ConfigProto configProto =ConfigProto.newBuilder().setGpuOptions(GPUOptions.newBuilder().build()).build();
options.put("ConfigProto", configProto); 
model.load(modelPath, prefix, options);
&lt;/denchmark-code&gt;


use Criteria API

&lt;denchmark-code&gt;ConfigProto configProto =ConfigProto.newBuilder().setGpuOptions(GPUOptions.newBuilder().build()).build();
ByteArrayOutputStream bos = new ByteArrayOutputStream();
configProto.writeTo(bos);
bos.close();
String str = new String(Base64.getEncoder().encode(bos.toByteArray()));

Criteria&lt;Image, DetectedObjects&gt; criteria = Criteria.builder()
                .optApplication(Application.CV.OBJECT_DETECTION)
                .setTypes(Image.class, DetectedObjects.class)
                .optOptions("ConfigProto", str)
                .build();
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='IrisDinge' date='2020-12-31T03:18:45Z'>
		
I tried on Windows 2019, I'm not able to reproduce your error.
cd examples
..\gradlew run -Dmain=ai.djl.examples.inference.ObjectDetectionWithTensorflowSavedModel -Dai.djl.default_engine=TensorFlow

[INFO ] - Detected objects image has been saved in: build/output/detected-tensorflow-model-dog_bike_car.png
[INFO ] - [
        class: "bicycle", probability: 0.80220, bounds: [x=0.147, y=0.209, width=0.576, height=0.603]
        class: "car", probability: 0.73779, bounds: [x=0.596, y=0.145, width=0.297, height=0.149]
        class: "dog", probability: 0.72259, bounds: [x=0.172, y=0.397, width=0.261, height=0.548]
]

Would you please run the following command:
cd djl
gradlew debugEnv -Dai.djl.default_engine=TensorFlow

It should print something like:
[DEBUG] - Using cache dir: C:\Users\Administrator\.djl.ai\tensorflow
TensorFlow:2.3.1, capabilities: [
        MKL,
        CUDA,
]
TensorFlow Library: C:\Users\Administrator\.djl.ai\tensorflow\2.3.1-cu101-win-x86_64\jnitensorflow.dll


Thk for ur reply;)
Yes, i got the result as same as urs:
[DEBUG] - Using cache dir: C:\Users\TME-DJ.djl.ai\tensorflow
TensorFlow:2.3.1, capabilities: [
MKL,
CUDA,
]
TensorFlow Library: C:\Users\TME-DJ.djl.ai\tensorflow\2.3.1-cu101-win-x86_64\jnitensorflow.dll
		</comment>
		<comment id='4' author='IrisDinge' date='2020-12-31T03:33:07Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;

plus, i found something interesting:
Once the server get restart(no one is on it except me, gpu's memory is empty), the example with tensorflow engine can do inference and get the results. However, if someone else begin to use GPU, it'll appear bug aboved, the memory is enough though.
Anyway, i'll try check ur suggestion out. Many thks!!!
		</comment>
		<comment id='5' author='IrisDinge' date='2021-01-05T19:02:40Z'>
		&lt;denchmark-link:https://github.com/IrisDinge&gt;@IrisDinge&lt;/denchmark-link&gt;
 feel free to reopen this issue if you have further questions.
		</comment>
	</comments>
</bug>