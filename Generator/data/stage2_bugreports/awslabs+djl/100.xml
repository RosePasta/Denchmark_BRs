<bug id='100' author='yanchaoguo' open_date='2020-06-14T05:59:26Z' closed_time='2020-06-19T22:51:25Z'>
	<summary>TrainResnetWithCifar10例子中存在一个错误</summary>
	<description>
我按照例子在自己的10分类数据集上进行测试 发现存储的模型无法得到正确的输出 抛出数组越界的错误，经排查： newBlock.add(x -&gt; new NDList(x.singletonOrThrow().squeeze()));这段代码会导致预测时网络出错
应改为newBlock.add(Blocks.batchFlattenBlock());
参考：
&lt;denchmark-link:https://github.com/awslabs/djl/blob/468ba99815d57e17ae349d072da36be1cc31cdbd/model-zoo/src/main/java/ai/djl/basicmodelzoo/cv/classification/ResNetV1.java&gt;https://github.com/awslabs/djl/blob/468ba99815d57e17ae349d072da36be1cc31cdbd/model-zoo/src/main/java/ai/djl/basicmodelzoo/cv/classification/ResNetV1.java&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='yanchaoguo' date='2020-06-14T15:39:40Z'>
		&lt;denchmark-link:https://github.com/yanchaoguo&gt;@yanchaoguo&lt;/denchmark-link&gt;
 I'm working on a fix and adding inference in example.
The last block should be: newBlock.add(Blocks.batchFlattenBlock(10));
		</comment>
		<comment id='2' author='yanchaoguo' date='2020-06-14T16:05:24Z'>
		
@yanchaoguo I'm working on a fix and adding inference in example.
The last block should be: newBlock.add(Blocks.batchFlattenBlock(10));

When will it be submitted? Besides, can you provide an example about image feature extraction to examples
		</comment>
		<comment id='3' author='yanchaoguo' date='2020-06-15T21:31:34Z'>
		Fixed with commit: &lt;denchmark-link:https://github.com/awslabs/djl/commit/1775305152a3611876b0bd3487d9d202761d22ea&gt;1775305&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/yanchaoguo&gt;@yanchaoguo&lt;/denchmark-link&gt;
 for the image feature extraction, the solution you proposed is specific to MXNet. We didn't find a generic and elegant way to implement such an API. Use can implement such a solution as you described themselves. We will keep to explore a solution in future.
		</comment>
		<comment id='4' author='yanchaoguo' date='2020-06-17T01:36:40Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 迁移学习例子中如果将 newBlock.add(x -&gt; new NDList(x.singletonOrThrow().squeeze()));这段代码替换为newBlock.add(Blocks.batchFlattenBlock()); 在我的10分类数据集上进行相同epoch模型训练，会得到更好的准确率
`
SequentialBlock newBlock = new SequentialBlock();
SymbolBlock block = (SymbolBlock) old.getBlock();
block.removeLastBlock();
newBlock.add(block);
//        newBlock.add(x -&gt; new NDList(x.singletonOrThrow().squeeze()));
newBlock.add(Blocks.batchFlattenBlock());
newBlock.add(Linear.builder().setOutChannels(outChannels).build());
// newBlock.add(Blocks.batchFlattenBlock(10));
newBlock.add(Blocks.batchFlattenBlock());
old.setBlock(newBlock);
`
		</comment>
		<comment id='5' author='yanchaoguo' date='2020-06-18T23:24:41Z'>
		Hi &lt;denchmark-link:https://github.com/yanchaoguo&gt;@yanchaoguo&lt;/denchmark-link&gt;
 经讨论我们会改成用batchFlatten()，不过我们觉得并不会影响训练准确度。因为不管是squeeze还是batch flatten都只是改变形状，并不会多学习到一些feature。
请问在你的数据集上准确率能提升多少？跑相同的epoch，准确率的差别会不会是由于初始化的不同导致的？假设给与足够多的epoch数，不管用哪个，理论上应该都能到达相同的准确率。
		</comment>
		<comment id='6' author='yanchaoguo' date='2020-06-19T00:02:35Z'>
		
Hi @yanchaoguo 经讨论我们会改成用batchFlatten()，不过我们觉得并不会影响训练准确度。因为不管是squeeze还是batch flatten都只是改变形状，并不会多学习到一些feature。
请问在你的数据集上准确率能提升多少？跑相同的epoch，准确率的差别会不会是由于初始化的不同导致的？假设给与足够多的epoch数，不管用哪个，理论上应该都能到达相同的准确率。

这是我做的实验  十个类汽车数据集 每个类大学有500张图片  都是训练了50次，结果是batchFlatten()这种方式得到的模型 效果要远远好于另一种，你们也可以测试一下
链接: &lt;denchmark-link:https://pan.baidu.com/s/1C6YFzERknzsYFaVzm5mjug&gt;https://pan.baidu.com/s/1C6YFzERknzsYFaVzm5mjug&lt;/denchmark-link&gt;
 提取码: qx8g  源码工程
		</comment>
		<comment id='7' author='yanchaoguo' date='2020-06-19T22:49:23Z'>
		&lt;denchmark-link:https://github.com/yanchaoguo&gt;@yanchaoguo&lt;/denchmark-link&gt;
 in our transfer learning example, we use MXNet model which doesn't flatten right before the Linear block (fullyconnected) as MXNet &lt;denchmark-link:https://beta.mxnet.io/api/ndarray/_autogen/mxnet.ndarray.FullyConnected.html&gt;fullyconnected&lt;/denchmark-link&gt;
 can do the flatten operation for you. So you need to explicitly add batchFlatten() here. It is pretty common in TensorFlow and PyTorch model as well. Our training API currently follows MXNet style, you can also use Linear block optFlatten(true) to achieve the same goal but it is not recommended. In addition, you don't need the second batchFlatten here because the shape is already like (batchSize, outputChannel * ouputWidth * outputHeight). The shape will remain the same after the second batchFlatten. We have the fix merged in the master &lt;denchmark-link:https://github.com/awslabs/djl/commit/0be801baf6bf5609f58949714e0212e13533e650&gt;0be801b&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='yanchaoguo' date='2020-06-19T22:51:25Z'>
		I will go ahead to close the issue, please feel free to reopen the issue.
		</comment>
	</comments>
</bug>