<bug id='534' author='jinlong-hao' open_date='2021-01-15T09:11:17Z' closed_time='2021-01-17T03:27:20Z'>
	<summary>Error for infference pytorch RNN model with cpu when trainning with gpu and init hidden tensor in forward</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I trainned a rnn model in pytorch with hidden tensor  init in forward method as below and export the model as torchscript.
I infference the model all right on the same computer with gpu support. But when i deploy the model on a new linux computer
use pytorch-native-cpu with out gpu, all goes wrong.
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.i2o = nn.Linear(input_size + hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim = 1)
        
        
    def forward(self, input):
        hidden = self.initHidden().to(input.device)
        for i in range(0, len(input)):
            combined = torch.cat((input[i], hidden), 1)
            hidden = self.i2h(combined)
            output = self.i2o(combined)
            output = self.softmax(output)
        return output
    
    def initHidden(self):
        return torch.zeros(1, self.hidden_size)
    
n_hidden = 128
rnn = RNN(n_letters, n_hidden, n_category).to(device)

example = torch.rand(3, 1, 57).to(device)
hidden = rnn.initHidden().to(device)
trace_script_model = torch.jit.trace(rnn, (example))
trace_script_model.save('./output/char-rnn/char-rnn2/char-rnn2.pt')
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

the infference all righ in both cpu and gpu server.
Criteria&lt;NDList, NDList&gt; criteria =
                Criteria.builder()
                        .setTypes(NDList.class, NDList.class)
                        .optModelUrls("file:///mnt/f/char-rnn2")
                        .optTranslator(new NoopTranslator(){
                            @Override
                            public Batchifier getBatchifier() {
                                return null;
                            }
                        })
                        .build();

        ZooModel&lt;NDList, NDList&gt; model = ModelZoo.loadModel(criteria);
        Predictor&lt;NDList, NDList&gt; predictor = model.newPredictor();

        float[] input = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0,

                1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0,

                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0};

        NDArray inputArray = NDManager.newBaseManager().create(input, new Shape(3, 1, 57));
        NDList ndList = new NDList(inputArray);
        NDList resultList = predictor.predict(ndList);
        NDArray resultArray = resultList.get(0);
        System.out.println(resultArray);
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Exception in thread "main" ai.djl.engine.EngineException: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript, serialized code (most recent call last):
  File "code/__torch__/___torch_mangle_4.py", line 14, in forward
    _2 = self.i2h
    _3 = torch.zeros([1, 128], dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    hidden = torch.to(_3, dtype=6, layout=0, device=torch.device("cuda:0"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)
             ~~~~~~~~ &lt;--- HERE
    input0 = torch.cat([torch.select(input, 0, 0), hidden], 1)
    _4 = (_2).forward(input0, )

Traceback of TorchScript, original code (most recent call last):
&lt;ipython-input-5-69119c0bca28&gt;(12): forward
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\torch\nn\modules\module.py(704): _slow_forward
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\torch\nn\modules\module.py(720): _call_impl
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\torch\jit\__init__.py(1109): trace_module
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\torch\jit\__init__.py(955): trace
&lt;ipython-input-12-0c453926b15f&gt;(3): &lt;module&gt;
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\IPython\core\interactiveshell.py(3417): run_code
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\IPython\core\interactiveshell.py(3337): run_ast_nodes
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\IPython\core\interactiveshell.py(3146): run_cell_async
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\IPython\core\async_helpers.py(68): _pseudo_sync_runner
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\IPython\core\interactiveshell.py(2922): _run_cell
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\IPython\core\interactiveshell.py(2877): run_cell
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\ipykernel\zmqshell.py(536): run_cell
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\ipykernel\ipkernel.py(306): do_execute
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\gen.py(209): wrapper
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\ipykernel\kernelbase.py(545): execute_request
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\gen.py(209): wrapper
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\ipykernel\kernelbase.py(268): dispatch_shell
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\gen.py(209): wrapper
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\ipykernel\kernelbase.py(365): process_one
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\gen.py(748): run
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\gen.py(787): inner
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\ioloop.py(743): _run_callback
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\ioloop.py(690): &lt;lambda&gt;
c:\users\lenovo\appdata\local\programs\python\python37\lib\asyncio\events.py(88): _run
c:\users\lenovo\appdata\local\programs\python\python37\lib\asyncio\base_events.py(1786): _run_once
c:\users\lenovo\appdata\local\programs\python\python37\lib\asyncio\base_events.py(541): run_forever
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\tornado\platform\asyncio.py(149): start
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\ipykernel\kernelapp.py(612): start
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\traitlets\config\application.py(837): launch_instance
c:\users\lenovo\.virtualenvs\02-pytorch-vqj5pwve\lib\site-packages\ipykernel_launcher.py(16): &lt;module&gt;
c:\users\lenovo\appdata\local\programs\python\python37\lib\runpy.py(85): _run_code
c:\users\lenovo\appdata\local\programs\python\python37\lib\runpy.py(193): _run_module_as_main
RuntimeError: Could not run 'aten::empty_strided' with arguments from the 'CUDA' backend. 'aten::empty_strided' is only available for these backends: [CPU, BackendSelect, Autograd, Profiler, Tracer].

	at ai.djl.pytorch.jni.PyTorchLibrary.moduleForward(Native Method)
	at ai.djl.pytorch.jni.IValueUtils.forward(IValueUtils.java:211)
	at ai.djl.pytorch.engine.PtSymbolBlock.forward(PtSymbolBlock.java:110)
	at ai.djl.nn.Block.forward(Block.java:117)
	at ai.djl.inference.Predictor.predict(Predictor.java:117)
	at ai.djl.inference.Predictor.batchPredict(Predictor.java:144)
	at ai.djl.inference.Predictor.predict(Predictor.java:112)
	at cn.eppdev.test.djl.pytorch.AppMain.testCharRnn(AppMain.java:80)
	at cn.eppdev.test.djl.pytorch.AppMain.main(AppMain.java:44)

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;How to Reproduce?&lt;/denchmark-h&gt;

as below
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

(Paste the commands you ran that produced the error.)

export that model in pytorch with cuda
run the java code with pytorch-native-cpu and without gpu.

&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;


I move the hidden tensor to parameter, and implement the loop in java,  all works right. But that's not what i want.

&lt;denchmark-h:h2&gt;Environment Info&lt;/denchmark-h&gt;

trainning evironment:

os: win10
pytorch version: 1.7.0
cuda: 10.2

infference all right env:

os: win10
cuda: 10.2
pytorch-engine: pytorch-native-cuda102
djl-version: 0.9.0
pytorch-engine-version: 1.7.0

infference error env:

os: archlinux x64
cuda: none(in virtualbox)
pytorch-engine: pytorch-native-cpu
djl-version: 0.9.0
pytorch-engine-version: 1.7.0

&lt;denchmark-code&gt;PASTE OUTPUT HERE
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jinlong-hao' date='2021-01-15T17:35:41Z'>
		&lt;denchmark-link:https://github.com/jinlong-hao&gt;@jinlong-hao&lt;/denchmark-link&gt;
 This is a limitation of pytorch jit. If you trace the original model on cpu it should work on cpu.
You should be able to reproduce the same error on python.
		</comment>
		<comment id='2' author='jinlong-hao' date='2021-01-15T19:50:44Z'>
		&lt;denchmark-link:https://github.com/jinlong-hao&gt;@jinlong-hao&lt;/denchmark-link&gt;
 you have to retrace the model on cpu to make it work on cpu. btw in your forward, you probably need script as the input should change with different input size. But in trace mode, the size is fixed to the input you call trace function with
		</comment>
		<comment id='3' author='jinlong-hao' date='2021-01-16T01:21:39Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/stu1130&gt;@stu1130&lt;/denchmark-link&gt;
 got it.  is there any solution to use different input size?
		</comment>
		<comment id='4' author='jinlong-hao' date='2021-01-16T06:54:05Z'>
		&lt;denchmark-link:https://github.com/jinlong-hao&gt;@jinlong-hao&lt;/denchmark-link&gt;
 I modified the model a little bit
&lt;denchmark-code&gt;class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.i2o = nn.Linear(input_size + hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim = 1)

    def forward(self, input):
        hidden = self.initHidden().to(input.device)
        # jit script is picky
        # here you have to declare the output
        # before you can use it
        output = torch.zeros((1, 10))
        for i in range(0, len(input)):
            combined = torch.cat((input[i], hidden), 1)
            hidden = self.i2h(combined)
            output = self.i2o(combined)
            output = self.softmax(output)
        return output

    def initHidden(self):
        return torch.zeros(1, self.hidden_size)

rnn = RNN(n_letters, n_hidden, n_category).to(device)
trace_script_model = torch.jit.script(rnn)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='jinlong-hao' date='2021-01-17T03:27:19Z'>
		&lt;denchmark-link:https://github.com/stu1130&gt;@stu1130&lt;/denchmark-link&gt;
 got it. thanks a lot!!
		</comment>
	</comments>
</bug>