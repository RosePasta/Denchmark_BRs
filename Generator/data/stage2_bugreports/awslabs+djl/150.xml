<bug id='150' author='evgzakharov' open_date='2020-08-06T15:11:11Z' closed_time='2021-01-05T19:51:57Z'>
	<summary>jvm crushes in native when try to running torch model</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Hi. I have some problems with running a model from &lt;denchmark-link:https://github.com/emiliantolo/pytorch_nsfw_model&gt;https://github.com/emiliantolo/pytorch_nsfw_model&lt;/denchmark-link&gt;
 in DJL. Jvm crashes with an error in native. I try to run it with openjdk 8, zulu 8, zulu 13.
&lt;denchmark-h:h3&gt;Expected Behavior&lt;/denchmark-h&gt;

Expected that a model will run correctly
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x0000000134a917ae, pid=12860, tid=9219
#
# JRE version: OpenJDK Runtime Environment 
(Zulu13.28+11-CA) (13.0.1+10) (build 13.0.1+10-MTS)
# Java VM: OpenJDK 64-Bit Server VM (13.0.1+10-MTS, mixed mode, sharing, tiered, compressed oops, g1 gc, bsd-amd64)
# Problematic frame:
# C  [libtorch_cpu.dylib+0x2a957ae]  torch::jit::Expr::Expr(c10::intrusive_ptr&lt;torch::jit::Tree, c10::detail::intrusive_target_default_null_type&lt;torch::jit::Tree&gt; &gt; const&amp;)+0x2e
#
# No core dump will be written. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /Users/evgenyzakharov/Workspace/pytorch_nsfw_model_jvm/hs_err_pid12860.log
#
# If you would like to submit a bug report, please visit:
#   http://www.azulsystems.com/support/
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
&lt;/denchmark-code&gt;

And in dump error message is next:
&lt;denchmark-code&gt;Stack: [0x000070000ea99000,0x000070000eb99000],  sp=0x000070000eb95f50,  free space=1011k
Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libtorch_cpu.dylib+0x2a957ae]  torch::jit::Expr::Expr(c10::intrusive_ptr&lt;torch::jit::Tree, c10::detail::intrusive_target_default_null_type&lt;torch::jit::Tree&gt; &gt; const&amp;)+0x2e
C  [libtorch_cpu.dylib+0x2d21cb5]  torch::jit::ScriptTypeParser::parseClassConstant(torch::jit::Assign const&amp;)+0xd5
C  [libtorch_cpu.dylib+0x2a9097a]  torch::jit::SourceImporterImpl::importClass(c10::QualifiedName const&amp;, torch::jit::ClassDef const&amp;, bool)+0x210a
C  [libtorch_cpu.dylib+0x2a8c61a]  torch::jit::SourceImporterImpl::importNamedType(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, torch::jit::ClassDef const&amp;)+0x64a
C  [libtorch_cpu.dylib+0x2a88d2b]  torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&amp;)+0xfb
&lt;/denchmark-code&gt;

Full error log:
&lt;denchmark-link:https://github.com/awslabs/djl/files/5035749/hs_err_pid12860.log&gt;hs_err_pid12860.log&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;How to Reproduce?&lt;/denchmark-h&gt;

Download model from repository &lt;denchmark-link:https://github.com/emiliantolo/pytorch_nsfw_model&gt;https://github.com/emiliantolo/pytorch_nsfw_model&lt;/denchmark-link&gt;
  and try to run with DJL
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;


Download model
Convert model to ".pt" with script or trace:

model = models.resnet50()
model.fc = nn.Sequential(nn.Linear(2048, 512),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(512, 10),
                                 nn.LogSoftmax(dim=1))
model.load_state_dict(torch.load('ResNet50_nsfw_model.pth', map_location=torch.device('cpu')))
model.eval()

#script
export = torch.jit.script(model)
torch.jit.save(export, "out.pt")

#trace
image = Image.open(data_dir+"1.jpg")
image_tensor = test_transforms(image).float()
image_tensor = image_tensor.unsqueeze_(0)
input = Variable(image_tensor)
net_trace = torch.jit.trace(model, input)
net_trace.save("out.pt")

Try to use model in DJL with next criteria:

val criteria = Criteria.builder()
        .setTypes(
            Image::class.java,
            Classifications::class.java
        )
        .optModelZoo(DefaultModelZoo("&lt;path to folder with model&gt;"))
        .optModelName("out.pt")
        .optTranslator(translator)
        .optProgress(ProgressBar())
        .build()
Version in build.gradle.kts:
implementation("ai.djl:api:0.6.0")
runtimeOnly("ai.djl.pytorch:pytorch-engine:0.6.0")
runtimeOnly("ai.djl.pytorch:pytorch-native-auto:1.5.0")
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;


Try to use different version of jvm (openjdk 8, zulu 8 zulu 13)
Try to use different version of torch (1.6.0, 1.4.0)

&lt;denchmark-h:h2&gt;Environment Info&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;OS:uname:Darwin 18.7.0 Darwin Kernel Version 18.7.0: Thu Jun 18 20:50:10 PDT 2020; root:xnu-4903.278.43~1/RELEASE_X86_64 x86_64
rlimit: STACK 8192k, CORE 0k, NPROC 1418, NOFILE 10240, AS infinity, DATA infinity, FSIZE infinity
load average:2.75 2.65 2.72

CPU:total 8 (initial active 8) (4 cores per cpu, 2 threads per core) family 6 model 158 stepping 9, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, 3dnowpref, lzcnt, ht, tsc, tscinvbit, bmi1, bmi2, adx, fma

Memory: 4k page, physical 16777216k(62596k free), swap 6291456k(874240k free)

vm_info: OpenJDK 64-Bit Server VM (13.0.1+10-MTS) for bsd-amd64 JRE (13.0.1+10-MTS) (Zulu13.28+11-CA), built on Oct  9 2019 12:07:25 by "zulu_re" with clang 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='evgzakharov' date='2020-08-06T15:15:09Z'>
		&lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;
 Thanks for reporting this issue. Will try to reproduce it.
		</comment>
		<comment id='2' author='evgzakharov' date='2020-08-07T10:14:14Z'>
		: today try to run this model in docker with linux images (ubuntu). Application crash too, but with another error. Also it crash as with  method either with .
image: adoptopenjdk/openjdk8:jdk8u262-b10:
&lt;denchmark-link:https://github.com/awslabs/djl/files/5040764/hs_err_pid122_1_8.log&gt;hs_err_pid122_1_8.log&lt;/denchmark-link&gt;

image: adoptopenjdk/openjdk13:jdk-13.0.2_8-ubuntu-slim:
&lt;denchmark-link:https://github.com/awslabs/djl/files/5040765/hs_err_pid138_13.log&gt;hs_err_pid138_13.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='evgzakharov' date='2020-08-07T16:55:52Z'>
		Rebuild the project pytorch-native today to pytorch 1.6.0 and now model at least successfully imports and start (on macos). But result is not same as I got at python version.
For example for one image result is next:

jvm
-2.36343 -0.16445 -8.09794 -2.90209 -6.06068
python
-2.90546  -0.10754  -9.972915 -3.06928 -7.21443
May be this is related somehow to model export. But this numbers is same as for script version and for trace version.

		</comment>
		<comment id='4' author='evgzakharov' date='2020-08-07T18:03:39Z'>
		Hi &lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;
,
I tried to reproduce your issue but failed on my mac. Here is the settings I used:
&lt;denchmark-code&gt;$ pip3 list
Package     Version
----------- -------
future      0.18.2
numpy       1.19.1
Pillow      7.2.0
pip         20.2.1
setuptools  49.2.1
torch       1.5.0
torchvision 0.6.0

&lt;/denchmark-code&gt;

Script to convert to torchscript
import torch
from torch import nn
from torchvision import models

model = models.resnet50()
model.fc = nn.Sequential(nn.Linear(2048, 512),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(512, 10),
                                 nn.LogSoftmax(dim=1))
model.load_state_dict(torch.load('ResNet50_nsfw_model.pth', map_location=torch.device('cpu')))
model.eval()

#trace
image = torch.ones((1, 3, 224, 224))
net_trace = torch.jit.trace(model, image)
net_trace.save("out.pt")
After that, you can prepare a synset.txt along with your pt file:
&lt;denchmark-link:https://github.com/awslabs/djl/files/5042964/synset.txt&gt;synset.txt&lt;/denchmark-link&gt;

I just used 1.jpg from the repo to run for the inference
&lt;denchmark-code&gt;dependencies {
    implementation "ai.djl:api:0.6.0"
    runtimeOnly "ai.djl.pytorch:pytorch-model-zoo:0.6.0"
    runtimeOnly "ai.djl.pytorch:pytorch-native-auto:1.5.0"
}
&lt;/denchmark-code&gt;

    Criteria&lt;Image, Classifications&gt; criteria = Criteria.builder()
            .setTypes(Image.class, Classifications.class)
            .optModelUrls(Paths.get("").toUri().toString())
            .optModelName("out")
            .optTranslator(ImageClassificationTranslator.builder()
                    .setPipeline(new Pipeline()
                            .add(new Resize(224))
                            .add(new CenterCrop(224, 224))
                            .add(new ToTensor())
                            .add(new Normalize(
                                    new float[]{0.485f, 0.456f, 0.406f},
                                    new float[]{0.229f, 0.224f, 0.225f})))
                    .optApplySoftmax(true).build())
            .build();
    ZooModel&lt;Image, Classifications&gt; model = ModelZoo.loadModel(criteria);
    Predictor&lt;Image, Classifications&gt; predictor = model.newPredictor();
    ImageFactory factory = ImageFactory.getInstance();
    Image image = factory.fromFile(Paths.get("1.jpg"));
    System.out.println(predictor.predict(image));
		</comment>
		<comment id='5' author='evgzakharov' date='2020-08-07T18:17:05Z'>
		
Rebuild the project pytorch-native today to pytorch 1.6.0 and now model at least successfully imports and start (on macos). But result is not same as I got at python version.
For example for one image result is next:

jvm
-2.36343 -0.16445 -8.09794 -2.90209 -6.06068
python
-2.90546  -0.10754  -9.972915 -3.06928 -7.21443
May be this is related somehow to model export. But this numbers is same as for script version and for trace version.


It could caused by the transformation applied. You can try to follow the above line to create a pipeline to see if there is any improvement on the result.
		</comment>
		<comment id='6' author='evgzakharov' date='2020-08-07T18:18:15Z'>
		Also, there could be a difference between the inference result between torchscripted model. You can try to compare the inference result by using Torchscripted model (pt) comparing to the DJL's one to see if there is any difference.
		</comment>
		<comment id='7' author='evgzakharov' date='2020-08-07T18:45:28Z'>
		
I tried to reproduce your issue but failed on my mac. Here is the settings I used:

Sorry, my fault. Tried again 1.5.0 version of pytorch and model successfully imports. I tried different version of pytorch at first time, but with all have same error. At first it was 1.6.0, later I install 1.4.0, and 1.5.0. Possible there was a mistake with reinstalling pytorch version, or mb pip raised version of library upon attempt.

You can try to follow the above line to create a pipeline to see if there is any improvement on the result.

I have same transformation, but result is not same.

Also, there could be a difference between the inference result between torchscripted model. You can try to compare the inference result by using Torchscripted model (pt) comparing to the DJL's one to see if there is any difference.

Thanks, will try to compare results.
		</comment>
		<comment id='8' author='evgzakharov' date='2020-08-07T19:03:44Z'>
		Tried to load Torchscripted model in python
&lt;denchmark-code&gt;test_model = torch.jit.load("out.pt")
raw_result = test_model(prepare_input("4.jpg"))
print(str(raw_result.data.numpy()))
&lt;/denchmark-code&gt;

And result is same as for original model:
&lt;denchmark-code&gt; -2.9054637  -0.10754118  -9.972915  -3.0692832   -7.214436 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='evgzakharov' date='2020-08-07T20:10:32Z'>
		I have compare image pixels and figure out that some of them slightly not same in jvm (ImageIO) and python (PIL). And found in &lt;denchmark-link:https://stackoverflow.com/questions/39649292/imageio-reading-slightly-different-rgb-values-than-other-methods&gt;stackoverflow&lt;/denchmark-link&gt;
 discussion about it.
All looks like that only in OSX results is not same. I will try later to check results in docker image.
		</comment>
		<comment id='10' author='evgzakharov' date='2020-08-07T21:24:27Z'>
		&lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;

How you use PIL in this case?
I'm not aware of difference for read image between ImageIO and PIL. The difference is introduced with you process the image (resize, corp, etc) with different library.
In DJL, we actually using PyTorch operator to process the image. If you also use pytorch operator the result should be the same.
		</comment>
		<comment id='11' author='evgzakharov' date='2020-08-07T23:55:39Z'>
		&lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;
 you can probably use sum operator to check if images are the same right before the forward call
		</comment>
		<comment id='12' author='evgzakharov' date='2020-08-08T06:26:30Z'>
		
How you use PIL in this case?

In python image open by
&lt;denchmark-code&gt;from PIL import Image
Image.open(data_dir+"4.jpg")
&lt;/denchmark-code&gt;


you can probably use sum operator to check if images are the same right before the forward call

If I do it right, the numbers is next:
jvm:       115013565
python: 115011805

I will try later to check results in docker image

Today have try it in docker. And result is same (jvm is not same as in python) as start in macos.
I will look further and try different options and write here if I found anything.
		</comment>
		<comment id='13' author='evgzakharov' date='2020-08-10T12:28:11Z'>
		Some moments which I have found:

Resize(224) in DJL resize image to (224,224) image, but in python transforms.Resize(224) resizes image proportionally, so the end dimensions is (298, 224).
after explicitly set in python to resize to (224,224) pixels transformation is still not same. And in java transformation is made for float32 values. But if set transformation to transforms.Resize((224, 224), interpolation=Image.BICUBIC) pixels start to be more "nearest".
For example for (70, 5) :
python with Image.BICUBIC: (23, 23, 28)
python with Image.BILINEAR (default): (25, 26, 31)
jvm: (23.442574, 23.725086, 29.160065)
If compare value after all image processing, than values be next for (70,5):
jvm: 2.1953566, 2.3738246, 2.5854964
python: 2.1975, 2.3761, 2.6051
And this value for python is same for BICUBIC and for BILINEAR for this point, but for some others start to appear small difference
result for python with all variants is not same:
jvm: [-2.363431453704834, -0.16445206105709076, -8.097947120666504, -2.902099132537842, -6.060682773590088]
python BICUBIC : [[ -1.7852671   -0.23901895  -9.423489    -3.131952    -6.7816606 ]]
python BILINEAR: [[ -1.6852896   -0.28112033  -8.666439    -2.858017    -6.1577053 ]]
if remove almost all image conversion and leave only resize and ToTensor then result be still not same (but more closely to each other compare to other tries):
jvm: -1.518589735031128, -0.3182249069213867, -5.83787727355957, -3.0340001583099365, -5.990426063537598
python:  -1.2186315   -0.47063854  -5.9082274   -2.7171867   -4.5112333
if take only tensor transformation, results will be next:
jvm: [-0.7599513530731201, -0.8078536987304688, -4.547921657562256, -2.6093368530273438, -6.064263343811035]
python: [ -0.73268026  -0.8299884   -4.553094    -2.6514175   -6.100883 ]

So my opinion is that Resize is step where most discrepancies start to appear.
		</comment>
		<comment id='14' author='evgzakharov' date='2020-08-11T01:10:03Z'>
		&lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;
 I am creating a PR to improve DJL PyTorch resize operator, which will use interpolate under the hood(we used to use bilinear upsampling and it is recommended to use interpolate for down sampling) and expose the mode in which you can pass in something like bilinear, bicubic, etc as you did with PyTorch Python. I hope that will help reduce the discrepancy of python &amp; DJL.
		</comment>
		<comment id='15' author='evgzakharov' date='2020-08-12T04:23:10Z'>
		&lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;
 The PR &lt;denchmark-link:https://github.com/awslabs/djl/commit/b748fc3713400d8feccb71aa9c1b28708e7f22c3&gt;b748fc3&lt;/denchmark-link&gt;
 has been merged. You can try it out the  0.7.0-SNAPSHOT.
		</comment>
		<comment id='16' author='evgzakharov' date='2020-08-12T08:38:31Z'>
		&lt;denchmark-link:https://github.com/stu1130&gt;@stu1130&lt;/denchmark-link&gt;
 Thanks, i tried with this version and result is next:
With only resize and tensor:
jvm BILINEAR: -1.5374747514724731, -0.32685360312461853, -5.549479007720947, -2.892700672149658, -5.387859344482422
jvm BICUBIC: -1.2648584842681885, -0.44646814465522766, -4.6644062995910645, -2.778036117553711, -5.07496452331543
python BILINEAR: -0.971618   -0.6533397  -5.0309176  -2.5527706  -4.0848236
python BICUBIC: -1.2186315   -0.47063854  -5.9082274   -2.7171867   -4.5112333
With all image transformations:
jvm BILINEAR: -2.368337869644165, -0.16734620928764343, -8.279645919799805, -2.871751546859741, -5.625779151916504
jvm BICUBIC: -2.223297595977783, -0.1944298893213272, -7.384432792663574, -2.768308639526367, -5.2872490882873535
jvm NEAREST: -2.7276082038879395, -0.19401878118515015, -6.727710247039795, -2.357511043548584, -4.191122055053711
jvm AREA: -1.8174500465393066, -0.25450366735458374, -8.783180236816406, -2.807745933532715, -6.339481830596924
python BILINEAR: -1.6852896   -0.28112033  -8.666439    -2.858017    -6.1577053
python BICUBIC: -1.7852671   -0.23901895  -9.423489    -3.131952    -6.7816606
python NEAREST: -2.0546317   -0.30949548  -5.93951     -2.2120388   -3.6522655
python BOX: 2.2717316  -0.1533065  -9.936253   -3.2708497  -6.930706
python HAMMING: -1.9043323  -0.2166003  -9.353272   -3.1095734  -6.7948
python LANCZOS: -1.7566973   -0.23691499  -9.667669    -3.2829366   -7.20023
Most closely numbers was at resize and tensor with BICUBIC, but if enable all transformations there start to appear big difference between them.
		</comment>
		<comment id='17' author='evgzakharov' date='2020-08-12T21:14:23Z'>
		&lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;
 here is the experiment I did
&lt;denchmark-code&gt;import torch
from torchvision import transforms

a = torch.rand(3, 200, 200)
a_255 = a * 255
print(a.sum())
a_img = transforms.ToPILImage()(a_255).convert('RGB')
a_to_tensor = transforms.ToTensor()(a_img)
print(a_to_tensor.sum())

a_img_resized = transforms.Resize((100, 100))(a_img)
a_to_tensor = transforms.ToTensor()(a_img_resized)
print(a_to_tensor.sum())
a_255_resized = torch.nn.functional.interpolate(a_255.unsqueeze(0), (100, 100), mode='bilinear', align_corners=False)
print((a_255_resized / 255).sum())
&lt;/denchmark-code&gt;

The sample output looks like this:
&lt;denchmark-code&gt;tensor(59970.6680)
tensor(59941.2422)
tensor(14999.9189)
tensor(14992.6719)
&lt;/denchmark-code&gt;

So I can see even with the default mode (bilinear), the difference is minimal.
Can I get the script so I can further investigate the difference?
		</comment>
		<comment id='18' author='evgzakharov' date='2020-08-13T07:25:46Z'>
		&lt;denchmark-link:https://github.com/stu1130&gt;@stu1130&lt;/denchmark-link&gt;
 All my above examples was be calculated at model in this repository:   &lt;denchmark-link:https://github.com/emiliantolo/pytorch_nsfw_model&gt;https://github.com/emiliantolo/pytorch_nsfw_model&lt;/denchmark-link&gt;

import torch
from torch import nn
from torchvision import transforms, models
from PIL import Image
from torch.autograd import Variable

data_dir = 'images/'

test_transforms = transforms.Compose([
                                      transforms.Resize((224, 224)),
                                      transforms.CenterCrop(224),
                                      transforms.ToTensor(),
                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                           std=[0.229, 0.224, 0.225])
                                      ])

model = models.resnet50()
model.fc = nn.Sequential(nn.Linear(2048, 512),
                         nn.ReLU(),
                         nn.Dropout(0.2),
                         nn.Linear(512, 10),
                         nn.LogSoftmax(dim=1))
model.load_state_dict(torch.load('ResNet50_nsfw_model.pth', map_location=torch.device('cpu')))
model.eval()

def prepare_input(input):
    image = Image.open(data_dir+input)
    image_tensor = test_transforms(image).float()
    image_tensor = image_tensor.unsqueeze_(0)
    return Variable(image_tensor)

if __name__ == '__main__':
    image_after_prepare = prepare_input("4.jpg")

    raw_result = model(image_after_prepare)
    print(str(raw_result.data.numpy()))
		</comment>
		<comment id='19' author='evgzakharov' date='2021-01-05T19:51:57Z'>
		Close the issue as the original issue is solved. &lt;denchmark-link:https://github.com/evgzakharov&gt;@evgzakharov&lt;/denchmark-link&gt;
 feel free to create another issue
		</comment>
	</comments>
</bug>