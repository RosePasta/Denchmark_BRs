<bug id='284' author='thhart' open_date='2020-11-06T21:07:07Z' closed_time='2020-11-07T21:50:14Z'>
	<summary>Onnx CPU inference performance problem</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I have a performance problem using the Onnx engine. Using Onnx runtime directly (without DJL) I have a warm-up of 3 s when doing inference but after short time the prediction goes fast down to 400 ms. All this is done in one Onnx session.
Using the DJL wrapper it looks like the warm-up is done always so prediction keeps at 3 s. All this is done in one DJL predictor.
This is on top  of 0.9-SNAPSHOT. I know all this is not much information and I try to provide a test case ASAP however  I wanted to ask you if you have any idea already in front?
	</description>
	<comments>
		<comment id='1' author='thhart' date='2020-11-06T21:20:16Z'>
		&lt;denchmark-link:https://github.com/thhart&gt;@thhart&lt;/denchmark-link&gt;
  You can use our benchmark tool to test the inference performance:
&lt;denchmark-code&gt;./gradlew benchmark --args="-n YOUR_MODEL_NAME -c 1000 -s 1,224,224,3" -Dai.djl.repository.zoo.location=file://$PWD/models/YOUR_MODEL_NAME -Dai.djl.default_engine=OnnxRuntime
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='thhart' date='2020-11-06T21:56:19Z'>
		&lt;denchmark-link:https://github.com/thhart&gt;@thhart&lt;/denchmark-link&gt;
 Can you share the code you tried to benchmark ONNX runtime on? We are creating new Environment everytime new input comes.
		</comment>
		<comment id='3' author='thhart' date='2020-11-06T22:07:58Z'>
		
We are creating new Environment everytime new input comes.

Yes, this I was afraid of and might explain my problems, will try to provide a test/case model/example asap, thanks for the quick reply.
		</comment>
		<comment id='4' author='thhart' date='2020-11-06T22:18:53Z'>
		&lt;denchmark-link:https://github.com/thhart&gt;@thhart&lt;/denchmark-link&gt;
 OrtEnvironment.getEnvironment() should not create a new instance, it re-use the static OrtEnvironment.INSTANCE if possible. It must be something else.
		</comment>
		<comment id='5' author='thhart' date='2020-11-06T22:24:39Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 Yes, your are right the environment is not recreated indeed, check in a debug, will dig further...
		</comment>
		<comment id='6' author='thhart' date='2020-11-06T22:34:34Z'>
		Tried to run this with a local model file but somehow benchmark is distorting the path and does not load the model.
 ./gradlew benchmark --args="-n yolov5s.onnx -c 1000 -s 1,224,224,3" -Dai.djl.repository.zoo.location=file://$PWD/yolov5s.onnx -Dai.djl.default_engine=OnnxRuntime 


&gt; Task :examples:benchmark FAILED
[INFO ] - Load library 1.4.0 in 106.253 ms.
[INFO ] - Running Benchmark on: cpu().
[ERROR] - Unexpected error
ai.djl.repository.zoo.ModelNotFoundException: No matching model with specified Input/Output type found.
        at ai.djl.repository.zoo.ModelZoo.loadModel(ModelZoo.java:173) ~[api-0.9.0-SNAPSHOT.jar:?]
        at ai.djl.examples.inference.benchmark.util.AbstractBenchmark.loadModel(AbstractBenchmark.java:301) ~[main/:?]
        at ai.djl.examples.inference.benchmark.Benchmark.predict(Benchmark.java:44) ~[main/:?]
        at ai.djl.examples.inference.benchmark.util.AbstractBenchmark.runBenchmark(AbstractBenchmark.java:131) [main/:?]
        at ai.djl.examples.inference.benchmark.Benchmark.main(Benchmark.java:33) [main/:?]


		</comment>
		<comment id='7' author='thhart' date='2020-11-07T02:10:14Z'>
		&lt;denchmark-link:https://github.com/thhart&gt;@thhart&lt;/denchmark-link&gt;

You need create a folder  and put yolov5s.onnx in , the call:
&lt;denchmark-code&gt;./gradlew benchmark --args="-n yolov5s -c 1000 -s 1,224,224,3" -Dai.djl.repository.zoo.location=file://$PWD/yolov5s -Dai.djl.default_engine=OnnxRuntime 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='thhart' date='2020-11-07T12:44:17Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 Hi Frank, checking all this I found out your Onnx engine is not implemented yet (see OrtNDArray for UnsupportedOperationException). All I do right now is to use Mxnet engine injection somehow as I have these in the dependency list as well. Looks like DJL is either by fault or not misusing the Mxnet layer then for tensor data handling before passing to Onnx models loaded. With this I successful get the Onnx model running. I am not into you architecture if this is wished or not however I can confirm it is working as the network is giving out correct results. Of course this might have other side effects as well.
I am still working on a test case...
		</comment>
		<comment id='9' author='thhart' date='2020-11-07T21:50:13Z'>
		The problem was in output parsing within YoloV5Translator. Created PR for this.
&lt;denchmark-link:https://github.com/awslabs/djl/pull/286&gt;#286&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>