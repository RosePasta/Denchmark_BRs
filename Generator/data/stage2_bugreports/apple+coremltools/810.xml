<bug id='810' author='leovinus2001' open_date='2020-07-23T17:30:38Z' closed_time='2020-08-12T17:15:38Z'>
	<summary>PyTorch to CoreML LSTM model conversion v4 fails with ValueError: Layer with name "_lstm_h0_reshaped_expanded" has already been added. Please use a unique name.</summary>
	<description>
Relevance:-----------------------------------------------------------
While there other open issues in the coremltools v4 TOT repo regarding PyTorch trained LSTMs, such as &lt;denchmark-link:https://github.com/apple/coremltools/issues/755&gt;#755&lt;/denchmark-link&gt;
 (multi layer LSTM) and &lt;denchmark-link:https://github.com/apple/coremltools/issues/776&gt;#776&lt;/denchmark-link&gt;
 (handling of LSTM initials h0/c0), I was looking for workarounds on how to build multi-layer bidirectional LSTMs. Now I ran into a blocking bug with error:

Layer with name "_lstm_h0_reshaped_expanded" has already been added. Please use a unique name.

In other words, another blocker on the conversion of PyTorch multilayer LSTMs (potential GRU/RNN as well).
On the good side, a little digging in the conversion code leads to potential fix as well (see code and potential solution below) but I need your please to help to assess whether this approach is best, whether the unidirectional LSTM and GRU also need fixes, etc.
Reproducible:-----------------------------------------------------------
Yes
Testcase:-----------------------------------------------------------
Attached. Run e.g. as
python3 -O ../testLstmTwoLayer.py
&lt;denchmark-link:https://github.com/apple/coremltools/files/4967763/testLstmTwoLayer.txt&gt;testLstmTwoLayer.txt&lt;/denchmark-link&gt;

Setup:-----------------------------------------------------------
Torch version : 1.5.1
CoreML tools version : TOT  &lt;denchmark-link:https://github.com/apple/coremltools/commit/705244e2be26c3fb7881fd7a731d25a55f5e4765&gt;705244e&lt;/denchmark-link&gt;
 July 23
Log:-----------------------------------------------------------
Torch version : 1.5.1
CoreML tools version : 4.0b1
small_model(
(lstm1): LSTM(28, 16, batch_first=True, bidirectional=True)
(lstm2): LSTM(32, 8, batch_first=True, bidirectional=True)
(fc1): Linear(in_features=16, out_features=11, bias=True)
)
Converting Frontend ==&gt; MIL Ops:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 46/47 [00:00&lt;00:00, 218.23 ops/s]
Running MIL optimization passes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00&lt;00:00, 17.78 passes/s]
Translating MIL ==&gt; MLModel Ops:  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                 | 22/32 [00:00&lt;00:00, 14936.01 ops/s]
Traceback (most recent call last):
File "../testLstmTwoLayer.py", line 74, in 
inputs=[ ct.TensorType(name="input1", shape=dummy_input.shape) ],
File "/Library/Python/3.7/lib/python/site-packages/coremltools/converters/_converters_entry.py", line 299, in convert
**kwargs
File "/Library/Python/3.7/lib/python/site-packages/coremltools/converters/mil/converter.py", line 122, in _convert
out = backend_converter(prog, **kwargs)
File "/Library/Python/3.7/lib/python/site-packages/coremltools/converters/mil/converter.py", line 72, in call
return load(*args, **kwargs)
File "/Library/Python/3.7/lib/python/site-packages/coremltools/converters/mil/backend/nn/load.py", line 239, in load
prog.functions["main"].outputs,
File "/Library/Python/3.7/lib/python/site-packages/coremltools/converters/mil/backend/nn/op_mapping.py", line 50, in convert_ops
mapper(const_context, builder, op)
File "/Library/Python/3.7/lib/python/site-packages/coremltools/converters/mil/backend/nn/op_mapping.py", line 1399, in lstm
_expand_dim(builder, initial_h + postfix, initial_h, [2, 3, 4])
File "/Library/Python/3.7/lib/python/site-packages/coremltools/converters/mil/backend/nn/op_mapping.py", line 208, in _expand_dim
name=node_name, input_name=input_name, output_name=node_name, axes=axes
File "/Library/Python/3.7/lib/python/site-packages/coremltools/models/neural_network/builder.py", line 7056, in add_expand_dims
spec_layer = self._add_generic_layer(name, [input_name], [output_name])
File "~/Library/Python/3.7/lib/python/site-packages/coremltools/models/neural_network/builder.py", line 1029, in _add_generic_layer
% name
ValueError: Layer with name "_lstm_h0_reshaped_expanded" has already been added. Please use a unique name.
Interpretation and potential fix: -----------------------------------------------------------
This is observed with git coremltools, TOT &lt;denchmark-link:https://github.com/apple/coremltools/commit/705244e2be26c3fb7881fd7a731d25a55f5e4765&gt;705244e&lt;/denchmark-link&gt;
 July 23
The error

Layer with name "_lstm_h0_reshaped_expanded" has already been added. Please use a unique name.

seems to indicate that the naming scheme in the converter is using a global name scheme.
While the testcase runs fine with ONE layer (set flag twoLayerModelFlag to False) when you run with TWO layers (twoLayerModelFlag=True) ) (or more presumably) the error occurs.
Originally, you have on TOT the use of fixed postfixes like "_expanded" (for h0) and "_expanded2" (for c0) which leads to the issue it seems. When you have 2 layers or more, it seems the same variable name is generated multiple times.
The line 1399 in coremltools/converters/mil/backend/nn/op_mapping.py
points to the code below (The line numbers might be a little off due to my logging lines).
Original code:
def lstm(const_context, builder, op):
...
&lt;denchmark-h:h1&gt;Roughly Line 1375 et al&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;elif direction == "bidirectional":
    # Expand initial_h and initial_c
    _expand_dim(builder, initial_h + "_expanded", initial_h, [2, 3, 4]) # &lt;------------------
     initial_h += "_expanded"
    
    # initial_h may have the same name as initial_c (e.g., same Var)
    _expand_dim(builder, initial_c + "_expanded2", initial_c, [2, 3, 4])  # &lt;------------------
    initial_c += "_expanded2"
&lt;/denchmark-code&gt;

Therefore, I tried a quick randomization fix along the lines below to generate UNIQUE names every time you handle a new BLSTM layer.
Potential fix:
def lstm(const_context, builder, op):
...
&lt;denchmark-h:h1&gt;Roughly Line 1375 et al&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;elif direction == "bidirectional":
    # Expand initial_h and initial_c
    if 1:
        # New
        import random
        mymax = 10000000 # Better, use sys.maxint
        n = random.randrange(1,mymax)
        postfix = "_expanded_" + str(n)
        
        n2 = random.randrange(1,mymax)
        postfix2 = "_expanded2_" + str(n2)

        mynodename = initial_h + "_expanded"
        print ("nodename " + str(mynodename))
        print ("postfix" + str(postfix))
        print ("postfix2" + str(postfix2))
    else:
		#ORG
        postfix  = "_expanded"
        postfix2 = "_expanded2"
        
    _expand_dim(builder, initial_h + postfix, initial_h, [2, 3, 4])  # &lt;------------------
    initial_h += postfix
    
    # initial_h may have the same name as initial_c (e.g., same Var)
    _expand_dim(builder, initial_c + postfix2, initial_c, [2, 3, 4])  # &lt;------------------
    initial_c += postfix2
&lt;/denchmark-code&gt;

While the conversion now completes, I have not checked whether "everything" is fine. which is why I ask for your help for a proper fix please. Note we might need a fix for other uniLSTM, GRU or RNN in the same file as well.
As the other issues  &lt;denchmark-link:https://github.com/apple/coremltools/issues/755&gt;#755&lt;/denchmark-link&gt;
 (multi layer LSTM) and &lt;denchmark-link:https://github.com/apple/coremltools/issues/776&gt;#776&lt;/denchmark-link&gt;
 (handling of initials h0/c0) are still blocking us, it would be an immense help if we could fix this new issue quickly such that I can properly stack multiple BLSTMs. Thanks.
Here is the output WITH the fix above:
python3 -O ../testLstmTwoLayer.py
WARNING:root:Keras version 2.4.3 detected. Last version known to be fully compatible of Keras is 2.2.4 .
Torch version : 1.5.1
CoreML tools version : 4.0b1
small_model(
(lstm1): LSTM(28, 16, batch_first=True, bidirectional=True)
(lstm2): LSTM(32, 8, batch_first=True, bidirectional=True)
(fc1): Linear(in_features=16, out_features=11, bias=True)
)
Converting Frontend ==&gt; MIL Ops:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 46/47 [00:00&lt;00:00, 213.31 ops/s]
Running MIL optimization passes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00&lt;00:00, 17.83 passes/s]
Translating MIL ==&gt; MLModel Ops:   0%|                                                                                                                                                                                                                                                                               | 0/32 [00:00&lt;?, ? ops/s]
nodename _lstm_h0_reshaped_expanded
postfix_expanded_6035530			# &lt;------------------ randomized
postfix2_expanded2_4582868			# &lt;------------------ randomized
nodename _lstm_h0_reshaped_expanded
postfix_expanded_3955283
postfix2_expanded2_1781011
Translating MIL ==&gt; MLModel Ops: 100%
	</description>
	<comments>
		<comment id='1' author='leovinus2001' date='2020-08-05T16:15:17Z'>
		Updated testcase for PR &lt;denchmark-link:https://github.com/apple/coremltools/pull/843&gt;#843&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/apple/coremltools/files/5029648/testLstmTwoLayer.txt&gt;testLstmTwoLayer.txt&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>