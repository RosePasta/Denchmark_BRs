<bug id='948' author='teijeong' open_date='2020-09-28T11:49:41Z' closed_time='2020-10-03T17:11:57Z'>
	<summary>Neural Engine returns weird values on iOS 14</summary>
	<description>
I was trying PoseFinder example, with a custom model manually converted from TensorFlow Lite's Pose Estimation Example.
(while investigating &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/43339&gt;tensorflow/tensorflow#43339&lt;/denchmark-link&gt;
)
The model runs fine on iOS 13, but starts to fail and keep putting similar weird values on iOS 14 when using Neural Engine.
Original model file: &lt;denchmark-link:https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite&gt;https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite&lt;/denchmark-link&gt;

Converted file: &lt;denchmark-link:https://drive.google.com/file/d/1m4TyADjohpTM42JQUa_Na-Irz7jLGUIm/view?usp=sharing&gt;https://drive.google.com/file/d/1m4TyADjohpTM42JQUa_Na-Irz7jLGUIm/view?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-link:https://imgur.com/a/q0ilWyX&gt;https://imgur.com/a/q0ilWyX&lt;/denchmark-link&gt;

Steps to reproduce:

Download project from https://developer.apple.com/documentation/coreml/detecting_human_body_poses_in_an_image
replace .mlmodel file with the one attached
from Model/PoseNet.swift file, change outputStride to 32 (line 35)
instead of putting configuration: .init() on line 43, initialize configuration separately and set configuration.computeUnits = .all before passing it to the initializer.

	</description>
	<comments>
		<comment id='1' author='teijeong' date='2020-09-28T22:50:19Z'>
		&lt;denchmark-link:https://github.com/teijeong&gt;@teijeong&lt;/denchmark-link&gt;
 we are looking into this. In the meanwhile, could you please file a report on feedbackassistant.apple.com?
		</comment>
		<comment id='2' author='teijeong' date='2020-09-28T23:09:49Z'>
		Done, thanks!
		</comment>
		<comment id='3' author='teijeong' date='2020-10-03T17:11:56Z'>
		For the model attached, posenet.mlmodel, I have verified that the modelâ€™s predictions remain same irrespective of the iOS version or the value of configuration.computeunits. That is,

I compared the outputs by setting configuration.computeUnits = .cpuOnly or configuration.computeUnits = .all, on iOS 14, the predictions did not change.
compared the outputs of iOS 13.6, and iOS 14.1 beta, and they match for all values of configuration.computeUnits .

I did notice a difference between  and the posenet model attached in &lt;denchmark-link:https://developer.apple.com/documentation/coreml/detecting_human_body_poses_in_an_image&gt;https://developer.apple.com/documentation/coreml/detecting_human_body_poses_in_an_image&lt;/denchmark-link&gt;
.
(When visualized with Netron (&lt;denchmark-link:https://github.com/lutzroeder/netron&gt;https://github.com/lutzroeder/netron&lt;/denchmark-link&gt;
))
  gives worse predictions compared to the model in the sample app.
For the latter, the model uses a relu6 layer which it constructs by the following sequence of layers:
(1) relu
(2) linear activation with scale=-1
(3) threshold unary with alpha = -6, and scale= 1
(4) linear activation with scale=-1
These combine to give the equivalent formula of : y =  -max(-6, max(0,x)), which is equivalent to relu6
Since posenet.mlmodel is manually converted, it seems incorrect. It has this sequence of layers instead:
(1) relu
(2) hreshold unary with alpha = -6, and scale= -1
(3) linear activation with scale=-1
These combine to give the equivalent formula of : y =  max(-6, max(0,x)), which is basically a relu, not a relu6.
If posenet.mlmodel intends to use relu6, then it could do that by either using the clip layer (available &gt;=iOS13), with alpha=0,  beta=6, or by using the same sequence of 4 layers as done by the model in the sample app.
		</comment>
		<comment id='4' author='teijeong' date='2020-10-12T02:44:35Z'>
		Hmm, do you mean y = -max(-6, -max(0,x)) for the first formula? From what I understood the second layer, threshold unary with alpha = -6, and scale= -1 should do the same as layer 2-3 fused in the original model, as scale in the unary layer is applied to the input before unary operation.
&lt;denchmark-link:https://github.com/aseemw&gt;@aseemw&lt;/denchmark-link&gt;
, can you share your testing method? my iPhone 11 is using iOS 13.6 and returns correct results when using "Single pose" estimation mode.
		</comment>
		<comment id='5' author='teijeong' date='2020-10-12T16:07:58Z'>
		Hi &lt;denchmark-link:https://github.com/teijeong&gt;@teijeong&lt;/denchmark-link&gt;
 , yeah you are right. My bad, I made a mistake there. The scale in the unary is indeed applied to the input first, before the unary activation. So the second sequence should be equivalent to the first and both should give the effective activation :  as you said.
I had the tested the model, attached to the radar/feedback report, using the pose estimation app. It wasn't incorrect on iOS13.6 on all the input images I tested with, but in general was less accurate than the model that comes with the sample app. Moreover, I did not see any difference between iOS13 and iOS14.
If you see the difference on a specific set of images, please attach them to the feedback report.
		</comment>
	</comments>
</bug>