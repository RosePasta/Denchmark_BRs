<bug id='951' author='ElteHupkes' open_date='2020-09-30T16:53:47Z' closed_time='2020-10-06T15:42:36Z'>
	<summary>PyTorch Converter: interpolate with scale factor results in incorrect / negative dimension</summary>
	<description>
&lt;denchmark-h:h2&gt;üêûDescribe the bug&lt;/denchmark-h&gt;

When trying to convert a model to CoreML, I encountered a
&lt;denchmark-code&gt;ValueError: Incompatible dim 2 in shapes (1, 16, -112, 112) vs. (1, 16, 144, 112)
&lt;/denchmark-code&gt;

I've managed to reduce the problem to a toy model (see the To Reproduce section) that sums two tensors, one of which has just been upscaled by a factor 2 using torch.nn.functional.interpolate.
&lt;denchmark-h:h2&gt;Trace&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-168-b098a9fb8bf0&gt; in &lt;module&gt;
     27     lx_trace,
     28     inputs=[ct.TensorType(name="input_1", shape=tst1.shape, dtype=np.float32),
---&gt; 29            ct.TensorType(name="input_2", shape=tst2.shape, dtype=np.float32)]
     30 )

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/_converters_entry.py in convert(model, source, inputs, outputs, classifier_config, minimum_deployment_target, **kwargs)
    308             outputs=outputs,
    309             classifier_config=classifier_config,
--&gt; 310             **kwargs
    311         )
    312 

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/converter.py in _convert(model, convert_from, convert_to, converter_registry, **kwargs)
    132     frontend_converter = frontend_converter_type()
    133 
--&gt; 134     prog = frontend_converter(model, **kwargs)
    135     common_pass(prog)
    136 

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/converter.py in __call__(self, *args, **kwargs)
     82         from .frontend.torch import load
     83 
---&gt; 84         return load(*args, **kwargs)
     85 
     86 

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/frontend/torch/load.py in load(model_spec, debug, **kwargs)
     84         raise e
     85     except Exception as e:
---&gt; 86         raise e
     87 
     88     return prog

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/frontend/torch/load.py in load(model_spec, debug, **kwargs)
     74 
     75     try:
---&gt; 76         prog = converter.convert()
     77     except RuntimeError as e:
     78         if debug and "convert function" in str(e):

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/frontend/torch/converter.py in convert(self)
    222 
    223             # Add the rest of the operations
--&gt; 224             convert_nodes(self.context, self.graph)
    225 
    226             graph_outputs = [self.context[name] for name in self.graph.outputs]

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/frontend/torch/ops.py in convert_nodes(context, graph)
     53             )
     54         else:
---&gt; 55             _add_op(context, node)
     56 
     57         # We've generated all the outputs the graph needs, terminate conversion.

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/frontend/torch/ops.py in add(context, node)
    304         raise ValueError("ADD does not support scale factor param")
    305 
--&gt; 306     add_node = mb.add(x=add_inputs[0], y=add_inputs[1], name=node.name)
    307     context.add(add_node)
    308 

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/ops/registry.py in add_op(cls, **kwargs)
     60             @classmethod
     61             def add_op(cls, **kwargs):
---&gt; 62                 return cls._add_op(op_cls, **kwargs)
     63 
     64             setattr(Builder, op_type, add_op)

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/builder.py in _add_op(cls, op_cls, **kwargs)
    190         curr_block()._insert_op_before(new_op, before_op=before_op)
    191         new_op.build_nested_blocks()
--&gt; 192         new_op.type_value_inference()
    193         if len(new_op.outputs) == 1:
    194             return new_op.outputs[0]

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/operation.py in type_value_inference(self, overwrite_output)
    176         existing _output_vars
    177         """
--&gt; 178         output_types = self.type_inference()
    179         if not isinstance(output_types, tuple):
    180             output_types = (output_types,)

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/ops/defs/elementwise_binary.py in type_inference(self)
     41         shapea = list(typea.get_shape())
     42         shapeb = list(typeb.get_shape())
---&gt; 43         ret_shape = broadcast_shapes(shapea, shapeb)
     44         return types.tensor(primitive_type, ret_shape)
     45 

~/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/mil/mil/ops/defs/_utils.py in broadcast_shapes(shape_x, shape_y)
     42                 raise ValueError(
     43                     "Incompatible dim {} in shapes {} vs. {}".format(
---&gt; 44                         i, shape_x, shape_y
     45                     )
     46                 )

ValueError: Incompatible dim 2 in shapes (1, 16, -112, 112) vs. (1, 16, 144, 112)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Run the following code snippet to get the error:
&lt;denchmark-code&gt;import torch.nn.functional as F
from torch import nn
import torch

class MockModel(nn.Module):    
    def forward(self, input_a, input_b):
        # - Upsample a
        out_a = F.interpolate(input_a, scale_factor=2, mode='bilinear', align_corners=False)
#         s = input_a.size(2) * 2, input_a.size(3) * 2
#         out_a = F.interpolate(input_a, size=s, mode='bilinear', align_corners=False)
        
        # - Sum with input
        return out_a + input_b

r,c=144,112
lx = MockModel().eval()
tst1 = torch.rand(1, 32, r//2, c//2)
tst2 = torch.rand(1, 32, r, c)

lx_trace = torch.jit.trace(lx, (tst1, tst2))

coreml_model = ct.convert(
    lx_trace,
    inputs=[ct.TensorType(name="input_1", shape=tst1.shape, dtype=np.float32),
           ct.TensorType(name="input_2", shape=tst2.shape, dtype=np.float32)]
)
&lt;/denchmark-code&gt;

Now if instead you were to uncomment the two lines below out_a to do the interpolation using an explicit size rather than a scale factor, the conversion will succeed.
Playing around with the rows / columns of the input tensors results in different sizing errors, not necessarily negative, but still wrong.
&lt;denchmark-h:h2&gt;System environment (please complete the following information):&lt;/denchmark-h&gt;


coremltools version: 4.0b3
OS: MacOS
macOS version: Catalina (10.15.6)
XCode version: 12.0.1
How you install python: conda
python version (e.g. 3.7): 3.7

	</description>
	<comments>
		<comment id='1' author='ElteHupkes' date='2020-10-01T20:12:01Z'>
		Nice testcase. When I ran it, it gave me

WARNING:root:Saving value type of float16 into a builtin type of i8, might lose precision!
WARNING:root:Saving value type of float16 into a builtin type of i8, might lose precision!

and then failed as you say.
The warning looks what we discussed in &lt;denchmark-link:https://github.com/apple/coremltools/issues/887&gt;#887&lt;/denchmark-link&gt;
, specifically &lt;denchmark-link:https://github.com/apple/coremltools/issues/887#issuecomment-683307279&gt;#887 (comment)&lt;/denchmark-link&gt;
  where it seems something is cast to int8 and the sign is in the way. I do not understand where that float16-&gt;i8 actually comes from.
Funnily enough, when I just pull the TOT for coremltools,  I do not get the WARNING anymore, and NO negative dimension. Instead, I get a warning after conversion
coremltools/models/model.py:121: RuntimeWarning: You will not be able to run predict() on this Core ML model. Underlying exception message was: Error compiling model: "Error reading protobuf spec. validator error: The .mlmodel supplied is of version 5, intended for a newer version of Xcode. This version of Xcode supports model version 4 or earlier.".
which probably means I need to update my Xcode.
Therefore, maybe update your coremltools version and give it another whirl?
		</comment>
		<comment id='2' author='ElteHupkes' date='2020-10-02T06:23:41Z'>
		I was under the impression I already had the latest version... But now I see on pypi there's a b4 that came out literally yesterday. I'll give it a shot later.
With regards to the
&lt;denchmark-code&gt;RuntimeWarning: You will not be able to run predict() on this Core ML model. Underlying exception message was: Error compiling model: "Error reading protobuf spec. validator error: The .mlmodel supplied is of version 5, intended for a newer version of Xcode. This version of Xcode supports model version 4 or earlier."
&lt;/denchmark-code&gt;

After I updated my model with the workaround, I got this too. Then I went and compiled it anyway, and it worked - at least on an iOS 14 simulator. I should probably check on a 13 simulator.
		</comment>
		<comment id='3' author='ElteHupkes' date='2020-10-06T15:42:36Z'>
		&lt;denchmark-link:https://github.com/leovinus2001&gt;@leovinus2001&lt;/denchmark-link&gt;
 Well, it seems you're right - I just tried this with b4 and the error is no longer there! Timing...
		</comment>
	</comments>
</bug>