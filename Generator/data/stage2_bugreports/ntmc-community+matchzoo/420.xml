<bug id='420' author='ZizhenWang' open_date='2018-10-18T14:23:41Z' closed_time='2018-10-29T02:06:38Z'>
	<summary>[hotfix] cdssm preprocess dont need SlidingWindowUnit</summary>
	<description>
the first layer of CDSSM model is Conv1D, which doesn't need a extra sliding window process ðŸ˜µ
	</description>
	<comments>
		<comment id='1' author='ZizhenWang' date='2018-10-18T19:00:09Z'>
		convince me it's a bug &lt;denchmark-link:https://github.com/ZizhenWang&gt;@ZizhenWang&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ZizhenWang' date='2018-10-19T09:18:05Z'>
		assume input text is
[word1, word2, word3, word4, ... ]
after word hashing ,it can be transformed to
[word1_ngram, word2_ngram, word3_ngram, word4_ngram, ...]
the CDSSM file define the model structure as Input -&gt; Conv1D -&gt; ...
so if the input data has shape as [word1_ngram, word2_ngram, word3_ngram, word4_ngram, ...],  Conv1D use the kernel_size to represent sliding window length, and convolute on the window contexts naturally, the inner compute formula can be W * [word1_ngram, word2_ngram, word3_ngram] + b, which is right
but with sliding window unit, the preprocessor output data like
[ [word1_ngram, word2_ngram, word3_ngram], [word2_ngram, word3_ngram, word4_ngram], ...]
input these data to Conv1D layer, the inner compute is
W * [[word1_ngram, word2_ngram, word3_ngram], [word2_ngram, word3_ngram, word4_ngram], [word3_ngram, word4_ngram, word5_ngram],] + b, which is wrong
		</comment>
		<comment id='3' author='ZizhenWang' date='2018-10-21T08:45:37Z'>
		no, the shape of the input data should be num_word_ngrams * 3dim_triletter.
After sliding window, the sentence "this is a test sentence" -&gt; [[this is a], [is a test], [a test sentence]]
Each word, say it's a 30000 dimensional vector, then the sentence will be represented as:
[0...1...1...0] # this is a 90000 d vector of `this is a`
[0...0...1...0] # 90000d of `is a test`
[1...0...1...0] # 90000d of `a test sentence`
So the input shape (x) should be num_word_ngrams (i.e. 3 here) * 90000.
Without sliding word unit, you didn't even have word_ngram, you only got letter_ngram, which is 30000d.
Letter-trigram -&gt; test -&gt; 30000d
Word-trigram -&gt; a test sentence -&gt; 90000d.
		</comment>
		<comment id='4' author='ZizhenWang' date='2018-10-21T09:36:27Z'>
		we should consider the convolution process, if the input data is [[this is a], [is a test], [a test sentence]] and the kernel size is 3, then the output is W * [[this is a], [is a test], [a test sentence]] + b = 270k-dim, not what we wanted like W * [this is a] + b = 90k-dim
		</comment>
	</comments>
</bug>