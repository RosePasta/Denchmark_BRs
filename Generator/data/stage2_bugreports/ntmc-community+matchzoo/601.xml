<bug id='601' author='Punchwes' open_date='2019-01-30T02:49:52Z' closed_time='2019-02-18T08:23:45Z'>
	<summary>Embedding dimension not match?</summary>
	<description>
The loading embedding matrix process in the DRMM tutorial seems to have problem. The embedding input dimension is set to be the vocabulary size (which is the length of term_index), but the result of your build_matrix function gives incompatible input dimension which raises error when calling the load_embedding_matrix function.
Is there anything wrong with it? I try to make some modifications to the source code to make compatible dimension but the final result has large gap between your DRMM tutorial, and the training loss does not decrease.
	</description>
	<comments>
		<comment id='1' author='Punchwes' date='2019-01-30T05:49:39Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/4717005/51960651-d2afb100-2494-11e9-96bc-1ae6552871eb.png&gt;&lt;/denchmark-link&gt;

Could you see these issue templates when you create this issue? There isn't enough information in your question and it's hard for us to help you (and the issue templates were designed to solve this problem). If you think the issue templates are too verbose, we could make some changes. Afterall, we want to help you and more information make that easier.
More specifically, you may want to post your error message, model.params before you load the embedding matrix, or the specific code modification you made to solve the problem. The vocab size actually should be len(term_index) + 1, just in case you're not aware of that.
		</comment>
		<comment id='2' author='Punchwes' date='2019-01-31T09:19:29Z'>
		Hi,
I am running the provided DRMM tutorial, and stuck at loading embedding matrix. The load_embedding_matrix function gives me the error:
&lt;denchmark-link:https://user-images.githubusercontent.com/19360686/52043352-70c97700-257a-11e9-903e-cd43b9020b9a.png&gt;&lt;/denchmark-link&gt;

len(term_index) + 1 gives me 16674 while preprocessor.context['vocab_size'] gives 16673.
I did not change any code in the DRMM tutorial and do not understand why.
&lt;denchmark-link:https://user-images.githubusercontent.com/19360686/52044225-9788ad00-257c-11e9-8243-6532f87a6882.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/19360686/52048187-bccde900-2585-11e9-9a27-e40993e8b02d.png&gt;&lt;/denchmark-link&gt;

As you see the two input dimension does not match.
		</comment>
		<comment id='3' author='Punchwes' date='2019-01-31T14:07:28Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/4717005/52059213-2eb52b00-25a4-11e9-8c95-b7cc3480e325.png&gt;&lt;/denchmark-link&gt;

Change this to vocab size + 1.
The tutorial was made before the "vocab size off by 1" bug in preprocessors was fixed. That's why.
		</comment>
		<comment id='4' author='Punchwes' date='2019-02-06T04:10:00Z'>
		I hope things are working well for you now. I’ll go ahead and close this issue, but I’m happy to continue further discussion whenever needed.
		</comment>
		<comment id='5' author='Punchwes' date='2019-02-11T02:17:27Z'>
		thanks very much, it solves the problem.
		</comment>
	</comments>
</bug>