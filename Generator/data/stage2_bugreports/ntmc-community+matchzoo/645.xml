<bug id='645' author='jellying' open_date='2019-03-14T07:29:35Z' closed_time='2019-03-15T07:07:04Z'>
	<summary>Preprocessing error on SNLI dataset.</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;

I modified the ARC-I tutorial on WikiQA to test it on SNLI. A bug occur when using the code below:
&lt;denchmark-code&gt;import keras
import pandas as pd
import numpy as np
import matchzoo as mz
train_pack = mz.datasets.snli.load_data('train', task='classification', target_label="entailment")
valid_pack = mz.datasets.snli.load_data('dev', task='classification', target_label="entailment")
predict_pack = mz.datasets.snli.load_data('test', task='classification', target_label="entailment")
preprocessor = mz.preprocessors.BasicPreprocessor(fixed_length_left=30, fixed_length_right=30, remove_stop_words=False)
train_pack_processed = preprocessor.fit_transform(train_pack)
valid_pack_processed = preprocessor.transform(valid_pack)
predict_pack_processed = preprocessor.transform(predict_pack)
classification_task = mz.tasks.Classification()
classification_task.metrics = 'acc'
model = mz.models.ArcI()
model.params['input_shapes'] = preprocessor.context['input_shapes']
model.params['task'] = classification_task
model.params['embedding_input_dim'] = preprocessor.context['vocab_size']
model.params['embedding_output_dim'] = 300
model.params['num_blocks'] = 1
model.params['left_filters'] = [128]
model.params['left_kernel_sizes'] = [3]
model.params['left_pool_sizes'] = [4]
model.params['right_filters'] = [128]
model.params['right_kernel_sizes'] = [3]
model.params['right_pool_sizes'] = [4]
model.params['conv_activation_func']= 'relu'
model.params['mlp_num_layers'] = 1
model.params['mlp_num_units'] = 100
model.params['mlp_num_fan_out'] = 1
model.params['mlp_activation_func'] = 'relu'
model.params['dropout_rate'] = 0.9
model.params['optimizer'] = 'adadelta'
model.guess_and_fill_missing_params()
model.build()
model.compile()
model.backend.summary()
glove_embedding = mz.embedding.load_from_file('/users/zhaojunyao/data/embeddings/Eng/glove.840B.300d.txt', mode='glove')
embedding_matrix = glove_embedding.build_matrix(preprocessor.context['vocab_unit'].state['term_index'])
model.load_embedding_matrix(embedding_matrix)
pred_x, pred_y = predict_pack_processed[:].unpack()
evaluate = mz.callbacks.EvaluateAllMetrics(model, x=pred_x, y=pred_y, batch_size=len(pred_x))
train_generator = mz.PairDataGenerator(train_pack_processed, num_dup=2, num_neg=1, batch_size=20)
print(len(train_generator))
history = model.fit_generator(train_generator, epochs=30, callbacks=[evaluate], workers=30, use_multiprocessing=True)
&lt;/denchmark-code&gt;

The error is below:
&lt;denchmark-code&gt;Using TensorFlow backend.
Processing text_left with chain_transform of Tokenize =&gt; Lowercase =&gt; PuncRemoval: 100%|██████████| 150736/150736 [00:40&lt;00:00, 3710.18it/s]
Processing text_right with chain_transform of Tokenize =&gt; Lowercase =&gt; PuncRemoval:  18%|█▊        | 85556/480041 [00:19&lt;01:25, 4608.28it/s]Traceback (most recent call last):
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/test_model_on_snli.py", line 9, in &lt;module&gt;
    train_pack_processed = preprocessor.fit_transform(train_pack)
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/engine/base_preprocessor.py", line 97, in fit_transform
    return self.fit(data_pack, verbose=verbose) \
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/preprocessors/basic_preprocessor.py", line 95, in fit
    verbose=verbose)
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/data_pack/data_pack.py", line 249, in wrapper
    func(target, *args, **kwargs)
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/data_pack/data_pack.py", line 381, in apply_on_text
    self._apply_on_text_both(func, rename, verbose=verbose)
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/data_pack/data_pack.py", line 409, in _apply_on_text_both
    self._apply_on_text_right(func, rename=right_name, verbose=verbose)
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/data_pack/data_pack.py", line 394, in _apply_on_text_right
    self._right[name] = self._right['text_right'].progress_apply(func)
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/tqdm/_tqdm.py", line 679, in inner
    result = getattr(df, df_function)(wrapper, **kwargs)
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/pandas/core/series.py", line 3591, in apply
    mapped = lib.map_infer(values, f, convert=convert_dtype)
  File "pandas/_libs/lib.pyx", line 2217, in pandas._libs.lib.map_infer
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/tqdm/_tqdm.py", line 675, in wrapper
    return func(*args, **kwargs)
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/preprocessors/chain_transform.py", line 19, in wrapper
    arg = unit.transform(arg)
  File "/users/zhaojunyao/pyitems/forked_MatchZoo/MatchZoo/matchzoo/preprocessors/units/tokenize.py", line 17, in transform
    return nltk.word_tokenize(input_)
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/__init__.py", line 128, in word_tokenize
    sentences = [text] if preserve_line else sent_tokenize(text, language)
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/__init__.py", line 95, in sent_tokenize
    return tokenizer.tokenize(text)
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py", line 1237, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py", line 1285, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py", line 1276, in span_tokenize
    return [(sl.start, sl.stop) for sl in slices]
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py", line 1276, in &lt;listcomp&gt;
    return [(sl.start, sl.stop) for sl in slices]
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py", line 1316, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py", line 312, in _pair_iter
    prev = next(it)
  File "/users/zhaojunyao/env/lib/python3.6/site-packages/nltk/tokenize/punkt.py", line 1289, in _slices_from_text
    for match in self._lang_vars.period_context_re().finditer(text):
TypeError: expected string or bytes-like object
Processing text_right with chain_transform of Tokenize =&gt; Lowercase =&gt; PuncRemoval:  18%|█▊        | 85794/480041 [00:20&lt;01:33, 4237.23it/s]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run the code again.
&lt;denchmark-h:h3&gt;Describe your attempts&lt;/denchmark-h&gt;


 I checked the documentation and found no answer
 I checked to make sure that this is not a duplicate issue

You should also provide code snippets you tried as a workaround, StackOverflow solution that you have walked through, or your best guess of the cause that you can't locate (e.g. cosmic radiation).
&lt;denchmark-h:h3&gt;Context&lt;/denchmark-h&gt;


OS [e.g. Windows 10, macOS 10.14]: CentOS 7
Hardware [e.g. CPU only, GTX 1080 Ti]: Tesla K80

MatchZoo version = 2.1
&lt;denchmark-h:h3&gt;Additional Information&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='jellying' date='2019-03-14T08:44:42Z'>
		The problem is that you have None (NA) types in your datapack object. The best thing to do is to manually drop nans when loading the data set.
something like:
def _read_data(path):
    table = pd.read_csv(path, sep='\t')
    df = pd.DataFrame({
        'text_left': table['sentence1'],
        'text_right': table['sentence2'],
        'label': table['gold_label']
    })
  df.dropna(axis=0, how='any', inplace=True)
  # continue
And to see whether the bug still occurs.
See &lt;denchmark-link:https://github.com/NTMC-Community/MatchZoo/blob/f58f176e04ebfbd38a3dcc540af7efecf6cb14c0/matchzoo/datasets/snli/load_data.py#L79-L85&gt;here&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jellying' date='2019-03-14T11:07:45Z'>
		Thanks. Add this line below and it worked well.
&lt;denchmark-code&gt;df.dropna(axis=0, how='any', inplace=True)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>