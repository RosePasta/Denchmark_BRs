<bug id='598' author='canjiali' open_date='2019-01-26T09:08:51Z' closed_time='2019-07-12T02:37:51Z'>
	<summary>The computation on NDCG may be wrong</summary>
	<description>
The original definition of NDCG is to take all the relevant documents in the corpus but not in your prediction into account when computing IDCG. However, in your implementation, NDCG is computed base on the predicted score and their ground truth. e.g., for a predicted score list y_pred = [0.4, 0.2, 0.5, 0.7] and the corresponding ground-truth y_true = [0, 1, 2, 0], you would treat the golden list as [2, 1, 0, 0]. But, there may be some other relevant documents in the corpus that you do not retrieve, say document a with relevance judgment 2 and document b with relevance judgment 1, then the golden list should be [2, 2, 1, 1] when computing IDCG. The way you compute NDCG would make it much higher than it is in some cases.
Thanks for your attention.
Reference:
&lt;denchmark-link:https://github.com/NTMC-Community/MatchZoo/blob/master/matchzoo/metrics/normalized_discounted_cumulative_gain.py#L34&gt;https://github.com/NTMC-Community/MatchZoo/blob/master/matchzoo/metrics/normalized_discounted_cumulative_gain.py#L34&lt;/denchmark-link&gt;

&lt;denchmark-link:https://en.wikipedia.org/wiki/Discounted_cumulative_gain&gt;https://en.wikipedia.org/wiki/Discounted_cumulative_gain&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='canjiali' date='2019-01-26T12:59:11Z'>
		Thanks for reporting this. Definitely worth attention.
		</comment>
		<comment id='2' author='canjiali' date='2019-01-29T09:29:15Z'>
		I'll work on this one
		</comment>
		<comment id='3' author='canjiali' date='2019-01-29T14:09:14Z'>
		@StKlaas Can you review &lt;denchmark-link:https://github.com/NTMC-Community/MatchZoo/pull/599&gt;#599&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='4' author='canjiali' date='2019-01-30T08:20:24Z'>
		@StKlaas you are right for the re-ranking when the candidate list is produced by another algorithm. We will fix this. Thanks for your warning!
		</comment>
		<comment id='5' author='canjiali' date='2019-01-30T09:00:06Z'>
		&lt;denchmark-link:https://github.com/bwanglzu&gt;@bwanglzu&lt;/denchmark-link&gt;
 The revised code for dcg is so confusing.
&lt;denchmark-link:https://github.com/faneshion&gt;@faneshion&lt;/denchmark-link&gt;
 yep, when your algorithm cannot recall all the relevant items, that would inevitably hurt the model evaluation, especially in IR. Thanks for your attention again.
		</comment>
		<comment id='6' author='canjiali' date='2019-07-12T02:37:51Z'>
		I hope things are working well for you now. I’ll go ahead and close this issue, but I’m happy to continue further discussion whenever needed.
		</comment>
		<comment id='7' author='canjiali' date='2019-09-29T11:33:33Z'>
		I am also working on using matchzoo for ad-hoc web search research. The problem is that we usually evaluate models by re-ranking a list of documents retrieved by a simple model such as BM25. For example, we have 2 queries in the test datapack. For query A, we have 1000 docs retrieved from a corpus, and for query B, we have another 1000 docs retrieved from a corpus.  If I understand correctly, matchzoo mixs all the 2000 documents and rank them, so it is different from IR evaluation
My workaround is to create a datapack for each test query, and then take the average. I wonder if there is any better solution?
		</comment>
	</comments>
</bug>