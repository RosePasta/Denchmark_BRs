<bug id='302' author='mrksr' open_date='2016-12-20T10:57:09Z' closed_time='2016-12-22T19:00:28Z'>
	<summary>Positive transform fails large numbers</summary>
	<description>
The positive transform implemented in GPflow.transforms.positive is an alias for the Log1pe-transform. The name suggests that it can be used to represent arbitrary positive numbers, however, it is actually only possible to to represent numbers up a value of about 700 due to the result of np.exp growing too large:
In [31]: GPflow.transforms.positive.forward(700)
Out[31]: 700.000001

In [32]: GPflow.transforms.positive.forward(800)
/(...)/GPflow/transforms.py:129: RuntimeWarning: overflow encountered in exp
  return np.log(1. + np.exp(x)) + self._lower
Out[32]: inf

In [33]: GPflow.transforms.positive.backward(700)
Out[33]: array([ 699.999999])

In [34]: GPflow.transforms.positive.backward(800)
/(...)/GPflow/transforms.py:138: RuntimeWarning: overflow encountered in exp
  return np.log(np.exp(y - self._lower) - np.ones(1, np_float_type))
Out[34]: array([ inf])
Since the positive transform degrades to an identity function for large values, it should be fairly easy to fix this. However, it is probably important to ensure that both forward and tf_forward behave the same.
I guess a similar problem will exist for the Logistic-transform.
	</description>
	<comments>
		<comment id='1' author='mrksr' date='2016-12-20T10:59:43Z'>
		This is also highlighted in &lt;denchmark-link:https://github.com/GPflow/GPflow/issues/300&gt;#300&lt;/denchmark-link&gt;
. I didn't think this was all too much of a problem, since values of 700 would be odd to encounter, but given the simplicity of the fix, it may be worth doing.
		</comment>
		<comment id='2' author='mrksr' date='2016-12-20T11:04:37Z'>
		We should fix this, marking as a bug.
If the optimal value is out of range, then we'll not get an overflow during optimization, because the tensorflow representation of the forward transform won't overflow. But when the optimization finishes, we get the overflow when putting the optimized vector back into the model state. Then, a successive call to optimize will fail, because the reverse transform (to initialize the optimizer) will fail.
		</comment>
		<comment id='3' author='mrksr' date='2016-12-20T11:09:25Z'>
		tf.nn.softplus should do the trick:
&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/nn/activation_functions_#softplus&gt;https://www.tensorflow.org/api_docs/python/nn/activation_functions_#softplus&lt;/denchmark-link&gt;

I'll have a look for a fix for logistic later I have a meeting with Zoubin this morning.
		</comment>
		<comment id='4' author='mrksr' date='2016-12-20T11:20:50Z'>
		We already use tf.nn.softplus on the tensorflow side, the problem comes when setting the values, at the end of optimization, which uses the numpy transform. I'll push a fix for Log1pe, which I think is important. I'll leave Logistic for you, &lt;denchmark-link:https://github.com/alexggmatthews&gt;@alexggmatthews&lt;/denchmark-link&gt;
 :)
		</comment>
		<comment id='5' author='mrksr' date='2016-12-20T11:35:20Z'>
		OK sorry for the misunderstanding.
		</comment>
		<comment id='6' author='mrksr' date='2016-12-21T12:20:46Z'>
		I have played around a bit with the Logistic transform and I think it can be left as-is actually, since the only exponentiation happens in a forward-transform and within floating point accuracy, it is not really possible to get values through the backward-transform which might be a problem in forward direction (if that makes sense):
In []: lg = GPflow.transforms.Logistic(0, 1)

In []: close_to_1 = 0.9999999999999999

In []: actually_1 = 0.99999999999999999

In []: lg.backward(close_to_1)
Out[]: 36.043653389117154

In []: lg.forward(lg.backward(close_to_1))
Out[]: 0.99999999999999978

In []: lg.backward(actually_1)
/nosave/sava_0/kaiser/python/GPflow/GPflow/transforms.py:159: RuntimeWarning: divide by zero encountered in log
  return -np.log((self.b - self.a) / (y - self.a) - 1.)
Out[]: inf

In []: close_to_1 == 1.
Out[]: False

In []: actually_1 == 1.
Out[]: True
It looks quite the same arount zero. I'd say while for the -transform it was actually plausible to run into problems before the fix in &lt;denchmark-link:https://github.com/GPflow/GPflow/commit/2049274338d68e9bcc35ca290c4dabcfe7d32fbb&gt;2049274&lt;/denchmark-link&gt;
 (thanks for that!), for the  transform, the optimization itself would have to do fairly strange things.
		</comment>
		<comment id='7' author='mrksr' date='2016-12-22T19:00:28Z'>
		Great investigating, &lt;denchmark-link:https://github.com/mrksr&gt;@mrksr&lt;/denchmark-link&gt;
 .
I take it this is fixed by &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/303&gt;#303&lt;/denchmark-link&gt;
 , marking as closed. Happy Christmas!
		</comment>
	</comments>
</bug>