<bug id='1479' author='elvijs' open_date='2020-05-20T11:30:16Z' closed_time='2020-05-21T09:33:03Z'>
	<summary>Esoteric bug whilst deepcopy-ing</summary>
	<description>
Our training procedure involves deepcopying a model. This fails when we have a parameter with tfp.distributions.LogNormal prior that is used early in training, but is subsequently disabled. The failure is TypeError: can't pickle HashableWeakRef objects.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

I've attempted to trim away all irrelevant bits from our full training pipeline. It looks like the root cause relates to sampling from tfp.distributions.LogNormal (although anecdotally, we've seen similar issues with some bijectors).
Minimal, reproducible example
import gpflow
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp


def boom() -&gt; None:
    m = tf.Module()
    m.prior = tfp.distributions.LogNormal(0.0, 1.0)  # error
    # m.prior = tfp.distributions.Normal(0.0, 1.0)  # no error

    @tf.function  # no error without the decorator
    def _sample() -&gt; tf.Tensor:
        m.prior.sample()
        # it looks like the sampling pollutes the graph - if you comment out the line above,
        # then we don't explode

    _sample()
    m.prior = None  # if you comment out this line, then we don't explode

    gpflow.utilities.deepcopy(m)


if __name__ == '__main__':
    boom()
Stack trace, or error message
&lt;denchmark-code&gt;  File "/home/elvijs/.PyCharmCE2019.2/config/scratches/scratch_7.py", line 39, in &lt;module&gt;
    boom()
  File "/home/elvijs/.PyCharmCE2019.2/config/scratches/scratch_7.py", line 35, in boom
    gpflow.utilities.deepcopy(gp_model)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/site-packages/gpflow/utilities/utilities.py", line 265, in deepcopy
    return copy.deepcopy(reset_cache_bijectors(input_module), memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 274, in _reconstruct
    y = func(*args)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 273, in &lt;genexpr&gt;
    args = (deepcopy(arg, memo) for arg in args)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 274, in _reconstruct
    y = func(*args)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 273, in &lt;genexpr&gt;
    args = (deepcopy(arg, memo) for arg in args)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 274, in _reconstruct
    y = func(*args)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 273, in &lt;genexpr&gt;
    args = (deepcopy(arg, memo) for arg in args)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 305, in _reconstruct
    key = deepcopy(key, memo)
  File "/home/elvijs/.virtualenvs/automl_res/lib/python3.7/copy.py", line 169, in deepcopy
    rv = reductor(4)
TypeError: can't pickle HashableWeakRef objects
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

I expected no blow-up.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


GPflow version: gpflow @ git+https://github.com/GPflow/GPflow.git@61f8d84ea170791460468ef39ce5c38d3ec20a2a (that is, I'm including the fixes in #1476)
GPflow installed from: pip install git+https://github.com/GPflow/GPflow.git@61f8d84ea170791460468ef39ce5c38d3ec20a2a#egg=gpflow,
TensorFlow version: 2.2.0
TensorFlow Probability version: 0.10.0
Python version: 3.7.5
Operating system: Ubuntu

	</description>
	<comments>
		<comment id='1' author='elvijs' date='2020-05-20T11:38:15Z'>
		To clarify, &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/1470&gt;#1470&lt;/denchmark-link&gt;
 doesn't seem to fix it
		</comment>
		<comment id='2' author='elvijs' date='2020-05-20T11:49:45Z'>
		The minimal example can be simplified further by removing the GPR model and Parameter, and it's not just an issue for LogNormal but it seems to be any TransformedDistribution subclass (e.g. Chi(), Gumbel(), ...).
Edited to add: Also note that when we keep the prior and don't set it to None, the code runs fine as well. And the same behaviour exists in tensorflow_probability==0.9.
		</comment>
		<comment id='3' author='elvijs' date='2020-05-20T11:56:49Z'>
		Thanks &lt;denchmark-link:https://github.com/elvijs&gt;@elvijs&lt;/denchmark-link&gt;
. This is a very neat explanation and it has a good failure example. The good sign is that a deepcopy doesn't fail with recursion. My suspicion is that LogNormal stores that reference somewhere else outside of  and  hashmaps.
Those references are a TFP's pain point. I hope that the issue will be resolved soon once and for all. Meanwhile, I'm looking at what I can do to resolve your bug.
Thanks.
		</comment>
		<comment id='4' author='elvijs' date='2020-05-20T11:58:39Z'>
		Alright, found the issue and how to fix it: the TransformedDistribution such as LogNormal contains a bijector. Bijectors have caches which prevent you from copying it; I think this only gets activated in graph mode (with tf.function). gpflow.utilities.deepcopy() calls gpflow.reset_cache_bijectors() first to clear those caches, but by setting the prior to None you remove any references that could be found by traversing the module you pass in to gpflow.utilities.deepcopy(). The blow-up is fixed if you manually reset caches before setting the prior to None.
As a workaround, we could add a setter to gpflow's Parameter.prior that resets the cache before overwriting it.
		</comment>
		<comment id='5' author='elvijs' date='2020-05-20T12:02:42Z'>
		I.e., your problem goes away if you add gpflow.utilities.reset_cache_bijectors(m.prior) above the line that sets m.prior = None.
Alternatively, you can manually create a reference so that it can be found through traversing m:
m.old_prior = m.prior ; m.prior = None
		</comment>
		<comment id='6' author='elvijs' date='2020-05-20T12:14:05Z'>
		
As a workaround, we could add a setter to gpflow's Parameter.prior that resets the cache before overwriting it.

Please, no. Prohibiting any attempts to change the prior would be more reasonable.
&lt;denchmark-link:https://github.com/elvijs&gt;@elvijs&lt;/denchmark-link&gt;
 , Q: Why do you want to reset prior to ? Changing objects in-place is always a dangerous thing to do. Would be a parameters snapshot and a copy of the object a reasonable alternative for you?
		</comment>
		<comment id='7' author='elvijs' date='2020-05-20T13:04:45Z'>
		Thanks for the quick response &lt;denchmark-link:https://github.com/st--&gt;@st--&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
! :)
We're tinkering with random initialisation before optimisation. We thus set parameter priors at random init stage, but disable them for training and evaluation. I agree that this is not ideal as we're changing the objects in-flight. &lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
 I actually suggest that we deepcopy the model and use the copy for random init, does that make sense to you?
		</comment>
		<comment id='8' author='elvijs' date='2020-05-20T13:06:24Z'>
		Actually, apologies, I think that's what you're suggesting anyway :D &lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='elvijs' date='2020-05-20T13:08:41Z'>
		Note that from the perspective of the interface, it might make sense to add a disable_prior() method on Parameter? I think our usage above is nasty, but still within what is allowed by the interface?
		</comment>
		<comment id='10' author='elvijs' date='2020-05-20T13:22:36Z'>
		I think it's more a fundamental tensorflow "tf.function (compute graphs) and mutating state in-place does not play well together" issue than anything we should resolve in GPflow. It's not specific to GPflow at all (except for us providing the deepcopy() method). So I don't see anything for us to do in gpflow about this.
		</comment>
		<comment id='11' author='elvijs' date='2020-05-20T13:33:32Z'>
		
So I don't see anything for us to do in gpflow about this.

I'm not sure I agree.
From my perspective, we're using GPflow in a way that's completely in line with what the docs and the Parameter API is allowing. If we should not be mutating Parameter attributes, then surely that should be communicated in some way?
Note that in an ideal world, GPflow would hide all the nasties from TF away, so users never have to worry about it..
		</comment>
		<comment id='12' author='elvijs' date='2020-05-20T22:31:27Z'>
		
From my perspective, we're using GPflow in a way that's completely in line with what the docs and the Parameter API is allowing. If we should not be mutating Parameter attributes, then surely that should be communicated in some way?

It's an issue with TensorFlow (Probability) though; note that I've edited the minimal example so that the only reference to gpflow is that you use the deepcopy utility function. We could try and add work-arounds for lots of specific cases in GPflow, but how would we know we've caught all of them? I'd rather keep our code base clean; if anything, with gpflow 2.0 we've tried moving away from hiding TensorFlow and making it easier to mix&amp;match gpflow &amp; other tensorflow-based code (such as TFP for bijectors, priors, and likelihoods).
Maybe you could open an issue or otherwise bring this to the attention of TensorFlow (probability) to encourage them to sort it out properly?
		</comment>
		<comment id='13' author='elvijs' date='2020-05-21T09:04:22Z'>
		Fair enough. I'd much prefer to only worry about GPflow and have it guide me towards using only the safe bits of TensorFlow. However, it sounds like you have explicitly decided against trying to do that, which given the wild ride of TF "features" is completely understandable.
When I have a bit of time, I'll try to produce a minimal example of the above that only uses TF code.
		</comment>
		<comment id='14' author='elvijs' date='2020-05-21T09:33:02Z'>
		Update: raised &lt;denchmark-link:https://github.com/tensorflow/probability/issues/944&gt;tensorflow/probability#944&lt;/denchmark-link&gt;
. Closing the present issue.
		</comment>
	</comments>
</bug>