<bug id='568' author='NMRobert' open_date='2017-11-20T22:00:38Z' closed_time='2017-11-26T13:03:27Z'>
	<summary>Predict call execution time degrades linearly with additional calls</summary>
	<description>

OS Platform and Distribution: Linux Ubuntu 17.04
TensorFlow installed from (source or binary): binary
Tensorflow version: tensorflow-gpu==1.4.0
GPflow version:  gpflow-1.0.0 (from github master source)
CUDNN installed: 6
GPU:  GeForce GTX 1080 Ti
Python Version: Python 3.5.3

Issue:
Repeated calls to .predict (either .predict_f or .predict_y) have degrading execution time when called successively. The initial call might take perhaps 10ms, but later calls might take as long as several seconds each. This poses a problem when many calls to *.predict are desired. I did not observe this behaviour in versions of gpflow prior to 1.0.0.  This occurs for at least the regression models (GPR, SGPR, SVGP), and possibly others.
&lt;denchmark-code&gt;import numpy as np
import matplotlib.pyplot as plt
import gpflow
import time

X = np.linspace(0,10, 100).reshape(-1,1)
y = np.sin(X) + np.random.normal(0, 0.25, size=100).reshape(-1,1)

k = gpflow.kernels.RBF(input_dim=1)
m = gpflow.models.gpr.GPR(X, y, kern=k)
m.compile()
gpflow.train.ScipyOptimizer().minimize(m)

print('Done optimize.')
predict_duration_log = []
for i in range(250):
    start = time.time()
    m.predict_y(X) 
    end = time.time()
    predict_duration_log.append(end-start)

plt.plot(predict_duration_log)
plt.show()
&lt;/denchmark-code&gt;

&lt;denchmark-link:url&gt;

&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='NMRobert' date='2017-11-21T10:20:23Z'>
		I replicated your experiment and came to the same conclusion. The code was running on CPU.
&lt;denchmark-h:h2&gt;GPflow 1.0&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/13780846/33067149-2d392af8-cea5-11e7-8213-8dcb910a30b0.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;GPflow 0.5&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/13780846/33067150-2d9d1626-cea5-11e7-9381-f32b00486570.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='NMRobert' date='2017-11-21T12:46:18Z'>
		&lt;denchmark-link:https://github.com/NMRobert&gt;@NMRobert&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vdutor&gt;@vdutor&lt;/denchmark-link&gt;
 thank you for reporting and confirming.
&lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
 I'm away from my desk but is this possibly because the graph is growing each time we call predict?
		</comment>
		<comment id='3' author='NMRobert' date='2017-11-21T13:44:30Z'>
		That's very interesting, I hope it is not happening because of report_uninitialized_variables... In each run gpflow now checks if variable needs re-initialization or not. Each iteration in the example above adds report_uninitialized_variables operation to the graph and looks like the TensorFlow doesn't recognize which operations must stay w/o touching and which it should run. But, this is just a theory, I will check it.
		</comment>
		<comment id='4' author='NMRobert' date='2017-11-21T20:00:31Z'>
		Bad news, I was right...
&lt;denchmark-h:h3&gt;GPflow w/o report_uninitialized_variables.&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/24483645/33094391-ed7b744a-cef7-11e7-84f4-96e06a3ebcf4.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='NMRobert' date='2017-11-22T16:29:41Z'>
		&lt;denchmark-link:https://github.com/alexggmatthews&gt;@alexggmatthews&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/NMRobert&gt;@NMRobert&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/vdutor&gt;@vdutor&lt;/denchmark-link&gt;
  and others :)
What I can do to fix it: turn off default initialization and checking by tf.report_unitialized_variables. But when you switch to another session, you will have to initialize the model explicitly. Any objections?
		</comment>
		<comment id='6' author='NMRobert' date='2017-11-23T11:42:10Z'>
		I found a way to slow down speed degradation w/o removing auto-initializing feature:
&lt;denchmark-link:https://user-images.githubusercontent.com/24483645/33171218-54f2994a-d043-11e7-9ebe-f19a6f3ee91c.png&gt;&lt;/denchmark-link&gt;

EDIT: this ^^^ image is wrong
		</comment>
		<comment id='7' author='NMRobert' date='2017-11-23T11:58:12Z'>
		&lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
 Thanks for your work on this! I'm not sure I fully understand the consequences of removing auto-initialisation; I don't personally think it would be an issue (for me), but might be confusing and/or unexpected behaviour for some users. What would the code for this look like?
		</comment>
		<comment id='8' author='NMRobert' date='2017-11-23T12:01:26Z'>
		I think that small hurdles for users who want to manage thier own sessions is okay. If you know enough to use a session, you know enough to initialize the tf variables.
		</comment>
		<comment id='9' author='NMRobert' date='2017-11-23T12:05:07Z'>
		&lt;denchmark-link:https://github.com/NMRobert&gt;@NMRobert&lt;/denchmark-link&gt;
, When you switch from one session to another you will have to initialize parameters explicitly, for e.g.: .
		</comment>
		<comment id='10' author='NMRobert' date='2017-11-23T13:30:18Z'>
		&lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
: The slower degradation, is that still present for "normal" users? I think this is quite seriously bad behaviour. For example, in my training code (which I let run for days), I run the prediction code periodically to monitor performance. It would be really annoying for this to slow down.
		</comment>
		<comment id='11' author='NMRobert' date='2017-11-24T00:29:22Z'>
		Here is speed results after improvements which I made:
&lt;denchmark-h:h2&gt;W/O checking if variables were initialized (sec):&lt;/denchmark-h&gt;

In [9]: woi_pd.describe(percentiles=[0.75,0.9,0.999])
Out[9]:
count    50000.000000
mean         0.002592
std          0.003511
min          0.001846
50%          0.002432
75%          0.002665
90%          0.003011
99.9%        0.005679
max          0.388551
dtype: float64
&lt;denchmark-h:h2&gt;WITH checking if variables were initialized (sec):&lt;/denchmark-h&gt;

In [10]: wi_pd.describe(percentiles=[0.75,0.9,0.999])
Out[10]:
count    50000.000000
mean         0.004250
std          0.003326
min          0.003169
50%          0.004069
75%          0.004368
90%          0.004832
99.9%        0.008807
max          0.405657
dtype: float64
Let's vote:

+1 leave auto-initializing as is
-1 we can live w/o initializing :)

		</comment>
		<comment id='12' author='NMRobert' date='2017-11-24T07:56:55Z'>
		+1
		</comment>
		<comment id='13' author='NMRobert' date='2017-11-24T09:47:13Z'>
		Hi Artem, these benchmark times are really small. I'm only interested in the relative performance degradation for predictions which take around 5 or more seconds. If this will be the absolute degradation then I'm happy with the auto initialise. In my vote I have assumed this is the case, so if not, let me know :)
		</comment>
		<comment id='14' author='NMRobert' date='2017-11-24T09:59:54Z'>
		&lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
, it is not a degradation. I'm sorry for confusion - I edited an image above - my previous experiments were wrong. Here is new graphs:
&lt;denchmark-link:https://user-images.githubusercontent.com/24483645/33205228-128468ac-d0fe-11e7-9e28-b7dc45670ddc.png&gt;&lt;/denchmark-link&gt;

So, there is only small overhead 2-3ms, which is constant.
		</comment>
		<comment id='15' author='NMRobert' date='2017-11-24T10:06:13Z'>
		Perfect, let's do auto init then! The only time when this would be a bad thing is if people call predict many times for small numbers of predictions. This isn't a good thing to do anyway, since we still recompute the Cholesky (annoyingly, there's still no good solution to this). I think there's no reason not to use auto-init.
Also, does this solution also fix the Iterator problem we spoke about?
		</comment>
		<comment id='16' author='NMRobert' date='2017-11-24T10:10:35Z'>
		I'm happy with the proposed solution that leaves auto-init intact.
		</comment>
		<comment id='17' author='NMRobert' date='2017-11-24T10:14:43Z'>
		Here is results w/o spikes, just for the record:
In [44]: woi_pd.describe(percentiles=[0.75,0.9,0.999])
Out[44]:
count    49995.000000
mean         0.002557
std          0.000407
min          0.001846
50%          0.002432
75%          0.002665
90%          0.003011
99.9%        0.005564
max          0.013085
dtype: float64
In [46]: wi_pd.describe(percentiles=[0.75,0.9,0.999])
Out[46]:
count    49995.000000
mean         0.004218
std          0.000633
min          0.003169
50%          0.004068
75%          0.004368
90%          0.004831
99.9%        0.008685
max          0.042645
dtype: float64
&lt;denchmark-link:https://user-images.githubusercontent.com/24483645/33205841-439d053c-d100-11e7-9214-c53f887ef6a0.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='NMRobert' date='2017-11-26T13:03:14Z'>
		Resolved at &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/575&gt;#575&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>