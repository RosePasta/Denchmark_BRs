<bug id='1439' author='clwgg' open_date='2020-04-16T00:00:21Z' closed_time='2020-05-07T12:22:36Z'>
	<summary>"ValueError: Cannot create a tensor proto whose content is larger than 2GB." after upgrade from 2.0.0rc1 to 2.0.0 release</summary>
	<description>
Hi,
I've been running standard GPR on a fairly large dataset using the 2.0.0rc1 version of GPflow. Since the upgrade to 2.0.0, I'm running into the error
&lt;denchmark-code&gt;ValueError: Cannot create a tensor proto whose content is larger than 2GB.
&lt;/denchmark-code&gt;

on a dataset which worked fine before.
Here is the full error message:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "main.py", line 177, in &lt;module&gt;
    main(args)
  File "main.py", line 64, in main
    build_allele(args)
  File "/path/to/1_model_sim/drivers.py", line 226, in build_allele
    opt_model_list(m)
  File "/path/to/1_model_sim/model.py", line 355, in opt_model_list
    m.trainable_variables)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/gpflow/optimizers/scipy.py", line 73, in minimize
    func, initial_params, jac=True, method=method, **scipy_kwargs
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/scipy/optimize/_minimize.py", line 610, in minimize
    callback=callback, **options)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py", line 345, in _minimize_lbfgsb
    f, g = func_and_grad(x)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py", line 295, in func_and_grad
    f = fun(x, *args)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/scipy/optimize/optimize.py", line 327, in function_wrapper
    return function(*(wrapper_args + args))
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/scipy/optimize/optimize.py", line 65, in __call__
    fg = self.fun(x, *args)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/gpflow/optimizers/scipy.py", line 95, in _eval
    loss, grad = _tf_eval(tf.convert_to_tensor(x))
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 568, in __call__
    result = self._call(*args, **kwds)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 497, in _initialize
    *args, **kwds))
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    /path/to/1_model_sim/venv/lib/python3.6/site-packages/gpflow/optimizers/scipy.py:88 _tf_eval  *
        loss, grads = _compute_loss_and_gradients(closure, variables)
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/gpflow/optimizers/scipy.py:145 _compute_loss_and_gradients  *
        loss = loss_closure()
    /path/to/1_model_sim/model.py:354 None  *
        opt.minimize(lambda: - m.log_marginal_likelihood(),
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/gpflow/models/gpr.py:75 log_marginal_likelihood  *
        log_prob = multivariate_normal(Y, m, L)
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/gpflow/logdensities.py:95 multivariate_normal  *
        d = x - mu
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py:927 r_binary_op_wrapper
        x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name="x")
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1314 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_conversion_registry.py:52 _default_conversion_function
        return constant_op.constant(value, dtype, name=name)
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py:258 constant
        allow_broadcast=True)
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py:296 _constant_impl
        allow_broadcast=allow_broadcast))
    /path/to/1_model_sim/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py:522 make_tensor_proto
        "Cannot create a tensor proto whose content is larger than 2GB.")

    ValueError: Cannot create a tensor proto whose content is larger than 2GB.

&lt;/denchmark-code&gt;

Here are some relevant excerpts from my code:
&lt;denchmark-code&gt;m = gpf.models.GPR(data=(X, Y),
                    kernel=gpf.kernels.Exponential(active_dims = [0,1]),
                    mean_function=None)

opt = gpf.optimizers.Scipy()
opt.minimize(lambda: - m.log_marginal_likelihood(),
                m.trainable_variables)
&lt;/denchmark-code&gt;

I would greatly appreciate any help with this. Thanks!
	</description>
	<comments>
		<comment id='1' author='clwgg' date='2020-04-16T00:25:33Z'>
		Some more info that may or may not be relevant: I've tried this a bunch of times with the same dataset and others of similar size, on different machines, with either version of GPflow (using the same versions of dependencies) and the error occurs reliably in 2.0.0 and 2.0.1, but not in 2.0.0rc1. Unfortunately I'm not very familiar yet with the GPflow codebase (or tensorflow), so I have a hard time figuring out what may induce this behavior. Versions of other relevant packages are:
&lt;denchmark-code&gt;protobuf               3.11.3
tensorflow             2.1.0
tensorflow-estimator   2.1.0
tensorflow-probability 0.9.0

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='clwgg' date='2020-04-16T10:29:00Z'>
		Hi &lt;denchmark-link:https://github.com/clwgg&gt;@clwgg&lt;/denchmark-link&gt;
, thanks for reporting this! Can you make a minimal example that reproduces this bug - e.g. using  to make a sufficiently large data set? Off the top of my head I can't think of anything that changed from rc1 to the 2.0.0 release that would have changed this - so any further help you can give us in narrowing it down would be good :)
		</comment>
		<comment id='3' author='clwgg' date='2020-04-16T23:36:52Z'>
		Yes, absolutely! The code snippet below reproduces the behavior I report above: it runs fine on 2.0.0rc1, but throws the error on 2.0.0.
I should add that I run these analyses on CPU on a machine with 128 GB of RAM.
&lt;denchmark-code&gt;import numpy as np

import tensorflow as tf
import gpflow as gpf
import tensorflow_probability as tfp
from gpflow.utilities import print_summary

gpf.config.set_default_positive_minimum(1e-7)

X, Y = np.random.random((2000, 2)), np.random.random((2000, 200000))

m = gpf.models.GPR(data=(X, Y),
                        kernel=gpf.kernels.Exponential(active_dims = [0,1]),
                                            mean_function=None)

print_summary(m)

opt = gpf.optimizers.Scipy()
opt.minimize(lambda: - m.log_marginal_likelihood(),
                    m.trainable_variables)

print_summary(m)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='clwgg' date='2020-04-17T17:06:16Z'>
		Thank you. Thanks to , it looks like the offending commit is &lt;denchmark-link:https://github.com/GPflow/GPflow/commit/426f6d2b74e3e46604241de91a16896df193a609&gt;426f6d2&lt;/denchmark-link&gt;
: Add jit flag to gpflow.optimizers.Scipy (&lt;denchmark-link:https://github.com/GPflow/GPflow/pull/1190&gt;#1190&lt;/denchmark-link&gt;
).
We since renamed the flag to , and indeed if you change your example as follows:
-opt.minimize(lambda: - m.log_marginal_likelihood(),
-                    m.trainable_variables)
+opt.minimize(m.training_loss,
+                    m.trainable_variables, compile=False)
then it also works in the latest 2.0.1 release and on the develop branch. You can manually reintroduce the bug by explicitly wrapping the training objective in tf.function():
objective = tf.function(m.training_loss)
opt.minimize(objective, m.trainable_variables, compile=False)
The underlying reason is that the compilation by tf.function() adds all constants into the graph - in this case this includes the data (X, Y) stored on the model. And the graph's size is capped... So it's an unfortunate interaction between TensorFlow internals and how GPflow makes use of TensorFlow.
Simply not using tf.function() can make your code significantly slower than it needs to be when e.g. the model objective gets called many times throughout the hyperparameter optimization. I think it might make a difference if you convert the data to tf.Tensor objects first: can you try the following change in your (actual) code:
-m = gpf.models.GPR(data=(X, Y),
-                        kernel=gpf.kernels.Exponential(active_dims = [0,1]),
-                                            mean_function=None)
+m = gpf.models.GPR(data=(tf.convert_to_tensor(X), tf.convert_to_tensor(Y)),
+                        kernel=gpf.kernels.Exponential(active_dims = [0,1]),
+                                            mean_function=None)
? Let me know if that solves your problem and then we can think what we can do to make GPflow work better out of the box.
		</comment>
		<comment id='5' author='clwgg' date='2020-04-17T18:46:47Z'>
		Thanks, &lt;denchmark-link:https://github.com/st--&gt;@st--&lt;/denchmark-link&gt;
 for investigating this issue! Indeed, Protobuf (the way how TF graph is represented) has a hard limit in 2GB, details here &lt;denchmark-link:https://stackoverflow.com/a/51473218/7788672&gt;https://stackoverflow.com/a/51473218/7788672&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/clwgg&gt;@clwgg&lt;/denchmark-link&gt;
, the rule of thumb would be using TF tensors as much as possible and minimizing NumPy and TensorFlow interactions.
		</comment>
		<comment id='6' author='clwgg' date='2020-04-18T02:30:07Z'>
		I can confirm that wrapping the data input to  in  fixes the issue I report. Thanks a lot &lt;denchmark-link:https://github.com/st--&gt;@st--&lt;/denchmark-link&gt;
 for your help with this!
		</comment>
		<comment id='7' author='clwgg' date='2020-04-18T12:14:51Z'>
		I don't understand why tf.function can't just convert the numpy arrays to tensors itself! So I shall blame it on them, it's not exactly our own bug. Still, we shouldn't have gpflow users suffer!
As a work-around for this bug, I think we should just change the models to explicitly convert from numpy arrays to tensors in the  argument to the model constructors. &lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
 what do you think? any other/better ideas?
		</comment>
		<comment id='8' author='clwgg' date='2020-04-20T10:17:51Z'>
		I don't think we can classify it as a bug :) It is a special and unpleasant "feature". Anyway, it might change in TF2.2. &lt;denchmark-link:https://github.com/st--&gt;@st--&lt;/denchmark-link&gt;
, If you don't mind I will test it on TF2.2 and make a PR with  in models.
		</comment>
		<comment id='9' author='clwgg' date='2020-04-20T15:43:43Z'>
		I would consider "Doesn't behave as [reasonably] expected" as a bug - if we had clearly stated "do not pass numpy arrays as data to model constructors", it'd be different, but in fact the notebooks all use this pattern.
Thanks for looking into it! I'll assign this issue to you then. By when do you think you'll be able to get to it?
		</comment>
		<comment id='10' author='clwgg' date='2020-04-21T09:49:26Z'>
		
I would consider "Doesn't behave as [reasonably] expected" as a bug

I was sarcastic :)

By when do you think you'll be able to get to it?

Wednesday, 22 Apr.
		</comment>
		<comment id='11' author='clwgg' date='2020-05-07T15:22:25Z'>
		Hi &lt;denchmark-link:https://github.com/clwgg&gt;@clwgg&lt;/denchmark-link&gt;
 , thanks for helping us discover this issue - it's fixed in the latest 2.0.2 release!
		</comment>
		<comment id='12' author='clwgg' date='2020-05-07T15:24:54Z'>
		Wonderful, thank you!
		</comment>
		<comment id='13' author='clwgg' date='2020-05-07T21:34:50Z'>
		(And of course if it doesn't resolve your issue after all, please reopen it!)
		</comment>
	</comments>
</bug>