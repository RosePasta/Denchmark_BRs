<bug id='281' author='hughsalimbeni' open_date='2016-11-24T21:19:56Z' closed_time='2016-11-26T15:06:41Z'>
	<summary>minibatches with _optimize_tf</summary>
	<description>
Apologies if I'm missing something with usage here, but it seems that minibatches are not actually changing when using a tensorflow optimizer. Is the optimize function intended to be called once per batch? This requires a recompile each time so I suspect not.
When I moved the lines
feed_dict = {} self.update_feed_dict(self._feed_dict_keys, feed_dict) 
(225 of model) into the loop it works fine, however, but otherwise only the first batch is used for all the iterations.
	</description>
	<comments>
		<comment id='1' author='hughsalimbeni' date='2016-11-25T09:21:46Z'>
		&lt;denchmark-link:https://github.com/hughsalimbeni&gt;@hughsalimbeni&lt;/denchmark-link&gt;
 I will have a look into this.
		</comment>
		<comment id='2' author='hughsalimbeni' date='2016-11-25T10:27:29Z'>
		&lt;denchmark-link:https://github.com/hughsalimbeni&gt;@hughsalimbeni&lt;/denchmark-link&gt;
: How could you tell that the same minibatch was used? I'm pretty sure I have tested this in the past and I found that repeated calls to  yielded noisy results, therefore I concluded that different minibatches were passed. Similar for optimisation.
		</comment>
		<comment id='3' author='hughsalimbeni' date='2016-11-25T11:02:37Z'>
		I noticed this when trying the following sanity check:
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

X, Y = mnist.train.next_batch(50000)
Xs, Ys = mnist.validation.next_batch(5000)

rand = lambda s: np.reshape(np.random.randn(np.prod(s)), s)

class Linear_model(GPflow.model.Model):
    def __init__(self, X, Y, b):
        self.X = GPflow.svgp.MinibatchData(X, b, np.random.RandomState(0))
        self.Y = GPflow.svgp.MinibatchData(Y, b, np.random.RandomState(0))
        self.W = GPflow.param.Param(rand((X.shape[1], Y.shape[1])))
        self.b = GPflow.param.Param(np.zeros(Y.shape[1]))
        GPflow.model.Model.__init__(self)
            
    def build_likelihood(self):
        Y_preds = self.build_predict(self.X)
        return tf.reduce_mean(tf.reduce_sum(self.Y * tf.log(Y_preds), 1))
        
    def build_predict(self, Xnew):
        return tf.nn.softmax(tf.matmul(Xnew, self.W) + self.b)
        
    @GPflow.param.AutoFlow((tf.float64, [None, None]), (tf.float64, [None, None]))
    def accuracy(self, Xnew, Ynew):
        Y_preds = tf.arg_max(self.build_predict(Xnew), 1)
        Y_true = tf.arg_max(Ynew, 1)
        correct = tf.cast(tf.equal(Y_preds, Y_true), tf.float64)
        return tf.reduce_mean(correct)
    
batch_size = 100
m = Linear_model(X, Y, b=batch_size)
m.optimize(tf.train.AdamOptimizer(), maxiter=5000)
print m.accuracy(Xs, Ys) # should get around 88%, but gets more like 58%

m_100 = Linear_model(X[:batch_size, :], Y[:batch_size, :], b=100)
m_100.optimize(tf.train.AdamOptimizer(), maxiter=5000)
print m_100.accuracy(Xs, Ys) # similar result, but with only 100 points
but when I move the dict update inside the loop I get the expected 88% with the full data and 58% with only the first 100 points.
		</comment>
		<comment id='4' author='hughsalimbeni' date='2016-11-25T11:03:20Z'>
		I think &lt;denchmark-link:https://github.com/hughsalimbeni&gt;@hughsalimbeni&lt;/denchmark-link&gt;
 is right. It looks like I might have broken this :(
Could we fix, and get the above sanity check in as a regression test?
		</comment>
		<comment id='5' author='hughsalimbeni' date='2016-11-25T11:07:43Z'>
		&lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
 I will fix this.
		</comment>
		<comment id='6' author='hughsalimbeni' date='2016-11-25T11:26:07Z'>
		It looks like a one line fix. I'm just working on a minimal regression test.
		</comment>
	</comments>
</bug>