<bug id='1578' author='maltamiranomontero' open_date='2020-10-01T18:30:11Z' closed_time='2020-10-08T15:04:56Z'>
	<summary>len() used with inducing variables does not work with tf.function</summary>
	<description>
&lt;denchmark-h:h1&gt;Bug / performance issue / build issue&lt;/denchmark-h&gt;

I create my own inducing variable class which has variable shape, rewriting the len function, but when I optimize the model I got an error calling that "'Tensor' object cannot be interpreted as an integer" on len function. I think is an unfortunate interaction between how len() is handled in python and tf.function.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Minimal, reproducible example
import numpy as np
import gpflow
import tensorflow as tf

class VariableInducingPoints(gpflow.inducing_variables.InducingPoints):
     def __init__(self, Z, name=None):
         super().__init__(Z, name=name)
         # overwrite with Variable with None as first element in shape so
         # we can assign arrays with arbitrary length along this dimension:
         self.Z = tf.Variable(Z, trainable=False, dtype=gpflow.default_float(),
             shape=(None, Z.shape[1])
         )

     def __len__(self):
         return tf.shape(self.Z)[0]  # dynamic shape
         # instead of the static shape returned by the InducingPoints parent class

X, Y = np.random.randn(50, 2), np.random.randn(50, 1)
Z1 = np.random.randn(13, 2)

k = gpflow.kernels.SquaredExponential()
m = gpflow.models.SGPR(data=(X, Y), kernel=k, inducing_variable=VariableInducingPoints(Z1))

Z2 = np.random.randn(29, 2)
m.inducing_variable.Z.assign(Z2)

opt = tf.optimizers.Adam()

@tf.function
def optimization_step():
    opt.minimize(m.training_loss, m.trainable_variables)

for _ in range(iter):
    optimization_step()
Stack trace, or error message
&lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
&lt;ipython-input-24-9a082736eedc&gt; in &lt;module&gt;
     38     
     39     
---&gt; 40     optimization_step()
     41 
     42 

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = "nonXla"
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--&gt; 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    695     self._concrete_stateful_fn = (
--&gt; 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    697             *args, **kwds))
    698 

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-&gt; 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3063     arg_names = base_arg_names + missing_arg_names
   3064     graph_function = ConcreteFunction(
-&gt; 3065         func_graph_module.func_graph_from_py_func(
   3066             self._name,
   3067             self._python_function,

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, "ag_error_metadata"):
--&gt; 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

TypeError: in user code:

    &lt;ipython-input-24-9a082736eedc&gt;:32 optimization_step  *
        optimizer.minimize(m.training_loss, m.trainable_variables)
    /home/maltamirano/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374 minimize  **
        grads_and_vars = self._compute_gradients(
    /home/maltamirano/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:429 _compute_gradients
        loss_value = loss()
    /home/maltamirano/anaconda3/lib/python3.8/site-packages/gpflow/models/training_mixins.py:64 training_loss
        return self._training_loss()
    /home/maltamirano/anaconda3/lib/python3.8/site-packages/gpflow/models/model.py:57 _training_loss
        return -(self.maximum_log_likelihood_objective(*args, **kwargs) + self.log_prior_density())
    /home/maltamirano/anaconda3/lib/python3.8/site-packages/gpflow/models/sgpr.py:154 maximum_log_likelihood_objective
        return self.elbo()
    /home/maltamirano/anaconda3/lib/python3.8/site-packages/gpflow/models/sgpr.py:164 elbo
        num_inducing = len(self.inducing_variable)

    TypeError: 'Tensor' object cannot be interpreted as an integer

&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Optimize without problem
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


GPflow version: 2.1.1
GPflow installed from: pip install gpflow
TensorFlow version: 2.3.1
Python version: 3.8.3
Operating system: Elementary OS 5.1.7 Hera

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Instead of optimizing the model with the inducing variables, I would like to optimize the model without optimizing the inducing variables, manually reassigning the inducing variables at each step of the optimization.
	</description>
	<comments>
		<comment id='1' author='maltamiranomontero' date='2020-10-02T11:36:14Z'>
		Thanks &lt;denchmark-link:https://github.com/maltamiranomontero&gt;@maltamiranomontero&lt;/denchmark-link&gt;
 for the thorough issue report. The underlying issue is that TensorFlow's function tracer in  doesn't know that  will be mapped to , and just gives up.
To support this use-case, we need to change the inducing variable interface to a plain method call (or maybe property). inducing_variable.length()? inducing_variable.num_inducing? Any thoughts?
		</comment>
		<comment id='2' author='maltamiranomontero' date='2020-10-08T10:15:50Z'>
		More minimal example of the len() vs tf.function() issue:
import tensorflow as tf
import numpy as np

class A(tf.Module):
    def __init__(self, a):
        self.a = tf.Variable(a)

    def __len__(self):
        return tf.shape(self.a)[0]

    @property
    def length(self):
        return self.__len__()

a = A(np.zeros((5,3,2)))

def test():
    return len(a)

test_fn = tf.function(test)

def test2():
    return a.length

test2_fn = tf.function(test2)

print(test2())  # works
print(test2_fn())  # works

print(test())  # works
print(test_fn())  # fails
		</comment>
		<comment id='3' author='maltamiranomontero' date='2020-10-08T15:05:22Z'>
		&lt;denchmark-link:https://github.com/maltamiranomontero&gt;@maltamiranomontero&lt;/denchmark-link&gt;
 Fixed! I've updated the answer in &lt;denchmark-link:https://stackoverflow.com/questions/64145766/reshape-of-inducing-variables-gpflow/64146443#64146443&gt;https://stackoverflow.com/questions/64145766/reshape-of-inducing-variables-gpflow/64146443#64146443&lt;/denchmark-link&gt;
 accordingly
		</comment>
	</comments>
</bug>