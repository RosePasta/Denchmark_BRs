<bug id='181' author='alexggmatthews' open_date='2016-08-25T11:05:18Z' closed_time='2016-10-27T15:19:47Z'>
	<summary>GPflow has become considerably slower on GPU with some recent changes.</summary>
	<description>
I have raised this as a bug because as you will see the effect is large.
The main changes I can see are:

The addition of user_ops for VecToTri and TriToVec
The use of batch matrix triangular solve.

The git hashes I'm comparing are:
new with changes &lt;denchmark-link:https://github.com/GPflow/GPflow/commit/23b47f590806f0da3cb71916d68674f8bee4d875&gt;23b47f5&lt;/denchmark-link&gt;

old without changes &lt;denchmark-link:https://github.com/GPflow/GPflow/commit/9c2faf62d04f8ec2ab35396553b5115d92d3c84b&gt;9c2faf6&lt;/denchmark-link&gt;

I'm running MNIST training with a minibatch size of 1000.
GPU operation is 70%  slower between these two hashes.
CPU operation is about the same. Did we observe a speed up when we merged these?
The new profiling code &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/174&gt;#174&lt;/denchmark-link&gt;
 allows us to give a detailed breakdown of the slow down.
&lt;denchmark-link:https://github.com/GPflow/GPflow/files/436776/user_ops_comparison.tar.gz&gt;user_ops_comparison.tar.gz&lt;/denchmark-link&gt;

The can be viewed in chrome using the GUI found at chrome://tracing .
We find that the batch matrix triangular solve does not parallelize as well as the triangular_solve which is arguably a flaw in TensorFlow itself.
We find that the VecToTri and TriToVec ops are currently very slow relatively considering they just reshape something.
What should we do about this?
I would argue we should find out exactly which recent changes are causing this and revert them as soon as possible. The new code looks really great and requires considerably skill but this is showing us that it isn't ready for master yet IMHO. The new user ops also make installation more difficult. We want our users at any moment to have the best available experience and we have to be dedicated to this if we are going to achieve our ambitions for the software.
Longer term I argue that we should move the code to pull requests and expedite &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/174&gt;#174&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/GPflow/GPflow/issues/171&gt;#171&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/GPflow/GPflow/issues/170&gt;#170&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
 thoughts?
	</description>
	<comments>
		<comment id='1' author='alexggmatthews' date='2016-08-25T11:11:00Z'>
		It is surprising that  and  are so slow, and this needs investigating. Is this an orthogonal problem to the GPU ops (&lt;denchmark-link:https://github.com/GPflow/GPflow/issues/171&gt;#171&lt;/denchmark-link&gt;
)? I.e. do the custom ops slow down the CPU execution significantly as well? If not, then focussing on the GPU ops would be the way to go. If so, I agree we should remove the ops from master until the core of the speed issue was resolved.
I agree that &lt;denchmark-link:https://github.com/GPflow/GPflow/issues/170&gt;#170&lt;/denchmark-link&gt;
 is important and would catch these issues.
I don't regard the setup issues as a problem though. The setup is automatically handled by setup.py. We did have a bug with gcc5, but this was resolved quickly and easily.
		</comment>
		<comment id='2' author='alexggmatthews' date='2016-08-25T11:16:20Z'>
		HI Alex,
could you point me to the code that's running  to generat these traces
please?
James.
On Thu, 25 Aug 2016 at 12:11 Mark van der Wilk &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

It is surprising that VecToTri and TriToVec are so slow, and this needs
investigating. Is this an orthogonal problem to the GPU ops (#171
#171)? I.e. do the custom ops
slow down the CPU execution significantly as well?
I agree that #170 #170 is
important and would catch these issues.
—
You are receiving this because you were assigned.
Reply to this email directly, view it on GitHub
#181 (comment), or mute
the thread
https://github.com/notifications/unsubscribe-auth/AAJbDyjn7dSI9avICOhtbEAgR1vl14X7ks5qjXhEgaJpZM4Js8zJ
.

		</comment>
		<comment id='3' author='alexggmatthews' date='2016-08-25T11:16:59Z'>
		Hi &lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
 no it is particularly a problem with GPU it seems. Whilst the CPU time of the VecToTri and TriToVec ops is significant in both cases it only seems to be a bottleneck on GPU for this example. The overall CPU run time is about the same as can be seen on the attached .json traces.
		</comment>
		<comment id='4' author='alexggmatthews' date='2016-08-25T11:18:52Z'>
		&lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
 that will need about 40min or so.
		</comment>
		<comment id='5' author='alexggmatthews' date='2016-08-25T11:19:49Z'>
		Perhaps the best thing to do is for the three of us to meet over skype?
On Thu, 25 Aug 2016 at 12:18 Alexander G. de G. Matthews &lt;
&lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
&gt; wrote:

@jameshensman https://github.com/jameshensman that will need about
40min or so.
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub
#181 (comment), or mute
the thread
https://github.com/notifications/unsubscribe-auth/AAJbD6o_I6gGJNn0VnS8mn7jLlN7k4v2ks5qjXodgaJpZM4Js8zJ
.

		</comment>
		<comment id='6' author='alexggmatthews' date='2016-08-25T11:21:47Z'>
		&lt;denchmark-link:https://github.com/alexggmatthews&gt;@alexggmatthews&lt;/denchmark-link&gt;
 So if adding a GPU version of the op removes the bottle in the GPU case, would this be an acceptable solution to keep the code in master?
Also, it's really easy to not use of the VecToTri and TriToVec ops. All that needs to be done is to remove the transform of q_sqrt.
		</comment>
		<comment id='7' author='alexggmatthews' date='2016-08-25T11:26:21Z'>
		Happy to Skype. Say 2pm?
		</comment>
		<comment id='8' author='alexggmatthews' date='2016-08-25T11:26:56Z'>
		2pm is great.
		</comment>
		<comment id='9' author='alexggmatthews' date='2016-08-25T11:27:35Z'>
		Works for me.
		</comment>
		<comment id='10' author='alexggmatthews' date='2016-08-25T11:31:36Z'>
		&lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;

Re installation via setup.py . I wouldn't describe it as a problem I would describe it as a relative inconvenience for users and developers. If we can find gains in other areas, as I expect we will in the long term then the inconvenience can be justified but can we justify it now?
Re GPU version of OP. If the GPU code gives a speed up then yes this is a good way to go.
In both cases I would argue that master should reflect the best we can offer our users now rather than what we hope to offer them in a week or a month.
		</comment>
		<comment id='11' author='alexggmatthews' date='2016-08-25T11:47:02Z'>
		I agree with your assessment of what the master branch should be about. However, I wonder what is the best thing to do in this specific case.
Regarding the setup procedure: From a developer / user's point of view, python setup.py develop needs to be run when installing GPflow anyway, right? So whether it compiles an extra file or not does not actually change this workflow.
I can see an argument that it's an extra bit of complexity, and a possible source of issues during installation. While it is true that we already did have a bug with compiling with gcc5, we did fix it really quickly. Many Python packages require compilation of C files during installation, including TensorFlow. And if TensorFlow compiles, our compiling command should compile as well, especially since we're just taking the command recommended by the TensorFlow docs.
Essentially, I don't see any gain from going back and removing the custom ops at this stage since:

I don't regard the modifications to setup.py as an issue
It's easy to avoid using the custom ops if we want to avoid the slowdown.
It's just more work, given that this stuff will be merged back in at some point later, once the GPU ops are done.

If you think the slowdown caused by the custom ops is unacceptable at the moment, we remove the transform in the declaration of q_sqrt in SVGP for the time being. This way we do not need to remove chunks of code from master, but we do offer users the fast execution, not held back by the custom ops.
		</comment>
		<comment id='12' author='alexggmatthews' date='2016-08-25T12:07:32Z'>
		&lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
 here is the requested code
There are some dead end changes on these branches so don't merge them. The "old" one is the faster one.
&lt;denchmark-link:https://github.com/GPflow/GPflow/tree/old_master_profile_mnist&gt;https://github.com/GPflow/GPflow/tree/old_master_profile_mnist&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/GPflow/GPflow/tree/master_profile_mnist&gt;https://github.com/GPflow/GPflow/tree/master_profile_mnist&lt;/denchmark-link&gt;

I'm using a Titan X GPU. Let me know if you have any problems.
		</comment>
		<comment id='13' author='alexggmatthews' date='2016-08-25T12:33:46Z'>
		&lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
 we need to remember we haven't broken down the slow down in to parts yet. Although it looks likely the user ops are a contributing factor we should do a test before and after the relevant pull request to confirm. We should also look into the other changes.
I'm afraid I do regard the current slow down, whatever the source, as something we can't allow on master. I think the GPU speed up is essential to our USP. I'm sorry if the solution turns out to be disruptive.
It was originally possible to use GPflow without running setups tools simply by adding it to your path but let's not get bogged down on that issue.
I'm not keen on having code that isn't being used on master but I'm sure we can work out a sensible compromise plan that isn't too labour intensive in the meeting.
		</comment>
		<comment id='14' author='alexggmatthews' date='2016-08-25T12:48:18Z'>
		&lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
 I've booked the Skype room in Baker building at 2pm.
		</comment>
		<comment id='15' author='alexggmatthews' date='2016-08-25T13:00:29Z'>
		Essentially, what I'm proposing, is to change svgp.py:105 from:
self.q_sqrt = Param(q_sqrt, transforms.LowerTriangular(q_sqrt.shape[2]))
to
self.q_sqrt = Param(q_sqrt)
While keeping the custom ops and the LowerTriangular transform in the codebase.
This would remove the speed penalty from the custom ops, while allowing users who do want the non-overparameterised version to use the transform.
		</comment>
		<comment id='16' author='alexggmatthews' date='2016-08-25T15:46:04Z'>
		Some improvement in &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/182&gt;#182&lt;/denchmark-link&gt;
 which has additional profiler info but not completely fixed yet.
		</comment>
		<comment id='17' author='alexggmatthews' date='2016-08-25T15:48:06Z'>
		What's the remaining source of slowdown? Is this caused by using the batch operations?
		</comment>
		<comment id='18' author='alexggmatthews' date='2016-08-25T15:51:01Z'>
		&lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
 I'm looking at &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/159&gt;#159&lt;/denchmark-link&gt;
 now.  I
		</comment>
		<comment id='19' author='alexggmatthews' date='2016-08-25T16:15:53Z'>
		&lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
 a revert of &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/159&gt;#159&lt;/denchmark-link&gt;
 together with &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/182&gt;#182&lt;/denchmark-link&gt;
 restores the speed I saw in my original experiments.
I understand the code should run better using the batch ops. This seems like a flaw with TensorFlow or something subtle with how we are using it.
The bottom line is the GPU code for this experiment on master is currently 45% slower than it was without the batch ops.
Code, which is not to be merged, here &lt;denchmark-link:https://github.com/GPflow/GPflow/tree/master_no_batch&gt;https://github.com/GPflow/GPflow/tree/master_no_batch&lt;/denchmark-link&gt;

As in the previous case I vote revert then re-introduce as a PR whilst we work out what is going wrong. What are your thoughts?
		</comment>
		<comment id='20' author='alexggmatthews' date='2016-08-25T16:17:07Z'>
		&lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
 also happy to hear any views although thought you may have had enough of this for one day.
		</comment>
		<comment id='21' author='alexggmatthews' date='2016-08-25T16:26:13Z'>
		Obviously this is all pretty annoying. Particularly for those who wrote the code. tbh I think as we increase profiler coverage there may will be more surprises like this in the pipeline. ie nice bits of code that turn out to work slower than less nice ones. Sigh.
		</comment>
		<comment id='22' author='alexggmatthews' date='2016-08-26T16:43:49Z'>
		&lt;denchmark-link:https://github.com/jameshensman&gt;@jameshensman&lt;/denchmark-link&gt;
 a hunch is that we might be able to restore most of the speed by removing calls to batch_mat_mul . This is the op that doesn't seem to have a GPU implementation when mat_mul does. If that is the case it could restore most of the speed with relatively little of the uglier code.
		</comment>
		<comment id='23' author='alexggmatthews' date='2016-08-30T11:43:54Z'>
		batch_matul apparently  work on gpu:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/605&gt;tensorflow/tensorflow#605&lt;/denchmark-link&gt;

but it doesn't get allocated there on my machine (using the latest google-pip)
		</comment>
		<comment id='24' author='alexggmatthews' date='2016-09-16T11:47:26Z'>
		Having spoken to someone from Google yesterday I know they are planning better integration of batch_mat_mul and mat_mul soon. I suspect this may improve the speed as well.
		</comment>
		<comment id='25' author='alexggmatthews' date='2016-10-27T15:19:47Z'>
		This issue is now out of date. Having put a lot of work into it as a team we are currently very happy with the GPU performance.
		</comment>
		<comment id='26' author='alexggmatthews' date='2017-01-31T14:40:11Z'>
		Hi everyone!
This problem seems to have resurfaced with tensorflow 0.12.1.
Using this code:
import GPflow
import tensorflow as tf
import numpy as np

import pods
pods.datasets.overide_manual_authorize = True

data = pods.datasets.oil_100()
Y = data['X']

Q = 2
M = 20 
N = Y.shape[0]
X_mean = GPflow.gplvm.PCA_reduce(Y, Q)
Z = np.random.permutation(X_mean.copy())[:M]

k = GPflow.ekernels.RBF(2, ARD=True)

m = GPflow.gplvm.BayesianGPLVM(
    X_mean=X_mean,
    X_var=0.1*np.ones((N, Q)),
    Y=Y,
    kern=k,
    M=M,
    Z=Z
)

m.likelihood.variance = 0.01
%time m.optimize(method='L-BFGS-B', disp=True, maxiter=1000)
On the same machine, the optimization takes about 1 Minute using only CPUs and about 10 Minutes when adding one GPU.
I tried something similar for non-LVMs:
import GPflow
import tensorflow as tf
import numpy as np

X = np.mgrid[0:100:10000j][:, None]
Y = np.sin(X) + 0.1 * np.random.randn(*X.shape)

num_data = X.shape[0]
num_inducing = 200

Z = X[np.random.permutation(num_data)[:num_inducing]].copy()

m = GPflow.sgpr.SGPR(X, Y, GPflow.ekernels.RBF(X.shape[1], ARD=True), Z)

%time m.optimize(method='L-BFGS-B', maxiter=1000)
And here, optimization using GPUs was considerably faster by about a factor of 10.
		</comment>
		<comment id='27' author='alexggmatthews' date='2017-01-31T14:44:56Z'>
		The issue above was concerned about a regression of speed within GPflow (i.e. it was fast on the GPU and it became slower - obviously this is fixed now).
The LVM code is relatively new, and hasn't really been extensively tested on GPUs, over that it runs. So the issue is more that it's slower on a GPU than a CPU, right? I think it's best to open a new issue for this, since this is unrelated to the old issue.
I recently re-thought some of the code in ekernels.py, and there is a huge opportunity for easy optimisation.
		</comment>
		<comment id='28' author='alexggmatthews' date='2017-01-31T14:46:32Z'>
		Okay, opening a new issue!
		</comment>
	</comments>
</bug>