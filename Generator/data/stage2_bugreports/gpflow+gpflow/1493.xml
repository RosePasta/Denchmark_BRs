<bug id='1493' author='SuperKam91' open_date='2020-06-03T08:47:05Z' closed_time='2020-06-03T10:13:59Z'>
	<summary>Strange `pickle`/`gpflow.utilities.freeze` behaviour with `SVGP` models</summary>
	<description>
&lt;denchmark-h:h1&gt;Bug / performance issue / build issue&lt;/denchmark-h&gt;

Hi there,
I have been trying to (crudely) train and save an  model on a toy dataset largely following the notebook example found at:
&lt;denchmark-link:https://gpflow.readthedocs.io/en/develop/notebooks/advanced/natural_gradients.html?highlight=Natural%20Gradient&gt;https://gpflow.readthedocs.io/en/develop/notebooks/advanced/natural_gradients.html?highlight=Natural%20Gradient&lt;/denchmark-link&gt;

Upon saving the model using pickle (I appreciate this is not recommended but I don't believe this is the main issue here), I discovered some unusual and what I presume is unintended behaviour:
If we do not call gpflow.utilities.freeze(model), before trying to pickle model, then we get an error.
If we do induce gpflow.utilities.freeze(model), then model can be pickled without error.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Minimal, reproducible example
import numpy as np
import gpflow
import tensorflow as tf
from gpflow.ci_utils import ci_niter
import pickle
rng = np.random.RandomState(123)
tf.random.set_seed(42)

def func(x):
    return np.sin(x * 3 * 3.14) + 0.3 * np.cos(x * 9 * 3.14) + 0.5 * np.sin(x * 7 * 3.14)

N = 10000  # Number of training observations
sigma = 0.2
X = rng.rand(N, 1) * 2 - 1  # X values
Y = func(X) + sigma * rng.randn(N, 1)  # Noisy Y values
data = (X, Y)

n_inducing_vars = 100
Z = X[:n_inducing_vars]
minibatch_size = 100
n_iterations = 100
#Define model object
model = gpflow.models.SVGP(gpflow.kernels.Matern12(), gpflow.likelihoods.Bernoulli(), inducing_variable=Z, num_data=N)
#Create minibatch object
data_minibatch = (
tf.data.Dataset.from_tensor_slices(data).prefetch(
    N).repeat().shuffle(N).batch(minibatch_size)
    )
data_minibatch_it = iter(data_minibatch)
model_objective = model.training_loss_closure(data_minibatch_it)
#Define optimiser
optimizer = tf.keras.optimizers.Adam(0.001)
#Optimise both variational parameters and kernel hyperparameters.
for step in range(ci_niter(n_iterations)):
    optimizer.minimize(model_objective,
                       var_list=model.trainable_variables
                       )
freeze = False
if not freeze:
    # pickle doesn't work
    pickle.dump(model, open('test1', 'wb'))
else:
    # if following code is executed, pickle works fine
    gpflow.utilities.freeze(model)
    pickle.dump(model, open('test1', 'wb'))
Stack trace, or error message
&lt;denchmark-code&gt;TypeError                                 Traceback (most recent call last)
&lt;ipython-input-6-3d5f537ca994&gt; in &lt;module&gt;
----&gt; 1 pickle.dump(model, open('test1', 'wb'))

TypeError: can't pickle HashableWeakRef objects
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behaviour&lt;/denchmark-h&gt;

Not saying that I expected the pickle to work in the first instance, as I know it isn't the recommended way of saving tensorflow-related objects in general. However, I certainly wouldn't expect it to fail in the first instance but succeed in the second. From looking at the codebase, I don't believe gpflow.utilities.freeze(model) should be mutating model, which it seems to be doing.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


GPflow version: 2.0.0 
GPflow installed from: pip install gpflow==2.0.0 
TensorFlow version: 2.1.0 
Python version: Python 3.6.9 
Operating system: macOS Catalina 10.15.4

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I tested this using gpflow==2.0.4 as I saw the code for freeze changed, but the bug seems to persist.
I would guess that in calling  on  it is inexplicably actually converting  into a "frozen" model, which then has the "constant" properties (&lt;denchmark-link:https://gpflow.readthedocs.io/en/master/notebooks/intro_to_gpflow2.html#TensorFlow-saved_model&gt;https://gpflow.readthedocs.io/en/master/notebooks/intro_to_gpflow2.html#TensorFlow-saved_model&lt;/denchmark-link&gt;
) that enable it to be pickled.
Other version numbers:
&lt;denchmark-code&gt;In [7]: dill.__version__
Out[7]: '0.2.8.2'

In [9]: tensorflow_probability.__version__
Out[9]: '0.9.0'

In [10]: keras.__version__
Out[10]: '2.3.1'

In [12]: pickle.format_version
Out[12]: '4.0'
&lt;/denchmark-code&gt;

Any clarity on this matter would be very much appreciated. Please note I haven't tested this on any models other than SVGP (but the mwe can easily be adapted to do so).
Cheers!
	</description>
	<comments>
		<comment id='1' author='SuperKam91' date='2020-06-03T10:12:32Z'>
		Thanks for the report. This is a very well known issue that has roots in TFP bijectors' caching mechanism. It will be fixed soon in TFP. Unfortunately, until that moment we will not be able to work around this problem in GPflow.
You can track progress here: &lt;denchmark-link:https://github.com/tensorflow/probability/pull/947&gt;tensorflow/probability#947&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='SuperKam91' date='2020-06-03T10:23:08Z'>
		Just to add, my favourite way of storing models is using  to get a dict of the parameters. Then I store that using &lt;denchmark-link:https://github.com/mverleg/pyjson_tricks&gt;json_tricks&lt;/denchmark-link&gt;
 as a json file. You need the extra library to deal with json-ing numpy arrays.
On loading, I re-create the object and assign the parameters using gpflow.utilities.multiple_assign().
		</comment>
		<comment id='3' author='SuperKam91' date='2020-06-03T11:14:34Z'>
		&lt;denchmark-link:https://github.com/SuperKam91&gt;@SuperKam91&lt;/denchmark-link&gt;
 it'd probably be more helpful to other users of GPflow if you re-post this as a question on StackOverflow (using the &lt;denchmark-link:https://stackoverflow.com/questions/tagged/gpflow&gt;[gpflow]&lt;/denchmark-link&gt;
 tag) - and I'd then happily repost the following explanation (so it's more easily discoverable than a closed issue):
The underlying explanation is that tensorflow_probability's bijectors keep a cache of tensors they have operated on, which for example allows them to exactly recover the original tensor in the following example:
import tensorflow as tf
import tensorflow_probability as tfp
bij = tfp.bijectors.Exp()
x = tf.constant(1.2345)
y = bij.forward(x)
assert bij.inverse(y) is x  # actual object identity, not just numerical equivalence
These caches, however, use the HashableWeakRef objects that can't be pickled - or even copied (using Python's copy.deepcopy function from the stdlib).
They only get created when you actually run tensors through the bijector - if you just create the model and don't optimise it, you can pickle (or copy) it just fine!
But of course that's not very useful. To work around this issue and allow copying even of "used" (e.g. trained) models, we have gpflow.utilities.reset_cache_bijectors(). This is called by gpflow.utilities.deepcopy() to allow copying. And gpflow.utilities.freeze() in turn needs to deepcopy so as to give you a frozen copy, instead of freezing the model in-place.
You can successfully pickle with freeze=False if you add the call to reset_cache_bijectors(model) before the pickle.dump!
if not freeze:
    gpflow.utilities.reset_cache_bijectors(model)  # with this added call, pickle *does* work
    pickle.dump(model, open('test1', 'wb'))
As &lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
 pointed out, this is ultimately an issue that tensorflow_probability would have to fix "properly" upstream in their own code.
		</comment>
		<comment id='4' author='SuperKam91' date='2020-06-03T14:30:44Z'>
		Hi &lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/markvdw&gt;@markvdw&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/st--&gt;@st--&lt;/denchmark-link&gt;
 ,
Thank you very much for your swift replies, very much appreciated.
I have created a Stackoverflow question pointing out this issue, so it can be read by the wider community:
&lt;denchmark-link:https://stackoverflow.com/questions/62174955/strange-pickle-gpflow-utilities-freeze-behaviour-with-gpflow-svgp-models&gt;https://stackoverflow.com/questions/62174955/strange-pickle-gpflow-utilities-freeze-behaviour-with-gpflow-svgp-models&lt;/denchmark-link&gt;

Thanks for the tip on storing models, I will likely use this approach in the future- here I was just experimenting with different methods (aware that pickle isn't the wisest option) and happened to stumble upon this issue in the process.
Your explanation to why model is being mutated, its relation to the bijector caching, and the proposed (temporary) solution was very thorough so thank you for that.
With regards to the TFP bijector issue itself. if I understand correctly, once this issue is solved in TFP, the weakref objects associated with the caches will be deep copyable, so that bijectors containing caches can be pickled/deepcopied?
Consequently, referring back to my example, a gpflow model could be pickled without having to call gpflow.utilities.reset_cache_bijectors (as in the proposed temporary solution)?
Finally this means that this operation doesn't need to be performed in gpflow.utilities.deepcopy either, meaning that gpflow.freeze will no longer mutate model in the original example.
Am I understanding these three parts correctly?
Cheers again,
Kamran
		</comment>
	</comments>
</bug>