<bug id='677' author='hughsalimbeni' open_date='2018-03-10T15:49:40Z' closed_time='2018-04-12T10:10:02Z'>
	<summary>broken with tensorflow 1.6</summary>
	<description>
tensorflow 1.6 does not currently work (fails with Adam), but the docs say 1.4+ is supported. This can cause problems for users (e.g. &lt;denchmark-link:https://github.com/ICL-SML/Doubly-Stochastic-DGP/issues/10&gt;ICL-SML/Doubly-Stochastic-DGP#10&lt;/denchmark-link&gt;
).
	</description>
	<comments>
		<comment id='1' author='hughsalimbeni' date='2018-03-16T10:49:02Z'>
		The issue has been discussed in the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/8057#issuecomment-284000067&gt;tensorflow repo&lt;/denchmark-link&gt;
. Without digging into the details, I tried the workaround proposed there which seems to do the trick. I would not call it a solution however.
diff --git a/gpflow/training/tensorflow_optimizer.py b/gpflow/training/tensorflow_optimizer.py
index 2ecef24..9fd1af6 100644
--- a/gpflow/training/tensorflow_optimizer.py
+++ b/gpflow/training/tensorflow_optimizer.py
@@ -87,6 +87,8 @@ class _TensorFlowOptimizer(optimizer.Optimizer):
                         yield slot
         extra_vars = [v for v in self.optimizer.__dict__.values() if isinstance(v, tf.Variable)]
         optimizer_vars = list(get_optimizer_slots())
+        if isinstance(self.optimizer, tf.train.AdamOptimizer):
+            optimizer_vars.extend(self.optimizer._get_beta_accumulators())
         full_var_list = list(set(optimizer_vars + extra_vars))
         misc.initialize_variables(full_var_list, session=session, force=False)
		</comment>
		<comment id='2' author='hughsalimbeni' date='2018-04-03T13:59:24Z'>
		I ran into the same problem when just now tried to run the example in this &lt;denchmark-link:http://gpflow.readthedocs.io/en/latest/notebooks/mcmc.html&gt;tutorial&lt;/denchmark-link&gt;
. The workaround solution &lt;denchmark-link:https://github.com/mrksr&gt;@mrksr&lt;/denchmark-link&gt;
 gave works for me.
		</comment>
		<comment id='3' author='hughsalimbeni' date='2018-04-03T14:08:09Z'>
		What about using ,  which was &lt;denchmark-link:https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/train/AdamOptimizer&gt;introduced in Tensorflow 1.5&lt;/denchmark-link&gt;
? Instead of .
Maybe something like:
try:
    full_var_list = optimizer.variables()
except AttributeError:
    extra_vars = [v for v in self.optimizer.__dict__.values() if isinstance(v, tf.Variable)]
    optimizer_vars = list(get_optimizer_slots())
    full_var_list = list(set(optimizer_vars + extra_vars))
		</comment>
		<comment id='4' author='hughsalimbeni' date='2018-04-03T17:55:04Z'>
		&lt;denchmark-link:https://github.com/rhaps0dy&gt;@rhaps0dy&lt;/denchmark-link&gt;
 that seems like a good way to go. I think this should go in asap as it's a bit frustrating/off-putting for new users if it doesn't work with a fresh tensorflow. Any objections &lt;denchmark-link:https://github.com/awav&gt;@awav&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='5' author='hughsalimbeni' date='2018-04-08T19:57:29Z'>
		&lt;denchmark-link:https://github.com/rhaps0dy&gt;@rhaps0dy&lt;/denchmark-link&gt;
's code still gave me the error. but &lt;denchmark-link:https://github.com/mrksr&gt;@mrksr&lt;/denchmark-link&gt;
 code worked!
		</comment>
		<comment id='6' author='hughsalimbeni' date='2018-04-12T10:10:02Z'>
		Solved &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/714&gt;#714&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>