<bug id='415' author='markvdw' open_date='2017-05-10T18:51:41Z' closed_time='2017-07-12T10:26:51Z'>
	<summary>Tests failing on GPU with tf1.1</summary>
	<description>
Does anybody else have an issue on the GPU with tf1.1? I've got this infuriating issue where the tests fail on the GPU. No error messages, just failed tests. I reinstalled CUDA (several times). TensorFlow package came straight from pip.
I traced down the issue to V_chol in WhitenTestGaussian.test_whiten being an identity matrix, when I don't think it should be.
I'd like to know if others are experiencing the same issue before I dig into this further. It would be a pretty serious TensorFlow bug if it passed through their tests unnoticed...
CUDA 8.0, cuDNN 5.1, issue occurs on all GPUs (1080, Titan X, K80)
	</description>
	<comments>
		<comment id='1' author='markvdw' date='2017-05-10T18:56:36Z'>
		What GPU are you using?  What cuda and what cudnn are you using?
		</comment>
		<comment id='2' author='markvdw' date='2017-05-10T18:58:06Z'>
		Issue updated.
		</comment>
		<comment id='3' author='markvdw' date='2017-05-11T08:18:50Z'>
		I have just run the testsuite of the current GPflow master in the current nvidia/cuda:cudnn docker image with a freshly installed Anaconda 4.3.1 and the tensorflow 1.1.0 cpu and gpu packages installed via pip. My setup is CUDA 8.0, cuDNN 5.1 and two Quadro M6000 24GB graphics cards (restricting to one does not seem to make a difference).
I can confirm that something fishy is going on. If I do not use the GPU by running CUDA_VISIBLE_DEVICES= nosetests testing/test_conditionals.py, the tests seem to pass reliably. If I just run nosetests testing/test_conditionals.py (and tensorflow uses the GPUs as can be seen in the logs), I have seen the tests pass and fail. If they fail, the result is something like this:
&lt;denchmark-code&gt;(...)
2017-05-11 08:15:40.092350: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU c
omputations.
2017-05-11 08:15:40.092371: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU c
omputations.
2017-05-11 08:15:40.092376: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU comp
utations.
2017-05-11 08:15:40.092380: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU com
putations.
2017-05-11 08:15:40.092383: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU comp
utations.
2017-05-11 08:15:40.231967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:
name: Quadro M6000 24GB
major: 5 minor: 2 memoryClockRate (GHz) 1.114
pciBusID 0000:03:00.0
Total memory: 23.90GiB
Free memory: 4.54GiB
2017-05-11 08:15:40.231993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0
2017-05-11 08:15:40.231998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y
2017-05-11 08:15:40.232007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
2017-05-11 08:15:40.549661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
.2017-05-11 08:15:40.757524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
2017-05-11 08:15:40.785963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
.2017-05-11 08:15:41.034387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
2017-05-11 08:15:41.066790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
.2017-05-11 08:15:41.294583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
2017-05-11 08:15:41.331850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:03:00.0)
F
======================================================================
FAIL: make sure that predicting using the whitened representation is the
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/bam-job-inputs/GPflow/testing/test_conditionals.py", line 151, in test_whiten
    self.assertTrue(np.all(np.abs(var_difference) &lt; 1e-4))
AssertionError: False is not true

----------------------------------------------------------------------
Ran 4 tests in 1.464s

FAILED (failures=1)
&lt;/denchmark-code&gt;

The same happens in testing/test_variational.py.
		</comment>
		<comment id='4' author='markvdw' date='2017-05-11T08:30:04Z'>
		Same here!!! It consistently fails for me. I lost about a day trying to figure out what was going on, I thought I had compiled tensorflow incorrectly or something...
Regardless, I'd suggest everyone hold off from v1.1 for the time being until we figure out what's going on. I suspect tf.matrix_triangular_solve() is giving incorrect results on the GPU. If anybody can confirm, that would be great.
		</comment>
		<comment id='5' author='markvdw' date='2017-05-11T08:46:55Z'>
		If I install TensorFlow v1.0.1 from pip, the tests pass on GPU.
		</comment>
		<comment id='6' author='markvdw' date='2017-05-11T08:51:55Z'>
		Agreed. That has been my solution.
		</comment>
		<comment id='7' author='markvdw' date='2017-05-11T10:14:15Z'>
		Yikes. Minimum breaking example and then if necessary report to TensorFlow main I guess. I'm guessing quite a few of us are frantically writing papers for NIPS?
		</comment>
		<comment id='8' author='markvdw' date='2017-05-11T10:25:16Z'>
		More likely to be an interface change than an actually bug in TF I would have thought.
		</comment>
		<comment id='9' author='markvdw' date='2017-05-11T11:33:28Z'>
		It's weird that the tests pass on the CPU though. The golden rule ought to be that the results are the same on CPU and GPU...
But yes re NIPS (good luck to all :)), so I won't look at it before the deadline.
		</comment>
		<comment id='10' author='markvdw' date='2017-05-23T15:03:35Z'>
		I've also noticed some very strange behaviour. I just pulled the latest master of GPflow and using tf 1.1.0 nothing seemed to work as expected. Eventually I tried this:
import numpy as np
from GPflow.likelihoods import Gaussian
from GPflow.kernels import RBF
from GPflow.sgpr import SGPR

X = np.array(((0., 1.))).reshape([2, 1])

model = SGPR(X, X.copy(), RBF(1), X.copy())
for _ in range(10):
    print model.compute_log_likelihood()
and got the following
&lt;denchmark-code&gt;[-2.84629964]
[-2.75810727]
[-2.75810727]
[-2.84629964]
[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.84629964]
[-2.84629964]
[-2.75810727]
&lt;/denchmark-code&gt;

which is obviously not correct
When I try the CPU I get:
&lt;denchmark-code&gt;[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.75810727]
[-2.75810727]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='markvdw' date='2017-05-23T15:26:21Z'>
		Wow, so it's non-deterministic as well?
		</comment>
		<comment id='12' author='markvdw' date='2017-05-23T15:33:11Z'>
		Whatever it it, is not helping my work right now! Just to confirm: I get the right answer on a GPU running tf 1.0.1
Can you reproduce the above? I'm on a K80
		</comment>
		<comment id='13' author='markvdw' date='2017-05-23T15:54:22Z'>
		Whoops, clicked the wrong button...
But what ever this is it is infuriating!
Also I've just tried downgrading to 1.0 and confirm that everything works as expected.
Another example that breaks with tf 1.1 but works with tf 1.0:
from scipy.cluster.vq import kmeans2
from get_data import get_mnist_data

X, Y, Xs, Ys = get_mnist_data()

M = 100
Z = kmeans2(X, M, minit='points')[0]

m_sgp = SVGP(X, Y, RBF(784, lengthscales=2, variance=2), 
             MultiClass(10), Z, 
             num_latent=10, minibatch_size=100)

m_sgp.optimize(tf.train.AdamOptimizer(0.01), maxiter=1)
preds = np.argmax(m_sgp.predict_y(Xs)[0], 1)
print np.average(np.array(pred==Ys, dtype=float))
This gets about 0.7 on 1.0 with or without GPU, but 0.1 on tf1.0 GPU
		</comment>
		<comment id='14' author='markvdw' date='2017-05-23T16:28:30Z'>
		This looks like exactly the same issue that I encountered, it is totally infuriating. Like I said above, I suspect it's an issue with tf.matrix_triangular_solve(), at least.
Do you need tf1.1 for anything? For now, I'm just using 1.0.1.
		</comment>
		<comment id='15' author='markvdw' date='2017-05-23T16:36:57Z'>
		I get
&lt;denchmark-code&gt;[-2.84629964]
[-2.84629964]
[-2.84629964]
[-2.84629964]
[-2.84629964]
[-2.84629964]
[-2.84629964]
[-2.84629964]
[-2.84629964]
[-2.84629964]
&lt;/denchmark-code&gt;

on tf1.1 on GPU, but the same as you for CPU.
		</comment>
		<comment id='16' author='markvdw' date='2017-05-23T16:44:44Z'>
		That's odd. Do you ever get the right answer (-2.75810727) if you run it for longer? I just tried with 3 points and I get a random choice between two different values ([-5.04162976] and [-4.76064517] if you add 2 to the inputs above)
		</comment>
		<comment id='17' author='markvdw' date='2017-05-23T16:55:48Z'>
		Ok, things have moved on in 2 weeks. These two issues may be related: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/9139&gt;tensorflow/tensorflow#9139&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/serving/issues/335&gt;tensorflow/serving#335&lt;/denchmark-link&gt;
. But, it looks like the problem is fixed in TensorFlow 1.2. If I install rc0, the GPflow tests pass.
		</comment>
		<comment id='18' author='markvdw' date='2017-07-12T10:26:51Z'>
		I'm closing this, since it has been fixed by using tf1.2.
		</comment>
	</comments>
</bug>