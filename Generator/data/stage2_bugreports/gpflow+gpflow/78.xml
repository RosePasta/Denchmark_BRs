<bug id='78' author='NMRobert' open_date='2016-06-01T16:17:01Z' closed_time='2016-11-08T13:40:50Z'>
	<summary>Non-invertible covariance matrix during regression optimization</summary>
	<description>
When I attempt to perform GP regression on a dataset where dim(X)=17 and dim(y)=1, I get an exception as follows after about 40 iterations of the L-BFGS-B solver:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 655, in _do_call
    return fn(*args)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 637, in _run_fn
    status, run_metadata)
  File "/usr/lib/python3.5/contextlib.py", line 66, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors.py", line 450, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.InvalidArgumentError: Input matrix is not invertible.
     [[Node: MatrixTriangularSolve = MatrixTriangularSolve[T=DT_DOUBLE, adjoint=false, lower=true, _device="/job:localhost/replica:0/task:0/cpu:0"](Cholesky, sub_1)]]
&lt;/denchmark-code&gt;

However, GPy can compute this no problem (identical data, identical kernel setup), resulting in this:
&lt;denchmark-code&gt;Name : GP regression
Objective : -369.9790741292725
Number of Parameters : 11
Number of Optimization Parameters : 11
Updates : True
Parameters:
  GP_regression.                |              value  |  constraints  |  priors
  sum.std_periodic.variance     |    0.0202908485016  |      +ve      |        
  sum.std_periodic.period       |      961.078688147  |      +ve      |        
  sum.std_periodic.lengthscale  |      624.779412263  |      +ve      |        
  sum.rbf.variance              |  0.000746482801573  |      +ve      |        
  sum.rbf.lengthscale           |      2.94584474279  |      +ve      |        
  sum.mul.linear.variances      |     0.157037643245  |      +ve      |        
  sum.mul.Mat52.variance        |     0.157037643064  |      +ve      |        
  sum.mul.Mat52.lengthscale     |               (3,)  |      +ve      |        
  Gaussian_noise.variance       |    0.0137070123953  |      +ve      |  
&lt;/denchmark-code&gt;

It's not clear what is causing this to happen - the problem setup is essentially identical. It's frustrating because the regression itself is extremely good - when it computes. Am I missing something obvious here? Cheers.
	</description>
	<comments>
		<comment id='1' author='NMRobert' date='2016-06-01T16:32:33Z'>
		hi,
We are thinking about the best way to solve this type of issue - see &lt;denchmark-link:https://github.com/GPflow/GPflow/pull/72&gt;#72&lt;/denchmark-link&gt;
.
Could you upload a minimal working example that reproduces this behaviour?
There are a couple ways to work around this problem:

adding a small jitter to the diagonal. Add to your kernel a White kernel with a fixed but small variance.
add a logistic transform to constrain the hyperparameters that cause the problem - for example to constraint a parameter to lie in the interval 7.3 to 19.4:
GPflow.transforms.Logistic(7.3, 19.4)

I can add more detail for these solutions if it would be useful.
		</comment>
		<comment id='2' author='NMRobert' date='2016-06-01T17:19:09Z'>
		Hi, thanks for the really fast response! :) I will prepare a minimal example for you asap. In the meantime, I am seeing some progress with fixing some of the hyperparameters - upon further inspection, it appears that the optimization is resulting in some of them being invalid (sometimes NaN, or +Inf). Example:
model.mean_function.c transform:(none) prior:None[ 0.15573284] model.kern.periodickernel.variance transform:+ve prior:None[ 0.] model.kern.periodickernel.lengthscales transform:+ve prior:None[ inf] model.kern.periodickernel.period transform:+ve prior:None[ 86.17313192] model.kern.white.variance transform:+ve prior:None [FIXED][ 0.001] model.kern.prod.linear.variance transform:+ve prior:None[ 0.14613454] model.kern.prod.matern52.variance transform:+ve prior:None[ 0.14613454] model.kern.prod.matern52.lengthscales transform:+ve prior:None[ inf  inf  inf] model.kern.rbf.lengthscales transform:+ve prior:None[ 1.64751096] model.kern.rbf.variance transform:+ve prior:None[ 0.00405476] model.likelihood.variance transform:+ve prior:None[ 0.01931217]
How do I use the logistic transform to constrain a parameter into a range in terms of the syntax - for example, if I wanted to constrain model.kern.periodickernel.lengthscales to non-infinite values? Is there a way I can just constrain every param into a finite range?
		</comment>
		<comment id='3' author='NMRobert' date='2016-06-01T21:45:10Z'>
		Hi,
To add the constraint to your model for a single parameter do
m.kern.matern32.lengthscales.transform = GPflow.transforms.Logistic(0.1, 10.)
This wil constraint the length scale for the Matern to lie in the [0.1,10] interval.
We don't have a mechanism currently to constrain all the parameters. You have to add the constraint to each parameter separately.
We are actively considering how to improve this area so any suggestions and a minimum working example will be very appreciated.
		</comment>
		<comment id='4' author='NMRobert' date='2016-06-02T10:41:18Z'>
		Thank you, I have been able to get the results I wanted by constraining the hyper-parameters. I have created a gist with some data and the python to create the error. I apologise if the demo is still somewhat convoluted/contrived, hopefully it helps in the debugging anyways. Note - this particular combination of kernels is not (necessarily) the issue, it was occurring with simpler combinations too, I just couldn't manage to reproduce any of those situations.  Currently using the newest master version of GPflow and TF.
&lt;denchmark-link:https://gist.github.com/NMRobert/8ea8b621fa12042b1dd78dfb9ca31371&gt;https://gist.github.com/NMRobert/8ea8b621fa12042b1dd78dfb9ca31371&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='NMRobert' date='2016-06-02T11:54:13Z'>
		Thanks &lt;denchmark-link:https://github.com/NMRobert&gt;@NMRobert&lt;/denchmark-link&gt;
 , I can reproduce the issue. We have a few different possible solutions being discussed that should help.
		</comment>
		<comment id='6' author='NMRobert' date='2016-11-07T18:08:14Z'>
		&lt;denchmark-link:https://github.com/NMRobert&gt;@NMRobert&lt;/denchmark-link&gt;
 we've made quite a few changes since this bug was reported. Is this still an issue with the latest version of GPflow? If so it would be great to have an example we can reproduce as a test case. Thanks very much.
		</comment>
		<comment id='7' author='NMRobert' date='2016-11-08T13:40:50Z'>
		Hi Alex,
I've tested on the most recent version of GPflow and the bug does not appear. I was unable to reproduce the issue at all - great work. In addition to this, I noticed that the computation seemed much faster than I remember it being previously. Great work to everybody involved!
		</comment>
		<comment id='8' author='NMRobert' date='2017-11-13T13:00:23Z'>
		I have this issue. I'll post a MVCE. Given that this is closed, I'll open a new issue.
		</comment>
		<comment id='9' author='NMRobert' date='2017-11-13T13:56:59Z'>
		&lt;denchmark-h:h2&gt;MVCE&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/Joshuaalbert/e34f71c65b28263161ae09f5b9a6f3be&gt;https://gist.github.com/Joshuaalbert/e34f71c65b28263161ae09f5b9a6f3be&lt;/denchmark-link&gt;

The data provides an easy to solve problem, yet TF cholesky often fails.
I know that this timeseries has a lengthscale around 200 which is where it is initialized.
I also added an untrainable white kernel of very high variance, and it still fails.
It also fails with and without a mean function.
It also fails with and without the logstic tranform.
&lt;denchmark-h:h2&gt;Back to the problem&lt;/denchmark-h&gt;

There is a serious problem here guys and gals.
Because of this same issue in my own code, for the exact same problem actually, I've asked that TF provide dynamic exception catching (&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/10332&gt;tensorflow/tensorflow#10332&lt;/denchmark-link&gt;
).
In a pythonic solution one would do:
&lt;denchmark-code&gt;try:
    #method using Cholesky
except:
    #method using svd and pseudo inv/det
&lt;/denchmark-code&gt;

This is how I solved this problem in my numpy code dealing with GPs.
The problem is that TF requires doing something like below which can be tedious to code in an elegant way.
&lt;denchmark-code&gt;def _no_cho_version(...):
    #using svd
def _cho_version(...):
   #using cho as preferable
tf.cond(did_cholesky_fail, _no_cho_version, _cho_version)

try:
    feed_dict[did_cholesky_fail] = False
    sess.run(..., feed_dict=feed_dict)
except:
    feed_dict[did_cholesky_fail] = True
    sess.run(..., feed_dict=feed_dict)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Matrix conditioning might be a solution (but it can fail if the minimum eigen values are set too close to machine precision)&lt;/denchmark-h&gt;

For example the below code would take a non pos-def matrix and make it pos-def.
&lt;denchmark-code&gt;Kf = (Kf + tf.transpose(Kf,perm=[0,2,1]))/2.
e,v = tf.self_adjoint_eig(Kf)
e = tf.where(e &gt; 1e-14, e, 1e-14*tf.ones_like(e))
Kf_pos_def = tf.matmul(tf.matmul(v,tf.matrix_diag(e),transpose_a=True),v)
&lt;/denchmark-code&gt;

Note that cholesky should work for pos-semi-def where there are some zero eigen values. You can test this yourself. So in theory I should be able to replace line e = tf.where(e &gt; 1e-14, e, 1e-14*tf.ones_like(e)) with e = tf.where(e &gt; 0, e, tf.zeros_like(e))
Conditioning the matrix also fails sometimes, because TF has a buggy eigen code.
It only fails when you ask for gradient though; the forward calculation work then.
		</comment>
	</comments>
</bug>