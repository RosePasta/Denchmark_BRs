<bug id='976' author='martiningram' open_date='2019-05-16T22:58:25Z' closed_time='2019-12-03T22:43:43Z'>
	<summary>Unable to save large model due to HDF5 header limitation</summary>
	<description>
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

System information:

Python version: Python 3.6.4
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): b'v1.13.1-0-g6612da8951' 1.13.1
GPflow installed from (source or binary): binary
GPflow version: 1.3.0

When trying to save a large multi-output model (L=24, M=100, N=1200, 371 outputs), the saver gives:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "autograd_gp/gpflow/run_gpflow.py", line 105, in &lt;module&gt;
    run_gpflow_inference()
  File "/home/martiningram/.local/lib/python3.6/site-packages/click/core.py", line 764, in __call__
    return self.main(*args, **kwargs)
  File "/home/martiningram/.local/lib/python3.6/site-packages/click/core.py", line 717, in main
    rv = self.invoke(ctx)
  File "/home/martiningram/.local/lib/python3.6/site-packages/click/core.py", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/martiningram/.local/lib/python3.6/site-packages/click/core.py", line 555, in invoke
    return callback(*args, **kwargs)
  File "autograd_gp/gpflow/run_gpflow.py", line 88, in run_gpflow_inference
    saver.save(join(output_dir, 'saved_model'), m)
  File "/home/martiningram/.local/lib/python3.6/site-packages/gpflow/saver/saver.py", line 37, in save
    context.serializer(context).dump(pathname, encoded_target)
  File "/home/martiningram/.local/lib/python3.6/site-packages/gpflow/saver/serializers.py", line 44, in dump
    h5file.create_dataset(name='data', data=data)
  File "/usr/local/easybuild/software/Tensorflow/1.13.1-intel-2017.u2-GCC-6.2.0-CUDA10-Python-3.6.4-GPU/lib/python3.6/site-packages/h5py/_hl/group.py", line 136, in create_dataset
    dsid = dataset.make_new_dset(self, shape, dtype, data, **kwds)
  File "/usr/local/easybuild/software/Tensorflow/1.13.1-intel-2017.u2-GCC-6.2.0-CUDA10-Python-3.6.4-GPU/lib/python3.6/site-packages/h5py/_hl/dataset.py", line 167, in make_new_dset
    dset_id = h5d.create(parent.id, None, tid, sid, dcpl=dcpl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5d.pyx", line 79, in h5py.h5d.create
ValueError: Unable to create dataset (object header message is too large)
&lt;/denchmark-code&gt;

I appear to be using h5py version 2.9.0. Is there some workaround I could use?
	</description>
	<comments>
		<comment id='1' author='martiningram' date='2019-12-03T22:43:43Z'>
		&lt;denchmark-link:https://github.com/martiningram&gt;@martiningram&lt;/denchmark-link&gt;
 have you managed to sort this out by now? I don't think there's anything we can do about this inside gpflow. It would be good if you could post this on stackoverflow.com instead, under the gpflow tag: &lt;denchmark-link:https://stackoverflow.com/questions/tagged/gpflow&gt;https://stackoverflow.com/questions/tagged/gpflow&lt;/denchmark-link&gt;
 - thanks!
		</comment>
	</comments>
</bug>