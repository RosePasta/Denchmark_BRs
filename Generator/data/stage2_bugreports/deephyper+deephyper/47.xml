<bug id='47' author='evvaletov' open_date='2020-09-21T00:10:26Z' closed_time='2020-09-22T22:46:31Z'>
	<summary>[BUG] mpirun not found when starting an AMBS job on ALCF's Cooley</summary>
	<description>
Describe the bug
AMBS jobs fail on ALCF's Cooley within about a minute of starting. The log file shows the error "FileNotFoundError: [Errno 2] No such file or directory: 'mpirun'". Jobs run successfully on ALCF's Theta.
To Reproduce
Set up and submit an AMBS job on ALFC's Cooley using a command like the following:
deephyper balsam-submit hps currentopt-cooley-06 -p g4blopt-cooley2.currentopt.problem.Problem -r g4blopt-cooley2.currentopt.model_run.run -t 720 -q default -n 4 -A HiMB_Beamline -j serial
Expected behavior
The AMBS job runs and completes successfully.
Desktop (please complete the following information):

OS: Red Hat Enterprise Linux Server 7.8 (Maipo)
System: Cooley
Python version: 3.6
DeepHyper Version [e.g. 0.1.11]: 0.1.11

Log file contents
&lt;denchmark-code&gt;[BalsamDB: g4bldb-cooley] (dh-env-cooley2)[valetov@cooleylogin1 g4bl]$ cat g4bldb-cooley/log/launcher_2020-09-20_235849.log 

20-Sep-2020 23:58:49|1371|    INFO|balsam.launcher.launcher:442] Loading Balsam Launcher
20-Sep-2020 23:58:49|1371|    INFO|balsam.core.transitions:65] Starting 5 transition processes
20-Sep-2020 23:58:49|1371|    INFO|balsam.launcher.worker:53] Built 4 COOLEY workers
20-Sep-2020 23:58:49|1371|    INFO|balsam.launcher.launcher:381] Starting MPI Fork ensemble process:
mpirun -n 4 --ppn 1  --hosts cc066.cooley,cc108.cooley,cc112.cooley,cc039.cooley   /home/valetov/anaconda/x86_64/envs/dh-env-cooley2/bin/python /home/valetov/anaconda/x86_64/envs/dh-env-cooley2/lib/python3.6/site-packages/balsam/launcher/mpi_ensemble.py --time-limit-min=716.999999988079 --wf-name=currentopt-cooley-06
20-Sep-2020 23:58:49|1389|    INFO|balsam.core.transitions:207] EXIT_FLAG: exiting main loop
20-Sep-2020 23:58:50|1390|    INFO|balsam.core.transitions:207] EXIT_FLAG: exiting main loop
20-Sep-2020 23:58:50|1388|    INFO|balsam.core.transitions:207] EXIT_FLAG: exiting main loop
20-Sep-2020 23:58:50|1387|    INFO|balsam.core.transitions:207] EXIT_FLAG: exiting main loop
20-Sep-2020 23:58:50|1386|    INFO|balsam.core.transitions:207] EXIT_FLAG: exiting main loop
20-Sep-2020 23:58:50|1371|    INFO|balsam.core.transitions:78] All Transition processes joined: done.
20-Sep-2020 23:58:50|1371|    INFO|balsam.launcher.launcher:428] Exit: Launcher exit graceful



20-Sep-2020 23:58:50|1371|   ERROR|balsam:47] Uncaught Exception &lt;class 'FileNotFoundError'&gt;: [Errno 2] No such file or directory: 'mpirun'
Traceback (most recent call last):
  File "/home/valetov/anaconda/x86_64/envs/dh-env-cooley2/lib/python3.6/site-packages/balsam/launcher/launcher.py", line 443, in &lt;module&gt;
    main(args)
  File "/home/valetov/anaconda/x86_64/envs/dh-env-cooley2/lib/python3.6/site-packages/balsam/launcher/launcher.py", line 423, in main
    launcher.run()
  File "/home/valetov/anaconda/x86_64/envs/dh-env-cooley2/lib/python3.6/site-packages/balsam/launcher/launcher.py", line 389, in run
    shell=False)
  File "/home/valetov/anaconda/x86_64/envs/dh-env-cooley2/lib/python3.6/subprocess.py", line 707, in __init__
    restore_signals, start_new_session)
  File "/home/valetov/anaconda/x86_64/envs/dh-env-cooley2/lib/python3.6/subprocess.py", line 1333, in _execute_child
    raise child_exception_type(errno_num, err_msg)
FileNotFoundError: [Errno 2] No such file or directory: 'mpirun'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='evvaletov' date='2020-09-21T14:59:58Z'>
		Hello &lt;denchmark-link:https://github.com/evvaletov&gt;@evvaletov&lt;/denchmark-link&gt;

So it looks like MPI is not present... however MPI should be available in the default environment on Cooley.
Can you verify if your  contains these lines:
 +mvapich2
 @default
Also, are you doing any modifications to your $PATH? in the ~/.bashrc for example.
		</comment>
		<comment id='2' author='evvaletov' date='2020-09-22T22:46:22Z'>
		Thanks, I tried re-adding +mvapich2 to ~/.soft.cooley, and it worked. I did not realize +mvapich2 is needed in ~/.soft.cooley also to run MPI applications and not just compile them.
		</comment>
	</comments>
</bug>