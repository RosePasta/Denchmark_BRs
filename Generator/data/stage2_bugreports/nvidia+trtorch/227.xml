<bug id='227' author='OronG13' open_date='2020-11-18T06:18:32Z' closed_time='2020-12-07T04:30:48Z'>
	<summary>While not compile\link any trtorch namespace member a not support for cuda devices runtime exception is rasied</summary>
	<description>
&lt;denchmark-h:h2&gt;Bug Description&lt;/denchmark-h&gt;

In case my code doesn't include any usage of trtorch namespace types or methods which means that my application doesn't include any activation or usage of any member of the trtorch namespace the following is happened:


torch::cuda::device_count return 0


torch::cuda::is_available return false


torch::cuda::cudnn_is_available return false


the torch::jit::load operation raise the following runtime exception:


PyTorch is not linked with support for cuda devices Exception raised from getDeviceGuardImpl at ../c10/core/impl/DeviceGuardImplInterface.h:216 (most recent call first): frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;) + 0xa0 (0x7fa256e9a8 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so) frame #1: &lt;unknown function&gt; + 0xb69334 (0x7fa310c334 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #2: at::native::to(at::Tensor const&amp;, c10::Device, c10::ScalarType, bool, bool, c10::optional&lt;c10::MemoryFormat&gt;) + 0x210 (0x7fa310aa78 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #3: &lt;unknown function&gt; + 0xeec714 (0x7fa348f714 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #4: &lt;unknown function&gt; + 0x1e345fc (0x7fa43d75fc in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #5: &lt;unknown function&gt; + 0xf37524 (0x7fa34da524 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #6: at::Tensor::to(c10::Device, c10::ScalarType, bool, bool, c10::optional&lt;c10::MemoryFormat&gt;) const + 0x154 (0x7fa356420c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #7: &lt;unknown function&gt; + 0x22dfd84 (0x7fa4882d84 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #8: torch::jit::Module::to_impl(c10::optional&lt;c10::Device&gt; const&amp;, c10::optional&lt;c10::ScalarType&gt; const&amp;, bool) + 0x328 (0x7fa4884508 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #9: torch::jit::Module::to(c10::Device, bool) + 0x44 (0x7fa48852b4 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so) frame #10: &lt;unknown function&gt; + 0x4594 (0x5580d83594 in ./TRTorchTester) frame #11: __libc_start_main + 0xe0 (0x7fa220a6e0 in /lib/aarch64-linux-gnu/libc.so.6) frame #12: &lt;unknown function&gt; + 0x49d0 (0x5580d839d0 in ./TRTorchTester)
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
1.Make the LeNet module exactly as described in the TRTorch getting started:
&lt;denchmark-link:url&gt;https://nvidia.github.io/TRTorch/tutorials/getting_started.html&lt;/denchmark-link&gt;

1.make the LeNet.pt using torch.jit.script
1.Create an application based on the following cpp code:
torch::jit::Module module;
&lt;denchmark-code&gt;try
{
	bool isCudaAvailable = torch::cuda::is_available();

	std::size_t devicesCount = torch::cuda::device_count();
	std::cout &lt;&lt; "CUDA devices count - " &lt;&lt; devicesCount &lt;&lt; '\n';
	
	if (isCudaAvailable)
		std::cout &lt;&lt; "CUDA available" &lt;&lt; '\n';
	else
		std::cout &lt;&lt; "CUDA NOT available" &lt;&lt; '\n';

	bool isCudnnAvailable = torch::cuda::cudnn_is_available();

	if (isCudnnAvailable)
		std::cout &lt;&lt; "CUDNN available" &lt;&lt; '\n';
	else
		std::cout &lt;&lt; "CUDNN NOT available" &lt;&lt; '\n';
	
	// Deserialize the ScriptModule from a file using torch::jit::load().
	module = torch::jit::load("../../../../Source/Applications/TRTorchTester/Input/lenet_scripted.pt");

	module.to(at::kCUDA);
	module.eval();

	auto in = torch::randn({ 1, 1, 32, 32 }, { torch::kCUDA });
	auto trt_mod = trtorch::CompileGraph(module, std::vector&lt;trtorch::CompileSpec::InputRange&gt;{ {in.sizes()}});
	auto out = trt_mod.forward({ in });
}
catch (const c10::Error &amp; e) {
	std::cerr &lt;&lt; "torch error - \n"&lt;&lt; e.what() &lt;&lt; '\n';
	return -1;
}
&lt;/denchmark-code&gt;



This code is OK which means that the runtime exception isn't raised and I'm getting the following prints:
CUDA devices count - 1
CUDA available
CUDNN available


If I only masked the following lines:
auto in = torch::randn({ 1, 1, 32, 32 }, { torch::kCUDA }); auto trt_mod = trtorch::CompileGraph(module, std::vector&lt;trtorch::CompileSpec::InputRange&gt;{ {in.sizes()}}); auto out = trt_mod.forward({ in });


(To emphasize, I didn't change the build properties at all which means that I'm still compile and link with the same declarations)
The above mentioned runtime exception is rasied and the prints are:
CUDA devices count - 0
CUDA NOT available
CUDNN NOT available
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

To get the same prints as I got with the full code and not to get any runtime exception of unavailability PyTorch CUDA.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Build information about the TRTorch compiler can be found by turning on debug messages


PyTorch Version (e.g., 1.0): 1.6
CPU Architecture: Jetson Xavier AGX
OS (e.g., Linux): Linux 18.04 - JetPack 4.4
How you installed PyTorch (conda, pip, libtorch, source): pip3
Build command you used (if compiling from source):
Are you using local sources or building from archives: Local sources
Python version: 3.6.9
CUDA version: 10.2
GPU models and configuration: Jetson Xavier AGX
Any other relevant information: JetPack 4.4

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='OronG13' date='2020-11-30T17:07:34Z'>
		&lt;denchmark-link:https://github.com/andi4191&gt;@andi4191&lt;/denchmark-link&gt;
 can you take a look at this?
		</comment>
		<comment id='2' author='OronG13' date='2020-12-04T20:20:48Z'>
		&lt;denchmark-link:https://github.com/OronG13&gt;@OronG13&lt;/denchmark-link&gt;
: I looked into this problem and came across an already reported &lt;denchmark-link:https://discuss.pytorch.org/t/error-pytorch-is-not-linked-with-support-for-cuda-devices/103807&gt;issue&lt;/denchmark-link&gt;
.
I tried using the following compilation flag as suggested on the link and it seems to run fine after that.
-Wl,--no-as-needed -ltorch_cuda -Wl,--as-needed
Can you try this and let us know if it works for you?
		</comment>
		<comment id='3' author='OronG13' date='2020-12-07T04:30:17Z'>
		Hello,
It works well and the problem is solved - Thanks!
I added the following command:
'set (CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} -Wl,--no-as-needed -ltorch_cuda -Wl,--as-needed")`
Now I'm getting the following report:
CUDA devices count - 1
CUDA available
CUDNN available
Without using this linker flags line it come back to report that they (CYDA &amp; CUDNN) are not available when no trtorch namespace is used.
		</comment>
	</comments>
</bug>