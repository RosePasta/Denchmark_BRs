<bug id='129' author='narendasan' open_date='2020-07-09T20:09:10Z' closed_time='2021-01-05T00:17:29Z'>
	<summary>Segfault in plugin based converters</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am seeing segfaults in the three sets of tests that deal with plugins:
test_pooling:
&lt;denchmark-code&gt;[ RUN      ] Converters.ATenAdaptiveAvgPool2DConvertsCorrectlyWithDynamicInput
...
Thread 11 "test_pooling" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fff1bfff700 (LWP 7989)]
0x00007fff96c8c763 in void at::parallel_for&lt;at::native::(anonymous namespace)::adaptive_avg_pool2d_single_out_frame&lt;float&gt;(float*, float*, long, long, long, long, long, long, long, long)::{lambda(long, long)#1}&gt;(long, long, long, at::native::(anonymous namespace)::adaptive_avg_pool2d_single_out_frame&lt;float&gt;(float*, float*, long, long, long, long, long, long, long, long)::{lambda(long, long)#1} const&amp;) [clone ._omp_fn.1] () from /home/narens/Developer/opensource/TRTorch/bazel-TRTorch/external/libtorch/lib/libtorch_cpu.so
(gdb) bt
#0  0x00007fff96c8c763 in void at::parallel_for&lt;at::native::(anonymous namespace)::adaptive_avg_pool2d_single_out_frame&lt;float&gt;(float*, float*, long, long, long, long, long, long, long, long)::{lambda(long, long)#1}&gt;(long, long, long, at::native::(anonymous namespace)::adaptive_avg_pool2d_single_out_frame&lt;float&gt;(float*, float*, long, long, long, long, long, long, long, long)::{lambda(long, long)#1} const&amp;) [clone ._omp_fn.1] () from /home/narens/Developer/opensource/TRTorch/bazel-TRTorch/external/libtorch/lib/libtorch_cpu.so
#1  0x00007fff9243243e in ?? () from /home/narens/Developer/opensource/TRTorch/bazel-TRTorch/external/libtorch/lib/libgomp-75eea7e8.so.1
#2  0x00007fff958a76db in start_thread (arg=0x7fff1bfff700) at pthread_create.c:463
#3  0x00007fff953b8a3f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
&lt;/denchmark-code&gt;

test_interpolate:
&lt;denchmark-code&gt;[ RUN      ] Converters.ATenUpsampleLinear1dConvertsCorrectlyWithoutAlignCorners

Thread 1 "test_interpolat" received signal SIGSEGV, Segmentation fault.
0x00007fff977a9425 in void c10::function_ref&lt;void (char**, long const*, long)&gt;::callback_fn&lt;at::native::(anonymous namespace)::cpu_kernel_vec&lt;at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda()#1}, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda()#2}&gt;(at::TensorIterator&amp;, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda()#1}&amp;&amp;, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda()#2}&amp;&amp;)::{lambda(char**, long const*, long)#1}&gt;(long, char**, long const*, long) ()
   from /home/narens/Developer/opensource/TRTorch/bazel-TRTorch/external/libtorch/lib/libtorch_cpu.so
&lt;/denchmark-code&gt;

test_serialization:
&lt;denchmark-code&gt;[ RUN      ] CompiledModuleForwardIsCloseSuite/ModuleTests.SerializedModuleIsStillCorrect/1

Thread 1 "test_serializat" received signal SIGSEGV, Segmentation fault.
0x00007fff977a9425 in void c10::function_ref&lt;void (char**, long const*, long)&gt;::callback_fn&lt;at::native::(anonymous namespace)::cpu_kernel_vec&lt;at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda()#1}, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda()#2}&gt;(at::TensorIterator&amp;, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda()#1}&amp;&amp;, at::native::(anonymous namespace)::fill_kernel(at::TensorIterator&amp;, c10::Scalar)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda()#2}&amp;&amp;)::{lambda(char**, long const*, long)#1}&gt;(long, char**, long const*, long) ()
   from /home/narens/Developer/opensource/TRTorch/bazel-TRTorch/external/libtorch/lib/libtorch_cpu.so
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Build libtrtorch on master
Run test suite

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

These tests should not be segfaulting here
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


Build information about the TRTorch compiler can be found by turning on debug messages


PyTorch Version (e.g., 1.0): 1.5.1
CPU Architecture: x86_64
OS (e.g., Linux): Ubuntu 18.04
How you installed PyTorch (conda, pip, libtorch, source): Bazel fetch of libtorch
Build command you used (if compiling from source): bazelisk test //tests -c opt --test_output=errors --jobs=5
Are you using local sources or building from archives: archives
Python version: 3.6
CUDA version: 10.2
GPU models and configuration: TITAN V
Any other relevant information:

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='narendasan' date='2020-07-09T20:20:39Z'>
		Seems like this is an issue specifically with 1.5.1/7.1 I cannot replicate on commit &lt;denchmark-link:https://github.com/NVIDIA/TRTorch/commit/cc2f2fd68a9a7eee19822b8aa23e0437b1bd29a0&gt;cc2f2fd&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='narendasan' date='2020-07-09T20:57:30Z'>
		Narrowed down the issue to TensorRT 7.1
		</comment>
		<comment id='3' author='narendasan' date='2020-07-15T00:03:37Z'>
		Issue is still relevant, currently have a WAR in the compiler to allow people to continue to use adaptive_avg_pool in dynamic mode but there is a pref regression as we were forced to move onto CPU until the issue of instantiating Tensors during TensorRT runtime can be solved.
		</comment>
		<comment id='4' author='narendasan' date='2020-08-14T00:04:32Z'>
		This issue has not seen activity for 30 days, Remove stale label or comment or this will be closed in 5 days
		</comment>
		<comment id='5' author='narendasan' date='2020-09-14T00:05:36Z'>
		This issue has not seen activity for 30 days, Remove stale label or comment or this will be closed in 5 days
		</comment>
		<comment id='6' author='narendasan' date='2020-10-16T00:06:35Z'>
		This issue has not seen activity for 30 days, Remove stale label or comment or this will be closed in 5 days
		</comment>
		<comment id='7' author='narendasan' date='2020-11-16T00:05:23Z'>
		This issue has not seen activity for 30 days, Remove stale label or comment or this will be closed in 5 days
		</comment>
		<comment id='8' author='narendasan' date='2020-12-31T00:16:48Z'>
		This issue has not seen activity for 30 days, Remove stale label or comment or this will be closed in 5 days
		</comment>
	</comments>
</bug>