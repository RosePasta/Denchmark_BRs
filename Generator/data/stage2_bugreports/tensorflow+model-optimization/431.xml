<bug id='431' author='corvoysier' open_date='2020-06-18T09:42:27Z' closed_time='2020-12-23T18:37:28Z'>
	<summary>Spurious Dequantize/Cast/Quantize sequence of ops at the end of a QAT TFLite model.</summary>
	<description>
Describe the bug
When converting a MNIST model trained using QAT, there is a spurious Dequantize/Cast/Quantize sequence of ops at the end of the TFLite model.
System information
TensorFlow version (installed from source or binary): TF nightly 2.3.0-dev20200616
TensorFlow Model Optimization version (installed from source or binary): 0.3.0
Python version: 3.6.9
Describe the expected behavior
Using the same workflow as the QAT MNIST example, produce a TFLite model from a slightly more complex model.
As with the tutorial model (see tutorial_integer.png), after the conversion to TFLite, the converted model should have the following structure:

input (float),
quantize,
several standard ops with integer tensors,
dequantize,
identity.

Describe the current behavior
The generated model contains a spurious dequantize/cast/quantize sequence before the final dequantize (see mnist_integer.png). This doesn't seem to affect the accuracy, but makes the model incompatible with the Edge TPU compiler.
Code to reproduce the issue
The training script to produce the model is in that gist:
&lt;denchmark-link:https://gist.github.com/corvoysier/e4d9c6b96b103928544a57054917aace&gt;https://gist.github.com/corvoysier/e4d9c6b96b103928544a57054917aace&lt;/denchmark-link&gt;

Screenshots
&lt;denchmark-link:https://user-images.githubusercontent.com/58857004/85004489-e88c4780-b157-11ea-8021-e8e8f8baedbe.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/58857004/85004498-eb873800-b157-11ea-98c0-dc5d8cab7599.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='corvoysier' date='2020-06-18T15:43:04Z'>
		Hi, we are actively working on integer input and output support in the conversion for QAT. (The lack of this support is why you are getting spurious dequantize and quantize operations that edgetpu doesn't like). We will update this issue when that is ready.
If this is blocking you, we have a workaround script that can change the input and output types for you to uint8, which EdgeTPU requires: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/python/modify_model_interface.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/python/modify_model_interface.py&lt;/denchmark-link&gt;

Hope that helps!
		</comment>
		<comment id='2' author='corvoysier' date='2020-06-18T16:49:10Z'>
		&lt;denchmark-link:https://github.com/corvoysier&gt;@corvoysier&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/suharshs&gt;@suharshs&lt;/denchmark-link&gt;

FYI, with , my model with the exact same code as above looks very different.
&lt;denchmark-link:https://user-images.githubusercontent.com/25712149/85047037-ea094000-b156-11ea-91bc-cd307c56ac6a.png&gt;&lt;/denchmark-link&gt;

So I set the tensor shapes to be static before converting is giving me better results:
&lt;denchmark-code&gt;q_aware_model.input.set_shape((1,) + q_aware_model.input.shape[1:])
return converter.convert()
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/25712149/85048750-63099700-b159-11ea-92d5-0265277b164d.png&gt;&lt;/denchmark-link&gt;

Accuracy is still good:
&lt;denchmark-code&gt;Baseline test accuracy: 0.9463000297546387
Quantized float test accuracy: 0.9865999817848206
Integer test accuracy: 0.9865
&lt;/denchmark-code&gt;

and it compiles:
&lt;denchmark-code&gt;edgetpu_compiler -s mnist_integer.tflite                                  vunam@penguin
Edge TPU Compiler version 2.1.302470888

Model compiled successfully in 76 ms.

Input model: mnist_integer.tflite
Input size: 332.88KiB
Output model: mnist_integer_edgetpu.tflite
Output size: 537.03KiB
On-chip memory used for caching model parameters: 130.50KiB
On-chip memory remaining for caching model parameters: 7.77MiB
Off-chip memory used for streaming uncached model parameters: 392.06KiB
Number of Edge TPU subgraphs: 1
Total number of operations: 10
Operation log: mnist_integer_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 8
Number of operations that will run on CPU: 2

Operator                       Count      Status

CONV_2D                        4          Mapped to Edge TPU
RESHAPE                        1          Mapped to Edge TPU
DEQUANTIZE                     1          Operation is working on an unsupported data type
FULLY_CONNECTED                1          Mapped to Edge TPU
MAX_POOL_2D                    2          Mapped to Edge TPU
QUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation
&lt;/denchmark-code&gt;

cheers!
		</comment>
		<comment id='3' author='corvoysier' date='2020-06-19T09:37:31Z'>
		&lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;


FYI, with 2.3.0-dev20200618, my model with the exact same code as above looks very different.

Are you sure the first model you obtain is from the nightly ? What you got is exactly what I had when I used TF 2.2 release, and it was actually a float quantized model (you can check the tensors).

So I set the tensor shapes to be static before converting is giving me better results:
q_aware_model.input.set_shape((1,) + q_aware_model.input.shape[1:])

Yes, that did the trick for me also. I now have the correct sequence.
		</comment>
		<comment id='4' author='corvoysier' date='2020-06-19T12:38:51Z'>
		Humm I may have made a mistake of using tf2.2 but I remember specifically checking tf version before trying your code. Not ðŸ’¯ on this
		</comment>
		<comment id='5' author='corvoysier' date='2020-06-19T16:15:08Z'>
		&lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
 I second what &lt;denchmark-link:https://github.com/corvoysier&gt;@corvoysier&lt;/denchmark-link&gt;
  said. I was trying out 2 difference version on my side. TF nightly give a clean graphs compare to TF 2.2. I believe I mentioned somewhere in documents that it's recommended to use optimization toolkit with TF nightly
		</comment>
		<comment id='6' author='corvoysier' date='2020-06-19T16:28:58Z'>
		&lt;denchmark-link:https://github.com/LLNLanLeN&gt;@LLNLanLeN&lt;/denchmark-link&gt;
 may I did made that mistake lol
		</comment>
		<comment id='7' author='corvoysier' date='2020-06-19T16:30:55Z'>
		&lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
  may be, haha. But the graphs turns out very well. I have a quick question actually, the script. Were you able to run this script as recommended above?
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/python/modify_model_interface.py&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/python/modify_model_interface.py&lt;/denchmark-link&gt;

I install the optimization tookit via pip, but when I run the command, it couldn't locate the file (I couldn't locate it in my environment either)
		</comment>
		<comment id='8' author='corvoysier' date='2020-06-19T16:37:58Z'>
		No, by modifying the script to set &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/431#issuecomment-646163906&gt;static input shape&lt;/denchmark-link&gt;
, I didn't have any troubles with the compiler so I didn't tried that work around
But from the &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/python/BUILD#L11&gt;build file&lt;/denchmark-link&gt;
 it looks like it should be something like this to run (from tensorflow root):
&lt;denchmark-code&gt;bazel run -c opt //tensorflow/lite/tools/optimize/python:modify_python_inferace -- --input model ...
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='corvoysier' date='2020-06-19T17:02:57Z'>
		&lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
 I see, thank you. I'll try the script out with bazel
		</comment>
		<comment id='10' author='corvoysier' date='2020-06-22T20:52:17Z'>
		&lt;denchmark-link:https://github.com/LLNLanLeN&gt;@LLNLanLeN&lt;/denchmark-link&gt;
 As &lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
 pointed out, you should be able to modify the model input and output by running
&lt;denchmark-code&gt;$ bazel run -c opt //tensorflow/lite/tools/optimize/python:modify_model_interface -- --input_file=&lt;path/to/quant_model.tflite&gt; --output_file=&lt;path/to/quant_model_int_io_type.tflite&gt; --input_type=INT8 --output_type=INT8
&lt;/denchmark-code&gt;

You can change 'INT8' to 'UINT8' as well.
If you are using , you can directly modify the input and output type of quantized models during conversion, by using this code: &lt;denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only&gt;https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only&lt;/denchmark-link&gt;

The exception to this is quantize-aware trained models. If the model is quantize-aware trained, then in order to modify the input and output type, you need to first convert the model to a quantized tflite model (which will always have float input and ouytput) and then use the bazel command provided above to modify the input and output type to tf.int8 or tf.uint8 .
		</comment>
		<comment id='11' author='corvoysier' date='2020-06-22T21:13:56Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
  thanks for confirming. I've managed to use the script
		</comment>
		<comment id='12' author='corvoysier' date='2020-06-28T02:40:28Z'>
		mark
		</comment>
		<comment id='13' author='corvoysier' date='2020-07-08T12:53:39Z'>
		
@LLNLanLeN As @Namburger pointed out, you should be able to modify the model input and output by running
$ bazel run -c opt //tensorflow/lite/tools/optimize/python:modify_python_inferace -- --input_file=&lt;path/to/quant_model.tflite&gt; --output_file=&lt;path/to/quant_model_int_io_type.tflite&gt; --input_type=INT8 --output_type=INT8

You can change 'INT8' to 'UINT8' as well.
If you are using tf-nightly, you can directly modify the input and output type of quantized models during conversion, by using this code: https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only
The exception to this is quantize-aware trained models. If the model is quantize-aware trained, then in order to modify the input and output type, you need to first convert the model to a quantized tflite model (which will always have float input and ouytput) and then use the bazel command provided above to modify the input and output type to tf.int8 or tf.uint8 .

The correct command is:
&lt;denchmark-code&gt;$ bazel run -c opt //tensorflow/lite/tools/optimize/python:modify_model_interface -- --input_file=&lt;path/to/quant_model.tflite&gt; --output_file=&lt;path/to/quant_model_int_io_type.tflite&gt; --input_type=INT8 --output_type=INT8
&lt;/denchmark-code&gt;

		</comment>
		<comment id='14' author='corvoysier' date='2020-07-22T21:09:18Z'>
		

@LLNLanLeN As @Namburger pointed out, you should be able to modify the model input and output by running
$ bazel run -c opt //tensorflow/lite/tools/optimize/python:modify_python_inferace -- --input_file=&lt;path/to/quant_model.tflite&gt; --output_file=&lt;path/to/quant_model_int_io_type.tflite&gt; --input_type=INT8 --output_type=INT8

You can change 'INT8' to 'UINT8' as well.
If you are using tf-nightly, you can directly modify the input and output type of quantized models during conversion, by using this code: https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only
The exception to this is quantize-aware trained models. If the model is quantize-aware trained, then in order to modify the input and output type, you need to first convert the model to a quantized tflite model (which will always have float input and ouytput) and then use the bazel command provided above to modify the input and output type to tf.int8 or tf.uint8 .

The correct command is:
$ bazel run -c opt //tensorflow/lite/tools/optimize/python:modify_model_interface -- --input_file=&lt;path/to/quant_model.tflite&gt; --output_file=&lt;path/to/quant_model_int_io_type.tflite&gt; --input_type=INT8 --output_type=INT8


I got ERROR: Skipping '//tensorflow/lite/tools/optimize/python:modify_python_inferace': no such target '//tensorflow/lite/tools/optimize/python:modify_python_inferace': target 'modify_python_inferace' not declared in package 'tensorflow/lite/tools/optimize/pyth, Do you know how to solve it? Thanks a lot!
		</comment>
		<comment id='15' author='corvoysier' date='2020-07-22T21:27:00Z'>
		&lt;denchmark-link:https://github.com/pl-ang&gt;@pl-ang&lt;/denchmark-link&gt;
 Could you re-run the command in comment? &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/431#issuecomment-647763584&gt;#431 (comment)&lt;/denchmark-link&gt;

(There was a typo in the command which was pointed out by &lt;denchmark-link:https://github.com/EscVM&gt;@EscVM&lt;/denchmark-link&gt;
. I have updated my original comment as well.
You need to modify  to   (as you can see I had misspelt ))
		</comment>
		<comment id='16' author='corvoysier' date='2020-07-23T08:00:27Z'>
		&lt;denchmark-link:https://github.com/pl-ang&gt;@pl-ang&lt;/denchmark-link&gt;
 Moreover, remember that you have to specify the full path to indicate the input and output file!
		</comment>
		<comment id='17' author='corvoysier' date='2020-07-23T15:26:52Z'>
		&lt;denchmark-link:https://github.com/MeghnaNatraj&gt;@MeghnaNatraj&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/EscVM&gt;@EscVM&lt;/denchmark-link&gt;
 Thank you so much for your answers! I already converted the input and output types to INT8 successfully.
However, when I use the GPU backend on the quantized MobileNetV1 tflite model by using the TensorFlow Lite delegate APIs on Android, it shows Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.
Do you know any reasons that may cause it? For the Android CPU part, the quantized model works well.
		</comment>
		<comment id='18' author='corvoysier' date='2020-07-23T15:31:17Z'>
		&lt;denchmark-link:https://github.com/pl-ang&gt;@pl-ang&lt;/denchmark-link&gt;
 I'm not exactly sure the issue, but it sounds like the solution is to make sure that the graph you export is static-size tensors. And you can do this by using the following line (make sure to use the correct variable name), It was recommended above by Namburger:
&lt;denchmark-code&gt;q_aware_model.input.set_shape((1,) + q_aware_model.input.shape[1:])
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='corvoysier' date='2020-07-23T17:29:59Z'>
		&lt;denchmark-link:https://github.com/LLNLanLeN&gt;@LLNLanLeN&lt;/denchmark-link&gt;
 Got it!
Do you know any method to convert the tensor type from int8 to uint8? Because I found another error in Android:
&lt;denchmark-code&gt;Following operations are not supported by GPU delegate:
    CONV_2D: Max version supported: 2. Requested version 3.
    DEPTHWISE_CONV_2D: Max version supported: 2. Requested version 3.
    MEAN: OP is supported, but tensor type isn't matched!
    PAD: Max version supported: 1. Requested version 2.
    RESHAPE: OP is supported, but tensor type isn't matched!
    SOFTMAX: Max version supported: 1. Requested version 2.
&lt;/denchmark-code&gt;

I think it probably caused by the tensor type based on [https://github.com/&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/32468&gt;tensorflow/tensorflow/issues/32468&lt;/denchmark-link&gt;
].  Do you have any suggestions? Thanks!
		</comment>
		<comment id='20' author='corvoysier' date='2020-07-23T18:13:21Z'>
		&lt;denchmark-link:https://github.com/pl-ang&gt;@pl-ang&lt;/denchmark-link&gt;
 could you create a separate issue to debug this error as it's unrelated? We can further debug it in that issue. Post a link to the issue here and i will take a look.
		</comment>
		<comment id='21' author='corvoysier' date='2020-07-23T18:19:21Z'>
		&lt;denchmark-link:https://github.com/pl-ang&gt;@pl-ang&lt;/denchmark-link&gt;
 How are you running the model? If you are using Java API and want to run a quantized model w/ GPU, look at &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu_advanced#running_quantized_models_experimental&gt;this&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='22' author='corvoysier' date='2020-07-23T19:11:44Z'>
		&lt;denchmark-link:https://github.com/pl-ang&gt;@pl-ang&lt;/denchmark-link&gt;
 unfortunately I'm not very familiar with the error you're having.
To convert Uint8 input, you can just use the same script and identify that input and output type is Uint8. But the problem is that, if your model is QAT, then I believe it's int8 by default. Hence if you force the input and output to be UInt8, you'll still see the Quantize and Dequantize nodes at the input and output
		</comment>
		<comment id='23' author='corvoysier' date='2020-07-23T19:12:19Z'>
		&lt;denchmark-link:https://github.com/srjoglekar246&gt;@srjoglekar246&lt;/denchmark-link&gt;
 I run the model by Basic usage Android (Java). I am trying to do that by the Advanced usage as below:
&lt;denchmark-code&gt;// NEW: Prepare GPU delegate with feature turned on.
GpuDelegate delegate = new GpuDelegate(new GpuDelegate.Options().setQuantizedModelsAllowed(true));

Interpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);
&lt;/denchmark-code&gt;

But it has cannot resolve method 'setQuantizedModelsAllowed(boolean)' error.
		</comment>
		<comment id='24' author='corvoysier' date='2020-07-23T19:35:10Z'>
		You might need to use a newer version of the AAR/binary.
		</comment>
		<comment id='25' author='corvoysier' date='2020-07-23T21:09:43Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/java/src/main/java/org/tensorflow/lite/gpu/GpuDelegate.java#L73&gt;This&lt;/denchmark-link&gt;
 is the method on .
I am not an expert with Android packaging, but is the  rule implementation required? Looks like the GPU delegate documentation requires only &lt;denchmark-link:https://www.tensorflow.org/lite/performance/gpu#step_2_edit_appbuildgradle_to_use_the_nightly_gpu_aar&gt;two&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='corvoysier' date='2020-07-24T01:44:24Z'>
		
This is the method on GpuDelegate.
I am not an expert with Android packaging, but is the lite:+ rule implementation required? Looks like the GPU delegate documentation requires only two

Thank you so much! The quantized model works well now with GPU!
		</comment>
		<comment id='27' author='corvoysier' date='2020-07-24T16:04:01Z'>
		Awesome. Thanks for verifying! Feel free to close the issue.
		</comment>
		<comment id='28' author='corvoysier' date='2020-09-20T14:16:17Z'>
		
Hi, we are actively working on integer input and output support in the conversion for QAT. (The lack of this support is why you are getting spurious dequantize and quantize operations that edgetpu doesn't like). We will update this issue when that is ready.
If this is blocking you, we have a workaround script that can change the input and output types for you to uint8, which EdgeTPU requires: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/python/modify_model_interface.py
Hope that helps!

Any news about support for QAT model ?
To run the script is necessary to build tf from source ??
		</comment>
		<comment id='29' author='corvoysier' date='2020-09-20T15:23:14Z'>
		&lt;denchmark-link:https://github.com/Mattrix00&gt;@Mattrix00&lt;/denchmark-link&gt;
  I believe the only way to run the script is to build from source ( I built from sources)
		</comment>
		<comment id='30' author='corvoysier' date='2020-09-22T07:18:08Z'>
		This issue has always been about the spurious Dequantize/Cast/Quantize in the middle of the converted model and never about the lack of support for integer inputs/outputs that I was well aware of at the time I created it.
The real fix to my issue was to statically set the input shape to (1, w, h, c), as I posted here: &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/431#issuecomment-646539366&gt;#431 (comment)&lt;/denchmark-link&gt;

I think it would be better to close it now, and maybe create a specific issue for people having issues with the script that &lt;denchmark-link:https://github.com/Namburger&gt;@Namburger&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/LLNLanLeN&gt;@LLNLanLeN&lt;/denchmark-link&gt;
 mentioned to use integer inputs and outputs (which again was NOT my issue in the first place).
		</comment>
		<comment id='31' author='corvoysier' date='2020-12-23T18:25:53Z'>
		The issue is resolved with TF 2.4 (the current default TF version in colab). From the &lt;denchmark-link:https://gist.github.com/corvoysier/e4d9c6b96b103928544a57054917aace&gt;gist&lt;/denchmark-link&gt;
 in &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/431#issue-641064608&gt;comment1&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/8790823/103025491-edeba400-4577-11eb-8423-b9a81fa2b85a.png&gt;&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>