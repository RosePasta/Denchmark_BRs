<bug id='439' author='IvanJuraga' open_date='2020-06-25T12:02:18Z' closed_time='2020-06-27T01:45:07Z'>
	<summary>QAT model lower accuracy than post train quant model</summary>
	<description>
System information
TensorFlow version (installed from source or binary): 2.3.0.dev20200609 (binary)
TensorFlow Model Optimization version (installed from source or binary):0.3.0 (binary)
Python version:3.6.9
I am trying to compare results of a base keras model and both quantization variants( QAT and just post train quant) and I manage to get both models but the problem is that the post train model is showing much better results (marginal drop compared to the base model). Now one would think the QAT approach would be even better but the problem is that the model shows about 30-35% loss increase. Both of the scripts are made by following the official guide on the TF site. The model alone is really basic( few CNN blocks and 3 branches of Dense layers. If necessary I can post the code snipets. I tried multiple things with the QAT : play around with the starting lr, size of the subset I'm using,slight changes to the model's architecture.
The thing that confuses me the most is I would understand that the QAT results are bad if the post train quant results are just as bad or worse but they are near perfect so I don't get why a superior solution is the worse one in this case.
	</description>
	<comments>
		<comment id='1' author='IvanJuraga' date='2020-06-27T01:45:06Z'>
		Hi &lt;denchmark-link:https://github.com/IvanJuraga&gt;@IvanJuraga&lt;/denchmark-link&gt;
,
How long did you train the QAT model for? Please note that QAT starts training the model with a relatively large range of values for activations [-6, 6] and uses moving averages with a co-efficient of 0.999 to converge. This often takes ~200 steps. So you might just need to train the model for longer. Please try that.
I'm considering changing some of these pre-fixed ranges to make this quicker.
Please let me know if that doesn't work.
Also, it's infeasible to understand these without reproduction code. So, in case you continue to have the issue please share the code for your model with a reproduction gist/colab.
Thanks.
		</comment>
		<comment id='2' author='IvanJuraga' date='2020-07-03T11:12:05Z'>
		Hello nutsiepully,
First off thank you for your quick response. Right now I'm training this model for about 20 epochs of the whole dataset (which consists of 595345 samples divided in batches of size 32).
The code is here: &lt;denchmark-link:https://colab.research.google.com/drive/1LYhS49gUV1dSzHlYIgbAVi-vV1eHw6JI#scrollTo=IpT4kiOaNgzM&gt;https://colab.research.google.com/drive/1LYhS49gUV1dSzHlYIgbAVi-vV1eHw6JI#scrollTo=IpT4kiOaNgzM&lt;/denchmark-link&gt;

I know it didn't even trained but the problem still remains and I can't publicly provide my training data so I can't reproduce with that data a valid code for qat.
P.S. I also forgot to mention that the base model created and trained using tf.1.14 as backend if that matters. Since there are no direct rules that you need to use the tf-nightly version to create the base model (one you use before the qat).
		</comment>
		<comment id='3' author='IvanJuraga' date='2020-07-08T19:06:08Z'>
		Happy to help. I requested access to your colab. Once I have it, I can try it out.
The number of training step seems enough in your case, given the numbers.
In general, QAT experiences an initial drop due to initial ranges, but then adjusts quickly. You should not be seeing a drop after that and the numbers should at least be comparable to post-training values.
But I'll need to reproduce it to investigate further.
		</comment>
		<comment id='4' author='IvanJuraga' date='2020-07-09T06:13:06Z'>
		Thank you for the time you are investing in helping me find the solution to my problem. Right now I managed to lower the loss closer to the base model loss / post_trained_quanted model by writing my own config setup and setting it like this:

class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):
# Configure how to quantize weights.
def get_weights_and_quantizers(self, layer):
return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=True, per_axis=False))]
# Configure how to quantize activations.
def get_activations_and_quantizers(self, layer):
    return [
        (layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=True, narrow_range=True, per_axis=False))]


the narrow_range flag being set up as True is not as important as the symmetric flag being True.
Right now I'm just trying to quant the top of the network (only dense layers) but the same loss increase is present so I'm trying to solve that and then apply the same solution to the rest of the network. After training the model I get these values inside the quanted model but I don't understand what each values represents so if you know I would appreciate it if you told me what each value stands for. I'm excluding the first values since those are just the weights and biases.

quant_fc_a               -1   -1.79   1.79   -348.78   348.78
quant_fc_b              -1   -1.99   1.99   -245.17   245.17
quant_fc_c               -1   -1.99   1.99   -295.15   295.15
quant_output_a            -1   -6.59   6.59   -878.11   878.11
quant_output_b          -1   -7.55   7.55   -385.94   385.94
quant_output_c           -1   -7.08   7.08   -414.6   414.6
quant_a                   -1   -0.97   0.97   -52.48   52.48
quant_b                 -1   -0.5   0.5   -24.63   24.63
quant_c                  -1   -0.5   0.5   -24.19   24.19

branches a and c are giving me the almost the same results as the base model but branch b is the one that is giving me all the headache.
		</comment>
		<comment id='5' author='IvanJuraga' date='2020-07-10T08:28:53Z'>
		Hello &lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;

I just wanted to tell you that I managed by only applying quant on the dense layers in the top of the model get a score similar to the base model (2 of the output losses are better while one is a bit worse but the overall loss is lower) and I managed to get that score by changing the quant_config (following the tutorial described on the tf site).
Now my question to you is is there any guidance on how to apply the same quant config on the rest of the model since when I just apply it on every layer I get an error.
Any help is much appreciated.
		</comment>
	</comments>
</bug>