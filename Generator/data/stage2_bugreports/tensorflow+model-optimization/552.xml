<bug id='552' author='debapriyamaji' open_date='2020-09-21T05:24:21Z' closed_time='2020-09-28T23:57:13Z'>
	<summary>CONV+BN+ReLU doesn't get merged with custom quantization</summary>
	<description>
Prior to filing: check that this should be a bug instead of a feature request. Everything supported, including the compatible versions of TensorFlow, is listed in the overview page of each technique. For example, the overview page of quantization-aware training is &lt;denchmark-link:https://www.tensorflow.org/model_optimization/guide/quantization/training&gt;here&lt;/denchmark-link&gt;
. An issue for anything not supported should be a feature request.
Describe the bug
I am trying to perform custom quantization in a n/w with ConV+BN+ReLU pattern. Now, these layers are not getting merged as is the case with default quantization. As it turns out, this is the expected behavior as specified in this piece of code:

&lt;denchmark-link:https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_transforms.py&gt;https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_transforms.py&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;class Conv2DBatchNormReLUQuantize(Conv2DBatchNormQuantize):
  """Ensure FQ does not get placed between Conv, BatchNorm and ReLU."""

  def pattern(self):
    return LayerPattern(
        # TODO(pulkitb): Enhance match to only occur for relu, relu1 and relu6
        'ReLU',
        inputs=[super(Conv2DBatchNormReLUQuantize, self).pattern()])

  def _replace(self, relu_layer_node, bn_layer_node, conv_layer_node):
    if _has_custom_quantize_config(
        relu_layer_node, bn_layer_node, conv_layer_node):
      return relu_layer_node

    conv_layer_node.layer['config']['activation'] = \
      keras.activations.serialize(quantize_aware_activation.NoOpActivation())
    bn_layer_node.metadata['quantize_config'] = \
      default_8bit_quantize_configs.NoOpQuantizeConfig()

    return relu_layer_node
&lt;/denchmark-code&gt;

Here, we are checking if any of the layer has a custom quantization and based on that we change the behavior of the Conv and BN layer. This does not seem to be the correct behavior as per my expectation.  Since I am not specifying anywhere that I don't want them to be merged, these layers must be merged by default
Expected Implementation
&lt;denchmark-code&gt;class Conv2DBatchNormReLUQuantize(Conv2DBatchNormQuantize):
  """Ensure FQ does not get placed between Conv, BatchNorm and ReLU."""

  def pattern(self):
    return LayerPattern(
        # TODO(pulkitb): Enhance match to only occur for relu, relu1 and relu6
        'ReLU',
        inputs=[super(Conv2DBatchNormReLUQuantize, self).pattern()])

  def _replace(self, relu_layer_node, bn_layer_node, conv_layer_node):
    #if _has_custom_quantize_config(
    #    relu_layer_node, bn_layer_node, conv_layer_node):
     # return relu_layer_node

    conv_layer_node.layer['config']['activation'] = \
      keras.activations.serialize(quantize_aware_activation.NoOpActivation())
    bn_layer_node.metadata['quantize_config'] = \
      default_8bit_quantize_configs.NoOpQuantizeConfig()

    return relu_layer_node
&lt;/denchmark-code&gt;

System information Linux
TensorFlow version (installed from source or binary):2.2
TensorFlow Model Optimization version (installed from source or binary):0.3
Python version:3.7
	</description>
	<comments>
		<comment id='1' author='debapriyamaji' date='2020-09-22T07:35:13Z'>
		&lt;denchmark-link:https://github.com/debapriyamaji&gt;@debapriyamaji&lt;/denchmark-link&gt;
 When you made this change, does the QAT process still worked as expected? (meaning that when you provide a custom config for QAT, does Batchnorm get folded after converting to tflite?)
		</comment>
		<comment id='2' author='debapriyamaji' date='2020-09-22T08:45:48Z'>
		&lt;denchmark-link:https://github.com/LLNLanLeN&gt;@LLNLanLeN&lt;/denchmark-link&gt;
 , yes they will get merged.  This line of code below ensures that no fakequant is placed after conv. Hence, conv and bn gets merged :
&lt;denchmark-code&gt;conv_layer_node.layer['config']['activation'] = \
      keras.activations.serialize(quantize_aware_activation.NoOpActivation())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='debapriyamaji' date='2020-09-24T12:53:12Z'>
		Thank you, I'll try it as soon as possible and get back to this thread :D. I was looking for this line since I first try to fold the Batchnorm, but glad that someone found it eventually. Just want to double check, what version of  are you using? &lt;denchmark-link:https://github.com/debapriyamaji&gt;@debapriyamaji&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='debapriyamaji' date='2020-09-24T14:06:48Z'>
		Hi, have you tried this method for per-tensor quantization?
Even with the above modification, in my TFLite model the BatchNorm still cannot be folded into the Conv2D layer before it.
It looks like this:
&lt;denchmark-link:https://user-images.githubusercontent.com/61812963/94155937-16e59580-feb2-11ea-9633-d04068c62360.JPG&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='debapriyamaji' date='2020-09-25T03:46:07Z'>
		Hi &lt;denchmark-link:https://github.com/jackjhliu&gt;@jackjhliu&lt;/denchmark-link&gt;
 ,
Can u please check whether those lines are getting executed by placing a breakpoint inside this function? If possible can you share the quant configuration as well? I will have a look . I have tested it for per tensor quantization.
I can see that no fakequant layer is placed between Conv and BN. Hence,  they should have been ideally merged.
BTW, which version  of TFMOT are you using? I have checked it for 0.3 and 0.4.
Regards - Debapriya
		</comment>
		<comment id='6' author='debapriyamaji' date='2020-09-25T05:03:28Z'>
		Hi &lt;denchmark-link:https://github.com/debapriyamaji&gt;@debapriyamaji&lt;/denchmark-link&gt;

Those lines have indeed been executed.
I am using TFMOT 0.4.0 .
This is my quantization config:
&lt;denchmark-code&gt;class AsymPerLayerQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):
    # Configure how to quantize weights.
    def get_weights_and_quantizers(self, layer):
        quantizer = LastValueQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False)
        if hasattr(layer, 'kernel'):
            return [(layer.kernel, quantizer)]
        else:
            return [(layer.depthwise_kernel, quantizer)]

    # Configure how to quantize activations.
    def get_activations_and_quantizers(self, layer):
        return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]

    def set_quantize_weights(self, layer, quantize_weights):
        # Add this line for each item returned in `get_weights_and_quantizers`
        # , in the same order
        if hasattr(layer, 'kernel'):
            layer.kernel = quantize_weights[0]
        else:
            layer.depthwise_kernel = quantize_weights[0]

    def set_quantize_activations(self, layer, quantize_activations):
        # Add this line for each item returned in `get_activations_and_quantizers`
        # , in the same order.
        layer.activation = quantize_activations[0]

    # Configure how to quantize outputs (may be equivalent to activations).
    def get_output_quantizers(self, layer):
        return []

    def get_config(self):
        return {}


def apply(model):
    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer
    quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model
    quantize_scope = tfmot.quantization.keras.quantize_scope

    def clone_fn(layer):
        if type(layer) in [layers.Conv2D, layers.DepthwiseConv2D, layers.Dense]:
            print (layer.name)
            return quantize_annotate_layer(layer, quantize_config=AsymPerLayerQuantizeConfig())
        return quantize_annotate_layer(layer)

    model = quantize_annotate_model(tf.keras.models.clone_model(model, clone_function=clone_fn))

    with quantize_scope({
        'AsymPerLayerQuantizeConfig': AsymPerLayerQuantizeConfig}):
             # Use `quantize_apply` to actually make the model quantization aware.
             quant_aware_model = tfmot.quantization.keras.quantize_apply(model)

             return quant_aware_model
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='debapriyamaji' date='2020-09-28T23:57:12Z'>
		Hi &lt;denchmark-link:https://github.com/debapriyamaji&gt;@debapriyamaji&lt;/denchmark-link&gt;
,
Please refer to &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/438#issuecomment-650471700&gt;this&lt;/denchmark-link&gt;
.
QAT by default supports the TFLite quantization [spec] (&lt;denchmark-link:https://www.tensorflow.org/lite/performance/quantization_spec&gt;https://www.tensorflow.org/lite/performance/quantization_spec&lt;/denchmark-link&gt;
), which treats Conv2D layers as per-channel.
The API allows the user to play with different techniques and modifications for quantization, but does not guarantee conversion to TFLite in those cases. So this is the expected behavior.
If your change works for you, that's great. But it's not something we would support out of the box. Indeed, if you want custom quantization you should be modifying the default transforms to define your own.
		</comment>
		<comment id='8' author='debapriyamaji' date='2020-09-29T23:49:15Z'>
		&lt;denchmark-link:https://github.com/jackjhliu&gt;@jackjhliu&lt;/denchmark-link&gt;
 I'm wondering if you have managed to resolve your issue. If not, I can offer an alternative solution
		</comment>
		<comment id='9' author='debapriyamaji' date='2020-09-30T00:56:34Z'>
		Hi &lt;denchmark-link:https://github.com/LLNLanLeN&gt;@LLNLanLeN&lt;/denchmark-link&gt;

I haven't found a suitable solution. Please share your solution.
Regards,
Jack Liu
		</comment>
		<comment id='10' author='debapriyamaji' date='2020-09-30T15:23:56Z'>
		&lt;denchmark-link:https://github.com/jackjhliu&gt;@jackjhliu&lt;/denchmark-link&gt;
  I'm not sure if you have try anything like this.
But the problem stem from the fact that BatchNorm doesn't get folded after QAT. I believe Batchnorm were per-tensor support, while Conv2d were per -channel (TF 2.0 default).  Hence, when you add the configuration for Conv2D to become per-tensor QAT, it couldn't get folded properly.
The solution I'm opt for might not be the best, but it has seemed to work quite well for me and my team.
I recommend you folded the Batchnorm layer before QAT, then quantize the model. The accuracy might be lost a little bit, but this way I'm able to avoid the issue of the Batchnorm laying still exists.
Even with this way, there are still issues, for example, after QAT and converting to TFLite, we'll see multiple Quantized and Dequantized nodes. These you can remove them and you're left with a model fully per-tensor QAT that still have decent accuracy.
Unfortunately the extra step, you need to modify the tflite file a little bit, I'm not familiar with this process (I only work on the quantize while my team helped removed these layers). But it is possible to remove it by editing the tflite file
		</comment>
		<comment id='11' author='debapriyamaji' date='2020-10-01T02:39:07Z'>
		Hi &lt;denchmark-link:https://github.com/LLNLanLeN&gt;@LLNLanLeN&lt;/denchmark-link&gt;

Thanks for your advice.
These ideas give me a lot of inspiration.
I will try to follow your method.
		</comment>
		<comment id='12' author='debapriyamaji' date='2020-10-01T02:46:57Z'>
		&lt;denchmark-link:https://github.com/jackjhliu&gt;@jackjhliu&lt;/denchmark-link&gt;
  you can view this github for an idea how to fold batchnorm with convolution:
&lt;denchmark-link:https://github.com/yaysummeriscoming/Merge_Batch_Norm&gt;https://github.com/yaysummeriscoming/Merge_Batch_Norm&lt;/denchmark-link&gt;

It doesn't work directly because it's keras, but if you read over it, you can sort of understand. Please let me know how it goes. I hope you successfully get what you set out to do.
		</comment>
	</comments>
</bug>