<bug id='372' author='Craftsman381' open_date='2020-04-24T03:24:01Z' closed_time='2020-11-04T01:43:13Z'>
	<summary>Layer up_sampling2d_36:&amp;lt;class 'tensorflow.python.keras.layers.convolutional.UpSampling2D'&amp;gt; is not supported. ou can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.</summary>
	<description>
Describe the bug
I am trying to use quantize_model() to optimise UNET model, which contains UpSampling2D layer and conversion of this layer is not supported by tensorflow_model_optimization right now.
System information
MacOS Catalina
Version: 10.15.2
TensorFlow installed from (source or binary):
binary
TensorFlow version:
2.1.0
TensorFlow Model Optimization version:
0.3.0
Python version:
3.7.4
Describe the expected behavior
Successfully quantize UpSampling2D layer.
Describe the current behavior
No support right now to quantize UpSampling2D layer.
Code to reproduce the issue
Provide a reproducible code that is the bare minimum necessary to generate the
problem.
&lt;denchmark-code&gt;def unet(pretrained_weights = None,input_size = (256,256,1)):

    inputs = Input(shape=input_size)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)

    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)

    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)

    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)

    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)

    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)

    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)

    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)

    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)

    model = Model(inputs = inputs, outputs = conv10)
    model.compile(optimizer = Adam(lr = 1e-4), loss = dice_coef_loss,metrics = [dice_coef])
    
    #model.summary()

    if(pretrained_weights):
    	model.load_weights(pretrained_weights)

    return model

model = unet()
quantize_model = tfmot.quantization.keras.quantize_model
q_aware_model = quantize_model(model)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Error produced by this &lt;denchmark-link:https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/quantize.py&gt;file&lt;/denchmark-link&gt;
 line no 372.
RuntimeError: Layer up_sampling2d_40:&lt;class 'tensorflow.python.keras.layers.convolutional.UpSampling2D'&gt; is not supported. You can quantize this layer by passing a tfmot.quantization.keras.QuantizeConfig instance to the quantize_annotate_layer API.
	</description>
	<comments>
		<comment id='1' author='Craftsman381' date='2020-04-24T10:18:00Z'>
		I am facing exactly the same issue today while trying to implement unet for Coral Edge_TPU.
Following are the findings-

I tried using the Conv2DTranspose instead of upsampling but it looks like Conv2DTranspose is also not supported.
 Layer conv2Dtranspose:&lt;class 'tensorflow.python.keras.layers.convolutional.Conv2DTranspose'&gt; is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.
Most of the official example quantized models of Edge TPU  do not need upscaling of the image. Their output is either a class or a number of bounding boxes with the class.
Although there are examples of models Deeplabv3 based quantized edgetpu segmentation, there is no help regarding the architecture and how the upscaling has been solved.
Similar unresolved yet closed old issue..
I have also posted a StackOverflow question for the same.
https://stackoverflow.com/questions/61406595/layer-up-sampling2dclass-tensorflow-python-keras-layers-convolutional-upsampl

Any help in this regard will be greatly appreciated.
		</comment>
		<comment id='2' author='Craftsman381' date='2020-04-24T16:43:10Z'>
		&lt;denchmark-link:https://github.com/suharshs&gt;@suharshs&lt;/denchmark-link&gt;
 Could you help take a look at this issue?
		</comment>
		<comment id='3' author='Craftsman381' date='2020-04-24T18:50:23Z'>
		&lt;denchmark-link:https://github.com/Mohit-Ak&gt;@Mohit-Ak&lt;/denchmark-link&gt;
 These optimization techniques are new in TF 2. You should try  API still if it not work for you then you should try tf-nightly. If this support is not present in tf-nightly too then you have to open this issue and they will add support in next release.
		</comment>
		<comment id='4' author='Craftsman381' date='2020-04-24T23:14:36Z'>
		
@Mohit-Ak These optimization techniques are new in TF 2. You should try quantize_annotate_layer API still if it not work for you then you should try tf-nightly. If this support is not present in tf-nightly too then you have to open this issue and they will add support in next release.


I am using TF-nightly 2.2.0-dev20200423.
Although I have not tried quantize_annotate_layer, I assume, If I quantize all the layers using "quantize_annotate_layer " except for the UpSampling2D/Conv2DTranspose layers, then the edgetpu-compiler would throw an error stating "model not quantized".
I had tried this earlier, where I only quantized a bunch of layers and not the whole model and fed it into the edgetpu-compiler to get the "model not quantized" error.

UPDATE
I tried using the resize_images function for instead of UpSampling2D but gave me the same error.
&lt;denchmark-code&gt;def resize_image(inp,  s, data_format):
    try:
        return Lambda(lambda x: keras.backend.resize_images(x,
                                                height_factor=s[0],
                                                width_factor=s[1],
                                                data_format=data_format,
                                                interpolation='bilinear'))(inp)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='Craftsman381' date='2020-04-28T07:01:09Z'>
		Hi &lt;denchmark-link:https://github.com/Mohit-Ak&gt;@Mohit-Ak&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Craftsman381&gt;@Craftsman381&lt;/denchmark-link&gt;
,
Thanks for the great feedback. We haven't added support for Upsampling2D and Conv2DTranspose yet. We are ramping up support for layers based on feedback from users. So thank you for that. Most of these layers are quite simple, and haven't been added due to conversion support.
 doesn't actually need any QAT handling. You can try passing a &lt;denchmark-link:https://github.com/tensorflow/model-optimization/blob/d328a642d74de85ac8d4213dfe122b94a7cbc554/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_configs.py#L44&gt;NoOpQuantizeConfig&lt;/denchmark-link&gt;
 to it as the  and the training should just work. Let us know if the conversion has issues.
As for Conv2DTranspose, we still have to add support. But we can now prioritize it with this feedback. Thanks!
		</comment>
		<comment id='6' author='Craftsman381' date='2020-04-30T21:12:22Z'>
		&lt;denchmark-link:https://github.com/Craftsman381&gt;@Craftsman381&lt;/denchmark-link&gt;
: FYI if your goal is to run UpSampling2D eventually in TFLite, the TFLite converter may not be able to convert it yet. You'll need to see if someone has filed an issue on github.com/tensorflow/tensorflow and request for it.
In general, the comments from testQuantizeSingleLayer_ProducesFullIntegerModel_TF2 in &lt;denchmark-link:https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/quantize_functional_test.py#L266&gt;this test file&lt;/denchmark-link&gt;
 suggest why they have not been added yet in the Keras QAT tool. UpSampling2D is under the comment "# Not done since TFLite converter doesn't support in TF2 yet", though things may have changed since so it's worth doing a quick test first.
		</comment>
		<comment id='7' author='Craftsman381' date='2020-05-14T10:36:05Z'>
		
Hi @Mohit-Ak @Craftsman381,
Thanks for the great feedback. We haven't added support for Upsampling2D and Conv2DTranspose yet. We are ramping up support for layers based on feedback from users. So thank you for that. Most of these layers are quite simple, and haven't been added due to conversion support.
Upsampling2D doesn't actually need any QAT handling. You can try passing a NoOpQuantizeConfig to it as the QuantizeConfig and the training should just work. Let us know if the conversion has issues.
As for Conv2DTranspose, we still have to add support. But we can now prioritize it with this feedback. Thanks!

tfmot.quantization.keras.QuantizeConfig` instance.You passed an instance of type: ABCMeta.
tfmot.quantization.keras.quantize_annotate_layer(UpSampling2D(interpolation=interpolation), NoOpQuantizeConfig)(y)
		</comment>
		<comment id='8' author='Craftsman381' date='2020-05-16T11:55:37Z'>
		Same point that &lt;denchmark-link:https://github.com/mlinxiang&gt;@mlinxiang&lt;/denchmark-link&gt;
 here.
&lt;denchmark-code&gt;import tensorflow_model_optimization as tfmot
from tensorflow_model_optimization.python.core.quantization.keras.default_8bit import default_8bit_quantize_configs
...
u = tfmot.quantization.keras.quantize_annotate_layer(
                UpSampling2D(size=size_divider), quantize_config=default_8bit_quantize_configs.NoOpQuantizeConfig) (u)
&lt;/denchmark-code&gt;

And get the same error:
quantize_config can only be a tfmot.quantization.keras.QuantizeConfig instance.You passed an instance of type: ABCMeta.
		</comment>
		<comment id='9' author='Craftsman381' date='2020-05-16T19:42:54Z'>
		Finally I get it working with a workaround:
&lt;denchmark-code&gt;import tensorflow_model_optimization as tfmot
from tensorflow_model_optimization.python.core.quantization.keras import quantize_config

###   Clone NoOpQuantizeConfig locally   ###
class NoOpQuantizeConfig(quantize_config.QuantizeConfig):
    """QuantizeConfig which does not quantize any part of the layer."""
    def get_weights_and_quantizers(self, layer):
        return []
    def get_activations_and_quantizers(self, layer):
        return []
    def set_quantize_weights(self, layer, quantize_weights):
        pass
    def set_quantize_activations(self, layer, quantize_activations):
        pass
    def get_output_quantizers(self, layer):
        return []
    def get_config(self):
        return {}

quantize_config = NoOpQuantizeConfig()

...

###   Wrap your upsampling layer with this   ###
u = tfmot.quantization.keras.quantize_annotate_layer(
                UpSampling2D(size=size_divider), quantize_config=quantize_config) (layer_input)

...

###     And then just compile and use   ###
with tf.keras.utils.custom_object_scope({'NoOpQuantizeConfig': NoOpQuantizeConfig}):
            quantized_model = tfmot.quantization.keras.quantize_model(generate_model())
            quantized_model.summary()
&lt;/denchmark-code&gt;

In my case it is a functional Model, but I'm pretty sure it also works with Sequentials.
If everything goes right, you can see in your summary every layer with the "quant_" prefix.
		</comment>
		<comment id='10' author='Craftsman381' date='2020-06-02T20:04:43Z'>
		&lt;denchmark-link:https://github.com/ianholing&gt;@ianholing&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mlinxiang&gt;@mlinxiang&lt;/denchmark-link&gt;
 - As long as the object you pass subclasses , it should work.
&lt;denchmark-link:https://github.com/ianholing&gt;@ianholing&lt;/denchmark-link&gt;
 - In your sample code, consider using . It also adds all the other quant related objects in scope.
		</comment>
		<comment id='11' author='Craftsman381' date='2020-06-10T19:30:44Z'>
		&lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ianholing&gt;@ianholing&lt;/denchmark-link&gt;
  I found another way to work around the quantized up-sampling problem.   In my case, I switched from UpSampling2D() to tf.image.resize(), because the former had a runtime issue with the resulting FP32 TFLite model using the TFLite GPU Delegate.  Regardless, since none of these up-sampling ops (UpSampling2D(), tf.image.resize(), Conv2DTranspose()) seem to be supported by QAT, this workaround may work for all of the above:
&lt;denchmark-code&gt;import tensorflow_model_optimization as tfmot

def annotate(layer):
    if layer._name.startswith('tf_op_layer_ResizeBilinear'):
        return layer   # pass thru; don't quantize tf.image.resize()
    # quantize everything else
    return tfmot.quantization.keras.quantize_annotate_layer(layer)

annotated_model = tf.keras.models.clone_model(model, clone_function=annotate)

quantized_model = tfmot.quantization.keras.quantize_apply(annotated_model)
quantized_model.summary()
&lt;/denchmark-code&gt;

However, this workaround is sub-optimal.   Yes, this allows me to create a QAT INT8 TFLite model.  But because the up-sampling operation is not quantized, if I run this TFLite model on a Qualcomm SOC DSP using the Hexagon DSP Delegate, everything after the up-sampling operation ("RESIZE_BILINEAR" in my case below, inside a BiFPN) falls back to running on CPU.   Excerpt from a benchmark:
&lt;denchmark-code&gt;msm8996:/data/local/tmp # taskset 0f  ./benchmark_model \
     --use_hexagon=true \
     --input_layer=images \
     --input_layer_shape=1,224,224,1 \
     --graph=test-bifpn-qat-int8.tflite
...
...
Number of nodes executed: 46
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	   TfLiteHexagonDelegate	        4	    15.281	    91.811%	    91.811%	     0.000	        4
	         RESIZE_BILINEAR	        6	     0.530	     3.184%	    94.995%	     0.000	        6
	       DEPTHWISE_CONV_2D	        7	     0.337	     2.025%	    97.020%	     0.000	        7
	                     ADD	       12	     0.308	     1.851%	    98.870%	     0.000	       12
	                QUANTIZE	        4	     0.064	     0.385%	    99.255%	     0.000	        4
	              DEQUANTIZE	       10	     0.064	     0.385%	    99.640%	     0.000	       10
	             MAX_POOL_2D	        3	     0.060	     0.360%	   100.000%	     0.000	        3

Timings (microseconds): count=59 first=16822 curr=16583 min=16495 max=16833 avg=16662.9 std=78
Memory (bytes): count=0
46 nodes observed
&lt;/denchmark-code&gt;

So my question is:  What is the current recommendation for a quantization friendly up-sampling operation (e.g. in a BiFPN, UNet, etc) for TF2 Keras?
Thanks!
		</comment>
		<comment id='12' author='Craftsman381' date='2020-06-14T07:47:35Z'>
		&lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/alanchiao&gt;@alanchiao&lt;/denchmark-link&gt;
 Looks like the TFLite Hexagon DSP Delegate already supports these quantized up-sampling ops:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/README.md&lt;/denchmark-link&gt;


Resize Bilinear
Resize Nearest Neighbor
TransposeConv2D

But I believe none of the above are currently supported by TF2 TFMOT model quantizer.  Please correct me if I'm wrong.  Then how can one generate a TFLite model that uses these quantized up-sampling operations?
Thanks
		</comment>
		<comment id='13' author='Craftsman381' date='2020-06-15T19:54:51Z'>
		Inspecting Google's &lt;denchmark-link:https://github.com/google-coral/edgetpu/blob/master/test_data/deeplabv3_mnv2_pascal_quant_edgetpu.tflite&gt;quantized EdgeTPU DeepLab v3 segmentation model&lt;/denchmark-link&gt;
 that &lt;denchmark-link:https://github.com/Mohit-Ak&gt;@Mohit-Ak&lt;/denchmark-link&gt;
 mentioned in this thread, looks like the model has a custom MobileNet v2 backbone plus a few ResizeBilinear ops to up-scale the tensors, among other things.   Diagram below.
How does one use TFMOT to map to quantization-friendly up-scaling ops like the ones used in this EdgeTPU model?
Thanks
&lt;denchmark-link:https://user-images.githubusercontent.com/61851501/84698752-33fae780-af05-11ea-933a-ab50e1d08820.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='Craftsman381' date='2020-06-15T22:20:10Z'>
		&lt;denchmark-link:https://github.com/holokai-ai&gt;@holokai-ai&lt;/denchmark-link&gt;

How did you wrap 'tf.image.resize()' in the Keras model? Thanks
		</comment>
		<comment id='15' author='Craftsman381' date='2020-06-15T22:22:29Z'>
		
Finally I get it working with a workaround:
import tensorflow_model_optimization as tfmot
from tensorflow_model_optimization.python.core.quantization.keras import quantize_config

###   Clone NoOpQuantizeConfig locally   ###
class NoOpQuantizeConfig(quantize_config.QuantizeConfig):
    """QuantizeConfig which does not quantize any part of the layer."""
    def get_weights_and_quantizers(self, layer):
        return []
    def get_activations_and_quantizers(self, layer):
        return []
    def set_quantize_weights(self, layer, quantize_weights):
        pass
    def set_quantize_activations(self, layer, quantize_activations):
        pass
    def get_output_quantizers(self, layer):
        return []
    def get_config(self):
        return {}

quantize_config = NoOpQuantizeConfig()

...

###   Wrap your upsampling layer with this   ###
u = tfmot.quantization.keras.quantize_annotate_layer(
                UpSampling2D(size=size_divider), quantize_config=quantize_config) (layer_input)

...

###     And then just compile and use   ###
with tf.keras.utils.custom_object_scope({'NoOpQuantizeConfig': NoOpQuantizeConfig}):
            quantized_model = tfmot.quantization.keras.quantize_model(generate_model())
            quantized_model.summary()

In my case it is a functional Model, but I'm pretty sure it also works with Sequentials.
If everything goes right, you can see in your summary every layer with the "quant_" prefix.

I've tried your solution, but I had this error 'ValueError: Unknown object: NoOpQuantizeConfig'
		</comment>
		<comment id='16' author='Craftsman381' date='2020-06-16T02:01:46Z'>
		
@holokai-ai
How did you wrap 'tf.image.resize()' in the Keras model? Thanks

&lt;denchmark-link:https://github.com/EscVM&gt;@EscVM&lt;/denchmark-link&gt;
  -- I'm not wrapping tf.image.resize() at the moment; I'm simply calling it intermixed with Keras layers, in a functional style.
		</comment>
		<comment id='17' author='Craftsman381' date='2020-06-16T08:23:14Z'>
		

@holokai-ai
How did you wrap 'tf.image.resize()' in the Keras model? Thanks

@EscVM -- I'm not wrapping tf.image.resize() at the moment; I'm simply calling it intermixed with Keras layers, in a functional style.

Ah ok! Indeed, it works. Thanks
		</comment>
		<comment id='18' author='Craftsman381' date='2020-06-16T14:27:34Z'>
		&lt;denchmark-link:https://github.com/holokai-ai&gt;@holokai-ai&lt;/denchmark-link&gt;

The problem is that if you need to use the Edge TPU compiler 'tf.image.resize()' is not compatible :)
		</comment>
		<comment id='19' author='Craftsman381' date='2020-06-16T19:13:39Z'>
		
@holokai-ai
The problem is that if you need to use the Edge TPU compiler 'tf.image.resize()' is not compatible :)

That's precisely why I posed the question in this thread to &lt;denchmark-link:https://github.com/alanchiao&gt;@alanchiao&lt;/denchmark-link&gt;

and &lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
 ---
For those of us who need to do edge inference on hardware that requires integer-only ops like EdgeTPU and Hexagon DSP, how can we map to quantization-friendly TFLite up-sampling ops?
Clearly Google has a way to do this (e.g. with the quantized ResizeBilinear in DeepLab v3 segmentation model running on EdgeTPU).
		</comment>
		<comment id='20' author='Craftsman381' date='2020-06-16T20:43:26Z'>
		


@holokai-ai
The problem is that if you need to use the Edge TPU compiler 'tf.image.resize()' is not compatible :)

That's precisely why I posed the question in this thread to @alanchiao
and @nutsiepully ---
For those of us who need to do edge inference on hardware that requires integer-only ops like EdgeTPU and Hexagon DSP, how can we map to quantization-friendly TFLite up-sampling ops?
Clearly Google has a way to do this (e.g. with the quantized ResizeBilinear in DeepLab v3 segmentation model running on EdgeTPU).

On this purpose it would be useful to have 'depth_to_space' :))
Anyway, about bilinear, I was thinking of training with your method and then make a little bit of surgery and substitute 'tf.image.resize()' with the 'UpSampling2D' Keras layer. I theory should work because bilinear doesn't make use of variables.

&lt;denchmark-link:https://github.com/alanchiao&gt;@alanchiao&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
  --  FWIW I tried the following hack:

reverted back to UpSampling2D() instead of tf.image.resize()
for UpSampling2D(), tired both interpolation="nearest" and interpolation="bilinear"
hacked one line in the TFMOT code to accept UpSampling2D():

&lt;denchmark-code&gt;holokai@deckard:~/model-optimization$ git diff
diff --git a/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py b/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py
index 433128f..d8bc1a1 100644
--- a/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py
+++ b/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantize_registry.py
@@ -97,7 +97,7 @@ class QuantizeRegistry(quantize_registry.QuantizeRegistry, _RNNHelper):
       _no_quantize(layers.Cropping2D),
       _no_quantize(layers.Cropping3D),
       # _no_quantize(layers.UpSampling1D),
-      # _no_quantize(layers.UpSampling2D),
+      _no_quantize(layers.UpSampling2D),
       # _no_quantize(layers.UpSampling3D),
       _no_quantize(layers.ZeroPadding1D),
       _no_quantize(layers.ZeroPadding2D),
&lt;/denchmark-code&gt;

The resulting model trained well on our task in Python TensorFlow, both for FP32 (96.1%) and INT8 QAT (94.8%).
However when I benchmarked the resulting TFLite model on our hardware, it blew up, so something went terribly bad in the TFLite conversion process:
&lt;denchmark-code&gt;msm8996:/data/local/tmp # taskset 0f ./benchmark_model \
    --use_hexagon=true \
    --input_layer=images \
    --input_layer_shape=1,224,224,1 \
    --graph=test-bifpn-qat-int8.tflite

...
...
Loaded model test-bifpn-qat-int8.tflite
INFO: Initialized TensorFlow Lite runtime.
loaded libadsprpc.so
INFO: Created TensorFlow Lite delegate for Hexagon.
INFO: Hexagon delegate: 78 nodes delegated out of 138 nodes with 4 partitions.

Applied Hexagon delegate, and the model graph will be partially executed w/ the delegate.
The input model file size (MB): 3.64997
Initialized session in 384.2ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
----------------
Timestamp: Wed Jan 21 02:10:46 1970


Log
hexagon/ops/src/op_input.c:153:out 1 too small (128 &lt; 50176)
hexagon/src/execute.c:142:execute() failed on node id=1 err=-1
hexagon/src/interface.c:1270:fail in execute_inner()

----------------
ERROR: Failed: Failed to execute graph.. STATE: FAILED_TO_EXECUTE_GRAPH
ERROR: Node number 140 (TfLiteHexagonDelegate) failed to invoke.

----------------
&lt;Lots of repeated errors above&gt;
...
...

count=16 first=48883 curr=29973 min=29841 max=48883 avg=31238.4 std=4558

Benchmarking failed.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='Craftsman381' date='2020-06-23T08:16:51Z'>
		&lt;denchmark-link:https://github.com/mlinxiang&gt;@mlinxiang&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/ianholing&gt;@ianholing&lt;/denchmark-link&gt;
 - Your code was failing since you were using  which is a class, and not  which is the object.
In
&lt;denchmark-code&gt;import tensorflow_model_optimization as tfmot
from tensorflow_model_optimization.python.core.quantization.keras.default_8bit import default_8bit_quantize_configs
...
u = tfmot.quantization.keras.quantize_annotate_layer(
                UpSampling2D(size=size_divider), quantize_config=default_8bit_quantize_configs.NoOpQuantizeConfig) (u)
&lt;/denchmark-code&gt;

you should replace NoOpQuantizeConfig with NoOpQuantizeConfig().
		</comment>
		<comment id='22' author='Craftsman381' date='2020-06-23T09:17:55Z'>
		Hi &lt;denchmark-link:https://github.com/holokai-ai&gt;@holokai-ai&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/EscVM&gt;@EscVM&lt;/denchmark-link&gt;
,
Thanks for the detailed feedback and report.
We pulled out UpSampling2D support prior to launch since we hadn't tested it end to end  (including conversion) and wanted to ensure numerical accuracy before releasing it. In the meanwhile, the best way to work around it is to use the approach both of you have used so far - which is providing a custom QuantizeConfig for the layer.
If you provide a config which is correct, and conversion is supported the the code should work and the TFLite model should just run.
For nearest neighbor interpolation, NoOpQuantizeConfig is the correct solution. And the patch above which uncomments the line is enough. For resize bilinear, there needs to be additional work to match the TFL kernel implementation correctly.
I'll be taking some time out to work on it. If you are interested in contributing a patch, I'm happy to help :)
&lt;denchmark-link:https://github.com/holokai-ai&gt;@holokai-ai&lt;/denchmark-link&gt;
, it's not clear to me why you are having trouble on your hardware. I was able to run a QAT model with  locally with TFLite interpreter running on CPU. Perhaps, the delegate code isn't handling ResizeBilinear correctly.
		</comment>
		<comment id='23' author='Craftsman381' date='2020-06-25T19:28:23Z'>
		
I'll be taking some time out to work on it. If you are interested in contributing a patch, I'm happy to help :)
@holokai-ai, it's not clear to me why you are having trouble on your hardware. I was able to run a QAT model with UpSampling2D locally with TFLite interpreter running on CPU. Perhaps, the delegate code isn't handling ResizeBilinear correctly.

&lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
 Thank you very much for taking the time to look into supporting quantized up-sampling ops!   We need those ops to run modern detection and segmentation networks, and we need them quantized for edge hardware that require integer-only ops.
I would imagine that Nearest Neighbor interpolation would be the easiest to support, because I believe there's no quantization involved; just copying bytes.
Yes, the fact that the QAT model with UpSampling2D() works on CPU, but not DSP, might point to an issue with the DSP Delegate.    However, I did mention earlier in this thread that I had a QAT model running with tf.image.resize(), and the DSP Delegate gracefully handled that non-quantized op that by falling back to CPU, and not crashing like UpSampling2D().   So it may still be an issue with the conversion process to TFLite.
		</comment>
		<comment id='24' author='Craftsman381' date='2020-06-27T01:49:30Z'>
		It's possible there might be an issue.
Once I get the time, I'll add support for UpSampling2D to ensure it can convert to TFLite, and ensure numerically stable execution on TFLite. Once that is in place, you can re-run the code and try the DSP delegate.
If there's still an issue, we can dig in further.
		</comment>
		<comment id='25' author='Craftsman381' date='2020-07-08T08:57:13Z'>
		Thanks for the great discussion, everyone! This issue has been helpful for my debugging!

As for Conv2DTranspose, we still have to add support. But we can now prioritize it with this feedback. Thanks!

&lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
 ,  would be super useful to me as well (for the same reasons as stated above by &lt;denchmark-link:https://github.com/Mohit-Ak&gt;@Mohit-Ak&lt;/denchmark-link&gt;
). Is there an ETA for adding QAT support? I believe it is already possible to quantize with TFLite and it is supported on EdgeTPU. I would be happy to help with this, but I would need some guidance. Would it be helpful to make a separate feature request issue for Conv2DTranspose QAT support?

The conversion works, but at the edge tpu complier says that the model is not quantized:

&lt;denchmark-link:https://github.com/EscVM&gt;@EscVM&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/38285&gt;this issue&lt;/denchmark-link&gt;
 discusses setting the input/output type when converting to an integer quantized TFLite model. While it appears that it doesn't currently support QAT, this comment on converting the input/output to or  might help solve that: &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/431#issuecomment-646109974&gt;#431 (comment)&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='26' author='Craftsman381' date='2020-07-13T14:14:20Z'>
		
Hi @holokai-ai , @EscVM,
Thanks for the detailed feedback and report.
We pulled out UpSampling2D support prior to launch since we hadn't tested it end to end (including conversion) and wanted to ensure numerical accuracy before releasing it. In the meanwhile, the best way to work around it is to use the approach both of you have used so far - which is providing a custom QuantizeConfig for the layer.
If you provide a config which is correct, and conversion is supported the the code should work and the TFLite model should just run.
For nearest neighbor interpolation, NoOpQuantizeConfig is the correct solution. And the patch above which uncomments the line is enough. For resize bilinear, there needs to be additional work to match the TFL kernel implementation correctly.
I'll be taking some time out to work on it. If you are interested in contributing a patch, I'm happy to help :)
@holokai-ai, it's not clear to me why you are having trouble on your hardware. I was able to run a QAT model with UpSampling2D locally with TFLite interpreter running on CPU. Perhaps, the delegate code isn't handling ResizeBilinear correctly.

Thank you &lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
 for your help. Can you provide an example of a possible custom  for the UpSampling2D layer?
		</comment>
		<comment id='27' author='Craftsman381' date='2020-08-15T02:45:37Z'>
		Did anyone find a solution for this? I have the same problem.
		</comment>
		<comment id='28' author='Craftsman381' date='2020-08-19T09:50:22Z'>
		I would also like to quantize the UpSampling2D layer. &lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
 Any updates on this?
		</comment>
		<comment id='29' author='Craftsman381' date='2020-09-29T14:31:48Z'>
		I've run into this problem as well. I believe this is the only layer stopping QAT for my detection model. Any updates?
		</comment>
		<comment id='30' author='Craftsman381' date='2020-10-14T13:11:03Z'>
		I met the same problem
		</comment>
		<comment id='31' author='Craftsman381' date='2020-10-20T11:45:04Z'>
		I met the same problem
		</comment>
		<comment id='32' author='Craftsman381' date='2020-11-24T12:53:03Z'>
		Is there any update on the Conv2DTranspose QAT support?
I'm working with version 2.5.0-dev20201123, and getting this error:
RuntimeError: Layer decoder_stage0a_transpose:&lt;class 'tensorflow.python.keras.layers.convolutional.Conv2DTranspose'&gt; is not supported. You can quantize this layer by passing a tfmot.quantization.keras.QuantizeConfig instance to the quantize_annotate_layer API.
		</comment>
	</comments>
</bug>