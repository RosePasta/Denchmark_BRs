<bug id='430' author='joyalbin' open_date='2020-06-18T06:35:56Z' closed_time='2020-06-18T15:33:44Z'>
	<summary>4bit activation accuracy is very low</summary>
	<description>
Hi,
While doing some experiments with the quantization aware training, I have observed the 4-bit weight and activation quantization accuracy on Mobilenet is very low.
Anyone can please help me to get some details on this?
	</description>
	<comments>
		<comment id='1' author='joyalbin' date='2020-06-18T15:33:44Z'>
		Hi, 4bit quantization can get very large amounts of quantization error. There are hard to resolve without model level changes. In general an arbitrarily low-level bitwidth isn't guaranteed to work with any model.
Some folks have had luck by incrementally lowering the bitwidth of weights from float to 16bit to 8bit to 4bit etc. Or keeping weights in 8bit and only quantizing 4bits. But this is really a point of open research.
Thanks!
		</comment>
	</comments>
</bug>