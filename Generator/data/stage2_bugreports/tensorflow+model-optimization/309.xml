<bug id='309' author='kmkolasinski' open_date='2020-03-20T19:24:50Z' closed_time='2020-05-05T07:06:07Z'>
	<summary>Quantization: QuantizeModelsTest.testModelEndToEnd() function does not check the correctness of quantization process</summary>
	<description>
Describe the bug
Hi, I'm trying to understand  the model optimization API and create quantized version of
the MobileNetV2 model for my side project (a small library for training custom detection models for mobile).  However,  I'm struggling to get any positive results using this library compared to old approach when I was using converter.representative_dataset for model conversion.
I found that there are some end-to-end tests in your repository in the
tensorflow_model_optimization.python.core.quantization.keras.quantize_models_test.py file,
however it seems these tests do not check whether the conversion actually makes sense or not. I mean the conversion is successful but it is so inaccurate that cannot be used in practice.
If you will check the predictions outputs of converted model you will see it produces only zeros (I get similar behavior when training on real data for much longer time).
The picture below shows the min/max and std values of the output of the converted MobileNetV2 model from your end to end test:
&lt;denchmark-link:https://user-images.githubusercontent.com/10145864/77197241-22ac6580-6ae5-11ea-9824-2cfccd7e95ea.png&gt;&lt;/denchmark-link&gt;

When I initialize this model with imagenet weights I still get different values at the outputs between keras quantized and converted to tflite model:
&lt;denchmark-link:https://user-images.githubusercontent.com/10145864/77197846-2ab8d500-6ae6-11ea-9bcb-6206ea4baf52.png&gt;&lt;/denchmark-link&gt;

What are your general suggestions for checking for possible sources of invalid conversion?
For example, I'm aware that I should finetune my quantized model for a couple of epochs, however I'm not sure how long or if this actually matters. Are batchnorm layers supported (I saw that composition of ConvBatchNormRelu is implemented).
In general (or maybe in near future), should I expect to get similar accuracy of quantized Keras model and converted to tflite one ?
Thanks,
Krzysztof
System information
TensorFlow installed from (source or binary): binary
TensorFlow version: tf-nightly==2.2.0.dev20200316
TensorFlow Model Optimization version: tf-model-optimization-nightly==0.2.1.dev20200320
(compiled from source code, from master branch)
Python version: 3.7.0
Code to reproduce the issue
This is slightly modified version of the _verify_tflite function from your tests, which I used to check the outputs of the converted models.
def _verify_tflite(tflite_file, x_test, y_test, model):
    interpreter = tf.lite.Interpreter(model_path=tflite_file)
    interpreter.allocate_tensors()
    input_index = interpreter.get_input_details()[0]['index']
    output_index = interpreter.get_output_details()[0]['index']
    
    keras_predictions = model.predict(x_test)
    tflite_predictions = []
    for x, _ in zip(x_test, y_test):
      x = x.reshape((1,) + x.shape)
      interpreter.set_tensor(input_index, x)
      interpreter.invoke()
      outputs = interpreter.get_tensor(output_index)
      tflite_predictions.append(outputs)

    return np.vstack(tflite_predictions), keras_predictions

tflite_predictions, keras_predictions = _verify_tflite(tflite_file, x_train, y_train, base_model)

print(tflite_predictions.min(), tflite_predictions.max(), tflite_predictions.std())
print(keras_predictions.min(), keras_predictions.max(), keras_predictions.std())
	</description>
	<comments>
		<comment id='1' author='kmkolasinski' date='2020-03-22T07:53:14Z'>
		Hi, some additional feedback and questions from my side. You are using MovingAverageQuantizer for quantizing activations with ema_decay=0.999, which means that I need to finetune my quantized model for more than few thousands steps until ema converge.
My question is, should I wait until ema converges or this is not necessary ? Let say, my quantized Keras model has 90% accuracy on evaluation dataset, however the same model converted to tflite has 0% accuracy. Can this be related with MovingAverageQuantizer ?
		</comment>
		<comment id='2' author='kmkolasinski' date='2020-04-03T01:01:52Z'>
		Hi &lt;denchmark-link:https://github.com/kmkolasinski&gt;@kmkolasinski&lt;/denchmark-link&gt;
,
Once you apply quantization to a model during training, the accuracy drops heavily in the beginning, but it will recover quite quickly after that if you are starting with pre-trained weights. And yes, part of the reason the drop is so huge is the EMA convergence.
The starting min/max ranges are chosen conservatively as -6, 6. That's a pretty wide range, and may lead to very high quantization loss in the beginning (say if all your values are in [-0.2, 0.2]). This can cascade over layers and lead to zeros.
However, the training recovers pretty quickly. We've tested internally with this on various models, including MobileNet v2 and managed to recover accuracy to near float levels. So, please try training with this for a few epochs and then verify the results. The time taken is different for various models.
The accuracy value of QAT trained TF models and associated TFLite models should be very similar. If you encounter a situation with a large TF-&gt;TFLite gap, please let us know. That would be a bug.
Also, please note that the default imagenet weights (and model) are optimized for inference, not training. They lack regularization, dropout etc. For QAT, it's better to start with the original trained model + weights.
		</comment>
		<comment id='3' author='kmkolasinski' date='2020-04-03T05:57:54Z'>
		Hi, thank you for the detailed answer. I see the gap between TF and TFLite models in my library. I will try to make a colab notebook which shows exactly what I do.  I can confirm that after training quantized models a bit longer than previously I managed to reduce the gap, but still it is quite visible. I wonder how this can be related to the fact that I'm training detection models, where I need to access predictions at any feature map location (I don't use average pool), so maybe the distribution of prediction values is multimodal/clustered (I just speculate here that maybe detection models are harder to quantize and it's not a bug). I still need to perform some tests with L2 regularization and try to disable quantization in most fragile places in the network.
		</comment>
		<comment id='4' author='kmkolasinski' date='2020-04-03T21:16:14Z'>
		Please share the code to reproduce if you do find a gap.
Regarding training, are you talking about TF-TFLite gap or TF-QAT TF gap. The latter can vary with different models.
		</comment>
		<comment id='5' author='kmkolasinski' date='2020-04-04T07:31:04Z'>
		I mean the gap between Keras model which has been quantized (TF-QAT) and then converted to TFLite version.
		</comment>
		<comment id='6' author='kmkolasinski' date='2020-04-04T09:51:02Z'>
		Please file a bug in that case. That should not happen.
		</comment>
		<comment id='7' author='kmkolasinski' date='2020-04-04T15:33:52Z'>
		Ok, last few comments. I believe this is not a bug, since this behavior seems to depend on many factors, like input image resolution, dataset type, training time etc.  I wanted to make a demo notebook to show this issue on example of SVHN detection dataset. However, it seems that for SVHN case it does convert to TFLite quite well ...
Here is my notebook if you are interested: &lt;denchmark-link:https://github.com/kmkolasinski/keras_detection/blob/master/notebooks/fpn_builder_svhn.ipynb&gt;https://github.com/kmkolasinski/keras_detection/blob/master/notebooks/fpn_builder_svhn.ipynb&lt;/denchmark-link&gt;

In the last few cells of this notebook I compare the outputs from three models:

Keras quantized (a reference model)
TFLite obtained from Keras model (with default optimizations)
TFLite obtained from Keras model, but with additional support of representative dataset.

The later seems to work the best in my case. I should compare these models on whole test set, however the runtime of TFLite models is really bad on x64 architectures and it takes ages.
Lastly, I have tested the same code on my company datasets and there were actually problems. The main difference for these datasets was that I had to use larger input image e.g. 512x512 instead of 128x128.
		</comment>
		<comment id='8' author='kmkolasinski' date='2020-04-28T10:34:55Z'>
		Hi &lt;denchmark-link:https://github.com/kmkolasinski&gt;@kmkolasinski&lt;/denchmark-link&gt;
,
Can you give me access to your notebook? It throws an authorization error if I try to open it.
		</comment>
		<comment id='9' author='kmkolasinski' date='2020-04-28T10:52:40Z'>
		Hi, I'm not sure if it will work anymore I have pushed few changes to the master recently. I will check it first and learn how to share Colab notebooks properly.
		</comment>
		<comment id='10' author='kmkolasinski' date='2020-04-28T18:27:05Z'>
		Hi, I'm testing my notebook, however it takes a few hour to train the model. I'm not sure whether this is a good example to show the issue. You could try to take MobileNetV2 and train it on random inputs for few thousands steps and check the conversion of such model.
		</comment>
		<comment id='11' author='kmkolasinski' date='2020-04-29T05:56:59Z'>
		That's ok. Just wanted a way to quickly reproduce the issue.
		</comment>
		<comment id='12' author='kmkolasinski' date='2020-04-29T06:25:28Z'>
		I will try to prepare some toy model with minimal number of dependencies. This SVHN example is not the best one to debug. Do you want to see a gap between Keras QAT model and QAT TFLite model ?   Or you want to see how the gap decreases with the number of training steps ?
		</comment>
		<comment id='13' author='kmkolasinski' date='2020-04-29T06:46:20Z'>
		Thanks a lot.
Having both is better. But simply seeing the gap between Keras QAT and QAT TFLite for a small model is also quite useful.
		</comment>
		<comment id='14' author='kmkolasinski' date='2020-04-29T06:51:25Z'>
		Ok, I will need few days for this, however I'm worry that you will not find the gap in small models.
		</comment>
		<comment id='15' author='kmkolasinski' date='2020-04-29T06:56:41Z'>
		Sure, take your time. Thanks.
That's ok. Doesn't need to be small. Any model that you can reproduce the issue on is enough for me.
		</comment>
		<comment id='16' author='kmkolasinski' date='2020-04-30T09:15:38Z'>
		&lt;denchmark-link:https://github.com/nutsiepully&gt;@nutsiepully&lt;/denchmark-link&gt;
, I can confirm similar behavior and here's a &lt;denchmark-link:https://colab.research.google.com/gist/sayakpaul/6a042b3ba74f77ef8b381ef93e701f84/qat-bad-accuracy.ipynb&gt;Colab notebook&lt;/denchmark-link&gt;
 that reproduces it. Accuracies in the QAT Keras model and TF Lite converted version differ from each other by a large margin.
		</comment>
		<comment id='17' author='kmkolasinski' date='2020-04-30T09:52:06Z'>
		&lt;denchmark-link:https://github.com/sayakpaul&gt;@sayakpaul&lt;/denchmark-link&gt;
  Could you make this Colab notebook public (it says that I need permissions)? I would like to see how do you reproduce this issue.
		</comment>
		<comment id='18' author='kmkolasinski' date='2020-04-30T09:57:12Z'>
		Thanks for noticing it &lt;denchmark-link:https://github.com/kmkolasinski&gt;@kmkolasinski&lt;/denchmark-link&gt;
. Just fixed it.
		</comment>
		<comment id='19' author='kmkolasinski' date='2020-05-05T07:06:07Z'>
		This is a duplicate of &lt;denchmark-link:https://github.com/tensorflow/model-optimization/issues/368&gt;this&lt;/denchmark-link&gt;
 bug. Closing this, and following up there.
		</comment>
	</comments>
</bug>