<bug id='608' author='j-o-d-o' open_date='2021-01-01T14:38:08Z' closed_time='2021-01-04T21:35:25Z'>
	<summary>Can not use load_model() for quantized models (.pb format)</summary>
	<description>
Describe the bug
When trying to use model: tf.keras.models.Model = tf.keras.models.load_model(model_path, compile=False) and model_path points to a quantized model, I get this error message:
&lt;denchmark-code&gt;Exception has occurred: KeyError
'__inference_conv2d_layer_call_and_return_conditional_losses_760539'
&lt;/denchmark-code&gt;

System information
OS: Linux Mint 19.3
TensorFlow version (installed from source or binary): 2.2.0 from binary (using conda)
TensorFlow Model Optimization version (installed from source or binary): 0.5.0 from binary (using pip)
Python version: 3.7
Describe the expected behavior
Model can be loaded and used for inference
Describe the current behavior
Fails with KeyError message.
Code to reproduce the issue
import tensorflow as tf
# model_path points to quantized model which has at least one Conv2D layer that is quantized
with tfmot.quantization.keras.quantize_scope():
  model: tf.keras.models.Model = tf.keras.models.load_model(model_path, compile=False)
	</description>
	<comments>
		<comment id='1' author='j-o-d-o' date='2021-01-04T21:35:25Z'>
		Using H5 format works, only using the SaveModel format has this issue. So I would claim this is a load_model() issue and is not related to the model optimization itself.
		</comment>
	</comments>
</bug>