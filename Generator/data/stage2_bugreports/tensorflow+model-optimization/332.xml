<bug id='332' author='psunn' open_date='2020-04-02T09:48:43Z' closed_time='2021-01-11T10:02:10Z'>
	<summary>TEST failures</summary>
	<description>
Describe the bug
current on python/core tests, 15 of them are failed
15 failed tests:

//tensorflow_model_optimization/python/core/internal/tensor_encoding/core:core_encoder_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/core:encoding_stage_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/core:gather_encoder_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/core:simple_encoder_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/encoders:common_encoders_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/stages:stages_impl_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/stages/research:clipping_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/stages/research:kashin_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/stages/research:misc_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/stages/research:quantization_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/testing:test_utils_test
//tensorflow_model_optimization/python/core/internal/tensor_encoding/utils:py_utils_test
//tensorflow_model_optimization/python/core/quantization/keras:quantize_test
//tensorflow_model_optimization/python/core/sparsity/keras:prune_integration_test
//tensorflow_model_optimization/python/core/quantization/keras:quantize_functional_test

I'm wondering is it expected? Any plan to look into them? Thanks
System information
TensorFlow installed from (source or binary): source
TensorFlow version: 2.1.0
TensorFlow Model Optimization version:

commit &lt;denchmark-link:https://github.com/tensorflow/model-optimization/commit/08dbdc43871e5e7cb6638ba96e1ad3bfec5837aa&gt;08dbdc4&lt;/denchmark-link&gt;
 (HEAD -&gt; master, origin/master, origin/HEAD)
Python version: 3.6.9
Describe the expected behavior
TESTs in python/core should all pass
Describe the current behavior
15 of the TESTs are failed now
Code to reproduce the issue
Provide a reproducible code that is the bare minimum necessary to generate the
problem.
bazelisk test --test_summary=detailed --test_output=all //tensorflow_model_optimization/python/core/...
	</description>
	<comments>
		<comment id='1' author='psunn' date='2020-04-03T12:32:13Z'>
		&lt;denchmark-link:https://github.com/alanchiao&gt;@alanchiao&lt;/denchmark-link&gt;
, we have been getting test failures in MOT for a couple of days, is it expected?
		</comment>
		<comment id='2' author='psunn' date='2020-04-03T16:36:25Z'>
		&lt;denchmark-link:https://github.com/PraChetit&gt;@PraChetit&lt;/denchmark-link&gt;
 for visibility on tensor encoding ones.
Generally there should not be test issues. The main issue is that we don't have CI testing right now to ensure the techniques work with the relevant TF versions that they should support (e.g. pruning: 1.14.0+, 2.0.0+ and quantization: nightly). We only know whether the closed-version of TFMOT at HEAD works against the closed-version of TF at head. Prior to each stable release, we then ensure the tests are all passing again, so long as they are for launched techniques.
It's in the backlog amongst things I have to do for QAT, Official Models, and others.
		</comment>
		<comment id='3' author='psunn' date='2021-01-11T10:02:00Z'>
		TESTs pass now, thanks.
		</comment>
	</comments>
</bug>