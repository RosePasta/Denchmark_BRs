<bug id='605' author='pjin257' open_date='2020-12-07T07:59:52Z' closed_time='2020-12-08T00:28:13Z'>
	<summary>Converting model to tflite without optimization option</summary>
	<description>
Describe the bug
I converted saved models to tflite without any optimization option. That's like,
model = tf.keras.models.load_model('saved_model')
converter = tf.lite.TFLiteConverter.from_keras_model(model)
I assumed above conversion is done under "default" optimization setting below:
converter.optimizations = [tf.lite.Optimize.DEFAULT]
However, I got a different result when I explicitly added the optimization option.
Converted tflite models got much smaller sizes with the option:
Without option: x1.02 ~ x3.84 smaller
With option: x4.08 ~ x11.999 smaller
It seems like quantization (32float -&gt; 8int) is implemented only with the option since it made the models x4 times smaller.
Then what made the first conversion without option got smaller models? Is it pruning?
Also, why one of the models didn't get so smaller? (only x1.02 smaller)
Thanks
System information
TensorFlow version (installed from source or binary): 2.3.0
Python version: 3.8.3
	</description>
	<comments>
	</comments>
</bug>