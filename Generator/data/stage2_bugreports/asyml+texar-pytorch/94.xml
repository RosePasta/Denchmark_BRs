<bug id='94' author='Codle' open_date='2019-07-08T08:12:20Z' closed_time='2019-07-09T15:53:00Z'>
	<summary>Not support Windows systemï¼Ÿ</summary>
	<description>
When I run the code in Windows, I met the error
RuntimeError: Expected tensor for argument &lt;denchmark-link:https://github.com/asyml/texar-pytorch/pull/1&gt;#1&lt;/denchmark-link&gt;
 'indices' to have scalar type Long; but got CUDAType instead
and I run the same codes in Linux, all things were OK.
I had checked the pytorch version were both 1.1.0.
	</description>
	<comments>
		<comment id='1' author='Codle' date='2019-07-08T08:14:38Z'>
		the codes I used were seq2seq_attn example, and the error was occured in embeding layer.
		</comment>
		<comment id='2' author='Codle' date='2019-07-08T14:41:21Z'>
		Thank you for the feedback! I'm guessing the error is not related to Windows, it's probably due to some mistakes in tensor device placement.
Could you share the hash of the commit you're working on, and the complete error message along with the stacktrace? This will help us locate the bug.
		</comment>
		<comment id='3' author='Codle' date='2019-07-08T14:57:54Z'>
		
Thank you for the feedback! I'm guessing the error is not related to Windows, it's probably due to some mistakes in tensor device placement.
Could you share the hash of the commit you're working on, and the complete error message along with the stacktrace? This will help us locate the bug.

I remove my cuda code. Now the file was copied form example/seq2seq_attn/seq2seq_attn.py and the error was
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "seq2seq_attn.py", line 186, in &lt;module&gt;
    main()
  File "seq2seq_attn.py", line 172, in main
    _train_epoch()
  File "seq2seq_attn.py", line 138, in _train_epoch
    loss = model(batch, mode="train")
  File "C:\Users\codle\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "seq2seq_attn.py", line 81, in forward
    inputs=self.source_embedder(batch['source_text_ids']),
  File "C:\Users\codle\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "c:\projects\texar-pytorch\texar\modules\embedders\embedders.py", line 221, in forward
    outputs = F.embedding(ids, embedding, **kwargs)
  File "C:\Users\codle\Anaconda3\lib\site-packages\torch\nn\functional.py", line 1506, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUType instead (while checking arguments for embedding)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Codle' date='2019-07-09T04:12:58Z'>
		Thank you for the feedback! RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUType instead (while checking arguments for embedding) My guess is that the type of batch['source_text_ids'] is wrong. batch['source_text_ids'] here should be LongTensor. We recently did some code refactor on data module and seq2seq_attn example. Could you try the lastest seq2seq_attn example to see if such error still exists?
		</comment>
		<comment id='5' author='Codle' date='2019-07-09T09:11:05Z'>
		
Thank you for the feedback! RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUType instead (while checking arguments for embedding) My guess is that the type of batch['source_text_ids'] is wrong. batch['source_text_ids'] here should be LongTensor. We recently did some code refactor on data module and seq2seq_attn example. Could you try the lastest seq2seq_attn example to see if such error still exists?

It's useless. But I checked the batch['source_text_ids'], the dtype is dtype=torch.int32. So I think this is the problem.
And I run the example successfully by add .long() in line 76, 89, 93
&lt;denchmark-code&gt;76:inputs=self.source_embedder(batch['source_text_ids'].long()),
89:inputs=self.target_embedder(batch['target_text_ids'][:, :-1].long()),
93:labels=batch['target_text_ids'][:, 1:].long(),
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Codle' date='2019-07-09T13:57:06Z'>
		&lt;denchmark-link:https://github.com/Codle&gt;@Codle&lt;/denchmark-link&gt;
 Could you confirm that you're using the latest version (commit &lt;denchmark-link:https://github.com/asyml/texar-pytorch/commit/5e899bf359829dec1e08de90bb337d0dd239d3fc&gt;5e899bf&lt;/denchmark-link&gt;
) of Texar-PyTorch? I could not reproduce the problem in the latest version.
		</comment>
		<comment id='7' author='Codle' date='2019-07-09T14:05:37Z'>
		
@Codle Could you confirm that you're using the latest version (commit 5e899bf) of Texar-PyTorch? I could not reproduce the problem in the latest version.

&lt;denchmark-code&gt;(base) C:\Users\codle\Documents&gt;pip uninstall texar
Uninstalling texar-0.0.1:
  Would remove:
    c:\users\codle\anaconda3\lib\site-packages\texar.egg-link
Proceed (y/n)? y
  Successfully uninstalled texar-0.0.1

(base) C:\Users\codle\Documents&gt;git clone https://github.com/asyml/texar-pytorch.git
Cloning into 'texar-pytorch'...
remote: Enumerating objects: 42, done.
remote: Counting objects: 100% (42/42), done.
remote: Compressing objects: 100% (39/39), done.
remote: Total 2899 (delta 1), reused 31 (delta 1), pack-reused 2857
Receiving objects: 100% (2899/2899), 1.08 MiB | 473.00 KiB/s, done.
Resolving deltas: 100% (2162/2162), done.

(base) C:\Users\codle\Documents&gt;cd texar-pytorch

(base) C:\Users\codle\Documents\texar-pytorch&gt;pip install -e .
Obtaining file:///C:/Users/codle/Documents/texar-pytorch
Requirement already satisfied: numpy in c:\users\codle\anaconda3\lib\site-packages (from texar==0.0.1) (1.16.2)
Requirement already satisfied: pyyaml in c:\users\codle\anaconda3\lib\site-packages (from texar==0.0.1) (5.1)
Requirement already satisfied: requests in c:\users\codle\anaconda3\lib\site-packages (from texar==0.0.1) (2.21.0)
Requirement already satisfied: funcsigs in c:\users\codle\anaconda3\lib\site-packages (from texar==0.0.1) (1.0.2)
Requirement already satisfied: mypy_extensions in c:\users\codle\anaconda3\lib\site-packages (from texar==0.0.1) (0.4.1)
Requirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in c:\users\codle\anaconda3\lib\site-packages (from requests-&gt;texar==0.0.1) (1.24.1)
Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in c:\users\codle\anaconda3\lib\site-packages (from requests-&gt;texar==0.0.1) (3.0.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\codle\anaconda3\lib\site-packages (from requests-&gt;texar==0.0.1) (2019.3.9)
Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in c:\users\codle\anaconda3\lib\site-packages (from requests-&gt;texar==0.0.1) (2.8)
Installing collected packages: texar
  Running setup.py develop for texar
Successfully installed texar

(base) C:\Users\codle\Documents\texar-pytorch&gt;cd examples\seq2seq_attn
(base) C:\Users\codle\Documents\texar-pytorch\examples\seq2seq_attn&gt;python prepare_data.py --data toy_copy
Successfully downloaded toy_copy.zip.

(base) C:\Users\codle\Documents\texar-pytorch\examples\seq2seq_attn&gt;python seq2seq_attn.py --config_model config_model --config_data config_toy_copy
Traceback (most recent call last):
  File "seq2seq_attn.py", line 184, in &lt;module&gt;
    main()
  File "seq2seq_attn.py", line 170, in main
    _train_epoch()
  File "seq2seq_attn.py", line 137, in _train_epoch
    loss = model(batch, mode="train")
  File "C:\Users\codle\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "seq2seq_attn.py", line 76, in forward
    inputs=self.source_embedder(batch['source_text_ids']),
  File "C:\Users\codle\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File "c:\users\codle\documents\texar-pytorch\texar\modules\embedders\embedders.py", line 221, in forward
    outputs = F.embedding(ids, embedding, **kwargs)
  File "C:\Users\codle\Anaconda3\lib\site-packages\torch\nn\functional.py", line 1506, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CUDAType instead (while checking arguments for embedding)


&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='Codle' date='2019-07-09T14:39:24Z'>
		I tried a experiment to reproduce the error:
&lt;denchmark-code&gt;In [17]: a = torch.from_numpy(np.array([1, 2, 3]))
In [18]: a
Out[18]: tensor([1, 2, 3], dtype=torch.int32)
In [19]: embed(a)
&lt;/denchmark-code&gt;

then the error was showed.
In the file data/data/paired_text_data.py , the line 409, 427 are used the function from_numpy and I add a parameter dtype=torch.long  in to(device=self.device). This example can also run, but I don't know add this parameter is right?
It seems numpy's problem
The C long in win64 is also int32....
from &lt;denchmark-link:https://stackoverflow.com/questions/36278590/numpy-array-dtype-is-coming-as-int32-by-default-in-a-windows-10-64-bit-machine&gt;stackoverflow&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='Codle' date='2019-07-09T15:30:21Z'>
		&lt;denchmark-link:https://github.com/Codle&gt;@Codle&lt;/denchmark-link&gt;
 Thank you for the details! I had no idea about  being platform-dependent.
The root cause of the issue is in the padded_batch method in texar/data/data/dataset_utils.py, where a NumPy array with dtype np.long is created. Changing dtype to the platform-independent np.int64 should work.
I am sorry for the inconvenience. We have not tested the code on Windows because we do not have Windows working environments. Thank you for your contribution!
		</comment>
	</comments>
</bug>