<bug id='251' author='YongtaoGe' open_date='2019-11-21T12:50:05Z' closed_time='2019-11-21T23:01:27Z'>
	<summary>Is this a bug in VAE?</summary>
	<description>



texar-pytorch/examples/vae_text/vae_train.py


        Lines 90 to 91
      in
      53d8ead






 input_size=(self.decoder_w_embedder.dim + 



 config_model.batch_size), 





	</description>
	<comments>
		<comment id='1' author='YongtaoGe' date='2019-11-21T18:09:30Z'>
		Yes, this is indeed wrong. This should be self.decoder_w_embedder.dim + config_model.latent_dims, and the unlucky coincidence here is that batch_size is equal to latent_dims in all our configs. Would you like to create a pull request for this?
		</comment>
		<comment id='2' author='YongtaoGe' date='2019-11-21T20:29:06Z'>
		&lt;denchmark-link:https://github.com/YongtaoGe&gt;@YongtaoGe&lt;/denchmark-link&gt;
 Thanks for letting us know this bug! &lt;denchmark-link:https://github.com/huzecong&gt;@huzecong&lt;/denchmark-link&gt;
 is right. It should be  instead of . I will fix it very soon.
		</comment>
	</comments>
</bug>