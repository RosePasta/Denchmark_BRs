<bug id='158' author='TomNong' open_date='2019-08-17T03:11:13Z' closed_time='2019-09-05T19:01:56Z'>
	<summary>Potential position embedding issue in decoders.</summary>
	<description>
Currently there is a potential issue in some decoders(e.g. AttentionRNNDecoder) which support position embedder.
In eval mode, given max_decoding_length, the dynamic_decode will stop at time==max_decoding_length - 1, while decoder helper may pass time = time + 1 to its position embedder. Since position_size of the position embedder  is usually equal to max_decoding_length, inputing time==max_decoding_length will cause index out of range error on position embedder.
Suggested by &lt;denchmark-link:https://github.com/huzecong&gt;@huzecong&lt;/denchmark-link&gt;
 , 
	</description>
	<comments>
		<comment id='1' author='TomNong' date='2019-08-17T15:58:48Z'>
		A better way to deal with this might be: split decoder.step into two methods, one for computing next_state and output, another for computing the next input. This way we can perform a check in dynamic_decode, without requiring all decoder classes to check for themselves.
		</comment>
		<comment id='2' author='TomNong' date='2019-08-19T16:35:20Z'>
		Does TransformerDecoder have such issue?
Does Texar-TF have such issue? &lt;denchmark-link:https://github.com/TomNong&gt;@TomNong&lt;/denchmark-link&gt;
 can you double-check?
Performing the check in dynamic_decode is good. We can refactor the decoder a bit (though I don't know the full details of your suggestion)
		</comment>
		<comment id='3' author='TomNong' date='2019-08-19T16:54:05Z'>
		&lt;denchmark-link:https://github.com/ZhitingHu&gt;@ZhitingHu&lt;/denchmark-link&gt;
 For  it does not have such issue.
For texar-TF, this issue is related with &lt;denchmark-link:https://github.com/asyml/texar/issues/199&gt;asyml/texar#199&lt;/denchmark-link&gt;
, actually we used  to avoid this issue.
		</comment>
	</comments>
</bug>