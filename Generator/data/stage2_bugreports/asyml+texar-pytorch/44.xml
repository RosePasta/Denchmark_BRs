<bug id='44' author='huzecong' open_date='2019-06-17T21:16:48Z' closed_time='2019-06-27T20:31:24Z'>
	<summary>Conv1DNetwork returns tensor of incorrect shape</summary>
	<description>
When using Conv1DNetwork with multiple filters and no dense layers, the returned tensor has incorrect shape.
For example:
char_embed_size = 8
char_cnn = tx.modules.Conv1DEncoder(
    in_channels=char_embed_size, hparams={
        "kernel_size": [3, 4, 5],
        "out_channels": 50,
        "num_dense_layers": 0,
        "conv_activation": tx.core.identity,
        "dropout_conv": [],
        "dropout_rate": 0.0,
    })
batch_size, seq_len = 20, 30
# (batch, in_channels, in_features)
input = torch.randn(batch_size, char_embed_size, seq_len)
output = char_cnn(input)
out_channels = 150  # out_channels is multiplied by length of kernel_size
assert output.size() == (batch_size, out_channels)  # raises AssertionError
# actual returned size is (batch_size, 50, 3)
This is because internally Conv1DNetwork constructs a MergeLayer that concats outputs from differently-sized kernels, but does not specify the dim argument, so the default value of 2 is used. However, for convolutional layers, the channel dimension (defaults to dimension 1 in PyTorch) should be concat'd.
The test cases failed to capture this bug because dense layers were used. This resulted in Flatten layer being added, and in_features to the following Linear layer is inferred by actually running an input example through the network.
	</description>
	<comments>
	</comments>
</bug>