<bug id='3188' author='shanshanpt' open_date='2020-09-18T06:01:30Z' closed_time='2020-09-25T05:19:40Z'>
	<summary>Lowering failed at ConvertHLOToLinalgOnBuffersPass with llvm-backend</summary>
	<description>
The source IR:
&lt;denchmark-code&gt;module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 0 : i32}} {
  func @predict_images(%arg0: tensor&lt;4x512x1x1xf32&gt;, %arg1: tensor&lt;2xi32&gt;, %arg2: tensor&lt;1x1x1024x512xf32&gt;, %arg3: tensor&lt;512xf32&gt;, %arg4: tensor&lt;f32&gt;, %arg5: tensor&lt;1x4x1x512xf32&gt;, %arg6: tensor&lt;1x1x1x512xf32&gt;, %arg7: tensor&lt;1x2x1x512xf32&gt;, %arg8: tensor&lt;4x2x1x512xf32&gt;, %arg9: tensor&lt;4x1x1x1024xf32&gt;, %arg10: tensor&lt;4x1x1x512xf32&gt;) -&gt; tensor&lt;1x1x1x512xf32&gt; attributes {iree.module.export} {
    %597 = "mhlo.broadcast_in_dim"(%arg3) {broadcast_dimensions = dense&lt;3&gt; : tensor&lt;1xi64&gt;} : (tensor&lt;512xf32&gt;) -&gt; tensor&lt;4x1x1x512xf32&gt;
    %598 = mhlo.add %arg10, %597 : tensor&lt;4x1x1x512xf32&gt;
    %599 = "mhlo.reshape"(%598) : (tensor&lt;4x1x1x512xf32&gt;) -&gt; tensor&lt;1x4x1x512xf32&gt;
    %600 = "mhlo.transpose"(%599) {permutation = dense&lt;[1, 0, 2, 3]&gt; : tensor&lt;4xi64&gt;} : (tensor&lt;1x4x1x512xf32&gt;) -&gt; tensor&lt;4x1x1x512xf32&gt;
    %601 = "mhlo.batch_norm_inference"(%600, %arg3, %arg3, %arg3, %arg3) {epsilon = 1.000000e-03 : f32, feature_index = 3 : i64} : (tensor&lt;4x1x1x512xf32&gt;, tensor&lt;512xf32&gt;, tensor&lt;512xf32&gt;, tensor&lt;512xf32&gt;, tensor&lt;512xf32&gt;) -&gt; tensor&lt;4x1x1x512xf32&gt;
    %602 = "mhlo.clamp"(%arg4, %601, %arg4) : (tensor&lt;f32&gt;, tensor&lt;4x1x1x512xf32&gt;, tensor&lt;f32&gt;) -&gt; tensor&lt;4x1x1x512xf32&gt;
    %603 = "mhlo.slice"(%602) {limit_indices = dense&lt;[3, 1, 1, 512]&gt; : tensor&lt;4xi64&gt;, start_indices = dense&lt;0&gt; : tensor&lt;4xi64&gt;, strides = dense&lt;1&gt; : tensor&lt;4xi64&gt;} : (tensor&lt;4x1x1x512xf32&gt;) -&gt; tensor&lt;3x1x1x512xf32&gt;
    %604 = "mhlo.transpose"(%603) {permutation = dense&lt;[1, 0, 2, 3]&gt; : tensor&lt;4xi64&gt;} : (tensor&lt;3x1x1x512xf32&gt;) -&gt; tensor&lt;1x3x1x512xf32&gt;
    %605 = "mhlo.slice"(%604) {limit_indices = dense&lt;[1, 1, 1, 512]&gt; : tensor&lt;4xi64&gt;, start_indices = dense&lt;0&gt; : tensor&lt;4xi64&gt;, strides = dense&lt;1&gt; : tensor&lt;4xi64&gt;} : (tensor&lt;1x3x1x512xf32&gt;) -&gt; tensor&lt;1x1x1x512xf32&gt;

    return %605 : tensor&lt;1x1x1x512xf32&gt;
  }
}
&lt;/denchmark-code&gt;

The command:
&lt;denchmark-code&gt;iree-opt  -iree-flow-transformation-pipeline  -iree-hal-target-backends=llvm-ir -iree-hal-transformation-pipeline  -mlir-disable-threading  -print-ir-after-all /tmp/test.mlir
&lt;/denchmark-code&gt;

The error msg:
&lt;denchmark-code&gt;&lt;unknown&gt;:0: error: failed to legalize operation 'linalg.generic'
&lt;unknown&gt;:0: note: see current operation: %5 = "linalg.generic"(%2, %4) ( {
^bb0(%arg0: f32, %arg1: f32):  // no predecessors
  %8 = "std.addf"(%arg0, %arg1) : (f32, f32) -&gt; f32
  "linalg.yield"(%8) : (f32) -&gt; ()
}) {args_in = 2 : i64, args_out = 1 : i64, indexing_maps = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d0 * 512 + d1 * 512 + d2 * 512 + d3)&gt;], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} : (tensor&lt;4x1x1x512xf32&gt;, tensor&lt;512xf32&gt;) -&gt; tensor&lt;2048xf32&gt;

// *** IR Dump After mlir::iree_compiler::(anonymous namespace)::ConvertHLOToLinalgOnBuffersPass Failed ***
"func"() ( {
  %0 = "iree.placeholder"() {binding = @legacy_io::@ret0, purpose = "interface buffer"} : () -&gt; memref&lt;4x1x1x512xf32&gt;
  %c0 = "std.constant"() {value = 0 : index} : () -&gt; index
  %1 = "hal.interface.load.tensor"(%c0) {binding = @legacy_io::@arg0} : (index) -&gt; tensor&lt;4x1x1x512xf32&gt;
  %2 = "hal.interface.load.tensor"(%c0) {binding = @legacy_io::@arg1} : (index) -&gt; tensor&lt;512xf32&gt;
  %3 = "linalg.generic"(%1, %2) ( {
  ^bb0(%arg0: f32, %arg1: f32):  // no predecessors
    %6 = "std.addf"(%arg0, %arg1) : (f32, f32) -&gt; f32
    "linalg.yield"(%6) : (f32) -&gt; ()
  }) {args_in = 2 : i64, args_out = 1 : i64, indexing_maps = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d0 * 512 + d1 * 512 + d2 * 512 + d3)&gt;], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} : (tensor&lt;4x1x1x512xf32&gt;, tensor&lt;512xf32&gt;) -&gt; tensor&lt;2048xf32&gt;
  %4 = "linalg.tensor_reshape"(%3) {reassociation = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;]} : (tensor&lt;2048xf32&gt;) -&gt; tensor&lt;1x4x1x512xf32&gt;
  %5 = "linalg.generic"(%4, %2, %2) ( {
  ^bb0(%arg0: f32, %arg1: f32, %arg2: f32):  // no predecessors
    %6 = "std.subf"(%arg0, %arg1) : (f32, f32) -&gt; f32
    %7 = "std.mulf"(%6, %arg2) : (f32, f32) -&gt; f32
    "linalg.yield"(%7) : (f32) -&gt; ()
  }) {args_in = 3 : i64, args_out = 1 : i64, indexing_maps = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d1, d0, d2, d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} : (tensor&lt;1x4x1x512xf32&gt;, tensor&lt;512xf32&gt;, tensor&lt;512xf32&gt;) -&gt; tensor&lt;4x1x1x512xf32&gt;
  "hal.interface.store.tensor"(%5, %c0) {binding = @legacy_io::@ret0} : (tensor&lt;4x1x1x512xf32&gt;, index) -&gt; ()
  "std.return"() : () -&gt; ()
}) {sym_name = "predict_images_ex_dispatch_0", type = () -&gt; ()} : () -&gt; ()

/tmp/x5.mlir:10:12: error: failed to run translation of source executable to target executable for backend llvm-ir*
    %598 = mhlo.add %arg10, %597 : tensor&lt;4x1x1x512xf32&gt;
           ^
/tmp/x5.mlir:10:12: note: see current operation: "hal.executable.target"() ( {
  "hal.executable.entry_point"() {interface = @legacy_io, ordinal = 0 : i32, signature = (tensor&lt;4x1x1x512xf32&gt;, tensor&lt;512xf32&gt;) -&gt; tensor&lt;4x1x1x512xf32&gt;, sym_name = "predict_images_ex_dispatch_0"} : () -&gt; ()
  "module"() ( {
    "func"() ( {
      %0 = "iree.placeholder"() {binding = @legacy_io::@ret0, purpose = "interface buffer"} : () -&gt; memref&lt;4x1x1x512xf32&gt;
      %c0 = "std.constant"() {value = 0 : index} : () -&gt; index
      %1 = "hal.interface.load.tensor"(%c0) {binding = @legacy_io::@arg0} : (index) -&gt; tensor&lt;4x1x1x512xf32&gt;
      %2 = "hal.interface.load.tensor"(%c0) {binding = @legacy_io::@arg1} : (index) -&gt; tensor&lt;512xf32&gt;
      %3 = "linalg.generic"(%1, %2) ( {
      ^bb0(%arg0: f32, %arg1: f32):  // no predecessors
        %6 = "std.addf"(%arg0, %arg1) : (f32, f32) -&gt; f32
        "linalg.yield"(%6) : (f32) -&gt; ()
      }) {args_in = 2 : i64, args_out = 1 : i64, indexing_maps = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d0 * 512 + d1 * 512 + d2 * 512 + d3)&gt;], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} : (tensor&lt;4x1x1x512xf32&gt;, tensor&lt;512xf32&gt;) -&gt; tensor&lt;2048xf32&gt;
      %4 = "linalg.tensor_reshape"(%3) {reassociation = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;]} : (tensor&lt;2048xf32&gt;) -&gt; tensor&lt;1x4x1x512xf32&gt;
      %5 = "linalg.generic"(%4, %2, %2) ( {
      ^bb0(%arg0: f32, %arg1: f32, %arg2: f32):  // no predecessors
        %6 = "std.subf"(%arg0, %arg1) : (f32, f32) -&gt; f32
        %7 = "std.mulf"(%6, %arg2) : (f32, f32) -&gt; f32
        "linalg.yield"(%7) : (f32) -&gt; ()
      }) {args_in = 3 : i64, args_out = 1 : i64, indexing_maps = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d1, d0, d2, d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} : (tensor&lt;1x4x1x512xf32&gt;, tensor&lt;512xf32&gt;, tensor&lt;512xf32&gt;) -&gt; tensor&lt;4x1x1x512xf32&gt;
      "hal.interface.store.tensor"(%5, %c0) {binding = @legacy_io::@ret0} : (tensor&lt;4x1x1x512xf32&gt;, index) -&gt; ()
      "std.return"() : () -&gt; ()
    }) {sym_name = "predict_images_ex_dispatch_0", type = () -&gt; ()} : () -&gt; ()
    "hal.interface"() ( {
      "hal.interface.binding"() {access = 1 : i32, binding = 0 : i32, set = 0 : i32, sym_name = "arg0", type = 7 : i32} : () -&gt; ()
      "hal.interface.binding"() {access = 1 : i32, binding = 1 : i32, set = 0 : i32, sym_name = "arg1", type = 7 : i32} : () -&gt; ()
      "hal.interface.binding"() {access = 6 : i32, binding = 2 : i32, set = 0 : i32, sym_name = "ret0", type = 7 : i32} : () -&gt; ()
      "hal.interface_end"() : () -&gt; ()
    }) {sym_name = "legacy_io", sym_visibility = "private"} : () -&gt; ()
    "module_terminator"() : () -&gt; ()
  }) : () -&gt; ()
  "hal.executable.target_end"() : () -&gt; ()
}) {sym_name = "llvm_ir", target_backend_filter = "llvm-ir*"} : () -&gt; ()

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='shanshanpt' date='2020-09-21T03:55:15Z'>
		One more case,
&lt;denchmark-code&gt;// *** IR Dump After mlir::iree_compiler::(anonymous namespace)::FusionOfTensorOpsPass ***
module {
  func @predict_images_ex_dispatch_0() {
    %c0 = constant 0 : index
    %0 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 : tensor&lt;4x1x1x512xf32&gt;
    %1 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 : tensor&lt;512xf32&gt;
    %2 = linalg.generic {args_in = 2 : i64, args_out = 1 : i64, indexing_maps = [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d0 * 512 + d1 * 512 + d2 * 512 + d3)&gt;], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} %0, %1 {
    ^bb0(%arg0: f32, %arg1: f32):  // no predecessors
      %6 = addf %arg0, %arg1 : f32
      linalg.yield %6 : f32
    }: tensor&lt;4x1x1x512xf32&gt;, tensor&lt;512xf32&gt; -&gt; tensor&lt;2048xf32&gt;
    %3 = linalg.tensor_reshape %2 [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2, d3)&gt;] : tensor&lt;2048xf32&gt; into tensor&lt;1x4x1x512xf32&gt;
    %4 = linalg.tensor_reshape %3 [affine_map&lt;(d0, d1, d2, d3) -&gt; (d0, d1, d2)&gt;, affine_map&lt;(d0, d1, d2, d3) -&gt; (d3)&gt;] : tensor&lt;1x4x1x512xf32&gt; into tensor&lt;4x512xf32&gt;
    %5 = linalg.generic {args_in = 3 : i64, args_out = 1 : i64, indexing_maps = [affine_map&lt;(d0, d1) -&gt; (d0, d1)&gt;, affine_map&lt;(d0, d1) -&gt; (d1)&gt;, affine_map&lt;(d0, d1) -&gt; (d1)&gt;, affine_map&lt;(d0, d1) -&gt; (d0, d1)&gt;], iterator_types = ["parallel", "parallel"]} %4, %1, %1 {
    ^bb0(%arg0: f32, %arg1: f32, %arg2: f32):  // no predecessors
      %6 = subf %arg0, %arg1 : f32
      %7 = mulf %6, %arg2 : f32
      linalg.yield %7 : f32
    }: tensor&lt;4x512xf32&gt;, tensor&lt;512xf32&gt;, tensor&lt;512xf32&gt; -&gt; tensor&lt;4x512xf32&gt;
    hal.interface.store.tensor %5, @legacy_io::@ret0, offset = %c0 : tensor&lt;4x512xf32&gt;
    return
  }
  hal.interface @legacy_io attributes {sym_visibility = "private"} {
    hal.interface.binding @arg0, set=0, binding=0, type="StorageBuffer", access="Read"
    hal.interface.binding @arg1, set=0, binding=1, type="StorageBuffer", access="Read"
    hal.interface.binding @ret0, set=0, binding=2, type="StorageBuffer", access="Write|Discard"
  }
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='shanshanpt' date='2020-09-21T04:49:33Z'>
		For the second case, could you post the MHLO level IR as well, ie, the IR before HLOToLinalgOnTensorsPass? Sometimes we might want to address the issue in an earlier stage. Thanks!
		</comment>
		<comment id='3' author='shanshanpt' date='2020-09-21T06:52:13Z'>
		This seems like more like a missing canonicalization for folding pairs of reshapes that needs to go into LinalgFoldUnitExtentDimsPass. Looking into this now.
		</comment>
		<comment id='4' author='shanshanpt' date='2020-09-21T10:15:47Z'>
		It looks like if a canonicalization pattern is added. The two tensor_reshape will be folded into tensor&lt;2048xf32&gt; -&gt; tensor&lt;4x512xf32&gt; which is an expanding case and could be fused with later generic op. Then everything looks good to me.
		</comment>
		<comment id='5' author='shanshanpt' date='2020-09-21T16:31:44Z'>
		Thats the plan. I will try to send the patch out soon.
		</comment>
		<comment id='6' author='shanshanpt' date='2020-09-21T22:51:20Z'>
		&lt;denchmark-link:https://github.com/shanshanpt&gt;@shanshanpt&lt;/denchmark-link&gt;
 I have the patch that works for your first example (havent sent it out yet). The second example also works, but there is a missing fusion. With the patch I get the following
&lt;denchmark-code&gt;#map0 = affine_map&lt;(d0, d1) -&gt; (d0, d1)&gt;
#map1 = affine_map&lt;(d0, d1) -&gt; (d1)&gt;
#map2 = affine_map&lt;(d0, d1) -&gt; (d0 * 512 + d1)&gt;
module {
  func @predict_images_ex_dispatch_0() {
    %c0 = constant 0 : index
    %0 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 : tensor&lt;512xf32&gt;
    %1 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 : tensor&lt;4x512xf32&gt;
    %2 = linalg.generic {args_in = 2 : i64, args_out = 1 : i64, indexing_maps = [#map0, #map1, #map2], iterator_types = ["parallel", "parallel"]} %1, %0 {
    ^bb0(%arg0: f32, %arg1: f32):  // no predecessors
      %4 = addf %arg0, %arg1 : f32
      linalg.yield %4 : f32
    }: tensor&lt;4x512xf32&gt;, tensor&lt;512xf32&gt; -&gt; tensor&lt;2048xf32&gt;
    %3 = linalg.generic {args_in = 3 : i64, args_out = 1 : i64, indexing_maps = [#map2, #map1, #map1, #map0], iterator_types = ["parallel", "parallel"]} %2, %0, %0 {
    ^bb0(%arg0: f32, %arg1: f32, %arg2: f32):  // no predecessors
      %4 = subf %arg0, %arg1 : f32
      %5 = mulf %4, %arg2 : f32
      linalg.yield %5 : f32
    }: tensor&lt;2048xf32&gt;, tensor&lt;512xf32&gt;, tensor&lt;512xf32&gt; -&gt; tensor&lt;4x512xf32&gt;
    hal.interface.store.tensor %3, @legacy_io::@ret0, offset = %c0 : tensor&lt;4x512xf32&gt;
    return
  }
  hal.interface @legacy_io attributes {sym_visibility = "private"} {
    hal.interface.binding @arg0, set=0, binding=0, type="StorageBuffer", access="Read"
    hal.interface.binding @arg1, set=0, binding=1, type="StorageBuffer", access="Read"
    hal.interface.binding @ret0, set=0, binding=2, type="StorageBuffer", access="Write|Discard"
  }
}
&lt;/denchmark-code&gt;

The two  dont get fused because the  that is used is not a permutation as expected &lt;denchmark-link:https://github.com/llvm/llvm-project/blob/825203daae7ffea0326757fdbb819682dfbef4f9/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L460&gt;here&lt;/denchmark-link&gt;
.
There are some changes to the fusion of linalg on tensors that could make it work (the same map is used in the producer and the consumer), but something is not clear to me in that example. We should not have maps of the form
affine_map&lt;(d0, d1) -&gt; (d0 * 512 + d1)&gt;. Could you post the IR at the start of the pipeline for this example. If we can avoid such maps, then that would be a better approach.
		</comment>
		<comment id='7' author='shanshanpt' date='2020-09-21T23:46:22Z'>
		Patch : &lt;denchmark-link:https://reviews.llvm.org/D88057&gt;https://reviews.llvm.org/D88057&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='shanshanpt' date='2020-09-22T02:20:52Z'>
		&lt;denchmark-link:https://github.com/MaheshRavishankar&gt;@MaheshRavishankar&lt;/denchmark-link&gt;
 thanks so much ! will test it.
		</comment>
		<comment id='9' author='shanshanpt' date='2020-09-23T03:09:52Z'>
		&lt;denchmark-link:https://github.com/MaheshRavishankar&gt;@MaheshRavishankar&lt;/denchmark-link&gt;
 LGTM, thanks
		</comment>
		<comment id='10' author='shanshanpt' date='2020-09-23T03:16:51Z'>
		I haven't submitted the patch yet. Will do it in a bit
		</comment>
		<comment id='11' author='shanshanpt' date='2020-09-25T02:31:16Z'>
		
I haven't submitted the patch yet. Will do it in a bit

Thanks! :)
		</comment>
		<comment id='12' author='shanshanpt' date='2020-09-25T04:54:36Z'>
		If this is not an issue on your end, can you close this issue?
		</comment>
		<comment id='13' author='shanshanpt' date='2020-09-25T05:19:38Z'>
		
If this is not an issue on your end, can you close this issue?

done
		</comment>
	</comments>
</bug>