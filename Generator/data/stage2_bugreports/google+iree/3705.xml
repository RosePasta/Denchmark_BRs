<bug id='3705' author='hanhanW' open_date='2020-11-03T20:40:32Z' closed_time='2020-11-04T23:18:42Z'>
	<summary>Compile error in llvm-ir when fusing gather ops with other ops.</summary>
	<description>
Hit an issue in LLVM-IR compilation when enable fusion on gather ops and other ops in &lt;denchmark-link:https://github.com/google/iree/pull/3682&gt;#3682&lt;/denchmark-link&gt;

With release BUILD, there is a log LLVM IR fails to verify, but a module is still generated. Thus it doesn't crash in compilation time.



iree/iree/compiler/Dialect/HAL/Target/LLVM/IR/LLVMIRTarget.cpp


        Lines 59 to 64
      in
      f52156c






 auto llvmModule = 



 mlir::translateModuleToLLVMIR(targetOp.getInnerModule(), context); 



 



 if (!llvmModule) { 



 return targetOp.emitError("Failed to translate executable to LLVM IR"); 



 } 





With debug BUILD, it hit an assertion :
&lt;denchmark-code&gt;iree-translate: ../third_party/llvm-project/llvm/lib/IR/Instructions.cpp:2296: void llvm::InsertValueInst::init(llvm::Value *, llvm::Value *, ArrayRef&lt;unsigned int&gt;, const llvm::Twine &amp;): Assertion `ExtractValueInst::getIndexedType(Agg-&gt;getType(), Idxs) == Val-&gt;getType() &amp;&amp; "Inserted value must match indexed type!"' failed.
&lt;/denchmark-code&gt;

To repro:
patch &lt;denchmark-link:https://github.com/google/iree/pull/3682&gt;#3682&lt;/denchmark-link&gt;

iree-translate -iree-mlir-to-vm-bytecode-module -iree-hal-target-backends=llvm-ir ~/x.mlir -o uni
Narrow down the test case to:
module {
  func @main_ex_dispatch_18(%arg0: tensor&lt;1x10xf32&gt;, %arg1: tensor&lt;1x10xf32&gt;, %arg2: tensor&lt;5x1x1xf32&gt;, %arg3: tensor&lt;i32&gt;) -&gt; tensor&lt;1x10xf32&gt; {
    %cst = constant dense&lt;1.000000e+01&gt; : tensor&lt;1x10xf32&gt;
    %cst_0 = constant dense&lt;-1.000000e+01&gt; : tensor&lt;1x10xf32&gt;
    %cst_1 = constant dense&lt;1.000000e+00&gt; : tensor&lt;1x10xf32&gt;
    %cst_2 = constant dense&lt;0.000000e+00&gt; : tensor&lt;1x10xf32&gt;
    %cst_3 = constant dense&lt;5.000000e-01&gt; : tensor&lt;1x10xf32&gt;
    %0 = "mhlo.torch_index_select"(%arg2, %arg3) {batch_dims = 0 : i64, dim = 0 : i64} : (tensor&lt;5x1x1xf32&gt;, tensor&lt;i32&gt;) -&gt; tensor&lt;1x1xf32&gt;
    %1 = "mhlo.reshape"(%0) : (tensor&lt;1x1xf32&gt;) -&gt; tensor&lt;1xf32&gt;
    %2 = "mhlo.broadcast_in_dim"(%1) {broadcast_dimensions = dense&lt;0&gt; : tensor&lt;1xi64&gt;} : (tensor&lt;1xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;
    %3 = mhlo.multiply %2, %cst_1 : tensor&lt;1x10xf32&gt;
    %4 = "mhlo.compare"(%3, %cst_2) {comparison_direction = "GT"} : (tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xi1&gt;
    %5 = mhlo.multiply %arg1, %cst_3 : tensor&lt;1x10xf32&gt;
    %6 = "mhlo.tanh"(%5) : (tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;
    %7 = mhlo.multiply %6, %cst_3 : tensor&lt;1x10xf32&gt;
    %8 = mhlo.add %7, %cst_3 : tensor&lt;1x10xf32&gt;
    %9 = mhlo.multiply %8, %arg0 : tensor&lt;1x10xf32&gt;
    %10 = "mhlo.select"(%4, %arg0, %9) : (tensor&lt;1x10xi1&gt;, tensor&lt;1x10xf32&gt;, tensor&lt;1x10xf32&gt;) -&gt; tensor&lt;1x10xf32&gt;
    return %10 : tensor&lt;1x10xf32&gt;
  }
}
The fusion on tensors looks good to me:
module {
  func @main_ex_dispatch_18_ex_dispatch_0__num_workgroups__(!shapex.ranked_shape&lt;[1,10]&gt;, !shapex.ranked_shape&lt;[1,10]&gt;, !shapex.ranked_shape&lt;[5,1,1]&gt;, !shapex.ranked_shape&lt;[]&gt;, !shapex.ranked_shape&lt;[1,10]&gt;) -&gt; (index, index, index) attributes {sym_visibility = "private"}
  func @main_ex_dispatch_18_ex_dispatch_0() attributes {hal.num_workgroups_fn = @main_ex_dispatch_18_ex_dispatch_0__num_workgroups__} {
    %c0 = constant 0 : index
    %cst = constant 1.000000e+00 : f32
    %cst_0 = constant 0.000000e+00 : f32
    %cst_1 = constant 5.000000e-01 : f32
    %0 = hal.interface.load.tensor @legacy_io::@arg2, offset = %c0 {operand_result_index = 2 : i32} : tensor&lt;5x1x1xf32&gt;
    %1 = hal.interface.load.tensor @legacy_io::@arg3, offset = %c0 {operand_result_index = 3 : i32} : tensor&lt;i32&gt;
    %2 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 {operand_result_index = 1 : i32} : tensor&lt;10xf32&gt;
    %3 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor&lt;10xf32&gt;
    %4 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor&lt;10xf32&gt;
    %5 = linalg.indexed_generic {indexing_maps = [affine_map&lt;(d0) -&gt; ()&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;], iterator_types = ["parallel"]} ins(%1, %4, %2, %3 : tensor&lt;i32&gt;, tensor&lt;10xf32&gt;, tensor&lt;10xf32&gt;, tensor&lt;10xf32&gt;) {
    ^bb0(%arg0: index, %arg1: i32, %arg2: f32, %arg3: f32, %arg4: f32):  // no predecessors
      %6 = mulf %arg3, %cst_1 : f32
      %7 = tanh %6 : f32
      %8 = mulf %7, %cst_1 : f32
      %9 = addf %8, %cst_1 : f32
      %10 = mulf %9, %arg4 : f32
      %11 = index_cast %arg1 : i32 to index
      %12 = extract_element %0[%11, %c0, %c0] : tensor&lt;5x1x1xf32&gt;
      %13 = mulf %12, %cst : f32
      %14 = cmpf "ogt", %13, %cst_0 : f32
      %15 = select %14, %arg2, %10 : f32
      linalg.yield %15 : f32
    } -&gt; tensor&lt;10xf32&gt;
    hal.interface.store.tensor %5, @legacy_io::@ret0, offset = %c0 {operand_result_index = 4 : i32} : tensor&lt;10xf32&gt;
    return
  }
Lowering to buffer's world also looks good to me.
func @main_ex_dispatch_18_ex_dispatch_0() attributes {hal.num_workgroups_fn = @main_ex_dispatch_18_ex_dispatch_0__num_workgroups__} {
  %0 = iree.placeholder for "interface buffer" {binding = @legacy_io::@ret0, operand_result_index = 4 : i32} : memref&lt;10xf32&gt;
  %c0 = constant 0 : index
  %cst = constant 1.000000e+00 : f32
  %cst_0 = constant 0.000000e+00 : f32
  %cst_1 = constant 5.000000e-01 : f32
  %1 = iree.placeholder for "interface buffer" {binding = @legacy_io::@arg2, operand_result_index = 2 : i32} : memref&lt;5x1x1xf32&gt;
  %2 = iree.placeholder for "interface buffer" {binding = @legacy_io::@arg3, operand_result_index = 3 : i32} : memref&lt;i32&gt;
  %3 = iree.placeholder for "interface buffer" {binding = @legacy_io::@arg1, operand_result_index = 1 : i32} : memref&lt;10xf32&gt;
  %4 = iree.placeholder for "interface buffer" {binding = @legacy_io::@arg0, operand_result_index = 0 : i32} : memref&lt;10xf32&gt;
  %5 = iree.placeholder for "interface buffer" {binding = @legacy_io::@arg0, operand_result_index = 0 : i32} : memref&lt;10xf32&gt;
  linalg.indexed_generic {indexing_maps = [affine_map&lt;(d0) -&gt; ()&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;, affine_map&lt;(d0) -&gt; (d0)&gt;], iterator_types = ["parallel"]} ins(%2, %5, %3, %4 : memref&lt;i32&gt;, memref&lt;10xf32&gt;, memref&lt;10xf32&gt;, memref&lt;10xf32&gt;) outs(%0 : memref&lt;10xf32&gt;) {
  ^bb0(%arg0: index, %arg1: i32, %arg2: f32, %arg3: f32, %arg4: f32, %arg5: f32):  // no predecessors
    %6 = mulf %arg3, %cst_1 : f32
    %7 = tanh %6 : f32
    %8 = mulf %7, %cst_1 : f32
    %9 = addf %8, %cst_1 : f32
    %10 = mulf %9, %arg4 : f32
    %11 = index_cast %arg1 : i32 to index
    %12 = load %1[%11, %c0, %c0] : memref&lt;5x1x1xf32&gt;
    %13 = mulf %12, %cst : f32
    %14 = cmpf "ogt", %13, %cst_0 : f32
    %15 = select %14, %arg2, %10 : f32
    linalg.yield %15 : f32
  }
  return
}
&lt;denchmark-link:https://github.com/asaadaldien&gt;@asaadaldien&lt;/denchmark-link&gt;
 Could you help on this?
	</description>
	<comments>
		<comment id='1' author='hanhanW' date='2020-11-03T20:45:53Z'>
		This is the only test (unidirectional_lstm.mlir test) that blocks the PR, btw.
		</comment>
		<comment id='2' author='hanhanW' date='2020-11-04T16:46:06Z'>
		Looking into this.....
		</comment>
		<comment id='3' author='hanhanW' date='2020-11-04T22:13:27Z'>
		Should be fixed by running CSE  after tensor fusion &lt;denchmark-link:https://github.com/google/iree/pull/3722&gt;#3722&lt;/denchmark-link&gt;
  so we load once, resulting a hal interface on buffers that is indexable and don't violate the ABI contract.
&lt;denchmark-code&gt;   %0 = hal.interface.load.tensor @legacy_io::@arg2, offset = %c0 {operand_result_index = 2 : i32} : tensor&lt;5x1x1xf32&gt;
    %1 = hal.interface.load.tensor @legacy_io::@arg3, offset = %c0 {operand_result_index = 3 : i32} : tensor&lt;i32&gt;
    %2 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 {operand_result_index = 1 : i32} : tensor&lt;10xf32&gt;
    %3 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor&lt;10xf32&gt;
    %4 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor&lt;10xf32
&lt;/denchmark-code&gt;

-&gt;
&lt;denchmark-code&gt;   %0 = hal.interface.load.tensor @legacy_io::@arg2, offset = %c0 {operand_result_index = 2 : i32} : tensor&lt;5x1x1xf32&gt;
    %1 = hal.interface.load.tensor @legacy_io::@arg3, offset = %c0 {operand_result_index = 3 : i32} : tensor&lt;i32&gt;
    %2 = hal.interface.load.tensor @legacy_io::@arg1, offset = %c0 {operand_result_index = 1 : i32} : tensor&lt;10xf32&gt;
    %3 = hal.interface.load.tensor @legacy_io::@arg0, offset = %c0 {operand_result_index = 0 : i32} : tensor&lt;10xf32&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='hanhanW' date='2020-11-05T07:46:52Z'>
		Thanks for the quick fix, this does fix the issue!
		</comment>
	</comments>
</bug>