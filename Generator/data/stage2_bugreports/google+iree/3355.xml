<bug id='3355' author='hanhanW' open_date='2020-10-06T09:14:35Z' closed_time='2020-11-30T09:16:47Z'>
	<summary>Irrelevant ops get folded into one dispatch function.</summary>
	<description>
I'm seeing a dispatch function after iree-flow-transformation-pipeline:
  func @foo(%arg0: tensor&lt;1x384x384xf32&gt;, %arg1: tensor&lt;1x384xf32&gt;) -&gt; (tensor&lt;384x384xf32&gt;, tensor&lt;1x1x384x384xf32&gt;) {
    %cst = constant dense&lt;-1.000000e+04&gt; : tensor&lt;1x1x384x384xf32&gt;
    %cst_0 = constant dense&lt;1.000000e+04&gt; : tensor&lt;1x1x384x384xf32&gt;
    %cst_1 = constant dense&lt;1.000000e+00&gt; : tensor&lt;1x384x384xf32&gt;
    %0 = "mhlo.reshape"(%arg0) : (tensor&lt;1x384x384xf32&gt;) -&gt; tensor&lt;384x384xf32&gt;
    %1 = "mhlo.reshape"(%arg1) : (tensor&lt;1x384xf32&gt;) -&gt; tensor&lt;1x1x384xf32&gt;
    %2 = "mhlo.broadcast_in_dim"(%1) {broadcast_dimensions = dense&lt;[0, 1, 2]&gt; : tensor&lt;3xi64&gt;} : (tensor&lt;1x1x384xf32&gt;) -&gt; tensor&lt;1x384x384xf32&gt;
    %3 = mhlo.multiply %2, %cst_1 : tensor&lt;1x384x384xf32&gt;
    %4 = "mhlo.reshape"(%3) : (tensor&lt;1x384x384xf32&gt;) -&gt; tensor&lt;1x1x384x384xf32&gt;
    %5 = mhlo.multiply %4, %cst_0 : tensor&lt;1x1x384x384xf32&gt;
    %6 = mhlo.add %5, %cst : tensor&lt;1x1x384x384xf32&gt;
    return %0, %6 : tensor&lt;384x384xf32&gt;, tensor&lt;1x1x384x384xf32&gt;
  }
This actually should be two dispatch functions:
  func @foo_1(%arg1: tensor&lt;1x384xf32&gt;) -&gt; (tensor&lt;1x1x384x384xf32&gt;) {
    %cst = constant dense&lt;-1.000000e+04&gt; : tensor&lt;1x1x384x384xf32&gt;
    %cst_0 = constant dense&lt;1.000000e+04&gt; : tensor&lt;1x1x384x384xf32&gt;
    %cst_1 = constant dense&lt;1.000000e+00&gt; : tensor&lt;1x384x384xf32&gt;
    %1 = "mhlo.reshape"(%arg1) : (tensor&lt;1x384xf32&gt;) -&gt; tensor&lt;1x1x384xf32&gt;
    %2 = "mhlo.broadcast_in_dim"(%1) {broadcast_dimensions = dense&lt;[0, 1, 2]&gt; : tensor&lt;3xi64&gt;} : (tensor&lt;1x1x384xf32&gt;) -&gt; tensor&lt;1x384x384xf32&gt;
    %3 = mhlo.multiply %2, %cst_1 : tensor&lt;1x384x384xf32&gt;
    %4 = "mhlo.reshape"(%3) : (tensor&lt;1x384x384xf32&gt;) -&gt; tensor&lt;1x1x384x384xf32&gt;
    %5 = mhlo.multiply %4, %cst_0 : tensor&lt;1x1x384x384xf32&gt;
    %6 = mhlo.add %5, %cst : tensor&lt;1x1x384x384xf32&gt;
    return %6 : tensor&lt;1x1x384x384xf32&gt;
  }

  func @foo_2(%arg0: tensor&lt;1x384x384xf32&gt;) -&gt; (tensor&lt;384x384xf32&gt;) {
    %0 = "mhlo.reshape"(%arg0) : (tensor&lt;1x384x384xf32&gt;) -&gt; tensor&lt;384x384xf32&gt;
    return %0 : tensor&lt;384x384xf32&gt;
  }
One of the dispatch functions is actually just a reshape op, which would causes a redundant memory copy in CPU/GPU backend. I think it's potentially  to move it out this dispatch function and see if it's possible to fuse with it's actual consumer. Or just tell IREE to pass the buffer to consumer directly?
This issue could be related to &lt;denchmark-link:https://github.com/google/iree/issues/1583&gt;#1583&lt;/denchmark-link&gt;
 -- Broadcasts should always fold into consumer dispatch regions, never producer regions.
	</description>
	<comments>
		<comment id='1' author='hanhanW' date='2020-11-22T03:48:29Z'>
		I think you've been making good progress here - anything still to do or is &lt;denchmark-link:https://github.com/google/iree/issues/1354&gt;#1354&lt;/denchmark-link&gt;
 going to remove the need for this?
		</comment>
		<comment id='2' author='hanhanW' date='2020-11-30T09:16:47Z'>
		I think the problem is that we could have a single reshape op in a dispatch function, and we probably want to improve it in hal layer or smoething. However, I highly suspect that this only happens when we have bunch of dont_fuse ops. So yes, we can remove this.
		</comment>
	</comments>
</bug>