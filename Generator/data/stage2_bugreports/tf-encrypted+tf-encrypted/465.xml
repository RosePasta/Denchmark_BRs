<bug id='465' author='avitalsh' open_date='2019-05-06T11:39:29Z' closed_time='2019-06-14T18:55:40Z'>
	<summary>MaxPooling conversion</summary>
	<description>
Hi,
I tried converting the following model to PrivateModel using the secure_model method:
&lt;denchmark-code&gt;input_img = tf.keras.Input(shape=(32, 32, 3))
x = tf.keras.layers.Conv2D(16, (3,3), padding='same')(input_img)
x = tf.keras.layers.MaxPool2D(pool_size=(2,2))(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(10)(x)
model = tf.keras.Model(inputs=input_img, outputs=x)
&lt;/denchmark-code&gt;

And got the following error:
File "/cs/labs/peleg/avitalsh/tools/temp/tf-encrypted/tf_encrypted/private_model.py", line 79, in secure_model
y = c.convert(remove_training_nodes(graph_def), tfe.convert.registry(), 'input-provider', inputs)
File "/cs/labs/peleg/avitalsh/tools/temp/tf-encrypted/tf_encrypted/convert/convert.py", line 92, in convert
outs = op_handler(self, nodes, input_list)
File "/cs/labs/peleg/avitalsh/tools/temp/tf-encrypted/tf_encrypted/convert/register.py", line 508, in flatten
input = converter.outputs[inputs[0]]
KeyError: 'max_pooling2d/MaxPool'
When I remove the MaxPool layer, secure_model works fine.
I am using tfe version 0.5.2. When using older version (0.4.0) it works fine (with the max pool layer).
Is this a bug? or did the API change between the version and i'm not using it correctly?
Thanks
	</description>
	<comments>
		<comment id='1' author='avitalsh' date='2019-05-06T12:23:08Z'>
		In the past, we have mainly stuck with using the tf.nn api when building models in plaintext. As such, the converter was originally built with these in mind, and so many tf.layers and tf.keras.layers wouldn't work by default.  By switching to using that api, you will find fewer bugs in the converter.
To know what's supported and what's not, the registry in tf_encrypted/convert/register.py contains a full list.  If the op generates only one or a few standard ops, it's likely supported (e.g. tf.nn.conv2d generates Conv2D and BiasAdd, which are both supported).
For layers that generate an entire subgraph, e.g. most tf.layers and tf.keras.layers, we usually require that they be registered, given reserved scopes, and converted as special ops.
We are currently working on expanding our support for converting keras layers. Please see &lt;denchmark-link:https://github.com/tf-encrypted/tf-encrypted/blob/master/tf_encrypted/convert/README.md&gt;the converter guide&lt;/denchmark-link&gt;
 for more information on how to add ops, and to see what's already supported.
		</comment>
		<comment id='2' author='avitalsh' date='2019-05-06T13:25:39Z'>
		I'm going to close this up if this takes care of your question, but feel free to reopen otherwise! :)
		</comment>
		<comment id='3' author='avitalsh' date='2019-05-28T10:52:00Z'>
		Hi &lt;denchmark-link:https://github.com/jvmancuso&gt;@jvmancuso&lt;/denchmark-link&gt;
 , Sorry for the late response but I don't think the problem is with the keras api. I tried debugging the conversion of the following model -
&lt;denchmark-code&gt;input_img = tf.keras.layers.Input(shape=(10,10,3))
x = tf.keras.layers.Conv2D(16, (3,3))(input_img)
x = tf.keras.layers.ReLU()(x)
x = tf.keras.layers.AveragePooling2D((2, 2), (2,2))(x)
x = tf.keras.layers.Flatten()(x)

model = tf.keras.Model(inputs=input_img, outputs=x)

s_model = tfe.private_model.secure_model(model)
&lt;/denchmark-code&gt;

In the conversion function there is this for loop -



tf-encrypted/tf_encrypted/convert/convert.py


         Line 82
      in
      1233cdd






 for node in node_list: 





That first works on the input node, which is fine, and then it moves on to the next layer which is conv.
Since conv is considered as a special op, we don't go in here -



tf-encrypted/tf_encrypted/convert/convert.py


         Line 83
      in
      1233cdd






 if node.name not in specop_outputs: 





and instead we move on the the else statement, where we enter this loop:



tf-encrypted/tf_encrypted/convert/convert.py


         Line 100
      in
      1233cdd






 for s in specop_dict: 





In the first iteration s is conv - good. In the next iteration s is Flatten.
The problem occurs when we reach this line -



tf-encrypted/tf_encrypted/convert/convert.py


         Line 110
      in
      1233cdd






 outs = op_handler(self, nodes, input_list) 





We run the op_handler function, which in this case is -



tf-encrypted/tf_encrypted/convert/register.py


         Line 544
      in
      1233cdd






 def _flatten(converter, node, inputs): 





Here we are looking for the input -



tf-encrypted/tf_encrypted/convert/register.py


         Line 545
      in
      1233cdd






 x_in = converter.outputs[inputs[0]] 





which should be the output of the AvgPool layer but since we didn't reach it yet in the conver function, we don't have the output yet -
&lt;denchmark-link:https://user-images.githubusercontent.com/17612668/58472619-52057680-814f-11e9-9ad3-b5d3def983f2.png&gt;&lt;/denchmark-link&gt;

And then we crash.
So I think the problem is with this for loop -



tf-encrypted/tf_encrypted/convert/convert.py


         Line 100
      in
      1233cdd






 for s in specop_dict: 





		</comment>
		<comment id='4' author='avitalsh' date='2019-05-30T15:41:28Z'>
		taking a look now!
		</comment>
		<comment id='5' author='avitalsh' date='2019-06-13T07:16:15Z'>
		Hi &lt;denchmark-link:https://github.com/jvmancuso&gt;@jvmancuso&lt;/denchmark-link&gt;
, did you get a chance to look into this?
		</comment>
		<comment id='6' author='avitalsh' date='2019-06-13T13:16:33Z'>
		Hi &lt;denchmark-link:https://github.com/avitalsh&gt;@avitalsh&lt;/denchmark-link&gt;
, I looked into it briefly before having to jump on something else.  I'll reinvestigate today and plan get back to you by tomorrow!
		</comment>
		<comment id='7' author='avitalsh' date='2019-06-14T16:42:42Z'>
		Hi &lt;denchmark-link:https://github.com/avitalsh&gt;@avitalsh&lt;/denchmark-link&gt;
, I've found two bugs related to this issue.
The problem with the for-loop was that it was registering all special ops as soon as it reached the first one, which dependencies to later special ops may not have been registered yet.  I've solved this by filtering out special ops not associated with the current node/subgraph being converted.
Another bug was in the function match_numbered_specop in convert.py.  The regex built to match numbered scopes for special ops was only recognizing unnumbered ones, i.e. model/conv2d/... would match on conv2d but model/conv2d_1/... would not match on conv2d_1, which again meant that certain ops/layers that were dependencies for other layers would go unregistered.  I fixed this by modifying the function to capture the right groups from the original regex.
I'm adding these fixes in an upcoming PR, which also has some new functionality around inspecting Keras models that should be helpful when checking for convertibility.
I also realized that depending on how you're calling secure_model, it could try to convert into Pond by default, in which case the pooling layer would throw an error. The script below is how I recreated and solved these issues, and they can also be used as an example for how to use the new tfe.convert functionality.
import tensorflow as tf
import tf_encrypted as tfe

shape = (10, 10, 3)
x = tf.keras.layers.Input(shape=shape)
y = tf.keras.layers.Conv2D(16, (3, 3))(x)
y = tf.keras.layers.MaxPooling2D((2, 2), (2, 2))(y)
y = tf.keras.layers.Flatten()(y)
y = tf.keras.layers.Dense(10)(y)

model = tf.keras.Model(inputs=x, outputs=y)

# Helper to inspect the incoming graph, to ensure that TFE has conversion
# functions for everything you're requesting.
sess = tf.keras.backend.get_session()
tfe.convert.inspect_subgraph(model, shape, sess)

# Idiomatic way of converting in a specific protocol
with tfe.protocol.SecureNN():
  s_model = tfe.private_model.secure_model(model)

# This one should work as well
prot = tfe.protocol.SecureNN()
s_model = tfe.private_model.secure_model(model, protocol=prot)
		</comment>
	</comments>
</bug>