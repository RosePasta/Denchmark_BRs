<bug id='488' author='eulerreich' open_date='2015-10-05T09:46:51Z' closed_time='2015-10-23T01:36:46Z'>
	<summary>numerical_grad caution</summary>
	<description>
I was implementing a differentiable Transpose function.
&lt;denchmark-code&gt;class Transpose(Function):
    def forward(self, inputs):
        x = inputs[0]
        return x.transpose(),
    def backward(self, inputs, grads):
        return grads[0].transpose(),
&lt;/denchmark-code&gt;

While testing the gradient with numerical_grad,
&lt;denchmark-code&gt;    def test_numerical_grad_cpu(self):
        x = np.random.randn(1, 10)
        x_var = Variable(x)
        y_var = self.f(x_var)
        y_var.grad = np.random.rand(10, 1)
        y_var.backward()
        cl = lambda: self.f.forward((x,))
        gx, = gradient_check.numerical_grad(cl, (x,), (y_var.grad,))
        gradient_check.assert_allclose(gx, x_var.grad)
&lt;/denchmark-code&gt;

(here self.f = Transpose())
the numerical gradient gx keeps coming back as 0. After much frustration, I finally figured out that I was returning a view of x in the above code, and in numerical_grad_cpu,
&lt;denchmark-code&gt;            flat_x[i] = orig + eps
            ys1 = f()
            flat_x[i] = orig - eps
            ys2 = f()
            flat_x[i] = orig
&lt;/denchmark-code&gt;

ys1 and ys2 end up being equal after the last line resetting flat_x[i] to the original value. I solved my problem by changing cl = lambda: self.f.forward((x,)) to cl = lambda: np.copy(self.f.forward((x,))).
I'm not sure how frequent this phenomenon could occur outside of transpose, but I just wanted to put this out here so that there could be a discussion. Perhaps a passing note in the documentation suffices here. Or doing ys1 = np.copy(f()) instead might work as well.
	</description>
	<comments>
		<comment id='1' author='eulerreich' date='2015-10-05T10:06:07Z'>
		Yes, I ran into exactly the same issue while implementing exactly the same function (see &lt;denchmark-link:https://github.com/chainer/chainer/pull/480&gt;#480&lt;/denchmark-link&gt;
). This will happen for all reshaping functions and generally all functions that re-use the memory of the input. There is a way to check if two arrays are sharing memory (&lt;denchmark-link:http://stackoverflow.com/questions/11286864/is-there-a-way-to-check-if-numpy-arrays-share-the-same-data&gt;http://stackoverflow.com/questions/11286864/is-there-a-way-to-check-if-numpy-arrays-share-the-same-data&lt;/denchmark-link&gt;
), perhaps numerical_grad should check this.
		</comment>
		<comment id='2' author='eulerreich' date='2015-10-06T00:58:37Z'>
		Thank you for the report! I have not noticed it. We should add a copy on numerical_grad. Copying without check might be enough, since small arrays are used for gradient checking. If it prohibitively slows down the whole tests, we should think about checking the memory share.
		</comment>
	</comments>
</bug>