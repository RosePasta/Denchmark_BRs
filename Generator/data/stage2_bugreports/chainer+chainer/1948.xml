<bug id='1948' author='benkrause' open_date='2016-12-01T16:47:54Z' closed_time='2017-12-04T09:02:48Z'>
	<summary>Problem training Penn treebank example with large hidden size on GPU</summary>
	<description>
There seems to be a problem training the
&lt;denchmark-link:https://github.com/pfnet/chainer/blob/master/examples/ptb/train_ptb.py#L139&gt; penn treebank tutorial example&lt;/denchmark-link&gt;

using large networks on a single GPU. When using a large hidden state and large minibatch, (in my case minibatch = 100 and hidden state size 4000 on a 12 GB tesla k40 GPU, although others have replicated my issue on other GPUs in python 2.7 ,python 3.4 and python 3.5), the program freezes at line 139 at the call to loss.backward(). This issue can be fixed in a very hacky way by inserting a statement print(loss.data) right before loss.backward() is called. With this print statement, training works correctly.
	</description>
	<comments>
		<comment id='1' author='benkrause' date='2016-12-01T19:08:38Z'>
		I have the same issue
		</comment>
		<comment id='2' author='benkrause' date='2017-12-04T09:02:32Z'>
		No longer able to reproduce the problem, so I will close the issue. It works for me using the specified parameters on 16GB P100.
		</comment>
	</comments>
</bug>