<bug id='261' author='houxianxu' open_date='2015-07-25T12:46:27Z' closed_time='2015-07-28T02:18:11Z'>
	<summary>Maybe there is a bug in chainer.optimizer.weight_decay(decay).</summary>
	<description>
In the &lt;denchmark-link:https://github.com/pfnet/chainer/blob/30807b4f9f40177f592099663b86a6bb3eb1f867/chainer/optimizer.py&gt;optimizer.py&lt;/denchmark-link&gt;
 file,   is implemented as following:
 def weight_decay(self, decay):
     for p, g, _ in self.tuples:
         g -= decay * p
However, the gradient is decayed, not the weight. So if we update the weights param -= lr * grad, because the grad becomes smaller, the param (weight) becomes larger, which is not we want.
As far as I know the regularized gradient decent is p = p * (1-decay) - lr * grad, so I think the weight should be decayed not the gradient.
	</description>
	<comments>
		<comment id='1' author='houxianxu' date='2015-07-27T06:24:44Z'>
		Ah, you are right. It is a bug of Optimizer. We should add the scaled weights to the gradients instead of subtract them. I cannot believe that everyone except you has not noticed. Thank you for the catch!
		</comment>
	</comments>
</bug>