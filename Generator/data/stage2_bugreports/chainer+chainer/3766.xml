<bug id='3766' author='niboshi' open_date='2017-11-02T04:41:34Z' closed_time='2019-08-15T09:36:03Z'>
	<summary>Too large error in numerical function tests</summary>
	<description>
This is a list of tests I noticed failure but could not fix by adjusting tolerances, due to too large errors.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/niboshi&gt;@niboshi&lt;/denchmark-link&gt;


 TestBatchNormalization.test_double_backward_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/bkvogel&gt;@bkvogel&lt;/denchmark-link&gt;


 TestBatchRenormalization.test_backward_cpu
 TestBatchRenormalization.test_forward_cpu
 TestFixedBatchRenormalization.test_backward_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/mitmul&gt;@mitmul&lt;/denchmark-link&gt;


 TestDeconvolution2DFunction.test_backward_cpu
 TestDeconvolutionND.test_backward_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/beam2d&gt;@beam2d&lt;/denchmark-link&gt;


 TestNStepBiRNN.test_backward_cpu
 TestNStepRNN.test_backward_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/kmaehashi&gt;@kmaehashi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/chainer/chainer/pull/4838&gt;#4838&lt;/denchmark-link&gt;


 TestMin.test_backward_multi_axis_invert_cpu
 TestMin.test_backward_negative_multi_axis_invert_cpu
 TestMax.test_backward_multi_axis_invert_cpu
 TestMax.test_backward_negative_multi_axis_invert_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/asi1024&gt;@asi1024&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/chainer/chainer/pull/4771&gt;#4771&lt;/denchmark-link&gt;


 TestAverage_param_26.test_backward_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/toslunar&gt;@toslunar&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/chainer/chainer/pull/5210&gt;#5210&lt;/denchmark-link&gt;


 TestL2Normalization.test_double_backward_cpu: https://ci.appveyor.com/project/pfnet/chainer/build/11072/job/3xwi9wm2ycjo8cqb, internal CI (daily_master_chainer-shuffled task 631, ID=10)
 TestL2Normalization_param_18.test_backward_cpu: https://ci.appveyor.com/project/pfnet/chainer/build/11015/job/99ivaixp53op2ido

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/delta2323&gt;@delta2323&lt;/denchmark-link&gt;


 TestMatMulVarVar_param_1.test_double_backward_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/toslunar&gt;@toslunar&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/chainer/chainer/pull/5218&gt;#5218&lt;/denchmark-link&gt;


 TestContrastive.test_double_backward_cpu

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

(unassigned)

 SpatialTransformerSampler. test_consistency_with_cudnn_gpu

	</description>
	<comments>
		<comment id='1' author='niboshi' date='2017-11-08T01:28:58Z'>
		Perhaps some of the failures are due to non-differentiable inputs.
&lt;denchmark-link:https://github.com/chainer/chainer/pull/3551&gt;#3551&lt;/denchmark-link&gt;
 could be helpful to solve them.
		</comment>
		<comment id='2' author='niboshi' date='2017-11-29T05:32:07Z'>
		TestAverage_param_26.test_backward_cpu generated quite a large error. Should this test be added to the list?
&lt;denchmark-code&gt;E                   AssertionError: 
E                   Not equal to tolerance rtol=0.1, atol=0.01
E                   
E                   (mismatch 100.0%)
E                    x: array(-0.2126242916502541)
E                    y: array(-0.13950088823685647)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://travis-ci.org/chainer/chainer/jobs/308804629&gt;https://travis-ci.org/chainer/chainer/jobs/308804629&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='niboshi' date='2017-12-02T04:17:52Z'>
		The double backward tests of Huber loss are also unstable. &lt;denchmark-link:https://github.com/chainer/chainer/pull/3867&gt;#3867&lt;/denchmark-link&gt;
 (not merged).
		</comment>
		<comment id='4' author='niboshi' date='2017-12-12T15:45:45Z'>
		I added . (&lt;denchmark-link:https://ci.appveyor.com/project/pfnet/chainer/build/7918/job/m340mgpqvi5eysk2&gt;https://ci.appveyor.com/project/pfnet/chainer/build/7918/job/m340mgpqvi5eysk2&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='5' author='niboshi' date='2018-04-27T04:01:19Z'>
		Added TestMatMulVarVar_param_1.test_double_backward_cpu.
		</comment>
		<comment id='6' author='niboshi' date='2018-05-14T05:34:35Z'>
		Assignment:
&lt;denchmark-link:https://github.com/asi1024&gt;@asi1024&lt;/denchmark-link&gt;
: average
&lt;denchmark-link:https://github.com/kmaehashi&gt;@kmaehashi&lt;/denchmark-link&gt;
: minmax
&lt;denchmark-link:https://github.com/niboshi&gt;@niboshi&lt;/denchmark-link&gt;
: barchnorm
&lt;denchmark-link:https://github.com/delta2323&gt;@delta2323&lt;/denchmark-link&gt;
: matmul
&lt;denchmark-link:https://github.com/mitmul&gt;@mitmul&lt;/denchmark-link&gt;
: deconv
&lt;denchmark-link:https://github.com/beam2d&gt;@beam2d&lt;/denchmark-link&gt;
: nsteprnn
&lt;denchmark-link:https://github.com/hvy&gt;@hvy&lt;/denchmark-link&gt;
: l2norm
&lt;denchmark-link:https://github.com/bkvogel&gt;@bkvogel&lt;/denchmark-link&gt;
: barchrenorm
		</comment>
		<comment id='7' author='niboshi' date='2018-05-14T07:32:48Z'>
		TestL2Normalization.test_double_backward_cpu seems to be fixed.
Could not reproduce the error after over 20000 repeats, and am currently running more repetitions. I also ran the corresponding GPU tests but without failures. Probably fixed by &lt;denchmark-link:https://github.com/chainer/chainer/pull/4190&gt;#4190&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='8' author='niboshi' date='2018-05-14T15:42:51Z'>
		Sorry, although no errors were encountered in the runs above, zero-divisions may occur. It is currently under discussion here &lt;denchmark-link:https://github.com/chainer/chainer/pull/4769&gt;#4769&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='9' author='niboshi' date='2018-05-31T05:57:49Z'>
		
TestMin.test_backward_multi_axis_invert_cpu
TestMin.test_backward_negative_multi_axis_invert_cpu
TestMax.test_backward_multi_axis_invert_cpu
TestMax.test_backward_negative_multi_axis_invert_cpu

I found other  cases are also failing sometimes (only on CPU).
Fixed in &lt;denchmark-link:https://github.com/chainer/chainer/pull/4825&gt;#4825&lt;/denchmark-link&gt;
. Confirmed to pass 20000 repeats.
		</comment>
		<comment id='10' author='niboshi' date='2018-06-05T09:17:24Z'>
		I tried  1000 times and found no errors. Do you think it is reasonable? &gt;  &lt;denchmark-link:https://github.com/kmaehashi&gt;@kmaehashi&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='niboshi' date='2018-06-11T06:28:03Z'>
		&lt;denchmark-link:https://github.com/delta2323&gt;@delta2323&lt;/denchmark-link&gt;


I tried  TestMatMulVarVar_param_*.test_double_backward_cpu 1000 times and found no errors. Do you think it is reasonable? &gt;  @kmaehashi

I tested and got error after 6234 repeats.
		</comment>
		<comment id='12' author='niboshi' date='2018-06-13T05:56:43Z'>
		Thank you for investigation. &lt;denchmark-link:https://github.com/toslunar&gt;@toslunar&lt;/denchmark-link&gt;
 told me that he encountered two test failures: one with float16 in AppVeyor (&lt;denchmark-link:https://ci.appveyor.com/project/pfnet/chainer/build/10580/job/3qn9udhdj5uf8uu1&gt;link&lt;/denchmark-link&gt;
) and the other with float64 in a Linux environment.
		</comment>
		<comment id='13' author='niboshi' date='2018-06-27T02:50:02Z'>
		&lt;denchmark-link:https://github.com/hvy&gt;@hvy&lt;/denchmark-link&gt;


TestL2Normalization.test_double_backward_cpu seems to be fixed.

It seems the error still exists.
&lt;denchmark-code&gt;E           assert_allclose failed: 
E             shape: () ()
E             dtype: float64 float64
E             i: (0,)
E             x[i]: 19.363414248235493
E             y[i]: 16.92678512278495
E             err[i]: 2.436629125450544
&lt;/denchmark-code&gt;

Found in &lt;denchmark-link:https://ci.appveyor.com/project/pfnet/chainer/build/11072/job/3xwi9wm2ycjo8cqb&gt;https://ci.appveyor.com/project/pfnet/chainer/build/11072/job/3xwi9wm2ycjo8cqb&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='niboshi' date='2018-06-27T03:23:12Z'>
		I see, thank you for the link. For the record, some changes related to  has been merged since then and is going to &lt;denchmark-link:https://github.com/chainer/chainer/pull/4769&gt;#4769&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='15' author='niboshi' date='2018-06-27T09:14:29Z'>
		Added TestL2Normalization_param_18.test_backward_cpu.
Modified the comment at the top as checkbox add added links to evidence URL.
		</comment>
		<comment id='16' author='niboshi' date='2018-07-17T07:35:41Z'>
		This one (TestContrastive) too? &lt;denchmark-link:https://ci.appveyor.com/project/pfnet/chainer/build/11408/job/k13ud3mudyo3l818&gt;https://ci.appveyor.com/project/pfnet/chainer/build/11408/job/k13ud3mudyo3l818&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;E           Base test method: TestContrastive.test_double_backward_cpu
E           Test parameters:
E             margin: 2
E             batchsize: 10
E             label_dtype: &lt;type 'numpy.int32'&gt;
E             input_dim: 3
E             reduce: no

...

E           gradients (numeric):  0.0777021832889
E           gradients (backward): 0.118000078067
E           
E           
E           Not equal to tolerance rtol=0.01, atol=0.001
E           
E           (mismatch 100.0%)
E            x: array(0.077702)
E            y: array(0.118)
E           
E           assert_allclose failed: 
E             shape: () ()
E             dtype: float64 float64
E             i: (0,)
E             x[i]: 0.0777021832889
E             y[i]: 0.118000078067
E             err[i]: 0.0402978947785
E           x: 0.07770218
E           y: 0.11800008
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='niboshi' date='2018-07-23T02:55:01Z'>
		Found another one:
&lt;denchmark-code&gt;tests.chainer_tests.functions_tests.array_tests.test_spatial_transformer_sampler.TestSpatialTransformerSamplerConsistencyWithCuDNN.test_consistency_with_cudnn_gpu
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;    @attr.gpu
    @attr.cudnn
    def test_consistency_with_cudnn_gpu(self):
        with chainer.using_config('use_cudnn', 'never'):
            x_gpu, grid_gpu, y_gpu = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))
        with chainer.using_config('use_cudnn', 'always'):
            x_cudnn, grid_cudnn, y_cudnn = self._apply_backward(
                cuda.to_gpu(self.x), cuda.to_gpu(self.grid),
                cuda.to_gpu(self.grads))
    
        testing.assert_allclose(y_gpu.data, y_cudnn.data)
        testing.assert_allclose(x_gpu.grad, x_cudnn.grad)
&gt;       testing.assert_allclose(grid_gpu.grad, grid_cudnn.grad)

grid_cudnn = variable([[[[-1.61672306, -1.87297201, -1.80047035],
            [ 1.27407634,...92229629,  1.64249659],
            [ 1.49996197,  1.31562626,  1.49765813]]]])
grid_gpu   = variable([[[[-1.61672306, -1.87297201, -1.80047035],
            [ 1.27407634,...92229629,  1.64249659],
            [ 1.49996197,  1.31562626,  1.49765813]]]])
self       = &lt;chainer_tests.functions_tests.array_tests.test_spatial_transformer_sampler.Te...nsformerSamplerConsistencyWithCuDNN testMethod=test_consistency_with_cudnn_gpu&gt;
x_cudnn    = variable([[[[ 0.72775787,  0.46074685,  0.90299946,  0.98334837],
            ...48426458],
            [ 0.82393873,  0.53375387,  0.14992146,  0.67558402]]]])
x_gpu      = variable([[[[ 0.72775787,  0.46074685,  0.90299946,  0.98334837],
            ...48426458],
            [ 0.82393873,  0.53375387,  0.14992146,  0.67558402]]]])
y_cudnn    = variable([[[[ 0.05469165,  0.        ,  0.        ],
            [ 0.49642903,...        ,  0.        ],
            [ 0.08204111,  0.40948758,  0.19721556]]]])
y_gpu      = variable([[[[ 0.05469165,  0.        ,  0.        ],
            [ 0.49642903,...        ,  0.        ],
            [ 0.08204111,  0.40948755,  0.19721536]]]])

tests/chainer_tests/functions_tests/array_tests/test_spatial_transformer_sampler.py:146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

...

E           AssertionError: 
E           Not equal to tolerance rtol=0.0001, atol=1e-05
E           
E           (mismatch 2.77777777778%)
E            x: array([[[[  6.434778e-01,   0.000000e+00,   0.000000e+00],
E                    [ -4.512197e-01,   2.712164e-01,   9.718041e-02],
E                    [ -5.254220e-01,   1.123957e-04,   0.000000e+00]],...
E            y: array([[[[  6.434778e-01,   0.000000e+00,   0.000000e+00],
E                    [ -4.512197e-01,   2.712166e-01,   9.718043e-02],
E                    [  5.601417e-01,   1.123729e-04,   0.000000e+00]],...
E           
E           assert_allclose failed: 
E             shape: (2, 2, 3, 3) (2, 2, 3, 3)
E             dtype: float32 float32
E             i: (0, 0, 2, 0)
E             x[i]: -0.525421977043
E             y[i]: 0.560141682625
E             err[i]: 1.08556365967
E           x: [[[[  6.43477798e-01   0.00000000e+00   0.00000000e+00]
E                 [ -4.51219708e-01   2.71216393e-01   9.71804112e-02]
E                 [ -5.25421977e-01   1.12395734e-04   0.00000000e+00]]
E           
E                [[  2.30249483e-02   0.00000000e+00   0.00000000e+00]
E                 [  3.09955776e-01  -1.71801639e+00   3.40243518e-01]
E                 [  6.58333004e-02   1.67952836e-01   0.00000000e+00]]]
E           
E           
E               [[[ -2.75782168e-01  -0.00000000e+00   0.00000000e+00]
E                 [  0.00000000e+00   0.00000000e+00  -0.00000000e+00]
E                 [ -9.66350511e-02   5.39744616e-01  -9.67617631e-02]]
E           
E                [[  5.15703976e-01   0.00000000e+00  -0.00000000e+00]
E                 [  0.00000000e+00  -0.00000000e+00   0.00000000e+00]
E                 [ -1.87675714e-01  -9.67469037e-01  -1.82941461e+00]]]]
E           y: [[[[  6.43477798e-01   0.00000000e+00   0.00000000e+00]
E                 [ -4.51219738e-01   2.71216571e-01   9.71804261e-02]
E                 [  5.60141683e-01   1.12372902e-04   0.00000000e+00]]
E           
E                [[  2.30249502e-02   0.00000000e+00   0.00000000e+00]
E                 [  3.09955776e-01  -1.71801639e+00   3.40243548e-01]
E                 [  6.58333376e-02   1.67952836e-01   0.00000000e+00]]]
E           
E           
E               [[[ -2.75782049e-01   0.00000000e+00   0.00000000e+00]
E                 [  0.00000000e+00   0.00000000e+00   0.00000000e+00]
E                 [ -9.66350585e-02   5.39744616e-01  -9.67618525e-02]]
E           
E                [[  5.15704036e-01   0.00000000e+00   0.00000000e+00]
E                 [  0.00000000e+00   0.00000000e+00   0.00000000e+00]
E                 [ -1.87675729e-01  -9.67469096e-01  -1.82941461e+00]]]]

atol       = 1e-05
e          = AssertionError('\nNot equal to tolerance rtol=0.0001, atol=1e-05\n\n(mismatch ...718043e-02],\n         [  5.601417e-01,   1.123729e-04,   0.000000e+00]],...',)
err        = array([[[[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],
         [  2...      [  1.49011612e-08,   5.96046448e-08,   0.00000000e+00]]]], dtype=float32)
f          = &lt;StringIO.StringIO instance at 0x7f2b51645320&gt;
i          = (0, 0, 2, 0)
opts       = {'edgeitems': 3, 'formatter': None, 'infstr': 'inf', 'linewidth': 75, ...}
rtol       = 0.0001
verbose    = True
x          = array([[[[  6.43477798e-01,   0.00000000e+00,   0.00000000e+00],
         [ -4...      [ -1.87675714e-01,  -9.67469037e-01,  -1.82941461e+00]]]], dtype=float32)
xx         = array([[[[  6.43477798e-01,   0.00000000e+00,   0.00000000e+00],
         [ -4...      [ -1.87675714e-01,  -9.67469037e-01,  -1.82941461e+00]]]], dtype=float32)
y          = array([[[[  6.43477798e-01,   0.00000000e+00,   0.00000000e+00],
         [ -4...      [ -1.87675729e-01,  -9.67469096e-01,  -1.82941461e+00]]]], dtype=float32)
yy         = array([[[[  6.43477798e-01,   0.00000000e+00,   0.00000000e+00],
         [ -4...      [ -1.87675729e-01,  -9.67469096e-01,  -1.82941461e+00]]]], dtype=float32)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='niboshi' date='2018-08-16T08:01:53Z'>
		I found tests.chainer_tests.functions_tests.loss_tests.test_triplet.TestTriplet_param_25.test_double_backward_cpu failed:
&lt;denchmark-code&gt;E           Not equal to tolerance rtol=5, atol=5
E           
E           (mismatch 100.0%)
E            x: array(607.9949159175158)
E            y: array(0.5946062767563218)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='niboshi' date='2018-08-20T08:38:00Z'>
		Got this error in tests.chainer_tests.distributions_tests.test_multivariate_normal.TestTriangularInv_param_0.test_backward_cpu:
&lt;denchmark-code&gt;E           AssertionError: Parameterized test failed.
E           
E           Base test method: TestTriangularInv.test_backward_cpu
E           Test parameters:
E             lower: True
E             d: 3
E             dtype: &lt;class 'numpy.float32'&gt;
E           
E           AssertionError: check_backward failed (eps=0.001 atol=0.01 rtol=0.01)
E           inputs[0]:
E           variable([[ 1.0614652e-03  0.0000000e+00  0.0000000e+00]
E                     [ 4.7808385e+00 -1.3598840e+01  0.0000000e+00]
E                     [-5.9871264e+00 -6.2220435e+00  5.6753006e+00]])
E           grad_outputs[0]:
E           [[-0.19745655  0.6807689  -0.91099995]
E            [-0.82742625  1.2080469   0.16151609]
E            [-0.6044902  -1.4036778  -0.10081272]]
E           directions[0]:
E           [[ 0.61543048  0.51692869  0.2387296 ]
E            [ 0.11662292  0.44575042 -0.06727291]
E            [ 0.26605338  0.04845154 -0.0841801 ]]
E           gradients (numeric):  1118226.5049290995
E           gradients (backward): 742323.1963387653
E           
E           
E           Not equal to tolerance rtol=0.01, atol=0.01
E           
E           (mismatch 100.0%)
E            x: array(1118226.504929)
E            y: array(742323.196339)
E           
E           assert_allclose failed: 
E             shape: () ()
E             dtype: float64 float64
E             i: (0,)
E             x[i]: 1118226.5049290995
E             y[i]: 742323.1963387653
E             err[i]: 375903.30859033414
E           x: 1118226.5049291
E           y: 742323.19633877

atol       = 0.01
casted_data = [array([[ 1.0614652e-03,  0.0000000e+00,  0.0000000e+00],
       [ 4.7808385e+00, -1.3598840e+01,  0.0000000e+00],
       [-5.9871264e+00, -6.2220435e+00,  5.6753006e+00]], dtype=float32)]
d_         = array([[ 0.61543048,  0.51692869,  0.2387296 ],
       [ 0.11662292,  0.44575042, -0.06727291],
       [ 0.26605338,  0.04845154, -0.0841801 ]])
delta      = array(0.)
detect_nondifferentiable = False
direction  = array([[ 0.61543048,  0.51692869,  0.2387296 ],
       [ 0.11662292,  0.44575042, -0.06727291],
       [ 0.26605338,  0.04845154, -0.0841801 ]])
directions = [array([[ 0.61543048,  0.51692869,  0.2387296 ],
       [ 0.11662292,  0.44575042, -0.06727291],
       [ 0.26605338,  0.04845154, -0.0841801 ]])]
dtype      = None
eps        = 0.001
f          = &lt;_io.StringIO object at 0x7fbd36362678&gt;
func       = &lt;function TestTriangularInv.check_backward.&lt;locals&gt;.f at 0x7fbd34230048&gt;
g          = array([[ 1.2062048e+06,  0.0000000e+00,  0.0000000e+00],
       [-1.0323401e+02, -3.6291389e+01,  0.0000000e+00],
       [ 1.0034475e+02,  3.5259232e+01,  1.4451724e+02]], dtype=float32)
grads      = [array([[ 1.2062048e+06,  0.0000000e+00,  0.0000000e+00],
       [-1.0323401e+02, -3.6291389e+01,  0.0000000e+00],
       [ 1.0034475e+02,  3.5259232e+01,  1.4451724e+02]], dtype=float32)]
gx         = array(1118226.5049291)
gx_accum   = 742323.1963387653
gy_        = array([[-0.19745655,  0.6807689 , -0.91099995],
       [-0.82742625,  1.2080469 ,  0.16151609],
       [-0.6044902 , -1.4036778 , -0.10081272]], dtype=float32)
i          = 0
no_grads   = [False]
norm       = 3.2279426728987732
params     = ()
rtol       = 0.01
scale      = 0.30979484499394006
skip       = False
variables  = [variable([[ 1.0614652e-03,  0.0000000e+00,  0.0000000e+00],
          [ 4.7808385e+00, -1.3598840e+01,  0.0000000e+00],
          [-5.9871264e+00, -6.2220435e+00,  5.6753006e+00]])]
x          = variable([[ 1.0614652e-03,  0.0000000e+00,  0.0000000e+00],
          [ 4.7808385e+00, -1.3598840e+01,  0.0000000e+00],
          [-5.9871264e+00, -6.2220435e+00,  5.6753006e+00]])
x_         = variable([[ 1.0614652e-03,  0.0000000e+00,  0.0000000e+00],
          [ 4.7808385e+00, -1.3598840e+01,  0.0000000e+00],
          [-5.9871264e+00, -6.2220435e+00,  5.6753006e+00]])
x_data     = (array([[ 1.0614652e-03,  0.0000000e+00,  0.0000000e+00],
       [ 4.7808385e+00, -1.3598840e+01,  0.0000000e+00],
       [-5.9871264e+00, -6.2220435e+00,  5.6753006e+00]], dtype=float32),)
xp         = &lt;module 'numpy' from '/usr/local/lib/python3.5/dist-packages/numpy/__init__.py'&gt;
xs         = [variable([[ 1.0614652e-03,  0.0000000e+00,  0.0000000e+00],
          [ 4.7808385e+00, -1.3598840e+01,  0.0000000e+00],
          [-5.9871264e+00, -6.2220435e+00,  5.6753006e+00]])]
y          = variable([])
y0_data    = [array([[ 9.4209399e+02,  0.0000000e+00,  0.0000000e+00],
       [ 3.3120465e+02, -7.3535688e-02,  0.0000000e+00],
       [ 1.3569686e+03, -8.0619916e-02,  1.7620212e-01]], dtype=float32)]
y_grad     = (array([[-0.19745655,  0.6807689 , -0.91099995],
       [-0.82742625,  1.2080469 ,  0.16151609],
       [-0.6044902 , -1.4036778 , -0.10081272]], dtype=float32),)

chainer/gradient_check.py:588: AssertionError
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='niboshi' date='2018-09-21T06:15:01Z'>
		I found  is still failing after &lt;denchmark-link:https://github.com/chainer/chainer/pull/5218&gt;#5218&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;E           AssertionError: Parameterized test failed.
E           
E           Base test method: TestContrastive.test_double_backward_cpu
E           Test parameters:
E             input_dim: 2
E             backward_options: {'rtol': 0.01, 'atol': 0.001}
E             forward_options: {'rtol': 0.01}
E             margin: 1
E             label_dtype: &lt;class 'numpy.int32'&gt;
E             dtype: &lt;class 'numpy.float32'&gt;
E             double_backward_options: {'rtol': 0.01, 'atol': 0.001}
E             batchsize: 5
E             reduce: mean
E           
E           AssertionError: check_double_backward failed (eps=0.001 atol=0.001 rtol=0.01)
E           input[0]:
E           [[-0.14472875  0.38679275]
E            [-0.9992647  -0.7701047 ]
E            [-0.22768772 -0.8715753 ]
E            [ 0.76145566 -0.10648748]
E            [-0.73959285 -0.92173237]]
E           input[1]:
E           [[ 0.81811965  0.6055438 ]
E            [-0.98366743 -0.77628446]
E            [ 0.48487967 -0.85336643]
E            [-0.544537    0.7870635 ]
E            [-0.688919   -0.07867873]]
E           grad_output[0]:
E           0.43653738498687744
E           grad_grad_input[0]:
E           [[-0.6076435   0.7568892 ]
E            [ 0.93220085  0.21079278]
E            [-0.96770096 -0.26870918]
E            [ 0.02825783 -0.1825841 ]
E            [-0.28561124 -0.10305689]]
E           grad_grad_input[1]:
E           [[-0.22836222 -0.9758099 ]
E            [-0.82734865  0.9971455 ]
E            [-0.74626887 -0.4890506 ]
E            [ 0.97995704  0.489104  ]
E            [ 0.6077846  -0.64225864]]
E           
E           check_backward failed (eps=0.001 atol=0.001 rtol=0.01)
E           inputs[0]:
E           variable([[-0.14472875  0.38679275]
E                     [-0.9992647  -0.7701047 ]
E                     [-0.22768772 -0.8715753 ]
E                     [ 0.76145566 -0.10648748]
E                     [-0.73959285 -0.92173237]])
E           inputs[1]:
E           variable([[ 0.81811965  0.6055438 ]
E                     [-0.98366743 -0.77628446]
E                     [ 0.48487967 -0.85336643]
E                     [-0.544537    0.7870635 ]
E                     [-0.688919   -0.07867873]])
E           inputs[2]:
E           variable(0.43653738)
E           grad_outputs[0]:
E           [[-0.6076435   0.7568892 ]
E            [ 0.93220085  0.21079278]
E            [-0.96770096 -0.26870918]
E            [ 0.02825783 -0.1825841 ]
E            [-0.28561124 -0.10305689]]
E           grad_outputs[1]:
E           [[-0.22836222 -0.9758099 ]
E            [-0.82734865  0.9971455 ]
E            [-0.74626887 -0.4890506 ]
E            [ 0.97995704  0.489104  ]
E            [ 0.6077846  -0.64225864]]
E           directions[0]:
E           [[-0.20586937  0.07702922]
E            [-0.10904909 -0.23849174]
E            [ 0.26469029  0.00451734]
E            [ 0.34906137 -0.2926354 ]
E            [-0.16201092  0.3161479 ]]
E           directions[1]:
E           [[ 2.71197778e-02 -1.14667988e-01]
E            [ 2.72654069e-01  1.56824075e-01]
E            [-2.38290250e-01  2.31144020e-02]
E            [ 6.22940796e-02 -5.15884130e-02]
E            [ 3.64625662e-01  2.76638880e-04]]
E           directions[2]:
E           0.4117377024962388
E           gradients (numeric):  -0.06160164762134057
E           gradients (backward): -0.06352012697316761
E           
E           
E           Not equal to tolerance rtol=0.01, atol=0.001
E           
E           (mismatch 100.0%)
E            x: array(-0.061602)
E            y: array(-0.06352)
E           
E           assert_allclose failed: 
E             shape: () ()
E             dtype: float64 float64
E             i: (0,)
E             x[i]: -0.06160164762134057
E             y[i]: -0.06352012697316761
E             err[i]: 0.0019184793518270404
E           x: -0.06160165
E           y: -0.06352013
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://ci.appveyor.com/project/pfnet/chainer/build/12422/job/9qx70kn98287ug4k&gt;https://ci.appveyor.com/project/pfnet/chainer/build/12422/job/9qx70kn98287ug4k&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='niboshi' date='2018-10-01T05:37:59Z'>
		tests.chainer_tests.functions_tests.math_tests.test_cumprod.TestCumprod_param_42.test_double_backward_cpu
&lt;denchmark-code&gt;E           AssertionError: Parameterized test failed.
E           
E           Base test method: TestCumprod.test_double_backward_cpu
E           Test parameters:
E             dtype: &lt;class 'numpy.float16'&gt;
E             axis: None
E             shape: (2, 3, 4)
E             contain_zero: True
E           
E           AssertionError: check_double_backward failed (eps=0.01 atol=0.1 rtol=0.1)
E           input[0]:
E           [[[-1.36132812  1.20703125  1.21972656  1.88476562]
E             [-1.16894531  1.94042969  1.19433594  1.63378906]
E             [ 1.12207031 -1.65722656 -0.71142578 -1.83398438]]
E           
E            [[-0.02766418 -1.79296875 -1.63183594 -1.60839844]
E             [-1.82714844  0.92626953 -1.01757812  1.90234375]
E             [-0.26538086  1.17382812 -1.65527344  0.        ]]]
E           grad_output[0]:
E           [ 0.7890625  -0.23339844  0.41308594 -0.78955078  0.0153656   0.06732178
E             0.34643555 -0.12005615  0.08740234  0.0141449   0.15356445  0.55175781
E             0.98095703 -0.58496094 -0.88867188 -0.04681396 -0.76464844  0.03085327
E            -0.35375977  0.22924805 -0.22497559 -0.92675781 -0.21252441 -0.30371094]
E           grad_grad_input[0]:
E           [[[-0.37158203 -0.33398438 -0.84960938 -0.98681641]
E             [ 0.23071289  0.79492188 -0.19946289 -0.41503906]
E             [ 0.94140625 -0.55615234 -0.78710938  0.89111328]]
E           
E            [[-0.12322998  0.41894531 -0.50585938 -0.05636597]
E             [ 0.13500977  0.17602539  0.47241211  0.84960938]
E             [ 0.52246094  0.37451172 -0.54833984  0.51123047]]]
E           
E           check_backward failed (eps=0.01 atol=0.1 rtol=0.1)
E           inputs[0]:
E           variable([[[-1.36132812  1.20703125  1.21972656  1.88476562]
E                      [-1.16894531  1.94042969  1.19433594  1.63378906]
E                      [ 1.12207031 -1.65722656 -0.71142578 -1.83398438]]
E           
E                     [[-0.02766418 -1.79296875 -1.63183594 -1.60839844]
E                      [-1.82714844  0.92626953 -1.01757812  1.90234375]
E                      [-0.26538086  1.17382812 -1.65527344  0.        ]]])
E           inputs[1]:
E           variable([ 0.7890625  -0.23339844  0.41308594 -0.78955078  0.0153656
E                      0.06732178  0.34643555 -0.12005615  0.08740234  0.0141449
E                      0.15356445  0.55175781  0.98095703 -0.58496094 -0.88867188
E                     -0.04681396 -0.76464844  0.03085327 -0.35375977  0.22924805
E                     -0.22497559 -0.92675781 -0.21252441 -0.30371094])
E           grad_outputs[0]:
E           [[[-0.37158203 -0.33398438 -0.84960938 -0.98681641]
E             [ 0.23071289  0.79492188 -0.19946289 -0.41503906]
E             [ 0.94140625 -0.55615234 -0.78710938  0.89111328]]
E           
E            [[-0.12322998  0.41894531 -0.50585938 -0.05636597]
E             [ 0.13500977  0.17602539  0.47241211  0.84960938]
E             [ 0.52246094  0.37451172 -0.54833984  0.51123047]]]
E           directions[0]:
E           [[[ 0.06059727  0.13180391 -0.21990135 -0.08799336]
E             [-0.21391383 -0.09542211 -0.00725452  0.20167908]
E             [-0.10407886 -0.01513626  0.16855523 -0.14799145]]
E           
E            [[-0.15536152  0.04265927 -0.00136735  0.19577839]
E             [ 0.03701355 -0.2592323   0.15141781 -0.04628432]
E             [ 0.23459321  0.05872311 -0.10999698 -0.04551938]]]
E           directions[1]:
E           [-0.01176352  0.02842442  0.10717944  0.09994417 -0.01643167  0.13827047
E             0.14975148 -0.00384256 -0.30347587  0.01541115 -0.25217787 -0.12263005
E            -0.07646868  0.06803888 -0.29394894  0.23103267  0.02597102  0.01134407
E             0.05500043  0.29891757 -0.1669256  -0.08890346 -0.14104846  0.03143474]
E           gradients (numeric):  0.4196718193177795
E           gradients (backward): 0.6821660046398819
E           
E           
E           Not equal to tolerance rtol=0.1, atol=0.1
E           
E           (mismatch 100.0%)
E            x: array(0.4196718193177795)
E            y: array(0.6821660046398819)
E           
E           assert_allclose failed: 
E             shape: () ()
E             dtype: float64 float64
E             i: (0,)
E             x[i]: 0.4196718193177795
E             y[i]: 0.6821660046398819
E             err[i]: 0.26249418532210234
E           x: 0.4196718193177795
E           y: 0.6821660046398819
&lt;/denchmark-code&gt;

		</comment>
		<comment id='22' author='niboshi' date='2018-12-30T10:41:08Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='23' author='niboshi' date='2019-01-30T21:36:21Z'>
		This issue is closed as announced. Feel free to re-open it if needed.
		</comment>
		<comment id='24' author='niboshi' date='2019-03-01T23:20:57Z'>
		This issue is closed as announced. Feel free to re-open it if needed.
		</comment>
		<comment id='25' author='niboshi' date='2019-06-17T09:03:55Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='26' author='niboshi' date='2019-08-15T09:36:03Z'>
		Now these tests are tracked at &lt;denchmark-link:https://github.com/chainer/chainer/issues/6903&gt;#6903&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>