<bug id='4068' author='toslunar' open_date='2017-12-08T12:34:05Z' closed_time='2018-01-18T00:53:21Z'>
	<summary>Link.to_gpu of Link.copy is inconsistent if there is an uninitialized Variable</summary>
	<description>
For data-parallel training the following code works because Link.to_gpu makes in-place copies of parameters on a GPU.
&lt;denchmark-code&gt;    model_0 = L.Classifier(MLP())
    model_0.predictor(np.zeros((1, 784), dtype='f'))
    model_1 = model_0.copy()
    model_0.to_gpu(0)
    model_1.to_gpu(1)
&lt;/denchmark-code&gt;

The issue is that the second line of the code is required.
cf. The following code works, too.
&lt;denchmark-code&gt;    model_0 = L.Classifier(MLP())
    model_1 = copy.deepcopy(model_0)
    model_0.to_gpu(0)
    model_1.to_gpu(1)
&lt;/denchmark-code&gt;

To reproduce the issue:
&lt;denchmark-code&gt;import chainer
import chainer.links as L
import cupy
l0 = L.Linear(None, 10)
l1 = l0.copy()
l0.to_gpu(0)
l1.to_gpu(1)
print(l1.W.data)  # =&gt;  None
print(l1.b.data.device)  # =&gt;  &lt;CUDA Device 1&gt;
l0(cupy.random.randn(3, 4).astype('f'))
print(l1.W.data.device)  # =&gt;  &lt;CUDA Device 0&gt;  (!!)
print(l1.b.data.device)  # =&gt;  &lt;CUDA Device 1&gt;
print(l0.W.data is l1.W.data)  # =&gt;  True  (!!)
&lt;/denchmark-code&gt;

Related: &lt;denchmark-link:https://github.com/chainer/chainer/issues/1953&gt;#1953&lt;/denchmark-link&gt;

	</description>
	<comments>
	</comments>
</bug>