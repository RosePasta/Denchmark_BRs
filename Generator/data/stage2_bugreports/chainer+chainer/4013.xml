<bug id='4013' author='soskek' open_date='2017-12-01T15:10:06Z' closed_time='2018-01-15T14:46:42Z'>
	<summary>Adam-based training with resumed trainer changes even if randomness is removed</summary>
	<description>
A trainer with Adam has been run until the end and it did snapshot at a middle checkpoint.
After that, if I run a trainer which is resumed with the snapshot, its (future) result is different from the end. Even if random seed is fixed and other explicit randomness is removed, it happens.
Furthermore, I found trainers with SGD do not cause the behavior.
Reproduction code is here.
&lt;denchmark-link:https://gist.github.com/soskek/3df39e70b116fba848eaa402cb9bbf5a&gt;https://gist.github.com/soskek/3df39e70b116fba848eaa402cb9bbf5a&lt;/denchmark-link&gt;

You can see the behavior as follows:
&lt;denchmark-code&gt;# Result changes if using Adam
python train_mnist.py --adam
python train_mnist.py --adam --resume result/snapshot_iter_4 --out result2
# Not if using SGD
python train_mnist.py
python train_mnist.py --resume result/snapshot_iter_4 --out result2
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='soskek' date='2017-12-03T10:15:53Z'>
		I suppose the bug occurred because t is not treated as Hyperparameter, while Adam rule depends on it.
I checked with
&lt;denchmark-code&gt;    trainer.run()
    print(optimizer.t)
    print(optimizer.alpha)
    print(model.predictor.l1.W.update_rule.t)
    print(model.predictor.l1.W.update_rule.hyperparam.alpha)
&lt;/denchmark-code&gt;

and found t is not loaded as an attr of update_rule (of each layer), while optimizer.t is loaded.
&lt;denchmark-code&gt;% python train_mnist.py --adam
epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time
1           2.37069                           0                                        0.00347109
2           2.25364                           0                                        0.0645422
3           2.14884                           0                                        0.123161
4           2.05857                           0                                        0.198989
5           1.97709                           1                                        0.255294
6           1.90448                           1                                        0.318864
7           1.82524                           1                                        0.374704
8           1.74365                           1                                        0.434473
8
0.001
8
0.001
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;% python train_mnist.py --adam --resume result/snapshot_iter_4 --out result2
epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time
1           2.37069                           0                                        0.00337989
2           2.25364                           0                                        0.0641428
3           2.14884                           0                                        0.133283
4           2.05857                           0                                        0.199131
5           1.97709                           1                                        0.204199
6           1.84183                           1                                        0.261725
7           1.72684                           1                                        0.326133
8           1.61708                           1                                        0.387963
8
0.001
4
0.001
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='soskek' date='2017-12-03T10:38:06Z'>
		Thank you for the great debug!!! Your idea looks correct.
Could I begin to make PR to fix it? Otherwise, if you are willing to fix it, it is also welcome.
		</comment>
		<comment id='3' author='soskek' date='2017-12-03T10:59:35Z'>
		I'd discuss how to fix this. There are at least two way:

by serializing t at chainer.optimizer.UpdateRule; or
by storing t of UpdateRule when t of Optimizer is loaded.

(I don't really think t is a hyperparameter, while my previous comment might be read as suggesting that.)
		</comment>
		<comment id='4' author='soskek' date='2017-12-03T11:25:27Z'>
		The first one looks more general and safe, because it does not refer to implementation of Optimizer. But, I am not very familiar with implementation of serializers. Which do you prefer?
		</comment>
		<comment id='5' author='soskek' date='2017-12-04T08:47:17Z'>
		
The first one looks more general and safe, because it does not refer to implementation of Optimizer.

I agree.  I'll write a PR.
FYI:

Trainer.serialize calls self.updater.serialize
StandardUpdater.serialize calls all optimizer.serialize
Optimizer.serialize calls existing rule.serialize

Optimizer.serialize also serializes self.t


UpdateRule.serialize calls serializer for each self._state[key]

UpdateRule.serialize does not touch self.t



		</comment>
		<comment id='6' author='soskek' date='2017-12-05T05:08:05Z'>
		Thank you!
		</comment>
	</comments>
</bug>