<bug id='7335' author='shu65' open_date='2019-05-30T07:04:17Z' closed_time='2019-06-14T00:48:38Z'>
	<summary>Grad is wrong when using `WeightDecay` of Optimizer Hook and loss scaling</summary>
	<description>
Grad is wrong when using WeightDecay and loss scaling.
The cause is that WeightDecay is performed before dividing by loss scale.
 of optimizer hook is performed here:
&lt;denchmark-link:https://github.com/chainer/chainer/blob/master/chainer/optimizer.py#L810&gt;https://github.com/chainer/chainer/blob/master/chainer/optimizer.py#L810&lt;/denchmark-link&gt;

Dividing by loss scale is here:
&lt;denchmark-link:https://github.com/chainer/chainer/blob/master/chainer/optimizer.py#L220&gt;https://github.com/chainer/chainer/blob/master/chainer/optimizer.py#L220&lt;/denchmark-link&gt;

Chainer version: v6.0.0 and v7.0.0a1
	</description>
	<comments>
	</comments>
</bug>