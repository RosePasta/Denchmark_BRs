<bug id='7673' author='onishy' open_date='2019-07-02T06:51:37Z' closed_time='2019-07-12T14:46:19Z'>
	<summary>Suggestion: Multiple Reporters cannot be active at the same time in chainer.report</summary>
	<description>
chainer: master
Python: 3.7.2
Running on CPU
I know I am doing something tricky, but I'm running multiple trainings in multi-threads using CPU (to build a web service that can train a model on-demand).
I found that the code fails when using chainer.report. This is due to "current" reporter feature, which means, only a single (and the last-created) Reporter could be active at the same time. This gives "Given observer is not registered to the reporter" error because "current" reporter may not be the one the model was add_observer-ed to. However, when running multiple trainings, it is useful to use multiple Reporters at the same time.
def report(values, observer=None):
    if _reporters:
        current = _reporters[-1]
        current.report(values, observer)
I suggest that Reporter could have a name (like reporter = reporter_module.Reporter(name)) so that chainer.reporter(name='hoge') can report to the correct reporter.
I want to ask if I am not misunderstanding and if there is any other way I can run multiple trainings at the same time. If not, and implementing this feature seems right on the way, I could help implement the feature and send a pull request.
	</description>
	<comments>
		<comment id='1' author='onishy' date='2019-07-02T08:13:58Z'>
		AFAIK, there is no reason why the current reporter is maintained by a global variable (the reporter API is introduced at the same time with Trainer, in which, IIRC, I did not consider multi-threaded trainer usage). In that sense, it'd be better to make it thread-local and make this API work better in multi-threaded workloads.
		</comment>
		<comment id='2' author='onishy' date='2019-07-02T15:25:31Z'>
		It should be fixed by simply making the variable thread-local. I'm working on the fix.
		</comment>
		<comment id='3' author='onishy' date='2019-07-03T03:45:13Z'>
		Thank you so much. FYI, the error was raised from the L.Classifier class, which reports its loss and accuracy in forward() function as following:
reporter.report({'loss': self.loss}, self)
As a quick fix for now, I made a manager class which has a dict of all reporters and it is working.
		</comment>
	</comments>
</bug>