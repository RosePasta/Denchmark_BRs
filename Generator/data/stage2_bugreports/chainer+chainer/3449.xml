<bug id='3449' author='iwiwi' open_date='2017-09-24T07:38:43Z' closed_time='2017-12-04T05:00:39Z'>
	<summary>`F.forget` does not work correctly when inputs are bare ndarray</summary>
	<description>
I found that F.forget does not work correctly when inputs to F.forget are bare xp.ndarray (rather than chainer.Variable). When all inputs to F.forget are bare ndarray, backward of F.forget is completely ignored and it is not called, even if trainable parameters are actually involved in the "forgotten" process (i.e., func parameter of F.forget).
As we usually want to use F.forget at the beginning of DNNs, it is natural to give bare ndarrays to F.forget. Therefore, I think this is a critical problem.
My minimal reproduction code is as follows. I used Chainer 2.1.0.
import chainer
import chainer.links as L
import chainer.functions as F
import numpy as np

lnk1 = L.Scale(axis=0, W_shape=(1,))
lnk2 = L.Scale(axis=0, W_shape=(1,))

def forward(x, use_forget):
    if use_forget:
        x = F.forget(lambda x_: lnk1(x_), x)
    else:
        x = lnk1(x)

    return lnk2(x)

def print_grad(x, use_forget):
    lnk1.cleargrads()
    y = forward(x, use_forget=use_forget)
    y.backward()
    print(lnk1.W.grad)

in_variable = chainer.Variable(np.array([1], dtype=np.float32))
in_ndarray = np.array([1], dtype=np.float32)

#
# Four test cases
#
print('Chainer version:', chainer.__version__)
print_grad(in_variable, use_forget=False)  # OK
print_grad(in_ndarray, use_forget=False)   # OK
print_grad(in_variable, use_forget=True)   # OK
print_grad(in_ndarray, use_forget=True)    # NG
$ python forget_test.py
Chainer version: 2.1.0
[ 1.]
[ 1.]
[ 1.]
None
	</description>
	<comments>
		<comment id='1' author='iwiwi' date='2017-11-02T12:15:10Z'>
		Thank you for reporting this.
This behavior is a result of calling &lt;denchmark-link:https://github.com/chainer/chainer/blob/master/chainer/variable.py#L1267-L1294&gt;as_variable()&lt;/denchmark-link&gt;
 inside the  to wrap all inputs in s. This may unchain the computational graph as ndarrays that are wrapped in  using this function will have their  attribute set to . A function only receiving inputs of this flag configuration will unchain the graph. In this case,  is cut from the graph.
I think fixing this issue might require non-trivial changes. &lt;denchmark-link:https://github.com/niboshi&gt;@niboshi&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='iwiwi' date='2017-11-02T21:49:01Z'>
		How about setting requires_grad=True on such variables?
		</comment>
		<comment id='3' author='iwiwi' date='2017-11-06T02:11:22Z'>
		&lt;denchmark-link:https://github.com/iwiwi&gt;@iwiwi&lt;/denchmark-link&gt;
 After offline discussions with some member, we came to the conclusion that &lt;denchmark-link:https://github.com/chainer/chainer/pull/3788&gt;#3788&lt;/denchmark-link&gt;
 is one possible, simple fix. Does this counteract your usage of ?
		</comment>
		<comment id='4' author='iwiwi' date='2017-11-06T02:16:55Z'>
		Thanks! This is exactly what I was doing in my usage to cope with this bug, so I suppose that it will work. :)
		</comment>
		<comment id='5' author='iwiwi' date='2017-12-04T05:02:24Z'>
		Fixed in &lt;denchmark-link:https://github.com/chainer/chainer/pull/3788&gt;#3788&lt;/denchmark-link&gt;
 (now merged)!
		</comment>
		<comment id='6' author='iwiwi' date='2017-12-04T05:20:11Z'>
		Great! Thanks!
		</comment>
	</comments>
</bug>