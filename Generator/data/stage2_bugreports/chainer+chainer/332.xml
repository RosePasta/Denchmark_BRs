<bug id='332' author='takerum' open_date='2015-08-19T13:58:51Z' closed_time='2015-08-25T06:06:11Z'>
	<summary>Unbiased estimation in batch normalization function</summary>
	<description>
Hi!
I'm now implementing neural network models with chainer.
I have a question regarding in the implementation of batch normalization function in chainer.
Following code is the computation of the average of means and the average of varinaces for the test mode in chainer/functions/batch_normalization.py
&lt;denchmark-code&gt;        m = ldim * rdim
        adjust = m / max(m - 1., 1.)  # unbiased estimation
        self.avg_mean *= decay
        self.avg_mean += (1 - decay) * adjust * mean
        self.avg_var *= decay
        self.avg_var += (1 - decay) * adjust * var
&lt;/denchmark-code&gt;

Why is the estimation of the "avg_mean" scaled by m/(m-1)?
I think only the "avg_var" needs to be scaled by m/(m-1) for unbiased estimation.
In the original paper of batch normalization:&lt;denchmark-link:http://arxiv.org/pdf/1502.03167v3.pdf&gt;http://arxiv.org/pdf/1502.03167v3.pdf&lt;/denchmark-link&gt;
, only the average of variances is also scaled by m/(m-1), and the average of means is not scaled.
Best,
	</description>
	<comments>
		<comment id='1' author='takerum' date='2015-08-20T07:12:53Z'>
		Thank you for reporting the suspicious code! I agree you that the scaling is not needed for avg_mean. It seems a bug, so I will fix it.
		</comment>
		<comment id='2' author='takerum' date='2015-08-20T15:26:40Z'>
		Thank you very much!
		</comment>
		<comment id='3' author='takerum' date='2015-08-23T13:06:47Z'>
		And I also found the codes that might be wrong in batch_normalization.py
&lt;denchmark-code&gt;       if self.is_finetune:
                self.N[0] += 1
                decay = 1. / self.N[0]
            else:
                decay = self.decay

            m = ldim * rdim
            adjust = m / max(m - 1., 1.)  # unbiased estimation
            self.avg_mean *= decay
            self.avg_mean += (1 - decay) * mean
            self.avg_var *= decay
            self.avg_var += (1 - decay) * adjust * var
&lt;/denchmark-code&gt;

In finetune mode, I think decay = 1. / self.N[0] is wrong and decay = 1 - 1. / self.N[0] is correct for finetuning the batch statistics.
Best,
		</comment>
	</comments>
</bug>