<bug id='8148' author='belldandyxtq' open_date='2019-09-19T10:06:15Z' closed_time='2019-09-26T11:32:13Z'>
	<summary>ChainerMN `test_create_mnbn_model.py` fails</summary>
	<description>
Error
ChainerMN test_create_mnbn_model.py fails with following messages
&lt;denchmark-code&gt;=================================== FAILURES ===================================
_____________ TestCreateMnBnModel.test_create_mnbn_model_chain_gpu _____________

self = &lt;test_create_mnbn_model.TestCreateMnBnModel testMethod=test_create_mnbn_model_chain_gpu&gt;

    @chainer.testing.attr.gpu
    def test_create_mnbn_model_chain_gpu(self):
&gt;       self.check_create_mnbn_model_chain(gpu=True)

chainermn_tests/links_tests/test_create_mnbn_model.py:107:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
chainermn_tests/links_tests/test_create_mnbn_model.py:56: in check_create_mnbn_model_chain
    mnbn_model(x)
../chainer/link.py:294: in __call__
    out = forward(*args, **kwargs)
chainermn_tests/links_tests/test_create_mnbn_model.py:19: in forward
    return chainer.functions.relu(self.bn(self.conv(x)))
../chainermn/links/batch_normalization.py:111: in __call__
    ret = func.apply((x, gamma, beta))[0]
../chainer/function_node.py:325: in apply
    outputs = self.forward(in_data)
../chainer/functions/normalization/batch_normalization.py:401: in forward
    running_var=self.running_var)
../chainer/functions/normalization/batch_normalization.py:80: in forward
    running_mean, running_var)
cupy/core/_kernel.pyx:539: in cupy.core._kernel.ElementwiseKernel.__call__
    ???
cupy/core/_kernel.pyx:582: in cupy.core._kernel.ElementwiseKernel._decide_params_type
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

&gt;   ???
E   TypeError: Type is mismatched. mean &lt;class 'numpy.float64'&gt; &lt;class 'numpy.float32'&gt; U

cupy/core/_kernel.pyx:292: TypeError
====================== 1 failed, 1 passed in 3.63 seconds ======================
&lt;/denchmark-code&gt;

Main Cause
mean and var have a different dtype &lt;numpy.float64&gt; from others &lt;numpy.float32&gt; , which violate the assumption that all the variables share the same dtype at 


chainer/chainer/functions/normalization/batch_normalization.py


         Line 66
      in
      23f1fdb






 cuda.elementwise( 





The cupy elementwise code was from old chainermn implementation, and chainermn got the dtype of mean and var only from gamma 


chainer/chainermn/functions/batch_normalization.py


         Line 39
      in
      b02a7c4






 tmp = xp.empty(gamma.size * 2, dtype=gamma.dtype) 




, which makes all the dtype the same.
While chainer has changed to promote the dtype of gamma with x (&lt;numpy.float64&gt;) ,


chainer/chainer/functions/normalization/batch_normalization.py


         Line 222
      in
      b02a7c4






 interm_dtype = numpy.promote_types(x.dtype, gamma.dtype) 




, which makes them float64, and causes this bug.
	</description>
	<comments>
	</comments>
</bug>