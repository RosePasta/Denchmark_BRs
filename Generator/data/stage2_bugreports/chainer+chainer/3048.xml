<bug id='3048' author='Hakuyume' open_date='2017-07-24T05:59:51Z' closed_time='2017-07-24T09:32:26Z'>
	<summary>Inconsistent behaivour of MultiprocessParallelUpdater</summary>
	<description>
MultiprocessParallelUpdater passes chainer.Variable to its target though StandardUpdater passes ndarray.
Here is an example code.
import argparse
import numpy as np

import chainer
from chainer.datasets import split_dataset_n_random
import chainer.functions as F
from chainer.training import updaters


class DummyNet(chainer.Chain):

    def __call__(self, x):
        print(type(x))
        return F.sum(x)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--batchsize', type=int, default=8)
    parser.add_argument('--gpus', type=int, nargs="*", default=[0])
    args = parser.parse_args()

    model = DummyNet()

    dataset = np.random.uniform(size=(64, 1, 3, 3))
    devices = tuple(args.gpus)

    iters = [
        chainer.iterators.MultiprocessIterator(i, args.batchsize)
        for i in split_dataset_n_random(dataset, len(devices))]

    optimizer = chainer.optimizers.MomentumSGD(lr=0.01, momentum=0.9)
    optimizer.setup(model)

    if len(devices) &gt; 1:
        updater = updaters.MultiprocessParallelUpdater(
            iters, optimizer, devices=devices)
    else:
        updater = chainer.training.StandardUpdater(
            iters[0], optimizer, device=devices[0])

    updater.update()


if __name__ == '__main__':
    main()
It's results.
StandardUpdater (single GPU):
&lt;denchmark-code&gt;$ python example.py --gpus 0
&lt;class 'cupy.core.core.ndarray'&gt;
&lt;/denchmark-code&gt;

MultiprocessParallelUpdater (multi GPUs)
&lt;denchmark-code&gt;$ python example.py --gpus 0 1
***/lib/python3.5/site-packages/chainer/training/updaters/multiprocess_parallel_updater.py:142: UserWarning: optimizer.lr is changed to 0.005 by MultiprocessParallelUpdater for new batch size.
  format(optimizer.lr))
&lt;class 'chainer.variable.Variable'&gt;
&lt;class 'chainer.variable.Variable'&gt;
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Hakuyume' date='2017-07-24T09:12:49Z'>
		&lt;denchmark-link:https://github.com/Hakuyume&gt;@Hakuyume&lt;/denchmark-link&gt;
 Thank you for reporting it! Could you check the version of Chainer? The problem you reported seems to be fixed by this PR: &lt;denchmark-link:https://github.com/chainer/chainer/pull/2817&gt;#2817&lt;/denchmark-link&gt;
. Because I tried to reproduce the problem, but I got this result:
&lt;denchmark-code&gt;$ python test.py --gpus 0
&lt;class 'cupy.core.core.ndarray'&gt;
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;$ python test.py --gpus 0 1
/home/shunta/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/chainer-3.0.0a1-py3.6.egg/chainer/training/updaters/multiprocess_parallel_updater.py:141: UserWarning: optimizer.lr is changed to 0.005 by MultiprocessParallelUpdater for new batch size.
  format(optimizer.lr))
&lt;class 'cupy.core.core.ndarray'&gt;
&lt;class 'cupy.core.core.ndarray'&gt;
&lt;/denchmark-code&gt;

with Chainer 3.0.0a (master) and CuPy 2.0.0a1 (master).
		</comment>
		<comment id='2' author='Hakuyume' date='2017-07-24T09:32:25Z'>
		&lt;denchmark-link:https://github.com/mitmul&gt;@mitmul&lt;/denchmark-link&gt;
 Thank you for your quick response. I used stable version of chainer and cupy. After I updated to development version, it worked properly (both returned ).
		</comment>
	</comments>
</bug>