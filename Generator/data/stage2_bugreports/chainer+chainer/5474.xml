<bug id='5474' author='Hakuyume' open_date='2018-10-11T16:29:56Z' closed_time='2018-10-23T16:40:09Z'>
	<summary>MPICommunicatorBase.allreduce does not work with 0-d array or cupy</summary>
	<description>
chainer/cupy == 5.0.0rc1
import chainermn
import cupy
import numpy as np

comm = chainermn.create_communicator()

# numpy scalar -&gt; NG
# TypeError: 'numpy.float64' object cannot be interpreted as an integer
comm.allreduce(np.array(1))

# numpy array -&gt; OK
comm.allreduce(np.array((2, 3)))

# cupy scalar -&gt; NG
# TypeError: 'numpy.float64' object cannot be interpreted as an integer
comm.allreduce(cupy.array(1))

# cupy array -&gt; NG
# AttributeError: 'tuple' object has no attribute 'reshape'
comm.allreduce(cupy.array((2, 3)))
	</description>
	<comments>
		<comment id='1' author='Hakuyume' date='2018-10-11T16:33:20Z'>
		In the case of 0-d array,  returns 1.0 of , which is not acceptable for .
&lt;denchmark-link:https://github.com/chainer/chainer/blob/master/chainermn/communicators/mpi_communicator_base.py#L491&gt;https://github.com/chainer/chainer/blob/master/chainermn/communicators/mpi_communicator_base.py#L491&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Hakuyume' date='2018-10-11T16:35:44Z'>
		In the case of cupy array,  returns a tuple but  calls  on it.
&lt;denchmark-link:https://github.com/chainer/chainer/blob/master/chainermn/communicators/_memory_utility.py#L121&gt;https://github.com/chainer/chainer/blob/master/chainermn/communicators/_memory_utility.py#L121&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/chainer/chainer/blob/master/chainermn/communicators/mpi_communicator_base.py#L492-L496&gt;https://github.com/chainer/chainer/blob/master/chainermn/communicators/mpi_communicator_base.py#L492-L496&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='Hakuyume' date='2018-10-12T08:52:47Z'>
		&lt;denchmark-link:https://github.com/Hakuyume&gt;@Hakuyume&lt;/denchmark-link&gt;
 For a scaler use case, I think you can use just  instead. I saw your code in &lt;denchmark-link:https://github.com/chainer/chainercv/pull/705&gt;chainer/chainercv#705&lt;/denchmark-link&gt;
 .
        positive = gt_mb_labels.array &gt; 0
        n_positive = xp.array(positive.sum())
        if comm:
            n_positive = cuda.to_cpu(n_positive).reshape(-1)
            n_positive = comm.allreduce(n_positive) / comm.size
can I think be replaced with
        positive = gt_mb_labels.array &gt; 0
        if comm:
            n_positive = comm.allreduce_obj(positive.sum()) / comm.size
I know this is only a workaround, but should work for you. I'll have to think another fix, as this is not only for allreduce() but in send/recv, and I think other methods.
		</comment>
		<comment id='4' author='Hakuyume' date='2018-10-12T09:22:00Z'>
		&lt;denchmark-link:https://github.com/kuenishi&gt;@kuenishi&lt;/denchmark-link&gt;
 Thank you!  worked well for my case.
I will leave this PR open since  still has some problem.
		</comment>
		<comment id='5' author='Hakuyume' date='2018-10-12T11:44:58Z'>
		Dekita
		</comment>
		<comment id='6' author='Hakuyume' date='2018-10-19T12:29:11Z'>
		merge to chainer/chainer will be via &lt;denchmark-link:https://github.com/chainer/chainer/pull/5535&gt;#5535&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='7' author='Hakuyume' date='2018-10-23T16:40:09Z'>
		&lt;denchmark-link:https://github.com/chainer/chainer/pull/5535&gt;#5535&lt;/denchmark-link&gt;
 was merged.
		</comment>
	</comments>
</bug>