<bug id='2021' author='yos1up' open_date='2016-12-16T02:55:44Z' closed_time='2016-12-16T12:45:23Z'>
	<summary>&amp;lt;type 'numpy.ndarray'&amp;gt; != &amp;lt;type 'numpy.float32'&amp;gt; error</summary>
	<description>
When I ran a script, the following error occurred.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "D:\python_codes\chainer1.17jidai\multilayer_SM\opd_best_approx.py", line 46, in &lt;module&gt;
    loss = eq.update()
  File "D:\python_codes\chainer1.17jidai\multilayer_SM\opd_best_approx.py", line 33, in update
    loss.backward()
  File "C:\Python27\lib\site-packages\chainer\variable.py", line 406, in backward
    _check_grad_type(func, x, gx)
  File "C:\Python27\lib\site-packages\chainer\variable.py", line 41, in _check_grad_type
    raise TypeError(make_message(msg))
TypeError: Function `Add` (_ + _) has a bug.

Please report this error to the issue tracker with the stack trace,
the information of your environment, and your script:
https://github.com/pfnet/chainer/issues/new.
Type of data and grad mismatch
&lt;type 'numpy.ndarray'&gt; != &lt;type 'numpy.float32'&gt;
&lt;/denchmark-code&gt;

The versions of Chainer, Python, and OS are 1.17.0, 2.7.10, Windows 7 (32bit), respectively.
&lt;denchmark-code&gt;import numpy as np
import matplotlib.pyplot as plt
import chainer
from chainer import functions as F
from chainer import links as L
from chainer import \
     cuda, gradient_check, optimizers, serializers, utils, \
     Chain, ChainList, Function, Link, Variable

def samul(sca, arr):
    return F.broadcast(sca, arr)[0] * arr

class Equ(Chain):
    def __init__(self, v): # v: np.array
        self.v = Variable(v.astype(np.float32))
        self.opt = optimizers.Adam()
        # Do I have to declare something optimized here?
        super(Equ,self).__init__(R1 = L.Linear(1,1,nobias=True, initialW=np.array([[0.5]]).astype(np.float32)),
                                R2 = L.Linear(1,1,nobias=True, initialW=np.array([[0.5]]).astype(np.float32)))
        self.opt.setup(self)
    def __call__(self, v, r1, r2): # args: Variable (O,M), scalar, scalar
        q = r1**2 + r2**2
        w = samul(F.arcsin(r1/(2*(1+q))**0.5)/F.arcsin(q/(1+q)),v[:,0:1]) + samul(F.arcsin(r2/(2*(1+q))**0.5)/F.arcsin(q/(1+q)),v[:,1:2])
        e1 = F.matmul(F.transpose(v[:,0:1]), w) - samul(r1 * ((2*(1+q)-r1**2)/(2*q+1))**0.5, F.matmul(F.transpose(w), w))
        e2 = F.matmul(F.transpose(v[:,1:2]), w) - samul(r2 * ((2*(1+q)-r2**2)/(2*q+1))**0.5, F.matmul(F.transpose(w), w))
        return (e1**2 + e2**2)[0,0] # (1,1)-&gt;scalar
    def update(self):
        r1 = self.R1(Variable(np.array([[1.0]]).astype(np.float32)))
        r2 = self.R2(Variable(np.array([[1.0]]).astype(np.float32)))
        loss = self(self.v, r1[0,0], r2[0,0])
        self.cleargrads()
        loss.grad = np.ones(loss.data.shape).astype(np.float32)
        loss.backward()
        self.opt.update()
        return loss.data
    def getR1(self):
        return self.R1.W.data[0,0]
    def getR2(self):
        return self.R2.W.data[0,0]


eq = Equ(np.array([[1., 0.5]])) # v: np.array
loss = []
for i in range(10000):
    print('R1=',eq.getR1(), 'R2=',eq.getR2(), 'loss=', loss)
    loss = eq.update()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='yos1up' date='2016-12-16T02:58:11Z'>
		Thank you for reporting. I will edit your comments for users who do not read Japanese.
		</comment>
		<comment id='2' author='yos1up' date='2016-12-16T06:32:00Z'>
		I updated chainer to 1.19.0, but the error message still appears.
The code below also causes an error message, a bit different from the error message in my first post.
&lt;denchmark-code&gt;import numpy as np
import matplotlib.pyplot as plt
import chainer
from chainer import functions as F
from chainer import links as L
from chainer import \
     cuda, gradient_check, optimizers, serializers, utils, \
     Chain, ChainList, Function, Link, Variable

a = Variable(np.array(1).astype(np.float32))
b = a/(a+1.0)
b.grad = np.ones(b.data.shape).astype(np.float32)
b.backward()
&lt;/denchmark-code&gt;

The error message is:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "D:/python_codes/chainer1.17jidai/multilayer_SM/backwardtest.py", line 13, in &lt;module&gt;
    b.backward()
  File "C:\Python27\lib\site-packages\chainer\variable.py", line 419, in backward
    x.grad = x.grad + gx  # copy
  File "C:\Python27\lib\site-packages\chainer\variable.py", line 192, in grad
    _check_grad_type(None, self, g)
  File "C:\Python27\lib\site-packages\chainer\variable.py", line 41, in _check_grad_type
    raise TypeError(make_message(msg))
TypeError: Type of data and grad mismatch
&lt;type 'numpy.ndarray'&gt; != &lt;type 'numpy.float32'&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='yos1up' date='2016-12-16T11:57:20Z'>
		Thank you for reporting a problem. I found that when a user uses zero-dimension array, it causes problem. I'll fix it.
		</comment>
	</comments>
</bug>