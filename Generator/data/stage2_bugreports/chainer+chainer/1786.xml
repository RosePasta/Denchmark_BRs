<bug id='1786' author='unnonouno' open_date='2016-10-18T10:12:25Z' closed_time='2016-10-31T16:09:09Z'>
	<summary>Unused params cause NoneType error</summary>
	<description>
When some params in a model are not used to calculate loss value, its gradient is None. It causes an error when an optimizer update its value.
&lt;denchmark-code&gt;import numpy

import chainer
import chainer.functions as F
import chainer.links as L
import chainer.optimizers as O


class MyModel(chainer.Chain):

    def __init__(self):
        super(MyModel, self).__init__(
            l1=L.Linear(3, 4),
            l2=L.Linear(3, 4),
        )

    def __call__(self, x):
        return F.sum(self.l1(x))


model = MyModel()
opt = O.Adam()
opt.setup(model)
model.cleargrads()
loss = model(numpy.random.uniform(-1, 1, (5, 3)).astype('f'))
loss.backward()
opt.update()
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='unnonouno' date='2016-10-20T15:33:46Z'>
		I have faced a problem like this in training Discriminator of  GAN.
In order to clarfify where grad get NoneType, I edited the adam.py to print the type of grad and the result below.
However, I might used all variables to calculate loss value, and optimizer propagated gradients to some extent.
&lt;denchmark-code&gt;## dis loss:  0.569088876247406
# type(grad):  &lt;class 'numpy.ndarray'&gt;
# type(grad):  &lt;class 'numpy.ndarray'&gt;
# type(grad):  &lt;class 'numpy.ndarray'&gt;
# type(grad):  &lt;class 'numpy.ndarray'&gt;
# type(grad):  &lt;class 'numpy.ndarray'&gt;
# type(grad):  &lt;class 'numpy.ndarray'&gt;
# type(grad):  &lt;class 'NoneType'&gt;

...

    m += (1 - self.beta1) * (grad - m)
TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>