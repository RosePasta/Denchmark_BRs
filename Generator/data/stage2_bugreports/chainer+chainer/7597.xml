<bug id='7597' author='shu65' open_date='2019-06-25T04:34:18Z' closed_time='2020-01-24T07:12:52Z'>
	<summary>Grad of weight decay is wrong when using `update_rule.add_hook`</summary>
	<description>
Thank you for fixing this issue &lt;denchmark-link:https://github.com/chainer/chainer/issues/7335&gt;#7335&lt;/denchmark-link&gt;

But, the grad of wight decay is wrong with this modification when using  .
The flow of the update is as follows.

call hook of an optimizer 


chainer/chainer/optimizer.py


         Line 810
      in
      5289f67






 self.call_hooks('pre') 





divide grad by loss scale 


chainer/chainer/optimizer.py


         Line 206
      in
      5289f67






 fp32_param.grad /= param._loss_scale 





call hook of an UpdateRule 


chainer/chainer/optimizer.py


         Line 208
      in
      5289f67






 hook(self, fp32_param) 





update parameters

so, grad of weight decay is loss_scale times larger than the expected value when using update_rule.add_hook
	</description>
	<comments>
		<comment id='1' author='shu65' date='2019-06-29T00:57:54Z'>
		when using fp32_update mode, this line is performed.



chainer/chainer/optimizer.py


         Line 208
      in
      68a5e44






 hook(self, fp32_param) 





But, weight decay with loss scale is wrong because fp32_param.__loss_scale is always None.
I think loss scale is copied from param to fp32_param.
Sample code is here:
&lt;denchmark-code&gt;        if self._use_fp32_update and param.dtype == numpy.float16:
            if self._fp32_param is None:
                self._fp32_param = variable.Variable(
                    param.array.astype(numpy.float32),
                    name=param.name)
            fp32_param = self._fp32_param
            fp32_param.grad = param.grad.astype(numpy.float32)
            fp32_param._loss_scale = param._loss_scale # add this line
            if fp32_param.data is not None:
                self._prepare(fp32_param)
&lt;/denchmark-code&gt;

when using dynamic loss scaling, loss scale is changed every iteration. So, loss scale requires to be copied every iteration.
		</comment>
		<comment id='2' author='shu65' date='2019-09-25T11:54:39Z'>
		Could you replace URLs of the master branch to URLs of the specific commit? The master branch is not fixed, so one cannot understand what you said after the master branch is updated.
		</comment>
		<comment id='3' author='shu65' date='2019-09-26T06:19:17Z'>
		
Could you replace URLs of the master branch to URLs of the specific commit?

Done.
		</comment>
		<comment id='4' author='shu65' date='2019-12-25T06:29:55Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='5' author='shu65' date='2020-01-24T07:12:51Z'>
		This issue is closed as announced. Feel free to re-open it if needed.
		</comment>
	</comments>
</bug>