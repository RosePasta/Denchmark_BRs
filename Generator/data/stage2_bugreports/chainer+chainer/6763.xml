<bug id='6763' author='kuenishi' open_date='2019-04-05T07:46:30Z' closed_time='2019-11-07T11:20:54Z'>
	<summary>Snapshot consistency discussion</summary>
	<description>
When using GPU most computations are concurrent with main thread. Also, several snapshot writers are concurrent with main thread, e.g. ThreadWriter or ProcessWriter . There are some cases that's okay for consistent snapshot, but others may or may not be okay. Here's the table for potential inconsistency depending on combination of compute backend and snapshot writers by my survey.



array\Writers
Simple
Thread
Process




numpy
○
×
○


cupy
△
△
×


ChainerX/cpu
○
×
○


Chainerx/gpu
△
△
×




○: Model-wide consistent snapshot ok
△: Array-wide consistent snapshot
×: Not consistent potentially

This potential issue could be more apparent when writing to remote and slow storage.
Potential solution for this situation would be

Add option to copy everything to on memory buffer at __call__ call of writers.
Force copy at __call__ call of writers.
Add option to copy everything to local temporary file and then move to remote.
Don't care.

As we can't avoid addition of another copy, the cost for this work would be performance and memory consumption of temporary snapshots. I want to discuss how do we fix this issue.
	</description>
	<comments>
		<comment id='1' author='kuenishi' date='2019-04-09T09:11:30Z'>
		&lt;denchmark-link:https://github.com/kuenishi&gt;@kuenishi&lt;/denchmark-link&gt;
 We discussed today. Probabily we don't have to care about the performance implication of making copies, because I/O should always be a bottleneck. So how about (2), which is the safest solution?
By the way, why is cupy-Simple combination "△"? I think in this case copies are always made.
		</comment>
		<comment id='2' author='kuenishi' date='2019-04-10T04:40:10Z'>
		
By the way, why is cupy-Simple combination "△"? I think in this case copies are always made.

Although copying arrays happens at main thread, GPU computations are to be done asynchronously. There could be a time where updating params in CUDA streams and copying at main thread are in race. Copying each array are atomic but as a whole model updating each array may interleave copying multiple arrays.

So how about (2), which is the safest solution?

I also have found another idea, which is adding a copy option flag passed to snapshot object. It might not be intuitive, but it would give the room for best choice to people keen at both performance and consistency, with the cost that leaving people a room to possibly make wrong choice.
		</comment>
		<comment id='3' author='kuenishi' date='2019-04-10T04:40:40Z'>
		But (2) is also fine to me, of course.
		</comment>
		<comment id='4' author='kuenishi' date='2019-04-11T07:11:52Z'>
		

By the way, why is cupy-Simple combination "△"? I think in this case copies are always made.

Although copying arrays happens at main thread, GPU computations are to be done asynchronously. There could be a time where updating params in CUDA streams and copying at main thread are in race. Copying each array are atomic but as a whole model updating each array may interleave copying multiple arrays.

Ah, I see. Thank you for explanation. Hmm, it sounds it would be a non-trivial fix.
		</comment>
		<comment id='5' author='kuenishi' date='2019-04-11T08:34:16Z'>
		Yes, in that case we have to synchronize CUDA stream before making a copy. I might think it's worth paying the cost ... only when taking snapshots are rare enough.
		</comment>
		<comment id='6' author='kuenishi' date='2019-07-10T10:17:30Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='7' author='kuenishi' date='2019-10-08T11:09:01Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='8' author='kuenishi' date='2019-11-07T11:20:52Z'>
		This issue is closed as announced. Feel free to re-open it if needed.
		</comment>
	</comments>
</bug>