<bug id='1973' author='soramichi' open_date='2016-12-07T00:31:54Z' closed_time='2016-12-14T03:53:36Z'>
	<summary>train_mnist using a GPU hangs after training finishes when the model is (a bit) large</summary>
	<description>
The train_mnist example works perfectly when the model size is set to the default,
but it stops when a bit larger model (e.g. -u 3072) is used.
&lt;denchmark-code&gt;$  python train_mnist.py -g 0 -e 2 -u 3072 # the models is bit larger than the default
GPU: 0
# unit: 3072
# Minibatch-size: 100
# epoch: 2

epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time
     total [####################..............................] 41.67%
this epoch [#########################################.........] 83.33%
       500 iter, 0 epoch / 2 epochs
    111.19 iters/sec. Estimated time to finish: 0:00:03.362567.
&lt;/denchmark-code&gt;

The execution hangs up here and there's no way to quit it other than sending a KILL signal.
Please note that the GPU memory is large enough and thus no spilling of data/model happens.
&lt;denchmark-code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla M40           On   | 0000:04:00.0     Off |                    0 |
| N/A   32C    P0   160W / 250W |    506MiB / 11443MiB |     93%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     24813    C   python                                         504MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

Things I found (env1 and env2 are described below):

It happens in multiple environments using different GPUs and different CUDA versions.
In env1, removing extensions that touch the data in the GPU (namely snapshot and logreport) solves the problem.
However in env2, chainer hangs even when all extensions are removed. What is worse, it stops after training.run() finishes and there's nothing to do in the python level.
The same happens with my own chainer program that uses completely different data/model from the mnist example. It works when the model is small, but it hangs up when the number of nodes are increased.

Judging from these facts (especially from 2 and 3), I guess there's a problem somewhere in an asynchronous cuda kernel, maybe the one that copies data back from GPU to CPU.




env1
env2




GPU
Tesla M40 (Maxwell, 12GB mem)
GRID K520 (Kepler, 4GB mem)


CPU
Intel Xeon E5-2690 v4 (Broadwell)
Intel Xeon E5-2670 (SandyBridge)*


SW versions
CUDA 8.0, cuDNN 5.1
CUDA 7.5, cuDNN 5.0


Misc

g2.2xlarge instance of AWS



* The CPU model of env2 might be reported from the virtual machine monitor and it can be different from actual physical CPU.
	</description>
	<comments>
		<comment id='1' author='soramichi' date='2016-12-12T08:45:27Z'>
		In my environment, Python 3.5.2 suffers this problem but Python 2.7.12 does not. Additionally, Python 3.5.2 works after removing cupy cache (usually stored in $HOME/.cupy) but hangs at next time.
		</comment>
		<comment id='2' author='soramichi' date='2016-12-12T11:09:58Z'>
		The following patch fixes this issue, but makes slower.
diff --git a/chainer/cuda.py b/chainer/cuda.py
index 5daaea0b..e173805e 100644
--- a/chainer/cuda.py
+++ b/chainer/cuda.py
@@ -216,14 +216,10 @@ def to_gpu(array, device=None, stream=None):

         if stream is not None:
             ret = cupy.empty_like(array)
-            mem = None
             if array_dev.id == -1:
                 # cpu to gpu
-                mem = cupy.cuda.alloc_pinned_memory(array.nbytes)
-                src = numpy.frombuffer(
-                    mem, array.dtype, array.size).reshape(array.shape)
-                src[...] = array
-                ret.set(src, stream)
+                src = array.copy(order='C')
+                ret.set(src)
             else:
                 # gpu to gpu
                 with array_dev:
@@ -234,7 +230,7 @@ def to_gpu(array, device=None, stream=None):
                 ret.data.copy_from_device_async(src.data, src.nbytes, stream)

             # to hold a reference until the end of the asynchronous memcpy
-            stream.add_callback(lambda *x: None, (src, mem, ret))
+            stream.add_callback(lambda *x: None, (src, ret))

             return ret
		</comment>
		<comment id='3' author='soramichi' date='2016-12-13T02:10:29Z'>
		diff --git a/chainer/cuda.py b/chainer/cuda.py
index 5daaea0b..00dce9ac 100644
--- a/chainer/cuda.py
+++ b/chainer/cuda.py
@@ -223,7 +223,7 @@ def to_gpu(array, device=None, stream=None):
                 src = numpy.frombuffer(
                     mem, array.dtype, array.size).reshape(array.shape)
                 src[...] = array
-                ret.set(src, stream)
+                ret.set(src)
             else:
                 # gpu to gpu
                 with array_dev:
		</comment>
	</comments>
</bug>