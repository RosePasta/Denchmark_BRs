<bug id='2515' author='abcteo' open_date='2017-04-07T09:03:56Z' closed_time='2017-12-07T22:03:52Z'>
	<summary>AttributeError: 'BatchNormalizationFunction' object has no attribute 'fixed_mean'</summary>
	<description>
I have tried the BatchNormalizationFunction with chainer 1.22 and encounter this error
pls help. Thanks
	</description>
	<comments>
		<comment id='1' author='abcteo' date='2017-04-10T23:24:47Z'>
		Thank you reporting the issue. Is it possible for you to give us the minimum script so that we can reproduce the error? Thanks.
		</comment>
		<comment id='2' author='abcteo' date='2017-05-11T14:03:56Z'>
		Reproduced with this code:
&lt;denchmark-code&gt;from chainer.functions.normalization import batch_normalization
import numpy

batch_normalization.BatchNormalizationFunction()(
    numpy.ones((1,1), numpy.float64),
    numpy.array([1.0]),
    numpy.array([1.0]))
&lt;/denchmark-code&gt;

Result:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test_bn.py", line 7, in &lt;module&gt;
    numpy.array([1.0]))
  File "/home/niboshi/w/repos/chainer/chainer/function.py", line 199, in __call__
    outputs = self.forward(in_data)
  File "/home/niboshi/w/repos/chainer/chainer/functions/normalization/batch_normalization.py", line 165, in forward
    mean = self.fixed_mean
AttributeError: 'BatchNormalizationFunction' object has no attribute 'fixed_mean'
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='abcteo' date='2017-05-16T22:31:57Z'>
		&lt;denchmark-link:https://github.com/niboshi&gt;@niboshi&lt;/denchmark-link&gt;
 Thank you for your investigation. I will check the code.
		</comment>
		<comment id='4' author='abcteo' date='2017-05-16T23:17:29Z'>
		It occurs when the length of inputs argument of the forward method is 3 and self.train is False (, which is the default value in its instantiation) when forward is called. This function seems to assume this case.
		</comment>
		<comment id='5' author='abcteo' date='2017-05-16T23:59:47Z'>
		Proposal for the improvement:




len(inputs)==5
len(inputs)==3




self.train==True
Warn that inputs[3:] is ignored,
no change


self.train==False
no change
Raise error



		</comment>
		<comment id='6' author='abcteo' date='2017-05-17T00:37:00Z'>
		I think we can also raise an error in the top-left case.
It's enforcement of specification, not breaking compatibility.
		</comment>
		<comment id='7' author='abcteo' date='2017-05-17T00:46:49Z'>
		And in v2, the default configuration.train value is True in contrary to BatchNormalizationFunction's default train  value of False in v1. We should inform the user in a strong way of this change, lest the user ignore the warning.
		</comment>
		<comment id='8' author='abcteo' date='2017-05-17T11:57:59Z'>
		
It's enforcement of specification, not breaking compatibility.

First of all, I think raising an error in these situation is a change of undocumented behavior, rather than enforcement of specification because F.BatchNormalization is not documented and therefore, we do not have specification to enforce for this function.
Theoretically speaking, we can change the behavior of F.BatchNormalization anyway because it is not documented and therefore, we do not have any compatibility to be kept. In that sense, I agree that raising an error does not break compatibility.
But users' code will be broken if F.BatchNormalization is instantiated directly (i.e. not via F.batch_normalization or F.fixed_batch_normalization) and its __call__ method is called under these conditions. I am not sure how many users use this function in this way, but I think we should not break compatibility by raising an error unless we have a reason that we strongly want to forbid this usage.
		</comment>
		<comment id='9' author='abcteo' date='2017-05-17T12:09:52Z'>
		Regarding the update to v2, the behavior does not change if users use  via  and . For , it sets  to  in v1, which is the default value of . For , it sets  to  in v1 and switches  to  &lt;denchmark-link:https://github.com/pfnet/chainer/blob/_v2/chainer/functions/normalization/batch_normalization.py#L363&gt;here&lt;/denchmark-link&gt;
 in v2.
		</comment>
		<comment id='10' author='abcteo' date='2017-05-17T14:08:17Z'>
		Let's separate the cases.


Called via F.batch_normalization or F.fixed_batch_normalization
-- No problem, both in v1 and v2


Directly call F.BatchNormalizationFunction with explicit train argument
-- Unexpected keyword argument in v2


Directly call F.BatchNormalizationFunction without train argument
-- If len(inputs) == 5, it will run in train=False mode in v1, and train=True mode in v2
-- If len(inputs) == 3, it will not run (bug) in v1, and will run in train=True mode in v2


Now I'm focusing on len(inputs) == 5 case in 3 (maybe this is rare, though).
The point is, it will run in different semantics between v1 and v2.
IMO it's better not to allow this code to run, even with warning, because it's almost certain that the user did not expect the new semantics.
The specification is not documented as you said, but it's implicitly assumed in the code. IMO it's OK to make it explicit.
		</comment>
		<comment id='11' author='abcteo' date='2017-05-18T07:11:16Z'>
		I come to think it is OK to forbid len(inputs) == 5 &amp;&amp; train==True case because backward method already forbids that. Although it is possible that users could employ this function for only forward propagation (e.g. inference mode), I think it is natural that the assumptions for the inputs are consistent between forward and backward.
The benefit of warning solution is that we can use this function in the same way in training and test mode. But I feel transparency and ignorance of the change of semantics is like two sides of a same coin. If we can use some function in the same way in the different context, we cannot notice even if  the context is changed. Considering that we want to avoid implicit changes of semantics, it is better to forbid at the const of transparency.
		</comment>
		<comment id='12' author='abcteo' date='2017-11-27T05:07:56Z'>
		&lt;denchmark-link:https://github.com/niboshi&gt;@niboshi&lt;/denchmark-link&gt;
  Let's resume the discussion, shall we?
		</comment>
		<comment id='13' author='abcteo' date='2017-12-05T03:03:42Z'>
		Does this problem still occur?
Looks like this has been solved in &lt;denchmark-link:https://github.com/chainer/chainer/pull/3275&gt;#3275&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='14' author='abcteo' date='2017-12-07T22:03:50Z'>
		Oh, I missed it. Then I'll close it.
		</comment>
	</comments>
</bug>