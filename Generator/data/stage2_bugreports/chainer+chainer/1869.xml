<bug id='1869' author='kaniblu' open_date='2016-11-12T14:05:36Z' closed_time='2017-11-27T04:46:21Z'>
	<summary>Multi-GPU bug in LSTM</summary>
	<description>
&lt;denchmark-h:h2&gt;The Problem&lt;/denchmark-h&gt;

Currently, when chainer.links.connection.LSTM is bound to a gpu device (say self._device_id), it fails to allocate cell state c in the same GPU device as itself (self._device_id), causing an error stating that the gpu devices are different when performing forwarding or backwarding on the link. This problem is apparent when we want to use a gpu device with id &gt; 0.
&lt;denchmark-h:h2&gt;Reproduction&lt;/denchmark-h&gt;

Assuming that there are two gpu devices on the host machine, binding both of lstm link and input variable to the second gpu device will throw an error when performing __call__.
&lt;denchmark-code&gt;from chainer import Variable, cuda, links as L
import numpy as np

lstm = L.LSTM(8, 8)
lstm.to_gpu(1)

var_x = Variable(cuda.to_gpu(np.random.random((8, 8)).astype(np.float32), 1))

assert lstm._device_id == var_x.data.device_id == 1
print("lstm device id: {}, var_x device id: {}".format(lstm._device_id, var_x.data.device.id))

#The following line will throw ValueError
lstm(var_x)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Proposed Solution&lt;/denchmark-h&gt;

In this previous example, the problem can be easily circumvented by adding a single line of following code:
&lt;denchmark-code&gt;with cuda.Device(1):
    lstm(var_x)
&lt;/denchmark-code&gt;

I'm not sure if this is the intended behavior from the developers, but the behavior is unintuitive and the error message might be confusing to some users. Intuitively, the link should allocate c in the appropriate device (same as the bound device of itself). This can be achieved quite simply by modifying line 242 in chainer/links/connection/lstm.py into something like this:
&lt;denchmark-code&gt;if xp == cupy:
    old_dev = cuda.Device()
    cuda.Device(self._device_id).use()

self.c = variable.Variable(xp.zeros((batch, self.state_size), dtype=x.dtype), volatile='auto')

if xp == cupy:
    old_dev.use()
&lt;/denchmark-code&gt;

But this does not seem optimal tbh. A better solution would have been having some kind of additional wrapping around allocation functions like xp.zeros that accepts device as a parameter.
As a side note, IMHO, there should be a common error checking function used in __call__ that detects whether the input variable resides in a different device from the link. (The sanitizing function should also be togglable through chainer.set_debug).
	</description>
	<comments>
		<comment id='1' author='kaniblu' date='2016-11-15T07:49:54Z'>
		Thank you for reporting this! As you noted, this was not the intended behaviour, but probably a bug (I did not reproduce it).
I believe &lt;denchmark-link:https://github.com/kamo-naoyuki&gt;@kamo-naoyuki&lt;/denchmark-link&gt;
 has (or is working on) a solution (&lt;denchmark-link:https://github.com/chainer/chainer/pull/1876&gt;#1876&lt;/denchmark-link&gt;
 was closed and instead the relevant PR is &lt;denchmark-link:https://github.com/chainer/chainer/pull/1882&gt;#1882&lt;/denchmark-link&gt;
.)
		</comment>
		<comment id='2' author='kaniblu' date='2017-11-27T04:46:08Z'>
		Closed as I confirmed the reproduction code (with slight modification replacing var_x.data.device_id with var_x.data.device.id) does not throw errors any longer.
		</comment>
	</comments>
</bug>