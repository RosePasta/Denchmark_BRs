<bug id='5943' author='yuzupepper' open_date='2018-12-28T04:09:52Z' closed_time='2019-01-11T02:34:42Z'>
	<summary>NStepLSTM fails to backward when chainer.config.train=False and cuDNN is enabled.</summary>
	<description>
NStepLSTM fails to backward when chainer.config.train=False and cuDNN is enabled.


Conditions

OS/Platform: Colaboratory
Chainer version: 5.0.0
CuPy version: 5.0.0
CUDA version: 9.2



Code to reproduce


&lt;denchmark-code&gt;import chainer
import chainer.links as L
import chainer.functions as F
import numpy as np
from chainer import cuda

gpu=0

lstm=L.NStepLSTM(1,2,2,0.0)
if gpu &gt;= 0:
    chainer.cuda.get_device(gpu).use()
    lstm.to_gpu()
xp = cuda.cupy if gpu&gt;=0 else np
x=[chainer.Variable(xp.random.random((3,2)).astype(xp.float32))]
with chainer.using_config('train', False):
    _,_,y=lstm(None,None,x)
    F.sum(y[0]).backward()
    print(x[0].grad)
&lt;/denchmark-code&gt;


Error messages, stack traces, or logs

&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-11-bcaf56bfc39a&gt; in &lt;module&gt;()
     15 with chainer.using_config('train', False):
     16     _,_,y=lstm(None,None,x)
---&gt; 17     F.sum(y[0]).backward()
     18     print(x[0].grad)

/usr/local/lib/python3.6/dist-packages/chainer/variable.py in backward(self, retain_grad, enable_double_backprop, loss_scale)
    961         """
    962         with chainer.using_config('enable_backprop', enable_double_backprop):
--&gt; 963             self._backward_main(retain_grad, loss_scale)
    964 
    965     def _backward_main(self, retain_grad, loss_scale):

/usr/local/lib/python3.6/dist-packages/chainer/variable.py in _backward_main(self, retain_grad, loss_scale)
   1038 
   1039                 _backprop_utils.backprop_step(
-&gt; 1040                     func, target_input_indexes, out_grad, in_grad)
   1041 
   1042                 for hook in hooks:

/usr/local/lib/python3.6/dist-packages/chainer/_backprop_utils.py in backprop_step(func, target_input_indexes, grad_outputs, grad_inputs)
    104     else:  # otherwise, backward should be overridden
    105         gxs = func.backward(
--&gt; 106             target_input_indexes, grad_outputs)
    107 
    108         if is_debug:

/usr/local/lib/python3.6/dist-packages/chainer/function.py in backward(self, target_input_indexes, grad_outputs)
    141 
    142         with cuda.get_device_from_array(*(in_data + grad_out_data)):
--&gt; 143             gxs = self._function.backward(in_data, grad_out_data)
    144         for x, gx in six.moves.zip(self.inputs, gxs):
    145             variable._check_grad_type(self, x, gx)

/usr/local/lib/python3.6/dist-packages/chainer/functions/connection/n_step_rnn.py in backward(self, inputs, grads)
    456             c_dx_descs.data, dxs.data.ptr, dhx_desc.value, dhx.data.ptr,
    457             dcx_desc_value, dcx_data_ptr, workspace.data.ptr, work_size,
--&gt; 458             self.reserve_space.data.ptr, self.reserve_space.size)
    459 
    460         dw = cuda.cupy.zeros_like(w)

AttributeError: 'NStepLSTM' object has no attribute 'reserve_space'
&lt;/denchmark-code&gt;

The error seems to occur because "reserve_space" in n_step_rnn.py is not defined when chainer.config.train=False.
How can I fix the error? Or the operation above is not supported?
	</description>
	<comments>
		<comment id='1' author='yuzupepper' date='2019-01-08T08:32:38Z'>
		I will try to reproduce it.
		</comment>
		<comment id='2' author='yuzupepper' date='2019-01-09T06:53:14Z'>
		I did reproduce the case on Colaboratory as well as my local environment with Chainer 6.0.0b1 and CuPy 6.6.0b1. I'll look into its detail.
		</comment>
		<comment id='3' author='yuzupepper' date='2019-01-09T07:21:22Z'>
		I suppose that when chainer.config.train=False Chainer runs in testing, or inference, mode so you usually do not need to perform backward computation. Can I ask you what you intend in the code above?
		</comment>
		<comment id='4' author='yuzupepper' date='2019-01-09T11:48:37Z'>
		Thank you for your reply.
I want to obtain Jacobian matrix of loss function in testing mode. As Guided Backpropagation and GradCAM use it, I think it's useful to analyze the model behavior.
		</comment>
		<comment id='5' author='yuzupepper' date='2019-01-10T05:09:19Z'>
		I understand. I'd like to conclude that the operation is not supported in Chainer as cuDNN itself does
not look to support backward computation in training mode. cudnnRNNBackwardData and cudnnRNNBackwardWeights require reserveSpace parameter which must be generated by cudnnRNNForwardTraining.
&lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNBackwardData&gt;https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNBackwardData&lt;/denchmark-link&gt;

Anyway, I think an appropriate error should be raised in this case.
		</comment>
		<comment id='6' author='yuzupepper' date='2019-01-10T14:07:16Z'>
		OK. I hope it will be supported someday.
Thank you.
		</comment>
	</comments>
</bug>