<bug id='5555' author='ir5' open_date='2018-10-23T08:51:57Z' closed_time='2018-12-07T07:24:23Z'>
	<summary>The behavior of F.forget is not appropriate for BatchNormalization</summary>
	<description>
The combination of F.forget and BatchNormalization will cause wrong computation result.
BatchNormalization link updates its internal statistics (avg_mean and avg_var) every time the forward computation is called during training time, on the other hand, F.forget will call forward computation twice.  So the combination of them will lead to wrong statistics result.
Example:
import chainer
import chainer.functions as F
import chainer.links as L

import numpy as np


class Model(chainer.Chain):

    def __init__(self):
        super().__init__()
        with self.init_scope():
            self.bn1 = L.BatchNormalization(5)

    def forward(self, x):
        h = self.bn1(x)
        # h = F.forget(self.bn1, x)
        return h


np.random.seed(666)
mlp = Model()

x = np.random.uniform(size=(2, 5)).astype('f')
y = mlp(x)
F.sum(y).backward()

with chainer.using_config('train', False):
    # test time
    y = mlp(x)
    print(y)
Output (without F.forget):
&lt;denchmark-code&gt;variable([[ 0.6917034   0.8193421   0.667606    0.71579635  0.9209751 ]
          [-0.02388343  0.36777574  0.01308255  0.06104017  0.45613596]])
&lt;/denchmark-code&gt;

Output (with F.forget):
&lt;denchmark-code&gt;variable([[ 0.6842567   0.79659545  0.6600341   0.70522314  0.89286864]
          [-0.05953163  0.32327673 -0.02182698  0.02312531  0.4058005 ]])
&lt;/denchmark-code&gt;

This unmatched behavior is not described in document.
	</description>
	<comments>
	</comments>
</bug>