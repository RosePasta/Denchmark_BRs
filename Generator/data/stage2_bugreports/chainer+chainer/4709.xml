<bug id='4709' author='andremoeller' open_date='2018-05-03T23:16:01Z' closed_time='2018-05-11T10:55:38Z'>
	<summary>Process does not exit when CUDARuntimeError occurs</summary>
	<description>
Hi,
My problem is: If I misuse MultiprocessParallelUpdater such that an exception is thrown, the python process does not exit, but just hangs.
The code to reproduce is at the bottom. It causes cupy.cuda.runtime.CUDARuntimeError: cudaErrorInitializationError: initialization error, but the python process does not exit.
I'd like the process to exit, but I am not sure how -- I can't use  or call  as in &lt;denchmark-link:https://github.com/chainer/chainermn/issues/236&gt;chainer/chainermn#236&lt;/denchmark-link&gt;
 since this code does not use mpi4py.
Thank you.

Conditions

Chainer version 4.0.0
CuPy version 4.0.0
OS/Platform ubuntu
CUDA/cuDNN version 9.0


Code to reproduce

&lt;denchmark-code&gt;import chainer
import chainer.functions as F
import chainer.links as L

class MLP(chainer.Chain):

    def __init__(self, n_units, n_out):
        super(MLP, self).__init__()
        with self.init_scope():
            # the size of the inputs to each layer will be inferred
            self.l1 = L.Linear(None, n_units)  # n_in -&gt; n_units
            self.l2 = L.Linear(None, n_units)  # n_units -&gt; n_units
            self.l3 = L.Linear(None, n_out)  # n_units -&gt; n_out

    def __call__(self, x):
        h1 = F.relu(self.l1(x))
        h2 = F.relu(self.l2(h1))
        return self.l3(h2)

def train():
    train, test = chainer.datasets.get_mnist()

    batch_size = 64
    learning_rate = 0.05

    model = L.Classifier(MLP(1000, 10))

    optimizer = chainer.optimizers.MomentumSGD(learning_rate)
    optimizer.setup(model)
    optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))

    # Set up a trainer
    num_gpus = 2
    devices = range(num_gpus)

    # this is just to force the error
    chainer.cuda.get_device_from_id(0).use()

    train_iters = [chainer.iterators.MultiprocessIterator(i, batch_size, n_processes=num_gpus) \
                   for i in chainer.datasets.split_dataset_n_random(train, len(devices))]

    updater = training.updaters.MultiprocessParallelUpdater(train_iters, optimizer, devices=range(num_gpus))
    updater.setup_workers()


if __name__=="__main__":
    train()
&lt;/denchmark-code&gt;

Stacktrace:
&lt;denchmark-code&gt;algo-1_1  | Process _Worker-1:
algo-1_1  | Traceback (most recent call last):
algo-1_1  |   File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
algo-1_1  |     self.run()
algo-1_1  |   File "/usr/local/lib/python3.5/dist-packages/chainer/training/updaters/multiprocess_parallel_updater.py", line 45, in run
algo-1_1  |     dev.use()
algo-1_1  |   File "cupy/cuda/device.pyx", line 101, in cupy.cuda.device.Device.use
algo-1_1  |   File "cupy/cuda/device.pyx", line 107, in cupy.cuda.device.Device.use
algo-1_1  |   File "cupy/cuda/runtime.pyx", line 184, in cupy.cuda.runtime.setDevice
algo-1_1  |   File "cupy/cuda/runtime.pyx", line 136, in cupy.cuda.runtime.check_status
algo-1_1  | cupy.cuda.runtime.CUDARuntimeError: cudaErrorInitializationError: initialization error

&lt;/denchmark-code&gt;

I had a problem similar to this one, but with mpi4py / chainermn, so the answer might be similar: &lt;denchmark-link:https://github.com/chainer/chainermn/issues/236&gt;chainer/chainermn#236&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='andremoeller' date='2018-05-07T06:38:47Z'>
		I reproduced this problem with this Dockerfile and scripts: &lt;denchmark-link:https://gist.github.com/mitmul/571a155d651df3b0dbe6ec129c846cb1&gt;https://gist.github.com/mitmul/571a155d651df3b0dbe6ec129c846cb1&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='andremoeller' date='2018-05-07T07:24:09Z'>
		If we can know if the CUDA context has been initialized, this may be able to be resolved, I suppose. How can I know it?
		</comment>
		<comment id='3' author='andremoeller' date='2018-05-07T08:26:00Z'>
		&lt;denchmark-link:https://github.com/andremoeller&gt;@andremoeller&lt;/denchmark-link&gt;
 This is not a complete solution I think, but after applying the following patch, the process terminates correctly when in the same situation. Could you check this PR?:
&lt;denchmark-link:https://github.com/chainer/chainer/pull/4717&gt;#4717&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='andremoeller' date='2018-05-08T20:06:40Z'>
		Thanks for looking into this! Unfortunately, we'll need to be able to avoid all such cases where execution hangs, and using the MultiProcessParallelUpdater is just a way to reproduce this. My understanding is you're still working on a more universal solution.
Thanks again!
		</comment>
	</comments>
</bug>