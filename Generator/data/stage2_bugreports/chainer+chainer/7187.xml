<bug id='7187' author='gwtnb' open_date='2019-05-19T12:24:21Z' closed_time='2019-12-12T09:55:22Z'>
	<summary>ConvolutionND fails with CUDNN_NOT_SUPPORTED on some conditions</summary>
	<description>
ConvolutionND fails with CUDNN_NOT_SUPPORTED on some conditions.
environment:

Ubuntu 18.04
CUDA: 10.1
cuDNN: 7.5.0
CuPy: https://github.com/cupy/cupy/tree/bdbd176ef9c3df47ff3337cd5e26f066196b4e90
Chainer: https://github.com/chainer/chainer/tree/80d2c96cb293a8529e2ad02526fa9aac7cb5bbb0

code:
&lt;denchmark-code&gt;$ python conv_nd.py | grep ^False | wc -l
384
$ python conv_nd.py | grep ^True | wc -l
192
$ cat conv_nd.py 
import chainer
import chainer.functions as F
import chainer.links as L
import cupy


for deterministic in False, True:
    chainer.global_config.cudnn_deterministic = deterministic

    for ndim in range(1, 3 + 1):
        for ksize in range(1, 3 + 1):
            for stride in 1, 2:
                for pad in range(0, 3):
                    for nobias in False, True:
                        for dilate in range(1, 5 + 1):
                            for groups in 1, 2, 4, 8:
                                try:
                                    conv = L.ConvolutionND(ndim, 32, 32, ksize=ksize, stride=stride, pad=pad, nobias=nobias, dilate=dilate, groups=groups)
                                    conv.to_gpu()

                                    x = cupy.ndarray((4, 32) + ndim * (16,), dtype=cupy.float32)
                                    y = conv(x)
                                    F.sum(y).backward()
                                except Exception as e:
                                    print(deterministic, ndim, ksize, stride, pad, nobias, dilate, groups, e)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='gwtnb' date='2019-05-28T11:34:16Z'>
		Maybe &lt;denchmark-link:https://github.com/chainer/chainer/issues/7304&gt;#7304&lt;/denchmark-link&gt;
 should be taken care too.
		</comment>
		<comment id='2' author='gwtnb' date='2019-08-14T08:51:15Z'>
		In my environment the error occurs if and only if: ndim == 3 and dilate &gt; 1 and ksize == 1, if autotune = False.
If autotune = True, the condition of failure fluctuates time to time.
&lt;denchmark-code&gt;&gt;&gt;&gt; cupy.show_config()
CuPy Version          : 7.0.0b2
CUDA Root             : /usr/local/cuda
CUDA Build Version    : 10000
CUDA Driver Version   : 10000
CUDA Runtime Version  : 10000
cuDNN Build Version   : 7402
cuDNN Version         : 7402
NCCL Build Version    : None
NCCL Runtime Version  : None
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='gwtnb' date='2019-11-12T09:16:18Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='4' author='gwtnb' date='2019-12-12T09:55:21Z'>
		This issue is closed as announced. Feel free to re-open it if needed.
		</comment>
	</comments>
</bug>