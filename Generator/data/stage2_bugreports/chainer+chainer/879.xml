<bug id='879' author='muupan' open_date='2016-01-21T08:25:52Z' closed_time='2018-08-17T02:53:46Z'>
	<summary>BatchNormalization's avg_var initialized with zeros can cause division by zero</summary>
	<description>
Currently, BatchNormalization's avg_var is initialized with zeros. This will cause division by zero when called it with test=True before test=False, so it might be better we use another values, say, ones, to initialize avg_var.
Calling BatchNormalization with test=True before test=False can seem unnatural, but it can happen in the reinforcement learning context: an agent selects an action by evaluating its value function or policy represented by a NN, observes consequences, and then (if it has enough experiences) updates its value function or policy.
	</description>
	<comments>
		<comment id='1' author='muupan' date='2017-10-23T14:40:31Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='2' author='muupan' date='2017-11-22T15:02:29Z'>
		This issue is closed as announced. Feel free to re-open it if needed.
		</comment>
		<comment id='3' author='muupan' date='2017-11-27T02:48:59Z'>
		&lt;denchmark-link:https://github.com/muupan&gt;@muupan&lt;/denchmark-link&gt;
 Do you think we should reopen this issue?
		</comment>
		<comment id='4' author='muupan' date='2017-12-27T10:30:06Z'>
		I think it is good to fix this issue by initializing var with 1s. PyTorch seems doing so.
&lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L24&gt;https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L24&lt;/denchmark-link&gt;

Are there any opinions against doing so?
		</comment>
		<comment id='5' author='muupan' date='2017-12-28T00:54:00Z'>
		One concern is that it changes the models learned with the same script. I think the effect is negligible for most use cases (the effect is made exponentially small against the number of training iterations), but it would be better to leave a way to reproduce the current behavior.
		</comment>
		<comment id='6' author='muupan' date='2018-01-09T04:17:11Z'>
		One simple solution would be to add an option such as initialize_population_variance_with_zero to __init__. Or more generally, we can give an initializer for initializing avg_var (and possibly avg_mean), although initializers are currently used for not persistent values but parameters.
How do you think of them?
		</comment>
		<comment id='7' author='muupan' date='2018-01-17T02:27:58Z'>
		+1 for using initializers for avg_var (and avg_mean for consistency).
		</comment>
		<comment id='8' author='muupan' date='2018-01-23T10:05:53Z'>
		&lt;denchmark-link:https://github.com/muupan&gt;@muupan&lt;/denchmark-link&gt;
 How about you?
		</comment>
		<comment id='9' author='muupan' date='2018-01-25T05:46:15Z'>
		üëç
		</comment>
		<comment id='10' author='muupan' date='2018-05-01T09:44:45Z'>
		I'd rather not add any options.  Just fix it, please.  Even if someone exploited the fact that avg_var is initialized with zeros, he or she can fix code easily by
&lt;denchmark-code&gt;bn = chainer.links.BatchNormalization(_)
bn.avg_var[...] = 0
&lt;/denchmark-code&gt;

(and all Chainer has to do is to write it in the version-up guide).  To anyone else, including beginners, avg_var should not be 0.
		</comment>
		<comment id='11' author='muupan' date='2018-05-09T04:34:14Z'>
		I will work on it.
		</comment>
	</comments>
</bug>