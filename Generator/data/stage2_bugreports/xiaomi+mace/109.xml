<bug id='109' author='rogerou' open_date='2018-07-23T03:54:55Z' closed_time='2018-08-03T02:21:22Z'>
	<summary>Convert Tensorflow model failed</summary>
	<description>
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
NDK version(e.g., 15c):r16b
GCC version(if compiling for host, e.g., 5.4.0): GCC 4.2.1
MACE version (Use the command: git describe --long --tags): master
Python version(2.7): 2.7
Bazel version (e.g., 0.13.0):0.15.2

&lt;denchmark-h:h3&gt;Model deploy file (*.yml)&lt;/denchmark-h&gt;

# The name of library
library_name: 1.1.0.170918_alpha_landmark
target_abis: [armeabi-v7a]
model_graph_format: file
model_data_format: file
models:
  mobilenet_v1: # model tag, which will be used in model loading and must be specific.
    platform: tensorflow
    # path to your tensorflow model's pb file. Support local path, http:// and https://
    model_file_path: /Users/ouzhijie/Downloads/1.1.0.170918_alpha_landmark.pb
    # sha256_checksum of your model's pb file.
    # use this command to get the sha256_checksum: sha256sum path/to/your/pb/file
    model_sha256_checksum: fba7bb3b3a39545ebf9462c8ecef402e27093f9d0547d8d41eb2604158fbae17 
    # define your model's interface
    # if there multiple inputs or outputs, write like blow:
    # subgraphs:
    # - input_tensors:
    #     - input0
    #     - input1
    #   input_shapes:
    #     - 1,224,224,3
    #     - 1,224,224,3
    #    output_tensors:
    #      - output0
    #      - output1
    #    output_shapes:
    #      - 1,1001
    #      - 1,1001
    subgraphs:
      - input_tensors:
          - input_imgs
        input_shapes:
          - 1,112,112,3
        output_tensors:
          - final_output
        output_shapes:
          - 1,171
    # cpu, gpu or cpu+gpu
    runtime: cpu+gpu
    winograd: 4
&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

A clear and concise description of what the bug is.
convert tensorflow model failed
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the problem:
1. cd /path/to/mace
2. python tools/converter.py convert --config_file=/path/to/your/model_deployment_file
&lt;denchmark-h:h3&gt;Error information / logs&lt;/denchmark-h&gt;

Please include the full log and/or traceback here.
mace_output_node_final_output (ImageToBuffer): []
start optimize gpu memory.
Traceback (most recent call last):
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter.py", line 300, in &lt;module&gt;
    main(unused_args=[sys.argv[0]] + unparsed)
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter.py", line 158, in main
    memory_optimizer.optimize_gpu_memory(output_graph_def)
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/memory_optimizer.py", line 227, in optimize_gpu_memory
    mem_optimizer.optimize()
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/memory_optimizer.py", line 112, in optimize
    op.output_shape[i].dims)
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/memory_optimizer.py", line 192, in get_op_mem_block
    mem_block[0] = output_shape[2]
  File "/private/var/tmp/_bazel_ouzhijie/e37d1db59dd99bf6e7b4652ee6375961/execroot/mace/bazel-out/darwin-fastbuild/bin/mace/python/tools/converter.runfiles/com_google_protobuf/python/google/protobuf/internal/containers.py", line 204, in __getitem__
    return self._values[key]
IndexError: list index out of range
Traceback (most recent call last):
  File "tools/converter.py", line 1595, in &lt;module&gt;
    flags.func(flags)
  File "tools/converter.py", line 785, in convert_func
    convert_model(configs)
  File "tools/converter.py", line 711, in convert_model
    ",".join(model_config.get(YAMLKeyword.graph_optimize_options, [])))
  File "/Users/ouzhijie/Documents/WorkSpace/mace/tools/sh_commands.py", line 524, in gen_model_code
    _fg=True)
  File "/usr/local/lib/python2.7/site-packages/sh.py", line 1413, in __call__
    raise exc
sh.ErrorReturnCode_1:
	</description>
	<comments>
		<comment id='1' author='rogerou' date='2018-07-23T09:42:36Z'>
		Can you add some log and see at which op it failed?
		</comment>
		<comment id='2' author='rogerou' date='2018-07-24T02:40:16Z'>
		All the logs here :
Transform model to one that can better run on device
convert reshape and matmul to fc
convert matmul to fc
convert matmul to fc
convert matmul to fc
Transform gpu winograd conv_6/convolution(Conv2D)
Transform gpu winograd conv_8/convolution(Conv2D)
Fold biasadd: conv_1/convolution(Conv2D)
Fold biasadd: conv_2/convolution(Conv2D)
Fold biasadd: conv_3/convolution(Conv2D)
Fold biasadd: conv_4/convolution(Conv2D)
Fold biasadd: conv_5/convolution(Conv2D)
Fold biasadd: conv_7/convolution(Conv2D)
Fold biasadd: fc/fc_2/MatMul(FullyConnected)
Fold biasadd: fc/fc_4/MatMul(FullyConnected)
Fold biasadd: fc/fc_5/MatMul(FullyConnected)
Fold biasadd: fc/fc_3/MatMul(FullyConnected)
Fold biasadd: conv_6/convolution_inverse_transform(WinogradInverseTransform)
Fold biasadd: conv_8/convolution_inverse_transform(WinogradInverseTransform)
Transpose filters to OIHW
Reshape fully connected weight shape
Transform buffer to image
update op with float data type
Sort by execution
Final ops:
conv_8/weights_0_b2i (BufferToImage): []
conv_6/weights_0_b2i (BufferToImage): []
input_imgs (BufferToImage): [[1L, 112L, 112L, 3L]]
div (Eltwise): [[1L, 112L, 112L, 3L]]
sub (Eltwise): [[1L, 112L, 112L, 3L]]
conv_1/weights_0_b2i (BufferToImage): []
conv_1/biases_0_b2i (BufferToImage): []
conv_1/convolution (Conv2D): [[1L, 56L, 56L, 16L]]
conv_1/Relu (Activation): [[1L, 56L, 56L, 16L]]
conv_1/Neg/_0__cf__0_0_b2i (BufferToImage): []
conv_1/Neg_1 (Eltwise): [[1L, 56L, 56L, 16L]]
conv_1/Relu_1 (Activation): [[1L, 56L, 56L, 16L]]
conv_1/mul (Eltwise): [[1L, 56L, 56L, 16L]]
conv_1/add (Eltwise): [[1L, 56L, 56L, 16L]]
conv_2/weights_0_b2i (BufferToImage): []
conv_2/biases_0_b2i (BufferToImage): []
conv_2/convolution (Conv2D): [[1L, 56L, 56L, 16L]]
conv_2/Relu (Activation): [[1L, 56L, 56L, 16L]]
conv_2/Neg/_1__cf__1_0_b2i (BufferToImage): []
conv_2/Neg_1 (Eltwise): [[1L, 56L, 56L, 16L]]
conv_2/Relu_1 (Activation): [[1L, 56L, 56L, 16L]]
conv_2/mul (Eltwise): [[1L, 56L, 56L, 16L]]
conv_2/add (Eltwise): [[1L, 56L, 56L, 16L]]
conv_3/weights_0_b2i (BufferToImage): []
conv_3/biases_0_b2i (BufferToImage): []
conv_3/convolution (Conv2D): [[1L, 28L, 28L, 16L]]
conv_3/Relu (Activation): [[1L, 28L, 28L, 16L]]
conv_3/Neg/_2__cf__2_0_b2i (BufferToImage): []
conv_3/Neg_1 (Eltwise): [[1L, 28L, 28L, 16L]]
conv_3/Relu_1 (Activation): [[1L, 28L, 28L, 16L]]
conv_3/mul (Eltwise): [[1L, 28L, 28L, 16L]]
conv_3/add (Eltwise): [[1L, 28L, 28L, 16L]]
conv_4/weights_0_b2i (BufferToImage): []
conv_4/biases_0_b2i (BufferToImage): []
conv_4/convolution (Conv2D): [[1L, 28L, 28L, 16L]]
conv_4/Relu (Activation): [[1L, 28L, 28L, 16L]]
conv_4/Neg/_3__cf__3_0_b2i (BufferToImage): []
conv_4/Neg_1 (Eltwise): [[1L, 28L, 28L, 16L]]
conv_4/Relu_1 (Activation): [[1L, 28L, 28L, 16L]]
conv_4/mul (Eltwise): [[1L, 28L, 28L, 16L]]
conv_4/add (Eltwise): [[1L, 28L, 28L, 16L]]
conv_5/weights_0_b2i (BufferToImage): []
conv_5/biases_0_b2i (BufferToImage): []
conv_5/convolution (Conv2D): [[1L, 14L, 14L, 32L]]
conv_5/Relu (Activation): [[1L, 14L, 14L, 32L]]
conv_5/Neg/_4__cf__4_0_b2i (BufferToImage): []
conv_5/Neg_1 (Eltwise): [[1L, 14L, 14L, 32L]]
conv_5/Relu_1 (Activation): [[1L, 14L, 14L, 32L]]
conv_5/mul (Eltwise): [[1L, 14L, 14L, 32L]]
conv_5/add (Eltwise): [[1L, 14L, 14L, 32L]]
conv_6/convolution_input_transform (WinogradTransform): [[36L, 32L, 16L]]
conv_6/convolution_matmul (MatMul): [[36L, 64L, 16L]]
conv_6/biases_0_b2i (BufferToImage): []
conv_6/convolution_inverse_transform (WinogradInverseTransform): [[1L, 14L, 14L, 64L]]
conv_6/Relu (Activation): [[1L, 14L, 14L, 64L]]
conv_6/Neg/_5__cf__5_0_b2i (BufferToImage): []
conv_6/Neg_1 (Eltwise): [[1L, 14L, 14L, 64L]]
conv_6/Relu_1 (Activation): [[1L, 14L, 14L, 64L]]
conv_6/mul (Eltwise): [[1L, 14L, 14L, 64L]]
conv_6/add (Eltwise): [[1L, 14L, 14L, 64L]]
conv_7/weights_0_b2i (BufferToImage): []
conv_7/biases_0_b2i (BufferToImage): []
conv_7/convolution (Conv2D): [[1L, 7L, 7L, 64L]]
conv_7/Relu (Activation): [[1L, 7L, 7L, 64L]]
conv_7/Neg/_6__cf__6_0_b2i (BufferToImage): []
conv_7/Neg_1 (Eltwise): [[1L, 7L, 7L, 64L]]
conv_7/Relu_1 (Activation): [[1L, 7L, 7L, 64L]]
conv_7/mul (Eltwise): [[1L, 7L, 7L, 64L]]
conv_7/add (Eltwise): [[1L, 7L, 7L, 64L]]
conv_8/convolution_input_transform (WinogradTransform): [[36L, 64L, 4L]]
conv_8/convolution_matmul (MatMul): [[36L, 128L, 4L]]
conv_8/biases_0_b2i (BufferToImage): []
conv_8/convolution_inverse_transform (WinogradInverseTransform): [[1L, 7L, 7L, 128L]]
conv_8/Relu (Activation): [[1L, 7L, 7L, 128L]]
conv_8/Neg/_7__cf__7_0_b2i (BufferToImage): []
conv_8/Neg_1 (Eltwise): [[1L, 7L, 7L, 128L]]
conv_8/Relu_1 (Activation): [[1L, 7L, 7L, 128L]]
conv_8/mul (Eltwise): [[1L, 7L, 7L, 128L]]
conv_8/add (Eltwise): [[1L, 7L, 7L, 128L]]
max_pooling_1/MaxPool (Pooling): [[1L, 3L, 3L, 128L]]
fc/fc_0/MatMul (MatMul): [[1L, 512L]]
fc/fc_0/biases_0_b2i (BufferToImage): []
fc/fc_0/BiasAdd (BiasAdd): [[1L, 512L]]
fc/fc_0/Relu (Activation): [[1L, 512L]]
fc/fc_0/Neg/_8__cf__8_0_b2i (BufferToImage): []
fc/fc_0/Neg_1 (Eltwise): [[1L, 512L]]
fc/fc_0/Relu_1 (Activation): [[1L, 512L]]
fc/fc_0/mul (Eltwise): [[1L, 512L]]
fc/fc_0/add (Eltwise): [[1L, 512L]]
fc/fc_3/weights_0_b2i (BufferToImage): []
fc/fc_3/biases_0_b2i (BufferToImage): []
fc/fc_3/MatMul (FullyConnected): [[1L, 166L]]
fc/fc_2/weights_0_b2i (BufferToImage): []
fc/fc_2/biases_0_b2i (BufferToImage): []
fc/fc_2/MatMul (FullyConnected): [[1L, 128L]]
fc/fc_2/Relu (Activation): [[1L, 128L]]
fc/fc_2/Neg/_10__cf__10_0_b2i (BufferToImage): []
fc/fc_2/Neg_1 (Eltwise): [[1L, 128L]]
fc/fc_2/Relu_1 (Activation): [[1L, 128L]]
fc/fc_2/mul (Eltwise): [[1L, 128L]]
fc/fc_2/add (Eltwise): [[1L, 128L]]
fc/fc_4/weights_0_b2i (BufferToImage): []
fc/fc_4/biases_0_b2i (BufferToImage): []
fc/fc_4/MatMul (FullyConnected): [[1L, 2L]]
fc/fc_1/MatMul (MatMul): [[1L, 128L]]
fc/fc_1/biases_0_b2i (BufferToImage): []
fc/fc_1/BiasAdd (BiasAdd): [[1L, 128L]]
fc/fc_1/Relu (Activation): [[1L, 128L]]
fc/fc_1/Neg/_9__cf__9_0_b2i (BufferToImage): []
fc/fc_1/Neg_1 (Eltwise): [[1L, 128L]]
fc/fc_1/Relu_1 (Activation): [[1L, 128L]]
fc/fc_1/mul (Eltwise): [[1L, 128L]]
fc/fc_1/add (Eltwise): [[1L, 128L]]
fc/fc_5/weights_0_b2i (BufferToImage): []
fc/fc_5/biases_0_b2i (BufferToImage): []
fc/fc_5/MatMul (FullyConnected): [[1L, 3L]]
network_output (Concat): [[1L, 171L]]
final_output (Reshape): [[1L, 171L]]
mace_output_node_final_output (ImageToBuffer): []
start optimize gpu memory.
Traceback (most recent call last):
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter.py", line 300, in &lt;module&gt;
    main(unused_args=[sys.argv[0]] + unparsed)
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/converter.py", line 158, in main
    memory_optimizer.optimize_gpu_memory(output_graph_def)
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/memory_optimizer.py", line 227, in optimize_gpu_memory
    mem_optimizer.optimize()
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/memory_optimizer.py", line 112, in optimize
    op.output_shape[i].dims)
  File "/Users/ouzhijie/Documents/WorkSpace/mace/bazel-bin/mace/python/tools/converter.runfiles/mace/mace/python/tools/memory_optimizer.py", line 192, in get_op_mem_block
    mem_block[0] = output_shape[2]
  File "/private/var/tmp/_bazel_ouzhijie/e37d1db59dd99bf6e7b4652ee6375961/execroot/mace/bazel-out/darwin-fastbuild/bin/mace/python/tools/converter.runfiles/com_google_protobuf/python/google/protobuf/internal/containers.py", line 204, in __getitem__
    return self._values[key]
IndexError: list index out of range
Traceback (most recent call last):
  File "tools/converter.py", line 1595, in &lt;module&gt;
    flags.func(flags)
  File "tools/converter.py", line 785, in convert_func
    convert_model(configs)
  File "tools/converter.py", line 711, in convert_model
    ",".join(model_config.get(YAMLKeyword.graph_optimize_options, [])))
  File "/Users/ouzhijie/Documents/WorkSpace/mace/tools/sh_commands.py", line 524, in gen_model_code
    _fg=True)
  File "/usr/local/lib/python2.7/site-packages/sh.py", line 1413, in __call__
    raise exc
sh.ErrorReturnCode_1:

  RAN: /usr/local/bin/python bazel-bin/mace/python/tools/converter -u --platform=tensorflow --model_file=/Users/ouzhijie/Downloads/1.1.0.170918_alpha_landmark.pb --weight_file= --model_checksum=e77bbd3a2cfe071eef174df696956866c75eced9960a9425ea369294c0944a99 --weight_checksum= --input_node=input_imgs --output_node=final_output --runtime=cpu+gpu --template=mace/python/tools --model_tag=mobilenet_v1 --input_shape=1,112,112,3 --dsp_mode=0 --embed_model_data=False --winograd=4 --obfuscate=0 --output_dir=mace/codegen/models/mobilenet_v1 --model_graph_format=file --data_type=fp16_fp32 --graph_optimize_options=

  STDOUT:


  STDERR: 
		</comment>
		<comment id='3' author='rogerou' date='2018-07-24T03:47:16Z'>
		&lt;denchmark-code&gt;max_pooling_1/MaxPool (Pooling): [[1L, 3L, 3L, 128L]]
fc/fc_0/MatMul (MatMul): [[1L, 512L]]
&lt;/denchmark-code&gt;

Is there a Rehape Op between above two layers in you tensorflow model?
		</comment>
		<comment id='4' author='rogerou' date='2018-07-24T03:48:17Z'>
		Can you provide model snippet of your proto text around max_pooling_1/MaxPool and
fc/fc_0/MatMul ?
		</comment>
		<comment id='5' author='rogerou' date='2018-07-24T07:03:37Z'>
		I am not sure  this node:
node {
  name: "max_pooling_1/MaxPool"
  op: "MaxPool"
  input: "conv_8/add"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
node {
  name: "gradients/fc/fc_0/MatMul_grad/MatMul"
  op: "MatMul"
  input: "gradients/fc/fc_0/BiasAdd_grad/tuple/control_dependency"
  input: "fc/fc_0/weights/read"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "transpose_a"
    value {
      b: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      b: true
    }
  }
}
node {
  name: "gradients/fc/fc_0/MatMul_grad/MatMul_1"
  op: "MatMul"
  input: "flat_1/Reshape"
  input: "gradients/fc/fc_0/BiasAdd_grad/tuple/control_dependency"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "transpose_a"
    value {
      b: true
    }
  }
  attr {
    key: "transpose_b"
    value {
      b: false
    }
  }
}
node {
  name: "gradients/fc/fc_0/MatMul_grad/tuple/group_deps"
  op: "NoOp"
  input: "^gradients/fc/fc_0/MatMul_grad/MatMul"
  input: "^gradients/fc/fc_0/MatMul_grad/MatMul_1"
}
node {
  name: "gradients/fc/fc_0/MatMul_grad/tuple/control_dependency"
  op: "Identity"
  input: "gradients/fc/fc_0/MatMul_grad/MatMul"
  input: "^gradients/fc/fc_0/MatMul_grad/tuple/group_deps"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "_class"
    value {
      list {
        s: "loc:@gradients/fc/fc_0/MatMul_grad/MatMul"
      }
    }
  }
}
node {
  name: "gradients/fc/fc_0/MatMul_grad/tuple/control_dependency_1"
  op: "Identity"
  input: "gradients/fc/fc_0/MatMul_grad/MatMul_1"
  input: "^gradients/fc/fc_0/MatMul_grad/tuple/group_deps"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "_class"
    value {
      list {
        s: "loc:@gradients/fc/fc_0/MatMul_grad/MatMul_1"
      }
    }
  }
}
node {
  name: "gradients/flat_1/Reshape_grad/Shape"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: "\000\002\000\000\003\000\000\000\003\000\000\000\200\000\000\000"
      }
    }
  }
}
node {
  name: "gradients/flat_1/Reshape_grad/Reshape"
  op: "Reshape"
  input: "gradients/fc/fc_0/MatMul_grad/tuple/control_dependency"
  input: "gradients/flat_1/Reshape_grad/Shape"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "Tshape"
    value {
      type: DT_INT32
    }
  }
}
node {
  name: "gradients/max_pooling_1/MaxPool_grad/MaxPoolGrad"
  op: "MaxPoolGrad"
  input: "conv_8/add"
  input: "max_pooling_1/MaxPool"
  input: "gradients/flat_1/Reshape_grad/Reshape"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "data_format"
    value {
      s: "NHWC"
    }
  }
  attr {
    key: "ksize"
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
  attr {
    key: "padding"
    value {
      s: "VALID"
    }
  }
  attr {
    key: "strides"
    value {
      list {
        i: 1
        i: 2
        i: 2
        i: 1
      }
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/Shape"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: "\000\002\000\000\007\000\000\000\007\000\000\000\200\000\000\000"
      }
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/Shape_1"
  op: "Const"
  attr {
    key: "dtype"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "value"
    value {
      tensor {
        dtype: DT_INT32
        tensor_shape {
          dim {
            size: 4
          }
        }
        tensor_content: "\000\002\000\000\007\000\000\000\007\000\000\000\200\000\000\000"
      }
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/BroadcastGradientArgs"
  op: "BroadcastGradientArgs"
  input: "gradients/conv_8/add_grad/Shape"
  input: "gradients/conv_8/add_grad/Shape_1"
  attr {
    key: "T"
    value {
      type: DT_INT32
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/Sum"
  op: "Sum"
  input: "gradients/max_pooling_1/MaxPool_grad/MaxPoolGrad"
  input: "gradients/conv_8/add_grad/BroadcastGradientArgs"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "keep_dims"
    value {
      b: false
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/Reshape"
  op: "Reshape"
  input: "gradients/conv_8/add_grad/Sum"
  input: "gradients/conv_8/add_grad/Shape"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "Tshape"
    value {
      type: DT_INT32
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/Sum_1"
  op: "Sum"
  input: "gradients/max_pooling_1/MaxPool_grad/MaxPoolGrad"
  input: "gradients/conv_8/add_grad/BroadcastGradientArgs:1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "Tidx"
    value {
      type: DT_INT32
    }
  }
  attr {
    key: "keep_dims"
    value {
      b: false
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/Reshape_1"
  op: "Reshape"
  input: "gradients/conv_8/add_grad/Sum_1"
  input: "gradients/conv_8/add_grad/Shape_1"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "Tshape"
    value {
      type: DT_INT32
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/tuple/group_deps"
  op: "NoOp"
  input: "^gradients/conv_8/add_grad/Reshape"
  input: "^gradients/conv_8/add_grad/Reshape_1"
}
node {
  name: "gradients/conv_8/add_grad/tuple/control_dependency"
  op: "Identity"
  input: "gradients/conv_8/add_grad/Reshape"
  input: "^gradients/conv_8/add_grad/tuple/group_deps"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "_class"
    value {
      list {
        s: "loc:@gradients/conv_8/add_grad/Reshape"
      }
    }
  }
}
node {
  name: "gradients/conv_8/add_grad/tuple/control_dependency_1"
  op: "Identity"
  input: "gradients/conv_8/add_grad/Reshape_1"
  input: "^gradients/conv_8/add_grad/tuple/group_deps"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "_class"
    value {
      list {
        s: "loc:@gradients/conv_8/add_grad/Reshape_1"
      }
    }
  }
}
		</comment>
		<comment id='6' author='rogerou' date='2018-07-24T08:15:51Z'>
		I have the same problem,have you solve it ?&lt;denchmark-link:https://github.com/rogerou&gt;@rogerou&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='rogerou' date='2018-07-24T08:41:59Z'>
		&lt;denchmark-link:https://github.com/rogerou&gt;@rogerou&lt;/denchmark-link&gt;
 ， the model you pasted, is it the model after optimization (using tensorflow transform_graph)?
		</comment>
		<comment id='8' author='rogerou' date='2018-07-24T09:19:25Z'>
		model struct with optimization &lt;denchmark-link:https://github.com/liyinhgqw&gt;@liyinhgqw&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;graph/max_pooling_1
Subgraph: 1 nodes
Attributes (0)
Inputs (1)
graph/conv_8/add ?×?×?×128
Outputs (1)
graph/flat_1/Reshape ?×?×?×128
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;graph/flat_1/Reshape/shape
Operation: Const
Attributes (2)
dtype
{"type":"DT_INT32"}
value
{"tensor":{"dtype":"DT_INT32","tensor_shape":{"dim":[{"size":2}]},"tensor_content":"\\377\\377\\377\\377\\200\\004\\000\\000"}}
Inputs (0)
Outputs (0)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;graph/flat_1/Reshape
Operation: Reshape
Attributes (2)
T
{"type":"DT_FLOAT"}
Tshape
{"type":"DT_INT32"}
Inputs (2)
graph/max_pooling_1/MaxPool ?×?×?×128
graph/flat_1/Reshape/shape 2
Outputs (3)
graph/fc/fc_2/MatMul ?×1152
graph/fc/fc_1/MatMul ?×1152
graph/fc/fc_0/MatMul ?×1152

&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;graph/max_pooling_1/MaxPool
Operation: MaxPool
Attributes (5)
T
{"type":"DT_FLOAT"}
data_format
{"s":"NHWC"}
ksize
{"list":{"i":[1,2,2,1]}}
padding
{"s":"VALID"}
strides
{"list":{"i":[1,2,2,1]}}
Inputs (1)
graph/conv_8/add ?×?×?×128
Outputs (1)
graph/flat_1/Reshape ?×?×?×128
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='rogerou' date='2018-07-24T11:29:52Z'>
		Could you attach your optimized model to help us debug this issue?
		</comment>
		<comment id='10' author='rogerou' date='2018-07-24T12:56:35Z'>
		I comment op MatMul  in memory_optimizer.py  and convert successfully &lt;denchmark-link:https://github.com/liyinhgqw&gt;@liyinhgqw&lt;/denchmark-link&gt;

 def get_op_mem_block(self, op_type, output_shape):
        mem_block = [0, 0]
        if op_type == 'WinogradTransform' :#or op_type == 'MatMul':
            mem_block[0] = output_shape[2]
            mem_block[1] = output_shape[0] * int((output_shape[1] + 3) / 4)
        else:
            if len(output_shape) == 2:  # only support fc/softmax
                mem_block[0] = int((output_shape[1] + 3) / 4)
                mem_block[1] = output_shape[0]
            else:
                mem_block[0] = output_shape[2] * int((output_shape[3] + 3) / 4)
                mem_block[1] = output_shape[0] * output_shape[1]
        return mem_block
		</comment>
		<comment id='11' author='rogerou' date='2018-07-25T01:58:12Z'>
		We will fix it and let you know in the first time.
Thanks.
		</comment>
		<comment id='12' author='rogerou' date='2018-07-26T00:36:19Z'>
		Fixed, please try master branch again.
		</comment>
		<comment id='13' author='rogerou' date='2018-07-26T08:34:53Z'>
		Convert successfully.
But CreateMaceEngineFromProto always return MACE_INVALID_ARGS with pb file or data file.
How can I load pb file and data file with CreateMaceEngineFromProto?
		</comment>
		<comment id='14' author='rogerou' date='2018-07-30T04:37:06Z'>
		Try this?
  std::shared_ptr&lt;mace::MaceEngine&gt; engine;
    create_engine_status =
        CreateMaceEngineFromProto(model_pb_data,
                                  FLAGS_model_data_file,
                                  input_names,
                                  output_names,
                                  device_type,
                                  &amp;engine);
		</comment>
		<comment id='15' author='rogerou' date='2018-07-30T09:53:05Z'>
		&lt;denchmark-link:https://github.com/liyinhgqw&gt;@liyinhgqw&lt;/denchmark-link&gt;
  not worked, the second param is string path not memory data
		</comment>
		<comment id='16' author='rogerou' date='2018-07-30T11:23:52Z'>
		Yes, it should be string path of data file.
Why do you want it be memory data?
		</comment>
		<comment id='17' author='rogerou' date='2018-07-31T03:45:33Z'>
		if  compiled into apk ,how can i get the path of the file?
		</comment>
		<comment id='18' author='rogerou' date='2018-07-31T05:10:57Z'>
		You can refer to the mace demo.
If you want to embed the model into mace library, you should use CreateMaceEngineFromCode.
See Advanced Usage in MACE doc.
		</comment>
		<comment id='19' author='rogerou' date='2018-08-02T06:19:28Z'>
		&lt;denchmark-link:https://github.com/rogerou&gt;@rogerou&lt;/denchmark-link&gt;
 if data compiled into apk, just pass an empty string.
		</comment>
		<comment id='20' author='rogerou' date='2018-08-02T10:23:14Z'>
		&lt;denchmark-link:https://github.com/nolanliou&gt;@nolanliou&lt;/denchmark-link&gt;
 if I want to switch  models  , how can i do?
compiled all the static libraries ?
		</comment>
		<comment id='21' author='rogerou' date='2018-08-02T11:10:55Z'>
		You could define multiple models in model-deployment file(.yml), then convert, only one model library will be generated.
		</comment>
	</comments>
</bug>