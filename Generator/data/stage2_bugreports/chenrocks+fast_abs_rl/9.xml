<bug id='9' author='QuietWoods' open_date='2018-07-27T08:19:19Z' closed_time='2018-07-27T08:31:48Z'>
	<summary>Possible bug in compute_rouge_n &amp; compute_rouge_l &amp; compute_rouge_l_summ</summary>
	<description>



fast_abs_rl/metric.py


        Lines 33 to 38
      in
      9e6c45d






 if mode == 'p': 



 score = precision 



 if mode == 'r': 



 score = recall 



 else: 



 score = f_score 





I assume this is  intended:
if mode == 'p':
     score = precision
elif mode == 'r':
      score = recall
else:
      score = f_score
	</description>
	<comments>
		<comment id='1' author='QuietWoods' date='2018-07-27T08:31:48Z'>
		Thanks for pointing this out! Fixed in &lt;denchmark-link:https://github.com/ChenRocks/fast_abs_rl/commit/a04b073dc9371172d80054204918fea2bd21f3c1&gt;a04b073&lt;/denchmark-link&gt;

Note that this does not affect the pretrained models since we use F-1 scores as the reward. And for the result we use official perl ROUGE package
		</comment>
	</comments>
</bug>