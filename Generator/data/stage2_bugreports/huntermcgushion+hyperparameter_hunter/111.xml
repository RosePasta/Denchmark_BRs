<bug id='111' author='caprone' open_date='2018-12-30T15:43:36Z' closed_time='2019-02-19T21:58:12Z'>
	<summary>Keras optimization issue</summary>
	<description>
HI Hunter!
seems to me that ther's a issue with Keras optimization..
when I define my custom "_build_fn_optimization" function,
i need to pass some parameters for a functional model initialization;
but seems that these parameters are not available in the scope of function;
then name error issue raises, for example:
"NameError: name 'maxlen' is not defined...
or am I wrong something?..
thanks
embedding_matrix = # load_glove.....
def _build_fn_optimization(embedding_matrix, maxlen=100, max_features=10000, embed_size=200):

    inp = Input(shape=(maxlen,))
    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)
    x = SpatialDropout1D(rate=Real(0.2, 0.5))(x)
    x = Bidirectional(CuDNNGRU(Integer(50, 150), return_sequences=True,
                               kernel_initializer=glorot_normal(seed=22),
                               recurrent_initializer=orthogonal(gain=1.0, seed=10)))(x)

    x = Flatten()(x)
    x = Dense(Integer(100, 200), activation="relu", kernel_initializer=glorot_normal(seed=1))(x)
    x = Dropout(0.1)(x)
    x = BatchNormalization()(x)

    x = Dense(1, activation="sigmoid")(x)
    model = Model(inputs=inp, outputs=x)
    model.compile(loss='binary_crossentropy', optimizer=Categorical(["adam", "rmsprop"]), metrics=["accuracy"])
    return model

def execute():
   
    env = Environment(
        train_dataset=x_train,
        target_column='target',
        root_results_path=Keras_path,
        do_predict_proba=True,  # True / 1
        metrics_map=['roc_auc_score'],
        # metrics_map=dict(f1=lambda y_true, y_pred: f1_score(y_true, y_pred, average="micro")),
        cross_validation_type=StratifiedKFold,
        cross_validation_params=dict(n_splits=5,  shuffle=True, random_state=32),
        runs=1
    )
   

    # Optimization ####################
    optimizer = BayesianOptimization(iterations=5,
                                     acquisition_optimizer_kwargs=dict(
                                                                     n_points=10000,
                                                                     n_restarts_optimizer=5,
                                                                     n_jobs=8),
                                    )

    optimizer.set_experiment_guidelines(
        model_initializer=KerasClassifier,
        model_init_params=dict(build_fn=_build_fn_optimization),
        model_extra_params=dict(
            fit=dict(
                    eval_set=[(env.train_input, env.train_target),
                              (env.validation_input, env.validation_target)],
                    early_stopping_rounds=5),
            # callbacks=[EarlyStopping(patience=5, monitor='val_acc', mode='max')],
            batch_size=Categorical([32, 64], transform="onehot"),
            epochs=500,
            verbose=0,
        ),
    )
	</description>
	<comments>
		<comment id='1' author='caprone' date='2018-12-31T00:45:35Z'>
		&lt;denchmark-link:https://github.com/caprone&gt;@caprone&lt;/denchmark-link&gt;
, thanks for opening this issue! I hadn't done a lot of testing with Keras , or  layers, so it's likely that you have found a HyperparameterHunter bug.
Could you provide a full, simplified example that I can copy and run on my machine to reproduce the problem?
Thanks for continuing to test new things with HyperparameterHunter and posting the problems you find! I really appreciate it!
		</comment>
		<comment id='2' author='caprone' date='2018-12-31T01:52:37Z'>
		Can you also try organizing your model architecture using Keras's Sequential API, rather than the functional model, to see if you have the same problem?
		</comment>
		<comment id='3' author='caprone' date='2019-01-01T14:38:08Z'>
		HI Hunter!
Oh.. this package is  too useful to do without it! ;)
Ok, also with sequential model, issue remains…
(and others different issues raises with different initializing structures, as You can see below)
I post a toy sequential example (data are integer matrix --texts data processed-- and label is binary array)
ps. embedding layer works good ( but only if it initializes  weights itself, because function doesn't accept  parameters, .. like  Glove or word2vec pre-trained weights 's matrix))
random_matrix = np.random.randint(0, 9999, (200, 100))
target = np.random.binomial(1, 0.30, size=200)
x_train = pd.DataFrame(random_matrix)
x_train['target'] = target
Keras_path = '../Keras_HyperparameterHunterAssets'
print(x_train.shape)


def _build_fn_optimization_seq(embedding_matrix, maxlen=100, max_features=9999, embed_size=200):

    model = Sequential(
        [

            Embedding(input_dim=max_features, output_dim=200, input_length=100,
                      trainable=True),
            SpatialDropout1D(rate=Real(0.2, 0.5)),
            # Bidirectional(CuDNNGRU(Integer(50, 150), return_sequences=True,
            #                        kernel_initializer=glorot_normal(seed=12300),
            #                        recurrent_initializer=orthogonal(gain=1.0, seed=10000))),
            Flatten(),
            Dense(Integer(100, 200), activation="relu"),  # kernel_initializer=glorot_normal(seed=12300)),
            # Dropout(0.12),
            # BatchNormalization(),
            Dense(1, activation="sigmoid")
        ]
    )
    model.compile(
        optimizer=Categorical(["adam", "rmsprop"]), loss="binary_crossentropy", metrics=["accuracy"]
    )
    return model


def execute():
  
    env = Environment(
        train_dataset=x_train,
        target_column='target',
        root_results_path=Keras_path,
        do_predict_proba=1,  # True / 1
        metrics_map=['roc_auc_score'],
        # metrics_map=dict(f1=lambda y_true, y_pred: f1_score(y_true, y_pred, average="micro")),
        cross_validation_type='StratifiedKFold',  
        cross_validation_params=dict(n_splits=3, shuffle=True, random_state=32),
        runs=1
    )

    # Optimization ####################
    optimizer = BayesianOptimization(iterations=2,
                                     acquisition_optimizer_kwargs=dict(
                                         n_points=10000,
                                         n_restarts_optimizer=5,
                                         n_jobs=8),
                                     base_estimator_kwargs=dict())

    optimizer.set_experiment_guidelines(
        model_initializer=KerasClassifier,
        model_init_params=dict(build_fn=_build_fn_optimization_seq),
     
        # --- If we try to validate on eval_set issure rises... ---
        # model_extra_params=dict(
        #     fit=dict(
        #             eval_set=[(env.train_input, env.train_target),
        #                       (env.validation_input, env.validation_target)],
        #             early_stopping_rounds=5),
        #     # callbacks=[EarlyStopping(patience=5, monitor='val_acc', mode='max')],
        #     batch_size=Categorical([32, 64], transform="onehot"),
        #     epochs=500,
        #     verbose=0,
        # ),
        model_extra_params=dict(
            callbacks=[ReduceLROnPlateau(patience=Integer(5, 10))],
            batch_size=Categorical([32, 64], transform="onehot"),
            epochs=10,
            verbose=0,
        ),
    )
    optimizer.go()


if __name__ == '__main__':
    execute()
but if instead we in initialize model with numeric values, without pass parameters, all seems works well:
def _build_fn_optimization_seq(embedding_matrix, maxlen=100, max_features=95000, embed_size=200):

    model = Sequential(
        [

            Embedding(input_dim=9999, output_dim=200, input_length=100,
                      trainable=True),
            SpatialDropout1D(rate=Real(0.2, 0.5)),
            # Bidirectional(CuDNNGRU(Integer(50, 150), return_sequences=True,
            #                        kernel_initializer=glorot_normal(seed=12300),
            #                        recurrent_initializer=orthogonal(gain=1.0, seed=10000))),
            Flatten(),
            Dense(Integer(100, 200), activation="relu"),  # kernel_initializer=glorot_normal(seed=12300)),
            # Dropout(0.12),
            # BatchNormalization(),
            Dense(1, activation="sigmoid")
        ]
    )
    model.compile(
        optimizer=Categorical(["adam", "rmsprop"]), loss="binary_crossentropy", metrics=["accuracy"]
    )
    return model

Another  issue is that Hunter seems can’t works with CuDNN’s layers…(probably  sklearn API issue?)
Another  issue raises with kernel_initializer like “glorot_normal”raises:  “AttributeError: 'VarianceScaling”;
Another issue raises whit use ‘eval_set’..raises:  "ValueError: fit is not a legal parameter".

		</comment>
		<comment id='4' author='caprone' date='2019-01-07T22:25:36Z'>
		&lt;denchmark-link:https://github.com/caprone&gt;@caprone&lt;/denchmark-link&gt;
, I'm still looking into what the problem is here. Sorry it's taking so long.
Thank you for the thorough examples.
Just to make sure we're on the same page, what does your working environment look like? OS? Python version? HyperparameterHunter version (or master)? Keras version?
		</comment>
		<comment id='5' author='caprone' date='2019-01-08T13:45:07Z'>
		HI &lt;denchmark-link:https://github.com/HunterMcGushion&gt;@HunterMcGushion&lt;/denchmark-link&gt;
 !
and thanks for all!!
my env:
win 10 -- [ 64bit ]--
anaconda: 4.5.12 / python 3.6.2   -- [64bit] --
keras base / GPU:  2.2.4  on  tensorflow :1.11.0 -- [GPU version] --
hyperparameter_hunter: 2.0.1
		</comment>
		<comment id='6' author='caprone' date='2019-01-24T22:38:02Z'>
		&lt;denchmark-link:https://github.com/caprone&gt;@caprone&lt;/denchmark-link&gt;
,
I'm very sorry for the long delay, but I've been working on updating the testing suite, and I wanted to add the issues you brought up here as test cases. I've just started looking into this again, and wanted to confirm that these are the issues you're having:

Embedding layer can’t be given initialized weights - Embedding alone works if it initializes own weights - Needs function parameters to be given weight initialization

...


Using eval_set as model_extra_params.fit kwarg

eval_set is actually an invalid argument for KerasClassifier. I think you meant to use validation_data
However, HyperparameterHunter automatically sets validation_data to be OOF data produced by cross validation scheme

I can imagine this would be unexpected behavior, so do you have any thoughts on this?




Doesn’t work with CuDNN layers (like CuDNNGRU)

...


Doesn’t work with kernel_initializer function values (like glorot_normal(seed=1230), instead of ”glorot_normal”)

Currently HyperparameterHunter expected initializer values to be their string names
I overlooked examples like the one you gave with orthogonal(gain=1.0, seed=10000), in which the initializers actually have parameters you may want to change, so I'm looking into this



Please let me know if I missed any problems you brought up, or if I misunderstood them. Again, I apologize for taking so long to get back to you, but I felt it was important to get unittests working properly before starting to work on the library normally again.
		</comment>
		<comment id='7' author='caprone' date='2019-01-26T12:33:35Z'>
		HI Hunter!
&lt;denchmark-link:https://github.com/HunterMcGushion&gt;@HunterMcGushion&lt;/denchmark-link&gt;

but I felt it was important to get unittests working properly
totally agree!
Yes, problems are that! thanks.
&lt;denchmark-link:https://github.com/HunterMcGushion&gt;@HunterMcGushion&lt;/denchmark-link&gt;

"eval_set is actually an invalid argument for KerasClassifier. I think you meant to use validation_data"
ok, yes, my purpose is to apply eralystopping on validation_data in tuning process .
Thanks !!!
		</comment>
		<comment id='8' author='caprone' date='2019-01-26T22:12:35Z'>
		Thanks for understanding.
I see. Perhaps it's a bad idea for HyperparameterHunter to automatically add the validation_data, especially since it isn't documented. Do you have any thoughts?
		</comment>
		<comment id='9' author='caprone' date='2019-01-28T12:15:53Z'>
		HI &lt;denchmark-link:https://github.com/HunterMcGushion&gt;@HunterMcGushion&lt;/denchmark-link&gt;

ok, can we instead  set "validation_split" argument in "model_extra_params" dict? in this way it's possible monitor on "val_loss" scores.
		</comment>
		<comment id='10' author='caprone' date='2019-02-06T21:33:55Z'>
		Sorry this has been more complicated than I expected it would be. I recently merged updates that (mostly) address the problems with using callable initializers. There are a few more wrinkles to work out, but you should be able to do what you were trying to now with callable initializers.
Regarding the other problems you brought up, I’m not sure when I’ll be able to tackle them, as there are quite a few other items on my todo list. If you, or anyone else is interested in submitting a PR, I’d be more than happy to work together on it.
Therefore, I’m going to break up this issue into several separate issues so we can keep better track of them and hopefully make it easier for someone else to track them down.
I’ll mention you in the new issues, so please feel free to jump in if I miss anything you brought up here.
As always, thank you for bringing these bugs up and opening the issue! I very much appreciate your help!
		</comment>
		<comment id='11' author='caprone' date='2019-02-19T21:58:12Z'>
		This issue has been separated into four new issues to facilitate tracking and clarify the problems. Therefore, this issue is being closed. The new issues are as follows:

#122 - Improve matching Keras kernel_initializer strings and callables
#123 - Keras Embedding layers with initialized weights
#124 - (Discussion) Confusion using validation_data with Keras model_extra_params[“fit”]
#125 - Keras CuDNN layers (like CuDNNGRU) not working

If any of the above are missing information brought up in &lt;denchmark-link:https://github.com/HunterMcGushion/hyperparameter_hunter/issues/111&gt;#111&lt;/denchmark-link&gt;
, please feel free to mention/paste it in the new issue
		</comment>
	</comments>
</bug>