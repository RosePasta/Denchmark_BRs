<bug id='154' author='HunterMcGushion' open_date='2019-07-03T02:42:27Z' closed_time='2019-07-09T03:16:38Z'>
	<summary>Bug optimizing Categorical-only spaces</summary>
	<description>
Conditions to summon bug (assume a blood offering has already been made):

Use BayesianOptPro
Use exclusively Categorical dimensions
(Kinda optional - See details below) At least one of the Categorical dimensions must be in FeatureEngineer

The above 3 conditions will cause  to break outright. However, problems still arise when omitting , while searching an exclusively multi- space. The effect of this bug is that saved experiment results are not properly marked as . Tests demonstrating the behavior of this second bug are added in &lt;denchmark-link:https://github.com/HunterMcGushion/hyperparameter_hunter/commit/4b60f1a51532c2c89ecc0541588d6cbaa7d64c49&gt;4b60f1a&lt;/denchmark-link&gt;
. Tests for both variants of the bug are added in &lt;denchmark-link:https://github.com/HunterMcGushion/hyperparameter_hunter/pull/152&gt;#152&lt;/denchmark-link&gt;
.
When the two xfail tests are marked as xpass, the bug is probably (hopefully) fixed.
&lt;denchmark-h:h2&gt;Code to reproduce error:&lt;/denchmark-h&gt;

from hyperparameter_hunter import Environment, BayesianOptPro, FeatureEngineer
from hyperparameter_hunter import Categorical, Integer
from hyperparameter_hunter.utils.learning_utils import get_boston_data
from sklearn.ensemble import AdaBoostRegressor

def nothing_transform(train_targets, non_train_targets):
   return train_targets, non_train_targets, lambda _: _

env = Environment(
   train_dataset=get_boston_data(),
   results_path="HHAssets",
   target_column="DIS",
   metrics=["median_absolute_error"],
   cv_params=dict(n_splits=3, random_state=1),
)

opt_1 = BayesianOptPro(iterations=5, random_state=32, n_initial_points=1)
opt_1.set_experiment_guidelines(
   model_initializer=AdaBoostRegressor,
   model_init_params=dict(),                                 # FLAG: This breaks
   # model_init_params=dict(n_estimators=Integer(10, 100)),  # FLAG: This works
   feature_engineer=FeatureEngineer([
      Categorical([nothing_transform], optional=True),
   ]),
)
opt_1.go()
&lt;denchmark-h:h2&gt;Notes&lt;/denchmark-h&gt;


All other optimization protocols are fine with exclusively Categorical search spaces
Switching the two model_init_params in the code above demonstrates that the problem is bypassed by adding a non-Categorical dimension

	</description>
	<comments>
	</comments>
</bug>