<bug id='816' author='johnarevalo' open_date='2019-12-27T17:21:27Z' closed_time='2019-12-31T19:59:45Z'>
	<summary>SWA does not work with BatchNormalization</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
TensorFlow version and how it was installed (source or binary): pip3 install tf-nightly (2.1.0.dev20191227)
TensorFlow-Addons version and how it was installed (source or binary): pip3 install tfa-nightly (0.7.0.dev20191219)
Python version: 3.7.6
Is GPU used? (yes/no): no

Describe the bug
 is raised when invoking the &lt;denchmark-link:https://github.com/tensorflow/addons/blob/886853/tensorflow_addons/optimizers/stochastic_weight_averaging.py#L181&gt;assign_average_vars&lt;/denchmark-link&gt;
 in a model with  layers

&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow_addons.optimizers.stochastic_weight_averaging import SWA

# DATA
train_data, test_data = tf.keras.datasets.boston_housing.load_data()
train_ds = tf.data.Dataset.from_tensor_slices(train_data).shuffle(404).batch(32)
test_ds = tf.data.Dataset.from_tensor_slices(test_data).batch(32)

# MODEL
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(64, activation='relu', use_bias=True))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(1))

# OPTIMIZER
opt = SWA(tf.keras.optimizers.SGD())
model.compile(optimizer=opt, loss='mean_squared_error')
model.fit(train_ds, epochs=20)

# AFTER FIT. KeyError is raised
opt.assign_average_vars(model.variables)

for x_train, _ in train_ds:
    model(tf.cast(x_train, tf.float32), training=True)

model.evaluate(train_ds)
model.evaluate(test_ds)
&lt;/denchmark-code&gt;


As workaround, I am ignoring  and  non-trainable variables by changing &lt;denchmark-link:https://github.com/tensorflow/addons/blob/886853/tensorflow_addons/optimizers/stochastic_weight_averaging.py#L205-L206&gt;this&lt;/denchmark-link&gt;
 line:
&lt;denchmark-code&gt;assign_op = tf.group(
            [var.assign(self.get_slot(var, 'average')) for var in var_list])
&lt;/denchmark-code&gt;

to:
&lt;denchmark-code&gt;assign_op = tf.group(
            [var.assign(self.get_slot(var, 'average')) for var in var_list if var.trainable])
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='johnarevalo' date='2019-12-27T18:20:13Z'>
		Thanks for the report!
cc &lt;denchmark-link:https://github.com/shreyashpatodia&gt;@shreyashpatodia&lt;/denchmark-link&gt;
 code owner.
		</comment>
		<comment id='2' author='johnarevalo' date='2019-12-27T18:38:49Z'>
		Thanks for pointing this out &lt;denchmark-link:https://github.com/johnarevalo&gt;@johnarevalo&lt;/denchmark-link&gt;
!
I think what is happening is that when var_list is passed to _create_slots it does not contain the moving_mean and moving_variance variables. Thus, the KeyError.
I think the optimal solution is to look at when _create_slots is called and replicating that logic in assign_average_vars for consistency.
Do you think this makes sense as a solution?
		</comment>
		<comment id='3' author='johnarevalo' date='2019-12-27T18:58:59Z'>
		Hey, took a look at this for &lt;denchmark-link:https://github.com/tensorflow/addons/pull/760&gt;#760&lt;/denchmark-link&gt;

The problem is that keras only passes trainable variables (var_list) to any optimizer. So moving_average and moving_variance of BatchNorm is never passed to the opt, and therefore we don't consider them when calculating the averaged update.
For now, &lt;denchmark-link:https://github.com/johnarevalo&gt;@johnarevalo&lt;/denchmark-link&gt;
 fix of checking when if the variable is trainable should work. But I'm not sure if that makes sense. The  and  of each BatchNorm is calculated based on the original variables and not the moving average weights.
		</comment>
		<comment id='4' author='johnarevalo' date='2019-12-28T01:44:37Z'>
		Yep. I don't think the moving_mean and moving_average learned during training are relevant for the averaged weights.
I have a note in my implementation saying that the user should make one more pass through the data to correctly calculate batch norm statistics. This would mean resetting the moving_mean and moving_variance and then running a pass through the data to learn the scale and the offset per batch norm layer.
I didn't make this part of the optimizer because I don't think this sort of logic where we make a pass through the data should exist in an optimizer. &lt;denchmark-link:https://github.com/Squadrick&gt;@Squadrick&lt;/denchmark-link&gt;
 do you think we should add an auxiliary function that can be used to make stuff work with batch norm?
		</comment>
		<comment id='5' author='johnarevalo' date='2019-12-28T09:57:46Z'>
		That seems to be the way the authors do it. Source code for implementation &lt;denchmark-link:https://github.com/timgaripov/swa/blob/4a2ddfdb2692eda91f2ac41533b62027976c605b/utils.py#L107&gt;here&lt;/denchmark-link&gt;
 for the batch norm update. I'll get started with adding this to &lt;denchmark-link:https://github.com/tensorflow/addons/pull/760&gt;#760&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/shreyashpatodia&gt;@shreyashpatodia&lt;/denchmark-link&gt;
 Could you take a look once I've completed the update?
		</comment>
		<comment id='6' author='johnarevalo' date='2019-12-28T12:20:51Z'>
		Yeah sure!

I can also help write the function if you want me to.
		</comment>
		<comment id='7' author='johnarevalo' date='2019-12-31T19:26:08Z'>
		Closed with &lt;denchmark-link:https://github.com/tensorflow/addons/pull/760&gt;#760&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='8' author='johnarevalo' date='2019-12-31T19:54:15Z'>
		Yep.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 On 31-Dec-2019, at 7:26 PM, Sean Morgan ***@***.***&gt; wrote:

 ﻿
 Closed with #760?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or unsubscribe.


		</comment>
	</comments>
</bug>