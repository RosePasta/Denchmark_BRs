<bug id='1194' author='georgesterpu' open_date='2020-03-02T00:26:21Z' closed_time='2020-03-06T09:24:12Z'>
	<summary>Error when using a mask in RNN + AttentionWrapper cell</summary>
	<description>
System information

OS Platform and Distribution:  manjaro linux testing
TensorFlow version: pypi tf-nightly 2.2.0.dev20200228
TensorFlow-Addons version: pypi tfa-nightly 0.9.0.dev20200228
Python version: 3.7.6
Is GPU used?: no

Describe the bug
A ValueError is raised when using a mask in a RNN layer taking a RNN cell wrapped with an AttentionWrapper.
Code to reproduce the issue
import tensorflow_addons as tfa
import tensorflow as tf

bs = 4
cell = tf.keras.layers.LSTMCell(3)
mechanism = tfa.seq2seq.LuongAttention(units=3)
cell = tfa.seq2seq.AttentionWrapper(cell, mechanism)
layer = tf.keras.layers.RNN(cell)


@tf.function
def test(masked=False):
    seq_len = tf.random.uniform(shape=(), minval=2, maxval=10, dtype=tf.int32)
    data = tf.ones(shape=(bs, seq_len, 3))
    mechanism.setup_memory(data)

    if masked:
        mask = tf.sequence_mask(lengths=bs * [seq_len])
    else:
        mask = None

    return layer(data, mask=mask)


test(masked=False); print('Test 1 passed')
test(masked=True); print('Test 2 passed')
Other info / logs
Test 1 passed
Traceback (most recent call last):
  File "/run/media/zenbook/work/phd/work/__/pb1.py", line 26, in &lt;module&gt;
    test(masked=True); print('Test 2 passed')
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 611, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2419, in __call__
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "/home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /run/media/zenbook/work/phd/work/__/pb1.py:22 test  *
        return layer(data, mask=mask)
    /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:652 __call__  **
        return super(RNN, self).__call__(inputs, **kwargs)
    /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:926 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:793 call
        zero_output_for_mask=self.zero_output_for_mask)
    /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4205 rnn
        **while_loop_kwargs)
    /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:2688 while_loop
        back_prop=back_prop)
    /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:229 while_loop
        len_orig_loop_vars], expand_composites=True))
    /home/zenbook/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow/python/ops/while_v2.py:1143 _check_shapes_compat
        "specify a less-specific shape." % (input_t.name, shape, t.shape))

    ValueError: Input tensor 'rnn/AttentionWrapperZeroState/zeros_3:0' enters the loop with shape (), but has shape (4, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.


Process finished with exit code 1
	</description>
	<comments>
		<comment id='1' author='georgesterpu' date='2020-03-02T14:19:38Z'>
		This is because the Keras RNN assumes that each tensor in the state has a batch dimension. However, the time field in AttentionWrapperState is just a scalar tensor.
I think we could work around not having this time field. &lt;denchmark-link:https://github.com/georgesterpu&gt;@georgesterpu&lt;/denchmark-link&gt;
 Do you want to look into that?
		</comment>
		<comment id='2' author='georgesterpu' date='2020-03-02T15:36:18Z'>
		Thanks, &lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
.
Assuming that the problem comes from the  field -  uses it to write the current alignment slice to a  when the  flag is True. I can see several possibilities, such as:

pass the current loop index from outside (the time parameter of the _step function declared inside keras.backend.rnn ?
manage the loop counter as a member of the AttentionWrapper class
extend the TensorArray class to support an append operation, since AttentionWrapper never leapfrogs over indices and only writes sequentially anyway.

Unfortunately, as a final year student, I lack the throughput to carry on with a more detailed investigation - this is why I created the issue in the first place ☺️
		</comment>
		<comment id='3' author='georgesterpu' date='2020-03-02T16:50:37Z'>
		Thanks looking into workarounds. Actually, as alignment_history has a dynamic size and we only append to it, we can do something like:
ta = ta.write(ta.size(), elem)

Unfortunately, as a final year student, I lack the throughput to carry on with a more detailed investigation - this is why I created the issue in the first place relaxed

No worries! Thank you for finding these issues. I can make a PR as I already have a fix locally.
		</comment>
		<comment id='4' author='georgesterpu' date='2020-03-02T17:30:36Z'>
		Also, any chance that
4) the shape_invariants parameter of the while_loop in keras.backend.rnn could be configured conveniently to work with an AttentionWrapperState ?
		</comment>
		<comment id='5' author='georgesterpu' date='2020-03-02T17:40:12Z'>
		If you dive into the K.rnn code you'll find that it applies a tf.where on each state tensor, including our time scalar. This is the issue. See the relevant code here:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L4070&gt;https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/backend.py#L4070&lt;/denchmark-link&gt;

compared to the Addons decoder code that ignores scalar tensors:



addons/tensorflow_addons/seq2seq/decoder.py


         Line 455
      in
      fcaa672






 # TensorArrays and scalar states get passed through. 





		</comment>
		<comment id='6' author='georgesterpu' date='2020-03-06T17:26:51Z'>
		&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 thanks a lot !
There is a corner case where this doesn't work: when setting  in the  constructor. . Crashing under both eager and graph modes.
		</comment>
		<comment id='7' author='georgesterpu' date='2020-03-06T17:28:09Z'>
		import tensorflow_addons as tfa
import tensorflow as tf
import os

os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

cell = tf.keras.layers.LSTMCell(3)
mechanism = tfa.seq2seq.LuongAttention(units=3)
cell = tfa.seq2seq.AttentionWrapper(cell, mechanism, alignment_history=True)
layer = tf.keras.layers.RNN(cell)


# @tf.function
def mytest():
    var_len = tf.random.uniform(shape=(), minval=2, maxval=10, dtype=tf.int32)
    lengths = tf.random.uniform(shape=(var_len,), minval=1, maxval=var_len + 1, dtype=tf.int32)
    data = tf.ones(shape=(var_len, var_len, 3))
    mask = tf.sequence_mask(lengths, maxlen=var_len)
    mechanism.setup_memory(data, lengths)

    return layer(data, mask=mask)

mytest()
		</comment>
		<comment id='8' author='georgesterpu' date='2020-03-06T17:43:17Z'>
		It seems the Keras RNN backend does not accept TensorArray in the inputs. &lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 Is there any plans for Keras to support TensorArray in cell states?
		</comment>
	</comments>
</bug>