<bug id='418' author='seanpmorgan' open_date='2019-08-13T18:03:10Z' closed_time='2019-08-19T21:00:57Z'>
	<summary>CuDNN is failing to load in newly updated custom-op image</summary>
	<description>
Since ~ yesterday we're seeing fails on our GPU tests that say
&lt;denchmark-code&gt;Failed to get convolution algorithm. This is probably because cuDNN failed to initialize
&lt;/denchmark-code&gt;

A simple reproducible test is:
&lt;denchmark-code&gt;import numpy as np
import tensorflow as tf

data = np.random.random((2, 4, 4, 3))
layer = tf.keras.layers.Conv2D(5, (2,2), input_shape=(4,4,3))
print(layer(data))
&lt;/denchmark-code&gt;

I checked and it's not because of any change to the nightly TF package (today's version still works on an older docker image). The only change to the docker images was &lt;denchmark-link:https://github.com/YINZHI-keji/tensorflow/commit/5f81522c88c38b897e4313c28bf6e6cc7cc1456e&gt;YINZHI-keji/tensorflow@5f81522&lt;/denchmark-link&gt;
 but my hunch is that the base docker image was updated between when custom-op-gpu was originally pushed and today. Perhaps they updated the cudnn version? It's currently 7.5 but I deleted my old image so I can't check if that's what it was before.
cc &lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/av8ramit&gt;@av8ramit&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='seanpmorgan' date='2019-08-13T18:13:54Z'>
		Indeed sounds like an cudnn version issue. I don't see the base image being updated recently though &lt;denchmark-link:https://hub.docker.com/r/nvidia/cuda/tags?page=3&gt;https://hub.docker.com/r/nvidia/cuda/tags?page=3&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='seanpmorgan' date='2019-08-13T18:29:33Z'>
		Will look a bit from my GPU desktop when I get home.
		</comment>
		<comment id='3' author='seanpmorgan' date='2019-08-13T18:51:10Z'>
		
Indeed sounds like an cudnn version issue. I don't see the base image being updated recently though https://hub.docker.com/r/nvidia/cuda/tags?page=3

Hmmm looks like 10.0-cudnn7-devel-ubuntu16.04 was updated 14 days ago and I think the custom-op-gpu-ubuntu16.04 was published 15 days ago? Certainly makes it less likely but I think it's possible?

Will look a bit from my GPU desktop when I get home.

Thanks! Just to be clear I should have mentioned this is for custom-op-gpu-ubuntu16.04 I tested this morning on my older docker image of it and everything worked. Then I cleared my image and re-pulled it after last nights build and it failed.
		</comment>
		<comment id='4' author='seanpmorgan' date='2019-08-13T19:40:28Z'>
		Quickly pulled the nightly-gpu-py3 image and it also fails on tf 1.15. Temporarily we could install the correct cudnn in the Dockerfile (needs to be tested). We should also raise an issue on tf-core to figure out why that cudnn is failing
		</comment>
		<comment id='5' author='seanpmorgan' date='2019-08-14T19:06:07Z'>
		I tried matching cudnn as v7.6  using apt-get install libcudnn7=7.6.2.24-1+cuda10.0
But this breaks our build since there is no cudnn.h in /usr/include anymore. Curious as to why the pip package was moved up to cudnn 7.6? Was that for cuda10.1?
		</comment>
		<comment id='6' author='seanpmorgan' date='2019-08-19T15:34:58Z'>
		&lt;denchmark-link:https://github.com/yifeif&gt;@yifeif&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/av8ramit&gt;@av8ramit&lt;/denchmark-link&gt;
 Would it be possible to trigger another build &amp; push that docker image up? Looks like the new  has cudnn 7.6 installed correctly
		</comment>
		<comment id='7' author='seanpmorgan' date='2019-08-19T20:22:16Z'>
		I just updated custom-op-gpu-ubuntu16, could you give it another try?
		</comment>
		<comment id='8' author='seanpmorgan' date='2019-08-19T21:00:57Z'>
		Tests are now passing.. thank you!
		</comment>
	</comments>
</bug>