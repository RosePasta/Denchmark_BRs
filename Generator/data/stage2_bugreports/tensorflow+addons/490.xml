<bug id='490' author='PhilipMay' open_date='2019-09-09T21:54:09Z' closed_time='2019-11-05T02:20:34Z'>
	<summary>Bug in F1 and FBeta implementation and test</summary>
	<description>
Describe the bug
The tests for FBeta and F1 both use a softmax function with just one output.




addons/tensorflow_addons/metrics/fbeta_test.py


         Line 122
      in
      7624954






 model.add(layers.Dense(1, activation='softmax')) 








addons/tensorflow_addons/metrics/f1_test.py


         Line 116
      in
      7624954






 model.add(layers.Dense(1, activation='softmax')) 






The effect is that the output (prediction) is always 1. The right activation function for a binary calssification (with just one output is sigmoid). But this is not the only problem. This bug in the test hides an even worse other bug.
The other bug is that F1 and FBeta both can not handle values other then 1 (1.0) and 0 (0.0) as the predicted result. But the predictions of a binary classification (with simoid) and multi class classification with softmax is always somewhere between 1 and 0 (one hot encoded for multi class). The result of this bug is that this implementation of F1 and FBeta always return 0.0 when the predicted results are not exactly 0 (0.0) or 1 (1.0) - which is not realistic.
Also all other tests of F1 and FBeta have values of 0 or 1 as the predicted results. This does not reflect reality.




addons/tensorflow_addons/metrics/fbeta_test.py


         Line 91
      in
      7624954






 preds = tf.constant([[0, 0, 1], [1, 1, 0], [1, 1, 1]], dtype=tf.int32) 








addons/tensorflow_addons/metrics/f1_test.py


         Line 88
      in
      7624954






 preds = tf.constant([[0, 0, 1], [1, 1, 0], [1, 1, 1]], dtype=tf.int32) 






Code to reproduce the issue
Just change softmax to sigmoid in the tests and change verbose=1 and see the results.
	</description>
	<comments>
		<comment id='1' author='PhilipMay' date='2019-09-09T22:20:31Z'>
		I guess something like this needs to be added:
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/2ff39d00faf8f7e433ddcae0aa278f6e573b0c55/tensorflow/python/keras/metrics.py#L2767&gt;https://github.com/tensorflow/tensorflow/blob/2ff39d00faf8f7e433ddcae0aa278f6e573b0c55/tensorflow/python/keras/metrics.py#L2767&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='PhilipMay' date='2019-09-09T22:23:37Z'>
		Thanks, &lt;denchmark-link:https://github.com/PhilipMay&gt;@PhilipMay&lt;/denchmark-link&gt;
. You are right that result will not be an exact number.
The initial implementation we had do not indicate the probability.
preds = tf.constant([[0, 0, 1], [1, 1, 0], [1, 1, 1]], dtype=tf.int32) 
In this [0, 0, 1] does not indicate probability. Think we have 3 classes and the predicted is class 3 which is encoded as [0,0,1]
We may need to add argmax to change that in case if we are feeding directly.
Will take up this one and post the updates
		</comment>
		<comment id='3' author='PhilipMay' date='2019-09-09T22:28:43Z'>
		
We may need to add argmax to change that in case if we are feeding directly.

Yes - if we want to be able to add the metric to keras at training time we need that. Mybe you could have a look at the metrics implementations in Tensorflow.
Thanks you very much. :-)
		</comment>
		<comment id='4' author='PhilipMay' date='2019-09-09T22:30:04Z'>
		&lt;denchmark-link:https://github.com/seanpmorgan&gt;@seanpmorgan&lt;/denchmark-link&gt;
 Can you please assign this issue to me?
		</comment>
		<comment id='5' author='PhilipMay' date='2019-09-10T10:54:53Z'>
		This here seems to work for me - not sure if it still has bugs in border cases or can be implemented more efficient:
    def update_state(self, y_true, y_pred, sample_weight=None):
        cond = tf.equal(y_pred, tf.reduce_max(y_pred))
        y_pred = tf.where(cond, y_pred, tf.zeros_like(y_pred))
        not_cond = tf.math.logical_not(cond)
        y_pred = tf.where(not_cond, y_pred, tf.ones_like(y_pred))
        
        y_true = tf.cast(y_true, tf.int32)
        y_pred = tf.cast(y_pred, tf.int32)
		</comment>
		<comment id='6' author='PhilipMay' date='2019-09-10T12:24:24Z'>
		For further tests I would suggest that you can compare your results with those from sklearn:
from sklearn.metrics import f1_score
ytrue = np.argmax(y_test, axis=-1)
ypred = np.argmax(y_hat_test, axis=-1)

print(f1_score(ytrue, ypred, average='macro'))  
print(f1_score(ytrue, ypred, average='micro'))  
print(f1_score(ytrue, ypred, average='weighted'))
print(f1_score(ytrue, ypred, average=None))
		</comment>
		<comment id='7' author='PhilipMay' date='2019-09-10T13:52:40Z'>
		Thanks &lt;denchmark-link:https://github.com/PhilipMay&gt;@PhilipMay&lt;/denchmark-link&gt;

I have a working code ready now using threshold option. That will help in both multiclass and multilabel. Will create a PR today
		</comment>
		<comment id='8' author='PhilipMay' date='2019-09-10T15:05:46Z'>
		
Thanks @PhilipMay
I have a working code ready now using threshold option. That will help in both multiclass and multilabel. Will create a PR today

That is great. Thanks. I need that fix for my project. :-)
Maybe you can add a comparison with f1_score from sklearn to the tests.
		</comment>
		<comment id='9' author='PhilipMay' date='2019-09-11T08:56:30Z'>
		&lt;denchmark-link:https://github.com/SSaishruthi&gt;@SSaishruthi&lt;/denchmark-link&gt;
 I do not think that this implementation with threshold fixes the problem. Here is why:
When you have a "single-label categorial classification" where one sample belongs to exactly one class of many possible classes you apply a softmax funcation. Let me give an example:
You habe 3 classes: dog, cat and bike and do a one hot encoding.
&lt;denchmark-code&gt;dog = [1, 0, 0]
cat = [0, 1, 0]
bike = [0, 0, 1]
&lt;/denchmark-code&gt;

Now when you put a dog into the model you might get the following from the softmax function:

This means: dog because 0.5 is the highest value. And this is the reason why a threshold is not valid. You should apply something like  or . Something like this: &lt;denchmark-link:https://github.com/tensorflow/addons/issues/490#issuecomment-529882338&gt;#490 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='PhilipMay' date='2019-09-11T13:39:07Z'>
		&lt;denchmark-link:https://github.com/PhilipMay&gt;@PhilipMay&lt;/denchmark-link&gt;

I am taking binary accuracy as a reference here: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/2ff39d00faf8f7e433ddcae0aa278f6e573b0c55/tensorflow/python/keras/metrics.py#L630&gt;https://github.com/tensorflow/tensorflow/blob/2ff39d00faf8f7e433ddcae0aa278f6e573b0c55/tensorflow/python/keras/metrics.py#L630&lt;/denchmark-link&gt;

In order for this to work you need to provide the threshold value as 0.49 above. I can probably fix the threshold default to be 0.5. I think below 0.5, it may not be considered as a good prediction.
I have this tested with scikit learn as (that's the usual procedure I follow)
well.&lt;denchmark-link:https://colab.research.google.com/drive/1qSq0SsYkPqjdKUgM1RM4kKM67X75ocFj&gt;https://colab.research.google.com/drive/1qSq0SsYkPqjdKUgM1RM4kKM67X75ocFj&lt;/denchmark-link&gt;

Goal here is to make it compatible with both multi-class and multi-label
		</comment>
	</comments>
</bug>