<bug id='125' author='guillaumekln' open_date='2019-04-01T12:21:44Z' closed_time='2019-04-02T18:48:50Z'>
	<summary>Segmentation fault with tfa.seq2seq.gather_tree</summary>
	<description>
System information

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: binary
TensorFlow version: 2.0.0a0
TensorFlow Addons installed from: PyPi
TensorFlow Addons version: 0.2.0
Python version and type: 2.7.12 (stock)
Is GPU used? No

Describe the bug
The gather_tree function from the seq2seq module fails with a Segmentation fault while the same code using tf.contrib.seq2seq does not.
Describe the expected behavior
The function should run without failures.
Code to reproduce the issue
import tensorflow as tf
import tensorflow_addons as tfa

step_ids = tf.constant([[[1, 2, 3], [1, 3, 3]]], dtype=tf.int32)  # [batch, beam, time]
parent_ids = tf.constant([[[0, 0, 0], [0, 1, 1]]], dtype=tf.int32)  # [batch, beam, time]
maximum_lengths = tf.constant([3], dtype=tf.int32)  # [batch]

step_ids = tf.transpose(step_ids, perm=[2, 0, 1])
parent_ids = tf.transpose(parent_ids, perm=[2, 0, 1])

ids = tfa.seq2seq.beam_search_decoder.gather_tree(
    step_ids, parent_ids, maximum_lengths, 3)
	</description>
	<comments>
		<comment id='1' author='guillaumekln' date='2019-04-01T12:26:52Z'>
		&lt;denchmark-link:https://github.com/qlzh727&gt;@qlzh727&lt;/denchmark-link&gt;
 Mind taking a look when time allows? Possible this could be related to the packaging, but we'll see.
		</comment>
		<comment id='2' author='guillaumekln' date='2019-04-01T14:28:23Z'>
		Humm, the gather_tree will eventually use a c op, which might be the cause here. Is there any log before it throw the error?
&lt;denchmark-link:https://github.com/seanpmorgan&gt;@seanpmorgan&lt;/denchmark-link&gt;
, we do have a unit test for gather_tree in &lt;denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/beam_search_ops_test.py&gt;https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/beam_search_ops_test.py&lt;/denchmark-link&gt;
. I am wondering is there any similar issue raised from user about using c ops after the recent reorg of the ops?
		</comment>
		<comment id='3' author='guillaumekln' date='2019-04-01T17:02:50Z'>
		Not that I've seen... for example here is a call to the c ops in image
&lt;denchmark-link:https://colab.research.google.com/drive/1_RFgu_glO5bJ1PRcxsMtiBR708fjsW7a&gt;https://colab.research.google.com/drive/1_RFgu_glO5bJ1PRcxsMtiBR708fjsW7a&lt;/denchmark-link&gt;

When I tried to run the snippet from above though... I got a kernel restart so will need to run locally in order to see what the crash says. Hopefully will have time to look into this a bit in the next day or so
		</comment>
		<comment id='4' author='guillaumekln' date='2019-04-02T14:44:02Z'>
		So I'm unable to replicate this on my local PC, but do get a crash on colab with no error message to work with. Because this works on local PC and passes tests which run this I'm heavily leaning on this being related to &lt;denchmark-link:https://github.com/tensorflow/addons/issues/119&gt;#119&lt;/denchmark-link&gt;

An example of core dump due to improper packaging can be seen here
&lt;denchmark-link:https://github.com/apache/arrow/issues/1450&gt;apache/arrow#1450&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 were you able to get any meaningful failure message?
		</comment>
		<comment id='5' author='guillaumekln' date='2019-04-02T14:46:41Z'>
		On second thought not totally sold its related to the packaging, because the same sequence of imports on different computers should both crash (if I totally follow the packaging issue).
		</comment>
		<comment id='6' author='guillaumekln' date='2019-04-02T14:50:57Z'>
		
@guillaumekln were you able to get any meaningful failure message?

No, it was a plain Segmentation fault. Will try to dig deeper.
		</comment>
		<comment id='7' author='guillaumekln' date='2019-04-02T15:34:46Z'>
		Thanks, here is another (probably) related issue. I bumped to gcc5 so we could build a py37 whl but that was probably not worth it. I'll get feedback from the SIG BUILD meeting today and push a 0.2.1 soon
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/27067&gt;tensorflow/tensorflow#27067&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='guillaumekln' date='2019-04-02T16:42:08Z'>
		&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 This should be fixed in 0.2.1 can you please confirm?
Looks good on the &lt;denchmark-link:https://colab.research.google.com/drive/16oMlt88q1vM2TS1RCyd6ZiDcKGHnh4B4&gt;colab notebook&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='guillaumekln' date='2019-04-02T18:48:50Z'>
		Yes, looks like it is working now. Thanks!
		</comment>
	</comments>
</bug>