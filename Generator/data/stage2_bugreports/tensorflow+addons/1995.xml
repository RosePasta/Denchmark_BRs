<bug id='1995' author='MichaelJanz' open_date='2020-07-13T14:24:08Z' closed_time='2020-07-13T14:47:29Z'>
	<summary>Making Encoder-Decoder work with Attention -&amp;gt; OperatorNotAllowedInGraphError</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
TensorFlow version and how it was installed (source or binary): 2.2 binary (both nightly and stable tested)
TensorFlow-Addons version and how it was installed (source or binary): pip, binary
Python version: 3.7
Is GPU used? (yes/no): yes

Describe the bug
Hi, I have the following network architecture:
&lt;denchmark-code&gt;encoder_inputs = keras.layers.Input(batch_input_shape=[32,None], dtype=np.int32)
decoder_inputs = keras.layers.Input(batch_input_shape=[32,None], dtype=np.int32)
embeddings = keras.layers.Embedding(vocab_size, 20)
encoder_embeddings = embeddings(encoder_inputs)
decoder_embeddings = embeddings(decoder_inputs)

encoder_1 = keras.layers.LSTM(128, return_state=True)
encoder_outputs, state_h, state_c = encoder_1(encoder_embeddings)
encoder_state = [state_h, state_c]
encoder_attention = tfa.seq2seq.attention_wrapper.LuongAttention(128, encoder_state)

sampler = tfa.seq2seq.sampler.TrainingSampler()
decoder_cell = keras.layers.LSTMCell(128)
decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(decoder_cell, encoder_attention, 128)

output_layer = keras.layers.Dense(vocab_size)

decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)


BATCH_SIZE = tf.shape(encoder_embeddings)[0]
initial_state = decoder_cell.get_initial_state(dtype=tf.float32, batch_size=BATCH_SIZE)


attention_init_state = tfa.seq2seq.AttentionWrapperState(list(attention_init_state.cell_state),attention_init_state.attention,attention_init_state.alignments,
                                                          attention_init_state.alignment_history,attention_init_state.attention_state)

final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state=attention_init_state,
                                                             sequence_length=[sequence_length], training=True)
y_proba = tf.nn.softmax(final_outputs.rnn_output)

model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])
&lt;/denchmark-code&gt;

I tried to implement attention into the Encoder-Decoder model, but I keep getting errors about the initial_state argument at decoder(....).
As far as I understood, I need to extract the initial_state of the attentionWrapper and pass it to the decoder call as inital_state.
Unforunatly, the documentation did not state how to do that. So I searched this repo for solutions and came to these line:
&lt;denchmark-code&gt;BATCH_SIZE = tf.shape(encoder_embeddings)[0]
initial_state = decoder_cell.get_initial_state(dtype=tf.float32, batch_size=BATCH_SIZE)

attention_init_state = tfa.seq2seq.AttentionWrapperState(list(attention_init_state.cell_state),attention_init_state.attention,attention_init_state.alignments,
                                                          attention_init_state.alignment_history,attention_init_state.attention_state)
&lt;/denchmark-code&gt;

When I execute the code, I receive the following error:
OperatorNotAllowedInGraphError: using a tf.Tensoras a Pythonbool is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
at initial_state = decoder_cell.get_initial_state(dtype=tf.float32, batch_size=BATCH_SIZE).
I would appreciate any feedback to my approach and what I need to do, to make it work properly. Since I tried different things, I am not sure if this message is the result of a bug, or if I need to change something.
Thank you for your help!
Code to reproduce the issue
To reproduce, use the above
Other info / logs
Full traceback:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
OperatorNotAllowedInGraphError            Traceback (most recent call last)
&lt;ipython-input-21-82be7a6256fa&gt; in &lt;module&gt;
     20 
     21 BATCH_SIZE = tf.shape(encoder_embeddings)[0]
---&gt; 22 initial_state = decoder_cell.get_initial_state(dtype=tf.float32, batch_size=BATCH_SIZE)
     23 
     24 

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow_addons\seq2seq\attention_wrapper.py in get_initial_state(self, inputs, batch_size, dtype)
   1970             )
   1971             with tf.control_dependencies(
-&gt; 1972                 self._batch_size_checks(batch_size, error_message)
   1973             ):  # pylint: disable=bad-continuation
   1974                 cell_state = tf.nest.map_structure(

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow_addons\seq2seq\attention_wrapper.py in _batch_size_checks(self, batch_size, error_message)
   1850                 batch_size, attention_mechanism.batch_size, message=error_message
   1851             )
-&gt; 1852             for attention_mechanism in self._attention_mechanisms
   1853         ]
   1854 

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow_addons\seq2seq\attention_wrapper.py in &lt;listcomp&gt;(.0)
   1850                 batch_size, attention_mechanism.batch_size, message=error_message
   1851             )
-&gt; 1852             for attention_mechanism in self._attention_mechanisms
   1853         ]
   1854 

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow\python\ops\check_ops.py in assert_equal_v2(x, y, message, summarize, name)
    646       execution or if `x` and `y` are statically known.
    647   """
--&gt; 648   return assert_equal(x=x, y=y, summarize=summarize, message=message, name=name)
    649 
    650 

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow\python\ops\check_ops.py in assert_equal(x, y, data, summarize, message, name)
    657       return None if context.executing_eagerly() else control_flow_ops.no_op()
    658   return _binary_assert('==', 'assert_equal', math_ops.equal, np.equal, x, y,
--&gt; 659                         data, summarize, message, name)
    660 
    661 

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow\python\ops\check_ops.py in _binary_assert(sym, opname, op_func, static_func, x, y, data, summarize, message, name)
    332       test_op = op_func(x, y)
    333       condition = math_ops.reduce_all(test_op)
--&gt; 334       if condition:
    335         return
    336 

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow\python\framework\ops.py in __bool__(self)
    924       `TypeError`.
    925     """
--&gt; 926     self._disallow_bool_casting()
    927 
    928   def __nonzero__(self):

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow\python\framework\ops.py in _disallow_bool_casting(self)
    537     else:
    538       # Default: V1-style Graph execution.
--&gt; 539       self._disallow_in_graph_mode("using a `tf.Tensor` as a Python `bool`")
    540 
    541   def _disallow_iteration(self):

~\anaconda3\envs\tf_nightly_env\lib\site-packages\tensorflow\python\framework\ops.py in _disallow_in_graph_mode(self, task)
    526     raise errors.OperatorNotAllowedInGraphError(
    527         "{} is not allowed in Graph execution. Use Eager execution or decorate"
--&gt; 528         " this function with @tf.function.".format(task))
    529 
    530   def _disallow_bool_casting(self):

OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='MichaelJanz' date='2020-07-13T14:33:32Z'>
		/cc &lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='MichaelJanz' date='2020-07-13T14:47:26Z'>
		See &lt;denchmark-link:https://github.com/tensorflow/addons/issues/1898&gt;#1898&lt;/denchmark-link&gt;
. There are compatibility issues when using the Keras functional API and  in eager mode. Disabling eager mode is a possible workaround:
tf.compat.v1.disable_eager_execution()
Let me close this issue as a duplicate.

As far as I understood, I need to extract the initial_state of the attentionWrapper and pass it to the decoder call as inital_state.
Unforunatly, the documentation did not state how to do that. So I searched this repo for solutions and came to these line:

You should not need to do that. The state returned by the decoder already includes AttentionWrapperState.
An issue I saw with your code is the following:
encoder_attention = tfa.seq2seq.attention_wrapper.LuongAttention(128, encoder_state)
The second argument should be the attention memory, a tensor of shape [batch_size, time, depth].
		</comment>
		<comment id='3' author='MichaelJanz' date='2020-07-13T16:18:01Z'>
		&lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;

Thanks for your fast reply!
I followed the advices on the &lt;denchmark-link:https://github.com/tensorflow/addons/tree/master/tensorflow_addons/seq2seq&gt;docs&lt;/denchmark-link&gt;
, which state that the encoder_state is defined as:
&lt;denchmark-code&gt;encoder = tf.keras.layers.LSTM(num_units, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_emb_inp)
encoder_state = [state_h, state_c]
&lt;/denchmark-code&gt;

and used as:
&lt;denchmark-code&gt;attention_mechanism = tfa.seq2seq.LuongAttention(
    num_units,
    encoder_state,
    memory_sequence_length=encoder_sequence_length)
&lt;/denchmark-code&gt;

Can you give me more details, what I can do there?
Also I changed
&lt;denchmark-code&gt;final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings, initial_state=attention_init_state,
                                                             sequence_length=[sequence_length], training=True)
&lt;/denchmark-code&gt;

to
&lt;denchmark-code&gt;final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings,
                                                             sequence_length=[sequence_length], training=True)
&lt;/denchmark-code&gt;

Which gives me the error: AttributeError: 'NoneType' object has no attribute 'dtype'
I think I did not get the usage of the encoder_states yet.
Thanks for all your help, I am a beginner in this field, although it really needs alot of time to learn and any help is really appreciated.
		</comment>
		<comment id='4' author='MichaelJanz' date='2020-07-13T16:34:47Z'>
		
Can you give me more details, what I can do there?

The README is actually wrong. We will fix that.

Which gives me the error: AttributeError: 'NoneType' object has no attribute 'dtype'

Can you try passing initial_state instead of attention_init_state?
		</comment>
		<comment id='5' author='MichaelJanz' date='2020-07-13T19:14:30Z'>
		I tried, but I ran into another shape error.
I think I will wait until the README is updated, my skills are not sufficient enough to dive that deep into the topic. Is there already an issue for updating the README? Else I can create it
		</comment>
	</comments>
</bug>