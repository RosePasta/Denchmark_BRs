<bug id='1093' author='georgesterpu' open_date='2020-02-14T12:57:44Z' closed_time='2020-06-02T09:57:03Z'>
	<summary>Error when reading the batch size in AttentionWrapper</summary>
	<description>
System information

OS Platform and Distribution: linux
TensorFlow version: pypi tf-nightly 2.2.0.dev20200212
TensorFlow-Addons version: pypi tfa-nightly 0.9.0.dev20200212
Python version: 3.7.5

Describe the bug
Hi,
I am trying to write my own attention mechanism by subclassing _BaseAttentionMechanism, but I am running into errors when in graph mode.
More precisely, the build method of my mechanism needs to create a variable with a shape equal to the batch size.
Since the mechanism.setup_memory method is called in advance, there is already a member in the superclass named self.batch_size, but using it leads to the error message below.
In the build method I also have access to self.keys, but trying to calculate the batch size with tf.shape(self.keys)[0] results in a similar error.
I also tried writing a new attention mechanism class inheriting directly from AttentionMechanism and tf.keras.layers.Layer, mostly by duplicating the code in _BaseAttentionMechanism and adding all the variables in a single build method, but it results in the same error message.
So the underlying limitation that I am currently facing is inferring the dynamic batch size in graph mode, and I don't understand if this is a limitation of tensorflow, or just a bad design pattern in my code.
Code to reproduce the issue
import tensorflow as tf
import tensorflow_addons as tfa


class MyAttention(tfa.seq2seq.LuongAttention):
    def __init__(self):
        super().__init__(
            units=3,
            memory=None,
            memory_sequence_length=None,
            scale=False,
            probability_fn='softmax',
            dtype=tf.float32,
        )

    def build(self, input_shape):
        tf.print(input_shape)
        super().build(input_shape)
        bs = self.batch_size
        # bs = tf.shape(self.keys)[0]
        # bs = input_shape[0][0]
        self.myVariable = tf.Variable(initial_value=tf.ones((bs, )), trainable=False)

    def _calculate_attention(self, query, state):
        # self.myVariable.assign_add([1.336, 1.338])
        return super()._calculate_attention(query, state)

cell = tf.keras.layers.LSTMCell(3)
mechanism = MyAttention()
cell = tfa.seq2seq.AttentionWrapper(
    cell=cell,
    attention_mechanism=mechanism,)
decoder = tfa.seq2seq.BasicDecoder(cell=cell, sampler=tfa.seq2seq.TrainingSampler())

dataset = tf.data.Dataset.from_tensor_slices(tf.ones((100, 7, 3))).batch(2)
my_iterator = iter(dataset)

@tf.function
def decode(it):

    data = next(it)
    bs = tf.shape(data)[0]
    mechanism.setup_memory(data)
    initial_state = cell.get_initial_state(batch_size=bs, dtype=tf.float32)
    print(decoder(
        inputs=data,
        initial_state=initial_state))

decode(my_iterator)
Other info / logs
    TypeError: An op outside of the function building code is being passed
    a "Graph" tensor. It is possible to have Graph tensors
    leak out of the function building context by including a
    tf.init_scope in your function building code.
    For example, the following function will fail:
      @tf.function
      def has_init_scope():
        my_constant = tf.constant(1.)
        with tf.init_scope():
          added = my_constant * 2
    The graph tensor has name: LuongAttention/strided_slice:0
Thank you in advance for any help !
	</description>
	<comments>
		<comment id='1' author='georgesterpu' date='2020-02-17T13:31:11Z'>
		I think this is related to how the self.batch_size attribute is captured by the tf.function. A possible workaround is to create your variable in setup_memory, e.g.:
def setup_memory(self, memory, memory_sequence_length=None, memory_mask=None):
    batch_size = tf.shape(memory)[0]
    if self.myVariable is None:
        self.myVariable = tf.Variable(initial_value=tf.ones([batch_size]), trainable=False)
    return super().setup_memory(
        memory,
        memory_sequence_length=memory_sequence_length,
        memory_mask=memory_mask
    )
		</comment>
		<comment id='2' author='georgesterpu' date='2020-02-17T16:16:12Z'>
		Thanks, &lt;denchmark-link:https://github.com/guillaumekln&gt;@guillaumekln&lt;/denchmark-link&gt;
 !
Your workaround does solve the problem for the code above, but when integrated into my project, I receive more errors:
2020-02-17 15:57:07.769863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-02-17 15:57:08.248314: W tensorflow/core/framework/op_kernel.cc:1727] OP_REQUIRES failed at resource_variable_ops.cc:309 : Not found: Resource localhost/seq2_seq_model/giraffe/Variable_946/N10tensorflow3VarE does not exist.
Traceback (most recent call last):
  File "/run/media/john_tukey/work/phd/90.avsr20/main.py", line 81, in &lt;module&gt;
    app.run(main)
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/absl/app.py", line 299, in run
    _run_main(main, args)
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "/run/media/john_tukey/work/phd/90.avsr20/main.py", line 75, in main
    learning_rates=learning_rates,
  File "/run/media/john_tukey/work/phd/90.avsr20/avsr/experiment.py", line 34, in run_experiment
    try_restore_from_prev_run=True,
  File "/run/media/john_tukey/work/phd/90.avsr20/avsr/avsr.py", line 212, in train
    loss, is_done = self._train_step(iterator)
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 576, in __call__
    result = self._call(*args, **kwds)
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", line 704, in _call
    return function_lib.defun(fn_with_cond)(*canon_args, **canon_kwds)
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 2414, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1660, in _filtered_call
    self.captured_inputs)
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 1741, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", line 598, in call
    ctx=ctx)
  File "/home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.
  (0) Not found:  Resource localhost/seq2_seq_model/giraffe/Variable_946/N10tensorflow3VarE does not exist.
	 [[{{node cond/else/_1/StatefulPartitionedCall/cond_1/else/_492/VariableShape_3}}]]
  (1) Not found:  Resource localhost/seq2_seq_model/giraffe/Variable_946/N10tensorflow3VarE does not exist.
	 [[{{node cond/else/_1/StatefulPartitionedCall/cond_1/else/_492/VariableShape_3}}]]
	 [[cond/else/_1/StatefulPartitionedCall/cond_1/else/_492/seq2_seq_model/giraffe/assert_equal_3/Assert/Assert/data_3/_810]]
0 successful operations.
0 derived errors ignored. [Op:__inference_fn_with_cond_11408]

Function call stack:
fn_with_cond -&gt; fn_with_cond
Plus this warning right before:
W0217 15:56:53.974507 140330407388992 deprecation.py:506] From /home/john_tukey/miniconda3/envs/tfn/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1809: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
I will try to come up with a new code snippet to reproduce the error. In the meantime, could you please take a guess at what might be wrong ? Thank you.
		</comment>
		<comment id='3' author='georgesterpu' date='2020-06-02T09:57:02Z'>
		Let me close this old issue. Feel free to reopen if you can share a small code snippet that reproduce the other error.
		</comment>
	</comments>
</bug>