<bug id='1968' author='avnx' open_date='2020-07-03T18:10:08Z' closed_time='2020-07-11T05:25:47Z'>
	<summary>Error with mixed precision</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
TensorFlow version and how it was installed (source or binary):  https://github.com/horovod/horovod/blob/master/Dockerfile.gpu (changed TENSORFLOW_VERSION=2.2.0 and NCCL_VERSION=2.5.6-1+cuda10.1)
TensorFlow-Addons version and how it was installed (source or binary): nightly (0.11.0-dev)
Python version: 3.6.9
Is GPU used? (yes/no): yes

Code doesn't work when applying mixed precision.
Am I doing something wrong?
Code to reproduce the issue
&lt;denchmark-code&gt;import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.mixed_precision import experimental as mixed_precision

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)

embedding_layer = tf.keras.layers.Embedding(37,
                                            37,
                                            name='embedding')

rnn_cell = tf.keras.layers.LSTMCell(10)
attention_mechanism = tfa.seq2seq.BahdanauAttention(units=10, name='BahdanauAtt')
attention_wrapper = tfa.seq2seq.AttentionWrapper(rnn_cell,
                                                 attention_mechanism,
                                                 attention_layer_size=10,
                                                 name='Att_wrapper')
mlp = tf.keras.layers.Dense(14, name='output_mlp')
decoder_cell = tfa.seq2seq.BasicDecoder(attention_wrapper,
                                        output_layer=mlp,
                                        sampler=tfa.seq2seq.ScheduledEmbeddingTrainingSampler(0.1,
                                                                                              embedding_layer),
                                        name='train_decoder')

@tf.function
def step(inputs, encoder_outputs):
    
    attention_mechanism.setup_memory(encoder_outputs)
    decoder_initial_state = attention_wrapper.get_initial_state(batch_size=32,
                                                                dtype=encoder_outputs.dtype)

    embeddings = embedding_layer(inputs)

    outputs = decoder_cell(embeddings,
                       initial_state=decoder_initial_state,
                       training=True)
    return outputs

encoder_outputs = tf.random.uniform([32, 17, 256], dtype=tf.float16)
inputs = tf.random.uniform([32, 27], maxval=37, dtype=tf.int32)
step(inputs, encoder_outputs)


&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py:163 call  *
        self,
    /usr/local/lib/python3.6/dist-packages/typeguard/__init__.py:262 wrapper  *
        retval = func(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py:397 body  *
        (next_outputs, decoder_state, next_inputs, decoder_finished) = decoder.step(
    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/basic_decoder.py:134 step  *
        cell_outputs, cell_state = self.cell(inputs, state, training=training)
    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/attention_wrapper.py:2052 call  *
        attention, alignments, next_attention_state = self._attention_fn(
    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/attention_wrapper.py:1595 _compute_attention  *
        context_ = tf.matmul(expanded_alignments, attention_mechanism.values)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper  **
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:2946 matmul
        a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:1526 batch_mat_mul_v2
        "BatchMatMulV2", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:506 _apply_op_helper
        inferred_from[input_arg.type_attr]))

    TypeError: Input 'y' of 'BatchMatMulV2' Op has type float16 that does not match type float32 of argument 'x'.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='avnx' date='2020-07-08T17:07:23Z'>
		The first issue is that we are forcing  at &lt;denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper.py#L559-L560&gt;https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper.py#L559-L560&lt;/denchmark-link&gt;
.
Then we have a similar issue at &lt;denchmark-link:https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/sampler.py#L394-L400&gt;https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/sampler.py#L394-L400&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='avnx' date='2020-07-08T17:48:51Z'>
		Yes, we should fix this. The seq2seq layers were designed before the new dtype policy.
While working on a fix, I find it a bit unexpected that tf.keras.layers.Embedding does not return float16 here.
		</comment>
	</comments>
</bug>