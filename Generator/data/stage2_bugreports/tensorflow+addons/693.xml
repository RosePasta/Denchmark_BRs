<bug id='693' author='danijar' open_date='2019-11-12T09:50:49Z' closed_time='2019-12-26T18:10:20Z'>
	<summary>Lost gradient after clipping in focal loss?</summary>
	<description>



addons/tensorflow_addons/losses/focal_loss.py


         Line 132
      in
      776b751






 y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon()) 





	</description>
	<comments>
		<comment id='1' author='danijar' date='2019-11-12T17:06:39Z'>
		cc &lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
 code owner
		</comment>
		<comment id='2' author='danijar' date='2019-11-12T17:19:08Z'>
		&lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
 any sample code to reproduce?
If it is the case though, then I suspect that epsilon added to y_pred in this line
p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))
should solve the issue. Need to test it though.
		</comment>
		<comment id='3' author='danijar' date='2019-11-14T05:15:22Z'>
		I just read through the code and wanted to highlight a potential problem. Looking at K.clip, it doesn't seem to provide a gradient for values that have been clipped. In this is indeed the case, a fix would be
clipped = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())
y_pred = y_pred - tf.stop_gradient(y_pred) + tf.stop_gradient(clipped)
		</comment>
		<comment id='4' author='danijar' date='2019-11-14T06:51:32Z'>
		Thank you &lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
 for looking into this and providing a fix. I will look into it once again and we will make the necessary code changes. Thank you again
&lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/facaiy&gt;@facaiy&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/seanpmorgan&gt;@seanpmorgan&lt;/denchmark-link&gt;
 sounds good?
		</comment>
		<comment id='5' author='danijar' date='2019-11-20T19:14:09Z'>
		&lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
 Sorry for the late response. I'm fine with this. Maybe provide a testcase to verify the patch is needed!
		</comment>
		<comment id='6' author='danijar' date='2019-12-10T19:10:26Z'>
		Sorry for the delay. I was on vacation. I will check and if needed, fix this during this weekend
		</comment>
		<comment id='7' author='danijar' date='2019-12-12T10:49:23Z'>
		K.clip is just clip by value. It doesn't seem to affect the gradients, hence no fix is needed IMO. Though looking for any additional inputs
		</comment>
		<comment id='8' author='danijar' date='2019-12-12T18:12:29Z'>
		What do you mean? Clip by value is not differentiable.
		</comment>
		<comment id='9' author='danijar' date='2019-12-12T20:26:01Z'>
		The same thing is being used here in  calculations.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4558-L4588&gt;https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/backend.py#L4558-L4588&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='danijar' date='2019-12-12T21:38:59Z'>
		&lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
 I understand the confusion.
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/f7f4049ded8ad2bfbaecde1ff2d42460cc9fa762/tensorflow/python/ops/clip_ops.py#L38&gt;tf.clip_by_value&lt;/denchmark-link&gt;
 uses  and  currently (ignoring the TODO).
Both  and  have their gradients defined &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/606a542c81df033d493d25e32b9f175b947fcbbb/tensorflow/core/ops/math_grad.cc#L617&gt;here&lt;/denchmark-link&gt;
. So yes, as &lt;denchmark-link:https://github.com/AakashKumarNain&gt;@AakashKumarNain&lt;/denchmark-link&gt;
 mentioned, using  shouldn't be a problem for differentiation.
As for the gradient, it'll be 0 outside the range of min_value and max_value.
		</comment>
		<comment id='11' author='danijar' date='2019-12-15T18:40:34Z'>
		Exactly. Is it supposed to be 0 outside of the range?
		</comment>
		<comment id='12' author='danijar' date='2019-12-15T20:24:29Z'>
		&lt;denchmark-link:https://github.com/danijar&gt;@danijar&lt;/denchmark-link&gt;
 My bad, you're right. A clip is used in BCE before taking a  for numerical stability. That won't be needed for focal loss.
Funnily enough in the tf.keras implementation of  BCE, that does both a clip followed by an output + eps. I think the addition of eps after the clip won't be needed.
		</comment>
		<comment id='13' author='danijar' date='2019-12-16T04:12:18Z'>
		My main point was that the gradient should probably be patched back onto the clipped tensor. Anyway, I'm not a user of the module.
		</comment>
		<comment id='14' author='danijar' date='2019-12-17T16:20:18Z'>
		We will patch it.
		</comment>
	</comments>
</bug>