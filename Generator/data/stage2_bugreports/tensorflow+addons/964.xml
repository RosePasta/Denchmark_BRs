<bug id='964' author='RomainBrault' open_date='2020-01-28T15:22:56Z' closed_time='2020-02-02T18:49:14Z'>
	<summary>TimeStopping Callback fail in verbose mode</summary>
	<description>
System information
OS Platform and Distribution (Ubuntu 19.04):
TensorFlow version and how it was installed (pip==2.1.0):
TensorFlow-Addons version and how it was installed (pip==0.7.0):
Python version: 3.7
Is GPU used? (yes):
Describe the bug
When the fit method stop before the timer, it raises with
AttributeError: 'TimeStopping' object has no attribute 'stopped_epoch'
Code to reproduce the issue
Use tqdm callback
Other info / logs
Proposed fix
...

    def on_epoch_end(self, epoch, logs={}):
        self.stopped_epoch = epoch
        if time.time() &gt;= self.stopping_time:
            self.model.stop_training = True

...

    def on_train_end(self, logs=None):
        if self.verbose &gt; 0:
            formatted_time = datetime.timedelta(seconds=self.seconds)
            msg = "Stopping at epoch {} after training for {}".format(
                self.stopped_epoch + 1, formatted_time
            )
            print(msg)
...
	</description>
	<comments>
		<comment id='1' author='RomainBrault' date='2020-01-28T20:40:41Z'>
		Hi &lt;denchmark-link:https://github.com/RomainBrault&gt;@RomainBrault&lt;/denchmark-link&gt;
 thank you so much for catching the bug! I apologize I forgot to consider the obvious case where training stopped before proposed time, your proposed fix looks good to me (putting ) outside of  statement in  method. If you have time, may you submit a simple PR for it? (If not I can do it) Thank you so much for your contribution!
		</comment>
		<comment id='2' author='RomainBrault' date='2020-01-29T17:02:02Z'>
		I would recommend some kind of flag to distinguish whether  was called due to the "timeout" or by any other reason.  &lt;denchmark-link:https://github.com/RomainBrault&gt;@RomainBrault&lt;/denchmark-link&gt;
, it will fix your "cosmetic problem", too. If no one doesn't mind, I can get down to this.
		</comment>
		<comment id='3' author='RomainBrault' date='2020-01-29T17:36:44Z'>
		
I would recommend some kind of flag to distinguish whether on_train_end was called due to the "timeout" or by any other reason. @RomainBrault, it will fix your "cosmetic problem", too. If no one doesn't mind, I can get down to this.

Hi &lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
, great idea and I think it would be helpful! I think one thing to consider is to see what  Callback is doing in similar case (training finished regularly without  effect took place and  was set to ), so when we handle that case here it's similar. &lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
 thanks for your contribution in advance!
		</comment>
		<comment id='4' author='RomainBrault' date='2020-01-29T19:15:32Z'>
		Here it is. &lt;denchmark-link:https://github.com/failure-to-thrive/addons/commit/bc99311646a92a1099021ea767c489e033fa61ed#diff-dbab0e9de0d8f25e76e13d3906e105a5&gt;failure-to-thrive@bc99311#diff-dbab0e9de0d8f25e76e13d3906e105a5&lt;/denchmark-link&gt;

If it is OK I will PR.
 utilizes the same technique.
		</comment>
		<comment id='5' author='RomainBrault' date='2020-01-29T21:10:26Z'>
		Looks good to me but just a small NIT, instead of setting self.stop_epochs to None set it to 0 and check if it's above 0 in at_train_end, so the implementation is more consistent w/ EarlyStopping, also may you also added on_train_begin and reset self.stop_epochs so it's re-usable? Thanks for your contribution!
		</comment>
		<comment id='6' author='RomainBrault' date='2020-01-30T05:54:33Z'>
		
Looks good to me but just a small NIT, instead of setting self.stop_epochs to None set it to 0 and check if it's above 0 in at_train_end, so the implementation is more consistent w/ EarlyStopping

Sorry, we may not do it. What if the first epoch (epoch == 0) turns out to be too long? In that case verbose output in the at_train_end will be missed. Possible the same issue in the EarlyStopping, by the way.

also may you also added on_train_begin and reset self.stop_epochs so it's re-usable?

Sure. I just was trying to make it consistent with EarlyStopping.
		</comment>
		<comment id='7' author='RomainBrault' date='2020-01-30T21:57:54Z'>
		&lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
 , you got a great point, in the current implementation, we always let an epoch finish, even if it goes over time (we check at the end of each epochs  instead of each batch, thus if the first epoch turns out to be too long we will still finish it and then stop). I think it's better this way than stopping in the middle of epoch. Therefore we can assume that the first epoch will finish (this isn't a problem for  because they are comparing metrics and some of them are aggregated at the end of epochs). What you think?
		</comment>
		<comment id='8' author='RomainBrault' date='2020-01-31T06:07:13Z'>
		
I think it's better this way than stopping in the middle of epoch.

Me, too. I suspect a model will be left in the some kind of inconsistent state.

What you think?

My offer still stands. The only think that I would change is the allowing the object (TimeStopping) to be reused. However, it might come into the code inconsistency with other Callbacks. Moreover, what if Callbacks are not supposed to be reused in the first place? I'm not a project maintainer, I may not make such decisions. It's up to you.
		</comment>
		<comment id='9' author='RomainBrault' date='2020-01-31T14:46:53Z'>
		&lt;denchmark-link:https://github.com/tensorflow/addons/pull/967&gt;#967&lt;/denchmark-link&gt;
 has merged and fixed the bug I believe. Do we want to keep this open for &lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
 s implementation or close and start a new one?
		</comment>
		<comment id='10' author='RomainBrault' date='2020-01-31T16:41:35Z'>
		
#967 has merged and fixed the bug I believe. Do we want to keep this open for @failure-to-thrive s implementation or close and start a new one?

&lt;denchmark-link:https://github.com/tensorflow/addons/pull/967&gt;#967&lt;/denchmark-link&gt;
 is a quick workaround to avoid crashes. It does its job very well but now we should move forward to a comprehensive solution, because for example &lt;denchmark-link:https://github.com/tensorflow/addons/pull/967#issuecomment-579628853&gt;#967 (comment)&lt;/denchmark-link&gt;
. It's still &lt;denchmark-link:https://github.com/tensorflow/addons/issues/964&gt;#964&lt;/denchmark-link&gt;
, I think we should stay here.
		</comment>
		<comment id='11' author='RomainBrault' date='2020-01-31T16:52:59Z'>
		&lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/seanpmorgan&gt;@seanpmorgan&lt;/denchmark-link&gt;
 ya I think we should keep this one open for &lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
's PR. &lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
 let me know if you have any more questions/confusions :)
		</comment>
		<comment id='12' author='RomainBrault' date='2020-01-31T17:54:23Z'>
		
@failure-to-thrive let me know if you have any more questions/confusions :)

They are here &lt;denchmark-link:https://github.com/tensorflow/addons/issues/964#issuecomment-580596414&gt;#964 (comment)&lt;/denchmark-link&gt;
.
But... Honestly, it's a not big deal that justify putting so much of our time and efforts into it. Let us just &lt;denchmark-link:https://github.com/tensorflow/addons/issues/964#issuecomment-579914295&gt;#964 (comment)&lt;/denchmark-link&gt;
. We can always get back to this issue anytime in the future.
		</comment>
		<comment id='13' author='RomainBrault' date='2020-01-31T19:08:20Z'>
		&lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
 sounds good, thanks!
		</comment>
		<comment id='14' author='RomainBrault' date='2020-02-02T18:49:14Z'>
		Closing this issue as &lt;denchmark-link:https://github.com/failure-to-thrive&gt;@failure-to-thrive&lt;/denchmark-link&gt;
's PR has been merged, thanks!
		</comment>
	</comments>
</bug>