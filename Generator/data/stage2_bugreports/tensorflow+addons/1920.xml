<bug id='1920' author='shkarupa-alex' open_date='2020-06-09T12:29:36Z' closed_time='2020-06-16T23:03:51Z'>
	<summary>LookAhead + RAdam + mixed_fp16 = apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
TensorFlow version and how it was installed (source or binary): source
TensorFlow-Addons version and how it was installed (source or binary): source
Python version: 3.8.2
Is GPU used? (yes/no): yes

Describe the bug
When i set global mixed precision policy tf.keras.mixed_precision.experimental.set_policy('mixed_float16') meta-optimizer LookAhead + RAdam raises error
Epoch 1/5
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-19-841302a83165&gt; in &lt;module&gt;
----&gt; 1 model.fit(
      2     train_ds,
      3     epochs=1 if lr_finder else 5,
      4     callbacks=callbacks,
      5     steps_per_epoch=findlr_steps if lr_finder else None,

~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    846                 batch_size=batch_size):
    847               callbacks.on_train_batch_begin(step)
--&gt; 848               tmp_logs = train_function(iterator)
    849               # Catch OutOfRangeError for Datasets of unknown size.
    850               # This blocks until the batch has finished executing.

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--&gt; 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    625       # This is the first call of __call__, so we have to initialize.
    626       initializers = []
--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)
    628     finally:
    629       # At this point we know that the initialization is complete (or less

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    503     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    504     self._concrete_stateful_fn = (
--&gt; 505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    506             *args, **kwds))
    507 

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2444       args, kwargs = None, None
   2445     with self._lock:
-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2447     return graph_function
   2448 

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)
-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)
   2778       self._function_cache.primary[cache_key] = graph_function
   2779       return graph_function, args, kwargs

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2655     arg_names = base_arg_names + missing_arg_names
   2656     graph_function = ConcreteFunction(
-&gt; 2657         func_graph_module.func_graph_from_py_func(
   2658             self._name,
   2659             self._python_function,

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, "ag_error_metadata"):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

TypeError: in user code:

    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:540 train_step  **
        _minimize(self.distribute_strategy, tape, self.optimizer, loss,
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1810 _minimize
        optimizer.apply_gradients(
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:245 apply_gradients
        return distribution_strategy_context.get_replica_context().merge_call(
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2427 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:269 _apply_gradients_cross_replica  **
        maybe_apply_op = smart_cond.smart_cond(should_apply_grads,
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py:58 smart_cond
        return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:507 new_func
        return func(*args, **kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:1177 cond
        return cond_v2.cond_v2(pred, true_fn, false_fn, name)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/ops/cond_v2.py:78 cond_v2
        true_graph = func_graph_module.func_graph_from_py_func(
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:981 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:261 apply_fn
        return distribution.extended.call_for_each_replica(
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/alex/.local/lib/python3.8/site-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:279 _apply_gradients
        return self._optimizer.apply_gradients(

    TypeError: apply_gradients() got an unexpected keyword argument 'experimental_aggregate_gradients'
Code to reproduce the issue
tf.keras.mixed_precision.experimental.set_policy('mixed_float16')
# model = ...

optimizer = Lookahead(RectifiedAdam(0.003))
model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',
    metrics=['accuracy'],
    run_eagerly=False,
)

# model.fit(...)
Other info / logs
Addons built from master branch today
	</description>
	<comments>
		<comment id='1' author='shkarupa-alex' date='2020-06-10T20:40:56Z'>
		using ranger on tpu in kaggle hangs up (timeout after about 20 minutes)
		</comment>
		<comment id='2' author='shkarupa-alex' date='2020-06-11T11:27:25Z'>
		&lt;denchmark-link:https://github.com/shkarupa-alex&gt;@shkarupa-alex&lt;/denchmark-link&gt;
 Do you have a very minimal dummy  to reproduce this?
		</comment>
		<comment id='3' author='shkarupa-alex' date='2020-06-12T09:38:11Z'>
		
@shkarupa-alex Do you have a very minimal dummy model to reproduce this?

Here it is &lt;denchmark-link:https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing&gt;https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='shkarupa-alex' date='2020-06-12T09:53:53Z'>
		Can you enable the access to the colab?
		</comment>
		<comment id='5' author='shkarupa-alex' date='2020-06-12T10:03:43Z'>
		Sorry. Access granted
&lt;denchmark-link:https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing&gt;https://colab.research.google.com/drive/1nxYsCOHbNWLtNzMnkBnwbfiKjF5pAbe3?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='shkarupa-alex' date='2020-06-12T10:56:10Z'>
		&lt;denchmark-link:https://github.com/shkarupa-alex&gt;@shkarupa-alex&lt;/denchmark-link&gt;
 Can you try &lt;denchmark-link:https://github.com/tensorflow/addons/pull/1924&gt;#1924&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='7' author='shkarupa-alex' date='2020-06-12T11:53:52Z'>
		No error after patch applied
		</comment>
	</comments>
</bug>