<bug id='1870' author='avnx' open_date='2020-05-21T19:11:52Z' closed_time='2020-05-24T05:31:26Z'>
	<summary>Error in types description of tfa.seq2seq.AttentionWrapper</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LT
TensorFlow version and how it was installed (source or binary): 2.2.0
TensorFlow-Addons version and how it was installed (source or binary): 0.10.0
Python version: 3.6.9
Is GPU used? (yes/no): yes

As I noticed you added type description to classes.
But I found that at least in my case there is a conflict of types in description with real mission of arguments.
According to docs attention_mechanism and attention_layer_size in tfa.seq2seq.AttentionWrapper could be lists (for MultiHead attention case) or single instances. But in types you specified that attention_mechanism and attention_layer_size must be tf.keras.layers.Layer and  Optional[FloatTensorLike] respectively.
&lt;denchmark-link:https://github.com/tensorflow/addons/blob/v0.9.1/tensorflow_addons/seq2seq/attention_wrapper.py#L1621&gt;https://github.com/tensorflow/addons/blob/v0.9.1/tensorflow_addons/seq2seq/attention_wrapper.py#L1621&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='avnx' date='2020-05-22T07:58:36Z'>
		Thanks for reporting. The description is correct but not the types annotation. The PR referenced above should fix this.
		</comment>
	</comments>
</bug>