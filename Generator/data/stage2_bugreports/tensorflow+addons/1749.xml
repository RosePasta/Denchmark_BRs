<bug id='1749' author='foxik' open_date='2020-04-27T23:01:30Z' closed_time='2020-05-05T18:48:58Z'>
	<summary>Incorrect type of maximum_iterations argument in tfa.seq2seq.decoder causes runtime error.</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Stable
TensorFlow version and how it was installed (source or binary): 2.1 binary
TensorFlow-Addons version and how it was installed (source or binary): 0.8.1 binary
Python version: 3.7
Is GPU used? (yes/no): no

Describe the bug
Calling tfa.seq2seq.decoder.BaseDecoder with a tf.Tensor for maximum_iterations, which worked before, now results in
&lt;denchmark-code&gt;TypeError: type of argument "maximum_iterations" must be one of (float, int, float16, float32, int8, int16, int32, int64, uint8, uint16, uint32, uint64, NoneType); got tensorflow.python.framework.ops.Tensor instead
&lt;/denchmark-code&gt;

Other info / logs
The problem is caused by a faulty type annotation
  maximum_iterations: Optional[Number] = None,
Note that this annotation is also used in tfa.seq2seq.decoder.dynamic_decode.
	</description>
	<comments>
		<comment id='1' author='foxik' date='2020-04-27T23:14:15Z'>
		Note that while tfa.seq2seq.decoder.BaseDecoder.__init__ is marked with @typechecked, tfa.seq2seq.decoder.dynamic_decode is not.
		</comment>
		<comment id='2' author='foxik' date='2020-04-28T10:00:27Z'>
		Thanks for reporting! The PR above should fix this one.
		</comment>
	</comments>
</bug>