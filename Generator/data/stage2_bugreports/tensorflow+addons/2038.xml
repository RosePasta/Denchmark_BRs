<bug id='2038' author='rogercw' open_date='2020-07-25T01:45:22Z' closed_time='2020-12-31T22:47:06Z'>
	<summary>Training error with model included dense_image_warp</summary>
	<description>
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3
TensorFlow version and how it was installed (source or binary): 2.0.0 / 2.2.0
TensorFlow-Addons version and how it was installed (source or binary): 0.6.0 / 0.10.0, pip
Python version: 3.6.9
Is GPU used? (yes/no): yes

Describe the bug
When the flow input to the dense_image_warp is the Keras layer output, the model cannot be trained (eager execution is disabled). May I know whether it is the limitation of dense_image_warp? Is its gradient for back-propagation defined? Thanks for your time and help.
Code to reproduce the issue
Here is how I defined my model:
&lt;denchmark-code&gt;inputs = tf.keras.layers.Input(shape=[30, 40, 3])
flows = tf.keras.layers.Conv2D(filters=2, kernel_size=(1, 1))
outputs = tf.keras.layers.Lambda(lambda a: tfa.image.dense_image_warp(a[0], a[1]))(inputs, flows)
net = tf.keras.Model(inputs, outputs)
&lt;/denchmark-code&gt;

Other info / logs
&lt;denchmark-code&gt;tensorflow.python.framework.errors_impl.InvalidArgumentError: None 'training/SGD/gradients/gradients/lambda/StatefulParitionedCall_grad/PartitionedCall': Connecting to invalid output 1 of source node lambda/StatefulPartitionedCell which has 1 outputs.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rogercw' date='2020-07-25T04:16:36Z'>
		Hi &lt;denchmark-link:https://github.com/rogercw&gt;@rogercw&lt;/denchmark-link&gt;
, thanks for reporting. Could you share the runnable minimal code snippet?  The code you provided seems incomplete.
Anyway, I think it is differentiable.
&lt;denchmark-link:https://colab.research.google.com/drive/11xtneQH4DVK0ZwCIGmErRSYviyFpXybL?usp=sharing&gt;https://colab.research.google.com/drive/11xtneQH4DVK0ZwCIGmErRSYviyFpXybL?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='rogercw' date='2020-07-30T09:31:55Z'>
		Hi &lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
, thanks for your quick response. Please find the example code below.
&lt;denchmark-code&gt;import tensorflow as tf
import os, glob
import cv2
import tensorflow_addons as tfa

class DataGenerator2:
    def __init__(self, image_size, image_dir):
        self.image_size=image_size
        self.image_dir=image_dir
        self.image_list = glob.glob(os.path.join(image_dir, '*.jpg'))
        self.dataset_size = len(self.image_list)
        self.iter_number = 0

    def __iter__(self):
        return self

    def get_tensor_shape(self):
        data_shape_in = (self.image_size[0], self.image_size[1], 3)
        data_shape_out = (self.image_size[0], self.image_size[1], 3)
        return data_shape_in, data_shape_out

    def __next__(self):
        if self.iter_number &gt;= self.dataset_size:
            self.iter_number = 0

        img_in = cv2.imread(self.image_list[self.iter_number])
        img_in = cv2.resize(img_in, (self.image_size[1], self.image_size[0]))
        img_out = cv2.imread(self.image_list[self.dataset_size - self.iter_number - 1])
        img_out = cv2.resize(img_out, (self.image_size[1], self.image_size[0]))
        self.iter_number += 1

        return img_in, img_out

if __name__ == '__main__':
    tf.compat.v1.disable_eager_execution()
    input_size = (60, 80)
    batch_size = 64
    prefetch_buffer = 4
    checkpoint_path = '/path/to/output/dir/'

    dataset_gen = DataGenerator2(image_size=input_size, image_dir='/path/to/dataset/')
    data_shape_in, data_shape_out = dataset_gen.get_tensor_shape()
    dataset = tf.data.Dataset.from_generator(lambda: dataset_gen,
                                            (tf.float32, tf.float32),
                                            (tf.TensorShape(data_shape_in),
                                            tf.TensorShape(data_shape_out)))
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(prefetch_buffer)

    inputs = tf.keras.layers.Input(shape=[*input_size, 3])
    flows = tf.keras.layers.Conv2D(filters=2, kernel_size=(1, 1))(inputs)
    outputs = tf.keras.layers.Lambda(lambda a: tfa.image.dense_image_warp(a[0], a[1]))([inputs, flows])
    model = tf.keras.Model(inputs, outputs)

    optimizer = tf.keras.optimizers.SGD()
    callback_list = [tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_path, "model.{epoch:03d}.ckpt"), save_weights_only=True)]

    model.compile(optimizer=optimizer,
                  loss=tf.keras.losses.mse)

    model.fit(dataset,
              epochs=10,
              steps_per_epoch=dataset_gen.dataset_size // batch_size,
              callbacks=callback_list)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='rogercw' date='2020-07-31T01:30:53Z'>
		I suspect it's the bug of tf.keras (with symbolic tensor). If I use custom model with tf.GradientTape, it works well (at least no errors).
&lt;denchmark-link:https://colab.research.google.com/drive/1oWUZLzt2pTRVV0awUf7WfDnrx1z9GaH8?usp=sharing&gt;https://colab.research.google.com/drive/1oWUZLzt2pTRVV0awUf7WfDnrx1z9GaH8?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='rogercw' date='2020-08-01T01:16:35Z'>
		Thanks &lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
 for helping debugging the issue. Inspired by your implementation, I notice that the error is gone when I run in eager execution mode. Also, I cannot run your code when the eager execution is disabled as well. Does that just mean  does not support non-eager execution? Or, do you still think it is  issue, and I should report it to TensorFlow's repo? Thanks.
		</comment>
		<comment id='5' author='rogercw' date='2020-08-01T04:01:41Z'>
		
Thanks @WindQAQ for helping debugging the issue. Inspired by your implementation, I notice that the error is gone when I run in eager execution mode. Also, I cannot run your code when the eager execution is disabled as well. Does that just mean dense_image_warp does not support non-eager execution? Or, do you still think it is tf.keras issue, and I should report it to TensorFlow's repo? Thanks.

I need to further investigate the issue, but any reason that you want to disable eager mode?
		</comment>
		<comment id='6' author='rogercw' date='2020-08-03T20:51:35Z'>
		Hi &lt;denchmark-link:https://github.com/WindQAQ&gt;@WindQAQ&lt;/denchmark-link&gt;
, the original codebase was migrated from TF1, and I tried to add additional functionalities using TFA. If it will take too much effort to investigate the issue, I could try to revise the codebase and enable eager execution. Thanks again for your time and help.
		</comment>
		<comment id='7' author='rogercw' date='2020-08-03T20:54:39Z'>
		
Hi @WindQAQ, the original codebase was migrated from TF1, and I tried to add additional functionalities using TFA. If it will take too much effort to investigate the issue, I could try to revise the codebase and enable eager execution. Thanks again for your time and help.

No worries. As I expect our codes work on both graph and eager modes, I'll mark this issue as bug. BTW, in most of cases, enable eager mode and decorate with tf.function won't cause performance degradation, and it's much easier to use. So it would be good that you can try it!
		</comment>
		<comment id='8' author='rogercw' date='2020-12-31T22:47:06Z'>
		Closes because it's not an issue in addons I believe and there is a workaround when not to diable eager execution.
		</comment>
	</comments>
</bug>