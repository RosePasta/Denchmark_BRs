<bug id='306' author='kl2792' open_date='2018-08-11T21:53:16Z' closed_time='2018-08-13T12:08:04Z'>
	<summary>Accuracy computation for seq2seq model</summary>
	<description>
&lt;denchmark-code&gt;0/10(t): 100%|██████████| 1000/1000 [02:30&lt;00:00,  6.75it/s, running_loss=0.0338, running_acc=0.326, loss=0.689, loss_std=1.17, acc=35.9, acc_std=0]
0/10(v): 100%|██████████| 20/20 [00:01&lt;00:00, 19.90it/s, val_loss=0.0341, val_loss_std=0.0122, val_acc=42, val_acc_std=0]
1/10(t): 100%|██████████| 1000/1000 [02:30&lt;00:00,  6.76it/s, running_loss=0.00997, running_acc=0.327, loss=0.019, loss_std=0.0166, acc=41.8, acc_std=0]
1/10(v): 100%|██████████| 20/20 [00:01&lt;00:00, 19.98it/s, val_loss=0.0126, val_loss_std=0.00798, val_acc=42.1, val_acc_std=0]
2/10(t): 100%|██████████| 1000/1000 [02:30&lt;00:00,  6.75it/s, running_loss=0.00493, running_acc=0.328, loss=0.00837, loss_std=0.00938, acc=41.8, acc_std=0]
2/10(v): 100%|██████████| 20/20 [00:01&lt;00:00, 19.89it/s, val_loss=0.00783, val_loss_std=0.00716, val_acc=42.2, val_acc_std=0]
3/10(t):  45%|████▌     | 454/1000 [01:08&lt;01:21,  6.73it/s, running_loss=0.00458, running_acc=0.316]
&lt;/denchmark-code&gt;

Are the accuracies correct? (running_acc=.326, acc=35.9?)
I may be misunderstanding something, but shouldn't running_acc and acc be the same at the end of each epoch?
	</description>
	<comments>
		<comment id='1' author='kl2792' date='2018-08-11T22:04:09Z'>
		That looks ok! Running_acc is a 50 batch average (by default) and so will be slightly different to acc which is an average over the whole epoch. Hope that helps :)
		</comment>
		<comment id='2' author='kl2792' date='2018-08-11T22:05:48Z'>
		That helps a lot!
P. S. Thanks for creating such a great library!
		</comment>
		<comment id='3' author='kl2792' date='2018-08-11T22:09:21Z'>
		Sorry, I actually have another question on the same subject -- do you know why acc_std=0 for both train &amp; validation?
		</comment>
		<comment id='4' author='kl2792' date='2018-08-11T23:05:38Z'>
		The standard deviation can be 0 if the variance we compute is negative, which can happen when the accuracies for each batch are very similar. This is due to precision errors casting from pytorch float tensors to python floats.
If your running accuracies you see throughout the epoch are all around 0.32 then this might be what's happening.
Looking closer I think the acc value should also be between 0 and 1, although I'll have to check. Could you post the shape of your network output? It might be as simple as the mean metric count not being updated correctly.
		</comment>
		<comment id='5' author='kl2792' date='2018-08-11T23:55:34Z'>
		Ah, I see -- thanks for the explanation, &lt;denchmark-link:https://github.com/MattPainter01&gt;@MattPainter01&lt;/denchmark-link&gt;
 ! I implemented a seq2seq model, so network output is (batch_size, max_length, num_tokens) due to padding, while y_true shape is (batch_size, max_length).
I was also wondering -- would it be possible for accuracy to have an ignore_index option like nn.CrossEntropyLoss? The 0 index is a placeholder index for me and it'd be nice to exclude it from accuracy computations. Happy to put in a PR for it if someone could give me a pointer to what code I'd need to edit.
		</comment>
		<comment id='6' author='kl2792' date='2018-08-12T09:56:15Z'>
		Seems like the categorical accuracy isn’t handling those outputs correctly, which it definitely should, I’ll take a look on Monday :) I’ll also open an issue for the ignore index option
		</comment>
	</comments>
</bug>