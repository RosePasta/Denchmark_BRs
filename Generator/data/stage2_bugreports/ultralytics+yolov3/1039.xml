<bug id='1039' author='JACKYLUO1991' open_date='2020-04-11T02:35:13Z' closed_time='2020-06-20T00:15:42Z'>
	<summary>Multi GPU testing two slow!</summary>
	<description>
Something maybe need to improve...
Thanks for your work. I used 4 GPUs to train the network. However, the testing speed is very slow.
	</description>
	<comments>
		<comment id='1' author='JACKYLUO1991' date='2020-04-11T02:35:56Z'>
		Hello &lt;denchmark-link:https://github.com/JACKYLUO1991&gt;@JACKYLUO1991&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://colab.research.google.com/drive/1G8T-VFxQkjDe4idzN8F-hbIBqkkkQnxw&gt;Google Colab Notebook&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov3&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;GCP Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
		</comment>
		<comment id='2' author='JACKYLUO1991' date='2020-05-11T02:39:04Z'>
		Same observation here. Testing on Multi GPU is very slow, while training does see a significant boost in performance in multiple GPUs.
		</comment>
		<comment id='3' author='JACKYLUO1991' date='2020-05-11T03:55:05Z'>
		&lt;denchmark-link:https://github.com/JACKYLUO1991&gt;@JACKYLUO1991&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/cesarandreslopez&gt;@cesarandreslopez&lt;/denchmark-link&gt;
 thanks for the feedback.
train and test use two different multi-gpu setups, nn.distributedDataParallel() and nn.DataParallel(), which could be the cause of the speed difference.
Can you guys run test.py directly to get a speed breakdown of inference/nms/total, to see if NMS is thee cause?
As a rough value, training single GPU on COCO should take about 20-30 min per epoch, and testing should take 1-2 min per epoch.
		</comment>
		<comment id='4' author='JACKYLUO1991' date='2020-05-11T04:43:20Z'>
		Will run it and report back here.
		</comment>
		<comment id='5' author='JACKYLUO1991' date='2020-05-11T05:13:38Z'>
		&lt;denchmark-link:https://github.com/cesarandreslopez&gt;@cesarandreslopez&lt;/denchmark-link&gt;
 great! Try to test with single and multi-gpu using  and  for example, and record the profiling line at the end of each:

		</comment>
		<comment id='6' author='JACKYLUO1991' date='2020-05-13T05:23:30Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 my machine was busy till today on other training, so haven't tested in the COCO data set yet. I have the following logs for training and testing.
The Dataset is quite large, one category only, 353518 samples. The machine is a DGX1 with 8 V100 GPUs. Params are as follows:
&lt;denchmark-code&gt;Namespace(adam=False, batch_size=64, bucket='', cache_images=False, cfg='cfg/modelspp.cfg', data='data/model.data', device='', epochs=13, evolve=False, img_size=[640], multi_scale=True, name='', nosave=False, notest=True, rect=False, resume=True, single_cls=False, weights='weights/last.pt')
&lt;/denchmark-code&gt;

Every Epoch takes about 1 hour and 13 minutes.
&lt;denchmark-code&gt;12/12     22.4G      1.48     0.959         0      2.44  2.28e+03       896: 100%|██████████| 5524/5524 [1:12:36&lt;00:00,  1.27it/s]
&lt;/denchmark-code&gt;

Once the system tests (after it's completed the epochs I've requested) it goes over all files again for testing (In this case my testing list is identical to the training list - just for measuring times as you requested).
&lt;denchmark-code&gt;Class    Images   Targets         P         R   mAP@0.5        F1:  88%|████████▊ | 4868/5524 [3:31:28&lt;42:11,  3.86s/it]
&lt;/denchmark-code&gt;

The above is still testing, but It will end up taking about 4 hours to test all the dataset, versus 1 hour and 12 minutes running an epoch.
I'll run the equivalent tests on COCO dataset as soon as the machine is available again.
		</comment>
		<comment id='7' author='JACKYLUO1991' date='2020-05-13T16:00:57Z'>
		&lt;denchmark-link:https://github.com/cesarandreslopez&gt;@cesarandreslopez&lt;/denchmark-link&gt;
 ah thanks for the benchmarks. Please post your  line after testing is complete.
Depending on your training and testing setup, training may faster than testing, or vice versa. If we use COCO as a data point, training takes about 25 min per epoch (120k images) with the default setup, whereas testing 5k images, again default setup takes about 2 min. So if I extrapolate 120/5*2min = 48 min to test the entire training set, about twice as long as it takes to train it.
		</comment>
		<comment id='8' author='JACKYLUO1991' date='2020-05-13T16:10:34Z'>
		&lt;denchmark-link:https://github.com/cesarandreslopez&gt;@cesarandreslopez&lt;/denchmark-link&gt;
 once we see the Speed: line we can determine if NMS is the source. If it's not you should probably try and test with a single GPU to see if this is really a multi-gpu issue or simply endemic to your dataset. The line to test single gpu is:
python test.py --device 0
		</comment>
		<comment id='9' author='JACKYLUO1991' date='2020-06-14T00:16:37Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
	</comments>
</bug>