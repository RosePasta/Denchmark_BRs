<bug id='1074' author='junglezhao' open_date='2020-04-20T13:41:56Z' closed_time='2020-07-26T00:23:13Z'>
	<summary>TypeError: forward() missing 1 required positional argument: 'x'</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

A clear and concise description of what the bug is.
Hi guys, when test.py runs at 99%, it occurs to a error like the following :
(I don't change the file...)
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test.py", line 255, in &lt;module&gt;
    opt.augment
  File "test.py", line 94, in test
    inf_out, train_out = model(imgs, augment=augment)
  File "/root/anaconda3/envs/yolov3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/anaconda3/envs/yolov3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 152, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/root/anaconda3/envs/yolov3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 162, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/root/anaconda3/envs/yolov3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/root/anaconda3/envs/yolov3/lib/python3.7/site-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
TypeError: Caught TypeError in replica 1 on device 1.
Original Traceback (most recent call last):
  File "/root/anaconda3/envs/yolov3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/root/anaconda3/envs/yolov3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'x'

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='junglezhao' date='2020-04-20T13:42:35Z'>
		Hello &lt;denchmark-link:https://github.com/junglezhao&gt;@junglezhao&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb&gt;Google Colab Notebook&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov3&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;GCP Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
		</comment>
		<comment id='2' author='junglezhao' date='2020-04-20T14:43:58Z'>
		use --augment
		</comment>
		<comment id='3' author='junglezhao' date='2020-04-20T16:45:38Z'>
		&lt;denchmark-link:https://github.com/junglezhao&gt;@junglezhao&lt;/denchmark-link&gt;
 I would make sure your code is up to date using git pull, and if the issue persists please provide minimum reproducible example code.
		</comment>
		<comment id='4' author='junglezhao' date='2020-04-21T17:25:45Z'>
		&lt;denchmark-link:https://github.com/qtw1998&gt;@qtw1998&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/junglezhao&gt;@junglezhao&lt;/denchmark-link&gt;
 yes an augment boolean can be passed to the model() forward method to conduct augmented inference for higher recall and better mAP, but it is not a required argument, as a default  value is supplied. Nevertheless you can do augmented inference from the command line with the  argparser argument:
python3 test.py --augment
python3 detect.py --augment



yolov3/models.py


         Line 232
      in
      4c4f4f4






 def forward(self, x, augment=False, verbose=False): 





		</comment>
		<comment id='5' author='junglezhao' date='2020-04-21T23:50:18Z'>
		
use --augment

ok ,thx. I chose to redownload rep and reset the config to solve this problem.
		</comment>
		<comment id='6' author='junglezhao' date='2020-04-26T17:21:18Z'>
		I am also getting a similar error. I followed the instructions given to train yolov3 on custom dataset. I have prepared my custom dataset according to the required format. When I start training, I get the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 422, in &lt;module&gt;
    train()  # train normally
  File "train.py", line 317, in train
    dataloader=testloader)
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/test.py", line 94, in test
    inf_out, train_out = model(imgs, augment=augment)  # inference and training outputs
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/venv/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 449, in forward
    outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/venv/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 474, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/venv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/venv/lib/python3.6/site-packages/torch/_utils.py", line 394, in reraise
    raise self.exc_type(msg)
TypeError: Caught TypeError in replica 3 on device 3.
Original Traceback (most recent call last):
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/venv/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/home/rajat/Desktop/Radspot/Object_detection/yolov3/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'x'
&lt;/denchmark-code&gt;

I think the error occurs during testing, but I have no idea why it is occurring. What can be the reason for this error?
		</comment>
		<comment id='7' author='junglezhao' date='2020-04-26T17:32:36Z'>
		I encountered the same bug when testing (on 8 GPUs), in the last minibatch to be precise.
A workaround is to skip the last test iteration: not really the definitive solution, but it works.
		</comment>
		<comment id='8' author='junglezhao' date='2020-04-26T18:08:26Z'>
		&lt;denchmark-link:https://github.com/leoll2&gt;@leoll2&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Rajat-Mehta&gt;@Rajat-Mehta&lt;/denchmark-link&gt;
 your code may be out of date, I would advise a  or to reclone the current repo.
		</comment>
		<comment id='9' author='junglezhao' date='2020-04-26T19:32:06Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I already tried to pull the latest code. That did not solve my problem.
This error is encountered while training and testing on multiple gpus, I tried to train on single GPU and that resolved my error.
		</comment>
		<comment id='10' author='junglezhao' date='2020-04-26T21:29:25Z'>
		&lt;denchmark-link:https://github.com/Rajat-Mehta&gt;@Rajat-Mehta&lt;/denchmark-link&gt;
 ok thank you. Are you able to reproduce the error on an open dataset like coco64.data? If so please send us exact code to reproduce and we can get started debugging it.
		</comment>
		<comment id='11' author='junglezhao' date='2020-05-02T11:19:20Z'>
		I updated pytorch from 1.4 to 1.5 and now the training process is not working on multiple GPUs even on coco dataset. But the training works fine when I train using single GPU.
		</comment>
		<comment id='12' author='junglezhao' date='2020-05-02T18:02:20Z'>
		&lt;denchmark-h:h2&gt;Reproduce Our Environment&lt;/denchmark-h&gt;

To access an up-to-date working environment (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled), consider a:

GCP Deep Learning VM with $300 free credit offer: GCP Quickstart Guide
Google Colab Notebook with 12 hours of free GPU time: Google Colab Notebook
Docker Image from https://hub.docker.com/r/ultralytics/yolov3. See Docker Quickstart Guide

		</comment>
		<comment id='13' author='junglezhao' date='2020-05-04T18:27:35Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , &lt;denchmark-link:https://github.com/junglezhao&gt;@junglezhao&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/leoll2&gt;@leoll2&lt;/denchmark-link&gt;
 I can confirm that this bug still exists. We are using an up to date repo and we get exactly the same error using 4 GPUs at exactly the same point . . The problem does not exist when using single or double GPUs.
Here is the full trace:
Class Images Targets P R mAP@0.5 F1: 100% 1548/1549 [22:49&lt;00:01, 1.01s/it]ATraceback (most recent call last):
File "/root/.trains/venvs-builds/3.6/task_repository/yolov3_training.git/test.py", line 98, in test
inf_out, train_out = model(imgs, augment=augment) # inference and training outputs
File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in call
result = self.forward(*input, **kwargs)
File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 449, in forward
outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)
File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 474, in parallel_apply
return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
output.reraise()
File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/_utils.py", line 394, in reraise
raise self.exc_type(msg)
TypeError: Caught TypeError in replica 3 on device 3.
Original Traceback (most recent call last):
File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
output = module(*input, **kwargs)
File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in call
result = self.forward(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'x'
Any other suggestion other than &lt;denchmark-link:https://github.com/leoll2&gt;@leoll2&lt;/denchmark-link&gt;
's skipping last iteration?
		</comment>
		<comment id='14' author='junglezhao' date='2020-05-04T18:33:08Z'>
		&lt;denchmark-link:https://github.com/berkerlogoglu&gt;@berkerlogoglu&lt;/denchmark-link&gt;
 thanks. Can you reproduce this error in a common environment (i.e. the docker image or a gcp vm) on an open dataset like coco64.data?
Without this we can not debug.
		</comment>
		<comment id='15' author='junglezhao' date='2020-05-05T15:52:47Z'>
		Hi &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
,
I am working with &lt;denchmark-link:https://github.com/berkerlogoglu&gt;@berkerlogoglu&lt;/denchmark-link&gt;
. I have tried your docker image. Here is my obversations:
The comment written by &lt;denchmark-link:https://github.com/berkerlogoglu&gt;@berkerlogoglu&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/1074#issuecomment-623629249&gt;#1074 (comment)&lt;/denchmark-link&gt;
, was using a custom validation which has approximately 99k images in a different docker. After your suggestion, I have tried it with your docker image and the error occurred again.
After that, I have tried with coco64.data, nothing happened. I thought error occurs in very big datasets and I tried some custom coco validation set with approximately 2k images.
First, I used 16 batch size which makes 125 batches to process, no error occurred.
Then, I used 2 batch size which makes 1000 batches to process and the same error occurred.
The error log is:
 Traceback (most recent call last): File "train.py", line 475, in train() # train normally File "train.py", line 349, in train dataloader=testloader) File "/root/.trains/venvs-builds/3.6/task_repository/yolov3_training.git/test.py", line 101, in test inf_out, train_out = model(imgs, augment=augment) # inference and training outputs File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 449, in forward outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs) File "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 474, in parallel_apply return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)]) File "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply output.reraise() File "/opt/conda/lib/python3.6/site-packages/torch/_utils.py", line 395, in reraise raise self.exc_type(msg) TypeError: Caught TypeError in replica 2 on device 2. Original Traceback (most recent call last): File "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker output = module(*input, **kwargs) File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) TypeError: forward() missing 1 required positional argument: 'x'
I hope we can find a way to solve this problem..
Thanks..
		</comment>
		<comment id='16' author='junglezhao' date='2020-05-05T16:24:51Z'>
		&lt;denchmark-link:https://github.com/kaanakan&gt;@kaanakan&lt;/denchmark-link&gt;
 I don't think the dataset size plays a big role. I got the error on a relatively small dataset (~2500 images, 400 validation).
		</comment>
		<comment id='17' author='junglezhao' date='2020-05-05T17:36:27Z'>
		&lt;denchmark-link:https://github.com/kaanakan&gt;@kaanakan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/leoll2&gt;@leoll2&lt;/denchmark-link&gt;
 ok thanks. I need to be able to reproduce the issue with a common dataset otherwise we can not debug it.
From what I gather above the error only appears on 4-GPU testing of specific datasets. It is not reproducible on coco64.data. Is it reproducible with coco2017.data or coco2014.data?
		</comment>
		<comment id='18' author='junglezhao' date='2020-05-10T23:01:25Z'>
		&lt;denchmark-link:https://github.com/joel5638&gt;@joel5638&lt;/denchmark-link&gt;
 as I've mentioned to the others, if you can reproduce this error in a reproducible environment with a reproducible dataset then we can debug. i.e. send us a google colab notebook producing the error on coco if you can.
		</comment>
		<comment id='19' author='junglezhao' date='2020-05-10T23:29:50Z'>
		So the fix is just make a check on the batch size
batch_sz = imgs.size()[0]
if batch_size == batch_sz:
# Disable gradients
with torch.no_grad()
etc.........
i tried to push but was not able, for some reason the batch size in the last epoch is not equal to the size of orginal batch_size that's  why the error occurs.
		</comment>
		<comment id='20' author='junglezhao' date='2020-05-10T23:54:06Z'>
		&lt;denchmark-link:https://github.com/joel5638&gt;@joel5638&lt;/denchmark-link&gt;
 can you please make the changes in the code
Thanks
		</comment>
		<comment id='21' author='junglezhao' date='2020-05-11T03:50:44Z'>
		&lt;denchmark-link:https://github.com/Hidayat722&gt;@Hidayat722&lt;/denchmark-link&gt;
 that's normal for batch sizes to vary, it should not cause a bug. We can not implement your proposed fix, as this will omit mAP computations on the last batch. If you can reproduce this error, please reproduce in a colab notebook on coco so that we may run it ourselves and debug.
		</comment>
		<comment id='22' author='junglezhao' date='2020-05-17T16:32:25Z'>
		Hello, I also encounter a similar problem. This error occurs when using multiple GPUs for training and testing. Is it caused by different kinds of GPUs? This is the details of my device
Using CUDA device0 _CudaDeviceProperties(name='TITAN V', total_memory=12058MB) device1 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11172MB) device2 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11172MB) device3 _CudaDeviceProperties(name='TITAN V', total_memory=12058MB) 
		</comment>
		<comment id='23' author='junglezhao' date='2020-05-17T18:46:21Z'>
		&lt;denchmark-link:https://github.com/leiyuncong1202&gt;@leiyuncong1202&lt;/denchmark-link&gt;
 it is not recommended to use different types of gpus togethor. In your case you might want to use --device 0,3 for example or --device 1,2
		</comment>
		<comment id='24' author='junglezhao' date='2020-05-27T08:49:52Z'>
		
@leiyuncong1202 it is not recommended to use different types of gpus togethor. In your case you might want to use --device 0,3 for example or --device 1,2

According to your suggestion, my problem has been solved. Thank you~
		</comment>
		<comment id='25' author='junglezhao' date='2020-06-19T00:27:08Z'>
		I also came across this error today when testing using 3 GPUs.
TypeError: forward() missing 1 required positional argument: 'x'
Edit:
Want to note that the issue seems to be related to the batch size. A batch size of 18 works but not a batch size of 21. Here is a similar issue found from another repo: &lt;denchmark-link:https://github.com/Eromera/erfnet_pytorch/issues/2&gt;Eromera/erfnet_pytorch#2&lt;/denchmark-link&gt;

		</comment>
		<comment id='26' author='junglezhao' date='2020-06-19T02:05:20Z'>
		&lt;denchmark-link:https://github.com/linzzzzzz&gt;@linzzzzzz&lt;/denchmark-link&gt;
 best practices is to use even numbers of GPUs at all times if you use &gt; 1.
		</comment>
		<comment id='27' author='junglezhao' date='2020-06-19T05:21:38Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  Thanks for the suggestion :)
		</comment>
		<comment id='28' author='junglezhao' date='2020-07-20T00:22:21Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
	</comments>
</bug>