<bug id='727' author='markson14' open_date='2019-12-18T08:36:56Z' closed_time='2020-03-11T00:09:47Z'>
	<summary>RuntimeError: CUDA error: unspecified launch failure</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

RuntimeError: CUDA error: unspecified launch failure, terminate called after throwing an instance of 'c10::Error
Training will be stopped when this happened.  It is able to train for couple epoches before problem occurred. Can somebody tell me the reasons? Thank you so much.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

torch = 1.1.0
python = 3.7.5
Desktop (please complete the following information):

OS: Ubuntu 16.04.5
Version 16.04.5

&lt;denchmark-link:https://user-images.githubusercontent.com/30102000/71069869-907a8580-21b4-11ea-9f30-78cd8a067e60.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='markson14' date='2019-12-18T17:58:03Z'>
		&lt;denchmark-link:https://github.com/markson14&gt;@markson14&lt;/denchmark-link&gt;
 I'm not sure, that's a new one. You may want to try training in a standardized environment:
&lt;denchmark-link:https://github.com/ultralytics/yolov3#reproduce-our-environment&gt;https://github.com/ultralytics/yolov3#reproduce-our-environment&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='markson14' date='2019-12-20T01:59:50Z'>
		To be more specific, this happen when I train yolov3.cfg on 8 GPUs. Currently, I train yolov3-spp.cfg on 4 GPUs and it hasn't occurred to me so far. I'm in 66 epoches now. I don't really think this would be the problems of network architecture.
		</comment>
		<comment id='3' author='markson14' date='2019-12-20T14:39:51Z'>
		We need to fix a few distributed computing issues. For example at the end of the epoch its need a cuda.synchronize().
I tried running the model in 8 Titan RTX and got many Apex and Segmentation Errors.
I went down to training it to one GPU at 16 per batch. And no errors.
As soon as I am free I am going to try to make a pull request with a few more distributive computing fixes.
		</comment>
		<comment id='4' author='markson14' date='2019-12-20T22:01:17Z'>
		&lt;denchmark-link:https://github.com/markson14&gt;@markson14&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/FranciscoReveriano&gt;@FranciscoReveriano&lt;/denchmark-link&gt;
 sometimes launch failure errors are simply hiding cuda out of memory errors, so if you reduce your batch-size this may fix the problem.
The repo has been tested with up to 8 GPUs on a GCP instance with no issues, though we saw reduced levels of speed increase past more than 2 GPUs for unknown reasons. For results of various multi-GPU configurations see &lt;denchmark-link:https://github.com/ultralytics/yolov3#speed&gt;https://github.com/ultralytics/yolov3#speed&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='markson14' date='2019-12-21T15:00:21Z'>
		That is true. I saw a speed increase in training in just One-GPU then training in Six-GPUs at the higher batch size.
I also stopped having CUDA Memory Errors when I went down to batch size 16.
Creo, que el programa necesita CUDA Synchronization and Apex Synchronization
As soon as I am done with the grayscale training. I am going to test it.
		</comment>
		<comment id='6' author='markson14' date='2020-03-06T00:09:29Z'>
		This issue is stale because it has been open 30 days with no activity. Remove stale label or comment or this will be closed in 5 days
		</comment>
	</comments>
</bug>