<bug id='258' author='ghost(ghost)' open_date='2019-05-05T01:42:18Z' closed_time='2019-08-06T15:46:03Z'>
	<summary>PyTorch 1.1 ONNX export error</summary>
	<description>
Hello with current master after setting ONNX_EXPORT to True, detect.py failed with
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same
$ python3 detect.py --cfg yolov3.cfg --weights yolov3.weights
Namespace(cfg='yolov3.cfg', conf_thres=0.5, data_cfg='data/coco.data', images='data/samples', img_size=416, nms_thres=0.5, weights='yolov3.weights')
Using CUDA device0 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11178MB)
/home/clement/Downloads/ultralytics/models.py:192: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
img_size = max(x.shape[-2:])
Traceback (most recent call last):
File "detect.py", line 140, in 
nms_thres=opt.nms_thres
File "detect.py", line 49, in detect
torch.onnx.export(model, img, 'weights/export.onnx', verbose=True)
File "/home/clement/.local/lib/python3.6/site-packages/torch/onnx/init.py", line 25, in export
return utils.export(*args, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/onnx/utils.py", line 131, in export
strip_doc_string=strip_doc_string)
File "/home/clement/.local/lib/python3.6/site-packages/torch/onnx/utils.py", line 363, in _export
_retain_param_name, do_constant_folding)
File "/home/clement/.local/lib/python3.6/site-packages/torch/onnx/utils.py", line 266, in _model_to_graph
graph, torch_out = _trace_and_get_graph_from_model(model, args, training)
File "/home/clement/.local/lib/python3.6/site-packages/torch/onnx/utils.py", line 225, in _trace_and_get_graph_from_model
trace, torch_out = torch.jit.get_trace_graph(model, args, _force_outplace=True)
File "/home/clement/.local/lib/python3.6/site-packages/torch/jit/init.py", line 231, in get_trace_graph
return LegacyTracedModule(f, _force_outplace, return_inputs)(*args, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 493, in call
result = self.forward(*input, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/jit/init.py", line 294, in forward
out = self.inner(*trace_inputs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in call
result = self._slow_forward(*input, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 481, in _slow_forward
result = self.forward(*input, **kwargs)
File "/home/clement/Downloads/ultralytics/models.py", line 199, in forward
x = module(x)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in call
result = self._slow_forward(*input, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 481, in _slow_forward
result = self.forward(*input, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/container.py", line 92, in forward
input = module(input)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in call
result = self._slow_forward(*input, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 481, in _slow_forward
result = self.forward(*input, **kwargs)
File "/home/clement/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 338, in forward
self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same
	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2019-05-05T04:46:31Z'>
		Okay I worked around this issue by running PyTorch in CPU only.
		</comment>
		<comment id='2' author='ghost(ghost)' date='2019-05-05T10:10:46Z'>
		&lt;denchmark-link:https://github.com/clementyau&gt;@clementyau&lt;/denchmark-link&gt;
 this seems to be a PyTorch 1.1 issue. See &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/19374&gt;pytorch/pytorch#19374&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='ghost(ghost)' date='2019-07-26T08:57:13Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , it seem to be fixed in master, and i reinstall pytorch by pip
following the command:


but still get error, any suggestion? Thanks
		</comment>
		<comment id='4' author='ghost(ghost)' date='2019-07-26T11:46:37Z'>
		&lt;denchmark-link:https://github.com/peterhsu2018&gt;@peterhsu2018&lt;/denchmark-link&gt;
 your install command will only reinstall the same 1.1.0 you probably already had before. If you want the latest pytorch (which I don't recommend), then you need to install nightly.
In any case, these are all direct PyTorch issues and not unique to this repo, so I would look for answers on the pytorch repo or the pytorch message boards.
		</comment>
		<comment id='5' author='ghost(ghost)' date='2019-07-30T11:21:47Z'>
		Also as an update, the torch.onnx export doesnt work with

pytorch 1.0.1 with cuda 10
pytorch nightly (1.2.XX) with cuda 10

		</comment>
		<comment id='6' author='ghost(ghost)' date='2019-07-30T12:27:29Z'>
		&lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/258#issuecomment-489389713&gt;#258 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='ghost(ghost)' date='2019-07-31T09:21:49Z'>
		&lt;denchmark-link:https://github.com/clementyau&gt;@clementyau&lt;/denchmark-link&gt;
 How did you change the code to use cpu only?
In detect.py, in line 24, I replaced the
 to 
But this doesn't work, let me know how you changed the code to use cpu only.
		</comment>
		<comment id='8' author='ghost(ghost)' date='2019-07-31T12:07:32Z'>
		In detect.py use device = torch_utils.select_device(force_cpu=True).
		</comment>
		<comment id='9' author='ghost(ghost)' date='2019-08-06T15:46:03Z'>
		This cpu-issue should be corrected in the latest commit.
		</comment>
	</comments>
</bug>