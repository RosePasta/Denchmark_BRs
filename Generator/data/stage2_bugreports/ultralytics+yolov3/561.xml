<bug id='561' author='SmallHedgehog' open_date='2019-10-15T13:34:06Z' closed_time='2020-03-12T00:09:41Z'>
	<summary>Exporting the Model to ONNX</summary>
	<description>
I convert this model to ONNX and running it using ONNX Runtime, but the output of ONNX Rumtime and PyToch are different with same input.
&lt;denchmark-link:https://user-images.githubusercontent.com/25521617/66836184-7beef680-ef93-11e9-9a5d-4a85798fbc99.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='SmallHedgehog' date='2019-10-15T16:11:30Z'>
		Sorry to interrupt, but could you share your code for converting to ONNX?
I can't manage to pass that step, even after having read the doc many times and several trials.
		</comment>
		<comment id='2' author='SmallHedgehog' date='2019-10-16T01:08:53Z'>
		
Sorry to interrupt, but could you share your code for converting to ONNX?
I can't manage to pass that step, even after having read the doc many times and several trials.

`
&lt;denchmark-code&gt;# Initialize model
model = Darknet(opt.cfg, img_size)

# Load weights
if opt.weights.endswith('.pt'):  # pytorch format
    model.load_state_dict(torch.load(opt.weights, map_location=device)['model'])
else:  # darknet format
    _ = load_darknet_weights(model, opt.weights)

# Fuse Conv2d + BatchNorm2d layers
# model.fuse()

# Eval mode
model.to(device).eval()

from PIL import Image
img0 = Image.open('test.jpg')
img = np.array(img0)

# Padded resize
img = pad_to_square(img, value=0)
img = np.array(resize(img, img_size))
# Normalize RGB
img = img.transpose(2, 0, 1)
img = img.astype(np.float32)
img /= 255.0  # 0 - 255 to 0.0 - 1.0

# img = torch.from_numpy(img).unsqueeze(0)
img = img.reshape((1, ) + img.shape)
img = torch.from_numpy(img)
xout = model(img)[0]
print('xout: ', xout.size())

# img = torch.zeros((1, 3) + img_size)  # (1, 3, 320, 192)
# torch.onnx.export(model, img, 'weights/export.onnx', verbose=True)
# Export the model
torch.onnx.export(model,                 # model being run
              img,                       # model input (or a tuple for multiple inputs)
              "weights/export.onnx",     # where to save the model (can be a file or file-like object)
              export_params=True,        # store the trained parameter weights inside the model file
              opset_version=10,          # the ONNX version to export the model to
              do_constant_folding=True,  # wether to execute constant folding for optimization
              input_names = ['input'],   # the model's input names
              output_names = ['output'], # the model's output names
              )

model = onnxruntime.InferenceSession('weights/export.onnx')



ort_inputs = {model.get_inputs()[0].name: img.numpy()}
ort_outs = model.run(None, ort_inputs)
out = ort_outs[0]

# out = model(img)[0]
print(out.shape)

# compare ONNX Runtime and PyTorch results
np.testing.assert_allclose(xout.numpy(), ort_outs[0], rtol=1e-03, atol=1e-05)
&lt;/denchmark-code&gt;

`
		</comment>
		<comment id='3' author='SmallHedgehog' date='2019-11-28T13:35:27Z'>
		
I convert this model to ONNX and running it using ONNX Runtime, but the output of ONNX Rumtime and PyToch are different with same input.


Sorry to bother you，did you fixed it？
		</comment>
		<comment id='4' author='SmallHedgehog' date='2019-11-30T02:02:03Z'>
		See &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/653&gt;#653&lt;/denchmark-link&gt;
 for a more direct conversion to onnx.
		</comment>
		<comment id='5' author='SmallHedgehog' date='2019-12-08T07:28:15Z'>
		

I convert this model to ONNX and running it using ONNX Runtime, but the output of ONNX Rumtime and PyToch are different with same input.


Sorry to bother you，did you fixed it？

I also meet this problem, I can convert successfully, but the output is now same between the onnx  and pytorch
		</comment>
		<comment id='6' author='SmallHedgehog' date='2019-12-09T12:46:08Z'>
		


I convert this model to ONNX and running it using ONNX Runtime, but the output of ONNX Rumtime and PyToch are different with same input.


Sorry to bother you，did you fixed it？

I also meet this problem, I can convert successfully, but the output is now same between the onnx and pytorch

I had fixed the problem.
It's not the Upsampling problem.
using pytorch1.2 and the opset9 version is also ok
		</comment>
		<comment id='7' author='SmallHedgehog' date='2019-12-09T12:49:27Z'>
		



I convert this model to ONNX and running it using ONNX Runtime, but the output of ONNX Rumtime and PyToch are different with same input.


Sorry to bother you，did you fixed it？

I also meet this problem, I can convert successfully, but the output is now same between the onnx and pytorch

I had fixed the problem.
It's not the Upsampling problem.
using pytorch1.2 and the opset9 version is also ok

Thank you！
		</comment>
		<comment id='8' author='SmallHedgehog' date='2020-03-07T00:09:23Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
		<comment id='9' author='SmallHedgehog' date='2020-03-31T14:22:54Z'>
		



I convert this model to ONNX and running it using ONNX Runtime, but the output of ONNX Rumtime and PyToch are different with same input.


Sorry to bother you，did you fixed it？

I also meet this problem, I can convert successfully, but the output is now same between the onnx and pytorch

I had fixed the problem.
It's not the Upsampling problem.
using pytorch1.2 and the opset9 version is also ok

Hi, the same problem. So I tried to replace pytorch1.4 with 1.2 as you suggested. But it says "AttributeError: module 'torch.jit' has no attribute 'unused'". Seems to be another problem arguing pytorch version is too old. Is there any solution without changeing pytorch version?
		</comment>
		<comment id='10' author='SmallHedgehog' date='2020-03-31T18:34:54Z'>
		&lt;denchmark-link:https://github.com/flyingmrwang&gt;@flyingmrwang&lt;/denchmark-link&gt;
 onnx model fuses obj and cls confidences. This is suitable for export to coreml and the iDetection iOS app.
		</comment>
		<comment id='11' author='SmallHedgehog' date='2020-04-28T09:26:26Z'>
		My environment: Pytorch v1.2  onnx V1.6.0 and opset V9 is ok. But Pytorch V1.4 failed.
		</comment>
		<comment id='12' author='SmallHedgehog' date='2020-04-28T16:43:29Z'>
		&lt;denchmark-link:https://github.com/Nioolek&gt;@Nioolek&lt;/denchmark-link&gt;
 you might want to try a verified environment:
&lt;denchmark-h:h2&gt;Reproduce Our Environment&lt;/denchmark-h&gt;

To access an up-to-date working environment (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled), consider a:

GCP Deep Learning VM with $300 free credit offer: GCP Quickstart Guide
Google Colab Notebook with 12 hours of free GPU time: Google Colab Notebook
Docker Image from https://hub.docker.com/r/ultralytics/yolov3. See Docker Quickstart Guide

		</comment>
	</comments>
</bug>