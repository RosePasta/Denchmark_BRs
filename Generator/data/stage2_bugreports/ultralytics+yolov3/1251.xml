<bug id='1251' author='joel5638' open_date='2020-05-28T19:55:41Z' closed_time='2020-07-22T00:21:42Z'>
	<summary>while training a model, validation in each epoch takes more time than training</summary>
	<description>
I have 19,000 images in the training and 13,000 images in testing.I'm trying to train a model using --multi-scale.
But the time taken for validation is higher for each epoch than the training. Is it because of the -multiscale?
if training is taking 26mins, validation is taking 30mins for each epoch
	</description>
	<comments>
		<comment id='1' author='joel5638' date='2020-05-28T21:28:55Z'>
		&lt;denchmark-link:https://github.com/joel5638&gt;@joel5638&lt;/denchmark-link&gt;
 multiscale is only used during training, not testing.
You're testing on too large a fraction of images. Best practice is to use roughly 80 20 split. This may also speed up your overall training.
		</comment>
		<comment id='2' author='joel5638' date='2020-05-28T21:29:29Z'>
		&lt;denchmark-link:https://github.com/joel5638&gt;@joel5638&lt;/denchmark-link&gt;
 also you can skip testing alltogethor except the last epoch by using --notest
		</comment>
		<comment id='3' author='joel5638' date='2020-05-29T05:19:18Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 oh okay. But by skipping the testing for all the epochs. We do not get the map scores, do we?
		</comment>
		<comment id='4' author='joel5638' date='2020-05-29T06:34:55Z'>
		&lt;denchmark-link:https://github.com/joel5638&gt;@joel5638&lt;/denchmark-link&gt;
 if you use --notest then testing is only performed after training is over. So you would only get the final mAP.
If you want to keep computing mAP just reduce your test set.
		</comment>
		<comment id='5' author='joel5638' date='2020-05-29T06:50:44Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 okay sure.
I see for first two epochs the validation was completed in 5mins. But after 2 epochs it is taking 30mins for each epoch.
Ideally it should take much longer time from first epoch. But it takes longer time after few epochs.
		</comment>
		<comment id='6' author='joel5638' date='2020-05-29T07:06:18Z'>
		&lt;denchmark-link:https://github.com/joel5638&gt;@joel5638&lt;/denchmark-link&gt;
 ah that's interesting. There is a  flag that is enabled after the first 3 epochs when testing that could be the cause of this. In coco this results in higher mAPs, but also increases the testing time a bit, maybe from 1:30 to 1:50 per epoch. You could try disabling it here by hard coding . How many classes do you have?


		</comment>
		<comment id='7' author='joel5638' date='2020-05-29T07:19:02Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 oh i see. Will make that change.
Even the map is very less. Its around 0.234 for all the epochs and it doesnt increase at all.
Is there something wrong with the training setup we have? May be data or something?
		</comment>
		<comment id='8' author='joel5638' date='2020-05-29T07:20:55Z'>
		Who knows. You have to look at your train*.jpg, test*.jpg to verify your labels, and then plot your results.png file to view your trends. You need a good ML guy to read the tea leaves too.
		</comment>
		<comment id='9' author='joel5638' date='2020-05-29T07:34:46Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 oh okay sure Glenn. Thanks for this. Helps better
And btw, i have 7 classes. Weâ€™re training on 7classes.
		</comment>
		<comment id='10' author='joel5638' date='2020-05-29T07:39:52Z'>
		I just remembered one more thing. NMS has timing logic which forces a premature exit if the process is taking too long. You might want to see if this is happening to you by putting a print() command in here, and maybe disabling it for a more accurate mAP, though this would be slower still of course:



yolov3/utils/utils.py


        Lines 539 to 541
      in
      cf7a4d3






 if (time.time() - t) &gt; time_limit: 



 break # time limit exceeded 



 





Time limit is 10 seconds per batch:



yolov3/utils/utils.py


         Line 479
      in
      cf7a4d3






 time_limit = 10.0 # seconds to quit after 





		</comment>
		<comment id='11' author='joel5638' date='2020-06-16T01:28:18Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I would like to know whether the model used for testing is the same model in the different scales you give for model performance?
		</comment>
		<comment id='12' author='joel5638' date='2020-06-16T02:22:52Z'>
		&lt;denchmark-link:https://github.com/TAOSHss&gt;@TAOSHss&lt;/denchmark-link&gt;
 yes, there's only one model. You can find it in the Google Drive folder.
		</comment>
		<comment id='13' author='joel5638' date='2020-06-16T03:33:24Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 That is to say, as long as I choose the multi-scale (320-640) training, I can get the performance at all scales (320,416,512,608)
		</comment>
		<comment id='14' author='joel5638' date='2020-06-16T06:02:33Z'>
		&lt;denchmark-link:https://github.com/TAOSHss&gt;@TAOSHss&lt;/denchmark-link&gt;
 yes that is correct.
		</comment>
		<comment id='15' author='joel5638' date='2020-06-16T06:15:02Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 First of all, thank you for your excellent work.I have seen another project of yours, YOLOV5, where it is not difficult to see that the performance of the model on the coco data set test set and verification set is very small.But I downloaded from the website (&lt;denchmark-link:https://pjreddie.com/media/files/yolov3.weights&gt;https://pjreddie.com/media/files/yolov3.weights&lt;/denchmark-link&gt;
) model, I use this model in coco validation set above test results better than given in the paper, this is due to the author to update the model or coco test set difficulty is greater than validation set?
		</comment>
		<comment id='16' author='joel5638' date='2020-06-16T06:23:52Z'>
		&lt;denchmark-link:https://github.com/TAOSHss&gt;@TAOSHss&lt;/denchmark-link&gt;
 if you use yolov3.weights are the official darknet-trained weights. Our inference implementation is slightly different than darknet, so you may see better results when run in this repo than what's shown in the original paper.
		</comment>
		<comment id='17' author='joel5638' date='2020-06-16T06:29:12Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 This is amazing. I used Yolov3.Weights as the pre-training model and used your reasoning process. The average result on the coco verification set was 8map higher
		</comment>
		<comment id='18' author='joel5638' date='2020-06-16T06:33:03Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 There may be other reasons for this. I noticed that the results in Yolov3 are replications of the results in the paper and are slightly better than the results in the paper, but not by much, right
		</comment>
		<comment id='19' author='joel5638' date='2020-06-16T06:40:25Z'>
		&lt;denchmark-link:https://github.com/TAOSHss&gt;@TAOSHss&lt;/denchmark-link&gt;
 inference here may be slightly higher, but not +8mAP.
Also all of the weights in this repo, and from the paper, were trained on coco2014, not 2017, so you can only test on 2014.
For 2017 only, go to yolov5.
		</comment>
		<comment id='20' author='joel5638' date='2020-06-16T07:00:02Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  Thank you very much for your answer
		</comment>
		<comment id='21' author='joel5638' date='2020-07-17T00:21:26Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
	</comments>
</bug>