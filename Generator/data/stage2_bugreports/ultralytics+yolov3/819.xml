<bug id='819' author='pprp' open_date='2020-01-31T11:15:40Z' closed_time='2020-02-01T14:10:51Z'>
	<summary>Using different batch size leads to different results when running test.py</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thank you for your excellent work, but I encountered a bug.
I only have 39 pictures for testing.
If batch size &gt; total number of pictures, the F1 score is 86.4% and map is 78.2%.
If batch size &lt;total number of pictures, the F1 score is 85% and the map is 77.7%.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

batch size = 64

&lt;denchmark-code&gt;python test.py --cfg cfg/yolov3.cfg --weights  weights/best.pt --batch-size 64
&lt;/denchmark-code&gt;

I got:
&lt;denchmark-code&gt;Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16276MB)
           device1 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16276MB)

               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11&lt;00:00, 11.50s/it]
                 all        39        65     0.962     0.785     0.782     0.864

&lt;/denchmark-code&gt;


batch size = 4

&lt;denchmark-code&gt;python test.py --cfg cfg/yolov3.cfg --weights  weights/best.pt --batch-size 4
&lt;/denchmark-code&gt;

I got:
&lt;denchmark-code&gt;Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16276MB)
           device1 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16276MB)

               Class    Images   Targets         P         R   mAP@0.5        F1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10&lt;00:00,  1.09s/it]
                 all        39        65     0.927     0.785     0.777      0.85

&lt;/denchmark-code&gt;

Do you know where the problem is? Thank you very much!
	</description>
	<comments>
		<comment id='1' author='pprp' date='2020-01-31T16:49:27Z'>
		&lt;denchmark-link:https://github.com/pprp&gt;@pprp&lt;/denchmark-link&gt;
 good investigation! This is actually not a bug, it is an effect of the rectangular inference that test.py runs. It orders batches by aspect ratio, grouping images of similar shapes into a batch, and applying the minimum letterbox required to that batch. That means that each of the batches have different letterboxed shapes. See rectangular inference &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/232&gt;#232&lt;/denchmark-link&gt;

When you do batch-size 64, likely a square letterbox is applied. In any case, the differences should be very minor, as you see. If you had a larger group of test images your results would be even closer I believe.
		</comment>
		<comment id='2' author='pprp' date='2020-02-01T14:10:51Z'>
		Thank you very much for your explanationÔºÅ
		</comment>
	</comments>
</bug>