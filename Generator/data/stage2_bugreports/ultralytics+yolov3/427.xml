<bug id='427' author='hajlyx' open_date='2019-08-06T12:25:03Z' closed_time='2019-08-06T12:50:38Z'>
	<summary>Something's wrong in train.py</summary>
	<description>
In train.py line 208: "results = (0, 0, 0, 0, 0)"
This line conficts with line 294:  "file.write(s + '%11.3g' * 7 % results + '\n')"
If I run train.py with --evolve --epochs 2, the program will break because the length of results is still 5, not 7.
In train.py line 378:  "hyp[k] = x[i + 5]"
This line may change to    "hyp[k] = x[i + 7]" to match the evolve params.
Don't know how to quote code pages...
	</description>
	<comments>
		<comment id='1' author='hajlyx' date='2019-08-06T12:39:42Z'>
		&lt;denchmark-link:https://github.com/hajlyx&gt;@hajlyx&lt;/denchmark-link&gt;
 ah of course! We made a breaking change to the repo recently, which adds two more columns to results.txt, which record the validation loss components (GIoU, obj, cls), where before we only recorded the total val loss, hence the two new cols.
I've updated the repo with the corrections you recommended just now! Can you git pull and try again?
		</comment>
		<comment id='2' author='hajlyx' date='2019-08-06T12:49:40Z'>
		Everything goes well now.  :)
		</comment>
		<comment id='3' author='hajlyx' date='2019-08-06T12:50:38Z'>
		Great! Thanks for the bug report, let us know if anything else is awry!
		</comment>
		<comment id='4' author='hajlyx' date='2019-08-06T12:54:53Z'>
		&lt;denchmark-link:https://github.com/hajlyx&gt;@hajlyx&lt;/denchmark-link&gt;
 BTW, if you are evolving hyperparameters, you may want to look at the most recent comments on &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/310&gt;#310&lt;/denchmark-link&gt;

It appears that a good guideline is to evolve to 10% of your final epoch count, so for evolving coco training (which runs to 273 epochs) a good evolve command would be here. It will take a long time naturally, but this is the game we are in.
python3 train.py --epochs 27 --evolve
		</comment>
		<comment id='5' author='hajlyx' date='2019-08-06T13:01:52Z'>
		OK, I'll try it if I get the time.
Thanks for your advice!
		</comment>
		<comment id='6' author='hajlyx' date='2019-08-07T07:38:36Z'>
		Emm....I'm here again.
I found a OOM error in train.py and I still have no idea to solve it.
When I run train.py with --evolve --epochs 10, after one train loop ( 10 epochs ), the gpu memory used increased about 700 MB, and it still increased in 2nd and 3rd loop. The program broke down when it got into 4th loop because of out of memory.
I checked the gpu memory when one train loop is finished, there is still 1.8 GB memory used.
Does anyone know why and how to solve it?
		</comment>
		<comment id='7' author='hajlyx' date='2019-08-07T09:21:24Z'>
		&lt;denchmark-link:https://github.com/hajlyx&gt;@hajlyx&lt;/denchmark-link&gt;
 yes you are correct, there is a memory leak when using , but we have not been able to debug this.
Your current alternatives are to reduce your --batch-size and increase your --accumulate or to evolve for only 1 generation in a bash for loop:
for i in {0..100}
do
   python3 train.py --img-size 320 --epochs 27 --batch-size 64 --accumulate 1 --evolve
done
		</comment>
		<comment id='8' author='hajlyx' date='2019-08-07T09:41:15Z'>
		Just do it as your mentioned.
I'll take some time to dig it. If I solve it, I'll show you. :)
		</comment>
		<comment id='9' author='hajlyx' date='2019-08-07T12:35:00Z'>
		If you could discover the cause of the memory leak that would be amazing. We spent a couple days debugging it with no success unfortunately.
		</comment>
	</comments>
</bug>