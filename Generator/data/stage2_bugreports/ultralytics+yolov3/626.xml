<bug id='626' author='15151874182' open_date='2019-11-18T01:41:29Z' closed_time='2019-11-18T02:22:13Z'>
	<summary>lr scheduler changed after --resume</summary>
	<description>
hi, at the begin, i training voc2007 dataset with learing rate set to 0.01 between 0-300epoches,and 0.001 for 300-400epoches.And the lr_scheduler figure ploted by this repo is shown. i stopped the programme at 188epoch, then, resume training. I found the lr start from 1E-4,is there any problems here?
	</description>
	<comments>
		<comment id='1' author='15151874182' date='2019-11-18T02:00:24Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/36249659/69019098-1656b580-09ea-11ea-871a-45035997ecc9.png&gt;&lt;/denchmark-link&gt;

after --resume
&lt;denchmark-link:https://user-images.githubusercontent.com/36249659/69019123-325a5700-09ea-11ea-8d80-dfb18252d8fc.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='15151874182' date='2019-11-18T02:22:13Z'>
		LR schedules are set at the beginning of training, and use the --epochs value to compute when they should be applied. --epochs should not be changed after beginning training or your results will be unpredictable.
For best results, always train entirely without stopping and resuming.



yolov3/train.py


        Lines 153 to 161
      in
      b4a71d0






 # Scheduler https://github.com/ultralytics/yolov3/issues/238 



 # lf = lambda x: 1 - x / epochs  # linear ramp to zero 



 # lf = lambda x: 10 ** (hyp['lrf'] * x / epochs)  # exp ramp 



 # lf = lambda x: 1 - 10 ** (hyp['lrf'] * (1 - x / epochs))  # inverse exp ramp 



 # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf) 



 # scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=range(59, 70, 1), gamma=0.8)  # gradual fall to 0.1*lr0 



 scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[round(opt.epochs * x) for x in [0.8, 0.9]], gamma=0.1) 



 scheduler.last_epoch = start_epoch - 1 



 





		</comment>
	</comments>
</bug>