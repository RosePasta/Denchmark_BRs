<bug id='622' author='Pari-singh' open_date='2019-11-16T01:33:10Z' closed_time='2019-11-24T22:49:09Z'>
	<summary>conf_threshold dilemma</summary>
	<description>
&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I have one more question, in issue &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/310&gt;#310&lt;/denchmark-link&gt;
 you mentioned we should keep the conf_threshold at 0.001 for getting the final score on validation set (running test.py) But then you also suggest to keep the threshold between 0.1-0.9 depending on recall-precision flavor we want for our detection.
What if I have to detect on a set and at the same time give them the mAP of my detection? Or lets say I give them the predictions on dataset and they test the mAP on their own out of that. What would you suggest in that case? Thanks!
	</description>
	<comments>
		<comment id='1' author='Pari-singh' date='2019-11-16T19:15:13Z'>
		See &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/422&gt;#422&lt;/denchmark-link&gt;

It's up to you to set your own conf-thresh to achieve the optimal P-R compromise for your situation.
		</comment>
	</comments>
</bug>