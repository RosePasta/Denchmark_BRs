<bug id='412' author='Ownmarc' open_date='2019-08-01T14:09:54Z' closed_time='2019-10-04T20:28:21Z'>
	<summary>train.py initialized as many times as there is workers every batch ?</summary>
	<description>
Is it normal to get the "train.py" to be initialized every batch * number of workers ?
I was trying to set up the tensorboard_writer and was defining it just bellow the imports in "train.py" and it was creating 4 writers every batch.
To reproduce, simply add a print bellow the imports in train.py and watch the training console. If I make num-workers = 8, it will print it 8 times. Got tensorboard to work by moving the import and the initialization under the if name == 'main', but I find it weird. Maybe is me not understanding how dataloader works.
with 4 workers :
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/62300291-8134ae80-b444-11e9-964f-5a078e232f0c.jpg&gt;&lt;/denchmark-link&gt;

with 8 workers:
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/62300477-dc66a100-b444-11e9-8618-ba2e1e0d68e4.jpg&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='Ownmarc' date='2019-08-01T16:28:26Z'>
		That's odd. I inserted a print statement there but it only prints the expected number of times (once). It shouldn't run every epoch either.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/62310501-fe364700-b489-11e9-9516-5518d4b0e1c8.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Ownmarc' date='2019-08-01T17:04:34Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 BTW I see you are on windows. We just got a windows machine here the other day to do training on premise and were surprised to see that the repo did not work correctly out of the box due to windows path issues. We've resolved these now with more OS-agnostic code in the last week or so, so the repo should correctly operate out of the box now under MacOS, Linux and Windows 10.
		</comment>
		<comment id='3' author='Ownmarc' date='2019-08-01T17:22:17Z'>
		Alright, I'll do a fresh pull. Yea, I needed to add an init.py in the utils folders to get it to work.
Also, I am on a computer with 2 GPUs, but pytorch is not supporting multiGPU training for Windows and MacOS, so I was having some trouble with the multi process part of the code. I was simply deleting it.
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/20380#issuecomment-516759817&gt;pytorch/pytorch#20380 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Ownmarc' date='2019-08-01T17:50:28Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 just did a fresh pull of the code, I didn't (beside adding the print) change anything and I am still getting this.
Anyway you can test it on Windows ? I'll be able to test it on a Linux server next week.
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/62315445-4f7f1000-b463-11e9-838d-06cbdc495e17.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Ownmarc' date='2019-08-01T20:13:33Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 haha, that is truly bizarre. I do see you have the very latest code now. This must be something external to the repo, maybe run settings or environment variables in Spyder or PyCharm are somehow causing this, or maybe it has to do with the utils folder requiring the init.py.
Our workflow here is to develop and debug on mac on cpu most of the time, with some GCP training on debian unix, and then some on premise training now with a single GPU windows machine also. All of these platforms are working as intended AFAIK right now. I can try to introduce the print statement on our windows machine too when it frees up later.
BTW I did not know about multi-gpu not working on windows or mac. Well, basically no gpu at all works on mac these days, it's frustrating :(
		</comment>
		<comment id='6' author='Ownmarc' date='2019-08-01T20:17:45Z'>
		Tested the exact same set-up on Linux and everything is fine!
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/62324592-e3f36d80-b477-11e9-8bff-b9a0a647e40b.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Ownmarc' date='2019-08-01T20:18:49Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 are you sure this is the only place in your code this statement appears?
One area I noticed that behaved pretty strangely was the dataloader during batch 0. I'd see it run the init function in datasets and call the first batch many more times than I was expecting during batch 0 (all before training started). For example if we had --num-workers 4 it might try and load up 16 batches of images instead of 4 the very first time (with a very long, noticeable delay also in that first batch). I spent several days trying to track it down and eventually gave up, as it only affected batch 0.
		</comment>
		<comment id='8' author='Ownmarc' date='2019-08-01T20:21:29Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , yea, the exact same code on Linux prints only once like it does for you on mac/cpu..  I dont have PyCharm/Spyder, maybe a bug with Pytorch..
Btw Windows doesn't require the init.py anymore, looks like you managed to fix that.
Please try the print trick on your Windows set-up and update me with that. If its the same thing, maybe we should open an issue in the pytorch repo, otherwise i'll double check my setup. I have a conda env specificly for this repo with pytorch 1.1 and python 3.7.3 and everything required, no extra.
		</comment>
		<comment id='9' author='Ownmarc' date='2019-08-01T20:27:40Z'>
		
@Ownmarc haha, that is truly bizarre. I do see you have the very latest code now. This must be something external to the repo, maybe run settings or environment variables in Spyder or PyCharm are somehow causing this, or maybe it has to do with the utils folder requiring the init.py.
Our workflow here is to develop and debug on mac on cpu most of the time, with some GCP training on debian unix, and then some on premise training now with a single GPU windows machine also. All of these platforms are working as intended AFAIK right now. I can try to introduce the print statement on our windows machine too when it frees up later.
BTW I did not know about multi-gpu not working on windows or mac. Well, basically no gpu at all works on mac these days, it's frustrating :(

Quick fix for multi-gpu would be to only allow it if we are running on linux.
		</comment>
		<comment id='10' author='Ownmarc' date='2019-08-01T20:36:04Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , I read here that, for Windows, the loop that consumes the DataLoader should be under the if name == main. Seems to be a problem specific to Windows!
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/12831&gt;pytorch/pytorch#12831&lt;/denchmark-link&gt;

Would there be an issue to move everything under the if name == main ? train.py is never imported anywhere
		</comment>
		<comment id='11' author='Ownmarc' date='2019-08-02T02:33:19Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 looks like Python, on Windows, can't fork when starting a new process so the new process as to rerun everything. Only way to fix that is to put the loop that uses the dataloader under the  guard.
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/12831#issuecomment-517520133&gt;pytorch/pytorch#12831 (comment)&lt;/denchmark-link&gt;

I'll wait for you to update that and I'll PR for the tensorboard support. This can be closed I guess.
		</comment>
		<comment id='12' author='Ownmarc' date='2019-08-02T14:39:39Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 I can reproduce the bug on Windows now, Mac and Unix are good. On Windows I saw the bug occurred in the default configuration as well as if I moved all the import down inside the  function.
The last alternative (which I didn't test) is to move them inside the train() function itself, which seems to be the recommended solution from the pytorch repo, but this would mean that everyone on unix and mac would also then be forced to reimport all the packages multiple times when using --evolve, so I'm a bit stuck.
I wonder if this is affecting our Windows training speed.  We just got a new Corsair i160 Windows 10 with a 2080 Ti, which is doing 34 minute epochs (default repo, 29 min train + 5 min test), whereas a Unix GCP V100 is doing 24 minute epochs (21 min train + 3 min test). Benchmarks show a V100/2080Ti FP16 speedup should be 22%, but we see 34/24=42%, so something is wrong (maybe due to Windows?).
&lt;denchmark-link:https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/&gt;https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='Ownmarc' date='2019-08-02T16:18:07Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 it occurs on every batch! Everytime the data loader is called, but it is a matter of a few seconds I think.
I can dualboot on linux/windows, I could run some tests on my 1080ti to see if the training takes much longer on Windows then Linux and update you with that.
You only moved the imports or all the code ? The way I understand it is that all of the code in train.py should be under the if name == main guard.
		</comment>
		<comment id='14' author='Ownmarc' date='2019-08-02T17:21:07Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 are you sure its every batch?
The problem will affect all of the 3 main files then, train.py, test.py and detect.py...
A problem with inlining the training code is that --evolve calls it to evolve the hyperparameters. So at the moment train() is called once to train normally, or is called in a for loop when evolving hyperparameters. It might make sense to split out evolve into it's own file, or use Facebook's Ax package for hyperparameter evolution instead, but then test.py will also be affected I think, definitely if called by itself, and maybe also when called during training?
		</comment>
		<comment id='15' author='Ownmarc' date='2019-08-02T20:35:40Z'>
		Yea its on every batch, see my first post.
I didn't use --evolve at all yet, so I can't help for that.
I'll try to different things to try to come up with a fix
		</comment>
		<comment id='16' author='Ownmarc' date='2019-08-05T16:29:25Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 we just recently resolved a multi-GPU training inefficiency. The model was being duplicated by accident every batch when calculating the loss. This is fixed and now we observe expected speed increases when moving from 1 to 2 GPUs. Not sure if this will benefit windows results as well, the new benchmarks are recorded from unix GCP VMs:
&lt;denchmark-link:https://github.com/ultralytics/yolov3#speed&gt;https://github.com/ultralytics/yolov3#speed&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='Ownmarc' date='2019-08-06T15:01:27Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , pytorch doesn't support the , it is simply not available under Windows, meaning we can't do multi-gpu training under Windows currently.
About the other issue, looks like moving everything under the name == main guard is fixing the problem of reimporting everything.. I understand from your previous reply that this would break other things ?
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/62551197-8ffed500-b839-11e9-9b7a-3870b880adf1.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='Ownmarc' date='2019-08-06T15:14:41Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 hey there. Ahh, I see. I got confused because I see you have two GPUs listed in your console output.
So we tested a fix that resolved the issue you were seeing. We simply brought the imports down into the if __name__ statement and left the train() function alone. This stopped the imports from loading multiple times, but we did not see any effects on training speed.
So I think what you are seeing is simply a onetime bug of multiple imports once when train.py is called, but afterward there seems to be no effect on the rest of the code.
		</comment>
		<comment id='19' author='Ownmarc' date='2019-08-06T15:44:45Z'>
		Ok great &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , what brought this to my attention was the tensorboard implementation I was adding to your code. I was trying to define my writer outside of the main guard and was getting many files written on each batch, thats why I opened this issue. I wanted to have imports and code that would look like yours but I was having this problem doing so.
I'll sumbit a PR later on for tensorboard :)
		</comment>
		<comment id='20' author='Ownmarc' date='2019-08-06T16:03:33Z'>
		ah yes, the tensorboard implementation!
FYI on the visualization topic we implemented a couple breaking changes lately. The model's modules names have been simplified (which unfortunately makes the repo incompatible with older trained *.pt models), but fine with everything else, and also 2 new outputs are recorded during training. The last 1 column of results.txt used to be total validation loss, but this is now broken out into it's 3 component (GIoU, obj, cls) losses using the last column plus 2 additional ones.
We added a new plotting function that overlays train and test losses for these 3 components, to make it easier to see if any of the loss components are overfitting. It would be awesome if this were available in tensorboard for realtime visualization:

&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/62555973-3b2c8000-b874-11e9-8a82-0391a07286aa.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='Ownmarc' date='2019-08-06T16:07:31Z'>
		Oh also, the best news today is that someone did some hardcore hyperparameter evolution and achieved 50.5 mAP when training YOLOv3-SPP 320 from scratch (no backbone at all), which is only -1.8 mAP less than the official  at 52.3 mAP! Things are finally coming together around here it seems...
&lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/310#issuecomment-518448296&gt;#310 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='Ownmarc' date='2019-08-06T16:10:59Z'>
		I'll add them, its really easy, pytorch did a great job at implementing tensorboard, its easier then it is with keras/tensorflow lol
		</comment>
		<comment id='23' author='Ownmarc' date='2019-10-04T20:28:21Z'>
		I'll close this issue for now as the original issue appears to have been resolved, and/or no activity has been seen for some time. Feel free to comment if this is not the case.
		</comment>
	</comments>
</bug>