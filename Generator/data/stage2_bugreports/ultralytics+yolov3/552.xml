<bug id='552' author='Belinda-great' open_date='2019-10-14T07:27:13Z' closed_time='2019-11-05T00:45:17Z'>
	<summary>AttributeError: module 'torch.distributed' has no attribute 'init_process_group'</summary>
	<description>
When i trained the yolov3-tiny, there has a bug, could you give me any tips to solve that?
Namespace(accumulate=2, adam=False, arc='defaultpw', batch_size=8, bucket='', cache_images=False, cfg='cfg/yolov3-tiny-1cls.cfg', data='data/coco.data', device='', epochs=273, evolve=False, img_size=416, img_weights=False, multi_scale=
False, name='', nosave=False, notest=False, prebias=False, rect=False, resume=False, transfer=False, var=None, weights='')
Using CUDA device0 _CudaDeviceProperties(name='GeForce GTX TITAN X', total_memory=12218MB)
device1 _CudaDeviceProperties(name='GeForce GTX TITAN X', total_memory=12218MB)
device2 _CudaDeviceProperties(name='GeForce GTX TITAN X', total_memory=12218MB)
device3 _CudaDeviceProperties(name='GeForce GTX TITAN X', total_memory=12218MB)
Traceback (most recent call last):
File "train.py", line 434, in 
train()  # train normally
File "train.py", line 181, in train
dist.init_process_group(backend='nccl',  # 'distributed backend'
AttributeError: module 'torch.distributed' has no attribute 'init_process_group'
	</description>
	<comments>
		<comment id='1' author='Belinda-great' date='2019-10-14T09:19:51Z'>
		I trained on windows,is the windows not support the init_process_group?
		</comment>
		<comment id='2' author='Belinda-great' date='2019-10-16T07:09:51Z'>
		
I trained on windows,is the windows not support the init_process_group?

Hi
What's the things going on? I also want to know is this repo can be used on windows 10
		</comment>
		<comment id='3' author='Belinda-great' date='2019-10-16T15:29:13Z'>
		Repo can be used on windows 10, but Pytorch doesn't support multi-gpu on Windows.
The quick fix is to change line 178 of train.py for it to not get in the multi-gpu if condition even if you have multiple gpu available. This is how I do it :
if torch.cuda.device_count() &gt; 100:
If you want to train using multi-gpu, I'd switch to Linux
		</comment>
	</comments>
</bug>