<bug id='725' author='litingsjj' open_date='2019-12-18T02:58:58Z' closed_time='2020-01-03T00:10:57Z'>
	<summary>Segmentation fault (core dumped)</summary>
	<description>
Hi，I use this project to training my cunstom dataset with yolov3-spp, after training 71 epochs, I get  this error. And anthor time, I also got this error after 2 epoch. Is there something wrong?
Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size 71/272     4.93G      1.98      2.16    0.0963      4.24       237       416:  65%|########################################2                     | 2394/3688 [15:30&lt;08:03,  2.68it/s]Segmentation fault (core dumped)
	</description>
	<comments>
		<comment id='1' author='litingsjj' date='2019-12-18T18:00:16Z'>
		Hello, thank you for your interest in our work! This issue seems to lack the minimum requirements for a proper response, or is insufficiently detailed for us to help you. Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove existing
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, PyTorch &gt;= 1.3 etc. You can also use our Google Colab Notebook and our Docker Image to test your code in working environment.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
		</comment>
		<comment id='2' author='litingsjj' date='2019-12-19T09:43:30Z'>
		
Hi，I use this project to training my cunstom dataset with yolov3-spp, after training 71 epochs, I get this error. And anthor time, I also got this error after 2 epoch. Is there something wrong?
Epoch gpu_mem GIoU obj cls total targets img_size 71/272 4.93G 1.98 2.16 0.0963 4.24 237 416: 65%|########################################2 | 2394/3688 [15:30&lt;08:03, 2.68it/s]Segmentation fault (core dumped)

I have same problem, did you find a way around?
		</comment>
		<comment id='3' author='litingsjj' date='2019-12-19T16:22:25Z'>
		I just got this error for the first time. From C++ a segmentation error usually occurs when we have a memory problem.
My hunch is that it has to do with the CUDA memory. Going to try a smaller batch size and see if I still get this.
		</comment>
		<comment id='4' author='litingsjj' date='2019-12-21T09:23:26Z'>
		&lt;denchmark-link:https://github.com/FranciscoReveriano&gt;@FranciscoReveriano&lt;/denchmark-link&gt;
 I only used 40% CUDA memory
		</comment>
		<comment id='5' author='litingsjj' date='2019-12-21T09:36:36Z'>
		Actually I fixed this problem by "decreasing data loader count" which I previously increased to get smoother batch transfer speed (getting rid of batch stutters in training phase). Seems like giving the CPU more parallel data loader thread than it can handle creates exceptions like this. You can leave batch size as it is if you don't hit the CUDA memory limit.
		</comment>
		<comment id='6' author='litingsjj' date='2019-12-21T14:51:52Z'>
		Yeah, I fixed mine by reducing the mini-batch size.
The code requires further synchronization with Apex and CUDA
I will try to do that this weekend. So we can train in larger batch sizes.
		</comment>
		<comment id='7' author='litingsjj' date='2019-12-21T20:07:05Z'>
		&lt;denchmark-link:https://github.com/nlztrk&gt;@nlztrk&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/FranciscoReveriano&gt;@FranciscoReveriano&lt;/denchmark-link&gt;
 the code tries to use up to 8 dataloader workers if possible, for both train and validate dataloaders. If the os cpu count is less than 8 it is reduced to that number, and if the batch_size is 1, 0 workers are used (no dataloader parallelization). The  variable sets the number of workers.



yolov3/train.py


        Lines 201 to 208
      in
      69da7e9






 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8])  # number of workers 



 dataloader = torch.utils.data.DataLoader(dataset, 



 batch_size=batch_size, 



 num_workers=nw, 



 shuffle=not opt.rect,  # Shuffle=True unless rectangular training is used 



 pin_memory=True, 



 collate_fn=dataset.collate_fn) 



 





		</comment>
		<comment id='8' author='litingsjj' date='2019-12-21T20:57:11Z'>
		Yeah I actually modified that nw definition line to force the count to be my cpu count, but it kept throwing segmentation errors that way. But when I set it to the default, it works fine.
		</comment>
		<comment id='9' author='litingsjj' date='2020-01-02T23:10:12Z'>
		I met this problem too, simply update pytorch 1.3.0 to pytorch 1.3.1 works for me.
		</comment>
		<comment id='10' author='litingsjj' date='2020-01-03T00:10:57Z'>
		I'll close this issue for now as the original issue appears to have been resolved, and/or no activity has been seen for some time. Feel free to comment if this is not the case.
		</comment>
		<comment id='11' author='litingsjj' date='2020-02-12T12:07:30Z'>
		Hi, Thank you for your great github!
I'm meeting the same issue above.. "Segmentation fault (core dumped)"
I used the environment of given Docker Image and only changed dataset.py for data loading while the formats are keeping the rules of Custom Training Tutorial.
The error occurs after 2, 3 epochs, and also same error occurs 2, 3 epochs after resuming the training.
I'm using 4 GPUs of RTX 2080 Ti and only using the 30% of GPU memory.
Could you please help where will be the problem??
		</comment>
		<comment id='12' author='litingsjj' date='2020-02-12T20:27:20Z'>
		&lt;denchmark-link:https://github.com/winner9497&gt;@winner9497&lt;/denchmark-link&gt;
 if pulling a docker image ultralytics/yolov3:v0 is always the latest. The message typically follows RAM or GPU RAM out of memory, so you might want to not --cache-images or decrease your --batch-size
		</comment>
		<comment id='13' author='litingsjj' date='2020-02-19T19:39:10Z'>
		I get this error as well. After debugging, I was able to determine that it fails in this line (train.py 430):
from torch.utils.tensorboard import SummaryWriter
It appears to be an open PyTorch issue: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/30651&gt;pytorch/pytorch#30651&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='litingsjj' date='2020-02-27T05:38:09Z'>
		&lt;denchmark-link:https://github.com/porterjenkins&gt;@porterjenkins&lt;/denchmark-link&gt;
 thats interesting! We’re you able to patch it by rearranging the import order?
Right now the imports are ordered by PyCharm automatically, under ‘optimize imports’
		</comment>
	</comments>
</bug>