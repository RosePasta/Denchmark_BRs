<bug id='1109' author='AliceSchaw' open_date='2020-04-28T15:23:11Z' closed_time='2020-05-11T05:45:54Z'>
	<summary>what  is the meaning of outputs?</summary>
	<description>
I trained my custom dataset and exported the onnx model.  the final names of output  are 325 and 328, the type of them is  float32[12348,4].  what  is the meaning of outputs?  my number of classes is 4.
onnx model:
&lt;denchmark-link:https://user-images.githubusercontent.com/29159540/80505428-e5ba7500-89a6-11ea-8c3c-a5e5ba842c38.png&gt;&lt;/denchmark-link&gt;

cfg files:
&lt;denchmark-link:https://github.com/ultralytics/yolov3/files/4546805/prune_0.93_keep_0.01_16_shortcut_yolov3-spp3-4cls.txt&gt;prune_0.93_keep_0.01_16_shortcut_yolov3-spp3-4cls.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AliceSchaw' date='2020-04-28T15:24:43Z'>
		Hello &lt;denchmark-link:https://github.com/AliceSchaw&gt;@AliceSchaw&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb&gt;Google Colab Notebook&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov3&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;GCP Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
		</comment>
		<comment id='2' author='AliceSchaw' date='2020-04-28T16:36:59Z'>
		&lt;denchmark-link:https://github.com/AliceSchaw&gt;@AliceSchaw&lt;/denchmark-link&gt;
 classes and boxes. boxes are nx4
		</comment>
		<comment id='3' author='AliceSchaw' date='2020-04-29T09:25:16Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Could you elaborate on how to interpret these values? I find them confusing as they are different from what the PyTorch version would output
		</comment>
		<comment id='4' author='AliceSchaw' date='2020-04-29T16:57:39Z'>
		@lostspirit0 these are designed for input into our iOS iDetection app.
You can export the raw model outputs as well, search the repo issues, there are many solutions already posted.
		</comment>
		<comment id='5' author='AliceSchaw' date='2020-05-02T02:31:42Z'>
		
@lostspirit0 these are designed for input into our iOS iDetection app.
You can export the raw model outputs as well, search the repo issues, there are many solutions already posted.

&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Could you give me a link, thank you?
		</comment>
		<comment id='6' author='AliceSchaw' date='2020-05-02T04:24:53Z'>
		&lt;denchmark-link:https://github.com/ultralytics/yolov3/issues?q=onnx+export&gt;https://github.com/ultralytics/yolov3/issues?q=onnx+export&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='AliceSchaw' date='2020-05-02T10:33:23Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 thanks for your reply. but i still do not understand the outputs,  my input is 448X448, and strides are[32, 16, 8], so i guess the size of boundign box is (14 x 14 + 28 x 28 + 56 x 56) x 3 = 12348.  but I want to get all the values that include the bbox of  x, y, width, height and score, so, the expected size of outputs should be  (14 x 14 + 28 x 28 + 56 x 56) x (4 + 1 + 4(class size)) * 3, but why is the size of outputs of the onnx  4 x 12348?
		</comment>
		<comment id='8' author='AliceSchaw' date='2020-05-02T18:01:41Z'>
		onnx model may have multiple outputs, you might want to visualize with netron.
		</comment>
		<comment id='9' author='AliceSchaw' date='2020-05-03T01:30:34Z'>
		
onnx model may have multiple outputs, you might want to visualize with netron.

Yes, this is the visuality of my onnx model.
&lt;denchmark-link:https://user-images.githubusercontent.com/29159540/80896406-20623b80-8d20-11ea-917f-5848022c2107.png&gt;&lt;/denchmark-link&gt;

Can you give me some examples how to get all the values that include the bboxs of x, y, width, height and class score ?
		</comment>
		<comment id='10' author='AliceSchaw' date='2020-05-03T01:49:19Z'>
		&lt;denchmark-link:https://github.com/AliceSchaw&gt;@AliceSchaw&lt;/denchmark-link&gt;
 ah perfect. Your outputs are right there. They're clearly labelled as 'boxes' and 'classes'. What seems to be the problem?
		</comment>
		<comment id='11' author='AliceSchaw' date='2020-05-10T07:04:49Z'>
		
@AliceSchaw ah perfect. Your outputs are right there. They're clearly labelled as 'boxes' and 'classes'. What seems to be the problem?

here is official example: &lt;denchmark-link:https://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/inference_demos/yoloV3_object_detection_onnxruntime_inference.ipynb&gt;https://github.com/onnx/onnx-docker/blob/master/onnx-ecosystem/inference_demos/yoloV3_object_detection_onnxruntime_inference.ipynb&lt;/denchmark-link&gt;

It is obviously different between the official example's onnx model and ultralytics's one.

The onnx model of official example  has 3 outputs:
boxes: (1 x n_candidates x 4), the coordinates of all anchor boxes
scores: (1 x 80 x n_candidates), the scores of all anchor boxes per class
indices: (n_box x 3), selected indices from the boxes tensor. The selected index format is (batch_index, class_index, box_index)

However,  here exported model has 2 outputs, any code example about utilizing the ultralytics's exported onnx model  to detect objects with onnx runtime?
		</comment>
		<comment id='12' author='AliceSchaw' date='2020-05-10T16:31:39Z'>
		&lt;denchmark-link:https://github.com/AliceSchaw&gt;@AliceSchaw&lt;/denchmark-link&gt;
 Ah I see. This repo's onnx integrates objectness and classification togethor, which is a common format used in mobile device detection, such as in our iOS iDetection app.
I don't think anyone actually uses onnx to run models, it's mostly just an interchange format to send your model elsewhere, i..e coreml, tflite, etc.
If you want you can try this:
&lt;denchmark-link:https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html#running-the-model-on-an-image-using-onnx-runtime&gt;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html#running-the-model-on-an-image-using-onnx-runtime&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='AliceSchaw' date='2020-05-11T05:45:49Z'>
		
@AliceSchaw Ah I see. This repo's onnx integrates objectness and classification togethor, which is a common format used in mobile device detection, such as in our iOS iDetection app.
I don't think anyone actually uses onnx to run models, it's mostly just an interchange format to send your model elsewhere, i..e coreml, tflite, etc.
If you want you can try this:
https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html#running-the-model-on-an-image-using-onnx-runtime

Thank you for your help and patience.
		</comment>
	</comments>
</bug>