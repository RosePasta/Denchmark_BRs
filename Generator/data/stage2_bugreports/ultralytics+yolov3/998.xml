<bug id='998' author='pfeatherstone' open_date='2020-04-02T08:52:45Z' closed_time='2020-05-04T06:58:52Z'>
	<summary>Question : why not freeze darknet53 backbone?</summary>
	<description>
Looking through the code, I can't see where you might freeze the pretrained darknet53 backbone. Is there a reason why you don't do this?
	</description>
	<comments>
		<comment id='1' author='pfeatherstone' date='2020-04-02T08:58:40Z'>
		Is it best to just initialise the backbone and let it carry on training with the rest of the layers as normal? Isn't there a risk of messing up the backbone early on in the training? Why not freeze the backbone for the first few epochs until learning rate gets below 1e-5 say, then unfreeze the backbone??
		</comment>
		<comment id='2' author='pfeatherstone' date='2020-04-02T17:59:06Z'>
		&lt;denchmark-link:https://github.com/pfeatherstone&gt;@pfeatherstone&lt;/denchmark-link&gt;
 freezing layers always results in worse performance. Loading pretrained weights and training normally produces the best results in all cases.
		</comment>
		<comment id='3' author='pfeatherstone' date='2020-04-02T18:05:43Z'>
		Isn’t there a risk of messing up the backbone early on in the training when the gradients are fairly high? One could argue it’s best to simply train end to end without any preloading. Hopefully there are enough residual connections that the gradients don’t vanish
		</comment>
		<comment id='4' author='pfeatherstone' date='2020-04-02T18:12:56Z'>
		&lt;denchmark-link:https://github.com/pfeatherstone&gt;@pfeatherstone&lt;/denchmark-link&gt;
 well it depends. For large datasets like COCO, we see better results training from scratch (no backbone, no pretrained weights). All our mAPs we report on COCO are trained from scratch.
For smaller datasets we do get better results when training from pretrained weights. In this case we use --weights yolov3-spp-ultralytics.pt to start training from.
You can see a direct comparison of the variety of training initializations for the tutorial coco64.data dataset in the readme. We don't show freezing layers result anymore, but it plateaus early and never rises above a certain level, since its frozen and can't adapt to new data.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/78283617-e4467b80-74d2-11ea-927e-58edc1410ea0.png&gt;&lt;/denchmark-link&gt;

A burnin period helps stabilize early training though, and is very important. We're working on a burnin update this week actually, and should commit it soon.
		</comment>
		<comment id='5' author='pfeatherstone' date='2020-04-02T18:21:40Z'>
		&lt;denchmark-link:https://github.com/pfeatherstone&gt;@pfeatherstone&lt;/denchmark-link&gt;
 also another point is that whenever you start training your LR will be high, and the pretrained weights last saw very small LRs, so when you start training the entire pretrained model will be jolted out of it's local minima, but in most cases this doesn't matter much since it will have 300+ epochs to settle back.
The main problem with pretrained weights is that they will lead to overtraining earlier, meaning you can train for less epochs, meaning you may end up with a lower best mAP.
		</comment>
		<comment id='6' author='pfeatherstone' date='2020-04-03T01:55:10Z'>
		Do you mean that when we are training a small dataset, the best choice should be to use pre-trained weights and use burnin?
		</comment>
		<comment id='7' author='pfeatherstone' date='2020-04-03T04:28:12Z'>
		@ChrisHaooooooo yup. We just made a commit with burnin updates, if you git pull you can get the latest.
		</comment>
		<comment id='8' author='pfeatherstone' date='2020-04-03T05:16:18Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thanks a lot!
		</comment>
		<comment id='9' author='pfeatherstone' date='2020-04-03T07:47:37Z'>
		I've noticed that your burnin scaling is linear rather than quadratic (as before), do you notice it being more stable this way?
		</comment>
		<comment id='10' author='pfeatherstone' date='2020-04-03T19:20:24Z'>
		&lt;denchmark-link:https://github.com/pfeatherstone&gt;@pfeatherstone&lt;/denchmark-link&gt;
 it probably doesn't make a big difference, we switched it to linear so we could simply use np.interp() to interpolate the initial lrs for the different param groups.
		</comment>
		<comment id='11' author='pfeatherstone' date='2020-05-04T00:07:51Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
	</comments>
</bug>