<bug id='197' author='Ownmarc' open_date='2019-04-08T02:59:32Z' closed_time='2019-04-12T20:03:49Z'>
	<summary>mAP and detection not working</summary>
	<description>
I think there is something wrong with the scaling of the bounding boxes. My mAP is always at zero even if my training is going well. Also, when I use detect.py, the bounding boxes are at the right places, but are really small.
I didn't touch anything in util.py and my .txt files for the images are right.
	</description>
	<comments>
		<comment id='1' author='Ownmarc' date='2019-04-08T11:06:38Z'>
		Git clone a clean copy of the repo and run one of the custom tutorials. If your results match ours then its your data.
&lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&lt;/denchmark-link&gt;


You should see this. The 10 image example only takes about 5 minutes on a GCP VM V100 instance.

&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/55669383-df76c980-5876-11e9-9806-691bd507ee17.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Ownmarc' date='2019-04-08T15:26:40Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/55736291-09ec9200-59f1-11e9-95c1-13514e5e6bb4.jpg&gt;&lt;/denchmark-link&gt;

Guess I am overfitting, labels are fine when I open them with the open source project labelimg
I had 21 classes, 450 images and ~50 objects per image
		</comment>
		<comment id='3' author='Ownmarc' date='2019-04-08T17:43:07Z'>
		Before you do any training, an obvious first step is to run a tutorial and make sure your results match.
If you are overfitting your mAP on the train group should be great, right? Have you checked that at least?
In any case, 99% of the times people can't get results they didn't format their data correctly or they've modified the default repository.
		</comment>
		<comment id='4' author='Ownmarc' date='2019-04-09T04:23:53Z'>
		Been trying to run it, but I can't get all the files right in windows, can't run the .sh file to set everything up.
I double checked my annotation data and everything is just like the yolo annotations. I tought my problem was the learning rate or the augmentation and did try several things to make it work tonight without any luck.
Would you mind trying my data to see if you can get something out of it ? Would be appreciated and maybe you'll be able to add guidance to the custom tutorial.
		</comment>
		<comment id='5' author='Ownmarc' date='2019-04-09T05:25:18Z'>
		After searching the official Darknet repo, I think this may have something to do with the anchors.. I probably need to change them for my custom data
		</comment>
		<comment id='6' author='Ownmarc' date='2019-04-09T09:08:49Z'>
		If your target sizes are different enough than the default anchors then yes you will want to vary the anchor dimensions. We used kmeans to do this with the xView data:
&lt;denchmark-link:https://github.com/ultralytics/xview-yolov3&gt;https://github.com/ultralytics/xview-yolov3&lt;/denchmark-link&gt;

You can run under linux using your GCP quickstart:
&lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Ownmarc' date='2019-04-09T09:31:04Z'>
		Also, to make sure your targets are in the right format, you can plot the training data by using the plotting script in train.py.
About your anchors, I'd be very surprised if the smallest or largest anchors weren't covering part of your training data. They span from 10 to 370 pixels wide in a 416 size image. Changing anchors is done to improve results, not to bring them the mAP from zero to something else. I still think there must be a problem elsewhere.


&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/55789336-7089b280-5aba-11e9-8f24-1a6cac480737.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Ownmarc' date='2019-04-09T09:59:54Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 good news maybe. I was posting a comment on a different issue when I realized we had inadvertantly introduced a bug in the master branch related to wh loss computation. This was fixed in our test branch but not the master. I fixed this and also hardcoded plotting of the first train and test batches. When you train normally now, two files will appear in your  directory, train_batch0.jpg and test_batch0.jpg.
You should git pull to incorporate the wh bug fix, and retrain, viewing the two images to make sure the boxes seem correctly aligned. I will add this tidbit to the tutorials as well, this should go far in helping ppl make sure their training and testing data is well formatted.
		</comment>
		<comment id='9' author='Ownmarc' date='2019-04-09T11:49:20Z'>
		Just checked your commit, it makes alot of sense since my training was getting worst the more I trained and it looked like a the loss on yolo layers wasn’t conputing correctly. I’ll keep you updated!
		</comment>
		<comment id='10' author='Ownmarc' date='2019-04-09T12:20:37Z'>
		train batch, everything looks normal:
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/55799833-517e2700-5aa0-11e9-8a9f-9f2bc137c98e.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='Ownmarc' date='2019-04-09T12:23:10Z'>
		Yep! Thanks alot, mAP is showing and increasing! I think we can close this
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/55799979-9f932a80-5aa0-11e9-8e25-423f59c570b4.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='Ownmarc' date='2019-04-09T12:38:14Z'>
		Hmm, it must have been that wh bug. Phew, we have to be careful here when we adjust the code. Ok, glad to hear its all working now!! I hope other people aren't running into the same problem. Probably leave open for a few days just in case anyone goes searching.
If anyone has training problems on custom data, please git pull the latest commit and try again, as a bug was present around the first week of April that has now been resolved!
		</comment>
		<comment id='13' author='Ownmarc' date='2019-04-09T12:46:39Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 hey wait a second, your screenshot is showing Recall &gt; 1 for several categories, which is a statistical impossibility. The high recall seems to be feeding to the mAP as well, causing it to increase above 1 for the same categories.
We validated our mAP against pycocotools and darknet very well, and now it matches to 1%. I just recomputed for another issue: &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/199#issuecomment-481216891&gt;#199 (comment)&lt;/denchmark-link&gt;

Do you know what might be causing this?
		</comment>
		<comment id='14' author='Ownmarc' date='2019-04-09T13:11:26Z'>
		It seems to take into account 1 object that is predicted with 2 bboxes almost on top of each other as 2 good predictions when there is, in fact, only 1 object!
		</comment>
		<comment id='15' author='Ownmarc' date='2019-04-09T13:29:33Z'>
		This is at 0.7 conf threshold, see this cannon having 2 bbox. They are probably counted as 2 good detections.
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/55804277-f94c2280-5aa9-11e9-9dcf-45921d0e1e7a.png&gt;&lt;/denchmark-link&gt;

Here we can see the cannon class at 1.01:
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/55804437-48925300-5aaa-11e9-9e7f-dc4df2fe6524.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='16' author='Ownmarc' date='2019-04-09T14:44:15Z'>
		Hmm. This is surely the finest test.py result I've ever seen.
It's pretty common to get two boxes for one object, that should just give you a P of 0.5 and an R of 1.0 for that instance.
Somehow your list of TPs is greater than the list of target objects, which should not be possible. In any case, it looks like the issue mellowed out eventually. I scanned the test.py code but didn't see anything out of the ordinary. Since this doesn't occur on COCO data I'll just forget about it for now.
		</comment>
		<comment id='17' author='Ownmarc' date='2019-04-09T14:47:39Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
, let me know if you want my dataset to test it!
		</comment>
		<comment id='18' author='Ownmarc' date='2019-04-09T15:06:11Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 maybe if you put it all in a Google Drive folder I can check it out when I have more free time! It would certainly be interesting to see what's causing the &gt; 1 recalls.
Do you think you could have duplicate rows in your labels file? Is it still there at the default test parameters, i.e. nms_thres 0.5?
		</comment>
		<comment id='19' author='Ownmarc' date='2019-04-09T15:27:08Z'>
		Yes, didn't change anything from master repo other then the init.py I need in the util folder (for windows), the font size of the plotting and setting visible gpu to the train script.
No duplicates, they were all made using a script from xml files which were hand annotated and checked using other scripts to make sure there was nothing impossible (like 8 gold_mines since a player can only have 7 maximum)
I have been training using darkflow and the exact same dataset and this was not hapenning. Maybe this can help you (from Darkflow repo):
&lt;denchmark-code&gt;import numpy as np

class BoundBox:
    def __init__(self, classes):
        self.x, self.y = float(), float()
        self.w, self.h = float(), float()
        self.c = float()
        self.class_num = classes
        self.probs = np.zeros((classes,))

def overlap(x1,w1,x2,w2):
    l1 = x1 - w1 / 2.;
    l2 = x2 - w2 / 2.;
    left = max(l1, l2)
    r1 = x1 + w1 / 2.;
    r2 = x2 + w2 / 2.;
    right = min(r1, r2)
    return right - left;

def box_intersection(a, b):
    w = overlap(a.x, a.w, b.x, b.w);
    h = overlap(a.y, a.h, b.y, b.h);
    if w &lt; 0 or h &lt; 0: return 0;
    area = w * h;
    return area;

def box_union(a, b):
    i = box_intersection(a, b);
    u = a.w * a.h + b.w * b.h - i;
    return u;

def box_iou(a, b):
    return box_intersection(a, b) / box_union(a, b);

def prob_compare(box):
    return box.probs[box.class_num]

def prob_compare2(boxa, boxb):
    if (boxa.pi &lt; boxb.pi):
        return 1
    elif(boxa.pi == boxb.pi):
        return 0
    else:
        return -1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='Ownmarc' date='2019-04-09T15:40:01Z'>
		Test loss seems to be good, here is the result.txt if it can help (21 classes) :
&lt;denchmark-link:https://user-images.githubusercontent.com/24617457/55814291-34575180-5abc-11e9-9044-41670b44e02c.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='Ownmarc' date='2019-04-10T22:44:10Z'>
		I was running into the same issue, but after pulling the repo again this morning, I still couldn't get mAP to not be zero. But even weirder is that my wh becomes inf after a while.
&lt;denchmark-link:https://user-images.githubusercontent.com/30191150/55918269-58dc2800-5ba7-11e9-9e21-3efc5d4dddc6.png&gt;&lt;/denchmark-link&gt;

I am training with transfer learning on a custom dataset with 1 class.
		</comment>
		<comment id='22' author='Ownmarc' date='2019-04-10T22:50:17Z'>
		&lt;denchmark-link:https://github.com/vivian-wong&gt;@vivian-wong&lt;/denchmark-link&gt;
 see &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/168&gt;#168&lt;/denchmark-link&gt;
 to control divergent width-height () losses.
		</comment>
		<comment id='23' author='Ownmarc' date='2019-04-11T03:53:06Z'>
		Working now! Thank you!
		</comment>
		<comment id='24' author='Ownmarc' date='2019-04-11T05:11:52Z'>
		
Working now! Thank you!

cong!
		</comment>
		<comment id='25' author='Ownmarc' date='2019-04-13T00:10:47Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
, was the mAP over 1.0 fixed or should we open a new issue ?
		</comment>
		<comment id='26' author='Ownmarc' date='2019-04-13T12:33:25Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 &gt; 1 recall is likely still an open issue as I have not worked on it due to an inability to rapidly reproduce it. Another user mentioned it as well. The darkflow iou code is nice to see, but their code only operates on one box at a time, whereas ours is vectorized for speed (computes many ious simultaneously). In any case, I don't think IOU is the problem.
If you raise a new issue specifically about the &gt; 1 recall, make sure you supply all the elements to reproduce the issue, i.e. a google drive folder with the trained model, the *.data and *.cfg files, and the *.txt file pointing to the training images and labels folders, and of course the folders themselves. This would be the most useful.
		</comment>
		<comment id='27' author='Ownmarc' date='2019-06-25T06:57:20Z'>
		Good morning everyone,
hi &lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 can you share me your weight, cfg and .data .names from your "clash of clans" detector. I realy wanna test it, maybe make a youtube video.
Best regards,
Antoine
		</comment>
		<comment id='28' author='Ownmarc' date='2019-06-25T09:14:13Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 mAP &gt; 1.0 has been fixed now.
		</comment>
		<comment id='29' author='Ownmarc' date='2019-06-25T12:13:28Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 cool, what was the issue ? Thanks
		</comment>
		<comment id='30' author='Ownmarc' date='2019-06-25T16:12:48Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 just a bug in the test.py mAP calculation. Non-exclusive target-anchor combinations were being allowed which caused some targets to count as multiple TPs.
		</comment>
		<comment id='31' author='Ownmarc' date='2020-03-16T08:18:41Z'>
		can anyone help me, i performed transfer learning on yolov3 and its detecting objects but not at the right place. cant upload the result because using company's network.
		</comment>
		<comment id='32' author='Ownmarc' date='2020-03-16T18:15:29Z'>
		&lt;denchmark-link:https://github.com/JalajK&gt;@JalajK&lt;/denchmark-link&gt;
 hello, thank you for your interest in our work! This issue seems to lack the minimum requirements for a proper response, or is insufficiently detailed for us to help you. Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove existing
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, PyTorch &gt;= 1.4 etc. You can also use our Google Colab Notebook and our Docker Image to test your code in a working environment.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
		</comment>
		<comment id='33' author='Ownmarc' date='2020-03-24T13:36:55Z'>
		
Git clone a clean copy of the repo and run one of the custom tutorials. If your results match ours then its your data.
https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data
python3 train.py --data data/coco_10img.data
You should see this. The 10 image example only takes about 5 minutes on a GCP VM V100 instance.
from utils import utils; utils.plot_results()


in the log.txt output what's the information about mAP?
		</comment>
		<comment id='34' author='Ownmarc' date='2020-03-24T21:09:32Z'>
		&lt;denchmark-link:https://github.com/Leprechault&gt;@Leprechault&lt;/denchmark-link&gt;
  to plot your mAP.
		</comment>
		<comment id='35' author='Ownmarc' date='2020-03-24T21:33:00Z'>
		Thanks, &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 !!! Works, but I have a conceptual question too about the way that mAP is calculated. Because in the output file () after each iteration a have e.g.  and I recognize the six variables as iteration, total loss, loss error, rate, time and number of images, but
I don't know where comes from the percentual mAP.
		</comment>
		<comment id='36' author='Ownmarc' date='2020-03-25T20:44:02Z'>
		&lt;denchmark-link:https://github.com/Leprechault&gt;@Leprechault&lt;/denchmark-link&gt;
 I don't know what log file you refer to.
mAP is computed in a standard manner, i.e. area under a PR curve.
		</comment>
		<comment id='37' author='Ownmarc' date='2020-03-28T08:58:03Z'>
		Okay, on the pre-trained weights, i did transfer learning on cnr parking dataset which has only one class i.e., car and format the data according to the yolo model.
in cfg file, i changed the all 3 yolo layers according to this dataset and the no. of filters.
after all this, im getting this kind of output. help me out guys.
&lt;denchmark-link:https://user-images.githubusercontent.com/54980273/77819356-42a0e200-7100-11ea-8621-21ef34904664.jpeg&gt;&lt;/denchmark-link&gt;

bounding boxes are shifted by same particular distance, I'm not getting it.
		</comment>
		<comment id='38' author='Ownmarc' date='2020-03-28T19:54:34Z'>
		&lt;denchmark-link:https://github.com/JalajK&gt;@JalajK&lt;/denchmark-link&gt;
 your labels look incorrect. You need to check your train_batch0.jpg and test_batch0.jpg produced when training starts.
		</comment>
		<comment id='39' author='Ownmarc' date='2020-03-30T11:54:30Z'>
		
@Leprechault I don't know what log file you refer to.
mAP is computed in a standard manner, i.e. area under a PR curve.

Thank &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I search my goal in the wrong file. Now, I try to get the mAP results in a *txt file using , but doesn't work (no output *txt file created). Any ideas?
		</comment>
		<comment id='40' author='Ownmarc' date='2020-03-30T16:53:41Z'>
		&lt;denchmark-link:https://github.com/Leprechault&gt;@Leprechault&lt;/denchmark-link&gt;
 suggest you raise the issue on the relevant repo.
		</comment>
	</comments>
</bug>