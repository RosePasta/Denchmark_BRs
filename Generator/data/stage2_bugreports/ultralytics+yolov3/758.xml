<bug id='758' author='Nioolek' open_date='2020-01-06T09:43:27Z' closed_time='2020-01-15T18:29:34Z'>
	<summary>when I trained and used the parameters --resume, it occured RuntimeError</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When I trained to epoch 7,I stoped it by myself. Then I use the parameter --resume to restart training, it occur the error:
RuntimeError: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 0
Specific error information will be shown below .
I guess the error was causer by loading the optimizer from pt file.
I used config file of 'yolov3-1cls.cfg' for only one class without modification.
There were no errors after I blocked lines 131-133 in train.py
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

**My environment:
PC

Ubuntu 16.04
python3.5
GPU 2080Ti * 2

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

&lt;denchmark-code&gt; Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
  7/19     4.79G      2.97     0.971         0      3.94        17       992:   0%| | 1/541 [00:02&lt;22:44,  2.53s/it]Traceback (most recent call last):
&lt;/denchmark-code&gt;

File "/mnt/work3/code/mouse_detect/yolov3/train.py", line 474, in 
train()  # train normally
File "/mnt/work3/code/mouse_detect/yolov3/train.py", line 318, in train
optimizer.step()
File "/usr/local/lib/python3.5/dist-packages/torch/optim/lr_scheduler.py", line 51, in wrapper
return wrapped(*args, **kwargs)
File "/usr/local/lib/python3.5/dist-packages/torch/optim/sgd.py", line 100, in step
buf.mul_(momentum).add_(1 - dampening, d_p)
RuntimeError: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 0
	</description>
	<comments>
		<comment id='1' author='Nioolek' date='2020-01-15T18:29:33Z'>
		&lt;denchmark-link:https://github.com/Nioolek&gt;@Nioolek&lt;/denchmark-link&gt;
 thanks for the bug report. FYI using  is the exact same as simply using . There is no difference.
We are unable to reproduce this issue on coco16.data, therefore the problem likely likes with your specific dataset, environment or changes to the default repo. Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove existing
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, PyTorch &gt;= 1.3 etc. You can also use our Google Colab Notebook and our Docker Image to test your code in working environment.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
		</comment>
	</comments>
</bug>