<bug id='303' author='zkchen95' open_date='2019-05-28T02:11:40Z' closed_time='2019-05-28T14:02:21Z'>
	<summary>the new epoch loss return back to the last epoch's loss</summary>
	<description>
I've trained with coco
The loss and mAP seem to be not converge, after a new epoch, the loss go back to the last epoch's value, as show in the following fig:
In a epoch, the loss was actually reduced.
&lt;denchmark-link:https://user-images.githubusercontent.com/37893487/58445721-48204b00-8130-11e9-9da9-5b71819134d6.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/37893487/58445741-5ec6a200-8130-11e9-8c71-df2fb9d6a696.png&gt;&lt;/denchmark-link&gt;

However, the next epoch, the loss go back to high:
&lt;denchmark-link:https://user-images.githubusercontent.com/37893487/58445776-96354e80-8130-11e9-8e02-572f200b5d15.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/37893487/58445793-a4836a80-8130-11e9-84e1-651af86cb1f9.png&gt;&lt;/denchmark-link&gt;

Though, the loss was reduced, why they didn't continue the loss of last epoch?
	</description>
	<comments>
		<comment id='1' author='zkchen95' date='2019-05-28T09:08:32Z'>
		I also encountered this problem, who can answer it? I wonder if this has any effect on training.
		</comment>
		<comment id='2' author='zkchen95' date='2019-05-28T14:02:21Z'>
		@ZzzackChen if you don't shuffle your training data then yes of course the first batch will have nearly the same loss and the trends will looks the same. This loss you see is averaged per epoch, the first batch is only itself, so what you are seeing is that the first batch has greater loss than average.
		</comment>
	</comments>
</bug>