<bug id='196' author='Mysorrows' open_date='2019-04-07T13:07:07Z' closed_time='2019-04-10T11:43:25Z'>
	<summary>Some errors and need help</summary>
	<description>

'utils.py' . If only people are detected, is it only necessary to return to [1]?

def coco80_to_coco91_class():   x = [1] return x 
2.Why does the program stop at Calculate mAP when a loss value exceeds e+3?  I use the program of three days ago, when conf_loss exceeds e+3, it will reappear.
3.I am currently using Transfer learning to detect an object (people).  My training set has only 200 pictures, and the verification has only 100 pictures.  At present, its effect is very bad. The values of P, R MAP and F1 are very low, and in most cases are less than 0.01.  Can you give me some advice?
4.'train.py'.  When use transfer learn，the  value of 'start_epoch'  should be 0 instead of being obtained from the model?
&lt;denchmark-link:https://user-images.githubusercontent.com/29940894/55683990-c6564180-5978-11e9-9056-d6f89d0eca4b.PNG&gt;&lt;/denchmark-link&gt;

5.'train.py'.  If Transfer of learning is interrupted, the last training cannot be continued after the next use --resume. All gradients will be calculated at this time.
6.Must the size of the picture be a multiple of 32?  Will the program scale to the default value of 416 pixels?
Desktop:

OS: [linux]

	</description>
	<comments>
		<comment id='1' author='Mysorrows' date='2019-04-07T17:08:14Z'>
		

'utils.py' . If only people are detected, is it only necessary to return to [1]?
def coco80_to_coco91_class(): x = [1] return x 


coco80_to_coco91_class() is only used when outputting pycocotools compatible JSON files. In any case Python is zero indexed so I have no idea what you intend x = x[1] to accomplish.

2.Why does the program stop at Calculate mAP when a loss value exceeds e+3? I use the program of three days ago, when conf_loss exceeds e+3, it will reappear.

test.py calculates mAP after every epoch regardless of loss.

3.I am currently using Transfer learning to detect an object (people). My training set has only 200 pictures, and the verification has only 100 pictures. At present, its effect is very bad. The values of P, R MAP and F1 are very low, and in most cases are less than 0.01. Can you give me some advice?

Run our tutorials. Tune hyperparameters if your performance is unacceptable. This is your job, not ours.
&lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&lt;/denchmark-link&gt;


4.'train.py'. When use transfer learn，the value of 'start_epoch' should be 0 instead of being obtained from the model?

Change it if you want.


5.'train.py'. If Transfer of learning is interrupted, the last training cannot be continued after the next use --resume. All gradients will be calculated at this time.
I don't understand. Is this a question or a statement?


6.Must the size of the picture be a multiple of 32? Will the program scale to the default value of 416 pixels?

Yes. Yes.

Desktop:

OS: [linux]


		</comment>
		<comment id='2' author='Mysorrows' date='2019-04-08T01:31:19Z'>
		
'If Transfer of learning is interrupted, the last training cannot be continued after the next use --resume. All gradients will be calculated at this time.'  This is  a question.

When Transfer of learning is interrupted and training is resumed next time, all layers need to be calculated instead of YOLOY layer alone.
Expected scenario:
When training continues, only the YOLO layer should be counted, not all layers.
		</comment>
		<comment id='3' author='Mysorrows' date='2019-04-08T01:48:42Z'>
		&lt;denchmark-link:https://github.com/Mysorrows&gt;@Mysorrows&lt;/denchmark-link&gt;
, best practice would be to train yolo layers for some epoch, then unfreeze everything to train the whole neural net to get better results.
Still, we do not have a way to freeze on resume, could be a great addition, but should be optional
		</comment>
		<comment id='4' author='Mysorrows' date='2019-04-08T02:18:06Z'>
		&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 . This is a good idea, I will try.
		</comment>
		<comment id='5' author='Mysorrows' date='2019-04-08T02:39:34Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
. &lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
.    Thank you for your help.
		</comment>
		<comment id='6' author='Mysorrows' date='2019-04-08T11:13:47Z'>
		Honestly, the speedup from transfer learning is not very significant for this repo, or any large object detection network in general. It is much more applicable to smaller ML tasks. Here with YOLOv3 we can assume roughly:

1/3 of the time is spent loading and augmenting the data
1/3 of the time spent on the forward pass through the network
1/3 spent computing the gradient and applying the optimizer

Transfer learning is only reducing some of the 3rd step. So the speedup is minimal, and like &lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;
 says training all the layers will produce better results.
		</comment>
		<comment id='7' author='Mysorrows' date='2019-04-08T11:27:43Z'>
		My training dataset has only 200 pictures. Is this a small ML task?  I use it to train and test pedestrians (people standing).
I don't know whether the full version of YOLOv3 model is suitable for it, or the tiny version is more suitable, or neither.
If all layers are trained, the data set is too small to reduce the result.
		</comment>
		<comment id='8' author='Mysorrows' date='2019-04-08T12:11:25Z'>
		&lt;denchmark-link:https://github.com/Mysorrows&gt;@Mysorrows&lt;/denchmark-link&gt;
 if you want the best results forget transfer learning, just train normally. Have you even visited the tutorials? We have examples that train on 1 image, 10 images, 100 images, etc.
&lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='Mysorrows' date='2019-04-08T13:42:13Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
   Yes, I have already seen these instructions.  I've seen everything about &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki&gt;https://github.com/ultralytics/yolov3/wiki&lt;/denchmark-link&gt;
 and it helped me.  Transfer of learning learned from it.
Perhaps because all the data are from coco, the results are excellent.  I also expect to be able to train the same effect on my own data set, but I don't know why the result is frustrating.
		</comment>
		<comment id='10' author='Mysorrows' date='2019-04-08T17:46:20Z'>
		&lt;denchmark-link:https://github.com/Mysorrows&gt;@Mysorrows&lt;/denchmark-link&gt;
 clearly the outlier here is your data. This repo trained on xView data as well with great results, so . This is on you to resolve, it's not our responsibility to hold your hand and debug your custom data for you.
&lt;denchmark-link:https://github.com/ultralytics/xview-yolov3&gt;https://github.com/ultralytics/xview-yolov3&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/55745360-5b1f7480-5a37-11e9-967c-a25a2a2e752c.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='Mysorrows' date='2019-04-09T02:41:31Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 . Yes, I know.  However, I am not sure whether the problem is a data set problem or fine tuning.  My data format is cls,x,y,w,h (they are floating point numbers less than 1).  I am looking for a solution to the problem.
		</comment>
		<comment id='12' author='Mysorrows' date='2019-04-09T10:27:08Z'>
		&lt;denchmark-link:https://github.com/Mysorrows&gt;@Mysorrows&lt;/denchmark-link&gt;
 maybe some good news for you. Another client was having problems, so we created an update that plots the first batch of training images overlaid with the target boxes, and repeats this at test time with the test images. If you  from your yolov3 directory you should have the latest updates (or simply git clone a clean copy). Then simply train and test for one epoch and post your pictures here. The pictures are saved as  and .
Then we can verify that your training data is correctly formatted.
		</comment>
		<comment id='13' author='Mysorrows' date='2019-04-09T12:04:55Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 .  Above is ‘train_batch0.jpg‘，The following is ’test_batch0.jpg' .
&lt;denchmark-link:https://user-images.githubusercontent.com/29940894/55798889-9a66c780-5b02-11e9-8104-275a74ab5626.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/29940894/55798899-9fc41200-5b02-11e9-94bd-ff53108c8783.jpg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='Mysorrows' date='2019-04-09T12:11:32Z'>
		&lt;denchmark-link:https://github.com/Mysorrows&gt;@Mysorrows&lt;/denchmark-link&gt;
 Perfect, so it seems like your labels are in the correct format. Why is there only 1 picture per batch though? The default batch size is 16, you should probably stick with that.

My training dataset has only 200 pictures. Is this a small ML task? I use it to train and test pedestrians (people standing).

The other problem could simply be your volume of data. 200 pictures is very few to expect the network to generalize from. Before trying to test on a seperate dataset (I see your train and test images are different), you want the model to show good results (overfitting) by training and testing on the same dataset. All of our tutorials do this.
		</comment>
		<comment id='15' author='Mysorrows' date='2019-04-09T12:23:22Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 . Because YOLO v3. pt has detected people, I hope it can detect people walking.  And that's why I used Transfer learning.
I modify the modified items locally and then push them to the cloud host for execution, so the batch size is only 1.
I have 200 pictures to train and 100 pictures to test.  If possible, I don't want it to appear over-fitting, but in fact, it is currently in this situation.
		</comment>
		<comment id='16' author='Mysorrows' date='2019-04-09T12:32:59Z'>
		Oh my goodness. Of course it can detect people walking. If I go on my balcony right now in Madrid and snap a picture of the street below me with our iPhone app it easily picks up all the pedestrians. You don't need to retrain for this, its already done and working:
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/55800491-2281a880-5ad4-11e9-9a9c-8b30e7c9d394.jpeg&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='Mysorrows' date='2019-04-09T12:48:05Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 . It should be inappropriate for me to describe it. I only want to detect  people walking, not all forms of people.  Not sitting people, or other classes.
And it's not because of wh_loss, I found it had a problem earlier and changed it to power loss.I had a problem with my dataset before.
		</comment>
		<comment id='18' author='Mysorrows' date='2019-04-09T13:25:18Z'>
		Ah I see. This repo is not appropriate for your needs then, and neither is your dataset. You want action detection, not object detection. You need a different sort of dataset for this, videos of the actions for training, with time replacing the RGB dimension in the net architecture.
We've had interest in this in the past, but no funding, and thus we haven't developed it.
		</comment>
		<comment id='19' author='Mysorrows' date='2019-04-10T01:33:35Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 . Yes, you are right.  But for a picture, this should be object detection.  When a person is standing, if his legs are open, he is probably walking.
		</comment>
		<comment id='20' author='Mysorrows' date='2019-04-10T11:43:25Z'>
		&lt;denchmark-link:https://github.com/Mysorrows&gt;@Mysorrows&lt;/denchmark-link&gt;
 we've solved a bug that caused poor performance. I commented this to you previously. You can see &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/197&gt;#197&lt;/denchmark-link&gt;
 for more information. As there seems to be no technical bug here I will close the issue.
		</comment>
		<comment id='21' author='Mysorrows' date='2019-04-10T11:58:51Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 . I have another problem. When I call the functions compute_loss and build_target, if ’wh yolo loss’ and ’yolo_method’ are used, this makes my program unable to continue to execute, which is similar to the state of downtime. However, using ‘wh power loss‘ and ’power method’ will not cause this problem.
I have updated the code to the latest version.  I asked before, but there seems to be no good answer.
		</comment>
		<comment id='22' author='Mysorrows' date='2019-04-10T14:56:01Z'>
		&lt;denchmark-link:https://github.com/Mysorrows&gt;@Mysorrows&lt;/denchmark-link&gt;
 to start off, I strongly suggest you simply  a clean copy of the repo and retry your training under default settings, since we have fixed a couple bugs lately.
 this doesn't work, and you suspect the reason is because your  loss is diverging, then yes you can swap 'yolo wh' for 'power wh', which is a much more stable  implementation. This change needs to take place in 3 areas of the code. Details are in &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/168&gt;#168&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='23' author='Mysorrows' date='2019-04-10T15:03:59Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 . Thank you, I will try it.
		</comment>
	</comments>
</bug>