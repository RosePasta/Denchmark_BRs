<bug id='255' author='zrenBruins' open_date='2019-05-03T06:05:13Z' closed_time='2019-07-25T01:25:54Z'>
	<summary>bug of training with "--evolve" option (training on a single machine with multi-GPU)</summary>
	<description>
1. Describe the bug
When training on a multi-GPU single machine, e.g., "python3 train.py --evolve --data data/coco_100img.data  --epochs 3", it will report error "RuntimeError: trying to initialize the default process group twice!" at the second evolve cycle when initializing the pytorch distributed training.
I tried to add "dist.destroy_process_group()" at the end of the train() function. But it still reports error "RuntimeError: Address already in use".
2. To Reproduce
Steps to reproduce the behavior:
1). use a multi-gpu single machine
2). run script "python3 train.py --evolve --data data/coco_100img.data  --epochs 3"
3). will see the error
3. Expected behavior
It should clear GPU memory at the end of every distributed training function. When evolving into the next generation (cycle), the dist.init_process_group() should work and start training for the next cycle.
4. Desktop (please complete the following information):

OS: Ubuntu
Version 18.04
PyTorch 1.0.1 (Glenn also noticed this problem in PyTorch 1.1)
Python 3.7

This problem is related to pytorch distributed package but I cannot figure it out.
Thank you so much for the help!
	</description>
	<comments>
		<comment id='1' author='zrenBruins' date='2019-05-03T16:20:20Z'>
		&lt;denchmark-link:https://github.com/zrenBruins&gt;@zrenBruins&lt;/denchmark-link&gt;
 this seems to be an issue affecting PyTorch 1.1 with train.py when using one or more GPUs. Are you receiving this error in PyTorch 1.0 also?
		</comment>
		<comment id='2' author='zrenBruins' date='2019-05-03T18:09:52Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Yes, I'm using PyTorch "1.0.1.post2". Do you have a clue how to solve this PyTorch related issue? Also, which PyTorch version should it work fine with?
		</comment>
		<comment id='3' author='zrenBruins' date='2019-05-03T20:39:35Z'>
		&lt;denchmark-link:https://github.com/zrenBruins&gt;@zrenBruins&lt;/denchmark-link&gt;
 I just tested with PyTorch 1.1 on GCP, everying seems fine using your command:
python3 train.py --evolve --data data/coco_100img.data --num-workers 2 --epochs 3
Namespace(accumulate=1, backend='nccl', batch_size=16, cfg='cfg/yolov3-spp.cfg', data_cfg='data/coco_100img.data', dist_url='tcp://127.0.0.1:9999', epochs=3, evolve=True, img_size=416, multi_scale=False, nosave=False, notest=False, num_workers=2, rank=0, resume=False, transfer=False, var=0, world_size=1)
Using CUDA device0 _CudaDeviceProperties(name='Tesla P4', total_memory=7611MB)
           device1 _CudaDeviceProperties(name='Tesla P4', total_memory=7611MB)

   Epoch       Batch        xy        wh      conf       cls     total  nTargets      time
     0/2         0/6      5.61      2.04       283      25.8       317        75      8.43
     0/2         1/6      6.05      2.11       283      25.9       318       132      0.67
     0/2         2/6      5.91      2.08       279        26       313       130     0.865
     0/2         3/6      5.91      1.98       233      25.9       266        84     0.655
     0/2         4/6      5.87      1.93       194        26       228        99     0.639
     0/2         5/6      5.88       1.9       166      25.9       199       139     0.651
     0/2         6/6      5.23      1.68       143      23.1       173        18      5.39

   Epoch       Batch        xy        wh      conf       cls     total  nTargets      time
     1/2         0/6       5.6      1.46      11.5      25.2      43.7        72      1.32
     1/2         1/6      5.82      1.49        14      25.3      46.6       135     0.666
     1/2         2/6      5.99      1.49      14.4      25.2      47.1       130     0.796
     1/2         3/6      5.69      1.39        13      25.1      45.2        84      0.73
...
		</comment>
		<comment id='4' author='zrenBruins' date='2019-05-03T20:43:26Z'>
		&lt;denchmark-link:https://github.com/zrenBruins&gt;@zrenBruins&lt;/denchmark-link&gt;
 ah yes, I see your error message further down after one training cycle completes. I'll leave the issue open. For now,  with 1 GPU seems to work well on PyTorch 1.1.
		</comment>
		<comment id='5' author='zrenBruins' date='2019-05-03T21:15:25Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thank you Glenn! May I know when you run it with no problem previously, which PyTorch version did you use?
		</comment>
		<comment id='6' author='zrenBruins' date='2019-05-03T21:20:35Z'>
		&lt;denchmark-link:https://github.com/zrenBruins&gt;@zrenBruins&lt;/denchmark-link&gt;
 I've been running it with PyTorch 1.1, but I saw your error message appear after the first training had been completed and it attempted to start the second.
When I tried with 1 GPU using PyTorch 1.1 everything worked well. This may not be a new issue, I think it is just that we have never run hyperparameter evolution with more than 1 GPU before!
		</comment>
		<comment id='7' author='zrenBruins' date='2019-05-03T21:23:32Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Got it. Thank you!
		</comment>
		<comment id='8' author='zrenBruins' date='2019-07-25T00:16:09Z'>
		&lt;denchmark-link:https://github.com/zrenBruins&gt;@zrenBruins&lt;/denchmark-link&gt;
 this issue should be resolved now. The problem was the distributed process group was not being destroyed at the end of the training. We have added code to do this now on L318 below. Can you test again?



yolov3/train.py


        Lines 315 to 321
      in
      169d117






 



 # Report time 



 print('%g epochs completed in %.3f hours.' % (epoch - start_epoch + 1, (time.time() - t0) / 3600)) 



 dist.destroy_process_group() if torch.cuda.device_count() &gt; 1 else None 



 torch.cuda.empty_cache() 



 return results 



 





		</comment>
		<comment id='9' author='zrenBruins' date='2019-07-25T01:25:54Z'>
		Thank you a lot &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 ! It's working now. :)
		</comment>
	</comments>
</bug>