<bug id='891' author='Hwijune' open_date='2020-03-04T07:18:39Z' closed_time='2020-04-20T08:11:50Z'>
	<summary>Segmentation fault (core dumped)</summary>
	<description>
hi, &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

There is always a segmentation fault error.
Sometimes it's over 1 epoch, but it's mostly stopped.
How can I fix it?
or Before I become 1 epoch, i want to save in schedule iteration
My environment
= pytorch 1.4.0, python 3.7.6, nvidia driver 440, cuda 10.2, opencv 4.1.2
&lt;denchmark-code&gt; Namespace(accumulate=1, adam=False, arc='default', batch_size=64, bucket='', cache_images=False, cfg='/home/phj8498/darknet/cfg/yolov3-spp3-mid-mask.cfg', data=   '/home/phj8498/darknet/data/3class_mid.data', device='0,1,2,3,4,5,6,7', epochs=5, evolve=False, img_size=[608], multi_scale=False, name='', nosave=False, notest   ='True', rect='True', resume='True', single_cls=False, var=None, weights='weights/last.pt')
&lt;/denchmark-code&gt;

Using CUDA Apex device0 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
device1 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
device2 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
device3 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
device4 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
device5 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
device6 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
device7 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)
&lt;denchmark-code&gt; Caching labels (1.63932e+06 found, 0 missing, 85598 empty, 68 duplicate, for 1.72492e+06 images): 100%|████████████| 1724917/1724917 [02:29&lt;00:00, 11543.61it/s]

 Caching labels (3053 found, 0 missing, 1544 empty, 0 duplicate, for 4597 images): 100%|██████████████████████████████████| 4597/4597 [00:00&lt;00:00, 14925.84it/s]
&lt;/denchmark-code&gt;

Model Summary: 225 layers, 6.38728e+07 parameters, 6.38728e+07 gradients
Using 8 dataloader workers
Starting training for 5 epochs...
&lt;denchmark-code&gt; Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size

 1/4     6.19G      2.82     0.779      5.83      9.43       284       608:  54%|████████████████▊              | 14644/26952 [4:23:07&lt;3:45:40,  1.10s/it] Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Hwijune' date='2020-03-04T08:03:31Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;
 see &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/725&gt;#725&lt;/denchmark-link&gt;
, may be related.
		</comment>
		<comment id='2' author='Hwijune' date='2020-03-04T08:43:56Z'>
		
@Hwijune see #725, may be related.

&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  thank you, i will test
&lt;denchmark-code&gt; #from torch.utils.tensorboard import SummaryWriter
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Hwijune' date='2020-03-04T16:42:13Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;
 yes that's a good idea. Did that produce any change for you?
		</comment>
		<comment id='4' author='Hwijune' date='2020-03-05T02:23:33Z'>
		
@Hwijune yes that's a good idea. Did that produce any change for you?

I passed the 1 epoch, but it was stopped.
I think it should be saved every 1000 iteration.
so that it can continue even if it stops.
		</comment>
		<comment id='5' author='Hwijune' date='2020-03-05T17:36:35Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;
 you can add this functionality by copying this code and placing it within your own logic:


Multi-GPU works well in VM instances we've tested using V100's, but we have not tested multi-GPU using 2080ti cards, so its possible the problem may be originating there. Do you see it when running a single GPU?
		</comment>
		<comment id='6' author='Hwijune' date='2020-03-06T02:52:35Z'>
		
@Hwijune you can add this functionality by copying this code and placing it within your own logic:



yolov3/train.py


        Lines 341 to 354
      in
      8b6c8a5






 save = (not opt.nosave) or (final_epoch and not opt.evolve) 



 if save: 



 with open(results_file, 'r') as f: 



 # Create checkpoint 



 chkpt = {'epoch': epoch, 



 'best_fitness': best_fitness, 



 'training_results': f.read(), 



 'model': model.module.state_dict() if type( 



 model) is nn.parallel.DistributedDataParallel else model.state_dict(), 



 'optimizer': None if final_epoch else optimizer.state_dict()} 



 



 # Save last checkpoint 



 torch.save(chkpt, last) 



 





Multi-GPU works well in VM instances we've tested using V100's, but we have not tested multi-GPU using 2080ti cards, so its possible the problem may be originating there. Do you see it when running a single GPU?

&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 thanks.  I'll test it

single 2080ti gpu test
change train.py code test

		</comment>
		<comment id='7' author='Hwijune' date='2020-03-14T16:53:37Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;
 I have the same setup as you (e.g., 4 2080ti GPU's) and am seeing similar errors. If I reduce the batch dramatically (down to 4), I can get it to run. But even so, it often crashes somewhat randomly. I would like to train on larger batch sizes and higher-res images if possible. Curious if you have made any progress you can share?
		</comment>
		<comment id='8' author='Hwijune' date='2020-03-14T21:55:47Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;
 did commenting out the TF summarywriter fix the issue for you?
		</comment>
		<comment id='9' author='Hwijune' date='2020-03-16T05:19:57Z'>
		
@Hwijune did commenting out the TF summarywriter fix the issue for you?

&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/porterjenkins&gt;@porterjenkins&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;#from torch.utils.tensorboard import SummaryWriter
&lt;/denchmark-code&gt;

TF Summary test didn't do well.
it's same. I don't know what to do yet.
		</comment>
		<comment id='10' author='Hwijune' date='2020-03-16T05:27:17Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;
 I had similar issues on a multi 2080ti machine, ended up "solving" it by just training on a single GPU, i.e. .
		</comment>
		<comment id='11' author='Hwijune' date='2020-03-17T01:46:32Z'>
		
@Hwijune I had similar issues on a multi 2080ti machine, ended up "solving" it by just training on a single GPU, i.e. train.py --device 0.

How can I use multi gpus?
When tested, the time was similar regardless of the number of gpu.
1epoch -&gt; 2, 4, 8 gpus
		</comment>
		<comment id='12' author='Hwijune' date='2020-03-17T02:13:07Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;
 multi-gpu:
python3 train.py --device 0,1
python3 train.py --device 0,1,2,3
etc.
If 2/4/8 GPU epoch takes same amount of time you are dataloader limited, need faster SSD, more CPU cores, etc. For fastest dataloading cache all images:
python3 train.py --cache
		</comment>
		<comment id='13' author='Hwijune' date='2020-03-17T04:45:07Z'>
		
@Hwijune 멀티 GPU :
python3 train.py-장치 0,1
python3 train.py-장치 0,1,2,3
기타
2/4/8 GPU epoch가 데이터 로더 제한 시간과 동일한 시간이 걸리면 더 빠른 SSD, 더 많은 CPU 코어 등이 필요합니다. 가장 빠른 데이터로드를 위해 모든 이미지를 캐시하십시오.
python3 train.py --cache

Server specification
ddr4 256g ram
1.8T ssd
intel xeon silver 4214 cpu @ 2.20ghz
&lt;denchmark-code&gt; nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8])  # number of workers
&lt;/denchmark-code&gt;

adjust the maximum value of the data loader?
		</comment>
		<comment id='14' author='Hwijune' date='2020-03-17T10:36:15Z'>
		&lt;denchmark-link:https://github.com/Hwijune&gt;@Hwijune&lt;/denchmark-link&gt;

 depends on the number of GPUs you have.
Pytorch developers suggest the following 
		</comment>
		<comment id='15' author='Hwijune' date='2020-03-18T04:03:42Z'>
		
@Hwijune
Number of workers depends on the number of GPUs you have.
Pytorch developers suggest the following workers = (4 * GPU_count)

&lt;denchmark-link:https://github.com/Lornatang&gt;@Lornatang&lt;/denchmark-link&gt;
 oh good. i'll test it
&lt;denchmark-code&gt;workers = (4 * GPU_count)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='16' author='Hwijune' date='2020-04-20T00:06:54Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
		<comment id='17' author='Hwijune' date='2020-04-20T08:12:50Z'>
		Less frequent after installing the latest version.
However, errors continue to occur
		</comment>
	</comments>
</bug>