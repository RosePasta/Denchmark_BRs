<bug id='377' author='92ypli' open_date='2019-07-18T11:01:40Z' closed_time='2019-07-27T00:55:35Z'>
	<summary>multi-scale training automatically stops, without error.</summary>
	<description>
Describe the bug
A clear and concise description of what the bug is.
multi-scale training automatically stops, without error.

&lt;denchmark-link:https://user-images.githubusercontent.com/22673185/61452464-5c453380-a98e-11e9-9877-13bc88ff1e93.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='92ypli' date='2019-07-18T15:47:26Z'>
		&lt;denchmark-link:https://github.com/92ypli&gt;@92ypli&lt;/denchmark-link&gt;
 the  you show in your workspace is a keyboard interrupt. Are you sure you didn't cause this yourself?
In any case without an error message there's obviously nothing for us to act on.
		</comment>
		<comment id='2' author='92ypli' date='2019-07-19T01:54:04Z'>
		Yes, many times it stops automatically, it seems to stop when changing the image scale。
platform：
2 titanx
batchsize：20
another question：how to set training hyperparameters?
		</comment>
		<comment id='3' author='92ypli' date='2019-07-20T10:51:49Z'>
		&lt;denchmark-link:https://github.com/92ypli&gt;@92ypli&lt;/denchmark-link&gt;
 I don't have any answer for the stop in training. Training should only ever stop if nan is detected, and then it will print an error explaining the stop to the screen.
Training hyperparameters are set at the beginning of train.py:



yolov3/train.py


        Lines 28 to 40
      in
      d6edefa






 # Training hyperparameters d 



 hyp = {'giou': 1.153,  # giou loss gain 



 'xy': 4.062,  # xy loss gain 



 'wh': 0.1845,  # wh loss gain 



 'cls': 24.28,  # cls loss gain 



 'cls_pw': 3.05,  # cls BCELoss positive_weight 



 'obj': 20.93,  # obj loss gain 



 'obj_pw': 2.842,  # obj BCELoss positive_weight 



 'iou_t': 0.2759,  # iou target-anchor training threshold 



 'lr0': 0.001357,  # initial learning rate 



 'lrf': -4.,  # final learning rate = lr0 * (10 ** lrf) 



 'momentum': 0.916,  # SGD momentum 



 'weight_decay': 0.000572}  # optimizer weight decay 





		</comment>
		<comment id='4' author='92ypli' date='2019-07-22T01:32:12Z'>
		How do you get these hyperparameters?
		</comment>
		<comment id='5' author='92ypli' date='2019-07-22T09:29:11Z'>
		i have the same problem.many times, when changing the image scale, it stops automatically. Have you solved the problem?
		</comment>
		<comment id='6' author='92ypli' date='2019-07-22T19:21:59Z'>
		&lt;denchmark-link:https://github.com/92ypli&gt;@92ypli&lt;/denchmark-link&gt;
 the parameters are evolved using .
&lt;denchmark-link:https://github.com/Queeniedoge&gt;@Queeniedoge&lt;/denchmark-link&gt;
 can you supply a minimum reproducible example of your issue?
		</comment>
		<comment id='7' author='92ypli' date='2019-07-23T13:03:03Z'>
		&lt;denchmark-link:https://github.com/92ypli&gt;@92ypli&lt;/denchmark-link&gt;
 a second person has raised an issue about training stopping when using multiple GPUs: &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/386&gt;#386&lt;/denchmark-link&gt;

Perhaps this issue could be multi-GPU related, and not multi-scale related? We successfully trained one epoch with 2 V100 GPUs just a couple days ago on a GCP instance, so we seem unable to reproduce the issue.
		</comment>
		<comment id='8' author='92ypli' date='2019-07-23T15:16:37Z'>
		&lt;denchmark-link:https://github.com/92ypli&gt;@92ypli&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Queeniedoge&gt;@Queeniedoge&lt;/denchmark-link&gt;
 see &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/386#issuecomment-514225498&gt;#386 (comment)&lt;/denchmark-link&gt;
 for a possible solution involving changing BIOS settings and disabling IOMMU.
		</comment>
		<comment id='9' author='92ypli' date='2019-07-27T00:55:32Z'>
		setting bs:20, two titanx, is ok. &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , thanks you.
		</comment>
	</comments>
</bug>