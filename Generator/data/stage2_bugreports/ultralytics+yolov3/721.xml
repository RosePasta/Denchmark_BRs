<bug id='721' author='wallcuber' open_date='2019-12-17T10:22:31Z' closed_time='2020-01-16T19:17:02Z'>
	<summary>run train.py, it says "AttributeError: module 'torch.distributed' has no attribute 'init_process_group'"</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

A clear and concise description of what the bug is.
my computer has four gpus. When I run train.py, it says"AttributeError: module 'torch.distributed' has no attribute 'init_process_group'". But it obviously has this attribute.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

run train.py

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

A clear and concise description of what you expected to happen.
AttributeError: module 'torch.distributed' has no attribute 'init_process_group'
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.
pytorch  1.3
Desktop (please complete the following information):

OS: [e.g. iOS]  win10
Version [e.g. 22]

Smartphone (please complete the following information):

Device: [e.g. iPhoneXS]
OS: [e.g. iOS8.1]
Version [e.g. 22]

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='wallcuber' date='2019-12-17T10:26:16Z'>
		train.py  line 183
		</comment>
		<comment id='2' author='wallcuber' date='2019-12-17T14:21:18Z'>
		You are probably on Windows with more than 1 GPU in your computer (or a Mac with more then 1 GPU and an older version of Pytorch). Pytorch doesn't support multi GPU training on Windows, see &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/20380&gt;pytorch/pytorch#20380&lt;/denchmark-link&gt;
 and this repo only check if you have more then 1 GPU to try to init a distributed training.
Change line 182 and line 397 :
if device.type != 'cpu' and torch.cuda.device_count() &gt; 1:
dist.destroy_process_group() if torch.cuda.device_count() &gt; 1 else None
to this :
if device.type != 'cpu' and torch.cuda.device_count() &gt; 99:
dist.destroy_process_group() if torch.cuda.device_count() &gt; 99 else None
		</comment>
		<comment id='3' author='wallcuber' date='2019-12-17T14:50:20Z'>
		I opened the feature request officialy on Pytorch repo, feel free to comment on it if thats something you would like added to the framework :
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/31363&gt;pytorch/pytorch#31363&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='wallcuber' date='2019-12-18T08:27:28Z'>
		My computer is Windows10 system.I change the code like what you said. Does this mean I'm using 4 gpus to train my   data?&lt;denchmark-link:https://github.com/Ownmarc&gt;@Ownmarc&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/58249279/71069063-f7973a80-21b2-11ea-9102-9906b12aa2b2.PNG&gt;&lt;/denchmark-link&gt;

I understand what you mean now. Is there any other way to use more than 1 gpu to train?
		</comment>
		<comment id='5' author='wallcuber' date='2019-12-18T12:40:10Z'>
		No, its only showing you that you have 4 visible gpus. Only 1 is being used for your training.
There is no way to use multi gpu for the same training on Windows until Pytorch adds support for it. You can still launch 4 different training on the 4 gpus if you have enough RAM, CPU and a decent cooling system. Could be great to do hyperparameters search.
If you really want to do multi gpu training, your best bet is to boot in linux (a vm or a docker on windows wont work since they wont have access to the gpus)
		</comment>
		<comment id='6' author='wallcuber' date='2019-12-19T01:38:07Z'>
		
No, its only showing you that you have 4 visible gpus. Only 1 is being used for your training.
There is no way to use multi gpu for the same training on Windows until Pytorch adds support for it. You can still launch 4 different training on the 4 gpus if you have enough RAM, CPU and a decent cooling system. Could be great to do hyperparameters search.
If you really want to do multi gpu training, your best bet is to boot in linux (a vm or a docker on windows wont work since they wont have access to the gpus)

thank you
		</comment>
		<comment id='7' author='wallcuber' date='2020-01-16T19:17:02Z'>
		I'll close this issue for now as the original issue appears to have been resolved, and/or no activity has been seen for some time. Feel free to comment if this is not the case.
		</comment>
		<comment id='8' author='wallcuber' date='2020-07-21T07:42:32Z'>
		
You are probably on Windows with more than 1 GPU in your computer (or a Mac with more then 1 GPU and an older version of Pytorch). Pytorch doesn't support multi GPU training on Windows, see pytorch/pytorch#20380 and this repo only check if you have more then 1 GPU to try to init a distributed training.
Change line 182 and line 397 :
if device.type != 'cpu' and torch.cuda.device_count() &gt; 1:
dist.destroy_process_group() if torch.cuda.device_count() &gt; 1 else None
to this :
if device.type != 'cpu' and torch.cuda.device_count() &gt; 99:
dist.destroy_process_group() if torch.cuda.device_count() &gt; 99 else None

thank you
		</comment>
	</comments>
</bug>