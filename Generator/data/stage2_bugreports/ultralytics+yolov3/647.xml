<bug id='647' author='MengtianLee' open_date='2019-11-22T08:57:37Z' closed_time='2020-01-31T06:52:13Z'>
	<summary>About maps</summary>
	<description>
&lt;denchmark-link:https://user-images.githubusercontent.com/42642078/69411124-49c97500-0d47-11ea-8e60-8617a59b3df8.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/42642078/69411167-636abc80-0d47-11ea-963c-1b854ca4a1c4.png&gt;&lt;/denchmark-link&gt;

When i trained on the net, i found that the map and precision dramatically decreased and maintain a low level during several epochs.However, everything soon became normal . I don't know whether this phenomenon is normal? If it is wrong, how can i figure it out? Please help me solve this issues, thanks a lot.
	</description>
	<comments>
		<comment id='1' author='MengtianLee' date='2019-11-22T23:21:13Z'>
		&lt;denchmark-link:https://github.com/MengtianLee&gt;@MengtianLee&lt;/denchmark-link&gt;
 looks fine, your mAP is not going to get any better than what you have there.
		</comment>
		<comment id='2' author='MengtianLee' date='2019-11-23T00:48:20Z'>
		It is completely normal. Although my personal opinion is that you might be overtraining at some point. If you are running +300 epochs and the model shows no real changes for a few epochs. It's usually a clear sign of overtraining.
		</comment>
		<comment id='3' author='MengtianLee' date='2019-11-23T02:08:38Z'>
		Yes, what &lt;denchmark-link:https://github.com/FranciscoReveriano&gt;@FranciscoReveriano&lt;/denchmark-link&gt;
 said also.
		</comment>
		<comment id='4' author='MengtianLee' date='2019-11-23T10:20:12Z'>
		
It is completely normal. Although my personal opinion is that you might be overtraining at some point. If you are running +300 epochs and the model shows no real changes for a few epochs. It's usually a clear sign of overtraining.

Thank you a lot. I guess that i should reduce the number of epoch to approximately 500, for 500 epochs is completely enough.
		</comment>
		<comment id='5' author='MengtianLee' date='2019-11-23T10:27:34Z'>
		
@MengtianLee looks fine, your mAP is not going to get any better than what you have there.

I suddenly feel relieved. BTW, you contribution is fantastic, and very useful for me.
		</comment>
		<comment id='6' author='MengtianLee' date='2019-11-23T15:26:14Z'>
		If you are doing 500 that is probably to much. Make sure you check your hyperparameters and study on how not to overfit.
		</comment>
		<comment id='7' author='MengtianLee' date='2020-01-29T12:36:30Z'>
		
Yes, what @FranciscoReveriano said also.

During the training the val classification and val objectiveness is higher, it is correct?and why?BTW, I use the best weight on test datasets, it perform well.
		</comment>
		<comment id='8' author='MengtianLee' date='2020-01-31T06:52:10Z'>
		

Yes, what @FranciscoReveriano said also.

During the training the val classification and val objectiveness is higher, it is correct?and why?BTW, I use the best weight on test datasets, it perform well.

I know why, i add APs of every class in the results.txt. So I plotted AP instead of val_loss. That's a stupid mistake.
		</comment>
	</comments>
</bug>