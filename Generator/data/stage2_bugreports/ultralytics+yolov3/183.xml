<bug id='183' author='RooneyYangsm' open_date='2019-04-02T11:30:20Z' closed_time='2019-04-09T08:53:00Z'>
	<summary>Same weights produce better inference results than darknet.</summary>
	<description>
I 've compared Yolo3 pytorch implementation result and darknet result with zidane.jpg
&lt;denchmark-link:https://github.com/ultralytics/yolov3.git&gt;https://github.com/ultralytics/yolov3.git&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/pjreddie/darknet&gt;https://github.com/pjreddie/darknet&lt;/denchmark-link&gt;

there are slight score difference.
pytorch implementation : person 1.00 person 1.00 tie 0.93 but
darknet implementation : tie: 90% persion 100% persion 98%
I expect same inference result because darknet and pytorch yolov3 used same weight param.
Is there any reason for the different result
&lt;denchmark-link:https://user-images.githubusercontent.com/49194138/55399443-1783c100-5586-11e9-9b5e-27f9ac65f6c8.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/49194138/55399462-1f436580-5586-11e9-93ba-a0e462e75447.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='RooneyYangsm' date='2019-04-02T11:48:05Z'>
		&lt;denchmark-link:https://github.com/RooneyYangsm&gt;@RooneyYangsm&lt;/denchmark-link&gt;
 yes the boxes are different because  To improve over darknet, we have to change the boxes buddy. This is not a bug, this is an enhancement (you're welcome). You can see the results in our README in the mAP section:
&lt;denchmark-link:https://github.com/ultralytics/yolov3#map&gt;https://github.com/ultralytics/yolov3#map&lt;/denchmark-link&gt;





ultralytics/yolov3
darknet




YOLOv3 320
51.8
51.5


YOLOv3 416
55.4
55.3


YOLOv3 608
58.2
57.9


YOLOv3-spp 320
52.4
-


YOLOv3-spp 416
56.5
-


YOLOv3-spp 608
60.7
60.6



sudo rm -rf yolov3 &amp;&amp; git clone https://github.com/ultralytics/yolov3
# bash yolov3/data/get_coco_dataset.sh
sudo rm -rf cocoapi &amp;&amp; git clone https://github.com/cocodataset/cocoapi &amp;&amp; cd cocoapi/PythonAPI &amp;&amp; make &amp;&amp; cd ../.. &amp;&amp; cp -r cocoapi/PythonAPI/pycocotools yolov3
cd yolov3
 
python3 test.py --save-json --conf-thres 0.001 --img-size 608 --batch-size 16
Namespace(batch_size=16, cfg='cfg/yolov3.cfg', conf_thres=0.001, data_cfg='cfg/coco.data', img_size=608, iou_thres=0.5, nms_thres=0.5, save_json=True, weights='weights/yolov3.weights')
Using cuda _CudaDeviceProperties(name='Tesla V100-SXM2-16GB', major=7, minor=0, total_memory=16130MB, multi_processor_count=80)
      Image      Total          P          R        mAP
Calculating mAP: 100%|█████████████████████████████████| 313/313 [08:54&lt;00:00,  1.55s/it]
       5000       5000     0.0966      0.786      0.579
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.331
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.582
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.344
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.198
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.427
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.281
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.437
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.494
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.577
 
python3 test.py --weights weights/yolov3-spp.weights --cfg cfg/yolov3-spp.cfg --save-json --img-size 608 --batch-size 8
Namespace(batch_size=8, cfg='cfg/yolov3-spp.cfg', conf_thres=0.001, data_cfg='data/coco.data', img_size=608, iou_thres=0.5, nms_thres=0.5, save_json=True, weights='weights/yolov3-spp.weights')
Using cuda _CudaDeviceProperties(name='Tesla V100-SXM2-16GB', major=7, minor=0, total_memory=16130MB, multi_processor_count=80)
       Image      Total          P          R        mAP
Calculating mAP: 100%|█████████████████████████████████| 625/625 [07:01&lt;00:00,  1.56it/s]
       5000       5000       0.12       0.81      0.611
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.607
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.386
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.207
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.391
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.485
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.296
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.464
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.494
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.331
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.517
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.618
		</comment>
		<comment id='2' author='RooneyYangsm' date='2019-04-09T08:53:00Z'>
		&lt;denchmark-link:https://github.com/RooneyYangsm&gt;@RooneyYangsm&lt;/denchmark-link&gt;
 I'm going to assume this issue is closed since you haven't responded within a week.
		</comment>
		<comment id='3' author='RooneyYangsm' date='2019-04-09T10:28:22Z'>
		Glenn-jocher.
I am studying the source to identify the improvement. If I understand the improvement, I will be back and leave comments.
		</comment>
		<comment id='4' author='RooneyYangsm' date='2019-04-15T02:07:08Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 - Nice work with this repo! When you say that this repo does better at inference than the original darknet repo, can you point me to the specific types of tweaks made here that helped achieve better inference performance? I don't think I saw anything detailed in the readme. Thanks!
		</comment>
	</comments>
</bug>