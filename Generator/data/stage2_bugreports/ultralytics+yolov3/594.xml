<bug id='594' author='zak834' open_date='2019-11-07T08:20:10Z' closed_time='2019-11-07T22:56:14Z'>
	<summary>CUDA Error: out of memory</summary>
	<description>
trian.py      CUDA Error: out of memory
	</description>
	<comments>
		<comment id='1' author='zak834' date='2019-11-07T08:36:43Z'>
		
trian.py CUDA Error: out of memory

I think you could try lower batchsize or change the inputshape of image .
		</comment>
		<comment id='2' author='zak834' date='2019-11-07T08:40:49Z'>
		

trian.py CUDA Error: out of memory

I think you could try lower batchsize or change the inputshape of image .

thanks~now ,batch=64,subdivisions=16.can i change to batch=1,subdivisions=1?
		</comment>
		<comment id='3' author='zak834' date='2019-11-07T08:47:03Z'>
		


trian.py CUDA Error: out of memory

I think you could try lower batchsize or change the inputshape of image .

thanks~now ,batch=64,subdivisions=16.can i change to batch=1,subdivisions=1?

I suggest that you could try batch-size=32,16,8 ... until you could run train.py.
		</comment>
		<comment id='4' author='zak834' date='2019-11-07T08:56:55Z'>
		



trian.py CUDA Error: out of memory

I think you could try lower batchsize or change the inputshape of image .

thanks~now ,batch=64,subdivisions=16.can i change to batch=1,subdivisions=1?

I suggest that you could try batch-size=32,16,8 ... until you could run train.py.

Thank you
		</comment>
		<comment id='5' author='zak834' date='2019-11-07T22:56:14Z'>
		&lt;denchmark-link:https://github.com/zak834&gt;@zak834&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Gaondong&gt;@Gaondong&lt;/denchmark-link&gt;
 to reduce memory usage change --batch-size and --accumulate arguments in train.py (not cfg file). For example try:

or for even lower memory (and equivalent training results):
python3 train.py --batch-size 4 --accumulate 16
		</comment>
		<comment id='6' author='zak834' date='2020-07-03T02:40:19Z'>
		Does accumulate have the same effect as subdivision in .cfg?Whether accumulate and subdivision represent one parameter?
		</comment>
	</comments>
</bug>