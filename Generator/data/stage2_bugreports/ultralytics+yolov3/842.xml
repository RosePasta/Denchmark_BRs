<bug id='842' author='alontrais' open_date='2020-02-11T09:46:20Z' closed_time='2020-05-13T00:09:04Z'>
	<summary>WARNING: non-finite loss, ending training  tensor([nan, nan, nan, nan], device='cuda:0')</summary>
	<description>
I cloned the newest version, when I run the train script I get this warning:
WARNING: non-finite loss, ending training  tensor([nan, nan, nan, nan], device='cuda:0')
Caching labels (20 found, 0 missing, 0 empty, 0 duplicate, for 20 images): 100%|█████████████████████| 20/20 [00:00&lt;00:00, 465.89it/s]
Caching labels (10 found, 0 missing, 0 empty, 0 duplicate, for 10 images): 100%|█████████████████████| 10/10 [00:00&lt;00:00, 468.26it/s]
Model Summary: 225 layers, 6.25787e+07 parameters, 6.25787e+07 gradients
Using 0 dataloader workers
Starting training for 2730 epochs...
&lt;denchmark-code&gt; Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
0/2729     14.9G      8.15   1.4e+03       111  1.52e+03       982   2.5e+03: 100%|███████████████| 20/20 [02:39&lt;00:00,  7.95s/it]
           Class    Images   Targets         P         R   mAP@0.5        F1: 100%|█████████████████| 5/5 [00:32&lt;00:00,  6.52s/it]
             all        10  1.08e+04         0         0         0         0

 Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
1/2729     14.9G      7.21  1.23e+03      82.3  1.32e+03  1.42e+03   2.5e+03: 100%|███████████████| 20/20 [02:14&lt;00:00,  6.71s/it]
           Class    Images   Targets         P         R   mAP@0.5        F1: 100%|█████████████████| 5/5 [00:11&lt;00:00,  2.24s/it]
             all        10  1.08e+04         0         0         0         0

 Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
2/2729     14.9G       7.7  5.06e+04      27.3  5.06e+04  1.31e+03   2.5e+03: 100%|███████████████| 20/20 [02:14&lt;00:00,  6.73s/it]
           Class    Images   Targets         P         R   mAP@0.5        F1: 100%|█████████████████| 5/5 [00:18&lt;00:00,  3.66s/it]
             all        10  1.08e+04   0.00031    0.0185  1.09e-05  0.000611
&lt;/denchmark-code&gt;

Model Bias Summary:    layer        regression        objectness    classification
89       0.00+/-0.02      -5.43+/-0.01      -5.00+/-0.01
101      -0.01+/-0.02      -5.83+/-0.05      -4.97+/-0.06
113      -0.02+/-0.10     -17.67+/-22.01     -2.75+/-2.99
&lt;denchmark-code&gt; Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
3/2729     14.9G      8.58  5.33e+04      24.8  5.33e+04  1.63e+03   2.5e+03: 100%|███████████████| 20/20 [02:14&lt;00:00,  6.72s/it]
           Class    Images   Targets         P         R   mAP@0.5        F1: 100%|█████████████████| 5/5 [00:11&lt;00:00,  2.21s/it]
             all        10  1.08e+04         0         0         0         0

 Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
4/2729     14.9G      9.05 -7.83e+03      31.4 -7.79e+03  1.24e+03   2.5e+03: 100%|███████████████| 20/20 [02:15&lt;00:00,  6.79s/it]
           Class    Images   Targets         P         R   mAP@0.5        F1: 100%|█████████████████| 5/5 [00:11&lt;00:00,  2.22s/it]
             all        10  1.08e+04         0         0         0         0

 Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
5/2729     14.9G      15.1  -3.9e+06  1.19e+03  -3.9e+06  1.74e+03   2.5e+03:  85%|████████████▊  | 17/20 [01:53&lt;00:20,  6.81s/it]WARNING: non-finite loss, ending training  tensor([nan, nan, nan, nan], device='cuda:0')
5/2729     14.9G      15.1  -3.9e+06  1.19e+03  -3.9e+06  1.74e+03   2.5e+03:  85%|████████████▊  | 17/20 [01:57&lt;00:20,  6.92s/it]
&lt;/denchmark-code&gt;

When I run the previous version a month ago, I don't get this warning
What could be the problem?
	</description>
	<comments>
		<comment id='1' author='alontrais' date='2020-02-11T17:21:23Z'>
		Hello, thank you for your interest in our work! This issue seems to lack the minimum requirements for a proper response, or is insufficiently detailed for us to help you. In general you can use &lt;denchmark-link:https://github.com/ultralytics/yolov3/compare&gt;https://github.com/ultralytics/yolov3/compare&lt;/denchmark-link&gt;
 to display differences between repository versions.
Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove existing
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, PyTorch &gt;= 1.3 etc. You can also use our Google Colab Notebook and our Docker Image to test your code in working environment.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
		</comment>
		<comment id='2' author='alontrais' date='2020-02-11T17:47:02Z'>
		&lt;denchmark-link:https://github.com/alontrais&gt;@alontrais&lt;/denchmark-link&gt;
 ah one thing pops out from your terminal output. Your images are very large, which is not a use case we have tested. The current objectness loss construction is probably causing this. You can try  on L372 when building your loss, which may help.


		</comment>
		<comment id='3' author='alontrais' date='2020-02-11T17:52:38Z'>
		&lt;denchmark-link:https://github.com/alontrais&gt;@alontrais&lt;/denchmark-link&gt;
 also, irrespective of the loss reduction, your obj loss is simply far too large. The 3 loss components should be more or less evenly balanced, so you can manually adjust (lower, in your case) your objetness loss hyperparameter on L28 to compensate:


		</comment>
		<comment id='4' author='alontrais' date='2020-02-13T03:10:42Z'>
		I encountered the same problem, it happened after changing the learning rate decay function, the hyperparameters suddenly increased during training
		</comment>
		<comment id='5' author='alontrais' date='2020-02-13T18:22:11Z'>
		&lt;denchmark-link:https://github.com/qwe3208620&gt;@qwe3208620&lt;/denchmark-link&gt;
 the hyperparameters are static during training except for the LR (and momentum if you use SGD). The LR scheduler modifies the LR, and a prebiasing step for the first 3 epochs increases bias-only LR and momentum.
		</comment>
		<comment id='6' author='alontrais' date='2020-03-06T11:02:16Z'>
		hi, I encountered the same problem when I trained my custom dataset, shown as the following pic.. and I lower the  giou weight and lr, but it doesn't work.
&lt;denchmark-link:https://user-images.githubusercontent.com/38133705/76078066-e1905d80-5fdc-11ea-9118-4bf0d5295d67.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='alontrais' date='2020-03-06T18:53:37Z'>
		&lt;denchmark-link:https://github.com/guancheng817&gt;@guancheng817&lt;/denchmark-link&gt;
 you might want to disable apex mixed precision training if you are using it.
		</comment>
		<comment id='8' author='alontrais' date='2020-03-29T04:29:40Z'>
		I encountered this problem too, I'm wondering if it's related to the empty label files or zero filled label files? &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

29/179     9.72G      5.06     0.906         0      5.97         5       608
29/179     9.72G      5.05      0.91         0      5.96        11       608
WARNING: non-finite loss, ending training  tensor([nan, nan, 0., nan], device='cuda:0')
		</comment>
		<comment id='9' author='alontrais' date='2020-03-29T05:23:54Z'>
		&lt;denchmark-link:https://github.com/yangxu351&gt;@yangxu351&lt;/denchmark-link&gt;
 no definitely not. If there are no labels then there is no loss other than obj loss.
If it occurs repeatably perhaps you could print the loss components and trace the origin? Your message says it occurs in the first two only, with the last loss being zero? The 3 are box, obj and cls.
So that means it's probably originating in the GIoU calculation, which feeds the obj loss as well.
		</comment>
		<comment id='10' author='alontrais' date='2020-03-29T05:26:09Z'>
		The giou computation is in utils.py. It has protected divides in place though, so I don't understand how it could generate an nan:
def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False):
    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4
    box2 = box2.t()

    # Get the coordinates of bounding boxes
    if x1y1x2y2:  # x1, y1, x2, y2 = box1
        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]
        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]
    else:  # transform from xywh to xyxy
        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2
        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2
        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2
        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2

    # Intersection area
    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \
            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)

    # Union Area
    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1
    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1
    union = (w1 * h1 + 1e-16) + w2 * h2 - inter

    iou = inter / union  # iou
    if GIoU or DIoU or CIoU:
        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width
        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height
        if GIoU:  # Generalized IoU https://arxiv.org/pdf/1902.09630.pdf
            c_area = cw * ch + 1e-16  # convex area
            return iou - (c_area - union) / c_area  # GIoU
		</comment>
		<comment id='11' author='alontrais' date='2020-03-29T14:06:08Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I noticed that all empty label files are filled with zero in the load_mosaic with labels = np.zeros((0, 5), dtype=np.float32).
WARNING: non-finite loss, ending training tensor([nan, nan, 0., nan], device='cuda:0')
for single-class task, the lcls is 0, but I cannot figure out why other loss will be nan
		</comment>
		<comment id='12' author='alontrais' date='2020-03-29T20:38:29Z'>
		&lt;denchmark-link:https://github.com/yangxu351&gt;@yangxu351&lt;/denchmark-link&gt;
 no those are not zeros, those are placeholders for the concatenation operation. a 0,5 array has no values.
		</comment>
		<comment id='13' author='alontrais' date='2020-03-31T17:56:15Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thanks for your reply. I'm still struggling with the nan losses. If the label file is empty, the tbox[i] is also empty when computing giou = bbox_iou(pbox.t(), tbox[i], x1y1x2y2=False, GIoU=True), is it ok if I set the giou=0 when the tbox[i] is empty?
		</comment>
		<comment id='14' author='alontrais' date='2020-03-31T18:30:07Z'>
		&lt;denchmark-link:https://github.com/yangxu351&gt;@yangxu351&lt;/denchmark-link&gt;
 giou loss is only generated for labels. If there is no label there is no GIoU loss, only obj loss.
		</comment>
		<comment id='15' author='alontrais' date='2020-04-04T00:56:32Z'>
		I encounter the same issue.
		</comment>
		<comment id='16' author='alontrais' date='2020-04-04T01:01:15Z'>
		&lt;denchmark-link:https://github.com/rinabuoy&gt;@rinabuoy&lt;/denchmark-link&gt;
 can you reliably reproduce the issue, and if so send us code? Thanks!
		</comment>
		<comment id='17' author='alontrais' date='2020-04-04T01:05:57Z'>
		I just pulled the latest version and trained a single-class detector. Whether I set mixed_precision = True or not, this error will appear.
&lt;denchmark-link:https://user-images.githubusercontent.com/9147160/78415308-17137f80-764b-11ea-8e50-c63caeae0326.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='alontrais' date='2020-04-04T01:13:31Z'>
		&lt;denchmark-link:https://github.com/rinabuoy&gt;@rinabuoy&lt;/denchmark-link&gt;
 thanks! Do you have a colab notebook that you could share so we can run to reproduce the error?
The error is caused because GIoU returns nan, though we can't reproduce it on COCO so its very hard/impossible to debug.
		</comment>
		<comment id='19' author='alontrais' date='2020-04-04T02:07:13Z'>
		Ok here you go
&lt;denchmark-link:https://drive.google.com/drive/folders/12aNIVPz3RFUpTbmniwdLkb7gcMh22DLt?usp=sharing&gt;https://drive.google.com/drive/folders/12aNIVPz3RFUpTbmniwdLkb7gcMh22DLt?usp=sharing&lt;/denchmark-link&gt;

Thank you
		</comment>
		<comment id='20' author='alontrais' date='2020-04-04T02:48:52Z'>
		When I changed the “giou” = 1.0 in hyp, the nan loss problem no longer occurs.

发自我的iPad
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 在 2020年4月3日，下午9:13，Glenn Jocher ***@***.***&gt; 写道：

 ﻿
 @rinabuoy thanks! Do you have a colab notebook that you could share so we can run to reproduce the error?

 The error is caused because GIoU returns nan, though we can't reproduce it on COCO so its very hard/impossible to debug.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or unsubscribe.


		</comment>
		<comment id='21' author='alontrais' date='2020-04-04T03:52:58Z'>
		With “giou” = 1.0, it ran epoch 40 and is still running. Will see if that error pops up in later epoch.
Thanks &lt;denchmark-link:https://github.com/yangxu351&gt;@yangxu351&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='alontrais' date='2020-04-04T03:55:52Z'>
		no, it seems fixed




Yang Xu, Ph.D. student,


Department of Computer and Communication Engineering, University of Science and Technology Beijing,


30 Xueyuan Road, Haidian District, Beijing 100083,


P.R. China.


Tel:(+86) 18518590836


Email:xuyang_ustb@xs.ustb.edu.cn / xuyang_91@qq.com







On Fri, Apr 3, 2020 at 11:53 PM -0400, "Rina Buoy" &lt;notifications@github.com&gt; wrote:
















With “giou” = 1.0, it ran epoch 40 and is still running. Will see if that error pops up in later epoch.



Thanks &lt;denchmark-link:https://github.com/yangxu351&gt;@yangxu351&lt;/denchmark-link&gt;






—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.
		</comment>
		<comment id='23' author='alontrais' date='2020-04-07T10:04:51Z'>
		I met the same problem, but it does not work for me by changing hyp['giou] to 1.0. I finally found it is related to momentum when using SGD. With pure SGD (momentum=0.), my code runs correctly. Can NOT figure out why. There is no division in momentum computation at all.
		</comment>
		<comment id='24' author='alontrais' date='2020-04-07T16:37:12Z'>
		&lt;denchmark-link:https://github.com/lampsonSong&gt;@lampsonSong&lt;/denchmark-link&gt;
 nan originates in the GIoU computation somehow. But I have not been able to reproduce, and again there are no divide by zeros in GIoU. It should be stable.
		</comment>
		<comment id='25' author='alontrais' date='2020-05-08T00:07:43Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
		<comment id='26' author='alontrais' date='2020-06-24T12:02:37Z'>
		Hey, I'm getting this error while training on a custom dataset. I've trained similar datasets before and haven't received this error. I have not changed any hyperparameters since then.
Command I ran
python train.py --data data/just_edges.data --cfg cfg/just_edges.cfg --weights weights/yolov3.pt --adam --epochs 500 --batch-size 32 --notest --cache-images --img-size 640
&lt;denchmark-link:https://user-images.githubusercontent.com/65915178/85549640-3092ea80-b5ee-11ea-9b71-dc889567356f.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/65915178/85549752-4d2f2280-b5ee-11ea-885f-30d54e764f6e.png&gt;&lt;/denchmark-link&gt;

I ran into this error before due to using the --evolve hyperparameter, which I've now removed. I've reduced the LR to 0.001 and the GIOU hyp to 1. This is running on a 4 GPU virtual machine running K80s.
Thanks for your help and kudos to the amazing work done on this repo!
		</comment>
		<comment id='27' author='alontrais' date='2020-06-24T18:16:28Z'>
		&lt;denchmark-link:https://github.com/kthomas441&gt;@kthomas441&lt;/denchmark-link&gt;
 recommend you try &lt;denchmark-link:https://github.com/ultralytics/yolov5&gt;https://github.com/ultralytics/yolov5&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>