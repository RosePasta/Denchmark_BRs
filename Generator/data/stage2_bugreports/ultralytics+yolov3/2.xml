<bug id='2' author='lianuo' open_date='2018-09-03T16:39:58Z' closed_time='2019-04-10T11:46:14Z'>
	<summary>Resume training from official yolov3 weights</summary>
	<description>
Thanks for your improvement of this YOLOv3 implementation.
I have just test the training ,got some problem .
I follow these steps.

load the original yolov3.weight to the model
train it on coco2014 with your train.py.
3.Got the following logs ,the precision is down fast from 0.5-&gt;0.1. but recall is up to 0.35.
see Screenshot here


4.I save the weight with precision0.2, and run the detect.py
the result like this ,
&lt;denchmark-link:https://user-images.githubusercontent.com/28760530/44996478-b2112700-afda-11e8-9399-eec286192a96.jpg&gt;&lt;/denchmark-link&gt;

if I do not train,the orginal wight can get this result:
&lt;denchmark-link:https://user-images.githubusercontent.com/28760530/45008197-d6e5b880-b033-11e8-89dc-97541686dd55.jpg&gt;&lt;/denchmark-link&gt;

I do not know whether I used wrong parameters or something else, lead to generation of many bbox .
could you give me some suggestion?
Thank you~
	</description>
	<comments>
		<comment id='1' author='lianuo' date='2018-09-04T10:14:44Z'>
		The loss is down, so I wounder whether the definition of loss lead to this problem?
		</comment>
		<comment id='2' author='lianuo' date='2018-09-04T10:17:36Z'>
		&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30193946/45025690-c30b7800-b06e-11e8-83c3-0117e8c361ce.jpg&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/30193946/45025692-c3a40e80-b06e-11e8-9bf0-1061773f1ef5.jpg&gt;&lt;/denchmark-link&gt;

same issue, i change cls to 2(bg and person), it looks like cls confidence have sth wrong in training
		</comment>
		<comment id='3' author='lianuo' date='2018-09-04T11:56:46Z'>
		&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Ricardozzf&gt;@Ricardozzf&lt;/denchmark-link&gt;
 I've not tried to continue training from the official yolov3 weights. It probably won't pick up smoothly where Joseph Redmon and company left off for a number of reasons, such as the optimizer starting with no knowledge of the previous optimizer's momentum and LR. There are also a few primary differences between my training and the official darknet training:

Issue #4: train.py uses the Adam optimizer in place of SGD. I could not get SGD to converge with the yolov3 learning rate.
Non Maximal Suppression (NMS) is not applied during training, so precision may appear artificially low while training, as many of the False Positives (FPs) in the denominator term P = TP / (TP + FP) are eliminated during testing but not training.
Issue #3:  I use CrossEntropyLoss in place of BinaryCrossEntropyLoss for classification loss during training. I made this change after observing better performance with CE vs BCE (I don't understand the reason for this, as darknet uses BCE). These two loss terms are on line 162 and 163 of models.py. Note that BCEWithLogitsLoss that I use produces the same loss as BinaryCrossEntropyLoss + torch.sigmoid() on the first term, but BCEWithLogitsLoss is preferable for numerical stability reasons. If you want to try to continue training from yolov3.weights you need to use BinaryCrossEntropy or BCEWithLogitsLoss as in the commented line below.

lcls = nM * CrossEntropyLoss(pred_cls[mask], torch.argmax(tcls, 1))
# lcls = nM * BCEWithLogitsLoss2(pred_cls[mask], tcls.float())
&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
 how many epochs did you train this way? If you make the switch the BCE does this help?
&lt;denchmark-link:https://github.com/Ricardozzf&gt;@Ricardozzf&lt;/denchmark-link&gt;
 your results don't look good. Are you training from scratch or resuming training from yolov3 weights like &lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
?  If you suspect class confidence has a problem it must be because I've swapped CE for BCE. You can switch BCE back on by switching the commented lines above. But also note that if you are training from scratch you need significant number of epochs before things start looking good. In my training I see about 0.50 mAP on COCO2014 validate set after 40 epochs (3 days of training on a 1080 Ti).
		</comment>
		<comment id='4' author='lianuo' date='2018-09-04T16:48:06Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thank you for reply!
I just try resume training from official yolov3 weights with
optimizer = torch.optim.SGD(model.parameters(), lr=.0001, momentum=.9, weight_decay=5e-4, nesterov=True)
and switch to  BCEWithLogitsLoss
the precision is down to 0.18 and recall grow to 0.6.just like previous  settings.
It is strange , that when I run test.py  with this trained weight , I can still have high sore,see the screamshot:
&lt;denchmark-link:https://user-images.githubusercontent.com/28760530/45045188-d33e4a00-b0a4-11e8-8c3a-e60e1b9dabce.png&gt;&lt;/denchmark-link&gt;

but when I run detect.py with this trained weight. the result is still not good.like this:
&lt;denchmark-link:https://user-images.githubusercontent.com/28760530/45045278-0aacf680-b0a5-11e8-962a-944a64883645.jpg&gt;&lt;/denchmark-link&gt;

Is this because of the method of evaluate mAP?
		</comment>
		<comment id='5' author='lianuo' date='2018-09-04T16:58:37Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 have you use the weights you trained  (0.50 mAP on COCO2014) to test a image?
could you share the weight or the test result of images?
it is a little strange that the score is high while the image testing result is not good...
Thank you so much for reply
		</comment>
		<comment id='6' author='lianuo' date='2018-09-04T17:01:08Z'>
		&lt;denchmark-link:https://github.com/Ricardozzf&gt;@Ricardozzf&lt;/denchmark-link&gt;
 thanks for you information.I am not alone ,haha.
		</comment>
		<comment id='7' author='lianuo' date='2018-09-05T01:56:02Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 thanks for your reply
i have trained the model from scratch for 14 epochs on a TITAN X. In order to make full use of GPU, i chaged  batch_size from 12 to 16, and other conf is default.
In training, the model looks good:
&lt;denchmark-link:https://user-images.githubusercontent.com/30193946/45066291-4faa4a80-b0f0-11e8-8904-32e56d10bf05.png&gt;&lt;/denchmark-link&gt;

In testing, I use crowdhuman &lt;denchmark-link:http://www.crowdhuman.org/&gt;dataset&lt;/denchmark-link&gt;
, the score is high
&lt;denchmark-link:https://user-images.githubusercontent.com/30193946/45066333-82544300-b0f0-11e8-842c-4bb7b4cd3555.png&gt;&lt;/denchmark-link&gt;

Although the score in training and testing is high,  the result processed by detect.py is bad, maybe one thing could be confirmed, testing score didn't match results of detect.py
I hope the information is useful to us.
		</comment>
		<comment id='8' author='lianuo' date='2018-09-05T12:56:11Z'>
		&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Ricardozzf&gt;@Ricardozzf&lt;/denchmark-link&gt;
 thats a good question, I will compare my  and  results. I am at epoch 37 training on COCO2014. If I run  I see this:
+ Sample [4998/5024] AP: 0.7528 (0.4926)
+ Sample [4999/5024] AP: 0.8333 (0.4927)
+ Sample [5000/5024] AP: 0.5543 (0.4927)
Mean Average Precision: 0.4927
If I then use the epoch 37 checkpoint  with  I see this on my example image, which is the same problem you guys are seeing.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/45094160-ca885b00-b11a-11e8-896e-450af6b707c4.jpg&gt;&lt;/denchmark-link&gt;

I'm wondering if I caused this by switching from BCE to CE. In xView when I used this code I had to increase my -conf_thresh in detect.py to ~0.99 to reduce FP. If I increase -conf_thresh to 0.99 now (and change -nms_thresh to 0.45 to match test.py) then I get this. Better, but still not quite right.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/45094248-0de2c980-b11b-11e8-9112-382f77f317a8.jpg&gt;&lt;/denchmark-link&gt;

This is a bit apples and oranges comparison though. The official weights are at 160 epochs and my latest.pt is only at 37 epochs, so its possible that training up to 160 will resolve this problem.
I don't understand why test.py is producing such a high mAP though, especially since it uses a very low -conf_thresh of 0.5. You guys are right, there is an unresolved issue somewhere. I will try and investigate more. The problem seems twofold:

Issue #5: test.py is possibly over-reporting mAP on trained checkpoints, even though it correctly reports mAP on the official YOLOv3 weights, an odd inconsistency. This seems to be the easiest issue to resolve, so I'll look at this first.
Trained weights seem to require much higher confidence thresholds (~0.99) than typically used in YOLOv3 (~0.8 commonly). This would seem to be unrelated to the CE vs BCE issue, as @lianuo trained from epoch 160 using BCE and still saw poor results.

Any ideas are appreciated as well!
		</comment>
		<comment id='9' author='lianuo' date='2018-09-10T14:59:50Z'>
		&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Ricardozzf&gt;@Ricardozzf&lt;/denchmark-link&gt;
 the overly-high mAPs you were seeing before should be partly fixed in the latest commits, which fixed mAP calculations (see issue &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/7&gt;#7&lt;/denchmark-link&gt;
). The official weights now produce .57 mAP, but the trained weights that before gave me 0.50 mAP now return about 0.13 mAP, much more in-line with the poor boxes you see in your images.
I still don't understand the actual cause of the poor training results however.
		</comment>
		<comment id='10' author='lianuo' date='2018-09-10T16:22:04Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thank you for reply~
		</comment>
		<comment id='11' author='lianuo' date='2018-09-10T16:23:48Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 the loss is still decrease when training ,do you think the loss function need to modify?
		</comment>
		<comment id='12' author='lianuo' date='2018-09-11T12:02:50Z'>
		No warm-up process found for SGD. According to the paper of YOLO9000 and the official code, we need to warm-up the first 1000 iterations to make it better converge:
warmup_lr = lr * batch_size / burn_in, where lr = 1e-3, batch_size = 64 and burn_in = 1000
		</comment>
		<comment id='13' author='lianuo' date='2018-09-11T12:49:06Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 The usage of CrossEntropyLoss might be incorrect. The input shape is (nB, nA, nG, nG, nC), but the pytorch-doc suggests it to be (nB, nC, ...). (See 
)
Besides, the torch.argmax(tcls, 1) fetches C from dim=1, but the shape of tcls is actually (nB, nA, nG, nG, nC). Maybe we need to permutate the dims so that C is at dim=1 .
		</comment>
		<comment id='14' author='lianuo' date='2018-09-11T17:24:08Z'>
		&lt;denchmark-link:https://github.com/xyutao&gt;@xyutao&lt;/denchmark-link&gt;
 I looked into the CELoss function, I think this part is ok. When I start training and debug this spot, the dimensions look good (assuming  and assuming we have 47 targets here in the first batch of  images). I think  is eliminating all the other dimensions:
tcls.shape
Out[2]: torch.Size([12, 3, 13, 13, 80])

tcls = tcls[mask]
tcls.shape
Out[3]: torch.Size([47, 80])

lcls = nM * CrossEntropyLoss(pred_cls[mask], torch.argmax(tcls, 1))
Out[4]: tensor(206.37325, grad_fn=&lt;MulBackward1&gt;)

pred_cls[mask].shape
Out[5]: torch.Size([47, 80])

torch.argmax(tcls, 1).shape
Out[6]: torch.Size([47])
I linked to your comment on the SGD warmup however, this is a good catch! Issue &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/4&gt;#4&lt;/denchmark-link&gt;
 is open on this. By the first 1000 iterations do you mean the first 1000 batches?
		</comment>
		<comment id='15' author='lianuo' date='2018-09-12T01:49:13Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Yeah, the first 1000 batches of batch_size=64.
		</comment>
		<comment id='16' author='lianuo' date='2018-09-13T08:29:07Z'>
		please help,i have the same error,
did you guys solve this problem?thanks!
		</comment>
		<comment id='17' author='lianuo' date='2018-09-18T22:08:33Z'>
		&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
 Hi, just wondering how you loaded a pre-trained weights. Did you add this line of code in ?
&lt;denchmark-code&gt;    # Initialize model 
    model = Darknet(opt.cfg, opt.img_size) 
    model.load_weights(opt.weights_path)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='18' author='lianuo' date='2018-09-18T22:33:06Z'>
		&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
 I found out from  that you add this line:
load_weights(model, weights_path)
But, now, I'm getting a different error from :
&lt;denchmark-link:https://user-images.githubusercontent.com/7217547/45720141-f3b3dd00-bb57-11e8-8ec8-66ebd7c4d268.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/7217547/45720159-01696280-bb58-11e8-829a-58c88d327299.png&gt;&lt;/denchmark-link&gt;

Have you encountered this problem; if yes, how do you deal with it?
		</comment>
		<comment id='19' author='lianuo' date='2018-09-18T23:32:19Z'>
		&lt;denchmark-link:https://github.com/jaelim&gt;@jaelim&lt;/denchmark-link&gt;
 you resume training from a trained model (i.e. latest.pt) by setting :


If you are seeing the error you mentioned it is because you failed to define a proper path to an image, or image folder in detect.py line 14 (no images are loaded). Make sure there are only image files in the path if you specify a path. Also please do not ask questions unrelated to the main issue title in this thread.



yolov3/detect.py


         Line 14
      in
      68de92f






 parser.add_argument('-image_folder', type=str, default='data/samples', help='path to images') 





		</comment>
		<comment id='20' author='lianuo' date='2018-09-20T16:27:03Z'>
		&lt;denchmark-link:https://github.com/xyutao&gt;@xyutao&lt;/denchmark-link&gt;
 I've switched from Adam to SGD with burn-in (which exponentially ramps up the learning rate from 0 to 0.001 over the first 1000 iterations) in commit &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/a722601ef61149cc9e5135f58c762310627c970a&gt;a722601&lt;/denchmark-link&gt;
:


&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/45847610-d6376c80-bd2b-11e8-8426-2df75c07adbc.png&gt;&lt;/denchmark-link&gt;

Unfortunately this caused width and height loss terms to diverge when training from scratch. I saw that these are the only unbounded outputs of the network (all the rest are sigmoided), so I was forced to sigmoid them and create new width and height calculations, after which the training converged. The original and updated ones I made in this commit are:



yolov3/models.py


        Lines 121 to 131
      in
      a722601






 # Width and height (yolo method) 



 # w = p[..., 2]  # Width 



 # h = p[..., 3]  # Height 



 # width = torch.exp(w.data) * self.anchor_w 



 # height = torch.exp(h.data) * self.anchor_h 



 



 # Width and height (power method) 



 w = torch.sigmoid(p[..., 2])  # Width 



 h = torch.sigmoid(p[..., 3])  # Height 



 width = ((w.data * 2) ** 2) * self.anchor_w 



 height = ((h.data * 2) ** 2) * self.anchor_h 





If I plot both of these in MATLAB it looks like the lack of a ceiling on the original code is causing the divergence problem. It may be that the original width/height equations are incorrect. Does anyone know where to find the original width and height darknet calcuations?
&gt;&gt; x=linspace(-3,3);
&gt;&gt; y1 = exp(x);
&gt;&gt; y2 = ((logsig(x) * 2).^2);
&gt;&gt; fig; plot(x,y1,'.-'); plot(x,y2,'.-'); h=gca; h.YLim=[0,5]; legend('original','updated'); xyzlabel('network output','anchor width multiple'); fcnfontsize(14)
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/45832408-197ce580-bd02-11e8-99e3-fc929d5e059d.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='lianuo' date='2018-09-23T20:55:18Z'>
		&lt;denchmark-link:https://github.com/lianuo&gt;@lianuo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Ricardozzf&gt;@Ricardozzf&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/xyutao&gt;@xyutao&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/CF2220160244&gt;@CF2220160244&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jaelim&gt;@jaelim&lt;/denchmark-link&gt;
 I have good news. , namely a problem -ing the various loss terms. This caused the  term to be 80 times too large (80 = COCO class count), which , which I believe was the major problem many of you saw in your training.
I fixed this in commit &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/cf9b4cfa5226c103cfd6e950fc789a6e45a96584&gt;cf9b4cf&lt;/denchmark-link&gt;
, and after the change observed that SGD with burn-in now converges with the original YOLO width/height calculations, so I placed those back in in commit &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/5d402ad31a53a44fd7dda46c75bb1dc43df9923b&gt;5d402ad&lt;/denchmark-link&gt;
.
Update: Sorry guys I think I might have spoken too soon. The changes help, but resuming training from yolov3.pt still causes P and R to drop from initially high values to lower values after ~50 batches. I think we are getting closer to the source of the problem however, which I feel is in the model loss term somewhere. TODO: I also need to ignore non-best anchors with &gt; 0.50 iou to match yolov3.
		</comment>
		<comment id='22' author='lianuo' date='2018-11-07T11:58:31Z'>
		&lt;denchmark-link:https://github.com/deeppower&gt;@deeppower&lt;/denchmark-link&gt;
 thanks for the feedback. You need to vary  to get the best mAP. Usually I test values between 0.01 - 0.50. In the current repo the best value seems to be around 0.1 - 0.2. For example if you run this you should get a higher mAP, around 0.40 I think (but yes still lower than what we want): 
My results look like this, comparing random initialization vs darknet53.conv.74 initialization. Your results look much smoother than mine. My training is on GCP preemptible instances which stop every 24 hours, or about every 10 epochs. I think this is causing spikes in my losses, which is very frustrating because theoretically the training should resume with no breaks at all (the model and the optimizer states are both saved and then replaced, so I don't understand my spikes... possibly a pytorch issue).
mAP is 0.42 at  at epoch 80. I will start some multiscale training here.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/48166529-2329cf00-e2e9-11e8-921b-2d5867526eb6.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='lianuo' date='2018-11-08T02:45:46Z'>
		&lt;denchmark-link:https://github.com/sporterman&gt;@sporterman&lt;/denchmark-link&gt;
 Sorry, i haven't trained my own dataset.
		</comment>
		<comment id='24' author='lianuo' date='2018-11-08T02:56:20Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thanks for your reply and great work. I have varied to 0.2, and mAP is 0.41 at epoch 68. There are still some problems that we need to solve.
		</comment>
		<comment id='25' author='lianuo' date='2018-11-08T11:39:40Z'>
		&lt;denchmark-link:https://github.com/deeppower&gt;@deeppower&lt;/denchmark-link&gt;
 yes the performance is still not as good for training as darknet unfortunately. I tried a few epochs of multi_scale training after epoch 80 and this did not seem to help. I've tried to align everything as closely as possible to darknet, so for example if you resume training from the official yolov3.pt weights the P and R values are very steady (though still dropping slightly over time). This makes me think the loss function is correct, or at least very close to the original darknet loss function. Inference works well, so the problem can not be there, it must be in the training-only code, which could be optimizer, LR scheduler, loss function, target building functions, IOU function, augmentation function...
		</comment>
		<comment id='26' author='lianuo' date='2018-11-10T17:40:33Z'>
		&lt;denchmark-link:https://github.com/nirbenz&gt;@nirbenz&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/okanlv&gt;@okanlv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/deeppower&gt;@deeppower&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/okanlv&gt;@okanlv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/xiao1228&gt;@xiao1228&lt;/denchmark-link&gt;
 . I thought about the problem a bit and decided that the loss terms needed rebalancing. In my last plot you can see Classification is consuming the great majority of the loss, which means that it is being optimised at the expense of all the other losses. Ideally the 6 losses would be roughly equal in magnitude so that they are all optimised with equal priority.
So I made a commit that multiplied Objectness loss by 10, and divided Classification loss by 10:



yolov3/models.py


        Lines 166 to 176
      in
      e04bb75






 if nM &gt; 0: 



 lx = k * MSELoss(x[mask], tx[mask]) 



 ly = k * MSELoss(y[mask], ty[mask]) 



 lw = k * MSELoss(w[mask], tw[mask]) 



 lh = k * MSELoss(h[mask], th[mask]) 



 



 # lconf = k * BCEWithLogitsLoss(pred_conf[mask], mask[mask].float()) 



 lconf = (k * 10) * BCEWithLogitsLoss(pred_conf, mask.float()) 



 



 lcls = (k / 10) * CrossEntropyLoss(pred_cls[mask], torch.argmax(tcls, 1)) 



 # lcls = k * BCEWithLogitsLoss(pred_cls[mask], tcls.float()) 





I ran this for most of the day on GCP, and after about 10 epochs I overlaid the 3 different trainings I'd done. This new approach seems vastly better, in particular at increasing Recall compared to before. I thought this was exciting enough to post the news right away, I'll have to train for another week to get to 70+ epochs and see the true effect. I'm wondering if there isn't a better way to more automatically balance these 6 equally important loss terms. They seem roughly equal now after 10 epochs, but maybe theres a way to update the balancing terms every epoch with the previous epochs gains. Any ideas?
UPDATE 1: mAP is 0.43 () at epoch 20. Updated plots below (green).
UPDATE 2: mAP is 0.46 () at epoch 35. Updated plots below (green).
UPDATE 3: mAP is 0.46 () at epoch 49 :( Jumps in loss observed during training, possibly due to many restarts of preemtable GCP VM. New commit &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/45c55677239a8c65caed83cc0025a84a06b490a5&gt;45c5567&lt;/denchmark-link&gt;
 to run test.py after each training epoch commit and . Starting new training from scratch using PyTorch 1.0 on GCP. Will post new comment when new results start coming in.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/48492108-f6157900-e820-11e8-9972-69405fa93c0d.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='27' author='lianuo' date='2018-11-16T13:04:30Z'>
		&lt;denchmark-link:https://github.com/deeppower&gt;@deeppower&lt;/denchmark-link&gt;
 Yes, objectness loss is higher than before because I multiplied it by 10x now. I'm trying to balance the loss terms so they contribute equally to the gradient, or else the largest loss terms will get optimized at the expense of the smaller loss terms. It appears to be working, though my loss term multiples are rather arbitrary unfortunately.
Ideally we want to take this a step further, and better equalize not just the loss terms, but the target distributions to something like zero mean and unity variance, which helps regression networks at least (not sure about object detection). Any experiments you can run on your own would help significantly, I'm just one man with one GPU here, so I can only try a finite sets of things to improve the results.
		</comment>
		<comment id='28' author='lianuo' date='2018-11-22T14:17:35Z'>
		&lt;denchmark-link:https://github.com/okanlv&gt;@okanlv&lt;/denchmark-link&gt;
 I have a question for you. Now that I've defaulted to start training from darknet53.conv.74, would it make sense to freeze those layers for a bit of time before allowing them to change?
I was thinking I could freeze them for the first epoch perhaps, which would be 7328 batches, or half epoch at least. The first 1000 batches are burn in. I feel like it would make sense to do this since the randomly initiated layers might converge must faster without the darknet53.conv.74 layers changing underneath them.
		</comment>
		<comment id='29' author='lianuo' date='2018-11-25T12:54:45Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 In the darknet repo, all layers are trained together after yolov3 is initialized with darknet53.conv.74 weights. In this &lt;denchmark-link:https://arxiv.org/abs/1411.1792&gt;paper&lt;/denchmark-link&gt;
, the authors have showed that updating the parameters of all the layers increases the performance compared to updating the parameters of only the top layers (related to fragile coadaptation of the layers, mentioned in the paper). That being said, your method might also work, because there are a few differences between your approach and the experiments in the paper. If you train yolov3 with your approach, could you share the loss graphs including your approach and the current method? It could be beneficial for further experiments.
		</comment>
		<comment id='30' author='lianuo' date='2018-11-27T13:04:22Z'>
		&lt;denchmark-link:https://github.com/nirbenz&gt;@nirbenz&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/okanlv&gt;@okanlv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/deeppower&gt;@deeppower&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/okanlv&gt;@okanlv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/xiao1228&gt;@xiao1228&lt;/denchmark-link&gt;
 I've started running studies to improve the COCO map when training from darknet53.conv.74. I started with the &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/2#issuecomment-437601721&gt;#2 (comment)&lt;/denchmark-link&gt;
 model that gets 0.46 mAP at epoch 35. The primary breakthrough there was simply rebalancing the loss terms, multiplying  and dividing  to get that 0.46 mAP.
All tests below are only run for the first epoch. Freezing the darknet53 layers (just for the first epoch) showed slightly positive results. It seems further rebalancing the loss terms has the biggest effect. In most ML regression problems the inputs and targets are always recalibrated to zero mean and unity variance, yolov3 does this for the inputs via batch_norm layers but does not do this for the regression targets (the bounding boxes), so I want to try this (regression problems that fail to do this have far worse performance).
Any other experiments you guys want let me know. I'll keep populating this comment as my results come in over the next week.



 
mAP (epoch 0)
Precision
Recall




default #2 (comment)
0.168
0.200
0.175


... + weight_decay=0
0.169
0.200
0.176


... + darknet53 frozen
0.172
0.210
0.179


... + lconf*16
0.181
0.214
0.188


... + lcls/4
0.231
0.268
0.243


... + dkn53 unfrozen + lconf*32
0.237
0.263
0.25


... + lconf*64
0.225
0.249
0.235


... + bbox targets normalization





... + additional experiments?






This is my selected configuration, +  in the above table, and in the latest commit. darknet53 is not frozen in the first epoch, as I found this hurts later epochs. I'm now training to about 50 epochs.
mAP is 0.45 () at epoch 12
mAP is 0.48 () at epoch 17
mAP is 0.50 () at epoch 45 (jumps in losses, not sure why again)
mAP is 0.522 ( at ) at epoch 62 (max mAP achieved)
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/49822374-3b27bf00-fd7d-11e8-9180-f0ac9fe2fdb4.png&gt;&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>