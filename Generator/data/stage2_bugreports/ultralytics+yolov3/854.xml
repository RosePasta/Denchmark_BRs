<bug id='854' author='ThorPham' open_date='2020-02-16T05:01:06Z' closed_time='2020-02-18T09:41:30Z'>
	<summary>Error when I want to resume or train with last.pt weight</summary>
	<description>
I want to resume to train but i have error.
python train.py --epoch 20 --batch-size 8 --cfg cfg/yolov3.cfg --weight weights/last.pt  --rect --data data/coco.data --single-cls
Traceback (most recent call last):
File "train.py", line 436, in 
train()  # train normally
File "train.py", line 295, in train
optimizer.step()
File "/home/thorpham/anaconda3/envs/thorpham/lib/python3.5/site-packages/torch/optim/sgd.py", line 100, in step
buf.mul_(momentum).add_(1 - dampening, d_p)
RuntimeError: The size of tensor a (32) must match the size of tensor b (128) at non-singleton dimension 0
	</description>
	<comments>
		<comment id='1' author='ThorPham' date='2020-02-16T08:27:28Z'>
		You'll need to use the --resume flag. Also, looks like the saved model might have been stored with a batch size of 32 (and accumulator of 4 thus making the size set in SGD to be 128). Resetting the previous state of optimizer in the model might help too (Commenting out 122-124 in train.py).
		</comment>
		<comment id='2' author='ThorPham' date='2020-02-16T09:59:10Z'>
		How to i saved model and stored with a batch size of 32 . My GPU only train max batch size is 16
		</comment>
		<comment id='3' author='ThorPham' date='2020-02-16T20:25:48Z'>
		&lt;denchmark-link:https://github.com/ThorPham&gt;@ThorPham&lt;/denchmark-link&gt;
 we just tested --resume and found that it is operating correctly. You must train and resume with the same exact settings. --epochs are absolute, not relative. In any case,  rather than using --resume if possible.
&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/74612106-2d737500-50b7-11ea-84a1-7b55e6acae5c.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ThorPham' date='2020-02-18T09:41:27Z'>
		Thank you
		</comment>
		<comment id='5' author='ThorPham' date='2020-03-19T13:14:59Z'>
		Meet the same error when resuming with exactly the same setting, trained with 4 GPUs, batch 16, accumulate 2.
		</comment>
		<comment id='6' author='ThorPham' date='2020-03-19T18:56:27Z'>
		&lt;denchmark-link:https://github.com/GoogleWiki&gt;@GoogleWiki&lt;/denchmark-link&gt;
 recommend train fully rather than resuming.
		</comment>
	</comments>
</bug>