<bug id='281' author='dkendall100' open_date='2019-05-15T17:35:30Z' closed_time='2019-05-28T14:12:15Z'>
	<summary>Transfer Learning Freezes</summary>
	<description>
Describe the bug
While attempting to implement transfer learning, I came across an issue. I did not make a good record of it happening, but I plan to recreate it again soon and update this issue.
I attempted to implement transfer learning and modified the system to detect a single class. While the model was training on the data set it froze on the 21st epoch, GPU resources were locked up, so I aborted the training.
I attempted to start transfer learning training again, but it did the same thing. Everything worked fine until the 21st epoch then it froze, and seemed to happen around the same percentage. I resolved to train the model on every layer instead of implementing the transfer learning switch, and everything worked fine.
To Reproduce
Steps to reproduce the behavior:

Try to train a single class model with transfer learning
Observe the training freeze on the 21st epoch

Expected behavior
The model to train without any problems.

&lt;denchmark-link:https://github.com/ultralytics/yolov3/files/3194791/transfer_learning_freeze.txt&gt;transfer_learning_freeze.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/43619961/57978127-3fab7000-79cc-11e9-8d1e-2e77f472bc6f.png&gt;&lt;/denchmark-link&gt;

Desktop (please complete the following information):

OS: Ubuntu 18.04

Smartphone (please complete the following information):
N/A
Additional context
I was training my model on an AMD Ryzen 1600X desktop computer with an NVIDIA RTX 2060 GPU.
	</description>
	<comments>
		<comment id='1' author='dkendall100' date='2019-05-21T11:09:19Z'>
		&lt;denchmark-link:https://github.com/dkendall100&gt;@dkendall100&lt;/denchmark-link&gt;
 the only freezing that we've noticed happens occasionally at the start of training during the NMS phase of testing. In certain configurations up to tens of thousands of FPs may be generated per image, causing NMS to take a very, very long time, up to 12 hours for full 5k.txt test set on COCO.
It seems the same thing happened to you, but in your case the underlying cause was a divergence in the wh loss terms. These are width and height of the bounding boxes, you can see they explode to 1e16 over the training. I think it could be that the wh divergence is more sensetive in transfer learning for whatever reason. One simple solution is to reduce the wh hyperparameter in the beginning of train.py. If you divide it by 2 for example, or by 4, this should keep the wh terms stable.
		</comment>
		<comment id='2' author='dkendall100' date='2019-05-28T16:28:27Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

I will try reducing that hp to avoid non-convergent loss. Thanks!
		</comment>
		<comment id='3' author='dkendall100' date='2019-05-28T17:22:54Z'>
		&lt;denchmark-link:https://github.com/dkendall100&gt;@dkendall100&lt;/denchmark-link&gt;
 to reduce wh loss divergence you can try to reduce the wh gain hyperparameter  in train.py, or possibly increase the burn-in period, also in trian.py.
		</comment>
	</comments>
</bug>