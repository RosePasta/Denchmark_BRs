<bug id='418' author='1451595897' open_date='2019-08-02T09:09:52Z' closed_time='2019-08-03T14:56:05Z'>
	<summary>The training model is twice as good as the pre-training model</summary>
	<description>
I trained on my data set and found that the size of the saved model was twice that of the pre-trained model. Was there any problem with the configuration when I trained?
	</description>
	<comments>
		<comment id='1' author='1451595897' date='2019-08-02T12:45:34Z'>
		&lt;denchmark-link:https://github.com/1451595897&gt;@1451595897&lt;/denchmark-link&gt;
 the pytorch *.pt models are dictionaries that include an optimizer of the model at the point in time when it stopped training (to be used if training is resumed). The optimizer carries a gradient of each parameter in the model, so it is the same size as the model, hence the training checkpoints are twice the size of the pretrained checkpoints, which do not carry optimizers.
You can remove the optimizer if you'd like to reduce the *.pt file size.
chkpt = torch.load('weights/best.pt')
chkpt.keys()
Out[8]: dict_keys(['epoch', 'best_fitness', 'training_results', 'model', 'optimizer'])
		</comment>
	</comments>
</bug>