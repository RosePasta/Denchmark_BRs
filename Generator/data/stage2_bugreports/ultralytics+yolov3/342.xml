<bug id='342' author='rayjan0114' open_date='2019-06-21T15:49:59Z' closed_time='2019-06-24T05:00:45Z'>
	<summary>Does the "reject = True" in build_targets cause a logical problem?</summary>
	<description>
Thanks for this great work!  I really appreciate it. I had two questions.
Describe the issue
(1). The formula of IoU  "iou = [wh_iou(x, gwh) for x in layer.anchor_vec]" always compare with anchor boxes, but those anchor boxes are prior fixed boxes which means we always compare the same thing.
It might be good at the beginning of training, but as the model has grown, won't it be better to compare with some new states of boxes?
(2). If (1) is true, the reject mask "j = iou &gt; iou_thres" will always neglect those targets which are not similar to the prior boxes, and will never be learned by our model?
But seems the issues above may be offset by the multi-scale process.
I'm not sure which part I miss, I hope anyone can help. Thanks in advance here.

&lt;denchmark-link:https://user-images.githubusercontent.com/47523852/59933683-80d7e980-947c-11e9-81ea-9ccebeb900d5.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='rayjan0114' date='2019-06-21T21:28:42Z'>
		&lt;denchmark-link:https://github.com/rayjan0114&gt;@rayjan0114&lt;/denchmark-link&gt;
 yes your interpretation is exactly correct. The first part compares the 3 anchors to the target, and assigns the most similar anchor to it. Without image augmentation this would always assign the same anchor to the target.
The second part rejects target-anchor matches that have a low IoU, about 0.29 currently. This results in increased performance in our tests, compared to accepting 100% of the matches.
		</comment>
		<comment id='2' author='rayjan0114' date='2019-06-22T03:29:14Z'>
		Thanks for the reply!
Is the reject mask idea from your own creativity or it's also how original YOLO do?
I am also curious about that have you ever tried to turn the reject to False after many epochs?
		</comment>
		<comment id='3' author='rayjan0114' date='2019-06-23T13:03:55Z'>
		&lt;denchmark-link:https://github.com/rayjan0114&gt;@rayjan0114&lt;/denchmark-link&gt;
 this parameter's value was determined by hyperparameter tuning, not by a person actually. In our experiments on the first epoch's results we tried many things, and setting  or setting  (equivalent) results in poorer results. We have not trained fully with this setting, we've simply looked at the results after epoch 0.
# Default training command: python3 train.py --data data/coco.data --img-size 320 --single-scale --batch-size 64 --accumulate 1 --epochs 1  # 0.449hr FP32 P100, 0.279/0.324hr V100 FP32/FP16
#   P         R       mAP        F1
0.111     0.268     0.122     0.144  # default
0.131     0.261     0.119     0.157  # scaleFill 
0.110     0.285     0.129     0.140  # scale_xy 1.2 
0.104     0.276     0.123     0.141  # scale_xy 1.5
0.109     0.286     0.124     0.132  # scale_xy 2.0
0.053     0.229     0.064    0.0768  # iou threshold = 0.0
0.114      0.28     0.125     0.139  # giou ** 2
		</comment>
		<comment id='4' author='rayjan0114' date='2019-06-24T05:00:36Z'>
		Thanks for all the sharing.
My intuition is there must still have something to improve here although the idea hasn't come out.
Anyway, I will give you feedback if I got any progress here, thanks again.
		</comment>
		<comment id='5' author='rayjan0114' date='2019-06-24T09:13:56Z'>
		&lt;denchmark-link:https://github.com/rayjan0114&gt;@rayjan0114&lt;/denchmark-link&gt;
 alexeyab/darknet does this differently. They have an ignore_threshold, which does not penalize  if the match is not the best but is still above 0.7 IoU. I think the current setup here is mimicking this functionality possibly, though I haven't had time to actually test out the alexeyab/darknet method to compare. If you have time perhaps you could do this?
		</comment>
		<comment id='6' author='rayjan0114' date='2019-06-24T09:15:25Z'>
		&lt;denchmark-link:https://github.com/rayjan0114&gt;@rayjan0114&lt;/denchmark-link&gt;
 also regardless of whether we keep this method or not, there should really be a smooth transition from 0 to 1 on this, as 0.29 is quite arbitrary, and I think this hard cutoff here could be improved by making it a tanh-shaped curve that integrates from 0 to 1 over the course of say 0.20 to 0.4.
		</comment>
		<comment id='7' author='rayjan0114' date='2019-07-02T12:09:52Z'>
		Thank you for the advice!
My device is so limited which is running slowly on my custom data
To do the simplest modification, I probably try this hand-tuned tanh for my own data after the normal round is finished.
"model.hyp['iou_t'] =  0.4-0.2 * np.tanh(1e-5 * epoch**2.3)**2".
&lt;denchmark-link:https://user-images.githubusercontent.com/47523852/60511567-dc1f9c80-9d04-11e9-8c99-9b79804c391a.png&gt;&lt;/denchmark-link&gt;

If I have time I would look at the AlexeyAB's code. Howerever it's progressing slowly here.
thanks again!
		</comment>
		<comment id='8' author='rayjan0114' date='2019-07-02T16:26:44Z'>
		&lt;denchmark-link:https://github.com/rayjan0114&gt;@rayjan0114&lt;/denchmark-link&gt;
 that is a very interesting strategy. I'd also considered modifying hyperparameters as a function of time/epoch, but this sort of tuning would take even longer, as right now each hyperparameter mutation is tested after only 1 epoch.
This is quite a frustrating problem in general due to the long training time of a single network. 273 epochs on a V100 takes about 1 week, making it very hard to quickly see the effect of changes.
		</comment>
		<comment id='9' author='rayjan0114' date='2020-06-03T12:02:44Z'>
		
Thank you for the advice!
My device is so limited which is running slowly on my custom data
To do the simplest modification, I probably try this hand-tuned tanh for my own data after the normal round is finished.
"model.hyp['iou_t'] = 0.4-0.2 * np.tanh(1e-5 * epoch**2.3)**2".

If I have time I would look at the AlexeyAB's code. Howerever it's progressing slowly here.
thanks again!

HI,you idea is great. can you post the exp results?
		</comment>
	</comments>
</bug>