<bug id='447' author='HoracceFeng' open_date='2019-08-13T10:26:35Z' closed_time='2019-08-14T06:33:51Z'>
	<summary>Error occurs when using multi_gpu train</summary>
	<description>
Hi &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
, this error is pretty weird. I use your code in cpu and single gpu, everything is prefect, but when I try to use multi_gpus, this error occurs:
Traceback (most recent call last):
File "train.py", line 299, in train
loss, loss_items = compute_loss(pred, targets, model, giou_loss=giou_loss)
File "/code/utils/utils.py", line 340, in compute_loss
txy, twh, tcls, tbox, indices, anchor_vec = build_targets(model, targets)
File "/code/utils/utils.py", line 398, in build_targets
print("check yololayer.ng", model.module.module_list[16][0].ng, model.module.module_list[23][0].ng)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 535, in getattr
type(self).name, name))
AttributeError: 'YOLOLayer' object has no attribute 'ng'
I have checked the YOLOLayer, the attr ng exists when I define the model using the class Darknet. But when it runs to compute_loss, the build_targets cannot find ng in YoloLayer. It only occurs in multi_gpus. [I also check "model.module.module_list", the ng just disappear, don't know why.]
BTW, I try to use multi_gpus but not distributed training, so I replace the DistributedDataParallel to DataParallel, not sure if this change makes the error.
Need help. I trap into this bug for a whole day ...
	</description>
	<comments>
		<comment id='1' author='HoracceFeng' date='2019-08-14T06:33:34Z'>
		OK, I finally find out why it happens.
Actually, when I change the DataParallel back to DistributedDataParallel, everything smooths. So the question becomes, what the difference between DataParallel and DistributedDataParallel? And why these two apis have differences?
I will close this issue. But still waiting for someone to answer. Thx.
		</comment>
		<comment id='2' author='HoracceFeng' date='2019-08-14T09:56:47Z'>
		&lt;denchmark-link:https://github.com/HoracceFeng&gt;@HoracceFeng&lt;/denchmark-link&gt;
 DataParallel is used in test.py. DistributedDataParallel is generally more advanced and is used in train.py.
		</comment>
		<comment id='3' author='HoracceFeng' date='2019-08-14T09:57:16Z'>
		&lt;denchmark-link:https://github.com/HoracceFeng&gt;@HoracceFeng&lt;/denchmark-link&gt;
 also this is more of a pytorch question, you might want to post there.
		</comment>
		<comment id='4' author='HoracceFeng' date='2019-12-12T08:24:45Z'>
		When I train own data using single GPU, it works,  But using two or four GPUs,  a mistake raises : AttributeError: 'DataParallel' object has no attribute 'module_list'
		</comment>
		<comment id='5' author='HoracceFeng' date='2019-12-12T18:18:13Z'>
		&lt;denchmark-link:https://github.com/LIUhansen&gt;@LIUhansen&lt;/denchmark-link&gt;
 multi-GPU currently operates correctly. For a working multi-GPU environment see &lt;denchmark-link:https://github.com/ultralytics/yolov3#reproduce-our-environment&gt;https://github.com/ultralytics/yolov3#reproduce-our-environment&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/26833433/70738005-af66ac80-1cc8-11ea-8c9b-2c4fa95cee92.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='HoracceFeng' date='2020-05-18T04:36:26Z'>
		
OK, I finally find out why it happens.
Actually, when I change the DataParallel back to DistributedDataParallel, everything smooths. So the question becomes, what the difference between DataParallel and DistributedDataParallel? And why these two apis have differences?
I will close this issue. But still waiting for someone to answer. Thx.

Have you addressed this problem?
		</comment>
	</comments>
</bug>