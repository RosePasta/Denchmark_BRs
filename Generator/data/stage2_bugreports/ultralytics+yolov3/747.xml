<bug id='747' author='aidanwiteck' open_date='2019-12-28T21:21:02Z' closed_time='2019-12-28T23:05:43Z'>
	<summary>Error when resuming training on custom data</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi, I am getting an error when I resume training on my own data. I get the following error message:
UnboundLocalError: local variable 'epoch' referenced before assignment
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:


I ran the following command and everything went smoothly. The weights saved and I was able to see how the model functioned on real data.
!python3 train.py --data data/coco1cls.data --epochs 10


I then tried to train off the saved weights of the first ten epochs by running the following command:
!python3 train.py --data data/coco1cls.data --epochs 10 --resume


However, I get the following error message:
&lt;denchmark-code&gt;Namespace(accumulate=4, adam=False, arc='default', batch_size=16, bucket='', cache_images=False, cfg='cfg/yolov3-spp.cfg', data='data/coco1cls.data', device='', epochs=10, evolve=False, img_size=416, multi_scale=False, name='', nosave=False, notest=False, prebias=False, rect=False, resume=True, transfer=False, var=None, weights='weights/last.pt')
Using CPU

Caching labels (20 found, 0 missing, 0 empty, 0 duplicate, for 20 images): 100%|‚ñà| 20/20 [00:00&lt;00:00, 4398.62it/s]
Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients
Using 8 dataloader workers
Starting training for 10 epochs...
Traceback (most recent call last):
  File "train.py", line 463, in &lt;module&gt;
    train()  # train normally
  File "train.py", line 397, in train
    print('%g epochs completed in %.3f hours.\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))
UnboundLocalError: local variable 'epoch' referenced before assignment

&lt;/denchmark-code&gt;

I am confused mainly because the only thing different with this is that I am resuming training, but I am getting an error message about epochs. What should I do? Thanks!
	</description>
	<comments>
		<comment id='1' author='aidanwiteck' date='2019-12-28T22:58:41Z'>
		I also received the same error. Change your --epochs to a higher number from what you left off at. For example if you trained before at 50 epochs use --epochs 75. The bug seems to be that start_epoch is larger than epoch.
		</comment>
		<comment id='2' author='aidanwiteck' date='2019-12-28T23:05:38Z'>
		Ah, you are right! Thanks so much.
		</comment>
	</comments>
</bug>