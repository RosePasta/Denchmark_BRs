<bug id='278' author='mlbach' open_date='2019-05-15T07:32:17Z' closed_time='2019-05-15T14:09:56Z'>
	<summary>Augment=False raises error</summary>
	<description>
&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thanks for creating and maintaining this great repo!
Describe the bug
I can successfully train a model on my own data. I used augmentation to enlarge my dataset in preparation and wanted to turn off augmentation at runtime, to speed up training. However, when I turn it off, I get a (seemingly unrelated) error:

/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [64,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [65,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [66,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [67,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [68,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [69,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [70,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
/media/nvidia/Jetspace/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda -&gt;auto::operator()(int)-&gt;auto: block: [0,0,0], thread: [71,0,0] Assertion index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; "index out of bounds" failed.
Traceback (most recent call last):
File "train.py", line 320, in 
multi_scale=opt.multi_scale,
File "train.py", line 197, in train
loss, loss_items = compute_loss(pred, targets, model)
File "/home/nvidia/Documents/Ampair/yolov3/utils/utils.py", line 288, in compute_loss
lxy += (k * h['xy']) * MSE(torch.sigmoid(pi[..., 0:2]), txy[i])  # xy loss
File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 489, in call
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py", line 435, in forward
return F.mse_loss(input, target, reduction=self.reduction)
File "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py", line 2156, in mse_loss
ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
RuntimeError: reduce failed to synchronize: device-side assert triggered

To Reproduce
Steps to reproduce the behavior:

Go to 'train.py' and set augment=False in LoadImagesAndLabels (line 122)
Run train.py to train the model
See error (occurs in the first 5-10 batches)

Expected behavior
Stable training without augmentation, just like it works with augment=True.
NVIDIA Jetson TX2:

Jetpack 3.3 | 3.2.1 [L4T 28.2.1]
Ubuntu 16.04 LTS (GNU/Linux 4.4.38-tegra aarch64)
Python 3.5
PyTorch 1.0.0

	</description>
	<comments>
		<comment id='1' author='mlbach' date='2019-05-15T14:09:56Z'>
		Hello, thank you for your interest in our work! We can not reproduce your issue. Note that augment is set to False in test.py, which runs after every training epoch, so if you trained successfully then you are already using augment=False successfully. Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove exising repo
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # git clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, Pytorch &gt;= 1.0, etc.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
		</comment>
	</comments>
</bug>