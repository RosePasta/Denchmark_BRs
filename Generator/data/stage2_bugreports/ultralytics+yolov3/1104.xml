<bug id='1104' author='vandesa003' open_date='2020-04-27T17:03:18Z' closed_time='2020-06-17T00:15:46Z'>
	<summary>[Multi-GPU training error]RuntimeError: Model replicas must have an equal number of parameters.</summary>
	<description>
Thanks for your great work! But I met some problem during distributed training on single machine(DDP).
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When training on multi GPUs(DDP), following error shows:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 422, in &lt;module&gt;
    train()  # train normally
  File "train.py", line 172, in train
    model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)
  File "/data/bohang/conda_env/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 287, in __init__
    self._ddp_init_helper()
  File "/data/bohang/conda_env/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 380, in _ddp_init_helper
    expect_sparse_gradient)
RuntimeError: Model replicas must have an equal number of parameters.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

REQUIRED: Code to reproduce your issue below
&lt;denchmark-code&gt;python3 train.py --data data/coco64.data --img-size 320 --epochs 3 --nosave
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Should be normally trained as I can do on single GPU.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.

OS: [Ubuntu 16.04]
GPU [V100 * 4]

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

It seems a &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/36268#issuecomment-619414463&gt;bug from pytorch1.5&lt;/denchmark-link&gt;
, but as the &lt;denchmark-link:https://github.com/ultralytics/yolov3/blob/master/README.md&gt;README.md &lt;/denchmark-link&gt;
 mentioned, I am using the latest pytorch1.5. However due to my env reason, I keep using python3.6.
	</description>
	<comments>
		<comment id='1' author='vandesa003' date='2020-04-27T17:04:04Z'>
		Hello &lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb&gt;Google Colab Notebook&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov3&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;GCP Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
		</comment>
		<comment id='2' author='vandesa003' date='2020-04-27T18:13:38Z'>
		&lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 yes, seems like a pytorch 1.5 bug. You can still use 1.4 without issue, we simply put this in the requirements because we have problems with people submitting bug reports on older versions of dependencies.
I suppose we will leave this open for now and follow the pytorch thread.
		</comment>
		<comment id='3' author='vandesa003' date='2020-04-28T01:52:45Z'>
		
@vandesa003 yes, seems like a pytorch 1.5 bug. You can still use 1.4 without issue, we simply put this in the requirements because we have problems with people submitting bug reports on older versions of dependencies.
I suppose we will leave this open for now and follow the pytorch thread.

Got it. Thanks for your reply. I will try on pytorch1.4 then.
		</comment>
		<comment id='4' author='vandesa003' date='2020-04-28T03:14:27Z'>
		Ok! BTW, the docker images also use 1.5, but a pre-release development version, which works bug free for multigpu (we are using it now with 2 T4's without issue).
		</comment>
		<comment id='5' author='vandesa003' date='2020-04-29T11:31:13Z'>
		
Ok! BTW, the docker images also use 1.5, but a pre-release development version, which works bug free for multigpu (we are using it now with 2 T4's without issue).

Wow, good to hear that. I've already turn to pytorch1.4 and the problem got fixed! Now I can start training, but sometimes I will got segmentation fault (core dumped) error. When I am training on a very small sampled dataset(~5k images), the system worked perfectly. But when I sampled more data(from the same source) around 500k, I will get the error.
I guess it's a kind of memory access error, but I don't know where and why. I am still working on solving the problem. Would you please give me some advice on it? Thanks a lot anyway!
		</comment>
		<comment id='6' author='vandesa003' date='2020-04-29T12:53:27Z'>
		Hi &lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
,
Are you using 4 GPUs when you get "segmentation fault"? We are also getting the same error when we are using 4 GPUs, not when single or two GPUs. We have investigated a little bit and found out that it is caused by Pytorch. We get the error code with "faulthandler" module in Python.

File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/autograd/init.py", line 99 in backward


File "/root/.trains/venvs-builds/3.6/lib/python3.6/site-packages/torch/tensor.py", line 195 in backward

There are several issues in PyTorch  stating that it is fixed in version 1.5 but when we install the version 1.5, we get "model replica" error.
I am writing this to you, just to give insight and if you find any solutions to this issue, could you please let me know? Thanks.
&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I also want to ask you about this problem. Do you have any advice on it? Thanks..
		</comment>
		<comment id='7' author='vandesa003' date='2020-04-29T17:04:48Z'>
		&lt;denchmark-link:https://github.com/kaanakan&gt;@kaanakan&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 sorry, no insights on this, though you should check your system RAM (not GPU ram) consumption, and not use --cache on large datasets. Other than that we only train with 1-2 GPUs using the docker images and never see segmentation faults, perhaps try the docker image:
&lt;denchmark-h:h2&gt;Reproduce Our Environment&lt;/denchmark-h&gt;

To access an up-to-date working environment (with all dependencies including CUDA/CUDNN, Python and PyTorch preinstalled), consider a:

GCP Deep Learning VM with $300 free credit offer: GCP Quickstart Guide
Google Colab Notebook with 12 hours of free GPU time: Google Colab Notebook
Docker Image from https://hub.docker.com/r/ultralytics/yolov3. See Docker Quickstart Guide

		</comment>
		<comment id='8' author='vandesa003' date='2020-04-30T02:22:48Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thanks a lot for your information! I think the segmentation faults may because the --cache option or the data itself. I will try your suggestion and will give a feedback here.
		</comment>
		<comment id='9' author='vandesa003' date='2020-05-08T13:42:13Z'>
		
@glenn-jocher Thanks a lot for your information! I think the segmentation faults may because the --cache option or the data itself. I will try your suggestion and will give a feedback here.

Try to use this command:
pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html
I have got the same issue and I have solved using this Pytorch version.
		</comment>
		<comment id='10' author='vandesa003' date='2020-05-09T04:20:15Z'>
		&lt;denchmark-link:https://github.com/adrianosantospb&gt;@adrianosantospb&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Thanks for your help! I fix the issue by degrade pytorch to 1.4 and for the nan loss, after I use the Apex, the issues are all gone. Amazing!
BTW, Apex really saved me a lot of time, it can speed up the training by x2.
		</comment>
		<comment id='11' author='vandesa003' date='2020-06-09T00:15:18Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
		<comment id='12' author='vandesa003' date='2020-06-11T06:43:32Z'>
		this bug still exists in pytorch 1.5.
		</comment>
		<comment id='13' author='vandesa003' date='2020-06-11T07:05:13Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 are you using Apex for training? Unfortunately we do almost all of our training on single GPU (T4 usually, V100 sometimes), so I've never run into this error message.
		</comment>
		<comment id='14' author='vandesa003' date='2020-06-19T10:28:36Z'>
		&lt;denchmark-link:https://github.com/jinfagang&gt;@jinfagang&lt;/denchmark-link&gt;
 and everybody, I also had this bug and had to downgrade pytorch to 1.4. However, after updating to yesterday's pytorch 1.5.1 release, things are working again!
		</comment>
		<comment id='15' author='vandesa003' date='2020-07-21T10:00:45Z'>
		&lt;denchmark-link:https://github.com/tjiagoM&gt;@tjiagoM&lt;/denchmark-link&gt;
  Seems work, but...
		</comment>
	</comments>
</bug>