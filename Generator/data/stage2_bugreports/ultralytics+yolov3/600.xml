<bug id='600' author='ruihangdu' open_date='2019-11-10T02:29:34Z' closed_time='2019-11-10T14:32:16Z'>
	<summary>dimensionality error with 1 image example</summary>
	<description>
Describe the bug
For the 1 image example, seems like the target tensor only has 1 dimension, causing an error in build_targets function.
To Reproduce
Steps to reproduce the behavior:

run

python train.py --data data/coco_1img.data
Expected behavior
train the network without exceptions
Screenshots
Output as follows
Namespace(accumulate=2, batch_size=32, bucket='', cache_images=False, cfg='cfg/yolov3-spp.cfg', data='data/coco_1img.data', epochs=273, evolve=False, img_size=416, img_weights=False, multi_scale=False, nosave=False, notest=False, rect=False, resume=False, transfer=False, xywh=False)
Using CPU

Reading labels (1 found, 0 missing, 0 empty for 1 images): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 1613.19it/s]
Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients

     Epoch   gpu_mem   GIoU/xy        wh       obj       cls     total   targets  img_size
  0%|                                                                                                                                                                                                                                                     | 0/1 [00:00&lt;?, ?it/s]/Users/ray/miniconda3/envs/viz/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.Upsampling is deprecated. Use nn.functional.interpolate instead.")
Traceback (most recent call last):
  File "train.py", line 395, in &lt;module&gt;
    accumulate=opt.accumulate)
  File "train.py", line 270, in train
    loss, loss_items = compute_loss(pred, targets, model, giou_loss=not opt.xywh)
  File "/Users/ray/WorkSpace2019Q3/yolov3/utils/utils.py", line 288, in compute_loss
    txy, twh, tcls, tbox, indices, anchor_vec = build_targets(model, targets)
  File "/Users/ray/WorkSpace2019Q3/yolov3/utils/utils.py", line 373, in build_targets
    b, c = t[:, :2].long().t()  # target image, class
IndexError: too many indices for tensor of dimension 1
Desktop (please complete the following information):

OS: OSX Mojave
Version [e.g. 22]

Smartphone (please complete the following information):

Device: [e.g. iPhoneXS]
OS: [e.g. iOS8.1]
Version [e.g. 22]

Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='ruihangdu' date='2019-11-10T05:16:41Z'>
		Thanks, this is happening because the default batch-size exceeds the dataset size. We should update the demo, in the time being you can use:
python train.py --data data/coco_1img.data --batch-size 1 --accumulate 1 --weights weights/darknet53.conv.74
		</comment>
		<comment id='2' author='ruihangdu' date='2019-11-10T05:31:41Z'>
		@larry0123du I tested the above change and it still produces an error. This seems to be caused by multiple dataloader workers being applied to a single image. To fix this you'll likely need to hardcode  into the dataloader arguments, and do the same in test.py.  Though as a general point, there is no good use of training a single image. You're much better served looking at the custom data training page on the wiki and ignoring the single image example.
&lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki&gt;https://github.com/ultralytics/yolov3/wiki&lt;/denchmark-link&gt;




yolov3/train.py


        Lines 198 to 204
      in
      579fdc5






 dataloader = torch.utils.data.DataLoader(dataset, 



 batch_size=batch_size, 



 num_workers=min([os.cpu_count(), batch_size, 16]), 



 shuffle=not opt.rect,  # Shuffle=True unless rectangular training is used 



 pin_memory=True, 



 collate_fn=dataset.collate_fn) 



 





		</comment>
		<comment id='3' author='ruihangdu' date='2019-11-10T14:32:16Z'>
		I think training on a single image would be useful for just quickly testing how the entire workflow works (especially on my local laptop due to resource constraints). But I guess the 16img one works as well. Thanks (and boiler up :))!
		</comment>
	</comments>
</bug>