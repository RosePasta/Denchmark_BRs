<bug id='1025' author='dongziqi001' open_date='2020-04-08T10:45:27Z' closed_time='2020-05-17T00:09:20Z'>
	<summary>RuntimeError: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 8.00 GiB total capacity; 5.86 GiB already allocated; 998.40 KiB free; 172.44 MiB cached)</summary>
	<description>
python train.py --data data/fruit.data --cfg cfg/yolov3-spp.cfg --epochs 2
Namespace(accumulate=4, adam=False, batch_size=16, bucket='', cache_images=False, cfg='cfg/yolov3-spp.cfg', data='data/fruit.data', device='', epochs=2, evolve=False, img_size=[416], multi_scale=False, name='
', nosave=False, notest=False, rect=False, resume=False, single_cls=False, weights='weights/yolov3-spp-ultralytics.pt')
Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2070', total_memory=8192MB)
Model Summary: 225 layers, 6.25787e+07 parameters, 6.25787e+07 gradients
Caching labels (45 found, 0 missing, 0 empty, 0 duplicate, for 45 images): 100%|████████████████████████████████████████████████████████████████
█████████████████████████████| 45/45 [00:00&lt;00:00, 7488.34it/s]
Caching labels (11 found, 0 missing, 0 empty, 0 duplicate, for 11 images): 100%|████████████████████████████████████████████████████████████████
█████████████████████████████| 11/11 [00:00&lt;00:00, 5501.05it/s]
Using 8 dataloader workers
Starting training for 2 epochs...
&lt;denchmark-code&gt; Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size
&lt;/denchmark-code&gt;

0%|                                                                                                                                                                                    | 0/3 [00:00&lt;?, ?it/s]T
raceback (most recent call last):
File "train.py", line 423, in 
train()  # train normally
File "train.py", line 256, in train
pred = model(imgs)
File "C:\Users\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 493, in call
result = self.forward(*input, **kwargs)
File "F:\work\202048\models.py", line 259, in forward
x = module(x)
File "C:\Users\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 493, in call
result = self.forward(*input, **kwargs)
File "C:\Users\Anaconda3\lib\site-packages\torch\nn\modules\container.py", line 92, in forward
input = module(input)
File "C:\Users\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 493, in call
result = self.forward(*input, **kwargs)
File "C:\Users\Anaconda3\lib\site-packages\torch\nn\modules\conv.py", line 338, in forward
self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 8.00 GiB total capacity; 5.86 GiB already allocated; 998.40 KiB free; 172.44 MiB cached)
This problem appears in training step. To train my own datasets(2 categories)
To solve this problem, I change the batch size to 1 in yolov3-spp.cfg. But it still shows  CUDA out of memory.
Does anyone know how to solve this problem??Thanks!!!
	</description>
	<comments>
		<comment id='1' author='dongziqi001' date='2020-04-08T10:46:07Z'>
		Hello &lt;denchmark-link:https://github.com/dongziqi001&gt;@dongziqi001&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://colab.research.google.com/drive/1G8T-VFxQkjDe4idzN8F-hbIBqkkkQnxw&gt;Google Colab Notebook&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov3&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;GCP Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
		</comment>
		<comment id='2' author='dongziqi001' date='2020-04-08T17:37:02Z'>
		&lt;denchmark-link:https://github.com/dongziqi001&gt;@dongziqi001&lt;/denchmark-link&gt;
 your batch size is 16 above. To reduce:
python3 train.py --batch-size 4
		</comment>
		<comment id='3' author='dongziqi001' date='2020-04-10T06:16:24Z'>
		It works!!
thank you for your guidence!!~
		</comment>
		<comment id='4' author='dongziqi001' date='2020-05-11T00:09:08Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
	</comments>
</bug>