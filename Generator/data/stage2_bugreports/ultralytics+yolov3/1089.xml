<bug id='1089' author='bzburr' open_date='2020-04-23T11:21:42Z' closed_time='2020-06-10T00:15:32Z'>
	<summary>resume error</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

A clear and concise description of what the bug is.
I was training with a custom dataset on my desktop PC - had been running for a about 48 hours and had a powercut.
I tried to resume the training using
python train.py --data data/signs.data --resume --batch-size 4 --accumulate 1 --nosave --weights weights/last.pt --cfg=cfg/yolov3-signs.cfg
and it didnt like me. i was very sad.
Namespace(accumulate=1, adam=False, batch_size=4, bucket='', cache_images=False, cfg='cfg/yolov3-signs.cfg', data='data/signs.data', device='', epochs=300, evolve=True, img_size=[800], multi_scale=False, name='', nosave=True, notest=False, rect=False, resume=True, single_cls=False, weights='weights\last.pt')
Using CUDA device0 _CudaDeviceProperties(name='GeForce GTX 1070', total_memory=8192MB)
device1 _CudaDeviceProperties(name='GeForce GTX 750 Ti', total_memory=2048MB)
Model Summary: 222 layers, 6.15237e+07 parameters, 6.15237e+07 gradients
Traceback (most recent call last):
File "train.py", line 116, in train
chkpt['model'] = {k: v for k, v in chkpt['model'].items() if model.state_dict()[k].numel() == v.numel()}
File "train.py", line 116, in 
chkpt['model'] = {k: v for k, v in chkpt['model'].items() if model.state_dict()[k].numel() == v.numel()}
KeyError: 'module_list.85.Conv2d.weight'
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File "train.py", line 472, in 
results = train()
File "train.py", line 121, in train
raise KeyError(s) from e
KeyError: "weights\last.pt is not compatible with cfg/yolov3-signs.cfg. Specify --weights '' or specify a --cfg compatible with weights\last.pt. See &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/657&gt;#657&lt;/denchmark-link&gt;
"
and it faled with
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Begin training custom model
have a power cut
try to use the --resume tag in the command like

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

A clear and concise description of what you expected to happen.
I'd expect training to restart where it left off
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.
Desktop (please complete the following information):

OS: [e.g. iOS]  Windows
Version [e.g. 22]  10

Smartphone (please complete the following information):

Device: [e.g. iPhoneXS]
OS: [e.g. iOS8.1]
Version [e.g. 22]

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='bzburr' date='2020-04-23T11:22:20Z'>
		Hello &lt;denchmark-link:https://github.com/bzburr&gt;@bzburr&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb&gt;Google Colab Notebook&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov3&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;GCP Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
		</comment>
		<comment id='2' author='bzburr' date='2020-04-23T12:46:35Z'>
		If this is a bug report, please provide screenshots and minimum viable code ?
i gave you the steps. its clearly a bug if the resume fails
		</comment>
		<comment id='3' author='bzburr' date='2020-04-23T12:54:51Z'>
		according to the error my first thought would be, the last.pt you are trying to resume is not the one which was interrupted. so check that. e.g. the timestamp
		</comment>
		<comment id='4' author='bzburr' date='2020-04-23T16:44:16Z'>
		&lt;denchmark-link:https://github.com/bzburr&gt;@bzburr&lt;/denchmark-link&gt;
 yes, like &lt;denchmark-link:https://github.com/sherman99&gt;@sherman99&lt;/denchmark-link&gt;
 says and as the error message says you are likely trying to resume training with a different cfg file. You need to use the same exact cfg file you started training with.
We are close to releasing a yolov4 repo where the model and cfg are integrated into one file for easier handling, but for now in yolov3 you always need to carry around both cfg and the matching *.pt.
		</comment>
		<comment id='5' author='bzburr' date='2020-04-23T21:51:46Z'>
		&lt;denchmark-link:https://github.com/bzburr&gt;@bzburr&lt;/denchmark-link&gt;
 yes, like &lt;denchmark-link:https://github.com/sherman99&gt;@sherman99&lt;/denchmark-link&gt;
 says and as the error message says you are likely trying to resume training with a different cfg file. You need to use the same exact cfg file you started training with.
no, i'm using the same config file i started the training with.
in additon there was only a single last.pt file in the weights folder.
bugger it, i'll just restart the whole training.
		</comment>
		<comment id='6' author='bzburr' date='2020-04-23T22:23:24Z'>
		&lt;denchmark-link:https://github.com/bzburr&gt;@bzburr&lt;/denchmark-link&gt;
 are you sure, because your second command shows --nosave, which only saves the last epoch. You will not be able to resume naturally if this is used.
Also, I recommend you git pull to get the latest, as there are updates often.
		</comment>
		<comment id='7' author='bzburr' date='2020-04-24T00:00:37Z'>
		--nosave  facepalm
		</comment>
		<comment id='8' author='bzburr' date='2020-04-24T00:59:58Z'>
		hehe. I suppose then the code was trying to load up a previously saved model that had fully completed training. We use --nosave for the tutorials because it speeds things up when your dataset is tiny. It probably takes about 1-2 seconds per epoch, so for normal training I would always save.
		</comment>
		<comment id='9' author='bzburr' date='2020-05-03T11:46:52Z'>
		I often use a silly method to solve this problem: I will save checkpoint .pt file every 5 epoch. When an error encountered while newly loading, I rename the newest checkpoint _xxepock.pt to last.pt, and I will resume to train again.
		</comment>
		<comment id='10' author='bzburr' date='2020-06-04T00:14:55Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
	</comments>
</bug>