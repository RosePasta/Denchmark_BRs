<bug id='491' author='centurions' open_date='2019-09-09T12:56:57Z' closed_time='2019-09-10T08:43:49Z'>
	<summary>Cuda memory</summary>
	<description>
Hi . thank you for your great work
How can I fix this ? (training mode)
RuntimeError: CUDA out of memory. Tried to allocate 338.00 MiB (GPU 0; 3.95 GiB total capacity; 1.62 GiB already allocated; 23.00 MiB free; 9.10 MiB cached)
	</description>
	<comments>
		<comment id='1' author='centurions' date='2019-09-09T13:33:58Z'>
		&lt;denchmark-link:https://github.com/centurions&gt;@centurions&lt;/denchmark-link&gt;
 your GPU is out of memory. You can use a smaller batch-size to reduce memory requirements. Instead of the default  you could try  etc.
		</comment>
		<comment id='2' author='centurions' date='2019-09-09T13:37:07Z'>
		Thank youuuuuuuuuuuuuuu
		</comment>
		<comment id='3' author='centurions' date='2019-09-10T07:19:59Z'>
		&lt;denchmark-link:https://github.com/centurions&gt;@centurions&lt;/denchmark-link&gt;
 Hi . thank you for your great work,when i changed batch-size to 16 and accumulate to 4,i still get the memory problem,how could I do?Could I try to run python3 train.py --batch-size 8 --accumulate 4ï¼Œand what effect will it have on the results?
		</comment>
		<comment id='4' author='centurions' date='2019-09-10T08:42:50Z'>
		&lt;denchmark-link:https://github.com/broliao&gt;@broliao&lt;/denchmark-link&gt;
 yes you can keep making your batch-size smaller and smaller. Larger batches are better for more stable training, but --accumulate can help you use smaller batches while performing optimizer steps on accumulated batch gradients to mimic a large batch.
The least memory using setting would be --batch-size 1 --accumulate 64 to mimic a batch size of 64.
		</comment>
	</comments>
</bug>