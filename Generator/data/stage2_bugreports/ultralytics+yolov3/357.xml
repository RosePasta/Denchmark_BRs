<bug id='357' author='oludash02' open_date='2019-07-02T13:14:46Z' closed_time='2019-07-03T00:12:44Z'>
	<summary>UnboundLocalError</summary>
	<description>
I training on my own dataset using the updated train.py code but got this error
Traceback (most recent call last):
File "train.py", line 341, in 
accumulate=opt.accumulate)
File "train.py", line 279, in train
if best_loss == test_loss:
UnboundLocalError: local variable 'best_loss' referenced before assignment
The previous code (before the changes) ran smoothly
	</description>
	<comments>
		<comment id='1' author='oludash02' date='2019-07-02T16:22:42Z'>
		&lt;denchmark-link:https://github.com/oludash02&gt;@oludash02&lt;/denchmark-link&gt;
 yes you are correct! We were able to reproduce this error, which was caused by a recent PR. We've just made a commit &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/a8cf64af31034dfd408663f76e8af37288cb5f2c&gt;a8cf64a&lt;/denchmark-link&gt;
 now that should fix this problem. Please  and try again!
		</comment>
		<comment id='2' author='oludash02' date='2019-07-03T00:12:31Z'>
		Thanks alot &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
! it works now...Do you have any suggestions on how to tune the hyperparameters for smaller datasets (&lt;7000 images single class detection) to avoid overfitting?
		</comment>
		<comment id='3' author='oludash02' date='2019-07-03T12:10:56Z'>
		&lt;denchmark-link:https://github.com/oludash02&gt;@oludash02&lt;/denchmark-link&gt;
 we are in the midst of a bit of hyperparameter tuning at the moment, but honestly the loss function, and the exact form of how the loss is computed is probably not fully optimized yet. There are just too many combinations to test rapidly.
You can optimize your own hyperparameters on your custom dataset though quite simply. This will evolve your hyperparameters on your own custom dataset, using the fitness (i.e. mAP) after epoch 0:
&lt;denchmark-code&gt;!python3 train.py --data data/custom.data --img-size 320 --single-scale --epochs 1 --giou --evolve
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='oludash02' date='2019-07-03T17:11:11Z'>
		Thanks &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='5' author='oludash02' date='2019-07-15T15:33:57Z'>
		Would you kindly let me know of evolving that you used in the select_best_evolve?
		</comment>
		<comment id='6' author='oludash02' date='2019-07-16T21:28:43Z'>
		&lt;denchmark-link:https://github.com/sanazss&gt;@sanazss&lt;/denchmark-link&gt;
 hyperparameter evolution may be accessed simply by adding the --evolve flag to your train.py statement. The current hyperparameters should work well in most situations however, and in any case hyperparameter evolution is an advanced step that you perform after you have already trained successfully and are seeking to improve on existing results.
In your case I'd recommend you simply run a few tutorials first and train normally using the default hyps.
		</comment>
	</comments>
</bug>