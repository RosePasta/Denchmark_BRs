<bug id='539' author='ylsxw' open_date='2019-10-08T13:04:54Z' closed_time='2019-10-25T17:18:51Z'>
	<summary>non-finite loss, ending training  tensor</summary>
	<description>
Hello, what caused it? How can I solve it?
&lt;denchmark-link:https://user-images.githubusercontent.com/52126624/66396400-43d43a80-ea0c-11e9-9ba7-051ea8bc0bec.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/52126624/66397138-c4e00180-ea0d-11e9-90eb-d5afd23c283d.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/52126624/66397529-9a427880-ea0e-11e9-8ca1-a8d6f76056ea.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/52126624/66397274-08d30680-ea0e-11e9-8ba5-7a30d9529d3a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/52126624/66397489-826af480-ea0e-11e9-992b-5e4d49aeeb30.png&gt;&lt;/denchmark-link&gt;

There should be no problem with my training steps.
	</description>
	<comments>
		<comment id='1' author='ylsxw' date='2019-10-16T11:51:46Z'>
		I've seen this before. This is caused by a GIoU loss divergence, but I don't know the exact reason behind it unfortunately. It may be a divide by zero error in the GIoU equation, though we've taken steps to prevent that already. I'll leave this open.
		</comment>
		<comment id='2' author='ylsxw' date='2019-10-17T03:56:56Z'>
		I found that by changing the batch_size,this error can be solved. looking forward you can find out the truth of the error.
		</comment>
		<comment id='3' author='ylsxw' date='2019-10-25T17:18:51Z'>
		&lt;denchmark-link:https://github.com/ylsxw&gt;@ylsxw&lt;/denchmark-link&gt;
 great! Also by lowering the GIoU weight this may go away.
		</comment>
	</comments>
</bug>