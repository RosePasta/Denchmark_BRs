<bug id='899' author='HuanglnQuan' open_date='2020-03-06T08:36:03Z' closed_time='2020-03-07T04:45:12Z'>
	<summary>The classification loss of verification set is much larger than that of training set</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Problem&lt;/denchmark-h&gt;

I am training my custom data sets with yolov3-spp. And my data sets have three classes(including background).
The model converges quickly, but the classification loss of verification set is always much larger than that of training set. I'm really confused about it.
&lt;denchmark-link:https://github.com/HuanglnQuan/Javaproject/blob/master/Snipaste_2020-03-06_16-31-39.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='HuanglnQuan' date='2020-03-06T19:06:23Z'>
		&lt;denchmark-link:https://github.com/HuanglnQuan&gt;@HuanglnQuan&lt;/denchmark-link&gt;
 looks fine. If you don't like what you see, you need to experiment with the settings, data, sizes etc.
		</comment>
		<comment id='2' author='HuanglnQuan' date='2020-03-07T03:10:36Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 but why there are such a big difference between val classification loss and train classification loss.
		</comment>
		<comment id='3' author='HuanglnQuan' date='2020-03-07T03:37:24Z'>
		&lt;denchmark-link:https://github.com/HuanglnQuan&gt;@HuanglnQuan&lt;/denchmark-link&gt;
 there's a multitude of factors at play, your dataset, optimizer, hyperparameters, network architecture, loss balancing, image size, multi scale, augmentation, etc etc. I can't teach ML to everyone, I've open sourced these tools to use as a starting point for your research not as a full service free educational service. If you want to understand your data you need to experiment with your setup.
		</comment>
		<comment id='4' author='HuanglnQuan' date='2020-03-07T04:45:12Z'>
		ok, thank you
		</comment>
	</comments>
</bug>