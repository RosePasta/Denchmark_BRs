<bug id='598' author='yiningzeng' open_date='2019-11-08T09:32:02Z' closed_time='2019-11-09T23:11:23Z'>
	<summary>train error</summary>
	<description>

err

Traceback (most recent call last):
  File "train.py", line 432, in &lt;module&gt;
    train()  # train normally
  File "train.py", line 268, in train
    pred = model(imgs)
  File "/root/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 541, in
__call__
    result = self.forward(*input, **kwargs)
  File "/root/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 459, in forward
    self.reducer.prepare_for_backward([])
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:518)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7f1331504813 in /root/anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: c10d::Reducer::prepare_for_backward(std::vector&lt;torch::autograd::Variable, std::allocator&lt;torch::autograd::Variable&gt; &gt; const&amp;) + 0x734 (0x7f1336759654 in /root/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #2: &lt;unknown function&gt; + 0x7bed8a (0x7f1336747d8a in /root/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #3: &lt;unknown function&gt; + 0x210ba4 (0x7f1336199ba4 in /root/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
&lt;omitting python frames&gt;
frame #29: __libc_start_main + 0xe7 (0x7f133a464b97 in /lib/x86_64-linux-gnu/libc.so.6)

environment
python3.7
pytorch1.3.1

	</description>
	<comments>
		<comment id='1' author='yiningzeng' date='2019-11-09T23:11:23Z'>
		Hello, thank you for your interest in our work! This is an automated response. Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove exising repo
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # git clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, Pytorch &gt;= 1.1, etc. You can also use our Google Colab Notebook to test your code in working environment.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
		</comment>
		<comment id='2' author='yiningzeng' date='2019-11-12T08:28:37Z'>
		
Hello, thank you for your interest in our work! This is an automated response. Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove exising repo
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # git clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, Pytorch &gt;= 1.1, etc. You can also use our Google Colab Notebook to test your code in working environment.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!

thanks. I found both batch_size and number of train.txt rows should always be divisible by the number of GPUs
		</comment>
		<comment id='3' author='yiningzeng' date='2019-11-12T22:22:50Z'>
		Yes the batch-size definitely. The number of images used should not matter. coco.data for example has 117263 images, an odd number.
		</comment>
	</comments>
</bug>