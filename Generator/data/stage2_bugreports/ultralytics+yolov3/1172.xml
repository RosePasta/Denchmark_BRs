<bug id='1172' author='vandesa003' open_date='2020-05-13T17:10:04Z' closed_time='2020-05-26T07:06:15Z'>
	<summary>ONNX model output boxes are all zeros.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , first of all, thanks again for your great work. I met this problem after I trained on my own dataset and convert the model to ONNX. While I am running the ONNX model on a normal input image, I got the output like this:
&lt;denchmark-code&gt;boxes:
[0, 0, 0, 0],
[0, 0, 0, 0],
[0, 0, 0, 0],
...
&lt;/denchmark-code&gt;

the classes output seems normal just between 0 and 1.
this is the ONNX model:
&lt;denchmark-link:https://user-images.githubusercontent.com/10252812/81841796-8dc26780-957d-11ea-9f6a-62281aedb318.png&gt;&lt;/denchmark-link&gt;

which I think should be correct.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

: Code to reproduce your issue below
First, I set  in [model.py])(
)
Then, due to the machine env problem, I have to use  in &lt;denchmark-link:https://github.com/ultralytics/yolov3/blob/b2fcfc573e5418c0b2ef0c0357bf51bc5cb027b6/detect.py#L47&gt;detect.py&lt;/denchmark-link&gt;

After this I convert the model to onnx:
&lt;denchmark-code&gt;python detect.py --cfg yolov3-spp.cfg \
    --names data/mydataset.names \
    --weights weights/best.pt \
    --source data/samples \
    --conf-thres 0.3 \
    --iou-thres 0.6
&lt;/denchmark-code&gt;

I will receive a warning during conversion:
&lt;denchmark-code&gt;yolov3/utils/layers.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if nx == na:  # same shape
&lt;/denchmark-code&gt;

which I think is related to 


yolov3/utils/layers.py


         Line 60
      in
      b2fcfc5






 if nx == na:  # same shape 





But I am not sure whether this warning will cause this issue.
After then I run the inference through onnxruntime. and got a normal classes output and a all zero output boxes.
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Expected behavior the boxes definitely not all zeros.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

If applicable, add screenshots to help explain your problem.

OS: [Ubuntu 1604]
GPU [V100]

	</description>
	<comments>
		<comment id='1' author='vandesa003' date='2020-05-13T17:14:27Z'>
		To make things more clear, I also tested with opset_version=11, but still the output boxes are all zeros. I am really confused why this happens. I've been trapped here for near one week... any hints would be appreciated!
		</comment>
		<comment id='2' author='vandesa003' date='2020-05-13T17:19:37Z'>
		&lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 the warning is normal, but opset 9 export is unsupported, so you are on your own if you choose to pass that argument.
Recommend you retry with the latest versions of pytorch and onnx, and opset 10 or 11.
		</comment>
		<comment id='3' author='vandesa003' date='2020-05-13T17:25:47Z'>
		&lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 also make sure you are using the latest code when you convert: run .
		</comment>
		<comment id='4' author='vandesa003' date='2020-05-14T03:57:40Z'>
		
@vandesa003 the warning is normal, but opset 9 export is unsupported, so you are on your own if you choose to pass that argument.
Recommend you retry with the latest versions of pytorch and onnx, and opset 10 or 11.

Sure &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I also tested with , but still receive all zeros boxes. Maybe I missed something related to the onnx version or other environment dependencies. Just make sure if this is a rare case, then it should be environment related issue. Thanks for your reply, I am closing this issue.
		</comment>
		<comment id='5' author='vandesa003' date='2020-05-16T06:40:51Z'>
		Hi &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 , finally I fixed the problem after pulling the new code. Thanks a lot! But I compared the box output from pytorch and onnx model and found that:

pytorch output:

&lt;denchmark-code&gt;tensor([[ 27.06218,  26.60020,  57.13964,  56.54650],
        [ 43.89460,  25.20766,  93.28602,  48.31281],
        [ 85.00977,  25.02055, 152.51794,  48.84522],
        ...,
        [395.99429, 409.55035,  48.99826,  17.14021],
        [402.78732, 409.71115,  35.31765,  18.58157],
        [410.01999, 410.44141,  21.80795,  18.26802]], device='cuda:0')
&lt;/denchmark-code&gt;


onnx output:

&lt;denchmark-code&gt;tensor([[0.0768, 0.0720, 0.0398, 0.0806],
        [0.0993, 0.0679, 0.1353, 0.0163],
        [0.1595, 0.0448, 0.5260, 0.0086],
        ...,
        [0.9423, 0.9808, 0.8031, 0.0051],
        [0.9615, 0.9808, 0.4474, 0.0102],
        [0.9808, 0.9808, 0.2708, 0.0243]])
&lt;/denchmark-code&gt;

Just wonder is the onnx box outputs are normalized? I need to multiply by the image size?
		</comment>
		<comment id='6' author='vandesa003' date='2020-05-16T16:37:37Z'>
		&lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 yes they are normalized. These are the requirements of the the coreml model in
our iDetection app.
		</comment>
		<comment id='7' author='vandesa003' date='2020-05-16T16:49:31Z'>
		
@vandesa003 yes they are normalized. These are the requirements of the the coreml model in
our iDetection app.

&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Oh I see. I tried to restore the result by multiply the image size, but it seems not exact same. How can I restore the exact result?
		</comment>
		<comment id='8' author='vandesa003' date='2020-05-16T17:29:36Z'>
		&lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 actually looking at the code there are no normalization steps, so they should be in pixel space. You can compare how the two outputs are handled here:



yolov3/models.py


        Lines 196 to 217
      in
      3f27ef1






 elif ONNX_EXPORT: 



 # Avoid broadcasting for ANE operations 



 m = self.na * self.nx * self.ny 



 ng = 1. / self.ng.repeat(m, 1) 



 grid = self.grid.repeat(1, self.na, 1, 1, 1).view(m, 2) 



 anchor_wh = self.anchor_wh.repeat(1, 1, self.nx, self.ny, 1).view(m, 2) * ng 



 



 p = p.view(m, self.no) 



 xy = torch.sigmoid(p[:, 0:2]) + grid # x, y 



 wh = torch.exp(p[:, 2:4]) * anchor_wh # width, height 



 p_cls = torch.sigmoid(p[:, 4:5]) if self.nc == 1 else \ 



 torch.sigmoid(p[:, 5:self.no]) * torch.sigmoid(p[:, 4:5])  # conf 



 return p_cls, xy * ng, wh 



 



 else:  # inference 



 io = p.clone()  # inference output 



 io[..., :2] = torch.sigmoid(io[..., :2]) + self.grid # xy 



 io[..., 2:4] = torch.exp(io[..., 2:4]) * self.anchor_wh # wh yolo method 



 io[..., :4] *= self.stride 



 torch.sigmoid_(io[..., 4:]) 



 return io.view(bs, -1, self.no), p # view [1, 3, 13, 13, 85] as [1, 507, 85] 



 





		</comment>
		<comment id='9' author='vandesa003' date='2020-05-16T17:30:43Z'>
		&lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 ah yes, I was correct originally. 1/ng is normalizing it in grid space.
		</comment>
		<comment id='10' author='vandesa003' date='2020-05-20T15:33:43Z'>
		First of all thanks for your work.
I'm trying to use your yolov3-tiny-1cls model into a tensorrt model for Jetson Nano.
I successfully converted the model to a onnx model (opset_version = 10) and to a tensorrt.
The problem is the shape of the output of the onnx model.
If I used the torch inference the prediction has shape (12096, 6) 
While the tensorrt prediction has shape (12096,) , (48384,)  -&gt; (12096, 5)
I just don't know how to use this prediction to draw the bounding boxes etc...
In the torch approach you used the function:
&lt;denchmark-code&gt;def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, multi_label=True, classes=None, agnostic=False):
    """
    Performs  Non-Maximum Suppression on inference results
    Returns detections with shape:
        nx6 (x1, y1, x2, y2, conf, cls)
    """
&lt;/denchmark-code&gt;

The output(prediction) of the torch model: with shape:
&lt;denchmark-code&gt;(1, 12096, 6)
[[[     24.503       23.25      89.051      86.157  0.00035801     0.97608]
  [     49.759      28.121       102.1      55.264   0.0097554     0.98109]
  [      79.55      28.874      125.83      53.467    0.012427     0.98558]
  ...
  [     364.86      508.49      47.539      30.411  7.7272e-05     0.97495]
  [     372.76       508.1      46.075      27.851   7.985e-05     0.97406]
  [     380.88      508.44      41.789      28.087  7.4096e-05     0.97476]]]
&lt;/denchmark-code&gt;

The output(prediction) of tensorrt/onnx model: with shape:
&lt;denchmark-code&gt;(12096,) #classes
(48384,) #boxes

[3.5801044e-04 9.7554326e-03 1.2427208e-02 ... 7.7271565e-05 7.9850302e-05 7.4095879e-05]
[0.06380871 0.04540921 0.23190494 ... 0.99304414 0.10882567 0.05485656]
&lt;/denchmark-code&gt;

I just want to know how to use these values to build the predictions of the model.
Thanks in advance.
		</comment>
		<comment id='11' author='vandesa003' date='2020-05-20T15:45:27Z'>
		&lt;denchmark-link:https://github.com/gasparramoa&gt;@gasparramoa&lt;/denchmark-link&gt;
 use netron to view.
		</comment>
		<comment id='12' author='vandesa003' date='2020-05-20T15:47:06Z'>
		
@gasparramoa use netron to view.

I used, I just don't know how to use the result to build the prediction.
&lt;denchmark-link:https://user-images.githubusercontent.com/44063794/82467391-85a48380-9ab9-11ea-9223-99a896b79f12.png&gt;&lt;/denchmark-link&gt;

Others details of the onnx model:
&lt;denchmark-link:https://user-images.githubusercontent.com/44063794/82467890-1b401300-9aba-11ea-9603-d11764d914ac.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='vandesa003' date='2020-05-20T16:11:56Z'>
		&lt;denchmark-link:https://github.com/gasparramoa&gt;@gasparramoa&lt;/denchmark-link&gt;
 the outputs are the boxes and the confidences (0-1) of each class (looks like you have a single-class model), you can see them right there in your screenshot. What else do you need?
		</comment>
		<comment id='14' author='vandesa003' date='2020-05-25T17:05:02Z'>
		So, what I need to do is to find the max value of confidence and use the bounding boxes for that confidence. Am I right?
		</comment>
		<comment id='15' author='vandesa003' date='2020-05-25T19:33:33Z'>
		&lt;denchmark-link:https://github.com/gasparramoa&gt;@gasparramoa&lt;/denchmark-link&gt;
 I can't advise you on this, if you want please open a new issue as this original issue is resolved.
		</comment>
		<comment id='16' author='vandesa003' date='2020-05-25T19:33:36Z'>
		This issue has been resolved in a commit in early May 2020. If you are having this issue update your code with git pull or clone a new repo.
		</comment>
		<comment id='17' author='vandesa003' date='2020-05-26T07:02:39Z'>
		&lt;denchmark-link:https://github.com/gasparramoa&gt;@gasparramoa&lt;/denchmark-link&gt;
 as I said in previous reply, the output of onnx model is normalised in anchor wise. You only need to remove the normalised process, then everything is ok.
		</comment>
		<comment id='18' author='vandesa003' date='2020-05-26T07:06:15Z'>
		
This issue has been resolved in a commit in early May 2020. If you are having this issue update your code with git pull or clone a new repo.

&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Sorry I should have closed this issue. Now I restored the normalised value and I can use it! thanks again for you guys great work! learned a lot from your repo.
		</comment>
		<comment id='19' author='vandesa003' date='2020-05-26T09:49:45Z'>
		
@gasparramoa as I said in previous reply, the output of onnx model is normalised in anchor wise. You only need to remove the normalised process, then everything is ok.

So I can not restore the result by multiply the image size? What is normalization in achor wise ? Can you give me a simple example of how to remove this normalization process please?
Thanks in advance, I really mean it.
		</comment>
		<comment id='20' author='vandesa003' date='2020-05-26T15:27:41Z'>
		

@gasparramoa as I said in previous reply, the output of onnx model is normalised in anchor wise. You only need to remove the normalised process, then everything is ok.

So I can not restore the result by multiply the image size? What is normalization in achor wise ? Can you give me a simple example of how to remove this normalization process please?
Thanks in advance, I really mean it.

Just multiply by the self.stride.
&lt;denchmark-code&gt;        elif ONNX_EXPORT:
            # Avoid broadcasting for ANE operations
            m = self.na * self.nx * self.ny
            # ng = 1. / self.ng.repeat(m, 1)
            grid = self.grid.repeat(1, self.na, 1, 1, 1).view(m, 2)
            # anchor_wh = self.anchor_wh.repeat(1, 1, self.nx, self.ny, 1).view(m, 2) * ng
            anchor_wh = self.anchor_wh.repeat(1, 1, self.nx, self.ny, 1).view(m, 2)

            p = p.view(m, self.no)
            xy = torch.sigmoid(p[:, 0:2]) + grid  # x, y
            wh = torch.exp(p[:, 2:4]) * anchor_wh  # width, height
            p_cls = torch.sigmoid(p[:, 4:5]) if self.nc == 1 else \
                torch.sigmoid(p[:, 5:self.no]) * torch.sigmoid(p[:, 4:5])  # conf
            # return p_cls, xy * ng, wh
            return p_cls, xy * self.stride, wh * self.stride
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='vandesa003' date='2020-05-27T17:14:24Z'>
		Thank you &lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 !!!
That was it!
Now I have exactly the same result in the torch model and in the TensorRT model.
		</comment>
		<comment id='22' author='vandesa003' date='2020-06-13T18:11:01Z'>
		&lt;denchmark-link:https://github.com/gasparramoa&gt;@gasparramoa&lt;/denchmark-link&gt;
 Where you able to get the onnx model into a tensorRT model? If so, did you use the onnx-tensorRT github for that? If not, what tool did you use and what is your performance?
I would like to look into this more as I am very curious.
Thanks in advance!
		</comment>
		<comment id='23' author='vandesa003' date='2020-06-13T18:17:40Z'>
		&lt;denchmark-link:https://github.com/gasparramoa&gt;@gasparramoa&lt;/denchmark-link&gt;
 @mbufi there's a request for tensorrt on our new repo as well. I personally don't have experience with it, but if you guys have time or suggestions that would be awesome.
&lt;denchmark-link:https://github.com/ultralytics/yolov5/issues/45&gt;ultralytics/yolov5#45&lt;/denchmark-link&gt;

There is a tensorrt export here as well that is already working for this repo:
&lt;denchmark-link:https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp&gt;https://github.com/wang-xinyu/tensorrtx/tree/master/yolov3-spp&lt;/denchmark-link&gt;

		</comment>
		<comment id='24' author='vandesa003' date='2020-06-13T18:36:43Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Yes I saw! Thanks for the suggestion. I may look into it.
		</comment>
		<comment id='25' author='vandesa003' date='2020-06-24T22:00:15Z'>
		&lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/gasparramoa&gt;@gasparramoa&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
  I'm running into some issues where the torch output and onnx model outputs do not match in the current version of the repo. Steps to reproduce:

First, I set ONNX_EXPORT = True and ran detect.py to generate an onnx file

&lt;denchmark-code&gt;python detect.py --cfg ./cfg/yolov3-spp.cfg --weights weights/yolov3-spp.pt
&lt;/denchmark-code&gt;



Then, I set ONNX_EXPORT = False and run detect.py normally and as expected, the outputs look correct on the sample images.


Then, I wanted to try running with onnx_runtime using my new onnx file. To do this, I replaced the pred = model(img, augment=opt.augment)[0] call in detect.py (so all the normal image preprocessing still runs) with the following:


&lt;denchmark-code&gt;session = onnxruntime.InferenceSession('weights/yolov3-spp.onnx')
in_img = {session.get_inputs()[0].name: img.numpy()}
out = session.run(None, in_img)[0]
&lt;/denchmark-code&gt;

However, when I was debugging, I saw that the onnxruntime output and the pytorch model outputs do not match:
pytorch output (after running inference but before nms):
&lt;denchmark-code&gt;tensor([[[1.89963e+01, 1.56430e+01, 2.04850e+02,  ..., 1.42084e-03, 1.65047e-03, 8.64788e-04],
         [4.88579e+01, 2.42638e+01, 1.55053e+02,  ..., 1.70676e-03, 1.44675e-03, 7.56415e-04],
         [8.29035e+01, 2.43567e+01, 1.74981e+02,  ..., 2.03217e-03, 1.57435e-03, 5.97334e-04],
         ...,
         [2.99602e+02, 1.88690e+02, 8.93882e+01,  ..., 1.16396e-03, 3.20018e-04, 2.71256e-04],
         [3.06881e+02, 1.88730e+02, 8.49935e+01,  ..., 2.39168e-03, 6.58945e-04, 7.91102e-04],
         [3.16741e+02, 1.88525e+02, 9.01153e+01,  ..., 1.65509e-03, 1.31225e-03, 1.66143e-03]]], grad_fn=&lt;CatBackward&gt;)
&lt;/denchmark-code&gt;

onnx runtime output (after running inference but before nms):
&lt;denchmark-code&gt;array([[ 1.2182e-07,    2.07e-09,  6.1938e-09, ...,  2.7896e-09,  6.1711e-10,  2.6199e-10],
       [  6.162e-06,   5.007e-08,  5.9215e-08, ...,  4.0838e-08,  1.0652e-08,  2.6904e-09],
       [ 3.3448e-05,  2.9604e-07,  2.3664e-07, ...,  1.4082e-07,  5.1649e-08,  7.7577e-09],
       ...,
       [ 3.7442e-05,  4.1841e-07,  1.5349e-06, ...,  2.9282e-07,  1.8697e-08,  3.0575e-08],
       [  8.224e-06,  1.4772e-07,  6.8398e-07, ...,  1.1183e-07,  6.8377e-09,  1.1774e-08],
       [ 8.2141e-07,  2.1218e-08,  6.2566e-08, ...,  1.6764e-08,  2.4703e-09,   3.212e-09]], dtype=float32)
&lt;/denchmark-code&gt;

I added the fix from &lt;denchmark-link:https://github.com/vandesa003&gt;@vandesa003&lt;/denchmark-link&gt;
 () in models.py but I'm still getting this issue. Any ideas why this might be happening?
		</comment>
		<comment id='26' author='vandesa003' date='2020-06-24T22:25:28Z'>
		&lt;denchmark-link:https://github.com/prathik-naidu&gt;@prathik-naidu&lt;/denchmark-link&gt;
 we offer model export to onnx and coreml as a service.  If you are interested please send us a request via email.
		</comment>
		<comment id='27' author='vandesa003' date='2020-06-24T22:44:52Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I'm just running on the current open source yolov3 code given that it has ONNX export functionality. Does this not work? I'm using the default yolov3-spp.cfg and yolov3-spp.pt files that are from the repo but still not able to match the outputs between pytorch and onnxruntime.
		</comment>
		<comment id='28' author='vandesa003' date='2020-06-24T22:55:07Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 yes, there is limited export functionality available here! If you can get by with this then great :)
		</comment>
		<comment id='29' author='vandesa003' date='2020-06-24T23:26:02Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 I see so just to clarify, what is currently possible with the export functionality in this repo? It seems like there is capability to export to an onnx file but that onnx file doesn't actually replicate the results of the pytorch model. Is that expected?
Is there something that needs to be changed with this open source code to get that working (not sure if I'm missing something) or does this functionality not exist?
		</comment>
		<comment id='30' author='vandesa003' date='2020-06-24T23:32:09Z'>
		&lt;denchmark-link:https://github.com/prathik-naidu&gt;@prathik-naidu&lt;/denchmark-link&gt;
 export works as intended here. If you need additional help we can provide it as a service.
		</comment>
		<comment id='31' author='vandesa003' date='2020-06-25T14:11:46Z'>
		
@glenn-jocher I see so just to clarify, what is currently possible with the export functionality in this repo? It seems like there is capability to export to an onnx file but that onnx file doesn't actually replicate the results of the pytorch model. Is that expected?
Is there something that needs to be changed with this open source code to get that working (not sure if I'm missing something) or does this functionality not exist?

So can't we use the exported onnx model normally? I wanted to use OPENCV of C + + to call the exported onnx model, and then use C + + reasoning to deploy the project. But if the prediction result of onnx model is not correct, does that mean that the result of subsequent deployment will also be incorrect
		</comment>
		<comment id='32' author='vandesa003' date='2020-06-25T14:21:49Z'>
		&lt;denchmark-link:https://github.com/sky-fly97&gt;@sky-fly97&lt;/denchmark-link&gt;
 export operates correctly.
		</comment>
		<comment id='33' author='vandesa003' date='2020-06-25T14:33:52Z'>
		
@sky-fly97 export operates correctly.

Oh, Thank you! I see that the above person said that the output of the exported onnx model is quite inconsistent with the original pytorch model, so I have such a question.By the way, thank you very much for your work, which is really important
		</comment>
		<comment id='34' author='vandesa003' date='2020-06-25T15:39:37Z'>
		&lt;denchmark-link:https://github.com/sky-fly97&gt;@sky-fly97&lt;/denchmark-link&gt;
 Let me know if you are able to get consistent results with your work. I'm still not able to figure out why the exported onnx model generates different results from the pytorch model (even on simple inputs like torch.ones). If export works correctly, I assume that means the model that is loaded from the onnx file should also work as well?
		</comment>
		<comment id='35' author='vandesa003' date='2020-06-26T03:29:16Z'>
		
@sky-fly97Â¶ÇÊûú‰Ω†ËÉΩÂíå‰Ω†ÁöÑÂ∑•‰ΩúÂèñÂæó‰∏ÄËá¥ÁöÑÁªìÊûúÔºåËØ∑ÂëäËØâÊàë„ÄÇÊàë‰ªçÁÑ∂Êó†Ê≥ïÂºÑÊ∏ÖÊ•ö‰∏∫‰ªÄ‰πàÂØºÂá∫ÁöÑonnxÊ®°Âûã‰ºö‰∫ßÁîü‰∏épyÊâãÁîµÊ®°Âûã‰∏çÂêåÁöÑÁªìÊûú(Âç≥‰ΩøÊòØÂú®ÂÉètorch.onesËøôÊ†∑ÁöÑÁÆÄÂçïËæìÂÖ•‰∏ä)„ÄÇÂ¶ÇÊûúÂØºÂá∫Â∑•‰ΩúÊ≠£Â∏∏ÔºåÊàëÂÅáËÆæËøôÊÑèÂë≥ÁùÄ‰ªéonnxÊñá‰ª∂Âä†ËΩΩÁöÑÊ®°Âûã‰πüÂ∫îËØ•Â∑•‰ΩúÂêóÔºü

OK„ÄÇI will try. I have another question, why does the onnx model take (320Ôºå192) as the input size.Does it matter?
		</comment>
		<comment id='36' author='vandesa003' date='2020-07-30T03:36:42Z'>
		
Ë∞¢Ë∞¢@ vandesa003 !!!
Â∞±ÊòØËøôÊ†∑ÔºÅ
Áé∞Âú®ÔºåÂú®Ââ≤ÁÇ¨Ê®°ÂûãÂíåTensorRTÊ®°Âûã‰∏≠ÔºåÊàëÂæóÂà∞ÁöÑÁªìÊûúÂÆåÂÖ®Áõ∏Âêå„ÄÇ

Hello, I have also successfully converted the downloaded yolov3.weights into onnx, but the error in converting to tensor RT is as follows:
[TensorRT] ERROR: Network must have at least one output
Traceback (most recent call last):
context = engine.create_ execution_ context()
AttributeError: 'NoneType' object has no attribute 'create_ execution_ context'
		</comment>
		<comment id='37' author='vandesa003' date='2020-08-21T12:50:38Z'>
		&lt;denchmark-link:https://github.com/prathik-naidu&gt;@prathik-naidu&lt;/denchmark-link&gt;
 , I have results similar to yours (onnx pred probabilities around 10e-7), have you figured it out ?
		</comment>
		<comment id='38' author='vandesa003' date='2020-09-15T12:10:04Z'>
		
Hi @glenn-jocher , finally I fixed the problem after pulling the new code. Thanks a lot! But I compared the box output from pytorch and onnx model and found that:
pytorch output:
tensor([[ 27.06218,  26.60020,  57.13964,  56.54650],
[ 43.89460,  25.20766,  93.28602,  48.31281],
[ 85.00977,  25.02055, 152.51794,  48.84522],
...,
[395.99429, 409.55035,  48.99826,  17.14021],
[402.78732, 409.71115,  35.31765,  18.58157],
[410.01999, 410.44141,  21.80795,  18.26802]], device='cuda:0')
onnx output:
tensor([[0.0768, 0.0720, 0.0398, 0.0806],
[0.0993, 0.0679, 0.1353, 0.0163],
[0.1595, 0.0448, 0.5260, 0.0086],
...,
[0.9423, 0.9808, 0.8031, 0.0051],
[0.9615, 0.9808, 0.4474, 0.0102],
[0.9808, 0.9808, 0.2708, 0.0243]])
Just wonder is the onnx box outputs are normalized? I need to multiply by the image size?

hi , my onnx model have two output
box (10647, 4)
class (10647, 2).
how to decode this output and get final result? could you give me some advice?
do you did it?
could you share your code?
thanksÔºÅÔºÅÔºÅÔºÅ
		</comment>
	</comments>
</bug>