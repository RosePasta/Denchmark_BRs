<bug id='875' author='changyu98' open_date='2020-02-27T08:52:50Z' closed_time='2020-02-27T23:59:20Z'>
	<summary>Parameter 'epoch' unfilled.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I got this warning message
warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
                              "initialization. Please, make sure to call `optimizer.step()` before "
                              "`lr_scheduler.step()`. See more details at "
                              "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

This problem exists in train.py 305 line.
I propose to change it to scheduler.step(epoch)

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Clear this error
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

Python==3.8.2
PyTorch==1.4
CUDA==10.1
CUDNN==7.4
Desktop (please complete the following information):

OS: [Ubuntu]
Version [18.04]

Smartphone (please complete the following information):

Device: [iPhoneX]
OS: [iOS]
Version [13.3]

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Good luck it.
	</description>
	<comments>
		<comment id='1' author='changyu98' date='2020-02-27T09:42:09Z'>
		&lt;denchmark-link:https://github.com/changyu98&gt;@changyu98&lt;/denchmark-link&gt;
 I don‚Äôt think you can pass a variable to step() like you show. See &lt;denchmark-link:https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&gt;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='changyu98' date='2020-02-27T11:02:12Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

I think I found another mistake.
In the code, you called mixed precision training, which is great because I am also using it, but you put after , which will result in .
I changed it.
I give my suggestions for changes:
    # Scheduler https://github.com/ultralytics/yolov3/issues/238
    # lf = lambda x: 1 - x / epochs  # linear ramp to zero
    # lf = lambda x: 10 ** (hyp['lrf'] * x / epochs)  # exp ramp
    # lf = lambda x: 1 - 10 ** (hyp['lrf'] * (1 - x / epochs))  # inverse exp ramp
    # lf = lambda x: 0.5 * (1 + math.cos(x * math.pi / epochs))  # cosine https://arxiv.org/pdf/1812.01187.pdf
    # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)
    # scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=range(59, 70, 1), gamma=0.8)  # gradual fall to 0.1*lr0
    if mixed_precision:
        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)
    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[round(epochs * x) for x in [0.8, 0.9]], gamma=0.1)
    scheduler.last_epoch = start_epoch
It is good works.
		</comment>
		<comment id='3' author='changyu98' date='2020-02-27T11:12:23Z'>
		Yes, I agree with you, this may be an error prompt that the authorities did not correct in time, this is the source code information, it seems that he does not rely heavily on the epoch parameter.
        with _enable_get_lr_call(self):
            if epoch is None:
                self.last_epoch += 1
                values = self.get_lr()
            else:
                warnings.warn(EPOCH_DEPRECATION_WARNING, DeprecationWarning)
                self.last_epoch = epoch
                if hasattr(self, "_get_closed_form_lr"):
                    values = self._get_closed_form_lr()
                else:
                    values = self.get_lr()
		</comment>
		<comment id='4' author='changyu98' date='2020-02-27T21:45:40Z'>
		
@glenn-jocher
I think I found another mistake.
In the code, you called mixed precision training, which is great because I am also using it, but you put amp.init() after scheduler, which will result in Seems like optimizer.step () has been overridden after learning rate scheduler initialization.
I changed it.
I give my suggestions for changes:
    # Scheduler https://github.com/ultralytics/yolov3/issues/238
    # lf = lambda x: 1 - x / epochs  # linear ramp to zero
    # lf = lambda x: 10 ** (hyp['lrf'] * x / epochs)  # exp ramp
    # lf = lambda x: 1 - 10 ** (hyp['lrf'] * (1 - x / epochs))  # inverse exp ramp
    # lf = lambda x: 0.5 * (1 + math.cos(x * math.pi / epochs))  # cosine https://arxiv.org/pdf/1812.01187.pdf
    # scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)
    # scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=range(59, 70, 1), gamma=0.8)  # gradual fall to 0.1*lr0
    if mixed_precision:
        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)
    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[round(epochs * x) for x in [0.8, 0.9]], gamma=0.1)
    scheduler.last_epoch = start_epoch
It is good works.

Ah, yes, this is a great idea! I've fixed this now in &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/d5815ebfd28ef44deeae0b61c8dc41e18f4197cf&gt;d5815eb&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>