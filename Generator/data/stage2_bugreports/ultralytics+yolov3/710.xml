<bug id='710' author='taylan24' open_date='2019-12-12T13:23:02Z' closed_time='2020-01-16T19:16:26Z'>
	<summary>one-class custom trained weights didn't worked in other platforms such as openvino, c++</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I trained the yolov3 model for wehicle detection from a fisheye camera,
the weights are working in detect.py code.
however I have to run the model with a c++ code in order to deploy the model to our devices.
I used exactly same .cfg file, darknet weights and coco.names in c++ code, but it detects nothing.
the original yolov3.weights works properly with the c++ code, but my custom one-class training code didn't worked. I tried to reduce conf threshold, even I removed the threshold, the model finds nothing.
What is my mistake?
Thank you in advance.
	</description>
	<comments>
		<comment id='1' author='taylan24' date='2019-12-12T18:05:54Z'>
		&lt;denchmark-link:https://github.com/taylan24&gt;@taylan24&lt;/denchmark-link&gt;
 I don't understand. What c code?
		</comment>
		<comment id='2' author='taylan24' date='2019-12-12T18:12:24Z'>
		the code is here. I suppose, the input of the model was incorrect, but I couldn't find.
the official pretrained yolov3.weights is working perfectly in the following code, but my custom class weights cannot find any objects.
`#include 
//#include "opencv2/opencv.hpp"
#include &lt;opencv2/opencv.hpp&gt;
#include &lt;opencv2/core/core.hpp&gt;
#include &lt;opencv2/highgui/highgui.hpp&gt;
#include &lt;opencv2/imgproc/imgproc.hpp&gt;
#include &lt;opencv2/videoio/videoio.hpp&gt;
#include &lt;opencv2/dnn/dnn.hpp&gt;
//lib dnn ekle
#include 
#include 
#include 
#include 
#include 
#include 
using namespace cv;
using namespace std;
#define confThreshold 0.01
#define nmsThreshold 0.01
std::vectorstd::string classes;
void drawPred(int classId, float conf, int left, int top, int right, int bottom, Mat&amp; frame)
{
&lt;denchmark-code&gt;rectangle(frame, Point(left, top), Point(right, bottom), Scalar(0, 255, 0));
std::string label = format("%.2f", conf);
if (!classes.empty())
{
    CV_Assert(classId &lt; (int)classes.size());
    label = classes[classId] + ": " + label;
}
int baseLine;

Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 0.5, 1, &amp;baseLine);
top = max(top, labelSize.height);

rectangle(frame, Point(left, top - labelSize.height),
          Point(left + labelSize.width, top + baseLine), Scalar::all(255), FILLED);
putText(frame, label, Point(left, top), FONT_HERSHEY_SIMPLEX, 0.5, Scalar());
&lt;/denchmark-code&gt;

}
// Get the names of the output layers
vector getOutputsNames(const cv::dnn::Net&amp; net)
{
static vector names;
if (names.empty())
{
//Get the indices of the output layers, i.e. the layers with unconnected outputs
vector outLayers = net.getUnconnectedOutLayers();
&lt;denchmark-code&gt;    //get the names of all the layers in the network
    vector&lt;String&gt; layersNames = net.getLayerNames();

    // Get the names of the output layers in names
    names.resize(outLayers.size());
    for (size_t i = 0; i &lt; outLayers.size(); ++i)
    names[i] = layersNames[outLayers[i] - 1];
}
return names;
&lt;/denchmark-code&gt;

}
// Remove the bounding boxes with low confidence using non-maxima suppression
void postprocess(Mat&amp; frame, const vector&amp; outs)
{
&lt;denchmark-code&gt;//qDebug() &lt;&lt; "inside post";
vector&lt;int&gt; classIds;
vector&lt;float&gt; confidences;
vector&lt;Rect&gt; boxes;


int tObjCount = 0;

//qDebug() &lt;&lt; "outs.size(" &lt;&lt; outs.size();
for (size_t i = 0; i &lt; outs.size(); ++i)
{
    // Scan through all the bounding boxes output from the network and keep only the
    // ones with high confidence scores. Assign the box's class label as the class
    // with the highest score for the box.
    float* data = (float*)outs[i].data;
    for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols)
    {
        Mat scores = outs[i].row(j).colRange(5, outs[i].cols);
        Point classIdPoint;
        double confidence;
        // Get the value and location of the maximum score

        minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint);
        //qDebug() &lt;&lt; "confidence: " &lt;&lt; confidence;
        if (confidence &gt; confThreshold)
        {
            int centerX = (int)(data[0] * frame.cols);
            int centerY = (int)(data[1] * frame.rows);
            int width = (int)(data[2] * frame.cols);
            int height = (int)(data[3] * frame.rows);
            int left = centerX - width / 2;
            int top = centerY - height / 2;

            classIds.push_back(classIdPoint.x);
            confidences.push_back((float)confidence);
            boxes.push_back(Rect(left, top, width, height));
        }
    }
}

// Perform non maximum suppression to eliminate redundant overlapping boxes with
// lower confidences
vector&lt;int&gt; indices;
&lt;/denchmark-code&gt;

//    qDebug() &lt;&lt; "num of boxes: " &lt;&lt; boxes.size();
//    qDebug() &lt;&lt; "num of confidences: " &lt;&lt; confidences.size();
&lt;denchmark-code&gt;cv::dnn::NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices);
&lt;/denchmark-code&gt;

//    qDebug() &lt;&lt; "indices size: " &lt;&lt; indices.size();
&lt;denchmark-code&gt;for (size_t i = 0; i &lt; indices.size(); ++i)
{
    int idx = indices[i];
    Rect box = boxes[idx];

    drawPred(classIds[idx], confidences[idx], box.x, box.y,
             box.x + box.width, box.y + box.height, frame);
}
&lt;/denchmark-code&gt;

}
int main(int argc, char *argv[])
{
QCoreApplication a(argc, argv);
&lt;denchmark-code&gt;QElapsedTimer timer;

std::string labelsPath = "/home/taylan/yolov3_birdeye/data/coco.names";
std::string weightsPath = "/home/taylan/yolov3_birdeye/weights/best.weights";
std::string configPath = "/home/taylan/yolov3_birdeye/cfg/yolov3-1cls.cfg";



ifstream ifs(labelsPath.c_str());
string line;
while(getline(ifs, line)) classes.push_back(line);


qDebug() &lt;&lt; "[INFO] loading YOLO from disk...";
cv::dnn::Net net = cv::dnn::readNetFromDarknet(configPath, weightsPath);
&lt;/denchmark-code&gt;

//    net.setPreferableBackend(DNN_BACKEND_OPENCV);
//    net.setPreferableTarget(DNN_TARGET_CPU);
&lt;denchmark-code&gt;//auto ln = net.getLayerNames();

// Runs the forward pass to get output of the output layers

VideoCapture cap("/home/taylan/out_fisheye_test.avi");


if(!cap.isOpened()){
  cout &lt;&lt; "Error opening video stream or file" &lt;&lt; endl;
  return -1;
}
std::cout &lt;&lt; "here" &lt;&lt; std::endl;
cap.set(CAP_PROP_POS_FRAMES, 40);

int counter = 0;
while(true){
    counter++;
    QString filename = QString::number(counter);

    Mat frame;
    cap &gt;&gt; frame;

    if (frame.empty()){
        break;
    }

    Mat dst;

    timer.start();

    imshow("orig", frame);

    auto blob = cv::dnn::blobFromImage(frame, 1./255.0, cv::Size(416,416),cv::Scalar(), true, false);

    net.setInput(blob);
    vector&lt;Mat&gt; outs;
    net.forward(outs, getOutputsNames(net));

    // Remove the bounding boxes with low confidence

    postprocess(frame, outs);

    // Put efficiency information. The function getPerfProfile returns the
        // overall time for inference(t) and the timings for each of the layers(in layersTimes)
    vector&lt;double&gt; layersTimes;
    double freq = getTickFrequency() / 1000;
    double t = net.getPerfProfile(layersTimes) / freq;
    string label = format("Inference time for a frame : %.2f ms", t);
    putText(frame, label, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 0, 255));
    //qDebug() &lt;&lt; "elapsed time is: " &lt;&lt; timer.elapsed();

    Mat detectedFrame;
    frame.convertTo(detectedFrame, CV_8U);

    cv::imshow("detected", detectedFrame);

    char c = (char) waitKey(25);
    if(c==27)break;
}


return a.exec();
&lt;/denchmark-code&gt;

}
`
		</comment>
		<comment id='3' author='taylan24' date='2019-12-12T18:13:16Z'>
		the main part is here
`while(true){
counter++;
QString filename = QString::number(counter);
&lt;denchmark-code&gt;Mat frame;
cap &gt;&gt; frame;

if (frame.empty()){
    break;
}

Mat dst;

timer.start();

imshow("orig", frame);

auto blob = cv::dnn::blobFromImage(frame, 1./255.0, cv::Size(416,416),cv::Scalar(), true, false);

net.setInput(blob);
vector&lt;Mat&gt; outs;
net.forward(outs, getOutputsNames(net));

// Remove the bounding boxes with low confidence

postprocess(frame, outs);

// Put efficiency information. The function getPerfProfile returns the
    // overall time for inference(t) and the timings for each of the layers(in layersTimes)
vector&lt;double&gt; layersTimes;
double freq = getTickFrequency() / 1000;
double t = net.getPerfProfile(layersTimes) / freq;
string label = format("Inference time for a frame : %.2f ms", t);
putText(frame, label, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 0, 255));
//qDebug() &lt;&lt; "elapsed time is: " &lt;&lt; timer.elapsed();

Mat detectedFrame;
frame.convertTo(detectedFrame, CV_8U);

cv::imshow("detected", detectedFrame);

char c = (char) waitKey(25);
if(c==27)break;
&lt;/denchmark-code&gt;

}
`
		</comment>
		<comment id='4' author='taylan24' date='2019-12-12T18:14:31Z'>
		&lt;denchmark-link:https://github.com/taylan24&gt;@taylan24&lt;/denchmark-link&gt;
 well this clearly seems to be your responsibility to implement if you are trying to build custom solutions, there's nothing we can do for you.
You can export to ONNX, and from there to a variety of other formats. I suggest you look into that approach as it's more standardized.
		</comment>
		<comment id='5' author='taylan24' date='2020-01-16T19:16:26Z'>
		After reviewing your question we believe that this issue falls outside of the scope of this repository, which is limited to PyTorch and ONNX model training, inference and deployment.
We suggest you raise the issue directly under the package or source causing the problem. We will close this issue for now.
		</comment>
	</comments>
</bug>