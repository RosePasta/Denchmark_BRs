<bug id='384' author='ifahim' open_date='2019-07-22T20:20:15Z' closed_time='2019-07-25T00:11:32Z'>
	<summary>dist_url is not defined</summary>
	<description>
Recently i added a second gpu on my machine. Earlier the train.py use to work without any issue but now I started seeing following error:
python train.py  --cfg 'yolov3.cfg' --data-cfg 'yolov3_pytorch.data'  --chkpt_freq 10  --epochs 200
Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2080', total_memory=7952MB) device1 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11177MB) loading darknet weights ........ Traceback (most recent call last): File "train.py", line 337, in &lt;module&gt; chkpt_freq = 10 File "train.py", line 141, in train dist.init_process_group(backend=backend, init_method=dist_url, world_size=world_size, rank=rank) NameError: name 'dist_url' is not defined 
Not sure why I started getting this.  Including this in the command does not help. dist_url='tcp://127.0.0.1:9999'

How to solve this issue?
I want to train on only one of my GPU. Is there a way to tell the code to use only one of the GPU?

	</description>
	<comments>
		<comment id='1' author='ifahim' date='2019-07-23T10:50:56Z'>
		&lt;denchmark-link:https://github.com/ifahim&gt;@ifahim&lt;/denchmark-link&gt;
 yes this makes sense. The distributed training settings are now inlined directly where they are used, whereas before they were defined at the argparser. These default settings work fine with multiple GPUs, we just tested this a couple days ago with 2 V100s.


		</comment>
		<comment id='2' author='ifahim' date='2019-07-23T10:53:02Z'>
		&lt;denchmark-link:https://github.com/ifahim&gt;@ifahim&lt;/denchmark-link&gt;
 BTW I should also say that we have been getting disappointing speedups using multi-gpu, there is an open issue on the topic: &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/269&gt;#269&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>