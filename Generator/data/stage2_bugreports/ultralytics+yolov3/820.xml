<bug id='820' author='PEAKWEI' open_date='2020-01-31T12:53:17Z' closed_time='2020-01-31T12:54:33Z'>
	<summary>The following error occurred with the parameter --adam</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

A clear and concise description of what the bug is.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

The following error occurred with the parameter --adam

&lt;denchmark-code&gt;raceback (most recent call last):
  File "train.py", line 437, in &lt;module&gt;
    train()  # train normally
  File "train.py", line 295, in train
    optimizer.step()
  File "D:\Program Files\Python\Python37\lib\site-packages\torch\optim\lr_scheduler.py", line 66, in wrapper
    return wrapped(*args, **kwargs)
  File "D:\Program Files\Python\Python37\lib\site-packages\torch\optim\adam.py", line 82, in step
    exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
KeyError: 'exp_avg'
  0%|

&lt;/denchmark-code&gt;

	</description>
	<comments>
	</comments>
</bug>