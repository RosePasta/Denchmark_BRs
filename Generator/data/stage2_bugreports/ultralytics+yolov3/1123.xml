<bug id='1123' author='xiao12mm' open_date='2020-05-03T05:07:23Z' closed_time='2020-06-27T00:16:32Z'>
	<summary>`WARNING: non-finite loss, ending training  tensor([nan, nan, 0., nan], device='cuda:0')`</summary>
	<description>
Before submitting a bug report, please ensure that you are using the latest versions of:

python 3.7
torch  1.5.0
this repo (run git status and git pull)

This error occurs after a part of training, and then the training is terminated，what 's wrong with it?
WARNING: non-finite loss, ending training  tensor([nan, nan, 0., nan], device='cuda:0')

OS: [e.g. Ubuntu 18.04]
GPU [e.g. 2080 Ti]

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='xiao12mm' date='2020-05-03T05:08:40Z'>
		Hello &lt;denchmark-link:https://github.com/xiao12mm&gt;@xiao12mm&lt;/denchmark-link&gt;
, thank you for your interest in our work! Please visit our &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;Custom Training Tutorial&lt;/denchmark-link&gt;
 to get started, and see our &lt;denchmark-link:https://github.com/ultralytics/yolov3/blob/master/tutorial.ipynb&gt;Google Colab Notebook&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://hub.docker.com/r/ultralytics/yolov3&gt;Docker Image&lt;/denchmark-link&gt;
, and &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart&gt;GCP Quickstart Guide&lt;/denchmark-link&gt;
 for example environments.
If this is a bug report, please provide screenshots and minimum viable code to reproduce your issue, otherwise we can not help you.
		</comment>
		<comment id='2' author='xiao12mm' date='2020-05-03T05:47:00Z'>
		&lt;denchmark-link:https://github.com/xiao12mm&gt;@xiao12mm&lt;/denchmark-link&gt;
 loss instabilities. Many causes, search issues. Solution is to decrease LR, increase burnin, verify labels in train*.jpg and test*.jpg.
		</comment>
		<comment id='3' author='xiao12mm' date='2020-05-03T14:56:37Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;

please help me
the picture of BUG
The problem occurred in the 61th epoch
···
&lt;denchmark-link:https://img-blog.csdnimg.cn/20200503141234949.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW8xM21t,size_16,color_FFFFFF,t_70&gt;https://img-blog.csdnimg.cn/20200503141234949.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW8xM21t,size_16,color_FFFFFF,t_70&lt;/denchmark-link&gt;

···
		</comment>
		<comment id='4' author='xiao12mm' date='2020-05-22T04:57:42Z'>
		&lt;denchmark-link:https://github.com/xiao12mm&gt;@xiao12mm&lt;/denchmark-link&gt;
  if you've figured it out can you tell me how?
		</comment>
		<comment id='5' author='xiao12mm' date='2020-05-22T05:08:32Z'>
		
@glenn-jocher
please help me
the picture of BUG
The problem occurred in the 61th epoch
···
https://img-blog.csdnimg.cn/20200503141234949.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYW8xM21t,size_16,color_FFFFFF,t_70
···

&lt;denchmark-link:https://github.com/xiao12mm&gt;@xiao12mm&lt;/denchmark-link&gt;
 your giou loss is clearly unstable, you should try reducing it's hyperparameter hyp['giou'] in train.py.
		</comment>
		<comment id='6' author='xiao12mm' date='2020-06-22T00:16:15Z'>
		This issue is stale because it has been open 30 days with no activity. Remove Stale label or comment or this will be closed in 5 days.
		</comment>
	</comments>
</bug>