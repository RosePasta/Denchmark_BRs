<bug id='167' author='xiaoyunl' open_date='2019-03-26T07:08:37Z' closed_time='2019-03-26T17:05:52Z'>
	<summary>train my own data ,the MAP are always be 0.</summary>
	<description>
I use the train.py to train my own data , my data has only 1 class . I have  changed the .data and .cfg . and run python train.py  followed  this tutorial &lt;denchmark-link:https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&gt;https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data&lt;/denchmark-link&gt;

it can run success. but i found there maybe has some problems. the MAP are always be 0.
&lt;denchmark-link:https://user-images.githubusercontent.com/36145304/54977452-5b038b80-4fd8-11e9-8753-2cd42ec7c915.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='xiaoyunl' date='2019-03-26T14:37:04Z'>
		&lt;denchmark-link:https://github.com/xiaoyunl&gt;@xiaoyunl&lt;/denchmark-link&gt;
 in your first comment, the mAP is likely zero because you used an older version of the repo. Use the absolute latest version. Also, it looks like you had zero images trained before you tried to run mAP. If you train on zero images your mAP will of course be zero.
In the second comment this is a well known issue with object detection and pytorch in general, NMS is slow and CPU intensive. See &lt;denchmark-link:https://github.com/pytorch/vision/issues/392&gt;pytorch/vision#392&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='xiaoyunl' date='2019-03-26T17:05:20Z'>
		&lt;denchmark-link:https://github.com/xiaoyunl&gt;@xiaoyunl&lt;/denchmark-link&gt;
 I've pushed a commit &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/47eab968abdb3be99e638c25e4bad8590b62abb7&gt;47eab96&lt;/denchmark-link&gt;
 which may alleviate some NMS speed issues. It enforces a hard limit of 100 boxes per image per class at the expense of about 0.001 mAP on the COCO test group (i.e. from 55.1 to 55.0 mAP).
Your main problem though I believe lies in your custom data, which you probably have not formatted correctly.
		</comment>
		<comment id='3' author='xiaoyunl' date='2019-03-27T00:30:14Z'>
		(py36) dasen@dasen-server:~/00project/yolov3-master$ python train.py
Namespace(accumulate=1, backend='nccl', batch_size=16, cfg='cfg/yolov3.cfg', data_cfg='cfg/coco.data', dist_url='tcp://192.168.1.249:22', epochs=270, img_size=416, multi_scale=False, num_workers=32, rank=0, resume=False, world_size=1)
Using cuda _CudaDeviceProperties(name='GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10986MB, multi_processor_count=68)
layer                                     name  gradient   parameters                shape         mu      sigma
0                          0.conv_0.weight      True          864        [32, 3, 3, 3]   -0.00339     0.0648
1                    0.batch_norm_0.weight      True           32                 [32]      0.987       1.07
2                      0.batch_norm_0.bias      True           32                 [32]     -0.698       2.07
3                          1.conv_1.weight      True        18432       [64, 32, 3, 3]   0.000298     0.0177
4                    1.batch_norm_1.weight      True           64                 [64]       0.88      0.389
5                      1.batch_norm_1.bias      True           64                 [64]     -0.409       1.01
6                          2.conv_2.weight      True         2048       [32, 64, 1, 1]   -0.00301     0.0306
7                    2.batch_norm_2.weight      True           32                 [32]      0.895      0.327
8                      2.batch_norm_2.bias      True           32                 [32]      -0.37      0.752
9                          3.conv_3.weight      True        18432       [64, 32, 3, 3]  -0.000943     0.0163
10                    3.batch_norm_3.weight      True           64                 [64]      0.912      0.923
11                      3.batch_norm_3.bias      True           64                 [64]     -0.477      0.534
12                          5.conv_5.weight      True        73728      [128, 64, 3, 3]  -0.000371     0.0137
13                    5.batch_norm_5.weight      True          128                [128]       1.03      0.338
14                      5.batch_norm_5.bias      True          128                [128]    -0.0887      0.888
15                          6.conv_6.weight      True         8192      [64, 128, 1, 1]   0.000322     0.0126
16                    6.batch_norm_6.weight      True           64                 [64]      0.571      0.218
17                      6.batch_norm_6.bias      True           64                 [64]     -0.397      0.829
18                          7.conv_7.weight      True        73728      [128, 64, 3, 3]  -0.000418    0.00752
19                    7.batch_norm_7.weight      True          128                [128]      0.429      0.456
20                      7.batch_norm_7.bias      True          128                [128]     -0.362      0.552
21                          9.conv_9.weight      True         8192      [64, 128, 1, 1]   -0.00196     0.0213
22                    9.batch_norm_9.weight      True           64                 [64]      0.902      0.308
23                      9.batch_norm_9.bias      True           64                 [64]     -0.392      0.442
24                        10.conv_10.weight      True        73728      [128, 64, 3, 3]  -0.000514     0.0108
25                  10.batch_norm_10.weight      True          128                [128]       0.94      0.629
26                    10.batch_norm_10.bias      True          128                [128]     -0.571      0.423
27                        12.conv_12.weight      True       294912     [256, 128, 3, 3]  -0.000328     0.0101
28                  12.batch_norm_12.weight      True          256                [256]      0.965      0.348
29                    12.batch_norm_12.bias      True          256                [256]       -0.1      0.741
30                        13.conv_13.weight      True        32768     [128, 256, 1, 1]  -3.11e-05    0.00793
31                  13.batch_norm_13.weight      True          128                [128]      0.683      0.297
32                    13.batch_norm_13.bias      True          128                [128]     -0.352        0.6
33                        14.conv_14.weight      True       294912     [256, 128, 3, 3]  -0.000135    0.00368
34                  14.batch_norm_14.weight      True          256                [256]       0.23      0.348
35                    14.batch_norm_14.bias      True          256                [256]     -0.464      0.328
36                        16.conv_16.weight      True        32768     [128, 256, 1, 1]  -0.000212       0.01
37                  16.batch_norm_16.weight      True          128                [128]      0.876      0.206
38                    16.batch_norm_16.bias      True          128                [128]     -0.337      0.307
39                        17.conv_17.weight      True       294912     [256, 128, 3, 3]  -0.000162    0.00487
40                  17.batch_norm_17.weight      True          256                [256]      0.412      0.301
41                    17.batch_norm_17.bias      True          256                [256]      -0.45       0.28
42                        19.conv_19.weight      True        32768     [128, 256, 1, 1]   -0.00077     0.0119
43                  19.batch_norm_19.weight      True          128                [128]      0.835      0.192
44                    19.batch_norm_19.bias      True          128                [128]     -0.408      0.375
45                        20.conv_20.weight      True       294912     [256, 128, 3, 3]  -0.000226    0.00547
46                  20.batch_norm_20.weight      True          256                [256]      0.533      0.402
47                    20.batch_norm_20.bias      True          256                [256]     -0.575       0.25
48                        22.conv_22.weight      True        32768     [128, 256, 1, 1]  -0.000538     0.0117
49                  22.batch_norm_22.weight      True          128                [128]      0.879      0.163
50                    22.batch_norm_22.bias      True          128                [128]     -0.364      0.308
51                        23.conv_23.weight      True       294912     [256, 128, 3, 3]   -0.00019    0.00565
52                  23.batch_norm_23.weight      True          256                [256]      0.544      0.402
53                    23.batch_norm_23.bias      True          256                [256]     -0.624      0.249
54                        25.conv_25.weight      True        32768     [128, 256, 1, 1]  -0.000797     0.0129
55                  25.batch_norm_25.weight      True          128                [128]      0.826      0.161
56                    25.batch_norm_25.bias      True          128                [128]     -0.446      0.346
57                        26.conv_26.weight      True       294912     [256, 128, 3, 3]  -0.000427      0.006
58                  26.batch_norm_26.weight      True          256                [256]      0.725      0.392
59                    26.batch_norm_26.bias      True          256                [256]     -0.744      0.257
60                        28.conv_28.weight      True        32768     [128, 256, 1, 1]  -0.000794     0.0131
61                  28.batch_norm_28.weight      True          128                [128]      0.831      0.143
62                    28.batch_norm_28.bias      True          128                [128]     -0.416      0.383
63                        29.conv_29.weight      True       294912     [256, 128, 3, 3]  -0.000323    0.00593
64                  29.batch_norm_29.weight      True          256                [256]      0.754      0.478
65                    29.batch_norm_29.bias      True          256                [256]      -0.73      0.276
66                        31.conv_31.weight      True        32768     [128, 256, 1, 1]   -0.00116     0.0142
67                  31.batch_norm_31.weight      True          128                [128]      0.768      0.159
68                    31.batch_norm_31.bias      True          128                [128]     -0.546      0.368
69                        32.conv_32.weight      True       294912     [256, 128, 3, 3]  -0.000317    0.00606
70                  32.batch_norm_32.weight      True          256                [256]      0.842      0.468
71                    32.batch_norm_32.bias      True          256                [256]     -0.711      0.225
72                        34.conv_34.weight      True        32768     [128, 256, 1, 1]   -0.00119     0.0127
73                  34.batch_norm_34.weight      True          128                [128]      0.866      0.113
74                    34.batch_norm_34.bias      True          128                [128]     -0.294      0.423
75                        35.conv_35.weight      True       294912     [256, 128, 3, 3]  -0.000133    0.00557
76                  35.batch_norm_35.weight      True          256                [256]      0.843      0.581
77                    35.batch_norm_35.bias      True          256                [256]     -0.672      0.218
78                        37.conv_37.weight      True  1.17965e+06     [512, 256, 3, 3]  -0.000165    0.00683
79                  37.batch_norm_37.weight      True          512                [512]       1.12      0.293
80                    37.batch_norm_37.bias      True          512                [512]     -0.727      0.579
81                        38.conv_38.weight      True       131072     [256, 512, 1, 1]  -0.000107    0.00573
82                  38.batch_norm_38.weight      True          256                [256]      0.942      0.149
83                    38.batch_norm_38.bias      True          256                [256]    -0.0305      0.316
84                        39.conv_39.weight      True  1.17965e+06     [512, 256, 3, 3]  -4.88e-05    0.00244
85                  39.batch_norm_39.weight      True          512                [512]      0.232      0.392
86                    39.batch_norm_39.bias      True          512                [512]     -0.543      0.206
87                        41.conv_41.weight      True       131072     [256, 512, 1, 1]  -0.000329     0.0062
88                  41.batch_norm_41.weight      True          256                [256]      0.887      0.175
89                    41.batch_norm_41.bias      True          256                [256]     -0.312       0.32
90                        42.conv_42.weight      True  1.17965e+06     [512, 256, 3, 3]  -5.83e-05     0.0032
91                  42.batch_norm_42.weight      True          512                [512]      0.446       0.39
92                    42.batch_norm_42.bias      True          512                [512]     -0.591      0.229
93                        44.conv_44.weight      True       131072     [256, 512, 1, 1]  -0.000549    0.00837
94                  44.batch_norm_44.weight      True          256                [256]      0.703      0.127
95                    44.batch_norm_44.bias      True          256                [256]     -0.666      0.288
96                        45.conv_45.weight      True  1.17965e+06     [512, 256, 3, 3]  -0.000209      0.004
97                  45.batch_norm_45.weight      True          512                [512]      0.539      0.343
98                    45.batch_norm_45.bias      True          512                [512]     -0.599      0.217
99                        47.conv_47.weight      True       131072     [256, 512, 1, 1]   -0.00087    0.00907
100                  47.batch_norm_47.weight      True          256                [256]      0.741      0.143
101                    47.batch_norm_47.bias      True          256                [256]     -0.576      0.373
102                        48.conv_48.weight      True  1.17965e+06     [512, 256, 3, 3]  -0.000128    0.00413
103                  48.batch_norm_48.weight      True          512                [512]      0.591      0.286
104                    48.batch_norm_48.bias      True          512                [512]     -0.698      0.197
105                        50.conv_50.weight      True       131072     [256, 512, 1, 1]  -0.000895    0.00897
106                  50.batch_norm_50.weight      True          256                [256]      0.747      0.123
107                    50.batch_norm_50.bias      True          256                [256]     -0.608      0.281
108                        51.conv_51.weight      True  1.17965e+06     [512, 256, 3, 3]  -0.000171    0.00397
109                  51.batch_norm_51.weight      True          512                [512]      0.611      0.379
110                    51.batch_norm_51.bias      True          512                [512]     -0.727      0.212
111                        53.conv_53.weight      True       131072     [256, 512, 1, 1]  -0.000844    0.00956
112                  53.batch_norm_53.weight      True          256                [256]      0.658      0.103
113                    53.batch_norm_53.bias      True          256                [256]     -0.729      0.244
114                        54.conv_54.weight      True  1.17965e+06     [512, 256, 3, 3]  -0.000186    0.00426
115                  54.batch_norm_54.weight      True          512                [512]      0.695      0.326
116                    54.batch_norm_54.bias      True          512                [512]     -0.698      0.196
117                        56.conv_56.weight      True       131072     [256, 512, 1, 1]  -0.000739     0.0082
118                  56.batch_norm_56.weight      True          256                [256]      0.911     0.0725
119                    56.batch_norm_56.bias      True          256                [256]     -0.373      0.176
120                        57.conv_57.weight      True  1.17965e+06     [512, 256, 3, 3]  -0.000133    0.00355
121                  57.batch_norm_57.weight      True          512                [512]      0.603      0.454
122                    57.batch_norm_57.bias      True          512                [512]     -0.679      0.189
123                        59.conv_59.weight      True       131072     [256, 512, 1, 1]   -0.00104     0.0104
124                  59.batch_norm_59.weight      True          256                [256]      0.731     0.0927
125                    59.batch_norm_59.bias      True          256                [256]     -0.631      0.275
126                        60.conv_60.weight      True  1.17965e+06     [512, 256, 3, 3]  -5.31e-05    0.00411
127                  60.batch_norm_60.weight      True          512                [512]      0.765       0.36
128                    60.batch_norm_60.bias      True          512                [512]     -0.653       0.17
129                        62.conv_62.weight      True  4.71859e+06    [1024, 512, 3, 3]  -0.000182    0.00485
130                  62.batch_norm_62.weight      True         1024               [1024]      0.969      0.258
131                    62.batch_norm_62.bias      True         1024               [1024]     -0.968      0.393
132                        63.conv_63.weight      True       524288    [512, 1024, 1, 1]  -0.000384    0.00566
133                  63.batch_norm_63.weight      True          512                [512]      0.821      0.216
134                    63.batch_norm_63.bias      True          512                [512]      -0.43      0.337
135                        64.conv_64.weight      True  4.71859e+06    [1024, 512, 3, 3]  -8.66e-05    0.00278
136                  64.batch_norm_64.weight      True         1024               [1024]      0.386      0.303
137                    64.batch_norm_64.bias      True         1024               [1024]     -0.449      0.211
138                        66.conv_66.weight      True       524288    [512, 1024, 1, 1]  -0.000645    0.00721
139                  66.batch_norm_66.weight      True          512                [512]      0.659      0.112
140                    66.batch_norm_66.bias      True          512                [512]     -0.743      0.276
141                        67.conv_67.weight      True  4.71859e+06    [1024, 512, 3, 3]  -7.31e-05    0.00334
142                  67.batch_norm_67.weight      True         1024               [1024]       0.51      0.289
143                    67.batch_norm_67.bias      True         1024               [1024]     -0.586      0.196
144                        69.conv_69.weight      True       524288    [512, 1024, 1, 1]  -0.000693    0.00694
145                  69.batch_norm_69.weight      True          512                [512]      0.708        0.1
146                    69.batch_norm_69.bias      True          512                [512]     -0.684      0.254
147                        70.conv_70.weight      True  4.71859e+06    [1024, 512, 3, 3]  -7.92e-05    0.00305
148                  70.batch_norm_70.weight      True         1024               [1024]        0.5      0.373
149                    70.batch_norm_70.bias      True         1024               [1024]     -0.622      0.214
150                        72.conv_72.weight      True       524288    [512, 1024, 1, 1]  -0.000817    0.00957
151                  72.batch_norm_72.weight      True          512                [512]       0.67      0.111
152                    72.batch_norm_72.bias      True          512                [512]     -0.857      0.347
153                        73.conv_73.weight      True  4.71859e+06    [1024, 512, 3, 3]  -6.56e-05    0.00375
154                  73.batch_norm_73.weight      True         1024               [1024]      0.848      0.295
155                    73.batch_norm_73.bias      True         1024               [1024]      -0.82      0.289
156                        75.conv_75.weight      True       524288    [512, 1024, 1, 1]   2.47e-05      0.018
157                  75.batch_norm_75.weight      True          512                [512]      0.516      0.288
158                    75.batch_norm_75.bias      True          512                [512]          0          0
159                        76.conv_76.weight      True  4.71859e+06    [1024, 512, 3, 3]    2.8e-06     0.0085
160                  76.batch_norm_76.weight      True         1024               [1024]      0.511       0.29
161                    76.batch_norm_76.bias      True         1024               [1024]          0          0
162                        77.conv_77.weight      True       524288    [512, 1024, 1, 1]  -4.85e-05      0.018
163                  77.batch_norm_77.weight      True          512                [512]      0.495      0.289
164                    77.batch_norm_77.bias      True          512                [512]          0          0
165                        78.conv_78.weight      True  4.71859e+06    [1024, 512, 3, 3]  -3.46e-08    0.00851
166                  78.batch_norm_78.weight      True         1024               [1024]      0.487      0.292
167                    78.batch_norm_78.bias      True         1024               [1024]          0          0
168                        79.conv_79.weight      True       524288    [512, 1024, 1, 1]  -3.48e-05      0.018
169                  79.batch_norm_79.weight      True          512                [512]      0.505      0.299
170                    79.batch_norm_79.bias      True          512                [512]          0          0
171                        80.conv_80.weight      True  4.71859e+06    [1024, 512, 3, 3]  -2.65e-06    0.00851
172                  80.batch_norm_80.weight      True         1024               [1024]      0.495      0.294
173                    80.batch_norm_80.bias      True         1024               [1024]          0          0
174                        81.conv_81.weight      True        18432     [18, 1024, 1, 1]  -1.48e-05     0.0181
175                          81.conv_81.bias      True           18                 [18]  -0.000668     0.0159
176                        84.conv_84.weight      True       131072     [256, 512, 1, 1]  -0.000106     0.0255
177                  84.batch_norm_84.weight      True          256                [256]      0.498      0.291
178                    84.batch_norm_84.bias      True          256                [256]          0          0
179                        87.conv_87.weight      True       196608     [256, 768, 1, 1]    1.8e-06     0.0208
180                  87.batch_norm_87.weight      True          256                [256]      0.514      0.298
181                    87.batch_norm_87.bias      True          256                [256]          0          0
182                        88.conv_88.weight      True  1.17965e+06     [512, 256, 3, 3]   3.08e-08      0.012
183                  88.batch_norm_88.weight      True          512                [512]      0.523      0.288
184                    88.batch_norm_88.bias      True          512                [512]          0          0
185                        89.conv_89.weight      True       131072     [256, 512, 1, 1]    -0.0001     0.0256
186                  89.batch_norm_89.weight      True          256                [256]      0.447      0.279
187                    89.batch_norm_89.bias      True          256                [256]          0          0
188                        90.conv_90.weight      True  1.17965e+06     [512, 256, 3, 3]   6.63e-06      0.012
189                  90.batch_norm_90.weight      True          512                [512]      0.515      0.284
190                    90.batch_norm_90.bias      True          512                [512]          0          0
191                        91.conv_91.weight      True       131072     [256, 512, 1, 1]  -9.55e-05     0.0255
192                  91.batch_norm_91.weight      True          256                [256]      0.484      0.292
193                    91.batch_norm_91.bias      True          256                [256]          0          0
194                        92.conv_92.weight      True  1.17965e+06     [512, 256, 3, 3]  -5.17e-06      0.012
195                  92.batch_norm_92.weight      True          512                [512]      0.515      0.288
196                    92.batch_norm_92.bias      True          512                [512]          0          0
197                        93.conv_93.weight      True         9216      [18, 512, 1, 1]   -0.00029     0.0255
198                          93.conv_93.bias      True           18                 [18]    0.00318     0.0266
199                        96.conv_96.weight      True        32768     [128, 256, 1, 1]   0.000481     0.0361
200                  96.batch_norm_96.weight      True          128                [128]      0.506      0.298
201                    96.batch_norm_96.bias      True          128                [128]          0          0
202                        99.conv_99.weight      True        49152     [128, 384, 1, 1]  -0.000129     0.0294
203                  99.batch_norm_99.weight      True          128                [128]      0.506       0.29
204                    99.batch_norm_99.bias      True          128                [128]          0          0
205                      100.conv_100.weight      True       294912     [256, 128, 3, 3]   1.55e-05      0.017
206                100.batch_norm_100.weight      True          256                [256]      0.511      0.275
207                  100.batch_norm_100.bias      True          256                [256]          0          0
208                      101.conv_101.weight      True        32768     [128, 256, 1, 1]  -0.000298     0.0361
209                101.batch_norm_101.weight      True          128                [128]      0.485        0.3
210                  101.batch_norm_101.bias      True          128                [128]          0          0
211                      102.conv_102.weight      True       294912     [256, 128, 3, 3]   2.59e-05      0.017
212                102.batch_norm_102.weight      True          256                [256]      0.512      0.302
213                  102.batch_norm_102.bias      True          256                [256]          0          0
214                      103.conv_103.weight      True        32768     [128, 256, 1, 1]  -0.000203     0.0361
215                103.batch_norm_103.weight      True          128                [128]      0.526      0.264
216                  103.batch_norm_103.bias      True          128                [128]          0          0
217                      104.conv_104.weight      True       294912     [256, 128, 3, 3]  -9.48e-06      0.017
218                104.batch_norm_104.weight      True          256                [256]      0.519      0.286
219                  104.batch_norm_104.bias      True          256                [256]          0          0
220                      105.conv_105.weight      True         4608      [18, 256, 1, 1]   0.000118     0.0359
221                        105.conv_105.bias      True           18                 [18]    0.00203     0.0387
Model Summary: 222 layers, 6.15237e+07 parameters, 6.15237e+07 gradients
Epoch       Batch        xy        wh      conf       cls     total  nTargets      time
Using cuda _CudaDeviceProperties(name='GeForce RTX 2080 Ti', major=7, minor=5, total_memory=10986MB, multi_processor_count=68)
&lt;denchmark-code&gt;  Image      Total          P          R        mAP
&lt;/denchmark-code&gt;

100%|???????????????????????????????????????????????????????????????????????????????????????????????????????????????????| 30/30 [12:38:15&lt;00:00, 245.94s/it]
467        467          0          0          0       54.7s
mAP Per Class:
Traceback (most recent call last):
File "train.py", line 229, in 
num_workers=opt.num_workers
File "train.py", line 197, in train
file.write(s + '%11.3g' * 3 % (P, R, mAP) + '\n')
UnboundLocalError: local variable 's' referenced before assignment
mAP is zero ,and report a error local variable 's' referenced before assignment.
		</comment>
		<comment id='4' author='xiaoyunl' date='2019-03-27T00:32:03Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/36145304/55042029-c9912980-506a-11e9-9bc0-00e09a8229a3.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='xiaoyunl' date='2019-03-27T07:10:20Z'>
		s isn't initialized
		</comment>
		<comment id='6' author='xiaoyunl' date='2019-03-27T14:41:57Z'>
		&lt;denchmark-link:https://github.com/xiaoyunl&gt;@xiaoyunl&lt;/denchmark-link&gt;
 yes I see that. It looks like you have no training data though. I don't see any batches reported during your training.
We have an updated branch that will merge soon with more accurate mAP calculations, you could try this if you want.
&lt;denchmark-link:https://github.com/ultralytics/yolov3/tree/map_update&gt;https://github.com/ultralytics/yolov3/tree/map_update&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='xiaoyunl' date='2019-05-15T14:15:01Z'>
		I've met the same problem. I also found that the label is none. Do you know how to fix it? Thanks! &lt;denchmark-link:https://github.com/xiaoyunl&gt;@xiaoyunl&lt;/denchmark-link&gt;

Nerver mind.. Iâ€˜ve solved it.I didn't normalized the bbox in labels.
		</comment>
		<comment id='8' author='xiaoyunl' date='2019-05-15T14:26:55Z'>
		&lt;denchmark-link:https://github.com/kylesjtu&gt;@kylesjtu&lt;/denchmark-link&gt;
 Hello, thank you for your interest in our work! This is an automated response. Please note that most technical problems are due to:

Your changes to the default repository. If your issue is not reproducible in a fresh git clone version of this repository we can not debug it. Before going further run this code and ensure your issue persists:

sudo rm -rf yolov3  # remove exising repo
git clone https://github.com/ultralytics/yolov3 &amp;&amp; cd yolov3 # git clone latest
python3 detect.py  # verify detection
python3 train.py  # verify training (a few batches only)
# CODE TO REPRODUCE YOUR ISSUE HERE

Your custom data. If your issue is not reproducible with COCO data we can not debug it. Visit our Custom Training Tutorial for exact details on how to format your custom data. Examine train_batch0.jpg and test_batch0.jpg for a sanity check of training and testing data.
Your environment. If your issue is not reproducible in a GCP Quickstart Guide VM we can not debug it. Ensure you meet the requirements specified in the README: Unix, MacOS, or Windows with Python &gt;= 3.7, Pytorch &gt;= 1.0, etc.

If none of these apply to you, we suggest you close this issue and raise a new one using the Bug Report template, providing screenshots and minimum viable code to reproduce your issue. Thank you!
		</comment>
	</comments>
</bug>