<bug id='902' author='jvnext1' open_date='2020-03-07T07:09:54Z' closed_time='2020-03-12T01:17:03Z'>
	<summary>Loss Increase after Resuming from last.pt</summary>
	<description>
&lt;denchmark-link:https://user-images.githubusercontent.com/61899861/76138747-37b8db80-6085-11ea-8497-62f971c16c48.png&gt;&lt;/denchmark-link&gt;

I stop after training 50 epoch , and then resume to train next 50epoch. But the loss got bad. is this normalï¼Ÿ
	</description>
	<comments>
		<comment id='1' author='jvnext1' date='2020-03-07T07:16:20Z'>
		&lt;denchmark-link:https://github.com/jvnext1&gt;@jvnext1&lt;/denchmark-link&gt;
 yes this happen sometimes when resuming saved models. I'm not sure what the cause is, as we follow all of the pytorch best practices for saving model and optimizer dictionaries, and we set the scheduler last_epoch value as well.
The best solution is to train the model from start to finish rather than stopping unfortunately.
		</comment>
		<comment id='2' author='jvnext1' date='2020-03-11T15:21:29Z'>
		We had the same problem and I think I found a solution. When resuming from a checkpoint the optimizer is loaded into the  but no  argument is added in the constructor. It is set afterwards as  in train.py. This leads to a bug as the constructor has already initialized the scheduler in a way that was dependent on the  argument to be set (see source code of lr_scheduler for details &lt;denchmark-link:https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#LambdaLR&gt;https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#LambdaLR&lt;/denchmark-link&gt;
).
The proposed solution is to pass the correct start_epoch in the constructor as such:
scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf,last_epoch = start_epoch)
Note that if you resume training and also change the total number of episodes, this will lead to another learning rate being calculated as the lr_lambda function is dependent on the total number of episodes.
Hope this will help and thank you for an otherwise excellent implementation!
		</comment>
		<comment id='3' author='jvnext1' date='2020-03-11T17:36:31Z'>
		@Papibarozza ah thank you for your proposed fix. Yes this sounds like it may be a bug! Have you had success in implementing this fix?
Also yes, resuming training with different --epochs will lead to bad things, especially with a cosine LR scheduler.
TODO: Implement proposed fix in &lt;denchmark-link:https://github.com/ultralytics/yolov3/issues/902#issuecomment-597696202&gt;#902 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='jvnext1' date='2020-03-11T19:23:08Z'>
		@Papibarozza commit &lt;denchmark-link:https://github.com/ultralytics/yolov3/commit/320f9c6601ae1bddae036a5094dfe81ac1441cc3&gt;320f9c6&lt;/denchmark-link&gt;
 should fix this now. Passing last_epoch = start_epoch led to an error, so I used start_epoch - 1, which I think is better, since at epoch 0 this will read at -1, the default value.
I think this might create the scheduler warning again though, i will need to check. Ah, no everything seems fine. Ok great! I have not checked whether this fixes the error on resume training, but it doesn't seem to be hurting anything at the start of training, so I will leave it as is following the commit.
		</comment>
		<comment id='5' author='jvnext1' date='2020-03-11T19:40:34Z'>
		&lt;denchmark-link:https://github.com/glenn-jocher&gt;@glenn-jocher&lt;/denchmark-link&gt;
 Yes we resumed training with this fix and completed some epochs, everything works as expected and we are seeing similar trends in the loss curve and metrics as we did before stopping the training. I think setting  like you propose will give us an off by one error when resuming error though so maybe there needs to be some more code to make this right.
		</comment>
		<comment id='6' author='jvnext1' date='2020-03-11T20:59:26Z'>
		@Papibarozza awesome!! This problem has been with us for over a year, I'm glad this finally solves it. Thank you for your solution!!
		</comment>
	</comments>
</bug>