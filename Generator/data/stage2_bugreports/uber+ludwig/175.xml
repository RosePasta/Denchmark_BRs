<bug id='175' author='Riz-waan' open_date='2019-03-06T04:10:02Z' closed_time='2019-03-07T06:09:40Z'>
	<summary>OOM Error on CSV</summary>
	<description>
I get an out of memory error when training on google's colab, I am not sure how to fix this, or if there is any way to fix it. I am trying to experiment on cornell movie curpses changed into a csv file made for a chat bot. Any help would be appreciated.
&lt;denchmark-code&gt;2019-03-06 04:01:20.392192: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 2.63GiB
2019-03-06 04:01:20.392223: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: 
Limit:                 10784227328
InUse:                  2820005888
MaxInUse:               4702584832
NumAllocs:                   49545
MaxAllocSize:           2621833216

2019-03-06 04:01:20.392630: W tensorflow/core/common_runtime/bfc_allocator.cc:271] ******************************_________________________________________**********************_______
2019-03-06 04:01:20.392733: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at pad_op.cc:137 : Resource exhausted: OOM when allocating tensor with shape[128,256,20003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1334, in _do_call
    return fn(*args)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[128,256,20003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node user2/loss_user2/Pad}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[{{node user2/loss_user2/mean_loss_user2}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/bin/ludwig", line 10, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 86, in main
    CLI()
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 64, in __init__
    getattr(self, args.command)()
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 67, in experiment
    experiment.cli(sys.argv[2:])
  File "/usr/local/lib/python3.6/dist-packages/ludwig/experiment.py", line 607, in cli
    experiment(**vars(args))
  File "/usr/local/lib/python3.6/dist-packages/ludwig/experiment.py", line 281, in experiment
    debug=debug
  File "/usr/local/lib/python3.6/dist-packages/ludwig/train.py", line 426, in train
    **model_definition['training']
  File "/usr/local/lib/python3.6/dist-packages/ludwig/models/model.py", line 515, in train
    is_training=True
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 929, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1328, in _do_run
    run_metadata)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[128,256,20003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node user2/loss_user2/Pad (defined at /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/loss_modules.py:66) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[node user2/loss_user2/mean_loss_user2 (defined at /usr/local/lib/python3.6/dist-packages/ludwig/features/sequence_feature.py:501) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'user2/loss_user2/Pad', defined at:
  File "/usr/local/bin/ludwig", line 10, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 86, in main
    CLI()
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 64, in __init__
    getattr(self, args.command)()
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 67, in experiment
    experiment.cli(sys.argv[2:])
  File "/usr/local/lib/python3.6/dist-packages/ludwig/experiment.py", line 607, in cli
    experiment(**vars(args))
  File "/usr/local/lib/python3.6/dist-packages/ludwig/experiment.py", line 281, in experiment
    debug=debug
  File "/usr/local/lib/python3.6/dist-packages/ludwig/train.py", line 409, in train
    debug=debug
  File "/usr/local/lib/python3.6/dist-packages/ludwig/models/model.py", line 110, in __init__
    **kwargs
  File "/usr/local/lib/python3.6/dist-packages/ludwig/models/model.py", line 184, in __build
    is_training=self.is_training
  File "/usr/local/lib/python3.6/dist-packages/ludwig/models/outputs.py", line 43, in build_outputs
    **kwargs
  File "/usr/local/lib/python3.6/dist-packages/ludwig/models/outputs.py", line 93, in build_single_output
    **kwargs
  File "/usr/local/lib/python3.6/dist-packages/ludwig/features/base_feature.py", line 314, in concat_dependencies_and_build_output
    **kwargs
  File "/usr/local/lib/python3.6/dist-packages/ludwig/features/sequence_feature.py", line 250, in build_output
    kwarg=kwargs
  File "/usr/local/lib/python3.6/dist-packages/ludwig/features/sequence_feature.py", line 299, in build_sequence_output
    class_biases
  File "/usr/local/lib/python3.6/dist-packages/ludwig/features/sequence_feature.py", line 497, in sequence_loss
    eval_logits
  File "/usr/local/lib/python3.6/dist-packages/ludwig/models/modules/loss_modules.py", line 66, in seq2seq_sequence_loss
    padded_logits = tf.pad(logits, [[0, 0], [0, difference], [0, 0]])
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py", line 2299, in pad
    result = gen_array_ops.pad(tensor, paddings, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 5539, in pad
    "Pad", input=input, paddings=paddings, name=name)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 3300, in create_op
    op_def=op_def)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,256,20003] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node user2/loss_user2/Pad (defined at /usr/local/lib/python3.6/dist-packages/ludwig/models/modules/loss_modules.py:66) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[node user2/loss_user2/mean_loss_user2 (defined at /usr/local/lib/python3.6/dist-packages/ludwig/features/sequence_feature.py:501) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Riz-waan' date='2019-03-06T06:03:12Z'>
		This depends on your model definition, your data, your batch size and a bunch of other things. Fundamentally your model is too big to fit in ram (unlikely) or you activations are too bigto fit in rum (more likely). The first case is solved decreasing the size of the model, the second is resolved decreasing the size of the model too and by reducing the batch size. Please specify everything that is needed for debugging, including a sample of the data and your full model definition.
		</comment>
		<comment id='2' author='Riz-waan' date='2019-03-06T23:22:55Z'>
		Model Definition: &lt;denchmark-link:https://drive.google.com/file/d/16whMsuxe5Akyg492M9oLzF18xZxG3gYi/view?usp=sharing&gt;https://drive.google.com/file/d/16whMsuxe5Akyg492M9oLzF18xZxG3gYi/view?usp=sharing&lt;/denchmark-link&gt;

Data Sample: &lt;denchmark-link:https://drive.google.com/file/d/1--uaO8zJixPJVuwKR0mad3C3qg4JCipP/view?usp=sharing&gt;https://drive.google.com/file/d/1--uaO8zJixPJVuwKR0mad3C3qg4JCipP/view?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 Above are the links to data and model definition. Please let me know if you need anything else
		</comment>
		<comment id='3' author='Riz-waan' date='2019-03-06T23:27:49Z'>
		Data and model look fine. What GPU are you using? How much memory does it have?
		</comment>
		<comment id='4' author='Riz-waan' date='2019-03-06T23:30:15Z'>
		&lt;denchmark-code&gt;╒══════════╕
│ TRAINING │
╘══════════╛

2019-03-06 04:00:57.944428: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-03-06 04:00:57.947824: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x30ca520 executing computations on platform Host. Devices:
2019-03-06 04:00:57.947898: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-03-06 04:00:58.023725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-03-06 04:00:58.024279: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x30ca680 executing computations on platform CUDA. Devices:
2019-03-06 04:00:58.024336: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2019-03-06 04:00:58.024767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 10.62GiB
2019-03-06 04:00:58.024811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-03-06 04:00:58.369062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-06 04:00:58.369151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-03-06 04:00:58.369179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-03-06 04:00:58.369518: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-03-06 04:00:58.369588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10284 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)

&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 Tesla K80 and 10 gigs I think, I am not sure, it is the gpu option in google colab.
		</comment>
		<comment id='5' author='Riz-waan' date='2019-03-06T23:36:39Z'>
		This is really weird, with 12gb it should be completely fine. You don't have additional preprocessing or anything else right?
Please try the following things:

remove the attention and remove reduce_output (will tell us if the problem is in the attention mechanism)
try a small batch size, for instance 8, to see if the problem is in the activations
try both things at the same time

		</comment>
		<comment id='6' author='Riz-waan' date='2019-03-06T23:40:02Z'>
		Thanks I'll try but Google Colab gave me this warning:
&lt;denchmark-link:https://user-images.githubusercontent.com/24848114/53921651-3f434000-403f-11e9-8b0f-22ffb2195f58.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='Riz-waan' date='2019-03-06T23:40:59Z'>
		I wonder if you closed the model before. If you don't close it, that could be the reason, memory was not released.
		</comment>
		<comment id='8' author='Riz-waan' date='2019-03-06T23:41:25Z'>
		But when I press manage sessions it goes to:
&lt;denchmark-link:https://user-images.githubusercontent.com/24848114/53921716-70237500-403f-11e9-9a7c-dffdb18bbdee.png&gt;&lt;/denchmark-link&gt;

Which clearly doesn't show the 10 gigs being used.
		</comment>
		<comment id='9' author='Riz-waan' date='2019-03-06T23:46:59Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 What do you mean close the model? How do I do that?
		</comment>
		<comment id='10' author='Riz-waan' date='2019-03-07T00:08:08Z'>
		Here is my notebook: &lt;denchmark-link:https://drive.google.com/open?id=1LX2ypNTjnD_h6xGftaSSVrYdor_GUEbA&gt;https://drive.google.com/open?id=1LX2ypNTjnD_h6xGftaSSVrYdor_GUEbA&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='Riz-waan' date='2019-03-07T01:31:28Z'>
		Thanks, I'm currently looking into it.
		</comment>
		<comment id='12' author='Riz-waan' date='2019-03-07T01:58:05Z'>
		I meant model.close(). It depends how are you using Ludwig, if from the command line or from the APIs.
Anyway, I figured it out.
First thing: by default Ludwig trims and pads sequences to 256, which is a lot for some applications. You may want to set the sequence_length_limit in both input and output features to be the actual maximum length in words that you want, which probably is shorter than 256.
Second thing: because the vocabulary is big, the final softmax is also big [batch_size x sequence_length x 20000] so you can either decrease the number of words that you consider with the most_common preprocessing parameter, or you have to reduce the batch size a lot.
Finally, another way to deal with this problem is to use an approximation to the softmax instead of the full softmax. So you can use a loss that is sampled_softmax_cross_entropy.
The combination of this things made it work and fit in memory for me with the following model definition:
&lt;denchmark-code&gt;input_features:
    -
        name: user1
        type: sequence
        encoder: rnn
        cell_type: lstm


output_features:
    -
        name: user2
        type: sequence
        decoder: generator
        cell_type: lstm
        loss:
            type: sampled_softmax_cross_entropy

training:
    batch_size: 96
&lt;/denchmark-code&gt;

Also, I fixed a little bug along the way, so you may want to use the code from master instead of the pip installed package (will release v0.1.1 soon with this and other bugfixes).
Finally, because of punctuation, you may want to use text instead of sequence as a datatype as the preprocessing is performed taking punctuation and other things into account in the case of text.
This actually made me think about the need for a different parameter: eval_batch_size as in this case you could increase the training batch size a lot because of the sampled softmax, but the eval one still needs to be small because the full softmax is computed. So this additional parameter is also coming soon.
Please let me know if this solves your issue.
		</comment>
		<comment id='13' author='Riz-waan' date='2019-03-07T02:23:05Z'>
		I actually added a PR &lt;denchmark-link:https://github.com/ludwig-ai/ludwig/pull/180&gt;#180&lt;/denchmark-link&gt;
 for eval_batch_size, so it will be available soon.
		</comment>
		<comment id='14' author='Riz-waan' date='2019-03-07T03:37:40Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 I am now getting the following error:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/usr/local/bin/ludwig", line 10, in &lt;module&gt;
    sys.exit(main())
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 86, in main
    CLI()
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 64, in __init__
    getattr(self, args.command)()
  File "/usr/local/lib/python3.6/dist-packages/ludwig/cli.py", line 67, in experiment
    experiment.cli(sys.argv[2:])
  File "/usr/local/lib/python3.6/dist-packages/ludwig/experiment.py", line 607, in cli
    experiment(**vars(args))
  File "/usr/local/lib/python3.6/dist-packages/ludwig/experiment.py", line 263, in experiment
    update_model_definition_with_metadata(model_definition, train_set_metadata)
  File "/usr/local/lib/python3.6/dist-packages/ludwig/train.py", line 454, in update_model_definition_with_metadata
    train_set_metadata[output_feature['name']]
  File "/usr/local/lib/python3.6/dist-packages/ludwig/features/text_feature.py", line 346, in update_model_definition_with_metadata
    if isinstance(output_feature[LOSS]['class_weights'], (list, tuple)):
KeyError: 'class_weights'
&lt;/denchmark-code&gt;

On this Model
&lt;denchmark-code&gt;input_features:
    -
        name: user1
        type: text
        encoder: rnn
        cell_type: lstm


output_features:
    -
        name: user2
        type: text
        decoder: generator
        cell_type: lstm
        loss:
            type: sampled_softmax_cross_entropy

training:
    batch_size: 96
&lt;/denchmark-code&gt;

What do you think could be the problem here?
		</comment>
		<comment id='15' author='Riz-waan' date='2019-03-07T04:22:11Z'>
		My bad. Fixed it in a new commit. Can you retry after pulling from master please?
If it still goes out of memory (it shouldn't, I tested on a GPU with 12Gb) decrease the batch size until it fits.
		</comment>
		<comment id='16' author='Riz-waan' date='2019-03-07T04:33:33Z'>
		&lt;denchmark-link:https://github.com/w4nderlust&gt;@w4nderlust&lt;/denchmark-link&gt;
 Currently it is at 38% on epoch 3 with this model:
&lt;denchmark-code&gt;input_features:
    -
        name: user1
        type: sequence
        encoder: rnn
        cell_type: lstm


output_features:
    -
        name: user2
        type: sequence
        decoder: generator
        cell_type: lstm
        loss:
            type: sampled_softmax_cross_entropy

training:
    batch_size: 96
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='Riz-waan' date='2019-03-07T06:09:40Z'>
		Great, it means it's working and there is no bug! So I'm closing this. obviouslt to getter better results you may want to play around with hiperparameters, regularization etc. but I'm glad it is actually working
		</comment>
	</comments>
</bug>