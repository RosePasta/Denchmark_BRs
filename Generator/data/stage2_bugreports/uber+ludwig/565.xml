<bug id='565' author='carlogrisetti' open_date='2019-11-11T17:15:20Z' closed_time='2019-12-25T16:02:33Z'>
	<summary>ludwig test sometimes does not produce correct confusion matrix values</summary>
	<description>
This has happened to me multiple times, and I am now trying to get to the root cause of this.
Basically, really often, this is the confusion matrix that I generate from the model+dataset I am working on:
&lt;denchmark-link:https://user-images.githubusercontent.com/4464640/68606216-45cb6680-04ae-11ea-88ef-42ed36eae966.png&gt;&lt;/denchmark-link&gt;

As you can see there are some actual values that are never predicted, but they should at least be predicted as unknown (note that I have set the --normalize flag, so total sum per-row should always be 1)
I checked in the test_statistics.json (attached)
&lt;denchmark-link:https://github.com/uber/ludwig/files/3832450/test_statistics.zip&gt;test_statistics.zip&lt;/denchmark-link&gt;

and the values reported for the confusion matrix always show a 0 in the corresponding column.
In the other result files, all source items have a corresponding prediction (right or wrong, that's not the point)
&lt;denchmark-link:https://github.com/uber/ludwig/files/3832460/results.zip&gt;results.zip&lt;/denchmark-link&gt;

This is the commandline I use to generate the results:
ludwig test --data_csv dataset.csv --split validation --model_path path\to\model --batch_size 12
This is the commandline I use to generate the confusion matrix:
ludwig visualize --visualization confusion_matrix --normalize --test_statistics results\test_statistics.json --ground_truth_metadata dataset.json
I also tried using hdf5 in place of the .csv, but with no avail. Actual predictions return a result (of course). Given the performance values of this example it might be the wrong result, but it's none the less a result that should be mapped in the confusion matrix.
For this specific one I cannot share the dataset (personal IDs) but anything else I can do, please advise.
	</description>
	<comments>
		<comment id='1' author='carlogrisetti' date='2019-11-11T20:06:18Z'>
		Not sure what is the problem here. As some labels are never predicted, the sum of the row is 0. In theory each value of the row should be 0/0 but i avoid doing division by zero so the resulting value is 0. It sounds all correct to me, what behavior would have you expected?
The relevant code is here anyway: &lt;denchmark-link:https://github.com/uber/ludwig/blob/master/ludwig/visualize.py#L2738-L2743&gt;https://github.com/uber/ludwig/blob/master/ludwig/visualize.py#L2738-L2743&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='carlogrisetti' date='2019-11-11T21:39:44Z'>
		Since PP_FRONTE, PG_RETRO and CF_FRONTE (for example, bunching up the last ones) are present in the validation dataset, I expected to see a value greater than 0, in whichever predicted label have they been predicted.
I just double-checked again and purged .hdf5 and .json, they are present in the validation dataset, but they are not in output, according to the confusion matrix (and more importantly according to the confusion matrix data within test_statistics.json).
For example, the last 3 values in my validation dataset are PG_FRONTE, PG_RETRO and PP_FRONTE. Looking at the results prediction.csv they have all been erroneously predicted to be CE_FRONTE. Although this is not correct for the purpose I'm training for, I expected to see a high value in the corresponding "CE_FRONTE" column, for all the bottom three rows, but they're empty.
It's not the visualizer that is doing wrong... it's the test that is not correctly enumerating the occurrences of the predictions. Within test_statistics.json I expected not to get a 0 for those values. The visualize you are referring to is just plotting what it is seeing in the test_statistics.json
		</comment>
		<comment id='3' author='carlogrisetti' date='2019-11-11T21:56:35Z'>
		Might that be that, since those categories are never predicted in the output, the array gets cut to the wrong size as per this: &lt;denchmark-link:https://github.com/uber/ludwig/blob/master/ludwig/utils/metrics_utils.py#L36&gt;https://github.com/uber/ludwig/blob/master/ludwig/utils/metrics_utils.py#L36&lt;/denchmark-link&gt;

?
The only peculiar thing about this dataset is that I never got a prediction for those labels (I know, right now I have a tiny validation dataset, with just one item per label, and that might be the edge-situation that is generating this issue).
		</comment>
		<comment id='4' author='carlogrisetti' date='2019-11-11T22:30:36Z'>
		In that piece of code predictions and conditions should be of the same size, the min is mostly a precaution, but if you run the test script you an put a break point / print there to see what they are and how long they are, if they are not the same size it's probably worth investigating.
I could help you out, but i need a reproducible setting ;)
		</comment>
		<comment id='5' author='carlogrisetti' date='2019-11-12T07:38:57Z'>
		That's not the case, as they are the same size. I however noticed these warnings in the ludwig test CLI output:
sklearn\metrics\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
'precision', 'predicted', average, warn_for)
sklearn\metrics\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
'precision', 'predicted', average, warn_for)
This seems to be the exact symptom, as per &lt;denchmark-link:https://github.com/scikit-learn/scikit-learn/blob/0.21.X/sklearn/metrics/classification.py#L1407-L1410&gt;https://github.com/scikit-learn/scikit-learn/blob/0.21.X/sklearn/metrics/classification.py#L1407-L1410&lt;/denchmark-link&gt;

In this case we have true_positive, false_positive and false_negative = 0 (label is not present in the predictions at all), so if I'm understanding correctly the behavior, sklearn will return 0 no matter what.
If you could just confirm me that I'm understanding this correctly, we could even think of closing this issue as a won't fix, since it's due to how sklearn treats these issues and it should only happen with such a small dataset used during test that some labels are not ever in the prediction results.
		</comment>
		<comment id='6' author='carlogrisetti' date='2019-11-12T07:55:04Z'>
		Just to have a proof of concept, I forcibly stopped early a training, and got it to a point where all ground truth classes are categorized in the same result class (TS_RETRO). As in previous examples I expected to see a vertical solid yellow colored line spanning all classes on the TS_RETRO column, since any input document has been classified as TS_RETRO only (no other result has been produced other than that class)
&lt;denchmark-link:https://user-images.githubusercontent.com/4464640/68652750-6cc77e00-052a-11ea-9a16-1de915952ad3.png&gt;&lt;/denchmark-link&gt;

Edit: you can also see that there's something wrong where, having the --normalize set, the maximum value is around 0.14 instead of 1.00. It's not showing the 1.00-filled column, hence the scale is misrepresented.
		</comment>
		<comment id='7' author='carlogrisetti' date='2019-11-14T19:01:13Z'>
		Now I get your point. So the solution should be to set the range of the color map to [0, 1] in case of normalization.
Moreover, it looks to me like the 2 axes of the plot are inverted: if all classes are predicted to be TS_RETRO there should be a column of colors different than purple, not a row, right?
Will investigate.
		</comment>
		<comment id='8' author='carlogrisetti' date='2019-11-14T21:42:08Z'>
		I don't think you should need to mess with the scale. This is the result of not having all the correct values in the matrix. If all other rows had a solid 1.0 in the TS_RETRO or whatever it is, the maximum value for the scale will be automatically set to 1.0
The issue is that, row by row, the sum of all the prediction distribution should always be 1.0, but for all the rows other than the populated one, the sum is 0.0
Thanks!
		</comment>
		<comment id='9' author='carlogrisetti' date='2019-11-15T00:32:26Z'>
		What about the columns and rows? again they look flipped to me, aren't they?
		</comment>
		<comment id='10' author='carlogrisetti' date='2019-11-15T07:44:54Z'>
		They are reversed indeed... I trained a classification model on the same dataset, tweaked a little bit,
&lt;denchmark-link:https://github.com/uber/ludwig/files/3850268/images.zip&gt;images.zip&lt;/denchmark-link&gt;

giving him only dogs as validation (forcing the model to come to the solution that everything's a dog), then I ran the confusion matrix, and this is the output:
&lt;denchmark-link:https://user-images.githubusercontent.com/4464640/68924757-c7a6e280-0781-11ea-88e9-00debebc1528.png&gt;&lt;/denchmark-link&gt;

It should definitely be reversed, you are right.
Type_predictions.csv only contain dog as a prediction (one validation sample per every one of the 4 classes), and I just don't know how to interpret the confusion_matrix key in the &lt;denchmark-link:https://github.com/uber/ludwig/files/3850272/test_statistics.zip&gt;test_statistics.zip&lt;/denchmark-link&gt;
. If the arrays are column-by-column they are right (ie: the second member shows 0 for UNK and 1 for anything else), and the graphic output is wrong. If the arrays are row-by-row they are wrong (the second row shows exactly what we see in the output), but at least the graphic visualization part is right.
If i flip the array values axis, from  to  I get this visualization, which also has the correct scale being calculated:
&lt;denchmark-link:https://user-images.githubusercontent.com/4464640/68925395-3afd2400-0783-11ea-9676-3102761d2664.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='carlogrisetti' date='2019-11-22T08:19:36Z'>
		Found it and fixed. It was an order mismatch between two parameters in the confusion_matrix results build.
I made a PR... it seems to be working (giving the correct results in CMs) but as usual you should double check that.
		</comment>
		<comment id='12' author='carlogrisetti' date='2019-12-25T16:02:31Z'>
		Merged, thanks!
		</comment>
	</comments>
</bug>