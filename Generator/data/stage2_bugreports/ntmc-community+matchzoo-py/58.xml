<bug id='58' author='lhohoz' open_date='2019-09-11T09:45:13Z' closed_time='2019-09-16T02:31:45Z'>
	<summary>RuntimeError: The size of tensor a (68) must match the size of tensor b (67) at non-singleton dimension 0</summary>
	<description>
Hi!
I encountered an error prompt (such as title) when running dssm.ipynb on my local machine. My environment is Windows10+Pytorch1.2+cuda9.0. Could you please tell me the solution.
Thanks.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

RuntimeError                              Traceback (most recent call last)
 in 
----&gt; 1 trainer.run()
~\AppData\Local\Continuum\anaconda3\lib\site-packages\matchzoo\trainers\trainer.py in run(self)
230         for epoch in range(self._start_epoch, self._epochs + 1):
231             self._epoch = epoch
--&gt; 232             self._run_epoch()
233             self._run_scheduler()
234             if self._early_stopping.should_stop_early:
~\AppData\Local\Continuum\anaconda3\lib\site-packages\matchzoo\trainers\trainer.py in _run_epoch(self)
258                 # Caculate all losses and sum them up
259                 loss = torch.sum(
--&gt; 260                     *[c(outputs, target) for c in self._criterions]
261                 )
262                 self._backward(loss)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\matchzoo\trainers\trainer.py in (.0)
258                 # Caculate all losses and sum them up
259                 loss = torch.sum(
--&gt; 260                     *[c(outputs, target) for c in self._criterions]
261                 )
262                 self._backward(loss)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\modules\module.py in call(self, *input, **kwargs)
545             result = self._slow_forward(*input, **kwargs)
546         else:
--&gt; 547             result = self.forward(*input, **kwargs)
548         for hook in self._forward_hooks.values():
549             hook_result = hook(self, input, result)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\matchzoo\losses\rank_hinge_loss.py in forward(self, y_pred, y_true)
63             y_pos, y_neg, y_true,
64             margin=self.margin,
---&gt; 65             reduction=self.reduction
66         )
67
~\AppData\Local\Continuum\anaconda3\lib\site-packages\torch\nn\functional.py in margin_ranking_loss(input1, input2, target, margin, size_average, reduce, reduction)
2206         raise RuntimeError(("margin_ranking_loss does not support scalars, got sizes: "
2207                             "input1: {}, input2: {}, target: {} ".format(input1.size(), input2.size(), target.size())))
-&gt; 2208     return torch.margin_ranking_loss(input1, input2, target, margin, reduction_enum)
2209
2210
RuntimeError: The size of tensor a (68) must match the size of tensor b (67) at non-singleton dimension 0
	</description>
	<comments>
		<comment id='1' author='lhohoz' date='2019-09-11T10:55:33Z'>
		Hi, did you modify any code in the tutorial?
		</comment>
		<comment id='2' author='lhohoz' date='2019-09-12T00:56:01Z'>
		Thank you your reply, In fact, I ran two versions, one with no changes and the other with minor changes, but with the same error.
		</comment>
		<comment id='3' author='lhohoz' date='2019-09-14T06:44:42Z'>
		Hi!
From the RuntimeError Traceback, it shows that error occurred in calling RankHingeLoss.forward().
But we define ranking_task = mz.tasks.Ranking(losses=mz.losses.RankCrossEntropyLoss(num_neg=4)) in dssm.ipynb.
So, please ensure that every code block in the tutorial is executed correctly.
Additionally, when you modify the code, please ensure the num_neg param passed to RankCrossEntropyLoss(or RankHingeLoss) and DataSet are identical. If not, similar error will occur.
		</comment>
		<comment id='4' author='lhohoz' date='2019-09-16T01:31:12Z'>
		Hi caiyinqiong,
Thank you for your answer. Problem solved. I used the wrong error function.
Thanks.
		</comment>
		<comment id='5' author='lhohoz' date='2019-09-16T02:31:45Z'>
		I am glad things are working well for you now. I’ll go ahead and close this issue, but I’m happy to continue further discussion whenever needed.
		</comment>
	</comments>
</bug>