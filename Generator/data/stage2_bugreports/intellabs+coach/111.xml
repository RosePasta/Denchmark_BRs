<bug id='111' author='sohakes' open_date='2018-09-24T18:51:16Z' closed_time='2018-10-15T20:06:49Z'>
	<summary>Distributed agents have the same rewards in different workers</summary>
	<description>
Hello,
Hope this is the right place to ask, but I didn't find anywhere else.
I'm trying to run Coach in a custom environment and preset, with a distributed agent, more specifically the Clipped PPO. It runs great, but I'm not sure if the behavior is the expected one. When logging the agent, the rewards in the same episodes between workers are always the same. An example:
Training - Name: main_level/agent Worker: 0 Episode: 5 Total reward: 4.64 Steps: 81 Training iteration: 0 
Training - Name: main_level/agent Worker: 1 Episode: 5 Total reward: 4.64 Steps: 81 Training iteration: 0 
Training - Name: main_level/agent Worker: 0 Episode: 6 Total reward: 12.33 Steps: 96 Training iteration: 0
Training - Name: main_level/agent Worker: 1 Episode: 6 Total reward: 12.33 Steps: 96 Training iteration: 0
I tried running for some training iterations and 4 workers, and the problem is the same. I thought it may be due to the exploration process, which is Additive Noise by default. But then I changed to e-greedy and the behavior is the same.
I would imagine this is happening because the behavior is the same between the workers. But from what I understand, it kind of defeats the purpose of training with a lot of agents. Could it be a problem in my environment that is causing this?
I installed coach from this git repository, so it should be updated.
Thanks!
	</description>
	<comments>
		<comment id='1' author='sohakes' date='2018-09-24T19:31:56Z'>
		I added np.random.seed() to the constructor of the exploration policy, and now the rewards are different between the workers. I think it's because my environment doesn't have any randomness, that's why this was happening. Initializing the seed solved it.
		</comment>
		<comment id='2' author='sohakes' date='2018-09-25T10:29:32Z'>
		Thanks &lt;denchmark-link:https://github.com/sohakes&gt;@sohakes&lt;/denchmark-link&gt;
.
Sounds like we may have a bug in the case of deterministic environments.
Just to verify for reproducing this case, did you use the  flag when running Coach?
		</comment>
		<comment id='3' author='sohakes' date='2018-09-25T15:35:27Z'>
		Thanks for the fast fix! I didn't used the --seed flag. Should I use it?
		</comment>
	</comments>
</bug>