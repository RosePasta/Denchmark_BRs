<bug id='108' author='bbalaji-ucsd' open_date='2018-09-18T05:57:28Z' closed_time='2018-09-18T16:11:56Z'>
	<summary>CartPole + ClippedPPO</summary>
	<description>
I have been trying to get clipped ppo to work with a discrete action space example. For some reason, "None" is being sent as an action to the environment, and that causes the code to collapse.
Here is where that occurs: &lt;denchmark-link:https://github.com/NervanaSystems/coach/blob/master/rl_coach/graph_managers/graph_manager.py#L334&gt;https://github.com/NervanaSystems/coach/blob/master/rl_coach/graph_managers/graph_manager.py#L334&lt;/denchmark-link&gt;

Error I get:
rl_coach/spaces.py", line 131, in val_matches_space_definition
if (self.low is not None and not np.all(val &gt;= self.low)) 
TypeError: '&lt;=' not supported between instances of 'float' and 'NoneType'
Here is my CartPole ClippedPPO preset file:
from rl_coach.agents.clipped_ppo_agent import ClippedPPOAgentParameters
from rl_coach.architectures.tensorflow_components.architecture import Dense
from rl_coach.base_parameters import VisualizationParameters, PresetValidationParameters
from rl_coach.core_types import TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, RunPhase
from rl_coach.environments.environment import MaxDumpMethod, SelectedPhaseOnlyDumpMethod, SingleLevelSelection
from rl_coach.environments.gym_environment import Mujoco, mujoco_v2, MujocoInputFilter
from rl_coach.environments.gym_environment import GymEnvironmentParameters
from rl_coach.exploration_policies.exploration_policy import ExplorationParameters
from rl_coach.filters.observation.observation_normalization_filter import ObservationNormalizationFilter
from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager
from rl_coach.graph_managers.graph_manager import ScheduleParameters
from rl_coach.schedules import LinearSchedule
####################
&lt;denchmark-h:h1&gt;Graph Scheduling&lt;/denchmark-h&gt;

####################
schedule_params = ScheduleParameters()
schedule_params.improve_steps = TrainingSteps(10000000)
schedule_params.steps_between_evaluation_periods = EnvironmentSteps(2048)
schedule_params.evaluation_steps = EnvironmentEpisodes(5)
schedule_params.heatup_steps = EnvironmentSteps(0)
#########
&lt;denchmark-h:h1&gt;Agent&lt;/denchmark-h&gt;

#########
agent_params = ClippedPPOAgentParameters()
agent_params.network_wrappers['main'].learning_rate = 0.0003
agent_params.network_wrappers['main'].input_embedders_parameters['observation'].activation_function = 'tanh'
agent_params.network_wrappers['main'].input_embedders_parameters['observation'].scheme = [Dense([64])]
agent_params.network_wrappers['main'].middleware_parameters.scheme = [Dense([64])]
agent_params.network_wrappers['main'].middleware_parameters.activation_function = 'tanh'
agent_params.network_wrappers['main'].batch_size = 64
agent_params.network_wrappers['main'].optimizer_epsilon = 1e-5
agent_params.network_wrappers['main'].adam_optimizer_beta2 = 0.999
agent_params.algorithm.clip_likelihood_ratio_using_epsilon = 0.2
agent_params.algorithm.clipping_decay_schedule = LinearSchedule(1.0, 0, 1000000)
agent_params.algorithm.beta_entropy = 0
agent_params.algorithm.gae_lambda = 0.95
agent_params.algorithm.discount = 0.99
agent_params.algorithm.optimization_epochs = 10
agent_params.algorithm.estimate_state_value_using_gae = True
#agent_params.input_filter = MujocoInputFilter()
#agent_params.exploration = AdditiveNoiseParameters()
#agent_params.pre_network_filter = MujocoInputFilter()
&lt;denchmark-h:h1&gt;E-Greedy schedule&lt;/denchmark-h&gt;

agent_params.exploration = ExplorationParameters()
agent_params.pre_network_filter.add_observation_filter('observation', 'normalize_observation',
ObservationNormalizationFilter(name='normalize_observation'))
###############
&lt;denchmark-h:h1&gt;Environment&lt;/denchmark-h&gt;

###############
env_params = Mujoco()
env_params.level = 'CartPole-v0'
vis_params = VisualizationParameters()
vis_params.video_dump_methods = [SelectedPhaseOnlyDumpMethod(RunPhase.TEST), MaxDumpMethod()]
vis_params.dump_mp4 = False
########
&lt;denchmark-h:h1&gt;Test&lt;/denchmark-h&gt;

########
preset_validation_params = PresetValidationParameters()
preset_validation_params.test = True
preset_validation_params.min_reward_threshold = 400
preset_validation_params.max_episodes_to_achieve_reward = 1000
graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,
schedule_params=schedule_params, vis_params=vis_params,
preset_validation_params=preset_validation_params)
	</description>
	<comments>
		<comment id='1' author='bbalaji-ucsd' date='2018-09-18T08:01:27Z'>
		Hi &lt;denchmark-link:https://github.com/bbalaji-ucsd&gt;@bbalaji-ucsd&lt;/denchmark-link&gt;
,
You should replace  with .
There was an additional bug in the clipped ppo implementation in the case of discrete controls, which I fixed in the following commit: &lt;denchmark-link:https://github.com/IntelLabs/coach/commit/73cc6e39d00105f8cda3dc39d73fe57dde3c966c&gt;73cc6e3&lt;/denchmark-link&gt;

I tested it with your preset and it converged after 2 training epochs.
		</comment>
		<comment id='2' author='bbalaji-ucsd' date='2018-09-18T16:05:45Z'>
		Thanks! Works now.
		</comment>
	</comments>
</bug>