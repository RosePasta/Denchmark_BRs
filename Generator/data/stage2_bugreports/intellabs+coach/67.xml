<bug id='67' author='tomchen1000' open_date='2018-02-21T22:18:58Z' closed_time='2018-08-19T10:43:38Z'>
	<summary>When running with multiple workers, got this error: InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'main/online/network_0/observation/observation' with dtype float and shape [?,4,1]</summary>
	<description>
I'm getting the following error when running this:
python3 coach.py -p CartPole_A3C -s 600 -n 2
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "./parallel_actor.py", line 170, in &lt;module&gt;
    agent.improve()
  File "/home/user1/gitrepo/coach/agents/agent.py", line 479, in improve
    network.sync()
  File "/home/user1/gitrepo/coach/architectures/network_wrapper.py", line 95, in sync
    self.update_online_network()
  File "/home/user1/gitrepo/coach/architectures/network_wrapper.py", line 112, in update_online_network
    self.online_network.set_weights(self.global_network.get_weights(), rate)
  File "/home/user1/gitrepo/coach/architectures/tensorflow_components/architecture.py", line 322, in set_weights
    old_weights, new_weights = self.tp.sess.run([self.get_weights(), weights])
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py", line 521, in run
    run_metadata=run_metadata)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py", line 892, in run
    run_metadata=run_metadata)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py", line 967, in run
    raise six.reraise(*original_exc_info)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/six.py", line 686, in reraise
    raise value
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py", line 952, in run
    return self._sess.run(*args, **kwargs)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py", line 1024, in run
    run_metadata=run_metadata)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py", line 827, in run
    return self._sess.run(*args, **kwargs)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'main/online/network_0/observation/observation' with dtype float and shape [?,4,1]
	 [[Node: main/online/network_0/observation/observation = Placeholder[dtype=DT_FLOAT, shape=[?,4,1], _device="/job:worker/replica:0/task:0/device:CPU:0"]()]]

Caused by op 'main/online/network_0/observation/observation', defined at:
  File "./parallel_actor.py", line 102, in &lt;module&gt;
    exec('agent = ' + tuning_parameters.agent.type + '(env_instance, tuning_parameters, replicated_device=device, '
  File "&lt;string&gt;", line 1, in &lt;module&gt;
  File "/home/user1/gitrepo/coach/agents/actor_critic_agent.py", line 26, in __init__
    PolicyOptimizationAgent.__init__(self, env, tuning_parameters, replicated_device, thread_id, create_target_network)
  File "/home/user1/gitrepo/coach/agents/policy_optimization_agent.py", line 37, in __init__
    self.replicated_device, self.worker_device)
  File "/home/user1/gitrepo/coach/architectures/network_wrapper.py", line 71, in __init__
    self.global_network, network_is_local=True)
  File "/home/user1/gitrepo/coach/architectures/tensorflow_components/general_network.py", line 40, in __init__
    TensorFlowArchitecture.__init__(self, tuning_parameters, name, global_network, network_is_local)
  File "/home/user1/gitrepo/coach/architectures/tensorflow_components/architecture.py", line 70, in __init__
    self.get_model(tuning_parameters)
  File "/home/user1/gitrepo/coach/architectures/tensorflow_components/general_network.py", line 124, in get_model
    input_placeholder, embedding = input_embedder()
  File "/home/user1/gitrepo/coach/architectures/tensorflow_components/embedders.py", line 34, in __call__
    self.input = tf.placeholder("float", shape=(None,) + self.input_size, name=self.get_name())
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py", line 3091, in _placeholder
    "Placeholder", dtype=dtype, shape=shape, name=name)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/home/user1/gitrepo/coach/coach_env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'main/online/network_0/observation/observation' with dtype float and shape [?,4,1]
	 [[Node: main/online/network_0/observation/observation = Placeholder[dtype=DT_FLOAT, shape=[?,4,1], _device="/job:worker/replica:0/task:0/device:CPU:0"]()]]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tomchen1000' date='2018-02-23T14:00:20Z'>
		Any chance you are using a proxy?
		</comment>
		<comment id='2' author='tomchen1000' date='2018-02-23T14:08:32Z'>
		Yes, I'm using aproxy.
Hmm..., why using a proxy would cause this issue? How to make it work with proxy?
		</comment>
		<comment id='3' author='tomchen1000' date='2018-02-23T14:23:20Z'>
		This causes an issue with the communication between the workers and the parameter server. This is something we came across just recently. Can you please try removing the proxy and see if it really is the problem in this case?
		</comment>
		<comment id='4' author='tomchen1000' date='2018-02-23T22:17:06Z'>
		Removing proxy (i.e. unset  environment variables like http_proxy, https_proxy) doesn't fix the problem.
It seems the issue was introduced after 0.9.0. It works on 0.9.0.
But doesn't work on master: I'm at commit point &lt;denchmark-link:https://github.com/IntelLabs/coach/commit/72d34f4063a62938433a6c820a33dc6a2e5d8fef&gt;72d34f4&lt;/denchmark-link&gt;
 (adding a flag to prevent summary).
		</comment>
		<comment id='5' author='tomchen1000' date='2018-03-12T21:00:09Z'>
		Hey, is there any update on this?
I've got the same problem and it appears  if im trying to store checkpoints with the '-s' option. If i leave that option out, everything runs as expected.
		</comment>
		<comment id='6' author='tomchen1000' date='2018-03-23T22:28:35Z'>
		Just to let you know, i've got it running with the settings
save_summaries_secs=None,
save_summaries_steps=None,
in the tf.train.MonitoredTrainingSession in parallel_actor.py
And also i had a problem in the clipped_ppo agent and had to comment out
#if self.tp.distributed and self.tp.agent.share_statistics_between_workers:
#    self.running_observation_stats.push(np.array([t.state['observation'] for t in dataset]))
I'm currently not sure what it does and i'm hoping for the best that it works anyways
		</comment>
		<comment id='7' author='tomchen1000' date='2018-08-19T10:43:38Z'>
		Hi,
Several issues were fixed with both the saved summaries, and distributed training. Please re-open if the issue persists.
		</comment>
	</comments>
</bug>