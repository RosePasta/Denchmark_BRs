<bug id='16' author='inejc' open_date='2017-11-01T13:03:07Z' closed_time='2018-02-13T08:00:42Z'>
	<summary>Loss becomes nan after some epochs</summary>
	<description>
&lt;denchmark-h:h5&gt;PyTorch and CUDA versions&lt;/denchmark-h&gt;

PyTorch: 0.2.0.post3
CUDA: 8.0
&lt;denchmark-h:h5&gt;Description&lt;/denchmark-h&gt;

Loss suddenly becomes nan (was somewhere between 0.4 and 0.5 in the previous epoch) when training on a dataset comprised of 74219 documents with vocabulary size 91417.
&lt;denchmark-h:h5&gt;Additional info (stack trace, etc.)&lt;/denchmark-h&gt;

Training on a GPU with the following parameters:
--data_file_name 'arxiv_min.csv' --num_epochs 500 --batch_size 512 --context_size 4 --num_noise_words 10 --vec_dim 300 --lr 0.001 --max_generated_batches 500 --num_workers 8
	</description>
	<comments>
	</comments>
</bug>