<bug id='2599' author='maxfy1992' open_date='2021-01-14T10:18:24Z' closed_time='2021-01-15T08:58:41Z'>
	<summary>binary op compute err by packing</summary>
	<description>
&lt;denchmark-code&gt;ncnn version
commit 6572e98f6656f1333189f46db9dad03068e1a697 (HEAD -&gt; master, origin/master, origin/HEAD, dd/master)
Author: nihuini &lt;nihuini@tencent.com&gt;
Date:   Wed Jan 13 16:56:51 2021 +0800

    handle onnx transpose 102 201 after lstm


model.param like below:
Input                     0                         0 1 0
MemoryData       754                      0 1 754 0=2 1=800
Convolution      365                        1 1 0 365 ...
BinaryOp             755                      2 1 365 754 755 0=2

When open Vulkan comute
365's shape is dims(2)w(2)h(200)c(1)pack(4) 
754's shape is dims(2)w(2)h(800)c(1)pack(1) 

BinaryOp(755)'s broadcast is true but it should be false..
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='maxfy1992' date='2021-01-14T10:20:44Z'>
		I work around by add code below
&lt;denchmark-code&gt;diff --git a/src/layer/memorydata.cpp b/src/layer/memorydata.cpp
index 1c50b010..359d89e2 100644
--- a/src/layer/memorydata.cpp
+++ b/src/layer/memorydata.cpp
@@ -49,6 +49,12 @@ int MemoryData::load_model(const ModelBin&amp; mb)
     {
         data.create(1);
     }
+
+    Option opt;
+    Mat packed;
+    convert_packing(data, packed, 4, opt);
+    data = packed;
+
     if (data.empty())
         return -100;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='maxfy1992' date='2021-01-14T11:25:24Z'>
		confirmed
		</comment>
		<comment id='3' author='maxfy1992' date='2021-01-15T08:41:17Z'>
		&lt;denchmark-code&gt;#include "net.h"

#include "datareader.h"

class DataReaderFromEmpty : public ncnn::DataReader
{
public:
    virtual int scan(const char* format, void* p) const
    {
        return 0;
    }
    virtual size_t read(void* buf, size_t size) const
    {
        // assign float values
        memset(buf, 0, size);

        float* p = (float*)buf;
        for (int i = 0; i &lt; size / 4; i++)
        {
            p[i] = i;
        }

        return size;
    }
};

void pretty_print(const ncnn::Mat&amp; m)
{
    for (int q=0; q&lt;m.c; q++)
    {
        const float* ptr = m.channel(q);
        for (int y=0; y&lt;m.h; y++)
        {
            for (int x=0; x&lt;m.w; x++)
            {
                printf("%f ", ptr[x]);
            }
            ptr += m.w;
            printf("\n");
        }
        printf("------------------------\n");
    }
}

int main()
{
    ncnn::Net net;
    net.opt.num_threads = 1;
    net.opt.use_vulkan_compute = true;

    net.load_param("test.param");

    DataReaderFromEmpty dr;
    net.load_model(dr);

    ncnn::Extractor ex = net.create_extractor();

    ncnn::Mat x(2, 800);
    x.fill(20.f);
    ex.input("x", x);

    ncnn::Mat y;
    ex.extract("y", y);

    pretty_print(y);

    return 0;
}
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;7767517
3 3
Input        0    0 1 x
MemoryData   1    0 1 w 0=2 1=800
BinaryOp     2    2 1 x w y 0=2
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='maxfy1992' date='2021-01-15T08:59:23Z'>
		修好了
		</comment>
	</comments>
</bug>