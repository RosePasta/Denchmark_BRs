<bug id='512' author='zeyuanlicbpm' open_date='2018-08-02T07:25:40Z' closed_time='2018-08-03T03:13:53Z'>
	<summary>Release和Debug运行过程中出现一个Bug：</summary>
	<description>
name: "Darkent2Caffe"
layer {
name: "data"
type: "Input"
top: "data"
input_param { shape: { dim: 1 dim: 3 dim: 416 dim: 416 } }
}
layer {
bottom: "data"
top: "layer1-conv"
name: "layer1-conv"
type: "Convolution"
convolution_param {
num_output: 32
kernel_size: 3
pad: 1
stride: 1
bias_term: false
}
}
layer {
bottom: "layer1-conv"
top: "layer1-conv"
name: "layer1-bn"
type: "BatchNorm"
batch_norm_param {
use_global_stats: true
}
}
layer {
bottom: "layer1-conv"
top: "layer1-conv"
name: "layer1-scale"
type: "Scale"
scale_param {
bias_term: true
}
}
layer {
bottom: "layer1-conv"
top: "layer1-conv"
name: "layer1-act"
type: "ReLU"
relu_param {
negative_slope: 0.1
}
}
layer {
bottom: "layer1-conv"
top: "layer2-conv"
name: "layer2-conv"
type: "Convolution"
convolution_param {
num_output: 64
kernel_size: 3
pad: 1
stride: 2
bias_term: false
}
}
layer {
bottom: "layer2-conv"
top: "layer2-conv"
name: "layer2-bn"
type: "BatchNorm"
batch_norm_param {
use_global_stats: true
}
}
layer {
bottom: "layer2-conv"
top: "layer2-conv"
name: "layer2-scale"
type: "Scale"
scale_param {
bias_term: true
}
}
layer {
bottom: "layer2-conv"
top: "layer2-conv"
name: "layer2-act"
type: "ReLU"
relu_param {
negative_slope: 0.1
}
}
layer {
bottom: "layer2-conv"
top: "layer3-conv"
name: "layer3-conv"
type: "Convolution"
convolution_param {
num_output: 32
kernel_size: 1
pad: 0
stride: 1
bias_term: false
}
}
layer {
bottom: "layer3-conv"
top: "layer3-conv"
name: "layer3-bn"
type: "BatchNorm"
batch_norm_param {
use_global_stats: true
}
}
layer {
bottom: "layer3-conv"
top: "layer3-conv"
name: "layer3-scale"
type: "Scale"
scale_param {
bias_term: true
}
}
layer {
bottom: "layer3-conv"
top: "layer3-conv"
name: "layer3-act"
type: "ReLU"
relu_param {
negative_slope: 0.1
}
}
layer {
bottom: "layer3-conv"
top: "layer4-conv"
name: "layer4-conv"
type: "Convolution"
convolution_param {
num_output: 64
kernel_size: 3
pad: 1
stride: 1
bias_term: false
}
}
layer {
bottom: "layer4-conv"
top: "layer4-conv"
name: "layer4-bn"
type: "BatchNorm"
batch_norm_param {
use_global_stats: true
}
}
layer {
bottom: "layer4-conv"
top: "layer4-conv"
name: "layer4-scale"
type: "Scale"
scale_param {
bias_term: true
}
}
layer {
bottom: "layer4-conv"
top: "layer4-conv"
name: "layer4-act"
type: "ReLU"
relu_param {
negative_slope: 0.1
}
}
layer {
bottom: "layer2-conv"
bottom: "layer4-conv"
top: "layer5-shortcut"
name: "layer5-shortcut"
type: "Eltwise"
eltwise_param {
operation: SUM
}
}
layer {
bottom: "layer5-shortcut"
top: "layer6-conv"
name: "layer6-conv"
type: "Convolution"
convolution_param {
num_output: 128
kernel_size: 3
pad: 1
stride: 2
bias_term: false
}
}
这个网络结构，按照以下代码提取最后一层特征：
ncnn::Net net;
net.load_param("yolov3_ncnn.param");
net.load_model("yolov3_ncnn.bin");
&lt;denchmark-code&gt;ncnn::Extractor ex = net.create_extractor();
int w = 416;
int h = 416;
unsigned char* rgbdata = new unsigned char[w * h * 3];
memset(rgbdata, 0, w * h * 3);
ncnn::Mat in = ncnn::Mat::from_pixels(rgbdata, ncnn::Mat::PIXEL_BGR, w, h);
ex.set_num_threads(4);
ncnn::Mat out;
ex.input("data", in);
ex.extract("layer5-shortcut", out);
printf("width: %d, height: %d", out.w, out.h);
&lt;/denchmark-code&gt;

Release模式下能够正常运行；但是Debug模式下，报错Vector越界错误；我跟踪定位到
&lt;denchmark-link:https://user-images.githubusercontent.com/33452561/43568709-e829c14e-9667-11e8-94d4-390cb7f5b536.png&gt;&lt;/denchmark-link&gt;

在进入到layer5-shortcut这层以后，图片中1位置的代码将opt.int8_scales的大小resize到2；但是在2处的递归调用过程中，这个opt.int8_scales的大小被修改为1；导致for循环进入到第二次时，3处的访问越界！  原因是什么呢？请大神指点！！！
	</description>
	<comments>
		<comment id='1' author='zeyuanlicbpm' date='2018-08-02T08:59:51Z'>
		我也碰到一样的bug
		</comment>
		<comment id='2' author='zeyuanlicbpm' date='2018-08-03T03:14:11Z'>
		fixed in &lt;denchmark-link:https://github.com/Tencent/ncnn/commit/2bc504925e385fd56a2189015a907e05dd050909&gt;2bc5049&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='zeyuanlicbpm' date='2018-08-04T06:17:06Z'>
		测试通过，谢谢！
		</comment>
	</comments>
</bug>