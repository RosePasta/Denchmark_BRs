<bug id='315' author='Lamply' open_date='2018-03-29T07:45:09Z' closed_time='2018-03-29T12:19:15Z'>
	<summary>ConvolutionDepthWise doesn't seem to work when dilation is not equal 1</summary>
	<description>
I got unexpected output when testing a network in 20180314 latest release version, which work well in 20180129 version.
After debugging, I found the error occurred in 3x3 dilation 2 ConvolutionDepthWise layer. The result after dilation 2 ConvolutionDepthWise is same with the init result of top_blob.
Here is a test example:
#include &lt;stdio.h&gt;
#include &lt;iostream&gt;
#include &lt;algorithm&gt;
#include &lt;vector&gt;
#include "net.h"
#include "layer_type.h"

using namespace ncnn;

// weights and bias
float weight_data[3*3*4] = {0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111, 0.11111111};
float bias_data[4] = {1, 1, 1, 1};

int main(int argc, char *argv[])
{
    // 1- set param
    int inw = 5;
    int inh = 5;
    int outw = 5;
    int outh = 5;
    int num_input = 4;
    int num_output = 4;
    int kernel_w = 3;
    int kernel_h = 3;
    int dilation_w = 2;
    int dilation_h = 2;
    int stride_w = 1;
    int stride_h = 1;
    int pad_w = 2;
    int pad_h = 2;
    int bias_term = 1;
    int weight_data_size = 36;
    int group = 4;

    // 2- set and display bottom blob
    Mat bottom_blob(inw, inh, num_input);
    if (bottom_blob.empty())
        return -100;

    std::cout &lt;&lt; "Produce all 1 to bottom_blob for init:" &lt;&lt; std::endl;
    for (int n = 0; n &lt; bottom_blob.c; ++n)
    {
        Mat bottom_blob_g = bottom_blob.channel(n);
        for (int r = 0; r &lt; bottom_blob_g.h; ++r)
        {
            float *ptr = bottom_blob_g.row(r);
            for (int c = 0; c &lt; bottom_blob_g.w; ++c)
            {
                ptr[c] = 1.0f;
                std::cout &lt;&lt; ptr[c] &lt;&lt; ' ';
            }
            std::cout &lt;&lt; std::endl;
        }
    }
    std::cout &lt;&lt; std::endl;

    // 3- set and display top blob
    Mat top_blob(outw, outh, num_output);
    if (top_blob.empty())
        return -100;

    std::cout &lt;&lt; "Produce all 3.12 to top_blob for checking:" &lt;&lt; std::endl;
    for (int n = 0; n &lt; top_blob.c; ++n)
    {
        Mat top_blob_g = top_blob.channel(n);
        for (int r = 0; r &lt; top_blob_g.h; ++r)
        {
            float *ptr = top_blob_g.row(r);
            for (int c = 0; c &lt; top_blob_g.w; ++c)
            {
                ptr[c] = 3.12f;
                std::cout &lt;&lt; ptr[c] &lt;&lt; ' ';
            }
            std::cout &lt;&lt; std::endl;
        }
    }
    std::cout &lt;&lt; std::endl;

    // 4- create ConvolutionDepthWise layer
    Layer* op = ncnn::create_layer(ncnn::LayerType::ConvolutionDepthWise);

    ncnn::ParamDict pd;
    pd.set(0, num_output);// num_output
    pd.set(1, kernel_w);
    pd.set(11, kernel_h);
    pd.set(2, dilation_w);
    pd.set(12, dilation_h);
    pd.set(3, stride_w);
    pd.set(13, stride_h);
    pd.set(4, pad_w);// pad_w
    pd.set(14, pad_h);// pad_h
    pd.set(5, bias_term);
    pd.set(6, weight_data_size);// weight_data_size
    pd.set(7, group);

    op-&gt;load_param(pd);

    Mat weight_data_g(3*3*10, (void*)weight_data);
    Mat bias_data_g(10, (void*)bias_data);

    ncnn::Mat weights[2];
    weights[0] = weight_data_g;
    weights[1] = bias_data_g;

    op-&gt;load_model(ModelBinFromMatArray(weights));

    op-&gt;forward(bottom_blob, top_blob);

    // 5- display result
    fprintf(stderr, "After ConvolutionDepthWise input %d x %d  pad = %d %d  ksize=%d %d  dilation=%d %d  stride=%d %d:\n", bottom_blob.w, bottom_blob.h, pad_w, pad_h, kernel_w, kernel_h, dilation_w, dilation_h, stride_w, stride_h);
    for (int n = 0; n &lt; top_blob.c; ++n)
    {
        Mat top_blob_g = top_blob.channel(n);
        for (int r = 0; r &lt; top_blob_g.h; ++r)
        {
            float *ptr = top_blob_g.row(r);
            for (int c = 0; c &lt; top_blob_g.w; ++c)
            {
                std::cout &lt;&lt; ptr[c] &lt;&lt; ' ';
            }
            std::cout &lt;&lt; std::endl;
        }
    }

    return 0;
}
and the result is:

Produce all 1 to bottom_blob for init:
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1
1 1 1 1 1


Produce all 3.12 to top_blob for checking:
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12


After ConvolutionDepthWise input 5 x 5  pad = 2 2  ksize=3 3  dilation=2 2  stride=1 1:
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12
3.12 3.12 3.12 3.12 3.12

	</description>
	<comments>
		<comment id='1' author='Lamply' date='2018-03-29T11:31:23Z'>
		I find that the final result produce in here:



ncnn/src/layer/x86/convolutiondepthwise_x86.cpp


         Line 140
      in
      415bfbd






 op-&gt;forward(bottom_blob_bordered_g, top_blob_g); 





is correct, but it doesn't effect the output top_blob. Then I manually element-wise assign the top_blob_g to top_blob, and problem disappear.
Further more, It can simply solve by adding an top_blob empty check before here:



ncnn/src/layer/convolution.cpp


         Line 137
      in
      415bfbd






 top_blob.create(outw, outh, num_output); 





		</comment>
	</comments>
</bug>