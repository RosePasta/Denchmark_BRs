<bug id='104' author='cgarciae' open_date='2019-04-19T22:50:37Z' closed_time='2019-05-23T15:58:42Z'>
	<summary>Simple Optimizer Test</summary>
	<description>
Hey S4SF team, I am trying a very simple AD exercise using Adam. Since Adam requires a Layer type I created this very simple Variable struct that just holds the state. In this example I am just trying to minimize x^2:
import TensorFlow

struct Variable : Layer {
    var x: Float

    @differentiable
    func call(_ input: Float) -&gt; Float {
        return x 
    }
}

var layer = Variable(x: 1.0)
let optimizer = Adam(for: layer)

let delta = layer.gradient { layer -&gt; Float in
    let x = layer.x
    return x * x
}

print("Layer0:", layer)
print("Delta:", delta)

optimizer.update(&amp;layer.allDifferentiableVariables, along: delta)

print("Layer1:", layer)
It appears that after update layer is not changing its x value:
&lt;denchmark-code&gt;Layer0: Variable(x: 1.0)
Delta: AllDifferentiableVariables(x: 2.0)
Layer1: Variable(x: 1.0)
&lt;/denchmark-code&gt;

Any idea why this not minimizing x?
	</description>
	<comments>
		<comment id='1' author='cgarciae' date='2019-04-19T22:55:00Z'>
		This is because standard optimizers &lt;denchmark-link:https://github.com/tensorflow/swift-apis/blob/cfefd63fa60a55d1da1e2a412ed561eb3448e691/Sources/DeepLearning/Optimizer.swift#L93&gt;only work on tensors&lt;/denchmark-link&gt;
. To optimize general numeric values, you can currently use the manifold optimizer .
		</comment>
		<comment id='2' author='cgarciae' date='2019-04-19T22:57:01Z'>
		We plan to generalize vector space protocols in order to be able to generalize these optimizers for all real vector types (Float, Double, Float80, Tensor&lt;BFloat16&gt;, Tensor&lt;Float16&gt;, etc). It's coming soon!
		</comment>
		<comment id='3' author='cgarciae' date='2019-04-19T23:51:02Z'>
		&lt;denchmark-link:https://github.com/rxwei&gt;@rxwei&lt;/denchmark-link&gt;
 thank for the info! But you will still need to create a Layer that uses these non-tensor types or will you be able to optimize other differentiable types like  directly?
		</comment>
		<comment id='4' author='cgarciae' date='2019-04-19T23:58:16Z'>
		Ideally we should be able to update any model with mixed parameters as long as the model conforms to VectorNumeric.
		</comment>
	</comments>
</bug>