<bug id='490' author='t-ae' open_date='2019-08-28T04:42:49Z' closed_time='2019-12-06T19:43:09Z'>
	<summary>Type inconsistency between Tensor.batchNormalized and BatchNorm</summary>
	<description>
epsilon of Tensor.batchNormalized is Float.



swift-apis/Sources/TensorFlow/Operators/NN.swift


         Line 36
      in
      b7f2a06






 epsilon: Scalar = 0.001 





But epsilon of BatchNorm is Tensor&lt;Float&gt;.



swift-apis/Sources/TensorFlow/Layers/Normalization.swift


         Line 34
      in
      b7f2a06






  @noDerivative public let epsilon: Tensor&lt;Scalar&gt; 





epsilon is a scalar (even BatchNorm's comment says so).
Changing BatchNorm.epsilon to Float improve API consistency.
My concern is that BatchNorm is aleready marked as @frozen. Is it OK to change this?
Also LayerNorm has epsilon: Tensor&lt;Float&gt; which is scalar indeed.
	</description>
	<comments>
		<comment id='1' author='t-ae' date='2019-08-28T04:47:15Z'>
		 must use  for now due to internal reasons (differentiation bug in the compiler). We need to wait till &lt;denchmark-link:https://bugs.swift.org/browse/TF-626&gt;TF-626&lt;/denchmark-link&gt;
 gets fixed.
		</comment>
		<comment id='2' author='t-ae' date='2019-08-28T04:53:36Z'>
		
My concern is that BatchNorm is aleready marked as @frozen. Is it OK to change this?

@frozen in our library is only for optimization purposes (no API resilience). None of our APIs are locked down.
		</comment>
		<comment id='3' author='t-ae' date='2019-12-06T19:43:09Z'>
		This has been solved by &lt;denchmark-link:https://github.com/tensorflow/swift-apis/pull/525&gt;#525&lt;/denchmark-link&gt;
 , so closing this issue.
		</comment>
	</comments>
</bug>