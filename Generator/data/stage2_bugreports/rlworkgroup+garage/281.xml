<bug id='281' author='ryanjulian' open_date='2018-08-14T01:07:08Z' closed_time='2018-10-30T22:50:14Z'>
	<summary>policy_ent_coeff does not seem to work in tf/NPO</summary>
	<description>
I can choose any value (or GaussianMLPPolicy params) I want, and it doesn't change the policy standard deviation. e.g. -1.0 won't cause the policy stddev to fall, and 1.0 won't cause it to rise. This may be because a gradient path is broken between the reward function and the policy std network.
	</description>
	<comments>
		<comment id='1' author='ryanjulian' date='2018-08-14T20:20:55Z'>
		What param do you set to -1.0? init_std?
		</comment>
		<comment id='2' author='ryanjulian' date='2018-08-14T20:24:17Z'>
		My assertion was
if policy_ent_coeff = -1.0, then during training I should see the policy entropy decrease rapidly (if most environment rewards are small)
likewise, if policy_ent_coeff = 1.0, then I should see policy entropy increase rapidly.
this is because the optimizer should be able to differentiate through the loss function all the way to the std_network of the policy. the term policy_ent_coeff tunes the magnitude of this gradient.
		</comment>
		<comment id='3' author='ryanjulian' date='2018-08-14T20:29:52Z'>
		Things to check:

Assuming your augmented rewards equation looks like r + (policy_ent_coeff * policy_entropy), ensure that policy_entropy is differentiable (symbolic) all the way back to the std_network of the policy. This means that your loss function retrieves policy_entropy by purely by feeding actions and observations, not by using the recorded mean/std (e.g. agent_infos, dist_infos).
Make sure that dist_infos are only used for forward pass calculations (e.g. KL divergence and likelihood ratio)

		</comment>
		<comment id='4' author='ryanjulian' date='2018-10-17T17:50:27Z'>
		&lt;denchmark-link:https://github.com/CatherineSue&gt;@CatherineSue&lt;/denchmark-link&gt;
 were you able to reproduce this?
		</comment>
		<comment id='5' author='ryanjulian' date='2018-10-17T18:05:47Z'>
		Yes. I reproduced the bug.
		</comment>
		<comment id='6' author='ryanjulian' date='2018-10-25T17:51:55Z'>
		For this issue, let's use

MaxEnt (e.g. augment the reward with an entropy term and a coefficient): \hat r = r + \alpha_1 * H
log-likelihood estimator: H ~ -logli(action| policy(state))
Turn off the gradient: H = tf.stop_gradient(H)

		</comment>
	</comments>
</bug>