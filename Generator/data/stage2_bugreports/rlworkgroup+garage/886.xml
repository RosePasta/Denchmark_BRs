<bug id='886' author='naeioi' open_date='2019-09-17T22:15:00Z' closed_time='2020-06-09T18:10:54Z'>
	<summary>Inconsistent flattening for dict observation between on- and off-policy samplers</summary>
	<description>
OffPolicyVectorizedSampler flattens a dict observation while other samplers do not. This breaks ContinuousMLPPolicyWithModel who assumes observation is provided as it is, at this line 


garage/src/garage/tf/policies/continuous_mlp_policy_with_model.py


         Line 150
      in
      7bb784e






 flat_obs = self.observation_space.flatten_n(observations) 





This issue can be reproduce by running examples/her_ddpg_fetchreach.py.
There are also inconsistences in polices' assumption on observation shape. For example, contradicting to ContinuousMLPPolicyWithModel, CategoricalConvPolicyWithModel assumes observation to be a ndarray.



garage/src/garage/tf/policies/categorical_conv_policy_with_model.py


        Lines 146 to 147
      in
      7bb784e






 # flat_obs = self.observation_space.flatten_n(observations) 



 probs = self._f_prob(observations) 





	</description>
	<comments>
		<comment id='1' author='naeioi' date='2019-09-17T23:29:41Z'>
		Fixing this is probably closely related to my current effort to refactor the sampler API, since I'm attempting to make all samplers us the same rollout (and hopefully, same post-processing) functions.
It's also worth mentioning that I'm trying to replace use of the current replay buffer with PathBuffer, which is simpler, but more capable and fixes a few difficult to fix bugs in the current replay buffer.
		</comment>
		<comment id='2' author='naeioi' date='2019-11-19T19:00:35Z'>
		&lt;denchmark-link:https://github.com/ahtsan&gt;@ahtsan&lt;/denchmark-link&gt;
 did you fix this?
		</comment>
		<comment id='3' author='naeioi' date='2019-11-19T19:27:46Z'>
		No I didn't. Currently in get_actions(), different policies have slightly different ways to handle. I don't have an exact answer for the following question -- Should we make policies flatten observations, except images?
Also we are unflattening the actions output inside policies.
&lt;denchmark-link:https://github.com/rlworkgroup/garage/blob/master/src/garage/tf/policies/continuous_mlp_policy.py#L129&gt;https://github.com/rlworkgroup/garage/blob/master/src/garage/tf/policies/continuous_mlp_policy.py#L129&lt;/denchmark-link&gt;

Why do we do that?
		</comment>
		<comment id='4' author='naeioi' date='2019-11-19T19:30:39Z'>
		I don't think there are many logical answers for why we do some of these things.
We are still working on getting the data API consistent among all the algos and primitives.
&lt;denchmark-link:https://github.com/krzentner&gt;@krzentner&lt;/denchmark-link&gt;
 do you think it would be helpful or overkill to define some types like  or ?
		</comment>
		<comment id='5' author='naeioi' date='2020-06-08T23:34:51Z'>
		The issue cannot be reproduced by running her_ddpg_fetchreach.py. I've run a couple of tests on the master branch in my local machine and all tests run successfully.
		</comment>
	</comments>
</bug>