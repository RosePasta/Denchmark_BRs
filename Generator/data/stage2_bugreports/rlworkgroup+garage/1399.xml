<bug id='1399' author='krzentner' open_date='2020-05-07T18:27:28Z' closed_time='2020-09-15T00:08:29Z'>
	<summary>Sampler's don't pickle (correctly)</summary>
	<description>
Currently, samplers generally don't implement the pickle interface. Furthermore, the one sampler that does (LocalSampler) assumes that there's no state contained in its workers that wasn't present when the sampler was constructed. This breaks in the following circumstances:

The environment was updated after the experiment was started, and the same environment update isn't replicated on resume.
The agent / policy has additional state besides its parameters. For example, exploration policies may have internal state that persists across epochs.

Historically, we assumed that such state wasn't possible, and that the runner would reconstruct the sampler in an idempotent way. However, adding pickling support to the existing samplers would not be particularly onerous, so we should just do that, since it would ensure resume works correctly.
	</description>
	<comments>
		<comment id='1' author='krzentner' date='2020-05-07T18:29:26Z'>
		Note that for the case of exploration strategies with LocalSampler, the setup "currently" works correctly because the exploration strategy never gets pickled in the first place, and is saved as a field on the algorithm.
		</comment>
		<comment id='2' author='krzentner' date='2020-06-15T20:43:56Z'>
		This issue will become critical once we move the algorithms that use exploration policies to use normal samplers. For the 2020.06 release, I think fixing this issue would introduce more complexity than we would like.
		</comment>
		<comment id='3' author='krzentner' date='2020-09-03T06:09:07Z'>
		This &lt;denchmark-link:https://github.com/rlworkgroup/garage/commit/9cf85deb52790279afcf8804ea02b0e80c34bea8&gt;example&lt;/denchmark-link&gt;
 adds an  method to exploration_policy to give worker the knowledge of new_trajectory when resuming. I have two questions about it.


For those exploration policies that need to keep their internal states (e.g. AddOrnsteinUhlenbeckNoise), it seems that it cannot resume their states because trajectory_batch does not contain these  states. In this situation, can we add these internal states into agent_info?


In this AddExpertActions example, the policy maintains a total_steps_so_far. If there are multiple workers, should every worker only count env steps performed by itself?


		</comment>
	</comments>
</bug>