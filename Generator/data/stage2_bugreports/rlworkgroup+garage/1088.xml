<bug id='1088' author='krzentner' open_date='2019-12-05T02:32:34Z' closed_time='2019-12-05T02:41:18Z'>
	<summary>Off-policy algorithms cannot optimize after every time step</summary>
	<description>
Our implementations of off-policy algorithms require using the OffPolicyVectorizedSampler. However, that sampler only collects complete paths. Setting batch_size to 1 only results in the environment being reset, then a single sample from the start of the rollout being gathered.
This is a major problem, since the algorithms are usually used in a configuration where they train after every sample.
	</description>
	<comments>
		<comment id='1' author='krzentner' date='2019-12-05T02:32:46Z'>
		&lt;denchmark-link:https://github.com/avnishn&gt;@avnishn&lt;/denchmark-link&gt;
 Found this out the hard way.
		</comment>
		<comment id='2' author='krzentner' date='2019-12-05T02:41:18Z'>
		Oh. Actually, reading OffPolicyVectorizedSampler.obtain_samples more carefully, the environment is not reset it _last_obses is set, so this bug (probably) does not exist. I'm closing this issue.
		</comment>
	</comments>
</bug>