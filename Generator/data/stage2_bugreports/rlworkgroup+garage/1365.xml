<bug id='1365' author='bordeauxred' open_date='2020-04-30T14:15:59Z' closed_time='2020-05-06T17:44:50Z'>
	<summary>HalfCheetahDirEnv meta-evaluation</summary>
	<description>
Hi there,
I tried to replicate the pearl results from the pearl paper, however for halfcheetah forward/backward (script attached as txt) I got, using the hyper parameters similar to the oyster repo &lt;denchmark-link:https://github.com/katerakelly/oyster/blob/master/configs/cheetah-dir.json&gt;https://github.com/katerakelly/oyster/blob/master/configs/cheetah-dir.json&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/katerakelly/oyster/blob/master/configs/default.py&gt;https://github.com/katerakelly/oyster/blob/master/configs/default.py&lt;/denchmark-link&gt;
 ) a strange result.
The returns are in absolute values as expected,  however the  Cheetah is either running always forward or always backward within one evaluation phase. (so the return for forward is very high (x) and for backward very low (-x) or vice versa)
As the directions are always sampled the sampling I tried a) to hack the sampling to include always a forward and a backward task and to shuffle the order b) to sample 8 test and train tasks to make sure each direction is at least seen once. The above described behavior of running always into the same direction within one meta-evaluation was observed in both cases.
Do you have by chance an intuition what might cause this and how to fix it?
&lt;denchmark-link:https://github.com/rlworkgroup/garage/files/4558838/pearl_half_cheetah_dir.txt&gt;pearl_half_cheetah_dir.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='bordeauxred' date='2020-04-30T16:37:03Z'>
		&lt;denchmark-link:https://github.com/bordeauxred&gt;@bordeauxred&lt;/denchmark-link&gt;
 thanks for reporting a possible bug! It's user reports like yours which really help us make the software better.
I think we need a little more information from you to help us track it down.
Can you give us a report with the following template?
&lt;denchmark-h:h1&gt;Expected Behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Observed Behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;Steps to Reproduce&lt;/denchmark-h&gt;






&lt;denchmark-h:h1&gt;Possible solutions or things you've tried (and whether they worked or not)&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;More info&lt;/denchmark-h&gt;

		</comment>
		<comment id='2' author='bordeauxred' date='2020-04-30T19:46:45Z'>
		Expected:
I expected to see for halfcheetahdir (using the oyster hyper-parameters  &lt;denchmark-link:https://github.com/katerakelly/oyster/blob/master/configs/cheetah-dir.json&gt;https://github.com/katerakelly/oyster/blob/master/configs/cheetah-dir.json&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/katerakelly/oyster/blob/master/configs/default.py&gt;https://github.com/katerakelly/oyster/blob/master/configs/default.py&lt;/denchmark-link&gt;
 ))a learning curve as in the paper pearl paper:
&lt;denchmark-link:https://user-images.githubusercontent.com/28216059/80748237-e547e880-8b24-11ea-9cad-52a31505e669.png&gt;&lt;/denchmark-link&gt;

i instead got
&lt;denchmark-link:https://user-images.githubusercontent.com/28216059/80751838-ccdacc80-8b2a-11ea-8519-900bcd7771fa.png&gt;&lt;/denchmark-link&gt;

steps to reproduce:
I can't attach .py files, thus I changed the format to .txt. Please change the type back to .py and you could run it.
&lt;denchmark-link:https://github.com/rlworkgroup/garage/files/4560858/pearl_half_cheetah_dir.txt&gt;pearl_half_cheetah_dir.txt&lt;/denchmark-link&gt;

Things I tried:
When using num_train_tasks = num_test_tasks = 2  the train/ test data sometimes consisting only of one direction, thus I a) sampled either 8 (which is not necessarily balanced) or b) changed the task sampling in halfcheetahdir to
if num_tasks ==2:
directions = [-1.,1.]
np.random.shuffle(directions)
in order to have a balanced test/ trainset.
To distinguish between the forward and backward task I extended the infos dict in the step function of the HalfCheetahDirEnv  (
&lt;denchmark-link:https://github.com/rlworkgroup/garage/files/4564045/half_cheetah_dir_env.txt&gt;half_cheetah_dir_env.txt&lt;/denchmark-link&gt;

) by:
if self._goal_dir == -1.:
task_name = "hc_backwards"
elif self._goal_dir == 1.:
task_name = "hc_forward"
infos = dict(reward_forward=forward_reward,
reward_ctrl=-ctrl_cost,
goal=self._goal_dir,
task_name = task_name)
I have no idea what causes this odd learning behavior which occurred for me over different seeds. Given that the absolute value of the return is as expected I think pearl is working and the problem lies in the meta evaluation.
Please let me know if you need any further information.
		</comment>
		<comment id='3' author='bordeauxred' date='2020-05-01T21:34:44Z'>
		&lt;denchmark-link:https://github.com/bordeauxred&gt;@bordeauxred&lt;/denchmark-link&gt;
 Thanks for the detailed report.
In the MetaTest plot above, what is num_test_tasks set to?
		</comment>
		<comment id='4' author='bordeauxred' date='2020-05-01T21:56:32Z'>
		If you want to speed up debugging, you could help us by sending us the experiment directory in question (in data/local) via a file-sharing service, and/or uploading the TensorBoard outputs to &lt;denchmark-link:https://tensorboard.dev&gt;https://tensorboard.dev&lt;/denchmark-link&gt;
 so we can look at them for you.
		</comment>
		<comment id='5' author='bordeauxred' date='2020-05-01T22:08:16Z'>
		Thanks again for the detailed report.
Looking at the graph, I think your guess that there's something wrong with how the MetaEvaluator and PEARL are coupled together is right. I've found what might be the bug in PEARL.adapt_policy, and I'm running an experiment now to verify it.
		</comment>
		<comment id='6' author='bordeauxred' date='2020-05-02T15:19:35Z'>
		&lt;denchmark-link:https://github.com/krzentner&gt;@krzentner&lt;/denchmark-link&gt;

Thank you, this solves the problem.
		</comment>
	</comments>
</bug>