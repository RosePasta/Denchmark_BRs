<bug id='5534' author='cuixing158' open_date='2020-10-19T05:08:36Z' closed_time='2020-10-23T18:52:49Z'>
	<summary>Fail: [ONNXRuntimeError] : 1 : FAIL : Type Error: Type parameter (T) bound to different types (tensor(float) and tensor(int64) in node (742_zero_point_Sub).</summary>
	<description>
Describe the bug
When using onnxruntime to inference int8 quantization model, there is an errorï¼š
python ort_useQuantize.py
&lt;denchmark-code&gt;Traceback (most recent call last):

  File "ort_useQuantize.py", line 58, in &lt;module&gt;
    sess = rt.InferenceSession(faceRecModel)# , sess_options=sess_options

  File "D:\anaconda3\lib\site-packages\onnxruntime\capi\session.py", line 158, in __init__
    self._load_model(providers or [])

  File "D:\anaconda3\lib\site-packages\onnxruntime\capi\session.py", line 177, in _load_model
    self._sess.load_model(providers)

Fail: [ONNXRuntimeError] : 1 : FAIL : Type Error: Type parameter (T) bound to different types (tensor(float) and tensor(int64) in node (742_zero_point_Sub).
&lt;/denchmark-code&gt;

Urgency
none.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10
ONNX Runtime installed from (source or binary):binary, pip install onnxruntime-gpu==1.4.0
ONNX Runtime version:1.4
Python version:3.7
Visual Studio version (if applicable):vs2015
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:10.1/7
GPU model and memory:gtx1050ti-4G


I refer to &lt;denchmark-link:https://github.com/microsoft/onnxruntime/tree/v1.0.0/onnxruntime/python/tools/quantization#quantize-an-onnx-model&gt;here&lt;/denchmark-link&gt;
 to carry out quantitative transformation of my model, the code is as follows
import onnx
from quantize import quantize, QuantizationMode

# Load the onnx model
model = onnx.load(r'model_ir_se50.onnx')
# Quantize
quantized_model = quantize(model, quantization_mode=QuantizationMode.IntegerOps)
# Save the quantized model
onnx.save(quantized_model, 'model_ir_se50_quantize.onnx')
than, i inference my model:
import onnxruntime as rt
faceRecModel = r'model_ir_se50_quantize.onnx' 
sess = rt.InferenceSession(faceRecModel) # Fail: [ONNXRuntimeError] : 1 : FAIL : Type Error: ...
Expected behavior
no error
Screenshots

'r'model_ir_se50.onnx'  model is &lt;denchmark-link:https://drive.google.com/file/d/1txdBVOoXuTox8l4s67eRLVvhBoawwBqk/view?usp=sharing&gt;here&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='cuixing158' date='2020-10-23T18:52:25Z'>
		&lt;denchmark-link:https://github.com/cuixing158&gt;@cuixing158&lt;/denchmark-link&gt;
 , the issue was fixed in 1.5 version. We also have API updated. Please refer to the doc here &lt;denchmark-link:url&gt;https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/quantization&lt;/denchmark-link&gt;
. For your model, I would recommend you to use the static quantization instead of dynamic to get better performance. You can do a level 1 perf improvement before quantization to get further improvement.
		</comment>
	</comments>
</bug>