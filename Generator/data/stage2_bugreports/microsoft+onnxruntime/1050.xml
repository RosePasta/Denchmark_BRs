<bug id='1050' author='yanivbenny' open_date='2019-05-16T13:25:07Z' closed_time='2019-05-16T22:02:43Z'>
	<summary>onnx Runtime error for keras Reshape layer with batch</summary>
	<description>
Hello
I have a keras model with tensorflow backend converted to onnx with keras2onnx. I'm trying to run prediction with onnxruntime.
The conversion to onnx ends without errors.
data_format = 'channels_last'
K.set_image_data_format(data_format)
input_shape = (100, 100, 3)
x = np.random.randn(5, *input_shape).astype(np.float32)
&lt;denchmark-h:h1&gt;Build model&lt;/denchmark-h&gt;

model = keras.models.Sequential()
model.add(keras.layers.Conv2D(2, (3, 3), padding='same', input_shape=input_shape))
model.add(keras.layers.Reshape(target_shape=(1,1,100 * 100 * 2)))
&lt;denchmark-h:h1&gt;Convert to onnx&lt;/denchmark-h&gt;

onnx_model = onnxmltools.convert_keras(model, target_opset=9)
sess = onnxruntime.InferenceSession(onnx_model.SerializeToString())
&lt;denchmark-h:h1&gt;Run prediction with onnx model&lt;/denchmark-h&gt;

feed = dict([(input.name, x) for n, input in enumerate(sess.get_inputs())])
pred_onnx = sess.run(None, feed)[0]
I get the error for reshaping a batch
File "C:\Users\t-yabenn\AppData\Local\Continuum\anaconda3\envs\ilabs_onnx_temp\lib\site-packages\onnxruntime\capi\session.py", line 72, in run
return self._sess.run(output_names, input_feed, run_options)
RuntimeError: Method run failed due to: [ONNXRuntimeError] : 1 : GENERAL ERROR : d:\3\s\onnxruntime\core\providers\cpu\tensor\reshape_helper.h:43 onnxruntime::ReshapeHelper::ReshapeHelper gsl::narrow_cast&lt;int64_t&gt;(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{5,100,100,2}, requested shape:{1,1,1,20000}
The reshape fails because onnx doesn't expect a batch, therefore it thinks the input is 5 times bigger than the output.
When I change the input to be x = np.random.randn(1, *input_shape).astype(np.float32) NO BATCH it runs perfectly.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
ONNX Runtime installed from (source or binary): pip install onnxruntime
ONNX Runtime version: 0.4.0
Python version: 3.6.0

Thnx
	</description>
	<comments>
		<comment id='1' author='yanivbenny' date='2019-05-16T19:55:11Z'>
		Hello @yanxben,
Thanks for raising this.
Thinking out aloud -


I think had the requested shape been {-1, 1, 1, 20000} instead of {1, 1, 1, 20000}, it would have worked as expected as ONNX does allow  one dimension in the new shape to be -1 so that it can be inferred (usually batch size).


The next question is how do we make the requested shape {-1, 1, 1, 20000} instead of {1, 1, 1, 20000} in the 'Reshape' node of the ONNX model. I feel that this has to be taken care of by the converter.


Adding some converter folks to shed light on this - &lt;denchmark-link:https://github.com/wenbingl&gt;@wenbingl&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jeffsaremi&gt;@jeffsaremi&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jiafatom&gt;@jiafatom&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='yanivbenny' date='2019-05-16T21:28:55Z'>
		the converter itself doesn't support the batch but there is some workaround for it, you can move the issue to keras2onnx github.
		</comment>
		<comment id='3' author='yanivbenny' date='2019-05-16T22:02:43Z'>
		@yanxben - The core problem is that the converted model doesn't seem to support batch inferencing, and the error you get from the runtime when trying to perform batch inferencing is just indicating that the model doesn't support that. Please open an issue in keras2onnx github repo to find out how to make the ONNX model support variable batch size.
Closing this as this is not a runtime issue. Please re-open if you you have any concerns.
Thanks!
		</comment>
		<comment id='4' author='yanivbenny' date='2019-05-20T09:25:49Z'>
		Thnx for the help, opened an issue in keras2onnx
		</comment>
	</comments>
</bug>