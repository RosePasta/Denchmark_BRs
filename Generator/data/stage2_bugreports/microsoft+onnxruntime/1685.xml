<bug id='1685' author='dmagee' open_date='2019-08-23T17:17:01Z' closed_time='2020-07-11T02:40:00Z'>
	<summary>CNTK generated ONNX File fails to load</summary>
	<description>
Describe the bug
The following onnx file fails to load in the C API:
&lt;denchmark-link:https://drive.google.com/open?id=1mdLaDgBFWDYU-KNJ9zoToLk2FcMhDtMZ&gt;https://drive.google.com/open?id=1mdLaDgBFWDYU-KNJ9zoToLk2FcMhDtMZ&lt;/denchmark-link&gt;

The error is:
C:\Software\onnxruntime-win-x64-gpu-0.5.0\Examples-build\Release&gt;example-app.exe
Using Onnxruntime C API
Load model from TISSUETYPE_8X_DSNET_12D.cntk2.7.onnx failed:[ShapeInferenceError] Attribute strides has incorrect size
This file was exported from CNTK under windows. It loads with netron (&lt;denchmark-link:https://lutzroeder.github.io/netron/&gt;https://lutzroeder.github.io/netron/&lt;/denchmark-link&gt;
), and python onnx module, but not TensorRT (see &lt;denchmark-link:https://devtalk.nvidia.com/default/topic/1061738/tensorrt/tensorrt-fails-to-load-onnx-file-created-by-cntk/post/5376384/#5376384&gt;https://devtalk.nvidia.com/default/topic/1061738/tensorrt/tensorrt-fails-to-load-onnx-file-created-by-cntk/post/5376384/#5376384&lt;/denchmark-link&gt;
).
Urgency
Stops people migrating away from CNTK.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
ONNX Runtime installed from (source or binary): Binary
ONNX Runtime version: 0.5.0
Python version: n/a
Visual Studio version (if applicable): 2017
CUDA/cuDNN version: n/a ?
GPU model and memory: NVIDIA 1080


Use Slightly modified version of &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/C_Api_Sample.cpp&gt;https://github.com/microsoft/onnxruntime/blob/master/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/C_Api_Sample.cpp&lt;/denchmark-link&gt;
 with model name changed (also had to replace ORT_ENABLE_BASIC with the code 1 as it doesn't seem to be  defined in v0.5.0).
Expected behavior
Should load model and nor report error as above.
Additional context
I suspect this could be something to do with inferred dimensions/parameters. Some ONNX code (netron, python onnx module) deal with such things well, others (onnxruntime c api, TensorRT) do not. I'm not sure if CNTK is producing invalid onnx or if the libraries that don't work aren't flexible enough, or maybe there is an onnx  version mismatch.
	</description>
	<comments>
		<comment id='1' author='dmagee' date='2019-08-23T18:50:24Z'>
		&lt;denchmark-link:https://github.com/ebarsoumMS&gt;@ebarsoumMS&lt;/denchmark-link&gt;
 : can you see if this is an issue with cntk export?
		</comment>
		<comment id='2' author='dmagee' date='2019-08-27T09:18:28Z'>
		The same file actually has issues reading in CNTK, although a bugfix to the CNTK reader sorts that. Worth linking the issue here:
&lt;denchmark-link:https://github.com/microsoft/CNTK/issues/3746&gt;microsoft/CNTK#3746&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='dmagee' date='2019-09-09T16:16:37Z'>
		&lt;denchmark-link:https://github.com/ebarsoumMS&gt;@ebarsoumMS&lt;/denchmark-link&gt;
 any progress on this? Is it likely a CNTK issue or a c++ onnxruntime issue? I had a quick look at the CNTK code and it seems to use onnx runtime (possibly not the latest version??) to do the export.
		</comment>
		<comment id='4' author='dmagee' date='2019-09-17T08:38:11Z'>
		I have the same problem. Have you solved it?
		</comment>
		<comment id='5' author='dmagee' date='2019-09-17T09:18:00Z'>
		&lt;denchmark-link:https://github.com/huangyiping-ai&gt;@huangyiping-ai&lt;/denchmark-link&gt;
 no, not yet. I'm still awaiting the guys from Microsoft to comment whether it's likely a cntk or onnxruntime issue. Would really like a solution!
		</comment>
		<comment id='6' author='dmagee' date='2019-09-17T09:51:27Z'>
		&lt;denchmark-link:https://github.com/dmagee&gt;@dmagee&lt;/denchmark-link&gt;

I converted the pytorch model into onnx model.
When I run
Import onnx
Model_onnx_path= "LPRNet_model.onnx"
Model = onnx. load (model_onnx_path)
Onnx. checker. check_model (model)
When onnx version = 1.1.? The shape error of onnx. checker. check_model
When onnx version = 1.3.0, onnx. checker. check_model passes
So, I guess on onnxruntime, is it also a version error? For reference only, the guess may be wrong.
		</comment>
		<comment id='7' author='dmagee' date='2019-09-17T12:17:20Z'>
		@huangyping-ai that looks like you are using the python version of onnxruntime. My model works with the python version, just not with the c++ version. I think you probably have a slightly different issue? I may be wrong.
		</comment>
		<comment id='8' author='dmagee' date='2019-10-02T18:06:07Z'>
		Update: I'm not sure if anyone at Microsoft is looking into this (&lt;denchmark-link:https://github.com/faxu&gt;@faxu&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/ebarsoumMS&gt;@ebarsoumMS&lt;/denchmark-link&gt;
 ?), but I downloaded the  onnxruntime source. The C API seems to be in a state of flux (my example and one of the provided examples doesn't compile due to various missing functions), but I got a C++ example working and it gives the same error as the binary release with c API:
Load model from TISSUETYPE_8X_DSNET_12D.cntk2.7.onnx failed:[ShapeInferenceError] Attribute strides has incorrect size
I tracked this down to old.cc ~ line 60:
&lt;denchmark-code&gt;  std::vector&lt;int64_t&gt; strides;
  if (getRepeatedAttribute(ctx, "strides", strides)) {
    if (strides.size() != n_input_dims) {
      fail_shape_inference("Attribute strides has incorrect size");
    }
  } else {
    strides.assign(n_input_dims, 1);
  }
&lt;/denchmark-code&gt;

strides.size() seems to be 2, whereas n_input_dims is 3. The latter is correct I think. The input is an RGB image so 3 x 1024 x 1024. All functions in the network deal with 3D data (whrgb or whnfmaps).
Still not sure how to fix it though. CNTK uses onnxruntime to write the file.
D.
		</comment>
		<comment id='9' author='dmagee' date='2019-10-03T13:26:18Z'>
		Further update: This seems to be an issue with pooling node (convPoolShapeInference1(), old.cc). This network does 3x3 max pooling on 3D data (W x H x RGB or later  W x H x nFmaps). To achieve this in CNTK you provide a 2D shape description (CNTK::NDShape(2, 3) in C++). Obviously this has an implicit 3rd dimension (the kernel is effectively 3x3x1). The same issue applies to strides ([2,2] and pad as well i.e. they are specified as 2D, but are implicitly 3D. It seems they are output to the onnx file in this implicit form. onnxruntime doesn't seem to handle this when reading the file. I tried adding a few lines to the parser (in old.cc) in the appropriate places between load and checks:
&lt;denchmark-code&gt;while(strides.size() &lt; n_input_dims) {
    strides.push_back(1) ;
}
&lt;/denchmark-code&gt;

...
while (kernel_shape.size() &lt; n_input_dims) {
kernel_shape.push_back(1);
}
...
while (pads.size() &lt; n_input_dims*2) {
pads.push_back(0);
}
However, now I'm getting the error:
failed:Node:Pooling86 Output:Pooling86_Output_0 [ShapeInferenceError] Can't merge shape info. Both source and target dimension have values but they differ. Source=15 Target=32 Dimension=2
This pool should go from 1x32x1024x512 -&gt; 1 x 32x511x255 (i.e. no padding in any dimension, and no pooling in the fmaps dimension), however my change above to the kernel shape seems to make it pool in that dimension as well causing the error (at least that's what I think is happening).
I still have no idea if this is an onnx issue or a cntk one (I guess that depends on what's in the onnx standard about implicit dimensions - is there even a standard??). However, netron and the python version of onnx seem to handle this fine.
		</comment>
		<comment id='10' author='dmagee' date='2019-11-11T22:03:29Z'>
		&lt;denchmark-link:https://github.com/dmagee&gt;@dmagee&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/huangyiping-ai&gt;@huangyiping-ai&lt;/denchmark-link&gt;
 are you still impacted by this issue with the latest version of ONNX Runtime? If so and you still require assistance, please let us know and we can investigate further.
		</comment>
		<comment id='11' author='dmagee' date='2019-11-16T05:06:44Z'>
		Hi &lt;denchmark-link:https://github.com/faxu&gt;@faxu&lt;/denchmark-link&gt;
 , I've come across this problem too. I'll test with the latest version of Onnx and report back.
		</comment>
		<comment id='12' author='dmagee' date='2019-11-16T05:29:14Z'>
		&lt;denchmark-code&gt;[ErrorCode:Fail] Load model from C:\Users\Igor\Documents\VLML\PythonResearchProjects\PythonExportCNTKToOnnx\VAE_Model_4.onnx failed:[ShapeInferenceError] Attribute dilations has incorrect size
   at Microsoft.ML.OnnxRuntime.NativeApiStatus.VerifySuccess(IntPtr nativeStatus)
   at Microsoft.ML.OnnxRuntime.InferenceSession.Init(String modelPath, SessionOptions options)
   at Microsoft.ML.OnnxRuntime.InferenceSession..ctor(String modelPath, SessionOptions options)
   at VLML.RunnerCore.UpdateDEBUG(PatchTracer tracer, Path ModelFile, IEnumerable`1 Input_Data...
&lt;/denchmark-code&gt;

Same error for two different models (one a &lt;denchmark-link:https://drive.google.com/file/d/1judW2Xw2JlKdJRt6JSWMv57PPRQ6_wJt/view?usp=sharing&gt;VAE &lt;/denchmark-link&gt;
 the other a ResNet Transfer Learning example which I'll link to when its finished uploading.
Thanks for looking into this
		</comment>
		<comment id='13' author='dmagee' date='2019-11-16T07:49:47Z'>
		I thought I'd commented last week, but yes it is still an issue in the latest binary release.
		</comment>
		<comment id='14' author='dmagee' date='2019-12-17T22:05:12Z'>
		From ONNX &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/Operators.md#MaxPool&gt;spec&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/onnx/defs/nn/defs.cc#L138-L140&gt;implementation&lt;/denchmark-link&gt;
, it seems to only support pooling2D for 4D input, and pooling3D for 5D input. The model exported from CNTK has a 5D input for pooling ops, but the kernel shape is 2D. In the same model, there are conv ops having reshape before/after to make sure input is 4D, and IMO Pooling ops should do the same in CNTK exporter.
For Netron and python onnx module, shape inference is either not invoked or silently ignored. Before the bug is fixed, a workaround is to edit the MaxPool/AveragePool nodes in the model, with Squeeze/Unsqueeze (assuming batch==1) before and after to make sure the input to pooling ops are 4D:
&lt;denchmark-code&gt;import onnx
from onnx import helper, shape_inference
mp = onnx.load('/path/to/model')
mp_fix = onnx.ModelProto()
mp_fix.CopyFrom(mp)
mp_fix.graph.ClearField('node')
mp_fix.graph.ClearField('value_info')
for n in mp.graph.node:
  if n.op_type in ['MaxPool', 'AveragePool']:
    mp_fix.graph.node.add().CopyFrom(helper.make_node('Squeeze', n.input, [n.input[0] + '_squeezed'], n.name + '_squeeze_before', axes=[1]))
    pool_node = mp_fix.graph.node.add()
    pool_node.CopyFrom(n)
    pool_node.input[0] += '_squeezed'
    pool_node.output[0] += '_before_unsqueeze'
    mp_fix.graph.node.add().CopyFrom(helper.make_node('Unsqueeze', pool_node.output, n.output, n.name + '_unsqueeze_after', axes=[1]))
  else:
    mp_fix.graph.node.add().CopyFrom(n)
mp_fix = shape_inference.infer_shapes(mp_fix)
onnx.save(mp_fix, '/path/to/fixed_model')
&lt;/denchmark-code&gt;

Now the fixed model should have no issue in loading in ONNX runtime.
		</comment>
		<comment id='15' author='dmagee' date='2020-02-28T00:58:39Z'>
		&lt;denchmark-link:https://github.com/dmagee&gt;@dmagee&lt;/denchmark-link&gt;
 does the fixed model work for you? It seems the root cause of the issue will take some time to investigate in CNTK and is lower priority at this time. Let us know if you're blocked on this with some urgency.
		</comment>
		<comment id='16' author='dmagee' date='2020-02-28T15:20:44Z'>
		I'm afraid we abandoned trying to get onnxruntime to work. We're working in a commercial environment and the fact that you couldn't sort this properly (even given the fact that we practically gave you the solution above) in a sensible timeframe meant we went looking for other solutions, Our networks work with TensorRT now (as of v7) and we're going forward with that, which has the added advantage of native TensorFlow support when we eventually move away from CNTK.
If you want to fix onnxruntime it for others I suggest looking at my comments above. I think the issue with the "Can't merge shape info." was that I added the implicit dimensions at the wrong end of the array. I suspect it's pretty easy to get onnxruntime to support this, and given both CNTK and pyTorch have generated such models possibly a good idea?
		</comment>
		<comment id='17' author='dmagee' date='2020-07-03T04:58:09Z'>
		This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.
		</comment>
		<comment id='18' author='dmagee' date='2020-07-11T02:39:30Z'>
		This issue has been automatically closed due to inactivity. Please reactivate if further support is needed.
		</comment>
	</comments>
</bug>