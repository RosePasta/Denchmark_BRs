<bug id='4922' author='rajeevbaalwan' open_date='2020-08-26T05:28:32Z' closed_time='2020-11-16T03:48:07Z'>
	<summary>Unable to load pytorch model exported to onnx using use_external_data_format=True Flag</summary>
	<description>
I'm trying to convert transformer encoder decoder model to onnx format but unable to export it in a single file, as protobuf capacity is limited to 2GB only
But pytorch allows to save onnx model to external data format
here is the doc for this parameter :

external_data_format (bool, default False) – If True, then the model is exported in ONNX external data format, in which case some of the model parameters are stored in external binary files and not in the ONNX model file itself. See link for format details: https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423 Also, in this case, argument ‘f’ must be a string specifying the location of the model. The external binary files will be stored in the same location specified by the model location ‘f’. If False, then the model is stored in regular format, i.e. model and parameters are all in one file. This argument is ignored for all export types other than ONNX.

This is how i exported the model :
&lt;denchmark-code&gt;def convert_to_onnx(features):
    import torch.onnx
    torch.onnx.export(model,
                      features,
                      '/home/rajeev/pb/espnet/agent_model/onnx_model/model_aten.onnx',
                      input_names= ['features'],
                      output_names=['hyp_text'],
                      dynamic_axes={
                          'features':{0:'batch_size',1: 'time_steps'},
                          'hyp_text':{0:'batch_size'}
                      },
                      verbose=True,
                      opset_version=12,
                      # operator_export_type= torch.onnx.OperatorExportTypes.ONNX_ATEN,
                      use_external_data_format=True,
                      enable_onnx_checker=True,
                      do_constant_folding=True
                      )
&lt;/denchmark-code&gt;

total size of all the files exported are of 9 GB and my actual model was of 1 GB.
/home/rajeev/pb/espnet/agent_model/onnx_model folder contains 5344 exported files along with model_aten.onnx file having size of only 4MB.
But when I am trying to load the model I am getting this error :
&lt;denchmark-code&gt;Error while loading the model: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /onnxruntime_src/onnxruntime/core/optimizer/optimizer_execution_frame.cc:61 onnxruntime::OptimizerExecutionFrame::Info::Info(const std::vector&lt;const onnxruntime::Node*&gt;&amp;, const InitializedTensorSet&amp;, std::unique_ptr&lt;onnxruntime::CPUExecutionProvider&gt;) [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Invalid fd was supplied: -1
&lt;/denchmark-code&gt;

Here is the way i am trying to load the model:
&lt;denchmark-code&gt;try:
    onnx_options = SessionOptions()
    _ = InferenceSession(path, onnx_options)
    print("Model correctly loaded")
except RuntimeException as re:
    print("Error while loading the model: {}".format(re))
&lt;/denchmark-code&gt;

I have not found any good solution for this.
Please me out achieving this.
	</description>
	<comments>
		<comment id='1' author='rajeevbaalwan' date='2020-08-26T17:58:17Z'>
		Hi, thanks for the feedback.
Can you please try to disable optimizer (Disables all optimizations) see if it works? &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md#python-api-usage&gt;https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md#python-api-usage&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='rajeevbaalwan' date='2020-11-01T07:41:53Z'>
		This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.
		</comment>
		<comment id='3' author='rajeevbaalwan' date='2020-11-16T03:47:49Z'>
		This issue has been automatically closed due to inactivity. Please reactivate if further support is needed.
		</comment>
	</comments>
</bug>