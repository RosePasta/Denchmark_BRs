<bug id='1212' author='vsooda' open_date='2019-06-12T12:27:17Z' closed_time='2019-06-25T21:55:35Z'>
	<summary>[TypeInferenceError] Graph attribute inferencing failed</summary>
	<description>

I use c api to load &lt;denchmark-link:https://github.com/keithito/tacotron/blob/master/models/tacotron.py&gt;tacotron&lt;/denchmark-link&gt;
 model witch I had converted to onnx fp format. It report error:
&lt;denchmark-code&gt;Load model from decoder_fp16.onnx failed:[TypeInferenceError] Graph attribute inferencing failed: Node:generic_loop_Loop__97 Tensor element type mismatch. 10 != 1
&lt;/denchmark-code&gt;

What is Node:generic_loop_Loop__97,  how can I debug.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
ONNX Runtime installed from (source or binary): binary
ONNX Runtime version: 0.4.0
Python version: c++ cpu

To Reproduce
the origin tensorflow model is something like:
&lt;denchmark-code&gt;      attention_cell = AttentionWrapper(
        GRUCell(hp.attention_depth),
        BahdanauAttention(hp.attention_depth, encoder_outputs),
        alignment_history=True,
        output_attention=False)                                                  # [N, T_in, attention_depth=256]
      
      # Apply prenet before concatenation in AttentionWrapper.
      attention_cell = DecoderPrenetWrapper(attention_cell, is_training, hp.prenet_depths)

      # Concatenate attention context vector and RNN cell output into a 2*attention_depth=512D vector.
      concat_cell = ConcatOutputAndAttentionWrapper(attention_cell)              # [N, T_in, 2*attention_depth=512]

      # Decoder (layers specified bottom to top):
      decoder_cell = MultiRNNCell([
          OutputProjectionWrapper(concat_cell, hp.decoder_depth),
          ResidualWrapper(GRUCell(hp.decoder_depth)),
          ResidualWrapper(GRUCell(hp.decoder_depth))
        ], state_is_tuple=True)                                                  # [N, T_in, decoder_depth=256]

      # Project onto r mel spectrograms (predict r outputs at each RNN step):
      output_cell = OutputProjectionWrapper(decoder_cell, hp.num_mels * hp.outputs_per_step)
      decoder_init_state = output_cell.zero_state(batch_size=batch_size, dtype=tf.float32)

      if is_training:
        helper = TacoTrainingHelper(inputs, mel_targets, hp.num_mels, hp.outputs_per_step)
      else:
        helper = TacoTestHelper(batch_size, hp.num_mels, hp.outputs_per_step)

      (decoder_outputs, _), final_decoder_state, _ = tf.contrib.seq2seq.dynamic_decode(
        BasicDecoder(output_cell, helper, decoder_init_state),
        maximum_iterations=hp.max_iters)                                         # [N, T_out/r, M*r]

      # Reshape outputs to be one output per entry
      mel_outputs = tf.reshape(decoder_outputs, [batch_size, -1, hp.num_mels])   # [N, T_out, M]

      # Add post-processing CBHG:
      post_outputs = post_cbhg(mel_outputs, hp.num_mels, is_training,            # [N, T_out, postnet_depth=256]
                               hp.postnet_depth)
      linear_outputs = tf.layers.dense(post_outputs, hp.num_freq)                # [N, T_out, F]

      # Grab alignments from the final decoder state:
      alignments = tf.transpose(final_decoder_state[0].alignment_history.stack(), [1, 2, 0])

&lt;/denchmark-code&gt;

Additional context
I can load model and inference with model float 32, but fail with fp 16.  I use the script to convert model from float 32 to float 16:
&lt;denchmark-code&gt;import onnxmltools
from onnxmltools.utils.float16_converter import convert_float_to_float16
input_onnx_model = 'decoder.onnx'
output_onnx_model = 'decoder_fp16.onnx'
onnx_model = onnxmltools.utils.load_model(input_onnx_model)
onnx_model = convert_float_to_float16(onnx_model)
onnxmltools.utils.save_model(onnx_model, output_onnx_model)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='vsooda' date='2019-06-12T18:44:15Z'>
		Can you attach decoder.onnx? generic_loop_Loop__97 is the name of a node in the graph and it looks like there's a type mismatch. 10 = float16 and 1 = float32.
		</comment>
		<comment id='2' author='vsooda' date='2019-06-14T02:40:48Z'>
		&lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;
  thank you. I'm checking model convert tools.
		</comment>
		<comment id='3' author='vsooda' date='2019-06-19T18:57:46Z'>
		Hi &lt;denchmark-link:https://github.com/vsooda&gt;@vsooda&lt;/denchmark-link&gt;
,
Were you able to resolve this ?
Thanks
		</comment>
		<comment id='4' author='vsooda' date='2019-06-20T02:05:51Z'>
		Hi &lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 ,
No. I had checked the model convert tool, couldn't not found any problem.
could onnxruntime support seq2seq model fp16 inference?  I was worried about wasting too much time on it, and finally found out that was onnxruntime kernel's problem
		</comment>
		<comment id='5' author='vsooda' date='2019-06-20T18:58:01Z'>
		Can you share your model ?
		</comment>
		<comment id='6' author='vsooda' date='2019-06-25T08:03:56Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 thank you. I had tried to ask our company to share the model,  but failed.  I will debug onnx convert tool and onnxruntime after I finish the job at hand. or I will try to use an open source model to reproduce this problem.
		</comment>
		<comment id='7' author='vsooda' date='2019-06-25T21:55:35Z'>
		Closing this issue for now. If you have an example model or portion of a model to share, feedback on error messaging/debuggability, let us know and feel free to reactivate in future.
		</comment>
	</comments>
</bug>