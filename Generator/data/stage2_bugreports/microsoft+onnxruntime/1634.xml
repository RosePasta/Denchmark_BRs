<bug id='1634' author='christian-vorhemus' open_date='2019-08-16T13:09:25Z' closed_time='2019-08-16T19:12:26Z'>
	<summary>Load model failed - invalid model?</summary>
	<description>
I trained the following model in Keras:
model = Sequential([
    InputLayer(input_shape=[3,image_height,image_width]),
    Conv2D(filters=32, kernel_size=5, strides=1, padding='same'),
    BatchNormalization(),
    Activation('relu'),
    MaxPool2D(pool_size=8, padding='same'),
    Conv2D(filters=48, kernel_size=3, strides=1, padding='same'),
    BatchNormalization(),
    Activation('relu'),
    MaxPool2D(pool_size=5, padding='same'),
    Conv2D(filters=64, kernel_size=3, strides=1, padding='same'),
    BatchNormalization(),
    Activation('relu'),
    MaxPool2D(pool_size=3, padding='same'),
    Conv2D(filters=32, kernel_size=5, strides=1, padding='same'),
    Flatten(),
    Dense(100, activation='relu'),
    Dropout(0.1),
    Dense(12, activation='softmax')
])
Once training is finished, I convert it with the onnxmltools to ONNX v5 format:
onnxmltools.utils.save_model()
When I now try to load this model with
import onnxruntime as rt
import onnx

print(onnx.__version__)
print(rt.__version__)

sess = rt.InferenceSession("./driver.onnx")
I get the following output:
&lt;denchmark-code&gt;1.5.0
0.5.0
Traceback (most recent call last):
  File "c:\Users\m\.vscode\extensions\ms-python.python-2019.6.24221\pythonFiles\ptvsd_launcher.py", line 43, in &lt;module&gt;
    main(ptvsdArgs)
  File "c:\Users\m\.vscode\extensions\ms-python.python-2019.6.24221\pythonFiles\lib\python\ptvsd\__main__.py", line 434, in main
    run()
  File "c:\Users\m\.vscode\extensions\ms-python.python-2019.6.24221\pythonFiles\lib\python\ptvsd\__main__.py", line 312, in run_file
    runpy.run_path(target, run_name='__main__')
  File "C:\Python36\lib\runpy.py", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File "C:\Python36\lib\runpy.py", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File "C:\Python36\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "c:\Users\m\Desktop\Self-driving-car\autcar\test.py", line 7, in &lt;module&gt;
    sess = rt.InferenceSession("./driver_keras.onnx")
  File "C:\Python36\lib\site-packages\onnxruntime\capi\session.py", line 29, in __init__
    self._sess.load_model(path_or_bytes)
RuntimeError: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from ./driver.onnx failed:This is an invalid model. Error in Node:TFNodes_flatten_1_strided_slice : Node (TFNodes_flatten_1_strided_slice) has input size 1 not in range [min=3, max=5].
&lt;/denchmark-code&gt;

You can download driver.onnx &lt;denchmark-link:https://1drv.ms/u/s!AlibP17lhWs9g7UVlwLhwKr6mZPAig?e=V5GHMR&gt;here&lt;/denchmark-link&gt;
.
Now, when you create the model with ONNX v4 it suddenly works! Take &lt;denchmark-link:https://1drv.ms/u/s!AlibP17lhWs9g7UWCXflk1kqldAfMA?e=q3U0j4&gt;this model&lt;/denchmark-link&gt;
 for example. It has the exact same model definition, the only difference is the format (ONNX v4 instead of ONNX v5)
	</description>
	<comments>
		<comment id='1' author='christian-vorhemus' date='2019-08-16T18:52:59Z'>
		&lt;denchmark-link:https://github.com/christian-vorhemus&gt;@christian-vorhemus&lt;/denchmark-link&gt;

how do you save you model with ONNX v4 version? downgrade your ONNX pip package?
		</comment>
		<comment id='2' author='christian-vorhemus' date='2019-08-16T19:12:26Z'>
		Hi,
This is a converter bug. The model is invalid, it can't pass onnx test.
&lt;denchmark-code&gt;C:\Python37\Scripts\check-model.exe driver.onnx
Traceback (most recent call last):
  File "c:\python37\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "c:\python37\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "C:\Python37\Scripts\check-model.exe\__main__.py", line 9, in &lt;module&gt;
  File "c:\python37\lib\site-packages\onnx\bin\checker.py", line 17, in check_model
    checker.check_model(model)
  File "c:\python37\lib\site-packages\onnx\checker.py", line 86, in check_model
    C.check_model(model.SerializeToString())
onnx.onnx_cpp2py_export.checker.ValidationError: Node (TFNodes_flatten_1_strided_slice) has input size 1 not in range [min=3, max=5].

==&gt; Context: Bad node spec: input: "TFNodes_flatten_1_Shape__5_0" output: "TFNodes_flatten_1_strided_slice_0" name: "TFNodes_flatten_1_strided_slice" op_type: "Slice"
&lt;/denchmark-code&gt;

The problematic node is a Slice OP. Slice op get changed in opset 10. Its new version requires at least 3 inputs. While the older one(opset&lt;=9) only has one input.  In your model, it's opset 10, but this node only has one input ,which is wrong.
Please report this bug to onnxmltools, &lt;denchmark-link:https://github.com/wenbingl&gt;@wenbingl&lt;/denchmark-link&gt;
 will help.
		</comment>
	</comments>
</bug>