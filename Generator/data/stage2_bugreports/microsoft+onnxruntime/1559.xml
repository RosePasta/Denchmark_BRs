<bug id='1559' author='ajaysg-zz' open_date='2019-08-05T10:59:05Z' closed_time='2019-09-18T00:17:38Z'>
	<summary>with reference to #1542</summary>
	<description>
&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 its a bug with onnxrt. before exporting the pytorch model to onnx, i have converted the model to float using the command
model1=model.float()
now all the tensors in the model1 is float.
mel_extra=torch.tensor(np.full((1,80,222),-10.0)).float()
dummy_input=torch.autograd.Variable(mel_extra).cuda().float()
audio = torch.tensor(np.full((1,222), -0.5)).float()
audio_input = torch.autograd.Variable(audio).cuda().float()
torch.onnx.export(model1, ((dummy_input, audio_input),), "./waveglow.onnx")
Then i passed the float tensors as mentioned above and traced the onnx graph. Onnx export was successful. When i try to use it in onnxrt by starting the session
sess = rt.InferenceSession("../waveglow.onnx")
it throws the error referenced in the issue.
	</description>
	<comments>
		<comment id='1' author='ajaysg-zz' date='2019-08-05T17:20:25Z'>
		&lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1542&gt;#1542&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/310&gt;#310&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ajaysg-zz' date='2019-08-06T09:53:03Z'>
		onnx runtime not supporting float. it supports only int64
		</comment>
		<comment id='3' author='ajaysg-zz' date='2019-09-18T00:17:38Z'>
		Hi @ajaysg,
After further investigation, it appears this is a PyTorch export issue and is tracked by this: &lt;denchmark-link:https://github.com/pytorch/pytorch/pull/24378&gt;pytorch/pytorch#24378&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>