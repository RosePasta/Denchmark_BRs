<bug id='2474' author='Mut1nyJD' open_date='2019-11-25T22:40:56Z' closed_time='2019-12-03T06:28:45Z'>
	<summary>An ONNX model that worked in 0.5.1 now fails in 1.0.0</summary>
	<description>
Describe the bug
I recently upgraded my ONNX Runtime from 0.5.1 to 1.0.0 after changing my C code to adapt to the new API I then found out that my previous ONNX model does no longer run in 1.0.0. My ONNX model uses OpSet-Level 9. It is a PyTorch1.3 ONNX export of a HRNet-backbone landmark detection model.
When loading the ONNX model in ONNXRuntime 1.0 I'll get an Exception:
op_kernel.cc:21 onnxruntime::OpKernelContext::OpKernelContext kernel != nullptr was false. OpKernel was null
it loaded fine in 0.5.1
	</description>
	<comments>
		<comment id='1' author='Mut1nyJD' date='2019-11-26T00:18:17Z'>
		If I attempt to export that model with opset 9 I see the following error:

python\python37\lib\site-packages\torch\onnx\symbolic_helper.py:198: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.
ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).
We recommend using opset 11 and above for models using this operator.

Exporting with opset 11 produces a model that loads successfully. Is that an option? I used the export instructions from this pytorch issue: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/23474&gt;pytorch/pytorch#23474&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Mut1nyJD' date='2019-11-26T00:31:03Z'>
		I also exported using opset 9 and the model produced loads successfully. Could you please share your model as well a the requested system info:
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ONNX Runtime installed from (source or binary):
ONNX Runtime version:
Python version:
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

		</comment>
		<comment id='3' author='Mut1nyJD' date='2019-11-26T07:40:49Z'>
		Sure please find here a download link to the ONNX model
&lt;denchmark-link:https://mut1nyjd.stackstorage.com/s/ZwctHMbvHJM1bhf&gt;https://mut1nyjd.stackstorage.com/s/ZwctHMbvHJM1bhf&lt;/denchmark-link&gt;


System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):


Windows 10


ONNX Runtime installed from (source or binary):


Binary NuGet package version


ONNX Runtime version:


0.51 and 1.0.0


Python version:


No Python using C Backend


Visual Studio version (if applicable):


15.9.8


CUDA/cuDNN version:


10.1/7.1


GPU model and memory:


GTX1060ti 6GB VRAM
		</comment>
		<comment id='4' author='Mut1nyJD' date='2019-11-26T11:33:24Z'>
		Forgot to say the output I'll get from ONNX Runtime 0.5.1 on this model on a test image is pretty close to what I'll get from the PyTorch model, so the model itself does not seem to be the problem
		</comment>
		<comment id='5' author='Mut1nyJD' date='2019-11-27T05:05:28Z'>
		I have not been able to reproduce on either linux or windows with CUDA 10.1. The cuDNN version I have is a bit newer on each (7.6) though but I don't expect that would matter.
Can you try completely uninstalling ONNX Runtime and reinstalling? There are only a few places we create OpKernelContext and I can't see how any of those would have a nullptr for the kernel.
		</comment>
		<comment id='6' author='Mut1nyJD' date='2019-11-27T09:17:54Z'>
		Hmm I tried a complete fresh install of ONNXRuntime with a new project also tried it with VS2015.
I installed latest CuDNN 7.6.5 for 10.1 but still seeing the same error.
I've installed the MKL-DNN version of ONNXRuntime 1.0.0 that works fine with the model.
I will try a different GPU and/with other driver and see if that makes a difference.
		</comment>
		<comment id='7' author='Mut1nyJD' date='2019-11-27T11:07:02Z'>
		Could you share the C code you're using? I tested using python so that's one other difference.
		</comment>
		<comment id='8' author='Mut1nyJD' date='2019-11-27T11:38:32Z'>
		Sure no problem, please find here my minimum test code
// ONNXRuntime
#include "onnxruntime_c_api.h"
// CUDA ONNXRuntime
#include "cuda_provider_factory.h"
`const OrtApi* g_ort = OrtGetApiBase()-&gt;GetApi(ORT_API_VERSION);`
void CheckStatus(OrtStatus* status)
{
	if (status != NULL) {
		const char* msg = g_ort-&gt;GetErrorMessage(status);
		fprintf(stderr, "%s\n", msg);
		g_ort-&gt;ReleaseStatus(status);
		exit(1);
	}
}
 
int main()
{
	OrtEnv* env = NULL;
	CheckStatus(g_ort-&gt;CreateEnv(ORT_LOGGING_LEVEL_WARNING, "test", &amp;env));
	// initialize session options if needed
	OrtSessionOptions* session_options;
	g_ort-&gt;CreateSessionOptions(&amp;session_options);
	// If you have CUDA ONNXRuntime installed otherwise don't use this line
	OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 0);
	g_ort-&gt;SetIntraOpNumThreads(session_options, 8);
	g_ort-&gt;SetInterOpNumThreads(session_options, 8);
	g_ort-&gt;SetSessionGraphOptimizationLevel(session_options, ORT_ENABLE_BASIC);
	OrtSession* session = NULL;
	CheckStatus(g_ort-&gt;CreateSession(env, L"hrnet_w18_landmarks.onnx", session_options, &amp;session));
	std::cout &lt;&lt; "ONNX model loaded succesfull\n"; 
}
		</comment>
		<comment id='9' author='Mut1nyJD' date='2019-11-27T11:43:13Z'>
		If you need a full .vcproj + code I can upload that as well
		</comment>
		<comment id='10' author='Mut1nyJD' date='2019-11-27T23:13:58Z'>
		I can reproduce the issue. Should have a fix later today.
		</comment>
		<comment id='11' author='Mut1nyJD' date='2019-11-28T07:29:29Z'>
		&lt;denchmark-link:https://github.com/skottmckay&gt;@skottmckay&lt;/denchmark-link&gt;

Great excellent! Happy you manage to reproduce it and yes sorry I misreported my setup it was CUDA 10.1/CuDNN 7.5 before I have so many CUDA/CuDNN variants installed I loose track from time to time.
		</comment>
		<comment id='12' author='Mut1nyJD' date='2019-11-28T21:48:46Z'>
		Two short term options if needed:


disable all optimizations so constant folding doesn't run

simple but has a performance cost
g_ort-&gt;SetSessionGraphOptimizationLevel(session_options, ORT_DISABLE_ALL);



run the optimizations with just the CPU execution provider enabled, save the model, and use that model with the CUDA execution provider. this is a one time step.

g_ort-&gt;SetOptimizedModelFilePath(session_options, L"hrnet_w18_landmarks.optimized.onnx");
comment out the line that adds the CUDA execution provider, set the optimized model file path, create a session and release it (no need to call Run). following that you should be able to use the CUDA execution provider with the optimized model



		</comment>
	</comments>
</bug>