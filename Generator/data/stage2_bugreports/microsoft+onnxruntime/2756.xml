<bug id='2756' author='vadimkantorov' open_date='2019-12-28T15:00:05Z' closed_time='2020-01-31T00:48:32Z'>
	<summary>Shape mismatch warnings when using dynamic axis for time dimension</summary>
	<description>
I'm exporting my PyTorch-trained convolutional speechrec model to ONNX, then I'm using onnxruntime (cpu) to run inference on a batch.
When using dynamic axis for the time dimension, I'm experiencing these strange warnings:
&lt;denchmark-code&gt;2019-12-28 14:28:56.147700928 [W:onnxruntime:Default, execution_frame.cc:335 AllocateMLValueTensorPreAllocateBuffer] Shape mismatch attempting to re-use buffer. {16,1024} != {16,1023}. Validate usage of dim_value (values should be &gt; 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
2019-12-28 14:28:56.147839016 [W:onnxruntime:Default, execution_frame.cc:335 AllocateMLValueTensorPreAllocateBuffer] Shape mismatch attempting to re-use buffer. {16,1024} != {16,1023}. Validate usage of dim_value (values should be &gt; 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
2019-12-28 14:28:56.147916603 [W:onnxruntime:Default, execution_frame.cc:335 AllocateMLValueTensorPreAllocateBuffer] Shape mismatch attempting to re-use buffer. {16,1023} != {16,1}. Validate usage of dim_value (values should be &gt; 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
&lt;/denchmark-code&gt;

Snippet to repro:
import numpy
import onnxruntime as rt

sess = rt.InferenceSession('model.onnx')
predict = sess.run(None, {'x': numpy.random.rand(1, 10000).astype(numpy.float32)})
print(predict)
model.onnx on my OneDrive: &lt;denchmark-link:https://1drv.ms/u/s!Apx8USiTtrYmqfdiyT5heiUnoaCVNw?e=1Tsmvk&gt;https://1drv.ms/u/s!Apx8USiTtrYmqfdiyT5heiUnoaCVNw?e=1Tsmvk&lt;/denchmark-link&gt;

onnxruntime.version == 1.0.0
	</description>
	<comments>
		<comment id='1' author='vadimkantorov' date='2019-12-30T04:39:42Z'>
		Hello - are your results correct though (despite the warnings) ?
		</comment>
		<comment id='2' author='vadimkantorov' date='2019-12-30T06:23:45Z'>
		Looks like a converter issue with Slice nodes. The 3 Slice nodes are claiming that the output shape matches the input shape. e.g. Slice_10, Slice_15 and Slice_20 all have input and output of shape {B, T} which is incorrect given the parameters to the Slice op.
The output shape should be {B, 1} for Slice_10 and {B, T-1} for the other two.
A symbolic name is assumed to only ever have one value. We re-use buffers where possible if the shape matches, and because of the incorrectly declared output shapes we are re-using a buffer. The warning happens when we see the actual shapes of the buffers don't match. As the re-use requires a smaller buffer we allow it, but it's just luck that the re-use requires a smaller buffer.
How was the model exported? From pytorch?
		</comment>
		<comment id='3' author='vadimkantorov' date='2019-12-30T10:07:01Z'>
		Actually it's an ONNX shape inferencing bug. The Slice shape inferencing first copies all the dimension values/names from the input to the output, but bails out on updating something if it's a symbolic value. It should instead clear the dimension so it is put in a state of unknown. The following code should be clearing any dim_param prior to the 'continue'.
&lt;denchmark-code&gt;    // input dim value is missing - cannot perform shape inference for
    // this axis
    if (!input_dim.has_dim_value())
      continue;
&lt;/denchmark-code&gt;

Luckily your model works as it will take an ONNX update to fix.
		</comment>
		<comment id='4' author='vadimkantorov' date='2019-12-30T10:39:49Z'>
		Yes, it was exported from PyTorch. It traces a LogFilterBank frontend that does preemphasis and thus uses T-1 tensor as intermediate value. It seems to still work correctly, yes.
		</comment>
		<comment id='5' author='vadimkantorov' date='2019-12-30T10:55:00Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 could you take a look? Based on the history I think you added the ONNX Slice shape inferencing change in question.
Is the following the correct fix? Works in my local hacked up testing.
&lt;denchmark-code&gt;    // input dim value is missing - cannot perform shape inference for
    // this axis. clear any dim_param so the dim is treated as unknown.
    if (!input_dim.has_dim_value()) {
      ctx.getOutputType(0)-&gt;mutable_tensor_type()-&gt;mutable_shape()-&gt;mutable_dim((int)axis)-&gt;clear_dim_param();
      continue;
    }

&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='vadimkantorov' date='2019-12-30T11:13:25Z'>
		Sure, I can take a look. Thanks for your investigation.
		</comment>
		<comment id='7' author='vadimkantorov' date='2020-01-02T14:06:54Z'>
		Looks like there are some ONNX build issues meaning it could take awhile to get the fix PR merged and consume it in ONNXRuntime. I ll close this issue when the fix PR is merged in ONNX and consumed here.
		</comment>
		<comment id='8' author='vadimkantorov' date='2020-04-17T11:28:40Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 I've just tried with onnxrt 1.2.0 and bug is still there: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/36796&gt;pytorch/pytorch#36796&lt;/denchmark-link&gt;

Note
&lt;denchmark-code&gt;2020-04-17 11:20:09.741222106 [W:onnxruntime:, execution_frame.cc:343 AllocateMLValueTensorPreAllocateBuffer] Shape mismatch attempting to re-use buffer. {16,7999} != {16,1}. Validate usage of dim_value (values should be &gt; 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
&lt;/denchmark-code&gt;

in the no-bug part.
Also, this bug I reported in PyTorch could actually be ONNXRT bug
		</comment>
		<comment id='9' author='vadimkantorov' date='2020-04-17T11:38:33Z'>
		The original warnings are also present
		</comment>
		<comment id='10' author='vadimkantorov' date='2020-04-20T00:07:27Z'>
		I believe the fix in the ONNX shape inferencing will be in ONNX v1.7, which is about to be released. That ONNX release should be included in the next onnxruntime release in a few weeks.
		</comment>
		<comment id='11' author='vadimkantorov' date='2020-06-19T13:17:27Z'>
		On onnxrt 1.3.0 / onnx 1.7.0 I still get (&lt;denchmark-link:https://1drv.ms/u/s!Apx8USiTtrYmqfdiyT5heiUnoaCVNw?e=1Tsmvk&gt;model.onnx @ OneDrive&lt;/denchmark-link&gt;
)
&lt;denchmark-code&gt;2020-06-19 12:54:02.851127515 [W:onnxruntime:, execution_frame.cc:381 AllocateMLValueTensorPreAllocateBuffer] Shape mismatch attempting to re-use buffer. {1,9999} != {1,1}. Validate usage of dim_value (values should be &gt; 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
&lt;/denchmark-code&gt;

Is it still onnxrt fault? Or is PyTorch to blame? This code just does some stft / audio frontend: &lt;denchmark-link:https://github.com/vadimkantorov/convasr/blob/master/models.py#L337-L349&gt;https://github.com/vadimkantorov/convasr/blob/master/models.py#L337-L349&lt;/denchmark-link&gt;
 and onnxruntime complains about number of channels changing (?)
Code:
import numpy, onnxruntime
# onnxruntime.set_default_logger_severity(0)
sess = onnxruntime.InferenceSession('model.onnx')
predict = sess.run(None, {'x': numpy.random.rand(1, 10000).astype(numpy.float32)})
Output with verbose mode:
&lt;denchmark-code&gt;2020-06-19 12:56:14.898532829 [I:onnxruntime:, inference_session.cc:174 ConstructorCommon] Creating and using per session threadpools since use_per_session_threads_ is true
2020-06-19 12:56:14.901844598 [I:onnxruntime:Default, bfc_arena.cc:15 BFCArena] Creating BFCArena for Cpu
2020-06-19 12:56:14.901871553 [V:onnxruntime:Default, bfc_arena.cc:32 BFCArena] Creating 21 bins of max chunk size 256 to 268435456
2020-06-19 12:56:14.901885790 [I:onnxruntime:, inference_session.cc:830 Initialize] Initializing session.
2020-06-19 12:56:14.903234317 [I:onnxruntime:, reshape_fusion.cc:37 ApplyImpl] Total fused reshape node count: 0
2020-06-19 12:56:14.908910355 [I:onnxruntime:, graph.cc:2622 CleanUnusedInitializers] Removing initializer 'backbone.12.conv.0.0.weight'. It is no longer used by any node.
2020-06-19 12:56:14.908935165 [I:onnxruntime:, graph.cc:2622 CleanUnusedInitializers] Removing initializer 'backbone.12.bn.0.bias'. It is no longer used by any node.
2020-06-19 12:56:14.908944982 [I:onnxruntime:, graph.cc:2622 CleanUnusedInitializers] Removing initializer 'backbone.12.bn.0.running_var'. It is no longer used by any node.
2020-06-19 12:56:14.908953648 [I:onnxruntime:, graph.cc:2622 CleanUnusedInitializers] Removing initializer 'backbone.12.bn.0.weight'. It is no longer used by any node.
2020-06-19 12:56:14.908963910 [I:onnxruntime:, graph.cc:2622 CleanUnusedInitializers] Removing initializer 'backbone.12.bn.0.running_mean'. It is no longer used by any node.
2020-06-19 12:56:14.909523538 [I:onnxruntime:, reshape_fusion.cc:37 ApplyImpl] Total fused reshape node count: 0
2020-06-19 12:56:14.910338279 [I:onnxruntime:, reshape_fusion.cc:37 ApplyImpl] Total fused reshape node count: 0
2020-06-19 12:56:14.919109006 [V:onnxruntime:, inference_session.cc:671 TransformGraph] Node placements
2020-06-19 12:56:14.919128546 [V:onnxruntime:, inference_session.cc:673 TransformGraph] All nodes have been placed on [CPUExecutionProvider].
2020-06-19 12:56:14.919185923 [I:onnxruntime:, session_state.cc:25 SetGraph] SaveMLValueNameIndexMapping
2020-06-19 12:56:14.919315235 [I:onnxruntime:, session_state.cc:70 SetGraph] Done saving OrtValue mappings.
2020-06-19 12:56:14.920389394 [I:onnxruntime:, session_state_initializer.cc:178 SaveInitializedTensors] Saving initialized tensors.
2020-06-19 12:56:15.290395359 [I:onnxruntime:, session_state_initializer.cc:223 SaveInitializedTensors] Done saving initialized tensors
2020-06-19 12:56:15.343228255 [I:onnxruntime:, inference_session.cc:919 Initialize] Session successfully initialized.
2020-06-19 12:56:15.343553378 [I:onnxruntime:, sequential_executor.cc:145 Execute] Begin execution
2020-06-19 12:56:15.343582156 [I:onnxruntime:Default, bfc_arena.cc:259 AllocateRawInternal] Extending BFCArena for Cpu. bin_num:0 rounded_bytes:256
2020-06-19 12:56:15.343602227 [I:onnxruntime:Default, bfc_arena.cc:143 Extend] Extended allocation by 1048576 bytes.
2020-06-19 12:56:15.343619216 [I:onnxruntime:Default, bfc_arena.cc:147 Extend] Total allocated bytes: 707714816
2020-06-19 12:56:15.343651678 [I:onnxruntime:Default, bfc_arena.cc:150 Extend] Allocated memory at 0x55e472cd1100 to 0x55e472dd1100
2020-06-19 12:56:15.354617476 [W:onnxruntime:, execution_frame.cc:381 AllocateMLValueTensorPreAllocateBuffer] Shape mismatch attempting to re-use buffer. {1,9999} != {1,1}. Validate usage of dim_value (values should be &gt; 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
2020-06-19 12:56:15.366721798 [I:onnxruntime:Default, bfc_arena.cc:259 AllocateRawInternal] Extending BFCArena for Cpu. bin_num:11 rounded_bytes:709632
2020-06-19 12:56:15.366803797 [I:onnxruntime:Default, bfc_arena.cc:143 Extend] Extended allocation by 2097152 bytes.
2020-06-19 12:56:15.366919541 [I:onnxruntime:Default, bfc_arena.cc:147 Extend] Total allocated bytes: 709811968
2020-06-19 12:56:15.367072539 [I:onnxruntime:Default, bfc_arena.cc:150 Extend] Allocated memory at 0x55e472dd1160 to 0x55e472fd1160
2020-06-19 12:56:15.403203038 [I:onnxruntime:Default, bfc_arena.cc:259 AllocateRawInternal] Extending BFCArena for Cpu. bin_num:13 rounded_bytes:2193408
2020-06-19 12:56:15.403261231 [I:onnxruntime:Default, bfc_arena.cc:143 Extend] Extended allocation by 4194304 bytes.
2020-06-19 12:56:15.403287056 [I:onnxruntime:Default, bfc_arena.cc:147 Extend] Total allocated bytes: 714006272
2020-06-19 12:56:15.403311620 [I:onnxruntime:Default, bfc_arena.cc:150 Extend] Allocated memory at 0x55e472fd11c0 to 0x55e4733d11c0
2020-06-19 12:56:15.485825873 [I:onnxruntime:Default, bfc_arena.cc:259 AllocateRawInternal] Extending BFCArena for Cpu. bin_num:14 rounded_bytes:4838400
2020-06-19 12:56:15.485879504 [I:onnxruntime:Default, bfc_arena.cc:143 Extend] Extended allocation by 8388608 bytes.
2020-06-19 12:56:15.485895414 [I:onnxruntime:Default, bfc_arena.cc:147 Extend] Total allocated bytes: 722394880
2020-06-19 12:56:15.485910275 [I:onnxruntime:Default, bfc_arena.cc:150 Extend] Allocated memory at 0x55e4733d1220 to 0x55e473bd1220
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='vadimkantorov' date='2020-06-24T14:29:34Z'>
		&lt;denchmark-link:https://github.com/skottmckay&gt;@skottmckay&lt;/denchmark-link&gt;
 should this issue be reopened?
		</comment>
		<comment id='13' author='vadimkantorov' date='2020-07-22T16:43:50Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 Could you please check &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/2756#issuecomment-646630674&gt;#2756 (comment)&lt;/denchmark-link&gt;
 ? I think this issue still persists...
		</comment>
		<comment id='14' author='vadimkantorov' date='2020-07-23T07:59:09Z'>
		The issue is that your model is using opset 10 and the fix was applied to the latest version of Slice (opset 11). Can you re-export the model using a later opset? 13 is the latest but not sure if pytorch supports that. Anything &gt;= 11 should be fine.
e.g. torch.onnx.export(..., opset_version=11, ...)
		</comment>
		<comment id='15' author='vadimkantorov' date='2020-07-23T08:00:18Z'>
		Will try! Thanks!
		</comment>
		<comment id='16' author='vadimkantorov' date='2020-07-23T13:35:39Z'>
		It seems to have helped! Case solved!
		</comment>
	</comments>
</bug>