<bug id='679' author='dashesy' open_date='2019-03-21T22:57:14Z' closed_time='2019-04-09T00:03:19Z'>
	<summary>Invalid Tracer Warning about tensor shape</summary>
	<description>
Describe the bug
After torch.onnx.export
I get this warning, for tensor.size(2)/tensor.shape[2] that is not a tensor. Even though .size() and .shape are tensors after indexing they are just normal integer. Even if I explicitly use int(tensor.shape[2]) the tracer warning is not fixed.
&lt;denchmark-code&gt;
TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!

assert x.shape[2] == x.shape[3] and x.shape[2] &gt; self.crop_size + 2

&lt;/denchmark-code&gt;

The conversion works, and the converted model is fine.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
ONNX Runtime installed from (source or binary): binary / pip
ONNX Runtime version: 0.3.0
Python version: 2

To Reproduce
class CenterCrop(nn.Module):
    def __init__(self, crop_size):
        """Crop from the center of a 4d tensor
        :param crop_size: the center crop size
        """
        super(CenterCrop, self).__init__()
        self.crop_size = crop_size

    def extra_repr(self):
        """Extra information
        """
        return 'crop_size={}'.format(
            self.crop_size
        )

    def forward(self, x):
        assert x.shape[2] == x.shape[3] and x.shape[2] &gt; self.crop_size + 2
        offset = (x.shape[2] - self.crop_size) / 2
        return x[:, :, offset:offset + self.crop_size, offset:offset + self.crop_size]
Expected behavior
Tracer to know when a tensor indexing results in an integer
Additional context
This is one of the modules I get the warning, but other custom modules may see similar warnings
	</description>
	<comments>
		<comment id='1' author='dashesy' date='2019-03-23T07:00:31Z'>
		&lt;denchmark-link:https://github.com/wenbingl&gt;@wenbingl&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/EmmaNingMS&gt;@EmmaNingMS&lt;/denchmark-link&gt;
 can someone from the converter team take a look? thanks!
&lt;denchmark-link:https://github.com/dashesy&gt;@dashesy&lt;/denchmark-link&gt;
 btw, we don't support python 2. can you check if you see the same issue with python 3.x?
		</comment>
		<comment id='2' author='dashesy' date='2019-03-25T17:38:54Z'>
		&lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;
, this is exported from torch model, &lt;denchmark-link:https://github.com/faxu&gt;@faxu&lt;/denchmark-link&gt;
 can we find any contractor who are working on pytorch exporter?
		</comment>
		<comment id='3' author='dashesy' date='2019-03-25T22:48:15Z'>
		&lt;denchmark-link:https://github.com/dashesy&gt;@dashesy&lt;/denchmark-link&gt;
 - Yes, this tracer warning is coming from PyTorch exporter and we have seen this before. Most of the times this warning is legitimate in that the JIT tracer cannot accurately capture the semantics of the dynamic ops, like the indexing op you show in your example, and instead captures a static constant for that specific run. This is what the warning is about. I think the tracer warning is pointing to a legit concern here. See the "Limitations" section in this &lt;denchmark-link:https://pytorch.org/docs/master/onnx.html?highlight=onnx#module-torch.onnx&gt;PyTorch page.&lt;/denchmark-link&gt;

The recommended way to capture this dynamic behavior is to use &lt;denchmark-link:https://pytorch.org/docs/stable/jit.htmlhttps://pytorch.org/docs/stable/jit.html&gt;TorchScript&lt;/denchmark-link&gt;
 in the export. Here's an example, that shows slicing, that shows the use of TorchScript:
&lt;denchmark-code&gt;@torch.jit.script
def slice_helper(x, offset ):
    return x[:offset ]

class CenterCrop(torch.nn.Module):
    def __init__(self, crop_size):
        """Crop from the center of a 4d tensor
        :param crop_size: the center crop size
        """
        super(CenterCrop, self).__init__()
        self.crop_size = crop_size
    
def forward(self, x):
        offset = (x.shape[2] - self.crop_size) / 2
        return slice_helper(x, offset )
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='dashesy' date='2019-03-26T01:53:05Z'>
		That makes sense for indexing, but what about the warning about assert statement?

assert x.shape[2] == x.shape[3] and x.shape[2] &gt; self.crop_size + 2

		</comment>
		<comment id='5' author='dashesy' date='2019-03-29T18:14:39Z'>
		I think it is the same root cause. Once this is a ScriptModule I expect to see this warning to go away as well.
		</comment>
		<comment id='6' author='dashesy' date='2019-04-09T00:03:19Z'>
		Closing this. &lt;denchmark-link:https://github.com/dashesy&gt;@dashesy&lt;/denchmark-link&gt;
 - Please reopen if this is still an issue.
		</comment>
		<comment id='7' author='dashesy' date='2019-04-10T01:17:53Z'>
		&lt;denchmark-link:https://github.com/spandantiwari&gt;@spandantiwari&lt;/denchmark-link&gt;
 I changed the code to this
@torch.jit.script
def center_slice_helper(x, h_offset, w_offset, crop_size):
    return x[:, :, h_offset:h_offset + crop_size, w_offset:w_offset + crop_size]


class CenterCrop(nn.Module):
    def __init__(self, crop_size):
        """Crop from the center of a 4d tensor
        :param crop_size: the center crop size
        """
        super(CenterCrop, self).__init__()
        self.crop_size = crop_size

    def extra_repr(self):
        """Extra information
        """
        return 'crop_size={}'.format(
            self.crop_size
        )

    def forward(self, x):
        assert x.shape[3] &gt; self.crop_size + 2 and x.shape[2] &gt; self.crop_size + 2, \
            "height: {} or width: {} is less than crop_size: {}".format(x.shape[2], x.shape[3], self.crop_size)
        h_offset = torch.tensor((x.shape[2] - self.crop_size) / 2)
        w_offset = torch.tensor((x.shape[3] - self.crop_size) / 2)
        return center_slice_helper(x, h_offset, w_offset, torch.tensor(self.crop_size))
I think it now respects dynamic sizes, but it does not remove warning unless I move the assert to the center_slice_helper torch script.

TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not  generalize to other inputs!                                                                                                                                                                                                                                                      &gt; assert x.shape[3] &gt; self.crop_size + 2 and x.shape[2] &gt; self.crop_size + 2, \

Also, I have to convert to torch.tensor() (as in the above script) or will get an another error:

RuntimeError: center_slice_helper() expected value of type Tensor for argument 'h_offset' in position 1, but instead got value of type int.                                                                                                                                     &gt; Value: 16                                                                                                                                                                                                                                                                       &gt;Declaration: center_slice_helper(Tensor x, Tensor h_offset, Tensor w_offset, Tensor crop_size) -&gt; Tensor

		</comment>
		<comment id='8' author='dashesy' date='2019-04-10T01:21:11Z'>
		Actually I get this error too:

h_offset = torch.tensor((x.shape[2] - self.crop_size) / 2)                                                                                                                                                                                                                    mtorch/deploy/transform_ops.py:62: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function

So it is not respecting dynamic sizes?
Can you please re-open the issue, or let me know how to fix it?
Also, let me know if I should open issue in other repo?
		</comment>
	</comments>
</bug>