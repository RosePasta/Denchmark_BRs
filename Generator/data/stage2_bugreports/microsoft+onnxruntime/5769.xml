<bug id='5769' author='feipxyz' open_date='2020-11-11T10:39:36Z' closed_time='2020-12-29T12:41:51Z'>
	<summary>The same input, sometimes the output is different</summary>
	<description>
I converted the yolov5s model to onnx, and used onnxruntime for forward calculation. When I use the onnxruntime cpu version, everything is ok, but when use onnxruntime-gpu, the same input occasionally has different output.
System information

windows10/centos7
https://pypi.tuna.tsinghua.edu.cn/simple pip install onnxruntime-gpu
ONNX Runtime version: 1.4.0/1.5.2
Python version: 3.7.6
CUDA/cuDNN version: cuda10.1
GPU: RTX2070/RTX2080TI


&lt;denchmark-link:https://drive.google.com/file/d/11PF5HpCes6LfwZ2qhJjnTVZNaiKUv_vz/view?usp=sharing&gt;model&lt;/denchmark-link&gt;

&lt;denchmark-link:https://drive.google.com/file/d/1brLpwYqILO23hJP_A88XTR7jjXZZDg4Y/view?usp=sharing&gt;test image&lt;/denchmark-link&gt;

import cv2
import numpy as np
import onnxruntime

def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):
    # Resize image to a 32-pixel-multiple rectangle https://github.com/ultralytics/yolov3/issues/232
    shape = img.shape[:2]  # current shape [height, width]
    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape)

    # Scale ratio (new / old)
    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    if not scaleup:  # only scale down, do not scale up (for better test mAP)
        r = min(r, 1.0)

    # Compute padding
    ratio_w, ratio_h = r, r  # width, height ratios
    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding
    # pdb.set_trace()
    if auto:  # minimum rectangle
        dw, dh = np.mod(dw, 64), np.mod(dh, 64)  # wh padding
    elif scaleFill:  # stretch
        dw, dh = 0.0, 0.0
        new_unpad = (new_shape[1], new_shape[0])
        ratio_w, ratio_h = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios

    dw /= 2  # divide padding into 2 sides
    dh /= 2

    if shape[::-1] != new_unpad:  # resize
        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
    return img, ratio_w, ratio_h, dw, dh



class Detection:
    def __init__(self, model_path, names, prob_threshold=0.6, iou_threshold=0.5, debug=False):
        # onnxruntime
        so = onnxruntime.SessionOptions()
        so.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC # ORT_ENABLE_EXTENDED ORT_ENABLE_ALL
        so.intra_op_num_threads = 4
        so.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL # ORT_PARALLEL ORT_SEQUENTIAL
        self.ort_session = onnxruntime.InferenceSession(model_path, sess_options=so)              
        _, self.net_input_channels, self.net_input_height, self.net_input_width = self.ort_session.get_inputs()[0].shape

        # param
        self.prob_threshold = prob_threshold
        self.iou_threshold = iou_threshold
        self.debug = debug
        self.names = names

    def prepare_image(self, image):
        image_scaled, ratio_w, ratio_h, dw, dh = letterbox(image, new_shape=(self.net_input_width, self.net_input_height), auto=False, scaleFill=False, scaleup=True)
        image_scaled = image_scaled[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB 
        image_scaled = image_scaled[np.newaxis, ...]
        image_scaled = np.ascontiguousarray(image_scaled)
        image_scaled = image_scaled.astype(np.float32) / 255.0

        return image_scaled, ratio_w, ratio_h, dw, dh

    def forward(self, image):
        # prepare image
        image_input, ratio_w, ratio_h, dw, dh = self.prepare_image(image)

        # inference
        ort_inputs = {self.ort_session.get_inputs()[0].name: image_input}
        output_name = self.ort_session.get_outputs()[0].name
        ort_outs = self.ort_session.run([output_name], ort_inputs)
        
        # post process
        pred = ort_outs[0]
        print(pred.sum())
        # result = self.post_process(pred, image.shape, ratio_w, ratio_h, dw, dh)
        # dets = result[0]

        return pred 


if __name__ == "__main__":
    model_path = 'yolov5s.model'
    image_path = 'test.jpg'
    image = cv2.imread(image_path)

    detector = Detection(model_path, "xxx")
    for i in range(1000):
        dets = detector.forward(image)
print infomation:
&lt;denchmark-code&gt;149046030.0
149577300.0
149577300.0
149577300.0
149577300.0
149577300.0
149577300.0
149577300.0
149577300.0
149577300.0
149577300.0
149577300.0
229191900.0
149577300.0
198781010.0
149577300.0
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='feipxyz' date='2020-11-16T19:15:44Z'>
		Is the attached model an onnx model or a CNTK model ? From the script, it looks like you are loading the provided model directly using ORT indicating that it is an ONNX model, but Netron complains that it is a CNTK model...
		</comment>
		<comment id='2' author='feipxyz' date='2020-11-26T06:06:37Z'>
		
Is the attached model an onnx model or a CNTK model ? From the script, it looks like you are loading the provided model directly using ORT indicating that it is an ONNX model, but Netron complains that it is a CNTK model...

It is an onnx model, I just changed the suffix from .onnx to .model. It is exported from pytorch model
		</comment>
		<comment id='3' author='feipxyz' date='2020-12-29T04:58:26Z'>
		I am not able to open the model file via Netron at all. Can you please just export it as .onnx and not mutate the file type ?
		</comment>
		<comment id='4' author='feipxyz' date='2020-12-29T05:22:09Z'>
		I used the ONNX API to "load" the uploaded model and re-saved it back to an .onnx model and I could open it.
		</comment>
		<comment id='5' author='feipxyz' date='2020-12-29T12:41:50Z'>
		Thanks for the repro script. I made a small change to detect changes and throw:
&lt;denchmark-code&gt;    if (self._sum == 0):
        self._sum = sum
    if (self._sum != sum):
        print("Change detected")
        print("Old sum: " + str(self._sum))
        print("New sum: " + str(sum))
        self._sum = sum
        raise Exception('Let us stop here')
    else:
        print("No change detected. Sum: " + str(self._sum))
&lt;/denchmark-code&gt;

I was able to repro this with 1.5.2:
&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/103281532-d8b3b280-4987-11eb-82b1-4a21cab88f22.png&gt;&lt;/denchmark-link&gt;

However with the latest release 1.6.0, I could not repro this and I ran multiple 1000 iteration runs successfully:
&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/103283353-34cd0580-498d-11eb-8041-ea881d031b0e.png&gt;&lt;/denchmark-link&gt;

This is what I think happened with the 1.5.2 release binary:
Previously we had this situation wherein data copying and kernel compute were happening in different CUDA streams. This caused (under some special circumstances) a memory usage race when memory pattern is enabled. Please see this comment here - &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/4829#issuecomment-706402218&gt;#4829 (comment)&lt;/denchmark-link&gt;
. I tried disabling memory pattern using  and even with 1.5.2, I could no longer repro the diff.
So, I think the root cause was memory usage race because of BFCArena usage + different CUDA streams used for copy and compute. For example, let us assume there are three nodes in a graph (toy example):
&lt;denchmark-code&gt;                           Node 1
                          /        \
                         /          \
                  Node 2      Node 3
                      /               \
             graph output    graph output
&lt;/denchmark-code&gt;

Let us say, Node 3 is being dropped down to CPU (due to lack of CUDA kernel etc.) and Node 1 and Node 2 are being run on CUDA. It basically boils down to this:
&lt;denchmark-code&gt;                           Node 1
                          /        \
                         /          \
                  Node 2      MemcpyToHost (copy to CPU)
                      /                \ 
            graph output       \                                            
                                      Node 3
                                         \
                                          \
                                         graph output
&lt;/denchmark-code&gt;

If the static memory pattern planner decided that CUDA buffers for the second output of Node 1 (the contents of which is due to be copied over the host) can be given to Node 2's output, there is a race introduced. The copy needs to run first ahead of the Node 2's computation otherwise Node 3 will operate with incorrect data. This was not guaranteed previosuly  as the copy and compute happened on different CUDA streams. This was fixed by &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/5445&gt;#5445&lt;/denchmark-link&gt;
 and now by default copy and compute happens on the same stream and hence the racing condition is removed.
The fix is available starting 1.6.0. Closing this out now. Please re-open if you have more questions/comments.
		</comment>
	</comments>
</bug>