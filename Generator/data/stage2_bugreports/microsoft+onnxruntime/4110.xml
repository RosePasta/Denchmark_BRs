<bug id='4110' author='mzmssg' open_date='2020-06-02T09:29:29Z' closed_time='2020-06-13T22:09:38Z'>
	<summary>Graph optimization break correctness</summary>
	<description>
Describe the bug
Two bugs:

Inference session output differently with/without optimization
Even we specify an intermediate node in a pattern as graph output, graph optimization still fuse them, then sess fail to find the output.

Urgency
None
To Reproduce
code:
import onnxruntime as ort
import numpy as np
import onnx

def remove_expand_shape(model):
    expand_node = [n for n in model.graph.node if n.op_type == 'Expand']
    if len(expand_node) != 1:
        raise "cannot find the single expand node in the BERT model."

    for i, v_info in enumerate(model.graph.value_info):
        if v_info.name == expand_node[0].output[0]:
            break    
    del model.graph.value_info[i]

def add_value_to_output(model, intermediate_tensor_name):
    intermediate_layer_value_info = helper.ValueInfoProto()
    intermediate_layer_value_info.name = intermediate_tensor_name
    model.graph.output.extend([intermediate_layer_value_info])

model_name = "bert_for_pretraining_without_loss_optimized.onnx"
model = onnx.load(model_name)
#remove_expand_shape(model)

#n = find_input_node(model, "209")
#print(n)
#add_value_to_output(model, "209")

# We convert model to accept variable-length batch size, so it can be any positive integer.
batch = 3
# This should match --max_seq_length when calling nv_run_pretraining.py.
sq_length = 512
# This should match vocab_size in bert_config.json in DeepLearningExamples/PyTorch/LanguageModeling/BERT.
vocab_size = 30528

# Create a fake data point.
input_ids = np.random.randint(low=0, high=vocab_size, size=(batch, sq_length), dtype=np.int64)
segment_ids = np.random.randint(low=0, high=2, size=(batch, sq_length), dtype=np.int64)
input_mask = np.ones((batch, sq_length), dtype=np.int64)

opt_enable_sess = ort.InferenceSession(model.SerializeToString())
opt_enable_result = opt_enable_sess.run(None, {'input1': input_ids, 'input2': segment_ids, 'input3': input_mask})

so = ort.SessionOptions()
so.graph_optimization_level=ort.GraphOptimizationLevel.ORT_DISABLE_ALL
opt_disable_sess = ort.InferenceSession(model.SerializeToString(), so)
opt_disable_result = opt_disable_sess.run(None, {'input1': input_ids, 'input2': segment_ids, 'input3': input_mask})

print(opt_enable_result[1])
print(opt_disable_result[1])
model:
&lt;denchmark-link:https://github.com/mzmssg/onnx_models/blob/master/bert_for_pretraining_without_loss_optimized.onnx&gt;bert_for_pretraining_without_loss_optimized.onnx&lt;/denchmark-link&gt;

output:
&lt;denchmark-link:https://user-images.githubusercontent.com/11887940/83501514-cdcc9880-a4f2-11ea-8d21-b00aa1ea7a58.png&gt;&lt;/denchmark-link&gt;

Expected behavior
sess with/without optimization should have a same result.
Additional context

It have a corrent result if remove expand shape by uncomment #remove_expand_shape(model).
I checked the intermedia value by value, seems it's caused by GELU fusion, because this two session have a same input before gelu part but output differently, but it should be irrelevant to the Expand operation.
The second bug could be reproduced by uncomment

&lt;denchmark-code&gt;#n = find_input_node(model, "209")
#print(n)
#add_value_to_output(model, "209")
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mzmssg' date='2020-06-03T01:56:17Z'>
		&lt;denchmark-link:https://github.com/mzmssg&gt;@mzmssg&lt;/denchmark-link&gt;
 : What version of onnxruntime and execution provider are you ?
		</comment>
		<comment id='2' author='mzmssg' date='2020-06-03T03:44:04Z'>
		&lt;denchmark-link:https://github.com/askhade&gt;@askhade&lt;/denchmark-link&gt;

The latest master code with cuda.
Build command is 
		</comment>
		<comment id='3' author='mzmssg' date='2020-06-03T17:20:12Z'>
		It is a known bug that many fusions check GetOutputEdgesCount() != 1 to see whether a subgraph can be fused but graph output is not counted in output edges. It is in our back log.
		</comment>
		<comment id='4' author='mzmssg' date='2020-06-04T04:59:30Z'>
		&lt;denchmark-link:https://github.com/tianleiwu&gt;@tianleiwu&lt;/denchmark-link&gt;
 : Thanks for adding the PR to add the check for graph outputs
I am debugging the first issue mentioned here : "Inference session output differently with/without optimization"
This is because BiasGelu fusion has a bug... It is not adding correct bias input for this node. In the cpu implementation we have a check to verify the dimension of bias which fails and it errors out... In cuda implementation we do not have this check because of which the inference continues and the end result is wrong.
BiasGelu:
&lt;denchmark-link:https://user-images.githubusercontent.com/6475296/83716483-e4531c80-a5e4-11ea-8a36-e8ece88db368.png&gt;&lt;/denchmark-link&gt;

CPU Execution Provider error:
Name:'BiasGelu' Status Message: Input 1 is expected to have 1 dimensions, got 3
PR on the way
		</comment>
		<comment id='5' author='mzmssg' date='2020-06-13T22:09:37Z'>
		&lt;denchmark-link:https://github.com/askhade&gt;@askhade&lt;/denchmark-link&gt;
, thanks for providing the fix.
&lt;denchmark-link:https://github.com/mzmssg&gt;@mzmssg&lt;/denchmark-link&gt;
, the issues shall be resolved. You can try the latest nightly build.
Close it for now. Feel free to re-open it if it is not as you expected.
		</comment>
	</comments>
</bug>