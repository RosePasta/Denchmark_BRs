<bug id='3296' author='rashedkoutayni' open_date='2020-03-23T11:08:32Z' closed_time='2020-07-11T04:39:48Z'>
	<summary>Segmentation Fault when running on Raspberry Pi 3B+</summary>
	<description>
I am trying to run ONNX Runtime on RaspberryPi 3 B+, which has armv7l quad-core CPU.
In order to perform the cross-compilation for the Python wheel file for the ARM architecture, I followed the instructions mentioned here:
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/README.md#arm-32v7&gt;https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/README.md#arm-32v7&lt;/denchmark-link&gt;

and I got the wheel file (onnxruntime-1.2.0-cp35-cp35m-linux_armv7l.whl).
Furthermore, I could successfully install ONNX and ONNX Runtime on the mentioned Raspberry Pi.
I have an ONNX model that I exported from PyTorch 1.4, as well as some other models from the ONNX model zoo.
When I load the ONNX model and check it, everything seems to be fine:
# Load the model :
model_path = "myModel.onnx"
onnx_model = onnx.load(model_path)
# Check the model :
onnx.checker.check_model(onnx_model)
print('The model is checked!')
However, when I create an ONNX Runtime session, I get segmentation fault:
print("Creating session...")
session = onnxruntime.InferenceSession(model_path)
print("Session created.")
The output:
The model is checked!
Creating session...
Speicherzugriffsfehler
where:
Speicherzugriffsfehler = Memory access error
Which is Segmentation Fault on another RaspberryPi 3B.
 1.2.3
 1.2.0
 Linux raspberrypi 4.14.52-v7+ &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1123&gt;#1123&lt;/denchmark-link&gt;
 SMP Wed Jun 27 17:35:49 BST 2018 armv7l GNU/Linux
The same happens when I try to run the session for other models from the model zoo ..
However, It works fine on my Desktop x86_64 Ubuntu.
Any suggestions please ? Thanks !
	</description>
	<comments>
		<comment id='1' author='rashedkoutayni' date='2020-03-27T10:47:03Z'>
		Anything can be done in this case please ?
Thanks !
		</comment>
		<comment id='2' author='rashedkoutayni' date='2020-04-01T10:46:30Z'>
		By the way, when I run the python script with faulthandler:
python3.5 -Xfaulthandler run_onnx.py 
This is what I get:
The model is checked!
Creating session...
Fatal Python error: Segmentation fault
Current thread 0x76fa0010 (most recent call first):
File "/home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/session.py", line 43 in _load_model
File "/home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/session.py", line 25 in __init__
File "run_onnx.py", line 68 in &lt;module&gt;
Speicherzugriffsfehler
So looks like the fault is coming from these two lines in session.py:
25: self._load_model(providers)
and
43: self._sess.load_model(providers)
I see providers as the common factor between these two lines. I read in the comments that if providers is not specified by the user, all providers will be used.
What does that exactly mean? and how does it help solve the problem?
Thanks !
		</comment>
		<comment id='3' author='rashedkoutayni' date='2020-04-01T11:02:36Z'>
		Running on gdb:
&lt;denchmark-code&gt;pi@raspberrypi:~/rashed $ gdb python3.5
GNU gdb (Raspbian 7.12-6) 7.12.0.20161007-git
Copyright (C) 2016 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "arm-linux-gnueabihf".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;.
Find the GDB manual and other documentation resources online at:
&lt;http://www.gnu.org/software/gdb/documentation/&gt;.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from python3.5...(no debugging symbols found)...done.

(gdb) run run_onnx.py 
Starting program: /usr/bin/python3.5 run_onnx.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/arm-linux-gnueabihf/libthread_db.so.1".
[New Thread 0x754f6470 (LWP 25710)]
[New Thread 0x74cf6470 (LWP 25711)]
[New Thread 0x724f6470 (LWP 25712)]
[Thread 0x724f6470 (LWP 25712) exited]
[Thread 0x74cf6470 (LWP 25711) exited]
[Thread 0x754f6470 (LWP 25710) exited]
The model is checked!
Creating session...
[New Thread 0x724f6470 (LWP 25740)]
[New Thread 0x74cf6470 (LWP 25741)]

Thread 1 "python3.5" received signal SIGSEGV, Segmentation fault.
0x6f751f1c in std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;::basic_string(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) () from /usr/lib/arm-linux-gnueabihf/libstdc++.so.6

(gdb) backtrace
#0  0x6f751f1c in std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;::basic_string(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) () from /usr/lib/arm-linux-gnueabihf/libstdc++.so.6
#1  0x6ef352b4 in onnxruntime::FreeDimensionOverrideTransformer::FreeDimensionOverrideTransformer(gsl::span&lt;onnxruntime::FreeDimensionOverride const&gt;) () from /home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.so
#2  0x6ef10758 in onnxruntime::optimizer_utils::GenerateTransformers(onnxruntime::TransformerLevel, gsl::span&lt;onnxruntime::FreeDimensionOverride const&gt;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) ()
   from /home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.so
#3  0x6eeef368 in onnxruntime::InferenceSession::AddPredefinedTransformers(onnxruntime::GraphTransformerManager&amp;, onnxruntime::TransformerLevel, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) ()
   from /home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.so
#4  0x6eef52ac in onnxruntime::InferenceSession::Initialize() ()
   from /home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.so
#5  0x6eecbbd4 in onnxruntime::python::InitializeSession(onnxruntime::InferenceSession*, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) () from /home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.so
#6  0x6eee1394 in void pybind11::cpp_function::initialize&lt;onnxruntime::python::addObjectMethods(pybind11::module&amp;)::{lambda(onnxruntime::InferenceSession*, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt;&amp;)#7}, void, onnxruntime::InferenceSession*, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt;&amp;, pybind11::name, pybind11::is_method, pybind11::sibling, char [35]&gt;(onnxruntime::python::addObjectMethods(pybind11::module&amp;)::{lambda(onnxruntime::InferenceSession*, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt;&amp;)#7}&amp;&amp;, void (*)(onnxruntime::InferenceSession*, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt;&amp;), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;, char const (&amp;) [35])::{lambda(pybind11::detail::function_call&amp;)#3}::_FUN(pybind11::detail::function_call) ()
   from /home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.so
#7  0x6eeddbd8 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()
   from /home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_pybind11_state.so
#8  0x0016abd0 in PyCFunction_Call ()
#9  0x000be58c in PyEval_EvalFrameEx ()
#10 0x0019a0d0 in ?? ()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='rashedkoutayni' date='2020-04-01T11:12:30Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 probably you have an idea or you can suggest someone who can help? thanks.
		</comment>
		<comment id='5' author='rashedkoutayni' date='2020-04-01T16:32:43Z'>
		&lt;denchmark-link:https://github.com/HectorSVC&gt;@HectorSVC&lt;/denchmark-link&gt;
  Could you help?  I don't have a Raspberry Pi.
&lt;denchmark-link:https://github.com/rashedkoutayni&gt;@rashedkoutayni&lt;/denchmark-link&gt;
 , can you share the model to us?
		</comment>
		<comment id='6' author='rashedkoutayni' date='2020-04-03T07:09:09Z'>
		out of curiosity, what happens if you leave onnx out of the picture. don't import onnx , don't load the model and don't run onnx checker.
i.e. only have
import onnxruntime
session = onnxruntime.InferenceSession(model_path)
		</comment>
		<comment id='7' author='rashedkoutayni' date='2020-04-03T12:06:28Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;

Sure! please find it attached.
&lt;denchmark-link:https://github.com/jywu-msft&gt;@jywu-msft&lt;/denchmark-link&gt;

It is unfortunately exactly the same behavior,, didn't work either.
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/4427054/model.zip&gt;model.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='rashedkoutayni' date='2020-04-03T12:26:50Z'>
		
@snnn
Sure! please find it attached.
@jywu-msft
It is unfortunately exactly the same behavior,, didn't work either.
model.zip

Thanks for trying. Hope you can bear with us. Raspberry pi hasn't been one of the devices we actively test on. (but we hope to change that soon)
Currently we don't have easy access to a raspberry pi for debugging.
I believe the raspberry pi build was working in the past, so it's possible there is a bug/regression.
Can you help us experiment to isolate whether there's something with your environment or whether there's a bug/regression?
Re-build the whl with an older releases. Let's start with rel-1.0.0 and move backwards from there if you keep encountering the same issue.
ie change &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/Dockerfile.arm32v7#L4&gt;https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/Dockerfile.arm32v7#L4&lt;/denchmark-link&gt;

from master to rel-1.0.0 and build the whl.
		</comment>
		<comment id='9' author='rashedkoutayni' date='2020-04-03T13:26:37Z'>
		&lt;denchmark-link:https://github.com/jywu-msft&gt;@jywu-msft&lt;/denchmark-link&gt;

Sure I would be glad to do so.
But first I have a question please; how do I determine for which Python version it is building? I had a lot of hassle running ONNX Runtime (cp35) on RaspberryPi 4 because it has Python3.7 and I don't want to get into that again with my RaspberryPi 3B + (if possible :) )
Thanks !
		</comment>
		<comment id='10' author='rashedkoutayni' date='2020-04-05T12:09:00Z'>
		Unfortunately I am unable to build ONNX-Runtime 1.0.0 in docker due to the following error:
/code/onnxruntime/onnxruntime/core/util/qmath.cc:36:30: error: unused parameter ‘thread_pool’ [-Werror=unused-parameter]
I tried several times, looks like something should be modified in the git repo before cloning it..
		</comment>
		<comment id='11' author='rashedkoutayni' date='2020-04-05T12:31:26Z'>
		
Unfortunately I am unable to build ONNX-Runtime 1.0.0 in docker due to the following error:
/code/onnxruntime/onnxruntime/core/util/qmath.cc:36:30: error: unused parameter ‘thread_pool’ [-Werror=unused-parameter]
I tried several times, looks like something should be modified in the git repo before cloning it..

sorry that was fixed later on. it's due to an unused parameter.
there's ways to work around it,
but it may be easier if you tried rel-0.5.0 instead of rel-1.0.0
thanks
		</comment>
		<comment id='12' author='rashedkoutayni' date='2020-04-06T10:52:22Z'>
		It worked!
Platform: RaspberryPi 3B+
Python Version: 3.5.3
ONNX-Runtime Version: 0.5.0
ONNX-Runtime Wheel File: onnxruntime-0.5.0-cp35-cp35m-linux_armv7l.whl
However, I got this warning:
/home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_validation.py:22: UserWarning: Unsupported architecture (32bit). ONNX Runtime supports 64bit architecture, only. warnings.warn('Unsupported architecture (%s). ONNX Runtime supports 64bit architecture, only.' % __my_arch__)
I attached both wheel files in case someone would like to try them, as well as the Dockerfile used to build the wheel file of version 0.5.0
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/4437783/onnxruntime-0.5.0-cp35-cp35m-linux_armv7l.zip&gt;onnxruntime-0.5.0-cp35-cp35m-linux_armv7l.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/4437784/onnxruntime-1.2.0-cp35-cp35m-linux_armv7l.zip&gt;onnxruntime-1.2.0-cp35-cp35m-linux_armv7l.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/4437799/Dockerfile_0.5.0.zip&gt;Dockerfile_0.5.0.zip&lt;/denchmark-link&gt;

So &lt;denchmark-link:https://github.com/jywu-msft&gt;@jywu-msft&lt;/denchmark-link&gt;
 how would you like to troubleshoot further?
		</comment>
		<comment id='13' author='rashedkoutayni' date='2020-04-06T11:09:58Z'>
		
It worked!
Platform: RaspberryPi 3B+
Python Version: 3.5.3
ONNX-Runtime Version: 0.5.0
ONNX-Runtime Wheel File: onnxruntime-0.5.0-cp35-cp35m-linux_armv7l.whl
However, I got this warning:
/home/pi/.local/lib/python3.5/site-packages/onnxruntime/capi/onnxruntime_validation.py:22: UserWarning: Unsupported architecture (32bit). ONNX Runtime supports 64bit architecture, only. warnings.warn('Unsupported architecture (%s). ONNX Runtime supports 64bit architecture, only.' % __my_arch__)
I attached both wheel files in case someone would like to try them, as well as the Dockerfile used to build the wheel file of version 0.5.0
onnxruntime-0.5.0-cp35-cp35m-linux_armv7l.zip
onnxruntime-1.2.0-cp35-cp35m-linux_armv7l.zip
Dockerfile_0.5.0.zip
So @jywu-msft how would you like to troubleshoot further?

that warning is just a warning. it doesn't impact actual functionality. we weren't originally thinking of supporting python binding for 32bit architecture. but later removed that warning in a later commit.
so just to confirm. your model loads and runs using onnxruntime 0.5.0 on raspberry pi.
but it encounters a segfault on 1.2.0 ?
		</comment>
		<comment id='14' author='rashedkoutayni' date='2020-04-06T11:20:12Z'>
		next, can you try building rel-1.1.0 ?
thanks a lot for helping out.
		</comment>
		<comment id='15' author='rashedkoutayni' date='2020-04-06T12:21:26Z'>
		Sorry I closed this issue by mistake. How to undo that?
Correct. my model loads and runs using onnxruntime 0.5.0 on raspberry pi.
but it encounters a segfault on 1.2.0
I'll try with rel-1.1.0 and let you know.
Thanks !
		</comment>
		<comment id='16' author='rashedkoutayni' date='2020-04-06T15:02:55Z'>
		Building failed due to the following reason:
&lt;denchmark-code&gt;[ 39%] Building CXX object CMakeFiles/onnxruntime_providers.dir/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc.o
In file included from /code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc:4:0:
/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.h: In constructor ‘onnxruntime::CPUExecutionProvider::CPUExecutionProvider(const onnxruntime::CPUExecutionProviderInfo&amp;)’:
/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.h:28:65: error: unused parameter ‘info’ [-Werror=unused-parameter]
   explicit CPUExecutionProvider(const CPUExecutionProviderInfo&amp; info)
                                                                 ^~~~
cc1plus: all warnings being treated as errors
CMakeFiles/onnxruntime_providers.dir/build.make:153: recipe for target 'CMakeFiles/onnxruntime_providers.dir/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc.o' failed
make[2]: *** [CMakeFiles/onnxruntime_providers.dir/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc.o] Error 1
CMakeFiles/Makefile2:226: recipe for target 'CMakeFiles/onnxruntime_providers.dir/all' failed
make[1]: *** [CMakeFiles/onnxruntime_providers.dir/all] Error 2
Makefile:140: recipe for target 'all' failed
make: *** [all] Error 2
Traceback (most recent call last):
  File "/code/onnxruntime/tools/ci_build/build.py", line 1043, in &lt;module&gt;
    sys.exit(main())
  File "/code/onnxruntime/tools/ci_build/build.py", line 975, in main
    build_targets(cmake_path, build_dir, configs, args.parallel)
  File "/code/onnxruntime/tools/ci_build/build.py", line 415, in build_targets
    run_subprocess(cmd_args)
  File "/code/onnxruntime/tools/ci_build/build.py", line 197, in run_subprocess
    completed_process = subprocess.run(args, cwd=cwd, check=True, stdout=stdout, stderr=stderr, env=my_env, shell=shell)
  File "/usr/lib/python3.5/subprocess.py", line 398, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['/usr/local/bin/cmake', '--build', '/code/onnxruntime/build/Linux/MinSizeRel', '--config', 'MinSizeRel']' returned non-zero exit status 2
The command '/bin/sh -c ./build.sh ${BUILDARGS} --update --build' returned a non-zero code: 1

&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='rashedkoutayni' date='2020-04-06T21:48:57Z'>
		
Building failed due to the following reason:
[ 39%] Building CXX object CMakeFiles/onnxruntime_providers.dir/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc.o
In file included from /code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc:4:0:
/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.h: In constructor ‘onnxruntime::CPUExecutionProvider::CPUExecutionProvider(const onnxruntime::CPUExecutionProviderInfo&amp;)’:
/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.h:28:65: error: unused parameter ‘info’ [-Werror=unused-parameter]
   explicit CPUExecutionProvider(const CPUExecutionProviderInfo&amp; info)
                                                                 ^~~~
cc1plus: all warnings being treated as errors
CMakeFiles/onnxruntime_providers.dir/build.make:153: recipe for target 'CMakeFiles/onnxruntime_providers.dir/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc.o' failed
make[2]: *** [CMakeFiles/onnxruntime_providers.dir/code/onnxruntime/onnxruntime/core/providers/cpu/cpu_execution_provider.cc.o] Error 1
CMakeFiles/Makefile2:226: recipe for target 'CMakeFiles/onnxruntime_providers.dir/all' failed
make[1]: *** [CMakeFiles/onnxruntime_providers.dir/all] Error 2
Makefile:140: recipe for target 'all' failed
make: *** [all] Error 2
Traceback (most recent call last):
  File "/code/onnxruntime/tools/ci_build/build.py", line 1043, in &lt;module&gt;
    sys.exit(main())
  File "/code/onnxruntime/tools/ci_build/build.py", line 975, in main
    build_targets(cmake_path, build_dir, configs, args.parallel)
  File "/code/onnxruntime/tools/ci_build/build.py", line 415, in build_targets
    run_subprocess(cmd_args)
  File "/code/onnxruntime/tools/ci_build/build.py", line 197, in run_subprocess
    completed_process = subprocess.run(args, cwd=cwd, check=True, stdout=stdout, stderr=stderr, env=my_env, shell=shell)
  File "/usr/lib/python3.5/subprocess.py", line 398, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['/usr/local/bin/cmake', '--build', '/code/onnxruntime/build/Linux/MinSizeRel', '--config', 'MinSizeRel']' returned non-zero exit status 2
The command '/bin/sh -c ./build.sh ${BUILDARGS} --update --build' returned a non-zero code: 1


I made a workaround to disable build warnings as errors.
can you build using 1.1.0-test instead of rel-1.1.0 ?  thanks!
		</comment>
		<comment id='18' author='rashedkoutayni' date='2020-04-07T16:52:43Z'>
		Hi again,
I built the test release you pointed to (1.1.0-test) and it's working as well!
Attached is the wheel file for whoever needs it.
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/4445981/onnxruntime-1.1.0-cp35-cp35m-linux_armv7l.zip&gt;onnxruntime-1.1.0-cp35-cp35m-linux_armv7l.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='rashedkoutayni' date='2020-04-07T16:55:01Z'>
		By the way, I noticed some difference between the installation of ONNX-Runtime v.1.2.0 and the installation of earlier versions ... I'm not sure if this helps ...
For 1.1.0-test and earlier versions:
&lt;denchmark-code&gt;pi@raspberrypi:~/rashed $ pip3 install onnxruntime-1.1.0-cp35-cp35m-linux_armv7l.whl 
Processing ./onnxruntime-1.1.0-cp35-cp35m-linux_armv7l.whl
Installing collected packages: onnxruntime
Successfully installed onnxruntime-1.1.0
&lt;/denchmark-code&gt;

While for 1.2.0 :
&lt;denchmark-code&gt;pi@raspberrypi:~/rashed $ pip3 install onnxruntime-1.2.0-cp35-cp35m-linux_armv7l.whl 
Processing ./onnxruntime-1.2.0-cp35-cp35m-linux_armv7l.whl
Collecting onnx&gt;=1.2.3 (from onnxruntime==1.2.0)
Collecting numpy&gt;=1.18.0 (from onnxruntime==1.2.0)
  Using cached https://www.piwheels.org/simple/numpy/numpy-1.18.2-cp35-cp35m-linux_armv7l.whl
Collecting protobuf (from onnx&gt;=1.2.3-&gt;onnxruntime==1.2.0)
  Using cached https://files.pythonhosted.org/packages/27/9c/ef816295b4b40298fd0a17bf8f0ba6cf3e0c44cb2ce72257168e09996b8b/protobuf-3.11.3-py2.py3-none-any.whl
Collecting six (from onnx&gt;=1.2.3-&gt;onnxruntime==1.2.0)
  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl
Collecting typing-extensions&gt;=3.6.2.1 (from onnx&gt;=1.2.3-&gt;onnxruntime==1.2.0)
  Downloading https://files.pythonhosted.org/packages/0c/0e/3f026d0645d699e7320b59952146d56ad7c374e9cd72cd16e7c74e657a0f/typing_extensions-3.7.4.2-py3-none-any.whl
Collecting setuptools (from protobuf-&gt;onnx&gt;=1.2.3-&gt;onnxruntime==1.2.0)
  Downloading https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl (582kB)
    100% |████████████████████████████████| 583kB 122kB/s 
Installing collected packages: setuptools, six, protobuf, numpy, typing-extensions, onnx, onnxruntime
Successfully installed numpy-1.18.2 onnx-1.6.0 onnxruntime-1.2.0 protobuf-3.11.3 setuptools-46.1.3 six-1.14.0 typing-extensions-3.7.4.2
&lt;/denchmark-code&gt;

		</comment>
		<comment id='20' author='rashedkoutayni' date='2020-04-08T01:25:35Z'>
		Thanks for your help so far.
Can you test on 1.2.0 (which segfaults) a modification to your test script?
when creating the session, do this:
&lt;denchmark-code&gt;so = onnxruntime.SessionOptions()
so.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL
session = onnxruntime.InferenceSession(model_path, sess_options=so)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='rashedkoutayni' date='2020-04-08T03:54:02Z'>
		
By the way, I noticed some difference between the installation of ONNX-Runtime v.1.2.0 and the installation of earlier versions ... I'm not sure if this helps ...

It is because starting from 1.2, the package explicitly depends ONNX. So you have to install ONNX first.
		</comment>
		<comment id='22' author='rashedkoutayni' date='2020-04-08T08:29:12Z'>
		No problem.
&lt;denchmark-link:https://github.com/jywu-msft&gt;@jywu-msft&lt;/denchmark-link&gt;
 I tried to create the session as you mentioned and it works!
&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 I see, thanks.
		</comment>
		<comment id='23' author='rashedkoutayni' date='2020-04-08T08:39:29Z'>
		So far, I tried three versions: 0.5.0, 1.1.0-test and 1.2.0
I noticed that 0.5.0 is the fastest in inference, then 1.1.0-test, while 1.2.0 is slowest among them.
		</comment>
		<comment id='24' author='rashedkoutayni' date='2020-04-08T08:43:54Z'>
		
So far, I tried three versions: 0.5.0, 1.1.0-test and 1.2.0
I noticed that 0.5.0 is the fastest in inference, then 1.1.0-test, while 1.2.0 is slowest among them.

yes, that was just a workaround to try to narrow down the issue.
by disabling optimizations (ORT_DISABLE_ALL), it will result in the slowest performance.
by default the optimization level is
ORT_ENABLE_ALL
you can try the other 2 levels
ORT_ENABLE_BASIC
ORT_ENABLE_EXTENDED
		</comment>
		<comment id='25' author='rashedkoutayni' date='2020-04-08T08:48:12Z'>
		btw, what was the perf difference that you measured between 0.5.0 and 1.1.0 ?
		</comment>
		<comment id='26' author='rashedkoutayni' date='2020-04-08T08:50:45Z'>
		

So far, I tried three versions: 0.5.0, 1.1.0-test and 1.2.0
I noticed that 0.5.0 is the fastest in inference, then 1.1.0-test, while 1.2.0 is slowest among them.

yes, that was just a workaround to try to narrow down the issue.
by disabling optimizations (ORT_DISABLE_ALL), it will result in the slowest performance.
by default the optimization level is
ORT_ENABLE_ALL
you can try the other 2 levels
ORT_ENABLE_BASIC
ORT_ENABLE_EXTENDED

For ORT_ENABLE_ALL, ORT_ENABLE_BASIC and ORT_ENABLE_EXTENDED, it segfaults.
It works only for ORT_DISABLE_ALL.
		</comment>
		<comment id='27' author='rashedkoutayni' date='2020-04-08T09:04:54Z'>
		
btw, what was the perf difference that you measured between 0.5.0 and 1.1.0 ?

I am running inference on a testset that comprises 2440 images.
The average total runtime for the whole testset is as follows:
0.5.0 -&gt; ~39 sec
1.1.0 -&gt; ~46 sec
1.2.0 -&gt; ~58 sec
for about 5 trials each.
P.S. I'm running 0.5.0 and 1.1.0 with default session options.
		</comment>
		<comment id='28' author='rashedkoutayni' date='2020-07-03T02:59:08Z'>
		This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.
		</comment>
		<comment id='29' author='rashedkoutayni' date='2020-07-11T04:39:29Z'>
		This issue has been automatically closed due to inactivity. Please reactivate if further support is needed.
		</comment>
	</comments>
</bug>