<bug id='1511' author='oohlala1204' open_date='2019-07-26T11:42:32Z' closed_time='2019-08-09T02:53:35Z'>
	<summary>GENERAL ERROR : Non-zero status code returned while running Node: concat_35 Status Message:</summary>
	<description>
I convert a tensorflow model to an ONNX modelï¼Œbut when I run the ONNX model, an error occurs,
RuntimeError: Method run failed due to: [ONNXRuntimeError] : 1 : GENERAL ERROR : Non-zero status code returned while running Node: concat_35 Status Message: Not satsified: inputs_n_dims[axis_index] == inputs_0_dims[axis_index]
concat.cc:47 onnxruntime::ConcatBase::PrepareForComputeNon concat axis dimensions must match: Axis 0 has mismatched dimensions of 1 and 16
My code is as follows:
&lt;denchmark-code&gt;import onnxruntime as ort


testX, testY=load_data("test")
sess = ort.InferenceSession("TextRCNN.onnx")
for input_meta in sess.get_inputs():
    print(input_meta)
for output_meta in sess.get_outputs():
    print(output_meta)

for input in testX:
    feed_data={"input_x:0":np.array([input], dtype=np.int32)}
    result = sess.run(["predictions:0"], feed_data)
    print(result)
&lt;/denchmark-code&gt;

I also print the input and output nodes:
&lt;denchmark-code&gt;NodeArg(name='input_x:0', type='tensor(int32)', shape=[None, 36])
NodeArg(name='predictions:0', type='tensor(int64)', shape=[16])
&lt;/denchmark-code&gt;

Here, "input_x:0" is the name of the input node, and "predictions:0" is the name of the output ndoe.
Any help would be appreciated!
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
ONNX Runtime version:0.4.0
CPU

	</description>
	<comments>
		<comment id='1' author='oohlala1204' date='2019-07-26T15:49:55Z'>
		&lt;denchmark-link:https://github.com/oohlala1204&gt;@oohlala1204&lt;/denchmark-link&gt;
 , what version of ORT are you using? And CPU or GPU? Could you share the model for debug?
		</comment>
		<comment id='2' author='oohlala1204' date='2019-07-29T11:44:58Z'>
		
@oohlala1204 , what version of ORT are you using? And CPU or GPU? Could you share the model for debug?

Thank you very much. The version of ORT is 0.4.0, and CPU is used. The converted onnx model can be downloaded from &lt;denchmark-link:url&gt;https://drive.google.com/file/d/1FO4hbgcvNnLbjkvdXHOdxMDdlHK1e9U8/view?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='oohlala1204' date='2019-08-05T23:01:47Z'>
		Hi &lt;denchmark-link:https://github.com/oohlala1204&gt;@oohlala1204&lt;/denchmark-link&gt;
,
There are several clarifications to be made:

From your Python snippet pasted above, it is not clear what the input data's shape is (I think it is [1, 16] from the error message). However, it's not clear from the model what the accepted input shape is as the first dimension seems to be a symbolic dimension value (obtained from manual model inspection using Netron):

id: input_x:0
type: int32[unk__48,36]

When I passed in an input of shape [16, 36], the model ran successfully:

import onnxruntime as ort
import numpy as np
sess = ort.InferenceSession("TextRCNN.onnx")
feed_data={"input_x:0":np.random.randint(1, 100, size=(16, 36), dtype=np.int32)}
result = sess.run(["predictions:0"], feed_data)
print(result)

I am not sure if the predictions are correct (it always gives the same values in the predicted array), but the model works fine.

So, there can be some conclusions based on this:


If the first dimension ("unk__48") is actually batch dimension, the model seems to have exported incorrectly because it doesn't seem to accept any value except 16 in the batch dimension (Did you train the original tf model with batch size = 16 ?). If you think this is what is happening, please open a bug in the tensorflow onnx converter code base and give them pertinent details. The ONNX model needs to be corrected to truly support batching.


If the first dimension has to be 16 and no other value is acceptable, then the model should have input shape defined as [16, 36] (and not ["unk_48", 36]). So this requires another issue in the converter code base. Even after feeding in random values of shape [16, 36], I keep getting the same values as outputs. So probably the model itself got incorrectly exported.


So an incorrect model conversion is probably the root cause for this issue and not ORT itself. Hope this helps.
Thanks
		</comment>
		<comment id='4' author='oohlala1204' date='2019-08-05T23:18:31Z'>
		Please close this issue if this really a model issue and open the bug in the appropriate code base. Keeping it open for your confirmation. :)
		</comment>
		<comment id='5' author='oohlala1204' date='2019-08-08T23:17:15Z'>
		Hi &lt;denchmark-link:https://github.com/oohlala1204&gt;@oohlala1204&lt;/denchmark-link&gt;
,
Did you get a chance to take a look at my response ?
Thanks
		</comment>
		<comment id='6' author='oohlala1204' date='2019-08-09T02:53:35Z'>
		
Hi @oohlala1204,
Did you get a chance to take a look at my response ?
Thanks

Thank you so much for the detailed explanation. I'm very grateful for your help.
According to the discussion and my experiment, I think I made a mistake when saving the model. I am trying to check and fix it.
		</comment>
	</comments>
</bug>