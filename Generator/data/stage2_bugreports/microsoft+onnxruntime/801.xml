<bug id='801' author='hariharans29' open_date='2019-04-09T20:16:44Z' closed_time='2019-05-15T18:10:32Z'>
	<summary>Bug: Pooling ops don't seem to handle explicit pads correctly</summary>
	<description>
Describe the bug
Pooling operators (MaxPool and AveragePool) do not seem to handle specified padding values correctly.
I tried an 'AveragePool' op for simple 3D data - [[[1, 2, 3, 4, 5]]] with kernel_shape = [2], pads = [2, 3], strides = [2], count_include_pad = 1 and expected the output to be [[[0, 1.5, 3.5, 2.5, 0]]], instead got this error -
RuntimeError: [ONNXRuntimeError] : 1 : GENERAL ERROR : Exception during initialization: D:\3\s\onnxruntime\core/providers/cpu/nn/pool_base.h:139 onnxruntime::PoolBase::PoolBase pads_[dim] &lt; kernel_shape_[dim] &amp;&amp; pads_[dim + kernel_shape_.size()] &lt; kernel_shape_[dim] was false. Pad should be smaller than kernel.
AFAIK, there isn't anything in the spec that prevents pad to be greater than kernel, in fact when strides &gt; 1, pad might have to be greater than kernel in cases where we want to produce a result with the same dimension value (like the pads generated by deprecated AutoPad attribute)
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
ONNX Runtime installed from (source or binary): Binary
ONNX Runtime version: 0.3.0
Python version: 3.6.8
Visual Studio version (if applicable): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

To Reproduce
import onnxruntime as rt
import numpy as np
import onnx
import onnxruntime.backend as backend
def _extract_value_info(arr, name):  # type: (np.ndarray, Text) -&gt; onnx.ValueInfoProto
return onnx.helper.make_tensor_value_info(
name=name,
elem_type=onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype],
shape=arr.shape)
def expect(node,
inputs,
outputs,
name,
**kwargs
):
present_inputs = [x for x in node.input if (x != '')]
present_outputs = [x for x in node.output if (x != '')]
inputs_vi = [_extract_value_info(arr, arr_name)
for arr, arr_name in zip(inputs, present_inputs)]
outputs_vi = [_extract_value_info(arr, arr_name)
for arr, arr_name in zip(outputs, present_outputs)]
graph = onnx.helper.make_graph(
nodes=[node],
name=name,
inputs=inputs_vi,
outputs=outputs_vi)
kwargs[str('producer_name')] = 'pool-test'
model_def = onnx.helper.make_model(graph, **kwargs)
onnx.checker.check_model(model_def)
pm = backend.prepare(model_def)
outs = list(pm.run(inputs))
for ref_o, o in zip(outputs, outs):
np.testing.assert_almost_equal(ref_o, o)
def pool_with_strides():
&lt;denchmark-code&gt;    x = np.array([[[1, 2, 3, 4, 5]]]).astype(np.float32)

    # Pool with strides=2 and padding 
    node_with_asymmetric_padding = onnx.helper.make_node(
        'AveragePool',
        inputs=['x'],
        outputs=['y'],
        kernel_shape=[2],
        count_include_pad = 1,
        pads=[2, 3],
        #auto_pad="SAME_UPPER",
        strides=[2]
    )
    y_with_asymmetric_padding = np.array([[[0, 1.5, 3.5, 2.5, 0]]]).astype(np.float32)
    expect(node_with_asymmetric_padding, inputs=[x], outputs=[y_with_asymmetric_padding],
           name='test_pool_with_strides_and_asymmetric_padding')        
&lt;/denchmark-code&gt;

if name == "main":
pool_with_strides()
Expected behavior
Expect explicit pads provided to be honored and produce results with the right shape accordingly
Screenshots
N/A
Additional context
N/A
	</description>
	<comments>
		<comment id='1' author='hariharans29' date='2019-05-15T18:10:32Z'>
		Closing this as spec is unclear as to how to handle cases when pad size &gt;= kernel size. When pad size &gt;= kernel size, atleast one value in the output tensor will be generated from a "pure padding region". Raised an issue in ONNX to clarify the correct functionality (or if it is even allowed in the first place) - &lt;denchmark-link:https://github.com/onnx/onnx/issues/1922&gt;onnx/onnx#1922&lt;/denchmark-link&gt;
. Since there is tracked separately in the place it should be, will close this issue as it is not actionable for the time being.
		</comment>
	</comments>
</bug>