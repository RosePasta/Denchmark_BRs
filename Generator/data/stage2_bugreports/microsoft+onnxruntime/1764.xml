<bug id='1764' author='andompesta' open_date='2019-09-06T04:07:05Z' closed_time='2019-09-09T20:48:40Z'>
	<summary>RuntimeError: [ONNXRuntimeError] : 1 : GENERAL ERROR : Load model from encoder.onnx failed:Type Error: Type parameter (T) bound to different types (tensor(double) and tensor(float) in node ().</summary>
	<description>
Describe the bug
I have a transformer model and I'm trying to export only the encoder of such model. I successfully generate the onnx file, but when I try to load it into the runtime environment I got the over-mentioned error
Urgency
None. At this stage I might just go with a pytorch plain demo
System information

OS Platform and Distribution : Currently working on a MacOS, but planing to deploy on Ubuntu
ONNX Runtime installed from (source or binary): pip
ONNX Runtime version: 0.5.0
Python version: 3.7.3
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source): pple LLVM version 10.0.1
CUDA/cuDNN version: None
GPU model and memory: None

To Reproduce
&lt;denchmark-code&gt;import onnx
import onnxruntime

onnx_model = onnx.load("encoder.onnx")
onnx.checker.check_model(onnx_model)
ort_session = onnxruntime.InferenceSession("encoder.onnx")
&lt;/denchmark-code&gt;

Expected behavior
I expect the model to be loaded correctly and being able to perform inference

graph from torch export: &lt;denchmark-link:https://drive.google.com/file/d/1AAVjFOR6YTuvlPKTfIe4IWW0LjMmEMc2/view?usp=sharing&gt;https://drive.google.com/file/d/1AAVjFOR6YTuvlPKTfIe4IWW0LjMmEMc2/view?usp=sharing&lt;/denchmark-link&gt;

onnx file:
&lt;denchmark-link:https://drive.google.com/file/d/1mf7vj97aK_7mlAHPaWM2KROP7z4I7Hy7/view?usp=sharing&gt;https://drive.google.com/file/d/1mf7vj97aK_7mlAHPaWM2KROP7z4I7Hy7/view?usp=sharing&lt;/denchmark-link&gt;

The onnx file is generated passing some random input to the model.
&lt;denchmark-code&gt;src_seq = torch.LongTensor([Constants.BOS, 11, Constants.SP, 12, Constants.SP, 13, Constants.EOW]).unsqueeze(0).to(device)
src_pos = torch.arange(1, src_seq.size(1) + 1).unsqueeze(0).to(device)
src_enc, src_att = model.encoders(src_seq, src_pos)

torch.onnx.export(model.encoders,
                      (src_seq, src_pos),
                      "encoder.onnx",
                      verbose=True,
                      export_params=True,  # store the trained parameter weights inside the model file
                      opset_version=10,  # the ONNX version to export the model to
                      do_constant_folding=True,  # wether to execute constant folding for optimization
                      example_outputs=(src_enc, src_att),
                      input_names=['src_seq', 'src_pos'],  # the model's input names
                      output_names=['src_enc', 'src_att'],  # the model's output names
                      dynamic_axes={'src_seq': {0: 'batch_size', 1:'src_len'},
                                    'src_pos': {0: 'batch_size', 1: 'src_len'},
                                    'src_enc': {0: 'batch_size', 1: 'src_len'},
                                    'src_att': {0: 'batch_size', 1: 'src_len', 2: 'src_len'}
                                    }
                      )
&lt;/denchmark-code&gt;

The entire model code is provided here: &lt;denchmark-link:https://drive.google.com/file/d/1sEorZoQPNL-qRdH8lsbbdAeSnrJae6Zz/view?usp=sharing&gt;https://drive.google.com/file/d/1sEorZoQPNL-qRdH8lsbbdAeSnrJae6Zz/view?usp=sharing&lt;/denchmark-link&gt;

Note sure if it is related but if I use
&lt;denchmark-code&gt;torch.jit.save(model, "model.native")
torch.jit.load("model.native")
&lt;/denchmark-code&gt;

it works perfectly
	</description>
	<comments>
		<comment id='1' author='andompesta' date='2019-09-07T03:31:54Z'>
		have you solve the problem?
		</comment>
		<comment id='2' author='andompesta' date='2019-09-09T03:27:52Z'>
		Nope, honestly I have no idea where I should start from :(
Any advice ?
		</comment>
		<comment id='3' author='andompesta' date='2019-09-09T18:54:07Z'>
		Hi &lt;denchmark-link:https://github.com/andompesta&gt;@andompesta&lt;/denchmark-link&gt;
,
Most likely this is an invalid ONNX model and this will be a bug to report to the Torch project. The root cause is probably there is a node that has inputs bound to different types (float and double) when it is expected to be homogeneous (either both float or both double). This is possible only due to model export/conversion. Since the node names seem to be missing, I ll try and find which node is the problematic node. You can use the info to report the bug. I ll update the thread when done.
		</comment>
		<comment id='4' author='andompesta' date='2019-09-09T20:48:40Z'>
		Hi &lt;denchmark-link:https://github.com/andompesta&gt;@andompesta&lt;/denchmark-link&gt;
,
I investigated the model. As suspected, it looks like an invalid ONNX model. I annotated the model
with random names for the node to localize the issue (attached the new model). The issue seems to be in this 'Where' node -
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3592567/Encoder_with_node_names.zip&gt;Encoder_with_node_names.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/64565439-9dabec80-d308-11e9-92c5-4e5d23378d05.png&gt;&lt;/denchmark-link&gt;

'X' for the 'Where' node is of type 'float64' but the input 'Y' (coming from the output of the upstream 'Div' node) is of type 'float32'. This is invalid per op spec - &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/Operators.md#Where&gt;https://github.com/onnx/onnx/blob/master/docs/Operators.md#Where&lt;/denchmark-link&gt;
. The inputs 'X' and 'Y' are both bound to type parameter 'T' , meaning that they SHOULD be homogeneous. Please open an issue with the project that helped you get the ONNX model with this information.
FWIW, the ONNX model checker (from the ONNX project) should ideally fail when validating this model. So, I opened an issue there - &lt;denchmark-link:https://github.com/onnx/onnx/issues/2292&gt;onnx/onnx#2292&lt;/denchmark-link&gt;
. Details on how to check an ONNX model may be found here - &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md&gt;https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md&lt;/denchmark-link&gt;
.
Since this is not an ORT issue, closing this issue. Please re-open if you need more clarifications.
Thanks!
		</comment>
		<comment id='5' author='andompesta' date='2019-09-20T08:57:52Z'>
		Hi &lt;denchmark-link:https://github.com/andompesta&gt;@andompesta&lt;/denchmark-link&gt;
, I hit the same problem, you said that
"
you Note sure if it is related but if I use
torch.jit.save(model, "model.native")
torch.jit.load("model.native")
it works perfectly
"
you mean that I can also use JIT to soulve the problem?
Thank you.
		</comment>
	</comments>
</bug>