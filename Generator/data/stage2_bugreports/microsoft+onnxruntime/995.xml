<bug id='995' author='acdifran' open_date='2019-05-08T23:09:56Z' closed_time='2019-05-11T15:57:27Z'>
	<summary>Bus Error on sess.run</summary>
	<description>
Describe the bug
I recently installed onnxruntime on a Jetson Nano device and am getting a bus error when running the hello world type example. The onnx parser in the current version of TensorRT on the Jetson Nano device does not support certain operations I would like to use in my onnx models, so I attempted to install onnxruntime directly with TensorRT and GPU support. The build and install seem to have succeeded with no errors present, but I am unable to run the example code successfully.
Running the "hello world" example crashes the python shell and gives the following error:
Bus error (core dumped)
Running the same code through gdb gives:
Thread 1 "python3" received signal SIGBUS, Bus error.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4 Tegra (Ubuntu) 18.04
ONNX Runtime installed from (source or binary): built from source
ONNX Runtime version: 0.4.0
Python version: 3.6.7
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source): 7.4.0
CUDA/cuDNN version: CUDA: 10.0 cuDNN: 7.3.1
GPU model and memory: 128-core Maxwell (Jetson Nano) / 4GB shared with CPU
TensorRT version is 5.0.6.3

To Reproduce

Build onnxruntime from source with the following command:

./build.sh --config MinSizeRel --arm --cudnn_home /usr/lib/aarch64-linux-gnu --cuda_home /usr/local/cuda --use_tensorrt --tensorrt_home /usr/src/tensorrt --build_shared_lib --enable_pybind --build_wheel  --numpy_version '1.16.3'
I specified the numpy version to be the same as the one I already have installed since I saw some past issues that seemed similar to mine when 2 different numpy versions were getting used.

Install the built whl file:

pip3 install my_output_file.whl

Run the "hello world" example:

&lt;denchmark-code&gt;import numpy
import onnxruntime as rt
sess = rt.InferenceSession("model.onnx")
input_name = sess.get_inputs()[0].name
X = numpy.random.random((3, 4, 5)).astype(numpy.float32)
pred_onnx = sess.run(None, {input_name: X})
print(pred_onnx)
&lt;/denchmark-code&gt;

This results in the errors listed above.
Expected behavior
Receive an output array for pred_onnx with no errors.
Additional Info
Breaking up the steps in the python shell, I can see that it throws the error at this line:
pred_onnx = sess.run(None, {input_name: X})
	</description>
	<comments>
		<comment id='1' author='acdifran' date='2019-05-09T00:42:10Z'>
		First, modify CMakeLists.txt to add jetson support
&lt;denchmark-code&gt;	-  set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -gencode=arch=compute_50,code=sm_50") # M series
	+  set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -gencode=arch=compute_53,code=sm_53") # Jetson support
&lt;/denchmark-code&gt;

Try the below build commandline instead (you can also add your numpy_version option if necessary):
./build.sh --config Release --update --build --build_wheel --use_tensorrt --cuda_home /usr/local/cuda --cudnn_home /usr/lib/aarch64-linux-gnu --tensorrt_home /usr/lib/aarch64-linux-gnu
		</comment>
		<comment id='2' author='acdifran' date='2019-05-09T13:15:08Z'>
		Hi &lt;denchmark-link:https://github.com/jywu-msft&gt;@jywu-msft&lt;/denchmark-link&gt;
 thank your for the suggestion on how to attempt to fix this. I performed the steps you listed above and reinstalled onnxruntime, but python is still crashing when attempting to run the example. Although, now it seems to have changed from a bus error to a segmentation fault.
		</comment>
		<comment id='3' author='acdifran' date='2019-05-09T19:48:56Z'>
		I attempted this on a different machine just to see what would happen and it is failing there as well.
Machine stats: AWS p3.2xlarge w/ Tesla V100
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 (based on deeplearning AMI)
ONNX Runtime installed from (source or binary): built from source
ONNX Runtime version: 0.4.0
Python version: 3.6.8
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source): 5.4.0
CUDA/cuDNN version: CUDA: 10.1 cuDNN: 7.5.0
GPU model and memory: Tesla V100
TensorRT version: 5.1.2.2
Build command was slightly different:
./build.sh --update --build --cudnn_home /usr/local/cuda/lib64 --cuda_home /usr/local/cuda --use_tensorrt --tensorrt_home /home/ubuntu/TensorRT-5.1.2.2 --build_wheel --config Release --numpy_version '1.16.1'
After installing the wheel file I ran the same example as before and still got the seg fault.
Also, for some more info:
I am using the model found here when running the example code: &lt;denchmark-link:https://github.com/onnx/models/tree/master/resnet50&gt;https://github.com/onnx/models/tree/master/resnet50&lt;/denchmark-link&gt;

Not sure if relevant/helpful, but when reading in the model, I get this output:
2019-05-09 19:45:21.930651033 [W:onnxruntime:Default, graph.cc:2208 CleanUnusedInitializers] gpu_0/imagenet1k_blobs_queue_f22e83c9-22cd-4a8b-a66d-113af6b832b4_0 exists in this graph's initializers but it is not used by any node
2019-05-09 19:45:42.538466587 [W:onnxruntime:InferenceSession, session_state_initializer.cc:508 SaveInputOutputNamesToNodeMapping] Graph input with name gpu_0/imagenet1k_blobs_queue_f22e83c9-22cd-4a8b-a66d-113af6b832b4_0 is not associated with a node.
		</comment>
		<comment id='4' author='acdifran' date='2019-05-09T21:45:18Z'>
		if i'm reading it correctly, your 'hello world' model is resnet50.
that model expects input shape of [1,3,224,224] but you're using [3,4,5] ?
(thought we should be doing input shape validation but maybe we aren't for python bindings.
default cpu returns error to some other validation failure and I think tensorrt seems to not do any validation and we encounter an invalid memory access.)
I tried this on jetson nano and it worked fine. can you try the same?
trttest@jetson1:~/resnet50$ python3
Python 3.6.7 (default, Oct 22 2018, 11:32:17)
[GCC 8.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import onnxruntime as ort
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; sess = ort.InferenceSession('model.onnx')
2019-05-09 12:51:58.219078817 [W:onnxruntime:Default, graph.cc:2208 CleanUnusedInitializers] gpu_0/imagenet1k_blobs_queue_f22e83c9-22cd-4a8b-a66d-113af6b832b4_0 exists in this graph's initializers but it is not used by any node
2019-05-09 12:54:58.559817450 [W:onnxruntime:InferenceSession, session_state_initializer.cc:508 SaveInputOutputNamesToNodeMapping] Graph input with name gpu_0/imagenet1k_blobs_queue_f22e83c9-22cd-4a8b-a66d-113af6b832b4_0 is not associated with a node.
&gt;&gt;&gt; sess.get_inputs()[0].name
'gpu_0/data_0'
&gt;&gt;&gt; sess.get_inputs()[0].shape
[1, 3, 224, 224]
&gt;&gt;&gt; x = np.random.rand(1,3,224,224).astype('f')
&gt;&gt;&gt; res = sess.run([], {'gpu_0/data_0':x})
&gt;&gt;&gt; res[0].size
1000
&gt;&gt;&gt; res[0].shape
(1, 1000)
&gt;&gt;&gt; res[0].dtype
dtype('float32')
		</comment>
		<comment id='5' author='acdifran' date='2019-05-09T22:03:51Z'>
		&lt;denchmark-link:https://github.com/jywu-msft&gt;@jywu-msft&lt;/denchmark-link&gt;
 oh wow, you're absolutely right! I never changed the shape to match the model I was using and the low level faults were really throwing me off making me think it was something with the install itself. Thanks for the help!
		</comment>
		<comment id='6' author='acdifran' date='2019-05-11T15:57:26Z'>
		closing this as it is resolved for the user.
will open another issue for avoiding low level failures in TensorRT provider.
		</comment>
		<comment id='7' author='acdifran' date='2019-07-11T17:43:32Z'>
		I had to add (not replace) the following to CMakeLists.txt at line &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/bfda9ca1c1e396411850e335f6c22ab1ffb90fe7/cmake/CMakeLists.txt#L563&gt;563&lt;/denchmark-link&gt;
 to make sure cuda compiler is found by the build system:
set(CMAKE_CUDA_COMPILER /usr/local/cuda/bin/nvcc)
I am compiling onnxruntime natively on Jetson TX2.
		</comment>
	</comments>
</bug>