<bug id='4014' author='freedenS' open_date='2020-05-21T10:13:03Z' closed_time='2020-06-29T10:10:02Z'>
	<summary>The output is different with the same input on CUDA</summary>
	<description>
Describe the bug
I trained a MaskRcnn model and convert it to onnx. When i use test images to do inference,it can give the correct output. But if i do the inference with a white image (all 255) first, and then use test images to do inference, it can't recognize correctly with onnxruntime-gpu1.3.0.
if i use onnxruntime1.3.0, it will not happen.
System information

OS Platform and Distribution :Windows 10 (c++) &amp; Linux Ubuntu 16.04(python3.7)
ONNX Runtime installed from :binary from pip install
ONNX Runtime version:1.3.0
Python version:3.7
Visual Studio version (if applicable):2017
GCC/Compiler version (if compiling from source):compiling from binary
CUDA/cuDNN version:CUDA10.1 &amp; cuDNN7.6.5
GPU model and memory:2080Ti 11G

To Reproduce

Describe steps/code to reproduce the behavior.
Attach the ONNX model to the issue (where applicable) to expedite investigation.
onnx model and images are here https://drive.google.com/open?id=1c7GDq8AYgneP6mdqFPSH1GKl7Oggn8Mq
do the inference with a white image (all 255) first, and then use test images to do inference

&lt;denchmark-code&gt;import os
import onnxruntime
import cv2
import torch #1.5.0
from detectron2.data.transforms as T #just for resize
filelist = os.listdir('./')
sess = onnxruntime.InferenceSession('./segmentation.onnx')
for i in filelist:
    inputs = cv2.imread('./' + i)
    transform_gen = T.ResizeShortestEdge([800,800],1333)
    inputs = transform_gen.get_transform(inputs).apply_image(inputs)
    inputs = torch.as_tensor(inputs.astype('float32').transpose(2,0,1))
    inputs = inputs.unsqueeze(0)
    res = sess.run(None, {sess.get_inputs()[0].name:inputs.numpy()})
    print (res)
&lt;/denchmark-code&gt;


If applicable, add screenshots to help explain your problem.
&lt;denchmark-link:https://user-images.githubusercontent.com/26213470/82542042-eb5d4200-9b83-11ea-8272-174bdc6d6972.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='freedenS' date='2020-05-23T08:32:59Z'>
		Is 1.jpg the fully white image and 2.jpg a normal test image ?
		</comment>
		<comment id='2' author='freedenS' date='2020-05-25T08:47:15Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 Yes
		</comment>
		<comment id='3' author='freedenS' date='2020-05-27T01:24:53Z'>
		Thanks. I ll take a look.
		</comment>
		<comment id='4' author='freedenS' date='2020-05-27T02:53:11Z'>
		Thank you!
		</comment>
		<comment id='5' author='freedenS' date='2020-06-08T08:43:29Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 anything new?
		</comment>
		<comment id='6' author='freedenS' date='2020-06-11T04:58:42Z'>
		Sorry - I haven't had time to look at this yet. I ll try and take a look soon.
My guess is that when you feed in all 255's, CUDA/ CuDNN /CuBLAS doesn't like it (maybe triggers a numerical overflow in some operation ?) and the CUDA EP goes into a bad state after this and hence you see issues with subsequent runs.
Also, usually there is a "pre-processing step" for image models which involves subtracting per channel mean and dividing by the per channel std deviation. Not sure, if the all 255 image causes an issue because of this (because clearly it is not "pre-processed").
Does this all 255 image issue block your use-case in any way or is it just that you are curious about it ?
		</comment>
		<comment id='7' author='freedenS' date='2020-06-11T06:58:04Z'>
		Thanks for your reply!
i move the preprocess into inference(include subtract mean and divide std deviation), so the preprocess of my model is only image resize.(will it cause the problem?)
This problem will happen only when the model is mask-rcnn, the Conv_3281 is in mask_head layer.
I test some faster-rcnn models, and haven't found this problem.
And i find that some normal images (such as other model's test image) could also cause this problem.
		</comment>
		<comment id='8' author='freedenS' date='2020-06-12T19:38:33Z'>
		Oh so the preprocessing is built into the model ? I see. Okay- I ll take a look at this very soon.
		</comment>
		<comment id='9' author='freedenS' date='2020-06-13T04:14:11Z'>
		Yes, the preprocessing(subtracting per channel mean, dividing by the per channel std deviation and padding for fpn) is built into the model.
In addition, I have made some other attempts.Hope it will be useful.

if i move the preprocessing out, and add image_size as second input which is used by clip.
result: I can only get segmentation fault (core dumped)
if i move the preprocessing out, and use current input's size for clip (after resize and padding, it may be out of image's boundary)
result: I got the correct result!
No matter which way I chooseï¼Œ i can get the correct result as long as i use onnxruntime1.3(cpu)
Thank you!

		</comment>
		<comment id='10' author='freedenS' date='2020-06-16T22:06:05Z'>
		Taking a look at the model as is (with preprocessing inside it)
		</comment>
		<comment id='11' author='freedenS' date='2020-06-18T20:46:47Z'>
		Hi &lt;denchmark-link:https://github.com/freedenS&gt;@freedenS&lt;/denchmark-link&gt;
  - What is the tensor dimension for height and width ? Is there any aspect ration to be maintained between the two ?
		</comment>
		<comment id='12' author='freedenS' date='2020-06-18T21:21:58Z'>
		Hi &lt;denchmark-link:https://github.com/freedenS&gt;@freedenS&lt;/denchmark-link&gt;
 ,
I couldn't repro this with the height == width == 224. See below - I run all 255s and random valued inputs alternately. I tried with 1.3.0 onnxruntime GPU package (you can uncomment the appropriate lines as you run on your box) . I set log_severity_level to 0 and confirmed that a lot (almost all nodes) got placed on the CUDA EP.
I think I might know why you are running into this issue. Are you using a fixed height and width combination between runs ? My guess is that you are varying height and width. The model is annotated with "dynamic" dim values for height and width - this is a little misleading - while ORT as an engine will accept any dim value, the model might inherently expect that there be some relationship (aspect ratio) between the height and width and the model will work on the assumption that the user has adhered to this requirement and when this is not met the model is bound to fail. You need to check what shapes the model was trained with and check if you need to adhere to any aspect ratio requirements.
&lt;denchmark-code&gt;
import onnxruntime as rt
import numpy as np

opt = rt.SessionOptions()

# Uncomment to see which nodes get placed on CUDA EP
#opt.log_severity_level = 0

sess = rt.InferenceSession(r'segmentation.onnx', opt)

input_name_0 = sess.get_inputs()[0].name

# run 1 - all 255
input_0 = np.zeros((1, 3, 224, 224), dtype = np.float32)
input_0 += 255
#print(input_0)
pred_onnx_0 = sess.run(None, {input_name_0: input_0})
print(pred_onnx_0[0])

# run 2 - random
input_1 = np.random.rand(1, 3, 224, 224).astype(np.float32)
#print(input_1)
pred_onnx_1 = sess.run(None, {input_name_0: input_1})
print(pred_onnx_1[0])

# run 3 - all 255
pred_onnx_2 = sess.run(None, {input_name_0: input_0})
print(pred_onnx_2[0])

# run 2 - random
pred_onnx_3 = sess.run(None, {input_name_0: input_1})
print(pred_onnx_3[0])

print ("Done")
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='freedenS' date='2020-06-18T21:38:44Z'>
		Oh I just saw your input script and noticed that you were feeding [800, 800] inputs and I could hit the issue with that. My guess is that there is something in the input combination (input shape + ConvTranspose attributes that CuDNN does not like). This doesn't look like it is an ORT issue in that it doesn't break runs after you feed it all 255s.
I ll debug what shapes the inputs have by the time it reaches the faulty ConvTranspose node in some time.
		</comment>
		<comment id='14' author='freedenS' date='2020-06-19T03:59:32Z'>
		
Hi @freedenS - What is the tensor dimension for height and width ? Is there any aspect ration to be maintained between the two ?

The height and width are equal to input's which are dynamic.
There isn't any aspect ration to be maintained between the height and width.
		</comment>
		<comment id='15' author='freedenS' date='2020-06-19T04:01:44Z'>
		
Hi @freedenS ,
I couldn't repro this with the height == width == 224. See below - I run all 255s and random valued inputs alternately. I tried with 1.3.0 onnxruntime GPU package (you can uncomment the appropriate lines as you run on your box) . I set log_severity_level to 0 and confirmed that a lot (almost all nodes) got placed on the CUDA EP.
I think I might know why you are running into this issue. Are you using a fixed height and width combination between runs ? My guess is that you are varying height and width. The model is annotated with "dynamic" dim values for height and width - this is a little misleading - while ORT as an engine will accept any dim value, the model might inherently expect that there be some relationship (aspect ratio) between the height and width and the model will work on the assumption that the user has adhered to this requirement and when this is not met the model is bound to fail. You need to check what shapes the model was trained with and check if you need to adhere to any aspect ratio requirements.

import onnxruntime as rt
import numpy as np

opt = rt.SessionOptions()

# Uncomment to see which nodes get placed on CUDA EP
#opt.log_severity_level = 0

sess = rt.InferenceSession(r'segmentation.onnx', opt)

input_name_0 = sess.get_inputs()[0].name

# run 1 - all 255
input_0 = np.zeros((1, 3, 224, 224), dtype = np.float32)
input_0 += 255
#print(input_0)
pred_onnx_0 = sess.run(None, {input_name_0: input_0})
print(pred_onnx_0[0])

# run 2 - random
input_1 = np.random.rand(1, 3, 224, 224).astype(np.float32)
#print(input_1)
pred_onnx_1 = sess.run(None, {input_name_0: input_1})
print(pred_onnx_1[0])

# run 3 - all 255
pred_onnx_2 = sess.run(None, {input_name_0: input_0})
print(pred_onnx_2[0])

# run 2 - random
pred_onnx_3 = sess.run(None, {input_name_0: input_1})
print(pred_onnx_3[0])

print ("Done")


I'll try this.
		</comment>
		<comment id='16' author='freedenS' date='2020-06-19T04:33:48Z'>
		Sorry - I realized the issue now. For the input dims you are feeding there is no output, and this "empty" output is handled fine in the first run but there is a bug that meant on the subsequent run, this "empty" output was not gracefully handled. I have a fix PR for out. Thanks for reporting.
		</comment>
		<comment id='17' author='freedenS' date='2020-06-19T04:36:31Z'>
		
Oh I just saw your input script and noticed that you were feeding [800, 800] inputs and I could hit the issue with that. My guess is that there is something in the input combination (input shape + ConvTranspose attributes that CuDNN does not like). This doesn't look like it is an ORT issue in that it doesn't break runs after you feed it all 255s.
I ll debug what shapes the inputs have by the time it reaches the faulty ConvTranspose node in some time.

I resize the input as is (min_size:800 and max_size:1333).
please allow me to describe the problem again.
There are img1 and img2.
img2 is a normal test image and img1 isn't a normal test image (because i test some other images and also cause this problem)
inference: img2
res: correct result
inference: img1, img2
res: empty result, errormsg(Conv_3281 or segmentation fault)
it looks like the model (after inference with img1) is different with before.
		</comment>
		<comment id='18' author='freedenS' date='2020-06-19T04:39:17Z'>
		The error will go away after &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/4281&gt;#4281&lt;/denchmark-link&gt;
.
Can you try this and see if this output is experienced ?
inference: img1, img1
res: empty result, errormsg(Conv_3281 or segmentation fault)
		</comment>
		<comment id='19' author='freedenS' date='2020-06-19T04:41:10Z'>
		get it!
Thank you!
		</comment>
		<comment id='20' author='freedenS' date='2020-06-19T06:12:01Z'>
		
The error will go away after #4281.
Can you try this and see if this output is experienced ?
inference: img1, img1
res: empty result, errormsg(Conv_3281 or segmentation fault)

I have tried this.
the size of img1 is equal to img2.(img1 is other model's train image, not fully white image)
inference: img1, img1
res: empty result, empty result
inference: img1, img1, img2
res: empty result, empty result, errormsg(Conv_3281 or segmentation fault)
		</comment>
		<comment id='21' author='freedenS' date='2020-06-19T06:18:07Z'>
		

The error will go away after #4281.
Can you try this and see if this output is experienced ?
inference: img1, img1
res: empty result, errormsg(Conv_3281 or segmentation fault)

I have tried this.
the size of img1 is equal to img2.(img1 is other model's train image, not fully white image)
inference: img1, img1
res: empty result, empty result
inference: img1, img1, img2
res: empty result, empty result, errormsg(Conv_3281 or segmentation fault)

Can you please try building from source with my PR and checking if the issue goes away ?
		</comment>
		<comment id='22' author='freedenS' date='2020-06-19T06:20:23Z'>
		ok, i'll take a try.
		</comment>
		<comment id='23' author='freedenS' date='2020-06-19T06:21:46Z'>
		Thanks, and just to be clear - I have used the original model you shared in the issue description (preprocessing inside I think)
		</comment>
		<comment id='24' author='freedenS' date='2020-06-19T06:24:24Z'>
		Ok. ^v^
		</comment>
		<comment id='25' author='freedenS' date='2020-06-19T13:21:52Z'>
		I have built from source with your PR and tested the model i shared, but the problem (Conv_3281) still exists.
		</comment>
		<comment id='26' author='freedenS' date='2020-06-19T19:10:39Z'>
		Strange. In my case - the error I hit was in a ConvTranspose node (some nodes below the Conv node you are experiencing issues with) -
&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/85174243-d7ac0680-b229-11ea-9637-d681399e96bd.png&gt;&lt;/denchmark-link&gt;

And I am guessing you build from master right (not rel-1.3.0) ?
Any chance you ' ll be able to share the tensors being used for the 2 runs ? You can construct 2 .pb files using the ONNX Python APIs here to convert from numpy arrays to .pb - &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md&gt;https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md&lt;/denchmark-link&gt;
. That will make it easier to investigate. Thanks!
		</comment>
		<comment id='27' author='freedenS' date='2020-06-22T09:24:37Z'>
		sorry for the late reply, the error I hit was in Conv_3281.
I have met the same problem with python3.7 (pip install from binary) and c++(download from &lt;denchmark-link:https://github.com/microsoft/onnxruntime/releases/download/v1.3.0/onnxruntime-win-x64-gpu-1.3.0.zip&gt;https://github.com/microsoft/onnxruntime/releases/download/v1.3.0/onnxruntime-win-x64-gpu-1.3.0.zip&lt;/denchmark-link&gt;
). My opencv version is 4.2
And i built from &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/4281&gt;#4281&lt;/denchmark-link&gt;
 , only got the same result.
I'll upload the pb files soon.
		</comment>
		<comment id='28' author='freedenS' date='2020-06-22T20:03:43Z'>
		Thanks!
I ll take a look again after you upload the pb files.
		</comment>
		<comment id='29' author='freedenS' date='2020-06-23T06:13:16Z'>
		Here are the pb files.&lt;denchmark-link:https://drive.google.com/file/d/1qLZpY3lU6lmVQFPmPTnK9_C2MRaoj2K4/view?usp=sharing&gt;https://drive.google.com/file/d/1qLZpY3lU6lmVQFPmPTnK9_C2MRaoj2K4/view?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='30' author='freedenS' date='2020-06-24T18:20:53Z'>
		Awesome, thanks.
		</comment>
		<comment id='31' author='freedenS' date='2020-06-25T05:36:23Z'>
		Looking at this. One question - do you get this issue when you run the two cases concurrently ? If so, can you try running one at a time sequentially ?
		</comment>
		<comment id='32' author='freedenS' date='2020-06-25T14:02:44Z'>
		sorry, what are the two cases mean?
		</comment>
		<comment id='33' author='freedenS' date='2020-06-26T06:36:30Z'>
		Hey, never mind. I think I figured out the issue (finally :))! I ll update the same PR- please try after I update it.
Thanks for reporting!
		</comment>
		<comment id='34' author='freedenS' date='2020-06-26T23:33:16Z'>
		Can you please try building from source again and checking (with the PR) ? I could get the two inputs to pass with the fixes.
		</comment>
		<comment id='35' author='freedenS' date='2020-06-28T02:21:07Z'>
		Thank you!
I'll try again.
		</comment>
		<comment id='36' author='freedenS' date='2020-06-29T10:09:56Z'>
		Awesome! It works!
Thank you very much!
		</comment>
	</comments>
</bug>