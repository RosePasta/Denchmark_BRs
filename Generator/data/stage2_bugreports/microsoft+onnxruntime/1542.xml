<bug id='1542' author='ajaysg-zz' open_date='2019-08-01T15:01:05Z' closed_time='2019-08-01T22:18:17Z'>
	<summary>RuntimeError: [ONNXRuntimeError] : 1 : GENERAL ERROR : Load model from waveglow.onnx failed:Type Error: Type parameter (T) bound to different types (tensor(int64) and tensor(float) in node ().</summary>
	<description>
Description
I am having a pytorch model which is exported to onnx format. I am trying to use that model in onnxruntime. But i am getting the error mentioned in the issue
System information

OS Platform and Distribution : Linux Ubuntu 16.04
ONNX Runtime installed from (source or binary): Binary via pip
ONNX Runtime version: onnxruntime==0.4.0 (CPU version)
Python version: 3.6
Visual Studio version (if applicable):No
GCC/Compiler version (if compiling from source): No
CUDA/cuDNN version:CUDA 9/cuDNN 7
GPU model and memory:Tesla K20XM and 6GB

Note: I am NOT USING onnxruntime-gpu
To Reproduce
import onnx
import onnxruntime as rt
sess = rt.InferenceSession("waveglow.onnx")
Expected Behavior
After the session creation i should be able to infer with my model.

&lt;denchmark-link:https://user-images.githubusercontent.com/22977639/62302941-de5a4f80-b498-11e9-86a6-e3aa22eb8590.png&gt;&lt;/denchmark-link&gt;

Model is from the repository &lt;denchmark-link:https://github.com/NVIDIA/waveglow/tree/4b1001fa3336a1184b8293745bb89b177457f09b&gt;https://github.com/NVIDIA/waveglow/tree/4b1001fa3336a1184b8293745bb89b177457f09b&lt;/denchmark-link&gt;

Link to original pytorch model: &lt;denchmark-link:https://drive.google.com/file/d/1cjKPHbtAMh_4HTHmuIGNkbOkPBD9qwhj/view?usp=sharing&gt;https://drive.google.com/file/d/1cjKPHbtAMh_4HTHmuIGNkbOkPBD9qwhj/view?usp=sharing&lt;/denchmark-link&gt;

Link to onnx model:
&lt;denchmark-link:https://drive.google.com/file/d/1N4D4pu-ibYBgm890ZJWRUkneQuyUKA_H/view?usp=sharing&gt;https://drive.google.com/file/d/1N4D4pu-ibYBgm890ZJWRUkneQuyUKA_H/view?usp=sharing&lt;/denchmark-link&gt;

Script used to convert the pytorch model to onnx by passing dummy input
waveglow_path = './waveglow_256channels.pt'
waveglow = torch.load(waveglow_path)['model']
mel_extra=torch.tensor(np.full((1,80,222),-10.0)).float()
dummy_input=torch.autograd.Variable(mel_extra).cuda().float()
audio = torch.tensor(np.full((1,222), -0.5)).float()
audio_input = torch.autograd.Variable(audio).cuda().float()
torch.onnx.export(waveglow, ((dummy_input, audio_input),), "./waveglow.onnx")
	</description>
	<comments>
		<comment id='1' author='ajaysg-zz' date='2019-08-01T22:18:17Z'>
		Please report this bug to Pytorch. The model itself is incorrect.
There is an Mul node, which has two inputs, one is "tensor(int64)", another is "tensor(float)".  They should be the same type.  I can't tell you it is which node, because the node name is empty.  It would be better if the Pytorch can generate a unique  name for each node.
		</comment>
		<comment id='2' author='ajaysg-zz' date='2019-08-05T06:25:08Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 its a bug with onnxrt. before exporting the pytorch model to onnx, i have converted the model to float using the command
model1=model.float()
now all the tensors in the model1 is float.
mel_extra=torch.tensor(np.full((1,80,222),-10.0)).float()
dummy_input=torch.autograd.Variable(mel_extra).cuda().float()
audio = torch.tensor(np.full((1,222), -0.5)).float()
audio_input = torch.autograd.Variable(audio).cuda().float()
torch.onnx.export(model1, ((dummy_input, audio_input),), "./waveglow.onnx")
Then i passed the float tensors as mentioned above and traced the onnx graph. Onnx export was successful. When i try to use it in onnxrt by starting the session
sess = rt.InferenceSession("../waveglow.onnx")
it throws the error referenced in the issue.
		</comment>
	</comments>
</bug>