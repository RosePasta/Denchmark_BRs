<bug id='1133' author='adaber' open_date='2019-05-30T03:15:11Z' closed_time='2019-05-31T20:46:44Z'>
	<summary>[ShapeInferenceError] Attribute dilations has incorrect size</summary>
	<description>
Hey, everyone :)
Not sure if this is a bug or it is something on my end but this is what led to this error.

Loaded a CNTK 2.4 .cmf model using the CNTK 2.7 library.
Saved the model as ONNX using the CNTK 2.7 library.
Used the ONNX C++ example here on GitHub to load the model and it gave me the error: "Load model from ... failed:[ShapeInferenceError] Attribute dilations has incorrect size"

System information

Windows 10
ONNX Runtime installed from binary
ONNX Runtime version: v0.4.0
Visual Studio version: 2017
CPU

Expected behavior
I expected the model to be loaded and the information about it displayed on the screen
Thanks!
	</description>
	<comments>
		<comment id='1' author='adaber' date='2019-05-30T06:07:58Z'>
		Hi &lt;denchmark-link:https://github.com/adaber&gt;@adaber&lt;/denchmark-link&gt;
,
Can you please share your ONNX model from step (2) ?
Thanks
		</comment>
		<comment id='2' author='adaber' date='2019-05-30T07:22:56Z'>
		No problem, hariharans29
This is just one of my models that doesn't do much and is used for testing purposes only. Hope it helps.
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3235873/TestModelCNTK27.zip&gt;TestModelCNTK27.zip&lt;/denchmark-link&gt;

Thanks
		</comment>
		<comment id='3' author='adaber' date='2019-05-30T19:32:09Z'>
		Hi &lt;denchmark-link:https://github.com/adaber&gt;@adaber&lt;/denchmark-link&gt;
,
Thanks for sharing the model.
It seems like the model has been incorrectly exported to ONNX. I tried loading the model and I hit the same error message (at load time, not at inference run time)
The root cause seems to be in the batch dimension - it seems to have both the symbolic dimension (for processing a batch of inputs) and also a '1' denoting that it can only process 1 input per inference run -
&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/58658765-1cd76080-82d6-11e9-93a8-667e7f6707b1.png&gt;&lt;/denchmark-link&gt;

The right input dimenions would have to be [Sequence, 3, 480, 640] (in this case a batch of input values can be processed) or [1, 3, 480, 640] (in this case only a single input can be processed) - but the exported model has both, which I am guessing was not intended.
This is either:

bug in the exporter logic
(or)
an incorrect configuration specified at export time

Please raise a issue in the appropriate forum (in this case it's probably the CNTK to ONNX exporter forum) and once the correction is made in the model, we can proceed to actually running the (correct version of the) model using ORT.
Hope this helps!
		</comment>
		<comment id='4' author='adaber' date='2019-05-30T19:56:14Z'>
		Thank you for looking into this, hariharans29
So, it is a problem on the CNTK exporter's end if I understood correctly ?
If so, I wonder if there is anyone else who has also run across this problem ? I just want to see if there's anything on my end that I can do to resolve this issue.
This is so unfortunate because my input variable is very straightforward and you'd think it should just work. I will create an issue post there and will direct them to this post.
Thanks!
		</comment>
		<comment id='5' author='adaber' date='2019-05-30T20:24:38Z'>
		Hi &lt;denchmark-link:https://github.com/adaber&gt;@adaber&lt;/denchmark-link&gt;
,
That's correct. I believe this is an exporter mistake (whether due to a bug or an incorrect configuration at export time I don't know yet).
Thanks for opening the issue there - I would also share the model export code just so that they may know if there was something wrong in the export configuration.
Thanks
		</comment>
		<comment id='6' author='adaber' date='2019-05-30T20:31:44Z'>
		&lt;denchmark-link:https://github.com/wenbingl&gt;@wenbingl&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/EmmaNingMS&gt;@EmmaNingMS&lt;/denchmark-link&gt;
 - FYI -&gt; &lt;denchmark-link:https://github.com/adaber&gt;@adaber&lt;/denchmark-link&gt;
 has a CNTK model that is being loaded in CNTK and then exported to ONNX. The problem is that the exported model has both symbolic dimension (Sequence) and '1' for the batch dimension. So the input dimension becomes [Sequence, 1, 3, 480, 640] instead of [Sequence, 3, 480, 640] or [1, 3, 480, 640].
Is there a quick solution from the converter side to convert the existing CNTK model directly into ONNX (as opposed to being loaded in CNTK and then exported to ONNX) for unblocking model run ?
		</comment>
		<comment id='7' author='adaber' date='2019-05-30T20:38:50Z'>
		hariharans29,
Just uploaded all three models (CNTK 2.4 and CNTK 2.7 "CNTKv2" models and CNTK 2.7 "ONNX" model) there. Hope they get around to look at this issue since the CNTK 2.7 version was their last major release.
Thanks for your help :)
		</comment>
		<comment id='8' author='adaber' date='2019-05-31T19:04:14Z'>
		Hi &lt;denchmark-link:https://github.com/adaber&gt;@adaber&lt;/denchmark-link&gt;
,
Can we close this issue since it's not really actionable on the ONNX Runtime side ? Would it be okay if once you have the correct model exported, and still have issues using ONNX Runtime, we then re-open this issue ?
Thanks!
		</comment>
		<comment id='9' author='adaber' date='2019-05-31T20:46:44Z'>
		Of course, hariharans29 :)
I will close it now. Thanks again for the help!
		</comment>
	</comments>
</bug>