<bug id='1164' author='oohlala1204' open_date='2019-06-05T07:47:46Z' closed_time='2019-06-06T08:51:37Z'>
	<summary>[ShapeInferenceError] Mismatch between number of source and target dimensions. Source=1 Target=0</summary>
	<description>
Describe the bug
I am trying to use onnxruntime to load my onnx model.
import onnxruntime as rt 
sess = rt.InferenceSession(onnx_path)
and I get the following error:
failed:Node:dropout/sub Output:dropout/sub [ShapeInferenceError] Mismatch between number of source and target dimensions. Source=1 Target=0
It seems that the shape of the tensor "Output:dropout/sub" is wrong.
but when I use the API in onnx to load my onnx model,
model=onnx.load(onnx_path)
tf_rep=prepare(model)
it works, and I print the nodes in the model to find the tensor "dropout/sub", and it is shown as follow
'dropout/sub/x': &lt;tf.Tensor 'Const_8:0' shape=(1,) dtype=float32&gt;
Any help would be appreciated.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
ONNX Runtime installed from (source or binary): binary
ONNX Runtime version: 0.4.0
Python version: 3.6
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

	</description>
	<comments>
		<comment id='1' author='oohlala1204' date='2019-06-05T09:17:58Z'>
		The error is from the 'dropout/sub' node but the node you've printed has a name of 'dropout/sub/x'. Is it the same node?
Not sure if you need to run shape inferencing as a separate step to calling onnx.load. &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#running-shape-inference-on-an-onnx-model&gt;https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#running-shape-inference-on-an-onnx-model&lt;/denchmark-link&gt;

The target having 0 dims would usually indicate it has a shape that is empty. A shape with no dims equates to a scalar though, so it's a mismatch with a tensor with 1 dimension. I would suggest removing the shape from the tensor proto (so TypeProto_Tensor.has_shape() returns false) as that will let shape inferencing fill in all the shape information.
		</comment>
		<comment id='2' author='oohlala1204' date='2019-06-05T19:07:03Z'>
		Hi &lt;denchmark-link:https://github.com/oohlala1204&gt;@oohlala1204&lt;/denchmark-link&gt;
,
Along with &lt;denchmark-link:https://github.com/skottmckay&gt;@skottmckay&lt;/denchmark-link&gt;
 's suggestions, I would:


Validate if your model is a valid onnx model first: https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#checking-an-onnx-model


Make sure the input tensor you are feeding is of the right shape corresponding to the expected input shape for the input


Paste the entire code snippet that you are using to invoke ONNX Runtime to run the model and share the model itself (if that's okay with you) for further debugging.


Thanks
		</comment>
		<comment id='3' author='oohlala1204' date='2019-06-06T08:51:32Z'>
		Thank you all! I regenerate the model using tf2onnx, and the error doesn't occur.
		</comment>
	</comments>
</bug>