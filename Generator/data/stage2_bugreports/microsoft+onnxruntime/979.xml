<bug id='979' author='elevir' open_date='2019-05-07T08:19:22Z' closed_time='2019-05-14T17:59:34Z'>
	<summary>Session::Run() function doesn't support multiple outputs</summary>
	<description>
Describe the bug
When I'm trying to infer two outputs of model at the same time an exception of access violation occurs.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1809
ONNX Runtime installed from (source or binary): source
ONNX Runtime version: 0.4.0


Code is same as &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp&gt;example&lt;/denchmark-link&gt;
, the difference is that a model has two outputs and two output node names passed into  method.

In the screenshot below  is nullptr, but 'output[1]' is  which is not equals to nullptr. If such using of inference is not supposed, then there must be an exception, otherwise it is bug and  must be checked for nullptr value before indexing.
&lt;denchmark-link:https://user-images.githubusercontent.com/8929511/57283586-29421200-70b8-11e9-9f42-650646f94d91.png&gt;&lt;/denchmark-link&gt;

This is code snippet of method
ORT_API_STATUS_IMPL(OrtRun, _In_ OrtSession* sess, _In_ OrtRunOptions* run_options, _In_ const char* const* input_names, _In_ const OrtValue* const* input, size_t input_len, _In_ const char* const* output_names1, size_t output_names_len, _Out_ OrtValue** output)
from onnxruntime_c_api.cc
	</description>
	<comments>
		<comment id='1' author='elevir' date='2019-05-07T17:01:32Z'>
		Show me your code please.
For example,
  Ort::Value output_tensor = session.Run(nullptr, input_node_names.data(), input_tensors, output_node_names.data(), 1);
Did you change the last parameter from 1 to 2?
		</comment>
		<comment id='2' author='elevir' date='2019-05-07T18:14:35Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 the code of  looks like
&lt;denchmark-code&gt;Ort::Value output_tensor = session.Run(nullptr, input_node_names.data(), input_tensors, output_node_names.data(), output_node_names.size());
&lt;/denchmark-code&gt;

The size of output_node_names equals to 2.
		</comment>
		<comment id='3' author='elevir' date='2019-05-07T21:47:45Z'>
		is it ok for you to share the model with us please?
		</comment>
		<comment id='4' author='elevir' date='2019-05-08T05:34:57Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/linkerzhang&gt;@linkerzhang&lt;/denchmark-link&gt;
 there is full code of model using.
&lt;denchmark-code&gt;#define CUDA
//#define TENSORRT

#include &lt;iostream&gt;
#include &lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;

#ifdef CUDA
#include &lt;onnxruntime/core/providers/cuda/cuda_provider_factory.h&gt;
#elif defined TENSORRT
#include &lt;onnxruntime/core/providers/tensorrt/tensorrt_provider_factory.h&gt;
#endif


void main()
{
	Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "env");
	Ort::SessionOptions sessOptions;

#ifdef CUDA
	OrtSessionOptionsAppendExecutionProvider_CUDA(sessOptions, 0);
#elif defined TENSORRT
	OrtSessionOptionsAppendExecutionProvider_Tensorrt(sessOptions);
#endif

	sessOptions.SetGraphOptimizationLevel(0);

	const wchar_t* modelPath = L"super_point.onnx";
	Ort::Session sess(env, modelPath, sessOptions);
	auto allocator = Ort::Allocator::Create_Default();
	// print number of model input nodes
	std::vector&lt;const char*&gt; inputNodeNames(1);
	std::vector&lt;int64_t&gt; inputNodeDims;
	char* inputName = sess.GetInputName(0, allocator);
	inputNodeNames[0] = inputName;
	Ort::TypeInfo typeInfo = sess.GetInputTypeInfo(0);
	auto tensorInfo = typeInfo.GetTensorTypeAndShapeInfo();
	auto tensorShape = tensorInfo.GetShape();

	size_t dataSize = OrtGetTensorShapeElementCount(tensorInfo);
	std::vector&lt;float&gt; inputData(dataSize, 0);
	auto allocInfo = Ort::AllocatorInfo::Create_Cpu(OrtAllocatorType::OrtArenaAllocator, OrtMemType::OrtMemTypeDefault);
	Ort::Value inputTensor[1] = { 
		Ort::Value::CreateTensor(
			allocInfo,
			inputData.data(), dataSize * sizeof(float), 
			tensorShape.data(), tensorShape.size(), 
			ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT) 
	};

	
	size_t outputsCount = sess.GetOutputCount();
	std::vector&lt;const char*&gt; outputNodeNames(outputsCount);
	for (size_t i = 0; i &lt; outputsCount; ++i) {
		char* outputNodeName = sess.GetOutputName(i, allocator);
		outputNodeNames[i] = outputNodeName;
	}
	//std::vector&lt;const char*&gt; outputNodeNames = { sess.GetOutputName(1, allocator) };

	Ort::Value output = sess.Run(nullptr, inputNodeNames.data(), inputTensor, outputNodeNames.data(), outputNodeNames.size());
        for (const auto&amp; dim : output.GetTensorTypeAndShapeInfo().GetShape())
		std::cout &lt;&lt; dim &lt;&lt; " ";
	std::cout &lt;&lt; std::endl;
}
&lt;/denchmark-code&gt;

And there is the model: &lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3155643/super_point.zip&gt;super_point.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='elevir' date='2019-05-08T06:23:29Z'>
		I hit an error of:
/data/src/onnxruntime/onnxruntime/core/framework/execution_frame.cc:64
onnxruntime::common::Status onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue(int, const onnxruntime::TensorShape *, onnxruntime::MLValue *&amp;) shape &amp;&amp; tensor.Shape() == *shape was false. MLValue shape verification failed. Current shape:{2,1,480,640} Requested shape:{2,256,60,80}
		</comment>
		<comment id='6' author='elevir' date='2019-05-08T06:39:07Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 is it thrown before running inference? Because I can succesfully infer model with one node (including output node with shape { 2, 256, 60, 80 })
		</comment>
		<comment id='7' author='elevir' date='2019-05-08T06:43:36Z'>
		No. It's inside sess.Run.
		</comment>
		<comment id='8' author='elevir' date='2019-05-08T07:42:07Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 strange. I have built Ort with Debug config and program still fails on access violation without errors related on Shape.
Just in case, I've updated message above with actual code.
		</comment>
		<comment id='9' author='elevir' date='2019-05-09T17:00:57Z'>
		I think I ran into a similar problem. I solved it by supplying an array of NULL initialized pointers to OrtValue, one for each expected output:
`std::vector&lt;OrtValue *&gt; vpOutputTensor(outputNodeNames.size() );
pSt = OrtRun( apS.get(), NULL, inputNodeNames.data(), (const OrtValue* const*)&amp;pInputTensor, 1, outputNodeNames.data(), outputNodeNames.size(), vpOutputTensor.data() );`
		</comment>
		<comment id='10' author='elevir' date='2019-05-09T20:27:07Z'>
		It's a bug in the cxx wrapper.
		</comment>
	</comments>
</bug>