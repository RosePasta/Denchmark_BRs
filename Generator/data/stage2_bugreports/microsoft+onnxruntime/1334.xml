<bug id='1334' author='minus-one' open_date='2019-07-02T21:56:28Z' closed_time='2019-07-19T20:54:10Z'>
	<summary>ssd-mobilenet model load fails with GPU backend</summary>
	<description>

Source of ssd-mobilenet: &lt;denchmark-link:https://zenodo.org/record/3163026/files/ssd_mobilenet_v1_coco_2018_01_28.onnx&gt;https://zenodo.org/record/3163026/files/ssd_mobilenet_v1_coco_2018_01_28.onnx&lt;/denchmark-link&gt;
 (taken via MLPerf/inference &lt;denchmark-link:https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection&gt;https://github.com/mlperf/inference/tree/master/v0.5/classification_and_detection&lt;/denchmark-link&gt;
)
So this model correctly loads with CPU backend. It fails with the GPU backend. I tested it with the pypi build and a more recent master build with cuda 10.0
I try to load the model and the following error throws up:
&lt;denchmark-code&gt;  self._sess.load_model(path_or_bytes)
RuntimeError: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Using an input in multiple nodes on different devices is not supported currently. Input:Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Slice/begin:0 is used by node Add__2519 (CUDAExecutionProvider) and node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Slice (CPUExecutionProvider).

&lt;/denchmark-code&gt;

Note that the model loads correctly with an onnxruntime build with backend of CPU.
Not sure if the issue is with the onnx file or the runtime or in the conversion. Any pointers / clarifications would help.
Also a bunch of these warnings popped up.

See attached file for whole log.&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3352451/dump.log&gt;dump.log&lt;/denchmark-link&gt;

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
ONNX Runtime installed from (source or binary):
Source - master / binary pypi
ONNX Runtime version:
0.4.0
Python version:
3.6.8
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source):
7.4.0
CUDA/cuDNN version:
10.0
GPU model and memory:

To Reproduce
Describe steps/code to reproduce the behavior:
Download the model from the above mentioned link.
open python,
import onnxruntime as rt
rt.InferenceSession('ssd_mobilenet_v1_coco_2018_01_28.onnx')
Expected behavior
A clear and concise description of what you expected to happen.
Model loads correctly
Screenshots
If applicable, add screenshots to help explain your problem.

Add any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.
So, in reference to &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1229&gt;#1229&lt;/denchmark-link&gt;

I downloaded the mentioned converted model from ssd_mobilenet_v2 as well and that fails similarly, although with CPU backend it did load correctly.
	</description>
	<comments>
	</comments>
</bug>