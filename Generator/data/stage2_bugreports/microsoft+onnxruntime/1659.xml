<bug id='1659' author='prometheusDE' open_date='2019-08-20T15:43:52Z' closed_time='2019-09-25T18:53:44Z'>
	<summary>[LSTM Inference] C++ API Scoring with internal state update</summary>
	<description>
Describe the bug
LSTM Network is not scoring properly past the first sequence value using the C++ interface.
Urgency
High
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
ONNX Runtime installed from (source or binary): source
ONNX Runtime version: 0.5.0


I exported a LSTM Model using the ONNX Exporter from Matlab (works now properly after the 1.Aug Update.) &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1016&gt;#1016&lt;/denchmark-link&gt;

When I recall/score the LSTM learnt on a sequence-to-sequence regression problem, only the first prediction in the sequence is correct (as in equal to the Matlab score). All other differ.
I followed the C++ API example here &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp&gt;Link&lt;/denchmark-link&gt;

Do I need to score the model in a certain way so the LSTM layers are updating their states? We score value by value in the sequence since we are predicting on an incoming stream of sensor data.
If necessary I can provide an example model and expected / actual score.
I have not tried the model in python since the ONNX model is embeded in a custom model format (with pre- and postprocessing) used in a C/C++ Library.
	</description>
	<comments>
		<comment id='1' author='prometheusDE' date='2019-08-21T23:05:26Z'>
		Can you clarify what you mean by 'score the model'?
Are you passing in the whole sequence when running the model (i.e. a single call to Ort::Session::Run)?
How do the results differ? Is it massively or by small amounts? Given there can be large numbers of floating point operations in LSTM nodes it's expected there will be small differences between different implementation of LSTM.
		</comment>
		<comment id='2' author='prometheusDE' date='2019-08-22T06:52:51Z'>
		Hi,
"scoring" the model means recalling or inferencing the model. Basically we have at any time only one input along a sequence, calculate the prediction but need to have the LSTM layers keep their internal state to predict the next input. We cannot wait to get the full sequence since we are working in a streaming environment.
Results are very different, its not only precision from floating point operations.
I will post a test case here soon. Thanks for the reply!
		</comment>
		<comment id='3' author='prometheusDE' date='2019-08-25T19:19:53Z'>
		The ORT operators are all stateless (their Compute method is const), so the LSTM node is not going to remember the state from a previous run and you'll need to handle that yourself.
There's a unit test that does this sort of thing at 


onnxruntime/onnxruntime/test/framework/inference_session_test.cc


         Line 1204
      in
      f25847b






 TEST(InferenceSessionTests, TestTruncatedSequence) { 





		</comment>
		<comment id='4' author='prometheusDE' date='2019-09-19T17:29:06Z'>
		&lt;denchmark-link:https://github.com/prometheusDE&gt;@prometheusDE&lt;/denchmark-link&gt;
 do you still need any assistance on this issue?
		</comment>
	</comments>
</bug>