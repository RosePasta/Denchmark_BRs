<bug id='2067' author='CoinCheung' open_date='2019-10-09T08:17:46Z' closed_time='2020-07-11T01:41:07Z'>
	<summary>Cannot infer model exported from pytorch</summary>
	<description>
Describe the bug
I build onnxruntime from source with tensorrt6.0 supported, and I tested it with following code:
import onnx
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.nn import Conv2d
from torch.nn import BatchNorm2d
import onnxruntime.backend as backend


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        self.conv1 = Conv2d(3,
                            64,
                            kernel_size=7,
                            stride=2,
                            padding=3,
                            bias=False)
        self.maxpool = nn.MaxPool2d(kernel_size=3,
                                    stride=2,
                                    padding=1,
                                    dilation=1,
                                    ceil_mode=False)
        self.layer1 = Conv2d(64,
                            256,
                            kernel_size=3,
                            stride=1,
                            padding=1,
                            bias=False)
        self.layer2 = Conv2d(256,
                            512,
                            kernel_size=3,
                            stride=2,
                            padding=1,
                            bias=False)
        self.layer3 = Conv2d(512,
                            1024,
                            kernel_size=3,
                            stride=2,
                            padding=1,
                            bias=False)
        self.layer4 = Conv2d(1024,
                            2048,
                            kernel_size=3,
                            stride=2,
                            padding=1,
                            bias=False)
        self.inner32 = nn.Conv2d(2048, 256, 1, padding=0, stride=1)
        self.inner16 = nn.Conv2d(1024, 256, 1, padding=0, stride=1)
        self.outer16 = nn.Conv2d(256, 256, 3, padding=1, stride=1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.maxpool(x)
        feat4 = self.layer1(x)
        feat8 = self.layer2(feat4)
        feat16 = self.layer3(feat8)
        feat32 = self.layer4(feat16)

        feat32 = self.inner32(feat32)
        feat16 = self.inner16(feat16)
        _, _, H, W = feat16.size()
        feat32 = F.interpolate(feat32, (H, W), mode='nearest')
        feat16 = feat16 + feat32
        return feat4, feat8, feat16, feat32



if __name__ == "__main__":
    resnet = Net()
    dummy_input = torch.randn(1, 3, 224, 224)
    oname = 'resnet50.onnx'
    torch.onnx.export(resnet, dummy_input, oname, verbose=True)

    model = onnx.load(oname)
    img = (cv2.resize(cv2.imread('./000000000139.jpg'), (224, 224)) / 255. - 0.45 ) / 0.225
    img = img.transpose(2, 0, 1)[np.newaxis, :, :, :].astype(np.float32)
    rep = backend.prepare(model, device="GPU")
    outputs = rep.run(img)
And I got the error message:


Traceback (most recent call last):
File "debug.py", line 87, in 
rep = backend.prepare(model, device="GPU")
File "/miniconda/envs/py36/lib/python3.6/site-packages/onnxruntime/backend/backend.py", line 80, in prepare
return cls.prepare(bin, device, **kwargs)
File "/miniconda/envs/py36/lib/python3.6/site-packages/onnxruntime/backend/backend.py", line 69, in prepare
inf = InferenceSession(model, options)
File "/miniconda/envs/py36/lib/python3.6/site-packages/onnxruntime/capi/session.py", line 23, in init
self._load_model()
File "/miniconda/envs/py36/lib/python3.6/site-packages/onnxruntime/capi/session.py", line 37, in _load_model
self._sess.read_bytes(self._path_or_bytes, providers)
onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : This is an invalid model. Graph output (36) does not exist in the graph.


System information


OS Platform and Distribution : Linux Ubuntu 16.04


ONNX Runtime installed from: source


ONNX Runtime version: 0.5.0


Python version: python3.6.8


GCC/Compiler version (if compiling from source): 5.4.0


CUDA/cuDNN version: 10.0/7.6.0


GPU model and memory: 2 2080ti/10g


	</description>
	<comments>
		<comment id='1' author='CoinCheung' date='2019-10-09T19:22:51Z'>
		Which PyTorch version are you using?
		</comment>
		<comment id='2' author='CoinCheung' date='2019-10-10T01:41:44Z'>
		I used pytorch built from source, the commit id is b96f49885f3c51d4ad0a2f02478aae77cf274f1c, and the version is 1.4+.
		</comment>
		<comment id='3' author='CoinCheung' date='2019-10-11T20:31:26Z'>
		I think you mean 1.3+, right. The build looks recent. We will take a look.
		</comment>
		<comment id='4' author='CoinCheung' date='2019-10-13T22:44:47Z'>
		Hey &lt;denchmark-link:https://github.com/CoinCheung&gt;@CoinCheung&lt;/denchmark-link&gt;
, I tried to export your model with b96f49885f3c51d4ad0a2f02478aae77cf274f1c and ONNX Runtime v0.5.0 but couldn't reproduce the error.
Could you try running "onnx.checker.check_model(model)" after loading the model and see if you get the same error message?
Could you also share the generated onnx model?
		</comment>
		<comment id='5' author='CoinCheung' date='2019-10-14T02:39:15Z'>
		Hi &lt;denchmark-link:https://github.com/lara-hdr&gt;@lara-hdr&lt;/denchmark-link&gt;
 , I have uploaded my onnx file together with the whl file that I built from source &lt;denchmark-link:https://drive.google.com/drive/folders/1PQnR19Udz8Hh6w1OZb7P1ViSbvds5Vj6?usp=sharing&gt;here&lt;/denchmark-link&gt;
. I built the python package depending on tensorrt6.
		</comment>
		<comment id='6' author='CoinCheung' date='2019-10-14T21:56:30Z'>
		I ran onnx.checker.check_model(model) on the provided model and tried running it with the dummy input and I did not get any error on CPU.
&lt;denchmark-link:https://github.com/faxu&gt;@faxu&lt;/denchmark-link&gt;
 the exported model looks good, this is not an issue with the converter, it could potentially be a backend issue.
		</comment>
		<comment id='7' author='CoinCheung' date='2020-07-03T05:58:05Z'>
		This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.
		</comment>
		<comment id='8' author='CoinCheung' date='2020-07-11T01:40:41Z'>
		This issue has been automatically closed due to inactivity. Please reactivate if further support is needed.
		</comment>
	</comments>
</bug>