<bug id='1034' author='GuanLuo' open_date='2019-05-15T03:31:36Z' closed_time='2019-06-04T23:32:28Z'>
	<summary>Can't Create Model Sessions on Different GPU</summary>
	<description>
Describe the bug
I am on a 2 GPU system, I tried to create one session on each GPU (so 2 sessions in total). I get the following error when it's creating the second session (i.e. creating the first session on GPU 0, and the second session on GPU1)
&lt;denchmark-code&gt;Failed to get allocator for location: OrtAllocatorInfo: [ name:Cuda id:0 mem_type:0 type:1]
&lt;/denchmark-code&gt;

Changing the creation order (first session on GPU1, second session on GPU0) will gives the same kind of error, but with different id
&lt;denchmark-code&gt;Failed to get allocator for location: OrtAllocatorInfo: [ name:Cuda id:1 mem_type:0 type:1]
&lt;/denchmark-code&gt;

System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
ONNX Runtime installed from (source or binary): source
ONNX Runtime version: 0.4.0
Python version:
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CUDA/cuDNN version: 10.1.105 / 7.5.0.56
GPU model and memory: Tesla P100, 16 GB

To Reproduce

Compile the code (modified from c_api_example) below, and run the executable
the model file "int32_int32_int32.onnx" is just a simple model that takes two inputs, returns two outputs for addition and subtraction. Will provide the generation script if requested, but the model itself seems irrelevant to the issue

&lt;denchmark-code&gt;#include &lt;assert.h&gt;
#include &lt;core/providers/cuda/cuda_provider_factory.h&gt;
#include &lt;core/session/onnxruntime_c_api.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;iostream&gt;
#include &lt;vector&gt;

//*****************************************************************************
// helper function to check for status
#define CHECK_STATUS(expr)                               \
  {                                                      \
    OrtStatus* onnx_status = (expr);                     \
    if (onnx_status != NULL) {                           \
      const char* msg = OrtGetErrorMessage(onnx_status); \
      fprintf(stderr, "%s\n", msg);                      \
      OrtReleaseStatus(onnx_status);                     \
      exit(1);                                           \
    }                                                    \
  }

int
main(int argc, char* argv[])
{
  //===================== Env Scope ======================
  // initialize  enviroment...one enviroment per process
  // enviroment maintains thread pools and other state info
  OrtEnv* env = nullptr;
  // [TODO] look into OrtCreateEnvWithCustomLogger()
  CHECK_STATUS(OrtCreateEnv(ORT_LOGGING_LEVEL_WARNING, "test", &amp;env));

  //===================== Model Scope ======================
  // session option also specifies GPU device
  // but other session option can be set "globally", just clone the session
  // when setting actual device
  OrtSessionOptions* session_options = OrtCreateSessionOptions();
  OrtSetSessionThreadPoolSize(session_options, 1);

  // disable graph optimization
  OrtSetSessionGraphOptimizationLevel(session_options, 0);

  //===================== Session Scope ======================
  // create multiple sessions and load models into memory
  std::vector&lt;const char*&gt; model_paths;
  {
    // two instances of the same model
    model_paths.emplace_back();
    model_paths.back() = "int32_int32_int32.onnx";
    model_paths.emplace_back();
    model_paths.back() = "int32_int32_int32.onnx";
  }

  std::vector&lt;OrtSession*&gt; sessions(model_paths.size());
  for (size_t idx = 0; idx &lt; model_paths.size(); idx++) {
    // To deploy on different device, need to set provider on cloned option
    // as you can remove a execution provider once it is appended
    OrtSessionOptions* context_options =
        OrtCloneSessionOptions(session_options);
    
    // Use CUDA, device 0. CPU if not set.
    // Use "idx" as we know we have two models and two devices
    OrtSessionOptionsAppendExecutionProvider_CUDA(context_options, idx);
    std::cout &lt;&lt; "here" &lt;&lt; std::endl;
    // [TODO] change session options for different session
    CHECK_STATUS(OrtCreateSession(
        env, model_paths[idx], context_options, &amp;sessions[idx]));
    std::cout &lt;&lt; "after here" &lt;&lt; std::endl;
    // session-wise
    OrtReleaseSessionOptions(context_options);
  }
}
&lt;/denchmark-code&gt;

Expected behavior
Expect to be able to create session on different GPU at the same time. Right now, I can create two sessions on the same GPU (GPU0 only or GPU1 only).
Screenshots
If applicable, add screenshots to help explain your problem.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='GuanLuo' date='2019-05-15T17:22:38Z'>
		Yes, it's a known problem.
		</comment>
		<comment id='2' author='GuanLuo' date='2019-05-17T21:11:35Z'>
		We should fix the following warning:
&lt;denchmark-code&gt;/home/chasun/src/onnxruntime/onnxruntime/core/providers/cuda/cuda_allocator.h:24:13: warning: private field 'device_id_' is not used [-Wunused-private-field]
  const int device_id_;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='GuanLuo' date='2019-05-19T00:43:21Z'>
		I think the problem is in thread_local of per_thread_context in CUDA execution provider. &lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
 can you try create the two sessions in two different threads and see if the problem still exists?
		</comment>
		<comment id='4' author='GuanLuo' date='2019-05-20T16:21:01Z'>
		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 So I just spawn a thread to execute ? Or the life time of the thread needs to be longer than that? Like once the session created, to run the model, I also need to run it in the same thread.
Anyway, I will try that.
		</comment>
		<comment id='5' author='GuanLuo' date='2019-05-28T18:07:26Z'>
		&lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
, yes please try spawn a thread to execute OrtCreateSession for different devices. Your inference code should run on the same thread too. Please let me know what do you find.
		</comment>
		<comment id='6' author='GuanLuo' date='2019-05-30T03:05:25Z'>
		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 Yes! It works if I spawn a new thread to execute OrtCreateSession, the following is what I changed in my code snap. Notice that I didn't run my inference code on the same thread, and everything seems fine. Is that expected? Or there is pitfall for doing that?
auto status = std::async(std::launch::async, &amp;OrtCreateSession,
        env, model_paths[idx], context_options, &amp;sessions[idx]);
CHECK_STATUS(status.get());
		</comment>
		<comment id='7' author='GuanLuo' date='2019-05-30T06:07:55Z'>
		Thanks for the confirmation, will work on a fix to it. Your workaround is OK, as long as the inference threads for different GPUs are separated.
		</comment>
		<comment id='8' author='GuanLuo' date='2019-05-30T16:15:28Z'>
		I just realized I wasn't being clear. I mean my inference code is run on the main thread (sessions are created on different threads via async), and the inference code send one request to each session.
		</comment>
		<comment id='9' author='GuanLuo' date='2019-05-31T23:51:38Z'>
		Thanks &lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
, I found your &lt;denchmark-link:https://github.com/NVIDIA/tensorrt-inference-server/commit/fbbbeefd24ee537b3074bd055444755fd7889b81&gt;workaround&lt;/denchmark-link&gt;
. Will try my fix with TRTIS later.
		</comment>
		<comment id='10' author='GuanLuo' date='2019-06-04T23:37:54Z'>
		&lt;denchmark-link:https://github.com/GuanLuo&gt;@GuanLuo&lt;/denchmark-link&gt;
 can you try the fix and see if your issue is solved?
		</comment>
		<comment id='11' author='GuanLuo' date='2019-06-05T02:25:27Z'>
		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 I tried with my sample code and the issue is resolved! I can deploy models on different GPUs without using threading. However, I am not going to try it on our server code until there is new release version for ONNX Runtime, hope you understand that.
		</comment>
		<comment id='12' author='GuanLuo' date='2019-06-05T18:42:23Z'>
		Awesome, thanks for the confirmation.
		</comment>
	</comments>
</bug>