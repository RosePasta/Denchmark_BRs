<bug id='495' author='lynic' open_date='2019-02-20T14:36:16Z' closed_time='2019-04-10T03:14:23Z'>
	<summary>Conv with auto_pad="SAME_UPPER" does not work as expected.</summary>
	<description>
Describe the bug
When set strides &gt; 1 with auto_pad="SAME_UPPER", the output shape is not the same as input.
if x.shape is {1,1,7,5}, strides={2,2}, then the output should be {1,1,7,5}, but I got {1,1,4,3}
(shapes (1, 1, 7, 5), (1, 1, 4, 3) mismatch)
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu16.04
ONNX Runtime installed from (source or binary): pip install onnxruntime
ONNX Runtime version: 0.2.1
Python version: 3.6
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version:
GPU model and memory:

To Reproduce
Describe steps/code to reproduce the behavior:
&lt;denchmark-code&gt;import onnx
# import onnxruntime as onnxrt
import onnxruntime.backend as backend 
# from onnx_tf import backend

import numpy as np

def _extract_value_info(arr, name):  # type: (np.ndarray, Text) -&gt; onnx.ValueInfoProto
    return onnx.helper.make_tensor_value_info(
        name=name,
        elem_type=onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype],
        shape=arr.shape)


def expect(node,  # type: onnx.NodeProto
           inputs,  # type: Sequence[np.ndarray]
           outputs,  # type: Sequence[np.ndarray]
           name,  # type: Text
           **kwargs  # type: Any
           ):  # type: (...) -&gt; None
    present_inputs = [x for x in node.input if (x != '')]
    present_outputs = [x for x in node.output if (x != '')]
    inputs_vi = [_extract_value_info(arr, arr_name)
                 for arr, arr_name in zip(inputs, present_inputs)]
    outputs_vi = [_extract_value_info(arr, arr_name)
                  for arr, arr_name in zip(outputs, present_outputs)]
    graph = onnx.helper.make_graph(
        nodes=[node],
        name=name,
        inputs=inputs_vi,
        outputs=outputs_vi)
    kwargs[str('producer_name')] = 'backend-test'
    model_def = onnx.helper.make_model(graph, **kwargs)
    onnx.checker.check_model(model_def)
    pm = backend.prepare(model_def)
    outs = list(pm.run(inputs))
    for ref_o, o in zip(outputs, outs):
        np.testing.assert_almost_equal(ref_o, o)

def export_conv_with_strides():  # type: () -&gt; None

        x = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 7, 5) input tensor
                        [5., 6., 7., 8., 9.],
                        [10., 11., 12., 13., 14.],
                        [15., 16., 17., 18., 19.],
                        [20., 21., 22., 23., 24.],
                        [25., 26., 27., 28., 29.],
                        [30., 31., 32., 33., 34.]]]]).astype(np.float32)
        W = np.array([[[[1., 1., 1.],  # (1, 1, 3, 3) tensor for convolution weights
                        [1., 1., 1.],
                        [1., 1., 1.]]]]).astype(np.float32)

        # Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)
        node_with_asymmetric_padding = onnx.helper.make_node(
            'Conv',
            inputs=['x', 'W'],
            outputs=['y'],
            kernel_shape=[3, 3],
            # pads=[1, 0, 1, 0],
            auto_pad="SAME_UPPER",
            strides=[2, 2],  # Default values for other attributes: dilations=[1, 1], groups=1
        )
        y_with_asymmetric_padding = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 7, 5) input tensor
                        [5., 6., 7., 8., 9.],
                        [10., 11., 12., 13., 14.],
                        [15., 16., 17., 18., 19.],
                        [20., 21., 22., 23., 24.],
                        [25., 26., 27., 28., 29.],
                        [30., 31., 32., 33., 34.]]]]).astype(np.float32)
        expect(node_with_asymmetric_padding, inputs=[x, W], outputs=[y_with_asymmetric_padding],
               name='test_conv_with_strides_and_asymmetric_padding')


if __name__ == "__main__":
    export_conv_with_strides()
&lt;/denchmark-code&gt;

Expected behavior
A clear and concise description of what you expected to happen.
output shape should be {1,1,7,5}
Screenshots
If applicable, add screenshots to help explain your problem.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='lynic' date='2019-04-05T21:21:06Z'>
		Hi &lt;denchmark-link:https://github.com/lynic&gt;@lynic&lt;/denchmark-link&gt;
 - is the output  correct ? It seems like it's a copy of  which I would imagine isn't correct given all 1's kernel ?
		</comment>
		<comment id='2' author='lynic' date='2019-04-10T03:14:23Z'>
		Hi &lt;denchmark-link:https://github.com/lynic&gt;@lynic&lt;/denchmark-link&gt;

It turns out the output shape generated by the runtime is quite valid according to the ONNX spec. For a precedent, please check out the test averagepool_2d_precomputed_same_upper in ONNX. It has auto_pad = "SAME_UPPER" and strides &gt; 1 (non-default) and the expected output dimension value contains a value lower than the corresponding input dimension value.
The rationale is like this -


When an autopad value of "SAME_UPPER" or "SAME_LOWER", pads are first computed so that the entire kernel can be accommodated even when operating on a border pixel ("SAME_UPPER"  and "SAME_LOWER" only differ in minor details dictating the distribution of pads to the front and end of the dimension)


The output shape is then computed as this: floor(((in_size - kernel + total_padding) / stride)) +1. For the special case of stride == 1, the output shape is the same as the input shape, not otherwise.


References:  &lt;denchmark-link:https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t&gt;https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t&lt;/denchmark-link&gt;
  (I particluary liked the third answer :))
Please re-open this issue if further clarification is required.
Also, please follow the answers here - &lt;denchmark-link:https://github.com/onnx/onnx/issues/1921&gt;onnx/onnx#1921&lt;/denchmark-link&gt;
. I raised this question after seeing the test mentioned above, but I later did some research to find that the test is quite valid, but it might be worthwhile to follow the explanation given by the Deep Learning experts.
cc: &lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>