<bug id='1135' author='HarryVancao' open_date='2019-05-30T19:31:52Z' closed_time='2019-12-17T18:22:13Z'>
	<summary>Handle the case where there is only one core in the machine.</summary>
	<description>
Describe the bug
I get a floating exception whenever I run a simple model that is just one convolution layer. I am running an onnx file generated by MATLAB.
I was able to view the stacktrace of the exception with gdb. It seems to crash in a function called MlasSgemmTryMultithread
Since this points to something relating to concurrency, I tried running the same model while setting the session_thread_pool_size parameter to 1. This allows the model to run to completion without problems.
Is this a known issue? I couldn't find any open issue relating to this crash.
System information

OS Platform and Distribution: Linux Debian 9
ONNX Runtime installed from: Binary
ONNX Runtime version: 0.4.0
Python version: 3.5

To Reproduce
Python script:
import onnxruntime as rt
import numpy as np
sessionOptions = rt.SessionOptions()
#sessionOptions.session_thread_pool_size = 1 # uncomment to successfully run.
session = rt.InferenceSession("convOnly.onnx", sess_options=sessionOptions);
input_name = session.get_inputs()[0].name
input = 10 * np.ones((1, 3, 5, 5), dtype = "single")
print("starting prediction: ")
prediction = session.run(None, {input_name : input} )
print(prediction)
I am attaching the script and the onnx file to this issue:
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3238462/floatingExceptionReproduction.zip&gt;floatingExceptionReproduction.zip&lt;/denchmark-link&gt;


I would expect the session to run inference without any crash. In addition, according to the Python API &lt;denchmark-link:https://microsoft.github.io/onnxruntime/api_summary.html#onnxruntime.SessionOptions.enable_sequential_execution&gt;documentation&lt;/denchmark-link&gt;
 I would think sequential execution is enabled by default.
	</description>
	<comments>
		<comment id='1' author='HarryVancao' date='2019-05-31T03:16:05Z'>
		Can't seem to reproduce this. See below. I've python 3.5.2 on Ubuntu 16.04 with onnxruntime-0.4.0.
&lt;denchmark-code&gt;pranav@pranav-HP-Z440-Workstation:~/floatingExceptionReproduction$ ls
convOnly.onnx  runConvOnly.py
pranav@pranav-HP-Z440-Workstation:~/floatingExceptionReproduction$ python runConvOnly.py
starting prediction:
[array([[1.0144626 , 0.9901579 , 0.999256  , 0.9900844 , 0.9937369 ,
        1.0026965 , 0.99749196, 1.0054768 , 1.0004812 , 0.97975045,
        0.9988375 , 0.9821757 , 1.0064821 , 1.0099634 , 1.0035425 ,
        1.0028011 , 1.0231298 , 1.0103304 , 0.9999087 , 0.98822653,
        1.0022982 , 0.995719  , 1.0056947 , 0.9950074 , 1.0064964 ,
        0.99475616, 1.0123382 , 1.0112967 , 1.003298  , 0.99383736,
        1.0020671 , 0.99677026]], dtype=float32)]
pranav@pranav-HP-Z440-Workstation:~/floatingExceptionReproduction$ cat runConvOnly.py
import onnxruntime as rt
import numpy as np


sessionOptions = rt.SessionOptions()
#sessionOptions.session_thread_pool_size = 1 # uncomment to prevent crash
session = rt.InferenceSession("convOnly.onnx", sess_options=sessionOptions);
input_name = session.get_inputs()[0].name
input = 10 * np.ones((1, 3, 5, 5), dtype = "single")
print("starting prediction: ")
prediction = session.run(None, {input_name : input} )
print(prediction)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='HarryVancao' date='2019-05-31T03:27:32Z'>
		Hi Pranav,
Thanks for investigating. However, that is a bit odd. I can try to investigate a little further on my end. Is there debugging step that I could try? One thing to note is that I am running Debian on a VM. Do you think that this could cause the issue? it is starting to feel like an environment related issue
Harry
		</comment>
		<comment id='3' author='HarryVancao' date='2019-05-31T05:40:50Z'>
		Something seems to be wrong in your VM. I tried on Debian 9 as well and it works just fine. See below.
&lt;denchmark-code&gt;pranav@debian:~$ uname -a
Linux debian 4.19.0-0.bpo.4-cloud-amd64 #1 SMP Debian 4.19.28-2~bpo9+1 (2019-03-27) x86_64 GNU/Linux

pranav@debian:~$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 9 (stretch)"
NAME="Debian GNU/Linux"
VERSION_ID="9"
VERSION="9 (stretch)"
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

pranav@debian:~$ python3 runConvOnly.py
starting prediction:
[array([[1.0144626 , 0.9901579 , 0.999256  , 0.9900844 , 0.9937369 ,
        1.0026965 , 0.99749196, 1.0054768 , 1.0004812 , 0.97975045,
        0.9988375 , 0.9821757 , 1.0064821 , 1.0099634 , 1.0035425 ,
        1.0028011 , 1.0231298 , 1.0103304 , 0.9999087 , 0.98822653,
        1.0022982 , 0.995719  , 1.0056947 , 0.9950074 , 1.0064964 ,
        0.99475616, 1.0123382 , 1.0112967 , 1.003298  , 0.99383736,
        1.0020671 , 0.99677026]], dtype=float32)]

pranav@debian:~$ ls
convOnly.onnx  floatingExceptionReproduction.zip  runConvOnly.py

pranav@debian:~$ cat runConvOnly.py
import onnxruntime as rt
import numpy as np


sessionOptions = rt.SessionOptions()
#sessionOptions.session_thread_pool_size = 1 # uncomment to prevent crash
session = rt.InferenceSession("convOnly.onnx", sess_options=sessionOptions);
input_name = session.get_inputs()[0].name
input = 10 * np.ones((1, 3, 5, 5), dtype = "single")
print("starting prediction: ")
prediction = session.run(None, {input_name : input} )
print(prediction)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='HarryVancao' date='2019-05-31T05:41:25Z'>
		Closing as per above comment.
		</comment>
		<comment id='5' author='HarryVancao' date='2019-05-31T12:50:28Z'>
		Hi Pranav,
Thanks for trying to reproduce again. As per your comment, I created a new VM running Debian 9 and it works now. Once difference that I noticed is that the new VM that I created has multiple CPU cores. Whereas my crashing VM only had one. I am speculating, but I this might be related.
I will let you know if I have more time to investigate the issue. But for now, lets keep this closed until I can lock down better reproduction.
Thanks,
Harry
		</comment>
		<comment id='6' author='HarryVancao' date='2019-05-31T16:52:02Z'>
		Hi  &lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;

I apologize for the continued comments in this issue. I am convinced that the floating point exception is caused by the VM having only one physical core available to it. I modified my existing VM to have 4 virtual processors (from 1) and inference runs without issue now. However, reverting back to 1 virtual processor restores the exception. This does seem to be a fairly rare condition to trigger the crash.
Please let me know if you are able to reproduce this exception.
Harry
		</comment>
		<comment id='7' author='HarryVancao' date='2019-05-31T20:31:47Z'>
		I am able to reproduce on a 1-core machine. Will follow up with a fix.
		</comment>
		<comment id='8' author='HarryVancao' date='2019-06-04T05:47:04Z'>
		&lt;denchmark-link:https://github.com/shschaefer&gt;@shschaefer&lt;/denchmark-link&gt;
 mentioned today that he'll be taking care of this as part of his threading related PR.
		</comment>
		<comment id='9' author='HarryVancao' date='2019-06-19T07:07:42Z'>
		This is being taken care of in this PR &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1197&gt;#1197&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='HarryVancao' date='2019-11-08T19:11:08Z'>
		Any update on this?
		</comment>
		<comment id='11' author='HarryVancao' date='2019-11-08T20:09:04Z'>
		This has already been resolved.
		</comment>
	</comments>
</bug>