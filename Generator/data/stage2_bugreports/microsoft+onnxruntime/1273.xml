<bug id='1273' author='maondra' open_date='2019-06-21T14:42:31Z' closed_time='2019-08-02T17:31:51Z'>
	<summary>Mobile GPU NVIDIA Quatro - performance issues</summary>
	<description>
ONNXRuntime doesn't use GPU for inference
System information


OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10


ONNX Runtime installed from (source or binary):
Pypi


ONNX Runtime version:
onnxruntime-gpu 0.4.0


Python version:
3.6


CUDA/cuDNN version: 9.1/7.1


GPU model and memory: NVIDIA Quatro P1000 and P2200



run your example (&lt;denchmark-link:https://pypi.org/project/onnxruntime-gpu/&gt;https://pypi.org/project/onnxruntime-gpu/&lt;/denchmark-link&gt;
) on the notebook with mobile GPU NVIDIA Quatro P1000 or P2200
Expected behavior
A clear and concise description of what you expected to happen.
Onnxruntime.get_device() -&gt; GPU but run on CPU
Expecting ONNXRuntime to run on GPU not CPU
Is it possible that ONNXRuntime evaluates to don't use GPU?
	</description>
	<comments>
		<comment id='1' author='maondra' date='2019-06-22T02:54:01Z'>
		Can you attach the model for debugging?
		</comment>
		<comment id='2' author='maondra' date='2019-07-03T21:01:10Z'>
		It should be possible to reproduce this issue using script from &lt;denchmark-link:https://pypi.org/project/onnxruntime-gpu/&gt;https://pypi.org/project/onnxruntime-gpu/&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='maondra' date='2019-07-08T07:03:39Z'>
		we should have a separate example for onnxruntime-gpu vs onnxruntime pypi project description.
currently CUDA/GPU support is for NN models, not traditional ML (e.g. scikit-learn)
you can confirm that the gpu package does indeed work by using a NN model from onnx model zoo.
		</comment>
		<comment id='4' author='maondra' date='2019-07-31T01:03:30Z'>
		&lt;denchmark-link:https://github.com/maondra&gt;@maondra&lt;/denchmark-link&gt;
 do you still require assistance on this issue? Here's an example of inferencing on GPU with tensorRT using azure machine learning: &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/docs/python/notebooks/onnx-inference-byoc-gpu-cpu-aks.ipynb&gt;https://github.com/microsoft/onnxruntime/blob/master/docs/python/notebooks/onnx-inference-byoc-gpu-cpu-aks.ipynb&lt;/denchmark-link&gt;

If you just want CUDA, you can install the gpu version from pypi.
		</comment>
	</comments>
</bug>