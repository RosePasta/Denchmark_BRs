<bug id='2699' author='xubin1994' open_date='2019-12-19T10:17:07Z' closed_time='2019-12-21T01:23:09Z'>
	<summary>the gather OP with some problem</summary>
	<description>
I have a model that uses torch.gather () API ,and the model runs well in pytorch.
warped_right_feature_map_a = torch.gather(right_input,dim=3, index=right_y_coordinate_a.long())
I successfully converted the model to an onnx file using torch.onnx.export () API, and the checkmodel is also verified,
import onnx
onnx_model = onnx.load("IresNet-1.onnx")
onnx.checker.check_model(onnx_model)
but when I run the onnx file using onnxruntime, the program crashes and without report message. If I comment out "torch.gather"" API, the program can run normally, In the torch.onnx doc，the "torch.gather" API is supported.So what is the problem?
	</description>
	<comments>
		<comment id='1' author='xubin1994' date='2019-12-19T19:59:06Z'>
		Hello,
Can you please share your onnx model (you can zip it up and upload) ?
Can you use the Gihub issue template to provide more information about the platform, processor architecture, OnnxRuntime version, CPU package vs CUDA package, etc. ?
		</comment>
		<comment id='2' author='xubin1994' date='2019-12-20T01:51:55Z'>
		
Hello,
Can you please share your onnx model (you can zip it up and upload) ?
Can you use the Gihub issue template to provide more information about the platform, processor architecture, OnnxRuntime version, CPU package vs CUDA package, etc. ?

platform:ubuntu 16.04
CPU：Intel(R) Xeon(R) CPU E5-2678 v3 @ 2.50GHz
GPU:2080TI
CUDA Version: 10.1
Driver Version: 430.40
OnnxRuntime version: onnxruntime-gpu ==1.1.0
my onnx model:&lt;denchmark-link:https://pan.baidu.com/s/1A9IjTCEySqKDNYxwylmong&gt;https://pan.baidu.com/s/1A9IjTCEySqKDNYxwylmong&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xubin1994' date='2019-12-21T01:23:09Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/71300525-838f9a80-234a-11ea-9db9-1784ccfb7bde.png&gt;&lt;/denchmark-link&gt;

The reason for the model run failure when you are exporting Gather is that the output is Gather is consumed by a OneHot operator where the process runs out of memory. (The model would fail even with the CPU ORT package - not just with the GPU package.)
Here is the spec of the OneHot op - &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/Operators.md#OneHot&gt;https://github.com/onnx/onnx/blob/master/docs/Operators.md#OneHot&lt;/denchmark-link&gt;

The indices input of the OneHot op is of shape {1, 32, 704, 1280} and the depth is 1280 too and axis = 3. So per spec the output shape of the OneHot op is: {1, 32, 704, 1280, 1280}. This is an INT64 tensor. Roughly this translates to about 1 * 32 * 704 * 1280 * 1280 * 8 bytes of memory. This is a lot of memory being requested and doesn't seem realistic.
So, even though the model is "valid" and the runtime has no bugs that the model run is triggering, the memory request is just too unrealistic. I think the model has not been exported correctly. Please check with the project/framework using which this ONNX model has been generated.
Closing this out as this is not an ORT issue. Please re-open for more clarifications.
P.S. For the next time, it would be great if you could please zip up the model file and just upload it here in the comments section.
		</comment>
	</comments>
</bug>