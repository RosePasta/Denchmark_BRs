<bug id='1669' author='24hours' open_date='2019-08-22T02:27:06Z' closed_time='2019-09-05T01:44:24Z'>
	<summary>Misleading error on TensorRT batching</summary>
	<description>
Describe the bug
When predicting with batch size &gt; 1, an error with message :
TRT enqueue failed: Set ORT_TRT_MAX_BATCH_SIZE environment variable to at least 16 is shown,
However, setting export ORT_TRT_MAX_BATCH_SIZE=16 will not fixed the issue and erorr continue to prompt.
Urgency
none
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
ONNX Runtime installed from (source or binary): source
ONNX Runtime version:
Python version: 3.7
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: cuda 10, cudnn 7.3
GPU model and memory: GTX 1080

To Reproduce
Describe steps/code to reproduce the behavior:
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots
If applicable, add screenshots to help explain your problem.
Additional context
According to this line:



onnxruntime/onnxruntime/core/providers/tensorrt/tensorrt_execution_provider.cc


         Line 338
      in
      24d17f4






 const char* batch_env = getenv("ORT_TENSORRT_MAX_BATCH_SIZE"); 





user should run export ORT_TENSORRT_MAX_BATCH_SIZE=16 instead
	</description>
	<comments>
		<comment id='1' author='24hours' date='2019-08-23T20:21:05Z'>
		To confirm, you're saying the error message is wrong (TRT should be TENSORRT) but if you set ORT_TENSORRT_MAX_BATCH_SIZE, it does work?
		</comment>
		<comment id='2' author='24hours' date='2019-08-24T07:39:12Z'>
		yes.
		</comment>
		<comment id='3' author='24hours' date='2019-08-26T03:03:50Z'>
		Thanks.
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1687&gt;#1687&lt;/denchmark-link&gt;
 will fix it.
		</comment>
		<comment id='4' author='24hours' date='2019-09-05T01:44:24Z'>
		This is fixed.
		</comment>
	</comments>
</bug>