<bug id='612' author='xadupre' open_date='2019-03-13T15:33:45Z' closed_time='2019-04-09T17:55:32Z'>
	<summary>TfidfVectorizer fails with empty strings</summary>
	<description>
Describe the bug
I'm trying to write a converter for TfIdfVectorizer when analyser=='char'. I use the regular expression to split a sentance into characters with '.' (does not work) but '[^\n]' works. With the first one, I get the following error:
INVALID_ARGUMENT : Input shape must have either [C] or [B,C] dimensions where C &gt; 0 and B &gt; 0
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
ONNX Runtime installed from (source or binary): master branch from github
ONNX Runtime version: master branch from github
Python version: 3.7

To Reproduce

clone https://github.com/xadupre/sklearn-onnx/tree/tfc
run test test_model_tfidf_vectorizer11_dot in tests/test_SklearnTfidfVectorizerConverterChar.py

Expected behavior
onnxruntime should return outputs.
	</description>
	<comments>
		<comment id='1' author='xadupre' date='2019-03-13T17:05:28Z'>
		&lt;denchmark-link:https://github.com/xadupre&gt;@xadupre&lt;/denchmark-link&gt;
 There are few things I want you to note.


Current implementation of tokenexp is the opposite of what it should be. I have created a PR below.


If you want to split something into individual characters the Tokenizer spec provides for a char tokenization mode which is enabled when you supply a single empty string as separator.


The rest below I believe are python related issues.

Per spec Tokenizer accepts UTF-8 strings for both input, output and attributes. This applies as well to StringNormalizer and TfIdfVectorizer.  It looks like pybind11 will automatically convert python strings to utf-8, so it should be good.
Per spec the regex supported (or intended to be supported) is BRE
The above error message is a result of TypeAndShapeInference function failure. It expects input as  Tensor of strings with the aforementioned dimensions. Will check what our python expects.

		</comment>
		<comment id='2' author='xadupre' date='2019-03-13T22:26:15Z'>
		I have submitted &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/617&gt;#617&lt;/denchmark-link&gt;
 to address this.
		</comment>
		<comment id='3' author='xadupre' date='2019-03-14T17:07:30Z'>
		I tried but that does not fix the issue. StringNormalizer can handle empty strings, Tokenizer too, but not TfidfVectorizer. It should handle empty strings and return an empty results.
		</comment>
		<comment id='4' author='xadupre' date='2019-03-14T17:12:58Z'>
		Another issue maybe introduced with changes &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/617&gt;#617&lt;/denchmark-link&gt;
 is the following.
&lt;denchmark-code&gt;corpus = numpy.array([
        'This is the first document.',
        'This document is the second document.',
        ]).reshape((2, 1))
vect = TfidfVectorizer(ngram_range=(1, 1), norm=None,
                       analyzer='word', token_pattern=".{1,2}")
vect.fit(corpus.ravel())
pred = vect.transform(corpus.ravel())
model_onnx = convert_sklearn(vect, 'TfidfVectorizer',
                             [('input', StringTensorType([1, 1]))])
&lt;/denchmark-code&gt;

The ONNX pipeline includes a couple of nodes which produce the following outputs
&lt;denchmark-code&gt;-------------- normalized (StringNormalizer)
['this is the first document.']
-------------- tokenized (Tokenizer)
[['th' 'is' ' i' 's ' 'th' 'e ' 'fi' 'rs' 't ' 'do' 'cu' 'me' 'nt' '.']]
-------------- flattened (Flatten)
[['th' 'is' ' i' 's ' 'th' 'e ' 'fi' 'rs' 't ' 'do' 'cu' 'me' 'nt' '.']]
-------------- tfidfvectorizer (TfidfVectorizer with regex = '.{1,2}'
2019-03-14 18:08:44.0281985 [E:onnxruntime:InferenceSession, inference_session.cc:541 onnxruntime::InferenceSession::Impl::Initialize] Exception during initialization: onnxruntime\onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc:371 onnxruntime::TfIdfVectorizer::TfIdfVectorizer (before_insert + ngrams) == impl_-&gt;str_set_.size() was false. poll_strings duplicate 1-grams detected
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='xadupre' date='2019-03-15T16:58:04Z'>
		I found two bugs while converting a TfIdfVectorizer. The first one comes from scikit-learn which creates ambiguities when trying to retrieve n-grams if the tokens contain spaces. The other bug comes from the tokenizer which loses one dimension if the expression to tokenize is empty. The code is the following:
&lt;denchmark-code&gt;corpus = np.array([
        'This is the first document.',
        'This document is the second document.',
        'And this is the third one.',
        ' ',
        ]).reshape((4, 1))
vect = TfidfVectorizer(ngram_range=(1, 2), norm=None)
vect.fit(corpus.ravel())
pred = vect.transform(corpus.ravel())
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import StringTensorType

model_onnx = convert_sklearn(vect, 'TfidfVectorizer',
                             [('input', StringTensorType([1, 1]))])
&lt;/denchmark-code&gt;

I then checked the intermediate outputs for two strings, an non-empty string and an empty string (differencs in yellow). When the string is empty, the function return an null dimension.
&lt;denchmark-code&gt;  if (max_tokens == 0) {
    output_dims.push_back(0);
    TensorShape output_shape(output_dims);
    ctx-&gt;Output(0, output_shape);
    return Status::OK();
  }
&lt;/denchmark-code&gt;

But if mark_ is true, we would expect to have that string surrounded by start/end markers (this code is after the first one).
&lt;denchmark-code&gt;  if (mark_) {
    max_tokens += 2;  // Start/end markers as separate tokens
  }
&lt;/denchmark-code&gt;

I compared the results for the intermediate nodes (graph is below)
with input: [array(['And this is the third one.'], dtype='&lt;U37')]
&lt;denchmark-code&gt;-&gt; node 'normalized'
['and this is the third one.']
-&gt; node 'tokenized'
[['and' 'this' 'is' 'the' 'third' 'one']]
-&gt; node 'flattened'
[['and' 'this' 'is' 'the' 'third' 'one']]
-&gt; node 'variable'
[[1.9162908 1.9162908 0.        0.        0.        0.        1.2231436
  1.2231436 1.9162908 0.        0.        1.2231436 0.        0.
  1.9162908 1.9162908 1.9162908 1.2231436 0.        1.5108256]]
&lt;/denchmark-code&gt;

with input: [array([' '], dtype='&lt;U37')]
&lt;denchmark-code&gt;-&gt; node 'normalized'
[' ']
-&gt; node 'tokenized'
2019-03-15 17:37:20.3345248 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes
[]
-&gt; node 'flattened'
2019-03-15 17:37:20.3356089 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes
2019-03-15 17:37:20.3356310 [W:onnxruntime:Default, bfc_arena.cc:201 onnxruntime::BFCArena::AllocateRawInternal] tried to allocate 0 bytes
[]
-&gt; node 'variable'
&lt;/denchmark-code&gt;

for the following pipeline...
&lt;denchmark-link:https://user-images.githubusercontent.com/22452781/54448320-b9647a80-474b-11e9-9242-e3ac0d70ba5f.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='xadupre' date='2019-03-22T23:03:00Z'>
		&lt;denchmark-link:https://github.com/xadupre&gt;@xadupre&lt;/denchmark-link&gt;
 I think we discussed this sufficiently with Wei-Sheng. Pls, talk to me if you feel there still an issue with regards to empty strings.
		</comment>
		<comment id='7' author='xadupre' date='2019-03-25T22:17:08Z'>
		Supporting empty input shape in TfidfVectorizer looks reasonable to me. For example, if we tokenized a document (i.e., a very long string) into a [C]-tensor, but unfortunately remove all the elements in StringNormalizer, and finally TfidfVectorizer gets [0]-tensor. What should be the TFIDF-representation of this document? I'd guess that representation is a [D]-vector where all elements are zeros, because the count of every vocabulary is 0. Note that D is the dictionary's max index in TfidfVectorizer.
		</comment>
	</comments>
</bug>