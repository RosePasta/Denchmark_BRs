<bug id='1402' author='joyIntel' open_date='2019-07-13T23:43:40Z' closed_time='2019-07-23T01:22:42Z'>
	<summary>Performance Test Issue With gt/Nuphar Branch on Windows &amp; Linux build Fails</summary>
	<description>
I am working on Nuphar EP. So I am using the gt/Nuphar branch. I have two separate issues.
&lt;denchmark-h:h1&gt;Issue 1 (Windows)&lt;/denchmark-h&gt;

In Windows, there is no building issues with it. But when I am using onnxruntime_perf_test for an onnx model, it is not generating the output.
Steps to reproduce -
git clone -b gt/Nuphar https://github.com/microsoft/onnxruntime.git
cd onnxruntime
git submodule update –-init -–recursive
build.bat --use_tvm --use_llvm --llvm_path=C:\llvm\install\path\lib\cmake\llvm --use_mklml --use_nuphar --config=Release
All the .exe files are successfully generated inside build folder.
cd build\Windows\Release\Release
onnxruntime_perf_test -m times -r 100 -e nuphar &lt;onnx model&gt; result.txt
I am getting an warning message -
onnxruntime\cmake\external\tvm\src\arithmetic\int_set.cc:535: cannot evaluate set type Cast
and result.txt not generated.
I think &lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 is the best person to answer this question.
&lt;denchmark-h:h1&gt;Issue 2 (Linux)&lt;/denchmark-h&gt;

In Linux, the build fails with following error (image attached)-
git clone -b gt/Nuphar https://github.com/microsoft/onnxruntime.git
cd onnxruntime
git submodule update –-init -–recursive
./build.sh --use_tvm --use_llvm --llvm_path=/llvm/install/path/lib/cmake/llvm --use_mklml --use_nuphar --config=Release
&lt;denchmark-link:https://user-images.githubusercontent.com/39498824/61177547-0d159200-a58d-11e9-82d6-b23c8bf6266c.png&gt;&lt;/denchmark-link&gt;

I understand that  is a development branch. So, there could be issues. But I think some recent changes are causing these issues. Earlier, I used this branch and did not face any issue. That time I used  commit Id. But I can not revert back also, because that commit no longer exists.
&lt;denchmark-link:https://user-images.githubusercontent.com/39498824/61177589-439fdc80-a58e-11e9-96ce-bb3b2579cf66.png&gt;&lt;/denchmark-link&gt;

`
	</description>
	<comments>
		<comment id='1' author='joyIntel' date='2019-07-14T00:09:32Z'>
		The second one is because your png lib version is too low. At this time point, you should either uninstall it or upgrade it to png 1.6. I will remove this source file from onnxruntime project.  I already sent out a pull request for it. And, please use Ubuntu 16.04 and our official install scripts to prepare the dev environment, so that you won't hit the problem again.
See: &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/tools/ci_build/github/linux/docker/scripts/install_ubuntu.sh&gt;https://github.com/microsoft/onnxruntime/blob/master/tools/ci_build/github/linux/docker/scripts/install_ubuntu.sh&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='joyIntel' date='2019-07-14T04:16:46Z'>
		Thanks &lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 for replying. Issue 2 is resolved now. I had to remove  from  so that warnings are not treated as error.
I still need help for Issue 1.
		</comment>
		<comment id='3' author='joyIntel' date='2019-07-14T05:27:46Z'>
		Thanks for your interest in Nuphar EP. Issue 1 is a warning from TVM when running int set analysis:
&lt;denchmark-code&gt;virtual R VisitExpr_(const Cast* op, Args... args) EXPR_FUNCTOR_DEFAULT;

  IntSet VisitExprDefault_(const Node* op, const Expr&amp; e) final {
    LOG(WARNING) &lt;&lt; "cannot evaluate set type " &lt;&lt; e-&gt;type_key();
    return IntSet::everything();
  }
&lt;/denchmark-code&gt;

The reason is that it tries to find out the integer range of an expression, and Cast would return anything. IMO this is a normal behavior and should not be warning. We will try to suppress it in our forked TVM repo.
		</comment>
		<comment id='4' author='joyIntel' date='2019-07-14T05:32:27Z'>
		I have a relevant patch somewhere locally on my disk. Let me try to apply it.
		</comment>
		<comment id='5' author='joyIntel' date='2019-07-14T23:29:05Z'>
		Issue 1 is not just related to that warning. When I run any onnx model with the onnxruntime_perf_test binary, no output file is generated (Windows).
The command is onnxruntime_perf_test -m times -r 100 -e nuphar &lt;onnx model&gt; result.txt
In Linux, I get the same warning but result.txt is generated. But in Windows, it is not.
		</comment>
		<comment id='6' author='joyIntel' date='2019-07-16T06:14:42Z'>
		Sounds like a crash, and may not related to the warning message. Could you share the model, preferably with test data if possible?
		</comment>
		<comment id='7' author='joyIntel' date='2019-07-16T17:54:46Z'>
		Hey &lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
,
I am trying to share my data and model. But it is above 10 MB. GitHub is not allowing me to do so. Could you refer me any other way of sharing ? I am using resnet50 model by the way.
		</comment>
		<comment id='8' author='joyIntel' date='2019-07-16T22:03:31Z'>
		It is possible that there are some code path allocated unaligned buffer which triggered AV in memory access. There's a flag in NupharProviderInfo to allow unaligned data, and it's set to true in onnx_test_runner/python binding, false in onnxruntime_perf_test. Please try onnx_test_runner or python if you continue to hit this issue in onnxruntime_perf_test.
		</comment>
		<comment id='9' author='joyIntel' date='2019-07-16T22:31:07Z'>
		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 , I am using this model and data - &lt;denchmark-link:https://github.com/onnx/models/tree/master/vision/classification/resnet/resnet50&gt;https://github.com/onnx/models/tree/master/vision/classification/resnet/resnet50&lt;/denchmark-link&gt;
.  I do not think using onnx_test_runner would serve my purpose. As I mentioned before, it is working in Linux.
		</comment>
		<comment id='10' author='joyIntel' date='2019-07-16T22:37:43Z'>
		I hit similar problems before both on Linux and Windows. Since it is caused by unaligned memory access, it's up to the allocator's mood.
As a temporary workaround, if you have to use onnxruntime_perf_test, you may change &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/gt/Nuphar/onnxruntime/test/perftest/ort_test_session.cc#L53&gt;this line&lt;/denchmark-link&gt;
 to allow unaligned buffer.
		</comment>
		<comment id='11' author='joyIntel' date='2019-07-16T23:21:10Z'>
		That works. Thanks a lot &lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='joyIntel' date='2019-07-17T08:09:42Z'>
		Hi Ke Deng, if the buffer is passed in from outside, then there is not alignment guarantee.
		</comment>
		<comment id='13' author='joyIntel' date='2019-07-17T18:19:49Z'>
		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 , I have a general query. Why the performance of Nuphar is pretty stable with increasing batch size ? This is not true for all other EPs. For other Execution Providers, when the batch size increases, the  increases.
		</comment>
		<comment id='14' author='joyIntel' date='2019-07-17T21:17:08Z'>
		Do you mean running a model with batch &gt; 1? Or just running multiple inference threads simultaneously? For the resnet50 model, I'd expect the majority of computation spent in convolution, which is not what currently Nuphar is optimized for because of nGraph/mkldnn/cpu(mlas) provider has a good coverage already. Conv ops uses other providers. Given that, I don't have a good explanation of the performance diff you observed.
		</comment>
		<comment id='15' author='joyIntel' date='2019-07-17T21:22:08Z'>
		&lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 When I do this - 



Resnet50
Default
ngraph
nuphar




BS1
17.67s
1.28s
7.24s


BS2
32.89s
1.94s
7.27s


BS32
516s
26.31s
7.35s



So, Nuphar is pretty stable with increasing batch size when other EPs are not.
		</comment>
		<comment id='16' author='joyIntel' date='2019-07-17T21:31:25Z'>
		OK I guess the model with batch has some issues in its shape inference. Nuphar execution provider relies on ONNX shape inference, and if graph.value_info still has batch == 1, you'll still run as if batch ==1.
Please use the &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/gt/Nuphar/onnxruntime/core/providers/nuphar/scripts/symbolic_shape_infer.py&gt;symbolic_shape_infer.py&lt;/denchmark-link&gt;
 to update the value_info in the model and try again.
Or, maybe just use &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#running-shape-inference-on-an-onnx-model&gt;ONNX shape inference&lt;/denchmark-link&gt;
 would be fine.
		</comment>
		<comment id='17' author='joyIntel' date='2019-07-17T21:34:37Z'>
		So, you are saying even when I am running BS32, nuphar is assuming as if batch == 1 ??
		</comment>
		<comment id='18' author='joyIntel' date='2019-07-17T21:57:30Z'>
		Nuphar's dependency on ONNX shape inference is stated in &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/gt/Nuphar/docs/execution_providers/Nuphar-ExecutionProvider.md&gt;its doc&lt;/denchmark-link&gt;
. Since it's compiler-based, it is critical to have the right model to statically declare the shapes upfront. Shape of data arrived at runtime is ignored, and we may add some checks at runtime when shapes mismatch.
		</comment>
		<comment id='19' author='joyIntel' date='2019-07-19T21:23:20Z'>
		Hey &lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 , I am very new using onnx. I can understand that I have to change here -
&lt;denchmark-link:https://github.com/onnx/onnx/blob/v1.5.0/onnx/helper.py#L290&gt;onnx.helper.make_tensor_value_info&lt;/denchmark-link&gt;
 to give a new batch size. I guess shape is "NCHW" means a data whose layout is (batch_size, channel, height, width). So, I just have to change the . But I am not sure how to do that.
		</comment>
		<comment id='20' author='joyIntel' date='2019-07-22T21:10:01Z'>
		Simply put a string in the shape list/tuple passed to make_tensor_value_info would be fine, like &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/onnx/test/shape_inference_test.py#L720-L726&gt;this&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='21' author='joyIntel' date='2019-07-22T23:32:12Z'>
		Hey &lt;denchmark-link:https://github.com/KeDengMS&gt;@KeDengMS&lt;/denchmark-link&gt;
 , I have attached the graph of my model (Resnet50). In every node, the shape information is present. So, I do not think it is caused by Nuphar Shape Inference Dependency.
I have the directory structure as mentioned like &lt;denchmark-link:https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/test/perftest&gt;this&lt;/denchmark-link&gt;

Still in these two following cases , I am getting the almost same result for Nuphar EP -
1&gt; onnxruntime_perf_test -m times -r 100 -e nuphar Models\FinalResnet50\converted_tf_fp32\BS1\converted_TF_FP32_Resnet50_withshapes.onnx result.txt
2&gt; onnxruntime_perf_test -m times -r 100 -e nuphar Models\FinalResnet50\converted_tf_fp32\ BS32\converted_TF_FP32_Resnet50_withshapes.onnx result.txt



Resnet50
Default
ngraph
nuphar




BS1
 17.67s
1.28s
7.18s


BS32
516s
26.31s
7.23s



In the first one, the batch size is 1, and in the second one the batch size is 32
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3419463/Resnet50_Graph.txt&gt;Resnet50_Graph.txt&lt;/denchmark-link&gt;

I am moving it to a new issue because the actual issues have been resolved here.
		</comment>
	</comments>
</bug>