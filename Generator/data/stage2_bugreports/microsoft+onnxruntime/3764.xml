<bug id='3764' author='jignparm' open_date='2020-04-30T11:34:32Z' closed_time='2020-05-01T21:36:22Z'>
	<summary>ORT throws GEMM error, but there's no GEMM operator in  model</summary>
	<description>
Describe the bug
Attached a model with 3 ops: MatMul, Mul and Add. It should run successfully, but ORT errors out with a message, referencing a gemm_0 op that doesn't exist in the model.
Where is gemm_0 coming from? Looks like it's generated on the fly, but that's not  correct because Gemm cannot handle these specific tensor shapes (whereas MatMul, Mul and Add should handle the shapes successfully).
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/4557884/matmul.zip&gt;matmul.zip&lt;/denchmark-link&gt;

======== GRAPH ===========
graph tf2onnx (
  %input2:0[FLOAT, 1x4]
  %input1:0[FLOAT, 3x1]
  %input:0[FLOAT, 3x3]
) initializers (
  %Const_1:0[FLOAT, scalar]
) {
  %MatMul:0 = MatMul(%input:0, %input1:0)
  %Mul_1:0 = Mul(%Const_1:0, %input2:0)
  %output:0 = Add(%MatMul:0, %Mul_1:0)
  return %output:0
}
&lt;denchmark-link:https://user-images.githubusercontent.com/13698702/80705768-865b8280-8ad6-11ea-98d6-b38521805c26.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/13698702/80704056-6e363400-8ad3-11ea-89b1-37edbd50a10d.png&gt;&lt;/denchmark-link&gt;

Urgency
Bug. A user issue is blocked.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): All OS
ONNX Runtime installed from (source or binary): 1.2.0 from Pypi
ONNX Runtime version: 1.2.0
Python version: 3.7
Visual Studio version (if applicable): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A

To Reproduce
Unzip model, and load it using ORT (see screenshot)
Expected behavior
Should be able to add and multiply without error.
Screenshots
See above
Additional context
Add any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.
	</description>
	<comments>
		<comment id='1' author='jignparm' date='2020-04-30T18:23:12Z'>
		&lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md&gt;https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md&lt;/denchmark-link&gt;

GraphOptimizationLevel::ORT_DISABLE_ALL -&gt; Disables all optimizations
Please disable it and try again.
		</comment>
		<comment id='2' author='jignparm' date='2020-04-30T20:04:14Z'>
		GraphOptimizationLevel::ORT_DISABLE_ALL runs OK.
So it sounds like an optimizer error? It probably  needs to check for shapes before fusing ops.
		</comment>
		<comment id='3' author='jignparm' date='2020-04-30T20:34:13Z'>
		Right. We have many many optimizers to be fixed.  Feel free to patch it by yourself.
		</comment>
		<comment id='4' author='jignparm' date='2020-04-30T21:54:25Z'>
		Thanks &lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
. I'll try a patch in tf2onnx to avoid this condition during model conversion -- to  unblock the user quickly.

We have many many optimizers to be fixed

Seems like a trade-off between speed and fix...
		</comment>
	</comments>
</bug>