<bug id='1745' author='qiuhaining' open_date='2019-09-02T06:12:07Z' closed_time='2019-09-09T18:16:39Z'>
	<summary>Floating point exception（core dumped）</summary>
	<description>
Describe the bug
"pred_onnx = session.run([hm,hmp,hw,reg], {"input.1": img_data})".
the bug occurs in this step.
the error is "Floating point exception（core dumped）
Urgency
the deadline is  the after tommorrow
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04(virtual machine)
ONNX Runtime installed from (source or binary):binary(pip install onnxruntime)
ONNX Runtime version:0.5
Python version:3.5
Visual Studio version (if applicable):no
GCC/Compiler version (if compiling from source):5.4.0
CUDA/cuDNN version: just CPU
GPU model and memory:just CPU

To Reproduce
Describe steps/code to reproduce the behavior:
if success,four outputs(hm,hmp,hw,reg) can be get!
Expected behavior
A clear and concise description of what you expected to happen.

this is my code
``
&lt;denchmark-link:https://user-images.githubusercontent.com/23293902/64093692-f79e3680-cd8b-11e9-8892-1004611b00fa.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/23293902/64093705-05ec5280-cd8c-11e9-80fb-aba15e83b65b.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/23293902/64093728-13a1d800-cd8c-11e9-82ac-cbda18fa9b4f.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='qiuhaining' date='2019-09-04T02:13:00Z'>
		Hello,
Thanks for reporting this. Some questions/requests-


What was the opset version of the original onnx model ? (Was it 9?)  Did the original model (before conversion to newer opset) work in ORT?


How was the onnx model obtained ? Was it converted/exported from another framework ? This will help identify the place to open a bug if the model is actually invalid.


Please attach the model if it can be shared.


Thanks!
		</comment>
		<comment id='2' author='qiuhaining' date='2019-09-04T02:24:27Z'>
		
Hello,
Thanks for reporting this. Some questions/requests-

What was the opset version of the original onnx model ? (Was it 9?)  Did the original model (before conversion to newer opset) work in ORT?
How was the onnx model obtained ? Was it converted/exported from another framework ? This will help identify the place to open a bug if the model is actually invalid.
Please attach the model if it can be shared.

Thanks!

Hello
1、The opset version of the original onnx model is 9.
2、The onnx model was converted by pytorch 1.0
3、I guess the model is OK  because it can be used successfully with onnxruntime in another linux OS (not VMware Workstation) .
4、But i cannot use onnxruntime-gpu ,my cuda version is 8.0
		</comment>
		<comment id='3' author='qiuhaining' date='2019-09-04T02:36:14Z'>
		Thanks for the reply.
Can you please share your opset 9 model (and if you have it opset 10 model too) ?
Is the issue using onnxrunime-gpu ? It says in the initial post that the issue is for "just cpu".? Is the model working now for CPU?
You need CUDA 10.1 to use 0.5 onnxruntime-gpu on Linux....
		</comment>
		<comment id='4' author='qiuhaining' date='2019-09-04T05:53:46Z'>
		
Thanks for the reply.
Can you please share your opset 9 model (and if you have it opset 10 model too) ?
Is the issue using onnxrunime-gpu ? It says in the initial post that the issue is for "just cpu".? Is the model working now for CPU?
You need CUDA 10.1 to use 0.5 onnxruntime-gpu on Linux....

Here is my onnx model. I found the model was consumed long time in another linux OS（not VMware Workstation） with CPU(0.7s per image)，so，i want to accelerate it using onnxruntime-gpu
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3572878/multivehiclesDet_768_448_hourglass.zip&gt;multivehiclesDet_768_448_hourglass.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='qiuhaining' date='2019-09-04T21:29:17Z'>
		Thanks - can you also attach the py code in the zip please? Its hard to re-create from the screenshot.
		</comment>
		<comment id='6' author='qiuhaining' date='2019-09-04T21:37:18Z'>
		Btw - I fed the model random inputs of the right shape and it seemed to work fine on Windows 10 using ORT 0.5. I think on Linux, it should be the same. I think you just need to update your CUDA to 10.1 and try running the model. Below is my trial script -
&lt;denchmark-code&gt;import onnxruntime as rt
import numpy

sess = rt.InferenceSession('C:\\Users\\hasesh\\Desktop\\hourglass.onnx')
input_name = sess.get_inputs()[0].name
X = numpy.random.random((1, 3, 448, 768)).astype(numpy.float32)
pred_onnx = sess.run(None, {input_name: X})
print (pred_onnx)
&lt;/denchmark-code&gt;

EDIT: It works fine with both CPU and GPU 0.5 ORT packages
		</comment>
		<comment id='7' author='qiuhaining' date='2019-09-05T21:56:03Z'>
		Hi &lt;denchmark-link:https://github.com/qiuhaining&gt;@qiuhaining&lt;/denchmark-link&gt;
 - Were you able to verify with CUDA 10.1 on Linux ?
		</comment>
		<comment id='8' author='qiuhaining' date='2019-09-09T18:16:39Z'>
		Closing this as the model seems to work with the right CUDA version with ORT 0.5. Please try with the right CUDA version (This is documented under the  in &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/README.md&gt;https://github.com/microsoft/onnxruntime/blob/master/README.md&lt;/denchmark-link&gt;
). If there are any more issues, please re-open the issue. Thanks!
		</comment>
		<comment id='9' author='qiuhaining' date='2019-09-10T01:28:54Z'>
		
Btw - I fed the model random inputs of the right shape and it seemed to work fine on Windows 10 using ORT 0.5. I think on Linux, it should be the same. I think you just need to update your CUDA to 10.1 and try running the model. Below is my trial script -
import onnxruntime as rt
import numpy

sess = rt.InferenceSession('C:\\Users\\hasesh\\Desktop\\hourglass.onnx')
input_name = sess.get_inputs()[0].name
X = numpy.random.random((1, 3, 448, 768)).astype(numpy.float32)
pred_onnx = sess.run(None, {input_name: X})
print (pred_onnx)

EDIT: It works fine with both CPU and GPU 0.5 ORT packages
thank you very much!!! I try it again!

		</comment>
	</comments>
</bug>