<bug id='2878' author='danielphil' open_date='2020-01-21T15:15:24Z' closed_time='2020-02-21T21:13:04Z'>
	<summary>Providing fewer tensor inputs to Java API than model declares as inputs</summary>
	<description>
If an ONNX model declares 10 inputs, is it valid to provide 9 instead?
I've been using the Java API and found that OrtSession.run() will accept fewer inputs in the input map than the model requires. This results in a crash in Java_ai_onnxruntime_OrtSession_run() (see java/src/main/native/ai_onnxruntime_OrtSession.c) when building the inputNames array because the for loop uses the number of inputs declared by the model as the limit for the loop instead of the number of elements in inputNamesArr.
	</description>
	<comments>
		<comment id='1' author='danielphil' date='2020-01-21T19:01:08Z'>
		Ooops. Line 213 in OrtSession.java should use inputNamesArray.length and outputNamesArray.length rather than numInputs and numOutputs. I'll make up a PR.
		</comment>
		<comment id='2' author='danielphil' date='2020-01-21T19:03:10Z'>
		I believe it is valid if you supply all the inputs required by the requested outputs, and that's the way the Java code is supposed to work (though the Java code can only validate that all the required inputs are supplied by calling the run method in the native code). I would appreciate someone more familiar with the ONNX spec confirming that though.
		</comment>
		<comment id='3' author='danielphil' date='2020-01-21T21:47:04Z'>
		Thanks for clarifying. ðŸ™‚ I was assuming that I'd need to provide all the inputs the model declares, but I forgot an input and got the crashing bug. I wanted to check this was the intended behavior though before I changed anything. Let me know if I can help out by creating a PR to fix.
		</comment>
		<comment id='4' author='danielphil' date='2020-01-24T02:24:24Z'>
		I have a patch for this, but I won't have time to write a unit test for it until next week which I'd like to do before making the PR. It's a one line fix you can see &lt;denchmark-link:https://github.com/Craigacp/onnxruntime/commit/ab31bcafbc9f20aa342f8dfaa4ee8c9bd66fd99e&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='5' author='danielphil' date='2020-02-03T15:49:31Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 after writing a unit test, it seems like onnx-runtime throws an error if I don't supply all the inputs even when they are not used by the requested outputs. I have a simple graph which accepts scalars a,b,c, and computes ab, bc, and ab + bc as separate outputs. When asking for the ab output I get an exception thrown out of the native runtime with the text:
&lt;denchmark-code&gt;2020-02-03 10:31:07.363937739 [E:onnxruntime:, sequential_executor.cc:183 Execute] Non-zero status code returned while running Mul node. Name:'bc' Status Message: /local/ExternalRepositories/onnxruntime/include/onnxruntime/core/framework/op_kernel.h:90 const T* onnxruntime::OpKernelContext::Input(int) const [with T = onnxruntime::Tensor] Missing Input: c:0
&lt;/denchmark-code&gt;

plus a stack trace. The test fails in Java on this &lt;denchmark-link:https://github.com/Craigacp/onnxruntime/blob/2b66aeb350f93487ab88c9d8ef35e28d693d2688/java/src/test/java/ai/onnxruntime/InferenceTest.java#L237&gt;line&lt;/denchmark-link&gt;
. The model is checked in &lt;denchmark-link:https://github.com/Craigacp/onnxruntime/blob/java-num-inputs-fix/onnxruntime/test/testdata/partial-inputs-test.onnx&gt;here&lt;/denchmark-link&gt;
. I built it using TF 1.15 and tf2onnx 1.5.4.
I wouldn't have expected it to work this way, is this a bug in the library, or did I misunderstand the spec? I can't see a specific test in either the C# or Python bindings which check this behaviour.
		</comment>
		<comment id='6' author='danielphil' date='2020-02-12T15:38:54Z'>
		&lt;denchmark-link:https://github.com/Craigacp&gt;@Craigacp&lt;/denchmark-link&gt;
 did you also test if an input is completely unused? (e.g. inputs: a, b output: c = a, i.e. b is unused)
		</comment>
		<comment id='7' author='danielphil' date='2020-02-12T16:26:34Z'>
		&lt;denchmark-link:https://github.com/mina-asham&gt;@mina-asham&lt;/denchmark-link&gt;
, that works. I made a smaller version of my test that accepts a,b,c and only outputs ab. There supplying c or not is irrelevant to if it executes. It's on the same branch.
		</comment>
		<comment id='8' author='danielphil' date='2020-02-13T15:27:26Z'>
		A little more investigation (i.e. turning on the execution plan printing) shows that it doesn't change the execution plan based upon the outputs requested as I thought.
&lt;denchmark-code&gt;[0] Mul (ab)
[1] Identity (ab_graph_outputs_Identity__4)
[2] Mul (bc)
[3] Add (abc)
Free ml-values: (5) ab_raw_output___3:0
[4] Identity (abc_graph_outputs_Identity__8)
Free ml-values: (6) abc_raw_output___7:0
[5] Identity (bc_graph_outputs_Identity__6)
Free ml-values: (3) bc_raw_output___5:0
&lt;/denchmark-code&gt;

It always uses the above execution plan no matter what outputs are requested, hence the failure. I can easily change the Java API to ensure all the inputs are required, but this still feels like an issue in the native library, as I would expect it to not do unnecessary potentially expensive computation.
		</comment>
		<comment id='9' author='danielphil' date='2020-02-13T21:02:36Z'>
		
A little more investigation (i.e. turning on the execution plan printing) shows that it doesn't change the execution plan based upon the outputs requested as I thought.
[0] Mul (ab)
[1] Identity (ab_graph_outputs_Identity__4)
[2] Mul (bc)
[3] Add (abc)
Free ml-values: (5) ab_raw_output___3:0
[4] Identity (abc_graph_outputs_Identity__8)
Free ml-values: (6) abc_raw_output___7:0
[5] Identity (bc_graph_outputs_Identity__6)
Free ml-values: (3) bc_raw_output___5:0

It always uses the above execution plan no matter what outputs are requested, hence the failure. I can easily change the Java API to ensure all the inputs are required, but this still feels like an issue in the native library, as I would expect it to not do unnecessary potentially expensive computation.

Thanks for the detailed analysis, that explains it, I think we can split this into two bits:

There is a bug in the Java code (the one you fixed on your branch), and that bug affects models with "unconnected" inputs (i.e. completely non-required inputs)
There is a potential bug in the native code where the request outputs are only used post evaluation to select specific outputs rather than limit the graph from the beginning

I think this issue was opened for the first point, I would suggest we submit that fix and open another issue for the native code that links to this one. Does that sound okay?
		</comment>
		<comment id='10' author='danielphil' date='2020-02-13T21:19:22Z'>
		Well it depends if the native library is working as intended or not. If it's not then the code I have in the branch is correct and when the native library is fixed everything will work. If it is working as intended then I think the Java API should check that all inputs are present before calling the native code.
		</comment>
		<comment id='11' author='danielphil' date='2020-02-21T17:05:02Z'>
		I've made a PR for the Java fix, I agree we should open another issue for the native library behaviour.
		</comment>
	</comments>
</bug>