<bug id='2700' author='pcaselles' open_date='2019-12-19T17:57:34Z' closed_time='2020-07-11T02:40:07Z'>
	<summary>Multiple Inferences in the same session - Cpp</summary>
	<description>
Describe the bug
I can call session.run(...) only once for each session, otherwise an error occur. It only happens with GPU enabled. The first inference of the session succesfuly works, after that the problem appears. If I create a new session for each inference, all works properly, but it's not the correct way of making multiple inferences. The same code in CPU (commenting the line for enableing GPU) succesfuly works.
Error
[E:onnxruntime:, sequential_executor.cc:165 onnxruntime::sequentialExecutor::Execute] Non-zero status code returned while running GatherND node.
Additional context
The main idea is to make multiple inferences in one session without creating it every time.
Expected behavior
I would expect to run succesfuly as it does in CPU mode.
System information

Windows 7
ONNX Runtime version: 1.0
CUDA/cuDNN version: 10.0
C++

To Reproduce
Ort::Env envv(ORT_LOGGING_LEVEL_WARNING, "test");
Ort::SessionOptions ses_options = Ort::SessionOptions();
ses_options.SetIntraOpNumThreads(1);
OrtSessionOptionsAppendExecutionProvider_CUDA(ses_options, 0);
ses_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);
Ort::Session sesi = Ort::Session(envv, config.path_model, ses_options);
auto mem_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
inputTensor = ...
auto output_tensors = sesi.Run(Ort::RunOptions{ nullptr }, input_node_names.data(), inputTensor, input_node_names.size(), output_node_names.data(), config.output_node_names.size());
auto output_tensors2 = sesi.Run(Ort::RunOptions{ nullptr }, input_node_names.data(), inputTensor, input_node_names.size(), output_node_names.data(), output_node_names.size());
	</description>
	<comments>
		<comment id='1' author='pcaselles' date='2019-12-19T20:42:48Z'>
		Can you please try this on Windows 10 ? Or you can share the model and I ll try running it with dummy data.
		</comment>
		<comment id='2' author='pcaselles' date='2019-12-21T17:55:32Z'>
		I've try it on windows 10 and the same problem has appeared. I've been using maskrcnn model. I generated this model from keras-onnx-master.
Here is the link to download it: &lt;denchmark-link:https://we.tl/t-r2Dw2bO4uZ&gt;https://we.tl/t-r2Dw2bO4uZ&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='pcaselles' date='2019-12-22T03:10:11Z'>
		Thanks. I ll take a look. Could you please try with the recent 1.1 release if you can ? I ll give it a shot when I get to it as well.
		</comment>
		<comment id='4' author='pcaselles' date='2019-12-23T23:17:25Z'>
		&lt;denchmark-link:https://github.com/wenbingl&gt;@wenbingl&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jiafatom&gt;@jiafatom&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/EmmaNingMS&gt;@EmmaNingMS&lt;/denchmark-link&gt;
 - Could you please investigate if there is a regression in keras-onnx-master wrt the Mask RCNN model ?
The model shared by the OP seem to have weird input/output shapes when compared with the Mask RCNN model shared a while ago to test against ORT.
OP's model -
&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/71384499-8c74ac00-2596-11ea-8ce2-50bf22b3a8c0.png&gt;&lt;/denchmark-link&gt;

Model shared by &lt;denchmark-link:https://github.com/jiafatom&gt;@jiafatom&lt;/denchmark-link&gt;
 -
&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/71384506-972f4100-2596-11ea-9a57-558a77dd557e.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/pcaselles&gt;@pcaselles&lt;/denchmark-link&gt;
 - How does your  tensor look like ? From the model you shared, it is expected to be of shape [N, 0, 0, 3] - this is basically an empty tensor. I am not sure how any inferencing works honestly given that the input is expected to be empty. In your first successful run, are the output labels correct ?
&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/71384556-d78ebf00-2596-11ea-898b-8f0ce6144ffb.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='pcaselles' date='2019-12-25T19:25:59Z'>
		I've linked a wrong model with wrong input tensors shape, I aplogise.
You can use the one uploaded in the modelzoo, or the one you provided in this post. It happens the same, in fact once I discovered this error, I've been using that model to be sure is well generated.
		</comment>
		<comment id='6' author='pcaselles' date='2019-12-26T03:58:52Z'>
		&lt;denchmark-link:https://github.com/pcaselles&gt;@pcaselles&lt;/denchmark-link&gt;
 - AFAIK, there is no keras Mask RCNN model in the model zoo. There is a Mask RCNN (from PyTorch), could you please provide a link to the exact model to avoid confusion ?
I was able to run the Mask RCNN model a couple of times provided after conversion by the keras-onnx folks using the same session instance. So the problem might be with the specific model you are referring to. So a link would help. Thanks.
		</comment>
		<comment id='7' author='pcaselles' date='2019-12-27T19:13:51Z'>
		You can use this model, the exact that is giving me problems: &lt;denchmark-link:https://we.tl/t-0jtXaTyxI3&gt;https://we.tl/t-0jtXaTyxI3&lt;/denchmark-link&gt;

This model is from tensorflow/keras using this: &lt;denchmark-link:https://github.com/onnx/keras-onnx&gt;https://github.com/onnx/keras-onnx&lt;/denchmark-link&gt;

Are you using gpu acceleration? What is more wird is that sometimes works and doesn't give any errors, but only few times.
		</comment>
		<comment id='8' author='pcaselles' date='2019-12-30T04:42:55Z'>
		Yes, I was using GPU acceleration. I ll try and see if I can run inferencing for about 10 times sequentially.
Is there a specific input that it errors out on ? If so, sharing that would help.
		</comment>
		<comment id='9' author='pcaselles' date='2020-01-02T08:37:37Z'>
		No, with different inputs always appear the same error.
		</comment>
		<comment id='10' author='pcaselles' date='2020-02-03T08:23:47Z'>
		I've used the new release of onnxruntime. I've seen an improvement, now the model can be inferenced in GPU, but with some issues:
After doing more than 300000 inferences we got more than 3% errors. Sometimes there are streaks, meaning that after doing 300 clean inferences suddenly almost 30  consecutive errors appear.
The error that shows up, is the same with the previous release, however now it only appear when fails, not always.
It's a normal behaviour?
		</comment>
		<comment id='11' author='pcaselles' date='2020-03-08T14:44:18Z'>
		Can you share your model link once again? I can't open it, please. &lt;denchmark-link:https://github.com/pcaselles&gt;@pcaselles&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='pcaselles' date='2020-07-03T04:58:01Z'>
		This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.
		</comment>
		<comment id='13' author='pcaselles' date='2020-07-11T02:39:37Z'>
		This issue has been automatically closed due to inactivity. Please reactivate if further support is needed.
		</comment>
	</comments>
</bug>