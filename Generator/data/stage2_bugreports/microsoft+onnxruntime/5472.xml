<bug id='5472' author='addisonklinke' open_date='2020-10-13T15:14:49Z' closed_time='2020-10-13T19:45:53Z'>
	<summary>PyTorch model 10x slower on CPU provider when exported with pre-trained weights</summary>
	<description>
Describe the bug
I ran a training in PyTorch and exported two version of my model to ONNX using torch.onnx.export() with

Randomly initialized weights
Pretrained weights loaded via model.load_state_dict()

When checking the inference speed on ORT's Python API, I noticed that the model with pre-trained weights is roughly 10x slower (7.7 vs. 75 ms) on the default CPU execution provider than the model with randomly initialized weights. When benchmarking inference in PyTorch (before the ONNX export), both the pretrained and randomly initialized models come in around 5 ms. Viewing the ONNX graphs in &lt;denchmark-link:https://github.com/lutzroeder/netron/&gt;Netron&lt;/denchmark-link&gt;
 and profiling detailed node times, I cannot spot any differences that would account for the 10x slowdown.
Urgency
None
System information

OS Platform and Distribution: Ubuntu 18.04
ONNX Runtime installed from: pip
ONNX Runtime version: 1.5.1
Python version: 3.6.11
Visual Studio version (if applicable): NA
GCC/Compiler version (if compiling from source): NA
CUDA/cuDNN version: NA
GPU model and memory: NA

To Reproduce

Pip install onnxruntime 1.5.1 and torch 1.6.0
Download and extract the attached zip file containing two ONNX models and the clocking Python script
Test and compare the two model versions

Sample command line output on Intel i7-8750H @ 2.2 GHz
&lt;denchmark-code&gt;$ python3 clock_speed.py -o model_pretrain.onnx
Clocking ORT inference for 1,000 iterations
Average inference 74.82 +/- 1.26 ms

$ python3 clock_speed.py -o model_init.onnx
Clocking ORT inference for 1,000 iterations
Average inference 7.74 +/- 0.51 ms
&lt;/denchmark-code&gt;

Expected behavior
Since the model architecture (as shown in Netron) and tensor data types (float32) are the same, the model inference speed should be roughly the same regardless of whether the weights are randomly initialized or pretrained.
Additional context
I also added the --profile --iterations 1 flags to the clock_speed.py script to save the JSON traces for a single iteration. These are included in the attached zip file as profile_init.json and profile_pretrain.json along with a comparison (profiles.csv) that was produced by running
&lt;denchmark-code&gt;python3 trace2csv profile_init.json profile.pretrain.json
&lt;/denchmark-code&gt;

Looking at the resulting CSV, I notice that 34 of the 54 convolutions have speeds within 10% of each other. The slowest node is 460 which I show highlighted in Netron in the following screenshots. The details of the operation look identical
Screenshots
&lt;denchmark-link:https://user-images.githubusercontent.com/39926682/95880693-0304b500-0d35-11eb-930a-93c6e001cf5a.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='addisonklinke' date='2020-10-13T16:38:16Z'>
		You're likely hitting a problem with denormals. PR &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/5391&gt;#5391&lt;/denchmark-link&gt;
 is addressing the problem. My understanding is that PyTorch should have the same slowdown unless torch.set_flush_denormal(true) is called.
		</comment>
		<comment id='2' author='addisonklinke' date='2020-10-13T19:45:52Z'>
		&lt;denchmark-link:https://github.com/tracysh&gt;@tracysh&lt;/denchmark-link&gt;
 Thank you for the pointer! I would have spent much longer debugging other potential causes without it. For others stumbling on this, you can use snippet below to determine whether your pretrained weights contain any denormals
&lt;denchmark-code&gt;import torch

checkpoint = torch.load('/path/to/pretrain.pth')
denormal = []
for k, v in checkpoint.items():
    denorms = (v != 0) &amp; (v.abs() &lt; 1.e-32)
    if denorms.sum() &gt; 0:
        denormal.append(k)
print(f'{len(denormal)} / {len(checkpoint.values())} denormal weights')
&lt;/denchmark-code&gt;

Following the suggestion on this PyTorch forum &lt;denchmark-link:https://discuss.pytorch.org/t/conv2d-is-very-slow-on-trained-weights-vs-random-weights/43377/2&gt;thread&lt;/denchmark-link&gt;
, I added  to the top of my ONNX export script and now the pretrained model is running at the expected inference speed
		</comment>
	</comments>
</bug>