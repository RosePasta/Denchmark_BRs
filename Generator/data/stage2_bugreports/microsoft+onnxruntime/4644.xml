<bug id='4644' author='PatriceVignola' open_date='2020-07-28T20:05:12Z' closed_time='2020-07-30T06:32:02Z'>
	<summary>Multinomial float16 is broken since a recent change to the Cast transformer</summary>
	<description>

Multinomial float16 throws an exception since a &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/4523&gt;recent change to the Cast transformer&lt;/denchmark-link&gt;


Run inference the following model: &lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/4990832/multinomial_float16.zip&gt;multinomial_float16.zip&lt;/denchmark-link&gt;

Expected behavior
The model completes inference without throwing an exception
Additional context
The change to the Cast transformer sets the output types of nodes who have a "dtype" attribute to float, but Multinomial is a special case that outputs only int32 or int64 tensors. Therefore, an exception is thrown during inference.
	</description>
	<comments>
		<comment id='1' author='PatriceVignola' date='2020-07-28T20:41:20Z'>
		Thanks, I ll take a look at this.
		</comment>
	</comments>
</bug>