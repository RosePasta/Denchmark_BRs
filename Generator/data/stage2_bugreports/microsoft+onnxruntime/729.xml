<bug id='729' author='lucienwang1009' open_date='2019-03-28T09:40:59Z' closed_time='2019-06-03T21:23:52Z'>
	<summary>Can't use parent graph's initializer in subgraphs</summary>
	<description>

ONNX IR version 4 relaxes the constraint that initializers must be a subset of graph's inputs: &lt;denchmark-link:https://github.com/onnx/onnx/pull/1718&gt;onnx/onnx#1718&lt;/denchmark-link&gt;
.
While in current onnxruntime, if a subgraph, e.g., the body-graph of If op, refers to an initializer of its parent graph and the initializer isn't amid graph's inputs, the runtime will screw up with a message:
INVALID_GRAPH : Load model from model.onnx failed:At top level graph without matching NodeArg that subgraph consumes. Name=const_1 Graph may not conform to the ONNX spec and contain initializers that are not graph inputs.
But if the initializer is in parent graph's inputs, everything will be fine.
Here is a sample model for your reference:
import onnx
from onnx import helper, numpy_helper
from onnx import AttributeProto, TensorProto, GraphProto
import numpy as np
import onnxruntime

# The protobuf definition can be found here:
# https://github.com/onnx/onnx/blob/master/onnx/onnx.proto


# Create one input (ValueInfoProto)
initializers = [
    onnx.helper.make_tensor(
     name='cond_1',
     data_type=onnx.TensorProto.BOOL,
     dims=[1],
     vals=[0]
    ),
    onnx.helper.make_tensor(
     name='const_1',
     data_type=onnx.TensorProto.FLOAT,
     dims=[2],
     vals=[1,2]
    ),
    onnx.helper.make_tensor(
     name='const_2',
     data_type=onnx.TensorProto.FLOAT,
     dims=[2],
     vals=[1,2]
    ),
]

# Create one output (ValueInfoProto)
Y = helper.make_tensor_value_info('output', TensorProto.FLOAT, [2])

# then graph
then_graph = helper.make_graph(
    [helper.make_node("Identity", inputs=["const_1"], outputs=["then_graph_out"], name="iden_1")],
    "then_graph",
    [],
    [helper.make_tensor_value_info("then_graph_out", TensorProto.FLOAT, [2])],
    []
)
else_graph = helper.make_graph(
    [helper.make_node("Identity", inputs=["const_2"], outputs=["else_graph_out"], name="iden_2")],
    "else_graph",
    [],
    [helper.make_tensor_value_info("else_graph_out", TensorProto.FLOAT, [2])],
    []
)
# If node
if_node = helper.make_node(
    "If",
    inputs=["cond_1"],
    outputs=["output"],
    else_branch=else_graph,
    then_branch=then_graph,
    name="if"
)

# Create the graph (GraphProto)
graph_def = helper.make_graph(
    [if_node],
    "test-model",
    [],
    [Y],
    initializers
)

# Create the model (ModelProto)
model_def = helper.make_model(graph_def,
                              producer_name='onnx-example')
onnx.save(model_def, "model.onnx")

sess = onnxruntime.InferenceSession("model.onnx")
result = sess.run(['output'], input_feed={})
print(result)
System information

ONNX Runtime version (you are using): 0.3.0

Describe the solution you'd like
According to ONNX document, subgraphs should be able to use initializers in its parent graph, no matter whether the initializer is the part of inputs.
Describe alternatives you've considered
Additional context
	</description>
	<comments>
		<comment id='1' author='lucienwang1009' date='2019-03-28T09:51:38Z'>
		The Graph constructor need to create a NodeArg for values in the initializer list. The current code assumes that just doing that for graph inputs is sufficient. We could call GetOrCreateNodeArg when iterating the initialziers and adding to the name_to_initial_tensor_ map.
&lt;denchmark-code&gt;// Copy initial tensors to a map.
for (auto&amp; tensor : graph_proto_-&gt;initializer()) {
  name_to_initial_tensor_[tensor.name()] = &amp;tensor;
}

// Collect all node arg name, type, shape information in the graph.
// type/shape information will be assigned to each node arg when going
// thru all nodes later.
for (auto&amp; graph_input : graph_proto_-&gt;input()) {
  if (graph_input.has_name() &amp;&amp; graph_input.has_type()) {
    name_to_type_map[graph_input.name()] = graph_input.type();
    // always create a NodeArg for graph input in case its from an initializer
    GetOrCreateNodeArg(graph_input.name(), &amp;graph_input.type());
  }
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='lucienwang1009' date='2019-03-28T10:04:44Z'>
		
The Graph constructor need to create a NodeArg for values in the initializer list. The current code assumes that just doing that for graph inputs is sufficient. We could call GetOrCreateNodeArg when iterating the initialziers and adding to the name_to_initial_tensor_ map.
// Copy initial tensors to a map.
for (auto&amp; tensor : graph_proto_-&gt;initializer()) {
  name_to_initial_tensor_[tensor.name()] = &amp;tensor;
}

// Collect all node arg name, type, shape information in the graph.
// type/shape information will be assigned to each node arg when going
// thru all nodes later.
for (auto&amp; graph_input : graph_proto_-&gt;input()) {
  if (graph_input.has_name() &amp;&amp; graph_input.has_type()) {
    name_to_type_map[graph_input.name()] = graph_input.type();
    // always create a NodeArg for graph input in case its from an initializer
    GetOrCreateNodeArg(graph_input.name(), &amp;graph_input.type());
  }
}


Thanks for quick action! This bug blocked tf2onnx upgrade to IR version 4.
		</comment>
		<comment id='3' author='lucienwang1009' date='2019-04-08T02:56:18Z'>
		New error:
RuntimeError: [ONNXRuntimeError] : 1 : GENERAL ERROR : Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/node_index_info.cc:82 onnxruntime::NodeIndexInfo::Init(const TValidNodes&amp;, onnxruntime::NodeIndex, const onnxruntime::MLValueNameIdxMap&amp;)::&lt;lambda(const onnxruntime::NodeArg&amp;, bool)&gt; [with TValidNodes = onnxruntime::ValidNodes&lt;const std::vector&lt;const onnxruntime::Node&gt; &gt;] status.IsOK() was false. Could not find MLValue with name 'const_1'*
		</comment>
		<comment id='4' author='lucienwang1009' date='2019-04-18T07:30:20Z'>
		This should be related with two issues:

One should be known issue in https://github.com/Microsoft/onnxruntime/blob/master/onnxruntime/core/optimizer/optimizer_execution_frame.cc#L53. It does not support control flow ops by design, @weixingzhang
One may be a bug in constant folding https://github.com/Microsoft/onnxruntime/blob/master/onnxruntime/core/optimizer/constant_folding.cc#L26. I believe that for constant folding it should use OpKernelContextInternal. @weixingzhang @kkaranasos

		</comment>
		<comment id='5' author='lucienwang1009' date='2019-04-22T18:49:06Z'>
		Constant folding uses OpKernelContext (and not OpKernelContextInternal) on purpose, so that we don't rely on the InferenceSession in the optimizer.
For now we disabled constant folding for subgraphs (&lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/858&gt;#858&lt;/denchmark-link&gt;
) until we provide a proper support for it.
		</comment>
		<comment id='6' author='lucienwang1009' date='2019-05-15T01:20:17Z'>
		I ran the test code with the latest onnxruntime code and it was successful. &lt;denchmark-link:https://github.com/lucienwang1009&gt;@lucienwang1009&lt;/denchmark-link&gt;
 can you please check to see if it works for you?
		</comment>
		<comment id='7' author='lucienwang1009' date='2019-06-03T21:23:52Z'>
		Closing as I can't reproduce. Please re-open if this is still an issue.
		</comment>
		<comment id='8' author='lucienwang1009' date='2019-06-04T01:28:04Z'>
		&lt;denchmark-link:https://github.com/skottmckay&gt;@skottmckay&lt;/denchmark-link&gt;
 It works fine now, Thanks!!
		</comment>
	</comments>
</bug>