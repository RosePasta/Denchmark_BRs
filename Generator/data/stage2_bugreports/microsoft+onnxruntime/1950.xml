<bug id='1950' author='WilliamZhaoz' open_date='2019-09-28T15:22:13Z' closed_time='2019-11-18T12:42:19Z'>
	<summary>quantized model can not be loaded.</summary>
	<description>
I convert my pytorch model to a onnx model useing pytorch.onnx, then I can inference it in onnxruntime well, however then I quantized the onnx model to a quantized onnx model using onnxruntime, then I can not inference it with a error: Load model from xxx.onnx faild: This is an invalid model, Error in Node : : Node () has input size 1 not in range[min  =3, max=5]. I did not change anything, only quantized it. 
I use this tool quantized my model:
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/quantization&gt;https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/quantization&lt;/denchmark-link&gt;

help my, how can I fix it.
Thanks.
	</description>
	<comments>
		<comment id='1' author='WilliamZhaoz' date='2019-09-30T19:38:49Z'>
		Before using the tool make sure your model's opset version is set to 10. This is because when we quantize the model we need to update the opset version to 10 as the quantization ops were introduced in this version.
You are seeing this error because your model has some other op which was also updated for version 10.
I will submit a change to error check this case and report this... but this wont fix your problem... you will still need to update your model to opset 10 and then use the quantization tool
		</comment>
		<comment id='2' author='WilliamZhaoz' date='2019-10-03T17:00:15Z'>
		Hi Ashwini Khade,
Thank you, I converted my pytorch model to get the onnx model, so please
tell me how can I know which ops in pytorch is opset version 10?
thanks.

Ashwini Khade &lt;notifications@github.com&gt; 于2019年10月1日周二 上午3:38写道：
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Before using the tool make sure your model's opset version is set to 10.
 This is because when we quantize the model we need to update the opset
 version to 10 as the quantization ops were introduced in this version.
 You are seeing this error because your model has some other op which was
 also updated for version 10.
 I will submit a change to error check this case and report this... but
 this wont fix your problem... you will still need to update your model to
 opset 10 and then use the quantization tool

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#1950?email_source=notifications&amp;email_token=AIWSN2IJ5SCRA4XOF4DW33TQMJIVBA5CNFSM4I3ORPX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7625XI#issuecomment-536719069&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AIWSN2MUK4OQ55IN476WD43QMJIVBANCNFSM4I3ORPXQ&gt;
 .



		</comment>
		<comment id='3' author='WilliamZhaoz' date='2019-10-03T17:12:15Z'>
		Re: Pytorch version - see their documentation ONNX export for setting opset_version - &lt;denchmark-link:https://pytorch.org/docs/stable/onnx.html&gt;https://pytorch.org/docs/stable/onnx.html&lt;/denchmark-link&gt;

You can also verify opsets by loading your ONNX model using a visualizer like Netron.
		</comment>
		<comment id='4' author='WilliamZhaoz' date='2019-10-03T17:16:35Z'>
		okay, thank you Faith,
BTW, can I  get inference acceleration when I inference quantized model
than before quantized?

Faith Xu &lt;notifications@github.com&gt; 于2019年10月4日周五 上午1:12写道：
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Re: Pytorch version - see their documentation ONNX export for setting
 opset_version - https://pytorch.org/docs/stable/onnx.html

 You can also verify opsets by loading your ONNX model using a visualizer
 like Netron.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#1950?email_source=notifications&amp;email_token=AIWSN2OLPQSRVH37K7VFTTTQMYRXLA5CNFSM4I3ORPX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAI5AUI#issuecomment-538038353&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AIWSN2I3MLED6AQLRBQHJE3QMYRXLANCNFSM4I3ORPXQ&gt;
 .



		</comment>
		<comment id='5' author='WilliamZhaoz' date='2019-10-08T00:34:22Z'>
		Could you clarify your question? Quantization may result in some performance improvements at the expense of precision, though the current level of quantization support is still limited to just a few operators.
		</comment>
		<comment id='6' author='WilliamZhaoz' date='2019-10-08T02:19:18Z'>
		Yes, my question is: after I quantized my model, can I speed up my
inference process even though at the expense of precision, a faster
inference speed is more important than accuracy for me.
I guess the answer is yes as you said.
		</comment>
		<comment id='7' author='WilliamZhaoz' date='2019-10-08T03:20:57Z'>
		Hi Faith Xu,

you told me for Pytorch version: - see their documentation ONNX export for
setting opset_version - &lt;denchmark-link:https://pytorch.org/docs/stable/onnx.html&gt;https://pytorch.org/docs/stable/onnx.html&lt;/denchmark-link&gt;


Are the ops in this link is opset10? but I can converted my pytorch model
to onnx model using torch.onnx successfully, only for quantized model,
there are something wrong, so maybe my modle are already meeting the ops in
the link, but not for quantization tools.

please tell me how can I found the op in pytorch that quantization tools do
not support.

thanks.



Zhiyuan Zhao &lt;zhiyuan.zhaochn@gmail.com&gt; 于2019年10月8日周二 上午10:19写道：
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Yes, my question is: after I quantized my model, can I speed up my
 inference process even though at the expense of precision, a faster
 inference speed is more important than accuracy for me.
 I guess the answer is yes as you said.



		</comment>
		<comment id='8' author='WilliamZhaoz' date='2019-10-08T06:32:22Z'>
		Hi Faith Xu,
I saw the quantized model in Netron, there are many extra ops, e.g.
reduceMax/reduceMin/Sub/Div/Floor/Cast/QuantizeLinear, with those ops, I
can speed up my model inference process?
thanks.

Zhiyuan Zhao &lt;zhiyuan.zhaochn@gmail.com&gt; 于2019年10月8日周二 上午11:20写道：
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Hi Faith Xu,

 you told me for Pytorch version: - see their documentation ONNX export for
 setting opset_version - https://pytorch.org/docs/stable/onnx.html

 Are the ops in this link is opset10? but I can converted my pytorch model
 to onnx model using torch.onnx successfully, only for quantized model,
 there are something wrong, so maybe my modle are already meeting the ops in
 the link, but not for quantization tools.

 please tell me how can I found the op in pytorch that quantization tools
 do not support.

 thanks.



 Zhiyuan Zhao ***@***.***&gt; 于2019年10月8日周二 上午10:19写道：

&gt; Yes, my question is: after I quantized my model, can I speed up my
&gt; inference process even though at the expense of precision, a faster
&gt; inference speed is more important than accuracy for me.
&gt; I guess the answer is yes as you said.
&gt;



		</comment>
		<comment id='9' author='WilliamZhaoz' date='2019-10-30T21:02:38Z'>
		&lt;denchmark-link:https://github.com/WilliamZhaoz&gt;@WilliamZhaoz&lt;/denchmark-link&gt;
 : These ops are added for dynamic quantization .i.e converting the input from FP32 to 8 bit during inference. In opset 11 we have added onnx functions to fuse these ops into 1 op "DynamicQuantizeLinear" This is a newly added function in opset 11 and you will need to covert your model to opset 11 first and then quantize it...
		</comment>
		<comment id='10' author='WilliamZhaoz' date='2019-10-31T10:02:17Z'>
		How can I convert my model to opset11? is there any tools for that?

Ashwini Khade &lt;notifications@github.com&gt; 于2019年10月31日周四 上午5:02写道：
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 @WilliamZhaoz &lt;https://github.com/WilliamZhaoz&gt; : These ops are added for
 dynamic quantization .i.e converting the input from FP32 to 8 bit during
 inference. In opset 11 we have added onnx functions to fuse these ops into
 1 op "DynamicQuantizeLinear" This is a newly added function in opset 11 and
 you will need to covert your model to opset 11 first and then quantize it...

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#1950?email_source=notifications&amp;email_token=AIWSN2OMQHJWTRFYGRHQI7LQRHY7JA5CNFSM4I3ORPX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECVYFVI#issuecomment-548111061&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/AIWSN2JAJJJ2ETBCAQX2NCDQRHY7JANCNFSM4I3ORPXQ&gt;
 .



		</comment>
		<comment id='11' author='WilliamZhaoz' date='2019-10-31T18:38:26Z'>
		Currently the only way is to reconvert your model from original framework...
		</comment>
		<comment id='12' author='WilliamZhaoz' date='2019-11-05T19:34:33Z'>
		&lt;denchmark-link:https://github.com/WilliamZhaoz&gt;@WilliamZhaoz&lt;/denchmark-link&gt;
 any other questions on this?
		</comment>
	</comments>
</bug>