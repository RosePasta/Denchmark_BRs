<bug id='1331' author='xadupre' open_date='2019-07-02T09:39:05Z' closed_time='2019-07-05T09:48:58Z'>
	<summary>Inconsistent results foor two consecutive results or crash with a specific ONNX model</summary>
	<description>
Describe the bug
The memory optimization fails to produce consistent results over two consecutive runs or fails.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
ONNX Runtime installed from (source or binary): from source (python %~dp0onnxruntime\tools\ci_build\build.py --build_dir %~dp0onnxruntime\build\Windows --config Release --build_wheel --numpy_version= --skip-keras-test)
ONNX Runtime version:
Python version: 3.7
Visual Studio version (if applicable): 2017 (no GPU)

To Reproduce
&lt;denchmark-link:https://github.com/xadupre/presentation/blob/master/2019/model_onnx.onnx?raw=true&gt;ONNX file&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;import numpy
import pandas
from io import StringIO

Xtest = pandas.read_csv(StringIO("""
1.000000000000000000e+02,1.061277971307766705e+02,1.472195004809226493e+00,2.307125069497626552e-02,4.539948095743629591e-02,2.855191098141335870e-01
1.000000000000000000e+02,9.417031896832908444e+01,1.249743892709246573e+00,2.370416174339620707e-02,2.613847280316268853e-02,5.097165413593484073e-01
1.000000000000000000e+02,9.305231488674536422e+01,1.795726729335217264e+00,2.473274733802270642e-02,1.349765645107412620e-02,9.410288840541443378e-02
1.000000000000000000e+02,7.411264142156210255e+01,1.747723020195752319e+00,1.559695663417645997e-02,4.230394035515055301e-02,2.225492746314280956e-01
1.000000000000000000e+02,9.326006195761877393e+01,1.738860294343326229e+00,2.280160135767652502e-02,4.883335335161764074e-02,2.806808409247734115e-01
1.000000000000000000e+02,8.341529291866362428e+01,5.119682123742423929e-01,2.488795768635816003e-02,4.887573336092913834e-02,1.673462179673477768e-01
1.000000000000000000e+02,1.182436477919874562e+02,1.733516391831658954e+00,1.533520930349476820e-02,3.131213519485807895e-02,1.955345358785769427e-01
1.000000000000000000e+02,1.228982583299257101e+02,1.115599996405831629e+00,1.929354155079938959e-02,3.056996308544096715e-03,1.197052763998271013e-01
1.000000000000000000e+02,1.160303269386108838e+02,1.018627021014927303e+00,2.248784981616459844e-02,2.688111547114307651e-02,3.326105131778724355e-01
1.000000000000000000e+02,1.163414374640396005e+02,6.644299545804077667e-01,1.508088417713602906e-02,4.451836657613789106e-02,3.245643044204808425e-01
""".strip("\n\r ")), header=None).values

from onnxruntime import InferenceSession, RunOptions, SessionOptions
opt = SessionOptions()
opt.enable_mem_pattern = True
opt.enable_cpu_mem_arena = True
sess = InferenceSession(filename, opt)  # see attachement
res1 = sess.run(None, {'X': Xtest.astype(numpy.float32)})[0]
res2 = sess.run(None, {'X': Xtest.astype(numpy.float32)})[0]
&lt;/denchmark-code&gt;

res1 and res2 are different. Second value produces inf values (it should not). In case memory optimization is disabled, the system crashes.
&lt;denchmark-code&gt;opt.enable_mem_pattern = False
opt.enable_cpu_mem_arena = False
&lt;/denchmark-code&gt;

Expected behavior
See above.
	</description>
	<comments>
		<comment id='1' author='xadupre' date='2019-07-02T20:34:00Z'>
		&lt;denchmark-link:https://github.com/xadupre&gt;@xadupre&lt;/denchmark-link&gt;
 The python version worries me a bit. My tests did not often worked until I downgraded it to 3.6.7.
What's pandas version?
		</comment>
		<comment id='2' author='xadupre' date='2019-07-02T21:01:31Z'>
		Yes, I have a repro in a debug build. It is enough to disable the arenas which incidentally helps with debugging as well.
&lt;denchmark-link:https://user-images.githubusercontent.com/11303988/60546315-e3798280-9cd1-11e9-8924-fb7b335768dc.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xadupre' date='2019-07-04T02:18:45Z'>
		I believe your model is bad.
Both the Scan nodes in the model have 2 outputs, and the output shape for both outputs in both Scan nodes is {'None', 'None'}. The problem is that 'None' is not a special name in ONNX and is treated as a variable name, which equates to the model saying all 4 outputs have exactly the same shape, and not to all 4 having unknown shapes.
Please see &lt;denchmark-link:https://github.com/onnx/onnx/blob/master/docs/IR.md#tensor-shapes&gt;https://github.com/onnx/onnx/blob/master/docs/IR.md#tensor-shapes&lt;/denchmark-link&gt;

That leads to the allocation planner thinking that it can re-use the output buffers from the first Scan node when running the second Scan node. The buffer sizes don't match, and we lack a sanity check at runtime to detect that.
Debugger output showing the dim_value instances for one of the Scan nodes.
&lt;denchmark-code&gt;-- | -- | -- | --
▶ | *pnode | {index_=13 name_="Scan" op_type_="Scan" ...} | const onnxruntime::Node
▶ | ((*pnode).definitions_.output_defs[0])-&gt;node_arg_info_.type_-&gt;value_.tensor_type_-&gt;shape().dim().Get(0).dim_param() | "None" | 
▶ | ((*pnode).definitions_.output_defs[0])-&gt;node_arg_info_.type_-&gt;value_.tensor_type_-&gt;shape().dim().Get(1).dim_param() | "None" | 
▶ | ((*pnode).definitions_.output_defs[1])-&gt;node_arg_info_.type_-&gt;value_.tensor_type_-&gt;shape().dim().Get(0).dim_param() | "None" | 
▶ | ((*pnode).definitions_.output_defs[1])-&gt;node_arg_info_.type_-&gt;value_.tensor_type_-&gt;shape().dim().Get(1).dim_param() | "None" | 
&lt;/denchmark-code&gt;

We can add a check at runtime when re-using a buffer to detect a size mismatch.
&lt;denchmark-link:https://github.com/gramalingam&gt;@gramalingam&lt;/denchmark-link&gt;
 should the allocation planner also special case an empty string and an '*' in a dim_param in SameShape()? Currently the check involving dim_param values doesn't take into account those special cases

To fix the model I would suggest removing the shape info from the Scan node outputs completely. I believe if you do that it can be inferred at runtime (i.e. has_shape() for the TensorProto should return false).
		</comment>
		<comment id='4' author='xadupre' date='2019-07-04T06:02:18Z'>
		Ok I see. I did not about the variables for shapes. I'll try to find the proper values. I faced other issues when leaving the values empty but I don't remember which ones. I was confused by the fact the same graph was working with less nodes so it did not seem related to the way I defined nodes. The allocation planner should be able to detect this kind of inconsistencies? From what you say, I guess using variables instead of * or empty shape may help the allocation planner and make the model faster but it is unclear why. onnx has a function onnx_checker which checks the consistency of a model. Maybe onnxruntime could have a similar function instead of looking for shape mismatch at every prediction call.
		</comment>
		<comment id='5' author='xadupre' date='2019-07-04T11:27:21Z'>
		I found out today that being able to use '*' or an empty string for dim_param was accidentally added to the ONNX spec and that text is slated for removal in a pending PR, so having no shape information would be best. Alternatively you could use a unique string in the dim_param for each unknown value instead of using 'None' in multiple places.
As the dim_param is essentially a variable name, I'm not sure what the allocation planner can do if the model incorrectly uses the variable name in multiple places. I will however add something to fail a prediction call if there's a mismatch.
There should be no significant difference to execution speed though. Not having a value just means the allocation planner will not incorrectly think two output shapes are equal and attempt to re-use a buffer for them. At worst there may be some extra allocations.
		</comment>
		<comment id='6' author='xadupre' date='2019-07-05T09:48:58Z'>
		I chose empty shapes. It works but I think it would better with some error messages. I'll close the issue.
		</comment>
	</comments>
</bug>