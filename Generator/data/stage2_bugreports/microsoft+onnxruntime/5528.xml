<bug id='5528' author='EnricoBeltramo' open_date='2020-10-18T02:11:00Z' closed_time='2021-01-05T09:34:56Z'>
	<summary>Set provider with openvino</summary>
	<description>
I'm using onnx with Openvino interface, but I have some errors:
If I do this operation:
session = onnxruntime.InferenceSession("siamrpnmobilenet.onnx")
session.set_providers(['OpenVINOExecutionProviders'], [{'device_type' : 'CPU_FP32'}])
I have follow error:
['OpenVINOExecutionProviders'] does not contain a subset of available providers ['CPUExecutionProvider', 'OpenVINOExecutionProvider']
File "/home/ulix/Progetti/pysot mod/Siamrpnonnx.py", line 7, in 
session.set_providers(['OpenVINOExecutionProviders'], [{'device_type' : 'CPU_FP32'}])
If I do deprecated code, it works fine
onnxruntime.capi._pybind_state.set_openvino_device("CPU_FP32")
session = onnxruntime.InferenceSession("siamrpnmobilenet.onnx")
I need to set the device because programmatically I need switch from MYRIAD to CPU and viceversa and  I have to use multiple MYRIAD devices (I suppose possible only with set_providers using device_id key)
	</description>
	<comments>
		<comment id='1' author='EnricoBeltramo' date='2020-10-20T16:32:38Z'>
		&lt;denchmark-link:https://github.com/kit1980&gt;@kit1980&lt;/denchmark-link&gt;
 The following error is due to a typo. Yes, there's a mistake in the documentation, but from the error it clearly says:
subset of available providers ['CPUExecutionProvider', 'OpenVINOExecutionProvider']
Point 1:
please, use:
session.set_providers(['OpenVINOExecutionProvider'], [{'device_type' : 'CPU_FP32'}])
This will resolve the error you are currently facing.
Point 2:
onnxruntime.capi._pybind_state.set_openvino_device("CPU_FP32") is now depreceated.
Please use the Latest OpenVINO 2021.1 Release and follow the latest documentation.
Point 3:
Yes, to use multiple Myriad Devices, you need to configure key value pairs.
-&gt; Key-Value pairs for config options can be set using the Session.set_providers API as follows:-
session.set_providers(['OpenVINOExecutionProvider'], [{Key1 : Value1, Key2 : Value2, ...}])
		</comment>
		<comment id='2' author='EnricoBeltramo' date='2020-10-21T22:25:51Z'>
		Thank you very much!
I didn't notice the typo error. After correction, it works fine!
Just another question:
If I disable optimizations as suggested in the guide:
options = onnxruntime.SessionOptions()
options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL
self.session_rpn = onnxruntime.InferenceSession("siamrpnmobilenet.onnx", options)
I have follow error:
[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /home/ulix/Progetti/pysot/onnxruntime/onnxruntime/core/providers/openvino/backends/basic_backend.cc:61 onnxruntime::openvino_ep::BasicBackend::BasicBackend(const onnx::ModelProto&amp;, onnxruntime::openvino_ep::GlobalContext&amp;, const onnxruntime::openvino_ep::SubGraphContext&amp;) [OpenVINO-EP]  Exception while Loading Network for graph: OpenVINOExecutionProvider_OpenVINO-EP-subgraph_1_0: data [1172] doesn't exist
with optimization enabled instead all works fine. What should be?
		</comment>
		<comment id='3' author='EnricoBeltramo' date='2020-10-22T04:47:16Z'>
		&lt;denchmark-link:https://github.com/EnricoBeltramo&gt;@EnricoBeltramo&lt;/denchmark-link&gt;


The model you are using for this study (SiamRPNMobileNet) is not officially supported by OpenVINO and has not been validated officially by OpenVINO.
This model has nodes with dynamic shapes, so currently OpenVINO doesn't support models having tensors with dynamic shapes.

As this is an PyTorch model, Here are the details about supported PyTorch models from OpenVINO:
&lt;denchmark-link:https://docs.openvinotoolkit.org/2021.1/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html#supported_pytorch_models_via_onnx_conversion&gt;https://docs.openvinotoolkit.org/2021.1/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html#supported_pytorch_models_via_onnx_conversion&lt;/denchmark-link&gt;

Point 3:
The reason why you are able to have this model running with OpenVINO EP is via subgraph partitioning.
This model is been divided into 3 subgraphs that would be run on OpenVINO Backend and the rest of the nodes are
being run by the Default CPU_Execution_Provider of OnnxRuntime.
Point 4:
Since the model is not validated on OpenVINO and as well as OpenVINO EP, we can't run the whole model on the OpenVINO EP. That is the reason why you see this model throws an error when the OnnxRuntime optimizations are disabled.
Thanks.
		</comment>
		<comment id='4' author='EnricoBeltramo' date='2020-10-22T05:53:31Z'>
		Thank you very much, now is really clear!
Is clear for me either why if I try to convert the model directly in native Openvino it don't works. It is possible to examinate how the model is split in subgraphs in order to make more analysis and optimitations?
		</comment>
		<comment id='5' author='EnricoBeltramo' date='2020-10-23T16:46:40Z'>
		&lt;denchmark-link:https://github.com/EnricoBeltramo&gt;@EnricoBeltramo&lt;/denchmark-link&gt;

As you are using a python sample to run the application, you can add this one line in your code to get more print statements and see as OpenVINO EP executes the model.
                 (Add this line of code in your python code)
But to answer your question of how you can actually see what are the subgraphs that were created to run using OpenVINO Backend and examine them, you need to create the build in Debug mode and run some sample cpp application.


Install the latest OpenVINO 2021.1 Binary Release in your Linux Machine
Here is where from you can download the latest OpenVINO Version:
&lt;denchmark-link:https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_linux.html&gt;https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_linux.html&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_linux.html#install-openvino&gt;https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_linux.html#install-openvino&lt;/denchmark-link&gt;

&lt;denchmark-link:https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?elq_cid=6637879_ts1602069401500&gt;https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?elq_cid=6637879_ts1602069401500&lt;/denchmark-link&gt;

step 2:
You need to Initialize the OpenVINO in your terminal.
Navigate to: /opt/intel/openvino_2021.1.110/bin  (This is where OpenVINO got installed in my machine)

source setupvars.sh

Now the OpenVINO Environment is Initialized
step 3:
Navigate to the directory or place where you want to clone the repository
git clone &lt;denchmark-link:https://github.com/microsoft/onnxruntime.git&gt;https://github.com/microsoft/onnxruntime.git&lt;/denchmark-link&gt;

cd onnxruntime
git submodule update --init --recursive
step 4:
Build the onnxruntime UEP Repository in Debug Mode
If you want to build for MYRIAD (VPU)
./build.sh --build --update --config Debug --use_openvino MYRIAD_FP16 --build_wheel
&lt;denchmark-code&gt;                           (or)
&lt;/denchmark-code&gt;

If you want to build for CPU
./build.sh --build --update --config Debug --use_openvino CPU_FP32 --build_wheel
There are other devices which we support as well, pls check the documentation.
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/OpenVINO-ExecutionProvider.md&gt;https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/OpenVINO-ExecutionProvider.md&lt;/denchmark-link&gt;

Here's the screenshot of the log looks when the build is complete without any errors:
&lt;denchmark-link:https://user-images.githubusercontent.com/29036417/97030283-2a048780-157c-11eb-9e76-56dcf128c432.png&gt;&lt;/denchmark-link&gt;

step 5:
Navigate to:
cd &lt;Directory_where_you_built&gt;/onnxruntime/build/Linux/Debug
step 6:
Type this line in the same terminal, this enables to get the dumps and some additional debug statements
export ORT_OPENVINO_ENABLE_DEBUG=1
step 7:
Run sample application using the already existing onnxruntime tools

./onnxruntime_perf_test -r 1 -c 1 -I -v -e openvino siamrpnmobilenet.onnx

Note:
Do &gt; ./onnxruntime_perf_test --h  (To explore more flags of this tool)
This will execute your model and you can find the subgraphs, when you do

ls  (Do it, After the application completed the run)

Use Netron viewer to view and inspect the subgraphs which would be dumped in the same location in .onnx format
Hope this helps :)
		</comment>
		<comment id='6' author='EnricoBeltramo' date='2020-10-26T09:34:48Z'>
		Thank you! very very clear: surely I will try it.
		</comment>
		<comment id='7' author='EnricoBeltramo' date='2020-12-25T15:08:01Z'>
		This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.
		</comment>
	</comments>
</bug>