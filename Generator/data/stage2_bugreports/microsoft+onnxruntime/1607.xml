<bug id='1607' author='neginraoof' open_date='2019-08-12T20:52:44Z' closed_time='2019-08-16T17:12:47Z'>
	<summary>Bidirectional RNN with initial hidden state does not return the expected results</summary>
	<description>
Describe the bug
ORT results for the bidirectional RNN with initial hidden state model does not match pytorch results. Comparing the output tensors, results corresponding to the backwards direction do not match.
Urgency
Models using bidirectional RNN with initial hidden state could have an unexpected behavior.
To Reproduce
A simple test for RNN model:
def test_rnn():
    input = torch.randn(3, 2, 2, requires_grad=True)
    h0 = torch.randn(2, 3, 2)
    input = (input, h0)
    model = torch.nn.RNN(input_size=2, hidden_size=2, num_layers=1, nonlinearity='relu',
                             bidirectional=True, dropout=0.2, batch_first=True)
    model.eval()
    output = model(*input)
    f = io.BytesIO()
    torch.onnx.export(model, tuple(input), f, verbose=True)
    def to_numpy(tensor):
        if tensor.requires_grad:
            return tensor.detach().cpu().numpy()
        else:
            return tensor.cpu().numpy()

    inputs = list(map(to_numpy, input))
    outputs = list(map(to_numpy, output))
    ort_sess = onnxruntime.InferenceSession(f.getvalue())
    ort_inputs = dict((ort_sess.get_inputs()[i].name, input) for i, input in enumerate(inputs))
    ort_outs = ort_sess.run(None, ort_inputs)

    # compare onnxruntime and PyTorch results
    [np.testing.assert_allclose(out, ort_out, rtol=0.01, atol=0.01) for out, ort_out in zip(outputs, ort_outs)]
Assertion fails due to the mismatch in outputs.

Here is a link to the ONNX model:
&lt;denchmark-link:https://drive.google.com/drive/folders/1NLjgFFz6An2a_fJy2zwpsj-n9B_RyZvH?usp=sharing&gt;https://drive.google.com/drive/folders/1NLjgFFz6An2a_fJy2zwpsj-n9B_RyZvH?usp=sharing&lt;/denchmark-link&gt;

I don't see this issue with LSTM or GRU models. The issue is only for basic RNN, bidirectional, with initial hidden state provided.
I don't see this issue when using a tensor filled with a constant value as initial hidden state:
h0 = torch.ones(2, 3, 2)
Looks like this is because a wrong subspan of the initial hidden tensor is used in the backwards direction for the basic RNN model.
	</description>
	<comments>
		<comment id='1' author='neginraoof' date='2019-08-13T18:47:29Z'>
		The RNN node is fallback to CPU since the activations is Relu, Relu which is not supported by cudnn. Actually, all nodes are running on CPU for this model because of RNN fallback.
Could someone take a look at CPU RNN implementation?
		</comment>
		<comment id='2' author='neginraoof' date='2019-08-16T17:13:50Z'>
		&lt;denchmark-link:https://github.com/neginraoof&gt;@neginraoof&lt;/denchmark-link&gt;
 - Thanks for the thorough diagnosis of the issue !
		</comment>
	</comments>
</bug>