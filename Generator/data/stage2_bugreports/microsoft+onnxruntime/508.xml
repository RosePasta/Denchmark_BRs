<bug id='508' author='aldanor' open_date='2019-02-21T23:52:47Z' closed_time='2019-05-15T17:19:26Z'>
	<summary>Score computation in TreeEnsembleRegressor implementation is wrong</summary>
	<description>
Here's an excerpt from &lt;denchmark-link:https://github.com/Microsoft/onnxruntime/blob/11b369a8641d9dc8989739f3c530a8cf330d7f8b/onnxruntime/core/providers/cpu/ml/treeregressor.cc&gt;treeregressor.c&lt;/denchmark-link&gt;
, where the final scores are computed:
&lt;denchmark-link:https://github.com/Microsoft/onnxruntime/blob/11b369a8641d9dc8989739f3c530a8cf330d7f8b/onnxruntime/core/providers/cpu/ml/treeregressor.cc#L243-L259&gt;https://github.com/Microsoft/onnxruntime/blob/11b369a8641d9dc8989739f3c530a8cf330d7f8b/onnxruntime/core/providers/cpu/ml/treeregressor.cc#L243-L259&lt;/denchmark-link&gt;

This code is generally wrong (moreover, it looks like the author meant to do something else):

All of those += operators (in SUM and AVERAGE cases) would be ever executed once, so they are essentially the same as =. Perhaps val was meant to be instantiated outside of this block? However, you can't just move it outside of the closest for loop, since this is meant to be aggregation over leaf values and not over targets.
With MIN and MAX cases, it's even worse: the final value ends up being either min(base_value, score[j]) or max(base_value, score[j]).
Even if val was instantiated outside of this block... for AVERAGE case, the docs say that it "defines how to aggregate leaf values within a target". If so, why do you divide by roots_.size()? In general, the number of leaf values within a target can be different for different targets, and, moreover, can even be path-dependent? (or are some implicit assumptions being made here about the tree structure?)

Regards.
	</description>
	<comments>
		<comment id='1' author='aldanor' date='2019-05-09T06:10:06Z'>
		This code was ported from a previous caffe2 based implementation as it is. Hope we can fix it for good.
		</comment>
		<comment id='2' author='aldanor' date='2019-05-11T03:35:07Z'>
		I'm looking into generating reference models for each of the aggregation functions, to verify the implementation. Will update on this thread shortly.
		</comment>
		<comment id='3' author='aldanor' date='2019-05-14T05:54:44Z'>
		&lt;denchmark-link:https://github.com/aldanor&gt;@aldanor&lt;/denchmark-link&gt;
 , thanks for the great comments. Besides the confusion in that section (I think the variable names are carried over from the classifier, which don't always apply for regression), I tried out several scikit-learn tree algorithms, including  RandomForestRegressor and ExtraTreesRegressor. Most of the models from sciki-learn use either SUM or AVERAGE, which work successfully.
For MIN and MAX aggregation functions, the individual tree scores were not being preserved, so the computation was not correct. I've updated the functions in the PR below.
Regarding the comment for the += operators, they are executed only 1 time. It's to add base_values the final computed tree-ensemble score. They only need to be executed once, because ProcessTreeNode() already computes the sum over all leaves, for all targets. The only computation left is the aggregation.
Please review the fix in#1022.
Below is a sample of how to generate tree models from scikit-learn, in case others are interested in scoring tree ensembles using OnnxRuntime.
Sample code using Scikit-Learn
&lt;denchmark-code&gt;# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.
import numpy
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor

numpy.random.seed(0)
X = [[0, 1], [1, 1], [2, 0]]
X = numpy.array(X, dtype=numpy.float32)
# 1 target
y = numpy.array([100, -10, 50], dtype=numpy.float32)

print ("X=", X);
print ("y=", y);

pipe = Pipeline([('rf', ExtraTreesRegressor(n_estimators=5))])
pipe.fit(X, y)
print("predict with pipeline\n", pipe.predict(X))

### Convert model
from skl2onnx.common.data_types import Int64TensorType, FloatTensorType, StringTensorType, DictionaryType, SequenceType
from skl2onnx import convert_sklearn
model_onnx = convert_sklearn(pipe, 'pipeline_lightgbm',
							 [('input', FloatTensorType([1, 2]))])
with open("TreeEnsembleRegressor.onnx", "wb") as f:
	f.write(model_onnx.SerializeToString())

### Score with onnxruntime.

import onnxruntime as rt
import numpy
sess = rt.InferenceSession("TreeEnsembleRegressor.onnx")
pred_onx = sess.run(None, {"input": X.astype(numpy.float32)})
print("predict with onnxruntime\n", [x[0] for x in pred_onx[0]])
&lt;/denchmark-code&gt;

Give the following output results and results ONNX model.
Sample Output
X= [[0. 1.]
[1. 1.]
[2. 0.]]
y= [100. -10.  50.]
predict with pipeline
[100. -10.  50.]
predict with onnxruntime
[100.0, -10.0, 50.0]

&lt;denchmark-link:https://user-images.githubusercontent.com/13698702/57662474-32af0b00-75df-11e9-9c74-25cc105f198e.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='aldanor' date='2019-05-15T19:42:32Z'>
		&lt;denchmark-link:https://github.com/jignparm&gt;@jignparm&lt;/denchmark-link&gt;
 Thanks for fixing this - looks good now! (also the more tests the better)
		</comment>
	</comments>
</bug>