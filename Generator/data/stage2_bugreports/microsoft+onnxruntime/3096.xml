<bug id='3096' author='ari9dam' open_date='2020-02-26T22:47:52Z' closed_time='2020-04-21T05:06:42Z'>
	<summary>Attribute 'alpha' appeared multiple times</summary>
	<description>
Describe the bug

Microsoft.ML.OnnxRuntime.OnnxRuntimeException: [ErrorCode:InvalidGraph] Node:fused _0 Attribute 'alpha' appeared multiple times. at Microsoft.ML.OnnxRuntime.InferenceSession..ctor

I am getting this error in inference time. Model did load successfully and was exported without any error. Here is the pytorch code and the onnx graph:
``
class FastTextWithVisualFeaturesAndContextCNNModel(nn.Module):
def init(self, vocab_size: int,
word_embedding_dim: int,
visual_embedding_dim: int,
n_classes: int,
visual_feature_size,
embedding_matrix=None,
dropout = 0):
super(FastTextWithVisualFeaturesAndContextCNNModel, self).init()
self._vocab_size = vocab_size
self._word_embedding_dim = word_embedding_dim
self._n_classes = n_classes
self._visual_embedding_dim = visual_embedding_dim
self._visual_feature_size = visual_feature_size
&lt;denchmark-code&gt;self._embedding_layer = None
if embedding_matrix is None:
    self._embedding_layer = nn.Embedding(self._vocab_size, self._word_embedding_dim)
else:
    self._embedding_layer = nn.Embedding.from_pretrained(embedding_matrix.clone().detach())

Ci = 1
Co = 500
Ks = [2, 3, 4]
# self.convsMain = nn.ModuleList([nn.Conv2d(Ci, Co, (K, word_embedding_dim)) for K in Ks])
self.convsMain0 = nn.Conv2d(Ci, Co, (2, word_embedding_dim))
self.convsMain1 = nn.Conv2d(Ci, Co, (3, word_embedding_dim))
self.convsMain2 = nn.Conv2d(Ci, Co, (4, word_embedding_dim))
self.dropout = nn.Dropout(0.2)
Co_ctx = 50
Ks_ctx = [2,3]
self.convsCtx0 = nn.Conv2d(Ci, Co_ctx, (2, word_embedding_dim))
self.convsCtx1 = nn.Conv2d(Ci, Co_ctx, (3, word_embedding_dim))


self.fc1 = nn.Linear(len(Ks) * Co, n_classes)
self.fc2 = nn.Linear(len(Ks_ctx)*Co_ctx, n_classes)
self._visual_layer = self.get_viusal_layer()
self._visual_layer.apply(self.init_weights)
&lt;/denchmark-code&gt;

def init_weights(self, m):
if type(m) == nn.Linear:
torch.nn.init.xavier_normal_(m.weight)
m.bias.data.fill_(0.01)
def get_viusal_layer(self):
return nn.Sequential(nn.BatchNorm1d(self._visual_feature_size),
nn.Linear(self._visual_feature_size, self._visual_embedding_dim),
nn.BatchNorm1d(self._visual_embedding_dim),
nn.LeakyReLU(),
nn.Linear(self._visual_embedding_dim,self._n_classes),
nn.LeakyReLU())
def set_visual_feature_size(self, visual_feature_size):
self._visual_feature_size = visual_feature_size
self._visual_layer = self.get_viusal_layer()
def forward(self, input_ids, input_masks, visual_features, context_tokens, context_masks, labels=None):
# cast input_masks to float
input_masks = input_masks.float()
# get the embeddings of each token in the input
masked_token_embeddings = self._embedding_layer(input_ids) * input_masks.unsqueeze(2)
# compute the hidden layer
masked_token_embeddings = masked_token_embeddings.unsqueeze(1)  # (N, Ci, W, D)
&lt;denchmark-code&gt;x = [F.relu(self.convsMain0(masked_token_embeddings)).squeeze(3),
     F.relu(self.convsMain1(masked_token_embeddings)).squeeze(3),
     F.relu(self.convsMain2(masked_token_embeddings)).squeeze(3)]  # [(N, Co, W), ...]*len(Ks)
x = [F.max_pool1d(i, int(i.size(2)), 1).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)
x = torch.cat(x, 1)
x = self.dropout(x)  # (N, len(Ks)*Co)

# cast input_masks to float
context_masks = context_masks.float()
# get the embeddings of each token in the input
masked_ctx_token_embeddings = self._embedding_layer(context_tokens) * context_masks.unsqueeze(2)
# compute the context representation
masked_ctx_token_embeddings = masked_ctx_token_embeddings.unsqueeze(1)  # (N, Ci, W, D)

y = [F.relu(self.convsCtx0(masked_ctx_token_embeddings)).squeeze(3),
     F.relu(self.convsCtx1(masked_ctx_token_embeddings)).squeeze(3)]  # [(N, Co, W), ...]*len(Ks)
y = [F.max_pool1d(i, int(i.size(2)), 1).squeeze(2) for i in y]  # [(N, Co), ...]*len(Ks)
y = torch.cat(y, 1)
y = self.dropout(y)

visual_features = visual_features.float()
predictions = self.fc1(x) + self.fc2(y) + self._visual_layer(visual_features)

output = (F.softmax(predictions,dim=-1),)
if labels is not None:
    ce_loss = nn.CrossEntropyLoss()
    loss = ce_loss(predictions,labels)
    output = (loss,) + output
return output
&lt;/denchmark-code&gt;

The onnx graph
graph torch-jit-export (
%input.1[INT64, 1x512]
%1[INT64, 1x512]
%2[FLOAT, 1x127]
%input.2[INT64, 1x100]
%4[INT64, 1x100]
) initializers (
%_embedding_layer.weight[FLOAT, 69285x300]
%_visual_layer.0.bias[FLOAT, 127]
%_visual_layer.0.num_batches_tracked[INT64, scalar]
%_visual_layer.0.running_mean[FLOAT, 127]
%_visual_layer.0.running_var[FLOAT, 127]
%_visual_layer.0.weight[FLOAT, 127]
%_visual_layer.1.bias[FLOAT, 300]
%_visual_layer.1.weight[FLOAT, 300x127]
%_visual_layer.2.bias[FLOAT, 300]
%_visual_layer.2.num_batches_tracked[INT64, scalar]
%_visual_layer.2.running_mean[FLOAT, 300]
%_visual_layer.2.running_var[FLOAT, 300]
%_visual_layer.2.weight[FLOAT, 300]
%_visual_layer.4.bias[FLOAT, 46]
%_visual_layer.4.weight[FLOAT, 46x300]
%fc1.bias[FLOAT, 46]
%fc1.weight[FLOAT, 46x300]
%fc2.bias[FLOAT, 46]
%fc2.weight[FLOAT, 46x300]
) {
%24 = Castto = 1
%25 = Gather(%_embedding_layer.weight, %input.1)
%26 = Unsqueezeaxes = [2]
%27 = Mul(%25, %26)
%28 = ReduceSumaxes = [1], keepdims = 0
%29 = ReduceSumaxes = [1], keepdims = 0
%30 = Unsqueezeaxes = [1]
%31 = Div(%28, %30)
%32 = Castto = 1
%33 = Gather(%_embedding_layer.weight, %input.2)
%34 = Unsqueezeaxes = [2]
%35 = Mul(%33, %34)
%36 = ReduceSumaxes = [1], keepdims = 0
%37 = ReduceSumaxes = [1], keepdims = 0
%38 = Unsqueezeaxes = [1]
%39 = Div(%36, %38)
%40 = Castto = 1
%41 = Gemm[alpha = 1, beta = 1, transB = 1](%31, %fc1.weight, %fc1.bias)
%42 = Gemm[alpha = 1, beta = 1, transB = 1](%39, %fc2.weight, %fc2.bias)
%43 = Add(%41, %42)
%44 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%40, %_visual_layer.0.weight, %_visual_layer.0.bias, %_visual_layer.0.running_mean, %_visual_layer.0.running_var)
%45 = Gemm[alpha = 1, beta = 1, transB = 1](%44, %_visual_layer.1.weight, %_visual_layer.1.bias)
%46 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%45, %_visual_layer.2.weight, %_visual_layer.2.bias, %_visual_layer.2.running_mean, %_visual_layer.2.running_var)
%47 = LeakyRelualpha = 0.00999999977648258
%48 = Gemm[alpha = 1, beta = 1, transB = 1](%47, %_visual_layer.4.weight, %_visual_layer.4.bias)
%49 = Add(%43, %48)
%50 = Softmaxaxis = 1
return %50
}
	</description>
	<comments>
		<comment id='1' author='ari9dam' date='2020-02-27T00:02:34Z'>
		Could you please disable all the onnxruntime optimizers and try it again?
		</comment>
		<comment id='2' author='ari9dam' date='2020-02-27T00:05:24Z'>
		Yes disabled and worked.
Thank you
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Feb 26, 2020, 4:03 PM Changming Sun ***@***.***&gt; wrote:
 Could you please disable all the onnxruntime optimizers and try it again?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;#3096?email_source=notifications&amp;email_token=ADL24YWGXNMYIESNHFNHNE3RE37JVA5CNFSM4K4PQ35KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOENCLWDA#issuecomment-591706892&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ADL24YVTGE2PXFMJEADTPITRE37JVANCNFSM4K4PQ35A&gt;
 .



		</comment>
		<comment id='3' author='ari9dam' date='2020-02-27T00:46:26Z'>
		May I know which ORT version you're using? Does it include &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/2791&gt;#2791&lt;/denchmark-link&gt;
 fix?
		</comment>
		<comment id='4' author='ari9dam' date='2020-02-28T01:16:42Z'>
		Can you share the model file and ORT version you used for testing? We're preparing for our next release (scheduled for next week) and ideally we would like to fix this as part it.
		</comment>
		<comment id='5' author='ari9dam' date='2020-03-03T19:14:36Z'>
		&lt;denchmark-link:https://github.com/ari9dam&gt;@ari9dam&lt;/denchmark-link&gt;
 any update on this?
		</comment>
		<comment id='6' author='ari9dam' date='2020-04-21T05:06:41Z'>
		Closing. Please re-open with requested info if you need assistance.
		</comment>
		<comment id='7' author='ari9dam' date='2020-12-09T23:11:11Z'>
		I also encounter this issue with a simple fc layer.
&lt;denchmark-link:https://user-images.githubusercontent.com/18554635/101699254-b20cf680-3a2f-11eb-9616-cb94f0b09585.png&gt;&lt;/denchmark-link&gt;

This is my model layout and onnx file. As can be seen, there is no duplicate alpha.
&lt;denchmark-link:https://user-images.githubusercontent.com/18554635/101699423-f5fffb80-3a2f-11eb-9e1f-e85687522fb7.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/18554635/101699427-f6989200-3a2f-11eb-81a2-e9dfe3d42e9e.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/harrysummer&gt;@harrysummer&lt;/denchmark-link&gt;
 found there is an  in fused operators (for my model, both Gemm and LeakyReLu have an attribute of the same name), so he disabled optimization when Gemm is followed by a LeakyReLu. This works for now, but we think it would be better if the name conflict is removed in the code.
		</comment>
	</comments>
</bug>