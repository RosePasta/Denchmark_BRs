<bug id='6004' author='jaroslavgratz' open_date='2020-12-02T17:24:26Z' closed_time='2020-12-04T08:42:07Z'>
	<summary>Incorrect INT8 quantized BERT model prediction on AVX2 only CPU</summary>
	<description>
Inference of a BERT INT8 quantized model gives incorrect result on AVX2 only (no AVX-512 support) CPU. Please note this issue is related to INT8 quantization and AVX2 CPU only . INT8 quantized model works correctly on AVX-512 CPU. UINT8 (unsigned int) quantized model works correctly on both AVX2 CPU and AVX-512 CPU.
It is possible to reproduce this bug on any CPU by running inference under valgrind (valgrind does't support AVX-512). Moreover valgrind reports access to uninitialized variables during inference (likely to be related to incorrect prediction result):
&lt;denchmark-code&gt;Conditional jump or move depends on uninitialised value(s)
   at 0x515C8E7: onnxruntime::DynamicQuantizeLinear&lt;unsigned char&gt;::Compute(onnxruntime::OpKernelContext*) const (in /home/jag/bootstrap/local/lib/libonnxruntime.so.1.5.3)
   by 0x5346538: onnxruntime::SequentialExecutor::Execute(onnxruntime::SessionState const&amp;, std::vector&lt;int, std::allocator&lt;int&gt; &gt; const&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt; const&amp;, std::vector&lt;int, std::allocator&lt;int&gt; &gt; const&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt;&amp;, std::unordered_map&lt;unsigned long, std::function&lt;onnxruntime::common::Status (onnxruntime::TensorShape const&amp;, OrtMemoryInfo const&amp;, OrtValue&amp;, bool&amp;)&gt;, std::hash&lt;unsigned long&gt;, std::equal_to&lt;unsigned long&gt;, std::allocator&lt;std::pair&lt;unsigned long const, std::function&lt;onnxruntime::common::Status (onnxruntime::TensorShape const&amp;, OrtMemoryInfo const&amp;, OrtValue&amp;, bool&amp;)&gt; &gt; &gt; &gt; const&amp;, onnxruntime::logging::Logger const&amp;) (in /home/jag/bootstrap/local/lib/libonnxruntime.so.1.5.3)
   by 0x533233B: onnxruntime::utils::ExecuteGraphImpl(onnxruntime::SessionState const&amp;, onnxruntime::FeedsFetchesManager const&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt; const&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt;&amp;, std::unordered_map&lt;unsigned long, std::function&lt;onnxruntime::common::Status (onnxruntime::TensorShape const&amp;, OrtMemoryInfo const&amp;, OrtValue&amp;, bool&amp;)&gt;, std::hash&lt;unsigned long&gt;, std::equal_to&lt;unsigned long&gt;, std::allocator&lt;std::pair&lt;unsigned long const, std::function&lt;onnxruntime::common::Status (onnxruntime::TensorShape const&amp;, OrtMemoryInfo const&amp;, OrtValue&amp;, bool&amp;)&gt; &gt; &gt; &gt; const&amp;, ExecutionMode, bool const&amp;, onnxruntime::logging::Logger const&amp;, bool) (in /home/jag/bootstrap/local/lib/libonnxruntime.so.1.5.3)
   by 0x5334318: onnxruntime::utils::ExecuteGraph(onnxruntime::SessionState const&amp;, onnxruntime::FeedsFetchesManager&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt; const&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt;&amp;, ExecutionMode, bool const&amp;, onnxruntime::logging::Logger const&amp;, bool) (in /home/jag/bootstrap/local/lib/libonnxruntime.so.1.5.3)
   by 0x4EAF207: onnxruntime::InferenceSession::Run(OrtRunOptions const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt; const&amp;, std::vector&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::allocator&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::vector&lt;OrtValue, std::allocator&lt;OrtValue&gt; &gt;*, std::vector&lt;OrtDevice, std::allocator&lt;OrtDevice&gt; &gt; const*) (in /home/jag/bootstrap/local/lib/libonnxruntime.so.1.5.3)
   by 0x4E7CF63: OrtApis::Run(OrtSession*, OrtRunOptions const*, char const* const*, OrtValue const* const*, unsigned long, char const* const*, unsigned long, OrtValue**) (in /home/jag/bootstrap/local/lib/libonnxruntime.so.1.5.3)
   by 0x13A2A6: Run (onnxruntime_cxx_inline.h:475)
   by 0x13A2A6: Run (onnxruntime_cxx_inline.h:466)
&lt;/denchmark-code&gt;

System information

OS Platform and Distribution: Debian Buster
ONNX Runtime installed from (source or binary): source
ONNX Runtime version: 1.5.3
Python version: 3.7.3
GCC/Compiler version (if compiling from source): GCC 8.3.0

	</description>
	<comments>
		<comment id='1' author='jaroslavgratz' date='2020-12-02T19:42:51Z'>
		Hi &lt;denchmark-link:https://github.com/jaroslavgratz&gt;@jaroslavgratz&lt;/denchmark-link&gt;
 , would you please give us a unit test or model with test data to repro the issue?
		</comment>
		<comment id='2' author='jaroslavgratz' date='2020-12-02T23:07:25Z'>
		I tried to run all our unit test on a machine with AVX2(but without AVX512), then I searched "DynamicQuantizeLinear", I didn't find the similar thing.  So maybe we don't have a unit test to cover it, or it only shows up in some special software environments.
		</comment>
		<comment id='3' author='jaroslavgratz' date='2020-12-03T01:16:09Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
, we have unit test for DynamicQuantizeLinear: &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/9ec1ed42a809170b87474f5822c4557101812399/onnxruntime/test/providers/cpu/tensor/dynamic_quantize_linear_test.cc&gt;https://github.com/microsoft/onnxruntime/blob/9ec1ed42a809170b87474f5822c4557101812399/onnxruntime/test/providers/cpu/tensor/dynamic_quantize_linear_test.cc&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='4' author='jaroslavgratz' date='2020-12-03T06:22:51Z'>
		See please int8 quantized model in attachement. Code to reproduce:
&lt;denchmark-code&gt;#include &lt;onnxruntime/core/session/onnxruntime_cxx_api.h&gt;
#include &lt;iostream&gt;

int main(int argc, char** argv)
{
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "onnxruntime");

    Ort::SessionOptions options;
    options.SetIntraOpNumThreads(1);
    options.SetInterOpNumThreads(1);
    Ort::Session session(env, "model.onnx", options);

    std::vector&lt;const char*&gt; inputNames = {"input_ids", "attention_mask", "token_type_ids" };
    std::vector&lt;const char*&gt; outputNames = {"output"};

    // dummy input
    std::vector&lt;int64_t&gt; ids(128, 1);
    std::vector&lt;int64_t&gt; attention_mask(128, 1);
    std::vector&lt;int64_t&gt; type_ids(128, 1);

    int64_t ids_dims[2] = {1, static_cast&lt;int64_t&gt;(ids.size())};
    int64_t type_ids_dims[2] = {1, static_cast&lt;int64_t&gt;(type_ids.size())};
    int64_t attention_mask_dims[2] = {1, static_cast&lt;int64_t&gt;(attention_mask.size())};

    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);

    std::vector&lt;Ort::Value&gt; inputs;
    inputs.emplace_back(Ort::Value::CreateTensor&lt;int64_t&gt;(memory_info, ids.data(), ids.size(), ids_dims, 2));
    inputs.emplace_back(Ort::Value::CreateTensor&lt;int64_t&gt;(memory_info, attention_mask.data(), attention_mask.size(), attention_mask_dims, 2));
    inputs.emplace_back(Ort::Value::CreateTensor&lt;int64_t&gt;(memory_info, type_ids.data(), type_ids.size(), type_ids_dims, 2));

    auto result = session.Run(Ort::RunOptions{nullptr}, inputNames.data(), inputs.data(), inputNames.size(), outputNames.data(), outputNames.size());

    std::cout &lt;&lt; "RESULT: " &lt;&lt; *result.front().GetTensorData&lt;float&gt;() &lt;&lt; std::endl;

    return 0;
}
&lt;/denchmark-code&gt;

Result on Xeon(R) Silver 4210 CPU (AVX-512 including VNNI): 1.34879
Result on AMD EPYC 7401P (no AVX-512): 1.43256
Result under valgrind (no AVX-512): 1.43256
Please note that I don't see valgrind errors about access to uninitialized data in this simplified test, however inference results are different on AVX2 and AVX-512 CPUs.
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/5634050/model.zip&gt;model.zip&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='jaroslavgratz' date='2020-12-03T17:54:03Z'>
		&lt;denchmark-link:https://github.com/jaroslavgratz&gt;@jaroslavgratz&lt;/denchmark-link&gt;
 , your issue is caused by saturation that is explained in another thread: &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/5849#issuecomment-733178450&gt;#5849 (comment)&lt;/denchmark-link&gt;
. Could you please try the reduce_range option when quantizing your model to see if the issue is resolved?
		</comment>
		<comment id='6' author='jaroslavgratz' date='2020-12-04T08:42:07Z'>
		Thank you for your explanation. In fact, different prediction on non-VNNI hardware is not an issue for me. I can ensure a model will run on VNNI machines in production. However, model accuracy loss can be quite high on non-VNNI hardware and that can be confusing for users not familiar with AVX instuction sets differences. Please consider use of a 32-bit accumulator for 8-bit multiply results for non-VNNI algorithm, that can improve user experience significantly.
		</comment>
	</comments>
</bug>