<bug id='380' author='jongse-park' open_date='2019-01-25T08:01:25Z' closed_time='2019-03-14T21:10:52Z'>
	<summary>ONNXRuntime Issue: Output:Y [ShapeInferenceError] Mismatch between number of source and target dimensions</summary>
	<description>
Describe the bug
A clear and concise description of what the bug is.
I am trying to build a onnx graph using helper APIs. The simplest example I started is the following. A MatMul op that takes two [1] matrix inputs (X and W), and produces [1] matrix output Y.
&lt;denchmark-code&gt;import numpy as np
import onnxruntime as rt
from onnx import *
from onnxmltools.utils import save_mode

initializer = []
initializer.append(helper.make_tensor(name="W", data_type=TensorProto.FLOAT, dims=(1,), vals=np.ones(1).tolist()))

graph = helper.make_graph(
    [
        helper.make_node('MatMul', ["X", "W"], ["Y"]),
    ],
    "TEST",
    [
        helper.make_tensor_value_info('X' , TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('W', TensorProto.FLOAT, [1]),
    ],
    [
        helper.make_tensor_value_info('Y', TensorProto.FLOAT, [1]),
    ],
    initializer=initializer,
    )

checker.check_graph(graph)
model = helper.make_model(graph, producer_name='TEST')
save_model(model, "model.onnx")
sess = rt.InferenceSession('model.onnx')
&lt;/denchmark-code&gt;

When I ran this, it complains like this:
&lt;denchmark-code&gt;Traceback (most recent call last):
File "onnxruntime_test.py", line 35, in &lt;module&gt;
sess = rt.InferenceSession('model.onnx')
File "/usr/local/lib/python3.5/dist-packages/onnxruntime/capi/session.py", line 29, in __init__
self._sess.load_model(path_or_bytes)
RuntimeError: [ONNXRuntimeError] : 1 : GENERAL ERROR : Node: Output:Y [ShapeInferenceError] Mismatch between number of source and target dimensions. Source=0 Target=1
&lt;/denchmark-code&gt;

I am stuck here for hours. Could anybody please give me any help?
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
ONNX Runtime installed from (source or binary): binary
ONNX Runtime version: 0.1.4
Python version: 3.5.2
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 9.1
GPU model and memory: Titan V

To Reproduce
Describe steps/code to reproduce the behavior:
python3 CODE.py
Expected behavior
A clear and concise description of what you expected to happen.
I expect the InferenceSession is created successfully without any issues.
Screenshots
If applicable, add screenshots to help explain your problem.
Additional context
Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='jongse-park' date='2019-03-14T21:10:52Z'>
		Hi, it's not an ONNX Runtime issue. It's an ONNX issue.
I changed your code as:
import numpy as np
import onnxruntime as rt
from onnx import *
from onnx import utils
initializer = []
initializer.append(helper.make_tensor(name="W", data_type=TensorProto.FLOAT, dims=(1,), vals=np.ones(1).tolist()))

graph = helper.make_graph(
    [
        helper.make_node('MatMul', ["X", "W"], ["Y"]),
    ],
    "TEST",
    [
        helper.make_tensor_value_info('X' , TensorProto.FLOAT, [1]),
        helper.make_tensor_value_info('W', TensorProto.FLOAT, [1]),
    ],
    [
        helper.make_tensor_value_info('Y', TensorProto.FLOAT, []),
    ],
    initializer=initializer,
    )

checker.check_graph(graph)
model = helper.make_model(graph, producer_name='TEST')
final_model = onnx.utils.polish_model(model)
onnx.save(final_model, 'model.onnx')
sess = rt.InferenceSession('model.onnx')
And it works fine.
The core part is: is it allowed to represent a scale with shape of "[1]", instead of "[]"? It's an ONNX spec issue, not runtime issue.
		</comment>
	</comments>
</bug>