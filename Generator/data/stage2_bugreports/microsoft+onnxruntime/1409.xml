<bug id='1409' author='xgirones' open_date='2019-07-15T21:47:29Z' closed_time='2020-07-25T07:40:18Z'>
	<summary>"CUDA driver version is insufficent for CUDA runtime version" exception raised when calling OrtCreateSessionFromArray in spite of the CUDA provider not being selected in session options</summary>
	<description>
Describe the bug
It should be possible to use the ONNX runtime shared library compiled with CUDA support on systems lacking a compatible GPU, provided that the CUDA provider is not selected in session options. It seems that this was the default behavior time ago. However, when calling the OrtCreateSessionFromArray function on an instance of the shared library compiled from code pulled from the master branch in 13/7/2019 the following exception is raised:
onnxruntime::CudaCall CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version.
The call stack is
&lt;denchmark-code&gt;onnxruntime.dll!onnxruntime::CudaCall&lt;enum cudaError,1&gt;(cudaError retCode, const char * exprString, const char * libName, cudaError successCode, const char * msg) Line 91 at e:\repo\ort\onnxruntime\core\providers\cuda\cuda_call.cc(91)
onnxruntime.dll!onnxruntime::GPUDataTransfer::GPUDataTransfer() Line 12	at e:\repo\ort\onnxruntime\core\providers\cuda\gpu_data_transfer.cc(12)
[Inline Frame] onnxruntime.dll!std::make_unique() Line 2539 at c:\program files (x86)\microsoft visual studio\2017\enterprise\vc\tools\msvc\14.16.27023\include\memory(2539)
onnxruntime.dll!onnxruntime::InferenceSession::InferenceSession(const onnxruntime::SessionOptions &amp; session_options, onnxruntime::logging::LoggingManager * logging_manager) Line 109 at e:\repo\ort\onnxruntime\core\session\inference_session.cc(109)
[Inline Frame] onnxruntime.dll!std::make_unique(const onnxruntime::SessionOptions &amp;&amp;) Line 2539	at c:\program files (x86)\microsoft visual studio\2017\enterprise\vc\tools\msvc\14.16.27023\include\memory(2539)
onnxruntime.dll!`anonymous namespace'::CreateSessionImpl&lt;&lt;lambda_4767107063818b362d43c342e703dc4b&gt; &gt;(OrtEnv * env, const OrtSessionOptions * options, OrtCreateSessionFromArray::__l3::&lt;lambda_4767107063818b362d43c342e703dc4b&gt; loader, OrtSession * * out) Line 370 at e:\repo\ort\onnxruntime\core\session\onnxruntime_c_api.cc(370)
onnxruntime.dll!OrtCreateSessionFromArray(OrtEnv * env, const void * model_data, unsigned __int64 model_data_length, const OrtSessionOptions * options, OrtSession * * out) Line 414 at e:\repo\ort\onnxruntime\core\session\onnxruntime_c_api.cc(414)
&lt;/denchmark-code&gt;

At first glance it appears that the cause could be the following code in inference_session.cc (line 109)
&lt;denchmark-code&gt;  // Register data transfer methods.
#ifdef USE_CUDA
  data_transfer_mgr_.RegisterDataTransfer(std::make_unique&lt;GPUDataTransfer&gt;());
#endif
&lt;/denchmark-code&gt;

Mi impression is that the data_transfer_mgr_.RegisterDataTransfer(std::make_unique&lt;GPUDataTransfer&gt;()); statement in the InferenceSession constructor is executed irrespectively of the CUDA provider being selected in the session options or not.
Moreover, it seems that the shared library has now  a direct dependency on cublas64_10.dll, whereas in previous versions of the library this was a delay-load dependency. This dll should be configured as delay-load like the other CUDA dependencies in order to enable the deployment of the onnxruntime shared library without having to redistribute the CUDA binaries on CPU only systems.
Urgency
I am stuck on this.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
ONNX Runtime installed from (source or binary): Source
ONNX Runtime version: Code pulled from the master branch in 13/7/2019
Python version: 3.6
Visual Studio version (if applicable): 2017
GCC/Compiler version (if compiling from source):
CUDA/cuDNN version: 10.1/7.6
GPU model and memory:

To Reproduce
Call the  OrtCreateSessionFromArray function on the onnxruntime shared library compiled with CUDA support on a system without a compatible GPU.
Expected behavior
1- It should be possible to use the ONNX Runtime shared library compiled with CUDA support on systems lacking a compatible GPU, provided that the CUDA provider is not selected in session options.
2- All CUDA dependencies should be configured as delay-load.
Screenshots
If applicable, add screenshots to help explain your problem.
Additional context
Add any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.
	</description>
	<comments>
		<comment id='1' author='xgirones' date='2019-07-16T18:46:54Z'>
		we have the delay link specified: 


onnxruntime/cmake/CMakeLists.txt


        Lines 583 to 585
      in
      d38badf






 set(CMAKE_SHARED_LINKER_FLAGS "${CMAKE_SHARED_LINKER_FLAGS} /DELAYLOAD:cublas64_${onnxruntime_CUDA_VERSION_MAJOR}${onnxruntime_CUDA_VERSION_MINOR}.dll") 



 set(CMAKE_SHARED_LINKER_FLAGS "${CMAKE_SHARED_LINKER_FLAGS} /DELAYLOAD:cudart64_${onnxruntime_CUDA_VERSION_MAJOR}${onnxruntime_CUDA_VERSION_MINOR}.dll") 



 set(CMAKE_SHARED_LINKER_FLAGS "${CMAKE_SHARED_LINKER_FLAGS} /DELAYLOAD:cudnn64_7.dll") 





However, in cuda 10.1, seems they changed the rules for some naming, they have cublas64_10.dll, and cudart64_101.dll which breaks the rule in our code. will fix it.
		</comment>
		<comment id='2' author='xgirones' date='2019-07-16T20:23:52Z'>
		&lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1418&gt;#1418&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='xgirones' date='2019-07-16T20:54:07Z'>
		Thanks for the quick fix of the delay-load import.
Any suggestion about the first part of the issue? That is the one that is really a stopper for me.
		</comment>
		<comment id='4' author='xgirones' date='2019-07-16T21:49:12Z'>
		There are 3 issues here:

Python api don't have an option to register provider manually.



onnxruntime/onnxruntime/python/onnxruntime_pybind_state.cc


        Lines 216 to 268
      in
      c2aa205






 inline void RegisterExecutionProvider(InferenceSession* sess, onnxruntime::IExecutionProviderFactory&amp; f) { 



 auto p = f.CreateProvider(); 



 auto status = sess-&gt;RegisterExecutionProvider(std::move(p)); 



 if (!status.IsOK()) { 



 throw std::runtime_error(status.ErrorMessage().c_str()); 



   } 



 } 



 



 void InitializeSession(InferenceSession* sess) { 



   onnxruntime::common::Status status; 



 



 #ifdef USE_TENSORRT 



   { 



 RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_Tensorrt()); 



   } 



 #endif 



 



 #ifdef USE_CUDA 



   { 



 RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_CUDA(0)); 



   } 



 #endif 



 



 #ifdef USE_MKLDNN 



   { 



 const bool enable_cpu_mem_arena = true; 



 RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_Mkldnn(enable_cpu_mem_arena ? 1 : 0)); 



   } 



 #endif 



 



 #if USE_NGRAPH 



   { 



 RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_NGraph("CPU")); 



   } 



 #endif 



 



 #ifdef USE_OPENVINO 



   { 



 RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_OpenVINO("CPU")); 



   } 



 #endif 



 



 #if 0  //USE_NUPHAR 



   { 



     RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_Nuphar(0, "")); 



   } 



 #endif 



 



 #ifdef USE_BRAINSLICE 



   { 



 RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_BrainSlice(0, -1, -1, false, "", "", "")); 



   } 



 #endif 





#ifdef USE_CUDA
{
RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_CUDA(0));
}
It used to work for you becase there used to be a package with cuda dll inside which is wrong.
Is it possible for you to use C API?
data_transfer_mgr_.RegisterDataTransfer(std::make_unique()); uses MACRO #ifdef USE_CUDA
#1420
CUDA dll delay load for cuda 10.1, Nvidia dll naming change.
#1418

		</comment>
		<comment id='5' author='xgirones' date='2019-07-16T21:50:50Z'>
		Thanks a lot! I'm working on the 2nd above - data_transfer_mgr_.RegisterDataTransfer(std::make_unique()); uses MACRO #ifdef USE_CUDA. &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1420&gt;#1420&lt;/denchmark-link&gt;
 is fixing the 2nd above
		</comment>
		<comment id='6' author='xgirones' date='2019-07-16T22:19:47Z'>
		For the 1st issue, we firstly need to provide a Python API to enable user to register the execution provider manually. Once the API is ready we can remove the static register code in Pybind. Please expect our future update on this.
		</comment>
		<comment id='7' author='xgirones' date='2019-07-17T15:49:05Z'>
		Thanks for your response.

It used to work for you becase there used to be a package with cuda dll inside which is wrong.
Is it possible for you to use C API?

I mostly use the C API exported by the shared library.  On the other hand, it will be great to be able to register the CUDA provider on Python: I use the Python version mainly for testing, and so far I have to keep two separate environments, one for CPU and another for GPU.
		</comment>
		<comment id='8' author='xgirones' date='2019-07-19T18:20:22Z'>
		For the 1st issue, we created a work item for this as it need time to make it happen. We need to find a solution for delay loading on Linux, then enable registering execution providing manually with session option (the session option set to CUDA for GPU package by default).
For now, please install CPU package on CPU only machine, or use C API.
		</comment>
		<comment id='9' author='xgirones' date='2019-07-19T19:56:55Z'>
		Using the C API did not work for me. I found this issue in the shared library.
		</comment>
		<comment id='10' author='xgirones' date='2019-07-19T22:09:30Z'>
		did you specify the option "--build_shared_lib" when you build it?
		</comment>
		<comment id='11' author='xgirones' date='2019-07-20T10:48:50Z'>
		yes, I used the following statement:
build.bat --config RelWithDebInfo --use_mkldnn --use_cuda --build_shared_lib  --cudnn_home %CUDNN_PATH% --cuda_home "%CUDA_PATH%"
		</comment>
		<comment id='12' author='xgirones' date='2019-07-21T22:01:49Z'>
		I have compiled the shared library from the code pulled today from the master branch and the problem is gone. Thanks a lot!
Unfortunately, a new issue has appeared when doing inference on batches of sequences of varying lengths from multiple threads (on a single session, and also when using one session per thread). Eventually this error appears:
OrtRun failed: Non-zero status code returned while running Node.
Any idea?
		</comment>
		<comment id='13' author='xgirones' date='2019-07-22T18:58:15Z'>
		single session with multiple threads not work for CUDA because of the memory life cycle issue. It should work if you have separate session (Ort::Session) on different thread. How about the CPU side? has same issue or only on GPU?
		</comment>
		<comment id='14' author='xgirones' date='2019-07-22T20:31:51Z'>
		This only occurs on GPU when using the model referenced in &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1400&gt;#1400&lt;/denchmark-link&gt;
. I also tried with a model that does not involve RNN and it worked fine.
Parallel RNN evaluation of batches of sequences with varying lengths used to work (although with incorrect results as reported in &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1400&gt;#1400&lt;/denchmark-link&gt;
) when employing one session per thread.
		</comment>
		<comment id='15' author='xgirones' date='2019-07-22T22:06:54Z'>
		&lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1463&gt;#1463&lt;/denchmark-link&gt;
 -- this one should fix it.
		</comment>
		<comment id='16' author='xgirones' date='2019-07-23T16:41:40Z'>
		Thanks. I have pulled the latest code from the master branch, recompiled the shared library, and tried again. I am afraid that the problem is still there.
OrtRun failed: Non-zero status code returned while running Node.
However, I can only reproduce it when I stress the system : 4 threads performing (CUDA) FFT and image resizing + 8 threads executing OrtRun (among other things) on the GPU. I can run other non RNN models without any problem, though.
		</comment>
		<comment id='17' author='xgirones' date='2019-07-23T20:40:24Z'>
		did you see OOM on CUDA memory? I still can't reproduce it using another GRU models with the model you provided (discrepancy_CPU_GPU).
		</comment>
		<comment id='18' author='xgirones' date='2019-07-24T20:40:14Z'>
		The CUDA FFT and NPP calls do not return any error status code, which I would expect if the GPU ran OOM. It only occurs with the RNN model, other models run fine.
I will investigate more on the issue in a couple of days, now I have to complete some tasks.
		</comment>
		<comment id='19' author='xgirones' date='2019-07-24T21:04:46Z'>
		Is it the same model you shared before (discrepancy_CPU_GPU)? Is it possible to share it?
		</comment>
		<comment id='20' author='xgirones' date='2019-07-25T18:54:21Z'>
		Yes, it is the same model as in the &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1400&gt;#1400&lt;/denchmark-link&gt;
 issue. I will try to investigate more during the next week.
		</comment>
		<comment id='21' author='xgirones' date='2019-07-28T18:22:30Z'>
		I managed to reproduce the issue running the discrepancy_CPU_GPU model alone. It seems that when using 256 seq minibatches and a couple of threads it is easier to make it appear. I employed the C API and the shared library because Python does not support true multi-threading. This is the error log:
&lt;denchmark-code&gt;2019-07-28 18:25:57.8231646 [E:onnxruntime:test, cuda_call.cc:93 onnxruntime::CudaCall] CUDNN failure 3: CUDNN_STATUS_BAD_PARAM ; GPU=0 ; hostname=NEBO ; expr=cudnnSetRNNDataDescriptor(tensor_, dataType, layout, static_cast&lt;int&gt;(max_seq_length), static_cast&lt;int&gt;(batch_size), static_cast&lt;int&gt;(data_size), seq_lengths, static_cast&lt;void*&gt;(&amp;padding_fill));
2019-07-28 18:25:57.8248668 [E:onnxruntime:test, cuda_call.cc:93 onnxruntime::CudaCall] CUDNN failure 3: CUDNN_STATUS_BAD_PARAM ; GPU=0 ; hostname=NEBO ; expr=cudnnSetRNNDataDescriptor(tensor_, dataType, layout, static_cast&lt;int&gt;(max_seq_length), static_cast&lt;int&gt;(batch_size), static_cast&lt;int&gt;(data_size), seq_lengths, static_cast&lt;void*&gt;(&amp;padding_fill));
2019-07-28 18:25:57.8259753 [E:onnxruntime:test, cuda_call.cc:93 onnxruntime::CudaCall] CUDNN failure 3: CUDNN_STATUS_BAD_PARAM ; GPU=0 ; hostname=NEBO ; expr=cudnnRNNForwardInferenceEx(CudnnHandle(), rnn_desc_, x_desc, x_data_input, hx_desc, hx_data, cx_desc, cx_data, weight_cached_ ? w_desc_cache_ : w_desc, weight_cached_ ? w_data_cache_.get() : w_data.get(), y_desc, y_data, y_h_desc, y_h_data,y_c_desc, y_c_data, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace_cuda.get(), workspace_bytes);
2019-07-28 18:25:57.8266446 [E:onnxruntime:, sequential_executor.cc:127 onnxruntime::SequentialExecutor::Execute] Non-zero status code returned while running Node:  Status Message:
&lt;/denchmark-code&gt;

		</comment>
		<comment id='22' author='xgirones' date='2019-07-30T16:36:14Z'>
		could you share some test data?
		</comment>
		<comment id='23' author='xgirones' date='2019-08-01T16:27:45Z'>
		It seems to occur irrespective of the data. I could be mistaken here but it looks like a race condition.
		</comment>
		<comment id='24' author='xgirones' date='2019-08-01T19:10:31Z'>
		please try the PR &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1544&gt;#1544&lt;/denchmark-link&gt;

		</comment>
		<comment id='25' author='xgirones' date='2019-08-07T14:42:22Z'>
		Thanks. I have pulled the cuda/rnn_race_condition branch, but I am still able to reproduce the issue.
		</comment>
		<comment id='26' author='xgirones' date='2019-08-09T20:17:41Z'>
		I tried with 12 threads and still can't reproduce it. Could you share the code? Also would you mind to try again with the latest master? Thanks!
		</comment>
		<comment id='27' author='xgirones' date='2019-08-16T16:51:14Z'>
		I just pulled the code from master and tried again, and got the same error:
&lt;denchmark-code&gt;2019-08-16 17:26:05.2081490 [E:onnxruntime:test, cuda_call.cc:93 onnxruntime::CudaCall] CUDNN failure 3: CUDNN_STATUS_BAD_PARAM ; GPU=0 ; hostname=; expr=cudnnSetRNNDataDescriptor(tensor_, dataType, layout, static_cast&lt;int&gt;(max_seq_length), static_cast&lt;int&gt;(batch_size), static_cast&lt;int&gt;(data_size), seq_lengths, static_cast&lt;void*&gt;(&amp;padding_fill));
2019-08-16 17:26:05.2092973 [E:onnxruntime:test, cuda_call.cc:93 onnxruntime::CudaCall] CUDNN failure 3: CUDNN_STATUS_BAD_PARAM ; GPU=0 ; hostname=; expr=cudnnSetRNNDataDescriptor(tensor_, dataType, layout, static_cast&lt;int&gt;(max_seq_length), static_cast&lt;int&gt;(batch_size), static_cast&lt;int&gt;(data_size), seq_lengths, static_cast&lt;void*&gt;(&amp;padding_fill));
2019-08-16 17:26:05.2100140 [E:onnxruntime:test, cuda_call.cc:93 onnxruntime::CudaCall] CUDNN failure 3: CUDNN_STATUS_BAD_PARAM ; GPU=0 ; hostname=; expr=cudnnRNNForwardInferenceEx(CudnnHandle(), rnn_desc, x_desc, x_data_input, hx_desc, hx_data, cx_desc, cx_data, weight_cached_ ? w_desc_cache_ : w_desc, weight_cached_ ? w_data_cache_.get() : w_data.get(), y_desc, y_data, y_h_desc, y_h_data, y_c_desc, y_c_data, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace_cuda.get(), workspace_bytes);
2019-08-16 17:26:05.2103945 [E:onnxruntime:, sequential_executor.cc:129 onnxruntime::SequentialExecutor::Execute] Non-zero status code returned while running Node
&lt;/denchmark-code&gt;

This is the function I am using for testing. It cannot be compiled as posted here because it has many dependencies (boost, tbb, private types), but I suppose it will show if I am using the ORT API incorrectly.
&lt;denchmark-code&gt;bool rnn_test_21() throw ( __EXCEPTION_ )
{
	CHECKERROR_NEST_TRY
	{
		static const __SIZE_T TEST_ID = 21;
		FolderUtils::_TestContext_aptr apTestCtx = FU.context_get( TEST_ID );

		typedef phd::engine::defaults::SIZE_T SIZE_T;
		typedef phd::engine::defaults::REAL_T REAL_T;
		typedef uint16_t COUNT_T;

		typedef nmspc_cne::Randomizer&lt;SIZE_T, REAL_T&gt; RND;

		static const optimization_target Opt = phd::engine::defaults::OPTIMIZATION_TARGET;
		static const bool bInterlocked = phd::engine::defaults::INTERLOCKED_REFERENCE_COUNTED;

	#if defined(_DEBUG)
		static const SIZE_T NUM_ITERATIONS = 10;
	#else
		static const SIZE_T NUM_ITERATIONS = 10000;
	#endif

		static const SIZE_T Ri = 32;
		static const SIZE_T Ci = 32;
		static const SIZE_T Ro = 24;
		static const SIZE_T Co = 16;

#if defined(_DEBUG)
		static const SIZE_T cRows = 16;
#else
		static const SIZE_T cRows = 256;
#endif

		static const SIZE_T rndSeed = 888;
		static const SIZE_T cElems2Purge = 10000;

		typedef ocr_basic_line_sampler_deferred &lt;SIZE_T, REAL_T, Ri, Ci, Ro, Co, RND, true, Opt, bInterlocked&gt; OBLS;
		typedef std::vector&lt;REAL_T&gt; real_vect;

		typedef CSEG::A2D A2D;

		OBLS::parms_t p;

		p.datasetPath = L"D:\\projects\\PhD\\datasets\\ocr_basic_dataset\\toy\\toy.xml";
		p.selectedGroups.push_back( OBLS::parms_t::SELECT_ALL_GROUPS() );
		p.glyphs = OBLS::parms_t::SELECT_ALL_GLYPHS();

		p.cMin_line_glyphs = 1;
		p.cMax_line_glyphs = 3;
		p.cAlgn_line_length = 4;

		p.fMin_glyph_separation_X = 0.6f;
		p.fMax_glyph_separation_X = 32.f;
		p.fSigma_rayleigh_glyph_separation_X = 1.25f;
		p.fMax_glyph_translation_Y = 0;
		p.fRange_glyph_aspect_ratio = 0.025f;
		p.fRange_glyph_shrink_ratio = 0.f;

		p.fRange_aspect_ratio = 0.1f;
		p.fRange_shrink_ratio = 0.1f;
		p.fRange_rotation = 0.0349066f;
		p.fMax_shear = 0.1f;

		p.mapInitSeed = 888;

#if defined(_DEBUG)
		p.fScale_noise_map_X = 8;
		p.fScale_noise_map_Y = 8;
		p.cMaps_distortion = 16;
#else
		p.fScale_noise_map_X = 32;
		p.fScale_noise_map_Y = 64;
		p.cMaps_distortion = 16384;
#endif
		p.fMap_alpha = 0.2f;
		p.fMap_sigma = 4.f;

		p.fRate_additive_noise = 0.005f;
		p.fRange_additive_noise_radius = 2.f;

		p.fRate_subtractive_noise = 0.0025f;
		p.fRange_subtractive_noise_radius = 2.0f;

		p.fRange_box_filter_radius = 2.0f;

		p.fRange_gamma = 1.5f;
		p.cIntervals_gamma = 16;

		p.bTranspose = true;

		OBLS::_self_aptr apOBLS = OBLS::create( p, cRows, rndSeed, cElems2Purge );
		OBLS::_sampler_results_pool_elem_aptr apSamples = apOBLS-&gt;generate_samples();
		OBLS::sampler_results_t &amp;rSR = apSamples-&gt;get().get();

#if 0
		// Sort batch in descending order according to the sequence lengths.
		std::sort( rSR.lines.begin(), rSR.lines.end(),
			[] ( const OBLS::line_sample_t &amp;rcL1, const OBLS::line_sample_t &amp;rcL2 ) -&gt; bool
		{
			return rcL1.fsample.numRows_get() &gt; rcL2.fsample.numRows_get();
		} );

		SIZE_T paddedR = rSR.lines.front().sample.numRows_get();
		BOOST_ASSERT( paddedR == rSR.lines.front().flabels.numRows_get() );
#else
		SIZE_T paddedR = 0;

		for ( size_t j = 0; j &lt; rSR.lines.size(); ++j )
		{
			if ( rSR.lines[ j ].fsample.numRows_get() &gt; paddedR )
			{
				paddedR = rSR.lines[ j ].fsample.numRows_get();
			}
		}
#endif

		// ONNX requires all sequences to be of the same size so we must allocate input tensors with the maximum sizes and pad the unused elements. 

		SIZE_T cBatchSize = (SIZE_T)rSR.lines.size();
		BOOST_ASSERT( cBatchSize == cRows );

		SIZE_T paddedInputC = rSR.lines.front().sample.numCols_get();
		SIZE_T paddedLabelC = rSR.lines.front().flabels.numCols_get();
		BOOST_ASSERT( paddedLabelC == (SIZE_T)apOBLS-&gt;alphabet_get().size() );

		SIZE_T cPaddedInputStep3 = paddedInputC * (SIZE_T)sizeof(REAL_T);
		SIZE_T cPaddedInputStep2 = paddedR * cPaddedInputStep3;
		SIZE_T cPaddedInputSize = cBatchSize * cPaddedInputStep2;

		SIZE_T cPaddedLabelStep3 = paddedLabelC * (SIZE_T)sizeof(REAL_T);
		SIZE_T cPaddedLabelStep2 = paddedR * cPaddedLabelStep3;

		real_vect vfInput( (size_t)cPaddedInputSize / sizeof(REAL_T) );
		std::vector&lt;int64_t&gt; vnLengths( (size_t)cBatchSize );

		REAL_T *pfCurInputSeq = vfInput.data();

		for ( size_t j = 0; j &lt; (size_t)cBatchSize; ++j )
		{
			OBLS::line_sample_t &amp;rL = rSR.lines[ (size_t)j ];

			SIZE_T cSeqLen = rL.fsample.numRows_get();
			BOOST_ASSERT( ( cSeqLen % p.cAlgn_line_length ) == 0 );

			vnLengths[ j ] = (int64_t)( cSeqLen / p.cAlgn_line_length );

			A2D a2dInputSeq( paddedInputC, cSeqLen, pfCurInputSeq, cPaddedInputStep3 );
			a2dInputSeq = rL.fsample;

			pfCurInputSeq = offset_ptr&lt;REAL_T&gt;( pfCurInputSeq, cPaddedInputStep2 );
		}

		// Initialize the ONNX runtime.

		_OrtEnv_aptr apEnv = onnxrt_create_env( ORT_LOGGING_LEVEL_WARNING, "test" );

		_OrtSessionOptions_aptr apSOpt = onnxrt_create_session_options();
		onnxrt_session_options_set_session_log_verbosity_level( apSOpt, OrtLoggingLevel::ORT_LOGGING_LEVEL_VERBOSE );
		onnxrt_session_options_set_graph_optimization_level( apSOpt, GraphOptimizationLevel::ORT_ENABLE_ALL );
		onnxrt_session_options_append_execution_provider_cuda( apSOpt, 0 );

		static const wchar_t *PATH_MODEL = L"D:\\temp\\_model.onnx";

		size_t NUM_SESSIONS = 2;

		std::vector&lt;onnxrt_session_info::_self_aptr&gt; sessions( NUM_SESSIONS );
		for ( size_t i = 0; i &lt; NUM_SESSIONS; ++i )
		{
			_OrtSession_aptr apS = onnxrt_create_session( apEnv, PATH_MODEL, apSOpt );
			sessions[ i ] = onnxrt_session_info::create( apS );
		}

		boost::mutex mux;
		boost::condition_variable cv;

		// Run the test.

		tbb::parallel_for( tbb::blocked_range&lt;SIZE_T&gt;( 0, NUM_ITERATIONS ),
			[ &amp; ]( const tbb::blocked_range&lt;SIZE_T&gt; &amp;rcR )
		{
			// Fetch a session from the session pool.
			onnxrt_session_info::_self_aptr apSI;

			{
				boost::mutex::scoped_lock lock( mux );

				while ( sessions.empty() )
				{
					cv.wait( lock );
				}

				apSI = sessions.back();
				sessions.pop_back();
			}

			_OrtSession_aptr apS = apSI-&gt;session_get();

			_OrtAllocatorInfo_aptr apAllocInfo = onnxrt_create_cpu_allocator_info( OrtArenaAllocator, OrtMemTypeDefault );

			// Create the input tensor.

			// Set the correct dimensions.
			onnxrt_shape_t inputDim = apSI-&gt;input_nodes_info_get().front().shape;
			inputDim[ 0 ] = (int64_t)cBatchSize;
			inputDim[ 2 ] = (int64_t)paddedR;

			_OrtValue_aptr apInputTensor = onnxrt_create_tensor_with_data_as_ort_value( apAllocInfo, vfInput.data(), vfInput.size() * sizeof(REAL_T), inputDim.data(), inputDim.size(), ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT );
			CHECKERROR_GENERIC( phd::onnxrt::onnxrt_is_tensor( apInputTensor ) ? S_OK : E_FAIL, _T( "Invalid input tensor returned" ) );

			// Create the lengths tensor.

			// Set the correct dimensions.
			onnxrt_shape_t lengthsDim = apSI-&gt;input_nodes_info_get().back().shape;
			lengthsDim[ 0 ] = (onnxrt_dim_t)cBatchSize;

			_OrtValue_aptr apLengthsTensor = onnxrt_create_tensor_with_data_as_ort_value( apAllocInfo, vnLengths.data(), vnLengths.size() * sizeof( onnxrt_dim_t ), lengthsDim.data(), lengthsDim.size(), ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64 );
			CHECKERROR_GENERIC( phd::onnxrt::onnxrt_is_tensor( apLengthsTensor ) ? S_OK : E_FAIL, _T( "Invalid input tensor returned" ) );

			for ( SIZE_T i = rcR.begin(); i &lt; rcR.end(); ++i )
			{
				// Run the model.

				onnxrt_values_t vapInputTensor( apSI-&gt;input_nodes_info_get().size() );
				BOOST_ASSERT( vapInputTensor.size() == 2 );

				vapInputTensor[ 0 ] = apInputTensor;
				vapInputTensor[ 1 ] = apLengthsTensor;
			
				onnxrt_values_t vapOutputTensor;
				onnxrt_run( vapOutputTensor, apS, apSI-&gt;input_raw_names_get(), apSI-&gt;output_raw_names_get(), vapInputTensor );
				BOOST_ASSERT( vapOutputTensor.size() == 1 );
				CHECKERROR_GENERIC( phd::onnxrt::onnxrt_is_tensor( vapOutputTensor.front() ) ? S_OK : E_FAIL, _T( "Invalid output tensor returned" ) );
			}

			// Return the session to the pool.
			{
				boost::mutex::scoped_lock lock( mux );
				sessions.push_back( apSI );
				apS.reset();
				apSI.release();
			
				cv.notify_one();
			}
		} );
	}
	CHECKERROR_NEST_CATCHBLOCK( _T( "rnn_test_21" ) )

	return true;
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='28' author='xgirones' date='2019-08-19T18:30:13Z'>
		I haven't try the code yet, but we do have similar issue reported from another user, and it turns out that it has 0 sequence in some batch. e.g, the batch_size 3, max_sequencelength = 2, the sequenceArray is [2, 0, 2], the 0 sequence in the batch is not supported by cudnn. I guess it's could be same for your case.
&lt;denchmark-link:https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnSetRNNDataDescriptor&gt;https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnSetRNNDataDescriptor&lt;/denchmark-link&gt;

CUDNN_STATUS_BAD_PARAM
Any one of these have occurred:
An element of seqLengthArray is less than or equal to zero or greater than maxSeqLength.
I'm working on that fix right now. will verify that fix for your case as well.
		</comment>
		<comment id='29' author='xgirones' date='2019-08-19T20:05:10Z'>
		Thanks. I will add a check for this condition and run the test again.
		</comment>
		<comment id='30' author='xgirones' date='2019-08-21T15:13:40Z'>
		I ran the test several times checking for the zero length sequence condition but it did not happen. The error CUDNN_STATUS_BAD_PARAM appears with non-zero sequence lengths. And in one of the executions I got a different error:
 Exception thrown at 0x000007FE9F75F57E (cudnn64_7.dll) in workbench.exe: 0xC0000005: Access violation reading location 0x00000000000000A8.
&lt;denchmark-code&gt;onnxruntime.dll!onnxruntime::cuda::CudnnRnnBase&lt;float&gt;::ComputeInternal(onnxruntime::OpKernelContext * ctx) Line 260	C++
onnxruntime.dll!onnxruntime::cuda::CudaKernel::Compute(onnxruntime::OpKernelContext * p_op_kernel_context) Line 42	C++
onnxruntime.dll!onnxruntime::SequentialExecutor::Execute(const onnxruntime::SessionState &amp; session_state, const std::vector&lt;int,std::allocator&lt;int&gt; &gt; &amp; feed_mlvalue_idxs, const std::vector&lt;OrtValue,std::allocator&lt;OrtValue&gt; &gt; &amp; feeds, const std::vector&lt;int,std::allocator&lt;int&gt; &gt; &amp; fetch_mlvalue_idxs, std::vector&lt;OrtValue,std::allocator&lt;OrtValue&gt; &gt; &amp; fetches, const std::unordered_map&lt;unsigned __int64,std::function&lt;onnxruntime::common::Status __cdecl(onnxruntime::TensorShape const &amp;,OrtValue &amp;)&gt;,std::hash&lt;unsigned __int64&gt;,std::equal_to&lt;unsigned __int64&gt;,std::allocator&lt;std::pair&lt;unsigned __int64 const ,std::function&lt;onnxruntime::common::Status __cdecl(onnxruntime::TensorShape const &amp;,OrtValue &amp;)&gt; &gt; &gt; &gt; &amp; fetch_allocators, const onnxruntime::logging::Logger &amp; logger) Line 121	C++
onnxruntime.dll!onnxruntime::utils::ExecuteGraph(const onnxruntime::SessionState &amp; session_state, onnxruntime::FeedsFetchesManager &amp; feeds_fetches_manager, const std::vector&lt;OrtValue,std::allocator&lt;OrtValue&gt; &gt; &amp; feeds, std::vector&lt;OrtValue,std::allocator&lt;OrtValue&gt; &gt; &amp; fetches, const std::unordered_map&lt;unsigned __int64,std::function&lt;onnxruntime::common::Status __cdecl(onnxruntime::TensorShape const &amp;,OrtValue &amp;)&gt;,std::hash&lt;unsigned __int64&gt;,std::equal_to&lt;unsigned __int64&gt;,std::allocator&lt;std::pair&lt;unsigned __int64 const ,std::function&lt;onnxruntime::common::Status __cdecl(onnxruntime::TensorShape const &amp;,OrtValue &amp;)&gt; &gt; &gt; &gt; &amp; fetch_allocators, bool sequential_execution, const bool &amp; terminate_flag, const onnxruntime::logging::Logger &amp; logger, bool cache_copy_info) Line 508	C++
onnxruntime.dll!onnxruntime::InferenceSession::Run(const OrtRunOptions &amp; run_options, const std::vector&lt;std::basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt; &gt;,std::allocator&lt;std::basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt; &gt; &gt; &gt; &amp; feed_names, const std::vector&lt;OrtValue,std::allocator&lt;OrtValue&gt; &gt; &amp; feeds, const std::vector&lt;std::basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt; &gt;,std::allocator&lt;std::basic_string&lt;char,std::char_traits&lt;char&gt;,std::allocator&lt;char&gt; &gt; &gt; &gt; &amp; output_names, std::vector&lt;OrtValue,std::allocator&lt;OrtValue&gt; &gt; * p_fetches) Line 738	C++
onnxruntime.dll!OrtRun(OrtSession * sess, const OrtRunOptions * run_options, const char * const * input_names, const OrtValue * const * input, unsigned __int64 input_len, const char * const * output_names1, unsigned __int64 output_names_len, OrtValue * * output) Line 461	C++
workbench.exe!phd::onnxrt::onnxrt_run(std::vector&lt;std::shared_ptr&lt;OrtValue&gt;,std::allocator&lt;std::shared_ptr&lt;OrtValue&gt; &gt; &gt; &amp; rOutVals, std::shared_ptr&lt;OrtSession&gt; &amp; rapS, const std::vector&lt;char const *,std::allocator&lt;char const *&gt; &gt; &amp; rcInputNames, const std::vector&lt;char const *,std::allocator&lt;char const *&gt; &gt; &amp; rcOutputNames, const std::vector&lt;std::shared_ptr&lt;OrtValue&gt;,std::allocator&lt;std::shared_ptr&lt;OrtValue&gt; &gt; &gt; &amp; rcInputVals, const OrtRunOptions *) Line 418	C++
workbench.exe!ocr_basic_dev_21::__l3::&lt;lambda&gt;(const tbb::blocked_range&lt;unsigned int&gt; &amp; rcR) Line 4747	C++
&lt;/denchmark-code&gt;

PS. When I use only a single thread the test always runs smoothly. I am using CUDA 10.1, I do not know if this is relevant for this case.
		</comment>
		<comment id='31' author='xgirones' date='2019-09-09T22:59:51Z'>
		I'm working on something high priority recently, so postpone further investigation once I'm free. Also welcome to have someone contribute it if possible.
		</comment>
		<comment id='32' author='xgirones' date='2019-09-13T08:00:05Z'>
		Thanks for the update, but I think that there is still a bug. I have had to disable GPU evaluation for my RNN models as  I need to run multiple parallel batched evaluations on small networks, which I think is the reason why I am seeing this problem as it increases the risk of a data race.
		</comment>
		<comment id='33' author='xgirones' date='2019-10-11T22:26:36Z'>
		&lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1826&gt;#1826&lt;/denchmark-link&gt;

		</comment>
		<comment id='34' author='xgirones' date='2020-07-18T07:16:15Z'>
		This issue has been automatically marked as stale due to inactivity and will be closed in 7 days if no further activity occurs. If further support is needed, please provide an update and/or more details.
		</comment>
		<comment id='35' author='xgirones' date='2020-07-25T07:39:56Z'>
		This issue has been automatically closed due to inactivity. Please reactivate if further support is needed.
		</comment>
	</comments>
</bug>