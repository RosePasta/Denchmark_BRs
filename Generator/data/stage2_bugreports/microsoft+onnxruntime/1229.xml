<bug id='1229' author='antonfr' open_date='2019-06-14T10:48:48Z' closed_time='2019-07-11T18:35:33Z'>
	<summary>Problem with loading converted onnx model</summary>
	<description>

I have converted to onnx ssdlite_mobilenet_v2_coco model from tensorflow detection model zoo (could be found &lt;denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md&gt;here&lt;/denchmark-link&gt;
). Now I'm trying to load the model using ML.NET and get an error
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
ONNX Runtime installed from (source or binary): NuGet
ONNX Runtime version: 0.4.0
Python version: 3.6.7
Visual Studio version (if applicable): 8.0.5
GCC/Compiler version (if compiling from source): none
CUDA/cuDNN version: none
GPU model and memory: none

To Reproduce
`        public struct ImageSettings
{
public const int ImageWidth = 300;
public const int ImageHeight = 300;
public const bool ChannelLast = true;
}
&lt;denchmark-code&gt;    public struct SSDSettings
    {
        public const string SSDInput = "image_tensor:0";
        public const string SSDDetectionsOutput = "num_detections:0";
        public const string SSDClassesOutput = "detection_classes:0";
        public const string SSDBoxesOutput = "detection_boxes:0";
        public const string SSDScoresOutput = "detection_scores:0";
    }

    public PredictionEngine&lt;ImageData, ImagePredictions&gt; LoadModel(string modelLocation,
                                                                   string imagesLocation,
                                                                   string tagsLocation)
    {
        IDataView data = mLContext.Data.LoadFromTextFile&lt;ImageData&gt;(path: tagsLocation,
                                                                    hasHeader: false);
        var pipeline = mLContext.Transforms.LoadImages(outputColumnName: "image_tensor:0",
                                                       imageFolder: imagesLocation,
                                                       inputColumnName: nameof(ImageData.ImagePath))
               .Append(mLContext.Transforms.ResizeImages(outputColumnName: "image_tensor:0",
                                                         imageWidth: ImageSettings.ImageWidth,
                                                         imageHeight: ImageSettings.ImageHeight,
                                                         inputColumnName: "image_tensor:0"))
               .Append(mLContext.Transforms.ExtractPixels(outputColumnName: "image_tensor:0",
                                                          interleavePixelColors: ImageSettings.ChannelLast))
               .Append(mLContext.Transforms.ApplyOnnxModel(modelFile: modelLocation,
                                                           outputColumnNames: new[] { SSDSettings.SSDDetectionsOutput,
                                                                                      SSDSettings.SSDClassesOutput,
                                                                                      SSDSettings.SSDBoxesOutput,
                                                                                      SSDSettings.SSDScoresOutput },
                                                           inputColumnNames: new[] { SSDSettings.SSDInput }));


        var model = pipeline.Fit(data);

        var predictionEngine = mLContext.Model.CreatePredictionEngine&lt;ImageData, ImagePredictions&gt;(model);
        return predictionEngine;
    }`
&lt;/denchmark-code&gt;

Expected behavior
Model should be loaded correctly
Screenshots
Additional context
With OnnxRuntime 0.4.0 I got
2019-06-12 14:32:53.802528 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name i__19 is not associated with a node. 2019-06-12 14:32:53.802581 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name cond__21 is not associated with a node. 2019-06-12 14:32:54.004760 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name i__42 is not associated with a node. 2019-06-12 14:32:54.004790 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name cond__44 is not associated with a node. 2019-06-12 14:32:54.005072 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name i is not associated with a node. Onnx type not supported
With earlier versions I got
Error initializing model :Microsoft.ML.OnnxRuntime.OnnxRuntimeException: [ErrorCode:InvalidGraph] Load model from /my/path/to/file/ssd_mobilenet.onnx failed:Node:Preprocessor/map/strided_slice Node (Preprocessor/map/strided_slice) has input size 4 not in range [min=1, max=1]. at Microsoft.ML.OnnxRuntime.InferenceSession..ctor(String modelPath, SessionOptions options) in C:\agent\_work\6\s\csharp\src\Microsoft.ML.OnnxRuntime\InferenceSession.cs:line 83 at Microsoft.ML.OnnxRuntime.InferenceSession..ctor(String modelPath) in C:\agent\_work\6\s\csharp\src\Microsoft.ML.OnnxRuntime\InferenceSession.cs:line 31 at Microsoft.ML.Transforms.Onnx.OnnxModel..ctor(String modelFile, Nullable 1 gpuDeviceId, Boolean fallbackToCpu) at Microsoft.ML.Transforms.Onnx.OnnxTransformer..ctor(IHostEnvironment env, Options options, Byte[] modelBytes)
	</description>
	<comments>
		<comment id='1' author='antonfr' date='2019-06-17T18:38:05Z'>
		Based on this error - Graph input with name i__19 is not associated with a node. - it looks like the converted model doesn't have any input by the name 'i__19'. You should take a look at the inputs in the onnx graph and use them for inference. Can you upload the converted onnx model here?
		</comment>
		<comment id='2' author='antonfr' date='2019-06-18T01:34:08Z'>
		Just curious &lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
 - did you try an inferencing run with 0.4.0 ?
From the below, I see a bunch of warnings (not errors), so I wonder if the model actually loaded correctly.
Additional context With OnnxRuntime 0.4.0 I got 2019-06-12 14:32:53.802528 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name i__19 is not associated with a node. 2019-06-12 14:32:53.802581 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name cond__21 is not associated with a node. 2019-06-12 14:32:54.004760 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name i__42 is not associated with a node. 2019-06-12 14:32:54.004790 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name cond__44 is not associated with a node. 2019-06-12 14:32:54.005072 [W:onnxruntime:InferenceSession, session_state_initializer.cc:502 SaveInputOutputNamesToNodeMapping] Graph input with name i is not associated with a node. Onnx type not supported 
As for the noisy "warnings", this should be addressed by &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1235&gt;#1235&lt;/denchmark-link&gt;
. Sometimes there could be some residual superfluous information in the converted model and the runtime might just be complaining about this. This should not affect model loading and inference run itself.
		</comment>
		<comment id='3' author='antonfr' date='2019-06-18T07:01:55Z'>
		&lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;
 here is my model
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3300163/ssd_mobilenet.onnx.zip&gt;ssd_mobilenet.onnx.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 I opened the same issue in &lt;denchmark-link:https://github.com/onnx/tensorflow-onnx/issues/591&gt;tensorflow-onnx repository&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/guschmue&gt;@guschmue&lt;/denchmark-link&gt;
 writes, that he successfully converted ssd mobilenet model, and he provided link to his repository, nevertheless, I had the same problem with his model, but with different graph input names.
		</comment>
		<comment id='4' author='antonfr' date='2019-06-18T18:58:30Z'>
		Hi &lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
,
Thanks for sharing the model. I think the model loads fine. I hit issues while performing the inference run. Here are some noteworthy points -

The model seems to have symbolic dimensions-

&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/59711518-1c890180-91c0-11e9-9ae5-661042f885d8.png&gt;&lt;/denchmark-link&gt;

So I followed the lead from your earlier snapshot where width = height = 300 and fed it random numpy data of type uint8 of shape [1, 300, 300, 3]

it crashes at a Gather node -

&lt;denchmark-link:https://user-images.githubusercontent.com/9969784/59711617-47735580-91c0-11e9-914f-0880a948de5b.png&gt;&lt;/denchmark-link&gt;

So either the conversion had an issue or the input shape is still not right. if the input shape is not right, please correct the shape below and give it a shot. If after correcting it, the model still doesn't run, it might be a conversion issue. So you can follow up with the converter tool owners.
This is the python script I used to load and invoke the model -
import onnxruntime as rt
import numpy as np
sess = rt.InferenceSession("ssd_mobilenet.onnx")
print("Done loading model")
input = np.ndarray(shape=(1, 300, 300, 3), dtype='uint8')
input_name = sess.get_inputs()[0].name
pred_onnx = sess.run(None, {input_name: input})[0]
		</comment>
		<comment id='5' author='antonfr' date='2019-06-18T22:07:50Z'>
		This will possibly be fixed by &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1233&gt;#1233&lt;/denchmark-link&gt;
 that just got checked in.
There are NonZero nodes earlier in the graph that provide an iteration count to a Loop node. If there are no matches the iteration count is zero. The shape of some of the Loop outputs wasn't correct in that case, leading to Gather breaking later on. This occurring is dependent on the input though, so it won't necessarily crash every time.
Longer term it would be nicer if the model had a shortcut path when NonZero returns no matches, but that's a question for the converter team as to whether that's achievable.
		</comment>
		<comment id='6' author='antonfr' date='2019-06-19T00:04:32Z'>
		Thanks &lt;denchmark-link:https://github.com/skottmckay&gt;@skottmckay&lt;/denchmark-link&gt;
. I built a python wheel including &lt;denchmark-link:https://github.com/microsoft/onnxruntime/pull/1233&gt;#1233&lt;/denchmark-link&gt;
 and the model didn't crash and finished its run successfully.
&lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
 - could you try building from source and checking if the results look okay (I only validated that the crash was resolved, still need to validate results) ?
		</comment>
		<comment id='7' author='antonfr' date='2019-06-19T20:45:31Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 Unfortunately I met problems with installing onnxruntime from source, though I have followed instructions:

Checkout the source tree: - done
Install cmake-3.13 or better from https://cmake.org/download/. - done
(optional) Install protobuf 3.6.1 from source code - met a problem, used brew install protobuf instead
(optional) Install onnx from source code (cmake/external/onnx) - got error error: package directory 'onnxruntime/backend' does not exist
Run ./build.sh --config RelWithDebInfo --build_wheel for Linux (or build.bat --config RelWithDebInfo --build_wheel for Windows). Upon successful build you should be able to find the wheel under dist folder. - got subprocess.CalledProcessError: Command '['/usr/local/Cellar/cmake/3.14.4/bin/cmake', '--build', '/Users/anton/onnxruntime/build/Linux/RelWithDebInfo', '--config', 'RelWithDebInfo']' returned non-zero exit status 2.

		</comment>
		<comment id='8' author='antonfr' date='2019-06-19T20:59:47Z'>
		&lt;denchmark-link:https://github.com/snnn&gt;@snnn&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;
 - any idea what's the issue ?
		</comment>
		<comment id='9' author='antonfr' date='2019-06-19T21:05:47Z'>
		I don't usually run the optional steps. It should work without that.
		</comment>
		<comment id='10' author='antonfr' date='2019-06-19T21:11:21Z'>
		&lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
 - what's the exact build error you get ?
		</comment>
		<comment id='11' author='antonfr' date='2019-06-19T21:18:44Z'>
		I just built this and it works just fine. The only change I had to make was to comment out the running of onnx_backend_test_series.py inside build.py. I ran it like this: . I'm at this commit &lt;denchmark-link:https://github.com/microsoft/onnxruntime/commit/23838d9c2a637c5eacafe29194c73eefd2f4943f&gt;23838d9&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='12' author='antonfr' date='2019-06-20T07:25:21Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 here is the full log in zip archive
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3309232/log_build.zip&gt;log_build.zip&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;
 tried to use your solution, unfortunately, result is the same.
		</comment>
		<comment id='13' author='antonfr' date='2019-06-20T08:40:36Z'>
		Additionally, when I'm trying python3 setup.py bdist_wheel, I get error: package directory 'onnxruntime/backend' does not exist
I asked my colleague to try building from source on his mac, he got the same problems as me.
		</comment>
		<comment id='14' author='antonfr' date='2019-06-20T22:32:46Z'>
		Are you using clang compiler on Linux ?
clang: error: linker command failed with exit code 1 (use -v to see invocation)
I don't think this is supported according to the OS/Compiler support matrix here - &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/BUILD.md&gt;https://github.com/microsoft/onnxruntime/blob/master/BUILD.md&lt;/denchmark-link&gt;

(&lt;denchmark-link:https://github.com/pranavsharma&gt;@pranavsharma&lt;/denchmark-link&gt;
 can correct me if I am wrong)
		</comment>
		<comment id='15' author='antonfr' date='2019-06-24T08:29:08Z'>
		&lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 yes, I'm using clang compiler on macOS, any ideas, if I could build onnxruntime from source and if yes, how could I do it?
		</comment>
		<comment id='16' author='antonfr' date='2019-06-24T18:37:02Z'>
		Yes, the steps to build onnxruntime from source are documented here - &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/BUILD.md&gt;https://github.com/microsoft/onnxruntime/blob/master/BUILD.md&lt;/denchmark-link&gt;
. Can you please check if you are missing something from the steps ?
		</comment>
		<comment id='17' author='antonfr' date='2019-06-24T20:32:21Z'>
		&lt;denchmark-link:https://github.com/HariharanS&gt;@HariharanS&lt;/denchmark-link&gt;
 I have described in details all steps above &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/1229#issuecomment-503739646&gt;#1229 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='18' author='antonfr' date='2019-06-26T18:18:31Z'>
		Hi &lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
,
This must be a local dev environment issue and probably not a build issue itself as Mac builds are run on a daily basis (and a per PR basis) and things look fine.
However, we will try building on a Mac and get back to you.
Thanks
		</comment>
		<comment id='19' author='antonfr' date='2019-06-27T06:07:46Z'>
		Thanks &lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
, waiting for results. From my side, I can provide you all information about my environment, that you might consider necessary.
		</comment>
		<comment id='20' author='antonfr' date='2019-06-27T23:09:42Z'>
		&lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
 -- can you do the following steps, and skip the optional steps listed above? The optional steps may be confusing the issue.
1&gt; remove any installation of protobuf
2&gt; start off with a clean Python installation (packages may include libprotobuf, which could interfere with the build). So uninstall Anaconda if you have it installed.

git clone https://github.com/microsoft/onnxruntime
cd onnxruntime
git submodule init
git submodule update --recursive
&lt;Edited &gt;Open the ./build.sh script, and  remove the --use_openmp flag.
./build.sh --config RelWithDebInfo --build_wheel

Can you update the thread if the steps above fail?
		</comment>
		<comment id='21' author='antonfr' date='2019-06-27T23:35:39Z'>
		Thanks &lt;denchmark-link:https://github.com/jignparm&gt;@jignparm&lt;/denchmark-link&gt;

Also, the errors in your logs are similar to the one raised midway of &lt;denchmark-link:https://github.com/microsoft/onnxruntime/issues/648&gt;#648&lt;/denchmark-link&gt;
 and the resolution was try building first without .  I notice that in your build logs,   did exist indicating that you were building with OpenMP. Can you try building without OpenMP please ?
		</comment>
		<comment id='22' author='antonfr' date='2019-06-28T00:07:00Z'>
		BTW, We'll remove the onnxruntime_USE_OPENMP option and keep it always off.
		</comment>
		<comment id='23' author='antonfr' date='2019-07-01T07:24:57Z'>
		&lt;denchmark-link:https://github.com/jignparm&gt;@jignparm&lt;/denchmark-link&gt;
 thanks a lot, remove --use_openmp and comment out onnx_backend_test_series.py works fine!
		</comment>
		<comment id='24' author='antonfr' date='2019-07-01T09:07:15Z'>
		One more question, how to add built from source onnxruntime to existing project in Visual Studio? I use project -&gt; add nuget packages -&gt;configure sources -&gt; add. What folder should I select?
		</comment>
		<comment id='25' author='antonfr' date='2019-07-01T21:58:35Z'>
		To add a "built from source onnxruntime" to an existing project in Visual Studio, you need to first generate a NuGet package, which includes the runtimes you need (.dll, .so or .dylib files). If you build locally from master branch on a Windows operating system, you'll only get .dll files for Windows. If that is good enough for you, then you can run msbuild command below to generate a .nupkg file.
The NuGet package includes the C# assemblies, so you need to build the C# projects as well as the native C++ projecst. Simply add the --build_csharp  flag  (e.g. "./build.sh --config RelWithDebInfo --build_wheel --build_csharp" ) to the build command.
This creates the NuGet package from source that you can add to Visual Studio.
&lt;denchmark-code&gt;cd onnxruntime\csharp
"C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin\msbuild" OnnxRuntime.CSharp.proj /p:Configuration=Debug /t:CreatePackage /p:PackageId=Microsoft.ML.OnnxRuntime
&lt;/denchmark-code&gt;

The package on NuGet.org contains runtimes for all three operating systems (Windows, Linux Ubuntu flavor, and MacOS), in case you need a package that needs to run in multiple environments.
		</comment>
		<comment id='26' author='antonfr' date='2019-07-02T06:41:43Z'>
		&lt;denchmark-link:https://github.com/jignparm&gt;@jignparm&lt;/denchmark-link&gt;
 with --build_csharp flag I got following errors (see log_file for full log)
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/files/3348632/log_build2.zip&gt;log_build2.zip&lt;/denchmark-link&gt;

Though without this flag everything works fine. Any other ways to create onnxruntime nugget package on macOS?
		</comment>
		<comment id='27' author='antonfr' date='2019-07-02T20:32:09Z'>
		From the log files, it seems like some Mono  header files are getting pulled into the build, even for the Native C++ library build, when you use the --build_csharp flag on MacOS. The C# dlls are usually compiled on Windows in our build systems, since they are cross-platform.

/Library/Frameworks/Mono.framework/Headers/png.h:597:3: note: 'png_time' declared here
} png_time;

Since you are able to build the native dylib library successfully without the  flag, another option is to build the C# project independently. Drop the --build_csharp flag from the command line, and build the &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/csharp/OnnxRuntime.CSharp.sln&gt;C# project&lt;/denchmark-link&gt;
 using . It relies on the  environment  variable &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/csharp/src/Microsoft.ML.OnnxRuntime/Microsoft.ML.OnnxRuntime.csproj#L13&gt;here&lt;/denchmark-link&gt;
  to point to the root directory of the native C++ build. Set that variable to point to the  root of the build directory before running  command.
A second (simpler?) option is to use a pre-existing NuGet package, rename it with .zip extension, unzip the contents, replace the native  library at runtimes/osx-64/native/libonnxruntime.dylib  (and optionally the C# library at lib/netstandard1.1/Microsoft.ML.OnnxRuntime.dll  in case there are changes in the C# code base). You should be able to re-zip these files into a NuGet package for debugging/development purposes.
		</comment>
		<comment id='28' author='antonfr' date='2019-07-02T21:05:12Z'>
		Thanks for your reply &lt;denchmark-link:https://github.com/jignparm&gt;@jignparm&lt;/denchmark-link&gt;
. As for the first option, path to my onnxruntime library is /Users/anton Is it the root to my build directory?
As for the second option, though I got  message, both libonnxruntime.dylib and Microsoft.ML.OnnxRuntime.dll aren't available.
		</comment>
		<comment id='29' author='antonfr' date='2019-07-02T21:42:44Z'>
		To generate a dylib file, add the --build_shared flag to the build script. It'll generate the dylib in the location below. The build configuration in this case is RelWithDebInfo, but other options could be Debug or Release.
The build directory root, in the example path below, would be /onnxruntime/build. If building C# independently, set OnnxRuntimeBuildDirectory to this value before starting the dotnet build.
Example path to dylib file:
&lt;denchmark-code&gt;/onnxruntime/build/Linux/RelWithDebInfo/libonnxruntime.dylib
&lt;/denchmark-code&gt;

		</comment>
		<comment id='30' author='antonfr' date='2019-07-09T17:48:37Z'>
		&lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
 -- are you able to build from source and load the model successfully now, as &lt;denchmark-link:https://github.com/hariharans29&gt;@hariharans29&lt;/denchmark-link&gt;
 was able to do?
		</comment>
		<comment id='31' author='antonfr' date='2019-07-11T18:35:33Z'>
		Closing this for now. &lt;denchmark-link:https://github.com/antonfr&gt;@antonfr&lt;/denchmark-link&gt;
 - Please reopen in case you have more issues / require further clarifications. Thanks!
		</comment>
	</comments>
</bug>