<bug id='4478' author='GrzegorzKarchNV' open_date='2020-07-10T11:30:08Z' closed_time='2020-08-10T09:07:13Z'>
	<summary>Assertion failed: f.pitches[i] &amp;gt;= s</summary>
	<description>
Describe the bug
A clear and concise description of what the bug is.
When loading an ONNX model in python with onnxruntime.InferenceSession(), I get the Assertion failed: f.pitches[i] &gt;= s.
The model was generated using torch.onnx.export, opset=12, in PyTorch container nvcr.io/nvidia/pytorch:20.06-py3
Urgency
I have a model in ONNX format that I would like to test.
System information

OS Platform and Distribution: Linux Ubuntu 16.04:
ONNX Runtime installed from docker: https://github.com/microsoft/onnxruntime/blob/v1.3.1/dockerfiles/Dockerfile.tensorrt
ONNX Runtime version: 1.3.1
Python version: 3.7
CUDA version: 10.2.89
CUDA driver version: 440.33.01
CUDNN version: 7.6.5.32
GPU model and memory: NVIDIA TitanV 12gb


Follow instructions to build onnxruntime-trt
&lt;denchmark-link:https://github.com/microsoft/onnxruntime/tree/master/dockerfiles#tensorrt&gt;https://github.com/microsoft/onnxruntime/tree/master/dockerfiles#tensorrt&lt;/denchmark-link&gt;

In the docker, with accessible ONNX model: &lt;denchmark-link:https://drive.google.com/file/d/1lPN2iANiI7mjm1V2XkguFPds2Q0KSzb8/view?usp=sharing&gt;https://drive.google.com/file/d/1lPN2iANiI7mjm1V2XkguFPds2Q0KSzb8/view?usp=sharing&lt;/denchmark-link&gt;

# python3.7
in interpreter:
&gt;&gt;&gt; import onnxruntime as ort
&gt;&gt;&gt; sess = ort.InferenceSession("waveglow.onnx")

ONNX model:

Expected behavior
Successful model loading
Additional context
command and full error:
&gt;import onnxruntime as ort
&gt;sess = ort.InferenceSession("triton_models/waveglow/1/waveglow.onnx")
2020-07-10 09:56:30.231246592 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:56:30 WARNING] /code/onnxruntime/cmake/external/onnx-tensorrt/onnx2trt_utils.cpp:235: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
2020-07-10 09:56:30.231284361 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:56:30 WARNING] /code/onnxruntime/cmake/external/onnx-tensorrt/onnx2trt_utils.cpp:261: One or more weights outside the range of INT32 was clamped
...
2020-07-10 09:56:32.539329200 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:56:32 WARNING] /code/onnxruntime/cmake/external/onnx-tensorrt/onnx2trt_utils.cpp:261: One or more weights outside the range of INT32 was clamped
2020-07-10 09:57:11.654768766 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:57:11     BUG] Assertion failed: f.pitches[i] &gt;= s
../builder/tacticOptimizer.cpp:617
Aborting...

2020-07-10 09:57:11.664716702 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:57:11   ERROR] ../builder/tacticOptimizer.cpp (617) - Assertion Error in shrink: 0 (f.pitches[i] &gt;= s)
---------------------------------------------------------------------------
EPFail                                    Traceback (most recent call last)
&lt;ipython-input-2-9e2b45df1a75&gt; in &lt;module&gt;
----&gt; 1 sess = ort.InferenceSession("triton_models/waveglow/1/waveglow.onnx")

/opt/miniconda/lib/python3.7/site-packages/onnxruntime/capi/session.py in __init__(self, path_or_bytes, sess_options, providers)
    156         self._path_or_bytes = path_or_bytes
    157         self._sess_options = sess_options
--&gt; 158         self._load_model(providers or [])
    159         self._enable_fallback = True
    160         Session.__init__(self, self._sess)

/opt/miniconda/lib/python3.7/site-packages/onnxruntime/capi/session.py in _load_model(self, providers)
    175             raise TypeError("Unable to load from type '{0}'".format(type(self._path_or_bytes)))
    176
--&gt; 177         self._sess.load_model(providers)
    178
    179         self._sess_options = self._sess.session_options

EPFail: [ONNXRuntimeError] : 11 : EP_FAIL : TensorRT EP could not build Engine for fused node: TensorrtExecutionProvider_TRTKernel_graph_torch-jit-export_0_0
	</description>
	<comments>
		<comment id='1' author='GrzegorzKarchNV' date='2020-07-10T21:17:20Z'>
		&lt;denchmark-link:https://github.com/stevenlix&gt;@stevenlix&lt;/denchmark-link&gt;
 Can you take a look at this? Is this model supported in TensorRT EP?
		</comment>
		<comment id='2' author='GrzegorzKarchNV' date='2020-07-17T19:08:47Z'>
		For the two inputs, mel and z, can you specify what shapes are you using in your test? I try to generate synthetic data to reproduce the issue.
BTW, current TensorRT release may have limited support to ONNX Opset12. That could be the issue.
		</comment>
		<comment id='3' author='GrzegorzKarchNV' date='2020-07-17T19:14:42Z'>
		One more thing, since the model has FP16 precision, to run on TRT, you need to set env variable ORT_TENSORRT_FP16_ENABLE to 1 (Details can be found in TRT EP doc, &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/TensorRT-ExecutionProvider.md#configuring-environment-variables&gt;https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/TensorRT-ExecutionProvider.md#configuring-environment-variables&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='4' author='GrzegorzKarchNV' date='2020-08-10T09:07:13Z'>
		Hi &lt;denchmark-link:https://github.com/stevenlix&gt;@stevenlix&lt;/denchmark-link&gt;
, I updated locally TensorRT version in the Dockerfile and I just noticed you did the same in &lt;denchmark-link:https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/Dockerfile.tensorrt&gt;https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/Dockerfile.tensorrt&lt;/denchmark-link&gt;
 . With the updated version loading ONNX model and running inference works as expected. Thanks!
		</comment>
	</comments>
</bug>