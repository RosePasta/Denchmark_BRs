<bug id='2128' author='chwan-rice' open_date='2020-08-30T05:07:54Z' closed_time='2020-09-13T04:06:43Z'>
	<summary>cannot request out_edges() for empty node sets on cuda</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm using the latest DGL version. The following code throws a DGLError: Check failed: dim != 0 (0 vs. 0) :
import dgl
import torch

g = dgl.DGLGraph(([0, 1], [1, 2])).to('cuda:0')
idx = torch.LongTensor([]).to('cuda:0')
g.out_edges(idx)
However, if the graph runs on CPU, the code works well. I'm not sure whether this is a bug.
	</description>
	<comments>
		<comment id='1' author='chwan-rice' date='2020-08-31T05:45:36Z'>
		Empty tensor should not be allowed in out_edges function. Do you have any expected behavior here?
		</comment>
		<comment id='2' author='chwan-rice' date='2020-08-31T06:18:09Z'>
		Actually, I just feel it's strange that out_edges has different behavior on CPU and GPU. It would be better if it returns an empty edge set.
		</comment>
		<comment id='3' author='chwan-rice' date='2020-09-03T16:44:42Z'>
		A similar problem is found in out_degrees
		</comment>
		<comment id='4' author='chwan-rice' date='2020-09-13T04:06:43Z'>
		Should be fixed in master.  Will go into 0.5.2 release.
		</comment>
	</comments>
</bug>