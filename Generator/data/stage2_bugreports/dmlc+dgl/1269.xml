<bug id='1269' author='gurdaspuriya' open_date='2020-02-18T04:10:57Z' closed_time='2020-03-23T10:18:27Z'>
	<summary>memory leak in `DGLHeteroGraph.send_and_recv()` when used along with Node UDF</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Memory leak is happening in this scenario:

When a node UDF is first used to project node features to a latent space
send_and_recv is used for message passing on the latent space.

No memory leak with multi_update_all(), and also when first step (node UDF to project into latent space) is omitted. All three scenarios can be reproduced with the attached toy code.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Attached .txt file contains a toy .py code to reproduce
&lt;denchmark-link:https://github.com/dmlc/dgl/files/4217286/dummy_top.txt&gt;dummy_top.txt&lt;/denchmark-link&gt;

Steps to reproduce the behavior:
Running the above code gives
#Without Node UDF and send_and_recv
epoch 0. allocated: 1.5273M, max allocated: 2.2979M, cached: 4.0000M, max cached: 4.0000M
epoch 1. allocated: 1.5273M, max allocated: 2.3008M, cached: 4.0000M, max cached: 4.0000M
epoch 2. allocated: 1.5273M, max allocated: 2.3008M, cached: 4.0000M, max cached: 4.0000M
#With Node UDF and send_and_recv
epoch 0. allocated: 3.1528M, max allocated: 3.5347M, cached: 4.0000M, max cached: 4.0000M
epoch 1. allocated: 3.9375M, max allocated: 4.3193M, cached: 6.0000M, max cached: 6.0000M
epoch 2. allocated: 4.7222M, max allocated: 5.1040M, cached: 6.0000M, max cached: 6.0000M
#With Node UDF and multi_update_all
epoch 0. allocated: 2.3760M, max allocated: 788.9995M, cached: 806.0000M, max cached: 806.0000M
epoch 1. allocated: 2.3760M, max allocated: 788.9995M, cached: 806.0000M, max cached: 806.0000M
epoch 2. allocated: 2.3760M, max allocated: 788.9995M, cached: 806.0000M, max cached: 806.0000M
&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Memory footprint should be same for all the epochs
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


DGL Version (e.g., 1.0): 0.5a200120
Backend Library &amp; Version (e.g., PyTorch 0.4.1, MXNet/Gluon 1.3): PyTorch 1.3.1
OS (e.g., Linux): Ubuntu
How you installed DGL (conda, pip, source): conda
Build command you used (if compiling from source):
Python version: 3.7.4
CUDA/cuDNN version (if applicable): 10.0
GPU models and configuration (e.g. V100): Tesla K80
Any other relevant information:

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='gurdaspuriya' date='2020-03-02T07:49:50Z'>
		Hi, I re-produced your issue and I will check it as soon as possible. Thanks so much!
		</comment>
		<comment id='2' author='gurdaspuriya' date='2020-03-08T15:28:55Z'>
		The issue can also be reproduced with the code below:
g.nodes['user'].data['h'] = torch.zeros((num_users, 100), device=device).requires_grad_()
g.nodes['game'].data['h'] = torch.zeros((num_games, 100), device=device).requires_grad_()
#weight = nn.ModuleDict({
#        name : nn.Linear(100, 100, bias=False).to(device) for name in g.ntypes
#    })
#for name in weight:
#    g.apply_nodes(lambda nodes: {'h': weight[name](nodes.data['h'])}, ntype=name);
for epoch in range(1000):
    for srctype, etype, dsttype in g.canonical_etypes:
        g.send_and_recv(g[etype].edges(), fn.copy_u('h', 'm'), fn.mean('m', 'x'), etype=etype);
    stat_cuda("epoch " + str(epoch))
Wrapping the send_and_recv with a with g.local_scope() statement could resolve this.  g.local_scope() automatically cleans up the new columns generated in the code block within the with context.
for epoch in range(1000):
    for srctype, etype, dsttype in g.canonical_etypes:
        with g.local_scope():
            g.send_and_recv(g[etype].edges(), fn.copy_u('h', 'm'), fn.mean('m', 'x'), etype=etype);
    stat_cuda("epoch " + str(epoch))
I suspect the reason of memory leak might be the requirement of computing gradients and repeated partial writes on a tensor by send_and_recv.  Essentially, since the node feature x is a part of the computation graph, and send_and_recv does partial updates (meaning that it will clone the tensor and replace some of the rows with the new values to maintain the contiguity of computation graph), the result may be that a new tensor is created with no way of freeing every time you write into the same feature tensor x.
This is only a conjecture, and I might be wrong as I am currently unable to reproduce it in pure PyTorch.  If you are looking for a workaround, then with g.local_scope() should work.
		</comment>
	</comments>
</bug>