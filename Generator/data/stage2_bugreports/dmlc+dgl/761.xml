<bug id='761' author='VoVAllen' open_date='2019-08-13T15:09:47Z' closed_time='2020-03-23T09:19:13Z'>
	<summary>[Bug] Reduce with max built-in having different behavior comparing to UDF</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Node with no in-edge will be initialized as -inf, which is inconsistent with UDF.
Sometimes also fails CI test.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:





&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


DGL Version (e.g., 1.0):
Backend Library &amp; Version (e.g., PyTorch 0.4.1, MXNet/Gluon 1.3):
OS (e.g., Linux):
How you installed DGL (conda, pip, source):
Build command you used (if compiling from source):
Python version:
CUDA/cuDNN version (if applicable):
GPU models and configuration (e.g. V100):
Any other relevant information:

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='VoVAllen' date='2019-08-15T13:57:42Z'>
		Please note this is also the case for batched_graph.max_nodes.
		</comment>
		<comment id='2' author='VoVAllen' date='2019-08-16T03:33:41Z'>
		Could you paste the CI failure message here?
		</comment>
		<comment id='3' author='VoVAllen' date='2019-08-16T06:25:11Z'>
		&lt;denchmark-link:http://ci.dgl.ai/blue/organizations/jenkins/DGL/detail/PR-750/3/pipeline&gt;http://ci.dgl.ai/blue/organizations/jenkins/DGL/detail/PR-750/3/pipeline&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='VoVAllen' date='2019-08-17T07:55:34Z'>
		&lt;denchmark-link:https://github.com/jermainewang&gt;@jermainewang&lt;/denchmark-link&gt;
 I think it's not appropriate to return  for zero degree nodes/graphs. If user use the returned  feature to do classification, they would get a gradient with all .
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch.nn.functional as F
&gt;&gt;&gt; W = torch.rand(3, 4, requires_grad=True)
&gt;&gt;&gt; W
tensor([[0.3388, 0.3281, 0.8115, 0.8196],
        [0.0738, 0.7006, 0.4559, 0.7633],
        [0.0359, 0.5973, 0.2692, 0.8890]], requires_grad=True)
&gt;&gt;&gt; x = torch.zeros(1, 3).fill_(-float('inf'))
&gt;&gt;&gt; y = x @ W
&gt;&gt;&gt; y
tensor([[-inf, -inf, -inf, -inf]], grad_fn=&lt;MmBackward&gt;)
&gt;&gt;&gt; loss = F.cross_entropy(y, torch.LongTensor([1]))
&gt;&gt;&gt; loss
tensor(nan, grad_fn=&lt;NllLossBackward&gt;)
&gt;&gt;&gt; loss.backward(torch.ones(1))
&gt;&gt;&gt; W.grad
tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]])
		</comment>
	</comments>
</bug>