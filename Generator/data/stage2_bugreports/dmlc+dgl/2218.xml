<bug id='2218' author='kimjh12' open_date='2020-09-21T05:02:42Z' closed_time='2020-09-29T09:27:17Z'>
	<summary>AssertionError when tape.gradient (Tensorflow)</summary>
	<description>
&lt;denchmark-h:h2&gt;‚ùì Questions and Help&lt;/denchmark-h&gt;

Hi, I'm trying to train a nn model with tf backend but I get AssertionError during tape.gradient. What I got stuck can be reproduced as following simple script:
graph = dgl.graph(([0,1],[2,3]))
v = tf.Variable(0.1)
with tf.GradientTape() as tape:
    graph.edata["w"] = tf.repeat(v, graph.number_of_edges())
    graph.update_all(fn.copy_e("w", "m"), fn.max("m", "ws"))
    pred = graph.ndata["ws"]
    loss = pred - .1 # just for test, should work tho
tape.gradient(loss, [v])
Output:

AssertionError                            Traceback (most recent call last)
in
6     pred = graph.ndata["ws"]
7     loss = pred - .1
----&gt; 8 tape.gradient(loss, [v])
~/.conda/envs/.../lib/python3.7/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
1071         output_gradients=output_gradients,
1072         sources_raw=flat_sources_raw,
-&gt; 1073         unconnected_gradients=unconnected_gradients)
1074
1075     if not self._persistent:
~/.conda/envs/.../lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
75       output_gradients,
76       sources_raw,
---&gt; 77       compat.as_str(unconnected_gradients.value))
~/.conda/envs/.../lib/python3.7/site-packages/tensorflow/python/ops/custom_gradient.py in actual_grad_fn(*result_grads)
444                          "@custom_gradient grad_fn.")
445     else:
--&gt; 446       input_grads = grad_fn(*result_grads)
447       variable_grads = []
448     flat_grads = nest.flatten(input_grads)
~/.conda/envs/.../lib/python3.7/site-packages/dgl/backend/tensorflow/sparse.py in grad(dZ)
142                     if op == 'div': dY = -dY / (Y ** 2)
143                 elif op in ['add', 'sub', 'copy_rhs']:
--&gt; 144                     dY = _scatter_nd(argY, _addsub(op, dZ), Y.shape[0])
145             dY = _reduce_grad(dY, Y.shape)
146         else:
~/.conda/envs/.../lib/python3.7/site-packages/dgl/backend/tensorflow/sparse.py in _scatter_nd(index, src, n_rows)
9
10 def _scatter_nd(index, src, n_rows):
---&gt; 11     assert index.shape == src.shape
12     shp = index.shape
13     ctx = context(src)
AssertionError:

It might be a bug but I'm not sure I'm doing right with the API. If I replace fn.max to fn.mean then it works fine and I expect the same result with fn.max and fn.min but they seem not to work. I'm using 0.5.2. Thanks for help in advance.
	</description>
	<comments>
		<comment id='1' author='kimjh12' date='2020-09-21T05:33:09Z'>
		Thanks for your report. Confirmed it's a bug in our computation kernel on tensorflow.
Currently you can work it around by expand_dims to the one-dim tensor, such as changing to
graph.edata["w"] = tf.expand_dims(tf.repeat(v, graph.number_of_edges()), -1)
in your posted code
		</comment>
	</comments>
</bug>