<bug id='438' author='jackroos' open_date='2019-03-08T13:36:22Z' closed_time='2019-03-19T04:02:15Z'>
	<summary>Picking/unpickling DGLGraph breaks feature set/get</summary>
	<description>
Hi, I am trying to emit BatchedDGLGraph in torch.utils.data.DataLoader, but it failed when num_workers &gt; 0. Could you help me solve this problem?
The error message is as following:

Traceback (most recent call last):
File "/xxx/anaconda3/envs/xxx/lib/python3.6/multiprocessing/queues.py", line 234, in _feed
obj = _ForkingPickler.dumps(obj)
File "/xxx/anaconda3/envs/xxx/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
cls(buf, protocol).dump(obj)
TypeError: can't pickle module objects

	</description>
	<comments>
		<comment id='1' author='jackroos' date='2019-03-08T23:23:12Z'>
		Hi, could you paste the code here? My guess is you use "register_reduce_func" somewhere and the function/module object is not pickleable. A walkaround is directly pass the reduce func to the message passing APIs you call (e.g. update_all, send_and_recv, etc.).
		</comment>
		<comment id='2' author='jackroos' date='2019-03-09T03:27:05Z'>
		
Hi, could you paste the code here? My guess is you use "register_reduce_func" somewhere and the function/module object is not pickleable. A walkaround is directly pass the reduce func to the message passing APIs you call (e.g. update_all, send_and_recv, etc.).

I do not use register_reduce_func . I just construct a DGLGraph with nodes, edges, ndata, edata in __getitem__() of my dataset, and use dgl.batch() in collate_fn of dataloader. My original code is a little complicated. I am trying to reproduce this error using toy example. But I find I can't reproduce the error now. By the way, if I remove the BatchedDGLGraph in the output of collate_fn. This error will disappear. So it must be related to the graph.
		</comment>
		<comment id='3' author='jackroos' date='2019-03-09T03:48:47Z'>
		Actually, I can't even pickle graph.ndata.
		</comment>
		<comment id='4' author='jackroos' date='2019-03-09T16:58:12Z'>
		I'll look into this.
		</comment>
		<comment id='5' author='jackroos' date='2019-03-11T11:26:56Z'>
		It is very weird. When I debug line by line in my code, I find the graph can't be pickled just after it be created and call add_nodes. But when I try to reproduce this in following simple code:
import dgl
import torch
import _pickle

graph = dgl.DGLGraph()
graph.add_nodes(5, {'type': torch.zeros(5, dtype=torch.int64)})
_pickle.dumps(graph)
the graph is pickled successfully. Do you have any ideas?
		</comment>
		<comment id='6' author='jackroos' date='2019-03-11T11:32:06Z'>
		In addition, in my original code, after graph created by:
&lt;denchmark-code&gt;graph = dgl.DGLGraph()
&lt;/denchmark-code&gt;

then _pickle.dumps(graph) will be None. But in the simple code of my previous comment, it's not None.
		</comment>
		<comment id='7' author='jackroos' date='2019-03-11T11:44:31Z'>
		
In addition, in my original code, after graph created by:
graph = dgl.DGLGraph()

then _pickle.dumps(graph) will be None. But in the simple code of my previous comment, it's not None.

Sorry, I debug again using IPython, it's not None. Previously I used 'evaluate' in PyCharm Debugger. But in IPython, it still can't be pickled just after add_nodes.
		</comment>
		<comment id='8' author='jackroos' date='2019-03-12T20:49:21Z'>
		Hi, I found the following script can reproduce the error on my side consistently. Could you help confirm?
import networkx as nx
import dgl
import torch
import pickle
import io
def _reconstruct_pickle(obj):
    f = io.BytesIO()
    pickle.dump(obj, f)
    f.seek(0)
    obj = pickle.load(f)
    f.close()
    return obj

def test_pickling_batched_graph():
    glist = [nx.path_graph(i + 5) for i in range(5)]
    glist = [dgl.DGLGraph(g) for g in glist]
    bg = dgl.batch(glist)
    bg.ndata['x'] = torch.randn((35, 5))
    bg.edata['y'] = torch.randn((60, 3))
    new_bg = _reconstruct_pickle(bg)

test_pickling_batched_graph()
		</comment>
		<comment id='9' author='jackroos' date='2019-03-12T20:57:27Z'>
		In fact, the problem is torch dtype. Here is a more precise reproduction. Note that if import networkx is deleted, the code works ...
import networkx as nx  # if this line is removed, it works !!
import torch
import pickle
import io

def _reconstruct_pickle(obj):
    f = io.BytesIO()
    pickle.dump(obj, f)
    f.seek(0)
    obj = pickle.load(f)
    f.close()
    return obj

def test():
    x = torch.float32
    new_bg = _reconstruct_pickle(x)

test()
		</comment>
		<comment id='10' author='jackroos' date='2019-03-13T00:55:29Z'>
		Similar dtype pickling issues are tracked in &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/14057&gt;pytorch/pytorch#14057&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='jackroos' date='2019-03-13T10:53:22Z'>
		Hi, &lt;denchmark-link:https://github.com/jermainewang&gt;@jermainewang&lt;/denchmark-link&gt;
. I can reproduce your result. But after I remove all 'import networkx' in my code. It still has 'pickle' problem. I guess some other library I import have the same problem with 'networkx'. Do you know any other library of this kind?
		</comment>
		<comment id='12' author='jackroos' date='2019-03-13T14:49:40Z'>
		Try this workaround. First, figure out the path of your DGL installation by:
&gt;&gt;&gt; import dgl
&gt;&gt;&gt; dgl.__path__
Then, in the DGL folder, remove the if in L31 in dgl/frame.py and unindent L32-L40. The change will use our own serializer for torch.dtype. Let me know whether this fixes your issue and we will include it in the next patch release.
		</comment>
		<comment id='13' author='jackroos' date='2019-03-14T02:41:32Z'>
		It does fix pickling error. But when I want to transfer data of graph to cuda by following code:
def graph_to_cuda(graph):
    for key in graph.ndata:
        graph.ndata[key] = graph.ndata[key].cuda(non_blocking=True)

    for key in graph.edata:
        graph.edata[key] = graph.edata[key].cuda(non_blocking=True)

    return graph
Another error happened:
Traceback (most recent call last):
  File "xxx.py", line 63, in xxx
    graph = graph_to_cuda(graph)
  File "xxx/graph.py", line 158, in graph_to_cuda
    graph.ndata[key] = graph.ndata[key].cuda(non_blocking=True)
  File "xxx/python3.6/site-packages/dgl/view.py", line 60, in __setitem__
    self._graph.set_n_repr({key : val}, self._nodes)
  File "xxx/lib/python3.6/site-packages/dgl/graph.py", line 1591, in set_n_repr
    self._node_frame[key] = val
  File "xxx/lib/python3.6/site-packages/dgl/frame.py", line 616, in __setitem__
    self.update_data(key, val, inplace=False)
  File "xxx/lib/python3.6/site-packages/dgl/frame.py", line 643, in update_data
    self.update_column(key, val, inplace=inplace)
  File "xxx/lib/python3.6/site-packages/dgl/frame.py", line 681, in update_column
    fcol.update(self._index, data, inplace)
  File "xxx/lib/python3.6/site-packages/dgl/frame.py", line 149, in update
    self.data = F.scatter_row(self.data, idx, feats)
  File "xxx/lib/python3.6/site-packages/dgl/backend/pytorch/tensor.py", line 115, in scatter_row
    return data.index_copy(0, row_index, value)
  File "xxx/lib/python3.6/site-packages/torch/tensor.py", line 312, in index_copy
    return self.clone().index_copy_(dim, index, tensor)
RuntimeError: Expected object of backend CPU but got backend CUDA for argument #4 'source'
		</comment>
		<comment id='14' author='jackroos' date='2019-03-14T02:56:41Z'>
		Could you show how the graph is created?
		</comment>
		<comment id='15' author='jackroos' date='2019-03-14T06:25:24Z'>
		It's something like this.
def construct_graph():
    graph = dgl.DGLGraph()
    graph.add_nodes(4, {'type': torch.zeros(4, dtype=torch.int64)})
    for i in range(4):
        for j in range(4):
            if i == j:
                continue
            edge_data = {'h': torch.tensor([[0.0, 0.0, 0.0, 0.0]]),
                         'type': torch.tensor([0])}
            graph.add_edge(i, j, edge_data)

    super_id = graph.number_of_nodes()
    graph.add_nodes(1, {'type': torch.tensor([2], dtype=torch.int64)})

    graph.add_nodes(4, {'type': torch.zeros(4, dtype=torch.int64)})
    for i1 in range(4):
        i = i1 + 4 + 1
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i, super_id, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(super_id, i, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i, i1, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i1, i, edge_data)
        for j1 in range(4):
            j = j1 + 4 + 1
            if i == j:
                continue
            edge_data = {'h': torch.tensor([[0.0, 0.0, 0.0, 0.0]]),
                         'type': torch.tensor([1])}
            graph.add_edge(i, j, edge_data)

    # assign in-degree for each node
    assign_norm(graph)

    return graph

def assign_norm(graph):

    def msg_func(edges):
        return {'msg': torch.ones_like(edges.data['type'])}

    def reduce_func(nodes):
        in_deg = torch.sum(nodes.mailbox['msg'], 1)
        norm = 1.0 / torch.sqrt(in_deg.float())
        return {'norm': norm}

    graph.update_all(msg_func, reduce_func, None)
		</comment>
		<comment id='16' author='jackroos' date='2019-03-15T14:11:28Z'>
		Any ideas to solve this? &lt;denchmark-link:https://github.com/jermainewang&gt;@jermainewang&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='jackroos' date='2019-03-18T03:24:50Z'>
		Hi &lt;denchmark-link:https://github.com/jackroos&gt;@jackroos&lt;/denchmark-link&gt;
 , sorry for the late reply! I tried your codes but cannot reproduce the error. Would you provide some environment information? Thank you.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;


DGL Version (e.g., 1.0):
Backend Library &amp; Version (e.g., PyTorch 0.4.1, MXNet/Gluon 1.3):
OS (e.g., Linux):
How you installed DGL (conda, pip, source):
Build command you used (if compiling from source):
Python version:
CUDA/cuDNN version (if applicable):
GPU models and configuration (e.g. V100):
Any other relevant information:

		</comment>
		<comment id='18' author='jackroos' date='2019-03-18T03:54:05Z'>
		
DGL: build from my forked version: https://github.com/jackroos/dgl, just modify the pickle part in frame.py.
Backend: PyTorch 1.0.0
OS: Linux, Ubuntu 16.04
Build command:

python setup.py install

Python version: 3.6.6
CUDA version: release 9.0, V9.0.176
cuDNN version: 7.0
GPU models: Tesla M40 24GB

		</comment>
		<comment id='19' author='jackroos' date='2019-03-18T03:59:49Z'>
		Tried your repo and still cannot reproduce the error. Could you verify this is the error example?
import torch
import dgl

def construct_graph():
    graph = dgl.DGLGraph()
    graph.add_nodes(4, {'type': torch.zeros(4, dtype=torch.int64)})
    for i in range(4):
        for j in range(4):
            if i == j:
                continue
            edge_data = {'h': torch.tensor([[0.0, 0.0, 0.0, 0.0]]),
                         'type': torch.tensor([0])}
            graph.add_edge(i, j, edge_data)

    super_id = graph.number_of_nodes()
    graph.add_nodes(1, {'type': torch.tensor([2], dtype=torch.int64)})

    graph.add_nodes(4, {'type': torch.zeros(4, dtype=torch.int64)})
    for i1 in range(4):
        i = i1 + 4 + 1
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i, super_id, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(super_id, i, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i, i1, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i1, i, edge_data)
        for j1 in range(4):
            j = j1 + 4 + 1
            if i == j:
                continue
            edge_data = {'h': torch.tensor([[0.0, 0.0, 0.0, 0.0]]),
                         'type': torch.tensor([1])}
            graph.add_edge(i, j, edge_data)

    # assign in-degree for each node
    assign_norm(graph)

    return graph

def assign_norm(graph):

    def msg_func(edges):
        return {'msg': torch.ones_like(edges.data['type'])}

    def reduce_func(nodes):
        in_deg = torch.sum(nodes.mailbox['msg'], 1)
        norm = 1.0 / torch.sqrt(in_deg.float())
        return {'norm': norm}

    graph.update_all(msg_func, reduce_func, None)

def graph_to_cuda(graph):
    for key in graph.ndata:
        graph.ndata[key] = graph.ndata[key].cuda(non_blocking=True)

    for key in graph.edata:
        graph.edata[key] = graph.edata[key].cuda(non_blocking=True)

    return graph

g = construct_graph()
g = graph_to_cuda(g)
print(g.ndata)
		</comment>
		<comment id='20' author='jackroos' date='2019-03-18T04:01:28Z'>
		Oh, could you also verify the DGL version by python -c 'import dgl; print(dgl.__version__)'?
		</comment>
		<comment id='21' author='jackroos' date='2019-03-18T04:38:38Z'>
		
python -c 'import dgl; print(dgl.version)'

0.2
		</comment>
		<comment id='22' author='jackroos' date='2019-03-18T04:39:54Z'>
		I can't reproduce the error based on this simple code also. I don't know if it's something related to dependency again. BTW, in the situation of num_workers=0, graph_to_cuda() can successfully transfer the graph to cuda. But now dataloader is the bottleneck of my code, so I must increase num_workers to improve my training speed.
		</comment>
		<comment id='23' author='jackroos' date='2019-03-18T22:36:16Z'>
		Finally bug confirmed! The cuda call failed after pickle and unpickle:
import torch
import dgl

def construct_graph():
    graph = dgl.DGLGraph()
    graph.add_nodes(4, {'type': torch.zeros(4, dtype=torch.int64)})
    for i in range(4):
        for j in range(4):
            if i == j:
                continue
            edge_data = {'h': torch.tensor([[0.0, 0.0, 0.0, 0.0]]),
                         'type': torch.tensor([0])}
            graph.add_edge(i, j, edge_data)

    super_id = graph.number_of_nodes()
    graph.add_nodes(1, {'type': torch.tensor([2], dtype=torch.int64)})

    graph.add_nodes(4, {'type': torch.zeros(4, dtype=torch.int64)})
    for i1 in range(4):
        i = i1 + 4 + 1
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i, super_id, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(super_id, i, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i, i1, edge_data)
        edge_data = {'h': torch.tensor([[0, 0., 0., 0.]]),
                     'type': torch.tensor([1])}
        graph.add_edge(i1, i, edge_data)
        for j1 in range(4):
            j = j1 + 4 + 1
            if i == j:
                continue
            edge_data = {'h': torch.tensor([[0.0, 0.0, 0.0, 0.0]]),
                         'type': torch.tensor([1])}
            graph.add_edge(i, j, edge_data)

    # assign in-degree for each node
    assign_norm(graph)

    return graph

def assign_norm(graph):

    def msg_func(edges):
        return {'msg': torch.ones_like(edges.data['type'])}

    def reduce_func(nodes):
        in_deg = torch.sum(nodes.mailbox['msg'], 1)
        norm = 1.0 / torch.sqrt(in_deg.float())
        return {'norm': norm}

    graph.update_all(msg_func, reduce_func, None)

def graph_to_cuda(graph):
    for key in graph.ndata:
        graph.ndata[key] = graph.ndata[key].cuda(non_blocking=True)

    for key in graph.edata:
        graph.edata[key] = graph.edata[key].cuda(non_blocking=True)

    return graph

import pickle
import io

def _reconstruct_pickle(obj):
    f = io.BytesIO()
    pickle.dump(obj, f)
    f.seek(0)
    obj = pickle.load(f)
    f.close()
    return obj

g = construct_graph()
gg = _reconstruct_pickle(g)
gg = graph_to_cuda(gg)
print(gg.ndata)
		</comment>
		<comment id='24' author='jackroos' date='2019-03-19T03:28:32Z'>
		Cool! Thank you so much for your effort! Look forward to the bug fix.
		</comment>
		<comment id='25' author='jackroos' date='2019-03-19T03:49:07Z'>
		Just merged the fix. The above example works on my machine. Could you pull the latest master and test it again?
		</comment>
		<comment id='26' author='jackroos' date='2019-03-19T03:59:16Z'>
		It works for me now! Many thanks to you again! üêÆüç∫!!!
		</comment>
	</comments>
</bug>