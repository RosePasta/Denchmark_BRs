<bug id='47' author='yhenon' open_date='2016-01-19T19:04:55Z' closed_time='2016-02-23T11:28:09Z'>
	<summary>Crash on request with invalid url</summary>
	<description>
Hi,
I've identified an issue that causes deepdetect to crash. It seems requests to an invalid domain cause an unhandled exception. This is similar to &lt;denchmark-link:https://github.com/jolibrain/deepdetect/issues/45&gt;#45&lt;/denchmark-link&gt;
 but the issue is separate.
I'm using a recent build of deepdetect (Jan 11th, commit &lt;denchmark-link:https://github.com/jolibrain/deepdetect/commit/6116a0c8984300438b4323bbad44c5d8ca9c2080&gt;6116a0c&lt;/denchmark-link&gt;
) on Ubuntu, which includes the fix that was made for &lt;denchmark-link:https://github.com/jolibrain/deepdetect/issues/45&gt;#45&lt;/denchmark-link&gt;

Request to a valid domain with a valid image -&gt; good reply, returns with code 200:
&lt;denchmark-code&gt;curl -X POST "http://host.com:8080/predict" -d "{\"service\":\"imgnet\",\"parameters\":{\"input\":{\"width\":224,\"height\":224},\"output\":{\"best\":5}},\"data\":[\"https://upload.wikimedia.org/wikipedia/commons/c/cd/US-NASA-Seal-EO10849.jpg\"]}"
{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","time":1938.0,"service":"imgnet"},"body":{"predictions":{"uri":"https://upload.wikimedia.org/wikipedia/commons/c/cd/US-NASA-Seal-EO10849.jpg","classes":[{"prob":0.3491594195365906,"cat":"n04548280 wall clock"},{"prob":0.22876758873462678,"cat":"n03706229 magnetic compass"},{"prob":0.14928314089775086,"cat":"n02708093 analog clock"},{"prob":0.07048133760690689,"cat":"n04192698 shield, buckler"},{"last":true,"prob":0.0419003888964653,"cat":"n04355338 sundial"}]}}}
&lt;/denchmark-code&gt;

Request to a valid domain with an invalid image url -&gt; returns with code 400
&lt;denchmark-code&gt;curl -X POST "http://host.com:8080/predict" -d "{\"service\":\"imgnet\",\"parameters\":{\"input\":{\"width\":224,\"height\":224},\"output\":{\"best\":5}},\"data\":[\"https://upload.wikimedia.org/wikipedia/commons/c/cd/US-NASA-Seal-EO108l49.jpg\"]}"
{"status":{"code":400,"msg":"BadRequest","dd_code":1005,"dd_msg":"Service Input Error"}}
&lt;/denchmark-code&gt;

Request to an invalid domain -&gt; crashes, unhandled exception
&lt;denchmark-code&gt;curl -X POST "http://host.com:8080/predict" -d "{\"service\":\"imgnet\",\"parameters\":{\"input\":{\"width\":224,\"height\":224},\"output\":{\"best\":5}},\"data\":[\"https://upload.wikimedia_bad.org/wikipedia/commons/c/cd/US-NASA-Seal-EO108l49.jpg\"]}"
curl: (52) Empty reply from server
[1]+  Aborted                 (core dumped) nohup ./deepdetect/build/main/dede --host host.com
&lt;/denchmark-code&gt;

Thanks for your help with this issue.
Best,
Yann
	</description>
	<comments>
		<comment id='1' author='yhenon' date='2016-01-20T07:59:07Z'>
		hey, thanks for catching this unfortunate regression again. The parallel loop should now be fixed wrt exceptions and there's a dedicated unit test to avoid further regressions. Let me know if it fixes your issue.
		</comment>
		<comment id='2' author='yhenon' date='2016-01-20T18:38:55Z'>
		Hi - this appears to fix the issue for all my test cases - I'll go ahead and close it. Thanks for your help.
		</comment>
		<comment id='3' author='yhenon' date='2016-01-29T22:43:03Z'>
		Hey - thanks again for your work here, but I think I'll go ahead and reopen this as I'm encountering another crash (tell me if you'd rather I open a new issue).
This one is proving to be tricky to reproduce, but the issue seems to be cURL timing out on HTTPS requests leading to an unhandled exception in deepdetect. However the curl request seems to be fine most of the time, the timeout only occurs sometimes.
&lt;denchmark-code&gt;curl -X POST "host.com:8080/predict" -d "{\"service\":\"imgnet\",\"parameters\":{\"input\":{\"width\":224,\"height\":224},\"output\":{\"best\":5}},\"data\":[\"https://scontent.cdninstagram.com/hphotos-xaf1/t51.2885-15/e15/11236339_839225329503090_301378127_n.jpg\"]}"

ERROR - service imgnet mllib bad param: Operation timed out after 0 milliseconds with 0 out of 0 bytes received
terminate called after throwing an instance of 'boost::exception_detail::clone_impl&lt;boost::exception_detail::error_info_injector&lt;boost::system::system_error&gt; &gt;'
what():  remote_endpoint: Transport endpoint is not connected
&lt;/denchmark-code&gt;

The strange curl message seems to be something others encounter(see here: &lt;denchmark-link:http://stackoverflow.com/questions/22485811/curl-operation-timed-out-after-0-milliseconds&gt;http://stackoverflow.com/questions/22485811/curl-operation-timed-out-after-0-milliseconds&lt;/denchmark-link&gt;
 )
		</comment>
		<comment id='4' author='yhenon' date='2016-01-29T23:44:30Z'>
		Thanks for the detailed report. As you guessed, I am unable to easily reproduce. However, you should be able to test the potential fix you are pointing to, by modifying src/utils/httpclient.hpp such that the new options can be passed to the curl wrapper used in get_call:
request.setOpt(curlpp::options::Url(url));
request.setOpt(curlpp::options::SslVerifyPeer(false));
request.setOpt(curlpp::options::SslVerifyHost(false));
request.setOpt(curlpp::options::SslVersion(3));
If you could also try various combinations of these options, I am interested in the results. Typically, the &lt;denchmark-link:https://en.wikipedia.org/wiki/Transport_Layer_Security#SSL_3.0&gt;version 3 of SSL is considered unsecure&lt;/denchmark-link&gt;
 so it does feel a bad idea to force it here.
That being said, the Web is a wild wild world, it is very likely that some broken or atypical server out there breaks dd calls, though it is expected that all situations will eventually get covered. If you are dealing with lots of content directly coming from the Web, I am making the suggestion that you first retrieve content with some dedicated tool (e.g. scrapy), store it temporarily and pass it on to dd. That would probably give you the two benefits of being more robust to the outside world, and limiting recurrent calls to useless or broken resources.
I am keeping this ticket open to gather all retrieval glitches then, and to fix them as they come.
		</comment>
		<comment id='5' author='yhenon' date='2016-01-30T00:03:42Z'>
		&lt;denchmark-link:https://github.com/yhenon&gt;@yhenon&lt;/denchmark-link&gt;
 the fix above catches exceptions around the curl calls, though in your case it will not retrieve the image, assuming it is possible to so.
		</comment>
		<comment id='6' author='yhenon' date='2016-01-30T17:01:27Z'>
		Thanks, the catch around the curl call seems to solve the issue. I'm not 100% clear on when curlpp throws an exception - but I think we can agree that failing to retrieve the image is significantly better than deepdetect exiting.
I do see your point though - I might move to a pipeline where I curl the image -&gt; upload to S3 -&gt; query with deepdetect, although this adds quite a bit of latency for the end-user.
Thanks again, I'll leave this open even though I'm not aware of any crashes with image retrieval currently.
		</comment>
		<comment id='7' author='yhenon' date='2016-01-30T20:05:43Z'>
		
we can agree that failing to retrieve the image is significantly better than deepdetect exiting.

Definitely!

I might move to a pipeline where I curl the image -&gt; upload to S3 -&gt; query with deepdetect, although this adds quite a bit of latency for the end-user.

There's support for passing images in base64 format, that comes with an encoding size overhead of course. Also, using batches wherever you can (and with GPU when possible) should yield best performance.
		</comment>
		<comment id='8' author='yhenon' date='2016-02-23T11:28:00Z'>
		Beware of &lt;denchmark-link:https://github.com/jolibrain/deepdetect/issues/68&gt;#68&lt;/denchmark-link&gt;
 and pull out the fix if using multiple images prediction at once.
		</comment>
	</comments>
</bug>