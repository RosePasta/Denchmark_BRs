<bug id='290' author='BasVanBoven' open_date='2017-04-18T11:55:45Z' closed_time='2017-04-20T14:35:10Z'>
	<summary>GPU Docker image fails on prediction call (GTX1080)</summary>
	<description>
&lt;denchmark-h:h4&gt;The problem:&lt;/denchmark-h&gt;

On my Ubuntu 16.04 system, which uses a GTX 1080 graphics card, I seem to be facing issues performing prediction calls on the Deepdetect GPU image. This happens even when I try to use the built-in imageserv-model. The Check failed (custom): (error) == (cudaSuccess) usually points to a memory error, but I have more than 6GB of free video memory according to nvidia-smi, which should be more than enough for the imageserv-model. Therefore, I am considering if this might be a DeepDetect-bug. When I set GPU to False, the call succeeds, albeit after quite some processing time. I am running the image in NVIDIA-Docker.
Thanks for all of your hard work; DeepDetect is an amazing piece of software. :)
&lt;denchmark-h:h4&gt;Configuration:&lt;/denchmark-h&gt;


DeepDetect GPU Docker image: beniz/deepdetect_gpu
DeepDetect Commit: 11adce0
GPU: GeForce GTX 1080
NVIDIA driver version: 375.39
Linux kernel version: 4.8.0-46-generic
Docker version: 17.03.1-ce, build c6d412e

&lt;denchmark-h:h4&gt;Steps taken:&lt;/denchmark-h&gt;


Spin up DeepDetect Docker container:

sudo nvidia-docker run -d -p 8080:8080 beniz/deepdetect_gpu

API PUT call:

curl -X PUT "http://localhost:8080/services/imageserv" -d "{\"mllib\":\"caffe\",\"description\":\"image classification service\",\"type\":\"supervised\",\"parameters\":{\"input\":{\"connector\":\"image\"},\"mllib\":{\"nclasses\":1000}},\"model\":{\"repository\":\"/opt/models/ggnet/\"}}"
{"status":{"code":201,"msg":"Created"}}

API PRED call:

curl -X POST "http://localhost:8080/predict" -d "{\"service\":\"imageserv\",\"parameters\":{\"input\":{\"width\":224,\"height\":224},\"output\":{\"best\":3},\"mllib\":{\"gpu\":true}},\"data\":[\"http://i.ytimg.com/vi/0vxOhd4qlnA/maxresdefault.jpg\"]}"
{"status":{"code":500,"msg":"InternalError","dd_code":1007,"dd_msg":"src/caffe/util/im2col.cu:61 / Check failed (custom): (error) == (cudaSuccess)"}}

Server log output:

INFO - 11:42:41 - Device id:                     0
INFO - 11:42:41 - Major revision number:         6
INFO - 11:42:41 - Minor revision number:         1
INFO - 11:42:41 - Name:                          GeForce GTX 1080
INFO - 11:42:41 - Total global memory:           8491368448
INFO - 11:42:41 - Total shared memory per block: 49152
INFO - 11:42:41 - Total registers per block:     65536
INFO - 11:42:41 - Warp size:                     32
INFO - 11:42:41 - Maximum memory pitch:          2147483647
INFO - 11:42:41 - Maximum threads per block:     1024
INFO - 11:42:41 - Maximum dimension of block:    1024, 1024, 64
INFO - 11:42:41 - Maximum dimension of grid:     2147483647, 65535, 65535
INFO - 11:42:41 - Clock rate:                    1771000
INFO - 11:42:41 - Total constant memory:         65536
INFO - 11:42:41 - Texture alignment:             512
INFO - 11:42:41 - Concurrent copy and execution: Yes
INFO - 11:42:41 - Number of multiprocessors:     20
INFO - 11:42:41 - Kernel execution timeout:      Yes[11:42:42] /opt/deepdetect/src/caffelib.cc:1185: Error while proceeding with prediction forward pass, not enough memory?
ERROR - 11:42:42 - service imageserv prediction call failed
ERROR - 11:42:42 - Tue Apr 18 11:42:42 2017 UTC - 172.17.0.1 "POST /predict" 500 437
	</description>
	<comments>
		<comment id='1' author='BasVanBoven' date='2017-04-18T12:04:36Z'>
		Hi, it's possible the public docker with GPU support is not compiled with support for Pascal architectures. Our GPU docker container is still using CUDA 7.5 that doesn't have good support for Pascal AFAIK. Thanks for the thorough report, we'll look into it.
		</comment>
		<comment id='2' author='BasVanBoven' date='2017-04-18T12:10:33Z'>
		Thanks for your very prompt reply and your explanation! I knew the Pascal architecture was troublesome in combination with CUDA 7.5, but I had eliminated that line of thought because running nvcc--version inside the container returned the following:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Tue_Jan_10_13:22:03_CST_2017
Cuda compilation tools, release 8.0, V8.0.61
		</comment>
		<comment id='3' author='BasVanBoven' date='2017-04-18T13:43:14Z'>
		&lt;denchmark-link:https://github.com/BasVanBoven&gt;@BasVanBoven&lt;/denchmark-link&gt;
 Could try the last version of the docker GPU out by any chance ? &lt;denchmark-link:https://hub.docker.com/r/beniz/deepdetect_gpu/&gt;https://hub.docker.com/r/beniz/deepdetect_gpu/&lt;/denchmark-link&gt;
 Thanks.
		</comment>
		<comment id='4' author='BasVanBoven' date='2017-04-18T14:21:32Z'>
		Of course! It will take some time to grab it due to a slow connection though, but I am pulling it in right now and will get back to you as soon as possible.
		</comment>
		<comment id='5' author='BasVanBoven' date='2017-04-18T15:29:25Z'>
		Unfortunately, it does not seem to be working yet. Still getting exactly the same error messages upon API call as I got before, both in the DeepDetect log and as a return. DeepDetect is now at commit &lt;denchmark-link:https://github.com/jolibrain/deepdetect/commit/8c2a32bb15453c66dfe975e9ad6cb367ff05979d&gt;8c2a32b&lt;/denchmark-link&gt;
 now, so I think the Docker image updated just fine.
		</comment>
		<comment id='6' author='BasVanBoven' date='2017-04-18T15:47:09Z'>
		The new GPU docker is built with support for Pascal architecture, so it's possible your problem comes from elsewhere, though we'll test on Pascal and report.
		</comment>
		<comment id='7' author='BasVanBoven' date='2017-04-20T14:35:10Z'>
		A beniz/deepdetect_gpu_pascal public docker container is now available and works with Pascal cards. We'll be working on merging this into a single container for most architectures.
		</comment>
		<comment id='8' author='BasVanBoven' date='2018-06-12T16:32:52Z'>
		Same issue for me. Same GPU and Ubuntu version. Docker version is 18.03.1-ce, build 9ee9f40, NVIDIA driver version is 390.30. Latest deepdetect_gpu docker image.
Server log:
[2018-06-12 16:21:19.052] [imageserv] [info] Net total flops=4748015616 / total params=20970816
[2018-06-12 16:21:19.052] [imageserv] [info] detected network type is classification
[2018-06-12 16:21:19.052] [caffe] [info] Device id:                     0.000000
[2018-06-12 16:21:19.052] [caffe] [info] Major revision number:         6.000000
[2018-06-12 16:21:19.052] [caffe] [info] Minor revision number:         1.000000
[2018-06-12 16:21:19.052] [caffe] [info] Name:                          GeForce GTX 1080
[2018-06-12 16:21:19.052] [caffe] [info] Total global memory:           8513585152.000000
[2018-06-12 16:21:19.052] [caffe] [info] Total shared memory per block: 49152.000000
[2018-06-12 16:21:19.052] [caffe] [info] Total registers per block:     65536.000000
[2018-06-12 16:21:19.052] [caffe] [info] Warp size:                     32.000000
[2018-06-12 16:21:19.052] [caffe] [info] Maximum memory pitch:          2147483647.000000
[2018-06-12 16:21:19.052] [caffe] [info] Maximum threads per block:     1024.000000
[2018-06-12 16:21:19.052] [caffe] [info] Maximum dimension of block:    1024.000000, 1024.000000, 64.000000
[2018-06-12 16:21:19.052] [caffe] [info] Maximum dimension of grid:     2147483647.000000, 65535.000000, 65535.000000
[2018-06-12 16:21:19.052] [caffe] [info] Clock rate:                    1733500.000000
[2018-06-12 16:21:19.052] [caffe] [info] Total constant memory:         65536.000000
[2018-06-12 16:21:19.052] [caffe] [info] Texture alignment:             512.000000
[2018-06-12 16:21:19.052] [caffe] [info] Concurrent copy and execution: Yes
[2018-06-12 16:21:19.052] [caffe] [info] Number of multiprocessors:     20.000000
[2018-06-12 16:21:19.052] [caffe] [info] Kernel execution timeout:      No
[2018-06-12 16:21:19.094] [imageserv] [error] Error while proceeding with supervised prediction forward pass, not enough memory? src/caffe/util/im2col.cu:61 / Check failed (custom): (error) == (cudaSuccess)
[2018-06-12 16:21:19.098] [imageserv] [error] prediction call failed
[2018-06-12 16:21:19.098] [api] [error] 172.17.0.1 "POST /predict" 500 160
		</comment>
		<comment id='9' author='BasVanBoven' date='2018-06-12T16:39:19Z'>
		See the FAQ, you are very likely to exceed your GPU's memory.
		</comment>
		<comment id='10' author='BasVanBoven' date='2018-06-14T18:53:15Z'>
		I set batch_size to 1 but still getting the same error. 8gb of memory isn't enough for bundled ggnet and resnet_50?
		</comment>
		<comment id='11' author='BasVanBoven' date='2018-06-15T08:42:35Z'>
		Are you using nvidia-docker ?
		</comment>
		<comment id='12' author='BasVanBoven' date='2018-06-15T09:48:04Z'>
		Sure.
		</comment>
		<comment id='13' author='BasVanBoven' date='2018-06-15T13:36:12Z'>
		Then you may want to post the full set of API calls and return values, and the full server logs.
		</comment>
		<comment id='14' author='BasVanBoven' date='2018-06-15T15:07:33Z'>
		Here it is:

Api calls: https://paste.fedoraproject.org/paste/dnwscLYCyy~GJEN-cJzeEA
Container logs: https://paste.fedoraproject.org/paste/j4R1Elrpi~10B3hvSRsT~A

		</comment>
		<comment id='15' author='BasVanBoven' date='2018-06-15T15:24:38Z'>
		do you have cudnn installed ?
		</comment>
		<comment id='16' author='BasVanBoven' date='2018-06-15T16:07:24Z'>
		Yes
		</comment>
		<comment id='17' author='BasVanBoven' date='2018-06-16T09:33:34Z'>
		When not a memory issue this type of cuda errors may be due to using a misconfigured build, e.g. lacking the underlying card specific build. This shouldn't be the case here since the docker gpu builds include the builds for cuda compute 6.1.
You may want to try setting the gpu flag to false in the predict call and see if it works.
I'm not sure why you are using the root account in your API call logs, but you shouldn't. If your host OS is a 16.04, I'd recommend you build DD and try it out without docker to at least make sure it is not an issue with your docker setup.
		</comment>
		<comment id='18' author='BasVanBoven' date='2018-06-16T12:03:01Z'>
		
You may want to try setting the gpu flag to false in the predict call and see if it works.

As I mentioned before the issue is 100% the same as for OP. So setting gpu flag to false solves the problem.

I'm not sure why you are using the root account in your API call logs, but you shouldn't.

Quite honestly I don't see any problem with it. This is a dedicated server rented for nothing but testing deepdetect. It's not a production setup so I think switching accounts just to run curl two times isn't necessary.
Anyway I'll try to build DD from source and see what happens.
&lt;denchmark-link:https://github.com/BasVanBoven&gt;@BasVanBoven&lt;/denchmark-link&gt;
 Did you succeed with the issue?
		</comment>
		<comment id='19' author='BasVanBoven' date='2018-06-17T16:15:22Z'>
		Let's transfer this to &lt;denchmark-link:https://github.com/jolibrain/deepdetect/issues/441&gt;#441&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='20' author='BasVanBoven' date='2018-06-19T07:50:33Z'>
		Hi , I want to pull  "beniz/deepdetect_gpu_pascal"(&lt;denchmark-link:https://github.com/jolibrain/deepdetect/issues/290#issuecomment-295760246&gt;#290 (comment)&lt;/denchmark-link&gt;
) but i not found "beniz/deepdetect_gpu_pascal".
how to pull  "beniz/deepdetect_gpu_pascal "?
		</comment>
		<comment id='21' author='BasVanBoven' date='2018-06-19T08:05:20Z'>
		It's gone. You can pull the regular gpu docker and it should work.
		</comment>
		<comment id='22' author='BasVanBoven' date='2018-06-21T11:23:42Z'>
		Thanks you, I pull the new gpu docker ("beniz/deepdetect_gpu") . It working without error (on gpu  1080 ti).
		</comment>
	</comments>
</bug>