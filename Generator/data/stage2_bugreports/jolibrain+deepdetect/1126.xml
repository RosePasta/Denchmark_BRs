<bug id='1126' author='YaYaB' open_date='2021-01-11T12:56:29Z' closed_time='2021-01-13T13:11:56Z'>
	<summary>bad predictions with number of documents superior to batch_size</summary>
	<description>
&lt;denchmark-link:https://github.com/jolibrain/deepdetect/files/5795853/replicate_issue.txt&gt;replicate_issue.txt&lt;/denchmark-link&gt;

(A Markdown syntax reminder is available here: &lt;denchmark-link:https://guides.github.com/features/mastering-markdown/&gt;https://guides.github.com/features/mastering-markdown/&lt;/denchmark-link&gt;
)
Before creating a new issue, please make sure that:

 the problem isn't already identified in the in the FAQ page,
 your problem isn't listed in an existing issue (open or closed one),
 all prerequisite tools and dependencies are installed.

If Ok, please give as many details as possible to help us solve the problem more efficiently.
&lt;denchmark-h:h4&gt;Configuration&lt;/denchmark-h&gt;


Version of DeepDetect:

 Locally compiled on:

 Ubuntu 18.04 LTS
 Other:


 Docker CPU
 Docker GPU
 Amazon AMI


Commit (shown by the server when starting):
Image with docker tag v0.12.0

&lt;denchmark-h:h4&gt;Your question / the problem you're facing:&lt;/denchmark-h&gt;

I've notice some weird prediction when the number of documents sent in prediction is higher that the batch_size in the deploy.prototxt. Usually it uses more memory to be able to make predictions but now it does not seem to do so and just return bad results.
You can use the attached fil to replicate the issue by doing
&lt;denchmark-code&gt;bash replicate_issue.sh
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Error message (if any) / steps to reproduce the problem:&lt;/denchmark-h&gt;


Create a service

Call



&lt;denchmark-code&gt;curl -X PUT "http://localhost:8080/services/imageserv" -d '{
        "mllib":"caffe",
        "description":"image classification service",
        "type":"supervised",
        "parameters":{
            "input":{
            "connector":"image",
            "width":224,
            "height":224
            },
            "mllib":{
                "nclasses":1000,
                "gpuid": 1,
                }
            }
        },
        "model":{
            "repository":"/opt/models/resnet_50"
        }
}'
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;- Output
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;[2021-01-11 12:40:06.949] [caffe] [info] Network initialization done.
[2021-01-11 12:40:06.949] [imageserv] [info] Using pre-trained weights from /opt/models/resnet_50/ResNet-50-model.caffemodel
[2021-01-11 12:40:07.909] [imageserv] [info] Net total flops=3858026368 / total params=25556032
[2021-01-11 12:40:07.909] [imageserv] [info] detected network type is classification
[2021-01-11 12:40:07.910] [api] [info] 172.17.0.1 "PUT /services/imageserv" 201 3183ms
&lt;/denchmark-code&gt;


Predict 3 images separately

Call



&lt;denchmark-code&gt;curl -X POST "http://localhost:8080/predict" -d '{
     "service":"imageserv",
     "parameters":{
       "input":{
         "width":224,
         "height":224
       },
       "output":{
         "best":3
       }
     },
     "data": ["https://picsum.photos/id/237/400/600"]
   }'
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;- Output
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","service":"imageserv","time":1005.0},"body":{"predictions":[{"classes":[{"prob":0.4076613783836365,"cat":"n02099712 Labrador retriever"},{"prob":0.24927398562431336,"cat":"n02093256 Staffordshire bullterrier, Staffordshire bull terrier"},{"last":true,"cat":"n03223299 doormat, welcome mat","prob":0.04856392741203308}],"uri":"https://picsum.photos/id/237/400/600"}]}}
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;- Call
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;curl -X POST "http://localhost:8080/predict" -d '{
      "service":"imageserv",
      "parameters":{
        "input":{
          "width":224,
          "height":224
        },
        "output":{
          "best":3
        }
      },
      "data": ["https://picsum.photos/id/250/400/600"]
    }'
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;- Output
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","service":"imageserv","time":303.0},"body":{"predictions":[{"classes":[{"prob":0.5831432938575745,"cat":"n04069434 reflex camera"},{"prob":0.3723478317260742,"cat":"n03976467 Polaroid camera, Polaroid Land camera"},{"last":true,"cat":"n03657121 lens cap, lens cover","prob":0.04316359758377075}],"uri":"https://picsum.photos/id/250/400/600"}]}}
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;- Call
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt; curl -X POST "http://localhost:8080/predict" -d '{
      "service":"imageserv",
      "parameters":{
        "input":{
          "width":224,
          "height":224
        },
        "output":{
          "best":3
        }
      },
      "data": ["https://picsum.photos/id/120/400/600"]
    }'
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;- Output
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","service":"imageserv","time":271.0},"body":{"predictions":[{"classes":[{"prob":0.19882911443710328,"cat":"n09472597 volcano"},{"prob":0.1562669426202774,"cat":"n02906734 broom"},{"last":true,"cat":"n04525038 velvet","prob":0.0657290369272232}],"uri":"https://picsum.photos/id/120/400/600"}]}}
&lt;/denchmark-code&gt;

We observe that we have predictions that seem to have a sense.

Predict all images in one request
Now let's try to predict all of those images in one request. We will do it several time

Call



&lt;denchmark-code&gt;curl -X POST "http://localhost:8080/predict" -d '{
      "service":"imageserv",
      "parameters":{
        "input":{
          "width":224,
          "height":224
        },
        "output":{
          "best":3
        }
      },
      "data": ["https://picsum.photos/id/237/400/600", "https://picsum.photos/id/250/400/600", "https://picsum.photos/id/120/400/600"]
    }'
&lt;/denchmark-code&gt;

We tried this request 3 times, here are the outputs we observed.
- Output
&lt;denchmark-code&gt;{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","service":"imageserv","time":365.0},"body":{"predictions":[{"classes":[{"prob":0.19882911443710328,"cat":"n09472597 volcano"},{"prob":0.1562669426202774,"cat":"n02906734 broom"},{"last":true,"cat":"n04525038 velvet","prob":0.0657290369272232}],"uri":"https://picsum.photos/id/120/400/600"},{"classes":[{"prob":1.0,"cat":"n02107683 Bernese mountain dog"},{"prob":0.0,"cat":"n01440764 tench, Tinca tinca"},{"last":true,"cat":"n01443537 goldfish, Carassius auratus","prob":0.0}],"uri":"https://picsum.photos/id/237/400/600"},{"classes":[{"prob":1.0,"cat":"n02107683 Bernese mountain dog"},{"prob":0.0,"cat":"n01440764 tench, Tinca tinca"},{"last":true,"cat":"n01443537 goldfish, Carassius auratus","prob":0.0}],"uri":"https://picsum.photos/id/250/400/600"}]}}



{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","service":"imageserv","time":359.0},"body":{"predictions":[{"classes":[{"prob":0.5831432938575745,"cat":"n04069434 reflex camera"},{"prob":0.3723478317260742,"cat":"n03976467 Polaroid camera, Polaroid Land camera"},{"last":true,"cat":"n03657121 lens cap, lens cover","prob":0.04316359758377075}],"uri":"https://picsum.photos/id/250/400/600"},{"classes":[{"prob":0.3576153516769409,"cat":"n02107683 Bernese mountain dog"},{"prob":0.27076518535614016,"cat":"n02107574 Greater Swiss Mountain dog"},{"last":true,"cat":"n03404251 fur coat","prob":0.16393133997917176}],"uri":"https://picsum.photos/id/120/400/600"},{"classes":[{"prob":0.3576153516769409,"cat":"n02107683 Bernese mountain dog"},{"prob":0.27076518535614016,"cat":"n02107574 Greater Swiss Mountain dog"},{"last":true,"cat":"n03404251 fur coat","prob":0.16393133997917176}],"uri":"https://picsum.photos/id/237/400/600"}]}}



{"status":{"code":200,"msg":"OK"},"head":{"method":"/predict","service":"imageserv","time":946.0},"body":{"predictions":[{"classes":[{"prob":0.4076613783836365,"cat":"n02099712 Labrador retriever"},{"prob":0.24927398562431336,"cat":"n02093256 Staffordshire bullterrier, Staffordshire bull terrier"},{"last":true,"cat":"n03223299 doormat, welcome mat","prob":0.04856392741203308}],"uri":"https://picsum.photos/id/237/400/600"},{"classes":[{"prob":0.999929666519165,"cat":"n02107574 Greater Swiss Mountain dog"},{"prob":0.00007033973815850914,"cat":"n02107683 Bernese mountain dog"},{"last":true,"cat":"n02107908 Appenzeller","prob":5.1516100413318e-11}],"uri":"https://picsum.photos/id/120/400/600"},{"classes":[{"prob":0.999929666519165,"cat":"n02107574 Greater Swiss Mountain dog"},{"prob":0.00007033973815850914,"cat":"n02107683 Bernese mountain dog"},{"last":true,"cat":"n02107908 Appenzeller","prob":5.1516100413318e-11}],"uri":"https://picsum.photos/id/250/400/600"}]}}
&lt;/denchmark-code&gt;

We observe that the output is not always the same. The order varies, no issue with that however the distribution of probabilities obtained do not correspond to what we observed when we used only one image at a time.
After testing many things I observed that de batch_size of the deploy_prototxt is set to 1. Changing this value to 3 made those problems disappear. However it was not like this in the previous versions before.
I tried modifying the value of this batch_size on the test request (what we usually do) however it does not work either. I even try modifying this value in the request we use to create the service but it does not work either.
TL;DR

predictions of N documents with N &gt; batch_size in deploy.prototxt returns wrong predictions for  N-batch_size last documents
it seems impossible to override this value as it was before

Potential solutions

Default batch_size

Either use the batch_size set by default in the backend (from the service creation if it was indicated or from the deploy.prototxt)
Either keep previous usage and allocate enough memory for managing the whole documents of the prediction


Be able to override the batch_size in the predict request

	</description>
	<comments>
		<comment id='1' author='YaYaB' date='2021-01-12T13:54:50Z'>
		Hi &lt;denchmark-link:https://github.com/YaYaB&gt;@YaYaB&lt;/denchmark-link&gt;
 and thanks for the very precise bug report.
We are investigating, it seems to be (yet another) strange behavior with cudnn
		</comment>
		<comment id='2' author='YaYaB' date='2021-01-12T15:38:47Z'>
		ok, that was a long shot, should be fixed by &lt;denchmark-link:https://github.com/jolibrain/caffe/pull/81&gt;jolibrain/caffe#81&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='YaYaB' date='2021-01-12T15:43:13Z'>
		FTR: the cudnn conv layer was not correctly reshaped after a batch size change (in your case a change between the first net allocation with default batch size of 1 and the new batch size computed from the predict request, ie 3 in your example). This bug comes from &lt;denchmark-link:https://github.com/jolibrain/caffe/commit/73dba3d301d8f92890eae36c6de9b34f08273965&gt;jolibrain/caffe@73dba3d&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='YaYaB' date='2021-01-12T15:46:58Z'>
		Great! thanks for the quick investigation, sounds clear to me. I'll try as soon as it is merged!
		</comment>
		<comment id='5' author='YaYaB' date='2021-01-13T13:12:37Z'>
		Next ci-master docker builds should carry the change.
		</comment>
	</comments>
</bug>