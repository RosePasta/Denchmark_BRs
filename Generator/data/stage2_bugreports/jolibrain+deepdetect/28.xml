<bug id='28' author='beniz' open_date='2015-10-09T16:14:31Z' closed_time='2015-12-02T07:43:15Z'>
	<summary>Caffe BlockingQueue locks on mutex when peeking after a service has exception</summary>
	<description>
In some cases, when a training service fails on CUDA memory allocation or other errors at model loading time, it seems the lock on Caffe BlockingQueue isn't released.
In server logs, this typically looks like server is hanging on:
&lt;denchmark-code&gt;INFO - Creating layer data
INFO - Creating Layer data
INFO - data -&gt; data
INFO - data -&gt; label
&lt;/denchmark-code&gt;

and upon debugging:
&lt;denchmark-code&gt;#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185
#1  0x000000000046f473 in boost::condition_variable::wait (this=0x7eff7562d348, m=...) at /usr/include/boost/thread/pthread/condition_variable.hpp:73
#2  0x00007ffff5c69e38 in caffe::BlockingQueue&lt;caffe::Datum*&gt;::peek() ()
from /build/lib/libcaffe.so
&lt;/denchmark-code&gt;

Possibly comes from the Caffe Solver object not being released upon exception. To be investigated shortly.
	</description>
	<comments>
		<comment id='1' author='beniz' date='2015-11-02T12:33:46Z'>
		Hi, I think I have the same problem:

I1102 13:22:46.576802 22488 net.cpp:106] Creating Layer w_traindata
I1102 13:22:46.576828 22488 net.cpp:411] w_traindata -&gt; traindata
*** Aborted at 1446466976 (unix time) try "date -d @1446466976" if you are using GNU date ***
PC: @     0x7f3a3f0fa414 __pthread_cond_wait
*** SIGTERM (@0x2ddc6e00005812) received by PID 22488 (TID 0x7f3a48104a40) from PID 22546; stack trace: ***
@     0x7f3a46709d40 (unknown)
@     0x7f3a3f0fa414 __pthread_cond_wait
@     0x7f3a478e14b3 boost::condition_variable::wait()
@     0x7f3a478e1770 caffe::BlockingQueue&lt;&gt;::peek()
@     0x7f3a478118cd caffe::DataLayer&lt;&gt;::DataLayerSetUp()
@     0x7f3a477f6bc3 caffe::BasePrefetchingDataLayer&lt;&gt;::LayerSetUp()
@     0x7f3a478b0c55 caffe::Net&lt;&gt;::Init()
@     0x7f3a478b1d05 caffe::Net&lt;&gt;::Net()
@     0x7f3a478bff7a caffe::Solver&lt;&gt;::InitTestNets()
@     0x7f3a478c0abd caffe::Solver&lt;&gt;::Init()
@     0x7f3a478c0dc9 caffe::Solver&lt;&gt;::Solver()
@     0x7f3a478da263 caffe::Creator_SGDSolver&lt;&gt;()
@           0x40f13e caffe::SolverRegistry&lt;&gt;::CreateSolver()
@           0x407860 train()
@           0x4056e1 main
@     0x7f3a466f4ec5 (unknown)
@           0x405dcd (unknown)
@                0x0 (unknown)
Terminated

I can't terminate the program on the command line with ^C, the above shows when killing the process.
		</comment>
		<comment id='2' author='beniz' date='2015-11-02T12:45:21Z'>
		Hey, in my experience this can only happen on the second call to /train, if the first one has failed. So usually you can still run fine once you've spotted the problem (most of the time something in prototxt file).
Though it looks the same error, if you are not falling in this exact case, it may well come from elsewhere (e.g. prototxt file). The thing to keep in mind is that Caffe does not return nicely on errors, it simply crashes, see &lt;denchmark-link:https://github.com/BVLC/caffe/issues/2976&gt;BVLC/caffe#2976&lt;/denchmark-link&gt;

DD's job is in part to catch and recover from these fatal errors but it may not always be possible. I still need a bit more time to clear this one out.
		</comment>
		<comment id='3' author='beniz' date='2015-11-02T13:15:19Z'>
		&lt;denchmark-link:https://github.com/meyerd&gt;@meyerd&lt;/denchmark-link&gt;
 you can try the above fix, but I cannot guarantee it does work for all Caffe error cases (some may not be recoverable).
		</comment>
		<comment id='4' author='beniz' date='2015-11-02T13:57:40Z'>
		&lt;denchmark-link:https://github.com/beniz&gt;@beniz&lt;/denchmark-link&gt;
 Thanks, I found out, what was causing the hang: If you open the same lmdb file twice (say for TEST and TRAIN in the cases you don't specify the phase when to include what, then it gets openend twice implicitely), then the peek on the blockingqueue seems to hang. In my case I was just specifying a simple net without wanting to specify different datasets for train and test out of lazyness. If I just copy the same file and then specify two different LMDBs all works well.
I don't know if that is a bug (I suspect, because as far as I know LMDB should be parallel readable).
		</comment>
		<comment id='5' author='beniz' date='2015-11-02T14:04:19Z'>
		OK, thanks for the report. I've seen the same report somewhere in the Caffe issues. It definitely could be fixed though it is cumbersome to train &amp; test from the same samples :)
		</comment>
	</comments>
</bug>