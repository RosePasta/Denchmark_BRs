<bug id='287' author='Zer0Credibility' open_date='2017-04-10T18:17:50Z' closed_time='2017-10-06T10:14:12Z'>
	<summary>1005 Error DeepDetect 1.2 AMI P2.xlarge</summary>
	<description>
DeepDetect 1.2 AMI P2.xlarge
called dd.info() which returned:
{u'status': {u'msg': u'OK', u'code': 200}, u'head': {u'services': [{u'mllib': u'tensorflow', u'name': u'iuris_detect', u'description': u'a-dede-service'}], u'commit': u'fabf94f1bec6f330b727484ede0efcbcc3acfe7c', u'version': u'0.1', u'method': u'/info', u'branch': u'ami'}}
called:
height = width = 299
data = ['http://www.deepdetect.com/img/ambulance.jpg']
parameters_input = {'width': height, 'height': width}
parameters_mllib = {'gpu': True}
parameters_output = {'best': 3}
which returned:
{u'status': {u'msg': u'BadRequest', u'dd_msg': u'Service Input Error', u'code': 400, u'dd_code': 1005}}
the log file contained the following:
INFO - 13:29:30 - googlenet does not need backward computation.
INFO - 13:29:30 - This network produces output label
INFO - 13:29:30 - This network produces output prob
DeepDetect [ commit fabf94f1bec6f330b727484ede0efcbcc3acfe7c ]
ERROR - 15:55:08 - Cannot create Cublas handle. Cublas won't be available.
ERROR - 15:55:08 - Cannot create Cusparse handle. Cusparse won't be available.
ERROR - 15:55:08 - Cannot create Curand generator. Curand won't be available.
ERROR - 16:01:07 - Cannot create Cublas handle. Cublas won't be available.
ERROR - 16:01:07 - Cannot create Cusparse handle. Cusparse won't be available.
ERROR - 16:01:07 - Cannot create Curand generator. Curand won't be available.
ERROR - 16:20:05 - Cannot create Cublas handle. Cublas won't be available.
ERROR - 16:20:05 - Cannot create Cusparse handle. Cusparse won't be available.
DeepDetect [ commit fabf94f1bec6f330b727484ede0efcbcc3acfe7c ]
DeepDetect [ commit fabf94f1bec6f330b727484ede0efcbcc3acfe7c ]
DeepDetect [ commit fabf94f1bec6f330b727484ede0efcbcc3acfe7c ]
I'm unsure what's causing the bad request error, as the code is essentially copied from the examples page in the documentation.
Thanks very much.
	</description>
	<comments>
		<comment id='1' author='Zer0Credibility' date='2017-04-10T18:24:27Z'>
		&lt;denchmark-link:https://github.com/Zer0Credibility&gt;@Zer0Credibility&lt;/denchmark-link&gt;
 thanks for reporting, it seems the server is restarting (that's when you see the  line). The first few  lines appear to be coming from Caffe calls. Do you experience the same problem with Caffe backend, using a Caffe model ?
		</comment>
		<comment id='2' author='Zer0Credibility' date='2017-04-10T18:51:43Z'>
		&lt;denchmark-link:https://github.com/beniz&gt;@beniz&lt;/denchmark-link&gt;
 thanks for the quick reply. I do get the same problem with Caffe, and I did restart the server a few times manually between yesterday and today.
		</comment>
		<comment id='3' author='Zer0Credibility' date='2017-04-10T19:03:43Z'>
		I have a very hard time just now getting the p2xlarge up, there aren't anymore on our building zone, so I need to transfer the AMIs. Have you tried the g2x instead ? I've never heard of p2xlarge problems so until I can reproduce, g2x may be an option.
		</comment>
		<comment id='4' author='Zer0Credibility' date='2017-04-10T19:13:55Z'>
		I'm labeling this as a bug for now because I was able to access a p2xlarge AMI and it seems the kernel has changed, so that the NVidia driver is not running, or something is in the way. I'm not sure how something like this can even happen but this is the second time I witness this.
EDIT: AMI is running kernel 4.4.0-72 but was built with kernel 4.4.0-59 (??)
		</comment>
		<comment id='5' author='Zer0Credibility' date='2017-04-10T19:27:47Z'>
		OK, bottom line, it works fine (GPU + tensorflow typically), but the kernel is in the way and needs to be removed:
&lt;denchmark-code&gt;sudo aptitude remove linux-image-4.4.0-72-generic
&lt;/denchmark-code&gt;

then reboot.
&lt;denchmark-link:https://github.com/Zer0Credibility&gt;@Zer0Credibility&lt;/denchmark-link&gt;
 PM on gitter if you can, and I should be able to put you back on track with your AMI. If somehow Amazon has updated the kernels, it will take a couple of weeks before they review our updates and upload the new version to the marketplace.
		</comment>
		<comment id='6' author='Zer0Credibility' date='2017-04-11T10:50:33Z'>
		Solved but we will need to re-build the AMI.
		</comment>
		<comment id='7' author='Zer0Credibility' date='2017-04-11T13:02:49Z'>
		&lt;denchmark-link:https://github.com/beniz&gt;@beniz&lt;/denchmark-link&gt;
 Thanks very much!
		</comment>
		<comment id='8' author='Zer0Credibility' date='2017-10-06T10:14:12Z'>
		Eventually found out that this comes from an auto-update of Ubuntu from within the AWS images. A new kernel is installed and after reboot the NVidia driver is not available for that kernel.
Up-to-date AWS images for CPU and GPU have been submitted to Amazon for review and this should be gone for a while. The new images also benefit from the new developments of DD, so that's good thing to have in addition.
		</comment>
	</comments>
</bug>