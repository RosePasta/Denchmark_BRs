<bug id='841' author='ydp' open_date='2018-04-10T09:28:56Z' closed_time='2018-10-30T17:04:11Z'>
	<summary>the same wide_and_deep model run as cpu vs gpu version have different number ops and performance</summary>
	<description>
Hi, all, I have a wide and deep model trained with tensorflow high level APIs. when I do inference with tensorflow serving, I complied both cpu and gpu version, it turns out that gpu is much slower.
the latency statistics as follow:
cpu:  18 ~ 20ms
gpu:  70 ~ 80ms
then, I dig into tensorflow code, in tensorflow/core/common_runtime/executor.cc, I print the cost time for each node in function Process(), did not find any good luck, but found one strange thing to me that cpu version have about 800+ ops (I mean node) executed, and gpu have 3300+ ops executed.
so, I guess the gpu version cost mush more time because it run much more ops?  Why the same model would run different number ops? Can anyone explain this?
And I have about 80+ features, handled with feature column with different kind feature_column method. (did not know this would affect)
Thanks very much.
	</description>
	<comments>
		<comment id='1' author='ydp' date='2018-04-10T09:36:33Z'>
		another fact I notices in the op list, in cpu version, I have follow one node:
&lt;denchmark-code&gt;head/predictions/logits/assert_equal/x
&lt;/denchmark-code&gt;

but in gpu version, I have more than one similar node:
&lt;denchmark-code&gt;head/predictions/logits/assert_equal/All/_4087
head/predictions/logits/Shape/_4089
head/predictions/logits/assert_equal/Assert/Assert/_4092
head/predictions/logits/assert_equal/Equal/_4085
head/predictions/logits/assert_equal/All/_4086
head/predictions/logits/assert_equal/Assert/Assert/_4091
&lt;/denchmark-code&gt;

Can someone help explain this?
Thanks very much.
		</comment>
		<comment id='2' author='ydp' date='2018-10-24T22:36:20Z'>
		&lt;denchmark-link:https://github.com/ydp&gt;@ydp&lt;/denchmark-link&gt;
  -  Hi, is this still an issue ?
		</comment>
		<comment id='3' author='ydp' date='2018-10-30T08:11:26Z'>
		&lt;denchmark-link:https://github.com/Harshini-Gadige&gt;@Harshini-Gadige&lt;/denchmark-link&gt;
  Thanks, you can close it now.
For anyone who might find this issue, I have now understand that tensorflow run graph, it will dispatch different kernel for different devices, op on different devices have different implementation, since one op might composed of other ops, so above results is expected.
Furthermore, my objective is to tune performance, just counting op numbers is not reliable, I have integrate pprof to profile the program execution.
		</comment>
	</comments>
</bug>