<bug id='1254' author='akreif' open_date='2019-02-06T21:03:14Z' closed_time='2019-03-21T17:34:04Z'>
	<summary>Bug in TFS MultiInference rpc method (undefined behavior)</summary>
	<description>
Hi,
there is probably a bug in MultiInference method. All issue with examples and explanations I've described at stack overflow: &lt;denchmark-link:https://stackoverflow.com/questions/54532063/bug-in-tfs-multiinference-rpc-method-undefined-behavior&gt;here&lt;/denchmark-link&gt;
.
TF and TFS version: 1.12
I will be very grateful if you look at the problem or explain to me what I'm doing wrong.
	</description>
	<comments>
		<comment id='1' author='akreif' date='2019-02-23T06:45:42Z'>
		Hi,
I think the confusion comes from the fact that a MultiInference request is designed to be applied to a single saved model, on a model version granularity level.  In fact, the model version specified by the first model spec in the request is the only one that matters (see &lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/multi_inference_helper.cc#L30&gt;here&lt;/denchmark-link&gt;
).
It would probably be cleaner to enforce the same model version, just like the same model name is already enforced &lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/inference.proto#L26&gt;here&lt;/denchmark-link&gt;
.
Then the above comment would be updated with something like:
"All ModelSpecs in a MultiInferenceRequest must access the same model name and version."
Happy to help with any additional concerns.
		</comment>
		<comment id='2' author='akreif' date='2019-02-27T21:36:05Z'>
		&lt;denchmark-link:https://github.com/akreif&gt;@akreif&lt;/denchmark-link&gt;
   Please take a look at the above response and post your concerns if any. Thanks !
		</comment>
		<comment id='3' author='akreif' date='2019-02-28T08:21:23Z'>
		Hi &lt;denchmark-link:https://github.com/christisg&gt;@christisg&lt;/denchmark-link&gt;
,
Your explanation makes me really sad. Your colleagues advertised TFS at TensorFlow Dev Summit 2018 and Google I/O '18 as software handling A/B tests out of the box. And I thought, MultiInference method is designed for it.
What should I do now? Do you have some "blessed" method for A/B test? Or maybe there is only one way to do this making two separate requests on different versions/version labels.
Have you considered upgrading MultiInference functionality? For example adding possibility of requesting model in different versions or even different names?
		</comment>
		<comment id='4' author='akreif' date='2019-03-16T01:09:55Z'>
		&lt;denchmark-link:https://github.com/akreif&gt;@akreif&lt;/denchmark-link&gt;
 thanks for your comment.
I think the important point is that MultiInference is not intended to be used for A/B testing - its intended use case is being able to do 2 different types of inferences (eg. a regression and a classification) in a a single request to TF Serving for efficiency purposes.
Your use case can be satisfied in one of the following two ways:


[Recommended] you can export 2 different versions of your models V1 and V2 that you'd like to A/B test across, and point your client to the different versions. You can make this more robust by using version labels (map "stable" to V1 and "experimental" to V2) so that your client code can stay the same even though V1 may become V3 and V2 may become V4 and so on... Look at this page [1] for more details on how to configure this.


If for whatever reason you don't want to use different model versions to do A/B testing (which is heavily recommended), then you can build both models into the same graph, and simply run the example through both models, and end up with the result of both models concatenated. This is weird and hacky - but if you can't use model versions, this is the best I can think of.


[1] &lt;denchmark-link:https://www.tensorflow.org/tfx/serving/serving_config#serving_multiple_versions_of_a_model&gt;https://www.tensorflow.org/tfx/serving/serving_config#serving_multiple_versions_of_a_model&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>