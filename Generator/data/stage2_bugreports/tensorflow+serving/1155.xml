<bug id='1155' author='ndvbd' open_date='2018-10-24T07:31:40Z' closed_time='2018-10-26T07:22:09Z'>
	<summary>tensorflow_model_server | Op type not registered 'PaddedBatchDatasetV2' in binary</summary>
	<description>
After training a model on my GPU server, and then trying to run it on CPU computer, I get:
Op type not registered 'PaddedBatchDatasetV2' in binary
I also upgraded the tensorflow_model_server.
I also tried to re-export it on the CPU computer, and run it with tensorflow_model_server with no success.
Can it be that the tensorflow_model_server doesn't have the core op PaddedBatchDatasetV2?
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;

Ubuntu 16.04
TensorFlow Serving installed: from binary, using apt-get upgrade tensorflow-model-server
TensorFlow Serving version:  what parameter should I pass to the tensorflow_model_server exec to query the version?
	</description>
	<comments>
		<comment id='1' author='ndvbd' date='2018-10-25T17:07:47Z'>
		
What is the version of TensorFlow (not serving) that you used to train your model?
Can you try serving your model using latest TF Serving? Using docker is typically the easiest/fastest. See these docs for more details.
--version flag (recently added) on the tensorflow_model_server binary should allow you to see version information.

		</comment>
		<comment id='2' author='ndvbd' date='2018-10-25T17:40:48Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
,

 does not return the version number. It shows the usage and flags, as if there is no --version parameter. Am I missing something?
		</comment>
		<comment id='3' author='ndvbd' date='2018-10-25T17:57:01Z'>
		strange &lt;denchmark-link:https://github.com/tensorflow/serving/blob/256450779fbef6aafde2461d9db44a62dc30f61c/tensorflow_serving/model_servers/main.cc#L148&gt;--version&lt;/denchmark-link&gt;
 does not work.
what does apt-cache show tensorflow-model-server report?
		</comment>
		<comment id='4' author='ndvbd' date='2018-10-25T17:58:21Z'>
		apt-cache show tensorflow-model-server
Package: tensorflow-model-server
Version: 1.9.0
Architecture: all
Maintainer: TensorFlow Serving team
Priority: optional
Section: contrib/devel
Filename: pool/tensorflow-model-server/t/tensorflow-model-server/tensorflow-model-server_1.9.0_all.deb
Size: 497254884
SHA256: 1a9808972eadf1c1ecd563731f4dd9c610c57289c14ae1b0a7cb6ca4a92f85db
SHA1: 45681c84ebad71d45d44737533667c1d24aa4075
MD5sum: b7f1e3177c57ccac117614035a14b140
Description: TensorFlow Serving ModelServer
Description-md5: 9b7e03f5296f318009581d6e285e2f89
Homepage: &lt;denchmark-link:https://github.com/tensorflow/serving&gt;https://github.com/tensorflow/serving&lt;/denchmark-link&gt;

Built-Using: Bazel
		</comment>
		<comment id='5' author='ndvbd' date='2018-10-25T18:02:38Z'>
		
ok you are using a relatively older TF Serving binary. --version flag does not exists in 1.9.0.
can you please updated to latest TF Serving 1.11.1 (given that you are already on TF 1.11.0). Its quite possible your op is not part of TF Serving 1.9.0

		</comment>
		<comment id='6' author='ndvbd' date='2018-10-26T07:22:00Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
 great solution. Indeed the solution was to upgrade to 1.11. The thing is that it was tricky to upgrade, because  didn't work by itself - it didn't print any error message regaring the fact that I don't have the latest version.. I had to remove the package completely, to do and only then to install it again. Then everything worked like a charm.
		</comment>
	</comments>
</bug>