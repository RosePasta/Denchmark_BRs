<bug id='1668' author='jameschanx' open_date='2020-06-17T22:19:36Z' closed_time='2020-07-19T20:54:14Z'>
	<summary>--per_process_gpu_memory_fraction flag does not provide accurate memory allocation</summary>
	<description>
&lt;denchmark-h:h2&gt;Bug Report&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution: EC2 g4dn.xlarge instance, Ubuntu 16.04 AWS Deep Learning AMI
TensorFlow Serving installed from (source or binary): docker pull tensorflow/serving:2.2.0-gpu
TensorFlow Serving version: 2.2.0

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

When I use --per_process_gpu_memory_fraction to allocate the amount of GPU ram to tensorflow serving, there is always an unexpected, additional amount being allocated.  For example, when I pass in 0.5 as the memory fraction, one would expect it to occupy half of the GPU ram.  However, the actual memory being occupied is more than half, by quite a bit.  As shown in the screenshot below, the total amount of ram is 15109 MiB, and half of that would be 7555 MiB.  This means somehow an additional 234 MiB was allocated (7789-7555=234).
&lt;denchmark-link:https://user-images.githubusercontent.com/42701249/84937315-6553ee80-b090-11ea-9aea-89eebbe0ddad.png&gt;&lt;/denchmark-link&gt;

When I set the fraction to .25, I get 4011 MiB, which again, is 234MiB over the expected 3777 MiB (15109/4=3777).
I cannot find any documentation describing this behavior, so I am led to believe it is a bug.  While this "additional memory" seems to be fairly predictable, it is very inconvenient when I try to allocate an exact amount of GPU memory to tensorflow serving.  Currently I have to first perform an experiment to find out how much is this additional memory and then I will manually subtract  that amount from my desired amount before calculating again the "adjusted" memory fraction.
Things to also note:

This additional amount is the same regardless of which model and how many model one deploys.  However, it is different if the GPU hardware and/or Cuda Tool Kit version is different.  In other word, it is highly coupled to the environment in which tensorflow-serving is running.
For some larger models such as ones based on Inception, when you send an inferencing request, the memory ticks up again by another set amount.  This is not observed using the toy example provided in tf-serving, but I personally observe it with other larger models.

&lt;denchmark-h:h3&gt;Exact Steps to Reproduce&lt;/denchmark-h&gt;

I followed the exact steps in the GPU portion of this page &lt;denchmark-link:https://www.tensorflow.org/tfx/serving/docker&gt;https://www.tensorflow.org/tfx/serving/docker&lt;/denchmark-link&gt;

The run command I am using is below:
docker run --runtime=nvidia -p 8501:8501 \ --mount type=bind,\ source=/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu,\ target=/models/half_plus_two \ -e MODEL_NAME=half_plus_two -t tensorflow/serving:latest-gpu --per_process_gpu_memory_fraction=0.5
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

2020-06-17 21:46:46.714354: I tensorflow_serving/model_servers/server.cc:86] Building single TensorFlow model file config:  model_name: half_plus_two model_base_path: /models/half_plus_two
2020-06-17 21:46:46.714515: I tensorflow_serving/model_servers/server_core.cc:464] Adding/updating models.
2020-06-17 21:46:46.714529: I tensorflow_serving/model_servers/server_core.cc:575]  (Re-)adding model: half_plus_two
2020-06-17 21:46:46.814754: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: half_plus_two version: 1}
2020-06-17 21:46:46.814776: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: half_plus_two version: 1}
2020-06-17 21:46:46.814785: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: half_plus_two version: 1}
2020-06-17 21:46:46.814809: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/half_plus_two/1
2020-06-17 21:46:46.815498: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2020-06-17 21:46:46.815517: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:295] Reading SavedModel debug info (if present) from: /models/half_plus_two/1
2020-06-17 21:46:46.815593: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-06-17 21:46:46.816735: I external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-06-17 21:46:46.841948: I external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-17 21:46:46.842752: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2020-06-17 21:46:46.842766: I external/org_tensorflow/tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2020-06-17 21:46:46.842829: I external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-17 21:46:46.843628: I external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-17 21:46:46.844374: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-06-17 21:52:28.194528: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-06-17 21:52:28.194557: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-06-17 21:52:28.194562: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-06-17 21:52:28.194700: I external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-17 21:52:28.195568: I external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-17 21:52:28.196347: I external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-06-17 21:52:28.197102: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3777 MB memory) -&gt; physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)
2020-06-17 21:52:28.209209: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.
2020-06-17 21:52:28.278492: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /models/half_plus_two/1
2020-06-17 21:52:28.282331: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:364] SavedModel load for tags { serve }; Status: success: OK. Took 341467520 microseconds.
2020-06-17 21:52:28.282533: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /models/half_plus_two/1/assets.extra/tf_serving_warmup_requests
2020-06-17 21:52:28.282691: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: half_plus_two version: 1}
2020-06-17 21:52:28.284355: I tensorflow_serving/model_servers/server.cc:355] Running gRPC ModelServer at 0.0.0.0:8500 ...
[warn] getaddrinfo: address family for nodename not supported
2020-06-17 21:52:28.285741: I tensorflow_serving/model_servers/server.cc:375] Exporting HTTP/REST API at:localhost:8501 ...
[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...
	</description>
	<comments>
		<comment id='2' author='jameschanx' date='2020-07-11T21:00:09Z'>
		

However, it is different if the GPU hardware and/or Cuda Tool Kit version is different.


Given this, it is very likely you're seeing the additional GPU memory allocated by the CUDA libraries.  This amount is difficult for TF to predict as well.
When  is unset we try to &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/17bf7027b14aaa75c0a75e7b652a81f3a975be28/tensorflow/core/common_runtime/gpu/gpu_device.cc#L974&gt;set aside&lt;/denchmark-link&gt;
 some extra memory for these system libraries.  We could extending changing this behavior and setting aside some system memory even when  is set, but that's too risky a change to cherry pick for TF 2.3, so the earliest we can make this happen is the 2.4 release.  It is risky because it is likely going to break some user workflows.
		</comment>
		<comment id='3' author='jameschanx' date='2020-07-17T06:51:51Z'>
		Thanks for the context, &lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/jameschanx&gt;@jameschanx&lt;/denchmark-link&gt;
, is there any additional follow-up needed, or can we close this issue?
		</comment>
		<comment id='4' author='jameschanx' date='2020-07-19T20:54:10Z'>
		Thank you so much for the response &lt;denchmark-link:https://github.com/sanjoy&gt;@sanjoy&lt;/denchmark-link&gt;
 it would be greatly appreciated to have the change in 2.4 if at all possible.  &lt;denchmark-link:https://github.com/christisg&gt;@christisg&lt;/denchmark-link&gt;
 I've gone ahead and closed the issue, thank you for the help.
		</comment>
	</comments>
</bug>