<bug id='1562' author='Arnold1' open_date='2020-02-28T19:22:52Z' closed_time='2020-03-13T07:30:18Z'>
	<summary>tf_serving_warmup_requests with model-warmup-options gives error</summary>
	<description>
&lt;denchmark-h:h2&gt;Bug Report&lt;/denchmark-h&gt;

If this is a bug report, please fill out the following form in full:
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.9
TensorFlow Serving installed from (source or binary): source
TensorFlow Serving version: 2.1.0

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Describe the problem clearly here. Be sure to convey here why it's a bug in
TensorFlow Serving.
&lt;denchmark-code&gt;2020-02-28 19:15:21.150722: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:117] Starting to read warmup data for model at /models/1/2/exported_models/12322323/123123123442/assets.extra/tf_serving_warmup_requests with model-warmup-options 
2020-02-28 19:15:21.150771: E tensorflow_serving/util/retrier.cc:37] Loading servable: {name: model-1-2 version: 123123123442} failed: Data loss: corrupted record at 0. Please verify your warmup data is in TFRecord format.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Exact Steps to Reproduce&lt;/denchmark-h&gt;

Please include all steps necessary for someone to reproduce this issue on their
own machine. If not, skip this section.
&lt;denchmark-code&gt;docker run --rm -it \
  ...
  --model_config_file=/serving/models.conf \
  --enable_batching=true \
  --batching_parameters_file=/serving/batching_parameters.txt \
  --monitoring_config_file=/serving/monitoring.conf \
  --enable_model_warmup=true
&lt;/denchmark-code&gt;

here my tf_serving_warmup_requests file, which I copied into assets.extra:
&lt;denchmark-code&gt;import tensorflow as tf
from tensorflow.python.framework import tensor_util
from tensorflow_serving.apis import model_pb2
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_log_pb2

def main():
    """Generate TFRecords for warming up."""
    with tf.python_io.TFRecordWriter("tf_serving_warmup_requests") as writer:
        predict_request = predict_pb2.PredictRequest()
        predict_request.model_spec.name = 'model-1-2'
        predict_request.model_spec.signature_name = 'serving_default'
        predict_request.inputs['price'].CopyFrom(tensor_util.make_tensor_proto([0.1], tf.float32))
        predict_request.inputs['height'].CopyFrom(tensor_util.make_tensor_proto([1], tf.int64))
        predict_request.inputs['domain'].CopyFrom(tensor_util.make_tensor_proto(["1".encode()], tf.string))

        log = prediction_log_pb2.PredictionLog(
            predict_log=prediction_log_pb2.PredictLog(request=predict_request))
        writer.write(log.SerializeToString())

if __name__ == "__main__":
    main()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached. Try to provide a reproducible test case that is the bare
minimum necessary to generate the problem.
&lt;denchmark-h:h3&gt;Other infos&lt;/denchmark-h&gt;

when I disable enable_model_warmup ( --enable_model_warmup=false):
than I don't get this strange Data loss error and the model can be served without issues...
	</description>
	<comments>
		<comment id='1' author='Arnold1' date='2020-02-28T19:37:24Z'>
		&lt;denchmark-link:https://github.com/christisg&gt;@christisg&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 any idea since this is similar to &lt;denchmark-link:https://github.com/tensorflow/serving/issues/1414&gt;#1414&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='2' author='Arnold1' date='2020-03-02T18:46:48Z'>
		&lt;denchmark-link:https://github.com/Arnold1&gt;@Arnold1&lt;/denchmark-link&gt;
 Please give me the exact steps you followed to reproduce the error. Thanks!
		</comment>
		<comment id='3' author='Arnold1' date='2020-03-11T06:56:11Z'>
		&lt;denchmark-link:https://github.com/Arnold1&gt;@Arnold1&lt;/denchmark-link&gt;
,
Can you please respond to &lt;denchmark-link:https://github.com/gowthamkpr&gt;@gowthamkpr&lt;/denchmark-link&gt;
's comment above. Thanks!
		</comment>
		<comment id='4' author='Arnold1' date='2020-03-12T20:45:32Z'>
		&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 we can close this ticket. I used the wrong file to pre-heat the model. is there a way to add autoloading at startup of models with a new tensorflow version? it would be better to have all models in memory when the tfs starts up and receives the first grpc request...
		</comment>
		<comment id='5' author='Arnold1' date='2020-03-13T07:30:18Z'>
		Closing the issue as it has been resolved.
		</comment>
	</comments>
</bug>