<bug id='1257' author='nishantagarwal' open_date='2019-02-11T09:39:44Z' closed_time='2019-03-05T12:31:10Z'>
	<summary>Tensorflow Serving container crashes for object detection models</summary>
	<description>
&lt;denchmark-h:h2&gt;Bug Report&lt;/denchmark-h&gt;

Tensorflow Serving 1.12 container crashes regularly when running predictions on &lt;denchmark-link:https://github.com/tensorflow/models/blob/59f7e80ac8ad54913663a4b63ddf5a3db3689648/research/object_detection/samples/configs/ssd_inception_v3_pets.config&gt;TensorFlow object detection models&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
TensorFlow Serving installed from (source or binary): Source
TensorFlow Serving version: 1.12
RAM: 256 GB
CPU Model: Intel(R) Xeon(R) CPU E5-2603 v4 @ 1.70GHz
CPU Count: 12

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Using TensorFlow 1.9, &lt;denchmark-link:https://github.com/tensorflow/models/blob/59f7e80ac8ad54913663a4b63ddf5a3db3689648/research/object_detection/samples/configs/ssd_inception_v3_pets.config&gt;SSD Inception V3&lt;/denchmark-link&gt;
 based object detection model was trained on custom dataset. Post training the model works fine without using Tensorflow Serving. When the same model is served using Tensorflow Serving, the Tensorflow serving container crashes regularly. Sometimes it crashes on first request itself and sometimes after few successful predictions. There are no concurrent requests made to Tensorflow Serving container.
If the Tensorflow Serving docker container is created using the latest code in master branch, then the container does not crash.

Is there any code change done after 1.12 which fixes this issue ?
What is the release date for next version of Tensorflow Serving ?

&lt;denchmark-h:h3&gt;Exact Steps to Reproduce&lt;/denchmark-h&gt;


Train any SSD or Faster RCNN model present in Tensorflow Research repo using Tensorflow 1.9.0.
Build Tensorflow Serving docker image using Dockerfile
Deploy the trained model in Tensorflow Serving container
Submit sync predictions request (stub.Predict(request, 30.0)) to the model served using Tensorflow Serving container.

&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

root@vm01:~# docker run -p 9998:8500 -m 100g --cpus="6" --mount type=bind,source=$(pwd),target=/models/dev_perf_tfod_incepv3_n50_e30000 -e MODEL_NAME=dev_perf_tfod_incepv3_n50_e30000 -t custom/tensorflow-serving:1.12
WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.
2019-02-08 10:37:17.040097: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: dev_perf_tfod_incepv3_n50_e30000 model_base_path: /models/dev_perf_tfod_incepv3_n50_e30000
2019-02-08 10:37:17.040358: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.
2019-02-08 10:37:17.040387: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: dev_perf_tfod_incepv3_n50_e30000
2019-02-08 10:37:17.140963: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: dev_perf_tfod_incepv3_n50_e30000 version: 1}
2019-02-08 10:37:17.141025: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: dev_perf_tfod_incepv3_n50_e30000 version: 1}
2019-02-08 10:37:17.141053: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: dev_perf_tfod_incepv3_n50_e30000 version: 1}
2019-02-08 10:37:17.141099: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /models/dev_perf_tfod_incepv3_n50_e30000/1
2019-02-08 10:37:17.141126: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/dev_perf_tfod_incepv3_n50_e30000/1
2019-02-08 10:37:17.320294: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2019-02-08 10:37:17.456976: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.
2019-02-08 10:37:17.457079: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:172] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /models/dev_perf_tfod_incepv3_n50_e30000/1/variables/variables.index
2019-02-08 10:37:17.457098: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key legacy_init_op on SavedModel bundle.
2019-02-08 10:37:17.457124: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: success. Took 315989 microseconds.
2019-02-08 10:37:17.457162: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:83] No warmup data file found at /models/dev_perf_tfod_incepv3_n50_e30000/1/assets.extra/tf_serving_warmup_requests
2019-02-08 10:37:17.457382: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: dev_perf_tfod_incepv3_n50_e30000 version: 1}
2019-02-08 10:37:17.462168: I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ...
[warn] getaddrinfo: address family for nodename not supported
2019-02-08 10:37:17.465124: I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...
[evhttp_server.cc : 237] RAW: Entering the event loop ...
double free or corruption (out)
/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 (core dumped) tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} "$@"
	</description>
	<comments>
		<comment id='1' author='nishantagarwal' date='2019-02-11T20:55:36Z'>
		can you post the crash stacktrace from the coredump to help debug further?
does it also crash with official &lt;denchmark-link:https://hub.docker.com/r/tensorflow/serving/tags&gt;tensorflow/serving:latest or tensorflow/serving:nightly&lt;/denchmark-link&gt;
 images? without details its hard to know if this is fixed in upcoming 1.13 release (trying the nightly will be closest to testing 1.13)
		</comment>
		<comment id='2' author='nishantagarwal' date='2019-02-12T09:54:29Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
 The official  image also crashes where as  works fine. I have attached the container logs for those images which crash.

log_tensorflow_serving_docker_devel_1.12.txt.zip
log_tensorflow_serving_docker_latest.txt

		</comment>
		<comment id='3' author='nishantagarwal' date='2019-02-12T18:01:45Z'>
		thanks for testing. as i said, nightly is closest to upcoming 1.13. we plan to publish 1.13.0-rc1 sometime today. i think we should test on this rc1, and if it reproduces there we should debug further and fix this.
its been a longer release cycle this time since 1.12, and it might be hard to bisect and find the offending change (given that it could be in TF Serving or TF core).
		</comment>
		<comment id='4' author='nishantagarwal' date='2019-02-12T18:13:52Z'>
		can you post your prediction (grpc) code, that i can run against the model server (to help reproduce at my end)? thanks!
		</comment>
		<comment id='5' author='nishantagarwal' date='2019-02-13T04:14:12Z'>
		
thanks for testing. as i said, nightly is closest to upcoming 1.13. we plan to publish 1.13.0-rc1 sometime today. i think we should test on this rc1, and if it reproduces there we should debug further and fix this.
its been a longer release cycle this time since 1.12, and it might be hard to bisect and find the offending change (given that it could be in TF Serving or TF core).

Thanks for sharing your views. When 1.13.0-rc1 gets published I will test on it.

can you post your prediction (grpc) code, that i can run against the model server (to help reproduce at my end)? thanks!

Below is a simplified version of code for prediction:
import grpc
import numpy as np
from tensorflow_serving.apis import model_service_pb2_grpc, model_management_pb2, get_model_status_pb2, predict_pb2, prediction_service_pb2_grpc
from tensorflow_serving.config import model_server_config_pb2
from tensorflow.contrib.util import make_tensor_proto
from tensorflow.core.framework import types_pb2

def predict_test(batch_size, serving_config):
    channel = grpc.insecure_channel(serving_config['hostport'], options=[('grpc.max_send_message_length', serving_config['max_message_length']), (
        'grpc.max_receive_message_length', serving_config['max_message_length'])])
    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)

    # Creating random images for given batch size
    images = np.random.rand(batch_size, 300, 300, 3)  
    
    request = predict_pb2.PredictRequest()
    request.model_spec.name = serving_config['model_name']
    request.model_spec.signature_name = serving_config['signature_name']
    request.inputs['inputs'].CopyFrom(make_tensor_proto(
        images, shape=[batch_size, 300, 300, 3], dtype=types_pb2.DT_UINT8))
    result = stub.Predict(request, serving_config['timeout'])
    channel.close()    
    return result


if __name__ == "__main__":
    serving_config = {
        "hostport": "127.0.0.1:9999",
        "max_message_length": 200 * 1024 * 1024,
        "timeout": 500,
        "signature_name": "serving_default",
        "model_name": "incepv3_n50_e30000"
    }
    predict_result = predict_test(100, serving_config)
		</comment>
		<comment id='6' author='nishantagarwal' date='2019-02-14T04:18:46Z'>
		jfyi -- &lt;denchmark-link:https://hub.docker.com/r/tensorflow/serving/tags&gt;1.13.0-rc1 docker images&lt;/denchmark-link&gt;
 are out.
and thanks for posting the code, will try it out soon (unless you get to testing 1.13.0-rc1 first :-)
		</comment>
		<comment id='7' author='nishantagarwal' date='2019-02-15T20:55:36Z'>
		&lt;denchmark-link:https://github.com/nishantagarwal&gt;@nishantagarwal&lt;/denchmark-link&gt;
 -- we plan to cut RC2 soon. if you can help us test RC1 that would be awesome. thanks!
		</comment>
		<comment id='8' author='nishantagarwal' date='2019-02-16T02:18:12Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
 - on RC1 it works fine :-)
		</comment>
		<comment id='9' author='nishantagarwal' date='2019-02-16T03:10:23Z'>
		&lt;denchmark-link:https://github.com/nishantagarwal&gt;@nishantagarwal&lt;/denchmark-link&gt;
 -- thanks for testing!
lets keep this issue open until we release 1.13.0 (in a week) to do a final check if 1.13.0 release holds fine for you.
		</comment>
		<comment id='10' author='nishantagarwal' date='2019-03-04T22:02:27Z'>
		We released 1.13.0 last week:
&lt;denchmark-link:https://github.com/tensorflow/serving/releases/tag/1.13.0&gt;https://github.com/tensorflow/serving/releases/tag/1.13.0&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/nishantagarwal&gt;@nishantagarwal&lt;/denchmark-link&gt;
 if you can pretty please test this on this released version, that would be great.
And please close this issue, if things work well (no crashes).
Thanks a lot for your help!
		</comment>
		<comment id='11' author='nishantagarwal' date='2019-03-05T12:31:10Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
 It works well on 1.13.0 without any crashes. So, I am closing this issue.
		</comment>
		<comment id='12' author='nishantagarwal' date='2019-05-10T08:02:28Z'>
		Did you test the grpc or restful api? &lt;denchmark-link:https://github.com/nishantagarwal&gt;@nishantagarwal&lt;/denchmark-link&gt;

Tensorflow Serving container crashes regularly  when I test the restful api.
		</comment>
		<comment id='13' author='nishantagarwal' date='2019-05-10T08:14:00Z'>
		I use tensorflow/serving:1.13.0-gpu
		</comment>
		<comment id='14' author='nishantagarwal' date='2019-05-10T09:13:34Z'>
		&lt;denchmark-link:https://github.com/fwz-fpga&gt;@fwz-fpga&lt;/denchmark-link&gt;
 I tested it for grpc on tensorflow/serving:1.13.0
		</comment>
		<comment id='15' author='nishantagarwal' date='2019-05-16T02:45:44Z'>
		do not fix .
		</comment>
	</comments>
</bug>