<bug id='1004' author='srihari-humbarwadi' open_date='2018-07-20T04:45:48Z' closed_time='2018-11-16T19:14:00Z'>
	<summary>Failed precondition error when using Tensorflow serving to serve pretrained keras xception model</summary>
	<description>
This is the code that i am using to export the keras model into tensorflow serving format.The exported model loads up successfully in tensorflow serving( without any warnings or errors). But when i use my client to make a request to the server, i get a FailedPrecondition error.
&lt;denchmark-code&gt; grpc._channel._Rendezvous: &lt;_Rendezvous of RPC that terminated with:
    status = StatusCode.FAILED_PRECONDITION
    details = "Attempting to use uninitialized value block11_sepconv2_bn/moving_mean
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;import sys
import os
import tensorflow as tf
from keras import backend as K
from keras.models import Model
from keras.models import load_model
from tensorflow.python.saved_model import builder as saved_model_builder
from tensorflow.python.saved_model import utils
from tensorflow.python.saved_model import tag_constants, signature_constants
from tensorflow.python.saved_model.signature_def_utils_impl import     
build_signature_def, predict_signature_def
from tensorflow.contrib.session_bundle import exporter
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

config = tf.ConfigProto( device_count = {'GPU': 2 , 'CPU': 12} ) 
sess = tf.Session(config=config) 
K.set_session(sess)
K._LEARNING_PHASE = tf.constant(0)
K.set_learning_phase(0) 

xception = load_model('models/xception/model.h5')
config = xception.get_config()
weights = xception.get_weights()

new_xception = Model.from_config(config)
new_xception.set_weights(weights)

export_path = 'prod_models/2'
builder = saved_model_builder.SavedModelBuilder(export_path)
signature = predict_signature_def(inputs={'images': new_xception.input},
                              outputs={'scores': new_xception.output})
with K.get_session() as sess:
    builder.add_meta_graph_and_variables(sess=sess,
                                     tags=[tag_constants.SERVING],
                                     signature_def_map={'predict': 
                                                       signature})
    builder.save()
&lt;/denchmark-code&gt;

Package versions
Python 3.6.3
tensorflow-gpu 1.8.0
Keras 2.1.5
CUDA 9.0.176
	</description>
	<comments>
		<comment id='1' author='srihari-humbarwadi' date='2018-07-21T01:28:06Z'>
		&lt;denchmark-link:https://github.com/martinwicke&gt;@martinwicke&lt;/denchmark-link&gt;
 can you please take a look? thanks!
		</comment>
		<comment id='2' author='srihari-humbarwadi' date='2018-07-23T14:47:55Z'>
		I believe if you lead the model like you do above, the (local) variables inside the mean may end up not being initialized (&lt;denchmark-link:https://github.com/fchollet&gt;@fchollet&lt;/denchmark-link&gt;
, correct me if I'm wrong, but they may be initialized in fit or something?).
&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
, do you know what the SavedModelBuilder would do with uninitialized variables? My suspicion is that it would leave them uninitialized, leading to this error. The question is whether this is reasonable behavior.
What is definitely not reasonable behavior is that ModelServer loads this model and doesn't complain (at least warn) that there are unininitialized variables.
		</comment>
		<comment id='3' author='srihari-humbarwadi' date='2018-07-23T17:14:37Z'>
		If i convert the graph variables to constants ie if i feeeze the graph i get the expected predictions.
		</comment>
		<comment id='4' author='srihari-humbarwadi' date='2018-07-23T20:58:17Z'>
		From the add_meta_graph_and_variables docs-- "This function assumes that the variables to be saved have been initialized."
If you add a call to fit, rather than just compiling via the load_model, I believe that should ensure everything is initialized? I would also recommend using tf.keras rather than the external Keras if your stack is TensorFlow and tf-serving.
CC &lt;denchmark-link:https://github.com/k-w-w&gt;@k-w-w&lt;/denchmark-link&gt;
 , @a30041839 , &lt;denchmark-link:https://github.com/allenlavoie&gt;@allenlavoie&lt;/denchmark-link&gt;
 , who are looking into SavedModels for Keras
		</comment>
		<comment id='5' author='srihari-humbarwadi' date='2018-07-24T01:41:54Z'>
		I can confirm that using SavedModelBuilder
After i call model.fit_generator the issue still persists
		</comment>
		<comment id='6' author='srihari-humbarwadi' date='2018-10-17T20:44:15Z'>
		&lt;denchmark-link:https://github.com/srihari-humbarwadi&gt;@srihari-humbarwadi&lt;/denchmark-link&gt;
  Is this still an issue?
		</comment>
		<comment id='7' author='srihari-humbarwadi' date='2018-10-18T23:11:54Z'>
		&lt;denchmark-code&gt;import sys
from keras.models import load_model
import tensorflow as tf
from keras import backend as K
from tensorflow.python.framework import graph_util
from tensorflow.python.framework import graph_io
from tensorflow.python.saved_model import signature_constants
from tensorflow.python.saved_model import tag_constants


K.set_learning_phase(0)
K.set_image_data_format('channels_last')

INPUT_MODEL = sys.argv[1]
NUMBER_OF_OUTPUTS = 1
OUTPUT_NODE_PREFIX = 'output_node'
OUTPUT_FOLDER= 'frozen'
OUTPUT_GRAPH = 'frozen_model.pb'
OUTPUT_SERVABLE_FOLDER = sys.argv[2]
INPUT_TENSOR = sys.argv[3]


try:
    model = load_model(INPUT_MODEL)
except ValueError as err:
    print('Please check the input saved model file')
    raise err

output = [None]*NUMBER_OF_OUTPUTS
output_node_names = [None]*NUMBER_OF_OUTPUTS
for i in range(NUMBER_OF_OUTPUTS):
    output_node_names[i] = OUTPUT_NODE_PREFIX+str(i)
    output[i] = tf.identity(model.outputs[i], name=output_node_names[i])
print('Output Tensor names: ', output_node_names)


sess = K.get_session()
try:
    frozen_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), output_node_names)    
    graph_io.write_graph(frozen_graph, OUTPUT_FOLDER, OUTPUT_GRAPH, as_text=False)
    print(f'Frozen graph ready for inference/serving at {OUTPUT_FOLDER}/{OUTPUT_GRAPH}')
except:
    print('Error Occured')



builder = tf.saved_model.builder.SavedModelBuilder(OUTPUT_SERVABLE_FOLDER)

with tf.gfile.GFile(f'{OUTPUT_FOLDER}/{OUTPUT_GRAPH}', "rb") as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

sigs = {}
OUTPUT_TENSOR = output_node_names
with tf.Session(graph=tf.Graph()) as sess:
    tf.import_graph_def(graph_def, name="")
    g = tf.get_default_graph()
    inp = g.get_tensor_by_name(INPUT_TENSOR)
    out = g.get_tensor_by_name(OUTPUT_TENSOR[0] + ':0')

    sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \
        tf.saved_model.signature_def_utils.predict_signature_def(
            {"input": inp}, {"outout": out})

    builder.add_meta_graph_and_variables(sess,
                                         [tag_constants.SERVING],
                                         signature_def_map=sigs)
    try:
        builder.save()
        print(f'Model ready for deployment at {OUTPUT_SERVABLE_FOLDER}/saved_model.pb')
        print('Prediction signature : ')
        print(sigs['serving_default'])
    except:
        print('Error Occured, please checked frozen graph')
&lt;/denchmark-code&gt;

when i freeze the keras graph and then export it, it works flawlessly. but if i do it according to the keras blog, it fails to run. i use the above script to get it to work
		</comment>
		<comment id='8' author='srihari-humbarwadi' date='2018-11-16T19:14:00Z'>
		Your current method is correct. I will close this issue since you have found the work around. Feel free to open the issue if you have follow up questions. Thanks!
		</comment>
	</comments>
</bug>