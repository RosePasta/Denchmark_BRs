<bug id='1680' author='mNemlaghi' open_date='2020-06-30T08:36:46Z' closed_time='2020-08-21T01:08:40Z'>
	<summary>Cannot serve custom layer based model</summary>
	<description>
Please go to Stack Overflow for help and support:
&lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow-serving&gt;https://stackoverflow.com/questions/tagged/tensorflow-serving&lt;/denchmark-link&gt;

If you open a GitHub issue, here is our policy:

It must be a bug, a feature request, or a significant problem with
documentation (for small docs fixes please send a PR instead).
The form below must be filled out.

Here's why we have that policy: TensorFlow developers respond to issues.
We want to focus on work that benefits the whole community, e.g., fixing bugs
and adding features. Support only helps individuals. GitHub also notifies
thousands of people when issues are filed. We want them to see you communicating
an interesting problem, rather than being redirected to Stack Overflow.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Feature Request&lt;/denchmark-h&gt;

If this is a feature request, please fill out the following form in full:
&lt;denchmark-h:h3&gt;Describe the problem the feature is intended to solve&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10
TensorFlow Serving installed from (source or binary): docker pull tensorflow/serving
TensorFlow Serving version: 2.2.0
Git version : v2.2.0-rc4-8-g2b96f3662b
Tensorflow version: 2.2.0

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

For an NLP project, in order to directly preprocess data in a model , I have implemented a custom tokenization layer with a StaticHashTable (see below). Afterwards, I prepend this layer to a base model. From adding layers to saving model in tensorflow format, everything seems working fine, I even can make computations with the saved model in Python. However, when I try to push the model to tensorflow serving, the inference server crashes, whereas the base model is perfectly servable.
&lt;denchmark-h:h3&gt;Exact Steps to Reproduce&lt;/denchmark-h&gt;

Here is how my custom layer is implemented :
import pickle
import tensorflow as tf

tokenizer = pickle.load(tokenizer_path) ##pickled tf.keras tokenizer

class OwnPadder(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(OwnPadder, self).__init__(name = "padder", trainable = False, **kwargs)
        self.tokenizer_keys = tf.constant(list(tokenizer.word_index.keys()), dtype = tf.string)
        self.tokenizer_values = tf.constant(list(tokenizer.word_index.values()), dtype = tf.int32)
        self.initializer = tf.lookup.KeyValueTensorInitializer(keys = self.tokenizer_keys,
                 values= self.tokenizer_values, key_dtype = tf.string, value_dtype = tf.int32)
        self.table = tf.lookup.StaticHashTable(self.initializer, default_value=tf.constant(-1), name = "static_hash")

    def call(self, inputs, **kwargs):
        pipe = tf.strings.split(inputs)
        pipe = pipe.to_tensor(shape = pipe.bounding_shape(), name = "uniform_tensor")[:,0,:]
        tokenized = self.table.lookup(pipe, name = "tokenize")
        masked = tf.ragged.boolean_mask(tokenized, tokenized&gt;0)
        res = masked.to_tensor(shape = [None, 100], name ="oad")
        return res

    def get_config(self):
        config = super(OwnPadder, self).get_config()
        return config
Now, the preprending phase :
base_model = tf.keras.models.load_model(base_model_path)
base_model.trainable = False
input_text = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name = "input")
 x = OwnPadder()(input_text)
 for layers in base_model.layers:
     x = layers(x)

holistic_model = tf.keras.models.Model(inputs = input_text, outputs = x)
holistic_model.compile()
final_model_path
holistic_model.save(final_model_path, save_format = "tf")
So, no bug here, everything seems fine until I try to serve with TensorflowServing, where the serving made a segmentation fault...
I tried to make a diagnostic with saved_model_cli --dir $final_model_path --all shell command with the newly saved model, which gave me expected inputs/outputs signatures, but ended with a weird TF Error.
WARNING:tensorflow:From $conda_envlib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2020-06-29 06:24:26.528835: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-06-29 06:24:26.528907: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)
2020-06-29 06:24:26.528947: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nlp-compute-2.sesamm.com): /proc/driver/nvidia/version does not exist
2020-06-29 06:24:26.529160: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-29 06:24:26.566924: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2397160000 Hz
2020-06-29 06:24:26.574709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f85f0000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-29 06:24:26.574758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Exception ignored in: &lt;function CapturableResourceDeleter.__del__ at 0x7f86ba4de8c8&gt;
Traceback (most recent call last):
  File "$conda_env/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py", line 191, in __del__
    self._destroy_resource()
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 580, in __call__
    result = self._call(*args, **kwds)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 506, in _initialize
    *args, **kwds))
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py", line 241, in restored_function_body
    return _call_concrete_function(function, inputs)
  File "$conda_envlib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py", line 72, in _call_concrete_function
    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py", line 101, in _call_flat
    cancellation_manager)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py", line 1760, in _call_flat
    flat_outputs = forward_function.call(ctx, args_with_tangents)
  File "$conda_envlib/python3.7/site-packages/tensorflow/python/eager/function.py", line 627, in call
    executor_type=executor_type)
  File "$conda_envlib/python3.7/site-packages/tensorflow/python/ops/functional_ops.py", line 1165, in partitioned_call
    f.add_to_graph(graph)
  File "$conda_envlib/python3.7/site-packages/tensorflow/python/eager/function.py", line 543, in add_to_graph
    g._add_function(self)
  File "$conda_env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py", line 3187, in _add_function
    gradient)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null
Please notice that the base model (without preprocessing layer) doesn't give this issue, and can be perfectly served via TensorflowServing. So I assume this is a matter of serialization of the new layer. Moreover, I tried to fit the new model from scratch (by fitting with raw data, compiling and saving preprocessed), and it also gave me an error.
Do you have any hint how to fix this ? What am I doing wrong here ? Many thanks in advance !
	</description>
	<comments>
		<comment id='1' author='mNemlaghi' date='2020-07-01T11:32:25Z'>
		&lt;denchmark-link:https://github.com/mNemlaghi&gt;@mNemlaghi&lt;/denchmark-link&gt;
,
Can you please share the files,  and  (if they are not confidential) so that we can reproduce your error. Thanks!
		</comment>
		<comment id='2' author='mNemlaghi' date='2020-07-02T13:16:49Z'>
		&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 thanks for the answer, model file is quite heavy, but please find tokenizer pickle attached (zipped) &lt;denchmark-link:https://github.com/tensorflow/serving/files/4864444/tokenizer.zip&gt;here&lt;/denchmark-link&gt;

About the base model, here is a summary of its layers as well as the output of , hope it helps :

Model summary :

In [3]: model.summary()                                                                                                                                                                                     
Model: "language_conv_net"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        multiple                  100803300 
_________________________________________________________________
conv1d (Conv1D)              multiple                  96064     
_________________________________________________________________
global_max_pooling1d (Global multiple                  0         
_________________________________________________________________
dropout (Dropout)            multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  195       
=================================================================
Total params: 100,899,559
Trainable params: 100,899,559
Non-trainable params: 0
_________________________________________________________________
And now saved_model_cli output of the base model
$ saved_model_cli show --dir  $model_path  --all 

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['input_1'] tensor_info:
        dtype: DT_INT32
        shape: (-1, 100)
        name: serving_default_input_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_1'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: StatefulPartitionedCall:0
  Method name is: tensorflow/serving/predict
WARNING:tensorflow:$conda_env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.

Defined Functions:
  Function Name: '__call__'
    Option #1
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_1')
        Argument #2
          DType: bool
          Value: False
    Option #2
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 100), dtype=tf.int32, name='inputs')
        Argument #2
          DType: bool
          Value: False
    Option #3
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_1')
        Argument #2
          DType: bool
          Value: True
    Option #4
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 100), dtype=tf.int32, name='inputs')
        Argument #2
          DType: bool
          Value: True

  Function Name: '_default_save_signature'
    Option #1
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_1')

  Function Name: 'call_and_return_all_conditional_losses'
    Option #1
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 100), dtype=tf.int32, name='inputs')
        Argument #2
          DType: bool
          Value: True
    Option #2
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 100), dtype=tf.int32, name='inputs')
        Argument #2
          DType: bool
          Value: False
    Option #3
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_1')
        Argument #2
          DType: bool
          Value: False
    Option #4
      Callable with:
        Argument #1
          input_1: TensorSpec(shape=(None, 100), dtype=tf.int32, name='input_1')
        Argument #2
          DType: bool
          Value: True

Thanks in advance !
		</comment>
		<comment id='3' author='mNemlaghi' date='2020-07-22T10:09:56Z'>
		Hi, do you have any update on this issue ? Thanks in advance !
		</comment>
		<comment id='4' author='mNemlaghi' date='2020-07-31T14:45:33Z'>
		&lt;denchmark-link:https://github.com/mNemlaghi&gt;@mNemlaghi&lt;/denchmark-link&gt;
 Please share the entire code for us to reproduce the issue. Thanks!
		</comment>
		<comment id='5' author='mNemlaghi' date='2020-08-12T04:38:31Z'>
		&lt;denchmark-link:https://github.com/mNemlaghi&gt;@mNemlaghi&lt;/denchmark-link&gt;
 Please respond to the above comment so that we can take the discussion further.
		</comment>
		<comment id='6' author='mNemlaghi' date='2020-08-21T01:08:40Z'>
		Closing this issue as it has been inactive for a long time. Please create a new issue or add additional comments to open this issue to continue the discussion. Thanks!
		</comment>
	</comments>
</bug>