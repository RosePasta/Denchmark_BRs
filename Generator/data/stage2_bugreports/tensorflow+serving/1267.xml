<bug id='1267' author='lebron374' open_date='2019-02-25T01:45:44Z' closed_time='2019-03-15T20:28:48Z'>
	<summary>tensorFlow serving batch configure ineffectiveness</summary>
	<description>
&lt;denchmark-h:h2&gt;docker run command&lt;/denchmark-h&gt;

docker run -t --rm -p 8500:8500 -p 8501:8501 
-v /home/zhi.wang/tensorflow-serving/model:/models 
-e MODEL_NAME=beidian_cart_ctr_wdl_model tensorflow/serving:1.12.0 
--enable_batching=true --batching_parameters_file=/models/batching_parameters.txt &amp;
&lt;denchmark-h:h2&gt;batching_parameters.txt&lt;/denchmark-h&gt;

num_batch_threads { value: 40 }
batch_timeout_micros { value: 5000}
max_batch_size {value: 20000000}
&lt;denchmark-h:h2&gt;server configuration&lt;/denchmark-h&gt;

40 cpu and 64G memory
&lt;denchmark-h:h2&gt;test result&lt;/denchmark-h&gt;

1 thread predict cost 30ms
40 thread predict one predict cost 300ms
&lt;denchmark-h:h2&gt;cpu usage&lt;/denchmark-h&gt;

cpu usage in docker can only up to 300% and host cpu usage is low
&lt;denchmark-h:h2&gt;java test script&lt;/denchmark-h&gt;

Features features = Features.newBuilder().putAllFeature(featureMap).build();
Example example = Example.newBuilder().setFeatures(features).build();
&lt;denchmark-code&gt;    Predict.PredictRequest.Builder predictRequestBuilder = Predict.PredictRequest.newBuilder();
    Model.ModelSpec.Builder modelSpecBuilder = Model.ModelSpec.newBuilder();
    modelSpecBuilder.setName("beidian_cart_ctr_wdl_model");
    modelSpecBuilder.setSignatureName("predict");

    predictRequestBuilder.setModelSpec(modelSpecBuilder);

    TensorShapeProto.Dim dim = TensorShapeProto.Dim.newBuilder().setSize(200).build();
    TensorShapeProto shapeProto = TensorShapeProto.newBuilder().addDim(dim).build();

    TensorProto.Builder tensor = TensorProto.newBuilder();
    tensor.setTensorShape(shapeProto);
    tensor.setDtype(DataType.DT_STRING);
    for (int i=0; i&lt;200; i++) {
        tensor.addStringVal(example.toByteString());
    }
    predictRequestBuilder.putInputs("examples", tensor.build());

    ManagedChannel channel = ManagedChannelBuilder.forAddress("10.2.176.43", 8500).usePlaintext().build();
    PredictionServiceGrpc.PredictionServiceBlockingStub predictionServiceBlockingStub = PredictionServiceGrpc.newBlockingStub(channel);

    Predict.PredictResponse predictResponse = null;
    while (true) {
        long start = System.currentTimeMillis();
        predictResponse = predictionServiceBlockingStub.predict(predictRequestBuilder.build());
        long end = System.currentTimeMillis();
        System.out.println(end - start);

        List&lt;Float&gt; floatList = predictResponse.getOutputsOrThrow("probabilities").getFloatValList();

        Map&lt;String, TensorProto&gt; outputsMap = predictResponse.getOutputsMap();
    }
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lebron374' date='2019-02-27T22:12:42Z'>
		Could you please provide system information as asked below
System information
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
TensorFlow Serving installed from (source or binary):
TensorFlow Serving version
		</comment>
		<comment id='2' author='lebron374' date='2019-02-27T22:18:13Z'>
		Please look at this existing issue &lt;denchmark-link:https://github.com/tensorflow/serving/issues/89&gt;#89&lt;/denchmark-link&gt;
 related to some performance and optimization tips.
		</comment>
		<comment id='3' author='lebron374' date='2019-03-12T18:35:25Z'>
		your code seems to be sending one request at-a-time (via blocking stub). if that is true,
what sort of batching are you expecting on the server?  the batching feature allows you
to batch multiple (concurrent) requests coming into the server.
		</comment>
		<comment id='4' author='lebron374' date='2019-03-15T20:28:48Z'>
		Closing as it is in "awaiting response" state for more than 3 days. Feel free to add comments and we will reopen.
		</comment>
	</comments>
</bug>