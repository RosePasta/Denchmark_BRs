<bug id='1287' author='karthikvadla' open_date='2019-03-19T03:16:59Z' closed_time='2019-05-31T20:46:21Z'>
	<summary>TFServing using only one CPU core on MKL docker image</summary>
	<description>
Hi Team,
Recently we noticed that TFServing is using only single CPU core while testing object detection model, even though we set TENSORFLOW_SESSION_PARALLELISM=14. OMP_NUM_THREADS=56 is related to intel mkl-dnn.
&lt;denchmark-code&gt;docker run \
 -p 8501:8501 \
 --name=tfserving_rfcn \
 -v "$(pwd)/rfcn:/models/rfcn" \
 -e MODEL_NAME=rfcn \
 -e OMP_NUM_THREADS=56 \
 -e TENSORFLOW_INTRA_OP_PARALLELISM=14 \
 -e TENSORFLOW_INTER_OP_PARALLELISM=14 \
 tensorflow/serving:mkl
&lt;/denchmark-code&gt;

 looks as below.
&lt;denchmark-link:https://user-images.githubusercontent.com/16564109/54577791-617f7b00-49ba-11e9-88e9-1284207f28d3.png&gt;&lt;/denchmark-link&gt;

Is this something to do with how model script is written ? or any other configuration to do at TFServing end.?
	</description>
	<comments>
		<comment id='1' author='karthikvadla' date='2019-03-19T23:11:38Z'>
		few questions:

do you get similar behavior with non-mkl, official docker images?
where is this env TENSORFLOW_SESSION_PARALLELISM used?

if you do not set any --xxx_parallelism flags on the model server, i'd expect the system to use all schedulable CPUs. assuming the docker container has access to all of these. is the htop command from within the docker container (i'd guess so, but wanted to confirm).
		</comment>
		<comment id='2' author='karthikvadla' date='2019-03-20T02:50:05Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;



do you get similar behavior with non-mkl, official docker images?
I tested commenting envs (which makes it similar to offical non-mkl dockerfile)  , re-build image and use. Assuming TFServing automatically schedules on available CPU's. But still i see same.


where is this env TENSORFLOW_SESSION_PARALLELISM used?
I Updated above description. We got this splitting of session_parallelism merged recently #1253.
We exposed envs explicitly because when using  intel optimized tensorflow. Tuning inter and intra gives optimal performance. Logic goes as explained in above mentioned PR.


I ran htop outside of container. But after you comment, i ran it inside container just to make sure i'm not missing anything. Still htop looks same inside or outside container.
&lt;denchmark-link:https://user-images.githubusercontent.com/16564109/54655707-2c3d6080-4a80-11e9-9c6c-4cf93244d883.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='karthikvadla' date='2019-03-20T19:02:52Z'>
		(trying to understand the problem) are you sending multiple concurrent requests to the model server or just one (large) request, and expect the model server to use all the cores to process one request?
		</comment>
		<comment id='4' author='karthikvadla' date='2019-03-20T19:11:39Z'>
		fwiw i tried running resnet on latest docker serving image, sending N concurrent requests and saw all the cores being used. this is without setting any additional environment variables.
for quick test, just follow this &lt;denchmark-link:https://medium.com/tensorflow/serving-ml-quickly-with-tensorflow-serving-and-docker-7df7094aa008&gt;blog post&lt;/denchmark-link&gt;
. may want to tweak  to some large value in the python client script. and observe docker usage (i did )
		</comment>
		<comment id='5' author='karthikvadla' date='2019-03-20T23:21:02Z'>
		okay let me test and update you.
		</comment>
		<comment id='6' author='karthikvadla' date='2019-03-20T23:30:01Z'>
		
(trying to understand the problem) are you sending multiple concurrent requests to the model server or just one (large) request, and expect the model server to use all the cores to process one request?

I'm sending multiple concurrent requests to the model server. Expectation was when i assign cores to TENSORFLOW_INTRA_OP_PARALLELISM, TENSORFLOW_INTER_OP_PARALLELISM OR just TENSORFLOW_SESSION_PARALLELISM, its not using as assigned. All the time only one core is utilized  100%.
		</comment>
		<comment id='7' author='karthikvadla' date='2019-03-21T23:22:01Z'>
		
okay let me test and update you.

any update on your testing with resnet model (from the blog post) i mentioned?
		</comment>
		<comment id='8' author='karthikvadla' date='2019-03-22T10:11:10Z'>
		yeah i tried from blog, It's using all cores when nothing is set.
&lt;denchmark-code&gt;for i in {1..10};do python /tmp/karthik/git-repo/tfserving/resnet/resnet_client.py; done
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16564109/54815793-f949c700-4c4f-11e9-9118-dc30e54c6d23.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='karthikvadla' date='2019-03-22T11:28:07Z'>
		I cross checked, testing  with intel-optimized tfserving image following this &lt;denchmark-link:https://github.com/IntelAI/models/blob/master/docs/image_recognition/tensorflow_serving/Tutorial.md&gt;tutorial&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;docker run \
        --name=tfserving_no_parall \
        --rm \
        -d \
        -p 8500:8500 \
        -v "/tmp/karthik/git-repo/tfserving/mkl_resnet/:/models/resnet50" \
        -e MODEL_NAME=resnet50 \
        -e OMP_NUM_THREADS=56 \
        -e TENSORFLOW_INTRA_OP_PARALLELISM=14 \
        -e TENSORFLOW_INTER_OP_PARALLELISM=14 \
        tensorflow/serving:parallel
&lt;/denchmark-code&gt;

I see it's using more than one core.
&lt;denchmark-link:https://user-images.githubusercontent.com/16564109/54819924-aaedf580-4c5a-11e9-8c92-1189b8b7d17b.png&gt;&lt;/denchmark-link&gt;

Is there anything to do with way model implemented? or type of client we use GRPC vs REST?
Because it's using only ONE core for RFCN(using grpc client) ü§∑‚Äç‚ôÇÔ∏è
		</comment>
		<comment id='10' author='karthikvadla' date='2019-03-22T13:44:29Z'>
		I do not think rest vs grpc matters. They both funnel into the same run session path.
You can try the grpc client (&lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/resnet_client_grpc.py&gt;https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/resnet_client_grpc.py&lt;/denchmark-link&gt;
) with resnet model to be doubly sure.
		</comment>
		<comment id='11' author='karthikvadla' date='2019-03-22T20:34:59Z'>
		yes the model can be a problem. but i'd expect it to impact single evaluation of a request. IOW its possible the model causes 1 core to be used when running a single request (as opposed to using all the available cores for running one request).
the fact that sending lot of multiple concurrent requests also causes just 1 core to be used is a bit puzzling.
		</comment>
		<comment id='12' author='karthikvadla' date='2019-03-25T17:48:21Z'>
		given that we are not able to reproduce this with standard (resnet) model and test models (in TF serving gitrepo), can you share your model+sample-requests?
		</comment>
		<comment id='13' author='karthikvadla' date='2019-03-25T18:04:59Z'>
		&lt;denchmark-link:https://github.com/mhbuehler&gt;@mhbuehler&lt;/denchmark-link&gt;
 did some experiments pinning  while building docker images. She saw while using ,  is using specified cores. Melanie, can you please share your findings.
		</comment>
		<comment id='14' author='karthikvadla' date='2019-03-25T22:10:58Z'>
		hmm. so things are working and all cores are being used for R-FCN? i am mildly confused :)
		</comment>
		<comment id='15' author='karthikvadla' date='2019-03-25T22:30:04Z'>
		As &lt;denchmark-link:https://github.com/karthikvadla&gt;@karthikvadla&lt;/denchmark-link&gt;
 said, I'm seeing proper usage of multiple cores when I build TFServing with MKL from tag 1.13.0, but not when I build from the head of the master branch. For now, this is a fine workaround, but there might be a regression in master. To replicate the problem:
Build TFServing with MKL with these commands (head of master):
&lt;denchmark-code&gt;$ docker build -f Dockerfile.devel-mkl -t tensorflow/serving:latest-devel-mkl .
$ docker build -f Dockerfile.mkl -t tensorflow/serving:mkl_master .
&lt;/denchmark-code&gt;

And build it again with these commands (release 1.13.0):
&lt;denchmark-code&gt;$ docker build -f Dockerfile.devel-mkl --build-arg TF_SERVING_VERSION_GIT_BRANCH="1.13.0" -t tensorflow/serving:latest-devel-mkl .
$ docker build -f Dockerfile.mkl --build-arg TF_SERVING_VERSION_GIT_BRANCH="1.13.0" -t tensorflow/serving:mkl_1_13_0 .
&lt;/denchmark-code&gt;

Now, if you run inference on a model using both the mkl_master and mkl_1_13_0 images, you should see the difference in performance. Let me know if you need any more info and I will try to provide you with a model and more instructions.
		</comment>
		<comment id='16' author='karthikvadla' date='2019-03-25T22:58:46Z'>
		do you see similar issue if you just use just TF?
you can experiment by building+installing TF+MKL (steps &lt;denchmark-link:https://www.tensorflow.org/guide/performance/overview#tensorflow_with_intel%C2%AE_mkl_dnn&gt;here&lt;/denchmark-link&gt;
), and then doing inference using a session with your model loaded (python code))?
i am trying to see if this is a build issue (with TF serving), or something is actually broken in TF.
		</comment>
		<comment id='17' author='karthikvadla' date='2019-03-26T00:06:05Z'>
		I saw a similar issue about TF-MKL using only one core when using Anaconda image: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/24172&gt;tensorflow/tensorflow#24172&lt;/denchmark-link&gt;

But it's about OMP/KMP environment variable settings affecting other operations that also use OpenMP, so it's likely unrelated to this issue.
		</comment>
		<comment id='18' author='karthikvadla' date='2019-03-26T01:01:25Z'>
		&lt;denchmark-link:https://github.com/penpornk&gt;@penpornk&lt;/denchmark-link&gt;
 just to confirm i re-check envs
&lt;denchmark-code&gt;$ docker exec -it 48bdbfa71255 env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=48bdbfa71255
TERM=xterm
TENSORFLOW_INTRA_OP_PARALLELISM=14
TENSORFLOW_INTER_OP_PARALLELISM=14
MODEL_NAME=resnet50
OMP_NUM_THREADS=56
LIBRARY_PATH=/usr/local/lib:$LIBRARY_PATH
LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
MODEL_BASE_PATH=/models
KMP_BLOCKTIME=1
KMP_SETTINGS=1
KMP_AFFINITY=granularity=fine,verbose,compact,1,0
MKLDNN_VERBOSE=0
HOME=/root
&lt;/denchmark-code&gt;

as from reference you mentioned OMP_PROC_BIND is not set here.
		</comment>
		<comment id='19' author='karthikvadla' date='2019-03-26T01:15:37Z'>
		
do you see similar issue if you just use just TF?
you can experiment by building+installing TF+MKL (steps here), and then doing inference using a session with your model loaded (python code))?
i am trying to see if this is a build issue (with TF serving), or something is actually broken in TF.

We will follow this &lt;denchmark-link:https://github.com/IntelAI/models/blob/master/benchmarks/object_detection/tensorflow/rfcn/README.md#fp32-inference-instructions&gt;guide&lt;/denchmark-link&gt;
 and at step 6  i&lt;denchmark-link:https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/tags&gt;ntelaipg/intel-optimized-tensorflow:latest-devel-mkl&lt;/denchmark-link&gt;
 to test inference. Checking with the team whether we have any tag based of tip of master.
		</comment>
		<comment id='20' author='karthikvadla' date='2019-03-26T01:57:51Z'>
		
@penpornk, @ezhulenev
Penporn, Eugene - do you know what can be happening here?

		</comment>
		<comment id='21' author='karthikvadla' date='2019-03-26T02:16:06Z'>
		
@netfs

do you get similar behavior with non-mkl, official docker images?
I tested commenting envs (which makes it similar to offical non-mkl dockerfile)  , re-build image and use. Assuming TFServing automatically schedules on available CPU's. But still i see same.


&lt;denchmark-link:https://github.com/karthikvadla&gt;@karthikvadla&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/mhbuehler&gt;@mhbuehler&lt;/denchmark-link&gt;
 TF-MKL is a different build from normal TF. So commenting the environment variables to TF-Serving will still run TF-MKL underneath (just with different threading configurations).
Could you please try running with TF-Serving + normal TF build and see if you still get the issue? It will help us narrow down the scope a bit. Thank you!
		</comment>
		<comment id='22' author='karthikvadla' date='2019-03-26T02:21:36Z'>
		
yeah i tried from blog, It's using all cores when nothing is set.
for i in {1..10};do python /tmp/karthik/git-repo/tfserving/resnet/resnet_client.py; done



&lt;denchmark-link:https://github.com/penpornk&gt;@penpornk&lt;/denchmark-link&gt;
 here i tested using official TFServing +TF docker image. I tested for  . Do you want me to test anything different than this?
		</comment>
		<comment id='23' author='karthikvadla' date='2019-03-26T02:23:49Z'>
		&lt;denchmark-link:https://github.com/karthikvadla&gt;@karthikvadla&lt;/denchmark-link&gt;
 can you test R-FCN with stock TF serving? that would help.
		</comment>
		<comment id='24' author='karthikvadla' date='2019-03-26T02:26:50Z'>
		
@karthikvadla can you test R-FCN with stock TF serving? that would help.

Sure i can do that.  stock TF serving? means pulling from docker hub?
Also do you have any location for R-FCN pre-trained saved model .pb which i can download and use. Right now, i have intel optimized R-FCN which uses MKL-DNN
		</comment>
		<comment id='25' author='karthikvadla' date='2019-03-26T02:37:14Z'>
		

@karthikvadla can you test R-FCN with stock TF serving? that would help.

Sure i can do that.  stock TF serving? means pulling from docker hub?

yes. if you want to test TF+TFS at master, pull tensorflow/serving:nightly

Also do you have any location for R-FCN pre-trained saved model .pb which i can download and use. Right now, i have intel optimized R-FCN which uses MKL-DNN

nope. sorry.
		</comment>
		<comment id='26' author='karthikvadla' date='2019-03-26T02:43:24Z'>
		&lt;denchmark-code&gt;docker run -p 8501:8501 --name tfserving_resnet \
--mount type=bind,source=/tmp/karthik/git-repo/tfserving/resnet,target=/models/resnet \
-e MODEL_NAME=resnet -t tensorflow/serving:nightly &amp;
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt; for i in {1..10};do python /tmp/karthik/git-repo/tfserving/resnet/resnet_client.py; done
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16564109/54967926-1e7f5380-4f36-11e9-903f-3e5366090abe.png&gt;&lt;/denchmark-link&gt;

 looks good with  too.
Will check with our data scientist whether i can find   model.
		</comment>
		<comment id='27' author='karthikvadla' date='2019-03-26T02:49:00Z'>
		Thank you! So the normal TF at head works fine.
The only big change related to MKL between r1.13 and head that I'm aware of is enabling MKL contraction kernels, but it is only for normal TF build (doesn't affect TF-MKL), and it has been disabled for a few weeks now.
If the culprit is on the TF-MKL side, one way to find it is to do a bisection on the commits. There are about 6,500 commits between TF-MKL1.13 and now so it will require testing ~13 times. You can find the commit hash by changing the commit offset (6500 in this case) of this URL
&lt;denchmark-link:https://github.com/tensorflow/tensorflow/commits/master?after=cdd0dcc6f78d2443d7713094c80a4a169531b6c1+6500&gt;https://github.com/tensorflow/tensorflow/commits/master?after=cdd0dcc6f78d2443d7713094c80a4a169531b6c1+6500&lt;/denchmark-link&gt;

The commit above is the first commit on Dec 12. I think r1.13 contains changes up to Dec 13 (not counting fixes) so this range should cover enough.
I'm working on other issues and can't try this right now. Would appreciate if you can help do the bisection (resnet50 is fine for me since it reproduces the problem). Otherwise, I'll get back to doing this later. Thank you!
		</comment>
		<comment id='28' author='karthikvadla' date='2019-04-03T20:59:42Z'>
		&lt;denchmark-link:https://github.com/karthikvadla&gt;@karthikvadla&lt;/denchmark-link&gt;
  Any update ?
		</comment>
		<comment id='29' author='karthikvadla' date='2019-04-03T21:05:47Z'>
		@hgadig yeah pinning to TF Version 1.13.0 did fix issue. master is unstable, still we don't know what causing issue.
Please see documentation where we are passing 
&lt;denchmark-link:https://github.com/IntelAI/models/blob/master/docs/general/tensorflow_serving/InstallationGuide.md#step-1-build-tensorflow-serving-docker-image&gt;https://github.com/IntelAI/models/blob/master/docs/general/tensorflow_serving/InstallationGuide.md#step-1-build-tensorflow-serving-docker-image&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;docker build \
     -f Dockerfile.devel-mkl \
     --build-arg TF_SERVING_VERSION_GIT_BRANCH="1.13.0" \
     -t tensorflow/serving:latest-devel-mkl .
&lt;/denchmark-code&gt;

		</comment>
		<comment id='30' author='karthikvadla' date='2019-04-30T04:26:16Z'>
		TFS+TF official nightly builds/docker images uses all cores, and these builds use mkl contraction kernels (by default). So MKL runs in this (so called) hybrid mode.
Regarding Docker.devel-mkl not working as expected, I will leave it for the community to contribute and get this working, mostly due to my lack of understanding. Will unassign myself.
		</comment>
		<comment id='31' author='karthikvadla' date='2019-05-10T06:51:13Z'>
		hiÔºåI am using tf serving docker,   I limit the cpu usage of docker with 16cores, but I found that when I send large number requests to TFS, the docker could not use up the cpu (almost only 200%cpu), how could I use up the cpu (nearly 1600%)? Thanks.
		</comment>
		<comment id='32' author='karthikvadla' date='2019-05-23T21:21:54Z'>
		i am going to mark this issue for "contributions", as this issue is specific to the &lt;denchmark-link:tensorflow_serving/tools/docker/Dockerfile.devel-mkl&gt;MKL docker image&lt;/denchmark-link&gt;
. people closer to this will be able to help resolve the performance issue seen here.
as I mentioned &lt;denchmark-link:https://github.com/tensorflow/serving/issues/1287#issuecomment-487817949&gt;above&lt;/denchmark-link&gt;
 official TFServing docker images work well with MKL (hybrid) mode.
&lt;denchmark-link:https://github.com/ginger0106&gt;@ginger0106&lt;/denchmark-link&gt;
 if you are seeing performance issues with official TF serving docker images, can you please open a new issue, with details (sample model, requests etc.) -- thanks!
		</comment>
		<comment id='33' author='karthikvadla' date='2019-05-28T18:15:34Z'>
		I just tested this issue against  of tensorflow serving. Seems like issue resolved. It' using the number of cores specified. To reproduce please visit intel &lt;denchmark-link:https://github.com/IntelAI/models/blob/master/docs/object_detection/tensorflow_serving/Tutorial.md&gt;model-zoo&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;docker run \
     --name=tfserving_rfcn \
     -p 8501:8501 \
     -v "/tmp/karthik/git-repo/tfserving/rfcn:/models/rfcn" \
     -e MODEL_NAME=rfcn \
     -e OMP_NUM_THREADS=56 \
     -e TENSORFLOW_INTER_OP_PARALLELISM=2 \
     -e TENSORFLOW_INTRA_OP_PARALLELISM=56 \
     tensorflow/serving:latest &amp;
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/16564109/58501672-cf25f100-8139-11e9-8169-8ab214cc669e.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
 do you think we should close this?
		</comment>
		<comment id='34' author='karthikvadla' date='2019-05-28T18:22:28Z'>
		yes we should close it. thanks for taking the time to verify it works with master.
for posterity can you point to the RFCN model that you used (assuming its publicly available), so folks looking at this bug know which model was being tested (as we were never able to reproduce this with stock resnet50 model).
		</comment>
		<comment id='35' author='karthikvadla' date='2019-05-28T18:24:30Z'>
		yeah sure, I told &lt;denchmark-link:https://github.com/mhbuehler&gt;@mhbuehler&lt;/denchmark-link&gt;
 to test on a different machine just to re-confirm. I will close once she updates me.
		</comment>
		<comment id='36' author='karthikvadla' date='2019-05-28T21:37:18Z'>
		Great.
+&lt;denchmark-link:https://github.com/penpornk&gt;@penpornk&lt;/denchmark-link&gt;

		</comment>
		<comment id='37' author='karthikvadla' date='2019-05-31T20:16:02Z'>
		Okay, I've re-tested and confirmed that the issue is resolved. I built the MKL docker image from master and did test runs for:

ResNet50, batch sizes 1 and 128
InceptionV3, batch sizes 1 and 128
R-FCN, batch sizes 1 and 128
SSD-MobileNet, batch sizes 1 and 128
Transformer-LT, batch sizes 1 and 64

All tests showed multiple cores being used and good performance. For posterity, the R-FCN model is &lt;denchmark-link:https://storage.googleapis.com/intel-optimized-tensorflow/models/rfcn_resnet101_fp32_coco_pretrained_model.tar.gz&gt;here&lt;/denchmark-link&gt;
 and instructions/scripts for running it in TFServing+MKL are &lt;denchmark-link:https://github.com/IntelAI/models/blob/master/docs/object_detection/tensorflow_serving/Tutorial.md&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='38' author='karthikvadla' date='2019-05-31T20:46:21Z'>
		Thanks &lt;denchmark-link:https://github.com/mhbuehler&gt;@mhbuehler&lt;/denchmark-link&gt;
 .I'm closing this issue now.
		</comment>
	</comments>
</bug>