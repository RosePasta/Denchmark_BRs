<bug id='1291' author='joekohlsdorf' open_date='2019-03-26T15:44:58Z' closed_time='2019-04-03T15:06:56Z'>
	<summary>Serving models from S3 causes high costs due to excessive ListBucket operations</summary>
	<description>
&lt;denchmark-h:h2&gt;Bug Report&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


TensorFlow Serving installed from (source or binary): official Docker image tensorflow/serving
TensorFlow Serving version: 1.12.0

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

We have deployed a couple of models where both the model config file and the model are stored in S3. In our case, we are seeing a usage of about 150MB/h for the ListBucket operation on the bucket in which the models and configuration are stored.
This adds up to a cost of about 1.000 USD per month.
&lt;denchmark-h:h3&gt;Exact Steps to Reproduce&lt;/denchmark-h&gt;


deploy model and configuration to S3
run tensorflow-serving like this: tensorflow_model_server --model_config_file=s3://modelbucket/model.yaml
after a couple of hours, download AWS usage report from here: https://console.aws.amazon.com/billing/home#/reports/usage
Useful Filters:

Services: Amazon Simple Storage Service
Operation: ListBucket
Report Granularity: Hours



&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I expect that this operation, if at all necessary, is done only once: At startup.
If this is done to verify the existence of a file, the same can be achieved which a much cheaper HEAD request. Please also see how a similar issue was fixed in boto: &lt;denchmark-link:https://github.com/boto/boto/issues/2078&gt;boto/boto#2078&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='joekohlsdorf' date='2019-03-26T17:01:44Z'>
		i suspect this is due to filesystem polling to detect+load new versions of servables as they become. you can disable or reduce the frequency by setting --file_system_poll_wait_seconds flag on the modelserver. the default is 1 second (afair).
happy to review patches to improve/optimize this path.
		</comment>
		<comment id='2' author='joekohlsdorf' date='2019-03-26T19:04:04Z'>
		I'll set it to -1 and will report back tomorrow if we see any change in the amount of these API calls.
Like I already mentioned, ListBucket should be a last resort action. Checking for file modification or existence can be achieved with HEAD requests and is &gt;10x cheaper. I sadly can't suggest a code modification as this portion of the code comes from tensorflow itself so I don't have a good overview what other code-paths we would break modifying it for the use-case of tensorflow-serving.
		</comment>
		<comment id='3' author='joekohlsdorf' date='2019-03-27T18:08:37Z'>
		After passing --file_system_poll_wait_seconds=2147483647 (MAXINT32, when setting it to -1 the server doesn't start up completely) the amount of ListBucket calls went down.
I'd consider this a workaround as the default setting leads to an unexpected behavior.
		</comment>
		<comment id='4' author='joekohlsdorf' date='2019-03-29T18:38:51Z'>
		&lt;denchmark-link:https://github.com/joekohlsdorf&gt;@joekohlsdorf&lt;/denchmark-link&gt;
  Thanks for the workaround.
Closing this issue for now and will reopen if there are any contributions posted for the same.
		</comment>
		<comment id='5' author='joekohlsdorf' date='2019-03-29T18:42:31Z'>
		Thanks &lt;denchmark-link:https://github.com/joekohlsdorf&gt;@joekohlsdorf&lt;/denchmark-link&gt;
. Can you please open a feature request OR rename (as a FR)+open this issue so we can keep track (and hopefully someone can help make relevant changes).
Thanks!
		</comment>
		<comment id='6' author='joekohlsdorf' date='2019-03-29T18:43:45Z'>
		Reopening in order to track this issue.
		</comment>
		<comment id='7' author='joekohlsdorf' date='2019-03-30T01:53:51Z'>
		I don't agree that this is a feature request, the wrong method for checking if a file has changed is used and it is causing 12.5x the expected cost. In my case over USD 1.000 per month!
This is standard behaviour and the result is completely unexpected.
		</comment>
		<comment id='8' author='joekohlsdorf' date='2019-04-03T15:06:55Z'>
		Closing this issue as per the discussion in &lt;denchmark-link:https://github.com/tensorflow/serving/issues/1295&gt;#1295&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='joekohlsdorf' date='2019-04-03T23:38:06Z'>
		Why was this issue closed? These are two separate problems.
This issue is about Tensorflow Serving using the wrong S3 API call to check if a file has changed.
The other issue about not being able to disable automatic model reloading.
		</comment>
		<comment id='10' author='joekohlsdorf' date='2019-04-03T23:44:40Z'>
		@unclepeddy  Please correct me if I misunderstood your comments in &lt;denchmark-link:https://github.com/tensorflow/serving/issues/1295&gt;#1295&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='joekohlsdorf' date='2019-04-04T01:13:31Z'>
		@hgadig no worries, my mistake for asking you to close this without an explanation .
&lt;denchmark-link:https://github.com/joekohlsdorf&gt;@joekohlsdorf&lt;/denchmark-link&gt;
 sorry about the expense you had to bear but I don't think there is any way to avoid this - please feel free to correct my understanding as I don't have much experience with S3 features.
First, and just to be clear, we poll the model directory provided to ModelServer in order to discover new model versions (not to look for "file modification or existence").
Second, you mention using  instead of  - this is in fact the case for when we only care about existence (starting &lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc#L221&gt;here&lt;/denchmark-link&gt;
 calling into &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L451&gt;core TF s3 implementation&lt;/denchmark-link&gt;
 to ensure the parent model directory exists before proceeding to check its children).
However, we then need to list the child directories of model_dir to find all the model versions that are to be aspired &lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc#L230&gt;here&lt;/denchmark-link&gt;
 which calls into &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L416&gt;core TF s3 implementation&lt;/denchmark-link&gt;
 which calls ListObjects. I'm assuming that's where the ListBucket operations are happening (not sure why the API name and what's reported on your usage report don't match).  My point is, I don't think there's a way to avoid listing the child directories as that is how we find the versions to be aspired. This is well documented &lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/architecture.md#servable-versions&gt;here&lt;/denchmark-link&gt;

Third, TFS cannot be using "the wrong S3 API" since it doesn't differentiate between different FS environments - it relies on TF core/platform/env library for this. Therefore given that we have to list directory children and we don't own the S3 client implementation, I unfortunately cannot think of a fix for this costly behavior, other than changing the polling frequency.
Let me know if you disagree!
		</comment>
		<comment id='12' author='joekohlsdorf' date='2020-06-10T00:16:17Z'>
		&lt;denchmark-link:https://github.com/joekohlsdorf&gt;@joekohlsdorf&lt;/denchmark-link&gt;
 came across this post as I was weighing a few different deployment strategies myself.  Was wondering if you still had issues with this.  And also, if a hybrid s3/non-s3 solution would work.
The hybrid solution I was thinking about would include hosting the saved_models in s3, but have the model.config file hosted on the server itself.  So you could have the server look for updates every second (or whatever you want), but would then only make s3 requests when it needs the model.
I haven't implemented this, just a thought.  So if there are blockers/disadvantages to this feel free to list those.
		</comment>
	</comments>
</bug>