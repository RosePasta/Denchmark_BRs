<bug id='927' author='gr8Adakron' open_date='2018-06-09T07:40:14Z' closed_time='2018-12-17T18:13:26Z'>
	<summary>AbortionError(code=StatusCode.INVALID_ARGUMENT, details="Expects arg[0] to be float but string is provided")</summary>
	<description>
Hey, I have a model which is been converted from MXnet to TF-Serving-format using &lt;denchmark-link:https://github.com/Microsoft/MMdnn&gt;MMdnn&lt;/denchmark-link&gt;
 . In the beginning, there were few errors but then it got converted somehow.
After converting I deployed the model in Tensorflow Serving and it is working.
&lt;denchmark-code&gt;tensorflow_model_server --port=9000 --model_name=inception --model_base_path=/home/afzal/serving/inception_model/ 2018-06-08 20:33:05.607602: I tensorflow_serving/model_servers/main.cc:154] Building single TensorFlow model file config:  model_name: inception model_base_path: /home/afzal/serving/inception_model/
2018-06-08 20:33:05.607718: I tensorflow_serving/model_servers/server_core.cc:444] Adding/updating models.
2018-06-08 20:33:05.607732: I tensorflow_serving/model_servers/server_core.cc:499]  (Re-)adding model: inception
2018-06-08 20:33:05.708874: I tensorflow_serving/core/basic_manager.cc:716] Successfully reserved resources to load servable {name: inception version: 9}
2018-06-08 20:33:05.708939: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: inception version: 9}
2018-06-08 20:33:05.708974: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: inception version: 9}
2018-06-08 20:33:05.709098: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /home/afzal/serving/inception_model/9
2018-06-08 20:33:05.709175: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/afzal/serving/inception_model/9
2018-06-08 20:33:05.876583: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-06-08 20:33:06.053354: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.
2018-06-08 20:33:06.282709: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.
2018-06-08 20:33:06.297435: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: success. Took 588263 microseconds.
2018-06-08 20:33:06.297540: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: inception version: 9}
2018-06-08 20:33:06.298970: I tensorflow_serving/model_servers/main.cc:316] Running ModelServer at 0.0.0.0:9000 ...

&lt;/denchmark-code&gt;

And this is my client program I am using to generate prediction:
&lt;denchmark-code&gt;from __future__ import print_function
from grpc.beta import implementations
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2

import os
import tensorflow as tf
import time

def main(_):
  host, port  = "localhost:9000".split(':')
  channel     = implementations.insecure_channel(host, int(port))
  stub        = prediction_service_pb2.beta_create_PredictionService_stub(channel)
  # Send request
  img_list    = os.listdir("image/")
  total_image = len(img_list)
  start_time  = time.time()
  for image_name in img_list:
    with open(f"image/{image_name}", 'rb') as f:
      print(f"================= image/{image_name} ====================")
      # See prediction_service.proto for gRPC request/response details.
      data = f.read()
      request = predict_pb2.PredictRequest()
      request.model_spec.name = 'inception'

      ''' For original Inception uncomment below three line '''
      # request.model_spec.signature_name = 'predict_images' 
      # request.inputs['images'].CopyFrom(
      #     tf.contrib.util.make_tensor_proto(data, shape=[1]))
      
      ''' For original Inception comment below three line '''
      print(data)
      request.model_spec.signature_name = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
      request.inputs['input'].CopyFrom(
          tf.contrib.util.make_tensor_proto(data, shape=[1]))
      # request.inputs['image'].CopyFrom(
      #     tf.contrib.util.make_tensor_proto(data[0], shape=[1,1,20]))  

      result = stub.Predict(request, 10.0)  # 10 secs timeout
      print(result.outputs['scores'].float_val[0])
      print(result.outputs['classes'].string_val[0])
  
  timetaken = str(format(float((time.time() - start_time)),'.3f'))
  print(f"{timetaken} secs &gt; {total_image}")

if __name__ == '__main__':
  tf.app.run()
&lt;/denchmark-code&gt;

This program runs fine on pre-build inception model in TF-serving format but, strangely it is giving some error when I am calling my converted model.
&lt;denchmark-link:https://user-images.githubusercontent.com/16715364/41189110-23a6c548-6be6-11e8-95f3-abf614b9ef5c.png&gt;&lt;/denchmark-link&gt;

 This is my converted &lt;denchmark-link:https://drive.google.com/open?id=1BhT0Y-uUwzLJNe45FFx3Avfy3WW4oKQf&gt;model file&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='gr8Adakron' date='2018-06-28T09:21:57Z'>
		&lt;denchmark-link:https://github.com/ewilderj&gt;@ewilderj&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/kchodorow&gt;@kchodorow&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/montanaflynn&gt;@montanaflynn&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/skye&gt;@skye&lt;/denchmark-link&gt;
 Anybody can help me with this?
		</comment>
		<comment id='2' author='gr8Adakron' date='2018-06-28T14:28:30Z'>
		Did you try asking for help over on MMdnn? As you point out, a regular TensorFlow model is working fine, so my suspicion is more on the converted model then that this is a TF Serving bug.
		</comment>
		<comment id='3' author='gr8Adakron' date='2018-06-29T12:49:19Z'>
		&lt;denchmark-link:https://github.com/ewilderj&gt;@ewilderj&lt;/denchmark-link&gt;
  Yep! I tried to find the solution with help of this issue : &lt;denchmark-link:https://github.com/microsoft/MMdnn/issues/221&gt;microsoft/MMdnn#221&lt;/denchmark-link&gt;

But they are not replying, they are simply pointing it towards the README file and theres nothing in it.
It would be good if I can get the client_script, I am trying since last 20days but couldn't find any.
		</comment>
		<comment id='4' author='gr8Adakron' date='2018-07-11T17:26:13Z'>
		&lt;denchmark-link:https://github.com/gr8Adakron&gt;@gr8Adakron&lt;/denchmark-link&gt;
 I also encountered the same error/specification, and might resolve it. Perhaps, you have to set inputs dtype of signature to string to do classify/regress, like below:

https://github.com/tensorflow/serving/blob/1.9.0/tensorflow_serving/example/mnist_saved_model.py#L62-L65
https://github.com/tensorflow/serving/blob/1.9.0/tensorflow_serving/example/mnist_saved_model.py#L101-L102

Please check your model's signature using  command. ref: &lt;denchmark-link:https://github.com/tensorflow/serving/issues/497&gt;#497&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='gr8Adakron' date='2018-08-16T13:56:03Z'>
		&lt;denchmark-link:https://github.com/ynqa&gt;@ynqa&lt;/denchmark-link&gt;
 Does this mean, for an image input, I need to creat a placeholder of type  and export this new placeholder into my ?
		</comment>
		<comment id='6' author='gr8Adakron' date='2018-10-23T18:59:40Z'>
		Is this still an issue ?
		</comment>
		<comment id='7' author='gr8Adakron' date='2018-11-19T04:33:14Z'>
		&lt;denchmark-link:https://github.com/ruanchong&gt;@ruanchong&lt;/denchmark-link&gt;
 Sorry for my replying delay. Yes, you have to set  format on calling  Service: &lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/input.proto&gt;https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/input.proto&lt;/denchmark-link&gt;

Or if you use  Service, you could define  as input formats: &lt;denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.proto&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.proto&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='gr8Adakron' date='2018-12-17T18:13:26Z'>
		Closing as this is resolved.
		</comment>
		<comment id='9' author='gr8Adakron' date='2019-01-15T04:33:49Z'>
		@hgadig
i am getting the same error, how did you resolved?
		</comment>
		<comment id='10' author='gr8Adakron' date='2019-02-27T08:43:47Z'>
		
@hgadig
i am getting the same error, how did you resolved?

i got this error when i use request.inputs["input_sentence"].ParseFromString(), then i use request.inputs["input_sentence"].CopyForm()  instead. it work.
		</comment>
	</comments>
</bug>