<bug id='874' author='YuchenJin' open_date='2018-05-08T21:24:20Z' closed_time='2019-03-13T17:37:44Z'>
	<summary>Runtime variability for batching</summary>
	<description>
I want to find out the execution time vs. batch size when serving an inception-v3 model using TF-serving. I measured the duration of executing a batch by adding timers to the  funtion in &lt;denchmark-link:https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/batching_session.cc#L611&gt;/batching/batching_session.cc&lt;/denchmark-link&gt;
.
My code is below:
auto t1 = std::chrono::high_resolution_clock::now();
status = wrapped_-&gt;Run(run_options, merged_inputs, output_tensor_names,
                         {} /* target node names */, &amp;combined_outputs,
                         &amp;run_metadata);
auto t2 = std::chrono::high_resolution_clock::now();
auto time_diff = std::chrono::duration_cast&lt;std::chrono::microseconds&gt;(t2-t1).count();
LOG(INFO) &lt;&lt; (batch-&gt;num_tasks()) &lt;&lt; ": " &lt;&lt; (time_diff);  // This will log the batch size and the execution time of running this batch.
My batching_parameters_file is configured as:
&lt;denchmark-code&gt;max_batch_size { value: 128 }
batch_timeout_micros { value: 0 } 
&lt;/denchmark-code&gt;

Setting:

Model served: Inception-v3.
GPU: TITAN X (Pascal)
Client request rate: 100req/s. A client at the same machine sends requests (images) to the server at uniform rate of 100 requestes per second.

I did an experiment with 9000 requests sent in total. The execution latency vs. batch size is plotted below.
&lt;denchmark-link:https://user-images.githubusercontent.com/15164320/39783360-a4f75486-52c9-11e8-8ebe-11009c334245.png&gt;&lt;/denchmark-link&gt;

We can observe the high variance of execution time of the same batch size, and some very small batches have larger execution time than batches with large batch size like 128. (I also tried tuning the batching parameters including num_batch_threads and max_enqueued_batches but this problem persists.) Do you guys know why it happens? Any idea is appreciated!
	</description>
	<comments>
		<comment id='1' author='YuchenJin' date='2018-05-16T17:21:29Z'>
		Bump. Here is a similar figure for ssd_mobilenet_v1_coco  on an unloaded Titan X at 100 req/s over 9000 req.
&lt;denchmark-link:https://user-images.githubusercontent.com/755774/40132841-b5728856-58f2-11e8-9648-e24abf9873aa.png&gt;&lt;/denchmark-link&gt;

Any other TF Serving users seeing this? Is this some known problem?
		</comment>
		<comment id='2' author='YuchenJin' date='2018-10-23T19:14:09Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
 - Hi, could you please look into this ?
		</comment>
		<comment id='3' author='YuchenJin' date='2019-02-27T21:58:58Z'>
		Please go through the following resources which may help you get a clear picture on batching and tuning the performance for server side batching.
&lt;denchmark-link:https://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching&gt;Batching guide for Tensorflow Serving&lt;/denchmark-link&gt;

&lt;denchmark-link:https://stackoverflow.com/questions/42519010/how-to-do-batching-in-tensorflow-serving&gt;Batching in Tensorflow Serving (for Inception V3)&lt;/denchmark-link&gt;

&lt;denchmark-link:https://mux.com/blog/tuning-performance-of-tensorflow-serving-pipeline/&gt;Tuning the Performance for server side batching&lt;/denchmark-link&gt;

Also, for any support related questions(other than bug/performance/feature request/docs) request you to post in the StackOverflow.
		</comment>
		<comment id='4' author='YuchenJin' date='2019-02-27T22:16:44Z'>
		Please look at this existing issue &lt;denchmark-link:https://github.com/tensorflow/serving/issues/89&gt;#89&lt;/denchmark-link&gt;
 related to some performance and optimization tips.
		</comment>
		<comment id='5' author='YuchenJin' date='2019-03-13T17:37:44Z'>
		Closing this issue as it is in "awaiting response" status for more than a week. Feel free to add your comments and we will reopen.
		</comment>
	</comments>
</bug>