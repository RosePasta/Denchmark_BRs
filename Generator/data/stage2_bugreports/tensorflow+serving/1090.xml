<bug id='1090' author='lxl910915' open_date='2018-09-14T09:10:41Z' closed_time='2019-06-13T19:23:29Z'>
	<summary>load large model (about 20G) from hdfs</summary>
	<description>
If model size is large, such as 20G, tf serving failed with log : "W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: hdfs://**"
Version: tf 1.8.0 &amp; tf serving 1.8.0
	</description>
	<comments>
		<comment id='1' author='lxl910915' date='2018-09-14T09:13:58Z'>
		What's more, can I debug tf serving using gdb? How to do this? If I add '-c gdb' when buiding tf serving with bazel, an error occure.
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/Scrt1.o:(.eh_frame+0x20): relocation truncated to fit: R_X86_64_PC32 against .text' /usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/crti.o: In function _init':
(.init+0x7): relocation truncated to fit: R_X86_64_REX_GOTPCRELX against undefined symbol __gmon_start__' /usr/lib/gcc/x86_64-linux-gnu/5/crtbeginS.o: In function deregister_tm_clones':
crtstuff.c:(.text+0x3): relocation truncated to fit: R_X86_64_PC32 against .tm_clone_table' crtstuff.c:(.text+0xa): relocation truncated to fit: R_X86_64_PC32 against symbol TMC_END' defined in .nvFatBinSegment section in bazel-out/k8-dbg/bin/tensorflow_serving/model_servers/tensorflow_model_server
crtstuff.c:(.text+0x1e): relocation truncated to fit: R_X86_64_REX_GOTPCRELX against undefined symbol _ITM_deregisterTMCloneTable' /usr/lib/gcc/x86_64-linux-gnu/5/crtbeginS.o: In function register_tm_clones':
crtstuff.c:(.text+0x43): relocation truncated to fit: R_X86_64_PC32 against .tm_clone_table' crtstuff.c:(.text+0x4a): relocation truncated to fit: R_X86_64_PC32 against symbol TMC_END' defined in .nvFatBinSegment section in bazel-out/k8-dbg/bin/tensorflow_serving/model_servers/tensorflow_model_server
crtstuff.c:(.text+0x6b): relocation truncated to fit: R_X86_64_REX_GOTPCRELX against undefined symbol _ITM_registerTMCloneTable' /usr/lib/gcc/x86_64-linux-gnu/5/crtbeginS.o: In function __do_global_dtors_aux':
crtstuff.c:(.text+0x92): relocation truncated to fit: R_X86_64_PC32 against .bss' crtstuff.c:(.text+0x9c): relocation truncated to fit: R_X86_64_GOTPCREL against symbol __cxa_finalize@@GLIBC_2.2.5' defined in .text section in /lib/x86_64-linux-gnu/libc.so.6
crtstuff.c:(.text+0xaa): additional relocation overflows omitted from the output
bazel-out/k8-dbg/bin/tensorflow_serving/model_servers/tensorflow_model_server: PC-relative offset overflow in GOT PLT entry for `cublasDsyr2k_v2@@libcublas.so.9.0'
collect2: error: ld returned 1 exit status
Target //tensorflow_serving/model_servers:tensorflow_model_server failed to build
INFO: Elapsed time: 4091.290s, Critical Path: 3273.58s
INFO: 3612 processes: 3612 local.
		</comment>
		<comment id='2' author='lxl910915' date='2018-09-18T22:46:25Z'>
		does this model load successful from local disk?
		</comment>
		<comment id='3' author='lxl910915' date='2018-09-20T08:40:06Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
 If we get the model from HDFS and load it from local disk, it works well.
		</comment>
		<comment id='4' author='lxl910915' date='2018-09-21T21:51:17Z'>
		so the large model itself is not a problem.
its the attempted read from hdfs (by TF) that is failing. we do link hfds &lt;denchmark-link:https://github.com/tensorflow/serving/blob/bc7d560e4fd170f195dfb6e9351f6dda14727d79/tensorflow_serving/model_servers/BUILD#L344&gt;here&lt;/denchmark-link&gt;
 in the modelserver. the "invalid argument" error is rather surprising.
is there anything else in the log/stderr that might help root cause this?
is this is problem only with large files/models or does the failure happen with any model hosted on hdfs?
finally can you please try this with latest or nightly modelserver binary or docker image (1.8 is kinda old :-) so we know things are not working on master.
thanks!
		</comment>
		<comment id='5' author='lxl910915' date='2018-09-26T20:16:47Z'>
		question not directly related to this issue: i to load models to tf serving make sure they are stored in memory. i only saw that you can load models from local disk or hdfs. are there other ways to directly talk to the api and send the model data? also, what is the total size of models tf serving is able to handle in memory?
		</comment>
		<comment id='6' author='lxl910915' date='2018-10-01T19:41:11Z'>
		models are loaded via the file interface -- so yes they need to be on some sort of filesystem (either local or network, either memory or disk backed). the size is limited by how much one can hold in RAM and the amount ModelServer has access to.
		</comment>
		<comment id='7' author='lxl910915' date='2018-10-23T18:00:06Z'>
		&lt;denchmark-link:https://github.com/lxl910915&gt;@lxl910915&lt;/denchmark-link&gt;
 - Hi, is this still an issue ?
		</comment>
		<comment id='8' author='lxl910915' date='2018-10-24T03:01:56Z'>
		&lt;denchmark-link:https://github.com/Harshini-Gadige&gt;@Harshini-Gadige&lt;/denchmark-link&gt;
  yes, it's still there! Our model is 100G, and memory is 256G, and network is 10GB
		</comment>
		<comment id='9' author='lxl910915' date='2018-11-07T13:13:15Z'>
		We build tf-serving using 'bazel build -c opt -c dbg --strip=never --color=yes --curses=yes --config=cuda --verbose_failures 
--output_filter=DONT_MATCH_ANYTHING 
--copt=-mavx --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 
tensorflow_serving/model_servers:tensorflow_model_server'.
The error is "Linking of rule '//tensorflow_serving/model_servers:tensorflow_model_server' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command ".
Can you tell the right way? Thank your very much. &lt;denchmark-link:https://github.com/Harshini-Gadige&gt;@Harshini-Gadige&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='lxl910915' date='2018-12-03T22:32:36Z'>
		&lt;denchmark-link:https://github.com/netfs&gt;@netfs&lt;/denchmark-link&gt;
  Any inputs please ?
		</comment>
		<comment id='11' author='lxl910915' date='2019-03-13T23:46:03Z'>
		
We build tf-serving using 'bazel build -c opt -c dbg --strip=never --color=yes --curses=yes --config=cuda --verbose_failures
--output_filter=DONT_MATCH_ANYTHING
--copt=-mavx --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0
tensorflow_serving/model_servers:tensorflow_model_server'.
The error is "Linking of rule '//tensorflow_serving/model_servers:tensorflow_model_server' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command ".
Can you tell the right way? Thank your very much. @Harshini-Gadige

can you use one of the official xxx-devel-gpu docker image to do the builds? it has necessary tool setup for doing such builds. you can also use tools/run_in_docker.sh wrapper to do the compile.
		</comment>
		<comment id='12' author='lxl910915' date='2019-04-03T21:08:27Z'>
		&lt;denchmark-link:https://github.com/lxl910915&gt;@lxl910915&lt;/denchmark-link&gt;
  Any update on this? Have you tried using official xxx-devel-gpu docker image ?
		</comment>
		<comment id='13' author='lxl910915' date='2019-06-13T19:23:29Z'>
		Closing due to lack of activity.
		</comment>
	</comments>
</bug>