<bug id='1086' author='agupta74' open_date='2018-09-13T21:53:49Z' closed_time='2018-12-17T18:10:32Z'>
	<summary>Model loaded successfully even if variable files are absent</summary>
	<description>
&lt;denchmark-h:h3&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
TensorFlow Serving installed from (source or binary): Sources
TensorFlow Serving version: 1.5.0

&lt;denchmark-h:h3&gt;Describe the problem&lt;/denchmark-h&gt;

Got the following message when TF serving model is getting loaded: external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:165] The specified SavedModel has no variables; no checkpoints were restored.
The issue seems to be that variables folder got created while the variable files were being copied/unzipped to this folder and it takes some time to copy/unzip these variable files as the variable file size is in GBs. As per the TF serving source code, it assumes that variable files not being present is OK but it should be returning an error since a model would typically have trained variables to be loaded.
&lt;denchmark-h:h3&gt;Exact Steps to Reproduce&lt;/denchmark-h&gt;

Export any model to protobuf format and copy the model while delaying the the copy (or not copying at all) of variable files
&lt;denchmark-h:h3&gt;Source code / logs&lt;/denchmark-h&gt;

Here are the logs:
2018-09-11 15:06:02.360258: I tensorflow_serving/core/basic_manager.cc:705] Successfully reserved resources to load servable {name: my_model version: 12}
2018-09-11 15:06:02.360271: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: my_model version: 12}
2018-09-11 15:06:02.360280: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: my_model version: 12}
2018-09-11 15:06:02.360302: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /my_path/my_model/12
2018-09-11 15:06:02.360314: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:236] Loading SavedModel from: /my_path/my_model/12
2018-09-11 15:06:03.172504: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:155] Restoring SavedModel bundle.
2018-09-11 15:06:03.172561: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:165] The specified SavedModel has no variables; no checkpoints were restored.
2018-09-11 15:06:03.172579: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running LegacyInitOp on SavedModel bundle.
2018-09-11 15:06:03.293689: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:284] Loading SavedModel: success. Took 932896 microseconds.
2018-09-11 15:06:03.296419: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: my_model version: 12}
Here is the source code snippet which is printing this message in loader.cc file:
.....
LOG(INFO) &lt;&lt; "Restoring SavedModel bundle.";
// Find path to variables to be restored in export directory.
const string variables_directory =
io::JoinPath(export_dir, kSavedModelVariablesDirectory);
// Check for saver checkpoints in v2 format. Models exported in the checkpoint
// v2 format will have a variables.index file. The corresponding
// variables are stored in the variables.data-?????-of-????? files.
const string variables_index_path = io::JoinPath(
variables_directory, MetaFilename(kSavedModelVariablesFilename));
if (!Env::Default()-&gt;FileExists(variables_index_path).ok()) {
LOG(INFO) &lt;&lt; "The specified SavedModel has no variables; no checkpoints "
"were restored.";
return Status::OK();
}
.........
	</description>
	<comments>
		<comment id='1' author='agupta74' date='2018-09-14T18:03:15Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 can you help?
it seems we should not (but presently do) load a model that does not have associated variables alongside.
		</comment>
		<comment id='2' author='agupta74' date='2018-09-14T18:10:23Z'>
		Variables are not strictly necessary-- one can imagine you have a bit of graph to serialize with ins and outs but no variables. (Consider, for example, TF Hub, and the use-cases there.) It doesn't make sense to restrict this on the saved model building or loading side explicitly.
Presumably the model throws an error with no vars at some point? What does that error look like?
		</comment>
		<comment id='3' author='agupta74' date='2018-09-14T21:11:37Z'>
		The error is thrown during the predict call with error message:
Error calling PredictServing: rpc error: code = FailedPrecondition desc = Attempting to use uninitialized value Model/xxxxx.
		</comment>
		<comment id='4' author='agupta74' date='2018-09-17T17:41:55Z'>
		It's hard to tell a priori that Variables will be missing. The best practice here is likely to send a few sample queries/warm-up the model; is that feasible in this case?
		</comment>
		<comment id='5' author='agupta74' date='2018-09-19T16:26:13Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 Yes I can send few sample queries to warm up the model but that does not help since the model is loaded successfully even though variables are absent. Is there an API to reload the model without having to relaunch TF serving process ? If so, I could reload the model if there is a predict error for the sample queries.
		</comment>
		<comment id='6' author='agupta74' date='2018-09-19T17:14:29Z'>
		Yes-- please see the &lt;denchmark-link:https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel&gt;saved_model_cli&lt;/denchmark-link&gt;
 for a command line interface, or &lt;denchmark-link:https://www.tensorflow.org/versions/r1.11/api_docs/python/tf/contrib/estimator/SavedModelEstimator&gt;SavedModelEstimator&lt;/denchmark-link&gt;
 for a python API.
		</comment>
		<comment id='7' author='agupta74' date='2018-09-19T17:42:20Z'>
		&lt;denchmark-link:https://github.com/karmel&gt;@karmel&lt;/denchmark-link&gt;
 -- &lt;denchmark-link:https://github.com/agupta74&gt;@agupta74&lt;/denchmark-link&gt;
 is asking about hot-reloading of the model. neither the cli nor the estimator would allow for that. please use &lt;denchmark-link:https://github.com/tensorflow/serving/blob/bec76d47a337ee6865ce0b4fcc9e97d5ab67b38f/tensorflow_serving/apis/model_service.proto#L22&gt;HandleReloadConfigRequest&lt;/denchmark-link&gt;
 API instead.
		</comment>
		<comment id='8' author='agupta74' date='2018-11-23T08:20:46Z'>
		I just ran into this issue as well. The copying function copied the saved_model first and then variables after. This caused serving to load the model without any variables - even after copying the variables finished.
Easy enough to fix in my case - just copy the saved_model last - but it is still a easy way to shoot yourself in the foot.
		</comment>
		<comment id='9' author='agupta74' date='2018-12-03T22:37:00Z'>
		&lt;denchmark-link:https://github.com/agupta74&gt;@agupta74&lt;/denchmark-link&gt;
  Did you get a chance to try on HandleReloadConfigRequest API as asked above ? If so, please post your observations.
		</comment>
		<comment id='10' author='agupta74' date='2018-12-17T18:10:32Z'>
		Closing as this is inactive and in "awaiting response" status for more than a week. Feel free to post your comments(if any) and we will reopen.
		</comment>
		<comment id='11' author='agupta74' date='2019-04-17T00:01:48Z'>
		Just replying here to say I ran into this today also. Using gsutil to upload a model directory, TF serving picked up the new version immediately after saved_model.pb was uploaded, before the variables were uploaded. It successfully loads the model, but fails evaluation.
I'm not particularly concerned about it, since there are workarounds, but the experience on this is a bit suboptimal.
So far as I can tell, your options for ensuring models are successfully loaded are:

Manually setting versions instead of using latest version policies
Force reloading config after the upload is complete (not 100% sure this will work)
Writing the model atomically (not possible on GCS/S3)
Writing the model in a specific order such that loading will fail until all files are uploaded (sucks)

Ideally, there's be a way to get TF serving to reject the model until the necessary files are in place.
		</comment>
		<comment id='12' author='agupta74' date='2020-11-17T21:24:17Z'>
		&lt;denchmark-link:https://github.com/colinmorelli&gt;@colinmorelli&lt;/denchmark-link&gt;
 Just read your comment above. I'm having a hard time to find the source of my error (see this &lt;denchmark-link:https://stackoverflow.com/questions/64879548/tensorflow-server-returns-an-error-when-using-model-on-s3&gt;post&lt;/denchmark-link&gt;
 on SO). My problem is: when model is embedded in my tensorflow-serving container, it's working. When it's located on S3, it does not ("Attempting to use uninitialized value ...). Could it be related to the upload misbehavior that you're mentioning?
		</comment>
	</comments>
</bug>