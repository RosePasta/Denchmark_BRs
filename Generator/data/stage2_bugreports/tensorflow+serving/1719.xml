<bug id='1719' author='spate141' open_date='2020-08-14T13:40:38Z' closed_time='2020-08-20T18:37:38Z'>
	<summary>Error with exporting TF2.2.0 model with tf.lookup.StaticHashTable &amp; LSTM layer for Serving</summary>
	<description>
&lt;denchmark-h:h1&gt;System information&lt;/denchmark-h&gt;


OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18
TensorFlow Serving installed from (source or binary): binary
TensorFlow Serving version: 2.2.0

&lt;denchmark-h:h2&gt;Related Issue with TF-Serving documentation:&lt;/denchmark-h&gt;

As mentioned here on &lt;denchmark-link:https://github.com/tensorflow/serving/issues/1606&gt;#1606&lt;/denchmark-link&gt;
; we need a better documentation about exporting TF2.X models to TF-Serving that involves StaticHashTables. Simply disabling eager execution works on most cases but if you're using the new LSTM layer from TF2.2.0; it won't give you the power of CUDA as mentioned here as the key requirement of LSTM cuDNN implementation &lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM&gt;(7. Eager execution is enabled in the outermost context.)&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Related post on tensorflow/tensorflow:&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/tensorflow/tensorflow/issues/42325&gt;tensorflow/tensorflow#42325&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;My Issue:&lt;/denchmark-h&gt;

I'm using StaticHashTable as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the model.output and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem.
Issue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with TF2.X + tf.compat.v1.disable_eager_execution()
tf.compat.v1.disable_eager_execution()
version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'
builder = saved_model_builder.SavedModelBuilder(export_path)

model_signature = tf.compat.v1.saved_model.predict_signature_def(
    inputs={
        'input': model.input
    }, 
    outputs={
        'output': model.output
    }
)

with tf.compat.v1.keras.backend.get_session() as sess:
    builder.add_meta_graph_and_variables(
        sess=sess,
        tags=[tf.compat.v1.saved_model.tag_constants.SERVING],
        signature_def_map={
            'predict': model_signature
        },
        # For initializing Hashtables
        main_op=tf.compat.v1.tables_initializer()
    )
    builder.save()
This will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables.
Here is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving error as below:
class MyModule(tf.Module):

    def __init__(self, model):
        super(MyModule, self).__init__()
        self.model = model
    
    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 16), dtype=tf.int32, name='input')])
    def predict(self, input):
        result = self.model(input)
        return {"output": result}

version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'

module = MyModule(model)
tf.saved_model.save(module, export_path, signatures={"predict": module.predict.get_concrete_function()})
Error:
AssertionError: Tried to export a function which references untracked object Tensor("2907:0", shape=(), dtype=resource).
TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
Any suggestion or am I missing anything on exporting TF2.2 model which is using the StaticHashTables in final Lambda layer for TensorFlow Serving?
Thanks!
	</description>
	<comments>
		<comment id='1' author='spate141' date='2020-08-19T11:54:18Z'>
		&lt;denchmark-link:https://github.com/spate141&gt;@spate141&lt;/denchmark-link&gt;
,
Can you please provide the complete reproducible code so that we can check it at our end? Thanks!
		</comment>
		<comment id='2' author='spate141' date='2020-08-19T14:09:54Z'>
		&lt;denchmark-link:https://github.com/rmothukuru&gt;@rmothukuru&lt;/denchmark-link&gt;
 Please find the code from this colab notebook: &lt;denchmark-link:https://colab.research.google.com/drive/1ch89Veylgg-0FzqGeC4QKDui-a01FKzp?usp=sharing&gt;https://colab.research.google.com/drive/1ch89Veylgg-0FzqGeC4QKDui-a01FKzp?usp=sharing&lt;/denchmark-link&gt;

I've added following sections:

Train a sample model
Load the sample model &amp; add a sample Lambda layer with StaticHashTable that simply demonstrate {0: 1000, 1: 2000} label conversion.
Try to export above model with TensorFlow 2.2.0 for serving (ERROR HERE)

		</comment>
		<comment id='3' author='spate141' date='2020-08-20T18:37:38Z'>
		Seems like I solved it! I would appreciate if you can add a version of this to the documentation so other can use it. To make the variables and the other elements from outside trackable, we need to write the Lambda layer using the subclassed Layer: tf.keras.layers.Layer
class LabelConverter(tf.keras.layers.Layer):
    
    def __init__(self, **kwargs):
        super(LabelConverter, self).__init__(**kwargs)

        # Implement your StaticHashTable here
        keys = tf.constant([0, 1], dtype=tf.int32)
        values = tf.constant([1000, 2000], dtype=tf.float32)
        table_init = tf.lookup.KeyValueTensorInitializer(keys, values)
        self.table = tf.lookup.StaticHashTable(table_init, -1)
    
    def build(self, input_shape):
        self.built = True
    
    def call(self, tensor_input):
        # this block is doing the transformation on input
        label_tensor = tf.cast(tensor_input[:, 0], tf.int32)
        score_tensor = tensor_input[:, 1]
        categories_tensor = self.table.lookup(label_tensor)
        return tf.stack((categories_tensor, score_tensor), axis=1)

Exporting TF2 model with StaticHashTable for Serving:

# adding on top of already trained keras model
extra_layer = LabelConverter()(model.output)
hash_table_model = tf.keras.models.Model(inputs=model.input, outputs=extra_layer)

version = 1
name = 'tmp_test_serving'
export_path = f'/data/{name}/{version}'
tf.saved_model.save(hash_table_model, export_path)
		</comment>
	</comments>
</bug>