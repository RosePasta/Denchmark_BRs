<bug id='1039' author='rlrs' open_date='2020-02-04T11:39:01Z' closed_time='2020-04-07T15:58:08Z'>
	<summary>Can't get Batch Independent GP to fit a single point</summary>
	<description>
I'm trying to obtain a simple multi-output GP in GPytorch, but I'm having some trouble. I followed the code in the docs for batch independent GPs, but when I test it on even a single training point it does not fit it correctly. Am I doing something wrong? Below is a small example:
&lt;denchmark-code&gt;import torch
import gpytorch

N = 1  # number of training points
D = 1  # input dimensions
K = 2  # target dimensions

class ExactGP(gpytorch.models.ExactGP):
    def __init__(self, X_train, Y_train, likelihood):
        super().__init__(X_train, Y_train, likelihood)
        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([K]))
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([K])), batch_shape=torch.Size([K]))

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultitaskMultivariateNormal.from_batch_mvn(gpytorch.distributions.MultivariateNormal(mean_x, covar_x))

X_train = torch.randn(N, D)
Y_train = torch.randn(N, K)
print("Y_train[0]:", Y_train[0])

likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=K)
likelihood.noise = 1e-4
model = ExactGP(X_train, Y_train, likelihood)
model.eval()
likelihood.eval()

mu = X_train[0:1, :]
posterior = model(mu)
print("Posterior:", posterior.mean, posterior.covariance_matrix)
&lt;/denchmark-code&gt;

Example output:

Y_train[0]: tensor([-0.5498, -0.7388])
Posterior: tensor([[-0.2749, -0.3693]], grad_fn=) tensor([[0.3466, 0.0000],
[0.0000, 0.3466]], grad_fn=)

In this case the mean is off by a factor of 2, but this seems to depend on N? I hope I'm making some obvious mistake.
	</description>
	<comments>
		<comment id='1' author='rlrs' date='2020-02-04T12:30:27Z'>
		In your example code you don't train your model. You just need to train the model most likely. The default initialization for the GP hypers has a constant mean of 0 and a relatively high likelihood noise of softplus(0)=0.7. I see you initialize the noise to be small, but the lengthscale is also quite large at init.
Those parameter values taken together will always bias your prediction away from the true label towards 0 (the mean). Try setting the lengthscale to be relatively small as well.
(Edited since I see you initialize the noise manually -- it's probably the lengthscale. But really, try training your model)
		</comment>
		<comment id='2' author='rlrs' date='2020-02-04T13:10:33Z'>
		Does training the model do anything except optimizing the hyperparameters? My point is that the posterior mean should, to my understanding, be roughly the same value as the single training target when the likelihood noise is small. Settings the lengthscale to be small doesn't help. In fact, when I optimize the hyperparameters, only the kernel scale seems to be changed so that the posterior mean (very roughly) matches the target:
&lt;denchmark-code&gt;import torch
import gpytorch

N = 1  # number of training points
D = 1  # input dimensions
K = 2  # target dimensions

class ExactGP(gpytorch.models.ExactGP):
    def __init__(self, X_train, Y_train, likelihood):
        super().__init__(X_train, Y_train, likelihood)
        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([K]))
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([K])), batch_shape=torch.Size([K]))

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultitaskMultivariateNormal.from_batch_mvn(gpytorch.distributions.MultivariateNormal(mean_x, covar_x))

X_train = torch.randn(N, D)
Y_train = torch.randn(N, K)
print("Y_train[0]:", Y_train[0])

likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=K)
model = ExactGP(X_train, Y_train, likelihood)

# set hyperparameters
model.likelihood.noise = 1e-4
model.covar_module.base_kernel.lengthscale = 0.01
model.covar_module.outputscale = 1.0

# train
model.train()
likelihood.train()
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
optimizer = torch.optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe')
def closure():
    optimizer.zero_grad()
    output = model(X_train)
    loss = -mll(output, Y_train)
    loss.backward()
    return loss
optimizer.step(closure)
print("Lengthscale:", model.covar_module.base_kernel.lengthscale)
print("Noise:", model.likelihood.noise)
print("Variance:", model.covar_module.outputscale)
model.eval()
likelihood.eval()

mu = X_train[0:1, :]
posterior = model(mu)
print("Posterior:", posterior.mean, posterior.covariance_matrix)
&lt;/denchmark-code&gt;

Results in:

Y_train[0]: tensor([0.6947, 0.0886])
/home/ralars/miniconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:282: UserWarning: The input matches the stored training data. Did you forget to call model.train()?
"The input matches the stored training data. Did you forget to call model.train()?", UserWarning
Lengthscale: tensor([[[0.0100]],
[[0.0100]]], grad_fn=)
Noise: tensor([1.0000e-04], grad_fn=)
Variance: tensor([8.7607e-05, 2.3896e-06], grad_fn=)
Posterior: tensor([[0.6904, 0.0690]], grad_fn=) tensor([[7.6333e-05, 0.0000e+00],
[0.0000e+00, 2.3640e-06]], grad_fn=)

Notice that only the kernel variance was changed, and the output still does not match the training target.
		</comment>
		<comment id='3' author='rlrs' date='2020-02-08T02:15:08Z'>
		Hi &lt;denchmark-link:https://github.com/rlrs&gt;@rlrs&lt;/denchmark-link&gt;
,
Sorry this took so long to look in to, we were both pretty busy with the ICML deadline which has now passed.
I looked in to this, and it's definitely just caused by the same issue as &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1024&gt;#1024&lt;/denchmark-link&gt;
.
In particular, if I change your code to just use our basic batch GP implementation instead of the from_batch_mvn version of things, everything works fine without training:
import torch
import gpytorch

N = 1  # number of training points
D = 1  # input dimensions
K = 2  # target dimensions

class ExactGP(gpytorch.models.ExactGP):
    def __init__(self, X_train, Y_train, likelihood):
        super().__init__(X_train, Y_train, likelihood)
        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([K]))
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([K])), batch_shape=torch.Size([K]))

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

X_train = torch.randn(K, N, 1)
print(X_train.shape)
Y_train = torch.randn(K, N)
print(Y_train)

likelihood = gpytorch.likelihoods.GaussianLikelihood(num_tasks=K, noise_constraint=gpytorch.constraints.Positive())
model = ExactGP(X_train, Y_train, likelihood)

# set hyperparameters
model.likelihood.noise = 1e-4
model.covar_module.base_kernel.lengthscale = 1e-5
model.covar_module.outputscale = 1.0

print("Lengthscale:", model.covar_module.base_kernel.lengthscale)
print("Noise:", model.likelihood.noise)
print("Variance:", model.covar_module.outputscale)
model.eval()
likelihood.eval()
with gpytorch.settings.max_cholesky_size(10000):
    posterior = model(X_train)
    print("Posterior:", posterior.mean, posterior.covariance_matrix)
We will try to get a fix up for the  approach to batch GPs ASAP. cc/ &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='rlrs' date='2020-02-08T02:41:38Z'>
		&lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 Are there good reasons to support the  mode of doing this? The problem I'm having is that it might add a lot of indexing overhead in terms of time complexity, which could be a trap since that means that our basic batch GP functionality might end up being a lot more performant.
		</comment>
		<comment id='5' author='rlrs' date='2020-02-09T18:44:17Z'>
		I'd be curious to see some benchmarks for what the added time complexity here would be.
		</comment>
		<comment id='6' author='rlrs' date='2020-02-11T18:44:31Z'>
		&lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;
 The reason was to have a more consistent interface with regular multitask models. I'm swamped with KDD until Friday but I can take a look then.
		</comment>
		<comment id='7' author='rlrs' date='2020-04-07T15:58:07Z'>
		Upon further investigation, this is not the same issue as &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1024&gt;#1024&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1065&gt;#1065&lt;/denchmark-link&gt;
 - it's a subtle design decision behind .
MultitaskGaussianLikelihood has two sources of noise: noise (which is a generic amount of task-independent noise applied to all tasks) and noise_covar (which is a factor that introduces inter-task dependent noise).
In your script above, you set noise = 1e-4 but do not set noise_covar (which will default to a diagonal matrix with a diagonal of roughly 0.69). This is the amount of noise that causes your posterior to be off.
In your initialization, if you do:
model.likelihood.noise = 1e-4
model.covar_module.base_kernel.lengthscale = 1e-5
model.covar_module.outputscale = 1.0
model.likelihood.noise_covar.noise = 1e-4  # New line
you'll see the problem go away.
		</comment>
	</comments>
</bug>