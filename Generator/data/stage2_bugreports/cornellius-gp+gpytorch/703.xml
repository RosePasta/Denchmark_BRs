<bug id='703' author='tadejkrivec' open_date='2019-05-22T07:39:33Z' closed_time='2019-05-22T08:04:38Z'>
	<summary>[question] GPflow vs GPyTorch comparison</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

I am trying to compare the GPyTorch and GPflow on some dataset. I am using the same kernel, the same data, and I initialize the kernel hyperparameters to the same values. After that I check if the marginal log likelihood values agree between frameworks. I basically get the same values. After that I ran an Adam optimizer on both models.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

This is the snippet for Adam optimization with GPyTorch:
# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam([
    {'params': model.parameters()},  # Includes GaussianLikelihood parameters
], lr=1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

training_iterations = 10
for i in range(training_iterations):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)*train_x.shape[0]
    loss.backward()
    optimizer.step()
    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, -loss.item()))
I get the following loss:
&lt;denchmark-code&gt;Iter 1/10 - Loss: -2893.835
Iter 2/10 - Loss: -1770.051
Iter 3/10 - Loss: -477.718
Iter 4/10 - Loss: 926.491
Iter 5/10 - Loss: 2390.616
Iter 6/10 - Loss: 3882.949
Iter 7/10 - Loss: 5386.975
Iter 8/10 - Loss: 6898.352
Iter 9/10 - Loss: nan
Iter 10/10 - Loss: nan
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Whereas for GPflow I get the following:
&lt;denchmark-code&gt;GPR with Adam: iteration 1 likelihood -2898.1686
GPR with Adam: iteration 2 likelihood -1769.1999
GPR with Adam: iteration 3 likelihood -478.7015
GPR with Adam: iteration 4 likelihood 926.5689
GPR with Adam: iteration 5 likelihood 2390.6707
GPR with Adam: iteration 6 likelihood 3882.2375
GPR with Adam: iteration 7 likelihood 5386.5491
GPR with Adam: iteration 8 likelihood 6896.0282
GPR with Adam: iteration 9 likelihood 8404.9174
GPR with Adam: iteration 10 likelihood 9904.9957
&lt;/denchmark-code&gt;

I think that this has to be some problem with how the marginal log likelihood is calculated? I have tried to run Adam to convergence using GPflow and then initialize the GPyTorch model with the learned parameters and I get nan when trying to calculate the marginal log likelihood.
&lt;denchmark-code&gt;tensor(nan, device='cuda:0', grad_fn=&lt;MulBackward0&gt;)

/home/user/miniconda/envs/py36/lib/python3.6/site-packages/gpytorch/utils/linear_cg.py:295: UserWarning: CG terminated in 1000 iterations with average residual norm nan which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.
  " a gpytorch.settings.max_cg_iterations(value) context.".format(k + 1, residual_norm.mean(), tolerance)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

GPyTorch Version 0.3.2
PyTorch Version 1.1.0
Ubuntu 18.04

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I have uploaded the full example on this repo: &lt;denchmark-link:https://github.com/tadejkrivec/Maglev-GpyTorch/blob/master/Maglev_simple_example.ipynb&gt;https://github.com/tadejkrivec/Maglev-GpyTorch/blob/master/Maglev_simple_example.ipynb&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='tadejkrivec' date='2019-05-22T08:04:38Z'>
		GPFlow uses double precision (fp64) by default. If I add:
train_x = train_x.double()
train_y = train_y.double()
model = model.double()
likelihood = likelihood.double()
To your notebook I get:
&lt;denchmark-code&gt;Iter 1/10 - Loss: -2900.370
Iter 2/10 - Loss: -1757.345
Iter 3/10 - Loss: -469.456
Iter 4/10 - Loss: 932.013
Iter 5/10 - Loss: 2394.330
Iter 6/10 - Loss: 3883.894
Iter 7/10 - Loss: 5389.565
Iter 8/10 - Loss: 6899.005
Iter 9/10 - Loss: 8405.065
Iter 10/10 - Loss: 9904.389
&lt;/denchmark-code&gt;

Which matches very closely.
Except in very extreme cases, you usually won't really have to use double precision (fp64), and GPyTorch gets a fairly substantial performance boost on large datasets by using float precision (fp32). The issue that's occuring on this particular data is that the model is able to fit the training data essentially perfectly by driving the hyperparameters to extreme values. Using the GPyTorch model, I get a training RMSE of 7e-06 after only 5 iterations of training even in fp32, so additional training is leading to numerical instability. I'm able to solve this by simply adding a tiny amount of noise to the labels:
train_y = train_y + 1e-3 * torch.randn_like(train_y)
And then everything works in float while still getting a training RMSE of 0.0003.
		</comment>
	</comments>
</bug>