<bug id='1177' author='wjmaddox' open_date='2020-06-11T21:23:50Z' closed_time='2020-06-11T21:58:00Z'>
	<summary>Batched Multi-Output GP preconditioner doesn't compute noise model properly[Bug]</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

The noise checking to see if the diagonal is constant in the  method for the preconditioner is incorrect for batched GPs. Specifically, &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/1e19a641d301218936008f828bfb6e8da081ad3d/gpytorch/lazy/added_diag_lazy_tensor.py#L96&gt;here&lt;/denchmark-link&gt;
 indexes the noise improperly when the noise could be batched.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
import torch
import gpytorch

class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood, batch_shape=torch.Size([])):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean(batch_shape=batch_shape)
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=batch_shape),
                                                        batch_shape=batch_shape)
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

## base case that works
train_x = torch.randn(301, 1)
train_y = torch.sin(3.1 * train_x) + 0.1 * torch.randn_like(train_x)

likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood) 

with gpytorch.settings.max_cholesky_size(100), gpytorch.settings.min_preconditioning_size(100):

    print(likelihood(model(train_x)).log_prob(train_y).sum())

### This doesn't work
### I'm playing with the settings here to force it to crash but it's a shaping error.
train_x = torch.randn(301, 1)
train_y = torch.cat((torch.sin(3.1 * train_x) + 0.1 * torch.randn_like(train_x),
                     torch.sin(train_x) + 0.1 * torch.randn_like(train_x)),dim=1)

likelihood = gpytorch.likelihoods.GaussianLikelihood(batch_shape=torch.Size([2]))
model = ExactGPModel(train_x, train_y, likelihood, batch_shape=torch.Size([2])) 

optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

for i in range(3):
    optimizer.zero_grad()

    with gpytorch.settings.max_cholesky_size(100), gpytorch.settings.min_preconditioning_size(100):

        loss = -likelihood(model(train_x)).log_prob(train_y.t()).sum()
        loss.backward()
        optimizer.step()
** Stack trace/error message **
The noise checking is actually incorrect so you see this error message --- note that we got pushed into the _init_cache_for_non_constant_diag method for a homoscedastic noise model.
&lt;denchmark-code&gt;~/Documents/GitHub/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _init_cache(self)
    100             self._init_cache_for_constant_diag(eye, batch_shape, n, k)
    101         else:
--&gt; 102             self._init_cache_for_non_constant_diag(eye, batch_shape, n)
    103 
    104         self._precond_lt = PsdSumLazyTensor(RootLazyTensor(self._piv_chol_self), self._diag_tensor)

~/Documents/GitHub/gpytorch/gpytorch/lazy/added_diag_lazy_tensor.py in _init_cache_for_non_constant_diag(self, eye, batch_shape, n)
    117     def _init_cache_for_non_constant_diag(self, eye, batch_shape, n):
    118         # With non-constant diagonals, we cant factor out the noise as easily
--&gt; 119         self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self / self._noise.sqrt(), eye)))
    120         self._q_cache = self._q_cache[..., :n, :] / self._noise.sqrt()
    121 

RuntimeError: Tensors must have same number of dimensions: got 3 and 2
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

The noise shaping should be done properly so that _init_cache_for_constant_diag gets called instead. It runs properly.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:
gpytorch 1.1.1
pytorch 1.5.0
&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I'll toss up a PR in a minute. It's not that hard to fix.
	</description>
	<comments>
	</comments>
</bug>