<bug id='1248' author='akern40' open_date='2020-08-11T18:35:42Z' closed_time='2020-08-20T13:11:37Z'>
	<summary>SVDKL on CIFAR-10/100 Example [Bug]</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm trying to use the &lt;denchmark-link:https://docs.gpytorch.ai/en/v1.1.1/examples/06_PyTorch_NN_Integration_DKL/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html&gt;SVDKL example&lt;/denchmark-link&gt;
 on gpytorch v1.1.1, however I'm running into a number of issues.

I'm not sure what from densenet import DenseNet is from - I don't know of a PyTorch-compatible package called densenet. The code seems to align with the DenseNet from torchvision.models, although making that change brings its own issues.
When training, during the forward pass, I get RuntimeError: Shapes are not broadcastable for mul operation. This is the most puzzling part, and I'm really not sure what's happening - it seems that, while calculating the predictive mean in the grid interpolation variational strategy, the shapes can't be broadcasted together.

&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Run the &lt;denchmark-link:https://docs.gpytorch.ai/en/v1.1.1/examples/06_PyTorch_NN_Integration_DKL/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html&gt;SVDKL example&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Stack trace/error message&lt;/denchmark-h&gt;

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-f33484a0f2d7&gt; in &lt;module&gt;
      1 for epoch in range(1, n_epochs + 1):
      2     with gpytorch.settings.use_toeplitz(False):
----&gt; 3         train(epoch)
      4         test()
      5     scheduler.step()

&lt;ipython-input-8-bc9a57700a55&gt; in train(epoch)
     21                 data, target = data.cuda(), target.cuda()
     22             optimizer.zero_grad()
---&gt; 23             output = model(data)
     24             loss = -mll(output, target)
     25             loss.backward()

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     26 
     27     def __call__(self, *inputs, **kwargs):
---&gt; 28         outputs = self.forward(*inputs, **kwargs)
     29         if isinstance(outputs, list):
     30             return [_validate_module_outputs(output) for output in outputs]

&lt;ipython-input-7-48adb3fc1e18&gt; in forward(self, x)
     12         # This next line makes it so that we learn a GP for each feature
     13         features = features.transpose(-1, -2).unsqueeze(-1)
---&gt; 14         res = self.gp_layer(features)
     15         return res
     16 

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/models/approximate_gp.py in __call__(self, inputs, prior, **kwargs)
     79         if inputs.dim() == 1:
     80             inputs = inputs.unsqueeze(-1)
---&gt; 81         return self.variational_strategy(inputs, prior=prior)

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/variational/multitask_variational_strategy.py in __call__(self, x, prior)
     41 
     42     def __call__(self, x, prior=False):
---&gt; 43         function_dist = self.base_variational_strategy(x, prior=prior)
     44         if (
     45             self.task_dim &gt; 0

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/variational/_variational_strategy.py in __call__(self, x, prior)
    125                 inducing_points,
    126                 inducing_values=variational_dist_u.mean,
--&gt; 127                 variational_inducing_covar=variational_dist_u.lazy_covariance_matrix,
    128             )
    129         elif isinstance(variational_dist_u, Delta):

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     26 
     27     def __call__(self, *inputs, **kwargs):
---&gt; 28         outputs = self.forward(*inputs, **kwargs)
     29         if isinstance(outputs, list):
     30             return [_validate_module_outputs(output) for output in outputs]

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/variational/grid_interpolation_variational_strategy.py in forward(self, x, inducing_points, inducing_values, variational_inducing_covar)
     91         # Compute test mean
     92         # Left multiply samples by interpolation matrix
---&gt; 93         predictive_mean = left_interp(interp_indices, interp_values, inducing_values.unsqueeze(-1))
     94         predictive_mean = predictive_mean.squeeze(-1)
     95 

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/utils/interpolation.py in left_interp(interp_indices, interp_values, rhs)
    181         num_data, num_columns = rhs.shape[-2:]
    182         interp_shape = torch.Size((*interp_indices.shape[:-1], num_data))
--&gt; 183         output_shape = _matmul_broadcast_shape(interp_shape, rhs.shape)
    184         batch_shape = output_shape[:-2]
    185 

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/utils/broadcasting.py in _matmul_broadcast_shape(shape_a, shape_b, error_msg)
     55         bc_shape = batch_shape_a
     56     else:
---&gt; 57         bc_shape = _mul_broadcast_shape(batch_shape_a, batch_shape_b)
     58     return bc_shape + tail_shape
     59 

~/.conda/envs/gpytorch/lib/python3.6/site-packages/gpytorch/utils/broadcasting.py in _mul_broadcast_shape(error_msg, *shapes)
     18             if any(size != non_singleton_sizes[0] for size in non_singleton_sizes):
     19                 if error_msg is None:
---&gt; 20                     raise RuntimeError("Shapes are not broadcastable for mul operation")
     21                 else:
     22                     raise RuntimeError(error_msg)

RuntimeError: Shapes are not broadcastable for mul operation
&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

As in the example online, I expect the model to run the forward and backward passes without issue.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

GPyTorch Version 1.1.1
PyTorch Version 1.5.0
Linux

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I found Issues &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1153&gt;#1153&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1084&gt;#1084&lt;/denchmark-link&gt;
, but can't make heads or tails of how those issues apply to this example. Any help would be greatly appreciated!
	</description>
	<comments>
		<comment id='1' author='akern40' date='2020-08-18T23:37:56Z'>
		Hi &lt;denchmark-link:https://github.com/akern40&gt;@akern40&lt;/denchmark-link&gt;
 -- I don't have any issues running the notebook with the model we provide. Note that  can be found in the example folder: &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/tree/master/examples/06_PyTorch_NN_Integration_DKL&gt;https://github.com/cornellius-gp/gpytorch/tree/master/examples/06_PyTorch_NN_Integration_DKL&lt;/denchmark-link&gt;

Could you try rerunning with that one and see if it fixes the issue?
		</comment>
		<comment id='2' author='akern40' date='2020-08-20T13:11:37Z'>
		Well I feel tremendously embarrassed - thanks for the help! That works like a charm. I had thought the two densenets differed by only the softmax layer, but there appear to be a number of convolutions and other layers that are different, and I ended up with a size mismatch.
		</comment>
		<comment id='3' author='akern40' date='2020-08-20T13:49:03Z'>
		No worries! Glad that it was a simple fix :)
		</comment>
	</comments>
</bug>