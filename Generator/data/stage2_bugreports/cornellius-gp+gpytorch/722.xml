<bug id='722' author='tmpethick' open_date='2019-06-05T12:01:50Z' closed_time='2019-06-06T06:26:50Z'>
	<summary>[Bug] Trapped with bad initialization</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

My problem is in reproducing result from GPy with GPyTorch when using the same (bad) initialisation for the hyperparameters.
I am using gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) as the kernel and setting the lengthscale higher than the domain interval (l=2.5).
The Adam optimizer gets trapped in a local minimum that has close to constant posterior mean.
This happens for a wide range of training hyperparameters (here with lr=0.1 and epochs=500).
In contrast GPy manage to escape by finding the more reasonable l=0.01.
It seems quite crucial for robustness that we are not too dependent on the initialization. Is there any way this can be achieved with GPyTorch?
PS: I am testing on a function f(x) = sin(60 x‚Å¥) that is increasingly oscillating.
PSS: Thanks for a very well thought out architecture!
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Pulled and modified from documentation example.
import math
import torch
import gpytorch
import numpy as np
from matplotlib import pyplot as plt
%matplotlib inline

bounds = np.array([[0,1]])
train_x = torch.linspace(bounds[0,0], bounds[0,1], 100)
train_y = np.sin(60 * train_x ** 4)

# We will use the simplest form of GP model, exact inference
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# initialize likelihood and model
likelihood = gpytorch.likelihoods.GaussianLikelihood()
# likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=torch.ones(1) * 0.0001)
model = ExactGPModel(train_x, train_y, likelihood)

model.initialize(**{
    'likelihood.noise': 0.5,
    'covar_module.base_kernel.lengthscale': 2.5,
    'covar_module.outputscale': 1.5
})
print("Initialized with: lengthscale=%.3f variance=%.3f noise=%.5f" % (model.covar_module.base_kernel.lengthscale.item(),
        model.covar_module.outputscale.item(),
        model.likelihood.noise.item()))

# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam([
    {'params': model.parameters()}, 
], lr=0.1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

training_iter = 500
for i in range(training_iter):
    # Zero gradients from previous iteration
    optimizer.zero_grad()
    # Output from model
    output = model(train_x)
    # Calc loss and backprop gradients
    loss = -mll(output, train_y)
    loss.backward()
    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f, variance: %.3f,   noise: %.5f' % (
        i + 1, training_iter, loss.item(),
        model.covar_module.base_kernel.lengthscale.item(),
        model.covar_module.outputscale.item(),
        model.likelihood.noise.item()
    ))
    optimizer.step()

# Prediction
# Get into evaluation (predictive posterior) mode
model.eval()
likelihood.eval()

# Test points are regularly spaced along [0,1]
# Make predictions by feeding model through likelihood
with torch.no_grad():
    test_x = torch.linspace(bounds[0,0], bounds[0,1], 1000)
    observed_pred = likelihood(model(test_x))

    # Initialize plot
    f, ax = plt.subplots(1, 1, figsize=(10, 10))

    # Get upper and lower confidence bounds
    lower, upper = observed_pred.confidence_region()
    # Plot training data as black stars
    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')
    # Plot predictive means as blue line
    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')
    # Shade between the lower and upper confidence bounds
    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)
    ax.set_ylim([-3, 3])
    ax.legend(['Observed Data', 'Mean', 'Confidence'])
** Stack trace/error message **
&lt;denchmark-code&gt;// Paste the bad output here!
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

With a lengthscale initalization of l=2.5 we get trapped whereas we should be able to find l=0.1. (see Additional context for GPy implementation which manages this with the same initialization).
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

GPyTorch Version: 0.3.2
PyTorch Version: 1.0.1.post2
Computer OS: MacOS 10.14
&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

This is to created the expected behaviour in GPy (note that GPy is required).
&lt;denchmark-code&gt;%matplotlib inline

import math
import numpy as np
import torch
import gpytorch
from matplotlib import pyplot as plt
import GPy

bounds = np.array([[0,1]])
train_x = torch.linspace(bounds[0,0], bounds[0,1], 100)
train_y = np.sin(60 * train_x ** 4)

kernel = GPy.kern.RBF(1, ARD=False)
model = GPy.models.GPRegression(train_x.numpy()[:,None], train_y.numpy()[:,None], kernel=kernel)
model.Gaussian_noise = 0.5
model.kern.lengthscale = 2.5
model.kern.variance = 1.5

model.optimize()

test_x = np.linspace(bounds[0,0], bounds[0,1], 1000)
mean, var = model.predict(test_x[:, None])
mean = mean[:,0]
var = var[:,0]

f, ax = plt.subplots(1, 1, figsize=(10, 10))

# Get upper and lower confidence bounds
lower, upper = mean - 2 * np.sqrt(var), mean + 2 * np.sqrt(var)
# Plot training data as black stars
ax.plot(train_x.numpy(), train_y.numpy(), 'k*')
# Plot predictive means as blue line
ax.plot(test_x, mean, 'b')
# Shade between the lower and upper confidence bounds
ax.fill_between(test_x, lower, upper, alpha=0.5)
ax.legend(['Observed Data', 'Mean', 'Confidence'])

model
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tmpethick' date='2019-06-05T16:41:42Z'>
		This is really interesting. The solution ended up being that GPy by default does not learn a constant mean.
Swapping out ConstantMean for ZeroMean solves the issue in GPyTorch. Similarly, modifying GPy to use a constant mean function instead of a zero mean function produces the constant (bad) fit out of GPy:
model = GPy.models.GPRegression(train_x.numpy()[:,None], train_y.numpy()[:,None], kernel=kernel, mean_function=GPy.mappings.Constant(1, 1, 1))
If I played around enough with the optimization, I was able to get it to converge (e.g., running Adam twice with 500 iterations and a learning rate of 0.01), so it seems like this really is just a local optimum that sometimes either package can escape and sometimes they can't.
In general, if you'd prefer not to use Adam, the PyTorch-LBFGS implementation of LBFGS works pretty well: &lt;denchmark-link:https://github.com/hjmshi/PyTorch-LBFGS/blob/master/examples/Gaussian_Processes/gp_regression.py&gt;https://github.com/hjmshi/PyTorch-LBFGS/blob/master/examples/Gaussian_Processes/gp_regression.py&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='tmpethick' date='2019-06-06T06:26:50Z'>
		Thanks for looking seriously into it! I should have considered the mean.
I did a more though study by plotting the negative log marginal likelihood as a function of the initial lengthscale. GPy turns out to also get trapped in a local optimum if you make the lengthscale extreme enough in either directions. For my model configurations GPy just happened to have a slightly wider interval for which it worked. What is interesting is that after changing to ZeroMean GPyTorch actually does better.
:
&lt;denchmark-link:https://user-images.githubusercontent.com/544312/59011059-b1404680-8833-11e9-91b9-0872818fd2e5.png&gt;&lt;/denchmark-link&gt;

:
&lt;denchmark-link:https://user-images.githubusercontent.com/544312/59011275-6115b400-8834-11e9-8c29-0957a7a839f2.png&gt;&lt;/denchmark-link&gt;

:
&lt;denchmark-link:https://user-images.githubusercontent.com/544312/59011277-6115b400-8834-11e9-8723-669c1a3a5072.png&gt;&lt;/denchmark-link&gt;

Thanks again!
		</comment>
	</comments>
</bug>