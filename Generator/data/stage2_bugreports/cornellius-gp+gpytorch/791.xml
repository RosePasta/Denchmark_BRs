<bug id='791' author='KeAWang' open_date='2019-07-16T21:14:58Z' closed_time='2019-07-16T23:02:45Z'>
	<summary>[Bug] SGPR Regularization different from Titias 2009?</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

The paper by &lt;denchmark-link:http://proceedings.mlr.press/v5/titsias09a.html&gt;Titsias 2009&lt;/denchmark-link&gt;
 derives a trace regularization term for the loss function of variational SGPR which has a coefficient of Tr(T)/(2 * sigma^2). But the code here implements it as just Tr(T) / sigma 

Is there a reason for this difference?
	</description>
	<comments>
		<comment id='1' author='KeAWang' date='2019-07-16T21:23:46Z'>
		There's a factor of 1/2 from 


gpytorch/gpytorch/mlls/exact_marginal_log_likelihood.py


         Line 35
      in
      b4aee6f






 res = res.add(0.5, added_loss) 




, but I think the noise should still be squared.
Also it seems weird to put the factor of 1/2 there instead of in InducingPointKernelAddedLossTerm
		</comment>
		<comment id='2' author='KeAWang' date='2019-07-16T22:07:27Z'>
		The squared issue is just a parameterization difference. For our likelihoods, noise directly returns the amount of diagonal noise we add.
		</comment>
	</comments>
</bug>