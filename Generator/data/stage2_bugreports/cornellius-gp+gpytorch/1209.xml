<bug id='1209' author='YuuuXie' open_date='2020-07-10T19:05:50Z' closed_time='2020-07-23T12:54:46Z'>
	<summary>[Bug] Inducing point kernel does not work with grad kernel</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

When I tried to combine the &lt;denchmark-link:https://docs.gpytorch.ai/en/v1.1.1/examples/02_Scalable_Exact_GPs/SGPR_Regression_CUDA.html&gt;Sparse GP&lt;/denchmark-link&gt;
 or &lt;denchmark-link:https://docs.gpytorch.ai/en/v1.1.1/examples/04_Variational_and_Approximate_GPs/SVGP_Multitask_GP_Regression.html&gt;Variational GP with multiple outputs&lt;/denchmark-link&gt;
 with the &lt;denchmark-link:https://docs.gpytorch.ai/en/latest/examples/08_Advanced_Usage/Simple_GP_Regression_Derivative_Information_2d.html&gt;2D derivative example&lt;/denchmark-link&gt;
, I got dimension mismatch issues. It seems to me that the SGPR does not really work for multitask, and though the SVGP works for multitask, it only accepts covariance matrix which has the same size as the input.
For example, for the gradient kernel, the covariance matrix has shape n(d+1) x n(d+1), while in the inducing kernel, e.g. gpytorch/variational/variational_strategy.py forward function (line 87-135), it seems that only the shape n x n is allowed?
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

I tried this in the &lt;denchmark-link:https://docs.gpytorch.ai/en/latest/examples/08_Advanced_Usage/Simple_GP_Regression_Derivative_Information_2d.html&gt;2D derivative example&lt;/denchmark-link&gt;

 41 class MultitaskGPModel(gpytorch.models.ApproximateGP):
 42     def __init__(self):
 43         # Let's use a different set of inducing points for each task
 44         inducing_points = torch.rand(num_tasks, 16, 2)
 45 
 46         # We have to mark the CholeskyVariationalDistribution as batch
 47         # so that we learn a variational distribution for each task
 48         variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(
 49             inducing_points.size(-2), batch_shape=torch.Size([num_tasks])
 50         )
 51 
 52         variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(
 53             gpytorch.variational.VariationalStrategy(
 54                 self, inducing_points, variational_distribution, learn_inducing_locations=True
 55             ),
 56             num_tasks=num_tasks,
 57         )
 58 
 59         super().__init__(variational_strategy)
 60 
 61         # The mean and covariance modules should be marked as batch
 62         # so we learn a different set of hyperparameters
 63         self.mean_module = gpytorch.means.ConstantMeanGrad(batch_shape=torch.Size([num_tasks]))
 64         self.covar_module = gpytorch.kernels.ScaleKernel(
 65             gpytorch.kernels.RBFKernelGrad(ard_num_dims=2, batch_shape=torch.Size([num_tasks])),
 66 #             gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_tasks])),
 67             batch_shape=torch.Size([num_tasks])
 68         )
 69 
 70 
 71     def forward(self, x):
 72         mean_x = self.mean_module(x)
 73         covar_x = self.covar_module(x)
 74         print('forward x', x.shape, mean_x.shape, covar_x.shape)
 75         return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)
 76 
 77 likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)  # Value + x-derivative + y-derivative
 78 model = MultitaskGPModel()
** Stack trace/error message **
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "2d_derv.py", line 114, in &lt;module&gt;
    output = model(train_x)
  File "/Users/xiey/anaconda3/lib/python3.7/site-packages/gpytorch/models/approximate_gp.py", line 81, in __call__
    return self.variational_strategy(inputs, prior=prior)
  File "/Users/xiey/anaconda3/lib/python3.7/site-packages/gpytorch/variational/independent_multitask_variational_strategy.py", line 47, in __call__
    function_dist = self.base_variational_strategy(x, prior=prior)
  File "/Users/xiey/anaconda3/lib/python3.7/site-packages/gpytorch/variational/variational_strategy.py", line 181, in __call__
    return super().__call__(x, prior=prior)
  File "/Users/xiey/anaconda3/lib/python3.7/site-packages/gpytorch/variational/_variational_strategy.py", line 134, in __call__
    variational_inducing_covar=variational_dist_u.lazy_covariance_matrix,
  File "/Users/xiey/anaconda3/lib/python3.7/site-packages/gpytorch/module.py", line 28, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/Users/xiey/anaconda3/lib/python3.7/site-packages/gpytorch/variational/variational_strategy.py", line 116, in forward
    + test_mean
RuntimeError: The size of tensor a (332) must match the size of tensor b (0) at non-singleton dimension 2
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

GPyTorch: 1.1.1
PyTorch: 1.5.0
macOS Catalina, 10.15.5

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Maybe this is related to &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1063&gt;#1063&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1121&gt;#1121&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='YuuuXie' date='2020-07-23T12:54:45Z'>
		Hi @YuuuuXie,
This is actually pretty easy to fix and is related to the fact that for variational inference the forward call should treat the outputs as though they are batch mode outputs. In practical terms, this means we just need a few lines of modification to forward -- the following code should work for you.
import gpytorch

num_latents = 3
num_tasks = 3

class MultitaskGPModel(gpytorch.models.ApproximateGP):
    def __init__(self):
        # Let's use a different set of inducing points for each task
        inducing_points = torch.rand(num_latents, 16, 2)

        # We have to mark the CholeskyVariationalDistribution as batch
        # so that we learn a variational distribution for each task
        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(
            inducing_points.size(-2), batch_shape=torch.Size([num_latents]),
        )

        variational_strategy = gpytorch.variational.LMCVariationalStrategy(
            gpytorch.variational.VariationalStrategy(
                self, inducing_points, variational_distribution, learn_inducing_locations=True
            ),
            num_tasks=num_tasks,
            num_latents=num_latents,
            latent_dim=-1,
        )

        super().__init__(variational_strategy)

        # The mean and covariance modules should be marked as batch
        # so we learn a different set of hyperparameters
        self.mean_module = gpytorch.means.ConstantMeanGrad(batch_shape=torch.Size([num_latents]))
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernelGrad(ard_num_dims=2)
        )


    def forward(self, x):
        mean_x = self.mean_module(x)
        batch_shape = mean_x.shape[:-2]
        mean_x = mean_x.view(*batch_shape, -1)
        covar_x = self.covar_module(x)
        print('forward x', x.shape, mean_x.shape, covar_x.shape)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)  # Value + x-derivative + y-derivative
model = MultitaskGPModel()
		</comment>
	</comments>
</bug>