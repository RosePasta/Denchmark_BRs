<bug id='755' author='g-benton' open_date='2019-06-25T17:16:40Z' closed_time='2019-06-27T18:49:08Z'>
	<summary>[Bug] Shape errors with GridInterpolationKernel</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

Dealing with a bug when using GridInterpolationKernel in which when each of the input dimensions is even in size everything runs, but when the input sizes are odd an internal shape error is thrown.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
# define SKI model
class GPRegressionModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)

        # SKI requires a grid size hyperparameter. This util can help with that
        grid_size = [50, 50]
        
        self.mean_module = gpytorch.means.ConstantMean()
        base = gpytorch.kernels.RBFKernel
        
        grid_kernels = [gpytorch.kernels.GridInterpolationKernel(
                base(),num_dims = train_x.size(-1), active_dims = (d),
            grid_size = grid_size[d]
        ) for d in range(train_x.size(-1))]
        
        self.covar_module = gpytorch.kernels.ProductKernel(*grid_kernels)        

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
        
# generate data
n = 30
train_x = torch.zeros(pow(n, 2), 2)
for i in range(n):
    for j in range(n):
        train_x[i * n + j][0] = float(i) / (n-1)
        train_x[i * n + j][1] = float(j) / (n-1)
# True function is sin( 2*pi*(x0+x1))
train_y = torch.sin((train_x[:, 0] + train_x[:, 1]) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(0.01)

in_dims = 1 if train_x.dim() == 1 else train_x.size(1)

# define the model and make the log prob call
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = GPRegressionModel(train_x, train_y, likelihood)

likelihood.train()
model.train()

with gpytorch.settings.num_trace_samples(100), torch.no_grad():
    print(likelihood(model(train_x)).log_prob(train_y))
    
# generate data with odd n
n = 35 
train_x = torch.zeros(pow(n, 2), 2)
for i in range(n):
    for j in range(n):
        train_x[i * n + j][0] = float(i) / (n-1)
        train_x[i * n + j][1] = float(j) / (n-1)
# True function is sin( 2*pi*(x0+x1))
train_y = torch.sin((train_x[:, 0] + train_x[:, 1]) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(0.01)

in_dims = 1 if train_x.dim() == 1 else train_x.size(1)

# define the model and make the log prob call
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = GPRegressionModel(train_x, train_y, likelihood)

likelihood.train()
model.train()

with gpytorch.settings.num_trace_samples(100), torch.no_grad():
    print(likelihood(model(train_x)).log_prob(train_y))
** Stack trace/error message **
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-2edc81b19af7&gt; in &lt;module&gt;
     14 
     15 with gpytorch.settings.num_trace_samples(100), torch.no_grad():
---&gt; 16     print(likelihood(model(train_x)).log_prob(train_y))

~/gpytorch/gpytorch/distributions/multivariate_normal.py in log_prob(self, value)
    126 
    127         # Get log determininat and first part of quadratic form
--&gt; 128         inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)
    129 
    130         res = -0.5 * sum([inv_quad, logdet, diff.size(-1) * math.log(2 * math.pi)])

~/gpytorch/gpytorch/lazy/lazy_tensor.py in inv_quad_logdet(self, inv_quad_rhs, logdet, reduce_inv_quad)
   1021                 )
   1022 
-&gt; 1023         args = self.representation()
   1024         if inv_quad_rhs is not None:
   1025             args = [inv_quad_rhs] + list(args)

~/gpytorch/gpytorch/lazy/lazy_tensor.py in representation(self)
   1251                 representation.append(arg)
   1252             elif hasattr(arg, "representation") and callable(arg.representation):  # Is it a LazyTensor?
-&gt; 1253                 representation += list(arg.representation())
   1254             else:
   1255                 raise RuntimeError("Representation of a LazyTensor should consist only of Tensors")

~/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in representation(self)
    306         # representation
    307         else:
--&gt; 308             return self.evaluate_kernel().representation()
    309 
    310     def representation_tree(self):

~/gpytorch/gpytorch/utils/memoize.py in g(self, *args, **kwargs)
     32         cache_name = name if name is not None else method
     33         if not is_in_cache(self, cache_name):
---&gt; 34             add_to_cache(self, cache_name, method(self, *args, **kwargs))
     35         return get_from_cache(self, cache_name)
     36 

~/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in evaluate_kernel(self)
    268             self.kernel.active_dims = None
    269             res = self.kernel(
--&gt; 270                 x1, x2, diag=False, last_dim_is_batch=self.last_dim_is_batch, **self.params
    271             )
    272             self.kernel.active_dims = temp_active_dims

~/gpytorch/gpytorch/kernels/kernel.py in __call__(self, x1, x2, diag, last_dim_is_batch, **params)
    374                 res = LazyEvaluatedKernelTensor(x1_, x2_, kernel=self, last_dim_is_batch=last_dim_is_batch, **params)
    375             else:
--&gt; 376                 res = super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params)
    377             return res
    378 

~/gpytorch/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     20 
     21     def __call__(self, *inputs, **kwargs):
---&gt; 22         outputs = self.forward(*inputs, **kwargs)
     23         if isinstance(outputs, list):
     24             return [_validate_module_outputs(output) for output in outputs]

~/gpytorch/gpytorch/kernels/kernel.py in forward(self, x1, x2, diag, **params)
    442             res = delazify(self.kernels[0](x1, x2, diag=diag, **params))
    443         else:
--&gt; 444             res = self.kernels[0](x1, x2, diag=diag, **params)
    445 
    446             if not diag:

~/gpytorch/gpytorch/kernels/kernel.py in __call__(self, x1, x2, diag, last_dim_is_batch, **params)
    374                 res = LazyEvaluatedKernelTensor(x1_, x2_, kernel=self, last_dim_is_batch=last_dim_is_batch, **params)
    375             else:
--&gt; 376                 res = super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params)
    377             return res
    378 

~/gpytorch/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     20 
     21     def __call__(self, *inputs, **kwargs):
---&gt; 22         outputs = self.forward(*inputs, **kwargs)
     23         if isinstance(outputs, list):
     24             return [_validate_module_outputs(output) for output in outputs]

~/gpytorch/gpytorch/kernels/grid_interpolation_kernel.py in forward(self, x1, x2, diag, last_dim_is_batch, **params)
    135         if self.grid_is_dynamic:  # This is true if a grid_bounds wasn't passed in
    136             if torch.equal(x1, x2):
--&gt; 137                 x = x1.contiguous().view(-1, self.num_dims)
    138             else:
    139                 x = torch.cat([x1.contiguous().view(-1, self.num_dims), x2.contiguous().view(-1, self.num_dims)])

RuntimeError: shape '[-1, 2]' is invalid for input of size 1225
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Same as in the first case above when n=30
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

gytorch 0.3.2, built from master 6/25
pytorch version 1.1.0
ubuntu 18.04

	</description>
	<comments>
		<comment id='1' author='g-benton' date='2019-06-25T17:41:01Z'>
		Also wanted to add that if I let gpytorch choose the size for even n,
&lt;denchmark-code&gt;        self.mean_module = gpytorch.means.ConstantMean()
        base = gpytorch.kernels.RBFKernel
        grid_kernels = [gpytorch.kernels.GridInterpolationKernel(
                base(),num_dims = train_x.size(-1), active_dims = (d),
            grid_size = gpytorch.utils.grid.choose_grid_size(train_x[:,d])
        ) for d in range(train_x.size(-1))]       
        self.covar_module = gpytorch.kernels.ProductKernel(*grid_kernels)        
&lt;/denchmark-code&gt;

I get either the error reported above or
&lt;denchmark-code&gt;IndexError                                Traceback (most recent call last)
&lt;ipython-input-36-7c1b6f7d147e&gt; in &lt;module&gt;
      6 
      7 with gpytorch.settings.num_trace_samples(100), torch.no_grad():
----&gt; 8     print(likelihood(model(train_x)).log_prob(train_y))

~/projects/gpytorch/gpytorch/distributions/multivariate_normal.py in log_prob(self, value)
    126 
    127         # Get log determininat and first part of quadratic form
--&gt; 128         inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)
    129 
    130         res = -0.5 * sum([inv_quad, logdet, diff.size(-1) * math.log(2 * math.pi)])

~/projects/gpytorch/gpytorch/lazy/lazy_tensor.py in inv_quad_logdet(self, inv_quad_rhs, logdet, reduce_inv_quad)
   1021                 )
   1022 
-&gt; 1023         args = self.representation()
   1024         if inv_quad_rhs is not None:
   1025             args = [inv_quad_rhs] + list(args)

~/projects/gpytorch/gpytorch/lazy/lazy_tensor.py in representation(self)
   1251                 representation.append(arg)
   1252             elif hasattr(arg, "representation") and callable(arg.representation):  # Is it a LazyTensor?
-&gt; 1253                 representation += list(arg.representation())
   1254             else:
   1255                 raise RuntimeError("Representation of a LazyTensor should consist only of Tensors")

~/projects/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in representation(self)
    306         # representation
    307         else:
--&gt; 308             return self.evaluate_kernel().representation()
    309 
    310     def representation_tree(self):

~/projects/gpytorch/gpytorch/utils/memoize.py in g(self, *args, **kwargs)
     32         cache_name = name if name is not None else method
     33         if not is_in_cache(self, cache_name):
---&gt; 34             add_to_cache(self, cache_name, method(self, *args, **kwargs))
     35         return get_from_cache(self, cache_name)
     36 

~/projects/gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in evaluate_kernel(self)
    268             self.kernel.active_dims = None
    269             res = self.kernel(
--&gt; 270                 x1, x2, diag=False, last_dim_is_batch=self.last_dim_is_batch, **self.params
    271             )
    272             self.kernel.active_dims = temp_active_dims

~/projects/gpytorch/gpytorch/kernels/kernel.py in __call__(self, x1, x2, diag, last_dim_is_batch, **params)
    374                 res = LazyEvaluatedKernelTensor(x1_, x2_, kernel=self, last_dim_is_batch=last_dim_is_batch, **params)
    375             else:
--&gt; 376                 res = super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params)
    377             return res
    378 

~/projects/gpytorch/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     20 
     21     def __call__(self, *inputs, **kwargs):
---&gt; 22         outputs = self.forward(*inputs, **kwargs)
     23         if isinstance(outputs, list):
     24             return [_validate_module_outputs(output) for output in outputs]

~/projects/gpytorch/gpytorch/kernels/kernel.py in forward(self, x1, x2, diag, **params)
    448 
    449         for kern in self.kernels[1:]:
--&gt; 450             next_term = kern(x1, x2, diag=diag, **params)
    451             if not x1_eq_x2:
    452                 # Again delazify if x1 != x2

~/projects/gpytorch/gpytorch/kernels/kernel.py in __call__(self, x1, x2, diag, last_dim_is_batch, **params)
    374                 res = LazyEvaluatedKernelTensor(x1_, x2_, kernel=self, last_dim_is_batch=last_dim_is_batch, **params)
    375             else:
--&gt; 376                 res = super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params)
    377             return res
    378 

~/projects/gpytorch/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     20 
     21     def __call__(self, *inputs, **kwargs):
---&gt; 22         outputs = self.forward(*inputs, **kwargs)
     23         if isinstance(outputs, list):
     24             return [_validate_module_outputs(output) for output in outputs]

~/projects/gpytorch/gpytorch/kernels/grid_interpolation_kernel.py in forward(self, x1, x2, diag, last_dim_is_batch, **params)
    165             base_lazy_tsr = base_lazy_tsr.repeat(*x1.shape[:-2], 1, 1)
    166 
--&gt; 167         left_interp_indices, left_interp_values = self._compute_grid(x1, last_dim_is_batch)
    168         if torch.equal(x1, x2):
    169             right_interp_indices = left_interp_indices

~/projects/gpytorch/gpytorch/kernels/grid_interpolation_kernel.py in _compute_grid(self, inputs, last_dim_is_batch)
    123 
    124         inputs = inputs.contiguous().view(-1, n_dimensions)
--&gt; 125         interp_indices, interp_values = Interpolation().interpolate(self.grid, inputs)
    126         interp_indices = interp_indices.view(*batch_shape, n_data, -1)
    127         interp_values = interp_values.view(*batch_shape, n_data, -1)

~/projects/gpytorch/gpytorch/utils/interpolation.py in interpolate(self, x_grid, x_target, interp_points)
     55                     grid_mins[first_out_of_range].item(),
     56                     grid_maxs[first_out_of_range].item(),
---&gt; 57                     x_target_min[first_out_of_range].item(),
     58                     x_target_max[first_out_of_range].item(),
     59                 )

IndexError: index 1 is out of bounds for dimension 0 with size 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='g-benton' date='2019-06-26T01:07:15Z'>
		I think the issue is that you want a 1-dimensional SKI kernel for each dimension. Maybe try changing num_dims to 1:
grid_kernels = [gpytorch.kernels.GridInterpolationKernel(
                base(),num_dims = 1, active_dims = (d),
            grid_size = grid_size[d]
        ) for d in range(train_x.size(-1))]
This is what I'm doing and it works in my code.
		</comment>
		<comment id='3' author='g-benton' date='2019-06-27T18:49:08Z'>
		&lt;denchmark-link:https://github.com/idelbrid&gt;@idelbrid&lt;/denchmark-link&gt;
's answer is correct -- when you use a  or , the assumption is that the underlying kernel is a 1 dimensional kernel, which is just the nature of the decomposition.
		</comment>
	</comments>
</bug>