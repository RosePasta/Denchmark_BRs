<bug id='1064' author='wjmaddox' open_date='2020-02-28T22:35:16Z' closed_time='2020-03-04T00:15:05Z'>
	<summary>BatchRepeatLazyTensor cannot evaluate when matrix is not square [Bug]</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

BatchRepeatLazyTensor is trying to expand the  in the  method to look like the expected output when the  is not the same size as the expected output.
From my limited tests, using torch batching standards is probably fine and the &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/ae42649da8199b97a4bc2869708b6a97fc99dc70/gpytorch/lazy/batch_repeat_lazy_tensor.py#L98&gt;batching expansions&lt;/denchmark-link&gt;
 (line 98-102) are unecessary as LazyTensors ought to be able to handle batched matmuls.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
x = torch.randn(256, 128)
x_lazy = gpytorch.lazify(x)
batch_repeated_x = gpytorch.lazy.BatchRepeatLazyTensor(x_lazy, torch.Size((10,)))
batch_repeated_x.evaluate()
** Stack trace/error message **
&lt;denchmark-code&gt;RuntimeError: The expanded size of the tensor (256) must match the existing size (128) at non-singleton dimension 1.  Target sizes: [10, 256, 128].  Tensor sizes: [10, 128, 128]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

It should evaluate. In particular, a 10 x 256 x 128 matrix should be returned.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

gpytorch.1.0.1
torch 1.3.1

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I believe that a potential fix is to have the _matmul method be simply self.base_lazy_tensor._matmul(rhs).
	</description>
	<comments>
		<comment id='1' author='wjmaddox' date='2020-03-04T00:15:04Z'>
		Should be closed by &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/1068&gt;#1068&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>