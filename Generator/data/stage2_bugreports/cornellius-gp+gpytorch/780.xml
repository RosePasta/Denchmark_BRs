<bug id='780' author='wtstephe' open_date='2019-07-11T13:44:01Z' closed_time='2019-07-11T18:42:01Z'>
	<summary>[Bug] RBF Kernel is sometimes not positive definite</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

When I create a RBFKernel with 512 evenly spaced points between 500 and 501, and form the resulting kernel matrix, the matrix seems numerically positive definite (minimum eigenvalue is -1e-6). But, when I do the same with 513 points, the kernel matrix is very far from positive definite (minimum eigenvalue is -0.1). This behavior seems to crop up when using 513 points or more -- everything seems fine when using 512 or less; the results are also dependent on the range from which the datapoints are generated.
Is this a bug, or am I grossly misusing some feature?
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
import numpy as np
import torch
import gpytorch

upper = 500
lower = 501
covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
covar_module.base_kernel.lengthscale = 1.0
covar_module.outputscale = 1.0


# Evaluate a RBF kernel on 512 evenly spaced points between 100 and
#  101. The resulting kernel matrix is numerically PSD (smallest eigenvalue
#  is -1e-6)
X = torch.linspace(lower, upper, 512)
covarX = covar_module(X, X).numpy()
print('Min eigenvalue with 512 points', np.linalg.eigvals(covarX).min())

# Evaluate a RBF kernel on 513 evenly spaced points between 100 and
#  101. The resulting kernel matrix is *not* numerically PSD (smallest eigenvalue
#  is -0.1)
X2 = torch.linspace(lower, upper, 513)
covarX2 = covar_module(X2, X2).numpy()
print('Min eigenvalue with 513 points', np.linalg.eigvals(covarX2).min())
** Stack trace/error message **
The output that I get from the above code is:
&lt;denchmark-code&gt;Min eigenvalue with 512 points -1.1961828e-06
Min eigenvalue with 513 points -0.18964253
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

I would expect the minimum eigenvalue with 513 points to not be so negative.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

GPyTorch Version: 0.3.3
PyTorch Version: 1.1.0
Computer OS: Ubuntu 18.04

	</description>
	<comments>
		<comment id='1' author='wtstephe' date='2019-07-11T15:30:31Z'>
		This most likely has to do with the pairwise distance comparison, below size 512 this uses torch.cdist, and above a custom implementation: &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/kernel.py#L28-L29&gt;https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/kernel.py#L28-L29&lt;/denchmark-link&gt;

Let's see what's going on there...
		</comment>
		<comment id='2' author='wtstephe' date='2019-07-11T15:46:36Z'>
		This seems to be a numerical precision issue when using floating point:
&lt;denchmark-code&gt;def custom_cdist(x1, x2):
    x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)
    x1_pad = torch.ones_like(x1_norm)
    x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)
    x2_pad = torch.ones_like(x2_norm)
    x1_ = torch.cat([-2. * x1, x1_norm, x1_pad], dim=-1)
    x2_ = torch.cat([x2, x2_pad, x2_norm], dim=-1)
    res = x1_.matmul(x2_.transpose(-2, -1))
    res.clamp_min_(0)
    return res

X2 = torch.linspace(lower, upper, 513).unsqueeze(-1)
X2d = X2.double()

diff = torch.max(torch.abs(torch.cdist(X2, X2).pow(2) - custom_cdist(X2, X2)))
diff_d = torch.max(torch.abs(torch.cdist(X2d, X2d).pow(2) - custom_cdist(X2d, X2d)))
&lt;/denchmark-code&gt;

diff: 0.0304
diff_d: 0.0
If you run your example in double you get
&lt;denchmark-code&gt;Min eigenvalue with 512 points (-1.3547796728686771e-14+0j)
Min eigenvalue with 513 points -4.6523617484013634e-10`
&lt;/denchmark-code&gt;

This should of course work in float as well though...
&lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='wtstephe' date='2019-07-11T16:05:30Z'>
		The numerical instability is caused by the very large values going in to the distance function (e.g., changing lower and upper to 0 and 1 will result in an identical kernel matrix as having them as 500 and 501, but works.
Normally we rely on the user to do data normalization, but given that (1) it seems to outright fail when the feature values get this large and (2) the kernel matrix is identical, we could subtract the mean of the features like so:
diff = torch.max(torch.abs(torch.cdist(X2, X2).pow(2) - custom_cdist(X2 - X2.mean(-2, keepdim=True), X2 - X2.mean(-2, keepdim=True))))
Then diff == 0
		</comment>
	</comments>
</bug>