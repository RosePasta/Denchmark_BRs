<bug id='501' author='tadesautels' open_date='2019-02-06T21:00:37Z' closed_time='2019-02-07T20:31:09Z'>
	<summary>Grid kernel has trouble with 1D inputs</summary>
	<description>
I'm new to both PyTorch and GPyTorch, so please forgive me if I'm doing something silly.
The GridKernel seems to have problems with a 1-dimensional input, where 1-dimensional and 2-dimensional inputs are handled differently for some reason.  2- (and 3-) dimensional cases run without the error below.
Working example:
From the current master branch:
Modify the test_grid_gp_regression.py script to create a similar script, here called test_1D_grid_gp_regression.py, where the diff is this:
&lt;denchmark-code&gt;$ diff test_grid_gp_regression.py test_1D_grid_gp_regression.py
15c15
&lt;     train_y = torch.sin((train_x[:, 0] + train_x[:, 1]) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(0.01)
---
&gt;     train_y = torch.sin((train_x[:, 0]) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(0.01)
58c58
&lt;         grid_bounds = [(0, 1), (0, 2)]
---
&gt;         grid_bounds = [(0, 1)]
&lt;/denchmark-code&gt;

I get the following error when running the test:
&lt;denchmark-code&gt;$ python test_1D_grid_gp_regression.py 
E.
======================================================================
ERROR: test_grid_gp_mean_abs_error (__main__.TestGridGPRegression)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "test_1D_grid_gp_regression.py", line 87, in test_grid_gp_mean_abs_error
    loss.backward()
  File "/usr/local/lib/python3.6/dist-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
  File "/home/gpytorch/gpytorch/functions/_matmul.py", line 34, in backward
    rhs = self.saved_tensors[0]
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation

----------------------------------------------------------------------
Ran 2 tests in 0.088s

FAILED (errors=1)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='tadesautels' date='2019-02-07T04:45:18Z'>
		&lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 I looked in to this, and it seems to be an issue with the backwarding through the default  for LazyTensor. This error only happens when we use Toeplitz math, and only when we use the preconditioner (which calls the  that results in the call to the  function in the first place).
I think there are two separate things we should do:

Fix the issue with the default getitem if it isn't already fixed by the getitem refactor PR open (obviously)
Implement a custom _getitem for ToeplitzLazyTensor. I'm working on this now.

		</comment>
		<comment id='2' author='tadesautels' date='2019-02-07T19:42:36Z'>
		&lt;denchmark-link:https://github.com/tadesautels&gt;@tadesautels&lt;/denchmark-link&gt;
 - thanks for the very reproducible bug report! &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/508&gt;#508&lt;/denchmark-link&gt;
 fixes the issue
		</comment>
		<comment id='3' author='tadesautels' date='2019-02-08T22:55:29Z'>
		New (stable) release that fixes this issue
		</comment>
	</comments>
</bug>