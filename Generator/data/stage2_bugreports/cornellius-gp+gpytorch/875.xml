<bug id='875' author='thjashin' open_date='2019-09-17T11:55:26Z' closed_time='2019-09-21T10:36:20Z'>
	<summary>reproducing the svgp results in exact 1 million gp paper</summary>
	<description>
Hi,
I tried to reproduced the svgp results in the &lt;denchmark-link:https://arxiv.org/pdf/1903.08114.pdf&gt;exact 1 million gp paper&lt;/denchmark-link&gt;
 during which I found several things:


It seems that the dataset does not split into 2/3 training and 1/3 test as described because the number of training data in table 1 is not 2/3 of the whole dataset, e.g., for kin40k, 25,600 is not 2/3 of 40000. So what's the true proportion used?


I cannot reproduce the results for elevators dataset if I randomly shuffle the dataset. And I found that the code for elevators in this repo has a mistake in shuffling data:


&lt;denchmark-code&gt;data = torch.Tensor(loadmat('elevators.mat')['data'])
X = data[:, :-1]
y = data[:, -1]

N = data.shape[0]
np.random.seed(0)
data = data[np.random.permutation(np.arange(N)),:]
&lt;/denchmark-code&gt;

it splits into feature and labels before doing the shuffle which means that the data used is not shuffled. When not shuffled, the number seems to match the one reported in the paper.

It seems that the test rmse in the paper are computed without considering the normalization of the testing data. For which the true rmse should multiply the std used for normalization. Can you clarify if you directly compute the rmse based on the normalized data?

Since the exact gp on 1 million data points paper refers to this library for the source code. I'm raising this issue here. Hope you can help clarify.
	</description>
	<comments>
		<comment id='1' author='thjashin' date='2019-09-17T12:38:05Z'>
		Hi thanks for trying to reproduce our paper. We plan to release the code used for the experiments soon.
Regarding the number of training points: we used 20% for test, and 20% of the remaining for validation (i.e. 80% * 20% = 16%). This leaves 64% for training, which is roughly 2/3.
Regarding shuffling: thanks for raising this issue about the deep gp tutorial. I'll open a separate issue about it. This is indeed a mistake in the example. Fortunately, we do the shuffling correctly in our code for the paper. We do something like this:
&lt;denchmark-code&gt;    # shuffle
    N = data.shape[0]
    np.random.seed(seed)
    data = data[np.random.permutation(np.arange(N)), :]

    # make train/test split
    n_train_val = int(0.8 * N)
    n_train = int(0.8 * n_train_val)

    train_x, train_y = data[:n_train, :-1], data[:n_train, -1]
    val_x, val_y = data[n_train:n_train_val, :-1], data[n_train:n_train_val, -1]
    test_x, test_y = data[n_train_val:, :-1], data[n_train_val:, -1]
&lt;/denchmark-code&gt;

Regarding the RMSE: As mentioned in the paper, we whitened all our training and testing inputs and outputs to be 0 mean and unit variance, i.e.
&lt;denchmark-code&gt;    # normalize features
    mean = train_x.mean(dim=-2, keepdim=True)
    std = train_x.std(dim=-2, keepdim=True) + 1e-6
    train_x_normalized = (train_x - mean) / std
    val_x_normalized = (val_x - mean) / std
    test_x_normalized = (test_x - mean) / std

    # normalize labels
    mean, std = train_y.mean(), train_y.std() + 1e-6
    train_y_normalized = (train_y - mean) / std
    val_y_normalized = (val_y - mean) / std
    test_y_normalized = (test_y - mean) / std
&lt;/denchmark-code&gt;

Hence as you said, this means the RMSE is computed using the normalized targets.
(pred_y - test_y_normalized).pow(2).mean().sqrt()
This is equivalent to the square root of the standardized mean square error (SMSE) which gives a scale-free measure of error since the SMSE is equal to
(pred_y - test_y).pow(2).mean().div(train_y.var()).sqrt()
		</comment>
		<comment id='2' author='thjashin' date='2019-09-17T13:00:10Z'>
		Hi &lt;denchmark-link:https://github.com/KeAWang&gt;@KeAWang&lt;/denchmark-link&gt;
 ,
Thanks for clarifying. I'm wondering how the validation set is used? This seems not mentioned in the paper. Am I supposed to just using the 64% training set and the remainly 20% test set for reproducing the results?
		</comment>
		<comment id='3' author='thjashin' date='2019-09-17T13:19:50Z'>
		We used the validation set from some datasets to choose settings like the tolerance for conjugate gradients in our solves, the max number of conjugate gradient steps, etc. We ended up using the same settings across all our datasets. We also used the validation set to get validation accuracies periodically during long training runs.
Yes you should just use the 64% for train and 20% for test if you want to reproduce the results.
		</comment>
		<comment id='4' author='thjashin' date='2019-09-21T10:36:20Z'>
		&lt;denchmark-link:https://github.com/KeAWang&gt;@KeAWang&lt;/denchmark-link&gt;
 Thanks!
		</comment>
	</comments>
</bug>