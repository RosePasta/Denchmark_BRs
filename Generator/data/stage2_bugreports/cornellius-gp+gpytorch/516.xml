<bug id='516' author='Lazloo' open_date='2019-02-12T19:10:50Z' closed_time='2019-11-12T20:39:54Z'>
	<summary>SKI for multidemnsional output</summary>
	<description>
Hey,
I would like to apply SKI for a data set that is multi-dimensional w.r.t. input and output. However, I feel I stuck an some part. I tried to orintate my code as much as possible on 05_Scalable_GP_Regression_Multidimensional but without the Deep Learning part.
&lt;denchmark-h:h1&gt;My data toy set:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import math
import torch
import numpy as np
import gpytorch
from torch.utils.data import TensorDataset, DataLoader

n = 20
train_x = torch.zeros(pow(n, 2), 2)
for i in range(n):
    for j in range(n):
        # Each coordinate varies from 0 to 1 in n=100 steps
        train_x[i * n + j][0] = float(i) / (n - 1)
        train_x[i * n + j][1] = float(j) / (n - 1)

train_y_1 = (torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1]) * (2 * math.pi) + torch.randn_like(train_x[:, 0]).mul(
    0.01)) / 4
train_y_2 = torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1]) * (2 * math.pi) + torch.randn_like(train_x[:, 0]).mul(
    0.01)

train_y = torch.stack([train_y_1, train_y_2], -1)

test_x = torch.rand((n, len(train_x.shape)))
test_y_1 = (torch.sin(test_x[:, 0]) + torch.cos(test_x[:, 1]) * (2 * math.pi) + torch.randn_like(test_x[:, 0]).mul(
    0.01)) / 4
test_y_2 = torch.sin(test_x[:, 0]) + torch.cos(test_x[:, 1]) * (2 * math.pi) + torch.randn_like(test_x[:, 0]).mul(0.01)
test_y = torch.stack([test_y_1, test_y_2], -1)

train_x = train_x.unsqueeze(0).repeat(2, 1, 1)
train_y = train_y.transpose(-2, -1)
train_x = train_x.cuda()
train_y = train_y.cuda()

train_dataset = TensorDataset(train_x, train_y)
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h1&gt;Creating the GPRegression class&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;softplus = torch.functional.F.softplus
class GPRegressionLayer(gpytorch.models.AbstractVariationalGP):
    def __init__(self, grid_size=32, grid_bounds=[(-1, 1)] * 2):
        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(num_inducing_points=grid_size * grid_size)
        variational_strategy = gpytorch.variational.GridInterpolationVariationalStrategy(self,
                                                                    grid_size=grid_size,
                                                                    grid_bounds=grid_bounds,
                                                                    variational_distribution=variational_distribution)
        super(GPRegressionLayer, self).__init__(variational_strategy)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(param_transform=softplus), param_transform=softplus
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# num_features is the number of final features extracted by the neural network, in this case 2.
num_features = 2
model = GPRegressionLayer().cuda()
likelihood = gpytorch.likelihoods.GaussianLikelihood(param_transform=softplus, num_tasks=2).cuda()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h1&gt;Training&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;model.train()
likelihood.train()

optimizer = torch.optim.Adam([
    {'params': model.parameters()},
], lr=0.1)

mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0), combine_terms=False)

scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)
num_epochs = 7
for i in range(num_epochs):
    scheduler.step()
    # Within each iteration, we will go over each minibatch of data
    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):
        optimizer.zero_grad()

        with gpytorch.settings.use_toeplitz(False):
            output = model(x_batch)
            log_lik, kl_div, log_prior = mll(output, y_batch)
            loss = -(log_lik - kl_div + log_prior).sum()
            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader),
                                                                        loss.item(), log_lik.item(), kl_div.item(),
                                                                        log_prior.item()))

        # The actual optimization step
        loss.backward()
        optimizer.step()
&lt;/denchmark-code&gt;

Unfortunately this return an error:

File "D:/Programmieren/convolutional_gaussian_process_regression/test_SVG.py", line 111, in 
output = model(x_batch)
File "D:\ProgramData\Anaconda3\lib\site-packages\gpytorch\models\abstract_variational_gp.py", line 22, in call
return self.variational_strategy(inputs)
File "D:\ProgramData\Anaconda3\lib\site-packages\gpytorch\variational\variational_strategy.py", line 186, in call
return super(VariationalStrategy, self).call(x)
File "D:\ProgramData\Anaconda3\lib\site-packages\gpytorch\module.py", line 20, in call
outputs = self.forward(*inputs, **kwargs)
File "D:\ProgramData\Anaconda3\lib\site-packages\gpytorch\variational\grid_interpolation_variational_strategy.py", line 44, in forward
interp_indices, interp_values = self._compute_grid(x)
File "D:\ProgramData\Anaconda3\lib\site-packages\gpytorch\variational\grid_interpolation_variational_strategy.py", line 37, in _compute_grid
interp_indices, interp_values = Interpolation().interpolate(self.grid, inputs)
File "D:\ProgramData\Anaconda3\lib\site-packages\gpytorch\utils\interpolation.py", line 147, in interpolate
dim_interp_values = dim_interp_values.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)
RuntimeError: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor

	</description>
	<comments>
		<comment id='1' author='Lazloo' date='2019-02-12T21:36:05Z'>
		A few problems I see:
class GPRegressionLayer(gpytorch.module.AbstractVariationalGP):
should be gpytorch.models.AbstractVariationalGP.
train_dataset = TensorDataset(train_x, train_y)
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
Probably the biggest issue here, when you enumerate this DataLoader you'll get each batch at a time because a TensorData just iterates over the first dimension of the tensor. If train x is b x n x d then next on the DataLoader is going to get you like a 1 x n x d tensor instead of a b x batch_size x d tensor.
See the pytorch docs on data loaders &lt;denchmark-link:https://pytorch.org/docs/stable/data.html&gt;here&lt;/denchmark-link&gt;
. You might want to transpose  and  before creating the TensorDataset, and then transposing  and  back on the way out.
		</comment>
		<comment id='2' author='Lazloo' date='2019-02-13T07:22:42Z'>
		Hey &lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;
,
Regarding:
class GPRegressionLayer(gpytorch.module.AbstractVariationalGP):
Sorry my bad, I changed that in the primary code above.
Regarding the second point. I tried to force the correct dimension by using
output = model(x_batch.repeat(2, 1, 1))
Unfortunately, that reveals the same error.
		</comment>
		<comment id='3' author='Lazloo' date='2019-11-12T20:39:53Z'>
		I believe this issue is going to be moot with the latest code in &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/903&gt;#903&lt;/denchmark-link&gt;
. Reopen if there's still issues.
		</comment>
	</comments>
</bug>