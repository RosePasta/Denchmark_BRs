<bug id='575' author='Duane321' open_date='2019-03-16T18:52:28Z' closed_time='2019-03-18T23:17:05Z'>
	<summary>linear kernel breaks when given certain types of active_dims</summary>
	<description>
Hey,
So the linear kernel will break with some types of given active_dims arguments. I'll lay those out here.
&lt;denchmark-code&gt;import numpy as np
import gpytorch
import torch
torch.set_default_tensor_type('torch.DoubleTensor')

X = np.array([range(100*10)]).reshape(-1,10)
X = torch.from_numpy(X).to(torch.float64)
y = torch.from_numpy(np.random.normal(size=100)).to(torch.float64)
&lt;/denchmark-code&gt;

Below, I've commented out some active_dims which either work or don't work.
&lt;denchmark-code&gt;class GP(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(GP, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
#         active_dims = list(range(1,10)) # Doesn't work!
        active_dims = list(range(3,10)) # Doesn't work!
#         active_dims = [0] + list(range(3,10)) # Doesn't work!
#         active_dims = [0] + list(range(2,10)) # Doesn't work!
#         active_dims = list(range(1)) # Works
#         active_dims = list(range(2)) # Works
#         active_dims = list(range(10)) # Works
        self.linear_kernel = gpytorch.kernels.LinearKernel(num_dimensions=len(active_dims),
                                                           active_dims=active_dims)
            
    def forward(self, x, requested_comp=None):
        
        mean_x = self.mean_module(x)
        linear_kernel_x = self.linear_kernel(x)
        covar_x = linear_kernel_x
        
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
&lt;/denchmark-code&gt;

Before I get to the error, we set up a few things:
&lt;denchmark-code&gt;likelihood = gpytorch.likelihoods.GaussianLikelihood()
likelihood.initialize(noise=8e-4) 
model = GP(X, y, likelihood)

learning_rate = .1
model.train()
likelihood.train()

optimizer = torch.optim.Adam([
    {'params': list(model.parameters())[2:]},
], lr=learning_rate)

mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
&lt;/denchmark-code&gt;

When we get into the optimization, it breaks:
&lt;denchmark-code&gt;for i in range(100):
    optimizer.zero_grad()
    output = model(X)
    ### Breaks on the next line! ###
    loss = -mll(output, y)
    loss.backward()
    if not i % 30:
        print('i: {0}, Loss: {1}'.format(i, loss.item()))
    optimizer.step()
&lt;/denchmark-code&gt;

With this error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-67-cb234b5fe124&gt; in &lt;module&gt;
     15     output = model(X)
     16     # Calc loss and backprop gradients
---&gt; 17     loss = -mll(output, y)
     18     loss.backward()
     19     if not i % 30:

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     18 
     19     def __call__(self, *inputs, **kwargs):
---&gt; 20         outputs = self.forward(*inputs, **kwargs)
     21         if isinstance(outputs, list):
     22             return [_validate_module_outputs(output) for output in outputs]

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py in forward(self, output, target, *params)
     26         # Get the log prob of the marginal distribution
     27         output = self.likelihood(output, *params)
---&gt; 28         res = output.log_prob(target)
     29 
     30         # Add terms for SGPR / when inducing points are learned

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/distributions/multivariate_normal.py in log_prob(self, value)
    123 
    124         # Get log determininat and first part of quadratic form
--&gt; 125         inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)
    126 
    127         res = -0.5 * sum([inv_quad, logdet, diff.size(-1) * math.log(2 * math.pi)])

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py in inv_quad_logdet(self, inv_quad_rhs, logdet, reduce_inv_quad)
    745                 )
    746 
--&gt; 747         args = self.representation()
    748         if inv_quad_rhs is not None:
    749             args = [inv_quad_rhs] + list(args)

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py in representation(self)
    980                 representation.append(arg)
    981             elif hasattr(arg, "representation") and callable(arg.representation):  # Is it a LazyTensor?
--&gt; 982                 representation += list(arg.representation())
    983             else:
    984                 raise RuntimeError("Representation of a LazyTensor should consist only of Tensors")

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in representation(self)
    306         # representation
    307         else:
--&gt; 308             return self.evaluate_kernel().representation()
    309 
    310     def representation_tree(self):

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/utils/memoize.py in g(self, *args, **kwargs)
     15         cache_name = name if name is not None else method
     16         if cache_name not in self._memoize_cache:
---&gt; 17             self._memoize_cache[cache_name] = method(self, *args, **kwargs)
     18         return self._memoize_cache[cache_name]
     19 

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in evaluate_kernel(self)
    260         with settings.lazily_evaluate_kernels(False):
    261             res = self.kernel(
--&gt; 262                 x1, x2, diag=False, batch_dims=self.batch_dims, **self.params
    263             )
    264         if self.squeeze_row:

~/.virtualenvs/GenRS/lib/python3.7/site-packages/gpytorch/kernels/kernel.py in __call__(self, x1, x2, diag, batch_dims, **params)
    313         # Select the active dimensions
    314         if self.active_dims is not None:
--&gt; 315             x1_ = x1_.index_select(-1, self.active_dims)
    316             if x2_ is not None:
    317                 x2_ = x2_.index_select(-1, self.active_dims)

RuntimeError: invalid argument 3: out of range at /Users/soumith/b101/2019_02_04/wheel_build_dirs/wheel_3.7/pytorch/aten/src/TH/generic/THTensor.cpp:350
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Duane321' date='2019-03-16T18:54:27Z'>
		&lt;denchmark-link:https://github.com/Duane321&gt;@Duane321&lt;/denchmark-link&gt;
 Have you pulled recently (specifically, since Thursdsay)? I very recently fixed a bug in  where we weren't transforming the variance parameter to be positive, which could cause learning to make it not psd anymore.
If this is happening on like today's master let me know and I'll take a look.
		</comment>
		<comment id='2' author='Duane321' date='2019-03-16T19:00:18Z'>
		Right before I did this, I uninstalled and re-installed the latest (unstable) version. So I think it might be a fresh issue. Unless, should I not be using the unstable version?
		</comment>
	</comments>
</bug>