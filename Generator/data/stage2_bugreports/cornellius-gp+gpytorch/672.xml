<bug id='672' author='Sabina321' open_date='2019-04-30T16:27:22Z' closed_time='2019-11-12T20:31:30Z'>
	<summary>[Question]</summary>
	<description>
Hi,
When optimizing the custom kernel mentioned in  an earlier post, I am having some stability issues. These issues don't seem to come up when using GPy. However, here, I often get an error after only a few iterations of training. When I reduce the learning rate I can get rid of the error but then the parameters are not really learned, the final values are very close to the initialized values.  Also, if instead of placing constraints on the parameters I put priors on them, training can run for more iterations. Are there any tips to make the process more stable?
	</description>
	<comments>
		<comment id='1' author='Sabina321' date='2019-05-01T15:39:34Z'>
		Can you be more specific than stability issues? How many examples are you training with? Are you initializing the parameters in any way (e.g., our default initial parameters differ from GPy)?
If you run all of your code in a with gpytorch.settings.fast_computations(log_prob=False, solves=False): context, does the problem go away?
We'll definitely need a bit more information than this to be of help. It could also be a small bug in your kernel implementation leading to it being not positive definite for example, which when using CG could easily show up as "stability" issues instead of crashing.
		</comment>
		<comment id='2' author='Sabina321' date='2019-05-02T13:45:33Z'>
		Thanks for the response! I'll try to clarify, part of the problem is that I'm not exactly sure what the problem is.
In both packages I set the initial parameters and I set them to the same values. The number of examples is 5278.
Running all of the code with that setting does not make the problem go away. It does however give a better error message:
Lapack Error syev : 2 off-diagonal elements didn't converge to zero at /Users/soumith/mc3build/conda-bld/pytorch_1549593514549/work/aten/src/TH/generic/THTensorLapack.cpp:383
If I add constraints in, instead of just using the priors, the optimization runs for fewer iterations and the error changes to:
forward(ctx, representation_tree, dtype, device, matrix_shape, batch_shape, inv_quad, logdet, probe_vectors, probe_vector_norms, *args)
61         lazy_tsr = ctx.representation_tree(*matrix_args)
62         with torch.no_grad():
---&gt; 63             preconditioner, precond_lt, logdet_correction = lazy_tsr.detach()._preconditioner()
64
65         ctx.preconditioner = preconditioner
ValueError: not enough values to unpack (expected 3, got 2)
I don’t think there are any issues with the kernel, because I’ve tested that for one clean test dataset I get the same learned parameter values in GPy and R and in GPytorch. However, when I go to this slightly messier dataset, gpytorch fails after only a few iterations, unless I make the learning rate really slow. But if I make the learning rate really slow, then the final values are hardly different than the initial ones. I don’t get this problem in GPy. It is much slower, but the learned values are much better. I think GPytorch should get to these values, but it seems to quickly get to an unstable region and can’t backtrack out of it.
For example, in the kernel below. Even when the parameter u1 is constrained to be positive, after a few iterations optimization fails and it looks like the value is negative. If I look at the eigenvalues during optimization, it’s clear that they have become negative and a few iterations later optimization crashes. If I monitor everything and stop optimizing once the eigenvalues become small but are still positive, then I can avoid the errors, but I often only run for a few iterations and am not really learning anything. Again, even though it is too slow to be practical GPy doesn’t have these issues. I’ve tried both the Adam and LBFGS optimizers and the problem persists in each.
&lt;denchmark-code&gt;class MyKernel(Kernel):
    def __init__(self, num_dimensions,user_mat, first_mat, variance_prior=None, offset_prior=None, active_dims=None):
        super(MyKernel, self).__init__(active_dims=active_dims)
        self.user_mat = user_mat
        self.first_mat = first_mat
        self.noise_mat = torch.eye(5278)
    
        self.register_parameter(name="u1", parameter=torch.nn.Parameter(1.0*torch.tensor(1.0)))
        self.register_parameter(name="raw_u1", parameter=torch.nn.Parameter(1.0*torch.tensor(1.0)))
   
        self.register_parameter(name="u2", parameter=torch.nn.Parameter(1.0*torch.tensor(1.0)))
        self.register_parameter(name="raw_u2", parameter=torch.nn.Parameter(1.0*torch.tensor(1.0)))
    
        self.register_parameter(name="rho", parameter=torch.nn.Parameter(2.0*torch.tensor(1.0)))
        self.register_parameter(name="raw_rho", parameter=torch.nn.Parameter(2.0*torch.tensor(1.0)))
    
    

        self.register_constraint("raw_u1",constraint= constraints.Positive())
        self.register_constraint("raw_u2",constraint= constraints.Positive())
    
        self.register_constraint("raw_rho",constraint= constraints.Interval(0,2))
    
        self.register_prior("u1_prior", gpytorch.priors.SmoothedBoxPrior(a=0,b=3,sigma=.5), "u1")
        self.register_prior("u2_prior", gpytorch.priors.SmoothedBoxPrior(a=0,b=3,sigma=.5), "u2")
        self.register_prior("rho_prior", gpytorch.priors.SmoothedBoxPrior(a=0,b=2,sigma=.5), "rho")

    def forward(self, x1, x2, batch_dims=None, **params):

   
        x1_ = torch.stack((x1[:,0],x1[:,5]),dim=1)
        x2_ = torch.stack((x2[:,0],x2[:,5]),dim=1)
 
   
        prod = MatmulLazyTensor(x1_[:,0:1], x2_[:,0:1].transpose(-1, -2))
    
    
        tone = prod * (self.u1)
   
    
        prod = MatmulLazyTensor(x1_[:,1:2], x2_[:,1:2].transpose(-1, -2))

        ttwo = prod * (self.u2)

    
        diagone = MatmulLazyTensor(x1_[:,0:1], x2_[:,1:2].transpose(-1, -2))
    

        diagtwo = MatmulLazyTensor(x1_[:,1:2], x2_[:,0:1].transpose(-1, -2))

         tthree = (diagone+diagtwo)*((self.rho-1)*(self.u1)**.5*(self.u2)**.5)
   
        random_effects = tone+ttwo+tthree
        final = random_effects*self.user_mat
    
            
        final = final+self.first_mat

        return final  
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Sabina321' date='2019-05-02T15:50:11Z'>
		In your kernel definition you aren't correctly dealing with the fact that some of those parameters need to be positive. For each parameter you should define a raw version and register a constraint, then a property that transforms the raw version with the constraint. When you register a parameter u1 and then use self.u1 it just gets an unconstrained value. You're ending up with negative parameter values.
Please take a look at, for example, gpytorch.kernels.scale_kernel.py to see how we handle the outputscale. You register one raw parameter, one constraint, and define both a getter and setter method.
		</comment>
		<comment id='4' author='Sabina321' date='2019-11-12T20:31:30Z'>
		closing for now - reopen if there's still questions :)
		</comment>
	</comments>
</bug>