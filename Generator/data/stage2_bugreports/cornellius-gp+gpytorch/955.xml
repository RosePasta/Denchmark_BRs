<bug id='955' author='jeffwillette' open_date='2019-11-19T01:36:38Z' closed_time='2019-11-29T16:56:01Z'>
	<summary>[Bug] Invalid index in gather during call to mll in training loop</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

I followed the docs related to DKL exactly and got the following error during the training loop when calculating the loss with the marginal likelihood function
** Stack trace/error message **
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "gp.py", line 24, in &lt;module&gt;
    exact.train()
  File "/st2/jeff/real_estate/models/gaussian_processes/exact.py", line 102, in train
    loss = -mll(output, train_y).sum()
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/module.py", line 22, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py", line 27, in forward
    res = output.log_prob(target)
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/distributions/multivariate_normal.py", line 128, in log_prob
    inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/lazy/batch_repeat_lazy_tensor.py", line 242, in inv_quad_logdet
    inv_quad_rhs, logdet, reduce_inv_quad=False
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py", line 1052, in inv_quad_logdet
    *args,
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/functions/_inv_quad_log_det.py", line 63, in forward
    preconditioner, precond_lt, logdet_correction = lazy_tsr._preconditioner()
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/lazy/added_diag_lazy_tensor.py", line 59, in _preconditioner
    self._piv_chol_self = pivoted_cholesky.pivoted_cholesky(self._lazy_tensor, max_iter)
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/utils/pivoted_cholesky.py", line 19, in pivoted_cholesky
    matrix_diag = matrix._approx_diag()
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/lazy/interpolated_lazy_tensor.py", line 90, in _approx_diag
    left_res = left_interp(self.left_interp_indices, self.left_interp_values, base_diag_root.unsqueeze(-1))
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch/utils/interpolation.py", line 187, in left_interp
    res = rhs_expanded.gather(-3, interp_indices_expanded).mul(interp_values_expanded)
RuntimeError: Invalid index in gather at /tmp/pip-req-build-58y_cjjl/aten/src/TH/generic/THTensorEvenMoreMath.cpp:472
loss: 57158.71 med: 0.30, minmax: 0.30 0.30 noise: 0.56: : 0it [00:08, ?it/s]
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

I am left unsure of what is causing the error and how to go about fixing it because it is initially successful in iterating and calculating the loss and then it crashes. The sizing of tensors must be correct, but there is must be some numerical instability and I am unsure about where to look for it.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:
gpytorch version: 0.3.6
torch version: `1.2.0
Ubuntu 18.04
	</description>
	<comments>
		<comment id='1' author='jeffwillette' date='2019-11-20T00:39:08Z'>
		@deltaskelta can you provide a more complete example? I don't know how to reproduce.
Can you also try on the latest version of gpytorch (off of the master branch on Github) and PyTorch 1.3.1?
		</comment>
		<comment id='2' author='jeffwillette' date='2019-11-20T12:55:28Z'>
		&lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 thanks for your response. I installed the most recent pytorch and gpytorch from the master branch and I still had the issue. I had time to create a small reproduction repo (&lt;denchmark-link:https://github.com/deltaskelta/gpytorch-955&gt;https://github.com/deltaskelta/gpytorch-955&lt;/denchmark-link&gt;
) with a minimal dataset. You can run it by doing...
&lt;denchmark-code&gt;python dkl.py
&lt;/denchmark-code&gt;

I printed the input tensor to make sure no weird values were showing up. If you let it run for a minute or so it should fail with the above error very quickly
		</comment>
		<comment id='3' author='jeffwillette' date='2019-11-25T17:13:30Z'>
		Thanks so much for the repo! I've reproduced the error - and will look into it.
		</comment>
		<comment id='4' author='jeffwillette' date='2019-11-25T17:27:45Z'>
		@deltaskelta - it looks like these NaNs came from a divide-by-zero error that &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/971&gt;#971&lt;/denchmark-link&gt;
 addresses. However, this error is mostly stemming from the neural network outputs collapsing to a single point, which will cause lots of other learning issues for the GP.
To solve that larger issue, I would either

Pre-train the neural network without a GP for a few iterations. This is especially useful for large NNs, like the one in your example.
Use batch normalization
Make sure that the outputs are scaled to be zero mean + unit variance.

		</comment>
		<comment id='5' author='jeffwillette' date='2019-12-26T02:10:17Z'>
		Hi,
I also received the same error in a GP for multiclass classification like the one in &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1003&gt;#1003&lt;/denchmark-link&gt;
 . I am using  and .
Fortunately it went away after normalizing the inputs properly.
		</comment>
	</comments>
</bug>