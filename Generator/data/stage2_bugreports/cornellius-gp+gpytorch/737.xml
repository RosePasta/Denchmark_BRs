<bug id='737' author='tzoiker' open_date='2019-06-16T21:52:53Z' closed_time='2019-11-12T20:23:38Z'>
	<summary>SpectralMixtureKernel initialize_from_data CUDA bug</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

When running  with CUDA tensors, in &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/spectral_mixture_kernel.py#L158&gt;this&lt;/denchmark-link&gt;
 line zero tensor is initialized without input device consideration, so that code fails later.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

import torch, gpytorch
train_x = torch.tensor([1.0,2.0]).cuda()
train_y = torch.ones(2).cuda()
kernel = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4).cuda()
kernel.initialize_from_data(train_x, train_y)
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-1-419d92435b05&gt; in &lt;module&gt;
      3 train_y = torch.ones(2).cuda()
      4 kernel = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4).cuda()
----&gt; 5 kernel.initialize_from_data(train_x, train_y)

~/storage/miniconda2/envs/water/lib/python3.6/site-packages/gpytorch/kernels/spectral_mixture_kernel.py in initialize_from_data(self, train_x, train_y, **kwargs)
    166         )
    167         # Draw means from Unif(0, 0.5 / minimum distance between two points)
--&gt; 168         self.raw_mixture_means.data.uniform_().mul_(0.5).div_(min_dist)
    169         self.raw_mixture_means.data = self.raw_mixture_means_constraint.inverse_transform(self.raw_mixture_means.data)
    170         # Mixture weights should be roughly the stdv of the y values divided by the number of mixtures

RuntimeError: expected backend CUDA and dtype Float but got backend CPU and dtype Float
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

I expected it to get device info from training data.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

GPyTorch 0.3.2
PyTorch 1.1.0
MacOS 10.14

	</description>
	<comments>
		<comment id='1' author='tzoiker' date='2019-06-16T22:03:32Z'>
		You need to cast your kernel to the GPU with a .cuda() call too.
		</comment>
		<comment id='2' author='tzoiker' date='2019-06-16T22:12:17Z'>
		
You need to cast your kernel to the GPU with a .cuda() call too.

&lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;
, I had mistakes in my comment, but the original source of error didn't get anywhere anyway. Check updated snippet and, please, reopen.
		</comment>
		<comment id='3' author='tzoiker' date='2019-07-30T18:18:33Z'>
		I can confirm I'm getting the same error. I can't seem to get the Spectral Mixture Model processing via CUDA in the same way many of the other gpytorch examples are laid out. I get exactly the same error at initialization of the kernel, as shown by &lt;denchmark-link:https://github.com/tzoiker&gt;@tzoiker&lt;/denchmark-link&gt;
 above. Attempting to cast the kernel to  does not get rid of the error. Perhaps our syntax is wrong?
To reproduce
n = 40
train_x = torch.zeros(pow(n, 2), 2)
for i in range(n):
    for j in range(n):
        train_x[i * n + j][0] = float(i) / (n-1)
        train_x[i * n + j][1] = float(j) / (n-1)
# True function is sin( 2*pi*(x0+x1))
train_x = train_x.float().cuda()
train_y = torch.sin((train_x[:, 0] + train_x[:, 1]) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(0.01)
train_y = train_y.float().cuda()

class SpectralMixtureGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(SpectralMixtureGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4,ard_num_dims=2).cuda()
        self.covar_module.initialize_from_data(train_x, train_y)

    def forward(self,x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = SpectralMixtureGPModel(train_x, train_y, likelihood).cuda()
		</comment>
		<comment id='4' author='tzoiker' date='2019-07-30T18:52:34Z'>
		I have come up with a solution that seems to be working, but it seems to require a different convention of where to cast to .cuda() andcpu() compared to other examples. For example, the implementation below is a variation on the gptorch KISS-GP multidimensional GP regression examples.
Here's what I've implemented for the above solution
Training data initialization:
Note: Training data is not cast to .cuda()
n = 40
train_x = torch.zeros(pow(n, 2), 2)
for i in range(n):
    for j in range(n):
        train_x[i * n + j][0] = float(i) / (n-1)
        train_x[i * n + j][1] = float(j) / (n-1)
# True function is sin( 2*pi*(x0+x1))
train_x = train_x.float()
train_y = torch.sin((train_x[:, 0] + train_x[:, 1]) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(0.01)
train_y = train_y.float()
Establish model
Note: Only the model is cast to .cuda()
class SpectralMixtureGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(SpectralMixtureGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4,ard_num_dims=2)
        self.covar_module.initialize_from_data(train_x, train_y)

    def forward(self,x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = SpectralMixtureGPModel(train_x, train_y, likelihood).cuda()
Train model
Note: Now that the model is casted to .cuda(), but the training data is not, we cast train_x and train_y to .cuda() where they are called in-situ:
# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

training_iter = 100
for i in range(training_iter):
    optimizer.zero_grad()
    output = model(train_x.cuda())
    loss = -mll(output, train_y.cuda())
    loss.backward()
    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))
    optimizer.step()
Test model
Note: text_x is cast to .cuda() in  order to generate  samples, pred_labels, but these  are  then cast back to .cpu() so that the rest of the analysis is handled without issue.
# Set model and likelihood into evaluation mode
model.eval()
likelihood.eval()

# Generate nxn grid of test points spaced on a grid of size 1/(n-1) in [0,1]x[0,1]
n = 10
test_x = torch.zeros(int(pow(n, 2)), 2)
for i in range(n):
    for j in range(n):
        test_x[i * n + j][0] = float(i) / (n-1)
        test_x[i * n + j][1] = float(j) / (n-1)

with torch.no_grad(), gpytorch.settings.fast_pred_var():
    observed_pred = likelihood(model(test_x.cuda()))
    pred_labels = (observed_pred.mean.view(n, n)).cpu()

# Calc abosolute error
test_y_actual = torch.sin(((test_x[:, 0] + test_x[:, 1]) * (2 * math.pi))).view(n, n)
delta_y = torch.abs(pred_labels - test_y_actual).detach().numpy()

# Define a plotting function
def ax_plot(f, ax, y_labels, title):
    im = ax.imshow(y_labels)
    ax.set_title(title)
    f.colorbar(im)

# Plot our predictive means
f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))
ax_plot(f, observed_ax, pred_labels, 'Predicted Values (Likelihood)')

# Plot the true values
f, observed_ax2 = plt.subplots(1, 1, figsize=(4, 3))
ax_plot(f, observed_ax2, test_y_actual, 'Actual Values (Likelihood)')

# Plot the absolute errors
f, observed_ax3 = plt.subplots(1, 1, figsize=(4, 3))
ax_plot(f, observed_ax3, delta_y, 'Absolute Error Surface')
This solution works for me (and is lightning fast!), but is it right?
		</comment>
		<comment id='5' author='tzoiker' date='2019-07-30T19:00:34Z'>
		&lt;denchmark-link:https://github.com/tzoiker&gt;@tzoiker&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/adam-rysanek&gt;@adam-rysanek&lt;/denchmark-link&gt;
 Sorry this took an embarrassingly long time to fix for how simple it was.
&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/85a000ba23bcc35b38c3e727df5c94607a483517&gt;85a000b&lt;/denchmark-link&gt;
 should resolve the issue internally.
		</comment>
		<comment id='6' author='tzoiker' date='2019-11-12T20:23:38Z'>
		looks like this should've been closed with &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/85a000ba23bcc35b38c3e727df5c94607a483517&gt;85a000b&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>