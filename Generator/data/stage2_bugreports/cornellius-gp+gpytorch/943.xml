<bug id='943' author='jeffwillette' open_date='2019-11-14T04:24:31Z' closed_time='2019-11-16T08:55:38Z'>
	<summary>[Bug] cholesky singular U</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

I followed the directions here for a variational GP (&lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/05_Scalable_GP_Regression_Multidimensional/SVGP_Regression_CUDA.html#Overview&gt;https://gpytorch.readthedocs.io/en/latest/examples/05_Scalable_GP_Regression_Multidimensional/SVGP_Regression_CUDA.html#Overview&lt;/denchmark-link&gt;
) with the HEAD build of gpytorch and it trains for a while and then crashes with...
** Stack trace/error message **
&lt;denchmark-code&gt;  File "main.py", line 93, in &lt;module&gt;
    pred = model(x)‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 999/1088 [00:42&lt;00:02, 31.22it/s]
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/models/approximate_gp.py", line 56, in __call__
    return self.variational_strategy(inputs, prior=prior)
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/variational/whitened_variational_strategy.py", line 240, in __call__
    return Module.__call__(self, x)
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/module.py", line 24, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/variational/whitened_variational_strategy.py", line 141, in forward
    induc_induc_covar = CholLazyTensor(induc_induc_covar.cholesky())                                                                
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/lazy/lazy_tensor.py", line 738, in cholesky
    res = self._cholesky()
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/utils/memoize.py", line 34, in g
    add_to_cache(self, cache_name, method(self, *args, **kwargs))
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/lazy/lazy_tensor.py", line 413, in _cholesky
    cholesky = psd_safe_cholesky(evaluated_mat).contiguous()
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/utils/cholesky.py", line 39, in psd_safe_cholesky
    raise e
  File "/st2/jeff/anaconda3/envs/jeff/lib/python3.7/site-packages/gpytorch-0.3.6-py3.7.egg/gpytorch/utils/cholesky.py", line 22, in psd_safe_cholesky
    L = torch.cholesky(A, upper=upper, out=out)
RuntimeError: cholesky_cpu: U(1,1) is zero, singular U.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

I'm not sure why it is running into a singular matrix there, as I would expect that case to be covered.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


0.3.6 - &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/cd1c8f8801e30219e034a735340f3e6cce2b83a8&gt;cd1c8f8&lt;/denchmark-link&gt;

1.2.0
Ubuntu
&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Additionally, I get a cuda OOM error every time I try to call .to(device) using gpytorch, even with minimal data, which is strange.
	</description>
	<comments>
		<comment id='1' author='jeffwillette' date='2019-11-14T05:53:52Z'>
		Unfortunately, pytorch's cholesky will error out with a singular warning even if the real cause are NaN values. Can you check whether the matrix does have NaN entries or whether it's in fact singular / not positive definite?
		</comment>
		<comment id='2' author='jeffwillette' date='2019-11-14T15:02:02Z'>
		@deltaskelta - this may or may not correct the issue, but can you replace WhitenedVariationalStrategy with VariationalStrategy? The examples for variational inference need some updating (to come soon).
Also &lt;denchmark-link:https://github.com/Balandat&gt;@Balandat&lt;/denchmark-link&gt;
 I'm putting up a quick PR to give better NaN warnings in .
		</comment>
		<comment id='3' author='jeffwillette' date='2019-11-15T04:13:03Z'>
		I was able to verify that the A going into the psd_safe_cholesky function is singular. It was extremely sparse with only a few values. I will try to investigate further but in the meantime, do you have any idea what would make this matrix so sparse?
Should it always be pretty dense? It looks like the function tries to make it invertible and generates a warning if it succeeded (&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/utils/cholesky.py#L43&gt;https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/utils/cholesky.py#L43&lt;/denchmark-link&gt;
)
The data I am using came from a neural network project that had some one hot encodings, do you think this is what is making the A so sparse? Should I be doing that differently with a gpytorch?
Edit: the above comment wasnt showing when i posted this. Ill try it out and report back
		</comment>
		<comment id='4' author='jeffwillette' date='2019-11-15T17:51:15Z'>
		@deltaskelta - when you say that A is sparse, is it dominated by the main diagonal? (E.g. does it essentially become the identity matrix)? Typically, when a kernel matrix becomes sparse it is because the lengthscale is very small, but this typically results in a very well conditioned matrix.
This shouldn't be an issue with one-hot encodings. The matrix A is a kernel matrix, and this would only become sparse when entries are very far apart from one another in euclidean space (assuming you're using an RBF/Matern kernel).
My guess as to what's happening: the learning dynamics are probably a bit too aggressive, and two of the inducing points are becoming very identical. This is the only case that I can imagine when the kernel matrix would become sparse but singular.
Can you try a few things:

Switching to VariationalStrategy and not using WhitenedVariationalStrategy?
Normalizing the outputs of the neural network before you supply them as inputs to the GP? One way to accomplish this is to make the last layer of your neural network a nn.BatchNorm1d layer with affine=False.

		</comment>
		<comment id='5' author='jeffwillette' date='2019-11-15T18:59:22Z'>
		@deltaskelta Are you normalizing your data? If your data has some sparse components, it sounds like you might not be. This can lead to trouble, because GPyTorch's default hyperparameter values for things like the lengthscale really assume you've normalized your data either to [0, 1] or z-scored it in some fashion so that distances aren't too different from 1 on average.
		</comment>
		<comment id='6' author='jeffwillette' date='2019-11-16T05:12:26Z'>
		&lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 it was sparse as in almost all zeros, not dominated by the diagonal
The input data did have some very large values as well as mostly sparse one hot categorical encodings. I normalized the large values to be in the range [0, 1] as suggested and the singular error is now gone. Thank you everyone for the help. I just have a few questions that remain because I would like to incorporate GP's into my research but I am getting confused about how to best use this library.


Are one hot encodings ok for inputs to GP's in gpytorch? I read a paper (https://arxiv.org/abs/1805.03463) that implied they were not ok in the case of a regular GP.


Should my regression targets also be normalized? I am trying to predict some very large values (in the range of e^12 - e^16) and when I use both the variational GP and the exact GP, my error decreases on every iteration, but my median prediction starts near 0 and only goes up by decimal values on every iteration so it would take forever to reach e^16.


How does the learning rate of GP's relate to that of neural networks? I see in the docs site that the learning rates are usually set to something like 0.1 which is already a lot higher than the normal 0.001-ish numbers that NN's generally recommend.


Can I use a pytorch Dataloader and mini batch train the GP or should it be accepting the whole dataset on every iteration? The tutorials show loading the whole set on every iteration but ID if that is required.


Thanks!
		</comment>
		<comment id='7' author='jeffwillette' date='2019-11-16T08:53:14Z'>
		@deltaskelta


Well, one hot encodings can be "bad" for some kernels in the sense that a lot of common kernels are functions of distances. The RBF kernel in particular is a product of functions of distances in each dimension, e.g. k(x, z) = \prod_i f(||[x]_i - [z]_i||). Distances aren't a great way of capturing one hot features necessarily because there are only two possible distances in each column.


Yeah I would normalize your targets in that case. What's more, since your targets span multiple orders of magnitude, I might even recommend trying to fit log y rather than just y -- presumably you care more about having the right order of magnitude.


Well, exact GPs at least are trained in batch mode and (without a feature extractor) only have 3 parameters, so large learning rates are fine. With a NN in the mix, you might want to use a smaller LR or even a different one for the NN and GP (see PyTorch tutorials on parameter groups).


You can use minibatch training if you use variational inference. We have a few instantiations of this implemented out of the box: SVGP+DKL, SV-DKL, and predictive LL GPs. The example notebooks for these are kind of spread all over atm, but I think we will be reorganizing them soon.


		</comment>
		<comment id='8' author='jeffwillette' date='2019-11-16T08:55:38Z'>
		Thanks for the info!
		</comment>
	</comments>
</bug>