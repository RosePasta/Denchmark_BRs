<bug id='1333' author='wjmaddox' open_date='2020-11-06T21:17:11Z' closed_time='2020-11-09T16:05:32Z'>
	<summary>Non-Square Hadamard Multiplication Fails [Bug]</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

Looks like hadamard matrix multiplication is calling a root decomposition which is not well-defined for non-square lazy tensors. The fix is to check to see if the two matrices are non-square here.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
import torch
import gpytorch

a = gpytorch.lazify(torch.randn(30, 10))
b = gpytorch.lazify(torch.randn(10, 10))

ab = a.matmul(b)
# is not a matmul lazy tensor

(ab * ab) # fails

(ab.evaluate() * ab.evaluate()) # does not fail
** Stack trace/error message **
&lt;denchmark-code&gt;~/Documents/GitHub/gpytorch/gpytorch/lazy/lazy_tensor.py in _mul_matrix(self, other)
    508             left_lazy_tensor = self if self._root_decomposition_size() &lt; other._root_decomposition_size() else other
    509             right_lazy_tensor = other if left_lazy_tensor is self else self
--&gt; 510             return MulLazyTensor(left_lazy_tensor.root_decomposition(), right_lazy_tensor.root_decomposition())

RuntimeError: root_decomposition only operates on (batches of) square (symmetric) LazyTensors. Got a MatmulLazyTensor of size torch.Size([30, 10]).
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Should return a Hadamard? or matmul lazy tensor rather than failing.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

gpytorch 1.2.0
torch 1.7.0
mac os

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I'd like to circumvent passing ab into a RootLazyTensor to get the diagonal out and so would like to directly do the Hadamard product here and reduce. (e.g. RootLazyTensor(ab).diag()).
	</description>
	<comments>
		<comment id='1' author='wjmaddox' date='2020-11-08T23:07:32Z'>
		&lt;denchmark-link:https://github.com/wjmaddox&gt;@wjmaddox&lt;/denchmark-link&gt;
 So, this is kind of intended.  was always really intended to be , and it's only recently with the addition of  that we've really even considered matrices that arent spd.
This assumption is kind of littered throughout the package, and this is an example of it. Another example is that MatmulLazyTensor doesn't override _solve, and therefore will obviously return wrong results on non spd matrices. Realistically, MatmulLazyTensor(AB)._solve should do like B._solve(A._solve(...)) or something.
		</comment>
		<comment id='2' author='wjmaddox' date='2020-11-09T16:05:32Z'>
		Ahh, that would make sense. I'll close because of the easy workaround but separating out generic lazy tensors from PSD lazy tensors is probably something that would be useful in the future.
		</comment>
	</comments>
</bug>