<bug id='784' author='danielecc' open_date='2019-07-13T09:13:26Z' closed_time='2019-07-13T09:53:21Z'>
	<summary>[Bug] get_fantasy_model does not work for SGPR with InducingPointKernel</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

Not sure if this should be considered a bug or a feature request, but gpytorch's implementation of SGPR using the InducingPointKernel kernel seems to not support get_fantasy_model.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

I am including the smallest mwe (or should I say mnwe) here. Note that I get the same behaviour by taking the &lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/05_Scalable_GP_Regression_Multidimensional/SGPR_Example_CUDA.html&gt;example tutorial for SGPR&lt;/denchmark-link&gt;
 and add a get_fantasy_model added at the end. I can post that too if required, but it is longer and might clutter the ticket.
Code snippet to reproduce
import gpytorch
import torch
from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel
from gpytorch.distributions import MultivariateNormal
from gpytorch.likelihoods import GaussianLikelihood
from gpytorch.means import ConstantMean

class GPRegressionModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = ConstantMean()
        self.base_covar_module = ScaleKernel(RBFKernel())
        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[:500, :], likelihood=likelihood)

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return MultivariateNormal(mean_x, covar_x)

train_X = torch.randn((100,5)).to("cpu")
train_y = torch.randn((100)).to("cpu")

likelihood = GaussianLikelihood()

model = GPRegressionModel(train_X, train_y, likelihood)
model.train()
model.eval()

test_pred = model(torch.randn((1,5)).to("cpu"))

model = model.get_fantasy_model(torch.randn((1,5)).to("cpu"), torch.randn((1)).to("cpu"))
Stack trace/error message
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "mwe_sgpr_fantasy.py", line 31, in &lt;module&gt;
    model = model.get_fantasy_model(torch.randn((1,5)).to("cpu"), torch.randn((1)).to("cpu"))
  File "/home/user/miniconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py", line 173, in get_fantasy_model
    new_model = deepcopy(self)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 306, in _reconstruct
    value = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 161, in deepcopy
    y = copier(memo)
  File "/home/user/miniconda3/lib/python3.7/site-packages/torch/tensor.py", line 23, in __deepcopy__
    raise RuntimeError("Only Tensors created explicitly by the user "
RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

I would expect a fantasized model to be returned efficiently.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:


GPyTorch Version 0.3.3


PyTorch Version 1.1.0


Ubuntu 18.04


&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

It seems that during the update, the new_model = deepcopy(self) tries to copy self._inducing_inv_root but detects that it is trainable by autograd and balks. I guess gpytorch made this design choice because of the goal of optimizing the inducing points as a hyperparameter, but as a tradeoff it does not allow for efficient updates.
So far I tried to replace the inducing points with a non-trainable version by setting  to , but it seems to not help. I would guess that &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/kernels/inducing_point_kernel.py#L45-L47&gt;any of these tensors multiplications&lt;/denchmark-link&gt;
 in the implementation of  could end up reactivating autograd, and I am afraid that without more knowledge of gpytorch's internals patching them one-by-one might end up in a long whack-a-mole.
	</description>
	<comments>
		<comment id='1' author='danielecc' date='2019-07-13T09:53:33Z'>
		Should be all fixed!
		</comment>
		<comment id='2' author='danielecc' date='2019-07-13T10:58:13Z'>
		Thanks for the quick reaction. This seems to fix the mwe, but not the SGPR example
import math
import torch
import gpytorch


train_n = 80

X = torch.randn((100,5))
y = torch.randn((100))

train_x = X[:train_n, :].contiguous().cuda()
train_y = y[:train_n].contiguous().cuda()

test_x = X[train_n:, :].contiguous().cuda()
test_y = y[train_n:].contiguous().cuda()


# ## Defining the GP Model
# 
# We now define the GP model. For more details on the use of GP models, see our simpler examples. This model constructs a base scaled RBF kernel, and then simply wraps it in an `InducingPointKernel`. Other than this, everything should look the same as in the simple GP models.

# In[14]:


from gpytorch.means import ConstantMean
from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel
from gpytorch.distributions import MultivariateNormal

class GPRegressionModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = ConstantMean()
        self.base_covar_module = ScaleKernel(RBFKernel())
        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[:500, :], likelihood=likelihood)

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return MultivariateNormal(mean_x, covar_x)


likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()
model = GPRegressionModel(train_x, train_y, likelihood).cuda()


# ## Training the model

# In[16]:


# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

training_iterations = 25
def train():
    for i in range(training_iterations):
        # Zero backprop gradients
        optimizer.zero_grad()
        # Get output from model
        output = model(train_x)
        # Calc loss and backprop derivatives
        loss = -mll(output, train_y)
        loss.backward()
        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))
        optimizer.step()
        torch.cuda.empty_cache()
        
# ## Making Predictions
# 
# The next cell makes predictions with SKIP. We use the same max_root_decomposition size, and we also demonstrate increasing the max preconditioner size. Increasing the preconditioner size on this dataset is **not** necessary, but can make a big difference in final test performance, and is often preferable to increasing the number of CG iterations if you can afford the space.

# In[17]:

train()

model.eval()
likelihood.eval()
with gpytorch.settings.max_preconditioner_size(10), torch.no_grad():
    with gpytorch.settings.use_toeplitz(False), gpytorch.settings.max_root_decomposition_size(30), gpytorch.settings.fast_pred_var():
        preds = model(test_x)


# In[18]:


print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))

model = model.get_fantasy_model(torch.randn((1,5)).cuda(), torch.randn((1)).cuda())
with output
&lt;denchmark-code&gt;Iter 2/25 - Loss: 1.423
Iter 3/25 - Loss: 1.423
Iter 4/25 - Loss: 1.422
Iter 5/25 - Loss: 1.422
Iter 6/25 - Loss: 1.421
Iter 7/25 - Loss: 1.420
Iter 8/25 - Loss: 1.420
Iter 9/25 - Loss: 1.419
Iter 10/25 - Loss: 1.418
Iter 11/25 - Loss: 1.418
Iter 12/25 - Loss: 1.417
Iter 13/25 - Loss: 1.417
Iter 14/25 - Loss: 1.416
Iter 15/25 - Loss: 1.416
Iter 16/25 - Loss: 1.415
Iter 17/25 - Loss: 1.414
Iter 18/25 - Loss: 1.414
Iter 19/25 - Loss: 1.413
Iter 20/25 - Loss: 1.413
Iter 21/25 - Loss: 1.412
Iter 22/25 - Loss: 1.412
Iter 23/25 - Loss: 1.411
Iter 24/25 - Loss: 1.411
Iter 25/25 - Loss: 1.410
Test MAE: 0.8142117857933044
Traceback (most recent call last):
  File "SGPR_Example_CUDA.py", line 99, in &lt;module&gt;
    model = model.get_fantasy_model(torch.randn((1,5)).cuda(), torch.randn((1)).cuda())
  File "/home/user/miniconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py", line 173, in get_fantasy_model
    new_model = deepcopy(self)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 306, in _reconstruct
    value = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 161, in deepcopy
    y = copier(memo)
  File "/home/user/miniconda3/lib/python3.7/site-packages/gpytorch/kernels/inducing_point_kernel.py", line 116, in __deepcopy__
    cp = copy.deepcopy(self, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 306, in _reconstruct
    value = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 220, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 220, in &lt;listcomp&gt;
    y = [deepcopy(a, memo) for a in x]
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 220, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 220, in &lt;listcomp&gt;
    y = [deepcopy(a, memo) for a in x]
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 220, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 220, in &lt;listcomp&gt;
    y = [deepcopy(a, memo) for a in x]
  File "/home/user/miniconda3/lib/python3.7/copy.py", line 161, in deepcopy
    y = copier(memo)
  File "/home/user/miniconda3/lib/python3.7/site-packages/torch/tensor.py", line 23, in __deepcopy__
    raise RuntimeError("Only Tensors created explicitly by the user "
RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment
&lt;/denchmark-code&gt;

Note that File "/home/user/miniconda3/lib/python3.7/site-packages/gpytorch/kernels/inducing_point_kernel.py", line 116, in __deepcopy__ should mean it is calling into the new fix, but it still finds something trainable.
I hope this is not related to me wrongly applying the patch, but  I also deleted the __pychache__ in kernel to force reparsing. If you think you need any other debug output please let me know.
		</comment>
		<comment id='3' author='danielecc' date='2019-07-13T18:18:16Z'>
		&lt;denchmark-link:https://github.com/danielecc&gt;@danielecc&lt;/denchmark-link&gt;
 Just pushed a hotfix that should fix the problem (again).
		</comment>
	</comments>
</bug>