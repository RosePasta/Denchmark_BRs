<bug id='699' author='samuelstanton' open_date='2019-05-18T20:09:41Z' closed_time='2019-06-06T01:16:49Z'>
	<summary>[Bug] `Interval.check` doesn't check for device compatibility before comparison</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

In Interval.check and Interval.check_raw if tensor is on a different device than self.upper_bound then the check fails. This use case can occur when initializing GP hypers from data on a GPU.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import torch
import gpytorch

class CustomGP(gpytorch.models.ExactGP):
    def __init__(self, inputs, targets, likelihood):
        super().__init__(inputs, targets, likelihood)
        self.likelihood.initialize(noise=targets.var())

new_device = torch.device('cuda:0')
train_x = torch.rand(30, device=new_device)
train_y = torch.rand(30, device=new_device)
likelihood = gpytorch.likelihoods.GaussianLikelihood()

gp = CustomGP(train_x, train_y, likelihood).to(new_device)
&lt;/denchmark-code&gt;

** Stack trace/error message **
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-1-82fa0692855f&gt; in &lt;module&gt;()
     12 likelihood = gpytorch.likelihoods.GaussianLikelihood()
     13 
---&gt; 14 gp = CustomGP(train_x, train_y, likelihood).to(new_device)

&lt;ipython-input-1-82fa0692855f&gt; in __init__(self, inputs, targets, likelihood)
      5     def __init__(self, inputs, targets, likelihood):
      6         super().__init__(inputs, targets, likelihood)
----&gt; 7         self.likelihood.initialize(noise=targets.var())
      8 
      9 new_device = torch.device('cuda:0')

~/Code/gpytorch/gpytorch/module.py in initialize(self, **kwargs)
     76                 raise AttributeError("Unknown parameter {p} for {c}".format(p=name, c=self.__class__.__name__))
     77             elif name not in self._parameters and name not in self._buffers:
---&gt; 78                 setattr(self, name, val)
     79             elif torch.is_tensor(val):
     80                 constraint = self.constraint_for_parameter_name(name)

~/anaconda3/envs/minerva/lib/python3.7/site-packages/torch/nn/modules/module.py in __setattr__(self, name, value)
    581                     buffers[name] = value
    582                 else:
--&gt; 583                     object.__setattr__(self, name, value)
    584 
    585     def __delattr__(self, name):

~/Code/gpytorch/gpytorch/likelihoods/gaussian_likelihood.py in noise(self, value)
     68     @noise.setter
     69     def noise(self, value: Tensor) -&gt; None:
---&gt; 70         self.noise_covar.initialize(noise=value)
     71 
     72     @property

~/Code/gpytorch/gpytorch/module.py in initialize(self, **kwargs)
     76                 raise AttributeError("Unknown parameter {p} for {c}".format(p=name, c=self.__class__.__name__))
     77             elif name not in self._parameters and name not in self._buffers:
---&gt; 78                 setattr(self, name, val)
     79             elif torch.is_tensor(val):
     80                 constraint = self.constraint_for_parameter_name(name)

~/anaconda3/envs/minerva/lib/python3.7/site-packages/torch/nn/modules/module.py in __setattr__(self, name, value)
    581                     buffers[name] = value
    582                 else:
--&gt; 583                     object.__setattr__(self, name, value)
    584 
    585     def __delattr__(self, name):

~/Code/gpytorch/gpytorch/likelihoods/noise_models.py in noise(self, value)
     49     @noise.setter
     50     def noise(self, value: Tensor) -&gt; None:
---&gt; 51         self._set_noise(value)
     52 
     53     def _set_noise(self, value: Tensor) -&gt; None:

~/Code/gpytorch/gpytorch/likelihoods/noise_models.py in _set_noise(self, value)
     54         if not torch.is_tensor(value):
     55             value = torch.as_tensor(value).to(self.raw_noise)
---&gt; 56         self.initialize(raw_noise=self.raw_noise_constraint.inverse_transform(value))
     57 
     58     def forward(

~/Code/gpytorch/gpytorch/module.py in initialize(self, **kwargs)
     79             elif torch.is_tensor(val):
     80                 constraint = self.constraint_for_parameter_name(name)
---&gt; 81                 if constraint is not None and not constraint.check_raw(val):
     82                     raise RuntimeError(
     83                         "Attempting to manually set a parameter value that is out of bounds of "

~/Code/gpytorch/gpytorch/constraints/constraints.py in check_raw(self, tensor)
     51     def check_raw(self, tensor):
     52         return bool(
---&gt; 53             torch.all((self.transform(tensor) &lt;= self.upper_bound))
     54             and torch.all(self.transform(tensor) &gt;= self.lower_bound)
     55         )

RuntimeError: Expected object of backend CUDA but got backend CPU for argument #2 'other'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

This code snippet should simply create an instance of CustomGP and then make sure everything is on the GPU.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


GPyTorch Version: 0.3.2
PyTorch Version: 1.1.0.dev20190516
Ubuntu 16.04 LTS

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I have a patch branch that compares tensor to self.upper_bound.to(tensor.device) I can do a quick pull request if that is an acceptable fix
	</description>
	<comments>
		<comment id='1' author='samuelstanton' date='2019-05-29T18:35:09Z'>
		&lt;denchmark-link:https://github.com/samuelstanton&gt;@samuelstanton&lt;/denchmark-link&gt;
 Since  extends , I think the right way to fix this is to override the  method to apply on the upper and lower bounds. That way, when you call , the upper and lower bounds for all constraints are cast over as well.
		</comment>
		<comment id='2' author='samuelstanton' date='2019-06-06T00:53:19Z'>
		&lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;
 I think the issue here is calling  with CUDA tensors before calling 
		</comment>
	</comments>
</bug>