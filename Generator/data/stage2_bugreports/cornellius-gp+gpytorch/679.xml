<bug id='679' author='KeAWang' open_date='2019-05-06T16:54:28Z' closed_time='2019-05-06T17:03:41Z'>
	<summary>[Bug] MLL bug when batch size = 1</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

When using training data of the shape N x 1, the ExactMarginalLogLikelihood does not return a scalar.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Just need to unsqueeze the last dimension in the Simple GP regression example
import math
import torch
import gpytorch

# unsqueeze to make data N x 1 instead of N
train_x = torch.linspace(0, 1, 100).unsqueeze(-1)
train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2

from IPython.core.debugger import set_trace
# We will use the simplest form of GP model, exact inference
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# initialize likelihood and model
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood)

model.train()
likelihood.train()
output = model(train_x)

loss = -mll(output, train_y)

# this will complain because now loss is of size N instead of a scalar
loss.backward()
** Stack trace/error message **
&lt;denchmark-code&gt;RuntimeError: grad can be implicitly created only for scalar outputs
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:


 0.3.2


 '1.1.0.dev20190506'


 Arch Linux


	</description>
	<comments>
		<comment id='1' author='KeAWang' date='2019-05-06T17:03:41Z'>
		No, this is happening because when you unsqueeze train_x where you are, you end up with train_y having shape n x 1, which we're going to interpret over all as n batches of 1 data point.
This works fine
train_x = torch.linspace(0, 1, 100)
train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2
# Now unsqueeze
train_x = train_x.unsqueeze(-1)
		</comment>
		<comment id='2' author='KeAWang' date='2019-05-06T17:58:48Z'>
		Oops wasn't thinking this morning...
		</comment>
	</comments>
</bug>