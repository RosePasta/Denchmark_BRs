<bug id='760' author='MichaelDoron' open_date='2019-06-27T11:30:53Z' closed_time='2019-06-27T19:04:10Z'>
	<summary>[Bug] Multiplication of two ScaleKernel(LinearKernel()) creates runtime error</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

Training a model using a multiplication of two ScaleKernel(LinearKernel()) evokes Lapack Error syev : 2 off-diagonal elements didn't converge to zero
Update: Notice that a multiplication of four LinearKernels creates the same error, even without ScaleKernel (commented out in the code snippet below).
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
&lt;denchmark-code&gt;import math
import torch
from time import time
import copy

torch.manual_seed(0)

data_samples = 400 
training_iterations = 100

class GPRegressionModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel()) * gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())
        # self.covar_module = gpytorch.kernels.LinearKernel() * gpytorch.kernels.LinearKernel() * gpytorch.kernels.LinearKernel() * gpytorch.kernels.LinearKernel()
        
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_module = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_module)

train_x = torch.linspace(0, 0.5, data_samples)
train_y = torch.sin(train_x * 100.0)

likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = GPRegressionModel(train_x, train_y, likelihood)
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=0.1)
model.train()
likelihood.train()

optimizer.zero_grad()
output = model(train_x)
loss = -mll(output, train_y)
loss.backward()
optimizer.step()    
for i in range(training_iterations):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)
    loss.backward()
    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))
    optimizer.step()

model.eval()
likelihood.eval()

with torch.no_grad():
    test_x = torch.linspace(0, train_x[-1] * 1, len(train_x) * 1)
    prediction = likelihood(model(test_x))
    mean = prediction.mean
    lower, upper = prediction.confidence_region()

print('Reached the end of the script without Runtime errors')
&lt;/denchmark-code&gt;

** Stack trace/error message **
&lt;denchmark-code&gt;RuntimeError: Lapack Error syev : 2 off-diagonal elements didn't converge to zero at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/TH/generic/THTensorLapack.cpp:296
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

The model should train without returning a runtime error.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:


0.3.3


1.1.0


macOS


	</description>
	<comments>
		<comment id='1' author='MichaelDoron' date='2019-06-27T18:47:52Z'>
		&lt;denchmark-link:https://github.com/MichaelDoron&gt;@MichaelDoron&lt;/denchmark-link&gt;
 A few things:


Using kernels to fit data for which the kernel has no support is always a recipe for numerical trouble. In this case, the product of linear kernels is equivalent to a degree 2 polynomial kernel, which is going to have a hard time fitting a sine function with 8 local optima in the training regime.


In this case, multiplying linear kernels together is a particularly nasty way of getting about this kernel. Using the equivalent ScaleKernel(PolynomialKernel(2)), which works just fine in your example.


I was also able to get it working as a product of LinearKernels by doing with gpytorch.settings.fast_computations(covar_root_decomposition=False): around the training code -- it seems like some of the approximate root decomposition code we use is eventually having trouble with this particular case because that kernel is unable to do anything except treat the data as random noise, which leads to very small slope parameters and therefore numerical instability.


		</comment>
		<comment id='2' author='MichaelDoron' date='2019-06-27T19:04:07Z'>
		I understand, thank you for the elaborate answer!
		</comment>
	</comments>
</bug>