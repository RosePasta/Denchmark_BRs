<bug id='758' author='sdaulton' open_date='2019-06-26T17:35:13Z' closed_time='2019-10-29T21:33:51Z'>
	<summary>[Bug] Batch-indexing lazy covariance matrix fails for batch-mode ExactGP</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

Evaluating a batch-mode ExactGP with training inputs of shape b x n x d and targets of shape b x n on a set of test points with shape m x d yields a lazy_covariance_matrix with shape b x m x m. Indexing along the batch dimension fails.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/files/3331406/batch_indexing_bug_lazy_covar.ipynb.txt&gt;batch_indexing_bug_lazy_covar.ipynb.txt&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Selecting a single index of the batch dimension:
&lt;denchmark-code&gt;mvn.lazy_covariance_matrix[0, :, :]
&lt;/denchmark-code&gt;

should yield the lazy covariance matrix for the 0th batch (with shape m x m)
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;



 0.3.2


 1.0.0a0


	</description>
	<comments>
		<comment id='1' author='sdaulton' date='2019-06-26T18:21:32Z'>
		This issue comes from having a batch-mode kernel, which is not "indexed" along the batch dimension when the LazyEvaluatedKernelTensor is indexed along the batch dimension--for this use case, the kernel would also need to be "indexed" along the batch dim when creating the new LazyEvaluatedKernelTensor: &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/lazy_evaluated_kernel_tensor.py#L121&gt;https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/lazy_evaluated_kernel_tensor.py#L121&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sdaulton' date='2019-07-09T14:22:32Z'>
		&lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 is this something you think you can fix in the near future? This is kind of important in order to get multi-output models working efficiently.
		</comment>
	</comments>
</bug>