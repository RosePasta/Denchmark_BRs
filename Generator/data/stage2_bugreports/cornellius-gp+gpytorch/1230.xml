<bug id='1230' author='josuelandinez' open_date='2020-07-30T02:28:11Z' closed_time='2020-07-30T02:45:20Z'>
	<summary>[Bug] Is it WhiteNoiseKernel deprecated?</summary>
	<description>
Hi,
I tried to use the WhiteKernelNoise:
a=gpytorch.kernels.WhiteNoiseKernel()
&lt;denchmark-h:h2&gt;and get:&lt;/denchmark-h&gt;

AttributeError                            Traceback (most recent call last)
 in 
----&gt; 1 a=gpytorch.kernels.WhiteNoiseKernel()
AttributeError: module 'gpytorch.kernels' has no attribute 'WhiteNoiseKernel'
I looked to several threads, seems it was implemented but seems is not anymore that is correct?, is there any hint or solution to implement it?. The reason to use this kernel my data have noise and also  it keeps stable the Cholesky decomposition while optimizing for several combinations of kernels like RBF, Mattern, Linear, etc using LBFGS. I already tested in the same data and kernels in sklearn and seems to work. However, I started to use gpytorch because I can get faster optimization. I already tested softmax transofrmations with another L-BSGF optimizer suggested in another gpytorch thread, however still the problem happens. I appreciated any insisght you can provide.
best regards,
Edgar
	</description>
	<comments>
		<comment id='1' author='josuelandinez' date='2020-07-30T02:45:20Z'>
		Hi &lt;denchmark-link:https://github.com/josuelandinez&gt;@josuelandinez&lt;/denchmark-link&gt;
 --
Use the FixedNoiseGaussianLikelihood, which replaced the functionality that used to be provided by WhiteNoiseKernel.
With that being said, if you are only adding a diagonal component to your kernel to stop numerical issues, GaussianLikelihood already has a minimum noise value that gets added that you can change (i.e., the default constraint is GreaterThan(1e-4)). If you are still having numerical problems, it's likely due to improper data normalization.
		</comment>
	</comments>
</bug>