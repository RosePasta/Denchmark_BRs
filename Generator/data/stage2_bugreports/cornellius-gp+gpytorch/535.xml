<bug id='535' author='Balandat' open_date='2019-02-23T00:14:50Z' closed_time='2019-04-15T15:28:27Z'>
	<summary>lazily_evaluate_kernels(False) does not work</summary>
	<description>
Repro:
Run &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/examples/01_Simple_GP_Regression/Simple_GP_Regression.ipynb&gt;Simple GP Regression example&lt;/denchmark-link&gt;
. Then calling
&lt;denchmark-code&gt;with torch.no_grad(), gpytorch.settings.lazily_evaluate_kernels(False):
    test_x = torch.linspace(0, 1, 51)
    observed_pred = likelihood(model(test_x))
&lt;/denchmark-code&gt;

results in the following error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-7-17d4be761e30&gt; in &lt;module&gt;()
      1 with torch.no_grad(), gpytorch.settings.lazily_evaluate_kernels(False):
      2     test_x = torch.linspace(0, 1, 51)
----&gt; 3     observed_pred = likelihood(model(test_x))

/data/users/balandat/fbsource/fbcode/buck-out/dev/gen/bento/kernels/bento_kernel_ae_lazarus#link-tree/gpytorch/models/exact_gp.py in __call__(self, *args, **kwargs)
    199             )
    200 
--&gt; 201             full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)
    202             if settings.debug().on():
    203                 if not isinstance(full_output, MultivariateNormal):

/data/users/balandat/fbsource/fbcode/buck-out/dev/gen/bento/kernels/bento_kernel_ae_lazarus#link-tree/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     18 
     19     def __call__(self, *inputs, **kwargs):
---&gt; 20         outputs = self.forward(*inputs, **kwargs)
     21         if isinstance(outputs, list):
     22             return [_validate_module_outputs(output) for output in outputs]

&lt;ipython-input-3-190c52404ce7&gt; in forward(self, x)
      8     def forward(self, x):
      9         mean_x = self.mean_module(x)
---&gt; 10         covar_x = self.covar_module(x)
     11         return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
     12 

/data/users/balandat/fbsource/fbcode/buck-out/dev/gen/bento/kernels/bento_kernel_ae_lazarus#link-tree/gpytorch/kernels/kernel.py in __call__(self, x1, x2, diag, batch_dims, **params)
    382                 res = LazyEvaluatedKernelTensor(self, x1_, x2_, batch_dims=batch_dims, **params)
    383             else:
--&gt; 384                 res = super(Kernel, self).__call__(x1_, x2_, batch_dims=batch_dims, **params)
    385 
    386             # Now we'll make sure that the shape we're getting makes sense

/data/users/balandat/fbsource/fbcode/buck-out/dev/gen/bento/kernels/bento_kernel_ae_lazarus#link-tree/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     18 
     19     def __call__(self, *inputs, **kwargs):
---&gt; 20         outputs = self.forward(*inputs, **kwargs)
     21         if isinstance(outputs, list):
     22             return [_validate_module_outputs(output) for output in outputs]

/data/users/balandat/fbsource/fbcode/buck-out/dev/gen/bento/kernels/bento_kernel_ae_lazarus#link-tree/gpytorch/kernels/scale_kernel.py in forward(self, x1, x2, batch_dims, diag, **params)
     92             outputscales = outputscales.unsqueeze(1).repeat(1, x1.size(-1)).view(-1)
     93 
---&gt; 94         orig_output = self.base_kernel(x1, x2, diag=diag, batch_dims=batch_dims, **params)
     95         if torch.is_tensor(orig_output):
     96             outputscales = outputscales.view(-1, *([1] * (orig_output.dim() - 1)))

/data/users/balandat/fbsource/fbcode/buck-out/dev/gen/bento/kernels/bento_kernel_ae_lazarus#link-tree/gpytorch/kernels/kernel.py in __call__(self, x1, x2, diag, batch_dims, **params)
    405                         raise RuntimeError(
    406                             "Error with {}.forward. Expected size {}. Got size {}.".format(
--&gt; 407                                 self.__class__.__name__, expected_shape, res.shape
    408                             )
    409                         )

RuntimeError: Error with RBFKernel.forward. Expected size torch.Size([151, 151]). Got size torch.Size([1, 151, 151]).
&lt;/denchmark-code&gt;

Not sure if this is even supposed to be used anymore - if not let's kill it.
	</description>
	<comments>
		<comment id='1' author='Balandat' date='2019-02-23T00:20:25Z'>
		Isn't this the preferred way to do call kernels on inputs instead of using the Kernel's forward function directly? That way hooks can get registered
		</comment>
		<comment id='2' author='Balandat' date='2019-02-23T00:35:36Z'>
		Well it’s used with True setting by default. If the False setting just doesn’t work anymore b/c of other code changes then we should get rid of that setting altogether.
		</comment>
		<comment id='3' author='Balandat' date='2019-02-23T00:37:18Z'>
		The lazily evaluate kernels flag is primarily for internal use to stop nested lazy evaluation. It is used for example here:



gpytorch/gpytorch/lazy/lazy_evaluated_kernel_tensor.py


         Line 197
      in
      d262f41






 with settings.lazily_evaluate_kernels(False): 





to prevent sub kernels from also being lazily evaluated.
I don't think it's well tested for use on actual outer kernels. It should maybe work there, but people should essentially never turn it off in user-facing code unless they really really know they want that.
		</comment>
		<comment id='4' author='Balandat' date='2019-02-23T00:46:49Z'>
		Fair enough. Should we mark this more clearly as internal then? E.g. name it _lazily_evaluate_kernels and indicate more clearly in docstring that should not need to be used outside of kernel compositions?
		</comment>
		<comment id='5' author='Balandat' date='2019-02-23T00:47:43Z'>
		I think that would be reasonable, yeah. I really don't see a use case where an end user would actually need to use that setting.
		</comment>
		<comment id='6' author='Balandat' date='2019-02-23T16:16:42Z'>
		I think that this flag should work in theory. Based on your error, it seems that there's some weird batch-shaping issues going on here.
Once we support true multi-batch mode in the kernels (&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/531&gt;#531&lt;/denchmark-link&gt;
) (and we stop this gross hackery with adding a batch dimension to every kernel), this error should go away.
As a temporary fix, we could add a comment that it shouldn't be used as part of the public API? (Alternatively, we could remove the docstring so it doesn't appear in the docs)
		</comment>
	</comments>
</bug>