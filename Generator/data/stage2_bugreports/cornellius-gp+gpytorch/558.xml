<bug id='558' author='tohein' open_date='2019-03-08T19:17:26Z' closed_time='2019-05-06T09:10:31Z'>
	<summary>dimension mismatch ScaleKernel + WhiteNoiseKernel</summary>
	<description>
Hey,
I was trying to use a WhiteNoiseKernel with a variance parameter but ran into size mismatch problems. Example:
&lt;denchmark-code&gt;import torch
import gpytorch
from gpytorch.models import AbstractVariationalGP
from gpytorch.variational import CholeskyVariationalDistribution
from gpytorch.variational import VariationalStrategy

class GPClassificationModel(AbstractVariationalGP):
    def __init__(self, train_x):
        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))
        variational_strategy = VariationalStrategy(self, train_x, variational_distribution)
        super(GPClassificationModel, self).__init__(variational_strategy)
        self.mean_module = gpytorch.means.ZeroMean()  
        self.covar_module1 = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
        self.covar_module2 = gpytorch.kernels.ScaleKernel(gpytorch.kernels.WhiteNoiseKernel(torch.ones(train_x.size(0))))
        self.covar_module = self.covar_module1 + self.covar_module2
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
        return latent_pred

model = GPClassificationModel(train_x)
likelihood = gpytorch.likelihoods.BernoulliLikelihood()

train_x = torch.linspace(0, 1, 100)
train_y = torch.sign(torch.cos(train_x * (4 * math.pi)))
model(train_x)
&lt;/denchmark-code&gt;

gives me the following error
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-19-f9c049d8537e&gt; in &lt;module&gt;
     21 train_x = torch.linspace(0, 1, 100)
     22 train_y = torch.sign(torch.cos(train_x * (4 * math.pi)))
---&gt; 23 model(train_x)

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/models/abstract_variational_gp.py in __call__(self, inputs, **kwargs)
     20             inputs = inputs.unsqueeze(-1)
     21 
---&gt; 22         return self.variational_strategy(inputs)

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py in __call__(self, x)
    178 
    179     def __call__(self, x):
--&gt; 180         self.initialize_variational_dist()
    181         if self.training:
    182             if hasattr(self, "_memoize_cache"):

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py in initialize_variational_dist(self)
     83         """
     84         if not self.variational_params_initialized.item():
---&gt; 85             prior_dist = self.prior_distribution
     86             eval_prior_dist = torch.distributions.MultivariateNormal(
     87                 loc=prior_dist.mean,

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/utils/memoize.py in g(self, *args, **kwargs)
     15         cache_name = name if name is not None else method
     16         if cache_name not in self._memoize_cache:
---&gt; 17             self._memoize_cache[cache_name] = method(self, *args, **kwargs)
     18         return self._memoize_cache[cache_name]
     19 

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/variational/variational_strategy.py in prior_distribution(self)
     64         out = self.model.forward(self.inducing_points)
     65         res = MultivariateNormal(
---&gt; 66             out.mean, out.lazy_covariance_matrix.evaluate_kernel().add_jitter()
     67         )
     68         return res

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in evaluate_kernel(self)
    209                 and self._cached_kernel_eval.size(0) == 1
    210             ):
--&gt; 211                 self._cached_kernel_eval = self._cached_kernel_eval[0]
    212 
    213             self._cached_kernel_eval = lazify(self._cached_kernel_eval)

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/lazy/lazy_tensor.py in __getitem__(self, index)
   1354 
   1355         # Call self._getitem - now that the index has been processed
-&gt; 1356         return self._getitem(*index)
   1357 
   1358     def __matmul__(self, other):

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py in _getitem(self, *indices)
     25 
     26     def _getitem(self, *indices):
---&gt; 27         results = tuple(lazy_tensor._getitem(*indices) for lazy_tensor in self.lazy_tensors)
     28         if isinstance(results[0], LazyTensor):
     29             return SumLazyTensor(*results)

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py in &lt;genexpr&gt;(.0)
     25 
     26     def _getitem(self, *indices):
---&gt; 27         results = tuple(lazy_tensor._getitem(*indices) for lazy_tensor in self.lazy_tensors)
     28         if isinstance(results[0], LazyTensor):
     29             return SumLazyTensor(*results)

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/lazy/constant_mul_lazy_tensor.py in _getitem(self, *indices)
     85             constant = constant.view(*constant.shape, *[1] * (base_lazy_tensor.dim() - constant.dim()))
     86 
---&gt; 87         return base_lazy_tensor * constant
     88 
     89     def _matmul(self, rhs):

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/lazy/diag_lazy_tensor.py in __mul__(self, other)
    102             return DiagLazyTensor(self._diag * other._diag)
    103         else:
--&gt; 104             other_diag = other.diag()
    105             new_diag = self._diag * other_diag
    106             corrected_diag = new_diag - other_diag

/anaconda3/envs/gpflow/lib/python3.6/site-packages/gpytorch/lazy/non_lazy_tensor.py in diag(self)
     55     def diag(self):
     56         if self.tensor.ndimension() &lt; 3:
---&gt; 57             return self.tensor.diag()
     58         else:
     59             row_col_iter = torch.arange(0, self.matrix_shape[-1], dtype=torch.long, device=self.device)

RuntimeError: Input must be 1-d or 2-d
&lt;/denchmark-code&gt;

Any idea what I am doing wrong here?
	</description>
	<comments>
		<comment id='1' author='tohein' date='2019-03-08T23:28:04Z'>
		&lt;denchmark-link:https://github.com/tohein&gt;@tohein&lt;/denchmark-link&gt;
 Thanks for providing minimal code that repros -- I'll look in to this as soon as I have a free minute! It's worth noting that  is largely being deprecated in favor of introducing heteroscedastic noise via the likelihood with an appropriate .
&lt;denchmark-link:https://github.com/Balandat&gt;@Balandat&lt;/denchmark-link&gt;
 Do you have a reasonable example of using a heteroscedastic noise model in the likelihood, since that is presumably now the preferred way of handling what WhiteNoiseKernel used to?
		</comment>
		<comment id='2' author='tohein' date='2019-03-19T23:07:46Z'>
		Sorry for the late reply. There are some issues with the WhiteNoiseKernel. In particular, in posterior mode the noise is not properly applied to the elements of the full (train+test) covariance matrix corresponding to the training inputs.
Instead of fixing the WhiteNoiseKernel, I suggest to move the known noise variances into the likelihood. That is, the likelihood would be instantiated with the observed noise levels, and when evaluated on an mvn would just add these variances to the first n_train diagonal elements. The other diagonal elements could either be added  a user-specified variance, or something derived from the observed variances (e.g. their mean when assuming homoskedasticity).
Note that in this case calling model.likelihood(posterior_mvn) with posterior_mvn the posterior over test points will not make much sense - one will have to distinguish between the likelihood of the full kernel matrix and the output likelihood.
&lt;denchmark-link:https://github.com/jacobrgardner&gt;@jacobrgardner&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/bletham&gt;@bletham&lt;/denchmark-link&gt;
, does this sound reasonable?
		</comment>
		<comment id='3' author='tohein' date='2019-05-06T09:10:31Z'>
		Thank you for your replies. The example above was artificial anyway, as moving the noise variances to the likelihood and integrating them out analytically will again result in a Bernoulli. I was actually trying to use another custom (non-Gaussian) likelihood. I guess I will just integrate out the noise numerically.
		</comment>
	</comments>
</bug>