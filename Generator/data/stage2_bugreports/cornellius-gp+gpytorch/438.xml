<bug id='438' author='Balandat' open_date='2018-12-19T07:49:28Z' closed_time='2019-01-16T19:40:24Z'>
	<summary>Numerical issues under new _covar_sq_dist helper</summary>
	<description>
From &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/434&gt;#434&lt;/denchmark-link&gt;
:

I'm happy to switch back, but I want to be very sure that it doesn't affect numerical stability. Can you make sure that we don't have to increase any error bounds on any of the tests?

Unfortunately, this does significantly affect numerical stability, in particular when using an algorithm for fitting that uses (expanding) line search (such as L-BFGS). With this change many of our tests that previously passed reliably now fail.
I'm assuming the issue is due to the fact that now the individual components rather than their differences are squared: x1_norm = x1.pow(2).sum(dim=-1, keepdim=True). This causes overflow issues especially when using torch.float dtype, so the resulting MVNs have nan-valued loc/covar parameters. Switching to torch.double fixes many of the instability issues, but that's of course not a good solution when running on the GPU.
	</description>
	<comments>
		<comment id='1' author='Balandat' date='2018-12-19T14:32:56Z'>
		I've put up a PR to revert the kernel PR for now, and I'll reopen it until we're satisfied that it actually works.
Could you provide an example test that fails with the change but passes without it?
		</comment>
		<comment id='2' author='Balandat' date='2018-12-19T14:35:28Z'>
		
Could you provide an example test that fails with the change but passes without it?

Will do. The actual test is pretty heavyweight and relies on other code, let me see if I can isolate what's being called during fittig that generates NaNs.
		</comment>
	</comments>
</bug>