<bug id='802' author='fonnesbeck' open_date='2019-07-21T20:15:30Z' closed_time='2019-07-21T20:51:19Z'>
	<summary>[Bug] Unable to estimate independent scale parameters for each input</summary>
	<description>
I have a spatio-temporal GP with three input dimensions (2 spatial coordinates plus time) and am trying to use ARD to have separate lengthscale parameters for each, as well as separate scale kernels for the spatial vs the temporal inputs. My covariance function looks like this:
self.covar_module = (
            gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.MaternKernel(nu=1/2, 
                                          active_dims=torch.tensor([0, 1]), 
                                          ard_num_dims=2,
                                          batch_size=batch_size),
                active_dims=torch.tensor([0, 1]), 
                ard_num_dims=2,
                batch_size=batch_size)
            * gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.RBFKernel(active_dims=torch.tensor([2]),
                                          batch_size=batch_size),
                active_dims=torch.tensor([2]),
                batch_size=batch_size))
However, when I fit the model, looking at the estimated parameters afterwards shows that the same scale is being estimated for both components:
&lt;denchmark-code&gt;likelihood.noise_covar.raw_noise tensor([-5.4645], device='cuda:0')
covar_module.kernels.0.raw_outputscale tensor(-3.7285, device='cuda:0')
covar_module.kernels.0.base_kernel.raw_lengthscale tensor([[2.4628, 3.4885]], device='cuda:0')
covar_module.kernels.1.raw_outputscale tensor(-3.7285, device='cuda:0')
covar_module.kernels.1.base_kernel.raw_lengthscale tensor([[3.7490]], device='cuda:0')
&lt;/denchmark-code&gt;

as having the same value to 4 decimal places seems unlikely otherwise.
Is this user error on my part, or is this a bug? Is it possible to estimate different scales for each input feature?
	</description>
	<comments>
		<comment id='1' author='fonnesbeck' date='2019-07-21T20:20:14Z'>
		The math doesn't really work out on multiplying two scaled kernels together. Basically what you are doing is k(x, x') = \alpha_{1}*k(x_{01}, x_{01}')*\alpha_{2}*k(x_{2}, x_{2}') = \alpha_{1}*\alpha_{2}*k_{unscaled}(x, x') = \alpha_{3}*k_{unscaled}(x, x').
Thus, since the outputscales just get multiplied together and can get pulled out to the front, you're  basically learning one outputscale split in to two values. Another way to see why this is a problem is to observe that (for example) \alpha_{1} = 0.25 and \alpha_{2} = 0.5 gives rise to the same kernel as \alpha_{1} = 0.5 and \alpha_{2} = 0.25.
		</comment>
		<comment id='2' author='fonnesbeck' date='2019-07-21T20:34:19Z'>
		Right, that's what I would expect if they were being applied to the same input dimensions, but when one has active dims [0,1] and the other has [2], then the scales aren't swappable, are they? Perhaps active_dims is not doing what I think it is (?)
		</comment>
		<comment id='3' author='fonnesbeck' date='2019-07-21T20:49:32Z'>
		No, they are still swappable. If I have a setup like
k1 = RBFKernel(active_dims=[0, 1], ard_num_dims=2)
k1.lengthscale = [1., 2.]
k2 = MaternKernel(active_dims=[2])
x = torch.randn(100, 3)
os1 = 0.25
os2 = 0.5
it's still the case that os1*k1(x)*os2*k2(x) = os1*os2*k1(x)*k2(x) = os2*k1(x)*os1*k2(x). Remember that the outputscale is a scalar parameter multiplied by the entire kernel matrix after it has already been computed: it's not an input dimensionality dependent thing (like the os1 and os2 values above).
		</comment>
		<comment id='4' author='fonnesbeck' date='2019-07-21T20:51:19Z'>
		Got it, thanks.
		</comment>
		<comment id='5' author='fonnesbeck' date='2019-07-22T13:58:47Z'>
		There is a somewhat confusing API issue here, however. Namely, its not clear why a kernel should be wrapped at all by ScaleKernel when it is simply pulled outside as a multiplier. In other words,
ScaleKernel(MaternKernel()) * RBFKernel()
seems superfluous when all that ends up happening is:
ScaleKernel() * MaternKernel() * RBFKernel()
Is there anything beyond this going on to the passed kernel that does not end up also being applied to the second kernel?
		</comment>
	</comments>
</bug>