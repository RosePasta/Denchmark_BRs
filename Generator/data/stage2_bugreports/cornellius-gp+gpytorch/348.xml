<bug id='348' author='sumitsk' open_date='2018-11-04T21:25:15Z' closed_time='2018-11-29T01:42:09Z'>
	<summary>GPR fails with LBFGS optimizer</summary>
	<description>
Running a simple 1D GP regression (RBF kernel) sporadically throws runtime error when using LBFGS optimizer. This happens when MLL loss suddenly increases by a big value.
	</description>
	<comments>
		<comment id='1' author='sumitsk' date='2018-11-04T21:28:11Z'>
		Yeah this is known behavior on our end with the PyTorch LBFGS implementation. This is due to some technical details of the implementation that force our models to try ridiculous hyperparameters as part of backtracking.
From what we've seen, exact GP regression works quite well with e.g. &lt;denchmark-link:https://github.com/hjmshi/PyTorch-LBFGS&gt;https://github.com/hjmshi/PyTorch-LBFGS&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sumitsk' date='2018-11-15T05:44:58Z'>
		&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/371&gt;#371&lt;/denchmark-link&gt;
 added the ability to use less extreme parameter transforms such as , which should help a lot in that regard. Can you test if that works for you?
		</comment>
		<comment id='3' author='sumitsk' date='2018-11-29T01:42:09Z'>
		Closing this because it should be fixed by recent stability improvements. That being said, we strongly recommend the use of &lt;denchmark-link:https://github.com/hjmshi&gt;@hjmshi&lt;/denchmark-link&gt;
's Implementation of LBFGS (linked to above) over the built in PyTorch one.
		</comment>
	</comments>
</bug>