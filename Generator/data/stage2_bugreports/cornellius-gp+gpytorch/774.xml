<bug id='774' author='fonnesbeck' open_date='2019-07-05T20:25:21Z' closed_time='2019-07-05T22:58:46Z'>
	<summary>[Bug] Batched GPs with CUDA fails with shape issue</summary>
	<description>
When attempting to do batch processing for cross-validation, the backward() step fails with what appears to be a shape issue.
I'm doing 10-fold cross-validation, with input and output shapes as follows:
&lt;denchmark-link:https://user-images.githubusercontent.com/81476/60743797-ae229e00-9f38-11e9-8418-196cd14cc1cc.png&gt;&lt;/denchmark-link&gt;

Thus, 3 input features and a single output. I've applied .contiguous().cuda() to the data and .cuda() to the model and likelihood. However, when I get to the training step, it fails:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-244-9676fb877471&gt; in &lt;module&gt;
     10         optimizer.step()
     11     print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))
---&gt; 12 train()

&lt;ipython-input-244-9676fb877471&gt; in train()
      5         output = model_cv(Xs)
      6         loss = -mll_cv(output, ys)
----&gt; 7         loss.backward()
      8         if not i % 10:
      9             print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))

/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    105                 products. Defaults to ``False``.
    106         """
--&gt; 107         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    108 
    109     def register_hook(self, hook):

/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     85         grad_tensors = list(grad_tensors)
     86 
---&gt; 87     grad_tensors = _make_grads(tensors, grad_tensors)
     88     if retain_graph is None:
     89         retain_graph = create_graph

/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in _make_grads(outputs, grads)
     26             if out.requires_grad:
     27                 if out.numel() != 1:
---&gt; 28                     raise RuntimeError("grad can be implicitly created only for scalar outputs")
     29                 new_grads.append(torch.ones_like(out))
     30             else:

RuntimeError: grad can be implicitly created only for scalar outputs

&lt;/denchmark-code&gt;

Running GPyTorch 0.3.2 and PyTorch 1.1.0 on GCS.
	</description>
	<comments>
		<comment id='1' author='fonnesbeck' date='2019-07-05T20:56:47Z'>
		This is not a bug - the issue with your code above is that since the model is batch-evaluated the loss is a tensor with multiple elements (one loss for each fold). Assuming you created the model components with the correct batch shapes then the losses are independent, and you can just optimize over the sum of the losses. The following should work:
&lt;denchmark-code&gt;losses = -mll_cv(output, ys)
loss = losses.sum()
loss.backward()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='fonnesbeck' date='2019-07-05T22:58:46Z'>
		Got it, thanks.
		</comment>
	</comments>
</bug>