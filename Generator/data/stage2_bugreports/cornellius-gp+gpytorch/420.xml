<bug id='420' author='Balandat' open_date='2018-12-07T22:45:32Z' closed_time='2019-02-22T19:30:39Z'>
	<summary>More issues with batch mode in MTGPs</summary>
	<description>
I'm running into some more issues with (alleged) size incompatibilities when using batch mode for multitask GPs. In particular,

fitting a MTGP in batch mode fails outright
fitting a MTPG in non-batch mode works, but evaluating that model in batch mode fails.

See &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/files/2658985/MTGP_batch_errors.ipynb.txt&gt;MTGP_batch_errors.ipynb.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
, fixing these kind of issues properly without a ton of special casing relies on &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/369&gt;#369&lt;/denchmark-link&gt;
 -  can we prioritize this?
	</description>
	<comments>
		<comment id='1' author='Balandat' date='2018-12-11T13:16:48Z'>
		&lt;denchmark-link:https://github.com/Balandat&gt;@Balandat&lt;/denchmark-link&gt;
 - sure, I can try to get it done by the end of the week
		</comment>
		<comment id='2' author='Balandat' date='2019-02-22T19:30:39Z'>
		Closing this out in favor of &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/531&gt;#531&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>