<bug id='1085' author='lmao14' open_date='2020-03-26T06:42:33Z' closed_time='2020-04-07T12:15:58Z'>
	<summary>[Bug] SVGP mulitclass classification</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi!
I am trying to implement a multiclass SVGP model and encounter an error. It seems due to a bug on slicing a Lazy Tensor.
print(gpytorch.__version__)
print(torch.__version__)

cov = model.forward(X).lazy_covariance_matrix
print(cov.shape)
print(cov[..., :10, 20:].shape)
print(cov[..., :10, 20:].evaluate().shape)
print(cov.evaluate()[..., :10, 20:].shape)
&lt;denchmark-code&gt;1.0.1
1.3.1
torch.Size([3, 100, 100])
torch.Size([3, 10, 10])
torch.Size([3, 10, 10])
torch.Size([3, 10, 80])
&lt;/denchmark-code&gt;

** Code snippet to reproduce **
import torch
import gpytorch
import numpy as np

# Generate data
N = 100
num_classes = 3
X = np.random.rand(N, num_classes)
Y = np.argmax(X, 1).reshape(-1,).astype(int)
X = torch.tensor(X).double()
Y = torch.tensor(Y).double()
inducing_points = X[20:]

# Define GP model
class GPClassificationModel(ApproximateGP):
    def __init__(self, inducing_points, num_classes):
        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0), batch_shape=torch.Size([num_classes]))
        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)
        variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(
            variational_strategy, num_tasks=num_classes,
        )
        super(GPClassificationModel, self).__init__(variational_strategy)
        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_classes]))
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_classes])), batch_shape=torch.Size([num_classes])
        )

    def forward(self,x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

model = GPClassificationModel(inducing_points=inducing_points,num_classes=num_classes).double()
likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_classes=num_classes,mixing_weights=False).double()

# Train
model.train()
likelihood.train()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
mll = VariationalELBO(likelihood, model, Y.numel(), combine_terms=True)
for i in range(100):
    optimizer.zero_grad()
    output = model(X)
    loss = -mll(output, Y)
    loss.backward()
    optimizer.step()
    scheduler.step()
** Stack trace/error message **
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-166-342ebe4bcdab&gt; in &lt;module&gt;
     41 for i in range(100):
     42     optimizer.zero_grad()
---&gt; 43     output = model(X)
     44     loss = -mll(output, Y)
     45     loss.backward()

~/anaconda3/envs/gpytorchgit/lib/python3.7/site-packages/gpytorch/models/approximate_gp.py in __call__(self, inputs, prior, **kwargs)
     79         if inputs.dim() == 1:
     80             inputs = inputs.unsqueeze(-1)
---&gt; 81         return self.variational_strategy(inputs, prior=prior)

~/anaconda3/envs/gpytorchgit/lib/python3.7/site-packages/gpytorch/variational/multitask_variational_strategy.py in __call__(self, x, prior)
     41 
     42     def __call__(self, x, prior=False):
---&gt; 43         function_dist = self.base_variational_strategy(x, prior=prior)
     44         if (
     45             self.task_dim &gt; 0

~/anaconda3/envs/gpytorchgit/lib/python3.7/site-packages/gpytorch/variational/variational_strategy.py in __call__(self, x, prior)
    163                 self.updated_strategy.fill_(True)
    164 
--&gt; 165         return super().__call__(x, prior=prior)

~/anaconda3/envs/gpytorchgit/lib/python3.7/site-packages/gpytorch/variational/_variational_strategy.py in __call__(self, x, prior)
    125                 inducing_points,
    126                 inducing_values=variational_dist_u.mean,
--&gt; 127                 variational_inducing_covar=variational_dist_u.lazy_covariance_matrix,
    128             )
    129         elif isinstance(variational_dist_u, Delta):

~/anaconda3/envs/gpytorchgit/lib/python3.7/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     22 
     23     def __call__(self, *inputs, **kwargs):
---&gt; 24         outputs = self.forward(*inputs, **kwargs)
     25         if isinstance(outputs, list):
     26             return [_validate_module_outputs(output) for output in outputs]

~/anaconda3/envs/gpytorchgit/lib/python3.7/site-packages/gpytorch/variational/variational_strategy.py in forward(self, x, inducing_points, inducing_values, variational_inducing_covar)
    106                 interp_term.transpose(-1, -2), (inducing_values - self.prior_distribution.mean).unsqueeze(-1)
    107             ).squeeze(-1)
--&gt; 108             + test_mean
    109         )
    110 

RuntimeError: The size of tensor a (80) must match the size of tensor b (100) at non-singleton dimension 1
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='lmao14' date='2020-04-07T12:00:10Z'>
		My guess is that this is related to &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/1024&gt;#1024&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='lmao14' date='2020-04-07T12:15:58Z'>
		Never mind - this is actually fixed on gpytorch#master from &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/1029&gt;#1029&lt;/denchmark-link&gt;
 . We'll hopefully have a new release out shortly that includes this fix.
I'm going to close this issue for now - install gpytorch#master for now (pip install --upgrade git+https://github.com/cornellius-gp/gpytorch.git) or wait for the next release soon :)
		</comment>
	</comments>
</bug>