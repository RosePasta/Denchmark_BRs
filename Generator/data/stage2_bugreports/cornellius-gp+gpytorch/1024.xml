<bug id='1024' author='GP781' open_date='2020-01-16T17:49:59Z' closed_time='2020-04-07T17:08:47Z'>
	<summary>[Bug] Batch Independent Multioutput GP returns an error at 4 outputs</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

When extending the notebook example &lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/03_Multitask_Exact_GPs/Batch_Independent_Multioutput_GP.html&gt;Batch Independent Multioutput GP&lt;/denchmark-link&gt;
 by two additional outputs GPyTorch returns  and . It seems that the line  is the reason for that.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
import math
import torch
import gpytorch
from matplotlib import pyplot as plt

train_x = torch.linspace(0, 1, 100)

train_y = torch.stack([
    torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,
    torch.sin(train_x * math.pi) + torch.randn(train_x.size()) * 0.2,
    torch.cos(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2,
    torch.cos(train_x * math.pi) + torch.randn(train_x.size()) * 0.2,
], -1)

class BatchIndependentMultitaskGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super().__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([4]))
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([4])),
            batch_shape=torch.Size([4])
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultitaskMultivariateNormal.from_batch_mvn(
            gpytorch.distributions.MultivariateNormal(mean_x, covar_x)
        )


likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=4)
model = BatchIndependentMultitaskGPModel(train_x, train_y, likelihood)

# this is for running the notebook in our testing framework
import os
smoke_test = ('CI' in os.environ)
training_iterations = 2 if smoke_test else 50


# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam([
    {'params': model.parameters()},  # Includes GaussianLikelihood parameters
], lr=0.1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

for i in range(training_iterations):
    optimizer.zero_grad()
    output = model(train_x)
    loss = -mll(output, train_y)
    loss.backward()
    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))
    optimizer.step()

# Set into eval mode
model.eval()
likelihood.eval()

# Initialize plots
f, ((y1_ax, y2_ax), (y3_ax, y4_ax)) = plt.subplots(2, 2, figsize=(8, 3))

# Make predictions
with torch.no_grad(), gpytorch.settings.fast_pred_var():
    test_x = torch.linspace(0, 1, 51)
    predictions = likelihood(model(test_x))
    mean = predictions.mean
    lower, upper = predictions.confidence_region()

# This contains predictions for both tasks, flattened out
# The first half of the predictions is for the first task
# The second half is for the second task

# Plot training data as black stars
y1_ax.plot(train_x.detach().numpy(), train_y[:, 0].detach().numpy(), 'k*')
# Predictive mean as blue line
y1_ax.plot(test_x.numpy(), mean[:, 0].numpy(), 'b')
# Shade in confidence
y1_ax.fill_between(test_x.numpy(), lower[:, 0].numpy(), upper[:, 0].numpy(), alpha=0.5)
y1_ax.set_ylim([-3, 3])
y1_ax.legend(['Observed Data', 'Mean', 'Confidence'])
y1_ax.set_title('Observed Values (Likelihood)')

# Plot training data as black stars
y2_ax.plot(train_x.detach().numpy(), train_y[:, 1].detach().numpy(), 'k*')
# Predictive mean as blue line
y2_ax.plot(test_x.numpy(), mean[:, 1].numpy(), 'b')
# Shade in confidence
y2_ax.fill_between(test_x.numpy(), lower[:, 1].numpy(), upper[:, 1].numpy(), alpha=0.5)
y2_ax.set_ylim([-3, 3])
y2_ax.legend(['Observed Data', 'Mean', 'Confidence'])
y2_ax.set_title('Observed Values (Likelihood)')

# Plot training data as black stars
y3_ax.plot(train_x.detach().numpy(), train_y[:, 2].detach().numpy(), 'k*')
# Predictive mean as blue line
y3_ax.plot(test_x.numpy(), mean[:, 2].numpy(), 'b')
# Shade in confidence
y3_ax.fill_between(test_x.numpy(), lower[:, 2].numpy(), upper[:, 2].numpy(), alpha=0.5)
y3_ax.set_ylim([-3, 3])
y3_ax.legend(['Observed Data', 'Mean', 'Confidence'])
y3_ax.set_title('Observed Values (Likelihood)')

# Plot training data as black stars
y4_ax.plot(train_x.detach().numpy(), train_y[:, 3].detach().numpy(), 'k*')
# Predictive mean as blue line
y4_ax.plot(test_x.numpy(), mean[:, 3].numpy(), 'b')
# Shade in confidence
y4_ax.fill_between(test_x.numpy(), lower[:, 3].numpy(), upper[:, 3].numpy(), alpha=0.5)
y4_ax.set_ylim([-3, 3])
y4_ax.legend(['Observed Data', 'Mean', 'Confidence'])
y4_ax.set_title('Observed Values (Likelihood)')
** Stack trace/error message **
&lt;denchmark-code&gt;RuntimeError: Attempting to tensor index a non-batch matrix's batch dimensions. Got batch index {batch_indices} but my shape was {self.shape}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

I'm attempting to use the &lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/03_Multitask_Exact_GPs/Batch_Independent_Multioutput_GP.html&gt;Batch Independent Multioutput GP&lt;/denchmark-link&gt;
 example to get additional outputs.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

GPyTorch v1.0.0
PyTorch v1.3.0
Windows

	</description>
	<comments>
		<comment id='1' author='GP781' date='2020-01-16T18:17:53Z'>
		In addition to the symptom of the bug here, there are two problems I see as well:

We are getting an InterpolatedLazyTensor in the predictive covariance matrix, which I don't think should happen, e.g. the functionality you are looking for can be accomplished in this notebook, but without this error or getting an ILT.
The RuntimeError string is clearly broken.

		</comment>
		<comment id='2' author='GP781' date='2020-01-16T18:45:12Z'>
		I have already thought in this direction, that is probably the reason. And thanks, I know &lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/08_Advanced_Usage/Simple_Batch_Mode_GP_Regression.html&gt;Batch GP Regression&lt;/denchmark-link&gt;
 and have already considered it as an alternative, but specifically because of the  option I wanted to compare it with &lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/03_Multitask_Exact_GPs/Batch_Independent_Multioutput_GP.html&gt;Batch Independent Multioutput GP&lt;/denchmark-link&gt;
. What exactly are the differences between these two possible approaches?
		</comment>
		<comment id='3' author='GP781' date='2020-01-16T19:42:00Z'>
		Pretty much just the interface is nicer in some ways for the multitask approach would be my guess. Not 100% sure if there are other rationales, since I think the new notebook is one of &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
's.
Either way, clearly looks like a bug! I'll look in to it.
		</comment>
		<comment id='4' author='GP781' date='2020-01-16T20:20:58Z'>
		I thought it is more than just the interface, since the model structures are not absolutely identical.
Maybe &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 can briefly outline the background of his new notebook.
Anyway, thanks for fixing the bug.
		</comment>
		<comment id='5' author='GP781' date='2020-01-25T21:00:21Z'>
		&lt;denchmark-h:h2&gt;Why the InterpolatedLazyTensor gets created&lt;/denchmark-h&gt;

I've been doing some digging. The reason the bug is only triggered with 4 tasks (and not 3 or 2) is because of &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/models/exact_prediction_strategies.py#L293-L299&gt;this if statement in exact_prediction_strategies&lt;/denchmark-link&gt;
:
if joint_covar.size(-1) &lt;= settings.max_eager_kernel_size.value():
    # this branch works
else:
    test_test_covar = joint_covar[..., self.num_train :, self.num_train :]
    test_train_covar = joint_covar[..., self.num_train :, : self.num_train]
Here, joint_covar is a BlockInterleavedLazyTensor, created from the batch of covariance matrices that a MultitaskMultivariateNormal has. The first part of the issue is that BlockInterleavedLazyTensor does not have a default __getitem__ method, so the generic one from LazyTensor is called. This winds up being inefficient, but it should work
The s are created because creating one is the default behaviour used when indexing generic s. The relevant function is &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/lazy_tensor.py#L182&gt;LazyTensor._getitem&lt;/denchmark-link&gt;
. Creating the  does not trigger the bug directly, but it causes it later on.
&lt;denchmark-h:h2&gt;What triggers the bug: indexing LazyEvaluatedKernelTensor&lt;/denchmark-h&gt;

The bug is triggered in the course of calling . The code attempts to get the diagonal of the aforementioned , which makes end up at &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/lazy_tensor.py#L294&gt;line 192 in LazyTensor._getitem&lt;/denchmark-link&gt;
:
base_lazy_tensor = self._getitem(_noop_index, _noop_index, *batch_indices)._expand_batch(final_shape)
Here self is a LazyEvaluatedKernelTensor, and that's where the bug is. If we replace this with:
base_lazy_tensor = self.evaluate().__getitem__((*batch_indices, _noop_index, _noop_index))
from .non_lazy_tensor import NonLazyTensor
base_lazy_tensor = NonLazyTensor(base_lazy_tensor)._expand_batch(final_shape)
then it works. The problem is with our good friend &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/gpytorch/lazy/lazy_evaluated_kernel_tensor.py#L43&gt;LazyEvaluatedKernelTensor._getitem&lt;/denchmark-link&gt;
. Here,  and  (in fact they are the same tensor). These tensors have no batch dimension, but the  argument is a huge tensor (containing  many times). Thus the  is triggered.
&lt;denchmark-h:h2&gt;How to fix this bug?&lt;/denchmark-h&gt;

Two things can (and probably should) be done to address this bug:

Add a __getitem__ method to BlockInterleavedLazyTensor, that is used when the indices are slices and the slice limits are multiples of the block dimension of the tensor. This would prevent the creation of an InterpolatedLazyTensor in this case, and be most efficient.
Fix the LazyEvaluatedKernelTensor indexing. In this case, if x1 and/or x2 have no batch dimension, it doesn't need to be indexed!

Do these seem like a good plan? I'd like to make a pull request with a bit of back and forth, but another day, I've already spent a couple of hours on this today.
PS: great catch, GP781!
		</comment>
		<comment id='6' author='GP781' date='2020-01-29T20:18:57Z'>
		Problem solving par excellence. Great bug fix, rhaps0dy!
By the way, can you summarize why there are two different approaches for independent multioutput GPs (notebook &lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/08_Advanced_Usage/Simple_Batch_Mode_GP_Regression.html&gt;Batch GP Regression&lt;/denchmark-link&gt;
 vs. &lt;denchmark-link:https://gpytorch.readthedocs.io/en/latest/examples/03_Multitask_Exact_GPs/Batch_Independent_Multioutput_GP.html&gt;Batch Independent Multioutput GP&lt;/denchmark-link&gt;
)?
		</comment>
		<comment id='7' author='GP781' date='2020-02-03T13:00:13Z'>
		Wow &lt;denchmark-link:https://github.com/rhaps0dy&gt;@rhaps0dy&lt;/denchmark-link&gt;
 - thanks so much for the detailed find! I'll try to work on a fix once ICML is all over.
&lt;denchmark-link:https://github.com/GP781&gt;@GP781&lt;/denchmark-link&gt;
 - they're basically the same approach, but with slight subtle differences. Batch independent multioutput GP returns MultitaskMultivariateNormal distributions (i.e. distributions with shape [(batch_size) x num_data x num_tasks]). The batch GP example return MultivariateNormal distributions (i.e. distributions with shape [(batch_size x num_tasks) x num_data]. They're functionally equivalent, but the multioutput example is preferable if you're actually modeling multi-output functions (this way you can use our existing multi-task infrastructure with e.g. multi-task likelihoods).
		</comment>
	</comments>
</bug>