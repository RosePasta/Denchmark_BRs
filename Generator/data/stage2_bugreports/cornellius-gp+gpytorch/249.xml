<bug id='249' author='tzoiker' open_date='2018-09-01T20:01:45Z' closed_time='2018-10-02T04:12:10Z'>
	<summary>KISS-GP + RBF kernel separate lengthscales error</summary>
	<description>
In KISS-GP setting with RBF kernel, for example, in 2D case (notebook attached), setting ard_num_dims=2 gives the following error during loss calculation:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-144-12b763efd692&gt; in &lt;module&gt;()
     19     output = model(train_x)
     20     # TODO: Fix this view call!!
---&gt; 21     loss = -mll(output, train_y)
     22     loss.backward()
     23     print('Iter %d/%d - Loss: %.3f' % (i + 1, n_iter, loss.item()))

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
    160 
    161     def __call__(self, *inputs, **kwargs):
--&gt; 162         outputs = self.forward(*inputs, **kwargs)
    163         if torch.is_tensor(outputs) or isinstance(outputs, RandomVariable) or isinstance(outputs, LazyVariable):
    164             return outputs

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py in forward(self, output, target)
     49 
     50         # Get log determininat and first part of quadratic form
---&gt; 51         inv_quad, log_det = covar.inv_quad_log_det(inv_quad_rhs=(target - mean).unsqueeze(-1), log_det=True)
     52 
     53         # Add terms for SGPR / when inducing points are learned

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_variable.py in inv_quad_log_det(self, inv_quad_rhs, log_det)
    577         matrix_size = self.size(-1)
    578         batch_size = self.size(0) if self.ndimension() == 3 else None
--&gt; 579         tensor_cls = self.tensor_cls
    580 
    581         args = lazy_var.representation()

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_variable.py in tensor_cls(self)
    882     def tensor_cls(self):
    883         if not hasattr(self, "_tensor_cls"):
--&gt; 884             first_item = self.representation()[0]
    885             if isinstance(first_item, Variable):
    886                 first_item = first_item.data

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_variable.py in representation(self)
    733                 representation.append(arg)
    734             elif isinstance(arg, LazyVariable):
--&gt; 735                 representation += list(arg.representation())
    736             else:
    737                 raise RuntimeError("Representation of a LazyVariable should consist only of Variables")

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_variable.py in representation(self)
    733                 representation.append(arg)
    734             elif isinstance(arg, LazyVariable):
--&gt; 735                 representation += list(arg.representation())
    736             else:
    737                 raise RuntimeError("Representation of a LazyVariable should consist only of Variables")

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_variable.py in representation(self)
    109 
    110     def representation(self):
--&gt; 111         return self.evaluate_kernel().representation()
    112 
    113     def representation_tree(self):

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_variable.py in evaluate_kernel(self)
     96                 x2 = self.x2
     97 
---&gt; 98             self._cached_kernel_eval = super(Kernel, self.kernel).__call__(x1, x2, **self.params)
     99             if self.squeeze_row:
    100                 self._cached_kernel_eval.squeeze_(-2)

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
    160 
    161     def __call__(self, *inputs, **kwargs):
--&gt; 162         outputs = self.forward(*inputs, **kwargs)
    163         if torch.is_tensor(outputs) or isinstance(outputs, RandomVariable) or isinstance(outputs, LazyVariable):
    164             return outputs

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/kernels/multitask_kernel.py in forward(self, x1, x2)
     61     def forward(self, x1, x2):
     62         covar_i = self.task_covar_module.covar_matrix
---&gt; 63         covar_x = self.data_covar_module.forward(x1, x2)
     64         if covar_x.size(0) == 1:
     65             covar_x = covar_x[0]

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/kernels/grid_interpolation_kernel.py in forward(self, x1, x2, **kwargs)
     52 
     53     def forward(self, x1, x2, **kwargs):
---&gt; 54         base_lazy_var = self._inducing_forward()
     55         if x1.size(0) &gt; 1:
     56             base_lazy_var = base_lazy_var.repeat(x1.size(0), 1, 1)

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/kernels/grid_interpolation_kernel.py in _inducing_forward(self)
     46     def _inducing_forward(self):
     47         inducing_points_var = Variable(self.inducing_points)
---&gt; 48         return super(GridInterpolationKernel, self).forward(inducing_points_var, inducing_points_var)
     49 
     50     def forward_diag(self, x1, x2, **kwargs):

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/kernels/grid_kernel.py in forward(self, x1, x2, **kwargs)
     38             if settings.use_toeplitz.on():
     39                 first_item = grid_var[:, 0:1].contiguous()
---&gt; 40                 covar_columns = self.base_kernel_module(first_item, grid_var, **kwargs).evaluate()
     41                 covars = [ToeplitzLazyVariable(covar_columns[i : i + 1].squeeze(-2)) for i in range(n_dim)]
     42             else:

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_variable.py in evaluate(self)
    115 
    116     def evaluate(self):
--&gt; 117         return self.evaluate_kernel().evaluate()
    118 
    119     def exact_predictive_mean(self, full_mean, train_labels, n_train, likelihood, precomputed_cache=None):

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/lazy/lazy_evaluated_kernel_variable.py in evaluate_kernel(self)
     96                 x2 = self.x2
     97 
---&gt; 98             self._cached_kernel_eval = super(Kernel, self.kernel).__call__(x1, x2, **self.params)
     99             if self.squeeze_row:
    100                 self._cached_kernel_eval.squeeze_(-2)

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
    160 
    161     def __call__(self, *inputs, **kwargs):
--&gt; 162         outputs = self.forward(*inputs, **kwargs)
    163         if torch.is_tensor(outputs) or isinstance(outputs, RandomVariable) or isinstance(outputs, LazyVariable):
    164             return outputs

~/anaconda/envs/py3/lib/python3.6/site-packages/gpytorch/kernels/rbf_kernel.py in forward(self, x1, x2)
    104     def forward(self, x1, x2):
    105         lengthscales = self.log_lengthscale.exp().mul(math.sqrt(2)).clamp(self.eps, 1e5)
--&gt; 106         diff = (x1.unsqueeze(2) - x2.unsqueeze(1)).div_(lengthscales.unsqueeze(1))
    107         return diff.pow_(2).sum(-1).mul_(-1).exp_()

RuntimeError: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 3
&lt;/denchmark-code&gt;

Setting ard_num_dims=1 or ard_num_dims=None works fine, as well as without kernel interpolation. Matern kernel works fine for ard_num_dims=2.
&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/files/2342721/RBF_ard_num_dims.ipynb.zip&gt;RBF_ard_num_dims.ipynb.zip&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='tzoiker' date='2018-09-02T13:46:46Z'>
		I see what the problem is, and I think this is related to some fixes we are planning to do for &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/248&gt;#248&lt;/denchmark-link&gt;
. Basically, the issue is that in  we handle the separate dimensions of the grid as batches rather than as dimensions because we only need the kernel between all grid points and the first point in each grid dimension only.
An easy solution to this for now is to do RBFKernel(batch_size=2) instead of RBFKernel(ard_num_dims=2) (you'll actually want to do this for MaternKernel as well).
&lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 What do you think the best longer term solution to this is?
		</comment>
		<comment id='2' author='tzoiker' date='2018-09-03T00:25:01Z'>
		I think that the best solution will be what’s proposed in &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/248&gt;#248&lt;/denchmark-link&gt;
 - separating the length scale from the kernel
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sep 2, 2018, 6:46 AM -0700, Jake Gardner ***@***.***&gt;, wrote:
 I see what the problem is, and I think this is related to some fixes we are planning to do for #248. Basically, the issue is that in GridKernel we handle the separate dimensions of the grid as batches rather than as dimensions because we only need the kernel between all grid points and the first point in each grid dimension only.
 An easy solution to this for now is to do RBFKernel(batch_size=2) instead of RBFKernel(ard_num_dims=2) (you'll actually want to do this for MaternKernel as well).
 @gpleiss What do you think the best longer term solution to this is?
 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or mute the thread.


		</comment>
		<comment id='3' author='tzoiker' date='2018-09-03T00:28:21Z'>
		&lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 I definitely still agree we should do that -- the additional issue here in my mind is consistency: lengthscales need to be  for SKI because of how Grid kernel calls  RBF kernel but  for everything else. Is this okay?
		</comment>
		<comment id='4' author='tzoiker' date='2018-10-02T04:12:10Z'>
		Fixed by &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/307&gt;#307&lt;/denchmark-link&gt;
 on the beta branch.
		</comment>
		<comment id='5' author='tzoiker' date='2020-05-26T00:09:50Z'>
		I've got a quick question related to this thread. Imagine, one needs to implement KissGP with separate length and separate grid size across dimensions. Is there any easy way to do it in gpytorch?
ProductStructureKernel doesn't seem to help as it doesn't accept different grid dimensions.
		</comment>
		<comment id='6' author='tzoiker' date='2020-06-08T19:14:30Z'>
		Do you need ProductStructure (i.e. SKIP), or do you just want vanilla KissGP?
This is possible with vanilla KissGP - just pass a list of grid sizes (one for each dimension) and add the ard_num_dims=d argument to the base kernel.
		</comment>
	</comments>
</bug>