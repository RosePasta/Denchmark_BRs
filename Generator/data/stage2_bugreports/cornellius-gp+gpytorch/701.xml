<bug id='701' author='ktran9891' open_date='2019-05-20T20:38:40Z' closed_time='2019-05-22T17:49:15Z'>
	<summary>[Bug] How to debug memory issues with multi-GPU</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

I am trying to run the &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/examples/01_Simple_GP_Regression/Simple_MultiGPU_GP_Regression.ipynb&gt;example notebook&lt;/denchmark-link&gt;
 for multi-GPU regression, but I am running out of memory during the prediction stage. Everything is very well-wrapped, which ironically makes this harder for me to debug.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;# Get into evaluation (predictive posterior) mode
model.eval()
likelihood.eval()

with torch.no_grad(), gpytorch.settings.fast_pred_var():
    latent_pred = model(test_x)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-8-f87329a69cc1&gt; in &lt;module&gt;
      4 
      5 with torch.no_grad(), gpytorch.settings.fast_pred_var():
----&gt; 6     latent_pred = model(test_x)

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/models/exact_gp.py in __call__(self, *args, **kwargs)
    263             # Make the prediction
    264             with settings._use_eval_tolerance():
--&gt; 265                 predictive_mean, predictive_covar = self.prediction_strategy.exact_prediction(full_mean, full_covar)
    266 
    267             # Reshape predictive mean to match the appropriate event shape

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/models/exact_prediction_strategies.py in exact_prediction(self, joint_mean, joint_covar)
    262 
    263         return (
--&gt; 264             self.exact_predictive_mean(test_mean, test_train_covar),
    265             self.exact_predictive_covar(test_test_covar, test_train_covar),
    266         )

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/models/exact_prediction_strategies.py in exact_predictive_mean(self, test_mean, test_train_covar)
    279         # For efficiency - we can use addmv in the 2d case
    280         if test_train_covar.dim() == 2:
--&gt; 281             res = torch.addmv(test_mean, delazify(test_train_covar), self.mean_cache)
    282         # In other cases - we'll use the standard infrastructure
    283         else:

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py in delazify(obj)
   1745         return obj
   1746     elif isinstance(obj, LazyTensor):
-&gt; 1747         return obj.evaluate()
   1748     else:
   1749         raise TypeError("object of class {} cannot be made into a Tensor".format(obj.__class__.__name__))

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/utils/memoize.py in g(self, *args, **kwargs)
     32         cache_name = name if name is not None else method
     33         if not is_in_cache(self, cache_name):
---&gt; 34             add_to_cache(self, cache_name, method(self, *args, **kwargs))
     35         return get_from_cache(self, cache_name)
     36 

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py in evaluate(self)
    284     @cached
    285     def evaluate(self):
--&gt; 286         return self.evaluate_kernel().evaluate()
    287 
    288     def ndimension(self):

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/utils/memoize.py in g(self, *args, **kwargs)
     32         cache_name = name if name is not None else method
     33         if not is_in_cache(self, cache_name):
---&gt; 34             add_to_cache(self, cache_name, method(self, *args, **kwargs))
     35         return get_from_cache(self, cache_name)
     36 

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py in evaluate(self)
    850             eye = torch.eye(num_rows, dtype=self.dtype, device=self.device)
    851             eye = eye.expand(*self.batch_shape, num_rows, num_rows)
--&gt; 852             res = self.transpose(-1, -2).matmul(eye).transpose(-1, -2).contiguous()
    853         else:
    854             eye = torch.eye(num_cols, dtype=self.dtype, device=self.device)

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/lazy/lazy_tensor.py in matmul(self, other)
   1085 
   1086         func = Matmul(self.representation_tree())
-&gt; 1087         return func(other, *self.representation())
   1088 
   1089     @property

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/functions/_matmul.py in forward(self, rhs, *matrix_args)
     19 
     20         lazy_tsr = self.representation_tree(*matrix_args)
---&gt; 21         res = lazy_tsr._matmul(rhs)
     22 
     23         to_save = [orig_rhs] + list(matrix_args)

~/miniconda3/envs/gpytorch/lib/python3.7/site-packages/gpytorch/lazy/cat_lazy_tensor.py in _matmul(self, rhs)
    254             # copy result back to output device
    255             res_list = [x.to(output_device) for x in res_list]
--&gt; 256             res = torch.sum(torch.stack(res_list), dim=0)
    257         else:
    258             output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)

RuntimeError: CUDA out of memory. Tried to allocate 2.49 GiB (GPU 0; 11.75 GiB total capacity; 4.18 GiB already allocated; 1.74 GiB free; 382.00 MiB cached)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Creation and assignment of latent_pred object
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


gpytorch=0.3.2
pytorch=1.1.0
OS=Ubuntu 18.04.2

&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

I am guessing that I should be allocating less GPU memory, but I am not sure how to do that. I should also note that my training finished in only 4 iterations, not 14 (as per the output of the example).
	</description>
	<comments>
		<comment id='1' author='ktran9891' date='2019-05-20T22:09:12Z'>
		&lt;denchmark-link:https://github.com/ktran9891&gt;@ktran9891&lt;/denchmark-link&gt;
 My guess in this case is with smaller GPUs you may need to use checkpointing at test time too. Try adding  to the list of contexts at test time (e.g., .  is a pretty small checkpoint size, but if this works it'll answer the question about what's going on.
Maybe we should change the notebook to search for a reasonable checkpoint size for making predictions too.
		</comment>
		<comment id='2' author='ktran9891' date='2019-05-21T15:05:00Z'>
		&lt;denchmark-code&gt;with torch.no_grad(), gpytorch.settings.fast_pred_var(), \
      gpytorch.beta_features.checkpoint_kernel(1):
    latent_pred = model(test_x)
&lt;/denchmark-code&gt;

I even went all the way down to 1, and it is still throwing a memory error. Is there any interaction with the check point size being used during training, or should they be independent?
		</comment>
		<comment id='3' author='ktran9891' date='2019-05-21T20:50:27Z'>
		&lt;denchmark-link:https://github.com/ktran9891&gt;@ktran9891&lt;/denchmark-link&gt;
 this does appear to be a legitimate memory regression since v0.3.1, on which the notebook works with reasonable checkpoint sizes. I'm looking in to this
		</comment>
		<comment id='4' author='ktran9891' date='2019-05-21T22:17:00Z'>
		&lt;denchmark-link:https://github.com/ktran9891&gt;@ktran9891&lt;/denchmark-link&gt;
 I've updated the notebook and it seems to work for me now. You may have to adjust the checkpoint sizes at test time.
		</comment>
		<comment id='5' author='ktran9891' date='2019-05-22T13:08:45Z'>
		It works now, thanks!
How exactly does the new code work? It seems that we initialize the model by making predictions on two data points, and then the initialization creates some cache that allows us to make predictions on everything. Is that the gist of it? If this needs to happen every time, would it be worth wrapping/hiding this for future users?
		</comment>
		<comment id='6' author='ktran9891' date='2019-05-22T17:49:15Z'>
		Great, glad it's working!
The way predictions currently work in GPyTorch is that the test time caches are computed the first time you make predictions. Ordinarily there's not a reason we'd explicitly call this out to people; however, on datasets this large we wanted to emphasize that making predictions with the model long term is quite fast (the second cell) despite the relatively lengthy setup time to compute the caches, which only needs to be done once.
		</comment>
		<comment id='7' author='ktran9891' date='2019-05-22T17:51:36Z'>
		Ah, I see. Thanks for all the help!
		</comment>
	</comments>
</bug>