<bug id='655' author='lmao14' open_date='2019-04-21T14:23:45Z' closed_time='2019-04-22T13:09:48Z'>
	<summary>ValueError: NonLazyTensor expects a matrix (or batches of matrices) - got a Tensor of size torch.Size([50]).</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi,
I encounter an error when running SVGP with an additive kernel, which did not happen to me when using gpytorch 0.2.1.
&lt;denchmark-code&gt;~/anaconda3/envs/gpytorch2/lib/python3.6/site-packages/gpytorch/variational/whitened_variational_strategy.py in forward(self, x)
    188 
    189             if self.training:
--&gt; 190                 data_covariance = DiagLazyTensor((data_data_covar.diag() - interp_data_data_var).clamp(0, math.inf))
    191             else:
    192                 neg_induc_data_data_covar = induc_induc_covar.inv_matmul(
&lt;/denchmark-code&gt;

** Code snippet to reproduce **
class SVGPRegressionModel(gpytorch.models.AbstractVariationalGP):
    def __init__(self, inducing_points):
        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(10)
        variational_strategy = gpytorch.variational.WhitenedVariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)
        super(SVGPRegressionModel, self).__init__(variational_strategy)
        self.mean_module = gpytorch.means.ZeroMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel() + gpytorch.kernels.MaternKernel(nu=2.5))

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

x = torch.randn(50, 2)
z = torch.randn(10, 2)
model = SVGPRegressionModel(z)
output = model(x)
	</description>
	<comments>
		<comment id='1' author='lmao14' date='2019-04-21T21:16:41Z'>
		I have a fix for this up in a PR (&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/656&gt;#656&lt;/denchmark-link&gt;
)
		</comment>
	</comments>
</bug>