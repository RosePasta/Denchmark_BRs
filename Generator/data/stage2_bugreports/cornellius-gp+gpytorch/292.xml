<bug id='292' author='Balandat' open_date='2018-09-26T19:05:59Z' closed_time='2018-10-01T14:41:07Z'>
	<summary>Inconsistent behavior of .diag() method for different Lazy Tensors in batch mode</summary>
	<description>
E.g:
&lt;denchmark-code&gt;&gt; cv.lazy_vars
[&lt;gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor at 0x7f6139087278&gt;,
 &lt;gpytorch.lazy.matmul_lazy_tensor.MatmulLazyTensor at 0x7f6139087780&gt;]

&gt; [lv.size() for lv in cv.lazy_vars]
[torch.Size([2, 1, 1]), torch.Size([2, 1, 1])]

&gt;diags = [lazy_var.diag().contiguous() for lazy_var in cv.lazy_vars]
&gt;[diag.size() for diag in diags]
[torch.Size([2]), torch.Size([2, 1])]
&lt;/denchmark-code&gt;

This causes inconsistencies when using batch mode. My suggestion is to ensure that all .diag() calls reduce the last two dimensions of the tensor, i.e. for a b1 x b2 x n x n LazyTensor this would return a Tensor of Size b1 x b2 x n. This is the same as the behavior of torch.diagonal(X, dim1=-2, dim2=-1).
	</description>
	<comments>
		<comment id='1' author='Balandat' date='2018-09-27T16:51:25Z'>
		This will probably be fixed with the kernel refactor I'm working on
		</comment>
		<comment id='2' author='Balandat' date='2018-10-01T14:41:07Z'>
		I'm going to call this closed by &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/293&gt;#293&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>