<bug id='496' author='Lazloo' open_date='2019-02-04T20:18:24Z' closed_time='2019-02-07T19:55:26Z'>
	<summary>NaNs encountered on first iteration</summary>
	<description>
Hey,
this question is related to &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/issues/481&gt;#481&lt;/denchmark-link&gt;
. I try to apply gpytorch to a toy data set with two input and two output variables:
&lt;denchmark-code&gt;import math
import torch
import numpy as np
import gpytorch
from matplotlib import pyplot as plt


n = 20
train_x = torch.zeros(pow(n, 2), 2)
for i in range(n):
    for j in range(n):
        # Each coordinate varies from 0 to 1 in n=100 steps
        train_x[i * n + j][0] = float(i) / (n-1)
        train_x[i * n + j][1] = float(j) / (n-1)

train_y_1 = (torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1]) * (2 * math.pi) + torch.randn_like(train_x[:, 0]).mul(0.01))/4
train_y_2 = torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1]) * (2 * math.pi) + torch.randn_like(train_x[:, 0]).mul(0.01)

train_y = torch.stack([train_y_1, train_y_2], -1)

test_x = torch.rand((n, len(train_x.shape)))
test_y_1 = (torch.sin(test_x[:, 0]) + torch.cos(test_x[:, 1]) * (2 * math.pi) + torch.randn_like(test_x[:, 0]).mul(0.01))/4
test_y_2 = torch.sin(test_x[:, 0]) + torch.cos(test_x[:, 1]) * (2 * math.pi) + torch.randn_like(test_x[:, 0]).mul(0.01)
test_y = torch.stack([test_y_1, test_y_2], -1)

train_x = train_x.unsqueeze(0).repeat(2, 1, 1)
train_y = train_y.transpose(-2, -1)
# Own
torch.manual_seed(1)
&lt;/denchmark-code&gt;

For sacalability I try to use the GridInterpolationKernel
&lt;denchmark-code&gt;class MultitaskGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        grid_size = gpytorch.utils.grid.choose_grid_size(train_x)
        self.covar_module = gpytorch.kernels.GridInterpolationKernel(
            gpytorch.kernels.MaternKernel(nu=1.5)
            , grid_size=grid_size, num_dims=train_x.shape[1],
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        # return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)
likelihood = gpytorch.likelihoods.GaussianLikelihood(num_tasks=2)
model = MultitaskGPModel(train_x, train_y, likelihood)

# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam([
    {'params': model.parameters()},  # Includes GaussianLikelihood parameters
], lr=0.1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

n_iter = 50
for i in range(n_iter):
    optimizer.zero_grad()
    output = model(train_x)
    # loss = -mll(output, train_y)
    loss = -mll(output, train_y).sum()
    loss.backward()
    # print('Iter %d/%d - Loss: %.3f' % (i + 1, n_iter, loss.item()))
    optimizer.step()
&lt;/denchmark-code&gt;

However, here I get the Error:

RuntimeError: The size of tensor a (2) must match the size of tensor b (400) at non-singleton dimension 0

Without the gpytorch.kernels.GridInterpolationKernel everything works fine. Can someone help?
THX
Lazloo
	</description>
	<comments>
		<comment id='1' author='Lazloo' date='2019-02-05T16:20:17Z'>
		self.covar_module = gpytorch.kernels.GridInterpolationKernel( 
    gpytorch.kernels.MaternKernel(nu=1.5),
    grid_size=grid_size, num_dims=train_x.shape[1]
)
Your num_dims argument is incorrect. As you have it defined, train_x is a 2 x n x d tensor. You want to use train_x.shape[-1] so that num_dims=d.
		</comment>
		<comment id='2' author='Lazloo' date='2019-02-05T20:21:11Z'>
		THX @&lt;denchmark-link:/gpleiss&gt;gpleiss&lt;/denchmark-link&gt;
 For the tip, however now I get:

RuntimeError: The first column and first row of the Toeplitz matrix should have the same first element, otherwise the value of T[0,0] is ambiguous. Got: c[0]=tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
requires_grad=True) and r[0]=tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
requires_grad=True)

		</comment>
		<comment id='3' author='Lazloo' date='2019-02-05T21:58:50Z'>
		When did this error occur? On the first iteration of training or later iterations?
		</comment>
		<comment id='4' author='Lazloo' date='2019-02-06T13:36:14Z'>
		HEy &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
, on the first iteration.
		</comment>
		<comment id='5' author='Lazloo' date='2019-02-07T05:26:54Z'>
		Shall I open rather a new issue as the headline of thus issue ist not an issue anymore?
		</comment>
		<comment id='6' author='Lazloo' date='2019-02-07T14:55:38Z'>
		&lt;denchmark-link:https://github.com/Lazloo&gt;@Lazloo&lt;/denchmark-link&gt;
 no we can track it here. &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 is just a little short staffed dealing with issues this week since I'm in the process of a cross country move :-).
		</comment>
		<comment id='7' author='Lazloo' date='2019-02-07T19:55:26Z'>
		&lt;denchmark-link:https://github.com/Lazloo&gt;@Lazloo&lt;/denchmark-link&gt;
 this is seems to have been fixed on the  branch, but has not made it into a stable release. We will be cutting a new release later this week, which will fix the issue.
For now, you can install the latest version of gpytorch using pip install git+https://github.com/cornellius-gp/gpytorch.git. I have confirmed that the issue is fixed on this branch.
		</comment>
		<comment id='8' author='Lazloo' date='2019-02-08T22:55:36Z'>
		New (stable) release that fixes this issue
		</comment>
		<comment id='9' author='Lazloo' date='2019-02-08T22:55:44Z'>
		v0.2.0
		</comment>
	</comments>
</bug>