<bug id='900' author='wecacuee' open_date='2019-10-15T04:44:25Z' closed_time='2019-10-16T19:25:07Z'>
	<summary>[Bug] `InterpolatedLazyTensor` cannot handle rectangular `base_lazy_tensors`</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

InterpolatedLazyTensor results in error when base_lazy_tensor is not square
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
    itplzt = InterpolatedLazyTensor(NonLazyTensor(torch.rand(2, 3)))
    res = itplzt @ torch.eye(3)
    assert res.shape == (2, 3)
** Stack trace/error message **
&lt;denchmark-code&gt;(py37) vdhiman@dwarf:~/wrk/BayesCBF_ws/BayesCBF$ python tests/test_interpolated_lazy_tensor.py 
Traceback (most recent call last):
  File "tests/test_interpolated_lazy_tensor.py", line 30, in &lt;module&gt;
    test_interpolated_lazy_tensor()
  File "tests/test_interpolated_lazy_tensor.py", line 7, in test_interpolated_lazy_tensor
    res = itplzt @ torch.eye(3)
  File "/home/vdhiman/wrk/gpytorch/gpytorch/lazy/lazy_tensor.py", line 1731, in __matmul__
    return self.matmul(other)
  File "/home/vdhiman/wrk/gpytorch/gpytorch/lazy/interpolated_lazy_tensor.py", line 393, in matmul
    right_interp_res = left_t_interp(self.right_interp_indices, self.right_interp_values, tensor, base_size)
  File "/home/vdhiman/wrk/gpytorch/gpytorch/utils/interpolation.py", line 202, in left_t_interp
    values = rhs.unsqueeze(-2) * interp_values.unsqueeze(-1)
RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

Expect res to be a Tensor of size (2,3)
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


GPyTorch Version 0.3.5
PyTorch Version 1.2.0
Ubuntu 16.04.6 LTS

&lt;denchmark-h:h2&gt;Additional information&lt;/denchmark-h&gt;

The test cases in test_interpolated_lazy_tensor.py carefully construct PSD, square symmetric base_lazy_tensors but the constructor or the documentation does not say so. I am specifically getting errors when InterpolatedLazyTensor is being constructed at lazy_tensor.py:307. In that case, the baze_lazy_tensor is not square.
Adding the following test case to test_interpolated_lazy_tensor.py fails almost all test cases
class TestInterpolatedLazyTensorRectangular(LazyTensorTestCase, unittest.TestCase):
    def create_lazy_tensor(self):
        itplzt = InterpolatedLazyTensor(NonLazyTensor(torch.rand(2, 3)))
        return itplzt

    def evaluate_lazy_tensor(self, lazy_tensor):
        return lazy_tensor.base_lazy_tensor.tensor
	</description>
	<comments>
		<comment id='1' author='wecacuee' date='2019-10-16T17:51:32Z'>
		I think this may just be a bug in how we handle the case where you don't specify the interpolation components at all, which you aren't in your code (num_rows is used for both):



gpytorch/gpytorch/lazy/interpolated_lazy_tensor.py


         Line 60
      in
      5081555






 right_interp_indices = torch.arange(0, num_rows, dtype=torch.long, device=base_lazy_tensor.device) 





I believe if they are specified, it may work for rectangular matrices? I'll look in to seeing if this is easy to fix -- I honestly don't remember if we intended ILT to work for rectangular matraices.
		</comment>
		<comment id='2' author='wecacuee' date='2019-10-16T18:24:54Z'>
		Thanks for responding.
I think that is a bug. But that is not the bug that I am facing for my usage. Maybe, I will open another ticket for it.
		</comment>
		<comment id='3' author='wecacuee' date='2019-10-16T18:42:04Z'>
		BTW Why does InterpolatedLazyTensor have both _matmul as well as matmul?
Also, is there a reason why unit tests are disabled on Travis?
		</comment>
		<comment id='4' author='wecacuee' date='2019-10-16T19:25:07Z'>
		Fixed by &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/906&gt;#906&lt;/denchmark-link&gt;

_matmul only gets run in nograd contexts / pytorch Functions. matmul actually calls the Matmul pytorch function.
		</comment>
	</comments>
</bug>