<bug id='1054' author='yosungho' open_date='2020-02-23T17:42:02Z' closed_time='2020-02-26T02:56:22Z'>
	<summary>[Question] Way to use ard_num_dims</summary>
	<description>
Thanks for sharing your great tool. I have a question for using ard_num_dims.
I am modifying my code to use different length-scale at each grid axis.
But, I am getting the error and do not understand what part is going wrong.
Please help me find any clue.
Error Message:
RuntimeError: Expected the input to have 2 dimensionality (based on the ard_num_dims argument). Got 1.
Since my test_x input is 2 dimension [478555, 2], I especially do not understand why it received only 1.
Train and Test Data Size
train_x:  torch.Size([20486, 2])
train_y:  torch.Size([20486])
test_x:  torch.Size([478555, 2])
Code Modification
ard_num_dims = 2
'covar_module.base_kernel.lengthscale': torch.tensor([0.1, 0.1]) # originally it was torch.tensor(0.1)
class GridGPRegressionModel(gpytorch.models.ExactGP):
        def __init__(self, train_x, train_y, likelihood):
            super(GridGPRegressionModel, self).__init__(train_x, train_y, likelihood)
            
            # SKI requires a grid size hyperparameter. This util can help with that
            grid_size = gpytorch.utils.grid.choose_grid_size(train_x)

            self.mean_module = gpytorch.means.ConstantMean()
            self.covar_module = gpytorch.kernels.GridInterpolationKernel(gpytorch.kernels.MaternKernel(nu=nu_factor, ard_num_dims = 2), grid_size=grid_size, num_dims=2)
            
        def forward(self, x):
            mean_x = self.mean_module(x)
            covar_x = self.covar_module(x)
            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=noises, learn_additional_noise=False)
model = GridGPRegressionModel(train_x, train_y, likelihood)

hypers = {
        'covar_module.base_kernel.lengthscale': torch.tensor([0.1, 0.1]),
        }
model.initialize(**hypers)

model.train()
likelihood.train()

model.eval()
likelihood.eval()

with torch.no_grad(), gpytorch.settings.fast_pred_var(), gpytorch.settings.max_root_decomposition_size(30):
        observed_pred = (model(test_x.float()))
	</description>
	<comments>
		<comment id='1' author='yosungho' date='2020-02-24T23:49:06Z'>
		Hi &lt;denchmark-link:https://github.com/yosungho&gt;@yosungho&lt;/denchmark-link&gt;
 - this is a bug on our end. &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/1059&gt;#1059&lt;/denchmark-link&gt;
 should fix it!
In the meantime, I think if you wrap your MaternKernel with a ScaleKernel then things should work as is. Otherwise, the fix should be in gpytorch#master shortly.
		</comment>
		<comment id='2' author='yosungho' date='2020-02-26T02:56:22Z'>
		Thanks &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;
 . It works with the ScaleKernel.
		</comment>
	</comments>
</bug>