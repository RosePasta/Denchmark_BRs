<bug id='1288' author='the-darklord' open_date='2020-09-16T07:16:26Z' closed_time='2020-09-16T17:48:44Z'>
	<summary>[Bug]Unexpected Error while implementing simpleGP</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

** Code snippet to reproduce **
import math
import torch
import gpytorch
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np

df = pd.read_csv('train.csv')
data = torch.Tensor(np.array(df))
train_x = data[:, :-1]
train_y = data[:, -1]

# We will use the simplest form of GP model, exact inference
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# initialize likelihood and model
likelihood = gpytorch.likelihoods.GaussianLikelihood()
model = ExactGPModel(train_x, train_y, likelihood)

training_iter = 50# if smoke_test else 50


# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

for i in range(training_iter):
    # Zero gradients from previous iteration
    optimizer.zero_grad()
    # Output from model
    output = model(train_x)
    # Calc loss and backprop gradients
    loss = -mll(output, train_y)
    loss.backward()
    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (
        i + 1, training_iter, loss.item(),
        model.covar_module.base_kernel.lengthscale.item(),
        model.likelihood.noise.item()
    ))
    optimizer.step()```

** Stack trace/error message **
Traceback (most recent call last):
File "issue.py", line 46, in 
output = model(train_x)
File "/vulcanscratch/gauravsh/myenv/lib/python3.6/site-packages/gpytorch/models/exact_gp.py", line 265, in call
raise RuntimeError("You must train on the training inputs!")
RuntimeError: You must train on the training inputs!```
&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

I was expecting for the training to take place seemlessly. However, the error raised here is very unexpected! please find the training file attached here -&gt; &lt;denchmark-link:url&gt;https://drive.google.com/file/d/1LvY9ltLOgGeb71NvwNBXTSC8slXVoMi3/view?usp=sharing&lt;/denchmark-link&gt;

I believe the error might be coming because of the peculiarity of data. If so can you please let me know what's the issue.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:


 1.2.0


1.6.0




&lt;denchmark-h:h2&gt;Additional context&lt;/denchmark-h&gt;

Add any other context about the problem here.
	</description>
	<comments>
		<comment id='1' author='the-darklord' date='2020-09-16T17:48:44Z'>
		Never mind there were NAN entries in the data...
		</comment>
	</comments>
</bug>