<bug id='475' author='Akella17' open_date='2019-01-25T14:46:19Z' closed_time='2019-01-28T14:36:40Z'>
	<summary>Encountered nan in confidence bounds (variational inference)</summary>
	<description>
Simple 3D GP Regression using AbstractVariationalGP module and AdditiveGridInterpolationVariationalStrategy:
Please find the ipython notebook in this link: &lt;denchmark-link:https://drive.google.com/file/d/1o8IkdLq_nrUF51IFdKyI64ZBLICk5is3/view?usp=sharing&gt;https://drive.google.com/file/d/1o8IkdLq_nrUF51IFdKyI64ZBLICk5is3/view?usp=sharing&lt;/denchmark-link&gt;

I wanted to know the reason why this might be occurring, and would also like to know if it would have any implications on the overall performance of GP regression (i.e. the kernel means and covariances have been finite and prolonged training seems to fit them better to the training data despite nans in confidence bounds).
	</description>
	<comments>
		<comment id='1' author='Akella17' date='2019-01-25T14:59:25Z'>
		Thanks for the example notebook. I'm investigating this now. Two things to note:

The reason you're seeing NaNs is because the returned variances are negative (which shouldn't be happening).
We are refactoring variational inference, so this issue might go away soon.

		</comment>
		<comment id='2' author='Akella17' date='2019-01-25T15:46:50Z'>
		I think I've encountered the bug. There should be a fix soon.
		</comment>
		<comment id='3' author='Akella17' date='2019-01-25T16:52:09Z'>
		First, even I thought that the reason for NaNs is the negatives in the covariance matrix. But I have encountered negative values in covariance even when the confidence predictions were finite. Secondly, even the  param_transform = softplus has no impact on the negatives in the covariance matrix.
Would I run into the same issue using Spectral Mixture Kernel as it can model negative covariances?
		</comment>
		<comment id='4' author='Akella17' date='2019-01-25T17:14:50Z'>
		Negative covariances are reasonable. Negative variances (the diagonal of the covariance) shouldn't occur.
This appears to be a bug in some of our internals, so switching to the spectral mixture kernel probably won't make a difference. It should be fixed once I patch the code.
		</comment>
		<comment id='5' author='Akella17' date='2019-01-28T14:36:40Z'>
		This is fixed in &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/061fa4fc206572a0fbf78341aaf84e0cb6fe42b0&gt;061fa4f&lt;/denchmark-link&gt;
 . Let me know if the problem persists.
		</comment>
	</comments>
</bug>