<bug id='503' author='KeAWang' open_date='2019-02-07T03:29:16Z' closed_time='2019-02-07T19:18:29Z'>
	<summary>Fast pred var? More like 1/3 as fast pred var</summary>
	<description>
It seems that after commit &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/0ff64169766bd22d1d8cae35ef0ab3f0d6f16e2e&gt;0ff6416&lt;/denchmark-link&gt;
, fast_pred_var is now 3x slower.
Using the following code
&lt;denchmark-code&gt;model.eval()
likelihood.eval()
import time

for i in range(10):
    start = time.time()
    with gpytorch.settings.fast_pred_var():
        preds = model(test_x)
    print(time.time() - start)
&lt;/denchmark-code&gt;

I get
&lt;denchmark-code&gt;0.05411267280578613
0.0015854835510253906
0.0012111663818359375
0.0011920928955078125
0.0011920928955078125
0.0011932849884033203
0.0012063980102539062
0.0011904239654541016
0.0011909008026123047
0.00119781494140625
&lt;/denchmark-code&gt;

from the previous commit &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/a1e8bcc26a4b432776620b242023adb7acf206e3&gt;a1e8bcc&lt;/denchmark-link&gt;
.
On the other hand, I get
&lt;denchmark-code&gt;0.05980277061462402
0.003302335739135742
0.0028259754180908203
0.0028007030487060547
0.0028228759765625
0.002866983413696289
0.0028259754180908203
0.0033342838287353516
0.0032806396484375
0.003268718719482422
&lt;/denchmark-code&gt;

from the breaking commit &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/commit/0ff64169766bd22d1d8cae35ef0ab3f0d6f16e2e&gt;0ff6416&lt;/denchmark-link&gt;
.
The code used to train the model is taken from the official fast_pred_var notebook:
&lt;denchmark-code&gt;import math
import torch
import gpytorch
from matplotlib import pyplot as plt

# Make plots inline
%matplotlib inline

import urllib.request
import os.path
from scipy.io import loadmat
from math import floor

if not os.path.isfile('skillcraft.mat'):
    print('Downloading \'skillcraft\' UCI dataset...')
    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&amp;id=1xQ1vgx_bOsLDQ3RLbJwPxMyJHW7U9Eqd', 'skillcraft.mat')
    
data = torch.Tensor(loadmat('skillcraft.mat')['data'])
X = data[:, :-1]
X = X - X.min(0)[0]
X = 2 * (X / X.max(0)[0]) - 1
y = data[:, -1]

# Use the first 80% of the data for training, and the last 20% for testing.
train_n = int(floor(0.4*len(X)))

train_x = X[:train_n, :].contiguous().cuda()
train_y = y[:train_n].contiguous().cuda()

test_x = X[train_n:, :].contiguous().cuda()
test_y = y[train_n:].contiguous().cuda()

class GPRegressionModel(gpytorch.models.ExactGP):
        def __init__(self, train_x, train_y, likelihood):
            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)
            self.mean_module = gpytorch.means.ConstantMean()
            self.covar_module = gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.RBFKernel()
            )
            
        def forward(self, x):
            mean_x = self.mean_module(x)
            covar_x = self.covar_module(x)
            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()
model = GPRegressionModel(train_x, train_y, likelihood).cuda()

# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam([
    {'params': model.parameters()},  # Includes GaussianLikelihood parameters
], lr=0.1)

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

training_iterations = 20
def train():
    for i in range(training_iterations):
        optimizer.zero_grad()
        output = model(train_x)
        loss = -mll(output, train_y)
        loss.backward()
        print('Iter %d/%d - Loss: %.3f' % (i + 1,
                                           training_iterations,
                                           loss.item()))
        optimizer.step()
&lt;/denchmark-code&gt;

On the other hand, this does not appear to affect forward passes without the fast_pred_var option. Although that could just because forwards without fast_pred_var are already very slow.
	</description>
	<comments>
		<comment id='1' author='KeAWang' date='2019-02-07T03:50:49Z'>
		My best guess: the kernel matrix computation for k_x*X is taking slightly longer (about 0.002s longer) as a result of something in the commit. Maybe something gets optimized poorly by the JIT when things are inlined? &lt;denchmark-link:https://github.com/gpleiss&gt;@gpleiss&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='KeAWang' date='2019-02-07T19:18:29Z'>
		Closed by &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/507&gt;#507&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>