<bug id='1204' author='yucho147' open_date='2020-07-09T01:06:49Z' closed_time='2020-07-10T00:10:43Z'>
	<summary>[Question]error only with Periodic Kernel</summary>
	<description>
Hi,
I am having trouble with the implementation of periodic kernel.
I'd appreciate your advice.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

I create models with only different kernel as follows
from gpytorch.distributions import MultivariateNormal
from gpytorch.kernels import PeriodicKernel, RBFKernel, ScaleKernel
from gpytorch.likelihoods import GaussianLikelihood
from gpytorch.means import ConstantMean
from gpytorch.mlls import VariationalELBO
from gpytorch.models import ApproximateGP
from gpytorch.variational import (CholeskyVariationalDistribution,
                                  VariationalStrategy)
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
import torch


class PeriodicModel(ApproximateGP):
    def __init__(self, inducing_points):
        variational_distribution = CholeskyVariationalDistribution(
            inducing_points.size(0)
        )
        variational_strategy = VariationalStrategy(
            self,
            inducing_points,
            variational_distribution,
            learn_inducing_locations=True
        )
        super(PeriodicModel, self).__init__(variational_strategy)
        self.mean_module = ConstantMean()
        self.covar_module = ScaleKernel(
            PeriodicKernel()
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return MultivariateNormal(mean_x, covar_x)

class PeriodicRBFModel(ApproximateGP):
    def __init__(self, inducing_points):
        variational_distribution = CholeskyVariationalDistribution(
            inducing_points.size(0)
        )
        variational_strategy = VariationalStrategy(
            self,
            inducing_points,
            variational_distribution,
            learn_inducing_locations=True
        )
        super(PeriodicRBFModel, self).__init__(variational_strategy)
        self.mean_module = ConstantMean()
        self.covar_module = ScaleKernel(
            PeriodicKernel() * RBFKernel()
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return MultivariateNormal(mean_x, covar_x)

class RBFModel(ApproximateGP):
    def __init__(self, inducing_points):
        variational_distribution = CholeskyVariationalDistribution(
            inducing_points.size(0)
        )
        variational_strategy = VariationalStrategy(
            self,
            inducing_points,
            variational_distribution,
            learn_inducing_locations=True
        )
        super(RBFModel, self).__init__(variational_strategy)
        self.mean_module = ConstantMean()
        self.covar_module = ScaleKernel(
            RBFKernel()
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return MultivariateNormal(mean_x, covar_x)
I create sample data as follow
# create sample data #######
num = 1000
epochs = 20
lr = 1e-1
x = torch.arange(num, dtype=torch.float32).reshape(-1, 1).contiguous()
y = torch.sin(torch.arange(num) * 0.05) + torch.randn(num) / 3

train_dataset = TensorDataset(x, y)
train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)

indices = torch.randperm(len(x))[:100]
inducing_points = x[indices]
############################
And  I get an error only with using periodic kernel.
Why do I get it?
# PeriodicModel ############
model = PeriodicModel(inducing_points)
likelihood = GaussianLikelihood()
mll = VariationalELBO(likelihood, model, y.size(0))
optimizer = torch.optim.Adam(
    [{'params': model.parameters()},
     {'params': likelihood.parameters()}],
    lr=lr)

for epoch in range(epochs):
    print(f'epoch : {epoch}')
    model.train()
    likelihood.train()
    for x_batch, y_batch in train_loader:
        optimizer.zero_grad()
        output = model(x_batch)
        loss = - mll(output, y_batch)
        loss.backward()
        optimizer.step()
    print(loss.item())
with torch.no_grad():y_hat = likelihood(model(x)).mean
plt.plot(x.numpy(), y.numpy())
plt.plot(x.numpy(), y_hat.numpy())
plt.show()
############################
** Stack trace/error message **
&lt;denchmark-code&gt;---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-2-95272eb92f7b&gt; in &lt;module&gt;
     14     for x_batch, y_batch in train_loader:
     15         optimizer.zero_grad()
---&gt; 16         output = model(x_batch)
     17         loss = - mll(output, y_batch)
     18         loss.backward()

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/models/approximate_gp.py in __call__(self, inputs, prior, **kwargs)
     79         if inputs.dim() == 1:
     80             inputs = inputs.unsqueeze(-1)
---&gt; 81         return self.variational_strategy(inputs, prior=prior)

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/variational/variational_strategy.py in __call__(self, x, prior)
    173                 self.updated_strategy.fill_(True)
    174 
--&gt; 175         return super().__call__(x, prior=prior)

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/variational/_variational_strategy.py in __call__(self, x, prior)
    125                 inducing_points,
    126                 inducing_values=variational_dist_u.mean,
--&gt; 127                 variational_inducing_covar=variational_dist_u.lazy_covariance_matrix,
    128             )
    129         elif isinstance(variational_dist_u, Delta):

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/module.py in __call__(self, *inputs, **kwargs)
     26 
     27     def __call__(self, *inputs, **kwargs):
---&gt; 28         outputs = self.forward(*inputs, **kwargs)
     29         if isinstance(outputs, list):
     30             return [_validate_module_outputs(output) for output in outputs]

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/variational/variational_strategy.py in forward(self, x, inducing_points, inducing_values, variational_inducing_covar)
     95         # K_ZZ^{-1/2} K_ZX
     96         # K_ZZ^{-1/2} \mu_Z
---&gt; 97         L = self._cholesky_factor(induc_induc_covar)
     98         if L.shape != induc_induc_covar.shape:
     99             # Aggressive caching can cause nasty shape incompatibilies when evaluating with different batch shapes

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/utils/memoize.py in g(self, *args, **kwargs)
     32         cache_name = name if name is not None else method
     33         if not is_in_cache(self, cache_name):
---&gt; 34             add_to_cache(self, cache_name, method(self, *args, **kwargs))
     35         return get_from_cache(self, cache_name)
     36 

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/variational/variational_strategy.py in _cholesky_factor(self, induc_induc_covar)
     68     @cached(name="cholesky_factor")
     69     def _cholesky_factor(self, induc_induc_covar):
---&gt; 70         L = psd_safe_cholesky(delazify(induc_induc_covar).double())
     71         return L
     72 

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/utils/cholesky.py in psd_safe_cholesky(A, upper, out, jitter)
     46             except RuntimeError:
     47                 continue
---&gt; 48         raise e

~/.pyenv/versions/3.7.5/lib/python3.7/site-packages/gpytorch/utils/cholesky.py in psd_safe_cholesky(A, upper, out, jitter)
     23     """
     24     try:
---&gt; 25         L = torch.cholesky(A, upper=upper, out=out)
     26         return L
     27     except RuntimeError as e:

RuntimeError: cholesky_cpu: U(19,19) is zero, singular U.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;


I need to know why the error is occurring.
Please give me an example of a model using periodic kernel(+scale kernel).

&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;


GPyTorch Version : 1.1.1
PyTorch Version : 1.5.0
Computer OS : macOS Catalina 10.15.5

&lt;denchmark-h:h2&gt;Supplement&lt;/denchmark-h&gt;

I don't get an error with other implementations
# PeriodicRBFModel #########
model = PeriodicRBFModel(inducing_points)
likelihood = GaussianLikelihood()
mll = VariationalELBO(likelihood, model, y.size(0))
optimizer = torch.optim.Adam(
    [{'params': model.parameters()},
     {'params': likelihood.parameters()}],
    lr=lr)

for epoch in range(epochs):
    print(f'epoch : {epoch}')
    model.train()
    likelihood.train()
    for x_batch, y_batch in train_loader:
        optimizer.zero_grad()
        output = model(x_batch)
        loss = - mll(output, y_batch)
        loss.backward()
        optimizer.step()
    print(loss.item())
with torch.no_grad():y_hat = likelihood(model(x)).mean
plt.plot(x.numpy(), y.numpy())
plt.plot(x.numpy(), y_hat.numpy())
plt.show()
############################

# RBFModel #################
model = RBFModel(inducing_points)
likelihood = GaussianLikelihood()
mll = VariationalELBO(likelihood, model, y.size(0))
optimizer = torch.optim.Adam(
    [{'params': model.parameters()},
     {'params': likelihood.parameters()}],
    lr=lr)

for epoch in range(epochs):
    print(f'epoch : {epoch}')
    model.train()
    likelihood.train()
    for x_batch, y_batch in train_loader:
        optimizer.zero_grad()
        output = model(x_batch)
        loss = - mll(output, y_batch)
        loss.backward()
        optimizer.step()
    print(loss.item())
with torch.no_grad():y_hat = likelihood(model(x)).mean
plt.plot(x.numpy(), y.numpy())
plt.plot(x.numpy(), y_hat.numpy())
plt.show()
############################
The figure below shows the result of executing on PeriodicRBFkernel.
&lt;denchmark-link:https://user-images.githubusercontent.com/53713732/86934684-f2153800-c176-11ea-8d21-af4d08ac84a5.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='yucho147' date='2020-07-10T00:10:42Z'>
		Initialization is important for Gaussian processes. Otherwise, you run into numerical linear algebra problems like the non-positive semi-definite matrix you saw.
The default initialization assumes that the data works well for data that is roughly [-1, 1]. Thus, you can either try an initialization that is more appropriate for for your choice of kernel, or rescale your data (and undo the rescaling after prediction). This fixes the runtime error for me.
Also the learning rate should be lowered for the model to learn for this example (I used 1e-2):
&lt;denchmark-link:https://user-images.githubusercontent.com/11478740/87102535-3b6d9200-c220-11ea-901c-d4fbecebf743.png&gt;&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>