<bug id='961' author='wjmaddox' open_date='2019-11-21T23:22:33Z' closed_time='2019-12-16T23:14:12Z'>
	<summary>[Bug] Pyro Examples are out of date</summary>
	<description>
&lt;denchmark-h:h1&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm attempting to run the Pyro examples in &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/tree/master/examples/09_Pyro_Integration&gt;examples/09_Pyro_Integration&lt;/denchmark-link&gt;
, but am getting backwards related issues.
&lt;denchmark-h:h2&gt;To reproduce&lt;/denchmark-h&gt;

Run the &lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/blob/master/examples/09_Pyro_Integration/Pyro_Variational_GP_Regression.ipynb&gt;Pyro GP Regression&lt;/denchmark-link&gt;
 tutorial into cell [5] and gives the following error. However, I've seen it through all three tutorials.
** Code snippet to reproduce **
Error seems to require clearing a gradient step in the iteration perspective, but I'm not particularly familiar enough with Pyro to come up with a fix off the top of my head.
** Stack trace/error message **
&lt;denchmark-code&gt;&lt;ipython-input-5-b72bfb4618e0&gt; in train(num_iter)
      9     for i in range(num_iter):
     10         model.zero_grad()
---&gt; 11         loss = svi.step(train_x, train_y)
     12         if not (i + 1) % 50:
     13             print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pyro/infer/svi.py in step(self, *args, **kwargs)
    117         # get loss and compute gradients
    118         with poutine.trace(param_only=True) as param_capture:
--&gt; 119             loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)
    120 
    121         params = set(site["value"].unconstrained()

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pyro/infer/trace_elbo.py in loss_and_grads(self, model, guide, *args, **kwargs)
    132             if trainable_params and getattr(surrogate_loss_particle, 'requires_grad', False):
    133                 surrogate_loss_particle = surrogate_loss_particle / self.num_particles
--&gt; 134                 surrogate_loss_particle.backward(retain_graph=self.retain_graph)
    135         warn_if_nan(loss, "loss")
    136         return loss

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    164                 products. Defaults to ``False``.
    165         """
--&gt; 166         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    167 
    168     def register_hook(self, hook):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     97     Variable._execution_engine.run_backward(
     98         tensors, grad_tensors, retain_graph, create_graph,
---&gt; 99         allow_unreachable=True)  # allow_unreachable flag
    100 
    101 

RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Expected Behavior&lt;/denchmark-h&gt;

The tutorial should work :). Sorry if this is just reporting a bug. But, it should be part of the examples refactor.
&lt;denchmark-h:h2&gt;System information&lt;/denchmark-h&gt;

Please complete the following information:

Pyro 1.0.0
gpytorch 0.3.6 (built from master)
pytorch 1.3.1
Ubuntu 18.04

	</description>
	<comments>
		<comment id='1' author='wjmaddox' date='2019-11-25T17:31:45Z'>
		This is very true. I'm doing an update on the examples today (&lt;denchmark-link:https://github.com/cornellius-gp/gpytorch/pull/954&gt;#954&lt;/denchmark-link&gt;
)
		</comment>
	</comments>
</bug>