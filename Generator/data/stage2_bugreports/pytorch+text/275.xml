<bug id='275' author='ruotianluo' open_date='2018-04-11T02:16:03Z' closed_time='2018-09-11T15:37:27Z'>
	<summary>wordvec of unk token</summary>
	<description>
&lt;denchmark-link:https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L242&gt;https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L242&lt;/denchmark-link&gt;

When the word is in the dictionary, the returned is a one-dim vector, while unk vector is a two-dim vector.
I assume this is not expected, right?
	</description>
	<comments>
		<comment id='1' author='ruotianluo' date='2018-04-28T11:18:05Z'>
		&lt;denchmark-link:https://github.com/jekbradbury&gt;@jekbradbury&lt;/denchmark-link&gt;
 What would be preferred out of those two? I could make a PR for this.
		</comment>
		<comment id='2' author='ruotianluo' date='2018-04-30T00:34:23Z'>
		Probably the one-dim vector. Does nothing else in the codebase currently rely on this __getitem__ method?
		</comment>
		<comment id='3' author='ruotianluo' date='2018-05-02T08:53:01Z'>
		&lt;denchmark-link:https://github.com/jekbradbury&gt;@jekbradbury&lt;/denchmark-link&gt;
 I've been searching through the codebase, but I wasn't able to find anything. It seems that after the vectors are finally loaded, it's up to user's model to deal with it. I'll see whether something breaks when I change this.
		</comment>
	</comments>
</bug>