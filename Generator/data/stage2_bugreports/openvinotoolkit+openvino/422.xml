<bug id='422' author='zilion22' open_date='2020-03-17T17:40:27Z' closed_time='2020-07-18T18:36:07Z'>
	<summary>RuntimeError: Cannot create internal buffer.</summary>
	<description>
Hi,
I am trying to do inference on a model based on a tweaked InceptionV3 keras/tf model.
I managed to freeze and mo_tf.py but during inference I got the following error:
File "src/infer_by_openvino.py", line 156, in setup_openvino net.batch_size = 1 File "ie_api.pyx", line 547, in openvino.inference_engine.ie_api.IEPlugin.load File "ie_api.pyx", line 557, in openvino.inference_engine.ie_api.IEPlugin.load RuntimeError: Cannot create internal buffer. Buffer can be overrun.
Do you have any idea what it can be?
Can I debug someho what exactly caused this?
Thanks,
	</description>
	<comments>
		<comment id='1' author='zilion22' date='2020-03-19T00:32:46Z'>
		Hello &lt;denchmark-link:https://github.com/zilion22&gt;@zilion22&lt;/denchmark-link&gt;

I am able to successfully freeze the Inception v3 model from here &lt;denchmark-link:https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#Convert_From_TF&gt;https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#Convert_From_TF&lt;/denchmark-link&gt;
 and convert it to Intermediate Representation and then do inference on it.

Can you please try to run your model with the benchmark tool from latest OpenVINO 2020.1 build? Do you see any errors?
openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py
If you see no errors, please specify what application you use for inference?

Best regards, Max.
		</comment>
		<comment id='2' author='zilion22' date='2020-03-20T14:33:23Z'>
		Hi Max,
Thanks for the quick response.
It gave me a little bit of headache to install OpenVino2020.1 but finally I managed to do it.
To respond your request:
1.
The error message is much longer now:
Inferencing the input data... Traceback (most recent call last): File "src/infer_by_openvino.py", line 188, in &lt;module&gt; main(sys.argv) File "src/infer_by_openvino.py", line 176, in main exec_net, input_blob = setup_openvino(args.xml_path, args.bin_path, args.output_node_names) File "src/infer_by_openvino.py", line 143, in setup_openvino net = inference_engine.IENetwork(model=xml_path, weights=bin_path) File "ie_api.pyx", line 980, in openvino.inference_engine.ie_api.IENetwork.__cinit__ RuntimeError: Check 'Dimension::merge(merged_channel_count, data_channel_count, filter_input_channel_count)' failed at /teamcity/work/scoring_engine_build/releases_2020_1/ngraph/src/ngraph/validation_util.cpp:328: While validating node 'Convolution[Convolution_2](Parameter_0: float{1,1,1024,1024}, Constant_1: float{32,3,3,3}) -&gt; (??)': Data batch channel count (1) does not match filter input channel count (3).
but still I cannot figure out the problem.
On the input it expects [:,1024,1024,:] and it meant have 4 outputs. 3 nodes with 120x120 and one with scalar.
It I have very similar architecture where I have 1 node with 120x120 and one scalar node, it seem to be working.
In addition,  unfortunately I have found this file
openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py
in neither OpenVino nor OpenVino Model Optimiizer apt packages.

This is our in-house application. In what way should I describe the application to you?

Thanks for any help,
Lev
		</comment>
		<comment id='3' author='zilion22' date='2020-03-21T23:24:08Z'>
		Hello &lt;denchmark-link:https://github.com/zilion22&gt;@zilion22&lt;/denchmark-link&gt;

Is there any directory missed from the path openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py or you just don't see benchmark_app.py file in the last directory?
And please also confirm if you follow the same process as described below?

Latest OpenVINO toolkit build being downloaded from https://software.intel.com/en-us/openvino-toolkit/choose-download
OpenVINO toolkit being installed and configured (Ubuntu 18.04) per https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html
We took the example of Inception V3 non-frozen model (including --mean_values and --scale values for model optimizer command) from https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#Convert_From_TF
Converted per this example https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Slim_Library_Models.html

If after these steps you are still not able to make this work, is this possible for you to share your model (keras/tf or IR format) with us that we could test it in our environment? Either your modification of Inception V3 model is for some reason not supported by DLDT/OpenVINO toolkit or you might be doing something else wrong.
Best regards, Max.
		</comment>
		<comment id='4' author='zilion22' date='2020-05-29T08:12:42Z'>
		&lt;denchmark-link:https://github.com/zilion22&gt;@zilion22&lt;/denchmark-link&gt;
 Looks like converted network contains incorrect weights for convolution layer.
Weights dimensions are {32,3,3,3}, but input is {1,1,1024,1024}, in this case weights should have dimensions: {32,1,3,3}
&lt;denchmark-link:https://github.com/lazarevevgeny&gt;@lazarevevgeny&lt;/denchmark-link&gt;
 Could you give an advice if the problem in the MO?
		</comment>
		<comment id='5' author='zilion22' date='2020-05-29T08:15:32Z'>
		Without the model or a simple reproducer we cannot provide any feedback.
		</comment>
		<comment id='6' author='zilion22' date='2020-07-18T18:36:07Z'>
		It seems that the issues is not actual anymore. Closing it. Feel free to reopen it or create a new one,
		</comment>
	</comments>
</bug>