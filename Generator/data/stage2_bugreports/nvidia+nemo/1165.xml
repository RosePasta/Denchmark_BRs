<bug id='1165' author='Honghe' open_date='2020-09-15T03:26:48Z' closed_time='2020-09-18T04:24:19Z'>
	<summary>TypeError: restore_from() takes 2 positional arguments but 3 were given</summary>
	<description>
Describe the bug
TypeError: restore_from() takes 2 positional arguments but 3 were given.
Steps/Code to reproduce bug
Re-train a stopped model.
&lt;denchmark-code&gt;PYTHONPATH=`pwd` OMP_NUM_THREADS=4 python -m torch.distributed.launch --nproc_per_node=2  examples/asr/speech2text.py --asr_model=/home/ubuntu/NeMo/examples/asr/configs/quartznet15x5-zh_pinyin.yaml --train_dataset=/home/ubuntu/Data/asr/data_aishell/train.json --do_not_normalize_text --amp_opt_level=O1 --batch_size=32 --num_epochs=200 --checkpoint_dir=/home/ubuntu/Data/nemo_model --checkpoint_save_freq=400
&lt;/denchmark-code&gt;

Expected behavior
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "examples/asr/speech2text.py", line 189, in &lt;module&gt;
    main()
  File "examples/asr/speech2text.py", line 184, in main
    lr_policy=lr_policy,
  File "/home/ubuntu/NeMo/nemo/core/neural_factory.py", line 305, in train
    amp_max_loss_scale=amp_max_loss_scale,
  File "/home/ubuntu/NeMo/nemo/backends/pytorch/actions.py", line 1401, in train
    _perform_on_action_start(callbacks, get_state(self))
  File "/home/ubuntu/NeMo/nemo/backends/pytorch/actions.py", line 1033, in _perform_on_action_start
    callback.on_action_start(state)
  File "/home/ubuntu/NeMo/nemo/core/callbacks.py", line 539, in on_action_start
    self.__restore_from(self._load_from_folder, state)
  File "/home/ubuntu/NeMo/nemo/core/callbacks.py", line 494, in __restore_from
    mod.restore_from(checkpoint, state["local_rank"])
TypeError: restore_from() takes 2 positional arguments but 3 were given
Traceback (most recent call last):
  File "examples/asr/speech2text.py", line 189, in &lt;module&gt;
    main()
  File "examples/asr/speech2text.py", line 184, in main
    lr_policy=lr_policy,
  File "/home/ubuntu/NeMo/nemo/core/neural_factory.py", line 305, in train
    amp_max_loss_scale=amp_max_loss_scale,
  File "/home/ubuntu/NeMo/nemo/backends/pytorch/actions.py", line 1401, in train
    _perform_on_action_start(callbacks, get_state(self))
  File "/home/ubuntu/NeMo/nemo/backends/pytorch/actions.py", line 1033, in _perform_on_action_start
    callback.on_action_start(state)
  File "/home/ubuntu/NeMo/nemo/core/callbacks.py", line 539, in on_action_start
    self.__restore_from(self._load_from_folder, state)
  File "/home/ubuntu/NeMo/nemo/core/callbacks.py", line 494, in __restore_from
    mod.restore_from(checkpoint, state["local_rank"])
TypeError: restore_from() takes 2 positional arguments but 3 were given
Traceback (most recent call last):
  File "/home/ubuntu/anaconda3/envs/nemo_asr/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/ubuntu/anaconda3/envs/nemo_asr/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/anaconda3/envs/nemo_asr/lib/python3.7/site-packages/torch/distributed/launch.py", line 263, in &lt;module&gt;
    main()
  File "/home/ubuntu/anaconda3/envs/nemo_asr/lib/python3.7/site-packages/torch/distributed/launch.py", line 259, in main
    cmd=cmd)
&lt;/denchmark-code&gt;

Environment details
If NVIDIA docker image is used you don't need to specify these.
Otherwise, please provide:

OS version Ubuntu 20.04 Anaconda
PyTorch version 1.4.0
Python version 3.7
NeMo version 9729847

	</description>
	<comments>
		<comment id='1' author='Honghe' date='2020-09-15T06:22:17Z'>
		Hi &lt;denchmark-link:https://github.com/Honghe&gt;@Honghe&lt;/denchmark-link&gt;
 could you please switch to the latest version available in the "main" branch?
		</comment>
		<comment id='2' author='Honghe' date='2020-09-15T10:46:20Z'>
		&lt;denchmark-link:https://github.com/okuchaiev&gt;@okuchaiev&lt;/denchmark-link&gt;
 After switched to main branch, I used the following training command, and met others errors.
&lt;denchmark-code&gt;PYTHONPATH=`pwd` python examples/asr/speech_to_text.py --config-name=quartznet_15x5_pinyin \
model.train_ds.manifest_filepath=/home/ubuntu/Data/asr/data_aishell/train.json \
model.validation_ds.manifest_filepath=/home/ubuntu/Data/asr/data_aishell/valid.json \
trainer.gpus=2 trainer.max_epochs=128 model.train_ds.batch_size=32 \
+trainer.precision=16 +trainer.amp_level=O1  \
+model.train_ds.num_workers=2 +model.validation_ds.num_workers=2
&lt;/denchmark-code&gt;


UserWarning: Detected call of lr_scheduler.step()beforeoptimizer.step().

&lt;denchmark-code&gt;torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
      "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
&lt;/denchmark-code&gt;


After several epochs, the training loss is still at about 90.
I different of config is labels: My labels numClass is 1200 instead of 28 default English alphabet of quartznet_15x3 .

&lt;denchmark-code&gt;Epoch 0:   0%|                  | 1/1875 [00:00&lt;25:31,  1.22it/
Epoch 5:  10%|████████████      | 191/1875 [02:20&lt;20:43,  1.35it/s, loss=85.872, v_num=4-57]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Honghe' date='2020-09-15T16:24:30Z'>
		&lt;denchmark-link:https://github.com/titu1994&gt;@titu1994&lt;/denchmark-link&gt;
 any comment on the warning? I think it's OK.
Loss of 85 after 5 epochs actually doesn't look bad to me. You typically need at least 50 epochs to start seeing OK results.
Can you share the stats of your dataset?
Btw, we do have pre-trained aishell model - I'll weight convert it to new nemo and update this bug. Maybe fine-tuning from it will help.
BTW, here is  what I'd recommend you do: (1) Start from pre-trained English model (e.g. - get your model like this quartznet = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name="QuartzNet15x5Base-En") ), then (2) use ".change_vocabulary()" method to reset decoder to your target alphabet.  - This will allow you to start training from pre-trained encoder (which contains most of the weights).
		</comment>
		<comment id='4' author='Honghe' date='2020-09-15T16:38:27Z'>
		The scheduler issue is due to pytorch lightning, and it happens slightly randomly when using ddp. It will skip only 1 step of the scheduler so it's not of much concern.
		</comment>
		<comment id='5' author='Honghe' date='2020-09-16T02:54:18Z'>
		&lt;denchmark-link:https://github.com/okuchaiev&gt;@okuchaiev&lt;/denchmark-link&gt;
 The stats of my dataset is (I use Mandarin Pinyin as labels, numClass is 1203):
&lt;denchmark-link:https://user-images.githubusercontent.com/1092722/93286347-a2538c80-f809-11ea-941d-5b515d4bfef6.png&gt;&lt;/denchmark-link&gt;

The dataset is a bit small. But the loss is in the 8 around several epochs, it seems the loss is stop too early.
&lt;denchmark-code&gt;Epoch 1:  39%|███████████        | 738/1875 [09:01&lt;13:54,  1.36it/s, loss=75.815, v_num=4-57]
Epoch 6:  66%|███████████       | 1246/1875 [15:02&lt;07:35,  1.38it/s, loss=17.173, v_num=4-57]
Epoch 7:  22%|███████████        | 406/1875 [04:52&lt;17:39,  1.39it/s, loss=16.040, v_num=4-57]
Epoch 7:  98%|███████████████▌  | 1837/1875 [22:11&lt;00:27,  1.38it/s, loss=15.156, v_num=4-57]
Epoch 14:  68%|██████████       | 1273/1875 [15:22&lt;07:16,  1.38it/s, loss=12.038, v_num=4-57]
Epoch 39:  36%|██████████         | 666/1875 [08:02&lt;14:36,  1.38it/s, loss=8.026, v_num=4-57]
Epoch 41:  51%|██████████         | 949/1875 [11:38&lt;11:21,  1.36it/s, loss=8.054, v_num=4-57]
Epoch 43:  40%|██████████         | 758/1875 [09:18&lt;13:43,  1.36it/s, loss=7.884, v_num=4-57]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='Honghe' date='2020-09-17T04:37:19Z'>
		hi &lt;denchmark-link:https://github.com/Honghe&gt;@Honghe&lt;/denchmark-link&gt;
 just added aishell-trained model.
I'd recommend you try fine-tuning from this one: QuartzNet15x5Base-Zh
		</comment>
		<comment id='7' author='Honghe' date='2020-09-17T06:59:01Z'>
		&lt;denchmark-link:https://github.com/okuchaiev&gt;@okuchaiev&lt;/denchmark-link&gt;
 There is a bug in transfer learning, I created a fix pull request: &lt;denchmark-link:https://github.com/NVIDIA/NeMo/pull/1178&gt;#1178&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='Honghe' date='2020-09-17T16:38:53Z'>
		thx &lt;denchmark-link:https://github.com/Honghe&gt;@Honghe&lt;/denchmark-link&gt;
 ! Could you please sign the commit in your PR
		</comment>
		<comment id='9' author='Honghe' date='2020-09-18T02:00:13Z'>
		&lt;denchmark-link:https://github.com/okuchaiev&gt;@okuchaiev&lt;/denchmark-link&gt;
 The transfer learning stops at Loss 7, it is still early stop.  My dataset is Aishell v1 with Chinese pinyin as labels (numClass 1200).
&lt;denchmark-link:https://user-images.githubusercontent.com/1092722/93545802-f0949700-f993-11ea-8fdd-5a82600f2a10.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1092722/93545838-05712a80-f994-11ea-85a2-155e010826c4.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='Honghe' date='2020-09-18T04:24:19Z'>
		looking at training_batch_wer and validation_wer looks like there is no more overfitting.
I don't quite understand what is meant by this: " it is still early stop".
Your plots do look reasonable to me. This is typical picture - you gain the most in the first few epochs and then it improves very slowly.
I'm closing this issue because the title (TypeError: ...) is no longer relevant to our discussion. Please open a new issue if you still have questions. Thanks for the bugfix!
		</comment>
	</comments>
</bug>