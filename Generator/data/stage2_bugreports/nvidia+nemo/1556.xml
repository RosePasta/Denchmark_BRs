<bug id='1556' author='ctlaltdefeat' open_date='2020-12-14T16:26:23Z' closed_time='2020-12-14T19:50:37Z'>
	<summary>Tacotron2 bug when batch dimension is 1</summary>
	<description>
Describe the bug
Training (or just passing input with ground-truth spectrogram) Tacotron2 with a batch dimension of 1 does not work due to a dimension problem when aggregating the gate outputs.
The line here



NeMo/nemo/collections/tts/modules/tacotron2.py


         Line 256
      in
      d01b7bb






 gate_outputs = torch.stack(gate_outputs).squeeze(-1).transpose(0, 1) 





should have unsqueeze instead of squeeze when the batch dimension is 1.
(I didn't want to submit a pull request as I'm not familiar with the conventions of the project)
	</description>
	<comments>
		<comment id='1' author='ctlaltdefeat' date='2020-12-14T19:50:55Z'>
		Thanks for catching this bug. It should be fixed now
		</comment>
	</comments>
</bug>