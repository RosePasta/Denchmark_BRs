<bug id='1289' author='PeganovAnton' open_date='2020-10-14T16:02:35Z' closed_time='2020-11-11T07:56:38Z'>
	<summary>Perplexity is computed incorrectly</summary>
	<description>
Describe the bug
Perplexity is computed incorrectly &lt;denchmark-link:https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/metrics/perplexity.py&gt;here&lt;/denchmark-link&gt;
 and in &lt;denchmark-link:https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/transformer_lm_model.py&gt;TransformerLMModel&lt;/denchmark-link&gt;
. Perplexity is an exponent of entropy whereas in these scripts it is computed as an exponent of cross-entropy.
	</description>
	<comments>
	</comments>
</bug>