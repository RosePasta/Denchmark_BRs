<bug id='1090' author='darraghdog' open_date='2020-08-30T13:18:47Z' closed_time='2020-09-01T09:12:56Z'>
	<summary>onnx inferencesession load hangs with quartznet asr config.</summary>
	<description>
Describe the bug
The quartznet onnx exported model cannot be loaded for inference. The code snippet below runs fine up until encoder_session = onnxruntime.InferenceSession(filename), at that point the code just hangs, no error but hangs. I left it for ~ 10mins.
See code snippet below. The model generated from the first config works for me, the model generated by the second config does not work for me. The decoder works fine.
System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X
ONNX Runtime installed from (source or binary): binary
ONNX Runtime version:

&lt;denchmark-code&gt;onnx.__version__ -- &gt; 1.7.0
onnxruntime.__version__ --&gt; 1.3.0
&lt;/denchmark-code&gt;


Python version: Python 3.7.6
Visual Studio version (if applicable):
GCC/Compiler version (if compiling from source): N/A... testing on CPU
CUDA/cuDNN version: N/A... testing on CPU
GPU model and memory: N/A... testing on CPU

To Reproduce
&lt;denchmark-code&gt;import os
import tempfile
import torch
import onnx
import pytest
from omegaconf import DictConfig
from nemo.collections.asr.modules import ConvASRDecoder, ConvASREncoder
   
# This config can be loaded via the Inference Session
encoder_dict = {
    'cls': 'nemo.collections.asr.modules.ConvASREncoder',
    'params': {
        'feat_in': 64,
        'activation': 'relu',
        'conv_mask': True,
        'jasper': [
            {'filters': 1024, 'repeat': 1, 'kernel': [1], 'stride': [1], 'dilation': [1],'dropout': 0.0,'residual': False, 'separable': True,'se': True, 'se_context_size': -1}
        ],
    },
}
   
# This config cannot be loaded via the Inference Session
encoder_dict = {
    'cls': 'nemo.collections.asr.modules.ConvASREncoder', 
    'params': {
        'feat_in': 64, 
        'activation': 'relu', 
        'conv_mask': True, 
        'jasper': 
            [{'filters': 256, 'repeat': 1, 'kernel': [33], 'stride': [2], 'dilation': [1], 'dropout': 0.0, 'residual': False, 'separable': True}, 
             {'filters': 256, 'repeat': 5, 'kernel': [33], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 256, 'repeat': 5, 'kernel': [33], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 256, 'repeat': 5, 'kernel': [33], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 256, 'repeat': 5, 'kernel': [39], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 256, 'repeat': 5, 'kernel': [39], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 256, 'repeat': 5, 'kernel': [39], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True},
             {'filters': 512, 'repeat': 5, 'kernel': [51], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [51], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [51], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [63], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [63], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [63], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [75], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [75], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 5, 'kernel': [75], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, 
             {'filters': 512, 'repeat': 1, 'kernel': [87], 'stride': [1], 'dilation': [2], 'dropout': 0.0, 'residual': False, 'separable': True}, 
             {'filters': 1024, 'repeat': 1, 'kernel': [1], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': False}
             ]
            }
        }

INPATH = './'
filename  = os.path.join(INPATH, 'qn_encoder.onnx')
encoder_instance = ConvASREncoder.from_config_dict(DictConfig(encoder_dict))
encoder_instance.eval()
encoder_instance.export(output=filename)
# Create dummy inputs
dummy_processed_signal  = torch.randn([1, 64, 5104])
dummy_processed_signal_len =  torch.tensor([5103])
# Check model
quartznet_encoder = onnx.load(filename)
onnx.checker.check_model(quartznet_encoder)
# Load inference session
import onnxruntime
encoder_session = onnxruntime.InferenceSession(filename)
&lt;/denchmark-code&gt;

Expected behavior
The inference session for the second config should load, just as well as the first session.
	</description>
	<comments>
		<comment id='1' author='darraghdog' date='2020-08-31T04:51:18Z'>
		Thank you for a detailed description! We'll have a look
		</comment>
		<comment id='2' author='darraghdog' date='2020-08-31T22:16:29Z'>
		This works for me on 'main' branch (torch=1.6, onnx=1.7, onnxruntime=1.4, onnx_opset_version=12). I checked both CPU onnxruntime and GPU (regular onnxruntime_gpu package).
		</comment>
		<comment id='3' author='darraghdog' date='2020-08-31T23:25:15Z'>
		I tried the code snippet (To Reproduce) on two machines and in all cases it worked fine (using code from the latest main branch).
Machine 1 - Ubuntu 20 onnxruntime 1.3 and onnxruntime 1.4 (tried both)
Machine 2 - MacBook Pro, onnxruntime 1.4
I do get a bunch of warnings like this:
&lt;denchmark-code&gt;2020-08-31 15:22:46.759571 [W:onnxruntime:, graph.cc:863 Graph] Initializer 99 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
&lt;/denchmark-code&gt;

but this is OK.
&lt;denchmark-link:https://github.com/darraghdog&gt;@darraghdog&lt;/denchmark-link&gt;
 could you please try again with the latest main branch and post the error message you get?
		</comment>
		<comment id='4' author='darraghdog' date='2020-09-01T09:12:56Z'>
		Thank you for the detailed checks.
It seems like a problem on my side, I have found if I kill the step (encoder_session = onnxruntime.InferenceSession(filename)) in the python console when it is hanging and run just that steps again, it loads the second time.
I will be running this in a docker at the end - so will try it there. I'm sure it will work.
FYI - I am getting about 5X to 8X speed up with onnx, and results look good.
In any case, below are my versions in case anyone faces this later.
&lt;denchmark-code&gt;(base) dhanley@Darraghs-MBP NeMo % pip freeze | grep 'torch\|onnx\|nemo-toolkit'
keras2onnx==1.7.0
nemo-toolkit==0.88.1b0
onnx==1.7.0
onnxconverter-common==1.7.0
onnxruntime==1.4.0
pytorch-lightning==0.9.0
torch==1.6.0
torch-stft==0.1.4
torchaudio==0.5.0
torchtext==0.6.0
torchvision==0.6.0
(base) dhanley@Darraghs-MBP NeMo % system_profiler SPSoftwareDataType           
Software:

    System Software Overview:

      System Version: macOS 10.15.6 (19G73)
      Kernel Version: Darwin 19.6.0
      Boot Volume: Macintosh HD
      Boot Mode: Normal
      Secure Virtual Memory: Enabled
      System Integrity Protection: Enabled

(base) dhanley@Darraghs-MBP NeMo % git branch
* main
  master
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>