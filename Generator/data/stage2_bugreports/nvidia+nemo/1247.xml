<bug id='1247' author='FerchichiNourchene' open_date='2020-10-01T15:47:07Z' closed_time='2020-10-02T16:42:21Z'>
	<summary>Multi-GPU BERT Pre-Training Hangs on GCP Virtual Machine</summary>
	<description>
Description of the bug
I'm running &lt;denchmark-link:https://github.com/NVIDIA/NeMo/blob/master/examples/nlp/language_modeling/bert_pretraining.py&gt;BERT Pre-training&lt;/denchmark-link&gt;
 on raw uncased text dataset with multiple GPUs on  (4 V100) and the run hangs after epoch 0. It is using 100% capacity of each one of the GPU while I'm only training with a small sample data.
I have tried multi-GPU runs with 2, 3 and 4 GPUs with different batch sizes (2,4, 8, 16, 32 and 64)  and different optimization level ("O0", "O1" and "O2") with no results.
Code to reproduce bug
&lt;denchmark-code&gt;python -m torch.distributed.launch --nproc_per_node=4 bert_pretraining.py \
--num_gpus 4\
--amp_opt_level "O0" \
--train_data path_to/train.txt \
--eval_data path_to/valid.txt \
--work_dir outputs/bert_lm \
--batch_size 4 \
--lr 0.01 \
--lr_policy CosineAnnealing \
--lr_warmup_proportion 0.05 \
--optimizer novograd \
--beta1 0.95 \
--beta2 0.25 \
--vocab_size 3200 \
--hidden_size 768 \
--intermediate_size 3072 \
--num_hidden_layers 12 \
--num_attention_heads 12 \
--hidden_act "gelu" \
--save_step_freq 200 \
data_text \
--dataset_name wikitext-2 \
--tokenizer sentence-piece \
--num_epochs 10 \
--sample_size 10000000 \
--mask_probability 0.15 \
--short_seq_prob 0.1 \
&lt;/denchmark-code&gt;

Behavior
&lt;denchmark-code&gt;You can find here after part of the log where the run gets blocked:
[NeMo I 2020-10-01 13:35:52 deprecated_callbacks:232] Step: 0
[NeMo I 2020-10-01 13:35:52 bert_pretraining:273] Loss: 5.397 MLM Loss: 4.865 NSP Loss: 0.532
[NeMo I 2020-10-01 13:35:52 deprecated_callbacks:247] Step time: 0.9880142211914062 seconds
[NeMo I 2020-10-01 13:35:52 deprecated_callbacks:316] Doing Evaluation ..............................
[NeMo I 2020-10-01 13:35:54 lm_bert_callback:65] Dev perplexity: 307.919
[NeMo I 2020-10-01 13:35:54 deprecated_callbacks:321] Evaluation time: 1.753828525543213 seconds
^C^CTraceback (most recent call last):
  File "/opt/tunbert/miniconda3/envs/tunbert-torch/lib/python3.7/subprocess.py", line 1019, in wait
    return self._wait(timeout=timeout)
  File "/opt/tunbert/miniconda3/envs/tunbert-torch/lib/python3.7/subprocess.py", line 1653, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/tunbert/miniconda3/envs/tunbert-torch/lib/python3.7/subprocess.py", line 1611, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt


$ nvidia-smi
     
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   40C    P0    77W / 300W |   3263MiB / 16160MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  On   | 00000000:00:05.0 Off |                    0 |
| N/A   38C    P0    60W / 300W |   3193MiB / 16160MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  On   | 00000000:00:06.0 Off |                    0 |
| N/A   41C    P0    69W / 300W |   3209MiB / 16160MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  On   | 00000000:00:07.0 Off |                    0 |
| N/A   39C    P0    61W / 300W |   3225MiB / 16160MiB |    100%      Default |
|                               |                      |                  N/A |
&lt;/denchmark-code&gt;

Environment overview

Environment location:  GCP
Method of NeMo install: pip install nemo_toolkit[all]==0.11.0

Environment details

python=3.7.9
pytorch=1.6.0
torchvision=0.7.0
cudatoolkit=10.1
nemo_toolkit[all]==0.11.0
apex==0.1

Additional context
Running with single GPU works fine.
GPU: Tesla V100
	</description>
	<comments>
		<comment id='1' author='FerchichiNourchene' date='2020-10-02T05:17:15Z'>
		You are using older version of NeMo (0.11), can you please switch to the most recent "1.0.0b1" version.
There are a lot of breaking changes. You can find bert pre-training script in new version here: &lt;denchmark-link:https://github.com/NVIDIA/NeMo/tree/main/examples/nlp/language_modeling&gt;https://github.com/NVIDIA/NeMo/tree/main/examples/nlp/language_modeling&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='FerchichiNourchene' date='2020-10-02T14:44:22Z'>
		&lt;denchmark-link:https://github.com/okuchaiev&gt;@okuchaiev&lt;/denchmark-link&gt;
  Thank you for your quick and precise answer. I have upgraded the nemo version to the most recent one and used the new script version. Running with single and multi GPU works fine on the GCP instance now!
Thank you :)
		</comment>
		<comment id='3' author='FerchichiNourchene' date='2020-10-02T16:42:21Z'>
		thanks for confirming! closing this issue
		</comment>
	</comments>
</bug>