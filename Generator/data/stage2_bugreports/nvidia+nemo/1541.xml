<bug id='1541' author='vinchg' open_date='2020-12-10T01:25:06Z' closed_time='2021-01-09T07:16:05Z'>
	<summary>Some transformers-based models may not work</summary>
	<description>

With the update of the requirements to transformers &gt;= 4.0.0 on the main branch, calling a model now passes  by default rather than a tuple previously: &lt;denchmark-link:https://github.com/huggingface/transformers/releases/tag/v4.0.0&gt;https://github.com/huggingface/transformers/releases/tag/v4.0.0&lt;/denchmark-link&gt;

The fix is simple - any transformers model call should explicitly state return_dict=False to maintain old functionality.
Steps/Code to reproduce bug
I've only looked at the text2sparql model myself - training and evaluating a model will error. I expect some of the other transformer models to do so as well.
Expected behavior
Model trains and evaluates properly.
Environment overview (please complete the following information)

Environment location: Local
Method of NeMo install: Installing from main branch:

&lt;denchmark-code&gt;pip install Cython
pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[nlp]
./reinstall.sh
&lt;/denchmark-code&gt;

Environment details

OS version: Ubuntu 20.04.1
PyTorch version: 1.8.0.dev20201207+cu110 (I'm using a RTX 3090)
Python version: 3.8.3

	</description>
	<comments>
	</comments>
</bug>