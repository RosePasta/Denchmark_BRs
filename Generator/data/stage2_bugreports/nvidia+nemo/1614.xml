<bug id='1614' author='1-800-BAD-CODE' open_date='2021-01-08T20:51:32Z' closed_time='2021-01-11T14:48:49Z'>
	<summary>Multiple optimizers drops logs</summary>
	<description>
Describe the bug
With multiple optimizers in the training_step, only the final optimizer's logs get printed/logged on the progress bar and in tfevents file. No logged values from the previous optimizers appear on the progress bar or in the tfevents.
Steps/Code to reproduce bug
Running examples/tts/melgan.py will show the issue. Only loss_generator is logged,  despite in the first optimizer's code self.log("loss_discriminator", sum_loss_dis, prog_bar=True, sync_dist=True)
Expected behavior
All values logged in the earlier optimizers should persist and be logged.
Environment overview (please complete the following information)

Environment location: Bare-metal
Method of NeMo install: source, current head (1.0.0b4)

Environment details

PyTorch version 1.7
Python version 3.7

	</description>
	<comments>
		<comment id='1' author='1-800-BAD-CODE' date='2021-01-11T14:48:49Z'>
		I updated pytorch lightning and that seems to have fixed it. Must have been a specific combination of PTL + NeMo that didn't work. Closing,
		</comment>
	</comments>
</bug>