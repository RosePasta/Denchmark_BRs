<bug id='66' author='VrUnRealEngine4' open_date='2015-04-13T18:56:07Z' closed_time='2015-06-09T18:27:24Z'>
	<summary>Not using GPU during inference</summary>
	<description>
I have been monitoring CUDA device activity during the entire running of the DIGITS sessions.
I recently noticed that that during, especially during, the "Test Several Images" sessions none of my CUDA devices become active.... It was my understanding that Caffe can be forced to use CUDA devices during all phases .... testing, training etc.... This would be particularity useful to have CUDA devices used during all phase; especially to people like myself who train and test large CNN with very large data sets on the order of 10,000 to 20,000 images.... obviously split across different test cycles because of host memory limits....
Could you give some pointers to turn on CUDA device usage, in the code, during this phase?
Thanks
	</description>
	<comments>
		<comment id='1' author='VrUnRealEngine4' date='2015-04-13T19:19:37Z'>
		Uncomment &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/27386ed3326157de091f44468c2afcef7097fb8c/digits/model/tasks/caffe_train.py#L884-L885&gt;this line&lt;/denchmark-link&gt;
 and you'll be testing on the GPU.
The reason it's not enabled by default is because I haven't yet nailed down all of the interactions for CPU-only caffe builds. I can detect whether there are GPUs on the system, but I can't detect whether the current version of caffe has been built in CPU-only mode. Since I'm not usually very worried about latency during manual testing, I hadn't gotten around to fixing this yet.
Thanks for pointing it out!
		</comment>
		<comment id='2' author='VrUnRealEngine4' date='2015-04-13T20:42:19Z'>
		Is this something you would want me to look into .... do you have any guidelines to follow up in adding the GPU support for all phases of a Caffe?
would what you say imply I could take your variant of Caffe build it to only use GPU?  I remember an option in the build configuration build to use GPU or CPU. What I didn't understand was it to exclusively use GPU for all computation of a network
		</comment>
		<comment id='3' author='VrUnRealEngine4' date='2015-04-13T22:40:17Z'>
		The two most obvious solutions would be within caffe itself:

Make set_mode_gpu() return false instead of throwing a SIGABRT

That's part of a larger discussion we're having with caffe about error handling


Or make caffe --version return "caffe 1.0 CPU-Only" or something instead of just "caffe".

Also part of a larger issue surrounding versioning



If you have any suggestions for an elegant way of detecting whether caffe is built in CPU-only mode, I'm all ears. You can run caffe device_query -gpu 0 and that will crash iff caffe is built in CPU-only mode, but that's far from elegant and not really an option in a production environment because of the error message popups.
		</comment>
		<comment id='4' author='VrUnRealEngine4' date='2015-04-14T20:12:50Z'>
		WOW!!!  Caffe "throwing a SIGABRT" is really brute force error handling ... HA!!! some neophyte developer wrote that behavior... right? .... :D...
I personally making your recommendation of returning "true/false" is much more reasonable than even throwing a exception since at the call to set_mode_gpu() is more of a consideration to execution path rather than an exception... There maybe a valid argument to call a function like "bool gpu_exist(void);" just to query Caffe or what ever Caffe uses to determine if Caffe can use the GPU for it processing
I don't see what too hard here. Do you need me to change and behavior of Caffe?
		</comment>
		<comment id='5' author='VrUnRealEngine4' date='2015-04-14T20:27:58Z'>
		
Caffe "throwing a SIGABRT" is really brute force error handling ... HA!!!

My comment was not intended as an insult to the caffe developers. SIGABRT is an easy way to get the current stacktrace without worrying about killing other threads, etc. It's a useful feature for developers, and caffe [it seems to me] is currently only really intended for developers.
That being said, it is probably time for their project to mature in some areas - like error handling. If you'd like to help them with that, I'm sure they would appreciate it.
		</comment>
		<comment id='6' author='VrUnRealEngine4' date='2015-04-16T21:50:33Z'>
		From &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/76#issuecomment-93843774&gt;#76 (comment)&lt;/denchmark-link&gt;
:

A quick solution is to look for some type of environment variable like "CAFFE_USE_GPU=1". At least for LINUX, the user of the system can simply enable/disable the GPU usage by an environment variable.
You are already setting a number of environmental variables before one can
use DIGITS/CAFFE, what do you think?

True, but I'm not particularly proud of that solution. I'd be ok with this as a quickfix, but only until we can do it properly somehow.
		</comment>
	</comments>
</bug>