<bug id='339' author='lukeyeager' open_date='2015-10-02T21:50:04Z' closed_time='2016-01-14T15:55:48Z'>
	<summary>Torch performance issues</summary>
	<description>
Breaking discussion from &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/324#issuecomment-142373731&gt;#324 (comment)&lt;/denchmark-link&gt;
 out into a separate issue.

@lukeyeager - I just wanted to make a note that a training job which takes 6sec with Caffe takes 38sec with Torch.
@gheinrich - The difference between Torch and Caffe should be less dramatic on "bigger" models.
You're right. Here's the results of a [very much non-rigorous] test I tried. It's one epoch on a dataset of ~40k images.



.
AlexNet (sec)
GoogLeNet (sec)




Caffe (v0.13.2, with cuDNN v3 and CNMeM)
126
722


Torch (cuDNN v3)
252
969


Slowdown
2x
1.3x




	</description>
	<comments>
		<comment id='1' author='lukeyeager' date='2015-10-09T17:45:35Z'>
		cc &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 as he expressed interest in this.
I have pushed a couple of changes &lt;denchmark-link:https://github.com/gheinrich/DIGITS/commits/dev/torch-speed&gt;there&lt;/denchmark-link&gt;
 which speed Torch training up (figures below are for 20 epochs of training LeNet on MNIST 45k training samples and 15k validation samples, with one validation per epoch):
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/fa00a945198c00a189601d3bb8713b009b05f11a&gt;fa00a94&lt;/denchmark-link&gt;
 removes some unnecessary computations of train set accuracy on CPU side: 187s -&gt; 142s
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/7635864105af716e45bf79382fac8db7093512b4&gt;7635864&lt;/denchmark-link&gt;
 implements a multi-threaded data loader for LMDB: 142s -&gt; 83s
The same benchmark takes 73s with Caffe so there is still some room for improvement.
For example I don't understand why training is slower when I copy tensors to GPU from the data loader threads (as opposed to doing it from the main thread).
The current version greedily allocates new tensors for each training example. It might speed things up to re-use a small number of tensors across batches.
		</comment>
		<comment id='2' author='lukeyeager' date='2015-10-09T17:50:38Z'>
		nice.
Could you point me to instructions on how to create the imagenet lmdb? I'm trying to get things up and running, but that's kind of a blocker for me at the moment...
Should I use caffe's imagenet instructions? &lt;denchmark-link:https://github.com/BVLC/caffe/tree/master/examples/imagenet&gt;https://github.com/BVLC/caffe/tree/master/examples/imagenet&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='lukeyeager' date='2015-10-09T18:39:12Z'>
		Thanks Soumith!
I have submitted PR#64 with decently complete cud_v4 bindings - please take a look:
&lt;denchmark-link:https://github.com/soumith/cudnn.torch/pull/63&gt;soumith/cudnn.torch#63&lt;/denchmark-link&gt;

I will merge it later today if there are no objections.
Best,
-Boris.
From: Soumith Chintala &lt;&lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:notifications@github.com&gt;mailto:notifications@github.com&lt;/denchmark-link&gt;
&gt;
Reply-To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:reply@reply.github.com&gt;reply@reply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:reply@reply.github.com&gt;mailto:reply@reply.github.com&lt;/denchmark-link&gt;
&gt;
Date: Friday, October 9, 2015 at 10:45 AM
To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;mailto:DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&gt;
Subject: Re: [DIGITS] Torch performance issues (&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;#339&lt;/denchmark-link&gt;
)
cc @soumithhttps://github.com/soumith as he expressed interest in this.
I have pushed a couple of changes therehttps://github.com/gheinrich/DIGITS/commits/dev/torch-speed which speed Torch training up (figures below are for 20 epochs of training LeNet on MNIST 45k training samples and 15k validation samples, with one validation per epoch):
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/fa00a945198c00a189601d3bb8713b009b05f11a&gt;fa00a94&lt;/denchmark-link&gt;
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/fa00a945198c00a189601d3bb8713b009b05f11a&gt;fa00a94&lt;/denchmark-link&gt;
 removes some unnecessary computations of train set accuracy on CPU side: 187s -&gt; 142s
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/7635864105af716e45bf79382fac8db7093512b4&gt;7635864&lt;/denchmark-link&gt;
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/7635864105af716e45bf79382fac8db7093512b4&gt;7635864&lt;/denchmark-link&gt;
 implements a multi-threaded data loader for LMDB: 142s -&gt; 83s
The same benchmark takes 73s with Caffe so there is still some room for improvement.
For example I don't understand why training is slower when I copy tensors to GPU from the data loader threads (as opposed to doing it from the main thread).
The current version greedily allocates new tensors for each training example. It might speed things up to re-use a small number of tensors across batches.
&lt;denchmark-h:h2&gt;&lt;/denchmark-h&gt;

Reply to this email directly or view it on GitHubhttps://github.com/&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;/issues/339&lt;/denchmark-link&gt;
#issuecomment-146944476.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

This email message is for the sole use of the intended recipient(s) and may contain
confidential information.  Any unauthorized review, use, disclosure or distribution
is prohibited.  If you are not the intended recipient, please contact the sender by
&lt;denchmark-h:h2&gt;reply email and destroy all copies of the original message.&lt;/denchmark-h&gt;

		</comment>
		<comment id='4' author='lukeyeager' date='2015-10-09T18:43:19Z'>
		Correction - had to resubmit PR against the right branch (R4), here:
&lt;denchmark-link:https://github.com/soumith/cudnn.torch/pull/64&gt;soumith/cudnn.torch#64&lt;/denchmark-link&gt;

From: Boris Fomitchev &lt;&lt;denchmark-link:mailto:bfomitchev@nvidia.com&gt;bfomitchev@nvidia.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:bfomitchev@nvidia.com&gt;mailto:bfomitchev@nvidia.com&lt;/denchmark-link&gt;
&gt;
Date: Friday, October 9, 2015 at 11:39 AM
To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:reply@reply.github.com&gt;reply@reply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:reply@reply.github.com&gt;mailto:reply@reply.github.com&lt;/denchmark-link&gt;
&gt;, NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;mailto:DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&gt;
Subject: Re: [DIGITS] Torch performance issues (&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;#339&lt;/denchmark-link&gt;
)
Thanks Soumith!
I have submitted PR#64 with decently complete cud_v4 bindings - please take a look:
&lt;denchmark-link:https://github.com/soumith/cudnn.torch/pull/63&gt;soumith/cudnn.torch#63&lt;/denchmark-link&gt;

I will merge it later today if there are no objections.
Best,
-Boris.
From: Soumith Chintala &lt;&lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:notifications@github.com&gt;mailto:notifications@github.com&lt;/denchmark-link&gt;
&gt;
Reply-To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:reply@reply.github.com&gt;reply@reply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:reply@reply.github.com&gt;mailto:reply@reply.github.com&lt;/denchmark-link&gt;
&gt;
Date: Friday, October 9, 2015 at 10:45 AM
To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;mailto:DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&gt;
Subject: Re: [DIGITS] Torch performance issues (&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;#339&lt;/denchmark-link&gt;
)
cc @soumithhttps://github.com/soumith as he expressed interest in this.
I have pushed a couple of changes therehttps://github.com/gheinrich/DIGITS/commits/dev/torch-speed which speed Torch training up (figures below are for 20 epochs of training LeNet on MNIST 45k training samples and 15k validation samples, with one validation per epoch):
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/fa00a945198c00a189601d3bb8713b009b05f11a&gt;fa00a94&lt;/denchmark-link&gt;
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/fa00a945198c00a189601d3bb8713b009b05f11a&gt;fa00a94&lt;/denchmark-link&gt;
 removes some unnecessary computations of train set accuracy on CPU side: 187s -&gt; 142s
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/7635864105af716e45bf79382fac8db7093512b4&gt;7635864&lt;/denchmark-link&gt;
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/7635864105af716e45bf79382fac8db7093512b4&gt;7635864&lt;/denchmark-link&gt;
 implements a multi-threaded data loader for LMDB: 142s -&gt; 83s
The same benchmark takes 73s with Caffe so there is still some room for improvement.
For example I don't understand why training is slower when I copy tensors to GPU from the data loader threads (as opposed to doing it from the main thread).
The current version greedily allocates new tensors for each training example. It might speed things up to re-use a small number of tensors across batches.
&lt;denchmark-h:h2&gt;&lt;/denchmark-h&gt;

Reply to this email directly or view it on GitHubhttps://github.com/&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;/issues/339&lt;/denchmark-link&gt;
#issuecomment-146944476.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

This email message is for the sole use of the intended recipient(s) and may contain
confidential information.  Any unauthorized review, use, disclosure or distribution
is prohibited.  If you are not the intended recipient, please contact the sender by
&lt;denchmark-h:h2&gt;reply email and destroy all copies of the original message.&lt;/denchmark-h&gt;

		</comment>
		<comment id='5' author='lukeyeager' date='2015-10-09T19:12:59Z'>
		&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;


Could you point me to instructions on how to create the imagenet lmdb?

You need to create the LMDB using DIGITS if you want to use DIGITS to train a model. This &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/master/docs/ImageFolderFormat.md&gt;page&lt;/denchmark-link&gt;
 explains how to structure your image folders in a way that DIGITS can understand. There is a section on this page that explains how to create a subset of imagenet (it takes a while to create the full imagenet LMDB).
This &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/master/docs/GettingStarted.md#creating-a-dataset&gt;page&lt;/denchmark-link&gt;
 then explains how to create the LMDB using DIGITS. Let us know if you need any more information. Thanks!
		</comment>
		<comment id='6' author='lukeyeager' date='2015-10-10T00:20:08Z'>
		This is now merged into master cudnn.torch/R4, thanks Soumith!
Best,
-Boris.
From: Boris Fomitchev
Sent: Friday, October 09, 2015 11:43 AM
To: NVIDIA/DIGITS; Soumith Chintala
Subject: Re: [DIGITS] Torch performance issues (&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;#339&lt;/denchmark-link&gt;
)
Correction - had to resubmit PR against the right branch (R4), here:
&lt;denchmark-link:https://github.com/soumith/cudnn.torch/pull/64&gt;soumith/cudnn.torch#64&lt;/denchmark-link&gt;

From: Boris Fomitchev &lt;&lt;denchmark-link:mailto:bfomitchev@nvidia.com&gt;bfomitchev@nvidia.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:bfomitchev@nvidia.com&gt;mailto:bfomitchev@nvidia.com&lt;/denchmark-link&gt;
&gt;
Date: Friday, October 9, 2015 at 11:39 AM
To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:reply@reply.github.com&gt;reply@reply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:reply@reply.github.com&gt;mailto:reply@reply.github.com&lt;/denchmark-link&gt;
&gt;, NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;mailto:DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&gt;
Subject: Re: [DIGITS] Torch performance issues (&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;#339&lt;/denchmark-link&gt;
)
Thanks Soumith!
I have submitted PR#64 with decently complete cud_v4 bindings - please take a look:
&lt;denchmark-link:https://github.com/soumith/cudnn.torch/pull/63&gt;soumith/cudnn.torch#63&lt;/denchmark-link&gt;

I will merge it later today if there are no objections.
Best,
-Boris.
From: Soumith Chintala &lt;&lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:notifications@github.com&gt;mailto:notifications@github.com&lt;/denchmark-link&gt;
&gt;
Reply-To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:reply@reply.github.com&gt;reply@reply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:reply@reply.github.com&gt;mailto:reply@reply.github.com&lt;/denchmark-link&gt;
&gt;
Date: Friday, October 9, 2015 at 10:45 AM
To: NVIDIA/DIGITS &lt;&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&lt;denchmark-link:mailto:DIGITS@noreply.github.com&gt;mailto:DIGITS@noreply.github.com&lt;/denchmark-link&gt;
&gt;
Subject: Re: [DIGITS] Torch performance issues (&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;#339&lt;/denchmark-link&gt;
)
cc @soumithhttps://github.com/soumith as he expressed interest in this.
I have pushed a couple of changes therehttps://github.com/gheinrich/DIGITS/commits/dev/torch-speed which speed Torch training up (figures below are for 20 epochs of training LeNet on MNIST 45k training samples and 15k validation samples, with one validation per epoch):
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/fa00a945198c00a189601d3bb8713b009b05f11a&gt;fa00a94&lt;/denchmark-link&gt;
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/fa00a945198c00a189601d3bb8713b009b05f11a&gt;fa00a94&lt;/denchmark-link&gt;
 removes some unnecessary computations of train set accuracy on CPU side: 187s -&gt; 142s
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/7635864105af716e45bf79382fac8db7093512b4&gt;7635864&lt;/denchmark-link&gt;
&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/7635864105af716e45bf79382fac8db7093512b4&gt;7635864&lt;/denchmark-link&gt;
 implements a multi-threaded data loader for LMDB: 142s -&gt; 83s
The same benchmark takes 73s with Caffe so there is still some room for improvement.
For example I don't understand why training is slower when I copy tensors to GPU from the data loader threads (as opposed to doing it from the main thread).
The current version greedily allocates new tensors for each training example. It might speed things up to re-use a small number of tensors across batches.
&lt;denchmark-h:h2&gt;&lt;/denchmark-h&gt;

Reply to this email directly or view it on GitHubhttps://github.com/&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/339&gt;/issues/339&lt;/denchmark-link&gt;
#issuecomment-146944476.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

This email message is for the sole use of the intended recipient(s) and may contain
confidential information.  Any unauthorized review, use, disclosure or distribution
is prohibited.  If you are not the intended recipient, please contact the sender by
&lt;denchmark-h:h2&gt;reply email and destroy all copies of the original message.&lt;/denchmark-h&gt;

		</comment>
		<comment id='7' author='lukeyeager' date='2016-01-14T15:55:48Z'>
		I don't think there is a general performance issue with the integration of Torch into DIGITS anymore. Some models train faster with Torch, other models train faster with Caffe. Some numbers below:

LeNet on MNIST (30 epochs):




Number of GPUs
Caffe
Torch




1
55s
56s


2
1m6s
1m27s



(training is slower with multiple GPUs presumably due to the communication overhead)

Alexnet on upscaled 256x256 CIFAR10 (5 epochs):




Number of GPUs
Caffe
Torch




1
9m31s
6m38s


2
7m46s
5m33s




GoogleNet on upscaled 256x256 CIFAR10 (1 epoch):




Number of GPUs
Caffe
Torch




1
4m3s
11m13s


2
3m12s
7m32s



(Torch slowliness mostly due to extra Batch Normalization layers)
		</comment>
		<comment id='8' author='lukeyeager' date='2016-01-14T18:08:34Z'>
		Interesting data points. Thanks for sharing. Just a note on BatchNorm, latest nn/cunn have super optimized batchnorm (faster than CuDNN R4).
		</comment>
	</comments>
</bug>