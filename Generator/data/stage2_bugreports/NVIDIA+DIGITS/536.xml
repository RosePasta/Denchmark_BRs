<bug id='536' author='michaelholm-ce' open_date='2016-01-21T22:29:16Z' closed_time='2016-02-23T00:30:18Z'>
	<summary>Problem with Batched Inference in example.py script</summary>
	<description>
I may be missing something here, or I have found a bug.
I believed that changing the batch_size to something greater than 1 (i.e. batched inference) should give the same individual scores, just maybe complete the task faster.  However, I have found that changing the batch_size actually changes the resulting scores (they are no longer correct).
Am I in need of a deeper lesson on neural networks, or have I found a bug?
Thanks,
Michael
	</description>
	<comments>
		<comment id='1' author='michaelholm-ce' date='2016-01-21T22:35:46Z'>
		I assume you're talking about the script at examples/classification/example.py.
Are you running into this bug - &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/409&gt;#409&lt;/denchmark-link&gt;
? Which version of DIGITS do you have?
$ ./digits-devserver --version
3.1-dev
		</comment>
		<comment id='2' author='michaelholm-ce' date='2016-01-22T14:41:43Z'>
		I'm running the script at examples/classification/example.py with digits version 3.1-dev.
I did read issue &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/409&gt;#409&lt;/denchmark-link&gt;
 and made the deep copy scores change, but the problem persisted.
		</comment>
		<comment id='3' author='michaelholm-ce' date='2016-01-22T15:06:57Z'>
		More specifically, I changed the example.py script only in the following way:
I replaced image_files = [args['image']] with
 im_path = args['image']
 if os.path.isdir(im_path):
      image_files = [os.path.join(im_path,f) for f in os.listdir(im_path)]
  elif os.path.isfile(im_path):
      image_files = [im_path]
and I changed batch_size=1 to batch_size=25 in the forward_pass function (and changed scores = output to scores = np.copy(output) in the same function).
I have a directory with N images.  When I run the images individually, I get the correct scores.  When I run the whole directory, the first image scores correctly and the remainder score incorrectly.
		</comment>
		<comment id='4' author='michaelholm-ce' date='2016-01-22T15:32:17Z'>
		I am wondering why you had to change the code to to a deep copy of scores since the change should already be there on 3.1-dev? (see this &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/master/examples/classification/example.py#L143&gt;line&lt;/denchmark-link&gt;
). Or am I missing anything?
		</comment>
		<comment id='5' author='michaelholm-ce' date='2016-01-22T15:39:31Z'>
		You're right &lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
 -- my mistake.  The truth is that I first saw the problem when working with an older version (when I did make the deep copy change).  When the problem persisted, I updated to version 3.1-dev (where I remade the other changes listed above) and the problem persisted.  Thanks for pointing that out.
		</comment>
		<comment id='6' author='michaelholm-ce' date='2016-01-22T16:07:33Z'>
		I tried making the same changes as you did however it feels to me like predictions are correct. Did you try using use_archive.py?
		</comment>
		<comment id='7' author='michaelholm-ce' date='2016-01-22T16:26:14Z'>
		To clarify &lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
 , are you saying that you ran  on a full directory (even as small as two images), and the output scores were identical to the scores you get when you run  on the images individually?
I have used use_archive.py in the past.  However, there was a minor change in the deploy.prototxt format among versions, and since use_archive.py takes a .tar file, that always causes an error.  They both call the same classify function, though, so I don't see that making any difference.
		</comment>
		<comment id='8' author='michaelholm-ce' date='2016-01-23T15:00:28Z'>
		Yes, I pushed a version of the code that includes your change in  &lt;denchmark-link:https://github.com/gheinrich/DIGITS/tree/dev/use-archive-with-dir&gt;there&lt;/denchmark-link&gt;
.
I have created a directory with images from the MNIST dataset. If I run the script on this directory then all images are correctly classified with high confidence:
&lt;denchmark-code&gt;$ ./use_archive.py ~/Downloads/20160117-135614-955c_epoch_5.0.tar.gz images/
Unknown file: train_val.prototxt
Unknown file: solver.prototxt
Processed 5/5 images ...
Classification took 0.00620698928833 seconds.
----------------------- Prediction for images/three.png ------------------------
 99.9999% - "3"
  0.0001% - "8"
  0.0000% - "7"
  0.0000% - "9"
  0.0000% - "2"
------------------------ Prediction for images/five.png ------------------------
 81.0012% - "5"
 10.7394% - "6"
  8.1783% - "8"
  0.0762% - "9"
  0.0022% - "3"
------------------------ Prediction for images/four.png ------------------------
 99.8313% - "4"
  0.1677% - "9"
  0.0007% - "7"
  0.0001% - "8"
  0.0000% - "1"
------------------------ Prediction for images/one.png -------------------------
 99.7627% - "1"
  0.1565% - "7"
  0.0508% - "4"
  0.0170% - "8"
  0.0072% - "2"
------------------------ Prediction for images/zero.png ------------------------
 99.8964% - "0"
  0.0868% - "6"
  0.0103% - "2"
  0.0051% - "9"
  0.0004% - "8"
Script took 0.253932952881 seconds.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='michaelholm-ce' date='2016-01-25T15:54:17Z'>
		Hello &lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
.  Thank you for running the test.  I see that in your test, the five image files were classified correctly.  Will you please take the test one step further and verify that you get the same scores (or very close) when you run the image files individually?
I am still quite concerned that there is something wrong here.  I checked out the branch you supplied, used a caffe model I trained with digits showing ~98% accuracy on the validation set, and tested on color images that are the exact size expected by the model (256x256).  In every case, the first image in a batch is classified correctly (and with identical score as when scored individually), but all the other test images in the directory have different scores.  When I change batch_size to 1 (as opposed to the default 25) in the forward_pass function, then all images are scored correctly, but this substantially slows down classification speed.
		</comment>
		<comment id='10' author='michaelholm-ce' date='2016-01-26T12:34:06Z'>
		On &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/4825f8de2374904139ca5ea3fd5a7df310f0b6d6&gt;4825f8d&lt;/denchmark-link&gt;
 I get exactly the same predictions when I classify images one by one:
&lt;denchmark-code&gt;(venv)greg@greg:~/ws/digits/examples/classification$ ./use_archive.py ~/Downloads/20160117-135614-955c_epoch_5.0.tar.gz images/three.png 
Unknown file: train_val.prototxt
Unknown file: solver.prototxt
Processed 1/1 images ...
Classification took 0.00297689437866 seconds.
----------------------- Prediction for images/three.png ------------------------
 99.9999% - "3"
  0.0001% - "8"
  0.0000% - "7"
  0.0000% - "9"
  0.0000% - "2"

Script took 0.312922000885 seconds.
(venv)greg@greg:~/ws/digits/examples/classification$ ./use_archive.py ~/Downloads/20160117-135614-955c_epoch_5.0.tar.gz images/five.png 
Unknown file: train_val.prototxt
Unknown file: solver.prototxt
Processed 1/1 images ...
Classification took 0.00299596786499 seconds.
------------------------ Prediction for images/five.png ------------------------
 81.0012% - "5"
 10.7394% - "6"
  8.1782% - "8"
  0.0762% - "9"
  0.0022% - "3"

Script took 0.267504930496 seconds.
(venv)greg@greg:~/ws/digits/examples/classification$ ./use_archive.py ~/Downloads/20160117-135614-955c_epoch_5.0.tar.gz images/four.png 
Unknown file: train_val.prototxt
Unknown file: solver.prototxt
Processed 1/1 images ...
Classification took 0.00307488441467 seconds.
------------------------ Prediction for images/four.png ------------------------
 99.8313% - "4"
  0.1677% - "9"
  0.0007% - "7"
  0.0001% - "8"
  0.0000% - "1"

Script took 0.2662088871 seconds.
(venv)greg@greg:~/ws/digits/examples/classification$ ./use_archive.py ~/Downloads/20160117-135614-955c_epoch_5.0.tar.gz images/one.png 
Unknown file: train_val.prototxt
Unknown file: solver.prototxt
Processed 1/1 images ...
Classification took 0.00297904014587 seconds.
------------------------ Prediction for images/one.png -------------------------
 99.7627% - "1"
  0.1565% - "7"
  0.0508% - "4"
  0.0170% - "8"
  0.0072% - "2"

Script took 0.259674787521 seconds.
(venv)greg@greg:~/ws/digits/examples/classification$ ./use_archive.py ~/Downloads/20160117-135614-955c_epoch_5.0.tar.gz images/zero.png 
Unknown file: train_val.prototxt
Unknown file: solver.prototxt
Processed 1/1 images ...
Classification took 0.00302720069885 seconds.
------------------------ Prediction for images/zero.png ------------------------
 99.8964% - "0"
  0.0868% - "6"
  0.0103% - "2"
  0.0051% - "9"
  0.0004% - "8"

Script took 0.258541107178 seconds.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='michaelholm-ce' date='2016-01-26T20:17:24Z'>
		Thank you, &lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
, for running the additional test.
I'm at a loss as to where to go from here.  I checked out the digits commit you shared above (&lt;denchmark-link:https://github.com/NVIDIA/DIGITS/commit/4825f8de2374904139ca5ea3fd5a7df310f0b6d6&gt;4825f8d&lt;/denchmark-link&gt;
) and still get garbage output (for all images after the first) when I run against a directory when  is greater than 1.
Will you share the exact model and images you used, please, so I can run an identical test?
		</comment>
		<comment id='12' author='michaelholm-ce' date='2016-01-26T20:52:53Z'>
		I used the canonical LeNet, trained on MNIST. The 5 images I used were randomly picked from the test folder. Or would you like me to upload the binary files (where)?
		</comment>
		<comment id='13' author='michaelholm-ce' date='2016-02-05T20:43:18Z'>
		&lt;denchmark-link:https://github.com/michaelholm-ce&gt;@michaelholm-ce&lt;/denchmark-link&gt;
 thanks for the bug report, this does seem to be a legit bug!

You are using cuDNN v4 on a Maxwell GPU, is that correct?
If you increase the number of images in your batch (e.g. from 5 to 30), do you start getting the right answers again?

		</comment>
		<comment id='14' author='michaelholm-ce' date='2016-02-23T00:30:18Z'>
		If anyone else runs into this issue, try upgrading from the cuDNN 4 release candidate (4.0.4) to the final release (4.0.7).
		</comment>
	</comments>
</bug>