<bug id='1173' author='lukeyeager' open_date='2016-10-17T20:19:44Z' closed_time='2016-12-05T23:15:48Z'>
	<summary>torch classify many task failed with error code -11</summary>
	<description>
Torch jobs on TravisCI have been failing for a while now (&gt;1 week, &lt;2 weeks):
&lt;denchmark-link:https://travis-ci.org/NVIDIA/DIGITS/jobs/167788079&gt;https://travis-ci.org/NVIDIA/DIGITS/jobs/167788079&lt;/denchmark-link&gt;

&lt;denchmark-link:https://travis-ci.org/lukeyeager/DIGITS/jobs/167736840&gt;https://travis-ci.org/lukeyeager/DIGITS/jobs/167736840&lt;/denchmark-link&gt;

&lt;denchmark-link:https://travis-ci.org/lukeyeager/DIGITS/jobs/167736180&gt;https://travis-ci.org/lukeyeager/DIGITS/jobs/167736180&lt;/denchmark-link&gt;

Things all the test failures have in common:

Framework is Torch
Test class is *LeNet*

These are the only tests which use a "real" network (i.e. one with convolution layers)


Test name is *classify_many* or *top_n*

These are the only tests which do inference on multiple inputs


They all fail with this message: torch classify many task failed with error code -11

In Python, when a script returns a negative error code, it means the script was killed with a signal. Signal 11 means "SIGSEGV Invalid memory reference".



	</description>
	<comments>
		<comment id='1' author='lukeyeager' date='2016-10-17T20:26:26Z'>
		Trying to find the offending change is difficult. Here's what has changed in the last two weeks:
&lt;denchmark-link:https://github.com/torch/distro/compare/5c7c762518374422873dff5a2da834704f0658db...bd5e664194953539e928546e987c615a481a8eee&gt;torch/distro@5c7c762...bd5e664&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/torch/nn/compare/c40e59edd2ddfdec166d3557cceaf25898bba52c...a8e63f2da3d3d84a7e1eed917572901a9ffba5d9&gt;torch/nn@c40e59e...a8e63f2&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/torch/torch7/compare/b1ce165d049ec54b38c728d227352f8d2f0d526d...4f7843e8be8de37d0474e9d4a529261b147e8a8e&gt;torch/torch7@b1ce165...4f7843e&lt;/denchmark-link&gt;

What's worse is I can't reproduce it locally yet.
&lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 does the numbered list above make you think of any potential breaking changes that may have gone into Torch recently?
		</comment>
		<comment id='2' author='lukeyeager' date='2016-10-17T21:03:18Z'>
		the bug seems to be around not able to load libcudnn.so
i'm digging into it a bit, but i wonder if anything wrt deploying cudnn onto these build boxes has changed recently
		</comment>
		<comment id='3' author='lukeyeager' date='2016-10-17T21:09:18Z'>
		I think these messages are just warnings:
&lt;denchmark-code&gt;Failed to load cudnn backend (is libcudnn.so in your library path?)
Failed to load cunn backend (is CUDA installed?)
Falling back to legacy nn backend
&lt;/denchmark-code&gt;

Thanks for taking a look! I was just hoping you'd think of something off the top of your head - I wasn't asking you to actually dig into it.
		</comment>
		<comment id='4' author='lukeyeager' date='2016-10-17T21:45:49Z'>
		&lt;denchmark-link:https://github.com/lukeyeager&gt;@lukeyeager&lt;/denchmark-link&gt;
 i could prob think of the offending change if i knew what the error message that is emmitted from the torch side. Looks like that isn't being logged right now?
		</comment>
		<comment id='5' author='lukeyeager' date='2016-10-17T21:48:28Z'>
		No, not right now. Once I figure out how to reproduce it, I'll post back here.
		</comment>
		<comment id='6' author='lukeyeager' date='2016-10-18T15:08:59Z'>
		Since it's a segmentation fault there is probably no error message from Torch. I'll see if I can isolate the particular line of code that is causing the crash.
		</comment>
		<comment id='7' author='lukeyeager' date='2016-10-18T19:13:02Z'>
		Hi &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 it seems like setting the number of threads to 1 () as in &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/1179&gt;#1179&lt;/denchmark-link&gt;
 - as opposed to the default  - "solves" this issue. Are you aware of any concurrency issues when running convolutions on CPU-only systems? Thanks for your offer to help by the way!
		</comment>
		<comment id='8' author='lukeyeager' date='2016-10-18T19:23:35Z'>
		&lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
 definitely not aware of issues with convolutions having concurrency issues.
This is because the logic to compute convolution layers simply delegates to the underlying BLAS library.
What i suspect is that you might be using OpenBLAS and not setting NO_AFFINITY=1 which is a known openblas issue with concurrency i think. &lt;denchmark-link:https://github.com/torch/distro/blob/master/install-deps#L18&gt;https://github.com/torch/distro/blob/master/install-deps#L18&lt;/denchmark-link&gt;

If you link Torch against MKL, or if OpenBLAS was properly compiled, no such issues should occur.
		</comment>
		<comment id='9' author='lukeyeager' date='2016-10-18T20:06:53Z'>
		Thanks for the pointer Soumith. We're installing libopenblas-dev package with apt so it is impractical for us to rebuild OpenBLAS. However doing export OPENBLAS_MAIN_FREE=1 at run-time also seems to address the issue. Cheers!
		</comment>
		<comment id='10' author='lukeyeager' date='2016-10-18T20:48:30Z'>
		Thanks for investigating &lt;denchmark-link:https://github.com/gheinrich&gt;@gheinrich&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='11' author='lukeyeager' date='2016-10-19T19:37:24Z'>
		It looks like &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/1179&gt;#1179&lt;/denchmark-link&gt;
 may not have fixed this entirely.
&lt;denchmark-link:https://travis-ci.org/lukeyeager/DIGITS/builds/169015952&gt;https://travis-ci.org/lukeyeager/DIGITS/builds/169015952&lt;/denchmark-link&gt;

&lt;denchmark-link:https://travis-ci.org/lukeyeager/DIGITS/jobs/169032948&gt;https://travis-ci.org/lukeyeager/DIGITS/jobs/169032948&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='lukeyeager' date='2016-11-29T01:33:26Z'>
		Hmm...
Torch &lt;denchmark-link:https://github.com/torch/distro/blob/4f5ad0ff2c/install-deps#L25&gt;installs&lt;/denchmark-link&gt;
 OpenBLAS to it's default location of  (&lt;denchmark-link:https://github.com/torch/distro/blob/4f5ad0ff2c/install.sh#L41&gt;this line&lt;/denchmark-link&gt;
 tells torch where to find it).
We &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/blob/435948676e/.travis.yml#L76&gt;cache&lt;/denchmark-link&gt;
 the torch build in DIGITS, but not the OpenBLAS build. Since ToT OpenBLAS has the same SONAME as the version installed with deb packages on 14.04, Torch loads the library just fine.
But it's possible that running a build of torch that was built for ToT OpenBLAS is unable to use the old version that comes on 14.04, and that's why we're seeing memory corruption issues.
Need to rewrite some stuff tomorrow to test this theory...
		</comment>
	</comments>
</bug>