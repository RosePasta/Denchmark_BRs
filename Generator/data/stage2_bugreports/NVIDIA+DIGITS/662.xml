<bug id='662' author='TimZaman' open_date='2016-03-29T13:19:05Z' closed_time='2016-04-10T14:12:11Z'>
	<summary>Learning rate 'Exponential Decay' policy wrong.</summary>
	<description>
Version 
I often have issues with the learning rate and the decay. Sometimes it tells me it cannot process 'nan', sometimes it's correct, sometimes it jitters a bit up and down, but most often it does this. I can reproduce this only half of the time.
Base learning rate: 1.0
Policy: Exponential Decay
Gamma: 0.95
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/7721540/14109283/8d07aa0e-f5c1-11e5-8e38-7a8fedae70a6.png&gt;&lt;/denchmark-link&gt;

Then this happens:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/7721540/14109287/90dd9634-f5c1-11e5-8c0a-2d6637aee321.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='TimZaman' date='2016-03-29T15:15:32Z'>
		Thanks for the bug report. The learning rate seems to be decreasing way too quickly indeed. Are you using Torch or Caffe? I think I can reproduce your issue with Torch.
		</comment>
		<comment id='2' author='TimZaman' date='2016-03-29T15:18:51Z'>
		Torch indeed. Did not try Caffe.
		</comment>
		<comment id='3' author='TimZaman' date='2016-03-29T16:27:52Z'>
		&lt;denchmark-link:https://github.com/TimZaman&gt;@TimZaman&lt;/denchmark-link&gt;
 do you mind verifying that &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/663&gt;#663&lt;/denchmark-link&gt;
 fixes the issue? Thanks!
		</comment>
		<comment id='4' author='TimZaman' date='2016-03-29T18:25:07Z'>
		Yes. Merci beaucoup Greg!
		</comment>
		<comment id='5' author='TimZaman' date='2016-03-31T13:36:06Z'>
		Hmm even after the update, still something fishy here: (500 epochs used)
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/7721540/14177361/3953c032-f756-11e5-9dc7-5046c61d8296.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/7721540/14177362/396686ea-f756-11e5-9e34-ed5be7f95949.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='TimZaman' date='2016-03-31T15:38:41Z'>
		I just tried running 500 epochs of training. This is what the learning rate looked like:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3889770/14181453/d4cee660-f75e-11e5-8936-342d639d7ac6.png&gt;&lt;/denchmark-link&gt;

I was using this version:
&lt;denchmark-code&gt;$ git describe
v3.2.0-73-gb4dd9c7
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='TimZaman' date='2016-04-06T03:09:02Z'>
		&lt;denchmark-link:https://github.com/TimZaman&gt;@TimZaman&lt;/denchmark-link&gt;
 were you able to verify that you were using the right version of DIGITS? The exponential decay is working for me for 500 epochs.
		</comment>
		<comment id='8' author='TimZaman' date='2016-04-10T14:12:10Z'>
		No response and this is working for me on v3.2.0-73-gb4dd9c7 so I am closing the bug.
		</comment>
		<comment id='9' author='TimZaman' date='2016-04-10T14:14:36Z'>
		i was using the most up to date version for sure. There's something going on with the learning rate for sure, i will try to hunt down the cause next time instead of posting the problem.
		</comment>
	</comments>
</bug>