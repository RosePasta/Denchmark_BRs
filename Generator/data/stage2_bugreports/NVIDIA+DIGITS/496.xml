<bug id='496' author='Liuftvafas' open_date='2016-01-07T15:16:15Z' closed_time='2016-05-10T20:25:05Z'>
	<summary>Torch SpatialBatchNormalization layer cannot be loaded from weights</summary>
	<description>
During the training a snapshot of model is saved for later use. It consists of model script, weights snapshot, mean data, and labels file.
However, if model contains SpatialBatchNormalization layer that's not enough. The layer contains fields running_mean and running_std which are calculated from training data but later lost during save/load snapshot procedure.
Maybe saving a sanitized model (with cleared intermediate states) instead of weights would be a good idea?
	</description>
	<comments>
		<comment id='1' author='Liuftvafas' date='2016-01-10T09:09:53Z'>
		Thanks for the report &lt;denchmark-link:https://github.com/Liuftvafas&gt;@Liuftvafas&lt;/denchmark-link&gt;
. Indeed we're using the top-level module's  method to save/reload learnable parameters.
In my understanding, running_mean and running_var are learned during training. Together with weight=gamma and bias=beta, these comprise the set of learnable parameters in nn.SpatialBatchNormalization. Therefore, I feel it would make sense to add running_mean and running_var to the parameters that are returned by getParameters(). Possibly this could be implemented by overloading nn.Module:getParameters() in nn.SpatialBatchNormalization? Doing it this way would keep the model abstraction unchanged, right?
cc &lt;denchmark-link:https://github.com/colesbury&gt;@colesbury&lt;/denchmark-link&gt;
 for information as he authored a recent rework of . Sorry for the spam but I think you might be interested in contributing to this discussion. Thanks in advance!
Note: we're not using torch.save and torch.load to save/load a model since we're only interested in saving the model for FPROP and we don't want to save gradients, which consume a lot of disk space.
		</comment>
		<comment id='2' author='Liuftvafas' date='2016-01-10T10:37:54Z'>
		running_mean and running_var are not learned in the sense how weights and biases are learned via back propagation in training. They are rather calculated from training data. If that does not contradict the learnable parameters idea, then it would be a good idea to add them to getParameters().
I was introduced with a model  function in &lt;denchmark-link:https://github.com/soumith&gt;@soumith&lt;/denchmark-link&gt;
 imagenet-multiGPU.torch code which removes gradients to save disk space but does not remove parameters like . Model takes only 2x of weights size on disk after it is applied.
		</comment>
		<comment id='3' author='Liuftvafas' date='2016-01-10T14:06:08Z'>
		Thanks, now I see what you mean by sanitized model.  I'm interested to know what you all think about adding running_mean and running_var to the table returned by parameters() and if it's better or worse than a sanitize() function.
		</comment>
		<comment id='4' author='Liuftvafas' date='2016-01-12T20:26:10Z'>
		I think adding running_mean and running_var to parameters() will break the optim package, since these parameters are not learned via SGD (or the other optimization methods in optim).
I think the best strategy for now is to / (&lt;denchmark-link:https://github.com/torch/nn/pull/526&gt;torch/nn#526&lt;/denchmark-link&gt;
) and save the entire model via .
		</comment>
		<comment id='5' author='Liuftvafas' date='2016-04-22T15:04:52Z'>
		Hi &lt;denchmark-link:https://github.com/Liuftvafas&gt;@Liuftvafas&lt;/denchmark-link&gt;
 I finally got around to doing this. I know it's late but can you review/try &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/pull/701&gt;#701&lt;/denchmark-link&gt;
? Thanks!
		</comment>
	</comments>
</bug>