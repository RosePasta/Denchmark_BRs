<bug id='59' author='lukeyeager' open_date='2015-04-10T01:08:41Z' closed_time='2015-04-17T22:51:12Z'>
	<summary>Standardize on BGR, but be ready for caffe fix</summary>
	<description>
Problem originally reported in &lt;denchmark-link:https://github.com/NVIDIA/DIGITS/issues/52&gt;#52&lt;/denchmark-link&gt;
. Remaining issues:

 Save unencoded images to database in BGR (so caffe will read them the same way as encoded images and resulting .caffemodel files will match)*
 Save mean.binaryproto in BGR (so the mean will be subtracted from the correct channels)
 Note in the dataset in which order the channels are stored (so we can keep track of which datasets were created in which ways)

* This will be inconvenient when adding other frameworks.
To be explicit, here is a table representing the problem and the changes:




Old, encoded
Old, unencoded
New, encoded
New, unencoded




Model Creation






Database format
JPEG
RGB
JPEG
BGR


Mean
RGB
RGB
BGR
BGR


Model trained incorrectly
_Yes_
No
No
No


Inference






Image needs channel swap
Yes
No
Yes
Yes


Mean needs channel swap
No
No
No
No



	</description>
	<comments>
		<comment id='1' author='lukeyeager' date='2015-04-16T18:39:38Z'>
		All of the listed features are implemented, and I've verified that data is being prepared as expected. After days of testing and fiddling with random number seeds, I still can't create reproducible results between PNG compression (lossless) and raw BGR, but I am fairly confident it's due to inconsistencies in caffe's results during training rather than any mistakes during data preparation. That's largely due to the fact that I can't reproduce results on back-to-back training sessions on the same GPU with the same data and network, either.
		</comment>
	</comments>
</bug>