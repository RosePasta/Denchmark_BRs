<bug id='208' author='svlandeg' open_date='2020-01-13T10:54:29Z' closed_time='2020-01-13T16:08:08Z'>
	<summary>chain operator on develop branch</summary>
	<description>
Something's wrong with the chain operator on the develop branch.
This works:
&lt;denchmark-code&gt;layer1 = Linear(nO=9, nI=3)
layer2 = Linear(nO=12, nI=9)
layer3 = Linear(nO=5, nI=12)
model = chain(layer1, layer2, layer3)
model.set_dim("nO", 5)
&lt;/denchmark-code&gt;

But this doesn't :
&lt;denchmark-code&gt;layer1 = Linear(nO=9, nI=3)
layer2 = Linear(nO=12, nI=9)
layer3 = Linear(nO=5, nI=12)
with Model.define_operators({"&gt;&gt;": chain}):
    model = layer1 &gt;&gt; layer2 &gt;&gt; layer3
model.set_dim("nO", 5)
&lt;/denchmark-code&gt;

The latter crashes saying that the dimension of nO can't be changed from 12 to 5. It seems to do this because it's referring to nO of the SECOND layer, not the third, thus running into an inconsistency.
What seems to happen is that the second chain operator &lt;denchmark-link:https://github.com/explosion/thinc/blob/develop/thinc/layers/chain.py#L25&gt;breaks out of its constructor&lt;/denchmark-link&gt;
 by extending  and returning that. But that seems to cause some trouble with the initialization flow....?
	</description>
	<comments>
		<comment id='1' author='svlandeg' date='2020-01-13T11:14:55Z'>
		Ah, hmm.
The chain operator's had a bunch of hacks that should be undone. The init logic is pretty horrible atm and doesn't work in lots of cases.
I think we should remove the claim that we can infer output sizes in chain. It'll let us unhack the init logic, and make things more consistent.
&lt;denchmark-link:https://github.com/ines&gt;@ines&lt;/denchmark-link&gt;
 Should we just make  required instead of optional? It's bad that currently you can write a network with shapes that can't be inferred.
		</comment>
		<comment id='2' author='svlandeg' date='2020-01-13T11:19:33Z'>
		I'm not sure that the inference of output sizes really is the problem. I actually think that there's an internal inconsistency in the self.layers field. In my example above, I think it crashes because model.layers[-1] is pointing to the wrong layer, and if that's the case, training/prediction is probably going to fail, too ?
		</comment>
		<comment id='3' author='svlandeg' date='2020-01-13T11:21:53Z'>
		Oh I see what you mean.
I bet it's an interaction with the operator overloading. I think I know how to debug this, thanks.
		</comment>
		<comment id='4' author='svlandeg' date='2020-01-13T11:46:26Z'>
		Note that this also doesn't work:
&lt;denchmark-code&gt;layer1 = Linear(nO=9, nI=3)
layer2 = Linear(nO=12, nI=9)
layer3 = Linear(nO=5, nI=12)
model1 = chain(layer1, layer2, layer3)

layer4 = Linear(nO=7, nI=5)
layer5 = Linear(nO=13, nI=7)
model2 = chain(layer4, layer5)

model = chain(model1, model2)
model.set_dim("nO", 13)
&lt;/denchmark-code&gt;

Now the final model seems to think its nO is 5 (from model1) instead of 13. So it looks like this is an issue when combining multiple models (and in my original post, using chain created only one model with 3 layers, instead of this recursive adding of layers)
		</comment>
		<comment id='5' author='svlandeg' date='2020-01-13T11:48:45Z'>
		We should just kill this auto-flattening case, it's Not Good. It will also lead to all sorts of problems with references.
I added it because for concatenate, it can be pretty inefficient to have nested concatenation instead of flat concatenation. But the same isn't true for chain, and the results can be surprising.
The specific bug happens because we set the chain nO dimension when we create the first chain object, then later we extend the outputs.
		</comment>
		<comment id='6' author='svlandeg' date='2020-01-13T16:08:08Z'>
		Fixed in PR &lt;denchmark-link:https://github.com/explosion/thinc/pull/209&gt;#209&lt;/denchmark-link&gt;
 - thanks Matt! :-)
		</comment>
	</comments>
</bug>