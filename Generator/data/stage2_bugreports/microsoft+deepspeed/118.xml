<bug id='118' author='astariul' open_date='2020-03-04T06:02:14Z' closed_time='2020-03-10T21:03:24Z'>
	<summary>TypeError: FP16_DeepSpeedZeroOptimizer is not an Optimizer</summary>
	<description>
I'm trying to use 1-Cycle scheduler, but I meet the following error :

TypeError: FP16_DeepSpeedZeroOptimizer is not an Optimizer

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Here is my configuration file :
&lt;denchmark-code&gt;{
    "train_batch_size": 64,
    "train_micro_batch_size_per_gpu": 1,
    "gradient_accumulation_steps": 16,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 3e-05,
            "betas": [
                0.9,
                0.999
            ],
            "eps": 1e-8,
            "weight_decay": 0.01
        }
    },
    "gradient_clipping": 0.1,
    "scheduler": {
        "type": "OneCycle",
        "params": {
            "cycle_first_step_size": 16000,
            "cycle_first_stair_count": 8000,
            "decay_step_size": 16000,
            "cycle_min_lr": 1e-06,
            "cycle_max_lr": 3e-05,
            "decay_lr_rate": 1e-07,
            "cycle_min_mom": 0.85,
            "cycle_max_mom": 0.99,
            "decay_mom_rate": 0.0
        }
    },
    "zero_optimization": true,
    "disable_allgather": true,
    "fp16": {
        "enabled": true,
        "loss_scale": 0,
        "min_loss_scale": 1
    }
}
&lt;/denchmark-code&gt;

When using another Scheduler (with FP16), I meet no problem.
	</description>
	<comments>
		<comment id='1' author='astariul' date='2020-03-04T06:37:32Z'>
		Thanks for reporting this bug. We will take a look at this as soon as possible. I just created two test cases that reproduce the error (one with ZeRO and one with FP16 but no ZeRO).
&lt;denchmark-link:https://github.com/microsoft/DeepSpeed/blob/jeffra/onecycle_bug/tests/unit/test_fp16.py#L147-L246&gt;https://github.com/microsoft/DeepSpeed/blob/jeffra/onecycle_bug/tests/unit/test_fp16.py#L147-L246&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='astariul' date='2020-03-05T06:21:18Z'>
		Note : I have the same error when I try the same configuration and LRRangeTest as scheduler.
		</comment>
		<comment id='3' author='astariul' date='2020-03-13T10:38:47Z'>
		I'm still meeting this issue in deepspeed/deepspeed:latest. How should I update the docker image to pull latest code from source ?
		</comment>
		<comment id='4' author='astariul' date='2020-03-13T14:42:52Z'>
		Hi @Colanim, it should be up to date. Can you tell us this info from inside your docker container?
python -c 'import deepspeed; print("deepspeed info:", deepspeed.__version__, deepspeed.__git_branch__, deepspeed.__git_hash__)'
Also I just looked at the lasted docker build, it prints this same version info and it looks to be aligned with the latest March 12th commit (&lt;denchmark-link:https://github.com/microsoft/DeepSpeed/commit/3d3f8d36a4e8c0b7e6358bccd90254fc7424ffcb&gt;3d3f8d3&lt;/denchmark-link&gt;
): &lt;denchmark-link:https://dev.azure.com/DeepSpeedMSFT/DeepSpeed/_build/results?buildId=416&amp;view=logs&amp;j=3dc8fd7e-4368-5a92-293e-d53cefc8c4b3&amp;t=a1aa9649-a94b-5ac4-3f5e-9bb6223edb04&amp;l=1717&gt;https://dev.azure.com/DeepSpeedMSFT/DeepSpeed/_build/results?buildId=416&amp;view=logs&amp;j=3dc8fd7e-4368-5a92-293e-d53cefc8c4b3&amp;t=a1aa9649-a94b-5ac4-3f5e-9bb6223edb04&amp;l=1717&lt;/denchmark-link&gt;

** info: 0.1.0 master 3d3f8d3
		</comment>
		<comment id='5' author='astariul' date='2020-03-14T10:07:08Z'>
		My bad, I didn't pull the latest image..
After doing docker pull deepspeed/deepspeed:latest, it's working üëç
		</comment>
	</comments>
</bug>