<bug id='518' author='jithunnair-amd' open_date='2020-11-09T23:20:23Z' closed_time='2020-11-19T16:17:16Z'>
	<summary>NCCL error in test_checkpointing.py distributed tests</summary>
	<description>
Many of the distributed tests in test_checkpointing.py seem to be incorrectly written: they end up using the same device 0 for the various ranks. One can verify this by setting the  environment variables while running the tests. Older versions of NCCL eg. 2.4.8 would allow this, but it causes errors with newer versions of NCCL  eg. 2.7.8. Attaching the run logs for both NCCL versions with above-mentioned environment variables set. DeepSpeed commit used is &lt;denchmark-link:https://github.com/microsoft/DeepSpeed/commit/7d4d742bf03f8e1707130391e0b39bd6d93a702a&gt;7d4d742&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/microsoft/DeepSpeed/files/5513800/test_checkpoint_unfused_optimizer.nccl_2.7.8.log&gt;test_checkpoint_unfused_optimizer.nccl_2.7.8.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/microsoft/DeepSpeed/files/5513801/test_checkpoint_unfused_optimizer.nccl_2.4.8.log&gt;test_checkpoint_unfused_optimizer.nccl_2.4.8.log&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jithunnair-amd' date='2020-11-11T18:47:35Z'>
		Thanks for the report and logs! As you can see, we test on older NCCL versions.
		</comment>
		<comment id='2' author='jithunnair-amd' date='2020-11-11T19:49:11Z'>
		Yes. However, this really seems to be an issue in the way the distributed test logic is written, because it ends up having multiple ranks on the same device (NCCL does not control the association of device to rank; it just uses the current CUDA device).
		</comment>
		<comment id='3' author='jithunnair-amd' date='2020-11-11T20:36:21Z'>
		Yep, understood. I just meant we had not encountered the error before due to older NCCL versions.
I haven't had a chance to dig into specifics yet, but am curious why the ranks are all pinned to the same device. Our distributed_test helper assigns devices: 


DeepSpeed/tests/unit/common.py


         Line 42
      in
      eea1c28






 torch.cuda.set_device(local_rank) 





		</comment>
		<comment id='4' author='jithunnair-amd' date='2020-11-12T05:32:05Z'>
		Right, I noticed the same myself. However, when profiling the CUDA calls, I noticed that cudaSetDevice(0) was being called after the cudaSetDevice(&lt;device&gt;) call triggered from the line above. I wasn't able to narrow down where the subsequent cudaSetDevice calls were being triggered from.
		</comment>
		<comment id='5' author='jithunnair-amd' date='2020-11-19T16:17:16Z'>
		Hi &lt;denchmark-link:https://github.com/jithunnair-amd&gt;@jithunnair-amd&lt;/denchmark-link&gt;
, this issue should now be resolved. Feel free to re-open if you're still seeing an issue though.
		</comment>
	</comments>
</bug>