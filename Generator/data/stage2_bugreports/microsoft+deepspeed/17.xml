<bug id='17' author='samyam' open_date='2020-02-05T19:49:41Z' closed_time='2020-02-07T21:11:08Z'>
	<summary>batch config issue?</summary>
	<description>
There are a few things in configure train batch size that does not seem correct to me, and there are few things that we do not currently support.

The following assertion

train_batch_size == train_micro_batch_size_per_gpu * gradient_accumulation_step * world_size
should always hold but currently it does not in some cases.
For example, when train_micro_batch_size_per_gpu and gradient accumulation steps are None in the ds_cofig its initialized to train_batch_size and 1 respectively which leads to
train_batch_size == train_batch_size * 1 * world_size


if train_micro_batch_size_per_gpu &gt; per_device_batch_size, we should throw a config error. Currently, its assigned to be equal to per_device_batch_size.


We do not currently support the user providing only the train_micro_batch_size or train_micro_batch_size and gradient _accumulation_steps.


	</description>
	<comments>
		<comment id='1' author='samyam' date='2020-02-07T21:11:08Z'>
		Fixed with pull request Samyamr/batchconfig &lt;denchmark-link:https://github.com/microsoft/DeepSpeed/pull/33&gt;#33&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>