<bug id='99' author='ShadenSmith' open_date='2020-02-21T23:58:28Z' closed_time='2020-06-04T22:07:14Z'>
	<summary>DeepSpeed using DistributedSampler with model parallelism</summary>
	<description>
DeepSpeed's data loader will use DistributedSampler by default unless another is provided:



DeepSpeed/deepspeed/pt/deepspeed_dataloader.py


         Line 43
      in
      001abe2






 data_sampler = DistributedSampler(dataset) 





If DeepSpeed is configured with model parallelism, or called from a library with a sub-group of the world processes, the default behavior of DistributedSampler is incorrect because it queries the global world size and rank information. We should specify num_replicas and rank when creating the sampler.
If mpu is provided to deepspeed.initialize(), we should query mpu.get_data_parallel_world_size() and mpu.get_data_parallel_rank() and forward that information to the sampler.
	</description>
	<comments>
	</comments>
</bug>