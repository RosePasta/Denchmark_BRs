<bug id='281' author='vfdev-5' open_date='2020-06-28T13:52:14Z' closed_time='2020-09-10T21:16:06Z'>
	<summary>optimizer_name() is None when used client optimizer</summary>
	<description>
There is a bug encountered when using client optimizer (Adam) and zero optimization:
class Args:
    pass

model = wide_resnet50_2(num_classes=100).to(device)
optimizer = Adam(model.parameters(), lr=0.001)

ds_args = Args()
ds_args.local_rank = 0
ds_args.deepspeed_config = None
ds_config_params = {
        "train_batch_size": batch_size,
        "steps_per_print": len(train_loader),
        "fp16": {
            "enabled": "true",
        },
        "zero_optimization": {
            "stage": 2,
        },
        # "zero_allow_untested_optimizer": "true"    
}
model_engine = DeepSpeedLight(ds_args, model, optimizer=optimizer, config_params=ds_config_params)
In this case self.optimizer_name() is None and the following check is incorrect:



DeepSpeed/deepspeed/pt/deepspeed_light.py


         Line 489
      in
      4d64b5b






 if self.optimizer_name() != ADAM_OPTIMIZER: 





'You are using an untested ZeRO Optimizer. Please add &lt;"zero_allow_untested_optimizer": true&gt; in the configuration file to use it.'
AssertionError: You are using an untested ZeRO Optimizer. Please add &lt;"zero_allow_untested_optimizer": true&gt; in the configuration file to use it.
	</description>
	<comments>
		<comment id='1' author='vfdev-5' date='2020-06-29T14:49:31Z'>
		Thanks for using DeepSpeed.
What happens when you set zero_allow_untested_optimizer to true?
		</comment>
		<comment id='2' author='vfdev-5' date='2020-06-29T14:51:03Z'>
		Yes, setting zero_allow_untested_optimizer to true solves the problem.
		</comment>
		<comment id='3' author='vfdev-5' date='2020-06-29T16:06:12Z'>
		Good catch!
In cases where we don't have an optimizer name from our config, maybe we should next defer to self.optimizer.__class__.__name__ before returning None from optimizer_name() ? In this case I believe that would give us "Adam" and do the correct thing. If no optimizer was specified we should still return None, I imagine.
Here's optimizer_name():



DeepSpeed/deepspeed/pt/deepspeed_light.py


        Lines 264 to 265
      in
      4d64b5b






 def optimizer_name(self): 



 return self._config.optimizer_name 





		</comment>
		<comment id='4' author='vfdev-5' date='2020-09-10T21:16:06Z'>
		Fixed by PR &lt;denchmark-link:https://github.com/microsoft/DeepSpeed/pull/349&gt;#349&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>