<bug id='138' author='LiweiPeng' open_date='2020-03-13T19:39:20Z' closed_time='2020-03-25T16:34:28Z'>
	<summary>zero optimizer static loss scale is not working</summary>
	<description>
the static loss scale in FP16_DeepSpeedZeroOptimizer is not working. One fix is to add at
&lt;denchmark-link:https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/pt/deepspeed_zero_optimizer.py#L188&gt;https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/pt/deepspeed_zero_optimizer.py#L188&lt;/denchmark-link&gt;

self.loss_scaler = LossScaler(static_loss_scale)
Also, change the following line
from deepspeed.pt.loss_scaler import DynamicLossScaler, LossScaler
Can you take a look at this issue? Thanks.
	</description>
	<comments>
		<comment id='1' author='LiweiPeng' date='2020-03-23T01:49:28Z'>
		Thanks for catching this and recommending a fix! I submitted a PR to address this bug. Please let me know if you see any issues.
		</comment>
	</comments>
</bug>