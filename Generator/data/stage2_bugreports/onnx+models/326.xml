<bug id='326' author='joddiy' open_date='2020-06-30T18:10:29Z' closed_time='2020-07-01T13:41:48Z'>
	<summary>The shape of the first output in GPT-2 is wrong</summary>
	<description>
&lt;denchmark-h:h1&gt;Bug Report&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Which model does this pertain to?&lt;/denchmark-h&gt;

GPT-2
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;

The correct shape of the first output should be [1, 1, 8, 50257], the 50257 is the size of the vocab_size.
However, the shape of the first output in the latest onnx gpt-2 model is [1, 1, 8, 768]. The 768 is the dimension of the embedding vector.
&lt;denchmark-link:https://user-images.githubusercontent.com/14108933/86161380-c1624c80-bb3f-11ea-9bf8-3dbb08c91618.png&gt;&lt;/denchmark-link&gt;

It must be wrong.
&lt;denchmark-h:h3&gt;Reproduction instructions&lt;/denchmark-h&gt;

System Information
OS Platform and Distribution (e.g. Linux Ubuntu 16.04):  Ubuntu 16.04
ONNX version (e.g. 1.6):  1.6
Backend/Runtime version (e.g. ONNX Runtime 1.1, PyTorch 1.2):  PyTorch 1.5.1
Provide a code snippet to reproduce your errors.
&lt;denchmark-code&gt;from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch


def f(t):
    return [[f(i) for i in t] if isinstance(t, (list, tuple)) else t]


def g(t, res):
    for i in t:
        res.append(i) if not isinstance(i, (list, tuple)) else g(i, res)
    return res

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

input_ids_1 = torch.tensor(
    [[tokenizer.encode("Here is some text to encode Hello World", add_special_tokens=True)]], device=device)
output_1 = model(input_ids_1) 
outputs_flatten = f(output_1)
outputs_flatten = g(outputs_flatten, [])
# the correct output shape: torch.Size([1, 1, 8, 50257])
print(outputs_flatten[0].shape)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Notes&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='joddiy' date='2020-07-01T13:41:45Z'>
		The reason is the GPT-2 model uses the GPT2Model, however, what we want is GPT2LMHeadModel.
I will try to submit a PR to add the GPT2LMHeadModel.
		</comment>
	</comments>
</bug>