<bug id='80' author='ironflood' open_date='2017-11-12T09:51:05Z' closed_time='2017-11-29T03:59:00Z'>
	<summary>A3C cartpole - inconsistent learning &amp; !random actions</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

When launching the A3C cartpole without code modification:


We could expect the actions in nextStep(Integer action) to be random initially (first few plays / epochs) - at least from one sequence to another during discovery - however the sequence from one play to another is almost the same, exemple of action sequence I get:
#1 0001101010101001110111110110100111111111
#2 00011010101000010101111101101001111111110101101010101101101010101
#3 0001101010101001010111110110100111111111010110001010110


The training I get seems to be inconsistent as shown in the monitor webapp:


&lt;denchmark-link:https://user-images.githubusercontent.com/11771531/32697723-e33df7c4-c7bc-11e7-9ad7-112f20c0d99c.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


dl4j 0.9.1
OSx
results obtained with the example cartpole

	</description>
	<comments>
		<comment id='1' author='ironflood' date='2017-11-15T01:27:31Z'>
		When we provide the same random seed, we get the same sequence of random actions. That's perfectly normal. Did you make sure to change the seed on each run?
		</comment>
		<comment id='2' author='ironflood' date='2017-11-15T02:14:06Z'>
		As to why the results are not that good, we need to do some fine-tuning. These parameters give me pretty good results:
    private static A3CDiscrete.A3CConfiguration CARTPOLE_A3C =
            new A3CDiscrete.A3CConfiguration(
                    123,            //Random seed
                    200,            //Max step By epoch
                    500000,         //Max step
                    8,              //Number of threads
                    20,              //t_max
                    10,             //num step noop warmup
                    0.01,           //reward scaling
                    0.99,           //gamma
                    1.0           //td-error clipping
            );


    private static final ActorCriticFactorySeparateStdDense.Configuration CARTPOLE_NET_A3C =  ActorCriticFactorySeparateStdDense.Configuration
    .builder().learningRate(0.001).l2(0).numHiddenNodes(300).numLayer(2).build();
		</comment>
		<comment id='3' author='ironflood' date='2017-11-17T06:47:25Z'>
		Thanks for having a look. So the seed has nothing to do here (and yes I change it for every training), when I said "between run" I meant between epochs = a play until reset() state. And even with the changed configuration the actions taken by the net are very similar from one epoch to another. It could make sense after many epochs, but it doesn't in the beginning as we can expect A3C to randomly discover patterns, except here for example the first few actions have never been changed, even after 10k+ iterations.
So the first few epoch action sequences taken are:
10100110111110011
1010011011111001101
1010011011111001
... and after 11k iterations:
1010011011101000100010100110111010011...0110111010011010
1010011011101000100010100100111010011...000111010011010
1010011010101001100010100110101010011...110101010011000
Notice how the first 9 actions ('101001101') have not changed a single time during all training? Gut feeling tells me there's something wrong, either in the gym (the initial state is always the same so no need to change the initial pattern) or in the action discovery.
Otherwise with the tuning changes you suggested the NN now learns correctly:
&lt;denchmark-link:https://user-images.githubusercontent.com/11771531/32932092-0e17004c-cb89-11e7-90ab-50def942e94a.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='ironflood' date='2017-11-17T06:51:17Z'>
		No matter how long I train for, the initial sequence of 9 actions have never been modified so the solution is most likely not optimal. Is there a configuration parameter I missed to modify the action randomness and thus foster exploration, just like epsilon in QLearning?
&lt;denchmark-link:https://user-images.githubusercontent.com/11771531/32934352-7a66dc46-cb8d-11e7-89fe-7ab0098f81e0.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ironflood' date='2017-11-17T07:50:02Z'>
		It's possible there is a bug somewhere. If the ACPolicy object gets reset with the same Random object on every episode, that would happen:
&lt;denchmark-link:https://github.com/deeplearning4j/rl4j/blob/master/rl4j-core/src/main/java/org/deeplearning4j/rl4j/policy/ACPolicy.java#L30&gt;https://github.com/deeplearning4j/rl4j/blob/master/rl4j-core/src/main/java/org/deeplearning4j/rl4j/policy/ACPolicy.java#L30&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ironflood' date='2017-11-17T12:13:55Z'>
		It could be the problem indeed, and what would be the best way to reset it? Does ACPolicy.java needs to be modified? Ideally each policy should be initialized with a different seed (each thread having a different one).
		</comment>
		<comment id='7' author='ironflood' date='2017-11-17T13:55:12Z'>
		We could modify ACPolicy directly and see, but ideally we should be able to set the seed, so we should check where ACPolicy gets instantiated and make sure we don't reset the Random object there.
		</comment>
		<comment id='8' author='ironflood' date='2017-11-29T03:59:00Z'>
		Fixed in commit &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/f605a751e0622de6b291a9313faa48862e957b9f&gt;f605a75&lt;/denchmark-link&gt;

Please let me know if you're still having issues with this though! Thanks for reporting
		</comment>
	</comments>
</bug>