<bug id='86' author='Ailanz' open_date='2018-01-23T15:54:55Z' closed_time='2018-01-25T00:25:04Z'>
	<summary>A3C Crashes on training with Comp Graph</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Hi, this is related to the recent fix that A3C networks requires a probability output (Softmax).
when running the A3C with
&lt;denchmark-code&gt;    private static ActorCriticFactoryCompGraphStdDense.Configuration NET_A3C =  ActorCriticFactoryCompGraphStdDense.Configuration
            .builder().l2(0.01).numHiddenNodes(20).useLSTM(false).numLayer(2).build();

&lt;/denchmark-code&gt;

we get:
&lt;denchmark-code&gt;java.lang.RuntimeException: Output from network is not a probability distribution
	at org.deeplearning4j.rl4j.policy.ACPolicy.nextAction(ACPolicy.java:65)
	at org.deeplearning4j.rl4j.policy.ACPolicy.nextAction(ACPolicy.java:20)
	at org.deeplearning4j.rl4j.learning.async.AsyncThreadDiscrete.trainSubEpoch(AsyncThreadDiscrete.java:77)
	at org.deeplearning4j.rl4j.learning.async.AsyncThread.run(AsyncThread.java:82)
&lt;/denchmark-code&gt;

It doesn't happen every run but about 30% of the run, it runs into this error after training for a bit.
This is from the latest Master branch.
full code to reproduce:
&lt;denchmark-code&gt;package rl;

import org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CDiscrete;
import org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CDiscreteDense;
import org.deeplearning4j.rl4j.mdp.toy.SimpleToy;
import org.deeplearning4j.rl4j.mdp.toy.SimpleToyState;
import org.deeplearning4j.rl4j.network.ac.ActorCriticFactoryCompGraphStdDense;
import org.deeplearning4j.rl4j.util.DataManager;

import java.io.IOException;

public class Toy {

    public static void main(String[] args) throws IOException {
        simpleToy();
    }

    public static void simpleToy() throws IOException {
        DataManager manager = new DataManager();
        SimpleToy mdp = new SimpleToy(20);
        int in = mdp.getObservationSpace().getShape()[0];
        A3CDiscreteDense&lt;SimpleToyState&gt; dql = new A3CDiscreteDense&lt;&gt;(mdp, NET_A3C, A3C_CONFIG, manager);
        dql.train();
        mdp.close();
    }

    private static A3CDiscrete.A3CConfiguration A3C_CONFIG =
            new A3CDiscrete.A3CConfiguration(
                    123,            //Random seed
                    800000,            //Max step By epoch
                    50000,         //Max step
                    12,              //Number of threads
                    5000000,              //t_max
                    0,             //num step noop warmup
                    1,           //reward scaling
                    0.99,           //gamma
                    10           //td-error clipping
            );

    private static ActorCriticFactoryCompGraphStdDense.Configuration NET_A3C =  ActorCriticFactoryCompGraphStdDense.Configuration
            .builder().l2(0.01).numHiddenNodes(20).useLSTM(false).numLayer(2).build();
    
}

&lt;/denchmark-code&gt;

Please describe your issue, along with:

expected behavior
encountered behavior

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version
platform information (OS, etc)
CUDA version, if used
NVIDIA driver version, if in use

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

If you'd like to help us fix the issue by contributing some code, but would
like guidance or help in doing so, please mention it!
	</description>
	<comments>
		<comment id='1' author='Ailanz' date='2018-01-24T03:45:53Z'>
		How did you run this example? I'm getting a different problem with 0.9.2-SNAPSHOT:
org.nd4j.linalg.exception.ND4JIllegalStateException: Op name subtract is missing!
	at org.nd4j.linalg.api.ops.DynamicCustomOp.opHash(DynamicCustomOp.java:258)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1533)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:3255)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:3225)
	at org.nd4j.linalg.lossfunctions.impl.LossL2.computeGradient(LossL2.java:119)
	at org.nd4j.linalg.lossfunctions.impl.LossMSE.computeGradient(LossMSE.java:56)
	at org.deeplearning4j.nn.layers.BaseOutputLayer.getGradientsAndDelta(BaseOutputLayer.java:173)
	at org.deeplearning4j.nn.layers.BaseOutputLayer.backpropGradient(BaseOutputLayer.java:148)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.calcBackpropGradients(MultiLayerNetwork.java:1357)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.backprop(MultiLayerNetwork.java:1307)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2311)
	at org.deeplearning4j.rl4j.network.ac.ActorCriticSeparate.gradient(ActorCriticSeparate.java:81)
	at org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CThreadDiscrete.calcGradient(A3CThreadDiscrete.java:106)
	at org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CThreadDiscrete.calcGradient(A3CThreadDiscrete.java:28)
	at org.deeplearning4j.rl4j.learning.async.AsyncThreadDiscrete.trainSubEpoch(AsyncThreadDiscrete.java:123)
	at org.deeplearning4j.rl4j.learning.async.AsyncThread.run(AsyncThread.java:82)
&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 ? &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='2' author='Ailanz' date='2018-01-24T03:50:12Z'>
		And sometimes also this:
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442)
	at java.util.HashMap$EntryIterator.next(HashMap.java:1476)
	at java.util.HashMap$EntryIterator.next(HashMap.java:1474)
	at java.util.HashMap.putMapEntries(HashMap.java:512)
	at java.util.HashMap.&lt;init&gt;(HashMap.java:490)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.getCustomOperations(NativeOpExecutioner.java:1519)
	at org.nd4j.linalg.api.ops.DynamicCustomOp.opHash(DynamicCustomOp.java:255)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1533)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:3255)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.subi(BaseNDArray.java:3225)
	at org.nd4j.linalg.lossfunctions.impl.LossL2.computeGradient(LossL2.java:119)
	at org.nd4j.linalg.lossfunctions.impl.LossMSE.computeGradient(LossMSE.java:56)
	at org.deeplearning4j.nn.layers.BaseOutputLayer.getGradientsAndDelta(BaseOutputLayer.java:173)
	at org.deeplearning4j.nn.layers.BaseOutputLayer.backpropGradient(BaseOutputLayer.java:148)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.calcBackpropGradients(MultiLayerNetwork.java:1357)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.backprop(MultiLayerNetwork.java:1307)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2311)
	at org.deeplearning4j.rl4j.network.ac.ActorCriticSeparate.gradient(ActorCriticSeparate.java:81)
	at org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CThreadDiscrete.calcGradient(A3CThreadDiscrete.java:106)
	at org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CThreadDiscrete.calcGradient(A3CThreadDiscrete.java:28)
	at org.deeplearning4j.rl4j.learning.async.AsyncThreadDiscrete.trainSubEpoch(AsyncThreadDiscrete.java:123)
	at org.deeplearning4j.rl4j.learning.async.AsyncThread.run(AsyncThread.java:82)
So it looks like an issue with the HashMap...
		</comment>
		<comment id='3' author='Ailanz' date='2018-01-24T03:53:36Z'>
		Hi, I ran into both of these issues as well. But they do tend to go away after running it for a couple of runs and / or rebuild rl4j from maven.
		</comment>
		<comment id='4' author='Ailanz' date='2018-01-24T03:54:59Z'>
		Ok, when it does run I get the following output:
&lt;denchmark-code&gt;java.lang.RuntimeException: Output from network is not a probability distribution: [   �,     �]
	at org.deeplearning4j.rl4j.policy.ACPolicy.nextAction(ACPolicy.java:65)
	at org.deeplearning4j.rl4j.policy.ACPolicy.nextAction(ACPolicy.java:20)
	at org.deeplearning4j.rl4j.learning.async.AsyncThreadDiscrete.trainSubEpoch(AsyncThreadDiscrete.java:77)
	at org.deeplearning4j.rl4j.learning.async.AsyncThread.run(AsyncThread.java:82)
&lt;/denchmark-code&gt;

Based on the latest commit: &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/4fbeac8b3b9354133c25d76adf6c65712e4ad505&gt;4fbeac8&lt;/denchmark-link&gt;

So we're just getting NaNs here. You'll need to adjust the network parameters so this doesn't happen: &lt;denchmark-link:https://deeplearning4j.org/troubleshootingneuralnets&gt;https://deeplearning4j.org/troubleshootingneuralnets&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='Ailanz' date='2018-01-24T04:13:33Z'>
		Thanks! Will do!
		</comment>
		<comment id='6' author='Ailanz' date='2018-01-24T04:23:40Z'>
		Given that alot of the paremeters were pre-configured:
&lt;denchmark-code&gt;    private static ActorCriticFactoryCompGraphStdDense.Configuration NET_A3C =  ActorCriticFactoryCompGraphStdDense.Configuration
            .builder().l2(0.01).numHiddenNodes(20).useLSTM(false).numLayer(2).build();
&lt;/denchmark-code&gt;

and
&lt;denchmark-code&gt;    private static A3CDiscrete.A3CConfiguration A3C_CONFIG =
            new A3CDiscrete.A3CConfiguration(
                    123,            //Random seed
                    800000,            //Max step By epoch
                    50000,         //Max step
                    12,              //Number of threads
                    5000000,              //t_max
                    0,             //num step noop warmup
                    1,           //reward scaling
                    0.99,           //gamma
                    10           //td-error clipping
            );
&lt;/denchmark-code&gt;

do you have any tips to prevent the Nans? On that page you provided, I tried to increase reward factor to 10, 100, and 1000 (hopefully to prevent underflow) but it still throws the same error of Nans,
Any advice?
Thanks!
		</comment>
		<comment id='7' author='Ailanz' date='2018-01-24T04:40:30Z'>
		
The learning rate is one of, if not the most important hyperparameter.

Please start reading...
		</comment>
		<comment id='8' author='Ailanz' date='2018-01-24T04:54:44Z'>
		Silly question, since I've updated to 0.9.2-SNAPSHOT, I noticed that the setter for learning rate is no longer accessible via:
&lt;denchmark-code&gt;  private static ActorCriticFactoryCompGraphStdDense.Configuration NET_A3C =  ActorCriticFactoryCompGraphStdDense.Configuration
            .builder().numHiddenNodes(120).useLSTM(false).numLayer(2).build();
&lt;/denchmark-code&gt;

what is an alternative way to set the learning rate on my network.
Thanks!
		</comment>
		<comment id='9' author='Ailanz' date='2018-01-24T06:02:49Z'>
		It is part of the Updater now. Check the updated examples:
&lt;denchmark-link:https://github.com/deeplearning4j/dl4j-examples/tree/sa_snapshot/rl4j-examples/src/main/java/org/deeplearning4j/examples/rl4j&gt;https://github.com/deeplearning4j/dl4j-examples/tree/sa_snapshot/rl4j-examples/src/main/java/org/deeplearning4j/examples/rl4j&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>