<bug id='56' author='ghost(ghost)' open_date='2017-08-14T19:45:40Z' closed_time='2017-09-06T12:51:18Z'>
	<summary>Somethings not right (or has changed) with history processor / pixels</summary>
	<description>
A few months back, I had a working, pixel-based, rl4j sample app that converged fairly quickly. After merging in many changes to rl4j over the past few months, it no longer works using the most recent code from' master'.
From what little I've been able to determine, I  the issue  be related to &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/d4231669f6be7562de41cc29cc9b414c2183a8f7&gt;d423166&lt;/denchmark-link&gt;
. Part of that commit was to change the fmpeg frame recording in the history processor to expect normalized pixels: 
That was not how it worked before, and it seems to conflict with the 'transform' function in the history processor where it does not expect normalized pixels.
Not sure where this leaves me:

Have other places in the code been changed to expect normalized pixels and 'transform' is out of date? In which case, how is vizdoom functioning and what does UINT8 compression of that matrix do if all the values are 0.0-1.0?

-or-

Are the pixel not suppose to be normalized and just that one line in 'record' is a mistake? If so, how does vizdoom function? and then what else could have changed to cause my sample to stop working?

	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2017-08-15T12:20:28Z'>
		Normalization is expected to happen in the MDP yes, like it was updated in VizDoom for example:
&lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/d4231669f6be7562de41cc29cc9b414c2183a8f7#diff-51a80d538994d44779d01c52ceaad156&gt;d423166#diff-51a80d538994d44779d01c52ceaad156&lt;/denchmark-link&gt;

Previously, it was hard coded to assume that anything would be [0, 256] and it would do it in concat() only:
&lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/63adcf02980cf8baea395adf1bb333ec0ed57c21#diff-0e254479821c42bc291337248dd9b58d&gt;63adcf0#diff-0e254479821c42bc291337248dd9b58d&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='ghost(ghost)' date='2017-08-15T12:21:56Z'>
		Could you point me to any other places where it expects to transform unnormalized data?
		</comment>
		<comment id='3' author='ghost(ghost)' date='2017-08-15T15:29:53Z'>
		Thanks for the reply.  I wasn't aware of that 1/256 in concat, and it makes sense that it was removed.
But something's still not right on my end.  The part where I was saying that transform is expecting non-normalized data is in HistoryProcessor.transform(INDArray raw):
&lt;denchmark-code&gt;Mat ocvmat = new Mat(shape[0], shape[1], CV_32FC(3), raw.data().pointer());
Mat cvmat = new Mat(shape[0], shape[1], CV_8UC(3));
ocvmat.convertTo(cvmat, CV_8UC(3));
&lt;/denchmark-code&gt;

This was working prior to normalization - if I do an imwrite of cvmat at this point with normalized data, I get a black image. If I don't normalize the observed pixels, it get the full color image I'd expect.  From what I've read online, opencv's CV_32F is expecting 0.0-1.0, so I would have expected the opposite results but that appears not to be the case because "record" is now denormalizing the data before writing it to the mpeg.
Why shouldn't transform also be denormalizing the data when converting to CV_8UC like in record?
		</comment>
		<comment id='4' author='ghost(ghost)' date='2017-08-16T12:45:08Z'>
		Ah, I see, thanks for reporting this! I forgot to update this method. We really need to start making better unit tests for all that :) Fixed this in the latest commit &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/5de6083f3f3135dcac1160ed18ccbe7a1faca012&gt;5de6083&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ghost(ghost)' date='2017-08-16T16:57:58Z'>
		Thanks, but I'm not sure that's correct.
You're renormalizing the data and then the code attempts to store it un UINT8 when it compresses it. The result is that all the pixels are either 0.0 or 1.0 when the history is used for training. At least that's what I'm seeing - unit tests are exactly what is needed here :)
I can play some more with some options, but interested in your thoughts since you know the code better.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2017-08-16T20:17:08Z'>
		Just a quick update:
I ran the updated code and things didn't seem to work well.  I then updated the HistoryProcessor code you checked in, but used Float8 compression instead of UINT8 and it worked. I noticed the docs for Float8 said it wasn't implemented,  but it worked - perhaps it's only implemented in native and not CUDA, etc? Again, I'm a bit out of my element in this codebase, so I'm not sure what ramifications changing that compression type has.
		</comment>
		<comment id='7' author='ghost(ghost)' date='2017-08-18T12:39:01Z'>
		Ah, I see, I didn't notice we were compressing it to integers, oops. &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 does Float8 work??
		</comment>
		<comment id='8' author='ghost(ghost)' date='2017-08-18T14:58:59Z'>
		I should also mention that there seems to be some precision loss using Float8, so even if it's working it might not be the best solution.
		</comment>
		<comment id='9' author='ghost(ghost)' date='2017-08-18T15:09:11Z'>
		That was for memory constraints. If your problem is small enough, we don't
need it. We could have a flag? Although for images byte from 0 to 255 is
enough without precision loss...
		</comment>
		<comment id='10' author='ghost(ghost)' date='2017-08-28T19:20:47Z'>
		If Float8 DOES work, then I propose we change it to Float8 and add a flag to turn off compression for when people need more that ~7 bits of precision.
		</comment>
		<comment id='11' author='ghost(ghost)' date='2017-08-31T14:24:35Z'>
		The problem with Float8 is that even if it works, it doesn't represent well numbers in the [0.0, 1.0] range.
&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 Do you think it would be possible to compress with Uint8 in fixed-point? Like with a scale or a shift for cases like this?
		</comment>
		<comment id='12' author='ghost(ghost)' date='2017-09-06T12:51:18Z'>
		I've worked around this issue in the latest commit &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/f515c6bc00dacd442ef9feb523b91b09c8ea3077&gt;f515c6b&lt;/denchmark-link&gt;
. It doesn't make sense to have compression as an option because we're not losing any accuracy in the case of images from video games. Though it's always possible to implement a new HistoryProcessor that uses different compression at a different scale, as required by the data we might be processing. In any case, let me know if this works well for your application. Thanks for the feedback!
		</comment>
		<comment id='13' author='ghost(ghost)' date='2017-09-07T17:39:41Z'>
		I think this approach makes sense, but it looks like there's a bug in commit &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/f515c6bc00dacd442ef9feb523b91b09c8ea3077&gt;f515c6b&lt;/denchmark-link&gt;
.
If you look at line 212 in &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/f515c6bc00dacd442ef9feb523b91b09c8ea3077#diff-228375870af85c3a7363e27ee3c76174&gt;QLearningDiscrete&lt;/denchmark-link&gt;
, the scaling is done within the loop over the memories. That means each memory is scaled multiple times resulting in a whole bunch of zeros (except for the last memory).  I made a local change to move that block of code after the next brace and that seemed to fix things for my sample.
There could be more problems hiding in there, but I didn't see anything more.
		</comment>
		<comment id='14' author='ghost(ghost)' date='2017-09-07T20:53:27Z'>
		Ah, yes, thanks for pointing this out. Fixed!
BTW, what is this sample that you are running? Could you send a pull request? I'm looking for a good way to test this thing, but am not having much luck tuning a game from ALE...
		</comment>
		<comment id='15' author='ghost(ghost)' date='2017-09-08T03:04:00Z'>
		Looks good.
I've been using my minecraft sample: &lt;denchmark-link:https://github.com/howard-abrams/dl4j-examples/blob/master/rl4j-examples/src/main/java/org/deeplearning4j/examples/rl4j/MalmoPixels.java&gt;https://github.com/howard-abrams/dl4j-examples/blob/master/rl4j-examples/src/main/java/org/deeplearning4j/examples/rl4j/MalmoPixels.java&lt;/denchmark-link&gt;

I haven't summited a PR for it yet because I'm waiting for &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/pull/21&gt;#21&lt;/denchmark-link&gt;
 to land first since it has dependencies on that.
		</comment>
		<comment id='16' author='ghost(ghost)' date='2017-09-08T06:05:11Z'>
		There, it's merged!
		</comment>
		<comment id='17' author='ghost(ghost)' date='2017-09-08T15:23:47Z'>
		Thanks - I'll put together a PR for the samples next.
		</comment>
	</comments>
</bug>