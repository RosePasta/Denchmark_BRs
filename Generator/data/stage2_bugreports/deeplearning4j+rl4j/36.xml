<bug id='36' author='ghost(ghost)' open_date='2017-05-16T16:02:11Z' closed_time='2017-07-07T13:22:53Z'>
	<summary>Async learning w/ history processor is broken</summary>
	<description>
When attempting to use A3CDiscreteConv or AsyncNStepQLearningDiscreteConv with a history processor, the history processor is ignored and therefore the input data won't match the shape of the network input and you'll get an error like this:
org.deeplearning4j.exception.DL4JInvalidInputException: Cannot do forward pass in Convolution layer (layer name = 0, layer index = 1): input array depth does not match CNN layer configuration (data input depth = 240, [minibatch,inputDepth,height,width]=[1, 240, 320, 3]; expected input depth = 1) 
The cause is rooted in a few places, the most glaring is that the history processor is never set on the "AsyncThreadDiscrete" class in both A3CDiscreteConv and AsyncNStepQLearningDiscreteConv:
&lt;denchmark-code&gt;    public AsyncThread newThread(int i) {
        AsyncThread at = super.newThread(i);
        at.setHistoryProcessor(hpconf);
        return super.newThread(i);
    }
&lt;/denchmark-code&gt;

(notice "at" is not returned, but instead another new thread :( )
But even after fixing that, the code in AsyncThreadDiscrete doesn't properly use the history processor.  I've attempted to fix it myself, and I can get the code to not crash, but the learning didn't seem to be converging (although that could be because I was trying A3C with only 2 threads).
I'm happy to submit a PR for this, but would probably need some pointers as to the intentions of the code AsyncThreadDiscrete.trainSubEpoch.
	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2017-05-18T05:58:43Z'>
		Is this happening only on master or with 0.8.0 as well?
		</comment>
		<comment id='2' author='ghost(ghost)' date='2017-05-18T20:42:26Z'>
		Unfortunately, my only test case is on a fork of master, but I haven't made changes to these files and as far as I can tell looking git, the thread creation code has always been this way and the AsyncThreadDiscrete class has never really used the history processor.
		</comment>
		<comment id='3' author='ghost(ghost)' date='2017-05-18T23:19:15Z'>
		I think there's 2 issues here, so let's try to separate them. Please send a pull request to have the history processor added. But then, you say it still doesn't work. Might be the same cause as &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/issues/37&gt;#37&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='4' author='ghost(ghost)' date='2017-05-19T00:46:02Z'>
		(as you already saw,) I created PR &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/pull/38&gt;#38&lt;/denchmark-link&gt;

With just that change and no others I get the same exception:
org.deeplearning4j.exception.DL4JInvalidInputException: Cannot do forward pass in Convolution layer (layer name = 0, layer index = 1): input array depth does not match CNN layer configuration (data input depth = 240, [minibatch,inputDepth,height,width]=[1, 240, 320, 3]; expected input depth = 1)
This seems different than issue &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/issues/37&gt;#37&lt;/denchmark-link&gt;
 which seems to indicate it is running vs with the conv+history it just crashes.
		</comment>
		<comment id='5' author='ghost(ghost)' date='2017-05-19T01:20:33Z'>
		Right, that's a third issue :)
		</comment>
		<comment id='6' author='ghost(ghost)' date='2017-05-19T01:44:43Z'>
		@howard-abrams we'll be doing some work on rl4j over the next month or so. We need to do some updates to get workspaces integrated among other things.
		</comment>
		<comment id='7' author='ghost(ghost)' date='2017-07-07T13:22:53Z'>
		I've fixed the usage of the history processor in commit &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/09d6e55ca61f792d7bc2c970970a31413b6b4379&gt;09d6e55&lt;/denchmark-link&gt;
 .
I don't know yet if A3C is working properly or not, so please continue testing this, if possible, and report about remaining issues. Thanks!
		</comment>
		<comment id='8' author='ghost(ghost)' date='2017-07-19T02:55:07Z'>
		@howard-abrams I've finally figured out what was wrong with A3C, it's now actually working: &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/be5c7125932afee1bcf068ec70ed29ee60ecc502&gt;be5c712&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='ghost(ghost)' date='2017-07-19T03:11:21Z'>
		Cool.  I attempted to merge &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/09d6e55ca61f792d7bc2c970970a31413b6b4379&gt;09d6e55&lt;/denchmark-link&gt;
 into my fork a week back but non-A3C examples (that use the history processor) would crash.  When I get a few minutes, I'll merge in &lt;denchmark-link:https://github.com/deeplearning4j/rl4j/commit/be5c7125932afee1bcf068ec70ed29ee60ecc502&gt;be5c712&lt;/denchmark-link&gt;
 and see if I still have the same problem. Once I get those working again, I can attempt the A3C test.
		</comment>
		<comment id='10' author='ghost(ghost)' date='2017-07-19T03:22:59Z'>
		Thanks for testing! But please merge the whole master branch, not just the commits!
		</comment>
		<comment id='11' author='ghost(ghost)' date='2017-07-21T22:04:31Z'>
		Yes, I've merge the entire master branch just now, but I get an exception with my sample that worked prior to the merge I did a week or two ago:
&lt;denchmark-code&gt;org.nd4j.linalg.exception.ND4JIllegalStateException: op.X dataType is [COMPRESSED] instead of expected [FLOAT]
	at org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner.validateDataType(DefaultOpExecutioner.java:489)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:622)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:100)
	at org.nd4j.linalg.convolution.Convolution.im2col(Convolution.java:131)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:339)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:248)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:392)
	at org.deeplearning4j.nn.layers.AbstractLayer.activate(AbstractLayer.java:317)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:707)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:847)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:788)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:779)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.silentOutput(MultiLayerNetwork.java:1818)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.output(MultiLayerNetwork.java:1810)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.output(MultiLayerNetwork.java:1783)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.output(MultiLayerNetwork.java:1864)
	at org.deeplearning4j.rl4j.network.dqn.DQN.output(DQN.java:38)
	at org.deeplearning4j.rl4j.learning.sync.qlearning.discrete.QLearningDiscrete.trainStep(QLearningDiscrete.java:134)
	at org.deeplearning4j.rl4j.learning.sync.qlearning.QLearning.trainEpoch(QLearning.java:92)
	at org.deeplearning4j.rl4j.learning.sync.SyncLearning.train(SyncLearning.java:42)
	at org.deeplearning4j.examples.rl4j.MalmoPixels.malmoCliffWalk(MalmoPixels.java:126)
	at org.deeplearning4j.examples.rl4j.MalmoPixels.main(MalmoPixels.java:69)

Thoughts on what went wrong?
&lt;/denchmark-code&gt;

		</comment>
		<comment id='12' author='ghost(ghost)' date='2017-07-22T00:37:36Z'>
		Do you get the same with ALE.java? It should be basically the same thing
and it runs here.
		</comment>
		<comment id='13' author='ghost(ghost)' date='2017-07-22T04:25:37Z'>
		After much effort with .so paths, I was able to get the ALE sample working (Not sure why, but I ended up needing to fake a symbolic link on my mac to /Users/saudet/projects/bytedeco/javacpp-presets/ale/cppbuild/macosx-x86_64/Arcade-Learning-Environment-5c7dfa5908a2bf8b1de354d0d9d44c9c3965abbb/libale.so - yikes)
Anyway, the ALE sample didn't complain like my Malmo sample does.  I'll have to debug it further - from my initial look, it seems that the "hstack" in QLearningDiscrete is not marked as compressed=true even though it's data buffer is compressed; but I'm not yet sure why that isn't the case in the ALE sample.
Not sure how much time I have to look into this, so any hints or help is welcome.
		</comment>
		<comment id='14' author='ghost(ghost)' date='2017-07-22T04:38:00Z'>
		@howard-abrams if you're dealing with compression, you actually need to set a special flag to not auto decompress data buffers in debug mode. The toString() among other things will automatically decompress the buffer upon access. Set -Dcompressiondebug=true if you are doing debug mode.
		</comment>
		<comment id='15' author='ghost(ghost)' date='2017-07-22T11:01:59Z'>
		Hum, must be SIP doing its thing. We'll need to figure out how to build ALE
to make it work with that enabled.

AFAIK, I didn't change anything with DQN's HistoryProcessor. That issue was
probably always there...
		</comment>
		<comment id='16' author='ghost(ghost)' date='2017-07-22T18:01:54Z'>
		Thanks &lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 - I wasn't aware of that flag, but it turns out the auto decompression wasn't effecting what I was seeing.
I've managed to get a bit further in my debugging and can now reproduce the problem by modifying the ALE sample: &lt;denchmark-link:https://github.com/saudet&gt;@saudet&lt;/denchmark-link&gt;
, if you change the history processor configuration in the ALE sample to use a history length of 1 (instead of 4), you'll see the issue.
From what I can tell, when you have more than one item in the history, all the frames are getting decompressed as part of the hstack concatenation.  When you only have 1 frame, the concat just returns the single frame directly (which is still compressed). The hstack is then reshaped, and during the reshape the code takes a shortcut where it creates a new NDArray (using Shape.newShapeNoCopy) that has the new shape, no compress flag, but directly uses the compressed buffer.
Hopefully that will be enough for you to reproduce and track down what changed.  This could be fixed by decompressing the data in the shortcut return in the concat, but since I don't think that code has changed it's possible this could have been a change in nd4j and I wouldn't want to workaround a issue that may pop up somewhere else at some point.
Let me know if I can provide more information or test out a fix.
		</comment>
		<comment id='17' author='ghost(ghost)' date='2017-07-22T22:48:26Z'>
		Yes, sounds like a bug in ND4J. Please open an issue about that over there,
explaining how to reproduce with that concat operation without RL4J. Thanks!
		</comment>
		<comment id='18' author='ghost(ghost)' date='2017-07-23T00:27:02Z'>
		Done - thanks much!
		</comment>
	</comments>
</bug>