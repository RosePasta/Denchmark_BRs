<bug id='26' author='jessetrana' open_date='2019-05-18T02:11:04Z' closed_time='2020-07-20T03:21:29Z'>
	<summary>Typo in dropout line?</summary>
	<description>



nsfw_model/training/mobilenetv2_transfer/train_initialization.py


         Line 46
      in
      80a60e3






 x - Dropout(0.4)(x) 





I think there is accidentally a "-" instead of a "=", which could be affecting the intended regularization.
	</description>
	<comments>
		<comment id='1' author='jessetrana' date='2019-05-20T03:28:17Z'>
		Good catch, would you like to submit a PR?
		</comment>
		<comment id='2' author='jessetrana' date='2019-05-20T05:03:28Z'>
		What was the original intent? I'm assuming the "-" is effectively evaluating rather than assigning and causing the line of dropout to basically disappear. "Reenabling" this line seems like it might cause more regularization than you intended.
If regularization is intended in both locations, it seems like a SpatialDropout2D might be a better fit while perhaps reducing the rate in both places. My limited experience has led me to something like this for use in my own finetuning:
    x = model_mn.output #basically MobileNetV2 without the top
    x = SpatialDropout2D(0.1)(x)
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, name='fc_final', activation='relu')(x)
    x = Dropout(0.25, name='dropout')(x)
    x = Dense(NUM_CLASSES, name='classifier', activation='softmax')(x)
I see slightly better accuracy with this approach than my prior approach of no spatial dropout and setting just the one dropout to 0.45. But of course, everybody's use case is a little different and in particular with your dataset being as large as it is, I'm not sure what the best route is. (My current dataset is about 100K with four classes but some oversampling due to class imbalance, as a reference point.) What variant do you think will work best for your situation?
(Also - I'm quite hesitant to submit a PR without running it myself, and do not currently have a great system to test this large of a dataset on.)
		</comment>
		<comment id='3' author='jessetrana' date='2019-06-06T13:50:45Z'>
		You're right.  I tried training with the dropout returning, and it destroyed training performance.
		</comment>
		<comment id='4' author='jessetrana' date='2019-06-07T01:13:57Z'>
		&lt;denchmark-link:https://github.com/GantMan/nsfw_model/pull/29&gt;#29&lt;/denchmark-link&gt;
 Here's a PR for nixing the bad dropout line.
		</comment>
		<comment id='5' author='jessetrana' date='2020-07-20T03:21:29Z'>
		This got merged a long time ago, closing!
		</comment>
	</comments>
</bug>