<bug id='153' author='tambetm' open_date='2015-12-07T15:43:28Z' closed_time='2016-07-01T21:07:46Z'>
	<summary>Prediction drops to 0 after certain number of epochs</summary>
	<description>
I'm using Neon for my deep Q-learning code - &lt;denchmark-link:https://github.com/tambetm/simple_dqn&gt;https://github.com/tambetm/simple_dqn&lt;/denchmark-link&gt;
. Recently I noticed an issue, that prediction of my network drops to 0 after certain number of epochs. This can be seen from Q-value graph:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3816322/11630840/f617e08a-9d07-11e5-83e0-ed652ec45442.png&gt;&lt;/denchmark-link&gt;

Normally it would look like this:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3816322/11630844/fcfde9c6-9d07-11e5-89bd-29a43f38ba76.png&gt;&lt;/denchmark-link&gt;

This plot was produced using Neon commit hash &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/7a56fa9645a51e97c05f2e5afbbd1df7057ae832&gt;7a56fa9&lt;/denchmark-link&gt;
 from October 30th. My code is exactly the same.
The most plausible explanation would be, that weights are truncated to 0 at some point. Because my code hasn't changed, I suspect something in Neon code related to saving and loading weights repeatedly. In my code I need to clone a model, and simplest (and most compatible) way of doing that is to just save and load the model. I do this ~45 times before network prediction drops (it doesn't drop always at the same moment).
Any ideas what change could have resulted in such a behavior and how to debug it?
	</description>
	<comments>
		<comment id='1' author='tambetm' date='2015-12-08T18:27:10Z'>
		Do you have access to the serialized model files for epoch 6 and 7?  If so, we can directly check to see if the weights are going to zero.  The weights are saved via pickle so I don't know how that would truncate on serialization.  Maybe there's an issue with another part of the model initialization from a serialized file.
Is &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/7a56fa9645a51e97c05f2e5afbbd1df7057ae832&gt;7a56fa9&lt;/denchmark-link&gt;
 a working commit (meaning linked to the lower plot)?  Which commit is the other plot run on?
Are you serializing and deserializing every epoch?  Can we maybe add a logging line to print the average Q-value for each iteration (mini-batch) between the epochs so see if this is sudden of slowly developing over the course of an epoch?
Thanks
		</comment>
		<comment id='2' author='tambetm' date='2015-12-10T20:17:11Z'>
		Thanks &lt;denchmark-link:https://github.com/nervetumer&gt;@nervetumer&lt;/denchmark-link&gt;
 for suggestions! I checked the serialized models and weights are not zeros. But all the serialized models after epoch 7 are exactly the same (at least means of weights of all layers).
Current hypothesis is, that it might be something related to RMSProp optimizer and state handling. While weight means seem ok, mean state values for all layers are 1.26117e-44 starting from epoch 7. Possibly related to commit &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/85a478edb63552edc396cce17ab80ae8c730ab2d&gt;85a478e&lt;/denchmark-link&gt;
, because that's the only commit that touched optimizers in between working and not working version. I also tried Adam, and while it doesn't have the exact same problem of predicting zero, it still fails to learn the way it used to.
Yes, &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/7a56fa9645a51e97c05f2e5afbbd1df7057ae832&gt;7a56fa9&lt;/denchmark-link&gt;
 is the working commit. Upper plot is done with latest GitHub HEAD, probably &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/e479ce351ce7149e566cb4be1d8101c1c3e4d179&gt;e479ce3&lt;/denchmark-link&gt;
. I'm serializing-deserializing several times during epoch.
Debugging it takes time, I will keep you informed.
		</comment>
		<comment id='3' author='tambetm' date='2015-12-10T20:59:11Z'>
		Have you tried running neon right before  &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/85a478edb63552edc396cce17ab80ae8c730ab2d&gt;85a478e&lt;/denchmark-link&gt;
 and after to isolate if it is the optimizer?
Are you using gradient_clip_value or gradient_clip_norm in your model? Before that commit &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/85a478edb63552edc396cce17ab80ae8c730ab2d&gt;85a478e&lt;/denchmark-link&gt;
, it was by clip_gradients set to True, and gradient_limit set to some value.
If the states all of sudden become almost 0 from epoch 7, are you able to see the "grad" value or the "state" from epoch 6? As states are updated by:
state[:] = decay * state + self.be.square(grad) * (1.0 - decay)
Then we can trace further down where causes this issue.
		</comment>
		<comment id='4' author='tambetm' date='2015-12-11T09:22:07Z'>
		I played with serialization test in test_model.py and I couldn't replicate the exact error, but found another weird result:
&lt;denchmark-code&gt;print mlp.eval(train_set, Misclassification())

# Serialize model
save_obj(mlp.serialize(keep_states=True), tmp_save)

# Load model
mlp = Model(layers=layers)
mlp.load_weights(tmp_save)

print mlp.eval(train_set, Misclassification()) 
&lt;/denchmark-code&gt;

This outputs:
&lt;denchmark-code&gt;[ 0.74373335]
[ 0.88763332]
&lt;/denchmark-code&gt;

Shouldn't the misclassification rates be exactly the same, if evaluated on the same dataset and with the same model? The difference is even more evident, when you increase n_test variable and let it train longer - after loading the model the result is always the same - 0.8876. Amusingly outputs and weights match...
Full code is here (just run python test_model.py):
&lt;denchmark-link:https://gist.github.com/tambetm/65542362cb24256350d8&gt;https://gist.github.com/tambetm/65542362cb24256350d8&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='tambetm' date='2015-12-13T20:29:59Z'>
		I'm looking into the serialization issue.
		</comment>
		<comment id='6' author='tambetm' date='2015-12-15T03:07:14Z'>
		Thanks for noting this issue with the test.  It is not a serialization issue but actually has to to with a buffer mismatch that happens when the the second instance of the Model object is generated.  Since the layers passed to Model (i.e. Model(layers=layers)) have already been instantiated, they do not get new buffers but a new shared buffer is generated for the MergeMultistream layer container.  So when the model is run the second time the layers in MergeMultistream output to the buffer used the first time around but the MergeMultistream  layer container is expecting those outputs in a new location.  This causes the outputs from the MergeMultistream layer on the second run to remain 0.
That said, we will try to push a fix for this.  In the meantime, I'm wondering if you can try it out to see if this is the root of your problem as well - if your model touch on the affected code then maybe this is your issue as well.
The code to try is here:
&lt;denchmark-link:https://gist.github.com/nervetumer/7a220708967b88cbdcf4&gt;https://gist.github.com/nervetumer/7a220708967b88cbdcf4&lt;/denchmark-link&gt;

So this should only affect container layers that inherit from the MergeBroadcast class.
		</comment>
		<comment id='7' author='tambetm' date='2015-12-15T10:33:18Z'>
		Did you mean to post only model.py? There is only one minor change there and it didn't really affect the result - the second evaluation still produces 0.8876.
&lt;denchmark-code&gt;[ 0.73153335]
[ 0.88763332]
&lt;/denchmark-code&gt;

I'm not using MergeMultistream in my code, so I guess the fix doesn't apply directly. But once we get this fixed, hopefully I can modify the test code to reproduce my error as well (so far no luck).
I also noticed, that version 1.1.3 (&lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/e479ce351ce7149e566cb4be1d8101c1c3e4d179&gt;e479ce3&lt;/denchmark-link&gt;
) tends to have worse performance than version 1.1.0 (&lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/7a56fa9645a51e97c05f2e5afbbd1df7057ae832&gt;7a56fa9&lt;/denchmark-link&gt;
) - average score in Breakout is 305 vs 319. I'm using the exact same model snapshot (breakout_77.pkl), not doing training at all, only prediction.
		</comment>
		<comment id='8' author='tambetm' date='2015-12-15T15:56:13Z'>
		Sorry that was the wrong file.  The one i meant to give you layers/containter.py :
&lt;denchmark-link:https://gist.github.com/nervetumer/7c538802bad3f7a60576&gt;https://gist.github.com/nervetumer/7c538802bad3f7a60576&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='tambetm' date='2015-12-16T20:44:41Z'>
		Give me a few days for this. All my Maxwell GPU-s are busy at the moment :).
		</comment>
		<comment id='10' author='tambetm' date='2015-12-18T21:12:18Z'>
		As expected, this container fix didn't fix my problem:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/3816322/11907451/ad5c73f0-a5dc-11e5-8e8b-01ba9d650fe1.png&gt;&lt;/denchmark-link&gt;

Back to testing...
		</comment>
		<comment id='11' author='tambetm' date='2016-01-04T09:55:09Z'>
		I can assure, that the problem is not related to saving-loading the model. When I turned off saving-loading, then &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/7a56fa9645a51e97c05f2e5afbbd1df7057ae832&gt;7a56fa9&lt;/denchmark-link&gt;
 still learned, but &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/367651890e646c6861dbdc93c1bf4788c51a363d&gt;3676518&lt;/denchmark-link&gt;
 didn't. Currently testing if &lt;denchmark-link:https://github.com/NervanaSystems/neon/issues/171#issuecomment-168426500&gt;#171 (comment)&lt;/denchmark-link&gt;
 fixes the problem.
		</comment>
		<comment id='12' author='tambetm' date='2016-01-14T20:54:07Z'>
		Hi &lt;denchmark-link:https://github.com/tambetm&gt;@tambetm&lt;/denchmark-link&gt;
,  Have you had a chance to test &lt;denchmark-link:https://github.com/NervanaSystems/neon/issues/171&gt;#171&lt;/denchmark-link&gt;
 with this?  I'm hoping to get some time to work on this soon.
		</comment>
		<comment id='13' author='tambetm' date='2016-01-15T07:02:13Z'>
		Nope. Tested with 64 threads as outlined here: &lt;denchmark-link:https://github.com/NervanaSystems/neon/issues/171#issuecomment-168426500&gt;#171 (comment)&lt;/denchmark-link&gt;
. Currently I'm out of ideas.
		</comment>
		<comment id='14' author='tambetm' date='2016-02-16T20:51:05Z'>
		Hi &lt;denchmark-link:https://github.com/tambetm&gt;@tambetm&lt;/denchmark-link&gt;
  can you speak to &lt;denchmark-link:https://github.com/yinyinl&gt;@yinyinl&lt;/denchmark-link&gt;
 comment above with respect to isolating the commit that caused this.  Is it for sure that &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/85a478edb63552edc396cce17ab80ae8c730ab2d&gt;85a478e&lt;/denchmark-link&gt;
 caused the break?
		</comment>
		<comment id='15' author='tambetm' date='2016-02-22T06:03:12Z'>
		I think i tracked down the commit that is causing your problem.  I think it is &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/adab385dace2ef75f58852157b55788cb56b8647&gt;adab385&lt;/denchmark-link&gt;
.  That said I have no idea why this would affect you at all.  Will need to look at it closely.
		</comment>
		<comment id='16' author='tambetm' date='2016-02-22T07:08:25Z'>
		Thanks &lt;denchmark-link:https://github.com/nervetumer&gt;@nervetumer&lt;/denchmark-link&gt;
 for tracking that down! I've been really busy last week
and this week doesn't look any better... Will test it ASAP.
On Mon, Feb 22, 2016 at 8:03 AM, nervetumer &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

I think i tracked down the commit that is causing your problem. I think it
is adab385
adab385.
That said I have no idea why this would affect you at all. Will need to
look at it closely.
â€”
Reply to this email directly or view it on GitHub
#153 (comment)
.

		</comment>
		<comment id='17' author='tambetm' date='2016-02-22T18:08:02Z'>
		If you can confirm that that is the commit causing the issue, that would help.  I'm looking at the commit and working with people here to see why it may be causing you an issue.
		</comment>
		<comment id='18' author='tambetm' date='2016-02-22T21:42:31Z'>
		&lt;denchmark-link:https://github.com/tambetm&gt;@tambetm&lt;/denchmark-link&gt;
.  I think I have verification that this is the commit that led to your model breaking. I believe the problem may be coming from line 143-145 of neon/layers/layer.py. I'm testing it now and will report as soon as enough epochs run to have some confidence.   Still not sure why this would cause a problem.
		</comment>
		<comment id='19' author='tambetm' date='2016-02-23T14:27:53Z'>
		Below is the Q plot for the commit SHA &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/adab385dace2ef75f58852157b55788cb56b8647&gt;adab385&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/12916726/13254425/1dac0c72-d9f6-11e5-9d0c-a969abd9167f.png&gt;&lt;/denchmark-link&gt;

If I comment out lines 143 and 144 in neon/layers/container.py
&lt;denchmark-code&gt;       if self.next_layer is None or self.next_layer.parallelism != self.paral
            self.owns_delta = True
&lt;/denchmark-code&gt;

I get a good run:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/12916726/13254500/615f06a4-d9f6-11e5-9dd2-b419b0135f12.png&gt;&lt;/denchmark-link&gt;

What is happening is that this code is switching the self.owns_delta from False to True for the Linear layers in your model.   Still not exactly sure why this is causing a strange, random error like this.  We will try to put a fit for this asap.  If you want comment those lines out for now.
		</comment>
		<comment id='20' author='tambetm' date='2016-02-24T04:55:47Z'>
		I guess you meant neon/layers/layer.py, no?
I'm trying to reproduce the error here with &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/adab385dace2ef75f58852157b55788cb56b8647&gt;adab385&lt;/denchmark-link&gt;
, but no luck yet (but likely my fault). Starting again from scratch; will leave it running overnight and report back tomorrow.
		</comment>
		<comment id='21' author='tambetm' date='2016-02-24T12:14:07Z'>
		I can also confirm, that there is no error with &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/c8003191bb309b6df9208631dbef74a4b2388203&gt;c800319&lt;/denchmark-link&gt;
 and the error occurs with &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/257b5d5aa8e154dce43dbe718d26f19aa1f3871a&gt;257b5d5&lt;/denchmark-link&gt;
 (which is one commit after &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/adab385dace2ef75f58852157b55788cb56b8647&gt;adab385&lt;/denchmark-link&gt;
,  &lt;denchmark-link:https://github.com/NervanaSystems/neon/commit/adab385dace2ef75f58852157b55788cb56b8647&gt;adab385&lt;/denchmark-link&gt;
 had a weird bug and I had to test with the next commit).
The lines that cause the error are beyond my depth with Neon, so I'm relying on you here. I wonder if it would be possible to create a simpler test case to reproduce the issue? I tried it once, based on test_model_serialize() in tests/test_model.py, but failed. Reproducing the issue currently takes couple of hours and that's the main bummer.
		</comment>
		<comment id='22' author='tambetm' date='2016-03-12T00:58:56Z'>
		We have a PR staged to fix this.
		</comment>
		<comment id='23' author='tambetm' date='2016-03-14T05:27:26Z'>
		We're delaying the fix until we're sure that it won't affect anything else, but in the meantime, adding the following lines to your existing code should accomplish the same thing:
Ensure same parallelism mode all the way through the network as follows
Prior to model initialization for  &lt;denchmark-link:https://github.com/tambetm/simple_dqn/blob/master/src/deepqnetwork.py#L42&gt;here&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;for l in self.model.layers.layers:
    l.parallelism = 'Disabled'
&lt;/denchmark-code&gt;

And prior to model initialization for  &lt;denchmark-link:https://github.com/tambetm/simple_dqn/blob/master/src/deepqnetwork.py#L62&gt;here&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;for l in self.target_model.layers.layers:
    l.parallelism = 'Disabled'
&lt;/denchmark-code&gt;

I should also note that you can now get_description from the model to accomplish your deep copy without serializing to disk (will still transfer weights from device to host and back) by doing:
&lt;denchmark-code&gt;pdict = self.model.get_description(get_weights=True)
self.target_model.deserialize(pdict, load_states=False)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='tambetm' date='2016-07-01T21:07:31Z'>
		This should be fixed now in 1.5.1.
		</comment>
	</comments>
</bug>