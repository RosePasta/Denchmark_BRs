<bug id='11' author='scttl' open_date='2015-05-01T05:37:44Z' closed_time='2015-09-09T08:47:36Z'>
	<summary>Avoid computing the derivative of the activation function during inference.</summary>
	<description>
The fprop() function computes the derivative of the activation function. This is unnecessary during inference.
	</description>
	<comments>
		<comment id='1' author='scttl' date='2015-05-27T19:17:21Z'>
		Does this occur for all models?  Which model specifically were you attempting to run inference on (not that it matters, just would like to start from a known path to scope out breadth of the issue)?
		</comment>
		<comment id='2' author='scttl' date='2015-05-28T07:28:15Z'>
		This occurs at the activation, not overall model (or layer) level.  As a result, it impacts any models that utilize these activations.
You can see the behavior in neon/transforms/logistic.py in fprop_func for instance.  To speed up computation during the training phase we update the input buffer with the derivative in that call instead of having to make a separate bprop_func call with additional calculations.
In situations where we just want to generate predictions without training, this additional computation is unnecessary so we should add a skip_derivative parameter and set it accordingly in these cases.  We did exactly this for neon/transforms/softmax.py but need to apply it consistently to some of the other activations.
		</comment>
	</comments>
</bug>