<bug id='233' author='thouis' open_date='2016-04-15T20:00:13Z' closed_time='2016-06-24T04:16:32Z'>
	<summary>crash with AssertionError: must use initialized bsum config</summary>
	<description>
The code below (slightly modified from that in &lt;denchmark-link:https://github.com/NervanaSystems/neon/issues/232&gt;#232&lt;/denchmark-link&gt;
) works on the cpu backend (and I believe on Maxwell), but not on my K40.
I'm at 1.3.0+344372b.
It crashes with:
&lt;denchmark-h:h1&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;trace&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "bsum_bug.py", line 64, in &lt;module&gt;
    model.fit(train, optimizer=opt_gdm, num_epochs=num_epochs, cost=cost, callbacks=Callbacks(model))
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/models/model.py", line 149, in fit
    self._epoch_fit(dataset, callbacks)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/models/model.py", line 179, in _epoch_fit
    self.bprop(delta)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/models/model.py", line 211, in bprop
    return self.layers.bprop(delta)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/container.py", line 209, in bprop
    error = l.bprop(error)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/container.py", line 402, in bprop
    l.bprop(error, alpha=alpha, beta=b)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/container.py", line 209, in bprop
    error = l.bprop(error)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/layers/layer.py", line 654, in bprop
    alpha=alpha, beta=beta)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/backends/nervanagpu.py", line 1643, in bprop_conv
    layer.bprop_kernels.bind_params(E, F, grad_I, alpha, beta, bsum)
  File "/net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/backends/convolution.py", line 293, in bind_params
    assert bsum is not None, "must use initialized bsum config"
AssertionError: must use initialized bsum config
&gt;&gt;&gt; import pdb
&gt;&gt;&gt; pdb.pm()
&gt; /net/seasfs02/srv/export/pfister_lab/share_root/thouis/neon/neon/backends/convolution.py(293)bind_params()
-&gt; assert bsum is not None, "must use initialized bsum config"
(Pdb) p self
&lt;neon.backends.convolution.BpropCuda object at 0x2b55fdd0c450&gt;
(Pdb) 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h1&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:h1&gt;code&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import numpy as np

from neon.initializers import GlorotUniform, Constant, Uniform
from neon.layers import Conv, GeneralizedCost, Dropout, SkipNode, Activation, Bias, Affine, MergeSum
from neon.models import Model
from neon.optimizers import GradientDescentMomentum
from neon.transforms import Rectlin, CrossEntropyBinary, Softmax
from neon.callbacks.callbacks import Callbacks
from neon.util.argparser import NeonArgparser
from neon.data import ArrayIterator

# parse the command line arguments
parser = NeonArgparser(__doc__)
args = parser.parse_args()

# hyperparameters
num_epochs = args.epochs
network_depth = 16
num_features = 128

def conv_params(fsize, relu=True, batch_norm=True):
    padding = {'pad_h': fsize[0] // 2, 'pad_w': fsize[1] // 2}  # always pad to preserve width and height
    return dict(fshape=fsize,
                activation=(Rectlin() if relu else None),
                padding=padding,
                batch_norm=batch_norm,
                init=GlorotUniform())

def resnet_module(nfm):
    sidepath = SkipNode()
    mainpath = [Conv(**conv_params((3, 3, nfm))),
                Bias(Constant()),
                Dropout(0.9),
                Conv(**conv_params((3, 3, nfm), relu=False))]
    return [MergeSum([sidepath, mainpath]),
            Activation(Rectlin())]

def build_model(depth, nfm):
    # input - expand to #nfm feature maps
    layers = [Conv(**conv_params((5, 5, nfm))),
              Dropout(0.8)]

    for d in range(depth):
        layers += resnet_module(nfm)

    # final output: 1 channel
    layers += [Dropout(0.5),
               Affine(362, init=Uniform(-1.0 / (362 * nfm), 1.0 / (362 * nfm)), activation=Softmax())]

    return Model(layers=layers)

X = X = np.random.rand(10000, 48 * 19 * 19)  # X.shape = (10000, 48*19*19)
y = np.random.randint(0, 362, 10000)  # y.shape = (10000, )
train = ArrayIterator(X=X, y=y, nclass=362, lshape=(48, 19, 19))

cost = GeneralizedCost(costfunc=CrossEntropyBinary())
opt_gdm = GradientDescentMomentum(learning_rate=0.01,
                                  momentum_coef=0.9,
                                  stochastic_round=args.rounding)

model = build_model(network_depth, num_features)
model.fit(train, optimizer=opt_gdm, num_epochs=num_epochs, cost=cost, callbacks=Callbacks(model))
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='thouis' date='2016-04-15T23:17:25Z'>
		oh, one thing i should mention is that bias is unnecessary if you are including batch norm since there is a bias-like term built in to the batch norm z-transform (the batch norm beta, not the accumulation beta)
so &lt;denchmark-link:https://github.com/NervanaSystems/neon/issues/232&gt;#232&lt;/denchmark-link&gt;
 would probably be ameliorated a bit by removing the biases altogether.
regarding the bsum issue for kepler arch, the problem is because we don't include the capability to calculate the sum across feature maps used for batch mean in those kernels.  To get around this issue there is a mode where batch sum is explicitly calculated outside of the kernel.  you can activate that by just providing a random seed on the command line (i.e. doing -r 0 when running script).
i think that should work...
		</comment>
		<comment id='2' author='thouis' date='2016-04-15T23:45:16Z'>
		Aha.  I was hoping for a position-dependent bias term.  Is there a way to get that?
Removing biases and adding "-r 0" does fix it.  Thanks.
		</comment>
		<comment id='3' author='thouis' date='2016-06-24T04:16:32Z'>
		I'm not quite sure what you meant by position-dependent bias terms.  If you mean some conv layers having batch norm and no bias, and some having bias and no batch norm depending on their position in the module, then the answer would be to define another conv_params dict that has batch_norm=False and a bias.
In an upcoming release we should also have support for non-deterministic batch summation for kepler.
		</comment>
	</comments>
</bug>