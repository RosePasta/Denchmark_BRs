<bug id='170' author='oleg-trott' open_date='2015-12-20T07:00:16Z' closed_time='2016-01-31T18:58:34Z'>
	<summary>Incorrect AutoDiff for some expressions with "max" ?</summary>
	<description>
The last result apparently violates the linearity of the gradient operator (as it should be the sum of the previous two)
&lt;denchmark-code&gt;In [352]: neon.__version__
Out[352]: '1.1.3'

In [353]: x = ng.array(np.array([[2, 3]]))

In [354]: Autodiff(op_tree=(0+x), be=ng, next_error=ng.ones(x.shape)).get_grad_asnumpyarray([x])
Out[354]: [array([[ 1.,  1.]], dtype=float32)]

In [355]: Autodiff(op_tree=ng.max(x), be=ng, next_error=ng.ones(x.shape)).get_grad_asnumpyarray([x])
Out[355]: [array([[ 0.,  1.]], dtype=float32)]

In [356]: Autodiff(op_tree=(ng.max(x) + x), be=ng, next_error=ng.ones(x.shape)).get_grad_asnumpyarray([x])
Out[356]: [array([[ 1.,  3.]], dtype=float32)]
&lt;/denchmark-code&gt;

I guess it is the second result that should be considered incorrect, which also differs from ng.max(x) + 0.
min and sum seem to be similarly affected:
&lt;denchmark-code&gt;In [382]: Autodiff(op_tree=(ng.sum(x)), be=ng, next_error=ng.ones(x.shape)).get_grad_asnumpyarray([x])
Out[382]: [array([[ 1.,  1.]], dtype=float32)]

In [383]: Autodiff(op_tree=(ng.sum(x) + 0), be=ng, next_error=ng.ones(x.shape)).get_grad_asnumpyarray([x])
Out[383]: [array([[ 2.,  2.]], dtype=float32)]
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='oleg-trott' date='2015-12-21T06:11:36Z'>
		This behavior is expected. The linearity of the gradient operator applies to element-wise operations. However, min, max and sum are reduction operations. In max(x) + x for example, the value of max(x) (scalar) gets broadcast to the subsequent operations.
You could checkout the following scripts to test against numerical gradients:
&lt;denchmark-code&gt;import numpy as np

def get_numerical_gradient(f, tensors, delta=1e-5):
    # buffer for gradients
    gradients = []
    for i in range(len(tensors)):
        tensors[i] = tensors[i].astype(np.float64)
        gradients.append(np.zeros(tensors[i].shape))

    # iterate through tensor
    for tensor, gradient in zip(tensors, gradients):

        tensor_flat = tensor.reshape((-1, ))
        gradient_flat = gradient.reshape((-1, ))

        # iterate through each element
        for idx in range(len(tensor_flat)):
            # backup
            backup = tensor_flat[idx]
            # increment
            tensor_flat[idx] = tensor_flat[idx] + delta
            f_inc = np.sum(f(*tensors))
            # decrement
            tensor_flat[idx] = backup - delta
            f_dec = np.sum(f(*tensors))
            # recover
            tensor_flat[idx] = backup
            # gradient
            gradient_flat[idx] = (f_inc - f_dec) / (2.0 * delta)

    return gradients

print get_numerical_gradient(lambda x: np.sum(np.max(x) + x), 
                             [np.array([2., 3.])], 
                             delta=1e-5)

# The result is [array([ 1.,  3.])]
&lt;/denchmark-code&gt;

Or, we could also use HIPS's &lt;denchmark-link:https://github.com/HIPS/autograd&gt;autograd&lt;/denchmark-link&gt;
 to verify the result:
&lt;denchmark-code&gt;import autograd.numpy as np
from autograd import grad

func_grad = grad(lambda x: np.sum(np.max(x) + x))
print func_grad(np.array([2., 3.]))  # [ 1.  3.]

func_grad = grad(lambda x: np.sum(np.max(x)))
print func_grad(np.array([2., 3.]))  # [ 0.  1.]

func_grad = grad(lambda x: np.sum(x))
print func_grad(np.array([2., 3.]))  # [ 1.  1.]
&lt;/denchmark-code&gt;

In the above examples, applying sum to the final results is equivalent to setting next_error to a matrix of ones in neon. Also notes that autodiff of slicing operations is not supported in neon yet and we plan to add that in the future.
		</comment>
		<comment id='2' author='oleg-trott' date='2015-12-21T06:56:42Z'>
		As I mentioned, it's the second result that's suspect, rather than the third.
Note that ng.max(x) and ng.max(x) + 0 can have different gradients in Neon:
&lt;denchmark-code&gt;In [498]: x = ng.array(np.array([[2, 3]]))

In [499]: def grad(e): return Autodiff(op_tree=e, be=ng, next_error=ng.ones(x.shape)).get_grad_asnumpyarray([x])

In [500]: grad(ng.max(x))
Out[500]: [array([[ 0.,  1.]], dtype=float32)]

In [501]: grad(ng.max(x)+0)
Out[501]: [array([[ 0.,  2.]], dtype=float32)]
&lt;/denchmark-code&gt;

I'm assuming that this is because the broadcasting isn't taken into account in the gradient calculation, even though the broadcasting itself takes place with or without +0:
&lt;denchmark-code&gt;In [502]: y = ng.zeros(x.shape)

In [503]: y[:] = ng.max(x)

In [504]: y.get()
Out[504]: array([[ 3.,  3.]], dtype=float32)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='oleg-trott' date='2015-12-21T07:04:47Z'>
		Try changing next_error's shape to the shape of the op_tree or set it to None. The shape ofnext_error shall be consistent to the output rather than the shape of x.
&lt;denchmark-code&gt;In [13]: x = ng.array(np.array([[2, 3]]))

In [14]: def grad(e): return Autodiff(op_tree=e, be=ng, next_error=ng.ones(e.shape)).get_grad_asnumpyarray([x])

In [15]: grad(ng.max(x))
Out[15]: [array([[ 0.,  1.]], dtype=float32)]

In [16]: grad(ng.max(x)+0.)
Out[16]: [array([[ 0.,  1.]], dtype=float32)]
&lt;/denchmark-code&gt;

A check will be added to ensure the shape consistence.
		</comment>
		<comment id='4' author='oleg-trott' date='2015-12-22T20:14:54Z'>
		I realize that it's the shape of next_error that triggers the bug -- that's why I mentioned broadcasting. Broadcasting is very useful, and it's obviously a differentiable operation. I think it's more helpful to fix its gradient than to forbid it. Perhaps add a 0 to all expressions? :-)
		</comment>
		<comment id='5' author='oleg-trott' date='2015-12-23T12:47:36Z'>
		Yeah, but I guess in that case it might be more reasonable to do the broadcasting manually. next_error is typically the back-propagated gradients from the final cost to an intermediate output (such as a layer's activation), so the shape of next_error shall be consistent to the shape of that intermediate output.
If the output shape needs broadcasting to match the shape of the next_error, we could just define another "broadcasted output" that is the same shape as the next_error.
		</comment>
	</comments>
</bug>