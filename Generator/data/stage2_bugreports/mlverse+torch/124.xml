<bug id='124' author='skeydan' open_date='2020-06-26T08:47:23Z' closed_time='2020-06-26T18:00:55Z'>
	<summary>cannot call weight$grad$zero_() - tensor undefined</summary>
	<description>
See marked lines/comments in the code below.
Error:
&lt;denchmark-code&gt; Error in (function (self)  : 
  Expected a Tensor of type Variable but found an undefined Tensor for argument #0 'self' (checked_cast_variable at 
&lt;/denchmark-code&gt;

Code:
&lt;denchmark-code&gt;d_in &lt;- 3
d_out &lt;- 1
n &lt;- 2
x &lt;- torch_randn(n, d_in)
y &lt;-
  x[, 0, NULL] * 0.2 - x[, 1, NULL] * 1.3 - x[, 2, NULL] * 0.5 + torch_randn(n, 1)


d_hidden &lt;- 32
w1 &lt;- torch_randn(d_in, d_hidden, requires_grad = TRUE)

learning_rate &lt;- 1e-4

y_pred &lt;- x$mm(w1)

loss &lt;- (y_pred - y)$pow(2)$sum()
loss$backward()

with_no_grad({

  ###########################
  # w1$grad is fine here
  ###########################

  w1 &lt;- w1 - learning_rate * w1$grad

  ############################
  # now w1$grad is undefined for some reason
  #############################

  # so this fails
  w1$grad$zero_()
})

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='skeydan' date='2020-06-26T12:27:18Z'>
		shouldn't this line:
&lt;denchmark-code&gt; # so this fails
  w1$grad$zero_()
&lt;/denchmark-code&gt;

be outside the no_grad context?
		</comment>
		<comment id='2' author='skeydan' date='2020-06-26T12:42:14Z'>
		it is not here: &lt;denchmark-link:https://github.com/pytorch/tutorials/blob/master/beginner_source/examples_autograd/two_layer_net_autograd.py&gt;https://github.com/pytorch/tutorials/blob/master/beginner_source/examples_autograd/two_layer_net_autograd.py&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='skeydan' date='2020-06-26T13:27:53Z'>
		right! i'll investigate it
		</comment>
		<comment id='4' author='skeydan' date='2020-06-26T16:55:15Z'>
		Hey, I think the prooblem is that when you do this:
&lt;denchmark-code&gt;w1 &lt;- w1 - learning_rate * w1$grad
&lt;/denchmark-code&gt;

You are creating a  copy of w, not modifying it in place and this copy doesn't indeed have agradient.
You should use $sub_ that will modify the tensor in place.
This works in python because the -= operator will modify the tensor in place.
		</comment>
		<comment id='5' author='skeydan' date='2020-06-26T18:00:55Z'>
		wow, you're a genius!
		</comment>
	</comments>
</bug>