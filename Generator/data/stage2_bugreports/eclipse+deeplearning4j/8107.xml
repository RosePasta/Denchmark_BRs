<bug id='8107' author='flyheroljg' open_date='2019-08-17T06:51:49Z' closed_time='2019-10-01T00:34:05Z'>
	<summary>The output from the current DQN and target DQN</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

When I study the file ‘QLearningDiscrete.java’, I find a question in the function ‘setTarge’.
Function setTarge should be prepare for updating the current DQN which approximates the values of behavior functions. Therefore, in the return value pair(obs, dqnOutputAr), the first is the input for training and the second is the input for labeling.
The standard DQN algorithm requires that obs and dqnOutputAr should be outputted by the current DQN and the target DQN, respectively. However, in your code, the variable ‘yTar’, which should be the target value of TD, is inferred from the current DQN, rather than the target DQN. Because the variable ‘dqnOutAr’ is outputted by the current DQN, both obs and dqnOutAr are outputted by the current DQN. Why is it? Did I not understand the standard DQN algorithm correctly?
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:
RL4J, file:  QLearningDiscrete.java
&lt;denchmark-h:h4&gt;Additional Information&lt;/denchmark-h&gt;

Where applicable, please also provide:
protected Pair&lt;INDArray, INDArray&gt; setTarget(ArrayList&lt;Transition&gt; transitions) {
if (transitions.size() == 0)
throw new IllegalArgumentException("too few transitions");
&lt;denchmark-code&gt;    int size = transitions.size();

    int[] shape = getHistoryProcessor() == null ? getMdp().getObservationSpace().getShape()
                    : getHistoryProcessor().getConf().getShape();
    int[] nshape = makeShape(size, shape);
    INDArray obs = Nd4j.create(nshape);
    INDArray nextObs = Nd4j.create(nshape);
    int[] actions = new int[size];
    boolean[] areTerminal = new boolean[size];

    for (int i = 0; i &lt; size; i++) {
        Transition&lt;Integer&gt; trans = transitions.get(i);
        areTerminal[i] = trans.isTerminal();
        actions[i] = trans.getAction();

        INDArray[] obsArray = trans.getObservation();
        if (obs.rank() == 2) {
            obs.putRow(i, obsArray[0]);
        } else {
            for (int j = 0; j &lt; obsArray.length; j++) {
                obs.put(new INDArrayIndex[] {NDArrayIndex.point(i), NDArrayIndex.point(j)}, obsArray[j]);
            }
        }

        INDArray[] nextObsArray = Transition.append(trans.getObservation(), trans.getNextObservation());
        if (nextObs.rank() == 2) {
            nextObs.putRow(i, nextObsArray[0]);
        } else {
            for (int j = 0; j &lt; nextObsArray.length; j++) {
                nextObs.put(new INDArrayIndex[] {NDArrayIndex.point(i), NDArrayIndex.point(j)}, nextObsArray[j]);
            }
        }
    }
    if (getHistoryProcessor() != null) {
        obs.muli(1.0 / getHistoryProcessor().getScale());
        nextObs.muli(1.0 / getHistoryProcessor().getScale());
    }

    INDArray dqnOutputAr = dqnOutput(obs);

    INDArray dqnOutputNext = dqnOutput(nextObs);
    INDArray targetDqnOutputNext = null;

    INDArray tempQ = null;
    INDArray getMaxAction = null;
    if (getConfiguration().isDoubleDQN()) {
        targetDqnOutputNext = targetDqnOutput(nextObs);
        getMaxAction = Nd4j.argMax(dqnOutputNext, 1);
    } else {
        tempQ = Nd4j.max(dqnOutputNext, 1);
    }


    for (int i = 0; i &lt; size; i++) {
        double yTar = transitions.get(i).getReward();
        if (!areTerminal[i]) {
            double q = 0;
            if (getConfiguration().isDoubleDQN()) {
                q += targetDqnOutputNext.getDouble(i, getMaxAction.getInt(i));
            } else
                q += tempQ.getDouble(i);

            yTar += getConfiguration().getGamma() * q;

        }



        double previousV = dqnOutputAr.getDouble(i, actions[i]);
        double lowB = previousV - getConfiguration().getErrorClamp();
        double highB = previousV + getConfiguration().getErrorClamp();
        double clamped = Math.min(highB, Math.max(yTar, lowB));

        dqnOutputAr.putScalar(i, actions[i], clamped);
    }

    return new Pair(obs, dqnOutputAr);
}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

If you'd like to help us fix the issue by contributing some code, but would
like guidance or help in doing so, please mention it!
	</description>
	<comments>
		<comment id='1' author='flyheroljg' date='2019-08-17T09:28:51Z'>
		It's entirely possible that there is a bug. Could you send a pull request with the proposed changes?
		</comment>
		<comment id='2' author='flyheroljg' date='2019-08-17T10:59:14Z'>
		Because I am a new user, I can not know how to send a pull request with the proposed changes. I think there one change in the file ‘QLearningDiscrete.java’.
The variable ‘dqnOutputAr’ should be outputted by the target DQN. The code can is change into the follow: “INDArray dqnOutputAr = targetDqnOutput(obs)”
As thus, in the return value pair(obs, dqnOutputAr), the obs is the training input. The target DQN output the dqnOutputAr for labeling.
		</comment>
		<comment id='3' author='flyheroljg' date='2019-08-17T11:11:54Z'>
		&lt;denchmark-link:https://github.com/saudet&gt;@saudet&lt;/denchmark-link&gt;
 Please assign me this issue, I'll make the changes.
		</comment>
		<comment id='4' author='flyheroljg' date='2019-08-18T02:26:18Z'>
		&lt;denchmark-link:https://github.com/aboulang2002&gt;@aboulang2002&lt;/denchmark-link&gt;
 Thanks! Feel free to roll this in your latest PR for simplicity :)
		</comment>
		<comment id='5' author='flyheroljg' date='2019-08-18T15:38:08Z'>
		I agree there is a problem here but I think it's more than just dqnOutputAr.
The Q-Learning algorithm says we should minimize the loss function:
L = SquaredErrorLoss(yTar - Q(s)) Eq.1
where yTar = reward + gamma * greedy(Q(nextS))
and where greedy() is simply to evaluate the Q-value for all actions and take the highest value.
In Eq.1, the Q-Network should be used for the computation of Q(s).
As for the computation of yTar, it is different whether we're doing standard DQN or Double-DQN:
Standard: We should only use the Target-Network. Everywhere.
Double-DQN: The Q-Network decides the action and the Target-Network computes the Q-value.
So, since in all cases it's the Target-Network that computes the Q-value, I think we should use targetDqnOutput() for dqnOutputAr (as proposed by &lt;denchmark-link:https://github.com/flyheroljg&gt;@flyheroljg&lt;/denchmark-link&gt;
) and use targetDqnOutputNext to compute Q-values.
At line 262 there is some kind of Q-value clamping:
&lt;denchmark-code&gt;            double previousV = dqnOutputAr.getDouble(i, actions[i]);
            double lowB = previousV - getConfiguration().getErrorClamp();
            double highB = previousV + getConfiguration().getErrorClamp();
            double clamped = Math.min(highB, Math.max(yTar, lowB));
&lt;/denchmark-code&gt;

I suppose its purpose is to reduce the impact of outliers and/or very large updates, but I couldn't find much info on that on the net.
Anyways, it seems more logical to me to use targetDqnOutput() there too.
TL,DR:

Make dqnOutputAr use the target network and rename it targetDqnOutputArray or targetDqnOutput
Move the assignation of targetDqnOutputNext (line 240) to its declaration (line 235)
Make sure we use targetDqnOutputNext everywhere, except where we decide the action in Double-DQN (line 241) (Q-Network should be used)

Am I mistaken?
		</comment>
		<comment id='6' author='flyheroljg' date='2019-10-01T00:34:05Z'>
		Fixed with pull &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/8250&gt;#8250&lt;/denchmark-link&gt;
. Thank you!
		</comment>
	</comments>
</bug>