<bug id='2886' author='develeon' open_date='2017-02-19T20:50:27Z' closed_time='2017-02-28T07:48:37Z'>
	<summary>Trouble running with Cudnn</summary>
	<description>
&lt;denchmark-code&gt;CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:4121 code=77(&lt;unknown&gt;) "cudaStreamSynchronize(*stream)" 
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:5044 code=77(&lt;unknown&gt;) "result" 
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:5044 code=77(&lt;unknown&gt;) "result" 
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:5192 code=77(&lt;unknown&gt;) "result" 
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:5140 code=77(&lt;unknown&gt;) "result" 
Failed on [34384019456] -&gt; [8650268160], size: [80], direction: [2], result: [77]
Ran for 4 minuttes

java.lang.AssertionError: MemcpyAsync failed: AllocationShape(offset=0, length=20, stride=1, elementSize=4, dataType=FLOAT)
	at org.junit.Assert.fail(Assert.java:88)
	at intelligence.net.ai.CnnTest2.TestNetwork(CnnTest2.java:307)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
&lt;/denchmark-code&gt;

---- code:
&lt;denchmark-code&gt;import java.io.File;
import java.util.Iterator;
import java.util.List;
import java.util.Random;
import java.util.ServiceLoader;

import org.datavec.api.io.filters.BalancedPathFilter;
import org.datavec.api.io.labels.ParentPathLabelGenerator;
import org.datavec.api.split.FileSplit;
import org.datavec.api.split.InputSplit;
import org.datavec.image.loader.NativeImageLoader;
import org.datavec.image.recordreader.ImageRecordReader;
import org.deeplearning4j.api.storage.StatsStorage;
import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;
import org.deeplearning4j.datasets.iterator.MultipleEpochsIterator;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.GradientNormalization;
import org.deeplearning4j.nn.conf.LearningRatePolicy;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.distribution.Distribution;
import org.deeplearning4j.nn.conf.distribution.GaussianDistribution;
import org.deeplearning4j.nn.conf.distribution.NormalDistribution;
import org.deeplearning4j.nn.conf.inputs.InputType;
import org.deeplearning4j.nn.conf.inputs.InvalidInputTypeException;
import org.deeplearning4j.nn.conf.layers.ConvolutionLayer;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.LocalResponseNormalization;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.SubsamplingLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.ui.api.UIServer;
import org.deeplearning4j.ui.stats.StatsListener;
import org.deeplearning4j.ui.storage.InMemoryStatsStorage;
import org.deeplearning4j.util.NetSaverLoaderUtils;
import org.junit.Assert;
import org.junit.Test;
import org.nd4j.jita.conf.CudaEnvironment;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.api.buffer.DataBuffer;
import org.nd4j.linalg.api.buffer.util.DataTypeUtil;
//import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.dataset.api.preprocessor.DataNormalization;
import org.nd4j.linalg.dataset.api.preprocessor.ImagePreProcessingScaler;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.factory.Nd4jBackend;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.util.Paths;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
 

public class CnnTest2 {
    protected static final Logger log = LoggerFactory.getLogger(CnnTest2.class);
    protected static int height = 227;
    protected static int width = 227;
    protected static int channels = 3;
    protected static int numExamples = 6444;
    protected static int numLabels = 24;
    protected static int batchSize = 24; //32; //128; //20;

    protected static long seed = 42;
    protected static Random rng = new Random(seed);
    protected static int listenerFreq = 1;
    protected static int iterations = 1;
    protected static int epochs = 50; //50; //100 //1000
    protected static double splitTrainTest = 0.8;
    protected static int nCores = 8;
    protected static boolean save = true;
    protected static boolean useGPU = true;
    protected static boolean useUI = true;
    
    protected static String modelType = "Custom"; // LeNet, AlexNet or Custom but you need to fill it out
    StatsStorage mStatsStorage=null;
    UIServer mUIServer=null;
    
    
    @Test
    public void TestNetwork() { 
		System.out.println("Available processors (cores): " + Runtime.getRuntime().availableProcessors());

		/* Total amount of free memory available to the JVM */
		System.out.println("Free memory (MB): " + Runtime.getRuntime().freeMemory()/1024);
		long allocatedMemory = (Runtime.getRuntime().totalMemory()-Runtime.getRuntime().freeMemory());
		long presumableFreeMemory = Runtime.getRuntime().maxMemory() - allocatedMemory;
		System.out.println("Presumably Free memory (MB): " + presumableFreeMemory/1024);
		/* This will return Long.MAX_VALUE if there is no preset limit */
		long maxMemory = Runtime.getRuntime().maxMemory();
		/* Maximum amount of memory the JVM will attempt to use */
		System.out.println("Maximum memory JVM attempt (MB): " + (maxMemory == Long.MAX_VALUE ? "no limit" : maxMemory/1024));

		/* Total memory currently in use by the JVM */
		System.out.println("Total memory (MB) in use by JVM: " + Runtime.getRuntime().totalMemory()/1024);
    			  
    	long startTime = System.currentTimeMillis();
    	try {
    		Nd4j.create(1);		
    	
			if (useUI) {
				// Initialize the user interface backend
				mUIServer = UIServer.getInstance();
				// Configure where the network information (gradients, score vs. time etc) is to be stored. Here: store in memory.
				mStatsStorage = new InMemoryStatsStorage(); // Alternative: new FileStatsStorage(File), for saving and loading later
				// Attach the StatsStorage instance to the UI: this allows the contents of the StatsStorage to be visualized
				mUIServer.attach(mStatsStorage);
			}
    	    
    		if(useGPU) {
    	//	    DataTypeUtil.setDTypeForContext(DataBuffer.Type.HALF);
		        CudaEnvironment.getInstance().getConfiguration()
		            // key option enabled
		            .allowMultiGPU(false)
		            // we're allowing larger memory caches
		            .setMaximumDeviceCache(2L * 1024L * 1024L * 1024L)
		            // cross-device access is used for faster model averaging over pcie
		            .allowCrossDeviceAccess(true);
		           
    		}
    		
        log.info("Load data....");
        System.out.println("Load data..");
        /**cd
         * Data Setup -&gt; organize and limit data file paths:
         *  - mainPath = path to image files
         *  - fileSplit = define basic dataset split with limits on format
         *  - pathFilter = define additional file load filter to limit size and balance batch content
         **/
        ParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator();
        //File mainPath = new File(System.getProperty("user.dir"), "dl4j-examples/src/main/resources/animals/");
        File mainPath = new File("/home/develeon/ai/training/");
        //File mainPath = new File("/home/ivar/workspace/codezdy/dl4j/newer/dl4j-examples/dl4j-examples/src/main/resources/animals/");
        FileSplit fileSplit = new FileSplit(mainPath, NativeImageLoader.ALLOWED_FORMATS, rng);
        BalancedPathFilter pathFilter = new BalancedPathFilter(rng, labelMaker, numExamples, numLabels, batchSize);

        /**
         * Data Setup -&gt; train test split
         *  - inputSplit = define train and test split
         **/
        InputSplit[] inputSplit = fileSplit.sample(pathFilter, numExamples * (1 + splitTrainTest), numExamples * (1 - splitTrainTest));
        InputSplit trainData = inputSplit[0];
        InputSplit testData = inputSplit[1];

        /**
         * Data Setup -&gt; normalization
         *  - how to normalize images and generate large dataset to train on
         **/
        DataNormalization scaler = new ImagePreProcessingScaler(0, 1);

        log.info("Build model....");
        System.out.println("Build model..");

        MultiLayerNetwork network = customModel();                
        network.init();
        
        
        network.setListeners(new ScoreIterationListener(listenerFreq));
        if(useUI) network.setListeners(new StatsListener(mStatsStorage));

        /**
         * Data Setup -&gt; define how to load data into net:
         *  - recordReader = the reader that loads and converts image data pass in inputSplit to initialize
         *  - dataIter = a generator that only loads one batch at a time into memory to save memory
         *  - trainIter = uses MultipleEpochsIterator to ensure model runs through the data for all epochs
         **/
        ImageRecordReader recordReader = new ImageRecordReader(height, width, channels, labelMaker);
        DataSetIterator dataIter;
        MultipleEpochsIterator trainIter;


        log.info("Train model....");
        System.out.println("Train model..");
        // Train without transformations
        recordReader.initialize(trainData, null);
        dataIter = new RecordReaderDataSetIterator(recordReader, batchSize, 1, numLabels);
        scaler.fit(dataIter);
        dataIter.setPreProcessor(scaler);
        trainIter = new MultipleEpochsIterator(epochs, dataIter, nCores);
        log.info("Model fit....");
        network.fit(trainIter);

        System.out.println("Evaluate model..");
        log.info("Evaluate model....");
        recordReader.initialize(testData);
        dataIter = new RecordReaderDataSetIterator(recordReader, batchSize, 1, numLabels);
        scaler.fit(dataIter);
        dataIter.setPreProcessor(scaler);
        Evaluation eval = network.evaluate(dataIter);
        System.out.println(eval.stats(true));
        //log.info(eval.stats(true));

        long estimatedTime = System.currentTimeMillis() - startTime;
        System.out.println("Took "+(estimatedTime/60000)+" minuttes");
   
     
        
    	} catch (Exception e) {
            long estimatedTime = System.currentTimeMillis() - startTime;
            System.out.println("Ran for "+(estimatedTime/60000)+" minuttes");
            Assert.fail(e.getMessage());
    	}
    }
    
    public MultiLayerNetwork customModel() {
        /**
         * Use this method to build your own custom model.
         **/
    	 double nonZeroBias = 1;
         double dropOut = 0.5;
         SubsamplingLayer.PoolingType poolingType = SubsamplingLayer.PoolingType.MAX;

         // TODO split and link kernel maps on GPUs - 2nd, 4th, 5th convolution should only connect maps on the same gpu, 3rd connects to all in 2nd
         MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                 .seed(seed)
                 .weightInit(WeightInit.DISTRIBUTION)
                 .dist(new NormalDistribution(0.0, 0.01))
                 .activation(Activation.RELU)
                 .updater(Updater.NESTEROVS)
                 .iterations(iterations)
                 .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer) // normalize to prevent vanishing or exploding gradients
                 .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                 .learningRate(1e-2)
                 .biasLearningRate(1e-2*2)
                 .learningRateDecayPolicy(LearningRatePolicy.Step)
                 .lrPolicyDecayRate(0.1)
                 .lrPolicySteps(100000)
                 .regularization(true)
                 .l2(5 * 1e-4)
                 .momentum(0.9)
                 .miniBatch(false)                
                 .list()
                 .layer(0, new ConvolutionLayer.Builder(new int[]{11, 11}, new int[]{4, 4}, new int[]{3, 3})
                         .name("cnn1")
                         .nIn(channels)
                         .nOut(96)
                         .build())
                 .layer(1, new LocalResponseNormalization.Builder()
                         .name("lrn1")
                         .build())
                 .layer(2, new SubsamplingLayer.Builder(poolingType, new int[]{3, 3}, new int[]{2, 2})
                         .name("maxpool1")
                         .build())
                 .layer(3, new ConvolutionLayer.Builder(new int[]{5, 5}, new int[]{1, 1}, new int[]{2, 2})
                         .name("cnn2")
                         .nOut(256)
                         .biasInit(nonZeroBias)
                         .build())
                 .layer(4, new LocalResponseNormalization.Builder()
                         .name("lrn2")
                         .k(2).n(5).alpha(1e-4).beta(0.75)
                         .build())
                 .layer(5, new SubsamplingLayer.Builder(poolingType, new int[]{3, 3}, new int[]{2, 2})
                         .name("maxpool2")
                         .build())
                 .layer(6, new ConvolutionLayer.Builder(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1})
                         .name("cnn3")
                         .nOut(384)
                         .build())
                 .layer(7, new ConvolutionLayer.Builder(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1})
                         .name("cnn4")
                         .nOut(384)
                         .biasInit(nonZeroBias)
                         .build())
                 .layer(8, new ConvolutionLayer.Builder(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1})
                         .name("cnn5")
                         .nOut(256)
                         .biasInit(nonZeroBias)
                         .build())
                 .layer(9, new SubsamplingLayer.Builder(poolingType, new int[]{3, 3}, new int[]{2, 2})
                         .name("maxpool3")
                         .build())
                 .layer(10, new DenseLayer.Builder()
                         .name("ffn1")
                         .nOut(4096)
                         .dist(new GaussianDistribution(0, 0.005))
                         .biasInit(nonZeroBias)
                         .dropOut(dropOut)
                         .build())
                 .layer(11, new DenseLayer.Builder()
                         .name("ffn2")
                         .nOut(4096)                        
                         .dist(new GaussianDistribution(0, 0.005))
                         .biasInit(nonZeroBias)
                         .dropOut(dropOut)
                         .build())
                 .layer(12, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                         .name("output")
                         .nOut(numLabels)
                         .activation(Activation.SOFTMAX)
                         .build())
                 .backprop(true)
                 .pretrain(false)
                 .setInputType(InputType.convolutional(height, width, channels))
                 .build();

         return new MultiLayerNetwork(conf);
    }
}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='develeon' date='2017-02-19T21:36:59Z'>
		
What happens when you run without cuDNN?
What's the code here: at intelligence.net.ai.CnnTest2.TestNetwork(CnnTest2.java:307)  ? I don't see any line numbers.
Please attach gist with your pom.xml

		</comment>
		<comment id='2' author='develeon' date='2017-02-19T21:42:30Z'>
		Hm. One more thing to check. Please try to totally disable UI.
		</comment>
		<comment id='3' author='develeon' date='2017-02-20T21:06:58Z'>
		1) It seems running with cuDNN and without UI is fine.
   And running without cuDNN and with UI also worked.

2) Ah that was in the catch block (had removed some comments and unrelated tests) so did not print the stacktrace,
but reran it .. if fails on network.fit

&lt;denchmark-link:https://gist.github.com/develeon/af7dfd436eb264b81d93797f01ed2784&gt;https://gist.github.com/develeon/af7dfd436eb264b81d93797f01ed2784&lt;/denchmark-link&gt;


3) &lt;denchmark-link:https://gist.github.com/develeon/e56ca11d02886f71bd5083eecbbd2783&gt;https://gist.github.com/develeon/e56ca11d02886f71bd5083eecbbd2783&lt;/denchmark-link&gt;


&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


---- On Sun, 19 Feb 2017 22:37:13 +0100 raver119 &amp;lt;notifications@github.com&amp;gt; wrote ---- 

 What happens when you run without cuDNN?
 What's the code here: at intelligence.net.ai.CnnTest2.TestNetwork(CnnTest2.java:307) ? I don't see any line numbers.
 Please attach gist with your pom.xml
  —
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

		</comment>
		<comment id='4' author='develeon' date='2017-02-20T21:17:28Z'>
		so, problem is cuDNN + UI. That helps a lot, thanks for highlighting this issue.
		</comment>
		<comment id='5' author='develeon' date='2017-02-28T06:41:12Z'>
		I am unable to reproduce this issue with the UIExample. It works just fine with cuDNN enabled on my machine (Fedora 23).
&lt;denchmark-link:https://github.com/develeon&gt;@develeon&lt;/denchmark-link&gt;
 Could you run &lt;denchmark-link:https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface/UIExample.java&gt;https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/userInterface/UIExample.java&lt;/denchmark-link&gt;
 and let us know if you are having issues with that one as well? Also, is this happening only when rendering the page of the UIServer? Or does it happen even if you do not have any browsers open? If it only happens when the browser is open, it's probably some bug in the video driver that is corrupting something when rendering the HTML5 of the UIServer...
		</comment>
		<comment id='6' author='develeon' date='2017-02-28T07:24:23Z'>
		I also tried the UIExample (which is a CNN) with cuDNN (Windows 10, cuda 8, cudnn 5.1) and DL4J master, and also couldn't reproduce any issues with it (all pages work fine). I also tried adding a LRN layer to the config, no issues there either.
		</comment>
		<comment id='7' author='develeon' date='2019-01-19T12:51:46Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>