<bug id='4736' author='AlexDBlack' open_date='2018-02-28T07:19:34Z' closed_time='2018-03-09T22:57:54Z'>
	<summary>Failing model zoo unit tests - CUDA</summary>
	<description>
CPU: All passing (occasional memory issues aside)
CUDA:
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/36774806-9e638744-1cb3-11e8-9fd8-075cf579a555.png&gt;&lt;/denchmark-link&gt;

testImageNetLabels: expected golden retriever, got:
&lt;denchmark-code&gt;o.d.z.TestImageNet - Predictions for batch  :
	0.605528%, bassinet
	0.589057%, tub
	0.565464%, paper_towel
	0.555924%, cradle
	0.526733%, book_jacket
&lt;/denchmark-code&gt;

CPU assigns 0.9751 probability to "golden retriever"
GPU assigns 0.0013 probability to "golden retriever"
Full probablities from each backend: &lt;denchmark-link:https://gist.github.com/AlexDBlack/c3cf0404e202974fa2c800899fcfc164&gt;https://gist.github.com/AlexDBlack/c3cf0404e202974fa2c800899fcfc164&lt;/denchmark-link&gt;

testDarknetLabels:
&lt;denchmark-code&gt;org.junit.ComparisonFailure: 
Expected :golden retriever
Actual   :torch
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-02-28T09:49:45Z'>
		Looks like a bug in first subsampling layer - I'm getting a bunch of 0s for some of the activations during CUDA forward pass... but not all of them. Before that, activations are pretty close for both backends.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-03-09T22:57:54Z'>
		Confirmed fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/libnd4j/pull/818&gt;deeplearning4j/libnd4j#818&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='AlexDBlack' date='2018-09-23T04:27:59Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>