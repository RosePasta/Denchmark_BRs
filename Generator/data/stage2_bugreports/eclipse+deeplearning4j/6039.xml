<bug id='6039' author='emillynge' open_date='2018-08-01T13:32:35Z' closed_time='2018-08-06T13:45:29Z'>
	<summary>libnd4j - new SoftMax Op overflows and results in NaN</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Please describe your issue, along with:

expected behavior

Use softmax in SameDiffLayer
Get values between 0 - 1


encountered behavior

Use softmax in SameDiffLayer
Get nan values when input is very large



The problem is caused by the new helper function in activations.cpp
&lt;denchmark-code&gt;template &lt;typename T&gt;
void softmax(const NDArray&lt;T&gt;&amp; input, NDArray&lt;T&gt;&amp; output, const int dimension) {

    const int rank = input.rankOf();

    if(input.isVector()) {
        
        if(rank == 1 || input.sizeAt(dimension) != 1)
            softMaxForVector&lt;T&gt;(input, output);
        else
            output = 1.;
    }
    else {
        
        NDArray&lt;T&gt; exponents = const_cast&lt;NDArray&lt;T&gt;&amp;&gt;(input).template transform&lt;simdOps::Exp&lt;T&gt;&gt;();
        NDArray&lt;T&gt; sumAlongDim = exponents.template reduceAlongDims&lt;simdOps::Sum&lt;T&gt;&gt;({dimension}, true);        
        output.assign(exponents / sumAlongDim);
    }
}
&lt;/denchmark-code&gt;

As seen, we are taking exp of the raw input which is likely to cause NaN values.
In SoftMaxOld this issue is handled by first subtracting the maximum value in each row.
This translation does not affect the result of the softmax function, but it prevents overflow in the exp computation.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version 1.0.0-SNAPSHOT
platform information (OS, etc): ubuntu 18
CUDA version, if used: N/A
NVIDIA driver version, if in use: N/A

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

If you'd like to help us fix the issue by contributing some code, but would
like guidance or help in doing so, please mention it!
I could do a PR, but I suspect it would be faster for one of the core contributors to include this small change in another PR.
	</description>
	<comments>
		<comment id='1' author='emillynge' date='2018-08-01T16:22:01Z'>
		cc &lt;denchmark-link:https://github.com/shyrma&gt;@shyrma&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='emillynge' date='2018-08-02T08:30:05Z'>
		Here is a code snippet that i would think fixes the issue:
&lt;denchmark-code&gt;        NDArray&lt;T&gt; maxAlongDim = const_cast&lt;NDArray&lt;T&gt;&amp;&gt;(input).template reduceAlongDims&lt;simdOps::Max&lt;T&gt;&gt;({dimension}, true);
        NDArray&lt;T&gt; translatedInput = const_cast&lt;NDArray&lt;T&gt;&amp;&gt;(input).template applyBroadcast&lt;simdOps::Subtract&lt;T&gt;&gt;({dimension}, maxAlongDim);
        NDArray&lt;T&gt; exponents = translatedInput.template transform&lt;simdOps::Exp&lt;T&gt;&gt;();
        NDArray&lt;T&gt; sumAlongDim = exponents.template reduceAlongDims&lt;simdOps::Sum&lt;T&gt;&gt;({dimension}, true);        
        output.assign(exponents / sumAlongDim);
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='emillynge' date='2018-08-02T09:25:09Z'>
		Hi, emillynge
Thanks for your participating !
I'll take into account your code and change this operation appropriately
		</comment>
		<comment id='4' author='emillynge' date='2018-08-02T09:50:29Z'>
		&lt;denchmark-link:https://github.com/shyrma&gt;@shyrma&lt;/denchmark-link&gt;
 you may also want to revisit any other method that implements a variant of this op like LogSoftMax.
I should not think any derivate/bp methods needs to be changed as softmax should be invariant to translation.
		</comment>
		<comment id='5' author='emillynge' date='2018-08-02T10:10:47Z'>
		FYI, I can see that currently LossBinaryXENT has issues specifically because LogSoftMax produces NaN values. So right now, I have to prepend my OutputLayer with an ActivationLayer to stop LossBinaryXENT calling LogSoftMax
		</comment>
		<comment id='6' author='emillynge' date='2018-08-02T10:15:49Z'>
		Seems like we had such (epx(raw_input)) in softmaxCrossEntropyWithLogits as well. Fixed now
		</comment>
		<comment id='7' author='emillynge' date='2018-08-06T13:45:29Z'>
		Issue should be fixed now i think.
Thanks for highlighting this problem.
		</comment>
		<comment id='8' author='emillynge' date='2018-09-21T10:21:31Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>