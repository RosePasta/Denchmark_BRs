<bug id='5461' author='AlexDBlack' open_date='2018-06-05T01:47:59Z' closed_time='2018-06-12T08:58:45Z'>
	<summary>DL4J: Benchmarks, resnet50: can't run batch size 16, can run batch 32</summary>
	<description>
I believe this is related to how CuDNN is configured... this particular model is set to ConvolutionLayer.AlgoMode.PREFER_FASTEST; I suspect it's the CuDNN mode internally that is the reason (i.e., batch size 32 uses a different mode that requires less memory).
Now, CuDNN (or at least some of the more recent versions?) does support specifying a maximum workspace size. We may be able to inspect the amount of available memory, and have CuDNN base it's algorithm selection on that.
Alternatively, if we detect an OOM, we might be able to enforce use of a less memory-intensive algorithm rather than failing outright.
&lt;denchmark-code&gt;Running test: v100beta_cuda91-cudnn, batch size 16
Exception in thread "main" java.lang.OutOfMemoryError: Failed to allocate memory within limits: totalBytes = 9G + 9G &gt; maxBytes = 12G
        at org.bytedeco.javacpp.Pointer.deallocator(Pointer.java:572)
        at org.deeplearning4j.nn.layers.BaseCudnnHelper$DataCache.&lt;init&gt;(BaseCudnnHelper.java:123)
        at org.deeplearning4j.nn.layers.convolution.CudnnConvolutionHelper.preOutput(CudnnConvolutionHelper.java:444)
        at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:323)
        at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:392)
        at org.deeplearning4j.nn.graph.vertex.impl.LayerVertex.doForward(LayerVertex.java:105)
        at org.deeplearning4j.nn.graph.ComputationGraph.ffToLayerActivationsInWS(ComputationGraph.java:1946)
        at org.deeplearning4j.nn.graph.ComputationGraph.computeGradientAndScore(ComputationGraph.java:1310)
        at org.deeplearning4j.nn.graph.ComputationGraph.computeGradientAndScore(ComputationGraph.java:1280)
        at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:178)
        at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)
        at org.deeplearning4j.optimize.Solver.optimize(Solver.java:54)
        at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:1104)
        at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:1036)
        at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:978)
        at org.deeplearning4j.benchmarks.BaseBenchmark.benchmark(BaseBenchmark.java:103)
        at org.deeplearning4j.benchmarks.BaseBenchmark$Benchmark.execute(BaseBenchmark.java:40)
        at org.deeplearning4j.benchmarks.BenchmarkCnn.run(BenchmarkCnn.java:103)
        at org.deeplearning4j.benchmarks.BenchmarkCnn.main(BenchmarkCnn.java:118)
Running test: v100beta_cuda91-cudnn, batch size 32
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-06-05T01:55:55Z'>
		We can set ConvolutionLayer.AlgoMode.NO_WORKSPACE though.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-06-05T02:52:58Z'>
		&lt;denchmark-link:https://github.com/saudet&gt;@saudet&lt;/denchmark-link&gt;
 That's not a good solution IMO - we want as fast as possible within the constraints of the available memory.
NO_WORKSPACE minimizes the memory, but will have a performance impact.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-06-12T08:58:45Z'>
		Fixed in previous optimization PRs; now can run batch sizes up to 192 on that machine.
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-09-21T19:59:20Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>