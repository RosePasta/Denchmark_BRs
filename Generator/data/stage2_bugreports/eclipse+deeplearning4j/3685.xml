<bug id='3685' author='psuszyns' open_date='2017-07-17T16:04:55Z' closed_time='2017-07-18T12:47:03Z'>
	<summary>Cannot use a network read from a serialized form to make predictions: NullPointerException</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Cannot use a network read from a serialized form to make predictions. The following scenario works when I am not serializing and then deserializing the network.
Following scenario is in Scala 2.11 and uses dl4j-spark: I first serialized the network to HDFS:
&lt;denchmark-code&gt;val outputStream = new ByteArrayOutputStream(1000000)
DL4JModelSerializer.writeModel(model.network.get, outputStream, true)
sc.parallelize(Array(outputStream.toByteArray)).map { byteArray =&gt;
  (NullWritable.get(), new BytesWritable(byteArray))
}.saveAsSequenceFile(networkPath)
&lt;/denchmark-code&gt;

then deserialised it using
&lt;denchmark-code&gt;val bytes: RDD[Array[Byte]] = sc.sequenceFile[NullWritable, BytesWritable](networkPath).map(_._2.copyBytes())
val inputStream = new ByteArrayInputStream(bytes.collect()(0))
val network = DL4JModelSerializer.restoreMultiLayerNetwork(inputStream, true)
&lt;/denchmark-code&gt;

It seemed to have worked but then I tried to actually use the network:
&lt;denchmark-code&gt;val networkConf = sc.broadcast(network.getLayerWiseConfigurations.toJson)
val networkParams = sc.broadcast(network.params())
input.mapPartitions { part =&gt; //input is a Spark Dataset
  val networkOnWorker = new MultiLayerNetwork(MultiLayerConfiguration.fromJson(networkConf.value))
  networkOnWorker.init()
  networkOnWorker.setParameters(networkParams.value) //this line fails with NullPointerException
  // here I want to use the network to make predictions
}
&lt;/denchmark-code&gt;

This fails with:
&lt;denchmark-code&gt;[Stage 9:&gt;                                                        (0 + 3) / 394]17/07/17 11:19:31 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 9.0 (TID 180, xxx.com, executor 3): java.lang.NullPointerException
at org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner.interceptIntDataType(DefaultOpExecutioner.java:92)
at org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner.checkForCompression(DefaultOpExecutioner.java:64)
at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:84)
at org.nd4j.linalg.api.ndarray.BaseNDArray.assign(BaseNDArray.java:1223)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.setParams(MultiLayerNetwork.java:875)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.setParameters(MultiLayerNetwork.java:1942)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

libraryDependencies += "org.nd4j" % "nd4j-api" % "0.8.0"
libraryDependencies += "org.deeplearning4j" % "dl4j-spark_2.11" % "0.8.0_spark_2"
libraryDependencies += "org.nd4j" % "nd4j-native" % "0.8.0" classifier "" classifier "linux-x86_64"
I'm running on a Cloudera cluster with Spark 2.1.0 running on YARN
	</description>
	<comments>
		<comment id='1' author='psuszyns' date='2017-07-17T16:29:27Z'>
		show full sbt config please. just to makse sure it's not mixed version issues.
		</comment>
		<comment id='2' author='psuszyns' date='2017-07-17T16:42:03Z'>
		My full sbt config is a bit complicated and is not contained in a single file but a part related to deeplearning4j is:
&lt;denchmark-code&gt;val deeplearning4jVersion = "0.8.0"
val deeplearning4jSparkVersion = "0.8.0_spark_2"
val nd4jApi = "org.nd4j" % "nd4j-api" % deeplearning4jVersion
val deeplearning4jSpark = "org.deeplearning4j" %% "dl4j-spark" % deeplearning4jSparkVersion excludeAll(
  ExclusionRule(organization = "org.apache.spark"),
  ExclusionRule(organization = "com.fasterxml.jackson.core")
)
val nd4jNative = "org.nd4j" % "nd4j-native" % deeplearning4jVersion classifier "" classifier {
  val osName = System.getProperty("os.name")
  val osArch = System.getProperty("os.arch")
  if (osName.toLowerCase contains "mac") s"macosx-x86_64"
  else if (osName.toLowerCase contains "linux") s"linux-x86_64"
  else if (osName.toLowerCase contains "win") s"windows-x86_64"
  else throw new RuntimeException(s"Classifier absent for system $osName")
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='psuszyns' date='2017-07-17T23:27:20Z'>
		&lt;denchmark-link:https://github.com/psuszyns&gt;@psuszyns&lt;/denchmark-link&gt;
 You can't use dl4j spark 2.11 with a spark 2.10 cluster. I believe cdh only supports scala 2.10. Did you check that first? Beyond that you still need kryo setup (see our spark page: &lt;denchmark-link:http://deeplearning4j.org/spark&gt;http://deeplearning4j.org/spark&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='4' author='psuszyns' date='2017-07-18T09:07:46Z'>
		I'm sure I can use Scala 2.11:
&lt;denchmark-code&gt;/usr/bin/spark2-submit --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.0.cloudera1
      /_/

Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_121
&lt;/denchmark-code&gt;

I'm using JavaSerialization in model training (after which the model is saved) but now I saw that I was lacking this configuration in the prediction spark job - I'll change that and update this thread. Thanks!
		</comment>
		<comment id='5' author='psuszyns' date='2017-07-18T12:47:03Z'>
		Yes, it was caused by using Kryo serialization without appropriate setup. Sorry for a false positive bug.
		</comment>
		<comment id='6' author='psuszyns' date='2017-07-18T13:30:51Z'>
		Ye, not reproducible for me in clean environment...
		</comment>
		<comment id='7' author='psuszyns' date='2018-09-26T00:27:33Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>