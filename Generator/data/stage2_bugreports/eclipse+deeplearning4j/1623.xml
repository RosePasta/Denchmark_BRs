<bug id='1623' author='DYelamos' open_date='2016-05-31T15:47:15Z' closed_time='2018-04-26T03:50:46Z'>
	<summary>ParagraphVectorsClassifierExample.java Exception in thread "main" java.lang.IllegalArgumentException: Length must be &amp;gt;= 1</summary>
	<description>
So i get this stack:
`"D:\Program Files\Java\jdk1.8.0_65\bin\java" -Didea.launcher.port=7535 "-Didea.launcher.bin.path=C:\Program Files (x86)\JetBrains\IntelliJ IDEA Community Edition 2016.1.2\bin" -Dfile.encoding=UTF-8 -classpath "D:\Program Files\Java\jdk1.8.0_65\jre\lib\charsets.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\deploy.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\access-bridge-64.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\cldrdata.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\dnsns.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\jaccess.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\jfxrt.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\localedata.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\nashorn.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\sunec.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\sunjce_provider.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\sunmscapi.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\sunpkcs11.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\ext\zipfs.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\javaws.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\jce.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\jfr.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\jfxswt.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\jsse.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\management-agent.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\plugin.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\resources.jar;D:\Program Files\Java\jdk1.8.0_65\jre\lib\rt.jar;D:\Dropbox\tfg\prueba_word2vec\dl4j-0.4-examples\target\classes;C:\Users\Dani.m2\repository\org\deeplearning4j\deeplearning4j-nlp\0.4-rc3.9\deeplearning4j-nlp-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\apache\lucene\lucene-analyzers-common\5.3.1\lucene-analyzers-common-5.3.1.jar;C:\Users\Dani.m2\repository\org\apache\lucene\lucene-core\5.3.1\lucene-core-5.3.1.jar;C:\Users\Dani.m2\repository\org\apache\lucene\lucene-queryparser\5.3.1\lucene-queryparser-5.3.1.jar;C:\Users\Dani.m2\repository\org\apache\lucene\lucene-queries\5.3.1\lucene-queries-5.3.1.jar;C:\Users\Dani.m2\repository\org\apache\lucene\lucene-sandbox\5.3.1\lucene-sandbox-5.3.1.jar;C:\Users\Dani.m2\repository\org\apache\directory\studio\org.apache.commons.codec\1.8\org.apache.commons.codec-1.8.jar;C:\Users\Dani.m2\repository\commons-codec\commons-codec\1.8\commons-codec-1.8.jar;C:\Users\Dani.m2\repository\it\unimi\dsi\dsiutils\2.2.2\dsiutils-2.2.2.jar;C:\Users\Dani.m2\repository\it\unimi\dsi\fastutil\6.5.15\fastutil-6.5.15.jar;C:\Users\Dani.m2\repository\com\martiansoftware\jsap\2.1\jsap-2.1.jar;C:\Users\Dani.m2\repository\commons-configuration\commons-configuration\1.8\commons-configuration-1.8.jar;C:\Users\Dani.m2\repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;C:\Users\Dani.m2\repository\commons-logging\commons-logging\1.1.1\commons-logging-1.1.1.jar;C:\Users\Dani.m2\repository\commons-collections\commons-collections\20040616\commons-collections-20040616.jar;C:\Users\Dani.m2\repository\org\cleartk\cleartk-snowball\2.0.0\cleartk-snowball-2.0.0.jar;C:\Users\Dani.m2\repository\org\apache\lucene\lucene-snowball\3.0.3\lucene-snowball-3.0.3.jar;C:\Users\Dani.m2\repository\org\cleartk\cleartk-util\2.0.0\cleartk-util-2.0.0.jar;C:\Users\Dani.m2\repository\org\apache\uima\uimaj-core\2.5.0\uimaj-core-2.5.0.jar;C:\Users\Dani.m2\repository\org\apache\uima\uimafit-core\2.0.0\uimafit-core-2.0.0.jar;C:\Users\Dani.m2\repository\commons-logging\commons-logging-api\1.1\commons-logging-api-1.1.jar;C:\Users\Dani.m2\repository\org\springframework\spring-core\3.1.2.RELEASE\spring-core-3.1.2.RELEASE.jar;C:\Users\Dani.m2\repository\org\springframework\spring-asm\3.1.2.RELEASE\spring-asm-3.1.2.RELEASE.jar;C:\Users\Dani.m2\repository\org\springframework\spring-context\3.1.2.RELEASE\spring-context-3.1.2.RELEASE.jar;C:\Users\Dani.m2\repository\org\springframework\spring-aop\3.1.2.RELEASE\spring-aop-3.1.2.RELEASE.jar;C:\Users\Dani.m2\repository\aopalliance\aopalliance\1.0\aopalliance-1.0.jar;C:\Users\Dani.m2\repository\org\springframework\spring-expression\3.1.2.RELEASE\spring-expression-3.1.2.RELEASE.jar;C:\Users\Dani.m2\repository\org\springframework\spring-beans\3.1.2.RELEASE\spring-beans-3.1.2.RELEASE.jar;C:\Users\Dani.m2\repository\org\cleartk\cleartk-type-system\2.0.0\cleartk-type-system-2.0.0.jar;C:\Users\Dani.m2\repository\org\cleartk\cleartk-opennlp-tools\2.0.0\cleartk-opennlp-tools-2.0.0.jar;C:\Users\Dani.m2\repository\org\apache\opennlp\opennlp-maxent\3.0.3\opennlp-maxent-3.0.3.jar;C:\Users\Dani.m2\repository\org\apache\opennlp\opennlp-tools\1.5.3\opennlp-tools-1.5.3.jar;C:\Users\Dani.m2\repository\net\sf\jwordnet\jwnl\1.3.3\jwnl-1.3.3.jar;C:\Users\Dani.m2\repository\org\apache\opennlp\opennlp-uima\1.5.3\opennlp-uima-1.5.3.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-assets\0.8.0\dropwizard-assets-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-core\0.8.0\dropwizard-core-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-util\0.8.0\dropwizard-util-0.8.0.jar;C:\Users\Dani.m2\repository\com\google\code\findbugs\jsr305\3.0.0\jsr305-3.0.0.jar;C:\Users\Dani.m2\repository\joda-time\joda-time\2.7\joda-time-2.7.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-jackson\0.8.0\dropwizard-jackson-0.8.0.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-jdk7\2.5.1\jackson-datatype-jdk7-2.5.1.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-guava\2.5.1\jackson-datatype-guava-2.5.1.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\module\jackson-module-afterburner\2.5.1\jackson-module-afterburner-2.5.1.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\datatype\jackson-datatype-joda\2.5.1\jackson-datatype-joda-2.5.1.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-validation\0.8.0\dropwizard-validation-0.8.0.jar;C:\Users\Dani.m2\repository\org\hibernate\hibernate-validator\5.1.3.Final\hibernate-validator-5.1.3.Final.jar;C:\Users\Dani.m2\repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;C:\Users\Dani.m2\repository\org\jboss\logging\jboss-logging\3.1.3.GA\jboss-logging-3.1.3.GA.jar;C:\Users\Dani.m2\repository\com\fasterxml\classmate\1.0.0\classmate-1.0.0.jar;C:\Users\Dani.m2\repository\org\glassfish\javax.el\3.0.0\javax.el-3.0.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-configuration\0.8.0\dropwizard-configuration-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-logging\0.8.0\dropwizard-logging-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-logback\3.1.0\metrics-logback-3.1.0.jar;C:\Users\Dani.m2\repository\org\slf4j\jul-to-slf4j\1.7.10\jul-to-slf4j-1.7.10.jar;C:\Users\Dani.m2\repository\org\slf4j\log4j-over-slf4j\1.7.10\log4j-over-slf4j-1.7.10.jar;C:\Users\Dani.m2\repository\org\slf4j\jcl-over-slf4j\1.7.10\jcl-over-slf4j-1.7.10.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-util\9.2.9.v20150224\jetty-util-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-metrics\0.8.0\dropwizard-metrics-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-jersey\0.8.0\dropwizard-jersey-0.8.0.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\core\jersey-server\2.16\jersey-server-2.16.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\media\jersey-media-jaxb\2.16\jersey-media-jaxb-2.16.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\ext\jersey-metainf-services\2.16\jersey-metainf-services-2.16.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-jersey2\3.1.0\metrics-jersey2-3.1.0.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\jaxrs\jackson-jaxrs-json-provider\2.5.1\jackson-jaxrs-json-provider-2.5.1.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\jaxrs\jackson-jaxrs-base\2.5.1\jackson-jaxrs-base-2.5.1.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\module\jackson-module-jaxb-annotations\2.5.1\jackson-module-jaxb-annotations-2.5.1.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet\2.16\jersey-container-servlet-2.16.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.16\jersey-container-servlet-core-2.16.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-server\9.2.9.v20150224\jetty-server-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-io\9.2.9.v20150224\jetty-io-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-webapp\9.2.9.v20150224\jetty-webapp-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-xml\9.2.9.v20150224\jetty-xml-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-continuation\9.2.9.v20150224\jetty-continuation-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-jetty\0.8.0\dropwizard-jetty-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-jetty9\3.1.0\metrics-jetty9-3.1.0.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-servlet\9.2.9.v20150224\jetty-servlet-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-security\9.2.9.v20150224\jetty-security-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-servlets\9.2.9.v20150224\jetty-servlets-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\jetty-http\9.2.9.v20150224\jetty-http-9.2.9.v20150224.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-lifecycle\0.8.0\dropwizard-lifecycle-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-core\3.1.0\metrics-core-3.1.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-jvm\3.1.0\metrics-jvm-3.1.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-servlets\3.1.0\metrics-servlets-3.1.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-json\3.1.0\metrics-json-3.1.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-healthchecks\3.1.0\metrics-healthchecks-3.1.0.jar;C:\Users\Dani.m2\repository\net\sourceforge\argparse4j\argparse4j\0.4.4\argparse4j-0.4.4.jar;C:\Users\Dani.m2\repository\org\eclipse\jetty\toolchain\setuid\jetty-setuid-java\1.0.2\jetty-setuid-java-1.0.2.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-servlets\0.8.0\dropwizard-servlets-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-annotation\3.1.0\metrics-annotation-3.1.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-views-mustache\0.8.0\dropwizard-views-mustache-0.8.0.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-views\0.8.0\dropwizard-views-0.8.0.jar;C:\Users\Dani.m2\repository\com\github\spullara\mustache\java\compiler\0.8.17\compiler-0.8.17.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-views-freemarker\0.8.0\dropwizard-views-freemarker-0.8.0.jar;C:\Users\Dani.m2\repository\org\freemarker\freemarker\2.3.21\freemarker-2.3.21.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-jackson\0.4-rc3.9\nd4j-jackson-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\deeplearning4j\deeplearning4j-core\0.4-rc3.9\deeplearning4j-core-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\io\netty\netty-buffer\4.0.28.Final\netty-buffer-4.0.28.Final.jar;C:\Users\Dani.m2\repository\io\netty\netty-common\4.0.28.Final\netty-common-4.0.28.Final.jar;C:\Users\Dani.m2\repository\org\nd4j\canova-api\0.0.0.15\canova-api-0.0.0.15.jar;C:\Users\Dani.m2\repository\org\slf4j\slf4j-api\1.7.12\slf4j-api-1.7.12.jar;C:\Users\Dani.m2\repository\ch\qos\logback\logback-classic\1.1.2\logback-classic-1.1.2.jar;C:\Users\Dani.m2\repository\ch\qos\logback\logback-core\1.1.2\logback-core-1.1.2.jar;C:\Users\Dani.m2\repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;C:\Users\Dani.m2\repository\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\Dani.m2\repository\org\apache\commons\commons-compress\1.8\commons-compress-1.8.jar;C:\Users\Dani.m2\repository\org\tukaani\xz\1.5\xz-1.5.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-api\0.4-rc3.9\nd4j-api-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-context\0.4-rc3.9\nd4j-context-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\apache\commons\commons-lang3\3.3.1\commons-lang3-3.3.1.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\core\jackson-core\2.5.1\jackson-core-2.5.1.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\core\jackson-databind\2.5.1\jackson-databind-2.5.1.jar;C:\Users\Dani.m2\repository\org\json\json\20131018\json-20131018.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\core\jackson-annotations\2.5.1\jackson-annotations-2.5.1.jar;C:\Users\Dani.m2\repository\org\projectlombok\lombok\1.16.4\lombok-1.16.4.jar;C:\Users\Dani.m2\repository\com\fasterxml\jackson\dataformat\jackson-dataformat-yaml\2.5.1\jackson-dataformat-yaml-2.5.1.jar;C:\Users\Dani.m2\repository\org\yaml\snakeyaml\1.12\snakeyaml-1.12.jar;C:\Users\Dani.m2\repository\org\deeplearning4j\deeplearning4j-ui\0.4-rc3.9\deeplearning4j-ui-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-client\0.8.0\dropwizard-client-0.8.0.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\core\jersey-client\2.16\jersey-client-2.16.jar;C:\Users\Dani.m2\repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;C:\Users\Dani.m2\repository\org\glassfish\hk2\hk2-api\2.4.0-b09\hk2-api-2.4.0-b09.jar;C:\Users\Dani.m2\repository\org\glassfish\hk2\hk2-utils\2.4.0-b09\hk2-utils-2.4.0-b09.jar;C:\Users\Dani.m2\repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b09\aopalliance-repackaged-2.4.0-b09.jar;C:\Users\Dani.m2\repository\org\glassfish\hk2\external\javax.inject\2.4.0-b09\javax.inject-2.4.0-b09.jar;C:\Users\Dani.m2\repository\org\glassfish\hk2\hk2-locator\2.4.0-b09\hk2-locator-2.4.0-b09.jar;C:\Users\Dani.m2\repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;C:\Users\Dani.m2\repository\io\dropwizard\metrics\metrics-httpclient\3.1.0\metrics-httpclient-3.1.0.jar;C:\Users\Dani.m2\repository\org\apache\httpcomponents\httpclient\4.3.5\httpclient-4.3.5.jar;C:\Users\Dani.m2\repository\org\apache\httpcomponents\httpcore\4.3.2\httpcore-4.3.2.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\connectors\jersey-apache-connector\2.16\jersey-apache-connector-2.16.jar;C:\Users\Dani.m2\repository\io\dropwizard\dropwizard-forms\0.8.0\dropwizard-forms-0.8.0.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\media\jersey-media-multipart\2.16\jersey-media-multipart-2.16.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\core\jersey-common\2.16\jersey-common-2.16.jar;C:\Users\Dani.m2\repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;C:\Users\Dani.m2\repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.16\jersey-guava-2.16.jar;C:\Users\Dani.m2\repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;C:\Users\Dani.m2\repository\org\jvnet\mimepull\mimepull\1.9.3\mimepull-1.9.3.jar;C:\Users\Dani.m2\repository\com\google\guava\guava\19.0\guava-19.0.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-native\0.4-rc3.9\nd4j-native-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-native\0.4-rc3.9\nd4j-native-0.4-rc3.9-windows-x86_64.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-native-api\0.4-rc3.9\nd4j-native-api-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\bytedeco\javacpp\1.2\javacpp-1.2.jar;C:\Users\Dani.m2\repository\org\nd4j\canova-nd4j-image\0.0.0.15\canova-nd4j-image-0.0.0.15.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-buffer\0.4-rc3.9\nd4j-buffer-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\nd4j\nd4j-common\0.4-rc3.9\nd4j-common-0.4-rc3.9.jar;C:\Users\Dani.m2\repository\org\reflections\reflections\0.9.10\reflections-0.9.10.jar;C:\Users\Dani.m2\repository\com\google\code\findbugs\annotations\2.0.1\annotations-2.0.1.jar;C:\Users\Dani.m2\repository\org\nd4j\canova-nd4j-common\0.0.0.15\canova-nd4j-common-0.0.0.15.jar;C:\Users\Dani.m2\repository\org\nd4j\canova-data-image\0.0.0.15\canova-data-image-0.0.0.15.jar;C:\Users\Dani.m2\repository\com\github\jai-imageio\jai-imageio-core\1.3.0\jai-imageio-core-1.3.0.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\imageio\imageio-jpeg\3.1.1\imageio-jpeg-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\imageio\imageio-core\3.1.1\imageio-core-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\imageio\imageio-metadata\3.1.1\imageio-metadata-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\common\common-lang\3.1.1\common-lang-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\common\common-io\3.1.1\common-io-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\common\common-image\3.1.1\common-image-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\imageio\imageio-tiff\3.1.1\imageio-tiff-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\imageio\imageio-psd\3.1.1\imageio-psd-3.1.1.jar;C:\Users\Dani.m2\repository\com\twelvemonkeys\imageio\imageio-bmp\3.1.1\imageio-bmp-3.1.1.jar;C:\Users\Dani.m2\repository\org\bytedeco\javacv\1.2\javacv-1.2.jar;C:\Users\Dani.m2\repository\org\bytedeco\javacpp-presets\opencv\3.1.0-1.2\opencv-3.1.0-1.2.jar;C:\Users\Dani.m2\repository\org\bytedeco\javacpp-presets\opencv\3.1.0-1.2\opencv-3.1.0-1.2-windows-x86.jar;C:\Users\Dani.m2\repository\org\nd4j\canova-nd4j-codec\0.0.0.15\canova-nd4j-codec-0.0.0.15.jar;C:\Users\Dani.m2\repository\org\jcodec\jcodec\0.1.5\jcodec-0.1.5.jar;C:\Users\Dani.m2\repository\jfree\jfreechart\1.0.13\jfreechart-1.0.13.jar;C:\Users\Dani.m2\repository\jfree\jcommon\1.0.16\jcommon-1.0.16.jar;C:\Program Files (x86)\JetBrains\IntelliJ IDEA Community Edition 2016.1.2\lib\idea_rt.jar" com.intellij.rt.execution.application.AppMain org.deeplearning4j.examples.nlp.paragraphvectors.ParagraphVectorsClassifierExample
o.d.m.s.SequenceVectors - Starting vocabulary building...
o.d.m.w.w.VocabConstructor - Sequences checked: [4438], Current vocabulary size: [27370]
o.d.m.s.SequenceVectors - Building learning algorithms:
o.d.m.s.SequenceVectors -           building ElementsLearningAlgorithm: [SkipGram]
o.d.m.s.SequenceVectors -           building SequenceLearningAlgorithm: [DBOW]
o.d.m.s.SequenceVectors - Starting learning process...
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [976072];  Lines vectorized so far: [4438]; learningRate: [0.001]
Exception in thread "main" java.lang.IllegalArgumentException: Length must be &gt;= 1
at org.nd4j.linalg.api.buffer.BaseDataBuffer.(BaseDataBuffer.java:537)
at org.nd4j.linalg.api.buffer.FloatBuffer.(FloatBuffer.java:40)
at org.nd4j.linalg.api.buffer.factory.DefaultDataBufferFactory.createFloat(DefaultDataBufferFactory.java:227)
at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1161)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:225)
at org.nd4j.linalg.cpu.nativecpu.NDArray.(NDArray.java:107)
at org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.create(CpuNDArrayFactory.java:239)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4030)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3996)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3270)
at org.deeplearning4j.examples.nlp.paragraphvectors.tools.MeansBuilder.documentAsVector(MeansBuilder.java:44)
at org.deeplearning4j.examples.nlp.paragraphvectors.ParagraphVectorsClassifierExample.checkUnlabeledData(ParagraphVectorsClassifierExample.java:118)
at org.deeplearning4j.examples.nlp.paragraphvectors.ParagraphVectorsClassifierExample.main(ParagraphVectorsClassifierExample.java:50)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
Process finished with exit code 1`
This is a picture of my resources:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/10819041/15680913/a3a79c34-2757-11e6-8328-16e9ef89b154.png&gt;&lt;/denchmark-link&gt;

an example of the 800 files that are each folder of the unlabeled folders is:
"6406_The Witcher 3 Wild Hunt"
or
"9799_DARK SOULS III"
I have used other names to mask the purpose of the data that I am analyzing, but i am given the same error if I use those.
Thanks for your time.
	</description>
	<comments>
		<comment id='1' author='DYelamos' date='2016-05-31T15:52:15Z'>
		Show me, how exactly you've configured ParagraphVectors.
		</comment>
		<comment id='2' author='DYelamos' date='2016-05-31T15:55:44Z'>
		public class ParagraphVectorsClassifierExample {
&lt;denchmark-code&gt;ParagraphVectors paragraphVectors;
LabelAwareIterator iterator;
TokenizerFactory tokenizerFactory;

private static final Logger log = LoggerFactory.getLogger(ParagraphVectorsClassifierExample.class);

public static void main(String[] args) throws Exception {

  ParagraphVectorsClassifierExample app = new ParagraphVectorsClassifierExample();
  app.makeParagraphVectors();
  app.checkUnlabeledData();
    /*
            Your output should be like this:

            Document 'health' falls into the following categories:
                health: 0.29721372296220205
                science: 0.011684473733853906
                finance: -0.14755302887323793

            Document 'finance' falls into the following categories:
                health: -0.17290237675941766
                science: -0.09579267574606627
                finance: 0.4460859189453788

                so,now we know categories for yet unseen documents
     */
}

void makeParagraphVectors()  throws Exception {
  ClassPathResource resource = new ClassPathResource("paravec/labeled");

  // build a iterator for our dataset
  iterator = new FileLabelAwareIterator.Builder()
          .addSourceFolder(resource.getFile())
          .build();

  tokenizerFactory = new DefaultTokenizerFactory();
  tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());

  // ParagraphVectors training configuration
  paragraphVectors = new ParagraphVectors.Builder()
          .learningRate(0.025)
          .minLearningRate(0.001)
          .batchSize(1000)
          .epochs(1)
          .iterate(iterator)
          .trainWordVectors(true)
          .tokenizerFactory(tokenizerFactory)
          .build();

  // Start model training
  paragraphVectors.fit();
}

void checkUnlabeledData() throws FileNotFoundException {
  /*
  At this point we assume that we have model built and we can check
  which categories our unlabeled document falls into.
  So we'll start loading our unlabeled documents and checking them
 */
 ClassPathResource unClassifiedResource = new ClassPathResource("paravec/unlabeled");
 FileLabelAwareIterator unClassifiedIterator = new FileLabelAwareIterator.Builder()
         .addSourceFolder(unClassifiedResource.getFile())
         .build();

 /*
  Now we'll iterate over unlabeled data, and check which label it could be assigned to
  Please note: for many domains it's normal to have 1 document fall into few labels at once,
  with different "weight" for each.
 */
 MeansBuilder meansBuilder = new MeansBuilder(
     (InMemoryLookupTable&lt;VocabWord&gt;)paragraphVectors.getLookupTable(),
       tokenizerFactory);
 LabelSeeker seeker = new LabelSeeker(iterator.getLabelsSource().getLabels(),
     (InMemoryLookupTable&lt;VocabWord&gt;) paragraphVectors.getLookupTable());

 while (unClassifiedIterator.hasNextDocument()) {
     LabelledDocument document = unClassifiedIterator.nextDocument();
     INDArray documentAsCentroid = meansBuilder.documentAsVector(document);
     List&lt;Pair&lt;String, Double&gt;&gt; scores = seeker.getScores(documentAsCentroid);

     /*
      please note, document.getLabel() is used just to show which document we're looking at now,
      as a substitute for printing out the whole document name.
      So, labels on these two documents are used like titles,
      just to visualize our classification done properly
     */
     Writer writer = null;
     try {
         writer = new BufferedWriter(new OutputStreamWriter(
             new FileOutputStream("log_first_try.txt"), "utf-8"));
         writer.write("Document '" + document.getLabel() + "' falls into the following categories: \n" );
         for (Pair&lt;String, Double&gt; score: scores) {
             writer.write("        " + score.getFirst() + ": " + score.getSecond()+"\n");

         }
         writer.write("With text:" +document.toString() +"\n\n");
     } catch (IOException ex) {
         // report
     } finally {
         try {
             writer.close();
         } catch (Exception ex) {/*ignore*/
         }
     }

 }

}
&lt;/denchmark-code&gt;

}
		</comment>
		<comment id='3' author='DYelamos' date='2016-05-31T15:58:03Z'>
		rgr, thanks.
		</comment>
		<comment id='4' author='DYelamos' date='2016-05-31T16:36:52Z'>
		just to be concise, i only made it 1 epoch long cause it takes a lot of time(around 4 hours) to complete the full 20. I made it 1 epoch long so that it would be shorter and i could check what was going on.
		</comment>
		<comment id='5' author='DYelamos' date='2016-05-31T20:26:24Z'>
		changing the data type to files with underscores only did not help.
		</comment>
		<comment id='6' author='DYelamos' date='2016-05-31T20:27:53Z'>
		It has nothing to do with your data.
		</comment>
		<comment id='7' author='DYelamos' date='2016-05-31T20:37:55Z'>
		Ow okay, can i help in any way?
		</comment>
		<comment id='8' author='DYelamos' date='2016-05-31T20:39:15Z'>
		No, just wait. After i finish current high-priority tasks i'll get to your issue :)
P.S. i mean tomorrow i'll take closer look there :)
		</comment>
		<comment id='9' author='DYelamos' date='2016-05-31T20:40:08Z'>
		Cheers mate. If you need anything letme know
		</comment>
		<comment id='10' author='DYelamos' date='2016-06-01T14:41:48Z'>
		So here's the problem:
You should train your model on both labeled/unlabeled data, so all words available in vocab. And your exception appears because not a single word from document was found in vocab.
		</comment>
		<comment id='11' author='DYelamos' date='2016-06-01T14:50:49Z'>
		oh really? but that doesnt make sense, if i receive a new word, then i will always receive this exception?
		</comment>
		<comment id='12' author='DYelamos' date='2016-06-01T14:51:29Z'>
		&lt;denchmark-link:https://github.com/DYelamos&gt;@DYelamos&lt;/denchmark-link&gt;
 it SHOULD be unk for new words.
		</comment>
		<comment id='13' author='DYelamos' date='2016-06-01T14:52:09Z'>
		&lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 I do know understand what you mean
		</comment>
		<comment id='14' author='DYelamos' date='2016-06-01T14:57:03Z'>
		but there is no way my document does not contain even a single word, i have 40,000 paragraphs on my labeled data, there is no way that my review does not contain even a single word on my dictonary
		</comment>
		<comment id='15' author='DYelamos' date='2016-06-01T15:31:20Z'>
		I've only said, that you should check - which path you're passing into iterator. Because it's REALLY unlikely to get 0 words hit in vocab.
		</comment>
		<comment id='16' author='DYelamos' date='2016-06-01T15:33:37Z'>
		The path is exactly as you had it in your example, i considered that too, i made one try, adding every sample to the train partition(doing 100% and then checking with itself, a bit sloppy on my side, but just to check that you are right) and it is actually correct. Could it be that the files contain non UTF-8 characters?
		</comment>
		<comment id='17' author='DYelamos' date='2016-06-01T15:37:19Z'>
		I'm having the same error and narrowed the problem down to the accent in the french word "implémenter".  Removing the accented "e" prevented the problem from happening.
		</comment>
		<comment id='18' author='DYelamos' date='2016-06-01T15:38:04Z'>
		That's surprising. Hold on there, i'll take a look again. That shouldn't ever affect anything.
		</comment>
		<comment id='19' author='DYelamos' date='2016-06-01T15:40:24Z'>
		Shite, well that is a HUGE problem for me, I am working with binomials, trinomials and others, and with at least 2 different languages that both have accents. Should i treat my text to remove every non ascii character from there?
		</comment>
		<comment id='20' author='DYelamos' date='2016-06-01T15:41:18Z'>
		No, you don't have to do that. As i've said earlier - that shouldn't ever happen.
Are you using windows by any chance?
		</comment>
		<comment id='21' author='DYelamos' date='2016-06-01T15:41:35Z'>
		I just took a look at my stack traces; and I was working my through narrowing down the problem.  One of my tests I went from having the stack in this post i.e.
java.lang.IllegalArgumentException: Length must be &gt;= 1
at org.nd4j.linalg.api.buffer.BaseDataBuffer.(BaseDataBuffer.java:540)
at org.nd4j.linalg.api.buffer.FloatBuffer.(FloatBuffer.java:40)
at org.nd4j.linalg.api.buffer.factory.DefaultDataBufferFactory.createFloat(DefaultDataBufferFactory.java:227)
at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1157)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:225)
at org.nd4j.linalg.cpu.NDArray.(NDArray.java:114)
at org.nd4j.linalg.cpu.CpuNDArrayFactory.create(CpuNDArrayFactory.java:234)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4026)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3992)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3266)
at MeansBuilder.documentAsVector(:31)
to having
o.d.c.a.BaseClusteringAlgorithm - Completed clustering iteration 4
java.nio.charset.MalformedInputException: Input length = 1
at java.nio.charset.CoderResult.throwException(CoderResult.java:281)
at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:339)
at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
at java.io.InputStreamReader.read(InputStreamReader.java:184)
at java.io.BufferedReader.fill(BufferedReader.java:161)
at java.io.BufferedReader.readLine(BufferedReader.java:324)
at java.io.BufferedReader.readLine(BufferedReader.java:389)
at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:72)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at .clusterInputTextFile(:46)
at .generateClusterFiles(:47)
at .clusterData(:69)
I didn't notice the stacks changing.  The second stack was prevented by removing the accented e.  I'll go back and narrow down exactly what caused the first stack and repost.
		</comment>
		<comment id='22' author='DYelamos' date='2016-06-01T15:43:26Z'>
		yes i am using windows. windows 10 to be precise, would that be a problem?
Would this be more stable on any linux distribution?
		</comment>
		<comment id='23' author='DYelamos' date='2016-06-01T15:44:20Z'>
		Force your app to use UTF-8 as encoding, and check if that helps.
		</comment>
		<comment id='24' author='DYelamos' date='2016-06-01T15:45:48Z'>
		it does use utf-8, but that includes stuff like hearts and japanese kanjis for example. Would those be an issue?
		</comment>
		<comment id='25' author='DYelamos' date='2016-06-01T15:46:30Z'>
		No, please force app to use UTF-8 explicitly.
		</comment>
		<comment id='26' author='DYelamos' date='2016-06-01T16:07:00Z'>
		Would this suffice.
text=text.replaceAll("[^\\\\p{ASCII}]","");
I am parsing every file to look like this.
would this work?
My files are created like this:
&lt;denchmark-code&gt;
writer = new BufferedWriter(new OutputStreamWriter(
                                new FileOutputStream("./Games/" + gameName + "/" + fileTitle), "utf-8"));
&lt;/denchmark-code&gt;

Which means they are already encoded as such...
		</comment>
		<comment id='27' author='DYelamos' date='2016-06-01T16:20:51Z'>
		btw adding every file to the folder actually worked and i got output
		</comment>
		<comment id='28' author='DYelamos' date='2016-06-01T16:48:34Z'>
		I was able to narrow at least two instances of the stack to being caused by two different German words; these words were not likely in the data when the pargraphVector model was built.  It looks like calling getWordVector on each of these words results in an empty array which is likely genesis of the stack.
scala&gt; paragraphVectors.getWordVector("suchergebnisse")
res45: Array[Double] = null
scala&gt;  paragraphVectors.getWordVector("unvollstndige")
res47: Array[Double] = null
scala&gt; paragraphVectors.getWordVector("test")
res46: Array[Double] = Array(-0.030363162979483604, 0.0453033521771431, -0.04195807874202728,.....
Stack:
java.lang.IllegalArgumentException: Length must be &gt;= 1
at org.nd4j.linalg.api.buffer.BaseDataBuffer.(BaseDataBuffer.java:540)
at org.nd4j.linalg.api.buffer.FloatBuffer.(FloatBuffer.java:40)
at org.nd4j.linalg.api.buffer.factory.DefaultDataBufferFactory.createFloat(DefaultDataBufferFactory.java:227)
at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1157)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:225)
at org.nd4j.linalg.cpu.NDArray.(NDArray.java:114)
at org.nd4j.linalg.cpu.CpuNDArrayFactory.create(CpuNDArrayFactory.java:234)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4026)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3992)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3266)
at MeansBuilder.documentAsVector(:31)
at .getClusterCenterAndInputs(:48)
at $anonfun$clusterInputTextFile$2.apply(:49)
at $anonfun$clusterInputTextFile$2.apply(:46)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at .clusterInputTextFile(:46)
at .generateClusterFiles(:47)
at .clusterData(:69)
... 36 elided
		</comment>
		<comment id='29' author='DYelamos' date='2016-06-01T17:02:22Z'>
		&lt;denchmark-link:https://github.com/valncrt&gt;@valncrt&lt;/denchmark-link&gt;
 thanks a lot, that helps
		</comment>
		<comment id='30' author='DYelamos' date='2016-06-02T01:05:39Z'>
		I did some more testing and for my test case I'm passing in sentences/lines of text and there are two conditions where I get the stack.

All the words in the sentence/line (in my case two german words only) don't exist in the paragraphVec model the error occurs
there are no words on the line (my case there was a single space and a new line)

The workaround is to catch these cases and adjust the incoming text to something which will be found.  For my use case appending a neutral word like "test" will work for other use cases this may not be appropriate.  I think the code should catch each of these conditions and issue a warning that the text "such and such" has been ignored and continue on processing.  My workaround in the MeansBuilder.documentAsVector method is below (in scala) a 'for' loop in java should do the equivalent thing.
From:
val documentAsTokens: java.util.List[String]  = tokenizerFactory.create(document.getContent()).getTokens()
To: (check for incoming words not in the model, also check for single space and double space followed by a newline character, in each case replace occurrences with the string "test" - which is in the model)
val documentAsTokensComplete: java.util.List[String]  = tokenizerFactory.create(document.getContent()).getTokens()
val documentAsTokens = documentAsTokensComplete.asScala.map( x =&gt; if(paragraphVectors.getWordVector(x) == null)  "test" else x) .map( x =&gt; if( x == " \n")  "test" else x) .map( x =&gt; if( x == "  \n")  "test" else x)
		</comment>
		<comment id='31' author='DYelamos' date='2016-06-02T01:54:54Z'>
		The above regular expressions to remove lines with no words didn't work correctly.  I removed the empty lines them source data instead.  This is the MeansBuild class I ended up using which worked.
import scala.collection.JavaConverters._
import org.deeplearning4j.models.paragraphvectors.ParagraphVectors
import java.util.concurrent.atomic.AtomicInteger
import org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable
import org.deeplearning4j.models.word2vec.VocabWord
import org.deeplearning4j.text.documentiterator.LabelledDocument
import org.deeplearning4j.text.tokenization.tokenizerfactory.TokenizerFactory
import org.nd4j.linalg.factory.Nd4j
import org.nd4j.linalg.api.ndarray.INDArray
import scala.collection.mutable.StringBuilder
class MeansBuilder(lookupTable: InMemoryLookupTable[VocabWord], tokenizerFactory: TokenizerFactory,paragraphVectors:ParagraphVectors) {
private val vocabCache = lookupTable.getVocab
def documentAsVector(document: LabelledDocument): INDArray = {
// println("New line being processed....")
val documentAsTokensComplete: java.util.List[String]  = tokenizerFactory.create(document.getContent()).getTokens()
val documentAsTokens = documentAsTokensComplete.asScala.map( x =&gt; if(paragraphVectors.getWordVector(x) == null)  "test" else x)
val cnt: AtomicInteger = new AtomicInteger(0)
documentAsTokens.foreach { word =&gt;
println("Word is: "+ word)
// println("Word has token?: "+ vocabCache.hasToken(word))
if (vocabCache.containsWord(word)) cnt.incrementAndGet()
}
// val cnt1 = if(cnt.get() ==0) 1   else cnt.get() //This is a workaround I came up with for bug &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/1623&gt;https://github.com/deeplearning4j/deeplearning4j/issues/1623&lt;/denchmark-link&gt;

val allWords: INDArray = Nd4j.create(cnt.get(), lookupTable.layerSize())
cnt.set(0)
documentAsTokens.foreach { word =&gt;
if (vocabCache.containsWord(word))
allWords.putRow(cnt.getAndIncrement(), lookupTable.vector(word))
}
allWords.mean(0)
}
}
		</comment>
		<comment id='32' author='DYelamos' date='2016-06-07T16:13:54Z'>
		Closing issue:
When singling out the corpus that had only 1 word repeated over and over again, the issue was solved.
Cheers
		</comment>
		<comment id='33' author='DYelamos' date='2016-06-07T16:15:22Z'>
		So, you've found the way to reproduce that issue?
Can i have detailed instructions, how to reproduce your issue?
		</comment>
		<comment id='34' author='DYelamos' date='2016-06-07T16:25:34Z'>
		Yes, my issue happened as one of my 4000 reviews was(and i am quoting):
"clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap__clap"
which was only 1 word so once the neural network was supplied with this, it gave me the error that you pointed out, which was that my paragraph had no known words. After i removed special characters and accepted only alphanumerics, question marks, and other more "normal" characters it worked out fine.
		</comment>
		<comment id='35' author='DYelamos' date='2016-06-07T16:26:17Z'>
		Don't close an issue please, since it's definitely a bug and should be verified, confirmed and fixed.
		</comment>
		<comment id='36' author='DYelamos' date='2016-06-07T16:26:39Z'>
		apologies
		</comment>
		<comment id='37' author='DYelamos' date='2016-06-07T16:27:55Z'>
		So, in other words, there's edge case, when minWordFrequency meets 1-word document, with disabled unks. Should be easy fix.
		</comment>
		<comment id='38' author='DYelamos' date='2016-06-07T16:37:46Z'>
		Yes that is exactly what i found the problem to be. Also, words in spanish sometimes caused exceptions, after turning words like tarántula into tarantula(same word in English but i am sure you get my point) it became a lot more stable
		</comment>
		<comment id='39' author='DYelamos' date='2016-11-30T14:35:58Z'>
		Update on this?
		</comment>
		<comment id='40' author='DYelamos' date='2016-11-30T14:39:23Z'>
		It was supposed to be fixed. The problem was that at some point, i found a 1 word document(basically my parser took a comment that was" HAHAHAHAHAHAHAHAHAAH" for almost a page) and tried to process it. When it does, it fails, I do suppose it is its intended behavior, but raver119 should be able to bring you some more insight into this
		</comment>
		<comment id='41' author='DYelamos' date='2016-11-30T14:40:34Z'>
		Thanks for the update!
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Nov 30, 2016 22:39, "beeva-danielyelamos" ***@***.***&gt; wrote:
 It was supposed to be fixed. The problem was that at some point, i found a
 1 word document(basically my parser took a comment that was"
 HAHAHAHAHAHAHAHAHAAH" for almost a page) and tried to process it. When it
 does, it fails, I do suppose it is its intended behavior, but raver119
 should be able to bring you some more insight into this

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/1623#issuecomment-263889334&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABF18vCbbitHQJbkIrsZlr2vEnNxCUwXks5rDYqdgaJpZM4IqrPF&gt;
 .



		</comment>
		<comment id='42' author='DYelamos' date='2016-11-30T14:41:21Z'>
		Mmmm, it shouldn't be an issue for quite some time now. Especially after overhaul happened with 0.7.0. Could you please check that and confirm/decline?
		</comment>
		<comment id='43' author='DYelamos' date='2016-11-30T14:44:13Z'>
		I am no longer working on that project mate. I could try to run the project but it has been sitting in a repo dead for over 4 months now. If there has been a significant overhaul my code might not work anymore. I could, however download a new project and make a document that fits that description in my free time this weekend.
		</comment>
		<comment id='44' author='DYelamos' date='2016-11-30T14:44:26Z'>
		Thanks!
		</comment>
		<comment id='45' author='DYelamos' date='2016-11-30T14:45:25Z'>
		I will try to get back to you this weekend, till then
		</comment>
		<comment id='46' author='DYelamos' date='2018-09-22T15:14:04Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>