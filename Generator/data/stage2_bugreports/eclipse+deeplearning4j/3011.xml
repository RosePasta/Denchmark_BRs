<bug id='3011' author='fac2003' open_date='2017-03-09T18:57:22Z' closed_time='2017-03-11T16:12:26Z'>
	<summary>MemcpyAsync failed on latest master with ParallelWrapper</summary>
	<description>
Training a model with 4 GPUs with ParallelWrapper fails with various exceptions.
Here's an output log with the errors:
&lt;denchmark-link:http://pastebin.com/ZQ4JNLce&gt;http://pastebin.com/ZQ4JNLce&lt;/denchmark-link&gt;

This is the CudaEnvironment config:
&lt;denchmark-code&gt;    final long GB = 1024 * 1024 * 1024L;
    CudaEnvironment.getInstance().getConfiguration()
            .enableDebug(false)
            .allowMultiGPU(true)
            .setMaximumGridSize(1024)
            .setMaximumBlockSize(1024)
            .setMaximumDeviceCacheableLength(1*GB)
            .setMaximumDeviceCache(4L * GB)
            .setMaximumHostCacheableLength(1*GB)
            .setMaximumHostCache(16L *GB)
            // cross-device access is used for faster model averaging over pcie
            .allowCrossDeviceAccess(true);
&lt;/denchmark-code&gt;

and the ParallelWrapper config:
wrapper = new ParallelWrapper.Builder(graph)
.prefetchBuffer(1)
.workers(4)
.averagingFrequency(1)
.reportScoreAfterAveraging(false)
.useLegacyAveraging(false)
.useMQ(true)
.build();
	</description>
	<comments>
		<comment id='1' author='fac2003' date='2017-03-09T19:48:33Z'>
		I need to see nn configuration.
P.s. prefetchBuffer of 1 doesn't make too much sense...
		</comment>
		<comment id='2' author='fac2003' date='2017-03-09T19:49:36Z'>
		Please enable debug mode, and provide new output.
		</comment>
		<comment id='3' author='fac2003' date='2017-03-09T19:58:33Z'>
		Do you mean   CudaEnvironment.getInstance().getConfiguration()
.enableDebug(true)
or another debug?
		</comment>
		<comment id='4' author='fac2003' date='2017-03-09T20:02:56Z'>
		Yes, exactly that one.
		</comment>
		<comment id='5' author='fac2003' date='2017-03-09T23:38:05Z'>
		Any news there? Fix helped?
		</comment>
		<comment id='6' author='fac2003' date='2017-03-10T01:31:02Z'>
		I got a similar error unfortunately. Possibly a different bug. I will rerun the build overnight to double check.
		</comment>
		<comment id='7' author='fac2003' date='2017-03-10T04:21:58Z'>
		Same error log as before. Best would be to try running the example I packaged and sent you earlier.
		</comment>
		<comment id='8' author='fac2003' date='2017-03-10T18:05:21Z'>
		So, TL/DR here: EarlyStopping model is run through conventional ParallelWrapper, thus causing boom. Further investigations to be done. But besides of that - EarlyStoppingParallelWrapper should be used here.
		</comment>
		<comment id='9' author='fac2003' date='2017-03-10T18:16:33Z'>
		I don't think so, for the reasons I stated by email. We don't use the dl4j
early stopping mechanisms and call ParallelWrapper with an entire epoch.
The code runs fine in parallel for 39 epochs with a minibatch of 2048, then
crashes. The code crashes immediately with a minibatch of 32.
On Fri, Mar 10, 2017, 13:05 raver119 ***@***.***&gt; wrote:
 So, TL/DR here: EarlyStopping model is run through conventional
 ParallelWrapper, thus causing boom. Further investigations to be done. But
 besides of that - EarlyStoppingParallelWrapper should be used here.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/3011#issuecomment-285741132&gt;https://github.com/deeplearning4j/deeplearning4j/issues/3011#issuecomment-285741132&lt;/denchmark-link&gt;
&gt;,
 or mute the thread
 &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/ABpfl-gO-JiDsiaRVvT-VWgnHXT06xr7ks5rkZDwgaJpZM4MYfU4&gt;https://github.com/notifications/unsubscribe-auth/ABpfl-gO-JiDsiaRVvT-VWgnHXT06xr7ks5rkZDwgaJpZM4MYfU4&lt;/denchmark-link&gt;
&gt;
 .

-- 

Fabien Campagne, PhD -- &lt;denchmark-link:http://campagnelab.org&gt;http://campagnelab.org&lt;/denchmark-link&gt;


                                          @FabienCampagne
&lt;&lt;denchmark-link:https://twitter.com/FabienCampagne&gt;https://twitter.com/FabienCampagne&lt;/denchmark-link&gt;
&gt;

Assistant Professor, Dept. of Physiology and Biophysics

                         Institute for Computational Biomedicine

Associate Director,   Biomedical Informatics Core,

                      Clinical Translational Science Center

Weill Medical College of Cornell University

phone:  (646)-962-5613  1305 York Avenue

fax:    (646)-962-0383  Box 140

New York, NY 10021

Learn about Data Analysis with MetaR. &lt;denchmark-link:http://metaR.campagnelab.org&gt;http://metaR.campagnelab.org&lt;/denchmark-link&gt;

&lt;&lt;denchmark-link:http://metar.campagnelab.org/&gt;http://metar.campagnelab.org/&lt;/denchmark-link&gt;
&gt;
		</comment>
		<comment id='10' author='fac2003' date='2017-03-10T18:28:01Z'>
		You're clearly using dl4j EarlyStopping:

18:45:32.635 [main] DEBUG o.d.optimize.solvers.BaseOptimizer - Hit termination condition on iteration 0: score=Infinity, oldScore=8.155203382669457, condition=org.deeplearning4j.optimize.terminations.EpsTermination@7f934ed6

So right now there's no sense digging into your code. Instead of that, i'll try to write compact code that reproduces this behavior, and will fix it after that.
		</comment>
		<comment id='11' author='fac2003' date='2017-03-10T18:36:27Z'>
		This might be confusing. We use one early stopping result class as a data
structure to return a score with a model. It is not used in any other way.
The log may be confusing if we assume we use the full early stopping
mechanisms. We do not. I can replace this class with a pojo if it would
make you feel better.
On Fri, Mar 10, 2017, 13:28 raver119 ***@***.***&gt; wrote:
 You're clearly using dl4j EarlyStopping:

 18:45:32.635 [main] DEBUG o.d.optimize.solvers.BaseOptimizer - Hit
 termination condition on iteration 0: score=Infinity,
 oldScore=8.155203382669457,
 ***@***.***

 So right now there's no sense digging into your code. Instead of that,
 i'll try to write compact code that reproduces this behavior, and will fix
 it after that.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/3011#issuecomment-285746829&gt;https://github.com/deeplearning4j/deeplearning4j/issues/3011#issuecomment-285746829&lt;/denchmark-link&gt;
&gt;,
 or mute the thread
 &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/ABpfly9sifXTrdjqkB0gH8kQz-WpzTnFks5rkZZBgaJpZM4MYfU4&gt;https://github.com/notifications/unsubscribe-auth/ABpfly9sifXTrdjqkB0gH8kQz-WpzTnFks5rkZZBgaJpZM4MYfU4&lt;/denchmark-link&gt;
&gt;
 .

-- 

Fabien Campagne, PhD -- &lt;denchmark-link:http://campagnelab.org&gt;http://campagnelab.org&lt;/denchmark-link&gt;


                                          @FabienCampagne
&lt;&lt;denchmark-link:https://twitter.com/FabienCampagne&gt;https://twitter.com/FabienCampagne&lt;/denchmark-link&gt;
&gt;

Assistant Professor, Dept. of Physiology and Biophysics

                         Institute for Computational Biomedicine

Associate Director,   Biomedical Informatics Core,

                      Clinical Translational Science Center

Weill Medical College of Cornell University

phone:  (646)-962-5613  1305 York Avenue

fax:    (646)-962-0383  Box 140

New York, NY 10021

Learn about Data Analysis with MetaR. &lt;denchmark-link:http://metaR.campagnelab.org&gt;http://metaR.campagnelab.org&lt;/denchmark-link&gt;

&lt;&lt;denchmark-link:http://metar.campagnelab.org/&gt;http://metar.campagnelab.org/&lt;/denchmark-link&gt;
&gt;
		</comment>
		<comment id='12' author='fac2003' date='2017-03-10T18:39:16Z'>
		That's why i'm speaking about reproducible code.
		</comment>
		<comment id='13' author='fac2003' date='2017-03-10T18:44:40Z'>
		The problem is reproducible in the code I sent. It fails every time. The
code may be new and confusing to you, but it is very efficient and this may
be the reason we get into some race condition of sorts.
On Fri, Mar 10, 2017, 13:39 raver119 ***@***.***&gt; wrote:
 That's why i'm speaking about reproducible code.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/3011#issuecomment-285749632&gt;https://github.com/deeplearning4j/deeplearning4j/issues/3011#issuecomment-285749632&lt;/denchmark-link&gt;
&gt;,
 or mute the thread
 &lt;&lt;denchmark-link:https://github.com/notifications/unsubscribe-auth/ABpfl8ZY0Q3CwxgX1CxuTLK44Rrunlodks5rkZjlgaJpZM4MYfU4&gt;https://github.com/notifications/unsubscribe-auth/ABpfl8ZY0Q3CwxgX1CxuTLK44Rrunlodks5rkZjlgaJpZM4MYfU4&lt;/denchmark-link&gt;
&gt;
 .

-- 

Fabien Campagne, PhD -- &lt;denchmark-link:http://campagnelab.org&gt;http://campagnelab.org&lt;/denchmark-link&gt;


                                          @FabienCampagne
&lt;&lt;denchmark-link:https://twitter.com/FabienCampagne&gt;https://twitter.com/FabienCampagne&lt;/denchmark-link&gt;
&gt;

Assistant Professor, Dept. of Physiology and Biophysics

                         Institute for Computational Biomedicine

Associate Director,   Biomedical Informatics Core,

                      Clinical Translational Science Center

Weill Medical College of Cornell University

phone:  (646)-962-5613  1305 York Avenue

fax:    (646)-962-0383  Box 140

New York, NY 10021

Learn about Data Analysis with MetaR. &lt;denchmark-link:http://metaR.campagnelab.org&gt;http://metaR.campagnelab.org&lt;/denchmark-link&gt;

&lt;&lt;denchmark-link:http://metar.campagnelab.org/&gt;http://metar.campagnelab.org/&lt;/denchmark-link&gt;
&gt;
		</comment>
		<comment id='14' author='fac2003' date='2017-03-10T19:14:14Z'>
		Yep, totally agree, your code is reproducing issue with 100% chance.
But there's 3 options:

bug in ParallelWrapper
bug in CUDA backend
bug in your code.

Right now, i clearly see that EarlyStopping model IS used. Within conventional ParallelWrapper. Despite you decline that. Take a look at this output. Termination messages are coming from ParallelWrapper trainer threads.
&lt;denchmark-code&gt;22:09:43.084 [ParallelWrapper trainer 0] INFO  o.d.o.listeners.PerformanceListener - iteration 1; iteration time: 0 ms; samples/sec: Infinity; batches/sec: Infinity; score: 8.219831531962942;
22:09:43.090 [ParallelWrapper trainer 1] INFO  o.d.o.listeners.PerformanceListener - iteration 1; iteration time: 0 ms; samples/sec: Infinity; batches/sec: Infinity; score: 8.445574060647667;
22:09:43.185 [ParallelWrapper trainer 1] DEBUG o.d.optimize.solvers.BaseOptimizer - Hit termination condition on iteration 0: score=Infinity, oldScore=8.445574060647667, condition=org.deeplearning4j.optimize.terminations.EpsTermination@dea4b46
22:09:43.188 [ParallelWrapper trainer 0] DEBUG o.d.optimize.solvers.BaseOptimizer - Hit termination condition on iteration 0: score=Infinity, oldScore=8.219831531962942, condition=org.deeplearning4j.optimize.terminations.EpsTermination@456681
22:09:43.438 [ParallelWrapper trainer 0] INFO  o.d.o.listeners.PerformanceListener - iteration 6; iteration time: 23 ms; samples/sec: 1391.304; batches/sec: 43.478; score: NaN;
22:09:43.440 [ParallelWrapper trainer 1] INFO  o.d.o.listeners.PerformanceListener - iteration 6; iteration time: 21 ms; samples/sec: 1523.810; batches/sec: 47.619; score: NaN;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='fac2003' date='2017-03-10T19:24:43Z'>
		I have a feeling we are not talking about the same thing. You call EpsTermination early stopping. For us it's something else (stopping training when validation performance starts to go up).
Also what do you mean exactly by "Early Stopping model". We use a computational graph, I was not aware they had an "early stopping" property. In other words, what methods called on a computationgraph would cause it to become "Early Stopping"?
		</comment>
		<comment id='16' author='fac2003' date='2017-03-10T19:56:49Z'>
		We're talking about the same thing: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/tree/1f8af820c29cc5567a2c5eaa290f094c4d1492a7/deeplearning4j-nn/src/main/java/org/deeplearning4j/earlystopping/trainer&gt;https://github.com/deeplearning4j/deeplearning4j/tree/1f8af820c29cc5567a2c5eaa290f094c4d1492a7/deeplearning4j-nn/src/main/java/org/deeplearning4j/earlystopping/trainer&lt;/denchmark-link&gt;

Stuff from here is used to do "stopping training when validation performance starts to go up" - that's what causes to become model EarlyStopping. And those TerminationConditions messages - is a sign that model was configured to be early stopped.
But problem is - that stuff isn't thread safe. I don't know, at this moment, when exactly things go wrong, i just know that it's happening due to EarlyStopping.
		</comment>
		<comment id='17' author='fac2003' date='2017-03-10T19:57:23Z'>
		I mean - there's a reason there's special ParallelWrapper implementation for EarlyStopping.
		</comment>
		<comment id='18' author='fac2003' date='2017-03-10T20:35:24Z'>
		&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-scaleout/deeplearning4j-scaleout-parallelwrapper/src/main/java/org/deeplearning4j/parallelism/EarlyStoppingParallelTrainer.java&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-scaleout/deeplearning4j-scaleout-parallelwrapper/src/main/java/org/deeplearning4j/parallelism/EarlyStoppingParallelTrainer.java&lt;/denchmark-link&gt;

If you have any UX issues or if it misbehaves, please let me know I'll fix it.
		</comment>
		<comment id='19' author='fac2003' date='2017-03-11T16:12:25Z'>
		Issue is fixed.
		</comment>
		<comment id='20' author='fac2003' date='2018-10-02T03:22:30Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>