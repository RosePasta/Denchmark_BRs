<bug id='6691' author='AlexDBlack' open_date='2018-11-09T11:51:17Z' closed_time='2018-11-21T00:23:17Z'>
	<summary>DL4J CuDNN: don't fallback if exception is memory error</summary>
	<description>
CuDNN implementations fall back on built-in implementation if an exception is encountered during CuDNN execution.
This makes sense, except for when it's a memory issue, at which point we should simply propagate the exception immediately.
&lt;denchmark-code&gt;2018-11-09 11:45:27 WARN  BatchNormalization:370 - CuDNN BatchNormalization forward pass execution failed - falling back on built-in implementation
java.lang.RuntimeException: Failed to allocate 411041792 bytes from DEVICE [0] memory
	at org.nd4j.jita.memory.CudaMemoryManager.allocate(CudaMemoryManager.java:76)
	at org.nd4j.jita.workspace.CudaWorkspace.alloc(CudaWorkspace.java:213)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:471)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:416)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:255)
	at org.nd4j.linalg.jcublas.buffer.CudaFloatDataBuffer.&lt;init&gt;(CudaFloatDataBuffer.java:61)
	at org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createFloat(CudaDataBufferFactory.java:331)
	at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1500)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.&lt;init&gt;(BaseNDArray.java:285)
	at org.nd4j.linalg.jcublas.JCublasNDArray.&lt;init&gt;(JCublasNDArray.java:120)
	at org.nd4j.linalg.jcublas.JCublasNDArrayFactory.createUninitialized(JCublasNDArrayFactory.java:165)
	at org.nd4j.linalg.factory.Nd4j.createUninitialized(Nd4j.java:4442)
	at org.nd4j.linalg.workspace.BaseWorkspaceMgr.createUninitialized(BaseWorkspaceMgr.java:288)
	at org.deeplearning4j.nn.layers.normalization.CudnnBatchNormalizationHelper.preOutput(CudnnBatchNormalizationHelper.java:249)
	at org.deeplearning4j.nn.layers.normalization.BatchNormalization.preOutput(BatchNormalization.java:365)
	at org.deeplearning4j.nn.layers.normalization.BatchNormalization.activate(BatchNormalization.java:317)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-12-21T00:49:27Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>