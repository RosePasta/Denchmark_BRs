<bug id='6518' author='bhorkar' open_date='2018-10-03T18:22:56Z' closed_time='2018-10-11T12:55:13Z'>
	<summary>Dl4j - cuda stuck waiting at org.nd4j.nativeblas.Nd4jCuda$NativeOps.streamSynchronize</summary>
	<description>
I have attached jstack output.
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/2443242/dump_thread.log&gt;dump_thread.log&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/2443251/pom.xml.txt&gt;pom.xml.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/2443258/LSTMPrediction.java.txt&gt;LSTMPrediction .java.txt&lt;/denchmark-link&gt;

Scenario when this issue was observed:

RDD as exported to a directory (dataset_%.bin files)
FileDatasetiterator is used to iterate the data (attached in above .java file).
.fit is operation stuck waiting or the input.
Use cuda -9.0 (issue also observed with cuda 9.2). However, no issue with nd4j-native.

Platform:
$ uname -a
Linux sr-mlgpu05 4.15.0-29-generic &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/31&gt;#31&lt;/denchmark-link&gt;
-Ubuntu SMP x86_64 x86_64 x86_64 GNU/Linux
	</description>
	<comments>
		<comment id='1' author='bhorkar' date='2018-10-03T18:25:50Z'>
		Sure there's no issues with nd4j-native, there's no CUDA methods used.
Is your issue reproducible without spark?
		</comment>
		<comment id='2' author='bhorkar' date='2018-10-03T18:37:14Z'>
		Also, what's your GPU model name?
		</comment>
		<comment id='3' author='bhorkar' date='2018-10-03T18:48:12Z'>
		I tried using same pom.xml file and MnistClassifier.java example on the this machine. Same issue.  So nothing to do with spark/etc. Mostly with GPU?
GPU: V-100.
nvidia-smi
Wed Oct  3 11:46:41 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |
| N/A   35C    P0    43W / 250W |   1653MiB / 32510MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |
| N/A   34C    P0    35W / 250W |      0MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     17704      C   java                                        1642MiB |
		</comment>
		<comment id='4' author='bhorkar' date='2018-10-04T03:53:56Z'>
		Try different initial weights distribution please, and tell me what happens.
		</comment>
		<comment id='5' author='bhorkar' date='2018-10-04T16:00:23Z'>
		Hello,

I tried weightInit.ZERO. It is working!. Some weights work, some dont. Is
there any specific weights I should avoid?

Weight initialization might affect the accuracy, hence the question.

Abhijeet
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Oct 3, 2018 at 8:55 PM raver119 ***@***.***&gt; wrote:
 Try different initial weights distribution please, and tell me what
 happens.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/6518#issuecomment-426876746&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABdAEVHEMx9TUbI2v5Oerslm790ppqipks5uhYbKgaJpZM4XGmIi&gt;
 .



		</comment>
		<comment id='6' author='bhorkar' date='2018-10-04T16:02:06Z'>
		I'm afraid that's a bug which was considered fixed... Probably still not.
		</comment>
		<comment id='7' author='bhorkar' date='2018-10-04T16:14:57Z'>
		Thanks!.
Does the bug affect all initialization weights?
Do you have previous bug#.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Thu, Oct 4, 2018 at 9:03 AM raver119 ***@***.***&gt; wrote:
 I'm afraid that's a bug which was considered fixed... Probably still not.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/6518#issuecomment-427074859&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABdAESFqQhriK2I6Ivopl8YZ3Ub0OcNUks5uhjFogaJpZM4XGmIi&gt;
 .



		</comment>
		<comment id='8' author='bhorkar' date='2018-10-04T16:16:29Z'>
		Bug affects one rng op and V100 devices.
		</comment>
		<comment id='9' author='bhorkar' date='2018-10-10T03:56:11Z'>
		Hm. That's not the same bug.
		</comment>
		<comment id='10' author='bhorkar' date='2018-10-10T03:56:31Z'>
		You've been using simple XAVIER as weight init, right?
		</comment>
		<comment id='11' author='bhorkar' date='2018-10-10T16:11:47Z'>
		Yes. I had used XAVIER as weight.


Abhijeet
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Oct 9, 2018 at 8:58 PM raver119 ***@***.***&gt; wrote:
 You've been using simple XAVIER as weight init, right?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/6518#issuecomment-428429686&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABdAEXRulQ_ftNogwqYzCdaZrMav_QHTks5ujXBUgaJpZM4XGmIi&gt;
 .



		</comment>
		<comment id='12' author='bhorkar' date='2018-10-11T12:55:12Z'>
		Thanks for highlighting this problem, issue was fixed.
		</comment>
		<comment id='13' author='bhorkar' date='2018-10-12T00:25:09Z'>
		FYI: I've temporarily reverted the fix PR.
Once the fix has been re-applied, you can access it via snapshots:
&lt;denchmark-link:https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots&gt;https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='bhorkar' date='2018-11-11T01:24:00Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>