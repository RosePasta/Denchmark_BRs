<bug id='1226' author='mlaubre' open_date='2016-03-03T16:29:58Z' closed_time='2016-03-04T12:03:46Z'>
	<summary>deserialized wordvector lookup returns wrong vector</summary>
	<description>
Hi
The serialized word vector returns different set of similar words and word vectors before and after serialization. Not always true though. On one particular vector set I tested on, all the words at the beginning of the vectors file were handled correctly by DL4J while lookup of the vectors for the words at the end of the vectors file returned incorrect vectors. Those vectors always belonged to some other word. The vector file itself seems correct and matches perfectly the original trained word vectors.
Here's the testbench sample code:
&lt;denchmark-link:https://gist.github.com/mlaubre/9252aeefcb49f5d8af75&gt;https://gist.github.com/mlaubre/9252aeefcb49f5d8af75&lt;/denchmark-link&gt;

&lt;denchmark-link:https://gist.github.com/mlaubre/75b5fddaaf7db497bcc0&gt;https://gist.github.com/mlaubre/75b5fddaaf7db497bcc0&lt;/denchmark-link&gt;

Still, great work with the DL4J and Word2Vec!
Regards
Martin
&lt;denchmark-link:mailto:martin.laubre@gmail.com&gt;martin.laubre@gmail.com&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='mlaubre' date='2016-03-03T16:31:44Z'>
		Can i also see output of your tests?
		</comment>
		<comment id='2' author='mlaubre' date='2016-03-03T16:38:58Z'>
		You mean logs? I skipped the vectors building part, since it takes really long time. Building the vectors isn't the issue anyway. But output of the loading is:
&lt;denchmark-code&gt;2016.03.03 18:35:40:095 INFO  SimilarWordsLearnerSampler.loadWordVectors - Suggested word "politics" vector: "[0.05410495400428772, 0.055306341499090195, 0.22979769110679626, 0.037167470902204514, -0.057258766144514084, 0.16231495141983032, 0.12689904868602753, -0.23967678844928741, -0.08722330629825592, -0.03754518926143646, 0.07163061201572418, 0.01559577975422144, -0.19220715761184692, -0.00640055350959301, 0.17414811253547668, 0.09359981119632721, 0.027030890807509422, 0.16089439392089844, -0.1341850459575653, 0.16071535646915436, 0.08140275627374649, -0.047229185700416565, -0.05530290678143501, -0.22815489768981934, 0.08812293410301208, 0.03267144784331322, -0.06302914768457413, 0.16997110843658447, -0.2176140993833542, 0.1494605392217636, -0.08278873562812805, -0.17973698675632477, -0.19162064790725708, 0.1373089849948883, -0.15987829864025116, -0.04308868199586868, 0.1109694167971611, -0.04471954330801964, -0.15543949604034424, -0.1595800817012787, -0.0925736278295517, 0.09423516690731049, 6.699381629005075E-4, 0.22118763625621796, 0.08734998852014542, -0.029725145548582077, -0.0702403336763382, -0.19504183530807495, -0.025883767753839493, -0.036759521812200546, -0.0594451017677784, 0.06510626524686813, 0.010589606128633022, -0.13945025205612183, 0.128936767578125, 0.11801709234714508, -0.10835256427526474, 0.11631780862808228, 0.0952887311577797, 0.02517165057361126, -0.2588529586791992, -0.03109845705330372, 0.22649313509464264, 0.13730941712856293, -0.053444474935531616, -0.11317358165979385, 0.08676069974899292, -0.015557714737951756, 0.04709192365407944, -0.027524692937731743, -0.18748050928115845, -0.06764014065265656, 0.12912580370903015, 0.05724327638745308, -0.04577477276325226, -0.08229352533817291, -0.30579790472984314, 0.08479457348585129, 0.011733409948647022, 0.01190182939171791, 0.1308308094739914, 0.17759494483470917, 0.015629280358552933, -0.014051641337573528, 0.03706348314881325, -0.04579024761915207, 0.16255490481853485, 0.22510527074337006, 0.01557337399572134, -0.0070971231907606125, 0.10597404092550278, -0.1848011016845703, -0.02092031203210354, 0.046172477304935455, -0.04666861146688461, 0.012451292015612125, -0.05961956828832626, -0.15624476969242096, -0.057020097970962524, -0.03495519980788231]"
2016.03.03 18:35:42:001 INFO  SimilarWordsLearnerSampler.loadWordVectors - Actual word with the vector is "nutrition"
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mlaubre' date='2016-03-03T17:17:02Z'>
		Have you tried the same code using latest snapshot? 0.4-rc3.9-SNAPSHOT
		</comment>
		<comment id='4' author='mlaubre' date='2016-03-03T17:36:27Z'>
		At current master i see this test passing:
&lt;denchmark-link:https://gist.github.com/raver119/ec95fc3f74d9b4700ed0&gt;https://gist.github.com/raver119/ec95fc3f74d9b4700ed0&lt;/denchmark-link&gt;

So, it explicitly checks word by word correpsonding vectors.
		</comment>
		<comment id='5' author='mlaubre' date='2016-03-03T21:36:02Z'>
		+1 Experienced this same issue today with 0.4-rc3.8
		</comment>
		<comment id='6' author='mlaubre' date='2016-03-03T21:55:06Z'>
		The problem here actually appears to be in writing words to file.  I have two instances of the same term in my word vectors file with different word vectors.
		</comment>
		<comment id='7' author='mlaubre' date='2016-03-03T22:13:06Z'>
		&lt;denchmark-link:https://github.com/bmckown&gt;@bmckown&lt;/denchmark-link&gt;
 can i see your source code that reproduces this error?
I definitely need to see your source code, since that's technically impossible to have two instances of the same term in one vocab :)
They just have to be different. Rule of generic HashMap/ConcurrentMap implementation in java :)
		</comment>
		<comment id='8' author='mlaubre' date='2016-03-03T22:23:51Z'>
		&lt;denchmark-link:https://github.com/bmckown&gt;@bmckown&lt;/denchmark-link&gt;
 and some details on training corpus would be nice to have too: language, encoding, size. As well as example of such term that had two instances in one vocab.
		</comment>
		<comment id='9' author='mlaubre' date='2016-03-03T23:04:43Z'>
		I believe the issue comes from this line &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/deeplearning4j-parent-0.4-rc3.8/deeplearning4j-scaleout/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.java#L303&gt;https://github.com/deeplearning4j/deeplearning4j/blob/deeplearning4j-parent-0.4-rc3.8/deeplearning4j-scaleout/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.java#L303&lt;/denchmark-link&gt;

I'm doing some analysis on twitter and a great example is the twitter handle @Harry_Styles.  In many places in my data set, Harry Styles, @Harry_Styles and Harry_Styles appears.  The vocab cache makes a distinction between the three when training, but upon saving, all spaces in words are transformed into underscores causing Harry Styles to become Harry_Styles which conflicts with another word in the cache.
		</comment>
		<comment id='10' author='mlaubre' date='2016-03-04T08:27:55Z'>
		Hm, that's definitely different from things &lt;denchmark-link:https://github.com/mlaubre&gt;@mlaubre&lt;/denchmark-link&gt;
 mentioned. He was referring single-element tokens. Like politics.
Your case is another issue and definitely should be fixed/improved.
		</comment>
		<comment id='11' author='mlaubre' date='2016-03-04T09:14:56Z'>
		&lt;denchmark-link:https://github.com/bmckown&gt;@bmckown&lt;/denchmark-link&gt;
 i've fixed your issue here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/1133&gt;https://github.com/deeplearning4j/deeplearning4j/pull/1133&lt;/denchmark-link&gt;

However, it will be merged into master a bit later.
		</comment>
		<comment id='12' author='mlaubre' date='2016-03-04T10:01:43Z'>
		Just in case anyone wants to see my side of the problem, here's the expirebox link to the vector file that proves my case:
&lt;denchmark-link:http://expirebox.com/download/2073258d94163c5a033bbf0917f3b0a3.html&gt;http://expirebox.com/download/2073258d94163c5a033bbf0917f3b0a3.html&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='mlaubre' date='2016-03-04T10:04:22Z'>
		&lt;denchmark-link:https://github.com/mlaubre&gt;@mlaubre&lt;/denchmark-link&gt;
 can you come to gitter, so we could speed this up?
		</comment>
		<comment id='14' author='mlaubre' date='2016-03-04T10:39:05Z'>
		So, original issue is definitely caused by encoding shift.
I.e. in provided file i see multiple entries for element "?"
Here's the list of elements that were merged during vocab deserialization: &lt;denchmark-link:https://gist.github.com/raver119/6c2db9e0f63732f8419a&gt;https://gist.github.com/raver119/6c2db9e0f63732f8419a&lt;/denchmark-link&gt;

Solution yet to be decided here.
		</comment>
		<comment id='15' author='mlaubre' date='2016-03-04T12:03:46Z'>
		So, solution is available at the same pr: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/1133&gt;https://github.com/deeplearning4j/deeplearning4j/pull/1133&lt;/denchmark-link&gt;

Solution was confirmed. Will be merged into master a bit later.
		</comment>
		<comment id='16' author='mlaubre' date='2019-01-21T09:37:34Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>