<bug id='6431' author='Charele' open_date='2018-09-12T17:53:21Z' closed_time='2018-09-23T23:57:43Z'>
	<summary>DL4J: SimpleRnnParamInitializer does not respect biasInit configuration</summary>
	<description>
Let's see the init() method of SimpleRnnParamInitializer:
... ...
INDArray rw = WeightInitUtil.initWeights(nOut, nOut, new long[]{nOut, nOut}, rwInit, rwDist, 'f', m.get(RECURRENT_WEIGHT_KEY));
m.put(RECURRENT_WEIGHT_KEY, rw);
} else {
m = getSubsets(paramsView, nIn, nOut, true);
}
&lt;denchmark-code&gt;    conf.addVariable(WEIGHT_KEY);
    conf.addVariable(RECURRENT_WEIGHT_KEY);
    conf.addVariable(BIAS_KEY);

    return m;
}
&lt;/denchmark-code&gt;

It initialize the "W" and "RW",  forget the init of parameter "b"(will keep default 0)
It's a lost? or bias 0 value is helpful for RNN?
	</description>
	<comments>
		<comment id='1' author='Charele' date='2018-09-13T00:38:10Z'>
		Thanks for reporting. Looks like yes, SimpleRNN does dave a biasInit config option but it doesn't use that in the weight initializer.
		</comment>
		<comment id='2' author='Charele' date='2018-10-24T00:09:52Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>