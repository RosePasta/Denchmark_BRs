<bug id='5955' author='AlexDBlack' open_date='2018-07-24T13:13:20Z' closed_time='2018-08-07T00:20:26Z'>
	<summary>DL4J SameDiff layers execution failing on CUDA</summary>
	<description>
2 failures on CUDA - passing on CPU:
CNN1DGradientCheckTest.testCnn1DWithLocallyConnected1D and testCnnLocallyConnected2D
&lt;denchmark-code&gt;o.d.g.GradientCheckUtil - Setting softmax clipping epsilon to 0.0 for class org.nd4j.linalg.lossfunctions.impl.LossMCXENT loss function to avoid spurious gradient check failures
Error at [/repos/deeplearning4j/libnd4j/include/ops/declarable/generic/transforms/concat.cpp:121:0]:
CONCAT op: at least one input array must be non-empty!

java.lang.RuntimeException: Op validation failed

	at org.nd4j.nativeblas.Nd4jCuda$NativeOps.calculateOutputShapesDouble(Native Method)
	at org.nd4j.nativeblas.Nd4jCuda$NativeOps.calculateOutputShapesDouble(Nd4jCuda.java:402)
	at org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner.calculateOutputShape(CudaExecutioner.java:2552)
	at org.nd4j.linalg.api.ops.DynamicCustomOp.calculateOutputShape(DynamicCustomOp.java:555)
	at org.nd4j.autodiff.samediff.SameDiff.generateOutputVariableForOp(SameDiff.java:8684)
	at org.nd4j.linalg.api.ops.DynamicCustomOp.outputVariables(DynamicCustomOp.java:193)
	at org.nd4j.linalg.api.ops.DynamicCustomOp.outputVariables(DynamicCustomOp.java:175)
	at org.nd4j.autodiff.functions.DifferentialFunction.outputVariable(DifferentialFunction.java:401)
	at org.nd4j.autodiff.functions.DifferentialFunctionFactory.concat(DifferentialFunctionFactory.java:1250)
	at org.nd4j.autodiff.samediff.SameDiff.concat(SameDiff.java:7149)
	at org.nd4j.autodiff.samediff.SameDiff.concat(SameDiff.java:7134)
	at org.deeplearning4j.nn.conf.layers.LocallyConnected1D.defineLayer(LocallyConnected1D.java:189)
	at org.deeplearning4j.nn.layers.samediff.SameDiffLayer.doInit(SameDiffLayer.java:221)
	at org.deeplearning4j.nn.layers.samediff.SameDiffLayer.activate(SameDiffLayer.java:79)
	at org.deeplearning4j.nn.layers.AbstractLayer.activate(AbstractLayer.java:267)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.ffToLayerActivationsInWs(MultiLayerNetwork.java:1045)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2592)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:245)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:152)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:144)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:136)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:130)
	at org.deeplearning4j.gradientcheck.CNN1DGradientCheckTest.testCnn1DWithLocallyConnected1D(CNN1DGradientCheckTest.java:117)
&lt;/denchmark-code&gt;

Edit: looks like most (all?) of the SameDiff layers are failing on CUDA, all passing on native.
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/43181525-00ea8a9e-9021-11e8-8f0e-17bbcf070c0b.png&gt;&lt;/denchmark-link&gt;

The other main type of failure:
&lt;denchmark-code&gt;java.lang.RuntimeException: vector::_M_range_check: __n (which is 0) &gt;= this-&gt;size() (which is 0)

	at org.nd4j.nativeblas.Nd4jCuda$NativeOps.calculateOutputShapesDouble(Native Method)
	at org.nd4j.nativeblas.Nd4jCuda$NativeOps.calculateOutputShapesDouble(Nd4jCuda.java:402)
	at org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner.calculateOutputShape(CudaExecutioner.java:2552)
	at org.nd4j.linalg.api.ops.impl.transforms.arithmetic.bp.BaseArithmeticBackpropOp.calculateOutputShape(BaseArithmeticBackpropOp.java:46)
	at org.nd4j.autodiff.samediff.SameDiff.generateOutputVariableForOp(SameDiff.java:8684)
	at org.nd4j.linalg.api.ops.DynamicCustomOp.outputVariables(DynamicCustomOp.java:193)
	at org.nd4j.linalg.api.ops.DynamicCustomOp.outputVariables(DynamicCustomOp.java:175)
	at org.nd4j.autodiff.functions.DifferentialFunctionFactory.addBp(DifferentialFunctionFactory.java:1641)
	at org.nd4j.linalg.api.ops.impl.transforms.arithmetic.AddOp.doDiff(AddOp.java:62)
	at org.nd4j.autodiff.functions.DifferentialFunction.diff(DifferentialFunction.java:554)
	at org.nd4j.autodiff.samediff.SameDiff$3.define(SameDiff.java:9291)
	at org.nd4j.autodiff.samediff.SameDiff.defineFunction(SameDiff.java:9108)
	at org.nd4j.autodiff.samediff.SameDiff.defineFunction(SameDiff.java:9092)
	at org.nd4j.autodiff.samediff.SameDiff.createGradFunction(SameDiff.java:9203)
	at org.nd4j.autodiff.samediff.SameDiff.execBackwards(SameDiff.java:9170)
	at org.deeplearning4j.nn.layers.samediff.SameDiffLayer.backpropGradient(SameDiffLayer.java:113)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.calcBackpropGradients(MultiLayerNetwork.java:1773)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2526)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:245)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:152)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:144)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:136)
	at org.deeplearning4j.gradientcheck.GradientCheckUtil.checkGradients(GradientCheckUtil.java:130)
	at org.deeplearning4j.nn.layers.samediff.TestSameDiffDense.gradientCheck(TestSameDiffDense.java:424)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-08-06T13:42:57Z'>
		Fixes were merged to master. Please confirm problem solved.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-08-07T00:20:26Z'>
		All samediff tests confirmed passing on CUDA - with one exception: SameDiffOutput (JVM crash - issue incoming)
CNN1DGradientCheckTest and LocallyConnectedLayerTest also confirmed passing.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-09-21T10:21:29Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>