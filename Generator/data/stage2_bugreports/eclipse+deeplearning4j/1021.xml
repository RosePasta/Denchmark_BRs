<bug id='1021' author='AlexDBlack' open_date='2016-01-06T12:23:07Z' closed_time='2018-04-25T22:02:05Z'>
	<summary>Test time forward pass may (unnecessarily) store activations</summary>
	<description>
During network training: activations are stored during the forward pass, so they can be used later in backprop step.
Layer inputs also appear to be stored at test time, even though they won't be reused later.
For example:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/layers/BaseLayer.java#L354&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/layers/BaseLayer.java#L354&lt;/denchmark-link&gt;

This appears to cause a (small, constant size) memory leak.
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-04-25T22:02:05Z'>
		We have this concept implemented long ago, it can be turned on/off via CacheMode option.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-09-22T18:13:40Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>