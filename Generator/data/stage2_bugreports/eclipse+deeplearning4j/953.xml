<bug id='953' author='dkmisra' open_date='2015-12-12T00:52:46Z' closed_time='2016-05-04T17:31:53Z'>
	<summary>[Clarification needed] AdaGrad sum of gradient squares initialized with 1</summary>
	<description>
&lt;denchmark-h:h2&gt;Clarification Needed&lt;/denchmark-h&gt;

I see that AdaGrad sum of gradient square is initialized with 1. Seems like it was added to prevent 0/0 form when initial gradients are 0. However, this 1 in the denominator will give wrong results by a significant margin in future steps (a gradient can be order of magnitude smaller).
Also there is an epsilon addition at every step?
	</description>
	<comments>
		<comment id='1' author='dkmisra' date='2015-12-12T01:51:50Z'>
		So yes: the motivation was to avoid numerical issues (0/0 -&gt; NaN)
I don't believe there was any strong theoretical motivation for the 1-based initialization. I agree it could be problematic in general (squared gradients could be very small, hence sum of squared gradients could be dominated by initialization).
Perhaps it would be better to simply initialize the adagrad gradients with an epsilon once, rather than at every step.
Should be straightforward to modify, feel free to contribute a pull request for this if you want. :)
Also note that RMSProp may need a review for the same sorts of reasons: &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/blob/510532dc4b5b63a2f17c1fd5b294e1ec7281cfe2/nd4j-api/src/main/java/org/nd4j/linalg/learning/RmsProp.java&gt;https://github.com/deeplearning4j/nd4j/blob/510532dc4b5b63a2f17c1fd5b294e1ec7281cfe2/nd4j-api/src/main/java/org/nd4j/linalg/learning/RmsProp.java&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='dkmisra' date='2016-05-04T17:31:53Z'>
		Closed with &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/pull/888&gt;deeplearning4j/nd4j#888&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='dkmisra' date='2019-01-21T03:53:16Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>