<bug id='5906' author='cwhite102' open_date='2018-07-15T00:20:05Z' closed_time='2018-07-24T09:38:11Z'>
	<summary>Keras Yolo model works on CPU, but gives garbage results with CUDA</summary>
	<description>
My Yolo model works on CPU but gives garbage results with CUDA. (seemingly random bounding boxes)
However, if I run the Model Zoo Yolo2, it works on both CPU and CUDA, so the CUDA setup seems ok.
(DL4J 1.0.0-beta, CUDA 9.1, cuDNN 7, Windows 10, Geforce GTX 1060 3GB card)
This was a darknet trained model, converted to Keras (with YAD2K) and loaded into DL4J from the h5 file.
I did not serialize the dl4j model, instead I just 'ran' with the ComputationGraph model.
It was loaded similar to the description in the YOLO2 Zoo model javadocs.
Code that loads the Keras Model...  (same results with 416 vs 608 sizes)
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/2195362/YoloKerasModelLoader.java.txt&gt;YoloKerasModelLoader.java.txt&lt;/denchmark-link&gt;

Note - The original javadoc code for the output layer had "conv2d_23" but the loaded model only went to "conv2d_22"
Original yolo config
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/2195360/yolo-obj.cfg.txt&gt;yolo-obj.cfg.txt&lt;/denchmark-link&gt;

YAD2K conversion log
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/2195364/cw-convert-log.txt&gt;cw-convert-log.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='cwhite102' date='2018-07-17T04:31:01Z'>
		On a whim I tried saving the ComputationGraph model to disk and re-loading it.  This 'fixed' (or should I say, worked around) the issue.  I was able to get the same results on CUDA as I was with the CPU.  So the issue seems to be with using the model directly from the Keras model loader, or TransferLearning graph builder.
&lt;denchmark-code&gt;		model.save(saveFile, /*save updater*/false);
		model = ComputationGraph.load(saveFile, false);
&lt;/denchmark-code&gt;

Hope this helps you figure it out.
		</comment>
		<comment id='2' author='cwhite102' date='2018-07-17T04:47:57Z'>
		&lt;denchmark-link:https://github.com/cwhite102&gt;@cwhite102&lt;/denchmark-link&gt;
 Huh, that's unexpected. But thanks, that should help track this down quicker...
		</comment>
		<comment id='3' author='cwhite102' date='2018-07-24T09:38:11Z'>
		I can confirm that I was able to reproduce a difference between 1.0.0-beta native/cuda and 1.0.0-beta with CUDNN. Saving/loading the model did not result in any difference for me, however.
I can also confirm that native/cuda/cudnn are all producing identical results when tested on current snapshot/master.
I suspect it may be the same issues discussed in this comment (and the next one): &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/5836#issuecomment-405526228&gt;https://github.com/deeplearning4j/deeplearning4j/issues/5836#issuecomment-405526228&lt;/denchmark-link&gt;

Perhaps try again with snapshots - if you see any further issues, let us know and we can take another look.
&lt;denchmark-link:https://deeplearning4j.org/snapshots&gt;https://deeplearning4j.org/snapshots&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='cwhite102' date='2018-09-21T13:59:32Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>