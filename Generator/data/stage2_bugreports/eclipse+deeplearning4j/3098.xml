<bug id='3098' author='sorensf' open_date='2017-03-23T14:00:49Z' closed_time='2017-03-25T14:10:25Z'>
	<summary>Cannot use CPU ND4j backend for training when cuDNN dependency is added</summary>
	<description>
I tried building a DL4j jar with both GPU and CPU backend for ND4j while also having enabled cuDNN. Training on GPU worked fine, but after switching the backend to CPU an exception is thrown from CudnnConvolutionHelper.
	</description>
	<comments>
		<comment id='1' author='sorensf' date='2017-03-25T10:07:10Z'>
		It works just fine here. Could you provide more details about how you are building and running your application?
		</comment>
		<comment id='2' author='sorensf' date='2017-03-25T11:40:02Z'>
		Sure. (Most of this is copied from when described my setup in &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/3038&gt;#3038&lt;/denchmark-link&gt;
).
I am using an environment as described in the quickstart guide with:

And then using

To build the dl4j-examples-0.8-SNAPSHOT-bin.jar. To use GPU I have changed the root pom.xml to
 (&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/858376/pom.xml.txt&gt;pom.xml (.).txt&lt;/denchmark-link&gt;
)
And I have changed the pom.xml in the sub-directory "dl4j-examples" to the following &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/869965/pom.xml.dl4j-examples.txt&gt;pom.xml.dl4j-examples.txt&lt;/denchmark-link&gt;
 (changes are marked with ).
		</comment>
		<comment id='3' author='sorensf' date='2017-03-25T11:50:54Z'>
		Ok, could you also copy/paste what shows up on the console when you run an example?
		</comment>
		<comment id='4' author='sorensf' date='2017-03-25T11:57:51Z'>
		Sorry, I will not have access to the code again for a while. But I can describe as best I can from memory, although the of cource might not be of much use. I was using the system variable BACKEND_PRIORITY_CPU=200 to switch to CPU.
When running on GPU is was like (copied again from &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/3038&gt;#3038&lt;/denchmark-link&gt;
)
INFO | Loaded [JCublasBackend] backend
INFO | Number of threads used for NativeOps: 32
INFO | Reflections took 297 ms to scan 3 urls, producing 29 keys and 189 values
INFO | Backend used: [CUDA]; OS: [Windows 7]
INFO | Cores: [12]; Memory: [31,3GB];
INFO | Blas vendor: [CUBLAS]
INFO | Device name: [GeForce GTX 1080]; CC: [6.1]; Total/free memory: [8589934592]
When running on CPU it instead printed it was using MKL, and none of the GPU related things were printed.
		</comment>
		<comment id='5' author='sorensf' date='2017-03-25T12:09:51Z'>
		If it helps, I initially asked about the issue in the gitter channel where &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 mentioned it was something that was missed and I should make an issue, so he might know more about it.
		</comment>
		<comment id='6' author='sorensf' date='2017-03-25T12:23:11Z'>
		
When running on CPU it instead printed it was using MKL, and none of the GPU related things were printed.

And then it failed with cuDNN, right?
		</comment>
		<comment id='7' author='sorensf' date='2017-03-25T12:25:36Z'>
		Yes, at the point where model.fit(..) was called, it throws.
		</comment>
		<comment id='8' author='sorensf' date='2017-03-25T12:48:52Z'>
		By the way, if you still have trouble reproducing the issue. The model I used when the exception was thrown was the custom model from: &lt;denchmark-link:https://gist.github.com/sorensf/7309f2ee9811c6b1d462e037ad123fd5&gt;https://gist.github.com/sorensf/7309f2ee9811c6b1d462e037ad123fd5&lt;/denchmark-link&gt;
, I guess that might have an effect that it is convolutional.
		</comment>
		<comment id='9' author='sorensf' date='2017-03-25T14:07:57Z'>
		I see what's going on. Basically, we need to make sure that nd4j-cuda is loaded as backend before trying to load cuDNN. Thanks for reporting!
		</comment>
		<comment id='10' author='sorensf' date='2017-09-12T09:30:30Z'>
		I've recently upgraded to Dl4j 0.9.1 for this fix, but it still doesn't seem to work in my setup, actually it has slightly gotten worse. My workflow is still that I train a model on GPU and then use it in a VM with CPU (simulated here with BACKEND_PRIORITY_CPU = 200), previously this was possible, and only failed when I tried training on CPU. Now I am no longer able to deserialize the GPU trained models at all on the CPU VM - see the output for the stacktrace
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/1295322/deserialization-exception.txt&gt;deserialization-exception.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='sorensf' date='2017-09-12T09:34:17Z'>
		This issue was already fixed yesterday.
		</comment>
	</comments>
</bug>