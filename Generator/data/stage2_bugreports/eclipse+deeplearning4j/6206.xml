<bug id='6206' author='maclema' open_date='2018-08-19T01:40:50Z' closed_time='2018-08-20T21:41:33Z'>
	<summary>CUDNN_STATUS_MAPPING_ERROR with batchSize of 1</summary>
	<description>
A CUDNN_STATUS_MAPPING_ERROR exception is thrown when calling fit() on a LSTM network when the batchSize is 1. If the batch size is greater than one everything works as expected.
Environment:
&lt;denchmark-code&gt;OS: Ubuntu 18.04
CUDA: 9.2, V9.2.88
cuDNN: 7.2.1
DL4J: 1.0.0-beta2
ND4j Backend: nd4j-cuda-9.2-platform
&lt;/denchmark-code&gt;

Model:
&lt;denchmark-code&gt;int hiddenSize = 256;
int numFeatures = 1280;

NeuralNetConfiguration.Builder builder = new NeuralNetConfiguration.Builder();
builder.seed( 123 );
builder.biasInit( 0 );
builder.updater( new Adam( 0.001 ) );
builder.weightInit( WeightInit.XAVIER );
builder.miniBatch( true );

MultiLayerConfiguration conf = builder.list() //
	.layer( new LSTM.Builder().activation( Activation.TANH ).nIn( numFeatures ).nOut( hiddenSize ).build() ) //
	.layer( new LSTM.Builder().activation( Activation.TANH ).nOut( hiddenSize ).build() ) //
	.layer( new LSTM.Builder().activation( Activation.TANH ).nOut( hiddenSize ).build() ) //
	.layer( new RnnOutputLayer.Builder( LossFunction.MSE ).activation( Activation.IDENTITY ).nOut( numFeatures ).build() ) //
	.backpropType( BackpropType.TruncatedBPTT ).tBPTTBackwardLength( 7 ) //
	.build();

MultiLayerNetwork net = new MultiLayerNetwork( conf );
net.init();
return net;
&lt;/denchmark-code&gt;

Training array shape: [1, 1280, 120]
Exception:
&lt;denchmark-code&gt;Exception in thread "Thread-19" java.lang.RuntimeException: cuDNN status = 7: CUDNN_STATUS_MAPPING_ERROR
	at org.deeplearning4j.nn.layers.BaseCudnnHelper.checkCudnn(BaseCudnnHelper.java:44)
	at org.deeplearning4j.nn.layers.recurrent.CudnnLSTMHelper.activate(CudnnLSTMHelper.java:528)
	at org.deeplearning4j.nn.layers.recurrent.LSTMHelpers.activateHelper(LSTMHelpers.java:206)
	at org.deeplearning4j.nn.layers.recurrent.LSTM.activateHelper(LSTM.java:163)
	at org.deeplearning4j.nn.layers.recurrent.LSTM.activate(LSTM.java:135)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.ffToLayerActivationsInWs(MultiLayerNetwork.java:1038)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2579)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2545)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:160)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:51)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fitHelper(MultiLayerNetwork.java:2148)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:2105)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:2170)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='maclema' date='2018-08-20T05:23:50Z'>
		I'm unable to reproduce this. I've run the code (see below for exactly what I'm running) on two systems (both 1.0.0-beta2 and current master) without any issues:

Windows 10, CUDA 9.2 with CuDNN, Titan X (Pascal)
Ubuntu 17.10, CUDA 9.2 with CuDNN, GTX 970

Can you confirm that the code below is sufficient to reproduce the issue?
If not - it might be caused by (or only reproducible in the context of) other parts of your code (such as the data pipeline. If that's the case, then a complete minimal example (not just the network configuration) to reproduce this would help a lot.
&lt;denchmark-code&gt;        int hiddenSize = 256;
        int numFeatures = 1280;

        NeuralNetConfiguration.Builder builder = new NeuralNetConfiguration.Builder();
        builder.seed( 123 );
        builder.biasInit( 0 );
        builder.updater( new Adam( 0.001 ) );
        builder.weightInit( WeightInit.XAVIER );
        builder.miniBatch( true );

        MultiLayerConfiguration conf = builder.list() //
            .layer( new LSTM.Builder().activation( Activation.TANH ).nIn( numFeatures ).nOut( hiddenSize ).build() ) //
            .layer( new LSTM.Builder().activation( Activation.TANH ).nOut( hiddenSize ).build() ) //
            .layer( new LSTM.Builder().activation( Activation.TANH ).nOut( hiddenSize ).build() ) //
            .layer( new RnnOutputLayer.Builder( LossFunctions.LossFunction.MSE ).activation( Activation.IDENTITY ).nOut( numFeatures ).build() ) //
            .backpropType( BackpropType.TruncatedBPTT ).tBPTTBackwardLength( 7 ) //
            .build();

        MultiLayerNetwork net = new MultiLayerNetwork( conf );
        net.init();

        INDArray in = Nd4j.create(1, numFeatures, 120);
        INDArray labels = Nd4j.create(1, numFeatures, 120);

        net.output(in);
        net.fit(new DataSet(in, labels));
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='maclema' date='2018-08-20T21:41:33Z'>
		Hmm. For some reason my code seems to be working as expected today. I'll keep any eye out for this error again and open a new issue with full example if I come across it.
Thanks
		</comment>
		<comment id='3' author='maclema' date='2018-08-21T00:08:12Z'>
		Ok, thanks for letting me know.
I'm happy to take a look if you do run into it again.
		</comment>
		<comment id='4' author='maclema' date='2018-09-21T09:21:17Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>