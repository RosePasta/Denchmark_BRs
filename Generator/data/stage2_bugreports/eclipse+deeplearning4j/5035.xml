<bug id='5035' author='AlexDBlack' open_date='2018-05-03T10:51:24Z' closed_time='2018-05-04T07:19:43Z'>
	<summary>Col2Im: can't handle uninitialized (or any non-zeros?) result arrays</summary>
	<description>
The old Col2Im could handle uninitialized arrays - hence we used createUninitialized in ConvolutionLayer backprop (for obvious performance reasons).
However, the new implementation can't handle this, and was the cause of the following issue: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/5032&gt;https://github.com/deeplearning4j/deeplearning4j/issues/5032&lt;/denchmark-link&gt;

I'm not sure if the new implementation can be modified to maintain this old behaviour, or if we have no choice but to use zeros (i.e., Nd4j.create)?
Note that this doesn't reliably reproduce it for me - and when it does fail, it's usually early (first few iterations).
&lt;denchmark-code&gt;@Test
    public void test(){

        int inDepth = 1;
        int miniBatch = 1;
        int inH = 5;
        int inW = 5;

        int[] strides = {1,1};
        int[] kernel = {2,2};
        int[] dilation = {1,1};
        int[] pad = {0,0};
        int[] outSize = {4,4};

        int kH = kernel[0];
        int kW = kernel[1];
        int outH = outSize[0];
        int outW = outSize[1];

        INDArray epsNext2d = Nd4j.rand(new int[]{4, 16}).dup('f');

        for( int i=0; i&lt;10000; i++ ) {


            INDArray epsNextOrig = Nd4j.createUninitialized(new int[]{inDepth, miniBatch, inH, inW}, 'c');
            INDArray epsNext = epsNextOrig.permute(1, 0, 2, 3);
            INDArray eps6d = Shape.newShapeNoCopy(epsNext2d, new int[]{kW, kH, inDepth, outW, outH, miniBatch}, true);

            //Calculate epsilonNext by doing im2col reduction.
            //Current col2im implementation expects input with order: [miniBatch,channels,kH,kW,outH,outW]
            //currently have [kH,kW,inDepth,outW,outH,miniBatch] -&gt; permute first
            eps6d = eps6d.permute(5, 2, 1, 0, 4, 3);


            Convolution.col2im(eps6d, epsNext, strides[0], strides[1], pad[0], pad[1], inH, inW, dilation[0], dilation[1]);
            MatchCondition condition = new MatchCondition(epsNext, Conditions.isNan());
            int match = Nd4j.getExecutioner().exec(condition, Integer.MAX_VALUE).getInt(0);
            if(match &gt; 0){
                throw new RuntimeException(String.valueOf(i));
            }
        }
    }
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-05-03T11:57:11Z'>
		Note also that this (or the 3d equivalent I guess?) may be causing 3d CNN gradient checks to fail.
Note also if we do end up allowing uninitialized here, ConvolutionLayer needs to be changed back from create to createUninitialized
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-05-03T15:03:20Z'>
		ConvLayer may be fixed back now, fix applied
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-05-04T02:26:42Z'>
		CPU confirmed fixed, CUDA still has the same issue.
PR here ready to go once this in confirmed fixed for CUDA: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5044&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5044&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='AlexDBlack' date='2018-05-04T02:50:13Z'>
		Hm, but there were no changes to CUDA col2im kernel.
		</comment>
		<comment id='5' author='AlexDBlack' date='2018-05-04T02:51:53Z'>
		Can you please run on my libnd4j branch r119_cuda_col2im?
		</comment>
		<comment id='6' author='AlexDBlack' date='2018-05-04T07:08:08Z'>
		CNNs look OK (gradient checks passing for CUDA) with createUninitialized for col2im on branch r119_cuda_col2im
		</comment>
		<comment id='7' author='AlexDBlack' date='2018-05-04T07:19:06Z'>
		okay merging then. thank you.
		</comment>
		<comment id='8' author='AlexDBlack' date='2018-09-22T05:24:30Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>