<bug id='8644' author='AbdelmajidB' open_date='2020-01-27T19:47:59Z' closed_time='2020-02-18T14:59:52Z'>
	<summary>Test issue in semantic segmentation using DL4J</summary>
	<description>
I have trained a U-net based model on DL4J on medical images. The code above is for testing the model. my objective is to test the model on 34 2d medical images separately and get the segmentation metrics (accuracy, recall…) for each image, so i tried two ways:
1- when i put all the 34 images in the test folder (“testI”) and i run the code which will load all the images and test image by image, i get good segmentation performance for all the images,
2- but when i put just one image that i want to test from the 34 in test folder and i execute the code i get different results in term of accuracy, recall…), that means i get different segmentation results for the same images by using the two approaches.
This is my label generator code: &lt;denchmark-link:https://gist.github.com/AbdelmajidB/fccb871836746dd4f0d6e32828eadcdf&gt;https://gist.github.com/AbdelmajidB/fccb871836746dd4f0d6e32828eadcdf&lt;/denchmark-link&gt;
 The following code is used test the model image by image: &lt;denchmark-link:https://gist.github.com/AbdelmajidB/92f5fc3013c81b28c898b64118b6adb5&gt;https://gist.github.com/AbdelmajidB/92f5fc3013c81b28c898b64118b6adb5&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AbdelmajidB' date='2020-01-27T19:48:37Z'>
		Thanks!
		</comment>
		<comment id='2' author='AbdelmajidB' date='2020-01-30T06:05:28Z'>
		If I understand the problem description correctly, this means that somehow you're getting a different labels array, or maybe different network outputs, depending on how you create the iterators.
Have you been able to narrow it down to which one is the problem?
A test case that creates both data pipelines, compares the labels, and compares the output arrays, and shows a difference, would help us here.
		</comment>
		<comment id='3' author='AbdelmajidB' date='2020-02-01T23:17:33Z'>
		the problem is that i get different network outputs for the same images using the code above. as i described before i want to get the segmentation evaluation metrics (F1 score, accuracy...) for each image separately. so when i put in test folder an image with ground truth and i execute the code, i get the output segmentation for that image, but when i add a second image and i execute the code i get the output segmentation for each image, but for the first image tested before i get different output segmentation and so on as i add other images, which is not normal. I can't understand the cause of this issue.
		</comment>
		<comment id='4' author='AbdelmajidB' date='2020-02-02T09:53:38Z'>
		you can find here:
1- The test images: &lt;denchmark-link:https://drive.google.com/file/d/1nF-Rmqa76F9Wl86YMVwKDs75M9D615eV/view?usp=sharing&gt;https://drive.google.com/file/d/1nF-Rmqa76F9Wl86YMVwKDs75M9D615eV/view?usp=sharing&lt;/denchmark-link&gt;

2- The pretrained model: &lt;denchmark-link:https://drive.google.com/file/d/196w4g_ZibH9evPj5HTCB29u25aAxovOC/view?usp=sharing&gt;https://drive.google.com/file/d/196w4g_ZibH9evPj5HTCB29u25aAxovOC/view?usp=sharing&lt;/denchmark-link&gt;

3- MyPathLabelGenerator :&lt;denchmark-link:https://gist.github.com/AbdelmajidB/fccb871836746dd4f0d6e32828eadcdf&gt;https://gist.github.com/AbdelmajidB/fccb871836746dd4f0d6e32828eadcdf&lt;/denchmark-link&gt;

4- The code for testing: &lt;denchmark-link:https://gist.github.com/AbdelmajidB/92f5fc3013c81b28c898b64118b6adb5&gt;https://gist.github.com/AbdelmajidB/92f5fc3013c81b28c898b64118b6adb5&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='AbdelmajidB' date='2020-02-11T14:30:08Z'>
		Hello &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 , please have you any idea about this issue? i uploaded in the last post resources that can you need .
		</comment>
		<comment id='6' author='AbdelmajidB' date='2020-02-11T19:15:44Z'>
		Looking at your code for testing, it looks very much like you are just seeing the randomization in filesplit doing what it is supposed to do.
If you pass a random instance to file split, you will get the files back in random order.  So if you add files to a directory between runs, the output will be in different orders.
If I'm right, the easiest way to solve that problem is to not pass rng in line 18.
If that doesn't fix the issue, we will need either a self-contained testcase (junit 4) or a self-contained project, that we can just clone and run, so we can see what is going on.
		</comment>
		<comment id='7' author='AbdelmajidB' date='2020-02-11T20:31:51Z'>
		&lt;denchmark-link:https://github.com/treo&gt;@treo&lt;/denchmark-link&gt;
 thank you for your reply. If doesn't fix the issue.
i removed rng from line 18, and i show the path of loaded images in MyPathLabelGenerator to see the results of metrics for each image, so if i put all the 59 images in test folder i get the following results: &lt;denchmark-link:https://gist.github.com/AbdelmajidB/2d48e945bd81e59a37d461b0eee05572&gt;https://gist.github.com/AbdelmajidB/2d48e945bd81e59a37d461b0eee05572&lt;/denchmark-link&gt;

and if i put just image "1.png" i get this results: &lt;denchmark-link:https://gist.github.com/AbdelmajidB/868c8259e82b36cc98a2131731551370&gt;https://gist.github.com/AbdelmajidB/868c8259e82b36cc98a2131731551370&lt;/denchmark-link&gt;

so you can see that image "1.png" with the other images it is segmented with good values of F1 score.., but when the image tested alone (in second case) i get bad values.
		</comment>
		<comment id='8' author='AbdelmajidB' date='2020-02-13T23:38:31Z'>
		&lt;denchmark-link:https://github.com/treo&gt;@treo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
, please, have you any idea about this issue??
		</comment>
		<comment id='9' author='AbdelmajidB' date='2020-02-14T00:10:49Z'>
		Can you create a self-contained project that we can run? that would make debugging this a lot easier
		</comment>
		<comment id='10' author='AbdelmajidB' date='2020-02-14T07:18:29Z'>
		&lt;denchmark-link:https://github.com/treo&gt;@treo&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 you can find here the project that you can run: &lt;denchmark-link:https://drive.google.com/file/d/16Ov_8Zxj5KDATc78nBXSek3GA07-AYOJ/view?usp=sharing&gt;https://drive.google.com/file/d/16Ov_8Zxj5KDATc78nBXSek3GA07-AYOJ/view?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='AbdelmajidB' date='2020-02-18T14:59:52Z'>
		The issue is actually pretty simple: You are using  to normalize your data.
This is a scaler that has to be fit to your data, and you do fit it here:
&lt;denchmark-link:https://gist.github.com/AbdelmajidB/92f5fc3013c81b28c898b64118b6adb5#file-semanticsegmentationload2-java-L29-L31&gt;https://gist.github.com/AbdelmajidB/92f5fc3013c81b28c898b64118b6adb5#file-semanticsegmentationload2-java-L29-L31&lt;/denchmark-link&gt;

The problem with the approach you have taken here is that you did a classic data prep mistake:
You fit it to the data you are trying to evaluate. So if there is only a single picture, it fits to that single picture and thereby changes the range of values entirely.
There are two possible solutions for you:

You serialize your initial NormalizerMinMaxScaler that you've used when training the model.
Use ImagePreProcessingScaler instead of NormalizerMinMaxScaler, as it is a scaler that just assumes an input range of 0 to 255.

		</comment>
	</comments>
</bug>