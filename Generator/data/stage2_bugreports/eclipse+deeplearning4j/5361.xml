<bug id='5361' author='AlexDBlack' open_date='2018-05-28T08:35:52Z' closed_time='2018-09-21T08:13:26Z'>
	<summary>ND4J: issues with multiple ops</summary>
	<description>
Ops requiring backprop at libnd4j level (a non-exhaustive list)

cumprod
conv3d: https://github.com/deeplearning4j/deeplearning4j/blob/74bd70162979bace33cf28b160aeb61131e13bcc/libnd4j/include/ops/declarable/generic/convo/conv3d_ops.cpp#L110-L112

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Transpose op
Confirmed fixed
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Fill op
Confirmed fixed 18/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

avgpool3dnew (edit: maxpool3dnew also)
Edit: bad test (dilation 0) + lack of validation at libnd4j level.
Additional validation will be merged to c++ level soon.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

clipbynorm
edit: passing as of 20/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

CumSum op
Now passing, 21/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

CumProd op
Now passing, 21/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

eye op
Edit, 21/06: Now failing at java level.
&lt;denchmark-code&gt;@Test
    public void testEye(){

        int[] rows = new int[]{3,3,3,3};
        int[] cols = new int[]{3,2,2,2};
        int[][] batch = new int[][]{null, null, {4}, {3,3}};
        INDArray[] expOut = new INDArray[4];

        expOut[0] = Nd4j.eye(3);
        expOut[1] = Nd4j.create(new double[][]{{1,0,0},{0,1,0}});
        expOut[2] = Nd4j.create(4,3,2);
        for( int i=0; i&lt;4; i++ ){
            expOut[2].get(NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.all()).assign(expOut[1]);
        }
        expOut[3] = Nd4j.create(3,3,3,2);
        for( int i=0; i&lt;3; i++ ){
            for( int j=0; j&lt;3; j++ ) {
                expOut[3].get(NDArrayIndex.point(i), NDArrayIndex.point(j), NDArrayIndex.all(), NDArrayIndex.all()).assign(expOut[1]);
            }
        }


        for(int i=0; i&lt;3; i++ ) {
            INDArray out = Nd4j.create(expOut[i].shape());

            DynamicCustomOp.DynamicCustomOpsBuilder op = DynamicCustomOp.builder("eye")
                    .addOutputs(out)
                    .addIntegerArguments(rows[i], cols[i]);
            if(batch[i] != null){
                op.addIntegerArguments(batch[i]);
            }

            Nd4j.getExecutioner().exec(op.build());

            assertEquals(expOut[i], out);
        }
    }
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Stack op - validation issue
Confirmed fixed 22/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Diag Part
Confirmed fixed 22/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

permute
Confirmed fixed 18/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Moments
21/06: Bad test, op returns variance.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

normalize moments
Bad test, 23/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

zeros_like and ones_like
Confirmed OK 21/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

onehot
Passing 21/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

entropy op
Fixed 21/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

IsFinite, IsInfinite, IsMax, IsNan
Edit: just bad test on my part.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Random Exponential
Confirmed passing, 26/06
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Reduction backprop ops: multiple failing.
testMinAlongDimensionBp()
(not zeroing out the non-min values)
-&gt; Same issue for testMaxAlongDimensionBP, testNormMaxAlongDimensionBP
Fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5971&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5971&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Local Response Normalization
This op is failing gradient checks. I don't know if it's due to forward, backward or both.
All gradient sare failing with no real patterns (other than being way bigger than they should be).
Test here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/f0a1c1f12785a0e5088dd8400826755e0229aa6d/nd4j/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/autodiff/opvalidation/LayerOpValidation.java#L128&gt;https://github.com/deeplearning4j/deeplearning4j/blob/f0a1c1f12785a0e5088dd8400826755e0229aa6d/nd4j/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/autodiff/opvalidation/LayerOpValidation.java#L128&lt;/denchmark-link&gt;

I had a look through the args; they are being passed correctly as far as I can tell from Java. Beyond that, I couldn't work out the cause looking at the libnd4j implementation.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

slice and strided_slice
It is possible to crash the JVM by passing invalid indices to slice (and, looking at the code: strided slice also). For example, size of 1000 for one of the dimensions indexing a 10x10 array.
Still possible to crash JVM as of 21/06.
Slice fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5682/files&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5682/files&lt;/denchmark-link&gt;

22/06 - (I haven't rebuilt to test). Not sure about strided_slice.
	</description>
	<comments>
		<comment id='8' author='AlexDBlack' date='2018-06-01T11:39:48Z'>
		
Another issue: Eye op:

The requiring of one input array is actually formal here, we just need to satisfy c++ signature of CUSTOM_OP_IMPL, because If I remember correctly each c++ operation requires at least one input array (raver may confirm or not this statement since he designed CUSTOM_OP_IMPL).
In fact we don't use input array in code:
CUSTOM_OP_IMPL(eye, 1, 1, false, 0, 2) {
helpers::eye(*OUTPUT_VARIABLE(0));
return Status::OK();
}
and one can simply pass empty array at all
		</comment>
		<comment id='9' author='AlexDBlack' date='2018-06-01T16:18:22Z'>
		it can be negative, so there will be no input required. not a big deal.
we use inputs only to ensure graph consistency.
		</comment>
		<comment id='10' author='AlexDBlack' date='2018-06-02T01:44:28Z'>
		Shape calculation for Unstack op is incorrect, at least for some edge cases:
It's returning a rank 1 array here, when it should be returning a rank 2 array
Edit: confirmed fixed
		</comment>
		<comment id='13' author='AlexDBlack' date='2018-06-02T10:01:13Z'>
		Average Pooling 2D - Backprop fixed
		</comment>
		<comment id='14' author='AlexDBlack' date='2018-06-02T14:41:05Z'>
		HI, Alex.
Concerning "Average Pooling 2D - Backprop".
Pull request "Shyrma helpers &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/5293&gt;#5293&lt;/denchmark-link&gt;
" contains rewritten/profiled pooling ops.
I hope there are no mistakes in this pr.
However we are still waiting for this pr to be merged.
		</comment>
		<comment id='15' author='AlexDBlack' date='2018-06-02T19:06:25Z'>
		We've been waiting for CI to merge things. Now CI works for libnd4j, and once jenkins confirms  - things will be merged.
		</comment>
		<comment id='16' author='AlexDBlack' date='2018-06-19T13:01:19Z'>
		Regarding "avgpool3dnew (edit: maxpool3dnew also) Added 19/06"
The infinite loop was caused by zero dilation.
So the reasons of fail are :

bad java test (with dilation = 0)
absence of corresponding validation on c++ side (has been fixed  now)

		</comment>
		<comment id='17' author='AlexDBlack' date='2018-06-19T13:07:23Z'>
		Regarding "Ops requiring backprop at libnd4j level (a non-exhaustive list) conv3d"
At present we use conv3dnew instead of  conv3d,
and conv3dnew_bp is available
		</comment>
		<comment id='18' author='AlexDBlack' date='2018-06-19T15:10:39Z'>
		Regarding "clip by norm along whole dimensions "
Seems like your output:
[0.6404504, 0.505514, 0.14394252, 0.0017040009, 0.55996114, 0.3924779, 0.7414847, 0.4127324, 0.24026828, 0.26093036, 0.46741188, 0.01863421, 0.08528871, 0.529365, 0.5510694]
is not correct because its norm is ~1.67
As for me I got:
[0.405392, 0.319980, 0.091113, 0.001079, 0.354444, 0.225846, 0.426676, 0.237501, 0.138259, 0.150149, 0.268965, 0.010723, 0.049078, 0.304615, 0.317105]
Regarding "clip by norm along dimension 0 "
Since I could not deduce input array content, I wrote similar test and compared its output to tf output and both were identaical
&lt;denchmark-code&gt;    
    NDArray&lt;double&gt; x('c', {3, 5});
    NDArray&lt;double&gt; exp('c', {3, 5}, {1.,  2.,  2.89271,  3.50524,  4.00892, 6.,  7.,  7.71389,  7.88678,  8.01784, 11., 12., 12.53507, 12.26833, 12.02676});    

    NDArrayFactory&lt;double&gt;::linspace(1, x);

    nd4j::ops::clipbynorm&lt;double&gt; op;
    auto result = op.execute({&amp;x}, {15.f}, {0});
    auto output = result-&gt;at(0);
        
    ASSERT_TRUE(exp.isSameShape(output));
    ASSERT_TRUE(exp.equalsTo(output));

    delete result;
}

&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='AlexDBlack' date='2018-06-20T01:56:13Z'>
		
Regarding "clip by norm along whole dimensions "

Are you referring to testClipByNorm()? That is along dimension 1, not clipping on the whole array.
However, the test is missing an int arg (i.e., bad test). If I add that, it passes.
testClipByNorm0() and testClipByNorm1() (i.e., along dimension 0 and 1) seem to be passing now
		</comment>
		<comment id='20' author='AlexDBlack' date='2018-06-20T10:54:00Z'>
		Regarding testMeanAlongDim0BP
Can't confirm this, below is analogous  c++ test. It produces exactly your expected numbers
Maybe we have input arguments discrepancy again ...
&lt;denchmark-code&gt;////////////////////////////////////////////////////////////////////////////////
TEST_F(DeclarableOpsTests8, reduceMeanBP_test4) {

    NDArray&lt;double&gt; x('c', {3, 4}, {1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12. });
    NDArray&lt;double&gt; gradO1('c', {4}, {1., 2., 3., 4.});
    NDArray&lt;double&gt; gradO2('c', {1, 4}, {1., 2., 3., 4.});
    NDArray&lt;double&gt; exp('c', {3,4}, {0.333333, 0.666667, 1.000000, 1.333333, 0.333333, 0.666667, 1.000000, 1.333333, 0.333333, 0.666667, 1.000000, 1.333333});
                                     
    nd4j::ops::reduce_mean_bp&lt;double&gt; op;

    auto result = op.execute({&amp;x, &amp;gradO1}, {0}, {0});
    ASSERT_EQ(ND4J_STATUS_OK, result-&gt;status());    
    auto output = result-&gt;at(0);    
    ASSERT_TRUE(exp.isSameShape(output));
    ASSERT_TRUE(exp.equalsTo(output));
    delete result;

    result = op.execute({&amp;x, &amp;gradO2}, {1}, {0});
    ASSERT_EQ(ND4J_STATUS_OK, result-&gt;status());    
    output = result-&gt;at(0);    
    ASSERT_TRUE(exp.isSameShape(output));
    ASSERT_TRUE(exp.equalsTo(output));
    delete result;
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='AlexDBlack' date='2018-06-20T13:07:18Z'>
		[ RUN      ] DeclarableOpsTests8.NormalizeMoments_SGO_1
Means [46.000000, 47.000000, 48.000000, 49.000000, 50.000000, 51.000000, 52.000000, 53.000000, 54.000000, 55.000000]
Variance [-2024.333374, -2117.333252, -2212.333252, -2309.333252, -2408.333252, -2509.333252, -2612.333252, -2717.333252, -2824.333252, -2933.333252]
[       OK ] DeclarableOpsTests8.NormalizeMoments_SGO_1 (0 ms)
///
this is a test for normalize_moments with given input.
		</comment>
		<comment id='22' author='AlexDBlack' date='2018-06-20T13:51:29Z'>
		Regarding eye op
Now it is corrected and can take zero or any other number (for graph consistency) of input arrays
		</comment>
		<comment id='23' author='AlexDBlack' date='2018-06-20T14:11:34Z'>
		Regarding Stack op - validation issue
below is explanation of dimension(axis) int argument from tf doc page (&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/stack&gt;https://www.tensorflow.org/api_docs/python/tf/stack&lt;/denchmark-link&gt;
):
axis: An int. The axis to stack along. Defaults to the first dimension. Negative values wrap around, so the valid range is [-(R+1), R+1).
If i understand correctly right round bracket means "excluding" (as for left square bracket, I think it is typo and should be round instead)
		</comment>
		<comment id='24' author='AlexDBlack' date='2018-06-20T15:12:32Z'>
		zeros_like and ones_like
Should work now
&lt;denchmark-code&gt;////////////////////////////////////////////////////////////////////////////////
TEST_F(DeclarableOpsTests8, zeros_as_test1) {

    NDArray&lt;double&gt; x(10.);
    NDArray&lt;double&gt; y(100.);
    NDArray&lt;double&gt; exp(0.);
                                                                          
    nd4j::ops::zeros_as&lt;double&gt; op;

    Nd4jStatus status = op.execute({&amp;x}, {&amp;y}, {}, {});
    ASSERT_EQ(ND4J_STATUS_OK, status);    
    
    ASSERT_TRUE(y.isSameShape(exp));
    ASSERT_TRUE(y.equalsTo(exp));

}

////////////////////////////////////////////////////////////////////////////////
TEST_F(DeclarableOpsTests8, ones_as_test1) {

    NDArray&lt;double&gt; x(10.);
    NDArray&lt;double&gt; y(100.);
    NDArray&lt;double&gt; exp(1.);

    nd4j::ops::ones_as&lt;double&gt; op;

    Nd4jStatus status = op.execute({&amp;x}, {&amp;y}, {}, {});
    ASSERT_EQ(ND4J_STATUS_OK, status);    
    
    ASSERT_TRUE(y.isSameShape(exp));
    ASSERT_TRUE(y.equalsTo(exp));        
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='25' author='AlexDBlack' date='2018-06-21T01:12:13Z'>
		I have updated the issue.
Now passing: testCumSum, testCumProd.
Moments was bad test.
Eye: I am seeing a NPE at the java level now. Will look into that.
OneHot fixed. Entropy fixed.
PR open for reduction BP, hopefully fixed.

If i understand correctly right round bracket means "excluding" (as for left square bracket, I think it is typo and should be round instead)

Yes, standard notation is "square brackets including, round brackets excluding". It's not a typo.
so [-(R+1), R+1) means lowest valid value is -(R+1) and highest valid value is R

zeros_like and ones_like - Should work now

95% sure this is fixed... seeing some issues at Java level (probably unrelated), will confirm later.
		</comment>
		<comment id='26' author='AlexDBlack' date='2018-06-22T12:52:43Z'>
		[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from DeclarableOpsTests8
[ RUN      ] DeclarableOpsTests8.NormalizeMoments_SGO_1
Means [46.000000, 47.000000, 48.000000, 49.000000, 50.000000, 51.000000, 52.000000, 53.000000, 54.000000, 55.000000]
Variance [825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000]
Expected [825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000, 825.000000]
Expected means [46.000000, 47.000000, 48.000000, 49.000000, 50.000000, 51.000000, 52.000000, 53.000000, 54.000000, 55.000000]
[       OK ] DeclarableOpsTests8.NormalizeMoments_SGO_1 (5 ms)
[----------] 1 test from DeclarableOpsTests8 (5 ms total)
[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (5 ms total)
[  PASSED  ] 1 test.
this is output for test for normalize_moments with gitven by java input.
Also python output (TF) with code:
###############################################################
import tensorflow as tf
x = tf.constant([[ 1,  2,  3,  4,  5,  6,  7,  8,  9,  10],
[11, 12, 13, 14, 15, 16, 17, 18, 19,  20],
[21, 22, 23, 24, 25, 26, 27, 28, 29,  30],
[31, 32, 33, 34, 35, 36, 37, 38, 39,  40],
[41, 42, 43, 44, 45, 46, 47, 48, 49,  50],
[51, 52, 53, 54, 55, 56, 57, 58, 59,  60],
[61, 62, 63, 64, 65, 66, 67, 68, 69,  70],
[71, 72, 73, 74, 75, 76, 77, 78, 79,  80],
[81, 82, 83, 84, 85, 86, 87, 88, 89,  90],
[91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
], dtype=tf.float32)
ssSum = tf.reduce_sum(x, (0));
ssSquaredSum = tf.reduce_sum(tf.multiply(x,x), (0));
with tf.Session() as sess:
print ssSum.eval()
print ssSquaredSum.eval()
y = tf.nn.normalize_moments(10.0, ssSum, ssSquaredSum, None)
print y[0].eval()
print y[1].eval()
#############################################################################
2018-06-22 15:51:17.058519: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
[460. 470. 480. 490. 500. 510. 520. 530. 540. 550.]
[29410. 30340. 31290. 32260. 33250. 34260. 35290. 36340. 37410. 38500.]
[46. 47. 48. 49. 50. 51. 52. 53. 54. 55.]
[825. 825. 825. 825. 825. 825. 825. 825. 825. 825.]
		</comment>
		<comment id='27' author='AlexDBlack' date='2018-06-22T12:54:40Z'>
		&lt;denchmark-link:https://github.com/shugeo&gt;@shugeo&lt;/denchmark-link&gt;
 for moments op, I don't think the problem is the op implementation per-se - I think the probelm is that the result isn't ending up in the array that is passed from Java.
		</comment>
		<comment id='28' author='AlexDBlack' date='2018-06-22T16:35:24Z'>
		So, the same issue as i've fixed for reduce_mean.
		</comment>
		<comment id='29' author='AlexDBlack' date='2018-06-22T17:03:03Z'>
		No, it's not the same issue. It's multiple issues on java side.

shift argument was missing, so op actually was throwing exception swallowed by Op.validation
AbstractList shouldn't be used as holder for TArgs and IArgs
test itself is probably wrong

&lt;denchmark-code&gt;java.lang.AssertionError: expected null, but was:&lt;Output 1 failed: INDArray equality failed:
Expected:
[[  916.6667,  916.6667,  916.6667,  916.6667,  916.6667,  916.6667,  916.6667,  916.6667,  916.6667,  916.6667]]
Actual:
[[  825.0000,  825.0000,  825.0000,  825.0000,  825.0000,  825.0000,  825.0000,  825.0000,  825.0000,  825.0000]]&gt;
&lt;/denchmark-code&gt;

So, actual result is the same result TF produces. But assertion fails. So, expectations are wrong?
		</comment>
		<comment id='30' author='AlexDBlack' date='2018-06-23T05:43:02Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 yes, you are right about normalize_moments
2 issues here:
(a) missing shift arg (it's optional for TF, but I don't mind having it required)
(b) it calculates non-bias-corrected variance (test used bias corrected)
With those changes, test now passes.
		</comment>
		<comment id='31' author='AlexDBlack' date='2018-06-23T16:34:39Z'>
		Random Exponential
If I understand correctly, the equality mean=std=1/lambda is not true for probability of exponential distribution:
F = 1 - exp(-lambda*x),   -inf &lt; x &lt; +inf,  and if  x&lt;0 then F=0
more precisely: mean(F) != 1/lambda
however for inverse exponential distribution:
x = -log(1 - F) / lambda,    0 &lt;= F &lt; 1
following equality is true:  mean(x) == 1/lambda
		</comment>
		<comment id='32' author='AlexDBlack' date='2018-06-25T00:47:45Z'>
		&lt;denchmark-link:https://github.com/shyrma&gt;@shyrma&lt;/denchmark-link&gt;
 I am assuming that the "exponential distribution" op is actually implementing random sampling from the negative exponential distribution:
p(x) = lambda * exp(-lambda * x)
&lt;denchmark-link:https://en.wikipedia.org/wiki/Exponential_distribution&gt;https://en.wikipedia.org/wiki/Exponential_distribution&lt;/denchmark-link&gt;

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/distributions/Exponential&gt;https://www.tensorflow.org/api_docs/python/tf/distributions/Exponential&lt;/denchmark-link&gt;

&lt;denchmark-link:http://www.wolframalpha.com/input/?i=negative+exponential+distribution&gt;http://www.wolframalpha.com/input/?i=negative+exponential+distribution&lt;/denchmark-link&gt;

The mean and variance (stdev) are from the first link.
Do you have a reference for the math there? Looks like you are quoting the CDF of the (negative) exponential distribution, the mean/variance (1/lambda for both) for which is available at the above links.
		</comment>
		<comment id='33' author='AlexDBlack' date='2018-06-25T10:48:54Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;

It may be my last comment was not so clear, sorry about that.
I meant I'm not sure what is
RandomExponential op = new RandomExponential(shape, out, lambda);
?

Is that pdf p(x) = lambda * exp(-lambda * x)  ?
Is that cdf F(x) = 1 -   exp(-lambda * x)   ?
is that icdf (inverse) x = -log(1 - F) / lambda  ?

as for c++ side, currently we have
exponential distribution =  1 -   exp(-lambda * x)
And it should be mentioned  x is uniformly distributed random variable in range (0, 1], which is wrong i think
So if you mean to use cdf, then I suspect we have wrong random variable range in c++: should be (-max, max) instead of (0,1]
		</comment>
		<comment id='34' author='AlexDBlack' date='2018-06-26T13:26:44Z'>
		Local Response Normalization
Forward test is successful:
python output is the same as libnd4j:
import tensorflow as tf
import numpy as np
xVal = np.arange(1, 226)
xVal4 = np.reshape(xVal, (3,3,5,5))
print xVal4
x = tf.constant(xVal4, dtype=tf.float64) #([[[1.8, 2.5, 4.0, 9.0],[2.1, 2.4, 3.0, 9.0]], [[2.1, 2.1, 0.7, 0.1], [3.0, 4.2, 2.2, 1.0]]])
#size = tf.constant([2, 2, 3])
cr = tf.nn.local_response_normalization( x, depth_radius=2,  bias=1, alpha=1, beta=0.5, name=None)
exp4 = np.reshape(exp, (3,3,5,5))
with tf.Session() as sess:
print x.eval()
print '----------------------------------------------------------------'
print cr.eval()
		</comment>
		<comment id='35' author='AlexDBlack' date='2018-07-10T00:08:06Z'>
		Not sure why this issue was closed. I still see cumsum/cumprod gradients failing.
		</comment>
		<comment id='36' author='AlexDBlack' date='2018-07-10T17:27:28Z'>
		cumsum gradients pass
		</comment>
		<comment id='37' author='AlexDBlack' date='2018-09-05T09:50:04Z'>
		Hi Alex
back propagation for conv3d is already available, however it has slightly different name ;-)
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/a300b32f690b9c50b4d0e8d62dbf19b9c6720680/libnd4j/include/ops/declarable/generic/convo/conv3d.cpp#L168&gt;https://github.com/deeplearning4j/deeplearning4j/blob/a300b32f690b9c50b4d0e8d62dbf19b9c6720680/libnd4j/include/ops/declarable/generic/convo/conv3d.cpp#L168&lt;/denchmark-link&gt;

		</comment>
		<comment id='38' author='AlexDBlack' date='2018-09-21T08:13:26Z'>
		I think most of these are fixed. I'll open new/separate issues if required.
		</comment>
		<comment id='39' author='AlexDBlack' date='2018-10-21T08:55:10Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>