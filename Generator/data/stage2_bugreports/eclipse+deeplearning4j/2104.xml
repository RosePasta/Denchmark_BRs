<bug id='2104' author='anovstrup' open_date='2016-09-14T22:35:58Z' closed_time='2016-10-03T21:00:34Z'>
	<summary>Poor GPU performance relative to CPU</summary>
	<description>
This issue is similar to &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/2053&gt;#2053&lt;/denchmark-link&gt;
, with a few key differences:

I'm using a recurrent neural network.
Performance is consistently and significantly worse with the GPU backend (not merely comparable).
I have a small dataset (16x9x308), which is probably at least partly to blame. However, the nsight profiling results with a larger dataset may suggest that DL4J can close the gap somewhat (see below).

I'm using the latest release of DeepLearning4J (0.5.0).
Minimized Test Case
package com.stottlerhenke.illuminate;

import java.util.List;

import com.stottlerhenke.illuminate.training.PerformanceListener;

import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.*;
import org.deeplearning4j.nn.conf.layers.GravesLSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.DataSetPreProcessor;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions;

public class GpuProblem {

    public static int timeSeriesCount = 16;
    public static int inputCount = 9;
    public static int timeSteps = 308;
    public static int numHiddenNodes = 20;
    public static int truncatedBPTTLength = 100;
    public static int epochCount = 3;

    public static class MyDatasetIterator implements DataSetIterator {

        private DataSetPreProcessor preProcessor;

        int cursor = 0;

        @Override
        public boolean hasNext() {
            return cursor &lt; epochCount;
        }

        @Override
        public DataSet next() {
            return next(1);
        }

        @Override
        public DataSet next(int num) {

            DataSet ds = createDataset();
            if (preProcessor != null)
                preProcessor.preProcess(ds);
            cursor += num;
            System.out.println("NEW CURSOR " + cursor);
            return ds;
        }

        private DataSet createDataset() {
            INDArray createDataSetinput = Nd4j.zeros(timeSeriesCount, inputCount, timeSteps);
            INDArray createDataSetlabels = Nd4j.zeros(timeSeriesCount, 2, timeSteps);
            return new DataSet(createDataSetinput, createDataSetlabels);
        }

        @Override
        public int totalExamples() {
            throw new UnsupportedOperationException();
        }

        @Override
        public int inputColumns() {
            return inputCount;
        }

        @Override
        public int totalOutcomes() {
            return 2;
        }

        @Override
        public boolean resetSupported() {
            return true;
        }

        @Override
        public void reset() {
            cursor = 0;
            System.out.println("RESET NEW CURSOR " + cursor);
        }

        @Override
        public int batch() {
            return timeSeriesCount;
        }

        @Override
        public int cursor() {
            throw new UnsupportedOperationException();
        }

        @Override
        public int numExamples() {
            throw new UnsupportedOperationException();
        }

        @Override
        public void setPreProcessor(DataSetPreProcessor preProcessor) {
            this.preProcessor = preProcessor;

        }

        @Override
        public DataSetPreProcessor getPreProcessor() {
            return this.preProcessor;
        }

        @Override
        public List&lt;String&gt; getLabels() {
            return null;
        }

    }

    public static void main(String[] args) {
        MultiLayerConfiguration.Builder builder =
                new NeuralNetConfiguration.Builder()
                        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)
                        .updater(Updater.ADAM).adamMeanDecay(0.9).adamVarDecay(0.999)
                        .regularization(true).l1(1e-4).dropOut(0.5)
                        .weightInit(WeightInit.XAVIER)
                        .learningRate(5e-4)
                        .list()
                        .layer(0, new GravesLSTM.Builder().nIn(inputCount).nOut(numHiddenNodes)
                                .activation("tanh").build())
                        .layer(1, new RnnOutputLayer.Builder().nIn(numHiddenNodes)
                                .activation("softmax")
                                .lossFunction(LossFunctions.LossFunction.MCXENT)
                                .nIn(numHiddenNodes).nOut(2).build())
                        .pretrain(false)
                        .backprop(true)
                        .backpropType(BackpropType.TruncatedBPTT)
                   .tBPTTBackwardLength(truncatedBPTTLength).tBPTTForwardLength(truncatedBPTTLength);

        MultiLayerNetwork net = new MultiLayerNetwork(builder.build());
        net.init();

        net.setListeners(new PerformanceListener(1));

        net.fit(new MyDatasetIterator());
    }
}
Performance Results
CPU
&lt;denchmark-code&gt;iteration 1; iteration time: 341 ms; samples/sec: 46.921; batches/sec: 2.933;
iteration 2; iteration time: 170 ms; samples/sec: 94.118; batches/sec: 5.882;
iteration 3; iteration time: 158 ms; samples/sec: 101.266; batches/sec: 6.329;
iteration 4; iteration time: 181 ms; samples/sec: 88.398; batches/sec: 5.525;
iteration 5; iteration time: 127 ms; samples/sec: 125.984; batches/sec: 7.874;
iteration 6; iteration time: 122 ms; samples/sec: 131.148; batches/sec: 8.197;
iteration 7; iteration time: 122 ms; samples/sec: 131.148; batches/sec: 8.197;
iteration 8; iteration time: 119 ms; samples/sec: 134.454; batches/sec: 8.403;
iteration 9; iteration time: 119 ms; samples/sec: 134.454; batches/sec: 8.403;
&lt;/denchmark-code&gt;

GPU
&lt;denchmark-code&gt;iteration 1; iteration time: 1667 ms; samples/sec: 9.598; batches/sec: 0.600;
iteration 2; iteration time: 1266 ms; samples/sec: 12.638; batches/sec: 0.790;
iteration 3; iteration time: 1241 ms; samples/sec: 12.893; batches/sec: 0.806;
iteration 4; iteration time: 1192 ms; samples/sec: 13.423; batches/sec: 0.839;
iteration 5; iteration time: 1204 ms; samples/sec: 13.289; batches/sec: 0.831;
iteration 6; iteration time: 1178 ms; samples/sec: 13.582; batches/sec: 0.849;
iteration 7; iteration time: 1137 ms; samples/sec: 14.072; batches/sec: 0.880;
iteration 8; iteration time: 1141 ms; samples/sec: 14.023; batches/sec: 0.876;
iteration 9; iteration time: 1183 ms; samples/sec: 13.525; batches/sec: 0.845;
&lt;/denchmark-code&gt;

I also tested with more data by increasing the number of time series from 16 to 16,000. With this change, the GPU does outperform the CPU (as expected); however, profiling with nsight shows only 9.1% GPU utilization (although I was only able to profile one iteration due to an issue with nsight that caused the test application to terminate early with an access violation, and the utilization percentage is probably unduly influenced by the time period before network training begins).
	</description>
	<comments>
		<comment id='1' author='anovstrup' date='2016-09-14T22:50:15Z'>
		Performance listener output for 16k time series count:
CPU
&lt;denchmark-code&gt;iteration 1; iteration time: 7332 ms; samples/sec: 2182.215; batches/sec: 0.136;
iteration 2; iteration time: 7488 ms; samples/sec: 2136.752; batches/sec: 0.134;
iteration 3; iteration time: 7410 ms; samples/sec: 2159.244; batches/sec: 0.135;
iteration 4; iteration time: 7426 ms; samples/sec: 2154.592; batches/sec: 0.135;
iteration 5; iteration time: 7472 ms; samples/sec: 2141.328; batches/sec: 0.134;
iteration 6; iteration time: 9813 ms; samples/sec: 1630.490; batches/sec: 0.102;
&lt;/denchmark-code&gt;

GPU
&lt;denchmark-code&gt;iteration 1; iteration time: 7090 ms; samples/sec: 2256.699; batches/sec: 0.141;
iteration 2; iteration time: 4913 ms; samples/sec: 3256.666; batches/sec: 0.204;
iteration 3; iteration time: 4931 ms; samples/sec: 3244.778; batches/sec: 0.203;
iteration 4; iteration time: 5129 ms; samples/sec: 3119.516; batches/sec: 0.195;
iteration 5; iteration time: 4910 ms; samples/sec: 3258.656; batches/sec: 0.204;
iteration 6; iteration time: 4995 ms; samples/sec: 3203.203; batches/sec: 0.200;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='anovstrup' date='2016-09-15T00:09:43Z'>
		So you have 2 layers, one of size 20, and one of size 2... with minibatch size 16... of course it's going to be slow on GPU.
GPUs need a sufficient amount of parallelism for good performance; your network and data is way too small to get anywhere close to full efficiency.
		</comment>
		<comment id='3' author='anovstrup' date='2016-09-15T00:19:31Z'>
		Important hardware specs: I ran these tests on a machine with 2 6-core Xeon E5-2620V2 processors. The GPU is a GeForce GLX 980 Ti.
		</comment>
		<comment id='4' author='anovstrup' date='2016-10-03T21:00:34Z'>
		So, looks like &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 is totally right here, and increasing performance with higher amounts of input data confirms that.
		</comment>
		<comment id='5' author='anovstrup' date='2019-01-20T17:57:08Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>