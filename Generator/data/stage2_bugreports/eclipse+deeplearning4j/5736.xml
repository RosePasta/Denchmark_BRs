<bug id='5736' author='AlexDBlack' open_date='2018-06-28T09:55:22Z' closed_time='2018-06-28T12:40:18Z'>
	<summary>SameDiff: Gather op test failing</summary>
	<description>
I believe this is a java side failure due to Gather.doDiff()
&lt;denchmark-code&gt;o.n.l.a.o.DynamicCustomOp - Populating inputs and outputs for op null: Successful
Error at [D:/jenkins/ws/dl4j-master-windows-x86_64-cpu/libnd4j/include/ops/declarable/generic/helpers/ScatterHelper.h:53:0]:
scatter_add: updates shapes should match
o.n.l.c.n.o.NativeOpExecutioner - Failed to execute op scatter_add. Attempted to execute with 3 inputs, 1 outputs, 0 targs and 0 iargs. Please see above message (printed out from c++) for a possible cause of error.

java.lang.IllegalStateException: Exception encountered during gradient check
	at org.nd4j.autodiff.validation.OpValidation.validateHelper(OpValidation.java:150)
	at org.nd4j.autodiff.validation.OpValidation.validate(OpValidation.java:92)
	at org.nd4j.autodiff.validation.OpValidation.validate(OpValidation.java:87)
	at org.nd4j.autodiff.opvalidation.ShapeOpValidation.testGather(ShapeOpValidation.java:1238)
Caused by: java.lang.RuntimeException: Op validation failed
	at org.nd4j.nativeblas.Nd4jCpu$NativeOps.execCustomOpDouble(Native Method)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1732)
	at org.nd4j.autodiff.samediff.SameDiff.exec(SameDiff.java:7406)
	at org.nd4j.autodiff.samediff.SameDiff.exec(SameDiff.java:6295)
	at org.nd4j.autodiff.samediff.SameDiff.execBackwards(SameDiff.java:6333)
	at org.nd4j.autodiff.validation.GradCheckUtil.checkGradients(GradCheckUtil.java:239)
	at org.nd4j.autodiff.validation.GradCheckUtil.checkGradients(GradCheckUtil.java:166)
	at org.nd4j.autodiff.validation.OpValidation.validateHelper(OpValidation.java:148)
	... 35 more
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;    @Test
    public void testGather(){
        OpValidationSuite.ignoreFailing();  //Exception during gradient check
        List&lt;INDArray&gt; inArrs = new ArrayList&lt;&gt;();
        List&lt;Integer&gt; axis = new ArrayList&lt;&gt;();
        List&lt;INDArray&gt; indices = new ArrayList&lt;&gt;();

        inArrs.add(Nd4j.linspace(1,48,48).reshape(2,4,3,2));
        indices.add(Nd4j.trueVector(new double[]{1,0}));
        axis.add(-2);

        for( int i=0; i&lt;inArrs.size(); i++ ){

            INDArray in = inArrs.get(i);
            INDArray idx = indices.get(i);
            int a = axis.get(i);
            int aNorm = (a &gt;= 0 ? a : a + in.rank());

            INDArray expOut;
            if(idx.rank() == 0){
                INDArrayIndex[] get = new INDArrayIndex[in.rank()];
                for( int j=0; j&lt;aNorm; j++ ){
                    get[j] = NDArrayIndex.all();
                }
                get[aNorm] = NDArrayIndex.point(idx.getInt(0));
                for( int j=aNorm+1; j&lt;in.rank(); j++ ){
                    get[j] = NDArrayIndex.all();
                }
                expOut = in.get(get);
            } else if (idx.rank() == 1){
                long[] shape = in.shape().clone();
                shape[aNorm] = idx.length();
                expOut = Nd4j.create(shape);

                INDArrayIndex[] get = new INDArrayIndex[in.rank()];
                INDArrayIndex[] put = new INDArrayIndex[in.rank()];
                for( int j=0; j&lt;aNorm; j++ ){
                    get[j] = NDArrayIndex.all();
                    put[j] = NDArrayIndex.all();
                }
                for( int j=aNorm+1; j&lt;in.rank(); j++ ){
                    get[j] = NDArrayIndex.all();
                    put[j] = NDArrayIndex.all();
                }

                for(int j=0; j&lt;idx.length(); j++ ){
                    get[aNorm] = NDArrayIndex.point(idx.getInt(j));
                    put[aNorm] = NDArrayIndex.point(j);
                    expOut.put(put, in.get(get));
                }
            } else {
                throw new RuntimeException("Rank 2+ tests not yet implemented");
            }


            SameDiff sd = SameDiff.create();
            SDVariable sdIn = sd.var("in", in);
            SDVariable sdIdx = sd.var("idx", idx);
            SDVariable gather = sd.gather(sdIn, sdIdx, a);

            SDVariable loss = gather.std(true);

            String err = OpValidation.validate(new TestCase(sd)
                    .expected(gather, expOut)
                    .gradCheckSkipVariables("idx"));

            assertNull(err);
        }
    }
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-06-28T12:40:18Z'>
		&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/5740&gt;#5740&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='AlexDBlack' date='2018-06-28T12:46:24Z'>
		&lt;denchmark-link:https://github.com/farizrahman4u&gt;@farizrahman4u&lt;/denchmark-link&gt;
 Thanks  :)
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-09-21T16:59:32Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>