<bug id='5423' author='AlexDBlack' open_date='2018-06-01T08:59:50Z' closed_time='2018-07-25T08:00:31Z'>
	<summary>DL4J: Workspace closing not robust enough under all possible errors</summary>
	<description>
After a whole lot of errors (wrong input and memory errors) eventually a forward pass workspace isn't closed properly:
&lt;denchmark-code&gt;08:07:09.502 [application-akka.actor.default-dispatcher-13] ERROR io.skymind.skil.play.Results - Error [POST] /predict
java.lang.RuntimeException: Failed to allocate 29503488 bytes from DEVICE [0] memory
	at org.nd4j.jita.memory.CudaMemoryManager.allocate(CudaMemoryManager.java:60)
	at org.nd4j.jita.workspace.CudaWorkspace.alloc(CudaWorkspace.java:198)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:454)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:399)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:269)
	at org.nd4j.linalg.jcublas.buffer.CudaFloatDataBuffer.&lt;init&gt;(CudaFloatDataBuffer.java:64)
	at org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createFloat(CudaDataBufferFactory.java:323)
	at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1481)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.&lt;init&gt;(BaseNDArray.java:261)
	at org.nd4j.linalg.jcublas.JCublasNDArray.&lt;init&gt;(JCublasNDArray.java:113)
	at org.nd4j.linalg.jcublas.JCublasNDArrayFactory.createUninitialized(JCublasNDArrayFactory.java:262)
	at org.nd4j.linalg.factory.Nd4j.createUninitialized(Nd4j.java:5128)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:346)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:392)
	at org.deeplearning4j.nn.graph.vertex.impl.LayerVertex.doForward(LayerVertex.java:105)
	at org.deeplearning4j.nn.graph.ComputationGraph.outputOfLayersDetached(ComputationGraph.java:2156)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1591)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1576)
	at org.deeplearning4j.parallelism.ParallelInference$InferenceWorker.run(ParallelInference.java:382)
	Suppressed: java.lang.RuntimeException: Failed to allocate 29504512 bytes from DEVICE [0] memory
		at org.nd4j.jita.memory.CudaMemoryManager.allocate(CudaMemoryManager.java:60)
		at org.nd4j.jita.workspace.CudaWorkspace.init(CudaWorkspace.java:70)
		at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.initializeWorkspace(Nd4jWorkspace.java:486)
		at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.close(Nd4jWorkspace.java:632)
		at org.deeplearning4j.nn.graph.ComputationGraph.outputOfLayersDetached(ComputationGraph.java:2200)
		... 3 common frames omitted
08:07:09.503 [application-akka.actor.default-dispatcher-35] WARN  i.s.s.service.PredictionServiceImpl - Error generating prediction
java.lang.RuntimeException: Failed to allocate 12846080 bytes from DEVICE [0] memory
	at org.nd4j.jita.memory.CudaMemoryManager.allocate(CudaMemoryManager.java:60)
	at org.nd4j.jita.workspace.CudaWorkspace.init(CudaWorkspace.java:70)
	at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.initializeWorkspace(Nd4jWorkspace.java:486)
	at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.close(Nd4jWorkspace.java:632)
	at org.deeplearning4j.nn.graph.ComputationGraph.outputOfLayersDetached(ComputationGraph.java:2221)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1591)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1576)
	at org.deeplearning4j.parallelism.ParallelInference$InferenceWorker.run(ParallelInference.java:382)
08:07:09.503 [application-akka.actor.default-dispatcher-35] ERROR io.skymind.skil.play.Results - Error [POST] /predict
java.lang.RuntimeException: Failed to allocate 12846080 bytes from DEVICE [0] memory
	at org.nd4j.jita.memory.CudaMemoryManager.allocate(CudaMemoryManager.java:60)
	at org.nd4j.jita.workspace.CudaWorkspace.init(CudaWorkspace.java:70)
	at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.initializeWorkspace(Nd4jWorkspace.java:486)
	at org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.close(Nd4jWorkspace.java:632)
	at org.deeplearning4j.nn.graph.ComputationGraph.outputOfLayersDetached(ComputationGraph.java:2221)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1591)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1576)
	at org.deeplearning4j.parallelism.ParallelInference$InferenceWorker.run(ParallelInference.java:382)
08:07:09.536 [application-akka.actor.default-dispatcher-29] WARN  i.s.s.service.PredictionServiceImpl - Error generating prediction
java.lang.NullPointerException: null
08:07:09.536 [application-akka.actor.default-dispatcher-29] ERROR io.skymind.skil.play.Results - Error [POST] /predict
java.lang.NullPointerException: null
08:07:09.557 [application-akka.actor.default-dispatcher-33] INFO  i.s.s.service.PredictionServiceImpl - Submitting results
08:07:09.787 [application-akka.actor.default-dispatcher-32] WARN  i.s.s.service.PredictionServiceImpl - Error generating prediction
org.nd4j.linalg.workspace.ND4JWorkspaceException: Expected no workspace active before call to outputOfLayersDetached - Open/active workspaces: [WS_LAYER_ACT_0]
	at org.nd4j.linalg.workspace.WorkspaceUtils.assertNoWorkspacesOpen(WorkspaceUtils.java:38)
	at org.deeplearning4j.nn.graph.ComputationGraph.outputOfLayersDetached(ComputationGraph.java:2027)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1591)
	at org.deeplearning4j.nn.graph.ComputationGraph.output(ComputationGraph.java:1576)
	at org.deeplearning4j.parallelism.ParallelInference$InferenceWorker.run(ParallelInference.java:382)
&lt;/denchmark-code&gt;

Some thoughts from chat:

yeah, so there's a ton of errors (memory, wrong input) before the first workspace issue
so my guess is that one of them isn't caught properly
I wonder if the commit is a problem? if a memory exception occurs during the commit (that is part of workspace closing) maybe the workspace won't close...
or maybe we have multiple that need closing, and the commit failure stops the remaining ones from closing

1.0.0-beta, CUDA 9.1
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-06-01T16:49:42Z'>
		Is there some isolated code we could run?
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-06-02T00:23:46Z'>
		Not afaik, which is part of the problem... &lt;denchmark-link:https://github.com/wmeddie&gt;@wmeddie&lt;/denchmark-link&gt;
 brought this up on slack (part of a SKIL deplopment)
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-06-02T01:40:24Z'>
		Let me see if it shows up again on another server, and If so I'll try whipping something up.
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-07-25T08:00:31Z'>
		I'm going to close this - we haven't had any reports of this recently. And it's non-actionable if we can't reproduce/isolate it or validate if it's fixed...
		</comment>
		<comment id='5' author='AlexDBlack' date='2018-09-21T13:59:14Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>