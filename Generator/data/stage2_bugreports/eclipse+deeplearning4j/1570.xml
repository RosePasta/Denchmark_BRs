<bug id='1570' author='davireis' open_date='2016-05-20T02:29:14Z' closed_time='2016-05-21T04:43:31Z'>
	<summary>No spark 2.11 package for canova-spark</summary>
	<description>
Yesterday I was migrating from rc3.8 to rc3.9, and I see that dl4j-spark depends on spark-core_${scala.binary.version}, which is perfect because I can set this in my build to get the 2.11 spark version, compatible with the rest of my build. However, it also depends on canova-spark, which depends on the hardcoded spark-core_2.10. Can we use the same build setup from dl4j-spark for canova-spark?
	</description>
	<comments>
		<comment id='1' author='davireis' date='2016-05-21T04:43:30Z'>
		This is fixed now.  &lt;denchmark-link:https://github.com/deeplearning4j/Canova/pull/168&gt;https://github.com/deeplearning4j/Canova/pull/168&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='davireis' date='2019-01-21T02:52:46Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>