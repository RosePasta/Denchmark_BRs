<bug id='7125' author='SchmaR' open_date='2019-02-07T15:55:44Z' closed_time='2019-04-01T18:54:46Z'>
	<summary>ND4j: Memory leak if calling INDArray.getRow()</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Hi,
I'm working on the current snapshot build of DL4J/ND4J. If I call INDArray.getRow() this creates LongBuffer, AtomicBoolean and ArrayList instances which the garbage collector can't discard. This behavior leads finally to an OutOfMemoryException in the heap space. I encounter this problem if running a CpuBackend.
This error is not happening in Beta3. I have not tested if it is present on CUDA.
You can find minimal example code for reproduction here: &lt;denchmark-link:https://gist.github.com/SchmaR/5291bf21d812ac020307251eb4cdcf97&gt;https://gist.github.com/SchmaR/5291bf21d812ac020307251eb4cdcf97&lt;/denchmark-link&gt;

While I was investigating the problem, I noticed that it might be linked to a reference in DirectShapeInfoProvider.longCache, which seems to hold the last existing reference to the LongBuffer objects. Attached you find screenshots showing the reference chain as well as the stack trace of the initialitation of the LongBuffer objects.
&lt;denchmark-h:h5&gt;Backend Configuration Log&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;16:29:34.633 [main] INFO  org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend
16:29:35.346 [main] INFO  org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 4
16:29:35.608 [main] INFO  org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 4
16:29:35.616 [main] INFO  o.n.l.a.o.e.DefaultOpExecutioner - Backend used: [CPU]; OS: [Linux]
16:29:35.617 [main] INFO  o.n.l.a.o.e.DefaultOpExecutioner - Cores: [8]; Memory: [1.8GB];
16:29:35.617 [main] INFO  o.n.l.a.o.e.DefaultOpExecutioner - Blas vendor: [MKL]
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1245103/52423362-3753b700-2af8-11e9-976d-6b1847b08c7a.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1245103/52423395-4dfa0e00-2af8-11e9-9a39-041bfb4b1726.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1245103/52423430-6833ec00-2af8-11e9-9e37-6df8882f74ab.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version 1.0.0 SNAPSHOT build: nd4j-api-1.0.0-20190207.151315-14704.jar
platform information: Ubuntu 18.10, 64 Bit, Intel I7
CUDA version, if used
NVIDIA driver version, if in use

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

If I can help to figure out what's going wrong let me know.
Aha! Link: &lt;denchmark-link:https://skymindai.aha.io/features/ND4J-57&gt;https://skymindai.aha.io/features/ND4J-57&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='SchmaR' date='2019-02-07T16:27:07Z'>
		Not a bug.
Shape buffers are cached, reused. And never released. And since they are very small, you have to do something VERY unusual to make a problem out of that.
		</comment>
		<comment id='2' author='SchmaR' date='2019-02-07T16:27:38Z'>
		So, please tell us what you're doing there in order to help you.
		</comment>
		<comment id='3' author='SchmaR' date='2019-02-07T16:32:21Z'>
		Or, you mean that cache is broken somehow?
		</comment>
		<comment id='4' author='SchmaR' date='2019-02-07T16:38:36Z'>
		No, i've checked with debugger - after 100k iterations there's only 4 shapes in cache, and they are perfectly reused.
		</comment>
		<comment id='5' author='SchmaR' date='2019-02-07T16:39:19Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/12250879/52426931-08493f80-2b10-11e9-9173-f932515f47dc.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='SchmaR' date='2019-02-07T17:23:12Z'>
		Hmm, if I run the code in the gist on my machine I end up with a large amount of LongBuffers which are consuming all of my heap memory. I don't face the behavior on beta3 but in the current snapshot.
Is there any information I could provide which could help to figure out what's going wrong?
		</comment>
		<comment id='7' author='SchmaR' date='2019-02-07T17:26:38Z'>
		Where exactly you see those LongBuffers?
		</comment>
		<comment id='8' author='SchmaR' date='2019-02-07T17:33:25Z'>
		If I either use Visual JVM or the Memory profiling view in IntelliJ. According to Visual JVM the buffers are consuming the most of my memory. If I trigger a garbage collection step in Visual JVM none of the LongBuffers are cleaned up. (in the snapshot build) If I do the same on beta3 the LongBuffers get discarded and I don't run out of heap memory while iterating over indArray.getRow(i % 3).
		</comment>
		<comment id='9' author='SchmaR' date='2019-02-07T17:46:36Z'>
		Obviously, i'm using snapshots too, and i can't reproduce behavior you're describing. Maybe you're running some different code?
		</comment>
		<comment id='10' author='SchmaR' date='2019-02-07T23:25:32Z'>
		This is what I'm seeing (Snapshots, master, windows 10, native):
&lt;denchmark-link:https://gist.github.com/AlexDBlack/83041fa0511954f97cd85e9a6544f6cb&gt;https://gist.github.com/AlexDBlack/83041fa0511954f97cd85e9a6544f6cb&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/52449246-b85b8e80-2b8a-11e9-8393-e045c289ba48.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Here's the same thing slightly modified to add GC. The memory snapshot was collected after "DONE GC" was printed in the code.
&lt;denchmark-link:https://gist.github.com/AlexDBlack/db9b37bd041628f6c32b4acb64c7a1b2&gt;https://gist.github.com/AlexDBlack/db9b37bd041628f6c32b4acb64c7a1b2&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/52449512-be9e3a80-2b8b-11e9-8391-32205a8b4caa.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='11' author='SchmaR' date='2019-02-08T08:20:20Z'>
		Hmmm.... wrappedBuffer abused somewhere?
		</comment>
		<comment id='12' author='SchmaR' date='2019-03-06T14:58:45Z'>
		Hi,
I've further investigated this issue.
Every call of Shape.shapeOf() results in a new LongBuffer. (&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/fceb662a76e8f72f2eed7def33c727c4f9a3eb09/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/indexing/NDArrayIndex.java#L329&gt;NDArrayIndex.java:329&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/fceb662a76e8f72f2eed7def33c727c4f9a3eb09/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/indexing/NDArrayIndex.java#L300&gt;NDArrayIndex.java:300&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/eb6faca4b49456fd5c8b5f77d75a8daedbc0dca5/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ndarray/BaseNDArray.java#L3070&gt;BaseNDArray.java:3070&lt;/denchmark-link&gt;
) Let's name it "Shape LongBuffer". The (underlying) DataBuffer of the INDArray which calls it holds a reference to this new "Shape LongBuffer". The "underlying DataBuffer" is the result of INDArray.shapeInfoDataBuffer() of the original INDArray on which we call getRow(). (&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/6d36bbba5d7f58944e68d878d770a9a68f2247e0/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/indexing/ShapeOffsetResolution.java#L365&gt;ShapeOffsetResolution.java:365&lt;/denchmark-link&gt;
) This "underlying LongBuffer" is referenced by the "Shape LongBuffer" as wrappedBuffer. (&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/57ca916b9e7f325429c62a49cc942d1e705aae98/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/BaseDataBuffer.java#L176&gt;BaseDataBuffer.java:176&lt;/denchmark-link&gt;
) I assume that, because of these cyclic references, the garbage collection can't remove the no longer needed local instances. This holds even if INDArray.close() is called and null is assigned to the INDArray variable. This reference cycle seems to be not cleaned up at any point.
Before the creation of the Shape "LongBuffer" it is not checked if already a suitable Buffer exists. There are a lot of equal objects (&lt;denchmark-link:https://user-images.githubusercontent.com/1245103/53889157-32662280-4026-11e9-9e5a-4da00d0c67f3.png&gt;DataBuffer references seem to be all equal screenshot&lt;/denchmark-link&gt;
) with different  (&lt;denchmark-link:https://user-images.githubusercontent.com/1245103/53889075-15315400-4026-11e9-9dad-237c9cc21a2c.png&gt;DataBuffer references System.identityHashCode screenshot&lt;/denchmark-link&gt;
 ) building up.
Here is a visualization of the cyclic references &lt;denchmark-link:https://user-images.githubusercontent.com/1245103/53889628-039c7c00-4027-11e9-9a88-df1f5c293abf.png&gt;dataBufferReferences.png&lt;/denchmark-link&gt;

I've tested this using the following code based on &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
  code. (&lt;denchmark-link:https://gist.github.com/ad46ed6061c7d8b5471dbda66f69307d&gt;Gist MemoryLeakTest.java&lt;/denchmark-link&gt;
)
I've further investigated the matter and isolated a call of  (&lt;denchmark-link:https://gist.github.com/SchmaR/5a81df75ae978618b9bf49efa81132ab&gt;Gist MemoryLeakShapeTest.java&lt;/denchmark-link&gt;
)
After Garbage Collection I end up with the following memory consumption:
&lt;denchmark-code&gt;DONE GC
Max memory:7090
total memory:1365
free memory:684
used memory:680
&lt;/denchmark-code&gt;

Do you see a workaround or a way to fix this issue properly?
&lt;denchmark-h:h3&gt;Stack Traces for MemoryLeakTest.java&lt;/denchmark-h&gt;

A call of INDArray.getRow results in the following stack traces that can be categorized in Shape Information Views and Data Views. I track every call to BaseDataBuffer(DataBuffer underlyingBuffer, long length, long offset) because its the only point I'm are aware of which adds new elements to either INDArray.data.references or INDArray.shapeInformation.references.
&lt;denchmark-h:h4&gt;Shape Information Views&lt;/denchmark-h&gt;

At first this stack is generated:
&lt;denchmark-code&gt;&lt;init&gt;:162, BaseDataBuffer (org.nd4j.linalg.api.buffer)
&lt;init&gt;:105, LongBuffer (org.nd4j.linalg.api.buffer)
create:72, DefaultDataBufferFactory (org.nd4j.linalg.api.buffer.factory)
createBuffer:1122, Nd4j (org.nd4j.linalg.factory)
shapeOf:2834, Shape (org.nd4j.linalg.api.shape)
resolve:329, NDArrayIndex (org.nd4j.linalg.indexing)
get:4993, BaseNDArray (org.nd4j.linalg.api.ndarray)
getRow:5076, BaseNDArray (org.nd4j.linalg.api.ndarray)
main:19, MemoryLeakTest (org.nd4j)
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;&lt;init&gt;:162, BaseDataBuffer (org.nd4j.linalg.api.buffer)
&lt;init&gt;:105, LongBuffer (org.nd4j.linalg.api.buffer)
create:72, DefaultDataBufferFactory (org.nd4j.linalg.api.buffer.factory)
createBuffer:1122, Nd4j (org.nd4j.linalg.factory)
shapeOf:2834, Shape (org.nd4j.linalg.api.shape)
isRowVectorShape:1594, Shape (org.nd4j.linalg.api.shape)
resolve:336, NDArrayIndex (org.nd4j.linalg.indexing)
get:4993, BaseNDArray (org.nd4j.linalg.api.ndarray)
getRow:5076, BaseNDArray (org.nd4j.linalg.api.ndarray)
main:19, MemoryLeakTest (org.nd4j)
&lt;/denchmark-code&gt;

Secondly this one:
&lt;denchmark-code&gt;&lt;init&gt;:162, BaseDataBuffer (org.nd4j.linalg.api.buffer)
&lt;init&gt;:105, LongBuffer (org.nd4j.linalg.api.buffer)
create:72, DefaultDataBufferFactory (org.nd4j.linalg.api.buffer.factory)
createBuffer:1122, Nd4j (org.nd4j.linalg.factory)
shapeOf:2834, Shape (org.nd4j.linalg.api.shape)
resolve:329, NDArrayIndex (org.nd4j.linalg.indexing)
exec:365, ShapeOffsetResolution (org.nd4j.linalg.indexing)
get:4995, BaseNDArray (org.nd4j.linalg.api.ndarray)
getRow:5076, BaseNDArray (org.nd4j.linalg.api.ndarray)
main:19, MemoryLeakTest (org.nd4j)
&lt;/denchmark-code&gt;

Third:
&lt;denchmark-code&gt;&lt;init&gt;:162, BaseDataBuffer (org.nd4j.linalg.api.buffer)
&lt;init&gt;:105, LongBuffer (org.nd4j.linalg.api.buffer)
create:72, DefaultDataBufferFactory (org.nd4j.linalg.api.buffer.factory)
createBuffer:1122, Nd4j (org.nd4j.linalg.factory)
shapeOf:2834, Shape (org.nd4j.linalg.api.shape)
isRowVectorShape:1594, Shape (org.nd4j.linalg.api.shape)
resolve:336, NDArrayIndex (org.nd4j.linalg.indexing)
exec:365, ShapeOffsetResolution (org.nd4j.linalg.indexing)
get:4995, BaseNDArray (org.nd4j.linalg.api.ndarray)
getRow:5076, BaseNDArray (org.nd4j.linalg.api.ndarray)
main:19, MemoryLeakTest (org.nd4j)
&lt;/denchmark-code&gt;

Fourth
&lt;denchmark-code&gt;&lt;init&gt;:162, BaseDataBuffer (org.nd4j.linalg.api.buffer)
&lt;init&gt;:105, LongBuffer (org.nd4j.linalg.api.buffer)
create:72, DefaultDataBufferFactory (org.nd4j.linalg.api.buffer.factory)
createBuffer:1122, Nd4j (org.nd4j.linalg.factory)
shapeOf:2834, Shape (org.nd4j.linalg.api.shape)
shapeOf:3070, BaseNDArray (org.nd4j.linalg.api.ndarray)
subArray:2579, BaseNDArray (org.nd4j.linalg.api.ndarray)
get:5035, BaseNDArray (org.nd4j.linalg.api.ndarray)
getRow:5076, BaseNDArray (org.nd4j.linalg.api.ndarray)
main:19, MemoryLeakTest (org.nd4j)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Data Views&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;&lt;init&gt;:162, BaseDataBuffer (org.nd4j.linalg.api.buffer)
&lt;init&gt;:79, FloatBuffer (org.nd4j.linalg.api.buffer)
create:67, DefaultDataBufferFactory (org.nd4j.linalg.api.buffer.factory)
createBuffer:1122, Nd4j (org.nd4j.linalg.factory)
&lt;init&gt;:192, BaseNDArray (org.nd4j.linalg.api.ndarray)
&lt;init&gt;:80, NDArray (org.nd4j.linalg.cpu.nativecpu)
create:390, CpuNDArrayFactory (org.nd4j.linalg.cpu.nativecpu)
create:4375, Nd4j (org.nd4j.linalg.factory)
create:2172, BaseNDArray (org.nd4j.linalg.api.ndarray)
subArray:2589, BaseNDArray (org.nd4j.linalg.api.ndarray)
get:5035, BaseNDArray (org.nd4j.linalg.api.ndarray)
getRow:5076, BaseNDArray (org.nd4j.linalg.api.ndarray)
main:19, MemoryLeakTest (org.nd4j)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='SchmaR' date='2019-05-01T19:22:23Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>