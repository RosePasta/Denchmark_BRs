<bug id='650' author='xnuter' open_date='2015-08-29T23:06:13Z' closed_time='2017-03-12T15:24:48Z'>
	<summary>DL4J is super slow on GoogleNews-vectors file</summary>
	<description>
I tried to execute the following example on DL4J (loading pre-trained vectors file):
&lt;denchmark-code&gt;File gModel = new File("./GoogleNews-vectors-negative300.bin.gz");

Word2Vec vec = WordVectorSerializer.loadGoogleModel(gModel, true);

InputStreamReader r = new InputStreamReader(System.in);

BufferedReader br = new BufferedReader(r);

for (; ; ) {
    System.out.print("Word: ");
    String word = br.readLine();

    if ("EXIT".equals(word)) break;

    Collection&lt;String&gt; lst = vec.wordsNearest(word, 20);

    System.out.println(word + " -&gt; " + lst);
}
&lt;/denchmark-code&gt;

But it is super slow (taking ~10 minutes to calculate the nearest words, though they are correct).
There is enough memory (-Xms20g -Xmx20g).
When I run the same Word2Vec example from &lt;denchmark-link:https://code.google.com/p/word2vec/&gt;https://code.google.com/p/word2vec/&lt;/denchmark-link&gt;

it gives the nearest words very quickly.
DL4J uses ND4J which claims to be twice as fast as Numpy: &lt;denchmark-link:http://nd4j.org/benchmarking&gt;http://nd4j.org/benchmarking&lt;/denchmark-link&gt;

Is anything wrong with my code?
It is based on &lt;denchmark-link:https://github.com/deeplearning4j/dl4j-0.4-examples.git&gt;https://github.com/deeplearning4j/dl4j-0.4-examples.git&lt;/denchmark-link&gt;
 (I didn't touch any dependencies, just tried to read the Google pre-trained vectors file). Word2VecRawTextExample works just fine (but the data size is relatively small), I only replaced the main method with the code above.
	</description>
	<comments>
		<comment id='1' author='xnuter' date='2015-08-29T23:08:03Z'>
		How much ram did you give the jvm?
On Aug 30, 2015 8:06 AM, "xnuter" &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

I tried to execute the following example on DL4J (loading pre-trained
vectors file):
File gModel = new File("./GoogleNews-vectors-negative300.bin.gz");
Word2Vec vec = WordVectorSerializer.loadGoogleModel(gModel, true);
InputStreamReader r = new InputStreamReader(System.in);
BufferedReader br = new BufferedReader(r);
for (; ; ) {
System.out.print("Word: ");
String word = br.readLine();
if ("EXIT".equals(word)) break;

Collection&lt;String&gt; lst = vec.wordsNearest(word, 20);

System.out.println(word + " -&gt; " + lst);

}
But it is super slow (taking ~10 minutes to calculate the nearest words,
though they are correct).
There is enough memory (-Xms20g -Xmx20g).
When I run the same Word2Vec example from
https://code.google.com/p/word2vec/
it gives the nearest words very quickly.
DL4J uses ND4J which claims to be twice as fast as Numpy:
http://nd4j.org/benchmarking
Is there anything wrong with my code?
It is based on https://github.com/deeplearning4j/dl4j-0.4-examples.git (I
didn't touch any dependencies, just tried to read the Google pre-trained
vectors file). Word2VecRawTextExample works just fine (but the data size is
relatively small), I only replaced the main method with the code above.
—
Reply to this email directly or view it on GitHub
https://github.com/deeplearning4j/deeplearning4j/issues/650.

		</comment>
		<comment id='2' author='xnuter' date='2015-08-29T23:09:21Z'>
		20g (it gives OOM on 10g, it works with 12g I tried to give 20 to see if it helped). Apparently 20g is sufficient but still slow.
		</comment>
		<comment id='3' author='xnuter' date='2015-08-29T23:12:00Z'>
		I have a full set up on EC2 server, though it's down now. But I can start it and give you access to it, if you cannot reproduce that locally.
		</comment>
		<comment id='4' author='xnuter' date='2015-08-29T23:19:48Z'>
		K just making sure it wasn't an oom. We can look in to it a bit more. Make
sure you are using rc0.
Thanks!
On Aug 30, 2015 8:12 AM, "xnuter" &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

I have a full set up on EC2 server, though it's down now. But I can start
it and give you access to it, if you cannot reproduce that locally.
—
Reply to this email directly or view it on GitHub
https://github.com/deeplearning4j/deeplearning4j/issues/650#issuecomment-136061819
.

		</comment>
		<comment id='5' author='xnuter' date='2015-08-29T23:25:07Z'>
		As I said earlier I just pulled &lt;denchmark-link:https://github.com/deeplearning4j/dl4j-0.4-examples.git&gt;https://github.com/deeplearning4j/dl4j-0.4-examples.git&lt;/denchmark-link&gt;
  without touching any dependencies (and it's 0.4-rc0). BTW, while it calculates the nearest words the CPU is ~100% (on a single core). So it's busy.
		</comment>
		<comment id='6' author='xnuter' date='2015-08-29T23:26:38Z'>
		Oh sorry about that. Replying via email didn't see the whole thing ;) We
can take a look then.
On Aug 30, 2015 8:25 AM, "xnuter" &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

As I said earlier I just pulled
https://github.com/deeplearning4j/dl4j-0.4-examples.git without touching
any dependencies (and it's 0.4-rc0). BTW, while it calculates the nearest
words the CPU is ~100% (on a single core). So it's busy.
—
Reply to this email directly or view it on GitHub
https://github.com/deeplearning4j/deeplearning4j/issues/650#issuecomment-136062227
.

		</comment>
		<comment id='7' author='xnuter' date='2015-09-01T17:00:00Z'>
		Is there any update for this problem?
		</comment>
		<comment id='8' author='xnuter' date='2015-09-03T00:38:21Z'>
		We haven't looked in to this yet. We've been focused on other optimizations.
		</comment>
		<comment id='9' author='xnuter' date='2016-02-09T00:46:32Z'>
		Has this been resolved?
		</comment>
		<comment id='10' author='xnuter' date='2016-02-09T01:39:36Z'>
		Oh thank sfor flagging this. &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 fix this.
		</comment>
		<comment id='11' author='xnuter' date='2016-07-02T18:20:38Z'>
		sir can u tell how to train the data of dl4j i don't want to use google news doc model i have my owndataset, how to use it in training
		</comment>
		<comment id='12' author='xnuter' date='2016-07-03T09:45:40Z'>
		Please check examples &lt;denchmark-link:https://github.com/deeplearning4j/dl4j-0.4-examples/&gt;https://github.com/deeplearning4j/dl4j-0.4-examples/&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='xnuter' date='2016-07-09T02:55:18Z'>
		It takes me 1294 seconds to load GoogleNews-vectors-negative300.bin using nd4j-cuda-7.5 0.4-rc3.10.  This is on a Macbook Pro with a 2.5 GHz Intel Core i7 and an NVIDIA GeForce GT 750M 2048 MB, with a 12 GB JVM heap.
There's no detail on why this issue was closed--was there a fix committed?
		</comment>
		<comment id='14' author='xnuter' date='2016-07-09T08:04:27Z'>
		Yes, somewhere around November there was fixes commited.  Regarding your 1294 seconds - please link exact macbook model name.
		</comment>
		<comment id='15' author='xnuter' date='2016-07-09T16:08:25Z'>
		It's a MacBookPro11,3 with 16 GB RAM.
		</comment>
		<comment id='16' author='xnuter' date='2016-07-09T16:16:22Z'>
		And for comparison, using nd4j-native 0.4-rc3.10 macosx-x86_64 only takes 114 seconds to load the same model.
		</comment>
		<comment id='17' author='xnuter' date='2016-07-09T16:19:02Z'>
		Please, give me full model name for your laptop.
		</comment>
		<comment id='18' author='xnuter' date='2016-07-09T19:09:38Z'>
		Sorry, I'm not sure what other information you're asking for.  It's a MacBook Pro, the model ID is MacBookPro11,3--you can google that and find every spec, e.g. &lt;denchmark-link:https://support.apple.com/en-us/HT201300&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;  Model Name:   MacBook Pro
  Model Identifier: MacBookPro11,3
  Processor Name:   Intel Core i7
  Processor Speed:  2.5 GHz
  Number of Processors: 1
  Total Number of Cores:    4
  L2 Cache (per Core):  256 KB
  L3 Cache: 6 MB
  Memory:   16 GB
  Boot ROM Version: MBP112.0138.B17
&lt;/denchmark-code&gt;

		</comment>
		<comment id='19' author='xnuter' date='2016-07-09T19:17:37Z'>
		So, that's ME294 model, with 512GB SSD, right? And you're using dl4j 3.10 as well, right?
		</comment>
		<comment id='20' author='xnuter' date='2016-07-09T19:20:47Z'>
		RIght. problem is pretty clear. That's one more place where putScalar happens, which is really bad for CUDA.
Thanks for flagging this.
		</comment>
		<comment id='21' author='xnuter' date='2016-07-09T19:23:37Z'>
		I think it's MGXC2xx/A, 15.4”/2.5 Quad-core i7/16GB but upgraded with 1 TB SSD. Using deeplearning4j-nlp 0.4-rc3.10.
		</comment>
		<comment id='22' author='xnuter' date='2016-07-09T20:31:04Z'>
		Ok. That's not putScalar. Issue will be investigated.
Thanks for flagging this once again.
		</comment>
		<comment id='23' author='xnuter' date='2016-11-23T14:21:17Z'>
		Update on this?
		</comment>
		<comment id='24' author='xnuter' date='2017-03-12T11:46:33Z'>
		Hi,
I have saved the trained model of word2vecSentimentRNN and when i load the word2vecSentimentRNN , I see a performance issue. It takes 10mins+ to load the googlenews-vector-negative.bin file..
Why the googlenews-vector-negative.bin takes too much time to load. This creates a performance issue ryt..?? Is this issue resolved..?? Please help me to fix this..
		</comment>
		<comment id='25' author='xnuter' date='2017-03-12T15:25:11Z'>
		We've already seen this. It's heap spaced related. Increase it.
		</comment>
		<comment id='26' author='xnuter' date='2018-10-02T01:22:30Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>