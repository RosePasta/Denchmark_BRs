<bug id='8836' author='DanieleMarchei' open_date='2020-04-14T22:37:54Z' closed_time='2020-04-17T13:33:22Z'>
	<summary>Loading a model causes the score to jump</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I have observed a strange phenomenon during training. If I train my MultiLayerNetwork from start to finish the score curve is smooth, while if I save the model and then load it to continue training, the score curve has a sudden spike exactly in the place where the loading has occurred.
&lt;denchmark-link:https://user-images.githubusercontent.com/6474669/79279178-f645f800-7e9c-11ea-8db8-d2d88b940981.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/6474669/79279194-fcd46f80-7e9c-11ea-9ae6-6d452dad26b4.png&gt;&lt;/denchmark-link&gt;

You can see this effect in the images above. In the first one, I train my network for 300 epochs and everything looks good. In the second image, I stopped the training at epoch 150, saved the model (with updater), loaded again and finished training. You can see a clear spike in epoch 150.
I first encountered this problem while training a much bigger network. It seems that training a loaded model also improves its performances.
&lt;denchmark-link:https://user-images.githubusercontent.com/6474669/79280087-019a2300-7e9f-11ea-9358-148bf201477b.png&gt;&lt;/denchmark-link&gt;

During the above training, I saved a checkpoint somewhere after 50000 epochs. Before this, the score value was stable, but then, after loading, the score improved.
What could be the cause of this problem?
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version 1.0.0-beta6
Windows 10 64 bit

&lt;denchmark-h:h4&gt;Additional Information&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/DanieleMarchei/98fbd0f0c466a96e611dc98fc7b864bc&gt;Gist with a minimal example&lt;/denchmark-link&gt;

My gradle
&lt;denchmark-code&gt;compile 'com.github.sh0nk:matplotlib4j:0.4.0'
compile 'org.slf4j:slf4j-api:1.7.2'
compile 'org.slf4j:slf4j-simple:1.7.2'

compile group: 'org.bytedeco.javacpp-presets', name: 'openblas', version: '0.3.5-1.4.4'
compile "org.nd4j:nd4j-native:1.0.0-beta6"
compile "org.deeplearning4j:deeplearning4j-core:1.0.0-beta6"
compile group: 'org.nd4j', name: 'nd4j-native', version: '1.0.0-beta6', classifier: 'windows-x86_64-avx2'

compile 'me.tongfei:progressbar:0.8.1'
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='DanieleMarchei' date='2020-04-15T07:02:25Z'>
		hm... that's something I'd expect to see if you didn't saving/loading updater state (i.e., adam optimizer in this case), but I see you're doing that (and appears to be correct)
thanks for the code, I'll see if I can reproduce it from that
		</comment>
		<comment id='2' author='DanieleMarchei' date='2020-04-17T01:49:48Z'>
		I have slightly modified the code you provided.
If I correct the loop limits (line 72/73 in the gist) I get exactly the same results. Before making that correction, I get charts like you show (but the training/data isn't equivalent between the cases without that correction)
&lt;denchmark-link:https://gist.github.com/AlexDBlack/2c080999422af3394edbb33fb7d5868a&gt;https://gist.github.com/AlexDBlack/2c080999422af3394edbb33fb7d5868a&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/79523270-147c4580-80a1-11ea-86f5-800ce2cb1fe2.png&gt;&lt;/denchmark-link&gt;

I also set a RNG seed for your network configuration (via .seed(12345)) to ensure repeatability of parameter initialization between runs.
I have also checked everything for the loaded vs. original model (parameters, gradients, updater state, output, scores) and it's correct. Though I note we already have many tests doing exactly this sort of thing - for example here (amongst other places) - &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/util/ModelSerializerTest.java&gt;https://github.com/eclipse/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/util/ModelSerializerTest.java&lt;/denchmark-link&gt;

That leaves your original issue:

I first encountered this problem while training a much bigger network. It seems that training a loaded model also improves its performances.

I would need to see the network configuration and how you are saving/loading the network.
Again, the most likely explanation here is you're not saving (or loading) updater state. That, combined with some of the adaptive updaters (Adagrad especially, but also Adam etc too potentially) could cause that sort of issue.
		</comment>
		<comment id='3' author='DanieleMarchei' date='2020-04-17T12:47:17Z'>
		In the light of your answer, I double-checked all my code (for the bigger network) and found out that the error was caused by a mistake not related to dl4j. Now everything is back to normal.
Thank you for your time and patience :)
		</comment>
	</comments>
</bug>