<bug id='7849' author='lastrealklaus' open_date='2019-06-05T09:47:07Z' closed_time='2019-06-06T14:18:07Z'>
	<summary>SameDiff logsoftmax evaluation</summary>
	<description>
i would expect that calculating logsoftmax of same input results in same output. but when i run this:
&lt;denchmark-code&gt;public static void main(String[] args) {
	final var inputArr = Nd4j.rand(new int[] { 1, 4 }, new NormalDistribution());
	System.err.println();
	System.err.println("created input array: " + inputArr);
	System.err.println("nd4j.oldlogsoftmax:  " + Nd4j.getExecutioner().exec(new OldLogSoftMax(inputArr.dup())));

	for (int i = 0; i &lt; 10; i++) {
		SameDiff sd = SameDiff.create();
		final var input = sd.var(inputArr.dup());
		var softmax = sd.nn().logSoftmax(input);
		System.err.println("samediff.logsoftmax: " + softmax.eval());
	}
}
&lt;/denchmark-code&gt;

the output may look like this:
&lt;denchmark-code&gt;created input array: [[    0.1869,   -1.4918,   -0.6497,   -0.8864]]
nd4j.oldlogsoftmax:  [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]
samediff.logsoftmax: [[       NaN,       NaN,       NaN,       NaN]]
samediff.logsoftmax: [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]
samediff.logsoftmax: [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]
samediff.logsoftmax: [[       NaN,       NaN,       NaN,       NaN]]
samediff.logsoftmax: [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]
samediff.logsoftmax: [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]
samediff.logsoftmax: [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]
samediff.logsoftmax: [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]
samediff.logsoftmax: [[       NaN,       NaN,       NaN,       NaN]]
samediff.logsoftmax: [[   -0.6738,   -2.3525,   -1.5104,   -1.7472]]

&lt;/denchmark-code&gt;

the output varies as do numbers of NaN outputs.
tried it with 1.0.0-beta4 and snapshot build of today.
	</description>
	<comments>
		<comment id='1' author='lastrealklaus' date='2019-06-05T09:50:57Z'>
		Which backend was used to produce this? CPU or CUDA?
		</comment>
		<comment id='2' author='lastrealklaus' date='2019-06-05T10:51:43Z'>
		i'm using native backend. can i use the cuda backend for samediff? thought i read, that this is currently not available.
		</comment>
		<comment id='3' author='lastrealklaus' date='2019-06-05T10:53:20Z'>
		Please don't. I'm just trying to imagine what could go wrong, and backend used is the first question I should've asked.
		</comment>
		<comment id='4' author='lastrealklaus' date='2019-06-05T11:49:13Z'>
		while looking for a workaround i found another problem:
&lt;denchmark-code&gt;	final var inputArr = Nd4j.rand(new int[] { 1, 4 }, new NormalDistribution());
	final var sd = SameDiff.create();
	final var input3 = sd.var(inputArr.dup());
	final var custom2 = sd.math().logSumExp(input3);
	System.err.println("samediff.logSumExp:  " + custom2.eval());
&lt;/denchmark-code&gt;

prints
&lt;denchmark-code&gt;samediff.logSumExp:  0
[ERROR] Unknown opNum=11 on D:/jenkins/ws/dl4j-master-windows-x86_64-cpu/libnd4j/include/loops/cpu/reduce/reduce_float.cpp:124
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='lastrealklaus' date='2019-06-05T11:50:56Z'>
		Hm. That's separate issue. Just a bad mapping. Open new issue about this one please.
		</comment>
		<comment id='6' author='lastrealklaus' date='2019-06-05T11:54:26Z'>
		done &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/7850&gt;#7850&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='lastrealklaus' date='2019-06-05T11:55:29Z'>
		Thank you. This issue is already fixed, but fix will be merged only after migration process finished.
		</comment>
		<comment id='8' author='lastrealklaus' date='2019-06-06T14:18:06Z'>
		Hi, guys
thanks for catching  this
logSoftMax works correctly now
		</comment>
	</comments>
</bug>