<bug id='9060' author='zhangy10' open_date='2020-07-30T08:51:13Z' closed_time='2020-08-30T11:22:31Z'>
	<summary>Running Error for using Causal Convolution1D</summary>
	<description>
I am trying to build a TCN network referring to https://github.com/locuslab/TCN. The basic network structure has 3 TCN blocks, and each block includes 2 Causal Convolution1D layers with the same kernel size and dilation size.
I use 1.0.0-beta7 to run the codes, and running on a Mac laptop with Catalina 10.15.1.
However, I got this issue:
&lt;denchmark-code&gt;o.n.l.c.n.o.NativeOpExecutioner - Failed to execute op conv1d. Attempted to execute with 3 inputs, 1 outputs, 0 targs,0 bargs and 6 iargs. Inputs: [(FLOAT,[64,72,1],f), (FLOAT,[14,72,144],f), (FLOAT,[144],c)]. Outputs: [(FLOAT,[64,144,1],f)]. tArgs: -. iArgs: [14, 1, 0, 4, 2, 0]. bArgs: -. Op own name: "36effce9-b5f5-4cb6-946c-4420fa034eeb" - Please see above message (printed out from c++) for a possible cause of error.
Exception in thread "main" java.lang.RuntimeException: Op [conv1d] execution failed
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1594)
	at org.nd4j.linalg.factory.Nd4j.exec(Nd4j.java:6566)
	at org.deeplearning4j.nn.layers.convolution.Convolution1DLayer.causalConv1dForward(Convolution1DLayer.java:219)
	at org.deeplearning4j.nn.layers.convolution.Convolution1DLayer.preOutput(Convolution1DLayer.java:173)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:489)
	at org.deeplearning4j.nn.layers.convolution.Convolution1DLayer.activate(Convolution1DLayer.java:228)
	at org.deeplearning4j.nn.layers.AbstractLayer.activate(AbstractLayer.java:258)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.ffToLayerActivationsInWs(MultiLayerNetwork.java:1134)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2746)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2704)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:170)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:63)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fitHelper(MultiLayerNetwork.java:1715)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1636)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1623)
	at org.deeplearning4j.examples.convolution.ZacMDL.Grow.Models.GrowModel.fit(GrowModel.java:116)
	at org.deeplearning4j.examples.convolution.ZacMDL.Grow.GrowTraining.run(GrowTraining.java:194)
	at org.deeplearning4j.examples.convolution.ZacMDL.Grow.GrowMain.main(GrowMain.java:72)
Caused by: java.lang.RuntimeException: could not create a descriptor for a dilated convolution forward propagation primitive
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1924)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1573)
	... 18 more

Process finished with exit code 1


&lt;/denchmark-code&gt;

And below is my network codes, and the inputs are multi-sequence data, which the input shape is [64, 9, 128] as an example:
&lt;denchmark-code&gt;new NeuralNetConfiguration.Builder().seed(seed)
                   .activation(Activation.RELU)
                   .weightInit(new WeightInitNormal()) // better init
                   .updater(new Adam(config.getLearnRate()))
                   .list()
                   // block 1
                   .layer(new Convolution1D.Builder()
                              .kernelSize(14)
                              .name("Tcnn1")
                              .stride(1)
                              .nIn(9)
                              .nOut(36)
                              .convolutionMode(ConvolutionMode.Causal)
                              .dilation(1)
                              .build())
                   .layer(new Convolution1D.Builder()
                              .name("Tcnn2")
                              .kernelSize(14)
                              .stride(1)
                              .nOut(36)
                              .convolutionMode(ConvolutionMode.Causal)
                              .dilation(1)
                              .build())

//                   // block 2
                   .layer(new Convolution1D.Builder()
                              .kernelSize(14)
                              .name("Tcnn3")
                              .stride(1)
                              .nOut(72)
                              .convolutionMode(ConvolutionMode.Causal)
                              .dilation(2)
                              .build())
                   .layer(new Convolution1D.Builder()
                              .name("Tcnn4")
                              .kernelSize(14)
                              .stride(1)
                              .nOut(72)
                              .convolutionMode(ConvolutionMode.Causal)
                              .dilation(2)
                              .build())

                   .layer(new GlobalPoolingLayer(PoolingType.AVG))


//                   // block 3
                   .layer(new Convolution1D.Builder()
                              .kernelSize(14)
                              .name("Tcnn5")
                              .stride(1)
                              .nOut(144)
                              .convolutionMode(ConvolutionMode.Causal)
                              .dilation(4)
                              .build())
                   .layer(new Convolution1D.Builder()
                              .name("Tcnn6")
                              .kernelSize(14)
                              .stride(1)
                              .nOut(144)
                              .convolutionMode(ConvolutionMode.Causal)
                              .dilation(4)
                              .build())

                   .layer(new GlobalPoolingLayer(PoolingType.AVG))

                   // output
                   .layer(new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
                              .name("output")
                              .nOut(6)
                              .activation(Activation.SOFTMAX) // radial basis function required
                              .build())
                   .setInputType(
                       InputType.recurrent(9, 128))
                   .build();

&lt;/denchmark-code&gt;

And the output network structure is:
&lt;denchmark-code&gt;
==================================================================================
LayerName (LayerType)         nIn,nOut   TotalParams   ParamsShape                
==================================================================================
Tcnn1 (Convolution1DLayer)    9,36       4,572         b:{1,36}, W:{36,9,14,1}    
Tcnn2 (Convolution1DLayer)    36,36      18,180        b:{1,36}, W:{36,36,14,1}   
Tcnn3 (Convolution1DLayer)    36,72      36,360        b:{1,72}, W:{72,36,14,1}   
Tcnn4 (Convolution1DLayer)    72,72      72,648        b:{1,72}, W:{72,72,14,1}   
layer4 (GlobalPoolingLayer)   -,-        0             -                          
Tcnn5 (Convolution1DLayer)    72,144     145,296       b:{1,144}, W:{144,72,14,1} 
Tcnn6 (Convolution1DLayer)    144,144    290,448       b:{1,144}, W:{144,144,14,1}
layer7 (GlobalPoolingLayer)   -,-        0             -                          
output (OutputLayer)          144,6      870           W:{144,6}, b:{1,6}         
----------------------------------------------------------------------------------
            Total Parameters:  568,374
        Trainable Parameters:  568,374
           Frozen Parameters:  0
==================================================================================

Total num of params: 568374


&lt;/denchmark-code&gt;

So, can you please double-check the issue? And when I remove the block 3 (dilation = 4), the network can work correctly. I am wondering if the input shape of block 3 gets some errors when forward pass.
Many thanks.
	</description>
	<comments>
		<comment id='1' author='zhangy10' date='2020-07-30T09:53:52Z'>
		Could you set:
&lt;denchmark-code&gt;        Nd4j.getEnvironment().setDebug(true);
&lt;/denchmark-code&gt;

and let me know the output?
		</comment>
		<comment id='2' author='zhangy10' date='2020-07-30T10:38:35Z'>
		All right, and I think the output looks the same after the setting:
&lt;denchmark-code&gt;o.n.l.c.n.o.NativeOpExecutioner - Failed to execute op conv1d. Attempted to execute with 3 inputs, 1 outputs, 0 targs,0 bargs and 6 iargs. Inputs: [(FLOAT,[64,72,1],f), (FLOAT,[14,72,144],f), (FLOAT,[144],c)]. Outputs: [(FLOAT,[64,144,1],f)]. tArgs: -. iArgs: [14, 1, 0, 4, 2, 0]. bArgs: -. Op own name: "10c3f844-f7bd-457d-87af-204e8cc23ccf" - Please see above message (printed out from c++) for a possible cause of error.
Exception in thread "main" java.lang.RuntimeException: Op [conv1d] execution failed
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1594)
	at org.nd4j.linalg.factory.Nd4j.exec(Nd4j.java:6566)
	at org.deeplearning4j.nn.layers.convolution.Convolution1DLayer.causalConv1dForward(Convolution1DLayer.java:219)
	at org.deeplearning4j.nn.layers.convolution.Convolution1DLayer.preOutput(Convolution1DLayer.java:173)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:489)
	at org.deeplearning4j.nn.layers.convolution.Convolution1DLayer.activate(Convolution1DLayer.java:228)
	at org.deeplearning4j.nn.layers.AbstractLayer.activate(AbstractLayer.java:258)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.ffToLayerActivationsInWs(MultiLayerNetwork.java:1134)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2746)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2704)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:170)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:63)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fitHelper(MultiLayerNetwork.java:1715)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1636)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1623)
	at org.deeplearning4j.examples.convolution.ZacMDL.Grow.Models.GrowModel.fit(GrowModel.java:116)
	at org.deeplearning4j.examples.convolution.ZacMDL.Grow.GrowTraining.run(GrowTraining.java:197)
	at org.deeplearning4j.examples.convolution.ZacMDL.Grow.GrowMain.main(GrowMain.java:73)
Caused by: java.lang.RuntimeException: could not create a descriptor for a dilated convolution forward propagation primitive
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1924)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1573)
	... 18 more

Process finished with exit code 1

&lt;/denchmark-code&gt;

But I debug it and find that the error locates in NativeOpExecutioner at 1103 line:
&lt;denchmark-code&gt;context.markInplace(op.isInplaceCall());
                    context.setRngStates(Nd4j.getRandom().rootState(), Nd4j.getRandom().nodeState());
                    context.setInputArrays(op.inputArguments());
                    context.setOutputArrays(op.outputArguments());
                    context.setBArguments(op.bArgs());
                    context.setIArguments(op.iArgs());
                    context.setTArguments(op.tArgs());
                    context.setDArguments(op.dArgs());
                    INDArray[] result = this.exec(op, context); // ****
                    Pair&lt;Long, Long&gt; states = context.getRngStates();

&lt;/denchmark-code&gt;

The op.config is  Conv1DConfig(k=14, s=1, p=0, d=4, dataFormat=NCW, paddingMode=CAUSAL), and the context has  org.nd4j.nativeblas.OpaqueContext[address=0x7f85f0078f30,position=0,limit=0,capacity=0,deallocator=null], and the executionMode is "UNDEFINED".
The op.inputArguments() includes 3 inputs, and the shapes are in the above output, which are [(FLOAT,[64,72,1],f), (FLOAT,[14,72,144],f), (FLOAT,[144],c)].
It seems that it works fine when d = 2, and I also try d = 8 and get the same issue.
So, please any thoughts about this?
		</comment>
		<comment id='3' author='zhangy10' date='2020-08-05T03:20:44Z'>
		&lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 I follow up on this. I am not sure if this structure can work in different environments. Please also double-check it.
		</comment>
		<comment id='4' author='zhangy10' date='2020-08-23T03:54:40Z'>
		&lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 Any updates for this issue? Thanks.
		</comment>
		<comment id='5' author='zhangy10' date='2020-08-23T11:35:13Z'>
		Hi, sorry I wasn't able to get to this for a while. I stripped down your example to 1 layer where the problematic parts are and am looking in to this now.
		</comment>
		<comment id='6' author='zhangy10' date='2020-08-23T16:01:02Z'>
		&lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 Thanks, and let me know if anything is wrong after that.
		</comment>
		<comment id='7' author='zhangy10' date='2020-08-24T12:50:12Z'>
		&lt;denchmark-link:https://github.com/zhangy10&gt;@zhangy10&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/KonduitAI/deeplearning4j/pull/519&gt;KonduitAI#519&lt;/denchmark-link&gt;
 looks like it was a bug.
I have a few more tests to merge yet, but essentially the problem was the way data is being passed down to c++ and validation that didn't quite fit the assumptions for causal convolutions.
		</comment>
		<comment id='8' author='zhangy10' date='2020-08-24T13:57:19Z'>
		&lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 Thanks for that, and please let me know once it is fixed in the snapshot.
		</comment>
		<comment id='9' author='zhangy10' date='2020-08-30T11:23:20Z'>
		&lt;denchmark-link:https://github.com/zhangy10&gt;@zhangy10&lt;/denchmark-link&gt;
  fixed but not in eclipse master yet.  Keep an eye out for a PR merge from our dev repo soon.
		</comment>
		<comment id='10' author='zhangy10' date='2020-09-03T05:58:07Z'>
		&lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 All right, thanks again, and will keep an eye on that.
		</comment>
	</comments>
</bug>