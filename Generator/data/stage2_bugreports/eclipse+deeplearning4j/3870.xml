<bug id='3870' author='AlexDBlack' open_date='2017-08-17T02:23:40Z' closed_time='2018-02-19T08:11:43Z'>
	<summary>Dropout - excessive memory use without workspaces</summary>
	<description>
See the config below.

No workspaces

No dropout: OK
With dropout:

CUDA: OOM very quickly (even with GC frequency 100)
CPU: Takes longer, but OOMs there also




With workspaces: memory use is flat

CPU:
&lt;denchmark-code&gt;Exception in thread "main" java.lang.OutOfMemoryError: Cannot allocate new FloatPointer(128000): totalBytes = 132M, physicalBytes = 14G
&lt;/denchmark-code&gt;

CUDA:
&lt;denchmark-code&gt;Exception in thread "main" java.lang.RuntimeException: Failed to allocate 12288000 bytes from DEVICE [0] memory
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/29393388-bb326ce8-8346-11e7-9caa-18c229cf77b4.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;    public static void main(String[] args){
        Nd4j.getMemoryManager().setAutoGcWindow(100);

        int numFeats = 960;
        int numClasses = 40;

        int i=0;
        ComputationGraphConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(123)
                .activation(Activation.TANH)
                .weightInit(WeightInit.XAVIER)
                .learningRate(0.01)
                .regularization(true)
                .updater(new Nesterovs(0.9))
//                .trainingWorkspaceMode(WorkspaceMode.SINGLE).inferenceWorkspaceMode(WorkspaceMode.SINGLE)
                .graphBuilder()
                .addInputs("input")
                .addLayer("1-"+(i+1), new DenseLayer.Builder().nIn(numFeats).nOut(numFeats / 2)
                        .dropOut(0.5)
                        .build(), "input")
                .addLayer("2-"+(i+1), new DenseLayer.Builder().nIn(numFeats / 2).nOut(numFeats / 2)
                        .dropOut(0.5)
                        .build(), "1-"+(i+1))
                .addLayer("out"+(i+1), new OutputLayer.Builder().nIn(numFeats / 2).nOut(numClasses).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(), "2-"+(i+1))
                .setOutputs("out"+(i+1))
                .pretrain(false).backprop(true)
                .build();

        ComputationGraph g = new ComputationGraph(conf);
        g.init();

        int mb = 3200;
        INDArray f = Nd4j.rand(mb, numFeats);
        INDArray l = Nd4j.zeros(mb, numClasses);
        for( i=0; i&lt;mb; i++ ){
            l.putScalar(i, i%numClasses, 1.0);
        }

        Evaluation evaluation = new Evaluation(numClasses);
        INDArray[] predicted = g.output(false, f);
        evaluation.eval(l, predicted[0]);
        System.out.println(String.format("Train set evaluation: Accuracy = %.4f, Precision = %.4f, Recall = %.4f, F1 = %.4f", evaluation.accuracy(), evaluation.precision(), evaluation.recall(), evaluation.f1()));


        int count = 10000000;
        long start = System.currentTimeMillis();
        for( i=0; i&lt;count; i++ ){
            g.fit(new INDArray[]{f}, new INDArray[]{l});
            if(i % 1000 == 0){
                System.out.println(i);
            }
        }
        long end = System.currentTimeMillis();

        System.out.println("TIME: " + (end-start));
    }
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2017-08-17T04:13:14Z'>
		It is worth pointing out this happens only when we perform the evaluation; if we skip it, then we get no OOM
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-02-19T08:11:43Z'>
		Closing this as fixed: memory use seems solid, even without workspaces. I ran this on master for about 10k iterations with dropout(0.5) - no OOM, and no evidence that memory use was growing over time.
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/36367844-a50193aa-15a8-11e8-9af9-2527ade2206a.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='AlexDBlack' date='2018-09-23T09:28:03Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>