<bug id='1451' author='christopher-beckham' open_date='2016-04-26T08:08:28Z' closed_time='2016-04-27T13:47:54Z'>
	<summary>Unable to get number of of rows for a non 2d matrix</summary>
	<description>
When I modify the LenetMnistExample such that there is no max pool layer and no fully connected layer (before the softmax), I get this error:
&lt;denchmark-code&gt;Exception in thread "main" java.lang.IllegalStateException: Unable to get number of of rows for a non 2d matrix
    at org.nd4j.linalg.api.ndarray.BaseNDArray.rows(BaseNDArray.java:3423)
    at org.nd4j.linalg.api.ndarray.BaseNDArray.mmul(BaseNDArray.java:2339)
    at org.deeplearning4j.nn.layers.BaseLayer.preOutput(BaseLayer.java:325)
    at org.deeplearning4j.nn.layers.BaseOutputLayer.preOutput2d(BaseOutputLayer.java:425)
    at org.deeplearning4j.nn.layers.BaseOutputLayer.output(BaseOutputLayer.java:258)
    at org.deeplearning4j.nn.layers.BaseOutputLayer.activate(BaseOutputLayer.java:221)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:513)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:636)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:590)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:1752)
    at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:132)
    at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:56)
    at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52)
    at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1168)
    at weka.classifiers.functions.LeNetMnistExample.main(LeNetMnistExample.java:91)

&lt;/denchmark-code&gt;

This is the architecture:
&lt;denchmark-code&gt;        MultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .regularization(true).l2(0.0005)
                .learningRate(0.01)//.biasLearningRate(0.02)
                //.learningRateDecayPolicy(LearningRatePolicy.Inverse).lrPolicyDecayRate(0.001).lrPolicyPower(0.75)
                .weightInit(WeightInit.XAVIER)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .updater(Updater.NESTEROVS).momentum(0.9)
                .list(4)
                .layer(0, new ConvolutionLayer.Builder(5, 5)
                        .nIn(nChannels)
                        .stride(1, 1)
                        .nOut(20)
                        .activation("identity")
                        .build())
                .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)
                        .kernelSize(2,2)
                        .stride(2,2)
                        .build())
                .layer(2, new ConvolutionLayer.Builder(5, 5)
                        .nIn(nChannels)
                        .stride(1, 1)
                        .nOut(50)
                        .activation("identity")
                        .build())
                .layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                        .nOut(outputNum)
                        .activation("softmax")
                        .build())
                .backprop(true).pretrain(false);
&lt;/denchmark-code&gt;

Did I do something stupid or is there some weird sort of bug here? We should be able to plop a fully-connected layer straight after a convolutional one.
(Am using latest build of DL4J)
Thanks.
	</description>
	<comments>
		<comment id='1' author='christopher-beckham' date='2016-04-26T08:15:05Z'>
		Try with ConvolutionLayerSetup as per this: &lt;denchmark-link:https://github.com/deeplearning4j/dl4j-0.4-examples/blob/master/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java#L81&gt;https://github.com/deeplearning4j/dl4j-0.4-examples/blob/master/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java#L81&lt;/denchmark-link&gt;

Or equivalently, add a .cnnInputSize(...) before the .backprop(true)
		</comment>
		<comment id='2' author='christopher-beckham' date='2016-04-27T01:13:32Z'>
		It appears that neither .cnnInputSize (or ConvolutionLayerSetup) fixes the issue, even when I have both of them. Here is the Pastebin for the .java file:
&lt;denchmark-link:http://pastebin.com/raw/VfjpkRUe&gt;http://pastebin.com/raw/VfjpkRUe&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='christopher-beckham' date='2016-04-27T13:47:54Z'>
		So turns out this is a bug in 0.4-rc3.8 that has already been fixed in 3.9/master. Basically the input preprocessor isn't being correctly added between the CNN and output layers.
Try adding this to your code:
        MultiLayerConfiguration conf = builder.build();
        Map&lt;Integer,InputPreProcessor&gt; map = new HashMap&lt;&gt;();
        map.put(0, conf.getInputPreProcess(0));
        map.put(3, new CnnToFeedForwardPreProcessor(8,8,50));
        conf.setInputPreProcessors(map);
		</comment>
		<comment id='4' author='christopher-beckham' date='2016-04-29T05:24:26Z'>
		Are we able to add in the feed forward preprocessor without manually having to calculate its output shape (which in this case is 50x8x8)? It would be nice for that to just be calculated automatically so fully-connected layers can go at the end of any kind of CNN architecture.
		</comment>
		<comment id='5' author='christopher-beckham' date='2016-04-29T06:30:39Z'>
		As noted, it's already been fixed in the current version of dl4j. So yes.
		</comment>
		<comment id='6' author='christopher-beckham' date='2019-01-21T05:53:02Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>