<bug id='3233' author='crockpotveggies' open_date='2017-04-11T21:32:11Z' closed_time='2017-04-13T09:13:00Z'>
	<summary>PW + Deserialized model w/ updater attempts memory reallocation</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

When using ParallelWrapper with a deserialized model, device memory will begin to leak and eventually force a shutdown. Does not occur when PW is used with a fresh model config.
Edit: This is not a memory leak, but a strange memory operation.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Master branch
Ubuntu 16.04
CUDA 8, cuDNN 6
NVIDIA driver 378
MagicQueue Enabled
Workspaces Enabled

Log output:
&lt;denchmark-code&gt;[main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [JCublasBackend] backend
[main] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 32
[main] INFO org.reflections.Reflections - Reflections took 141 ms to scan 1 urls, producing 29 keys and 192 values
[main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CUDA]; OS: [Linux]
[main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [12]; Memory: [28.4GB];
[main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [CUBLAS]
[main] INFO org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner - Device name: [TITAN X (Pascal)]; CC: [6.1]; Total/free memory: [12779978752]
[main] INFO org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner - Device name: [TITAN X (Pascal)]; CC: [6.1]; Total/free memory: [12782075904]
Available devices: [0, 1]
##### Heap utilization statistics [MB] #####
Used Memory:16
Free Memory:947
Total Memory:964
Max Memory:29127

##### Hyperparameters #####
Learning rate: 0.001
Batch size: 72
[main] INFO uk.org.lidalia.sysoutslf4j.context.SysOutOverSLF4J - Package org.slf4j.impl.SimpleLogger registered; all classes within it or subpackages of it will be allowed to print to System.out and System.err
[main] INFO uk.org.lidalia.sysoutslf4j.context.SysOutOverSLF4J - Replaced standard System.out and System.err PrintStreams with SLF4JPrintStreams
[main] INFO uk.org.lidalia.sysoutslf4j.context.SysOutOverSLF4J - Redirected System.out and System.err to SLF4J for this context
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Load data....
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Loading paths....
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Splitting data for production....
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Total training images is 307791
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Total test images is 617
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Initializing RecordReader and pipelines....
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Total training labels: 8644
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Total test labels: 8644
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Creating RecordReader iterator....
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Fitting normalizer...
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Loading existing model /home/justin/saved-models/FaceNet-RCROPSTAGE1-0.1LR-72BATCH-60.zip....
[main] INFO org.reflections.Reflections - Reflections took 1657 ms to scan 1 urls, producing 3979 keys and 22860 values
[main] INFO org.reflections.Reflections - Reflections took 111 ms to scan 1 urls, producing 405 keys and 1564 values
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Applying fine-tuning to existing model...
[main] INFO RCROPSTAGE2-0.001LR-72BATCH - Beginning parallel training....
[MQ_THREAD 0] INFO org.deeplearning4j.parallelism.MagicQueue - MQAD_THREAD started on device [0/0]
[MQ_THREAD 1] INFO org.deeplearning4j.parallelism.MagicQueue - MQAD_THREAD started on device [1/1]
[main] INFO org.nd4j.jita.workspace.CudaWorkspace - Allocating [ADSI_ITER] workspace of 376275456 bytes...
[MQ_THREAD 0] INFO org.nd4j.jita.workspace.CudaWorkspace - Allocating [MQAD_THREAD] workspace of 125425152 bytes...
[MQ_THREAD 1] INFO org.nd4j.jita.workspace.CudaWorkspace - Allocating [MQAD_THREAD] workspace of 125425152 bytes...
[Thread-76] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 0
[main] ERROR java.lang.Throwable - Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NullPointerException
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.parallelism.ParallelWrapper.fit(ParallelWrapper.java:377)
[main] ERROR java.lang.Throwable - 	at ai.bernie.train.TrainClassifier.run(TrainClassifier.java:201)
[main] ERROR java.lang.Throwable - 	at ai.bernie.train.TrainerRunner.run(TrainerRunner.java:141)
[main] ERROR java.lang.Throwable - 	at ai.bernie.train.TrainerRunner.main(TrainerRunner.java:163)
[main] ERROR java.lang.Throwable - Caused by: java.lang.RuntimeException: java.lang.NullPointerException
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.parallelism.trainer.DefaultTrainer.waitTillRunning(DefaultTrainer.java:271)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.parallelism.ParallelWrapper.fit(ParallelWrapper.java:375)
[main] ERROR java.lang.Throwable - 	... 3 more
[main] ERROR java.lang.Throwable - Caused by: java.lang.NullPointerException
[main] ERROR java.lang.Throwable - 	at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936)
[main] ERROR java.lang.Throwable - 	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:283)
[main] ERROR java.lang.Throwable - 	at org.nd4j.jita.handler.impl.CudaZeroHandler.relocateObject(CudaZeroHandler.java:875)
[main] ERROR java.lang.Throwable - 	at org.nd4j.jita.flow.impl.SynchronousFlowController.prepareActionAllWrite(SynchronousFlowController.java:108)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.layers.convolution.CudnnConvolutionHelper.backpropGradient(CudnnConvolutionHelper.java:247)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.backpropGradient(ConvolutionLayer.java:159)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.graph.vertex.impl.LayerVertex.doBackward(LayerVertex.java:121)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.graph.ComputationGraph.calcBackpropGradients(ComputationGraph.java:1379)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.graph.ComputationGraph.computeGradientAndScore(ComputationGraph.java:1070)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:160)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:56)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:59)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:927)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:882)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:703)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.parallelism.trainer.DefaultTrainer.run(DefaultTrainer.java:234)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='crockpotveggies' date='2017-04-12T13:35:25Z'>
		Where you see leak in this stack trace?
		</comment>
		<comment id='2' author='crockpotveggies' date='2017-04-12T13:41:35Z'>
		And where can i see code, that reproduces this behavior?
		</comment>
		<comment id='3' author='crockpotveggies' date='2017-04-12T13:50:18Z'>
		Well, it's definitely not a leak, it's something more weird.
It's relocation, after cudnn completed calculations, and attempt to allocate object without workspace. Obviously: bug. Not sure how it's possible yet.
		</comment>
		<comment id='4' author='crockpotveggies' date='2017-04-12T13:51:33Z'>
		Main concern here: why it suddenly decided to relocate something...
		</comment>
		<comment id='5' author='crockpotveggies' date='2017-04-12T15:05:25Z'>
		My guess is: you've tried to use deserialized model that was built without workspaces, thus had  workspaces disabled. Am i right?
		</comment>
		<comment id='6' author='crockpotveggies' date='2017-04-12T15:26:18Z'>
		You're close on the diagnosis, it appears the problem is with the Transfer Learning API.
Applying fine-tuning to existing model...
That's where I'm clearing learning rate settings and changing them. It would appear that when I use the transfer learning API, Workspace config isn't carried over.
		</comment>
		<comment id='7' author='crockpotveggies' date='2017-04-13T09:13:00Z'>
		Issue is fixed via enforced workspaceMode option in PW. Default value is SEPARATE though. So if you want SINGLE - set that option manually in PW builder.
		</comment>
		<comment id='8' author='crockpotveggies' date='2018-09-29T21:39:08Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>