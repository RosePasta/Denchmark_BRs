<bug id='8319' author='orausch' open_date='2019-10-28T14:06:51Z' closed_time='2019-11-02T00:25:54Z'>
	<summary>ActivationListener isn't called on execBackwards</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I want to get the calculate the loss and gradients for a batch at the same time (&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/8318&gt;#8318&lt;/denchmark-link&gt;
 related) in my training loop. &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 suggested to add an activation listener to the variable, but this doesn't seem to work. The following example does not print the expected output from the listener:
&lt;denchmark-link:https://gist.github.com/orausch/f78035f42940e5f614c04f32cd53a271&gt;https://gist.github.com/orausch/f78035f42940e5f614c04f32cd53a271&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

beta5
	</description>
	<comments>
		<comment id='1' author='orausch' date='2019-10-28T14:17:27Z'>
		As a user I would also expect sd.getArrForVarName("loss") to also work, but that returns null.
		</comment>
		<comment id='2' author='orausch' date='2019-11-01T11:54:10Z'>
		Thanks for reporting this, it has been fixed here: &lt;denchmark-link:https://github.com/KonduitAI/deeplearning4j/pull/21&gt;KonduitAI#21&lt;/denchmark-link&gt;

It will be merged back to Eclipse master soon
		</comment>
	</comments>
</bug>