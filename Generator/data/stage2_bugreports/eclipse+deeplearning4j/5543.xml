<bug id='5543' author='borkox' open_date='2018-06-09T08:26:13Z' closed_time='2018-07-27T11:15:34Z'>
	<summary>Precision Loss whe  saving ACPolicy using RL4J</summary>
	<description>
I made an education of RL agent using the A3CDiscreteConv. Then I saved the policy in a file. I played the agent with the policy which I still hold in memory. After that I played the policy which I loaded from file. The results were very different and I had a message telling me I will loose precision:
&lt;denchmark-code&gt;2018-06-09 11:14:17 INFO  c.r.r.a.EvaluatorA3C - Loading agent from ./rl4j/a3c_policy.bin
2018-06-09 11:14:17 INFO  o.n.l.f.Nd4jBackend - Loaded [CpuBackend] backend
2018-06-09 11:14:18 INFO  o.n.n.NativeOpsHolder - Number of threads used for NativeOps: 2
2018-06-09 11:14:19 INFO  o.n.n.Nd4jBlas - Number of threads used for BLAS: 2
2018-06-09 11:14:19 INFO  o.n.l.a.o.e.DefaultOpExecutioner - Backend used: [CPU]; OS: [Windows 10]
2018-06-09 11:14:19 INFO  o.n.l.a.o.e.DefaultOpExecutioner - Cores: [4]; Memory: [1.8GB];
2018-06-09 11:14:19 INFO  o.n.l.a.o.e.DefaultOpExecutioner - Blas vendor: [OPENBLAS]
2018-06-09 11:14:19 WARN  o.n.l.a.b.BaseDataBuffer - Loading a data stream with opType different from what is set globally. Expect precision loss
2018-06-09 11:14:19 WARN  o.n.l.a.b.BaseDataBuffer - Loading a data stream with opType different from what is set globally. Expect precision loss
2018-06-09 11:14:20 INFO  o.d.n.g.ComputationGraph - Starting ComputationGraph with WorkspaceModes set to [training: ENABLED; inference: ENABLED], cacheMode set to [NONE]

&lt;/denchmark-code&gt;

So I didn't changed any of the default datatypes in global configuration. How can I make the model from the file to be equal to the one in memory.
Here is some code:
&lt;denchmark-code&gt;
package com.rltrader.rl.a3c;

import com.rltrader.hprices.CachingPricesService;
import com.rltrader.model.CommonConfig;
import com.rltrader.model.ConfigurationModel;
import com.rltrader.model.LearnConfig;
import com.rltrader.rl.PricesObservation;
import com.rltrader.rl.mdp.LearningPricesMDP;
import lombok.extern.slf4j.Slf4j;
import org.deeplearning4j.api.storage.StatsStorage;
import org.deeplearning4j.optimize.api.TrainingListener;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.rl4j.learning.IHistoryProcessor;
import org.deeplearning4j.rl4j.learning.Learning;
import org.deeplearning4j.rl4j.learning.async.AsyncThread;
import org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CDiscrete;
import org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CDiscreteConv;
import org.deeplearning4j.rl4j.policy.ACPolicy;
import org.deeplearning4j.rl4j.util.DataManager;
import org.deeplearning4j.ui.api.UIServer;
import org.deeplearning4j.ui.stats.StatsListener;
import org.deeplearning4j.ui.storage.InMemoryStatsStorage;
import org.nd4j.linalg.learning.config.Nesterovs;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.io.IOException;
import java.lang.reflect.Field;

@Component
@Slf4j
public class LearnerA3C {



    @Value("${rl4j.work.folder}")
    private String rl4jDataFolder;

    @Value("${rl4j.work.a3c.policy}")
    private String policySaveFile;

    @Autowired
    private CachingPricesService cachingPricesService;

    @Autowired
    private ConfigurationModel configurationModel;

    @Autowired
    private EvaluatorA3C evaluatorA3C;


    public void startLearn() throws IOException {
        LearnConfig learn = configurationModel.getLearn();
        CommonConfig common = configurationModel.getCommon();

        //prepare UI
        UIServer uiServer = UIServer.getInstance();
        StatsStorage statsStorage = new InMemoryStatsStorage();
        uiServer.attach(statsStorage);
        int listenerFrequency = 10;
        TrainingListener[] listeners = new TrainingListener[]{
                new StatsListener(statsStorage, listenerFrequency),
                new ScoreIterationListener(listenerFrequency)
        };

        final A3CDiscrete.A3CConfiguration A3C =
                new A3CDiscrete.A3CConfiguration(
                        123,            //Random seed
                        learn.getMaxStepByEpoch(),          //Max step By epoch
                        learn.getMaxStep(),        //Max step
                        8,              //Number of threads
                        32,             //t_max
                        500,            //num step noop warmup
                        1,            //reward scaling
                        0.99,           //gamma
                        10.0            //td-error clipping
                );

        final ActorCriticFactory.Configuration NET_A3C =
                new ActorCriticFactory.Configuration(
                        0.00,   //l2 regularization
                        new Nesterovs(0.00001, 0.78), //learning rate
                        listeners,
                        true
                );


        log.info("============ LEARN (Actor Critic)===========");
        cachingPricesService.cacheData();

        DataManager manager = new DataManager(rl4jDataFolder, true);
        LearningPricesMDP mdp = new LearningPricesMDP(configurationModel, cachingPricesService);
        ActorCriticFactory actorCriticFactory
                = new ActorCriticFactory(NET_A3C);
        //define the training
//        ActorCriticCompGraph actorCritic = actorCriticFactory.buildActorCritic(
//                mdp.getObservationSpace().getShape(),
//                mdp.getActionSpace().getSize());

//        A3CDiscrete&lt;PricesObservation&gt; asyncLearning = new A3CDiscrete(
//                mdp,
//                actorCritic,
//                A3C,
//                manager){};
        IHistoryProcessor.Configuration hpconf = new IHistoryProcessor.Configuration(
                4,
                common.getWindowSize(),
                common.getPortfolioSymbols().length,
                common.getWindowSize(),
                common.getPortfolioSymbols().length,
                0,
                0,
                1
        );
        A3CDiscrete&lt;PricesObservation&gt; asyncLearning = new A3CDiscreteConv(
                mdp,
                actorCriticFactory,
                hpconf,
                A3C,
                manager){
            @Override
            protected void setHistoryProcessor(IHistoryProcessor.Configuration conf) {
                try {
                    Field field = Learning.class.getDeclaredField("historyProcessor");
                    field.setAccessible(true);
                    field.set(this, new HistoryProcessorStorage(conf));
                } catch (Exception e) {
                    throw new RuntimeException(e);
                }
            }
            @Override
            public AsyncThread newThread(int i) {
                try {
                    AsyncThread at = super.newThread(i);
                    Field field = AsyncThread.class.getDeclaredField("historyProcessor");
                    field.setAccessible(true);
                    field.set(at, new HistoryProcessorStorage(hpconf));
                    return at;
                }  catch (Exception e) {
                    throw new RuntimeException(e);
                }
            }
        };

        //train
        asyncLearning.train();

        //get the final policy
        ACPolicy&lt;PricesObservation&gt; pol = asyncLearning.getPolicy();

        //serialize and save (serialization showcase, but not required)
        log.info("Saving agent to file {}", policySaveFile);
        pol.save(policySaveFile);

        //close the mdp (close http)
        mdp.close();
        log.info("End learn");

        // ALERT: The evaluation is not the same if the policy is loaded from a file
        evaluatorA3C.evaluate(pol);

        System.exit(0);
    }
}

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='borkox' date='2018-06-10T08:03:22Z'>
		The warning messages are about image "compression" for the log, and can be
ignored. AC3 uses randomness as part of the policy. You'll need to give it
the same seed to get the same series of actions.
		</comment>
		<comment id='2' author='borkox' date='2018-06-10T13:32:46Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/21053237/41202014-bf883d1c-6ccb-11e8-874b-9f244c6f559f.png&gt;&lt;/denchmark-link&gt;

The seed is stored in the file. I am sure it is not that the problem. I also think the coefficients are the same, but I suspect the connections between nodes are not recovered and thus the network loaded only looks the same, but makes completely different calculations.
		</comment>
		<comment id='3' author='borkox' date='2018-06-10T14:25:53Z'>
		Ok, does this happen with simple MDPs like the A3CCartpole example?
		</comment>
		<comment id='4' author='borkox' date='2018-06-10T15:41:15Z'>
		I will try to make a failing test case, because otherwise I cannot explain it. But I took most of the code from A3CALE.
		</comment>
		<comment id='5' author='borkox' date='2018-06-10T18:11:08Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/21053237/41204850-8eb4440c-6cf2-11e8-9e65-a1603b0d1fd6.png&gt;&lt;/denchmark-link&gt;

I couldn't make a failing unit test, but I started my app in debug and the first difference, then I trained a policy, then I saved it and loaded from a file -&gt; I noticed is that the Random is not loaded, see the picture.
		</comment>
		<comment id='6' author='borkox' date='2018-06-10T18:45:29Z'>
		&lt;denchmark-link:https://user-images.githubusercontent.com/21053237/41205182-27984b7e-6cf7-11e8-8ba9-1ef12a10fe0e.png&gt;&lt;/denchmark-link&gt;

So I set the same Random()  objects on both policies, and I compared the output from logs and the outputs was exactly the same. So this was the problem, the random object was not persisted and I didn't noticed that. I don't know how you will classify this, is it a bug or not well explained. For me it is logical the Random() object to be saved, otherwise the policy behaves odd.
		</comment>
		<comment id='7' author='borkox' date='2018-06-11T02:29:34Z'>
		Yeah, I'd call that a bug :)
		</comment>
		<comment id='8' author='borkox' date='2018-06-11T07:33:18Z'>
		But I'm pretty sure ACPolicy.load("/path/to/model.zip", new Random(123)) does what you need...
Anyway, the seed used for the network configuration is different from the one used by ACPolicy... Should we still have it use the seed from the network by default instead of not using randomness by default?
		</comment>
		<comment id='9' author='borkox' date='2018-06-11T08:29:46Z'>
		Yes, I use this as a workaround, but I imagine that if I save with Random() inside it to restore with Random() inside it. It's up to your internal conventions to consider as a Bug or not.
		</comment>
		<comment id='10' author='borkox' date='2018-06-12T03:44:56Z'>
		&lt;denchmark-link:https://github.com/borkox&gt;@borkox&lt;/denchmark-link&gt;
 Right, let's try to be consistent with DL4J, at least...
&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 What is DL4J doing to restore the state of Random objects?
		</comment>
		<comment id='11' author='borkox' date='2018-06-12T03:47:46Z'>
		We dont store Random objects in dl4j models, we're storing seed. And with the same seed we're getting the same sequence on both backends. That's how it works there.
		</comment>
		<comment id='12' author='borkox' date='2018-06-12T03:52:41Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 Ok, but then if we save a model, reload it and restart training, we're going to get the same sequence of random numbers, right?
		</comment>
		<comment id='13' author='borkox' date='2018-09-21T12:58:54Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>