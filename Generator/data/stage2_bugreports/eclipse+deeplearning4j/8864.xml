<bug id='8864' author='lukaszbachman' open_date='2020-04-22T08:40:28Z' closed_time='2020-04-22T09:39:28Z'>
	<summary>JVM crash when allocating more memory than initial MMAP size</summary>
	<description>
JVM will crash when calling libnd4jcpu.so under following circumstances:

Memory Workspace is used
policy is set to: LocationPolicy.MMAP
initial size is set to X
one tries to create an INDArray which would consume more memory than  X (won't fit into the MMAP)

I also tried setting maxSize() but that doesn't seem to help - MMAP doesn't grow and JVM still crashes. I could tell that in case of a crash the entire MMAP is filled with data, so I'm guessing that the problem is about adding those extra numbers which won't fit.
You can try that out by running the code below - depending on the value of addMoreFloats the allocation will succeed (false) or fail (true) due to this overlfow.
&lt;denchmark-code&gt;public static void main(String[] args) {
    WorkspaceConfiguration mmap = WorkspaceConfiguration.builder()
      .initialSize(200 * 1024L * 1024L) // 200mbs
      .tempFilePath("/tmp/my.mmap")
      .policyLocation(LocationPolicy.MMAP)
      .build();
    try (MemoryWorkspace ws = Nd4j.getWorkspaceManager().getAndActivateWorkspace(mmap, "M2")) {
      int twoHundredMbsOfFloats = 52428800; // 200mbs % 4
      boolean addMoreFloats = true;
      if (addMoreFloats) {
        twoHundredMbsOfFloats += 1_000;
      }
      INDArray x = Nd4j.rand(DataType.FLOAT, twoHundredMbsOfFloats);
      }
}

&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


ND4J version: 1.0.0-beta6 (latest), CPU backend
Ubuntu 19.10 (I saw similar problems on Windows too, albeit error was different)

&lt;denchmark-h:h4&gt;Additional Information&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/lukaszbachman/c413fb5edf20891a0269fc7a6f85f323&gt;https://gist.github.com/lukaszbachman/c413fb5edf20891a0269fc7a6f85f323&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='lukaszbachman' date='2020-04-22T08:54:57Z'>
		Memory-mapped file shouldn't grow. But there shouldn't be crash as well.
		</comment>
		<comment id='2' author='lukaszbachman' date='2020-04-22T09:05:06Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 , so you mean to say that by design MMAP will only be as big as the value defined in ? I could implement a routine which retries the operation with a bigger MMAP. In such case it would be very handy to know how much data did not fit into the map. I can try to make that calculations myself, but maybe the library already knows that?
		</comment>
		<comment id='3' author='lukaszbachman' date='2020-04-22T09:08:14Z'>
		Yes it's design limitation. Try man mmap.
		</comment>
		<comment id='4' author='lukaszbachman' date='2020-04-22T09:39:25Z'>
		Fixed the crash. But keep in mind, in this particular case proper behavior will be "spill", so your array will be allocated in RAM, since it'll be considered a spilled array.
		</comment>
		<comment id='5' author='lukaszbachman' date='2020-04-22T10:07:05Z'>
		Sorry for asking, but where can I see this commit?
		</comment>
		<comment id='6' author='lukaszbachman' date='2020-04-22T10:08:34Z'>
		&lt;denchmark-link:https://github.com/KonduitAI/deeplearning4j/commit/78260efe54b331a74e68db96cf0f46609a257dd6&gt;KonduitAI@78260ef&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='lukaszbachman' date='2020-04-22T12:27:30Z'>
		Thanks!
		</comment>
	</comments>
</bug>