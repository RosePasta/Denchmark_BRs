<bug id='3449' author='crockpotveggies' open_date='2017-05-25T00:39:02Z' closed_time='2017-06-29T04:38:43Z'>
	<summary>Layers with no params trigger NPE for L1/L2 calc</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

On ComputationGraph (and possibly MLN), calcL1 and calcL2 is called for each layer in the network. The issue is that for layers with zero parameters like ZeroPaddingLayer, if the base layer methods for calculating L1/L2 are not overridden an NPE will be triggered.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Master
&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

Looks like some refactoring will be required if we don't want to override calcL1/L2. Some guidance on how to proceed will be helpful.
	</description>
	<comments>
		<comment id='1' author='crockpotveggies' date='2017-05-25T00:59:22Z'>
		As noted in gitter: my thinking is that layers like ZeroPaddingLayer don't have any parameters in the first place, and consequently the calculateXByParam (and similar) methods should never be called in the first place.
		</comment>
		<comment id='2' author='crockpotveggies' date='2017-06-29T04:38:43Z'>
		Test added here for L1/L2 calculation on all no-param layers: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/3585&gt;https://github.com/deeplearning4j/deeplearning4j/pull/3585&lt;/denchmark-link&gt;

I also checked the source on ZeroPaddingLayer, and it correctly overrides l1/l2 methods, returning 0.
		</comment>
		<comment id='3' author='crockpotveggies' date='2018-09-26T20:54:32Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>