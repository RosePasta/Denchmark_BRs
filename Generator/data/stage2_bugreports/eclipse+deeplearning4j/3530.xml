<bug id='3530' author='pierrelevy' open_date='2017-06-16T10:34:32Z' closed_time='2017-07-01T12:57:48Z'>
	<summary>Problem running with CUDA : java.lang.IllegalStateException: MemcpyAsync</summary>
	<description>
Hi,
I'm trying to run an image classifier with my GPU GeForce GTX 750Ti with 2Go of RAM.
Is the following error is simply an Out Of Memory error, or could be a bug ?
Thanks in advance
Best regards
Pierre
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

11:59:44.052 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 310 is 1.4216575899118291
11:59:45.774 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 311 is 1.292740208821207
11:59:47.585 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 312 is 1.4333530097937506
11:59:49.147 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 313 is 1.4792262790546982
11:59:50.848 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 314 is 1.4853299253535195
11:59:51.165 [main] WARN  o.n.j.handler.impl.CudaZeroHandler - Out of [DEVICE] memory, host memory will be used instead: deviceId: [0], requested bytes: [12]
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:3809 code=77() "cudaStreamSynchronize(*stream)"
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:4705 code=77() "result"
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:4705 code=77() "result"
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:4851 code=77() "result"
CUDA error at /projects/skymind/deploy/linux-x86_64/libnd4j/blas/cuda/NativeOps.cu:4801 code=77() "result"
Failed on [21499104256] -&gt; [8650336768], size: [80], direction: [2], result: [77]
Exception in thread "main" java.lang.IllegalStateException: MemcpyAsync failed: AllocationShape(offset=0, length=20, stride=1, elementSize=4, dataType=FLOAT)
at org.nd4j.jita.flow.impl.SynchronousFlowController.synchronizeToHost(SynchronousFlowController.java:63)
at org.nd4j.jita.flow.impl.GridFlowController.synchronizeToHost(GridFlowController.java:36)
at org.nd4j.jita.handler.impl.CudaZeroHandler.synchronizeThreadDevice(CudaZeroHandler.java:1189)
at org.nd4j.jita.allocator.impl.AtomicAllocator.synchronizeHostData(AtomicAllocator.java:318)
at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.getFloat(BaseCudaDataBuffer.java:884)
at org.nd4j.linalg.jcublas.buffer.CudaFloatDataBuffer.getDouble(CudaFloatDataBuffer.java:227)
at org.nd4j.linalg.api.ndarray.BaseNDArray.getDouble(BaseNDArray.java:3535)
at org.deeplearning4j.ui.stats.BaseStatsListener.getHistograms(BaseStatsListener.java:749)
at org.deeplearning4j.ui.stats.BaseStatsListener.iterationDone(BaseStatsListener.java:446)
at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:69)
at org.deeplearning4j.optimize.Solver.optimize(Solver.java:51)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:999)
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;



Deeplearning4j version
0.8.0


platform information (OS, etc)
Ubuntu 17.04


CUDA version
libraries cuda-8.0-1.3*.jar


nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44

NVIDIA driver version
375.66

	</description>
	<comments>
		<comment id='1' author='pierrelevy' date='2017-06-16T10:48:39Z'>
		99.9% it's just unlucky oom, caused by failed temp allocation for UI.
But at the same time, i don't like this crash. Proper fallback to host memory should be considered here.
		</comment>
		<comment id='2' author='pierrelevy' date='2017-06-16T10:49:51Z'>
		P.s. switch to 0.8.1-SNAPSHOT, and use workspaces, with training workspaceMode set to SEPARATE. Your stuff should use less memory there.
		</comment>
		<comment id='3' author='pierrelevy' date='2017-06-16T11:01:37Z'>
		Thanks very much :)
		</comment>
		<comment id='4' author='pierrelevy' date='2018-09-26T16:54:30Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>