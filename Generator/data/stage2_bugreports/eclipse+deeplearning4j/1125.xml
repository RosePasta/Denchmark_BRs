<bug id='1125' author='rohanar' open_date='2016-02-08T11:39:36Z' closed_time='2016-05-01T12:33:33Z'>
	<summary>SerializationUtils.saveObject(Object, File f) fails in current master (3.9-SNAPSHOT)</summary>
	<description>
Serializing ClusterSet object with SerializationUtils.saveObject(cs, new File(modelfilename));. This worked before updating to the latest in the mater branch.
Error is Caused by: java.io.NotSerializableException: java.nio.HeapIntBuffer
	</description>
	<comments>
		<comment id='1' author='rohanar' date='2016-02-14T14:22:34Z'>
		Could you provide more data? I was trying to reproduce it but saveObject works for me on current master.
		</comment>
		<comment id='2' author='rohanar' date='2016-02-14T15:47:22Z'>
		Yeah it's fine now. The current master issue was related to BaseNDArray having a heap int buffer for storing shape information. I moved that away (underneath the covers) to being a databuffer which fixed it. Thanks for highlighting this.
		</comment>
		<comment id='3' author='rohanar' date='2016-02-17T10:37:02Z'>
		SerializationUtils.readObject(new File(serialized-filename)) fails.
Exception in thread "main" java.lang.RuntimeException: java.lang.IllegalStateException: unread block data
at org.nd4j.linalg.util.SerializationUtils.readObject(SerializationUtils.java:40)
		</comment>
		<comment id='4' author='rohanar' date='2016-02-18T00:39:06Z'>
		Seems working, I pushed the test that code passes.
		</comment>
		<comment id='5' author='rohanar' date='2016-02-18T08:18:29Z'>
		I will pass my code in a gist for you guys to run it at your end.
		</comment>
		<comment id='6' author='rohanar' date='2016-02-18T09:16:24Z'>
		I have played with this a bit and found the following: The problem occurs with serializing/deserializing bigger ClusterSet models. In a collection of 2525 documents, the file sizes of the serialized ParagraphVectors and ClusterSet models are 91,176KB and 20,077,440KB respectively. Serialized ClusterSet file size is unusually big. Deserializing fails when reading the ClusterSet from this file. All works fine with smaller files.
I did try removing the poiints before serializing the ClusterSet by ClusterSet.removePoints() method. This will not delete the cluster centers, and seems to be sufficient for using the model for finding the cluster of new points. When this was done, the deserialized file size came down to 199KB  and deserializing worked fine. It may be worth finding out why ClusterSet (with points not removed) is that big, and possibly find a smarter way of serializing ClusterSet models.
		</comment>
		<comment id='7' author='rohanar' date='2016-05-01T12:33:33Z'>
		Closing due to lack of activity.
		</comment>
		<comment id='8' author='rohanar' date='2019-01-21T04:53:17Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>