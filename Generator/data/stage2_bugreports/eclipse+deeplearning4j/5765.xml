<bug id='5765' author='AlexDBlack' open_date='2018-07-02T10:12:54Z' closed_time='2018-07-24T01:42:31Z'>
	<summary>DL4J: JVM Crash on OCNNOutputLayerTest</summary>
	<description>
Occurs with both of these tests:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/ocnn/OCNNOutputLayerTest.java&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/ocnn/OCNNOutputLayerTest.java&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_INT_DIVIDE_BY_ZERO (0xc0000094) at pc=0x000000006976f19a, pid=17012, tid=0x00000000000031d8
#
# JRE version: Java(TM) SE Runtime Environment (8.0_151-b12) (build 1.8.0_151-b12)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.151-b12 mixed mode windows-amd64 compressed oops)
# Problematic frame:
# C  [libnd4jcpu.dll+0x2ef19a]
#
# Failed to write core dump. Minidumps are not enabled by default on client versions of Windows
#
# An error report file with more information is saved as:
# C:\DL4J\Git\deeplearning4j\deeplearning4j\deeplearning4j-core\hs_err_pid17012.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://gist.github.com/AlexDBlack/6b641f247f8cc769906a123e3f9fa056&gt;https://gist.github.com/AlexDBlack/6b641f247f8cc769906a123e3f9fa056&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-07-02T11:31:52Z'>
		Isolated to this test case:
&lt;denchmark-code&gt;    @Test
    public void test(){
        INDArray a = Nd4j.create(150,4,2);
        //INDArray b = Nd4j.create(150,4,1).dup('f');       //OK with this
        INDArray b = Nd4j.create(150,4).reshape('f', 150, 4, 1);

        Broadcast.mul(a,b,a,2);
    }
&lt;/denchmark-code&gt;

Question here: is this a valid broadcast?
If we were talking auto-broadcasting, we expect to iterate over dimension 2 (due to the 1s in one of the shapes).
However, I believe our manual BroadcastXOps take as arguments the dimensions that they match on (so the correct args would  be 0,1 here given arrays are rank 3)...
for example: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/b68d7f38a908d88d0c522058027e5c7b4f10429f/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ndarray/BaseNDArray.java#L3101-L3132&gt;https://github.com/deeplearning4j/deeplearning4j/blob/b68d7f38a908d88d0c522058027e5c7b4f10429f/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ndarray/BaseNDArray.java#L3101-L3132&lt;/denchmark-link&gt;

So, some questions:
(a) why did this previously execute without error?
(b) Was the past behaviour correct? (if not: how did it pass tests previously?)
Note also the '2' arg is from Shape.getBroadcastDimensions method.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-09-21T14:58:54Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>