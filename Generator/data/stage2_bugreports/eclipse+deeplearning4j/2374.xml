<bug id='2374' author='yylonly' open_date='2016-11-25T15:34:25Z' closed_time='2016-11-28T20:08:34Z'>
	<summary>Can not only use second GPU</summary>
	<description>
I have two GPUs on the server

+------------------------------------------------------+
| NVIDIA-SMI 352.93     Driver Version: 352.93         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40m          On   | 0000:02:00.0     Off |                    0 |
| N/A   50C    P0    78W / 235W |   4979MiB / 11519MiB |     75%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40m          On   | 0000:84:00.0     Off |                    0 |
| N/A   41C    P0    77W / 235W |   2574MiB / 11519MiB |     94%      Default |
+-------------------------------+----------------------+----------------------+

But when i try only use second GPU by
.useDevice(1)
I alway get the out of boundary error:

java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:294)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
at java.util.ArrayList.rangeCheck(ArrayList.java:653)
at java.util.ArrayList.get(ArrayList.java:429)
at org.nd4j.jita.constant.ConstantProtector.containsDataBuffer(ConstantProtector.java:56)
at org.nd4j.jita.constant.ProtectedCudaShapeInfoProvider.createShapeInformation(ProtectedCudaShapeInfoProvider.java:60)
at org.nd4j.linalg.jcublas.CachedShapeInfoProvider.createShapeInformation(CachedShapeInfoProvider.java:41)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:172)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:253)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:290)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:567)
at org.nd4j.linalg.jcublas.JCublasNDArray.(JCublasNDArray.java:258)
at org.nd4j.linalg.jcublas.JCublasNDArrayFactory.create(JCublasNDArrayFactory.java:224)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4477)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4439)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:3692)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.init(MultiLayerNetwork.java:396)
at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.init(MultiLayerNetwork.java:350)
at net.mydreamy.encrpyed.ministdes.TranningEncrypedDESMinistMultiGPUFFSingle.main(TranningEncrypedDESMinistMultiGPUFFSingle.java:258)
... 6 more

	</description>
	<comments>
		<comment id='1' author='yylonly' date='2016-11-25T15:35:23Z'>
		Please show me what exactly you're doing.
I mean .useDevice(1) &lt;--- what's that? Can i have more details?
		</comment>
		<comment id='2' author='yylonly' date='2016-11-25T15:40:29Z'>
		code is here:
&lt;denchmark-code&gt;	   Configuration cudaconfig = CudaEnvironment.getInstance().getConfiguration();

           cudaconfig.allowMultiGPU(false).allowCrossDeviceAccess(false).useDevice(1);
&lt;/denchmark-code&gt;

I test get device api though
log.info("Device list " + conf.getAvailableDevices().toString());
I can get two GPUs on the server

[net.mydreamy.encrpyed.ministdes.TestGPUEnvironment.main()] INFO net.mydreamy.encrpyed.ministdes.TestGPUEnvironment - Device list [0, 1]

		</comment>
		<comment id='3' author='yylonly' date='2016-11-25T15:43:51Z'>
		As temporary workaround - please get rid of both calls, and just use CUDA environment variables to route computations to specific device.
And meanwhile i'll check what's wrong there.
		</comment>
		<comment id='4' author='yylonly' date='2016-11-25T15:47:21Z'>
		
just use CUDA environment variables to route computations to specific device.

How to do that? Does you mean only use cudaconfig.useDevice(1) ?
		</comment>
		<comment id='5' author='yylonly' date='2016-11-25T15:50:11Z'>
		No, there's nVidia environment variables that limit number of devices available for given process.
&lt;denchmark-link:https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars&gt;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars&lt;/denchmark-link&gt;

You need the first one: CUDA_​VISIBLE_​DEVICES
		</comment>
		<comment id='6' author='yylonly' date='2016-11-25T15:54:26Z'>
		Okay, let me try.
Btw, i use cuda-7.5-platform
		</comment>
		<comment id='7' author='yylonly' date='2016-11-25T16:12:02Z'>
		
export CUDA_VISIBLE_DEVICES=1

that is work for my case: only use second GPU
however, I invoke .getAvailableDevices(), it shows that

INFO net.mydreamy.encrpyed.ministdes.TestGPUEnvironment - Device list [0]

When i change export CUDA_VISIBLE_DEVICES=0, the API still shows that

INFO net.mydreamy.encrpyed.ministdes.TestGPUEnvironment - Device list [0]

Maybe, this will help to figure out the bugs.
		</comment>
		<comment id='8' author='yylonly' date='2016-11-25T16:24:01Z'>
		No, that's not a bug. That's valid behavior. When you define CUDA_VISIBLE_DEVICE=1, for all subsequent calls it's assumed the only device, and is listed as 0.
I.e. if you have 8 devices on system and set CUDA_VISIBLE_DEVICES=1,3,5,6
on any cuda app you'll see them as 0,1,2,3. Routing will be provided by CUDA driver.
		</comment>
		<comment id='9' author='yylonly' date='2016-11-25T16:25:29Z'>
		Okay, I see
		</comment>
		<comment id='10' author='yylonly' date='2016-11-25T17:54:54Z'>
		That doesn't matter, same codebase is used, just different CUDA binaries.
		</comment>
		<comment id='11' author='yylonly' date='2016-11-25T18:24:38Z'>
		As speaking of original issue - yep, i confirm it's a bug. useDevice(int) causes undesired behaviour.
I'll take care of this one, thanks for highlighting.
		</comment>
		<comment id='12' author='yylonly' date='2016-11-28T10:30:20Z'>
		Issue is fixed/improved. Now it's definitely possible to force specific device/devices being used right from jvm. Will be merged a bit later today.
Thanks for highlighting this issue.
		</comment>
		<comment id='13' author='yylonly' date='2016-11-28T10:36:17Z'>
		Thanks. I will pull and test after the merging..
		</comment>
		<comment id='14' author='yylonly' date='2016-11-28T10:39:16Z'>
		Also, i've added method .useDevices(int... devices) that accepts multiple devices, if you want to use, say 2  devices out of 4.
		</comment>
		<comment id='15' author='yylonly' date='2016-11-28T10:53:32Z'>
		You could pull from my branches if you want: branch name is r119_fixes everywhere.
Here's the tests i've used: &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/blob/78c96a8a3f7aab948af84902c144e1b2123c4436/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/test/java/jcuda/jcublas/ops/DevicesTests.java#L18-L18&gt;https://github.com/deeplearning4j/nd4j/blob/78c96a8a3f7aab948af84902c144e1b2123c4436/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/test/java/jcuda/jcublas/ops/DevicesTests.java#L18-L18&lt;/denchmark-link&gt;

I've tested it on dual gpu system, works fine.
		</comment>
		<comment id='16' author='yylonly' date='2016-11-28T20:08:34Z'>
		Fix is merged.
		</comment>
		<comment id='17' author='yylonly' date='2019-01-20T10:09:00Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>