<bug id='1707' author='rasoolims' open_date='2016-06-17T06:21:23Z' closed_time='2016-06-17T09:30:43Z'>
	<summary>learningRateSchedule and setMomentumSchedule do not change learning rate and momentum over time!</summary>
	<description>
Map&lt;Integer, Double&gt; momentumSchedule = new HashedMap();
        double m = 0.001;
        for(int i=1;i&lt;100000;i++){
            momentumSchedule.put(i,Math.min(m,0.9999));
            m+=0.001;
 }

Map&lt;Integer, Double&gt; learningRateSchedule = new HashedMap();
        double lr = 0.1;
        for(int i=1;i&lt;100000;i++){
            learningRateSchedule.put(i,lr);
            lr*=0.96;
}

NeuralNetConfiguration.Builder confBuilder =  new NeuralNetConfiguration.Builder()
      .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
      .learningRateDecayPolicy(LearningRatePolicy.Schedule)
      .learningRateSchedule(learningRateSchedule)
       .regularization(true).l2(0.0001);
confBuilder.setMomentumSchedule(momentumSchedule);

/**
**  constructed deep net based on @confBuilder here (removed for readability)
**/


for (int i=0;i&lt;epoch,i++){
 while (trainIter.hasNext()) {
     net.fit(trainIter.next());

  // always print the same number (0.1 and 0.9) 
  System.out.println("learning rate:"+(net.getLayer(last_layer_index)).conf().getLearningRateByParam("W"));
    System.out.println("momentum:"+(net.getLayer(last_layer_index)).conf().getLayer().getMomentum());
    }
trainIter.reset()
}
	</description>
	<comments>
		<comment id='1' author='rasoolims' date='2016-06-17T07:47:13Z'>
		Confirmed as an issue. Problem here is that the iteration count in BaseOptimizer is not being updated currently. Thus iteration count is always being passed as 0, to the schedules.
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/b5473e0904485dc64c552ea94f24af0080c14a68/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BaseOptimizer.java#L288&gt;https://github.com/deeplearning4j/deeplearning4j/blob/b5473e0904485dc64c552ea94f24af0080c14a68/deeplearning4j-core/src/main/java/org/deeplearning4j/optimize/solvers/BaseOptimizer.java#L288&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/b5473e0904485dc64c552ea94f24af0080c14a68/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/updater/BaseUpdater.java#L88&gt;https://github.com/deeplearning4j/deeplearning4j/blob/b5473e0904485dc64c552ea94f24af0080c14a68/deeplearning4j-core/src/main/java/org/deeplearning4j/nn/updater/BaseUpdater.java#L88&lt;/denchmark-link&gt;

Fixing the iteration count is easy. However, we also want the iteration count to be persisted along with the network, such that if the network is saved + reloaded, the schedule is continued (not reset).
Edit: the iteration count isn't being updated for SGD, but it is being updated (along with schedules) for line search methods. Comment about needing to properly persist iteration count stands, however.
		</comment>
		<comment id='2' author='rasoolims' date='2016-06-17T09:31:28Z'>
		Thanks for flagging. That's now fixed + added an additional test based on your code.
I'll fix the iteration count persistance thing I mentioned separately.
		</comment>
		<comment id='3' author='rasoolims' date='2016-06-17T14:17:44Z'>
		How should I use the latest version in POM file?
		</comment>
		<comment id='4' author='rasoolims' date='2016-06-17T16:53:34Z'>
		For the meanwhile, this is how I fixed the problem for my code (working with the stable release):
 for(int i=0;i&lt;num_of_layers;i++){
       double lr =  (net.getLayer(i)).conf().getLearningRateByParam("W");
      (net.getLayer(i)).conf().setLearningRateByParam("W",lr*0.96);
       lr =  (net.getLayer(i)).conf().getLearningRateByParam("b");
       (net.getLayer(i)).conf().setLearningRateByParam("b",lr*0.96);
}
My manual checks show that it works (and also improves convergence speed)
		</comment>
		<comment id='5' author='rasoolims' date='2016-06-18T01:44:29Z'>
		I think this was already mentioned in gitter, but you'll have to build everything from source (which is non-trivial to do - need c++ build tools, etc).
If you do want to give that a shot, jump in here and we'll see what we can do. &lt;denchmark-link:https://gitter.im/deeplearning4j/deeplearning4j/earlyadopters&gt;https://gitter.im/deeplearning4j/deeplearning4j/earlyadopters&lt;/denchmark-link&gt;

Also fwiw it shouldn't be more than a couple of weeks before the next release (rough estimate). So if your workaround does everything you need, it might be easier to just wait.
		</comment>
		<comment id='6' author='rasoolims' date='2019-01-20T23:53:20Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>