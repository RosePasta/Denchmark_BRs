<bug id='4018' author='icechen' open_date='2017-09-07T07:46:40Z' closed_time='2019-11-10T14:21:56Z'>
	<summary>Out of memory under parallel inference with GPU</summary>
	<description>
&lt;denchmark-h:h3&gt;Issue Description&lt;/denchmark-h&gt;

My application is &lt;denchmark-link:https://en.wikipedia.org/wiki/Named-entity_recognition&gt;Named Entity Recognition&lt;/denchmark-link&gt;
 with dl4j's GravesBidirectionalLSTM. I modified the code from Word2VecSentimentRNN.java example and run the service on Play framework 2.6.x.
The precision/recall/f-score performace is close to state-of-the-art, but it does not pass the pressure test using  jemeter (150 concurrent users, throuput about 2xx sentence/s). In parallel inference mode the RAM consumption gradually increases at the rate about 2~20 MB/s, and finally runs out of memory (&gt; 60G) and starts using swap.
My JVM memory-related arguments:

-Xmx8G
-Dorg.bytedeco.javacpp.maxbytes=10G
-Dorg.bytedeco.javacpp.maxPhysicalBytes=10G

Some of my code snippets:

CUDA and workspace setting
tag the sentence
set parallel inference

As suggested in gitter, I also compared performances of CPU &amp; GPU, which shows GPU is several times faster:

GPU, about 3xx ms/sentence
CPU, about 28xx ms/sentence

Besides, memory usage under CPU inference is around 4.5G
&lt;denchmark-h:h3&gt;Version Information&lt;/denchmark-h&gt;

Server environment:

OS: ubuntu 14.04
CPU: i7-6850K 3.60GHz
RAM: 64G
Graphic card: NVIDIA TITAN X * 4
Java: 1.8.0_144
Maven: 3.5.0
Deeplearning4j version: 0.9.1
CUDA version: nd4j-cuda-8.0-platform
NVIDIA driver version: 375.66

Thanks.
	</description>
	<comments>
		<comment id='1' author='icechen' date='2017-09-07T13:32:42Z'>
		So, basically, you're wrapping your output method with your own workspace, right? And calling tagSentence() in parallel threads, right?
		</comment>
		<comment id='2' author='icechen' date='2017-09-07T14:13:20Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;

Yeah, I think so. I use jmeter to launch 150 concurrent threads and test my tagging API, which would call
tagSentence().
&lt;denchmark-link:https://imgur.com/cVa5imR&gt;Here is my jmeter thread setting&lt;/denchmark-link&gt;

I also tried using only try/catch without  workspace in tagSentence(), but it ended up the same - using up all memory.
		</comment>
		<comment id='3' author='icechen' date='2017-09-07T14:26:13Z'>
		That's not good. The way you use workspace will lead to separate workspace of unknown size for each thread.
		</comment>
		<comment id='4' author='icechen' date='2017-09-07T14:47:45Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;

Indeed I chose to use inference workspace when I trained the model:
&lt;denchmark-link:https://gist.github.com/icechen/458dee34a4a623a3591cfcdc4b831cac&gt;My model configuation&lt;/denchmark-link&gt;

So if I
(1) remove the line ".inferenceWorkspaceMode(WorkspaceMode.SINGLE)" before training, and
(2)not to use workspace in tagSentence(),
that might help. Did i understand correctly?
		</comment>
		<comment id='5' author='icechen' date='2017-09-07T14:57:05Z'>
		No. 1) is ok. Your 2) is the problem. Get rid of workspace in tagSentence method.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 7 сент. 2017 г., в 17:48, icechen ***@***.***&gt; написал(а):

 @raver119 &lt;https://github.com/raver119&gt;
 Indeed I chose to use inference workspace when I trained the model:

 My model configuation &lt;https://gist.github.com/icechen/458dee34a4a623a3591cfcdc4b831cac&gt;
 So if I
 (1) remove the line ".inferenceWorkspaceMode(WorkspaceMode.SINGLE)" before training, and
 (2)not to use workspace in tagSentence(),
 that might help. Did i understand correctly?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub &lt;https://github.com/deeplearning4j/deeplearning4j/issues/4018#issuecomment-327822660&gt;, or mute the thread &lt;https://github.com/notifications/unsubscribe-auth/ALru_yZi_JM9fgy5XABJ5mCMhoMldbzUks5sgAItgaJpZM4PPatm&gt;.



		</comment>
		<comment id='6' author='icechen' date='2017-09-07T23:57:49Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;

I &lt;denchmark-link:https://gist.github.com/icechen/bdc7ac581258b95233978e066d408fd7&gt;got rid of workspace&lt;/denchmark-link&gt;
, but it still ate up the memory:
&lt;denchmark-code&gt;Exception in thread "InferenceThread-0" java.lang.OutOfMemoryError: Cannot allocate new FloatPointer(1): totalBytes = 3K, physicalBytes = 14G
        at org.bytedeco.javacpp.FloatPointer.&lt;init&gt;(FloatPointer.java:76)
        at org.bytedeco.javacpp.FloatPointer.&lt;init&gt;(FloatPointer.java:41)
        at org.nd4j.linalg.jcublas.blas.JcublasLevel3.sgemm(JcublasLevel3.java:107)
        at org.nd4j.linalg.api.blas.impl.BaseLevel3.gemm(BaseLevel3.java:57)
        at org.nd4j.linalg.api.ndarray.BaseNDArray.mmuli(BaseNDArray.java:3011)
        at org.nd4j.linalg.api.ndarray.BaseNDArray.mmul(BaseNDArray.java:2812)
        at org.deeplearning4j.nn.layers.recurrent.LSTMHelpers.activateHelper(LSTMHelpers.java:214)
        at org.deeplearning4j.nn.layers.recurrent.GravesBidirectionalLSTM.activateOutput(GravesBidirectionalLSTM.java:211)
        at org.deeplearning4j.nn.layers.recurrent.GravesBidirectionalLSTM.activate(GravesBidirectionalLSTM.java:170)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:789)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:870)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward(MultiLayerNetwork.java:861)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.silentOutput(MultiLayerNetwork.java:1906)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.output(MultiLayerNetwork.java:1898)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.output(MultiLayerNetwork.java:1871)
        at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.output(MultiLayerNetwork.java:1952)
        at org.deeplearning4j.parallelism.ParallelInference$InferenceWorker.run(ParallelInference.java:337)
Caused by: java.lang.OutOfMemoryError: Physical memory usage is too high: physicalBytes = 14G &gt; maxPhysicalBytes = 14G
        at org.bytedeco.javacpp.Pointer.deallocator(Pointer.java:576)
        at org.bytedeco.javacpp.Pointer.init(Pointer.java:121)
        at org.bytedeco.javacpp.FloatPointer.allocateArray(Native Method)
        at org.bytedeco.javacpp.FloatPointer.&lt;init&gt;(FloatPointer.java:68)
        ... 17 more
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='icechen' date='2017-09-08T06:04:06Z'>
		Okay, lets step back a bit. When you launch on 1 device (i.e. using CUDA_VISIBLE_DEVICES=0 env variable) - how much memory your model uses to serve 1 user at a time?
		</comment>
		<comment id='8' author='icechen' date='2017-09-08T19:23:33Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;

With this setting (server: set 1 visible gpu, client: &lt;denchmark-link:https://imgur.com/M7Qrm8k&gt;1 user querying the service&lt;/denchmark-link&gt;
), after 7 hours the memory usage is around 5900MB and seems stop growing.
		</comment>
		<comment id='9' author='icechen' date='2017-09-08T19:25:43Z'>
		Now you know your memory requirements for 1 model. If you'll have 4 gpus, you'll have model (and all corresponding workspaces) allocated on each device. So you'll have to increase your Xmx accordingly.
		</comment>
		<comment id='10' author='icechen' date='2017-09-08T19:26:17Z'>
		p.s. check out our memory guide for details.
		</comment>
		<comment id='11' author='icechen' date='2017-09-10T02:03:43Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;

I've read your memory intruction, and it suggests:

allocate heap memory just as needed (using -Xmx), and do not allocate too much
more memory should be allocated for off-heap, because it's where INDArrays stay

I re-opened 4 GPUs for the server, and started to test memory usage from 1 user to 20 users.
&lt;denchmark-h:h3&gt;Heap memory usage&lt;/denchmark-h&gt;

I've kept monitoring heap memory and found &lt;denchmark-link:https://imgur.com/rMkGEpx&gt;it never goes beyond 4G&lt;/denchmark-link&gt;
 under all of the settings. So I didn't increase heap memory limit and leave  unchanged (8G).
&lt;denchmark-h:h3&gt;Off-heap and total memory usage&lt;/denchmark-h&gt;

These are experiments on memory usage under different number of users:



# of users
Total GPU memory usage
Total CPU memory usage




1
6000MB
8788MB


10
6024MB
8798MB


15
6076MB
8839MB


20
6096MB
&gt; 15G (oom)



From my observation, when the number of user hits certain threshold (like 20), the total memory usage grows faster until oom.
To my knowledge, the total CPU memory usage of a java process is
Total memory =  heap + off-heap
Accoring to your memory instruction:
&lt;denchmark-code&gt;Of note here is we allocate memory on the GPU equivalent to the amount of offheap memory you specify.
We don’t use anymore of your GPU than that.
&lt;/denchmark-code&gt;

Under 20 users &lt;denchmark-link:https://imgur.com/rHfV64j&gt;the GPU memory usage (my process ID is 2691)&lt;/denchmark-link&gt;
 is 
Since off-heap memory is equivalent to GPU memory usage, the total CPU memory usage would be
Total memory =  heap + off-heap = heap + 6096MB, which should be less than 11G, because heap memory
usage is under 4G.
Got no idea what causes additional off-heap memory.
		</comment>
		<comment id='12' author='icechen' date='2017-11-13T06:35:05Z'>
		Recently I used some temporary solution to solve the OOM problem during prediction:


During prediction, NDArrays are created in workspace before they are fed into ComputationGraph's prediction method


The output(INDArray) method in ComputationGraph detaches NDArrays after internal silentOutput(INDArray) is called. Therefore they are not in workspace.
So instead of directly calling output(INDArray), I did something like doEvaluation(DataSetIterator iterator, T... evaluations), which uses NDArrays produced by silentOutput method to produce the final outputs (in my case, tagged natural language sentence).
In this way, no additional NDarrays is allocated outside workspace.


In other words I put every single NDArray in workspace. Failing to do either (1) and (2) above would cause out of memory in the long run.
As for multiple workspaces in multithread environment discussed before, it dosen't seem to be a problem. Each thread consumes about 50MB memory, and there are atmost 150 threads. So RAM usage by workspace is no more than 10GB.
My solution involves a subclass inherited from ComputationGraph, because silentOutput(INDArray) is a protected method and untouchable from outside.
The system passed the pressure test which lasted for 7 days and RAM usage is stable now.
&lt;denchmark-link:https://gist.github.com/icechen/a42e2e8b0908f901289d6fbc6ac84bd0&gt;The code is provided for your reference.&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='icechen' date='2017-11-13T09:11:40Z'>
		It means: there's a bug in workspace use in dl4j. Somewhere around scopes and inference run.
		</comment>
	</comments>
</bug>