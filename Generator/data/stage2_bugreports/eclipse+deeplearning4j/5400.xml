<bug id='5400' author='AlexDBlack' open_date='2018-05-31T03:12:28Z' closed_time='2018-06-01T10:26:46Z'>
	<summary>SameDiff: creating gradient function can break forward pass?</summary>
	<description>
&lt;denchmark-code&gt;    @Test
    public void gradFnBug() {
        Nd4j.setDataType(DataBuffer.Type.DOUBLE);
        SameDiff sd = SameDiff.create();

        int nOut = 4;
        int minibatch = 10;
        SDVariable input = sd.var("in", new int[]{-1, nOut});

        SDVariable loss = sd.mean("loss", input);

        INDArray inputArr = Nd4j.rand(minibatch, nOut);
        sd.associateArrayWithVariable(inputArr, input);

        sd.createGradFunction();
        sd.execAndEndResult();  //Exception
    }
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;org.nd4j.linalg.exception.ND4JIllegalStateException: Array for loss does not exist. Please use putArrayForVertexId instead.

	at org.nd4j.autodiff.samediff.SameDiff.updateArrayForVarName(SameDiff.java:545)
	at org.nd4j.autodiff.samediff.SameDiff.exec(SameDiff.java:6133)
	at org.nd4j.autodiff.samediff.SameDiff.execAndEndResult(SameDiff.java:4926)
	at org.nd4j.autodiff.gradcheck.TempDebug.testReductionGradientsSimple2(TempDebug.java:81)
&lt;/denchmark-code&gt;

Executes fine without the createGradFunction() call first...
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-09-21T22:24:21Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>