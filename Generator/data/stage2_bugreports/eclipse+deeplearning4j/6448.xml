<bug id='6448' author='AlexDBlack' open_date='2018-09-17T07:34:00Z' closed_time='2018-09-18T09:42:26Z'>
	<summary>libnd4j: depthwise_conv2d op bug</summary>
	<description>
I'm seeing failures on TF import for this op. But here's a simple stand-alone test case that shows the op must be incorrect:
&lt;denchmark-code&gt;    @Test
    public void testDepthwiseConv(){
        INDArray inNHWC = Nd4j.linspace(1,3*3*2, 3*3*2).reshape('c', 1,3,3,2);  //NHWC
        INDArray w = Nd4j.ones(2,2,2,1);    //[kH,kW,cIn,depthMultiplier]
        //No bias

        INDArray out = Nd4j.create(1,3,3,2);

        DynamicCustomOp op = DynamicCustomOp.builder("depthwise_conv2d")
                .addInputs(inNHWC, w)
                .addOutputs(out)
                .addIntegerArguments(
                        2,2,  //Kernel
                        1,1,  //Stride
                        0,0,  //Padding
                        1,1,  //Dilation
                        1,      //Same
                        1       //1=NHWC
                ).build();

        Nd4j.getExecutioner().exec(op);
        System.out.println(out);
    }
&lt;/denchmark-code&gt;

Output:
&lt;denchmark-code&gt;[[[[   20.0000,   24.0000], 
   [   32.0000,   18.0000], 
   [   48.0000,   56.0000]], 

  [[   30.0000,   30.0000], 
   [   34.0000,   18.0000], 
   [         0,         0]], 

  [[         0,         0], 
   [         0,         0], 
   [         0,         0]]]]
&lt;/denchmark-code&gt;

Note that with positive weights, and positive input (all &gt; 0 for both arrays) it is not possible to get value of 0 in the output.
I can calculate and provide expected results if required.
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-09-17T10:32:23Z'>
		Hi, Mr Alex,what's the libnd4j?
I know Nd4j,but where I find libnd4j?
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-09-17T10:38:39Z'>
		&lt;denchmark-link:https://github.com/Charele&gt;@Charele&lt;/denchmark-link&gt;
 From now on, please ask general questions like in the gitter channel, not on issues, unless it directly pertains to the issue itself.
Libnd4j is in the repository with everything else, i.e., here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/tree/master/libnd4j&gt;https://github.com/deeplearning4j/deeplearning4j/tree/master/libnd4j&lt;/denchmark-link&gt;
 - it's the c++ code that underlies ND4J and by extension DL4J
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-09-18T03:25:38Z'>
		Sorry, I know, I will care later.
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-09-18T08:25:49Z'>
		Hi, Alex
Actually you made very important catch. Thanks a lot! It turns out this problem is caused by bug in mmul_NxN helper (matrix multiplication for arrays with rank&gt;2). mmul_NxN gave wrong result when was dealing with permuted/reshaped C (result) array. Previous version of mmul_NxN was based on old TAD stuff which is buggy for unusual shapes like {...,1} / {1,...} that is with trailing/leading unity. I've rewritten mmul_NxN and now it is based on new NDArray stuff which however works on the same principle as TAD.
Changes are already merged into master
		</comment>
		<comment id='5' author='AlexDBlack' date='2018-09-18T09:42:26Z'>
		&lt;denchmark-link:https://github.com/shyrma&gt;@shyrma&lt;/denchmark-link&gt;
 Thanks! I can confirm that I'm getting sensible results for that test now 
		</comment>
		<comment id='6' author='AlexDBlack' date='2018-10-18T09:46:10Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>