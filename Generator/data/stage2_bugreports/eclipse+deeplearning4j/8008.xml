<bug id='8008' author='AlexDBlack' open_date='2019-07-12T04:09:07Z' closed_time='2019-08-28T03:28:13Z'>
	<summary>libnd4j: Layer norm only supports largest axis, backprop doesn't support 4d?</summary>
	<description>
&lt;denchmark-code&gt;java.lang.RuntimeException: Op [layer_norm] execution failed

	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1669)
	at org.nd4j.linalg.factory.Nd4j.exec(Nd4j.java:7431)
	at org.nd4j.Temp.testLayerNormNCHW(Temp.java:127)
Caused by: java.lang.RuntimeException: NDArray::applyTrueBroadcast method: the shapes of this and other arrays are not suitable for broadcast operation !
	at org.nd4j.nativeblas.Nd4jCpu$NativeOps.execCustomOp(Native Method)
	at
&lt;/denchmark-code&gt;

This broadcast will only work when gain/bias is the last value? i.e., [mb,h,w,c] + [c] works, but obviously [mb,c,h,w] + [c] doesn't... we need [mb,c,h,w]+[c,1,1] at least (or equivalently +[1,c,1,1])
Backprop looks off too for the 4D case:
&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/blob/master/libnd4j/include/ops/declarable/generic/transforms/layer_norm.cpp#L79&gt;https://github.com/eclipse/deeplearning4j/blob/master/libnd4j/include/ops/declarable/generic/transforms/layer_norm.cpp#L79&lt;/denchmark-link&gt;

Consider the [n,c,h,w] axis 1 case - we should be summing over axes [0,2,3] not just [0].
And I can't find any 4d test cases in libnd4j either.
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2019-08-27T08:37:29Z'>
		Layer norm 4d case forward pass confirmed working and correct for both NCHW and NHWC cases (java tests added here: &lt;denchmark-link:https://github.com/SkymindIO/deeplearning4j/pull/174&gt;SkymindIO#174&lt;/denchmark-link&gt;
)
However, backprop is throwing an exception for both NCHW and NHWC cases. Reproducible with the following test case:
&lt;denchmark-code&gt;    @Test
    public void testLayerNorm4d() {
        int mb = 3;
        int ch = 4;
        for(boolean nchw : new boolean[]{true, false}) {
            INDArray x = Nd4j.rand(DataType.FLOAT, nchw ? new long[]{mb, ch, 8, 8} : new long[]{mb, 8, 8, ch});
            INDArray gain = Nd4j.rand(DataType.FLOAT, ch);
            INDArray bias = Nd4j.rand(DataType.FLOAT, ch);
            INDArray gradAtOut = x.like();

            INDArray xGrad = x.like();
            INDArray gainGrad = gain.like();
            INDArray bGrad = bias.like();

            Nd4j.exec(DynamicCustomOp.builder("layer_norm_bp")
                    .addInputs(x, gain, bias, gradAtOut)
                    .addOutputs(xGrad, gainGrad, bGrad)
                    .addIntegerArguments(1, 2, 3)
                    .addBooleanArguments(nchw)
                    .build());
        }
    }
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;o.n.l.c.n.o.NativeOpExecutioner - Failed to execute op layer_norm_bp. Attempted to execute with 4 inputs, 3 outputs, 0 targs,1 bargs and 3 iargs. Inputs: [(FLOAT,[3,4,8,8],c), (FLOAT,[4],c), (FLOAT,[4],c), (FLOAT,[3,4,8,8],c)]. Outputs: [(FLOAT,[3,4,8,8],c), (FLOAT,[4],c), (FLOAT,[4],c)]. tArgs: -. iArgs: [1, 2, 3]. bArgs: [true]. Input var names: [input, gain, bias, layernorm-grad]. Output var names: [input-grad, gain-grad, bias-grad] - Please see above message (printed out from c++) for a possible cause of error.
java.lang.RuntimeException: Op [layer_norm_bp] execution failed
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1710)

&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='AlexDBlack' date='2019-08-28T03:28:13Z'>
		Confirmed fixed, including backprop and gradient checks.
		</comment>
	</comments>
</bug>