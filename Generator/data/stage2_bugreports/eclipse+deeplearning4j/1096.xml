<bug id='1096' author='lightyang' open_date='2016-01-31T02:27:16Z' closed_time='2016-02-18T05:30:03Z'>
	<summary>simple mlp approxmating sin function not working</summary>
	<description>
I'm trying to build a simple mlp with one hidden layer to approximate the sin function, but things don't work as expected and SGD always results in inf score
import org.canova.api.records.reader.RecordReader
import org.canova.api.records.reader.impl.CSVRecordReader
import org.canova.api.split.FileSplit
import org.deeplearning4j.datasets.canova.RecordReaderDataSetIterator
import org.deeplearning4j.datasets.iterator.DataSetIterator
import org.deeplearning4j.nn.api.OptimizationAlgorithm
import org.deeplearning4j.nn.conf.{ MultiLayerConfiguration, NeuralNetConfiguration }
import org.deeplearning4j.nn.conf.Updater
import org.deeplearning4j.nn.conf.layers._
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
import org.deeplearning4j.nn.weights.WeightInit
import org.deeplearning4j.optimize.listeners.ScoreIterationListener
import org.nd4j.linalg.api.ndarray.INDArray
import org.nd4j.linalg.api.ops.impl.transforms.Sin
import org.nd4j.linalg.dataset.DataSet
import org.nd4j.linalg.factory.Nd4j
import org.nd4j.linalg.lossfunctions.LossFunctions
import scalax.chart.api._
import org.nd4s.Implicits._
import org.nd4j.linalg.api.buffer._

object RegressionExample {
  def main(args: Array[String]) = {

    def plotXY(x:INDArray, y:INDArray):Unit = {
      val dataPlot = for(i &lt;- 0 to y.length()-1) yield (x.getFloat(i), y.getFloat(i))
      val chart = XYLineChart(dataPlot)
      chart.show()
    }


    Nd4j.ENFORCE_NUMERICAL_STABILITY = true
    Nd4j.dtype = DataBuffer.Type.DOUBLE
    val numInputs = 1
    val numOutputs = 1
    val numHiddenNodes = 20
    val nSamples = 1000
    val x = Nd4j.linspace(-10, 10, nSamples).reshape(nSamples, 1)
    val y = Nd4j.getExecutioner().execAndReturn(new Sin(x, x.dup()))
    val dataSet = new DataSet(x, y)

    plotXY(x, y)

    val seed = 1234
    val iterations = 1000

    val conf: MultiLayerConfiguration = new NeuralNetConfiguration.Builder().miniBatch(false)
      .updater(Updater.SGD)
      .seed(seed) // Seed to lock in weight initialization for tuning
      .iterations(iterations) // # training iterations predict/classify &amp; backprop
      .learningRate(1e-2) // Optimization step size
      .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Backprop method (calculate the gradients)
      .list(2) // # NN layers (does not count input layer)
      .layer(0,
        new DenseLayer.Builder()
          .nIn(1) // # input nodes
          .nOut(numHiddenNodes) // # output nodes
          .activation("tanh")
          .weightInit(WeightInit.XAVIER)
          .build()
      ) // NN layer type
      .layer(1,
        new OutputLayer.Builder(LossFunctions.LossFunction.MSE)
          .nIn(numHiddenNodes) // # input nodes
          .nOut(1) // # output nodes
          .activation("identity")
          .weightInit(WeightInit.XAVIER)
          .build()
      ) // NN layer type
      .backprop(true)
      .build()

    val network = new MultiLayerNetwork(conf)
    network.init()
    network.setListeners(new ScoreIterationListener(50))
    network.fit(dataSet)
    val yEst = network.output(x)

    plotXY(x, yEst)
  }
}
Console output
&lt;denchmark-code&gt;o.d.o.l.ScoreIterationListener - Score at iteration 0 is 424.5752258300781
o.d.o.l.ScoreIterationListener - Score at iteration 50 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 100 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 150 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 200 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 250 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 300 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 350 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 400 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 450 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 500 is 1.4553687329353663E31
o.d.o.l.ScoreIterationListener - Score at iteration 550 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 600 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 650 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 700 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 750 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 800 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 850 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 900 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 950 is Infinity
o.d.o.s.BaseOptimizer - Objective function automatically set to minimize. Set stepFunction in neural net configuration to change default settings.
o.d.o.l.ScoreIterationListener - Score at iteration 0 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 50 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 100 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 150 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 200 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 250 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 300 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 350 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 400 is 1.634341028562817E24
o.d.o.l.ScoreIterationListener - Score at iteration 450 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 500 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 550 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 600 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 650 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 700 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 750 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 800 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 850 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 900 is Infinity
o.d.o.l.ScoreIterationListener - Score at iteration 950 is Infinity
&lt;/denchmark-code&gt;

The same network conf in torch works fine:
X = torch.linspace(-10, 10, 1000)
Y = torch.sin(X)
dataset={};
function dataset:size() return 1000 end
for i=1,dataset:size() do
    dataset[i] = {torch.Tensor{X[i]}, torch.Tensor{Y[i]}}
end

require "nn"
mlp = nn.Sequential();  -- make a multi-layer perceptron
inputs = 1; outputs = 1; HUs = 20; -- parameters
mlp:add(nn.Linear(inputs, HUs))
mlp:add(nn.Tanh())
mlp:add(nn.Linear(HUs, outputs))

criterion = nn.MSECriterion()  
trainer = nn.StochasticGradient(mlp, criterion)
trainer.learningRate = 0.01
trainer:train(dataset)
I've tried other optimization algos too - no inf scores but the results are not as good as the torch one
	</description>
	<comments>
		<comment id='1' author='lightyang' date='2016-01-31T02:34:51Z'>
		and the torch output
&lt;denchmark-code&gt;# StochasticGradient: training  
Out[7]:
# current error = 0.46634580311461  
Out[7]:
# current error = 0.39428116266875  
Out[7]:
# current error = 0.36005339778272  
Out[7]:
# current error = 0.33949976149945  
Out[7]:
# current error = 0.32309306285498  
Out[7]:
# current error = 0.30747405061414  
Out[7]:
# current error = 0.29178479922231  
Out[7]:
# current error = 0.27604582765425  
Out[7]:
# current error = 0.26046024375264  
Out[7]:
# current error = 0.24523474783723  
Out[7]:
# current error = 0.2306465653372   
Out[7]:
# current error = 0.21694113526093  
Out[7]:
# current error = 0.20423987699745  
Out[7]:
# current error = 0.19256480042189  
Out[7]:
# current error = 0.18183779676746  
Out[7]:
# current error = 0.17202432139559  
Out[7]:
# current error = 0.16322178718021  
Out[7]:
# current error = 0.15552944921552  
Out[7]:
# current error = 0.14893228333828  
Out[7]:
# current error = 0.14332199469 
Out[7]:
# current error = 0.13856329642451  
Out[7]:
# current error = 0.13453371716075  
Out[7]:
# current error = 0.13115757443581  
Out[7]:
# current error = 0.12861451498631  
Out[7]:
# current error = 0.12767352807061  
# StochasticGradient: you have reached the maximum number of iterations 
# training error = 0.12767352807061 
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/1108113/12699838/25d3c9a0-c806-11e5-8084-1ef1bce06105.png&gt;&lt;/denchmark-link&gt;

And the result could be improved if it's trained harder
		</comment>
		<comment id='2' author='lightyang' date='2016-02-14T17:00:02Z'>
		Working on that.
		</comment>
		<comment id='3' author='lightyang' date='2016-02-14T21:53:12Z'>
		&lt;denchmark-link:https://github.com/lightyang&gt;@lightyang&lt;/denchmark-link&gt;
 after analyse I found that
&lt;denchmark-code&gt;            case MSE: // mean squared error
                INDArray delta = outSubLabels.mul(derivativeActivation(preOut));
                gradient.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY, input.transpose().mmul(delta));  
&lt;/denchmark-code&gt;

so in your case delta is equal to outSubLabels due to the fact that you set
.activation("identity") in 1 layer. Actually what derivativeActivation do is computing derivative from  identity function. Of course if f(x) = x, then f'(x) = 1. So derivativeActivation returns only ones. outSubLabels are greater than 1 and repeating this iteration causes infinite growth of model matrix. I am wondering if this is bug of your test or deeplearning4j platform.
		</comment>
		<comment id='4' author='lightyang' date='2016-02-14T22:13:13Z'>
		&lt;denchmark-link:https://github.com/Zixxy&gt;@Zixxy&lt;/denchmark-link&gt;
 thanks. This is helpful actually. Here is the identity op itself:
&lt;denchmark-link:https://github.com/deeplearning4j/nd4j/blob/master/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/Identity.java&gt;https://github.com/deeplearning4j/nd4j/blob/master/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/Identity.java&lt;/denchmark-link&gt;

.deriviative() isn't overriden it just returns ones for the base op.
&lt;denchmark-link:https://proofwiki.org/wiki/Derivative_of_Identity_Function&gt;https://proofwiki.org/wiki/Derivative_of_Identity_Function&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='lightyang' date='2016-02-18T05:30:03Z'>
		Have a look at this example, I'm getting sensible results doing regression on a sin(x) function (and others)
&lt;denchmark-link:https://github.com/deeplearning4j/dl4j-0.4-examples/blob/master/src/main/java/org/deeplearning4j/examples/regression/RegressionMathFunctions.java&gt;https://github.com/deeplearning4j/dl4j-0.4-examples/blob/master/src/main/java/org/deeplearning4j/examples/regression/RegressionMathFunctions.java&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='lightyang' date='2019-01-21T10:37:30Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>