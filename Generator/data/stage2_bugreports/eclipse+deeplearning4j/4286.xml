<bug id='4286' author='papperwing' open_date='2017-11-12T10:00:55Z' closed_time='2017-11-29T03:26:48Z'>
	<summary>Score is different for same parameter in different version (0.9.1 vs master)</summary>
	<description>
After update I have started to get weird score for my NN so I tested if same behavior is on totaly same parameters on 0.9.1 and here are graphs:
master:
&lt;denchmark-link:https://user-images.githubusercontent.com/5627436/32697779-89f5a14a-c798-11e7-8d09-b8fe815a9265.png&gt;&lt;/denchmark-link&gt;

0.9.1:
&lt;denchmark-link:https://user-images.githubusercontent.com/5627436/32697781-91266eb8-c798-11e7-97e6-9cf41acaa44b.png&gt;&lt;/denchmark-link&gt;

Recently there have been change in computing l1/l2 in frozen layer could that affect whole computing of gradient or propagation of updates?
	</description>
	<comments>
		<comment id='1' author='papperwing' date='2017-11-13T01:33:48Z'>
		Certainly, the l1/l2 fix for compgraph + frozen layers did affect the gradients/updates.
&lt;denchmark-link:https://github.com/papperwing&gt;@papperwing&lt;/denchmark-link&gt;
 I assume I can pull the latest version here to reproduce this?
&lt;denchmark-link:https://github.com/papperwing/image-net&gt;https://github.com/papperwing/image-net&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='papperwing' date='2017-11-13T08:03:49Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 Yes You just have to change dl4j.version property in top-most pom.xml and in ModelBuilderImpl you must change way the learning rate is set when changing version to 0.9.1. Nothing else is necessary to change.
		</comment>
		<comment id='3' author='papperwing' date='2017-11-13T14:58:33Z'>
		I tried to run learning without freezing layers and it still behaves differently. So it will not be caused by frozen layer, because they shouldn't be there.
&lt;denchmark-link:https://user-images.githubusercontent.com/5627436/32731933-6a057e7a-c88b-11e7-8980-e4d5343e5903.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='papperwing' date='2017-11-15T01:07:14Z'>
		I can confirm that there is definitely something going on here... left is 0.9.1, right is master (run on Linux)
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/32813274-6198e722-c9fd-11e7-87ed-ed4b35124331.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='papperwing' date='2017-11-15T06:00:51Z'>
		Related, and possibly the cause: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/4302&gt;https://github.com/deeplearning4j/deeplearning4j/issues/4302&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='papperwing' date='2017-11-23T10:12:26Z'>
		Update: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/4302&gt;https://github.com/deeplearning4j/deeplearning4j/issues/4302&lt;/denchmark-link&gt;
 is fixed, but unfortunately this does not appear to be the cause :(
I'm still seeing a difference between 0.9.1 and master here.
		</comment>
		<comment id='7' author='papperwing' date='2017-11-23T20:49:55Z'>
		Is difference reproducible for both cpu &amp; cuda?
		</comment>
		<comment id='8' author='papperwing' date='2017-11-23T20:50:03Z'>
		and cuda with cudnn?
		</comment>
		<comment id='9' author='papperwing' date='2017-11-23T23:09:32Z'>
		I've only been testing with CPU currently. I spent a bunch of time last night looking into this, haven't isolated it yet. I'm focused on this "nearly flat score" issue I'm seeing currently.
It's not a  workspace/scope issue as far as I can tell. Params and updater state looks fine. Scores and activations look about the same for both.
But if I load the 0.9.1 net into master, I get quite different results during training.
		</comment>
		<comment id='10' author='papperwing' date='2017-11-24T05:47:37Z'>
		Try CUDA with cuDNN please.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 24 нояб. 2017 г., в 02:09, Alex Black ***@***.***&gt; написал(а):

 I've only been testing with CPU currently. I spent a bunch of time last night looking into this, haven't isolated it yet. I'm focused on this "nearly flat score" issue I'm seeing currently.

 It's not a workspace/scope issue as far as I can tell. Params and updater state looks fine. Scores and activations look about the same for both.
 But if I load the 0.9.1 net into master, I get quite different results during training.

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub &lt;https://github.com/deeplearning4j/deeplearning4j/issues/4286#issuecomment-346707747&gt;, or mute the thread &lt;https://github.com/notifications/unsubscribe-auth/ALru__rVCXaJvH-BGV6pAqE3HR6WaRJeks5s5fs5gaJpZM4Qa0uX&gt;.



		</comment>
		<comment id='11' author='papperwing' date='2017-11-27T07:59:38Z'>
		Success - at least for the flat score issue...
Turns out that was dropout(0) actually appling dropout with 0 retain probability.
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4326&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4326&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/commit/7e03ccfe54b95a8a9a3987c40e1b7f74b0c1f6f9&gt;deeplearning4j@7e03ccf&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/commit/bcc7f1ad7bd3fd7c5951f14cf0d1ccb81787f1a2&gt;deeplearning4j@bcc7f1a&lt;/denchmark-link&gt;

I'll run this longer to see if it behaves sensibly, before closing.
		</comment>
		<comment id='12' author='papperwing' date='2017-11-27T08:21:57Z'>
		O_o
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 27 нояб. 2017 г., в 10:59, Alex Black ***@***.***&gt; написал(а):

 Success - at least for the flat score issue...
 Turns out that was dropout(0) actually appling dropout with 0 retain probability.
 #4326 &lt;https://github.com/deeplearning4j/deeplearning4j/pull/4326&gt;
 7e03ccf &lt;deeplearning4j@7e03ccf&gt;
 bcc7f1a &lt;deeplearning4j@bcc7f1a&gt;
 I'll run this longer to see if it behaves sensibly, before closing.

 —
 You are receiving this because you commented.
 Reply to this email directly, view it on GitHub &lt;https://github.com/deeplearning4j/deeplearning4j/issues/4286#issuecomment-347104716&gt;, or mute the thread &lt;https://github.com/notifications/unsubscribe-auth/ALru_3X97gQyATOzRQyqwhqAau5nJGLWks5s6mv7gaJpZM4Qa0uX&gt;.



		</comment>
		<comment id='13' author='papperwing' date='2017-11-27T09:34:51Z'>
		Yeah, definitely seems to be learning properly now...
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/33259864-528a935c-d3b2-11e7-92f9-4edd6f93129b.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='papperwing' date='2017-11-29T03:26:48Z'>
		&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4326&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4326&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='papperwing' date='2018-09-24T06:43:46Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>