<bug id='7977' author='danielschonfeld' open_date='2019-07-05T00:19:45Z' closed_time='2019-07-18T08:54:07Z'>
	<summary>params(false) does indeed create a new/copy</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Documentation error/typo
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Latest
Please indicate relevant versions, including, if relevant:
DL4J
&lt;denchmark-h:h4&gt;Additional Information&lt;/denchmark-h&gt;

params(boolean) method states that if backwardsOnly=false, you do NOT get a copy, and changes to the given variable will change the underlying network.  The code is opposite of that, in fact if you pass false, you get a fresh copy unrelated to the pointer inside the original network it was created in.
&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

I'll be happy to submit a PR if you'll tell me what you'd like done
	</description>
	<comments>
		<comment id='1' author='danielschonfeld' date='2019-07-05T00:55:23Z'>
		This impacts both MultiLayerNetwork and ComputationGraph.
The javadoc does indeed not match the behaviour.
Thinking about this, I'm thinking we should simply remove that params(boolean) method entirely. I don't think it's particularly useful at this point. The no-args params() method is useful, and the paramTable() method is also useful. For most nets, params() and params(boolean) will be the same. For nets with autoencoder, VAE etc layers, the params(boolean) method is supposed to cut out the decoder weights/bias - but there's almost nothing you can actually use that vector for in practice (maybe transfer learning, but it's safer and less error prone to use paramTable() I think).
		</comment>
	</comments>
</bug>