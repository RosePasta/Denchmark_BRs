<bug id='8002' author='rnett' open_date='2019-07-10T22:29:57Z' closed_time='2019-07-12T02:36:00Z'>
	<summary>Libnd4j assign error for XLNet</summary>
	<description>
When running XLNet import test, I get:
&lt;denchmark-code&gt;Can't assign new value to the array: this shape [1024]; other shape: [-32000, 1024]
terminate called after throwing an instance of 'std::runtime_error'
  what():  NDArray::assign: lengths of arrays are mismatched
&lt;/denchmark-code&gt;

To reproduce, run org.nd4j.imports.TFGraphs.TFGraphTestZooModels and comment out "xlnet_cased_L-24_H-1024_A-16" from the ignores.
Make sure you have the latest version of the test resources.
	</description>
	<comments>
		<comment id='1' author='rnett' date='2019-07-11T01:24:20Z'>
		Could we isolate this a bit more?
Ideally to the op + the inputs. A standalone test case (using DynamicCustomOp) would be even better, as that can be translated in to a c++ test for easier/faster debugging and to prevent future regressions.
		</comment>
		<comment id='2' author='rnett' date='2019-07-11T01:57:35Z'>
		Will do.  Errors like this don't produce a stack trace, it would be good to add that at some point.
		</comment>
		<comment id='3' author='rnett' date='2019-07-11T02:02:24Z'>
		Some errors like this will produce a good stack trace - it really depends on the op type (custom ops are good, legacy ops not so much). Standardizing this would be great too...



deeplearning4j/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-native/src/main/java/org/nd4j/linalg/cpu/nativecpu/ops/NativeOpExecutioner.java


        Lines 2079 to 2135
      in
      1170827






 } catch (Exception e) { 



     val sb = new StringBuilder(); 



     sb.append("Inputs: [("); 



 int nIn = (context.getInputArrays() == null ? 0 : context.getInputArrays().size()); 



 for (int i = 0; i &lt; nIn; i++) { 



 if (i &gt; 0) 



             sb.append("), ("); 



         sb.append(Shape.shapeToStringShort(context.getInputArrays().get(i))); 



     } 



     sb.append(")]. Outputs: [("); 



 int nOut = (context.getOutputArrays() == null ? 0 : context.getOutputArrays().size()); 



 for (int i = 0; i &lt; nOut; i++) { 



 if (i &gt; 0) 



             sb.append("), ("); 



         sb.append(Shape.shapeToStringShort(context.getOutputArrays().get(i))); 



     } 



     sb.append(")]. tArgs: "); 



 int nT = (context.getTArguments() == null ? 0 : context.getTArguments().size()); 



 if (nT &gt; 0) { 



         sb.append(context.getTArguments()); 



     } else { 



         sb.append("-"); 



     } 



     sb.append(". iArgs: "); 



 int nI = (context.getIArguments() == null ? 0 : context.getIArguments().size()); 



 if (nI &gt; 0) { 



         sb.append(context.getIArguments()); 



     } else { 



         sb.append("-"); 



     } 



     sb.append(". bArgs: "); 



 int nB = (context.getBArguments() == null ? 0 : context.getBArguments().size()); 



 if (nB &gt; 0) { 



         sb.append(context.getBArguments()); 



     } else { 



         sb.append("-"); 



     } 



 if (op instanceof DifferentialFunction) { 



 String n = ((DifferentialFunction) op).getOwnName(); 



 if (n != null &amp;&amp; !n.equals(op.opName())) { 



             sb.append(". Op own name: \"").append(n).append("\""); 



         } 



     } 



 



 if(op instanceof DifferentialFunction &amp;&amp; ((DifferentialFunction)op).getSameDiff() != null){ 



         appendSameDiffInfo(sb, (DifferentialFunction) op); 



     } 



 



     log.error("Failed to execute op " + op.opName() + ". Attempted to execute with " + 



             nIn + " inputs, " + 



             nOut + " outputs, " + 



             nT + " targs," + 



             nB + " bargs and " + 



             nI + " iargs. " + 



             sb.toString() + 



 " - Please see above message (printed out from c++) for a possible cause of error."); 



 throw e; 





		</comment>
		<comment id='4' author='rnett' date='2019-07-11T02:45:28Z'>
		To reproduce:
@Test
public void testGather(){

    INDArray out = Nd4j.create(DataType.FLOAT, 128, 8, 1024);
    DynamicCustomOp op = new Gather();

    INDArray a = Nd4j.rand(DataType.FLOAT, 32000, 1024);//.sub(0.5).mul(2);

    INDArray b = Nd4j.rand(DataType.FLOAT, 128, 8).sub(0.5).mul(20).castTo(DataType.INT);

    op.addInputArgument(
            a,
            b,
            Nd4j.scalar(0f)
    );
    op.addIArgument(0);
    op.addOutputArgument(out);

    Nd4j.getExecutioner().exec(op);
}
Note that the .sub(0.5).mul(20).castTo(DataType.INT) on b is necessary, it works otherwise (although all indexes are 0).
		</comment>
		<comment id='5' author='rnett' date='2019-07-11T02:49:53Z'>
		Indexes are negative here, as they are in the graph, and it is possible that that is what is causing the error.
		</comment>
		<comment id='6' author='rnett' date='2019-07-11T03:03:01Z'>
		
Indexes are negative here, as they are in the graph

Yeah, my guess is that the real problem is earlier in the graph. Gather doesn't support negative indices:

Index tensor. Must be in range [0, params.shape[axis]).

&lt;denchmark-link:https://www.tensorflow.org/api_docs/python/tf/gather&gt;https://www.tensorflow.org/api_docs/python/tf/gather&lt;/denchmark-link&gt;

Our gather works the same way.
		</comment>
		<comment id='7' author='rnett' date='2019-07-12T00:48:47Z'>
		So it looks like it was due to having random inputs to XLNet instead of the index/mask setup you're supposed to use.  This has been fixed in test-resources and I now get OOM, so this looks to be fixed.
It would be nice to have some kind of assertion on the Libnd4j side that the indecies are positive, rather than erroring on an assign.
		</comment>
		<comment id='8' author='rnett' date='2019-07-12T00:59:39Z'>
		I think the reason no stack trace was provided is because c++ terminate was called, nothing was ever passed back up to Java.  Gather is a CustomOp, so it should have produced a stack trace w/ the code you linked earlier.  Seems like it needs some parameter validation.
		</comment>
		<comment id='9' author='rnett' date='2019-07-12T02:35:59Z'>
		
It would be nice to have some kind of assertion on the Libnd4j side that the indecies are positive, rather than erroring on an assign.

Agreed.
&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/8007&gt;#8007&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>