<bug id='1593' author='mstritt' open_date='2016-05-25T07:31:11Z' closed_time='2016-06-04T14:59:12Z'>
	<summary>JVM Crash (openlibblas/win64)</summary>
	<description>
rc3.9 / 0.4-rc3.9:windows-x86_64
when performing many output predictions JVM crashes in openlibblas:
&lt;denchmark-code&gt;Problem signature:
  Problem Event Name:   APPCRASH
  Application Name: java.exe
  Application Version:  7.0.400.43
  Application Timestamp:    521c3d55
  Fault Module Name:    libopenblas.dll
  Fault Module Version: 0.0.0.0
  Fault Module Timestamp:   a318a310
  Exception Code:   c0000005
  Exception Offset: 00000000010e33e3
  OS Version:   6.3.9600.2.0.0.256.48
  Locale ID:    1033

&lt;/denchmark-code&gt;

the problem is reproducable, see attached program.
When setting numClassifications to a low number, e.g. 1000 it works. For 10000 not.
When uncommenting the system.gc() in the classification method it works as well.
&lt;denchmark-code&gt;import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.ConvolutionLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.SubsamplingLayer;
import org.deeplearning4j.nn.conf.layers.setup.ConvolutionLayerSetup;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions;

public class ClassBug {

    private final int iterations = 10;
    private final int outputnum = 2;
    private final int dim = 30000;
    private final int seed = 124;
    private final int numClassifications = 10000;

    private MultiLayerNetwork model =   buildModelCNN3(iterations,outputnum,200,150,1,seed);

    public void trainModel() {
        int trainSize = 100;
        INDArray trainSet = Nd4j.create(trainSize,dim);
        int[] labels = new int[trainSize];
        for (int i=0; i&lt;trainSize; i++) {
            int clazz = i&lt;trainSize/2?0:1;
            INDArray row = Nd4j.create(1,dim);
            for (int j=0; j&lt;dim; j++) {
                row.putScalar(j,clazz);
            }
            trainSet.putRow(i,row);
            labels[i] = clazz;
        }

        model.init();
        model.setListeners(new ScoreIterationListener(2));
        model.fit(trainSet,labels);
    }


    public void classify() {
         for (int i=0; i&lt;numClassifications; i++) {
             INDArray row = Nd4j.create(1,dim);
             for (int j=0; j&lt;dim; j++) {
                 row.putScalar(j,0);
             }
             INDArray outcome = model.output(row);
             //System.gc();
         }
    }

    private MultiLayerNetworkMS buildModelCNN3(int iterations, int outputNum, int imgWidth, int imgHeight, int nChannels, int seed) {
        MultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .iterations(iterations)
                .regularization(true)
                .l2(0.0005)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .list(3)
                .layer(0, new ConvolutionLayer.Builder(5, 5)
                        .stride(2,2)
                        .nIn(nChannels)
                        .nOut(6)
                        .weightInit(WeightInit.DISTRIBUTION)
                        .activation("relu").name("layer0")
                        .build())
                .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, new int[] {2,2})
                        .name("layer1").build())
                .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                        .nOut(outputNum)
                        .weightInit(WeightInit.XAVIER)
                        .activation("softmax").name("layer2")
                        .build())
                .backprop(true).pretrain(false);

        new ConvolutionLayerSetup(builder,imgHeight,imgWidth,nChannels);
        MultiLayerConfiguration conf = builder.build();
        MultiLayerNetworkMS model = new MultiLayerNetworkMS(conf);
        return model;
    }

    public static void main(String[] args) {
        ClassBug cb = new ClassBug();
        cb.trainModel();
        cb.classify();
    }

}




&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='mstritt' date='2016-05-27T11:35:07Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 was this related to openblas?
		</comment>
		<comment id='2' author='mstritt' date='2016-06-04T14:59:11Z'>
		This is likely fixed with &lt;denchmark-link:https://github.com/saudet&gt;@saudet&lt;/denchmark-link&gt;
 recent changes that were affected by GC. Closing this. Let us know if it happens again in the next release.
		</comment>
		<comment id='3' author='mstritt' date='2019-01-21T01:52:40Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>