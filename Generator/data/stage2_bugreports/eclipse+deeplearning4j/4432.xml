<bug id='4432' author='mboyanov' open_date='2017-12-18T16:23:33Z' closed_time='2017-12-19T07:27:17Z'>
	<summary>Keras model import with LSTM return_sequences=False output type is wrong</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I am importing a Keras LSTM where return_sequences = False. On master, this is currently supported by wrapping the LSTM in a LastTimeStep. However, calling the getOutputType method is wrong as it returns InputTypeRNN regardless of the value for the returnSequences field.
GIVEN:
LSTM layer with return_sequences = False
WHEN:
calling getOutputType
THEN:
should return InputTypeFeedForward
ACTUAL:
return InputTypeRNN
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version - 0.9.2-SNAPSHOT

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

I have patched this in &lt;denchmark-link:https://github.com/mboyanov/deeplearning4j/commit/3b6fff721c2894bcfb22a18d9e4617a4c26812ac&gt;mboyanov@3b6fff7&lt;/denchmark-link&gt;
 , but I have no idea how to test it against the whole dl4j stack.
	</description>
	<comments>
		<comment id='1' author='mboyanov' date='2017-12-18T19:39:07Z'>
		&lt;denchmark-link:https://github.com/mboyanov&gt;@mboyanov&lt;/denchmark-link&gt;
 if I augment the test that you also touched like this:
&lt;denchmark-code&gt;        LSTM layer;
        LastTimeStep lts;
        if (rs) {
            KerasLstm kerasLstm = new KerasLstm(layerConfig);
            InputType outputType = kerasLstm.getOutputType(InputType.recurrent(1337));
            assertEquals(outputType, InputType.recurrent(N_OUT));
            layer = (LSTM) kerasLstm.getLSTMLayer();
        } else {
            KerasLstm kerasLts = new KerasLstm(layerConfig);
            lts = (LastTimeStep) kerasLts.getLSTMLayer();
            InputType outputType = kerasLts.getOutputType(InputType.recurrent(1337));
            assertEquals(outputType, InputType.feedForward(N_OUT));
            layer = (LSTM) lts.getUnderlying();
        }
&lt;/denchmark-code&gt;

it absolutely does work. Are you saying that if I add a preprocessor, it will fail? Can you please share how you test this?
		</comment>
		<comment id='2' author='mboyanov' date='2017-12-18T19:40:06Z'>
		btw, in your code you should check if the preprocessor is null:
&lt;denchmark-code&gt;        if (!returnSequences &amp;&amp; preProcessor != null) {
            return this.getLSTMLayer().getOutputType(-1, preProcessor.getOutputType(inputType[0]));
        }
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='mboyanov' date='2017-12-18T19:50:38Z'>
		My use case is that I'm importing a model where I'm concatenating the last time step with another two feedforward layers. Then, the merge vertex throws an exception, because you can't merge vertices with different output types.
The test does look better, I'll modify it and start a pull request tomorrow.
		</comment>
		<comment id='4' author='mboyanov' date='2017-12-19T06:53:18Z'>
		Started a pull request &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4434&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4434&lt;/denchmark-link&gt;
 .
		</comment>
		<comment id='5' author='mboyanov' date='2017-12-19T07:27:21Z'>
		thanks
		</comment>
		<comment id='6' author='mboyanov' date='2018-09-23T21:26:16Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>