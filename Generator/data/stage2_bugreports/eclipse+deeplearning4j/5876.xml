<bug id='5876' author='AlexDBlack' open_date='2018-07-11T05:44:21Z' closed_time='2018-07-11T08:33:21Z'>
	<summary>DL4J: Spark training deadlock (with OMP_NUM_THREAD=1 only?)</summary>
	<description>
Part of recent spark benchmarks/testing:
On a 16 core machine, if I configure 8 workers with OMP 1, 1344 minibatches of size 8, openblas, we end up completing 119 batches, then nothing...
&lt;denchmark-code&gt;2018-07-11 05:31:21 INFO  TaskSetManager:54 - Finished task 123.0 in stage 7.0 (TID 792) in 7836 ms on 10.0.2.7 (executor 3) (113/168)
2018-07-11 05:31:21 INFO  TaskSetManager:54 - Finished task 116.0 in stage 7.0 (TID 790) in 8051 ms on 10.0.2.11 (executor 4) (114/168)
2018-07-11 05:31:21 INFO  TaskSetManager:54 - Finished task 107.0 in stage 7.0 (TID 796) in 7653 ms on 10.0.2.5 (executor 5) (115/168)
2018-07-11 05:31:21 INFO  TaskSetManager:54 - Finished task 120.0 in stage 7.0 (TID 799) in 7542 ms on 10.0.2.10 (executor 2) (116/168)
2018-07-11 05:31:21 INFO  TaskSetManager:54 - Finished task 125.0 in stage 7.0 (TID 801) in 7634 ms on 10.0.2.9 (executor 0) (117/168)
2018-07-11 05:31:21 INFO  TaskSetManager:54 - Finished task 122.0 in stage 7.0 (TID 788) in 8564 ms on 10.0.2.8 (executor 1) (118/168)
2018-07-11 05:31:22 INFO  TaskSetManager:54 - Finished task 117.0 in stage 7.0 (TID 802) in 7768 ms on 10.0.2.5 (executor 5) (119/168)
&lt;/denchmark-code&gt;

It's consistent - 4 runs, 4 failures on exactly batch 119.
On first run I left it for 40 minutes, with no progress (full run should take ~5 min)
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-07-11T05:49:55Z'>
		Worker log: Looks like OOM not being handled here correctly is the culprit...
&lt;denchmark-link:https://gist.github.com/AlexDBlack/a0039ac988a660cd8e67c4a01eef7af1&gt;https://gist.github.com/AlexDBlack/a0039ac988a660cd8e67c4a01eef7af1&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='AlexDBlack' date='2018-09-21T15:59:04Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>