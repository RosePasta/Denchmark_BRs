<bug id='8165' author='cqiaoYc' open_date='2019-08-29T01:23:24Z' closed_time='2019-09-16T07:31:54Z'>
	<summary>A Probable Memory Leakage Problem</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

When my application (GA + LSTM) started, it took 2.5 minutes per 10 generations, but after 1800, it took 8 minutes. No improvement, when I removed the set up:
&lt;denchmark-code&gt;Nd4j.getMemoryManager().togglePeriodicGc(true);
Nd4j.getMemoryManager().setAutoGcWindow(500);
&lt;/denchmark-code&gt;

the result of jmap:
&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63902940-4e2f0c00-ca3e-11e9-914e-0b3405af68c9.JPG&gt;&lt;/denchmark-link&gt;

ConcurrentHashMap is not used in my app.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version : 1.0.0-beta4
Platform information (OS, etc) : centos 7.6
CUDA version, 10.0

&lt;denchmark-h:h4&gt;Additional Information&lt;/denchmark-h&gt;

Where applicable, please also provide:

Full log or exception stack trace (ideally in a Gist: gist.github.com)
pom.xml file or similar (also in a Gist)

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

If you'd like to help us fix the issue by contributing some code, but would
like guidance or help in doing so, please mention it!
	</description>
	<comments>
		<comment id='1' author='cqiaoYc' date='2019-08-29T01:32:17Z'>
		Given that number of objects, it's probably garbage collection overhead.
Is there anything special about your app, in terms of multi-threading, storing a bunch of different networks/arrays in memory, or anything like that?
Edit: also try snapshots, there have been some changes to how memory is managed, for CUDA.
		</comment>
		<comment id='2' author='cqiaoYc' date='2019-08-29T03:08:53Z'>
		Yes, there are 6 gpus with 6-threads in my app, storing about 2000 network params, and other arrays, but one network per a thread.
about snapshots, there is not cuda- linux-x86_64.jar, so can not be used.
I am currently using ubyte type ndarray with the shape of [16, netParams] to simulate bit operations of uint16 ndarray with the shape of [1, netParams] . If there are broadcasting bit operations, the performance of my app can be greatly improved. I desperately need them. I hope they are in beta 5.
the bit ops as :&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/8119&gt;#8119&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='cqiaoYc' date='2019-08-29T03:50:31Z'>
		Without the setup:
&lt;denchmark-code&gt;Nd4j.getMemoryManager().togglePeriodicGc(true);
Nd4j.getMemoryManager().setAutoGcWindow(500);
&lt;/denchmark-code&gt;

the Processe may be killed ,and no other prompts.
		</comment>
		<comment id='4' author='cqiaoYc' date='2019-08-29T04:43:44Z'>
		I highly doubt there's a leak. Show your source code please.
		</comment>
		<comment id='5' author='cqiaoYc' date='2019-08-29T05:43:52Z'>
		My source is too long. From the result of  jmap op, the classes that occupy a lot of memory are not written by me, and  the container class with large memory usage is not directly used by me. Leakages are most likely caused by improper use of container classes.
		</comment>
		<comment id='6' author='cqiaoYc' date='2019-08-29T05:46:25Z'>
		Huh... That doesn't help. You're seeing symptom here, and making assumptions based on class names. That's not how it works. Leaks in java are only possible when you keep references to objects, preventing GC from collecting garbage.
		</comment>
		<comment id='7' author='cqiaoYc' date='2019-08-29T12:10:13Z'>
		I will add that another way to debug this sort of leak is to use memory dump in tools such as yourkit java profiler. That should show you the classes that are holding the references to all of those objects.
If we had a screenshot of that, it would help us isolate this.
		</comment>
		<comment id='8' author='cqiaoYc' date='2019-08-29T12:11:28Z'>
		yourkit dump would work, yea
		</comment>
		<comment id='9' author='cqiaoYc' date='2019-08-30T02:04:55Z'>
		yourkit screenshot:
&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63988086-31153e80-cb0d-11e9-831c-c51bbbbcf2e1.JPG&gt;&lt;/denchmark-link&gt;

AbstractIndividual objects are stored in object pools (one pool per thread). They have big ndarrays (shape=[16,netParams]).
the code to type converting:
&lt;denchmark-code&gt;    INDArray netParamArray = allNetParamParts.castTo(DataType.INT).mul(getPartWeights(getGpuId()));
        netParams = netParamArray.sum(0);  //convert [16,netParams] ubyte values to [1,netParams] uint16
        
        netParams = netParams.castTo(DataType.FLOAT);        
        netParams.muli(paramsScaleFactor).subi(maxWeight);
        netParams = netParams.castTo(DataType.HALF);
&lt;/denchmark-code&gt;

If using bit ops, the first two lines of code can be optimized. the after code map int type values to HALF values.
		</comment>
		<comment id='10' author='cqiaoYc' date='2019-08-30T02:15:35Z'>
		Can you provide the actual dump file?
That screenshot you've provided only shows a small fraction of what we need here...
		</comment>
		<comment id='11' author='cqiaoYc' date='2019-08-30T02:21:30Z'>
		the size of  .snapshot file is 5.7G. How to send it to you? Have you a  QQ account?
		</comment>
		<comment id='12' author='cqiaoYc' date='2019-08-30T02:41:51Z'>
		Hm... in that case, maybe provide screenshots (of the paths) like you did earlier, for the following object types:

ConcurrentHashMap$Node
AllocationPoint
CudaPointer
CudaFloatDataBuffer
FloatPointer
String

		</comment>
		<comment id='13' author='cqiaoYc' date='2019-08-30T03:05:35Z'>
		screenshots:
&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63990266-fc59b500-cb15-11e9-9054-5ed2fb6680b8.JPG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63990273-01b6ff80-cb16-11e9-814a-2e08d7c3f2fe.JPG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63990283-07144a00-cb16-11e9-98ef-053aecb73e35.JPG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63990295-0c719480-cb16-11e9-976c-c7d3c55b33df.JPG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63990300-109db200-cb16-11e9-84f7-80f65c3c2036.JPG&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63990307-14313900-cb16-11e9-877d-9472941373f1.JPG&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='cqiaoYc' date='2019-08-30T05:11:07Z'>
		So, to be clear:
With periodicGc enabled - application doesn't go oom, just slows down.
Without periodicGc enabled - you see this in heap.
Am i getting this properly?
		</comment>
		<comment id='15' author='cqiaoYc' date='2019-08-30T05:36:18Z'>
		No, this is with periodicGc enabled. Without periodicGc enabled, app may be killed soon after starting.
		</comment>
		<comment id='16' author='cqiaoYc' date='2019-08-30T05:47:51Z'>
		
With periodicGc enabled - application doesn't go oom

?
		</comment>
		<comment id='17' author='cqiaoYc' date='2019-08-30T05:50:42Z'>
		Yes, the setup:
Nd4j.getMemoryManager().togglePeriodicGc(true);
Nd4j.getMemoryManager().setAutoGcWindow(600);
app can run more 2days, just slows down.
		</comment>
		<comment id='18' author='cqiaoYc' date='2019-08-30T05:51:44Z'>
		Looking at these screenshots and going by your description, we're about 95% sure that this is leaked references in your code.
i.e., you have some map, list, array, etc that's storing a bunch of INDArrays, that can't be garbage collected.
That's the only possible explanation here.
We can't say much beyond that without seeing actual code or having some way to run/reproduce this.
		</comment>
		<comment id='19' author='cqiaoYc' date='2019-08-30T06:05:20Z'>
		a bunch of INDArrays are stored in object pool (ConcurrentLinkedQueue), as:
&lt;denchmark-link:https://user-images.githubusercontent.com/29296200/63996777-2f5c7280-cb2f-11e9-9751-1c4d36cf4a9d.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='cqiaoYc' date='2019-08-30T23:04:56Z'>
		Previous single GPU versions had no such problems. There are two suspicious methods.
One is copy ndarray from other thread:
&lt;denchmark-code&gt; public INDArray copyNetParamPartsFromOtherGPU() {
         return Nd4j.createFromArray(allNetParamParts.data().asInt()).castTo(DataType.UBYTE);
}
&lt;/denchmark-code&gt;

Must change allNetParamParts.data().asInt() to allNetParamParts.data().dup().asInt()? or Arrays.copyOf(allNetParamParts.data().asInt(), netParamsCount)?
[It is more efficient: Nd4j.create(allNetParamParts.data().dup()). Is dup () necessary?]
There is a little problem: DataBuffer.array() return the backing array, and not the buffer itself. Suggestions for providing a method: DataBuffer.as(DataType dataType).
Another is sharing data among GPUs:
&lt;denchmark-code&gt;private Map&lt;Integer, INDArray&gt; features3D = new HashMap&lt;&gt;();
public INDArray getFeatures3D(int GpuId) {
        INDArray features3DArray = features3D.get(GpuId);
        if (features3DArray == null) {
            synchronized (features3D) {
                features3DArray = features3D.get(GpuId);
                if (features3DArray == null) {
                    build(GpuId, dataSets);
                    features3DArray = features3D.get(GpuId);
                }
            }
        }
        return features3DArray;
    }
&lt;/denchmark-code&gt;

Here GpuId is same as threadId. this object is used as a static object.
		</comment>
	</comments>
</bug>