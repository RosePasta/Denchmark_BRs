<bug id='2522' author='n-gregori' open_date='2016-12-16T17:59:47Z' closed_time='2018-04-26T03:03:37Z'>
	<summary>Can't allocate [HOST] memory: 301858816 on multy GPUs training</summary>
	<description>
hello, i've big trouble with training Google LeNet (Vgg too) with a pretty large image's dataset. now im' using aws instance with 8 GPUs Tesla K80 with 12g each and 490g of ram. this is the problem: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: Can't allocate [HOST] memory: 301858816; threadId: 105
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/657897/exeption.txt&gt;exeption.txt&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='n-gregori' date='2016-12-16T18:02:29Z'>
		Please, provide full output log. Exception itself is interesting, but useless. Full log needed.
		</comment>
		<comment id='2' author='n-gregori' date='2016-12-16T18:04:07Z'>
		Also, please show source code used to reproduce this crash. If you want to keep it private, you can send it to me as gist ( &lt;denchmark-link:https://gist.github.com/&gt;https://gist.github.com/&lt;/denchmark-link&gt;
 )
		</comment>
		<comment id='3' author='n-gregori' date='2016-12-16T18:07:47Z'>
		line 109 in MultiGpuGraphTrainer.scala cited in the exception is:
def trainEpoch(epochN: Int, save: Boolean, keep: Int): String = {
&lt;denchmark-code&gt;wrapper.fit(datasetTrain) 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='n-gregori' date='2016-12-16T18:11:22Z'>
		Hm, surprising, but it really looks like OOM message.
What CUDA version you have there?
		</comment>
		<comment id='5' author='n-gregori' date='2016-12-16T18:14:15Z'>
		CUDA version: 8.0
DL4J version: 0.7.0
		</comment>
		<comment id='6' author='n-gregori' date='2016-12-16T18:15:41Z'>
		Posting full log as gist for future reference: &lt;denchmark-link:https://gist.github.com/raver119/c144846f7dc0459c0a7a8816208af962&gt;https://gist.github.com/raver119/c144846f7dc0459c0a7a8816208af962&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='n-gregori' date='2016-12-16T18:17:13Z'>
		Show Dl4j-related source code please, and tell me what input data dimensions are. Image dimensions i mean.
		</comment>
		<comment id='8' author='n-gregori' date='2016-12-16T18:19:07Z'>
		P.s. also please consider using 0.7.1, it contains few important fixes for bugs found after 0.7.0 releases
		</comment>
		<comment id='9' author='n-gregori' date='2016-12-16T18:20:02Z'>
		the dataset thai i used is:
381 labels
100 images x label at 224x224 3 channels
		</comment>
		<comment id='10' author='n-gregori' date='2016-12-16T18:31:12Z'>
		Something non obvious: Could you share the architecture? (I know you said vgg 16 but config still helps) and were you using cudnn or raw dl4j?
		</comment>
		<comment id='11' author='n-gregori' date='2016-12-16T18:32:00Z'>
		cudnn is used there.
		</comment>
		<comment id='12' author='n-gregori' date='2016-12-16T18:32:33Z'>
		but i see suspicious reports from ParallelWrapper, and i want to see how exactly things are set up.
		</comment>
		<comment id='13' author='n-gregori' date='2016-12-16T18:41:55Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 i sent private gist to your email address
		</comment>
		<comment id='14' author='n-gregori' date='2016-12-16T18:44:12Z'>
		/cc &lt;denchmark-link:https://github.com/saudet&gt;@saudet&lt;/denchmark-link&gt;
 here just in case since he did cudnn.
		</comment>
		<comment id='15' author='n-gregori' date='2016-12-16T18:54:04Z'>
		&lt;denchmark-link:https://github.com/n-gregori&gt;@n-gregori&lt;/denchmark-link&gt;
 can you please also show deviceQuery output?
		</comment>
		<comment id='16' author='n-gregori' date='2016-12-16T18:56:22Z'>
		also, please apply these two changes to your config:

set 1GB max cache: .setMaximumDeviceCache(1024 * 1024 * 1024L)
fall back to legacy averaging: .useLegacyAveraging(true)

		</comment>
		<comment id='17' author='n-gregori' date='2016-12-16T19:03:26Z'>
		And one more thing: you say workers set to 8, but i see it's only 2 in logs. Are you sure about 8?
		</comment>
		<comment id='18' author='n-gregori' date='2016-12-16T21:37:06Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 initially i tried with 8 workers and finally with 2.
		</comment>
		<comment id='19' author='n-gregori' date='2017-01-13T11:25:41Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;

i've now installed 0.7.1, versions.
cuda settings:
CudaEnvironment.getInstance().getConfiguration()
.allowMultiGPU(true)
.allowCrossDeviceAccess(true)
.setMaximumDeviceCacheableLength(1024 * 1024 * 1024L)
.setMaximumDeviceCache(1024 * 1024 * 1024L)
.setMaximumHostCacheableLength(1024 * 1024 * 1024L)
.setMaximumHostCache(1024 * 1024 * 1024L)
.setMaximumGridSize(512)
.setMaximumBlockSize(512)
CudaEnvironment.getInstance().getConfiguration().allowMultiGPU(true)
and parallel wrapper:
&lt;denchmark-code&gt;wrapper = new ParallelWrapper.Builder(net.asInstanceOf[MultiLayerNetwork])
    .prefetchBuffer(prefetchBuffer)
    .workers(workers)
    .averagingFrequency(averagingFrequency)
    .reportScoreAfterAveraging(true)
    .useLegacyAveraging(true)
    .build()
&lt;/denchmark-code&gt;

using ec2-amazon instance p2.8xlarge:
8 GPUs Tesla K80 with 12g each and 490g of ram
i'm trying to train VggC net on the same dataset (381 labels
100 images x label at 224x224 3 channels)
finally i get the same exception....
What's wrong?
		</comment>
		<comment id='20' author='n-gregori' date='2017-01-17T17:51:30Z'>
		... seeing same issue on 0.7,2
		</comment>
		<comment id='21' author='n-gregori' date='2017-01-23T10:14:32Z'>
		&lt;denchmark-link:https://github.com/paulhanke&gt;@paulhanke&lt;/denchmark-link&gt;
 yep, we're aware of that. Fix for this issue will be the part of next major release.
		</comment>
		<comment id='22' author='n-gregori' date='2018-04-26T03:03:37Z'>
		Fixed long ago
		</comment>
		<comment id='23' author='n-gregori' date='2018-09-22T16:13:52Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>