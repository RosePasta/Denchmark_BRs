<bug id='4249' author='stolsvik' open_date='2017-11-05T21:42:54Z' closed_time='2017-12-14T03:05:13Z'>
	<summary>RuntimeException: "Can't allocate [HOST] memory" when using UIServer with StatsListener</summary>
	<description>
Got help from &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
, on whose request I am posting this issue.
When running a network and adding a StatsListener, I suddenly get the following Exception:
java.lang.RuntimeException: Can't allocate [HOST] memory: 44728320; threadId: 124219
	at org.nd4j.jita.memory.impl.CudaDirectProvider.malloc(CudaDirectProvider.java:59)
	at org.nd4j.jita.memory.impl.CudaCachingZeroProvider.malloc(CudaCachingZeroProvider.java:113)
	at org.nd4j.jita.memory.impl.CudaFullCachingProvider.malloc(CudaFullCachingProvider.java:91)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:237)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:258)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:470)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:396)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:216)
	at org.nd4j.linalg.jcublas.buffer.CudaFloatDataBuffer.&lt;init&gt;(CudaFloatDataBuffer.java:60)
	at org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createFloat(CudaDataBufferFactory.java:321)
	at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1468)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.&lt;init&gt;(BaseNDArray.java:260)
	at org.nd4j.linalg.jcublas.JCublasNDArray.&lt;init&gt;(JCublasNDArray.java:108)
	at org.nd4j.linalg.jcublas.JCublasNDArrayFactory.createUninitialized(JCublasNDArrayFactory.java:243)
	at org.nd4j.linalg.factory.Nd4j.createUninitialized(Nd4j.java:5054)
	at org.nd4j.linalg.api.shape.Shape.toOffsetZeroCopyHelper(Shape.java:247)
	at org.nd4j.linalg.api.shape.Shape.toOffsetZeroCopy(Shape.java:204)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.dup(BaseNDArray.java:1702)
	at org.nd4j.linalg.jcublas.JCublasNDArray.dup(JCublasNDArray.java:400)
	at org.deeplearning4j.ui.stats.BaseStatsListener.onGradientCalculation(BaseStatsListener.java:273)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:180)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1245)
	at TestingQuandl.runNetwork(TestingQuandl.groovy:1193)
	at TestingQuandl.runStatic(TestingQuandl.groovy:325)
	at TestingQuandl$runStatic.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:182)
	at TestingQuandl.run(TestingQuandl.groovy:110)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:585)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:632)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.doCall(ExploratoryGroovyLooper.groovy:180)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.call(ExploratoryGroovyLooper.groovy)
	at groovy.lang.Closure.run(Closure.java:495)
at java.lang.Thread.run(Thread.java:748)
raver119 evidently doesn't believe this is an actual OOM situation, and with his help I compiled dl4j and reqs from source, with some added logging and error code dumping. However, when trying to run this, I got into issue &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/issues/2245&gt;deeplearning4j/nd4j#2245&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='stolsvik' date='2017-11-06T22:44:34Z'>
		Here's a new stack trace with the compiled-from-sources jars:
&lt;denchmark-code&gt;CUDA error at /home/endre/git/libnd4j/blas/cuda/NativeOps.cu:4582 code=30(cudaErrorUnknown) "res" 

java.lang.RuntimeException: Can't allocate [HOST] memory: 884736; threadId: 124383
	at org.nd4j.jita.memory.impl.CudaDirectProvider.malloc(CudaDirectProvider.java:59)
	at org.nd4j.jita.memory.impl.CudaCachingZeroProvider.malloc(CudaCachingZeroProvider.java:113)
	at org.nd4j.jita.memory.impl.CudaFullCachingProvider.malloc(CudaFullCachingProvider.java:91)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:237)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:258)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:470)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:396)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:216)
	at org.nd4j.linalg.jcublas.buffer.CudaFloatDataBuffer.&lt;init&gt;(CudaFloatDataBuffer.java:60)
	at org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createFloat(CudaDataBufferFactory.java:321)
	at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1490)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.&lt;init&gt;(BaseNDArray.java:263)
	at org.nd4j.linalg.jcublas.JCublasNDArray.&lt;init&gt;(JCublasNDArray.java:108)
	at org.nd4j.linalg.jcublas.JCublasNDArrayFactory.createUninitialized(JCublasNDArrayFactory.java:243)
	at org.nd4j.linalg.factory.Nd4j.createUninitialized(Nd4j.java:5115)
	at org.nd4j.linalg.api.shape.Shape.toOffsetZeroCopyHelper(Shape.java:337)
	at org.nd4j.linalg.api.shape.Shape.toOffsetZeroCopy(Shape.java:294)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.dup(BaseNDArray.java:1709)
	at org.nd4j.linalg.jcublas.JCublasNDArray.dup(JCublasNDArray.java:400)
	at org.deeplearning4j.ui.stats.BaseStatsListener.onGradientCalculation(BaseStatsListener.java:262)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:179)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1238)
	at TestingQuandl.runNetwork(TestingQuandl.groovy:1196)
	at TestingQuandl.runStatic(TestingQuandl.groovy:326)
	at TestingQuandl$runStatic.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:182)
	at TestingQuandl.run(TestingQuandl.groovy:111)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:585)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:632)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.doCall(ExploratoryGroovyLooper.groovy:180)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.call(ExploratoryGroovyLooper.groovy)
	at groovy.lang.Closure.run(Closure.java:495)
	at java.lang.Thread.run(Thread.java:748)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='stolsvik' date='2017-11-06T22:49:21Z'>
		Just to keep these together, here's another one, with a different stack trace:
&lt;denchmark-code&gt;CUDA error at /home/endre/git/libnd4j/blas/cuda/NativeOps.cu:4582 code=30(cudaErrorUnknown) "res" 

java.lang.RuntimeException: Can't allocate [HOST] memory: 109609488; threadId: 124302
	at org.nd4j.jita.memory.impl.CudaDirectProvider.malloc(CudaDirectProvider.java:59)
	at org.nd4j.jita.memory.impl.CudaCachingZeroProvider.malloc(CudaCachingZeroProvider.java:116)
	at org.nd4j.jita.memory.impl.CudaFullCachingProvider.malloc(CudaFullCachingProvider.java:91)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:237)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:258)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:470)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:396)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:216)
	at org.nd4j.linalg.jcublas.buffer.CudaFloatDataBuffer.&lt;init&gt;(CudaFloatDataBuffer.java:60)
	at org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createFloat(CudaDataBufferFactory.java:321)
	at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1490)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.&lt;init&gt;(BaseNDArray.java:263)
	at org.nd4j.linalg.jcublas.JCublasNDArray.&lt;init&gt;(JCublasNDArray.java:108)
	at org.nd4j.linalg.jcublas.JCublasNDArrayFactory.createUninitialized(JCublasNDArrayFactory.java:243)
	at org.nd4j.linalg.factory.Nd4j.createUninitialized(Nd4j.java:5115)
	at org.deeplearning4j.nn.updater.BaseMultiLayerUpdater.&lt;init&gt;(BaseMultiLayerUpdater.java:130)
	at org.deeplearning4j.nn.updater.MultiLayerUpdater.&lt;init&gt;(MultiLayerUpdater.java:28)
	at org.deeplearning4j.nn.updater.MultiLayerUpdater.&lt;init&gt;(MultiLayerUpdater.java:24)
	at org.deeplearning4j.nn.updater.UpdaterCreator.getUpdater(UpdaterCreator.java:20)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.updateGradientAccordingToParams(BaseOptimizer.java:326)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:187)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1238)
	at TestingQuandl.runNetwork(TestingQuandl.groovy:1196)
	at TestingQuandl.runStatic(TestingQuandl.groovy:326)
	at TestingQuandl$runStatic.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:182)
	at TestingQuandl.run(TestingQuandl.groovy:111)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:585)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:632)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.doCall(ExploratoryGroovyLooper.groovy:180)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.call(ExploratoryGroovyLooper.groovy)
	at groovy.lang.Closure.run(Closure.java:495)
	at java.lang.Thread.run(Thread.java:748)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='stolsvik' date='2017-11-06T22:50:52Z'>
		and this, in RandomFactory...
&lt;denchmark-code&gt;CUDA error at /home/endre/git/libnd4j/blas/cuda/NativeOps.cu:4582 code=30(cudaErrorUnknown) "res" 

java.lang.RuntimeException: java.lang.RuntimeException: Can't allocate [HOST] memory: 80000000; threadId: 124295
	at org.nd4j.linalg.factory.RandomFactory.getRandom(RandomFactory.java:40)
	at org.nd4j.linalg.factory.Nd4j.getRandom(Nd4j.java:539)
	at org.deeplearning4j.nn.conf.NeuralNetConfiguration$Builder.seed(NeuralNetConfiguration.java:778)
	at TestingQuandl.runNetwork(TestingQuandl.groovy:1102)
	at TestingQuandl.runStatic(TestingQuandl.groovy:326)
	at TestingQuandl$runStatic.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:182)
	at TestingQuandl.run(TestingQuandl.groovy:111)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:585)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:632)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.doCall(ExploratoryGroovyLooper.groovy:180)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.call(ExploratoryGroovyLooper.groovy)
	at groovy.lang.Closure.run(Closure.java:495)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Can't allocate [HOST] memory: 80000000; threadId: 124295
	at org.nd4j.jita.memory.impl.CudaDirectProvider.malloc(CudaDirectProvider.java:59)
	at org.nd4j.jita.memory.impl.CudaCachingZeroProvider.malloc(CudaCachingZeroProvider.java:113)
	at org.nd4j.jita.memory.impl.CudaFullCachingProvider.malloc(CudaFullCachingProvider.java:91)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:237)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:258)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:470)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:396)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:216)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:327)
	at org.nd4j.linalg.jcublas.buffer.CudaDoubleDataBuffer.&lt;init&gt;(CudaDoubleDataBuffer.java:56)
	at org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createDouble(CudaDataBufferFactory.java:306)
	at org.nd4j.rng.NativeRandom.&lt;init&gt;(NativeRandom.java:77)
	at org.nd4j.linalg.jcublas.rng.CudaNativeRandom.&lt;init&gt;(CudaNativeRandom.java:31)
	at org.nd4j.linalg.jcublas.rng.CudaNativeRandom.&lt;init&gt;(CudaNativeRandom.java:27)
	at org.nd4j.linalg.jcublas.rng.CudaNativeRandom.&lt;init&gt;(CudaNativeRandom.java:23)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.lang.Class.newInstance(Class.java:442)
	at org.nd4j.linalg.factory.RandomFactory.getRandom(RandomFactory.java:28)
	... 15 more
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='stolsvik' date='2017-11-06T23:12:57Z'>
		Yet another one..
&lt;denchmark-code&gt;CUDA error at /home/endre/git/libnd4j/blas/cuda/NativeOps.cu:4582 code=30(cudaErrorUnknown) "res" 

  EGL Runner raised EXCEPTION! Took 27,792.307 ms (0.463 min)
java.lang.RuntimeException: Can't allocate [HOST] memory: 109609488; threadId: 124408
	at org.nd4j.jita.memory.impl.CudaDirectProvider.malloc(CudaDirectProvider.java:59)
	at org.nd4j.jita.memory.impl.CudaCachingZeroProvider.malloc(CudaCachingZeroProvider.java:116)
	at org.nd4j.jita.memory.impl.CudaFullCachingProvider.malloc(CudaFullCachingProvider.java:91)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:237)
	at org.nd4j.jita.handler.impl.CudaZeroHandler.alloc(CudaZeroHandler.java:258)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:470)
	at org.nd4j.jita.allocator.impl.AtomicAllocator.allocateMemory(AtomicAllocator.java:396)
	at org.nd4j.linalg.jcublas.buffer.BaseCudaDataBuffer.&lt;init&gt;(BaseCudaDataBuffer.java:216)
	at org.nd4j.linalg.jcublas.buffer.CudaFloatDataBuffer.&lt;init&gt;(CudaFloatDataBuffer.java:60)
	at org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createFloat(CudaDataBufferFactory.java:321)
	at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1490)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.&lt;init&gt;(BaseNDArray.java:263)
	at org.nd4j.linalg.jcublas.JCublasNDArray.&lt;init&gt;(JCublasNDArray.java:108)
	at org.nd4j.linalg.jcublas.JCublasNDArrayFactory.createUninitialized(JCublasNDArrayFactory.java:243)
	at org.nd4j.linalg.factory.Nd4j.createUninitialized(Nd4j.java:5115)
	at org.deeplearning4j.nn.updater.BaseMultiLayerUpdater.&lt;init&gt;(BaseMultiLayerUpdater.java:130)
	at org.deeplearning4j.nn.updater.MultiLayerUpdater.&lt;init&gt;(MultiLayerUpdater.java:28)
	at org.deeplearning4j.nn.updater.MultiLayerUpdater.&lt;init&gt;(MultiLayerUpdater.java:24)
	at org.deeplearning4j.nn.updater.UpdaterCreator.getUpdater(UpdaterCreator.java:20)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.updateGradientAccordingToParams(BaseOptimizer.java:326)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:187)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1238)
	at TestingQuandl.runNetwork(TestingQuandl.groovy:1196)
	at TestingQuandl.runStatic(TestingQuandl.groovy:326)
	at TestingQuandl$runStatic.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:182)
	at TestingQuandl.run(TestingQuandl.groovy:111)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:585)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:632)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.doCall(ExploratoryGroovyLooper.groovy:180)
	at com.stolsvik.machinelearning.tools.ExploratoryGroovyLooper$_loop_closure1.call(ExploratoryGroovyLooper.groovy)
	at groovy.lang.Closure.run(Closure.java:495)
	at java.lang.Thread.run(Thread.java:748)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='stolsvik' date='2017-11-07T02:17:13Z'>
		It does look like an OOM situation though. Might be caused by memory fragmentation due to excessive memory deallocations and reallocations. How long does the process run before that happens?
		</comment>
		<comment id='6' author='stolsvik' date='2017-11-07T21:39:46Z'>
		Typically "for a while". And after this has happened, it is very hard to do more in the same JVM.
Here's a couple of points:

I have 64 GB of RAM. First I tried upping the JVM heap, but then I realized that this is probably off-heap, so I tried lowering it, now running on 14GB heap (need some room for the features). My thinking is that there must be extreme amounts of bad memory management to not be able to find e.g. 104 MB of room continuous in something like 40GB of free RAM.
This happened after adding the UIServer and its StatsListener. Below, I've pasted some worrisome lines from that "dup" method.
I tried adding System.gc(), Thread.sleep(500), System.gc(), Thread.sleep(500) in the "epoch-loop". Didn't help.
I had this happening all the time when I was making DataSets myself, instead of using the DataSetIterator (with its Workspace).
Does a CUDA OOM give error code 30, "cudaErrorUnknown"?
Both setting the ND4J_DEBUG=true, OR setting the Nd4j.getExecutioner().setProfilingMode(OpExecutioner.ProfilingMode.ANY_PANIC), seems to "fix" the problem. Any idea why?  (Strike that, just got a crash with ND4J_DEBUG=true - I was hoping that I could work with this stuff, just 2x slow..)

Here's the code from the StatsListener's call up to malloc:
    @Override
    public void onGradientCalculation(Model model) {
        int iterCount = getModelInfo(model).iterCount;
        if (storeGradients() &amp;&amp; updateConfig.reportingFrequency() &gt; 0
                        &amp;&amp; (iterCount == 0 || iterCount % updateConfig.reportingFrequency() == 0)) {
            Gradient g = model.gradient();
            gradientsPreUpdateMap.clear();
            try (MemoryWorkspace ws = Nd4j.getMemoryManager().scopeOutOfWorkspaces()) {
                for (Map.Entry&lt;String, INDArray&gt; entry : g.gradientForVariable().entrySet()) {
                    gradientsPreUpdateMap.put(entry.getKey(), entry.getValue().dup()); //Need to clone: will be modified (updated) in-place soon...
                }
            }
        }
    }
Notice the explicit "scope out of workspaces" - this seems to ask for problems?
PS: Relying on GC for clearing external resources is rather bad practice, IMHO - and in many others' opinion, as I understand: "A correctly-written program cannot assume that finalizers will ever run at any point prior to program termination.". Then forcing GC by actually calling System.gc() at intervals is a bad hack to accomplish resource handling which relies on GC. There should be explicit .close() methods.
I've several times made the following logic to handle resources that should be closed: Upon instantiation of a "resource-shell-instance", store a DebugStacktraceException or somesuch in a field. In the finalizer (which one should not use, since RefQueues are much better), I check whether the user has closed the instance, and if not, I close it and log.error a harsh warning along with the stacktrace. It is then dead simple for the user to realize where such non-closed instances come from, and fix his code paths.
		</comment>
		<comment id='7' author='stolsvik' date='2017-11-08T00:38:29Z'>
		Note: I might have a fix. Running overnight, if it works, I'll submit pull req.
		</comment>
		<comment id='8' author='stolsvik' date='2017-11-08T03:00:27Z'>
		
There should be explicit .close() methods.

Yes that's exactly what "workspaces" are. The bit that you pointed out actually does seem to be the cause of the problem. What do you think &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='9' author='stolsvik' date='2017-11-08T07:28:05Z'>
		

There should be explicit .close() methods.


Yes that's exactly what "workspaces" are.

Well. Workspaces are an abstraction that handles caching, reuse etc etc - and thus also provide speed. I do understand that that they then also implicitly handles the close part, but that's an added benefit of employing Workspaces, not "exactly" what they are.
That is rather orthogonal to the other question: If I just create a NDArray somewhere, should I have to close it, or not? The answer is most definitely: They are a resource, and explicitly an external resource, and they should absolutely have an explicit close semantics. Relying on the GC for anything remotely like this is explicitly discouraged by everything explaining what GC is. GC should for such type of resources at most be a safety net. I would be happier if you in a ref-queue did System.exit(1) when you spotted an unclosed external resource, but the technique I explained in previous comment is a very happy middleway.
		</comment>
		<comment id='10' author='stolsvik' date='2017-11-08T07:29:55Z'>
		Btw: My fix worked out, so I'll submit a patch.
		</comment>
		<comment id='11' author='stolsvik' date='2017-11-08T07:33:17Z'>
		What exactly your patch contains?
At this point i'm still not sure what happens at your end, because i still remember that you had same allocation error right at the beginning of app lifecycle - random initialization.
		</comment>
		<comment id='12' author='stolsvik' date='2017-11-08T07:38:20Z'>
		Re: INDArray.close(), sure, we can add such method - it won't take too much time to add this. I believe we had it before. But i doubt we'll spend time updating dl4j codebase for it. Because there's lots of global INDArray uses, i.e. layer class fields like input field etc.
		</comment>
		<comment id='13' author='stolsvik' date='2017-11-11T10:17:50Z'>
		Referring to the PR &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4266&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4266&lt;/denchmark-link&gt;
 I had a comment, and just not let that idea disappear with the PR, I copy it in here:
"One thing I realized when committing this evident non-fix, was that what's the use of doing all that copying/duplication of the NDArrays? Why not just calculate the mean, std-dev and mean magnitudes right there in the onForwardPass(..) (x2) and onGradientCalculation(..)? Unless I am missing something, that would be a much better solution, alleviating the need to keep those NDArrays around. I guess it should both reduce memory footprint and increase speed too.
Notice also that I did some micro changes to the "if" within the onForwardPass - it was different between the two, which didn't make immediate sense. Also, the use of the two HashMaps storing the activations and gradient had slightly different semantics between them: Seemed like a better idea to just use two fixed instances, and clear them as fast as possible after use - not in front of the next use."
		</comment>
		<comment id='14' author='stolsvik' date='2017-12-14T03:05:13Z'>
		Closing this - PR should have fixed memory issues with stats listener - &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4292&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4292&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='stolsvik' date='2018-09-23T22:43:53Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>