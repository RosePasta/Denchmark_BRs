<bug id='1432' author='silentsnooc' open_date='2016-04-23T18:17:58Z' closed_time='2016-11-30T14:43:41Z'>
	<summary>ParagraphVectors crashing when using larger CSV file?</summary>
	<description>
I use some code from the current repository to load CSV files as a WordVectors model:
public class Dl4jModelLoader {

    private static final String whitespaceReplacement = "_Az92_";

    public static WordVectors loadTxtVectors(@NonNull InputStream stream, boolean skipFirstLine) throws IOException {
        AbstractCache&lt;VocabWord&gt; cache = new AbstractCache.Builder&lt;VocabWord&gt;().build();

        BufferedReader reader = new BufferedReader(new InputStreamReader(stream));
        List&lt;INDArray&gt; arrays = new ArrayList&lt;&gt;();

        if (skipFirstLine) {
            reader.readLine();
        }

        String line;
        while((line = reader.readLine()) != null) {
            String[] split = line.split(" ");
            String word = split[0].replaceAll(whitespaceReplacement, " ");
            VocabWord word1 = new VocabWord(1.0, word);

            word1.setIndex(cache.numWords());

            cache.addToken(word1);

            cache.addWordToIndex(word1.getIndex(), word);

            cache.putVocabWord(word);
            INDArray row = Nd4j.create(Nd4j.createBuffer(split.length - 1));
            for (int i = 1; i &lt; split.length; i++) {
                row.putScalar(i - 1, Float.parseFloat(split[i]));
            }
            arrays.add(row);
        }

        InMemoryLookupTable&lt;VocabWord&gt; lookupTable = (InMemoryLookupTable&lt;VocabWord&gt;) new InMemoryLookupTable.Builder&lt;VocabWord&gt;()
                .vectorLength(arrays.get(0).columns())
                .cache(cache)
                .build();

        INDArray syn = Nd4j.create(new int[]{arrays.size(), arrays.get(0).columns()});
        for (int i = 0; i &lt; syn.rows(); i++) {
            syn.putRow(i,arrays.get(i));
        }

        //Nd4j.clearNans(syn);
        lookupTable.setSyn0(syn);

        return WordVectorSerializer.fromPair(Pair.makePair((InMemoryLookupTable) lookupTable, (VocabCache) cache));
    }


}
However, calling  appears to crash the JVM if I use a larger file: &lt;denchmark-link:http://pastebin.com/SHY9CYpj&gt;http://pastebin.com/SHY9CYpj&lt;/denchmark-link&gt;

    val word2vecModel = Dl4jModelLoader.loadTxtVectors(in, true)

    val w2vLookupTable = word2vecModel.lookupTable().asInstanceOf[InMemoryLookupTable[VocabWord]]
    val w2vVocabCache = word2vecModel.vocab().asInstanceOf[VocabCache[VocabWord]]

    val nearest = word2vecModel.wordsNearest("computer", 10)

    for(n &lt;- nearest) {
      println(n)
    }

    val tokenizerFactory = new DefaultTokenizerFactory()
    tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor)

    val taggedSentenceJsonFile = new File(arguments.getInputFilePath)
    val labelAwareSentenceIterator = new MyLabelAwareSentenceIterator(taggedSentenceJsonFile)

    val paragraphVectors = new ParagraphVectors.Builder()
      .minWordFrequency(arguments.getWordMinCount)
      .layerSize(arguments.getVectorSize)
      .learningRate(arguments.getLearningRate)
      .windowSize(arguments.getContextWindowSize)
      .epochs(arguments.getNumEpochs)
      .iterations(arguments.getNumIterations)
      .iterate(labelAwareSentenceIterator)
      .tokenizerFactory(tokenizerFactory)
      .useAdaGrad(arguments.getUseAdaptiveGradient)
      .trainWordVectors(false)
      .vocabCache(w2vVocabCache)
      .lookupTable(w2vLookupTable)
      .build()
	</description>
	<comments>
		<comment id='1' author='silentsnooc' date='2016-11-23T14:34:56Z'>
		Still relevant?
		</comment>
		<comment id='2' author='silentsnooc' date='2016-11-23T14:44:45Z'>
		Highly doubt. Everything was overhauled for paravec alltogether with w2v.
And this crash, as i see, was caused by absent syn1 in deserialized model. There's few tests now covering such behaviour due to w2v/google model uptraining.
		</comment>
		<comment id='3' author='silentsnooc' date='2019-01-20T07:47:04Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>