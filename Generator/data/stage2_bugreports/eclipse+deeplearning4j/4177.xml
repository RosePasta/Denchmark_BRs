<bug id='4177' author='mirko1978' open_date='2017-10-16T08:21:56Z' closed_time='2018-04-25T23:35:56Z'>
	<summary>SparkWord2Vec deadlock with default initialisation</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I tried to run SparkWord2Vec giving default parameters or at least what I though so (I looked into Builder implementation)
import org.apache.spark.{SparkConf, SparkContext}
import org.deeplearning4j.models.embeddings.loader.VectorsConfiguration
import org.deeplearning4j.spark.models.sequencevectors.export.impl.HdfsModelExporter
import org.deeplearning4j.spark.models.sequencevectors.learning.elements.SparkSkipGram
import org.deeplearning4j.spark.models.word2vec.SparkWord2Vec
import org.deeplearning4j.text.tokenization.tokenizerfactory.DefaultTokenizerFactory
import org.nd4j.parameterserver.distributed.conf.VoidConfiguration

object W2VProgram  extends App{
    val sparkConf = new SparkConf().setAppName("Word2Vec")
    sparkConf.setMaster("local[*]")
        .set("spark.locality.wait", "0")
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    val sentences=sc.parallelize(Seq("one two thee four", "some once again", "one another sentence"))
    val config: VectorsConfiguration = new VectorsConfiguration()
    config.setTokenizerFactory(classOf[DefaultTokenizerFactory].getCanonicalName)
    val psConfig = new VoidConfiguration()
    val model: SparkWord2Vec = new SparkWord2Vec.Builder(psConfig, config)
        .workers(4)
        .setLearningAlgorithm(new SparkSkipGram())
        .setModelExporter(new HdfsModelExporter("./model"))
        .build()
    model.fitSentences(sentences)
}
The result is high CPU usage and never ending process. The same happen if you try to run SparkWord2VecTest
I did some deeper analysis and seems to me that the problem is around nd4j.parameterserver.
The loop at RoutedTransport.sendCommandToShard:465 never end because the message is never send.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version: 0.9.1_spark_2
platform information: OSX
Spark version: 2.2.0
Scala version: 2.11.11

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

The problem seems to be the default initialisation. SparkSequenceVectors.fitSequences:173 accept null values for VoidConfiguration, if so initialise it correctly. However, SparkWord2Vec and SparkWord2Vec.Builder don't allow a null VoidConfiguration.
Without any working default parameters or documentation example is not easy to make it works.
Changing line 20 on my example with the follow, make it works correctly
val psConfig: VoidConfiguration = VoidConfiguration.builder()
        .faultToleranceStrategy(FaultToleranceStrategy.NONE)
        .numberOfShards(2)
        .unicastPort(40123)
        .multicastPort(40124)
        .build()
	</description>
	<comments>
		<comment id='1' author='mirko1978' date='2018-03-22T11:25:04Z'>
		my sparkw2v can run normally on spark local, but on standalone it can not.  it does not run at SparkSequenceVectors 288.
		</comment>
		<comment id='2' author='mirko1978' date='2018-03-22T12:37:44Z'>
		I solved it. Because of the docker, increase the /dev/shm.
		</comment>
		<comment id='3' author='mirko1978' date='2018-09-22T17:13:42Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>