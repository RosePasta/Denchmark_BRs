<bug id='6876' author='AlexDBlack' open_date='2018-12-17T07:16:39Z' closed_time='2019-04-08T07:28:41Z'>
	<summary>SameDiff: Fix "gradient" calculation for non-floating point arrays</summary>
	<description>
Consider the following simple case:
&lt;denchmark-code&gt;    @Test
    public void testNonFpGrad(){
        SameDiff sd = SameDiff.create();
        SDVariable v1 = sd.var("v1", Nd4j.create(DataType.LONG, 3,4));
        SDVariable v2 = sd.var("v2", Nd4j.create(DataType.LONG, 3,4));

        SDVariable add = v1.add(v2);
        SDVariable sum = add.castTo(DataType.DOUBLE).sum();

        sd.execBackwards();
    }
&lt;/denchmark-code&gt;

Which (on my current branch) gives:
&lt;denchmark-code&gt;o.n.l.c.n.o.NativeOpExecutioner - Failed to execute op add_bp. Attempted to execute with 3 inputs, 2 outputs, 0 targs and 0 iargs. Inputs: [(LONG,[3,4],c), (LONG,[3,4],c), (LONG,[3,4],c)]. Outputs: [(LONG,[3,4],c), (LONG,[3,4],c)] - Please see above message (printed out from c++) for a possible cause of error.
&lt;/denchmark-code&gt;

Clearly, the problem here is that we are trying to calculate the "gradient" with respect to arrays with LONG datatype. However, by definition gradients are only defined for real-valued arrays.
An obvious hack here is to just return 0 "gradient" for these non-FP variables, but this is inefficient (still requires array allocations, memory, etc) and a little ugly (we still create gradient SDVariables corresponding to the "gradient" of these long arrays.
We need a better solution that:

Doesn't involve creating/returning 0s as a workaround when gradients aren't defined/don't exist
Actually validates that we aren't trying to create "gradient" SDVariables for non-FP SDVariables
Terminates the gradient calculation procedure early, accounting for situations where gradients are not defined even for some floating point variables

For example: float -&gt; cast to long type: the float variable can legitimately not have any gradient



	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-12-17T07:18:09Z'>
		Hm, we shouldn't be trying to calculate those. It should fail dtypes validation.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-12-17T08:52:06Z'>
		
Hm, we shouldn't be trying to calculate those. It should fail dtypes validation.

Sure, we should detect it earlier. But failing earlier doesn't change the fact that we should never have created that particular add_bp (or whatever) op in the first place :)
We'll also need to do a pass on gradient/backprop functions.
For example, we have segment_mean_bp that returns 2 "gradients" (including one for the indices).
It would make a lot more sense for it to just return gradients for the input - i.e., single output function.
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/af7155d61dc810d3e7139f15f98810e0255b2e17/libnd4j/include/ops/declarable/generic/parity_ops/segment_mean.cpp#L77-L80&gt;https://github.com/deeplearning4j/deeplearning4j/blob/af7155d61dc810d3e7139f15f98810e0255b2e17/libnd4j/include/ops/declarable/generic/parity_ops/segment_mean.cpp#L77-L80&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='AlexDBlack' date='2019-05-08T08:04:19Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>