<bug id='4542' author='AlexDBlack' open_date='2018-01-24T03:23:19Z' closed_time='2018-01-30T08:14:33Z'>
	<summary>ComputationGraph backprop: issues with workspaces</summary>
	<description>
I have noticed some issues in ComputationGraph.calcBackpropGradients
First - consider SEPARATE mode. It's possible (if using external errors, for example) that workspaceExternal/LOOP_EXTERNAL is not active. That has been fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4541&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4541&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1684&gt;https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1684&lt;/denchmark-link&gt;

Second - consider SINGLE mode. In this case,  the loop here is opening/closing workspaceExternal once for each vertex:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1710&gt;https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1710&lt;/denchmark-link&gt;

Thus, the leverageTo(workspaceExternal) is a no-op:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1752&gt;https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1752&lt;/denchmark-link&gt;

Consequently, the epsilons there aren't migrated, and hence will be invalidated at the end of the loop.
Not sure on the best solution here - we don't want to simply create another workspace and push all the epsilons to that, due to memory requirements for doing so (memory would be equal to the entire network activations size). But we also can't simply use a 'workspace in a loop' solution as some epsilons may be needed much later in the backprop procedure, not just in the next vertex backprop.
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-09-23T16:26:15Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>