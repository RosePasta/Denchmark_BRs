<bug id='1438' author='AlexDBlack' open_date='2016-04-24T14:10:56Z' closed_time='2016-04-27T13:17:26Z'>
	<summary>NaN scores with only one of L1 or L2 regularization (neither or both OK)</summary>
	<description>
To reproduce: run GradientCheckTests.testGradientGravesLSTMFull().
As far as I can tell: the reason is that we have NaNs scores here is due to having .regularization(true) with either l2 set (and l1 not), or l1 set (and l2 not). The unset l1/l2 value has NaN in the config, whereas previously it used to be zero if not set.
Side note: this test needs to be switched to the following, given the new configuration validation code:
    NeuralNetConfiguration.Builder conf = new NeuralNetConfiguration.Builder()
            .regularization(l1&gt;0.0 || l2&gt;0.0).seed(12345L);
    if(l1&gt;0.0) conf.l1(l1);
    if(l2&gt;0.0) conf.l2(l2);
    NeuralNetConfiguration.ListBuilder conf2 = conf.list()
            .layer(0, new GravesLSTM.Builder().nIn(nIn).nOut(layerSize)
                    .weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1))
                    .activation(afn).updater(Updater.NONE).build())
            .layer(1, new RnnOutputLayer.Builder(lf).activation(outputActivation).nIn(layerSize).nOut(nOut)
                    .weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1))
                    .updater(Updater.NONE).build())
            .pretrain(false).backprop(true);

    MultiLayerNetwork mln = new MultiLayerNetwork(conf2.build());
    mln.init();
cc &lt;denchmark-link:https://github.com/nyghtowl&gt;@nyghtowl&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2016-04-27T13:17:26Z'>
		Appears to be fixed as of most recent version.
		</comment>
		<comment id='2' author='AlexDBlack' date='2019-01-21T05:53:06Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>