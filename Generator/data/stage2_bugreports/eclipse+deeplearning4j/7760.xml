<bug id='7760' author='AlexDBlack' open_date='2019-05-20T09:22:15Z' closed_time='2019-11-08T02:08:35Z'>
	<summary>SameDiff: Large models can fail FlatBuffers export</summary>
	<description>
When running TFGraphTestZooModels with  model deeplabv3_pascal_train_aug_2018_01_04:
&lt;denchmark-code&gt;o.n.i.T.TFGraphTestZooModels - ----- Libnd4j Exec: deeplabv3_pascal_train_aug_2018_01_04 -----
o.n.i.T.TFGraphTestAllHelper - 
	RUNNING TEST deeplabv3_pascal_train_aug_2018_01_04...
o.n.a.e.NativeGraphExecutioner - Configuration: ExecutorConfiguration(profilingMode=DISABLED, executionMode=SEQUENTIAL, outputMode=VARIABLE_SPACE, gatherTimings=true, footprintForward=0, footprintBackward=0)
o.n.a.e.NativeGraphExecutioner - Buffer length: 268435456
&lt;String 0&gt; start: 0; end: 43; length: 43;
&lt;String 0&gt; start: 0; end: 29; length: 29;
&lt;String 0&gt; start: 0; end: 31; length: 31;
&lt;String 0&gt; start: 0; end: 54; length: 54;
Assertion failed!

Program: C:\Program Files\Java\jdk1.8.0_201\bin\java.exe
File: C:/DL4J/Git/deeplearning4j/libnd4j/blasbuild/cpu/flatbuffers-src/include/flatbuffers/flatbuffers.h, Line 668

Expression: size() &lt; FLATBUFFERS_MAX_BUFFER_SIZE
&lt;/denchmark-code&gt;

This is a sizeable model - looks like the export is too large. Though we should check for any redundant arrays in the export...
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2019-05-29T12:30:52Z'>
		In this particular model (that caused issue above) I can confirm the problem is not due to too large a model (though we will run into that eventually).
The exported buffer works out to 268,435,456 bytes (note this is same size reported in NativeGraphExecutioner logging above).
This also roughly aligns with what I've counted as the number of bytes for the data of all exported arrays.
Yet clearly we see Expression: size() &lt; FLATBUFFERS_MAX_BUFFER_SIZE being reported.
I see only three possiblities here.

There's a mismatch between the Java and C++ generated files (i.e., we forgot to update Java?)
Somehow the file is written incorrectly (some bug in how the flatbuffers files are used)
Somehow the offset value on Windows is different? (I haven't tested this model on Linux yet)
https://github.com/google/flatbuffers/blob/8d86b5347fbeba85f11b7bf479334f6700feb455/include/flatbuffers/base.h#L294-L295

As an aside, switching to 64-bit offsets is possible, and was straightforward (at least in 2015) according to this:
&lt;denchmark-link:https://groups.google.com/forum/#!topic/flatbuffers/Ltn_nVh23SQ&gt;https://groups.google.com/forum/#!topic/flatbuffers/Ltn_nVh23SQ&lt;/denchmark-link&gt;

The downside is incompatibilities (possibly including anything generated by flatc - Java, JS, Python etc? not sure on that).
The "multiple files" approach should also be doable, but it's not exactly ideal from a usability perspective.
		</comment>
		<comment id='2' author='AlexDBlack' date='2019-05-30T11:23:36Z'>
		I can rule out a mismatch between java/cpp flatbuffers generated files: regenerating with flatc 0.10.0 shows no diff between the current and generated files.
I also regenerated all files with flatbuffers 0.11.0 (upgrade from 0.10.0), with no difference. Same failure on this model.
		</comment>
		<comment id='3' author='AlexDBlack' date='2019-11-08T02:08:35Z'>
		Closing in favor of: &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/8312&gt;#8312&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>