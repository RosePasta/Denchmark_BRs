<bug id='5068' author='AlexDBlack' open_date='2018-05-07T04:01:31Z' closed_time='2018-05-08T03:44:17Z'>
	<summary>CuDNN not consistent between runs</summary>
	<description>
Windows 10, CuDNN 7, CUDA 9.1.
Training curves are not consistent between runs here.
&lt;denchmark-code&gt;    public static void main( String[] args ) throws Exception {
        int numClasses = 10;
        //imageHeight,imageWidth,channels
        int imageHeight = 240;
        int imageWidth = 240;
        int channels = 3;
        IActivation activation = new ActivationIdentity();
        MultiLayerConfiguration multiLayerConfiguration = new NeuralNetConfiguration.Builder()
                .trainingWorkspaceMode(WorkspaceMode.NONE).inferenceWorkspaceMode(WorkspaceMode.NONE)
                .weightInit(WeightInit.XAVIER).seed(42)
                .activation(new ActivationELU())
                .updater(Nesterovs.builder()
                        .momentum(0.9)
                        .learningRateSchedule(new StepSchedule(
                                ScheduleType.EPOCH,
                                1e-2,
                                0.1,
                                20)).build()).list(
                        new Convolution2D.Builder().nOut(96)
                                .kernelSize(11,11).biasInit(0.0)
                                .stride(4,4).build(),
                        new ActivationLayer.Builder().activation(activation).build(),
                        new BatchNormalization.Builder().build(),
                        new LocalResponseNormalization.Builder()
                                .alpha(1e-3).beta(0.75).k(2)
                                .n(5).build(),
                        new Pooling2D.Builder()
                                .poolingType(SubsamplingLayer.PoolingType.MAX)
                                .kernelSize(3,3).stride(2,2)
                                .build(),
                        new Convolution2D.Builder().nOut(256)
                                .kernelSize(5,5).padding(2,2)
                                .biasInit(0.0)
                                .stride(1,1).build(),
                        new ActivationLayer.Builder().activation(activation).build(),
                        new BatchNormalization.Builder().build(),
                        new LocalResponseNormalization.Builder()
                                .alpha(1e-3).beta(0.75).k(2)
                                .n(5).build(),
                        new Pooling2D.Builder()
                                .poolingType(SubsamplingLayer.PoolingType.MAX)
                                .kernelSize(3,3).stride(2,2)
                                .build(),
                        new Convolution2D.Builder().nOut(384)
                                .kernelSize(3,3).padding(1,1)
                                .biasInit(0.0)
                                .stride(1,1).build(),
                        new ActivationLayer.Builder().activation(activation).build(),
                        new Convolution2D.Builder().nOut(256)
                                .kernelSize(3,3).padding(1,1)
                                .stride(1,1).build(),
                        new ActivationLayer.Builder().activation(activation).build(),
                        new BatchNormalization.Builder().build(),
                        new Pooling2D.Builder()
                                .poolingType(SubsamplingLayer.PoolingType.MAX)
                                .kernelSize(3,3).stride(2,2)
                                .build(),
                        new DenseLayer.Builder().dropOut(0.5)
                                .nOut(4096)
                                .biasInit(0.0)
                                .build(),
                        new ActivationLayer.Builder().activation(activation).build(),
                        new OutputLayer.Builder().activation(new ActivationSoftmax())
                                .lossFunction(new LossNegativeLogLikelihood())
                                .nOut(numClasses)
                                .biasInit(0.0)
                                .build())
                .setInputType(InputType.convolutionalFlat(imageHeight,imageWidth,channels))
                .build();

        MultiLayerNetwork network = new MultiLayerNetwork(multiLayerConfiguration);
        network.init();

        DataSetIterator dataSetIterator = new FileDataSetIterator(new File("savedData"), true, null, 32, (String[])null);

        //Initialize the user interface backend
        UIServer uiServer = UIServer.getInstance();

        File storageDir = new File("statsstorage");
        if(storageDir.exists()) {
            storageDir.delete();
        }


        //Configure where the network information (gradients, score vs. time etc) is to be stored. Here: store in memory.
        StatsStorage statsStorage = new FileStatsStorage(storageDir);         //Alternative: new FileStatsStorage(File), for saving and loading later

        //Attach the StatsStorage instance to the UI: this allows the contents of the StatsStorage to be visualized
        uiServer.attach(statsStorage);

        //Then add the StatsListener to collect this information from the network, as it trains
        network.setListeners(new StatsListener(statsStorage));

        for(int i = 0; i &lt; 100; i++)
            network.fit(dataSetIterator);
    }
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;public class GenerateData {

    public static void main(String[] args) {
        File f = new File("savedData");
        f.mkdir();

        Nd4j.getRandom().setSeed(12345);
        int numClasses = 10;
        int imageHeight = 240;
        int imageWidth = 240;
        int channels = 3;
        int numBatches = 64;
        for(int i=0; i&lt;numBatches; i++ ){
            INDArray labels = Nd4j.rand(32, numClasses);
            Nd4j.getExecutioner().exec(new IsMax(labels, 1));
            DataSet ds = new DataSet(Nd4j.rand(new int[]{32, channels, imageHeight, imageWidth}), labels);
            ds.save(new File(f, i + ".bin"));
        }

        System.out.println("Done");
    }

}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-05-07T04:17:38Z'>
		It's normal for the results from cuDNN to vary a bit. It's not deterministic, but the output should still be correct...
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-05-07T23:54:21Z'>
		Update on this:

Part of the inconsistency is likely the dropout in the config on one layer (that I missed)
Bug was found and fix for subsampling (introduced after 1.0.0-alpha)
Batch norm also had an issue (cudnn decay param is (1 - our decay param))

Still looking at some small LRN differences between our impl and cudnn.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-05-08T01:19:37Z'>
		Confirming that the cause here is LRN only: the above net with Conv+BN+Subsampling CuDNN layers is consistent run-to-run.
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-05-08T03:44:17Z'>
		Confirmed fixed as of this PR: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5086&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5086&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='AlexDBlack' date='2018-09-22T04:24:32Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>