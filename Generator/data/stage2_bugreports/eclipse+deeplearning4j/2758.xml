<bug id='2758' author='oliverdain' open_date='2017-01-27T02:48:14Z' closed_time='2017-01-27T21:37:09Z'>
	<summary>ParagraphVectors seems to ignore all but 1 label</summary>
	<description>
I'm training a ParagraphVectors model with some text that has multiple labels. Each label has a unique meaning. For example, each document might be labeled with both its ID and it's difficulty (higher difficulty means it's harder to read).
I've implemented this with a simple custom LabelAwareSentenceIterator. My currentLabels method always returns the 2 labels for the document though the API also requires that I implement the (singular) getLabel method. Via logging I can see that both are called even though getLabel (singular) is, I think, deprecated.
I then train a ParagraphVectors model (the following is Kotlin but should be fairly readable):
&lt;denchmark-code&gt;    val trainingDoc2Vec = ParagraphVectors.Builder()
            .iterate(docIter)
            .iterations(10)
            .windowSize(5)
            .layerSize(10)
            .tokenizerFactory(DefaultTokenizerFactory())
            .build()
&lt;/denchmark-code&gt;

That works fine. Now I get a new document and I want to do 4 things:

Figure out which document from the training set is closest to it (i.e. the document label that's closest)
Infer the embedding for the new document (i.e. the embedding for the document label)
Figure out which difficulty tag is closest to it
Infer the embedding for the difficulty tag for it

I can do (1) by calling nearestLabels and (2) by calling inferVector (I think, it's not entirely clear which embedding this is computing but it does return a single vector of the correct length), but (3) and (4) seem to be impossible.
Calling  nearestLabels never returns one of the values for the difficulty label. inferVector returns only a single vector which is, I assume, the embedding for the first label. Calling getWordVectorMatrix with any of the value for my difficulty label returns null though calling getWordVectorMatrix with one of the value for my document id label works.
It appears that any label beyond 1 is simply ignored. Is that correct?
	</description>
	<comments>
		<comment id='1' author='oliverdain' date='2017-01-27T13:51:13Z'>
		3 and 4 are impossible by design: ParaVec Inference just a training round, so it'll return you 1 vector for your input document/text. And then you can do whatever you want with it. I.e. compare with other vectors.
As for second label etc: please show your iterator implementation as gist.
		</comment>
		<comment id='2' author='oliverdain' date='2017-01-27T17:06:23Z'>
		Could you expand a bit on why (3) and (4) are impossible.. My understanding of the model is that if I have documents with only a single tag and specify layerSize == n then the hidden layer has 2*n units: n for the words and n for the document label. During inference you freeze the weights for the words and learn new weights between the n hidden units and the label input, thus inferring the embedding for the document label. If I were to specify 2 labels for my document I think there's two options:

3*n hidden units: n for the words, n for label 1, and n for label 2. Inference would freeze the weights for the words and learn weights for both labels giving me either two vectors or one vector of length 2*n
2*n hidden units with the units activated by  the sum (or average) of the weights for the two labels (as is common for the word embeddings for the CBOW model). In this case inference would learn a single vector whose meaning is confusing since it doesn't tell you the embedded for either label but only their sum and the two different embeddings can't be recovered from that.

As (2) seems less useful I'd expect you to be doing (1). I can't quite tell from your answer if you are doing (2), something else, or if my understanding is just incorrect. If you could clarify that would be very helpful.
I can post our iterator code but since it relies on some data structures that are proprietary to us I don't think that will be too helpful. I'll see if I can cook up a simplified example that demonstrates the problem.
		</comment>
		<comment id='3' author='oliverdain' date='2017-01-27T17:10:34Z'>
		Well, sure you can keep your impl private, i don't really care about it. The only thing important for me is: which interface you're implementing. You should be implementing LabelAwareIterator (please note absence of Sentence word in name). I can't find any reason in the code atm, that could explain behavior you've mentioned. So probably you're using something else, which was limited by legacy design.
		</comment>
		<comment id='4' author='oliverdain' date='2017-01-27T17:17:57Z'>
		As for inferrence: in paravec inference is the same process as training round, but your label is not affected by other training rounds. So all you have for any given inference round defined - is your existing embeddings for words &amp; hs/neg states. So, with any given number of attempts, the same words will give you the same output vector, if target inferenceVector will be initialized in the same way &amp; rng seed will be the same.
		</comment>
		<comment id='5' author='oliverdain' date='2017-01-27T17:19:02Z'>
		Btw, you're welcome to check sources: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/tree/328afc86a99810346c222674bd38c95b07f70e71/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/learning/impl/sequence&gt;https://github.com/deeplearning4j/deeplearning4j/tree/328afc86a99810346c222674bd38c95b07f70e71/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/learning/impl/sequence&lt;/denchmark-link&gt;

methods you're going to check is called inferVector() obviously
		</comment>
		<comment id='6' author='oliverdain' date='2017-01-27T18:11:28Z'>
		Aha, i've found issue with LabelAwareSentenceIterator. Fix is incoming.
		</comment>
		<comment id='7' author='oliverdain' date='2017-01-27T19:32:39Z'>
		Fix for original issue is applied here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/2766&gt;https://github.com/deeplearning4j/deeplearning4j/pull/2766&lt;/denchmark-link&gt;

LabelAwareSentenceIterator is now uses all labels you have. However, i'd still encourage you to use LabelAwareIterator.
		</comment>
		<comment id='8' author='oliverdain' date='2017-01-27T20:19:46Z'>
		Thanks for the fix. Will change from LabelAwareSentenceIterator to LabelAwareIterator iterator when I get a chance (other things higher on my list) and let you know how that goes.
For inference, I do understand that it's just training. My question is about the structure of the net that's being trained. There's 2 legal doc2vec representations you could use for documents that have two labels: one with 3*n nodes in the hidden layer: n for the words, n for label 1 and n for label 2. With that representation when you did inference/training you'd learn a unique embedding for each of the two labels (the result of inferVector would either be 2 vectors or a single vector of size 2*n) and could thus answer questions like my original (3) and (4).
On the other hand, if you have a hidden layer with 2*n hidden nodes and treat all of the labels like you would the words for a CBOW model you have only a single embedding and that embedding is the sum (or average) of the embeddings for each label. That works but is, I think, less useful as the inference step only gives you the average of what it's learned for the two labels and you then can't back out the embedding for each individual label.
As I understand it, you are using the 2nd structure (CBOW-like representation where the activation in the hidden layer is the sum/average of the embeddings for all labels applied to the document). Is that correct? Is there an easy way to build a net with the structure I'm suggesting (e.g. easier than using the graph API to build a custom net)?
Thanks!
		</comment>
		<comment id='9' author='oliverdain' date='2017-01-27T20:22:20Z'>
		Yep, we're using 2nd approach, based on original w2v-&gt;paravec impl.
		</comment>
		<comment id='10' author='oliverdain' date='2017-01-27T21:37:09Z'>
		Since original bug is fixed &amp; merged, i'm closing this issue now.
However, feel free to open new issue if you'll have any more questions.
P.S. Thanks for highlighting this issue.
		</comment>
		<comment id='11' author='oliverdain' date='2017-01-27T21:44:24Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 sounds good. Thanks for the fix and for explaining the implementation.
I'd like to add a "vote" for an alternative implementation with all labels broken out as described above but clearly that's a feature request and not a bug.
		</comment>
		<comment id='12' author='oliverdain' date='2017-01-27T21:45:35Z'>
		Sure, just file new issue, and please don't forget to provide link to the paper :)
		</comment>
		<comment id='13' author='oliverdain' date='2017-01-27T21:57:42Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 Sorry, one more question: looking at the API for  it appears that labels are provided by  and  appears to provide only 1 label for each document (e.g. it has a  method which returns a  label). So it appears that  doesn't support multiple labels/document at all. Am I missing something?
		</comment>
		<comment id='14' author='oliverdain' date='2017-01-27T22:08:05Z'>
		LabelsSource is for generating random labels only. It's used as storage-only if your documents have labels.
		</comment>
		<comment id='15' author='oliverdain' date='2017-01-28T00:14:44Z'>
		I changed to LabelAwareIterator and that did not fix it. Here's my LabelAwareIterator:
&lt;denchmark-code&gt;class TaggedDocumentIterator(val docColumn: Column&lt;List&lt;String&gt;?&gt;,
        val labelColumns: List&lt;Column&lt;String?&gt;&gt;)
    : SimpleLabelAwareIterator(toIterableOfLabelledDoc(docColumn, labelColumns)) {

    // The row that will be used on the next call to nextSentence
    private var nextRow = 0
    // The current row is the one before the next row
    private val curRow: Int
        get() = nextRow - 1

    companion object {
        private val log = LoggerFactory.getLogger(TaggedDocumentIterator::class.java)

        private fun toIterableOfLabelledDoc(
                docColumn: Column&lt;List&lt;String&gt;?&gt;, labelColumns: List&lt;Column&lt;String?&gt;&gt;): Iterable&lt;LabelledDocument&gt; {
            labelColumns.forEach {
                check(it.size == docColumn.size)
            }
            val labelledDocList = (0 until docColumn.size).map { idx -&gt;
                val labels = labelColumns.mapNotNull { it[idx] }
                val doc = docColumn[idx]
                // As per class comment, we have to concatenate all the text for this to work
                val docStr = doc?.joinToString(" ") ?: ""
                DocWithLabels(docStr, labels)
            }
            return labelledDocList
        }
    }

    class DocWithLabels(content: String, labels: List&lt;String&gt;) : LabelledDocument() {
        init {
            this.content = content
            labels.forEach { this.addLabel(it) }
        }
    }
}
&lt;/denchmark-code&gt;

that was built by following the code for FileLabelAwareIterator. (the above is Kotlin but is hopefully easy enough to read). The DataSet class is ours but it's basically a matrix. Here's a test that shows that the iterator works:
&lt;denchmark-code&gt;@Test
    fun testWithManyLabels() {
        val docs = listOf(
                listOf("foo", "bar", "baz"),
                listOf("a", "b", "c", "d", "e", "f"),
                listOf("AnalyticSpot", "makes", "awesome", "software")
        )
        val docCol = ColumnId.create&lt;List&lt;String&gt;&gt;("doc")

        val labels1 = listOf("X", "Y", "Z")
        val label1Col = ColumnId.create&lt;String&gt;("label1")

        val labels2 = listOf("be", "bop", "bazzle")
        val label2Col = ColumnId.create&lt;String&gt;("label2")

        val labels3 = listOf("bi", "bim", "bap")
        val label3Col = ColumnId.create&lt;String&gt;("label3")

        val ds = DataSet.build {
            addColumn(docCol, docs)
            addColumn(label1Col, labels1)
            addColumn(label2Col, labels2)
            addColumn(label3Col, labels3)
        }

        val iter = TaggedDocumentIterator(ds.column(docCol),
                listOf(ds.column(label1Col), ds.column(label2Col), ds.column(label3Col)))

        assertThat(iter.hasNext()).isTrue()
        var doc = iter.nextDocument()
        assertThat(doc.content).isEqualTo(docs[0].joinToString(" "))
        assertThat(doc.labels).containsExactly(labels1[0], labels2[0], labels3[0])

        assertThat(iter.hasNext()).isTrue()
        doc = iter.nextDocument()
        assertThat(doc.content).isEqualTo(docs[1].joinToString(" "))
        assertThat(doc.labels).containsExactly(labels1[1], labels2[1], labels3[1])

        assertThat(iter.hasNext()).isTrue()
        doc = iter.nextDocument()
        assertThat(doc.content).isEqualTo(docs[2].joinToString(" "))
        assertThat(doc.labels).containsExactly(labels1[2], labels2[2], labels3[2])

        assertThat(iter.hasNext()).isFalse()
    }
&lt;/denchmark-code&gt;

But, as before, all labels but the first come back as null. That is, after training is successful I can call doc2Vec.getWordVectorMatrix(label) passing a value for the 1st label and I get a result but if I pass a value for the 2nd label I get null back. Perhaps theres a bug with LabelAwareDocumentIterator as well?
		</comment>
		<comment id='16' author='oliverdain' date='2019-01-19T18:26:33Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>