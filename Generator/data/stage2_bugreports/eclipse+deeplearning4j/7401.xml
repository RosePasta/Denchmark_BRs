<bug id='7401' author='rnett' open_date='2019-03-29T23:55:05Z' closed_time='2019-04-05T00:48:02Z'>
	<summary>SameDiff squaredNorm backprop fails</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

SameDiff squaredNorm backprop fails under certain circumstances.  Haven't been able to reproduce in isolation.
Specifically, the input array looks like [64,8,8], and the call looks like squaredNorm(x, true, 2).
Stacktrace:
[main] ERROR org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner - Failed to execute op reduce_sqnorm_bp. Attempted to execute with 2 inputs, 1 outputs, 1 targs and 1 iargs. Inputs: [(FLOAT,[64,8,8],c), (FLOAT,[64,8,1],c)]. Outputs: [(FLOAT,[64,8,8],c)]. tArgs: [1.0]. iArgs: [2] - Please see above message (printed out from c++) for a possible cause of error.
java.lang.RuntimeException: NDArray::operator*=: the shapes of this and other arrays are not suitable for broadcast operation !
(There isn't a c++ error message above)
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version: 1.0.0-SNAPSHOT
platform information (OS, etc): Windows 10
CPU Backend

	</description>
	<comments>
		<comment id='1' author='rnett' date='2019-03-30T00:26:29Z'>
		To reproduce:
@Test
public void test4() {
    SameDiff SD = SameDiff.create();
    SDVariable in = SD.zero("test", 64, 8, 8);
    SDVariable squaredNorm = SD.squaredNorm("testOut", in, true, 2);

    SDVariable scale = SD.math.sqrt(squaredNorm.plus(1e-5));

    SDVariable out = in.times(scale).div("out", scale.times(squaredNorm.plus(1)));

    System.out.println(Arrays.toString(out.eval().shape()));

    SD.execBackwards(null, Lists.asList("test", new String[0]));
    SD.grad("test").eval(); // &lt;-- fails here
}
Stacktrace looks like:
[64, 8, 8]
Can't assign new value to the array: this shape [64, 8, 8]; other shape: [64, 8, 1]
[main] ERROR org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner - Failed to execute op divide_bp. Attempted to execute with 3 inputs, 2 outputs, 0 targs and 0 iargs. Inputs: [(FLOAT,[64,8,8],c), (FLOAT,[64,8,1],c), (FLOAT,[],c)]. Outputs: [(FLOAT,[64,8,8],c), (FLOAT,[64,8,1],c)]. tArgs: -. iArgs: - - Please see above message (printed out from c++) for a possible cause of error.

java.lang.RuntimeException: Lengths of arrays are mismatched

	at org.nd4j.nativeblas.Nd4jCpu$NativeOps.execCustomOp(Native Method)
	at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1679)
	at org.nd4j.autodiff.samediff.internal.InferenceSession.getOutputs(InferenceSession.java:449)
	at org.nd4j.autodiff.samediff.internal.InferenceSession.getOutputs(InferenceSession.java:37)
	at org.nd4j.autodiff.samediff.internal.AbstractSession.output(AbstractSession.java:250)
	at org.nd4j.autodiff.samediff.SameDiff.exec(SameDiff.java:3357)
	at org.nd4j.autodiff.samediff.SDVariable.eval(SDVariable.java:1773)
	at layers.IssueTest.test4(IssueTest.java:27)
		</comment>
		<comment id='2' author='rnett' date='2019-03-30T00:40:45Z'>
		Make sure you pull the latest snapshots (or master, if you are building locally)
There were some broadcast fixes yesterday for issues like this - it might make been fixed already.
		</comment>
		<comment id='3' author='rnett' date='2019-03-30T01:29:03Z'>
		Yeah, I'm up to date, assuming the fixes have been pushed to the snapshot build.
		</comment>
		<comment id='4' author='rnett' date='2019-03-30T07:58:43Z'>
		OK, so there's actually 2 separate problems here. (Both present on master)
First: reduce_sqnorm_bp is definitely broken for along dimension case - cc &lt;denchmark-link:https://github.com/shyrma&gt;@shyrma&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;CustomOp op = DynamicCustomOp.builder("reduce_sqnorm_bp")
                        .addInputs(Nd4j.create(DataType.FLOAT, 8, 6, 4), Nd4j.create(DataType.FLOAT, 8, 6, 1))
                        .addOutputs(Nd4j.create(DataType.FLOAT, 8, 6, 4))
                        .addBooleanArguments(true)  //Keep dim = true
                        .addIntegerArguments(2) //Dimension
                        .build();
                Nd4j.exec(op);
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;java.lang.RuntimeException: NDArray::operator*=: the shapes of this and other arrays are not suitable for broadcast operation !
&lt;/denchmark-code&gt;

Second: the problem stems from how SameDiff currently handles loss functions. Specifically, it is assumed that the final output of the network represents the loss function value - i.e., it should be a scalar. In this particular network, the output is a rank 3 array.
I'm not currently happy with this design... clearly, we need a loss function which must be a scalar, but the whole "last output must be scalar and is interpreted as the loss value" output thing should be improved. At the very least, we should provide some sort of check/warning
Anyway, if you change squared norm to norm2 (to work around first issue) and add an out = out.mean(); to work around the second issue, the provided code runs fine.
		</comment>
		<comment id='5' author='rnett' date='2019-04-02T09:28:32Z'>
		NDArray::operator *= cause an exception.
		</comment>
		<comment id='6' author='rnett' date='2019-04-04T17:43:20Z'>
		Ok, fixed, you can to test and close it when not reproducible.
		</comment>
		<comment id='7' author='rnett' date='2019-04-05T00:48:02Z'>
		Confirmed passing now - thanks &lt;denchmark-link:https://github.com/shugeo&gt;@shugeo&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='rnett' date='2019-05-05T01:06:01Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>