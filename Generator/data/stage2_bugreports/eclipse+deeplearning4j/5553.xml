<bug id='5553' author='AlexDBlack' open_date='2018-06-11T04:32:48Z' closed_time='2018-06-11T23:35:28Z'>
	<summary>DL4J: CuDNN batch norm fails with FP16</summary>
	<description>
Model: RESNET50. OK on FP32, fails on FP16.
CUDA 9.1, V100
&lt;denchmark-code&gt;Exception in thread "main" java.lang.RuntimeException: cuDNN status = 3: CUDNN_STATUS_BAD_PARAM
        at org.deeplearning4j.nn.layers.BaseCudnnHelper.checkCudnn(BaseCudnnHelper.java:45)
        at org.deeplearning4j.nn.layers.normalization.CudnnBatchNormalizationHelper.preOutput(CudnnBatchNormalizationHelper.java:233)
        at org.deeplearning4j.nn.layers.normalization.BatchNormalization.preOutput(BatchNormalization.java:272)
        at org.deeplearning4j.nn.layers.normalization.BatchNormalization.activate(BatchNormalization.java:235)
        at org.deeplearning4j.nn.graph.vertex.impl.LayerVertex.doForward(LayerVertex.java:105)
        at org.deeplearning4j.nn.graph.ComputationGraph.ffToLayerActivationsInWS(ComputationGraph.java:2055)
        at org.deeplearning4j.nn.graph.ComputationGraph.computeGradientAndScore(ComputationGraph.java:1403)
        at org.deeplearning4j.nn.graph.ComputationGraph.computeGradientAndScore(ComputationGraph.java:1372)
        at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:162)
        at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:53)
        at org.deeplearning4j.optimize.Solver.optimize(Solver.java:54)
        at org.deeplearning4j.nn.graph.ComputationGraph.fitHelper(ComputationGraph.java:1195)
        at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:1144)
        at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:1112)
        at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:1050)
        at org.deeplearning4j.benchmarks.BaseBenchmark.benchmark(BaseBenchmark.java:103)
        at org.deeplearning4j.benchmarks.BaseBenchmark$Benchmark.execute(BaseBenchmark.java:40)
        at org.deeplearning4j.benchmarks.BenchmarkCnn.run(BenchmarkCnn.java:109)
        at org.deeplearning4j.benchmarks.BenchmarkCnn.main(BenchmarkCnn.java:124)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-06-11T04:45:27Z'>
		According to cuDNN's documentation

CUDNN_STATUS_BAD_PARAM
At least one of the following conditions are met:

One of the pointers alpha, beta, x, y, bnScaleData, bnBiasData, estimatedMean, estimatedInvVariance is NULL
Number of xDesc or yDesc tensor descriptor dimensions is not within the [4,5] range.
bnScaleBiasMeanVarDesc dimensions are not 1xC(x1)x1x1 for spatial or 1xC(xD)xHxW for per-activation mode (parenthesis for 5D).
epsilon value is less than CUDNN_BN_MIN_EPSILON
Dimensions or data types mismatch for xDesc, yDesc


Are there any of those that might be happening only with FP16 that you can think of?
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-06-11T05:03:31Z'>
		I did check the docs, that was my first thought. (1), (2) and (3) would all fail regardless of the data type.
epsilon is 1e-3 in that model (CUDNN_BN_MIN_EPSILON is 1e-5), so should be OK
xDesc and yDesc should match each other (again, wouldn't be data type specific)
So I'm thinking at this point that maybe BN doesn't support FP16 for all args?
Google search turned this up:
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/520#issuecomment-277741834&gt;pytorch/pytorch#520 (comment)&lt;/denchmark-link&gt;


it still requires fp32 for parameters

However, I haven't yet found any official confirmation of this...
Edit: If this is the case, we can possibly do a conversion of the mean/variance args (FP16 -&gt; FP32 -&gt; FP16).
Edit 2: The user apparently works for NVIDIA - that might be as close to an official confirmation as we can get. Search is still turning up nothing, and the CuDNN docs are not helpful either way.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-06-11T05:59:28Z'>
		Ah, this is in the docs for cudnnBatchNormalizationBackward():

bnScaleBiasDiffDesc
Shared tensor descriptor for all the 5 tensors below in the argument list (bnScale, resultBnScaleDiff, resultBnBiasDiff, savedMean, savedInvVariance). The dimensions for this tensor descriptor are dependent on normalization mode. Note: The data type of this tensor descriptor must be 'float' for FP16 and FP32 input tensors, and 'double' for FP64 input tensors.

		</comment>
		<comment id='4' author='AlexDBlack' date='2018-06-11T06:07:15Z'>
		There are functions for such data conversion scattered around, such as here:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/linalg/jcublas/JCublasNDArrayFactory.java#L1393&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/linalg/jcublas/JCublasNDArrayFactory.java#L1393&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 would know best what to call though :)
		</comment>
		<comment id='5' author='AlexDBlack' date='2018-06-11T06:55:16Z'>
		OK, so there's this handy  method that calls the linked methods.
For far it hasn't exploded using that; so I'll use that for now, and &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 can review tomorrow :)
		</comment>
		<comment id='6' author='AlexDBlack' date='2018-09-21T20:59:31Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>