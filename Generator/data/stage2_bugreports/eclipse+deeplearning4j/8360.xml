<bug id='8360' author='longzhendong' open_date='2019-11-06T07:07:26Z' closed_time='2019-11-07T12:36:16Z'>
	<summary>Op [reduce_sum_bp] execution failed  bug</summary>
	<description>
&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/8233&gt;#8233&lt;/denchmark-link&gt;
 ## Version Information
Please indicate relevant versions, including, if relevant:

1.0.0-beta5
win10
CUDA version,  no used

Code:
public static void main(String[] args) {
&lt;denchmark-code&gt;	SameDiff sd = SameDiff.create();
	SDVariable label = sd.var("label", DataType.FLOAT, new int[] { 3, 3, 4 });

	SDVariable m = sd.math().logSumExp(label, 1);
	sd.setLossVariables(m);

	ExternalErrorsFunction fn = sd.f().externalErrors(m);
	fn.outputVariable();
	System.out.println(sd.summary());
	if (!sd.hasGradientFunction()) {
		sd.createGradFunction("label");
	}
	sd.assignArray(Nd4j.rand(new int[] { 3, 3, 4 }), label);
	sd.execBackwards(null);

}
&lt;/denchmark-code&gt;

Exception:
Mismatched shape: [2, 3, 4, 4, 1, 8192, 1, 99]
Shape requested: : {1, 1, 1}
o.n.l.c.n.o.NativeOpExecutioner - Failed to execute op reduce_sum_bp. Attempted to execute with 2 inputs, 1 outputs, 1 targs,0 bargs and 0 iargs. Inputs: [(FLOAT,[3,3,4],c), (FLOAT,[3,4],c)]. Outputs: [(FLOAT,[3,3,4],c)]. tArgs: [0.0]. iArgs: -. bArgs: -. Op own name: "reduce_sum_bp_1". Input var names: [label, divide]. Output var names: [reduce_sum_bp_1] - Please see above message (printed out from c++) for a possible cause of error.
Exception in thread "main" java.lang.RuntimeException: Op [reduce_sum_bp] execution failed
at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1710)
at org.nd4j.autodiff.samediff.internal.InferenceSession.getOutputsHelper(InferenceSession.java:505)
at org.nd4j.autodiff.samediff.internal.InferenceSession.getOutputs(InferenceSession.java:119)
at org.nd4j.autodiff.samediff.internal.InferenceSession.getOutputs(InferenceSession.java:56)
at org.nd4j.autodiff.samediff.internal.AbstractSession.output(AbstractSession.java:335)
at org.nd4j.autodiff.samediff.SameDiff.directExecHelper(SameDiff.java:3181)
at org.nd4j.autodiff.samediff.SameDiff.execBackwards(SameDiff.java:4693)
at org.nd4j.autodiff.samediff.SameDiff.execBackwards(SameDiff.java:4629)
at org.nd4j.autodiff.samediff.SameDiff.execBackwards(SameDiff.java:4587)
at org.nd4j.autodiff.samediff.SameDiff.execBackwards(SameDiff.java:4596)
at org.deeplearning4j.examples.LN6.main(LN6.java:66)
Caused by: java.lang.RuntimeException: NDArray::reshapei: bad input shape!
at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:2006)
at org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(NativeOpExecutioner.java:1700)
... 10 more
	</description>
	<comments>
		<comment id='1' author='longzhendong' date='2019-11-06T07:07:47Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='longzhendong' date='2019-11-06T08:00:40Z'>
		What are you trying to accomplish here? i.e., what is your goal?
		</comment>
		<comment id='3' author='longzhendong' date='2019-11-06T08:07:32Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 The derivative of input to logSumExp, used for the implementation of CRF layer
		</comment>
		<comment id='4' author='longzhendong' date='2019-11-06T09:23:54Z'>
		But you're using an ExternalErrorsFunction... That's only used when you have gradients that are not coming from SameDiff...
So to be clear, your "full" network is something along the lines of:
SameDiff(label -&gt; logsumexp) -&gt; NotSameDiff(CRF)?
And you have gradient calculation that you want to pass in to SameDiff from the CRF implementation?
		</comment>
		<comment id='5' author='longzhendong' date='2019-11-06T09:31:21Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 "Label" is just a variable name, and the problem here is to compute the gradient to report an error
		</comment>
		<comment id='6' author='longzhendong' date='2019-11-07T12:06:55Z'>
		Ok, so this in unrelated to external errors function, it's bad LogSumExp (manually implemented) backprop here:
&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/custom/LogSumExp.java#L75-L85&gt;https://github.com/eclipse/deeplearning4j/blob/master/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/custom/LogSumExp.java#L75-L85&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='longzhendong' date='2019-11-07T12:14:26Z'>
		mmm, this op definitely needs C++ implementation. cc &lt;denchmark-link:https://github.com/shugeo&gt;@shugeo&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='longzhendong' date='2019-11-07T12:36:39Z'>
		Turns out cause was trivial mistake in constructor
&lt;denchmark-link:https://github.com/KonduitAI/deeplearning4j/pull/35/files&gt;https://github.com/KonduitAI/deeplearning4j/pull/35/files&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='longzhendong' date='2019-11-07T12:38:11Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 LogSumExp is on the list here btw: &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/8099&gt;#8099&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>