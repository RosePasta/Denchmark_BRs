<bug id='5419' author='AlexDBlack' open_date='2018-06-01T04:42:24Z' closed_time='2019-05-22T13:59:49Z'>
	<summary>SameDiff: Cache not invalidated by in-place INDArray modifications, later outputs incorrect</summary>
	<description>
This started today, was fine yesterday.
First exec is OK, subsequent ones are returning values from the original execution.
&lt;denchmark-code&gt;    @Test
    public void testOutMultiple(){

        SameDiff sd = SameDiff.create();

        SDVariable var = sd.var("in", Nd4j.linspace(1,5,5));
        SDVariable out = var.add(10);

        INDArray result = sd.execAndEndResult();

        assertEquals(Nd4j.linspace(1,5,5).addi(10), result);

        INDArray origArr = var.getArr();
        origArr.addi(100.0);

        result = sd.execAndEndResult();

        assertEquals(Nd4j.linspace(1,5,5).addi(110), result); //Fails here
    }
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;java.lang.AssertionError:
Expected :[[  111.0000,  112.0000,  113.0000,  114.0000,  115.0000]]
Actual   :[[   11.0000,   12.0000,   13.0000,   14.0000,   15.0000]]
&lt;/denchmark-code&gt;

The most likely cause is the changes here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5410&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5410&lt;/denchmark-link&gt;

(Of course, I could just revert that, but it should be possible to execute without duping the whole samediff instance)
Aha! Link: &lt;denchmark-link:https://skymindai.aha.io/features/ND4J-172&gt;https://skymindai.aha.io/features/ND4J-172&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-06-01T04:53:49Z'>
		cc &lt;denchmark-link:https://github.com/farizrahman4u&gt;@farizrahman4u&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='AlexDBlack' date='2018-06-01T05:00:44Z'>
		On it.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-06-01T05:02:46Z'>
		
but it should be possible to execute without duping the whole samediff instance

It's exactly the idea, we've discussed that with &lt;denchmark-link:https://github.com/farizrahman4u&gt;@farizrahman4u&lt;/denchmark-link&gt;
 few times. And problem here - we actually need that cache in some legit use cases. So we need to distinguish these cases somehow. And do that reliably.
Is it time to bring Sessions in? :)
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-06-01T05:25:44Z'>
		I might be wrong about the cause: just tested, confirmed it's NOT passing as of: &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/commit/aa65035755ec3b61e8f41d812c93e0b848241214&gt;aa65035&lt;/denchmark-link&gt;
 (Merge pull request &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/5409&gt;#5409&lt;/denchmark-link&gt;
 from deeplearning4j/fr_fix_gather_args ) which is the one before the PR I mentioned.
Edit: and IS passing as of: &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/commit/53583a7715db08625516d802f6df4fa1e7f801a5&gt;53583a7&lt;/denchmark-link&gt;
 (Merge pull request &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/5399&gt;#5399&lt;/denchmark-link&gt;
 from deeplearning4j/ab_5398_darknet_labels )
		</comment>
		<comment id='5' author='AlexDBlack' date='2018-06-01T05:32:36Z'>
		Confirmed cause: &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/commit/faf27a37c8959e7c21ee92f531f14829e1ae7270&gt;faf27a3&lt;/denchmark-link&gt;
 (Merge pull request &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/5373&gt;#5373&lt;/denchmark-link&gt;
 from deeplearning4j/fr_fix_double_spending)
		</comment>
		<comment id='6' author='AlexDBlack' date='2018-06-01T05:35:52Z'>
		This is the fix:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5421&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5421&lt;/denchmark-link&gt;

Issue is this brings back double spending issue when final op has multiple outputs.
Possible solution: instead of calling eval() on each output variable, do a single sd.exec() followed by getArr() on each of the output variables.
Unless there is a way to **  know **, when contents of an array change (through inplace ops, views etc), this is the best shot.
		</comment>
		<comment id='7' author='AlexDBlack' date='2018-06-01T05:49:18Z'>
		I guess the question is why we have the SDVariable.eval() method in the first place, and what the semantics should be there. The Javadoc is non-specific:

Evaluate the result of this variable

I don't think tracking array value changes (without changing the INDArray object associated with a SDVariable) is a good solution here.
So yeah, I think removing SDVariable.exec(), and having users use getArr() is the better solution here.
If need-be we can add a method for efficient partial graph execution in the future.
		</comment>
		<comment id='8' author='AlexDBlack' date='2018-06-01T05:51:59Z'>
		Lets keep SDVariable.eval(), which is basically sd.exec() + SDVariable.getArr(). It makes sense for single output variables.
		</comment>
		<comment id='9' author='AlexDBlack' date='2018-06-01T05:53:11Z'>
		variable.eval() is basically mimic of TF syntax, so everyone out there will be demanding it. so yes, we need it.
		</comment>
		<comment id='10' author='AlexDBlack' date='2018-06-01T05:53:50Z'>
		p.s. no, i'm not fan of eval(). i just think users are.
		</comment>
		<comment id='11' author='AlexDBlack' date='2018-06-01T06:01:17Z'>
		oops. deleted by mistake.
		</comment>
		<comment id='12' author='AlexDBlack' date='2018-06-01T06:03:11Z'>
		(For reference: question was "What's the TF equivalent of this situation?")
So yeah... the alternative is keep double spending, and properly document it.
So SDVariable.exec() + getArr() would be used for multiple outputs, or SameDiff.exec() + getArr()
Edit: though I guess that wouldn't match TF semantics either, I assume? (I don't know TF well enough :))
		</comment>
		<comment id='13' author='AlexDBlack' date='2018-06-01T06:03:39Z'>
		TF is different - you can't manipulate a tf.Variable from numpy space.
		</comment>
		<comment id='14' author='AlexDBlack' date='2018-06-01T06:11:27Z'>
		OK, so to summarize the options as I see them:

Rejected: Remove SDVariable.exec()
Keep double spending, document it. Have users use getArr() for all other outputs... Definitely a performance trap for users, but at least results are correct
Keep cache, require a manual "invalidate" step if you do getArr().someInplaceOp()... a usability trap for users, not great
Array change tracking: https://3.basecamp.com/3684163/buckets/3725554/messages/1106912583

&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 what would be the performance overhead using your counters/timer? I mean we already have that sort of thing for CUDA...
Edit: basically the way I would imagine this working is:
(a) SameDiff instance records the array logical counter at execution time
(b) Certain ops (replacing an INDArray for example) marks cache as invalid
(c) At next exec() time, we check if cache is still valid (stored vs. actual array counters). Then either invalidate or skip exec + return from cache
Edit 2: on reflection, this change tracking approach is certainly the best for users - least likely to lead to surprises, and less to learn. So it's just implementation complexity and performance overhead that are the downsides (though performance overhead should be minimal)...
		</comment>
		<comment id='15' author='AlexDBlack' date='2019-05-22T13:59:49Z'>
		Outdated - no longer relevant.
		</comment>
	</comments>
</bug>