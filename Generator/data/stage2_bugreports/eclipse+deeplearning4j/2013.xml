<bug id='2013' author='KayMKM' open_date='2016-08-17T13:08:47Z' closed_time='2016-08-24T09:55:09Z'>
	<summary>The function of Word2vec writeFullModel() throw the exception: java.lang.ArrayIndexOutOfBoundsException: 40</summary>
	<description>
I was using the writeFullModel() to save the Word2vec model, and there is an exception thrown:
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 40
at org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.listToArray(VocabularyHolder.java:179)
at org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.buildNode(VocabularyHolder.java:89)
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.writeFullModel(WordVectorSerializer.java:722)
at org.deeplearning4j.examples.TrainWord2vec.main(TrainWord2vec.java:83)
It seems that the size of haffman code exceeds 40.
And the vocab size is about 300000.
	</description>
	<comments>
		<comment id='1' author='KayMKM' date='2016-08-23T21:40:00Z'>
		Can i see source code with w2v configuration, and _full_ output log?
		</comment>
		<comment id='2' author='KayMKM' date='2016-08-24T03:01:24Z'>
		&lt;denchmark-code&gt;public static void main(String[] args) throws Exception {

    Word2Vec vec = new Word2Vec();

    for(int i = 0; i &lt; CORPUS_NUMBER; i++) {
        System.out.println("corpus " + String.valueOf(i + 1) + ":");
        String fileName = "corpus\\corpus_" + String.valueOf(i + 1) + "_word.cch";
        String filePath = new ClassPathResource(fileName).getFile().getAbsolutePath();
        log.info("Load &amp; Vectorize Sentences....");
        // Strip white space before and after for each line
        SentenceIterator iter = new BasicLineIterator(filePath);
        // Split on white spaces in the line to get words
        TokenizerFactory t = new DefaultTokenizerFactory();
        t.setTokenPreProcessor(new CommonPreprocessor());

        if(i == 0) {
            log.info("Building model....");
            vec = new Word2Vec.Builder()
                    .minWordFrequency(1)
                    .iterations(1)
                    .layerSize(200)
                    .seed(42)
                    .windowSize(5)
                    .iterate(iter)
                    .tokenizerFactory(t)
                    .build();
        }
        else {
            vec.setSentenceIter(iter);
        }

        log.info("Fitting Word2Vec model....");
        vec.fit();

        log.info("Writing word vectors to full model....");
        WordVectorSerializer.writeFullModel(vec, "\\word2vecModel\\word\\pathToWriteto200_" + String.valueOf(i + 1) + ".txt");

    }

    //UiServer server = UiServer.getInstance();
    //System.out.println("Started on port " + server.getPort());
}
&lt;/denchmark-code&gt;

output log:
corpus 1:
o.d.e.i.CreateWord2vec200 - Load &amp; Vectorize Sentences....
o.d.e.i.CreateWord2vec200 - Building model....
o.d.e.i.CreateWord2vec200 - Fitting Word2Vec model....
o.d.m.s.SequenceVectors - Starting vocabulary building...
o.d.m.w.w.VocabConstructor - Sequences checked: [100000], Current vocabulary size: [55602]
o.d.m.w.w.VocabConstructor - Sequences checked: [200000], Current vocabulary size: [82039]
o.d.m.w.w.VocabConstructor - Sequences checked: [300000], Current vocabulary size: [101484]
o.d.m.w.w.VocabConstructor - Sequences checked: [400000], Current vocabulary size: [114745]
o.d.m.w.w.VocabConstructor - Sequences checked: [500000], Current vocabulary size: [124670]
o.d.m.w.w.VocabConstructor - Sequences checked: [600000], Current vocabulary size: [133678]
o.d.m.w.w.VocabConstructor - Sequences checked: [700000], Current vocabulary size: [141663]
o.d.m.w.w.VocabConstructor - Sequences checked: [800000], Current vocabulary size: [148476]
o.d.m.w.w.VocabConstructor - Sequences checked: [900000], Current vocabulary size: [154257]
o.d.m.w.w.VocabConstructor - Sequences checked: [1000000], Current vocabulary size: [158973]
o.d.m.w.w.VocabConstructor - Sequences checked: [1100000], Current vocabulary size: [164598]
o.d.m.w.w.VocabConstructor - Sequences checked: [1200000], Current vocabulary size: [169631]
o.d.m.w.w.VocabConstructor - Sequences checked: [1300000], Current vocabulary size: [173485]
o.d.m.w.w.VocabConstructor - Sequences checked: [1400000], Current vocabulary size: [177261]
o.d.m.w.w.VocabConstructor - Sequences checked: [1500000], Current vocabulary size: [180600]
o.d.m.w.w.VocabConstructor - Sequences checked: [1600000], Current vocabulary size: [184151]
o.d.m.w.w.VocabConstructor - Sequences checked: [1700000], Current vocabulary size: [187644]
o.d.m.w.w.VocabConstructor - Sequences checked: [1800000], Current vocabulary size: [190822]
o.d.m.w.w.VocabConstructor - Sequences checked: [1900000], Current vocabulary size: [193381]
o.d.m.w.w.VocabConstructor - Sequences checked: [2000000], Current vocabulary size: [195955]
o.d.m.w.w.VocabConstructor - Sequences checked: [2100000], Current vocabulary size: [198858]
o.d.m.w.w.VocabConstructor - Sequences checked: [2103788], Current vocabulary size: [199044]
o.d.m.s.SequenceVectors - Building learning algorithms:
o.d.m.s.SequenceVectors -           building ElementsLearningAlgorithm: [SkipGram]
o.d.m.s.SequenceVectors - Starting learning process...
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [464136];  Lines vectorized so far: [100000]; learningRate: [0.02386658765936875]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [964411];  Lines vectorized so far: [200000]; learningRate: [0.022644824489605336]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [1484368];  Lines vectorized so far: [300000]; learningRate: [0.02137500538486313]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [1975073];  Lines vectorized so far: [400000]; learningRate: [0.02017669140138218]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [2451115];  Lines vectorized so far: [500001]; learningRate: [0.01901414707333544]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [2915388];  Lines vectorized so far: [600000]; learningRate: [0.017880649258686286]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3413732];  Lines vectorized so far: [700000]; learningRate: [0.016663279453442988]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3887075];  Lines vectorized so far: [800000]; learningRate: [0.015507441172629286]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [4354992];  Lines vectorized so far: [900000]; learningRate: [0.014364890438427008]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [4810322];  Lines vectorized so far: [1000000]; learningRate: [0.013252868581304207]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [5296274];  Lines vectorized so far: [1100000]; learningRate: [0.012066032537369972]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [5812159];  Lines vectorized so far: [1200000]; learningRate: [0.010806248082329356]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [6398301];  Lines vectorized so far: [1300000]; learningRate: [0.00937470480937389]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [6866588];  Lines vectorized so far: [1400000]; learningRate: [0.008231006281216943]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [7292039];  Lines vectorized so far: [1500000]; learningRate: [0.007192194141483824]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [7822236];  Lines vectorized so far: [1600000]; learningRate: [0.005897345802185381]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [8323656];  Lines vectorized so far: [1700000]; learningRate: [0.0046728743271119005]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [8780532];  Lines vectorized so far: [1800000]; learningRate: [0.0035573431510255576]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [9208903];  Lines vectorized so far: [1900000]; learningRate: [0.002510804160074179]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [9677770];  Lines vectorized so far: [2000000]; learningRate: [0.0013660579646692429]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [10218814];  Lines vectorized so far: [2100000]; learningRate: [1.0E-4]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [10237029];  Lines vectorized so far: [2103788]; learningRate: [1.0E-4]
o.d.e.i.CreateWord2vec200 - Writing word vectors to full model....
corpus 2:
o.d.e.i.CreateWord2vec200 - Load &amp; Vectorize Sentences....
o.d.e.i.CreateWord2vec200 - Fitting Word2Vec model....
o.d.m.s.SequenceVectors - Starting vocabulary building...
o.d.m.w.w.VocabConstructor - Sequences checked: [100000], Current vocabulary size: [55714]
o.d.m.w.w.VocabConstructor - Sequences checked: [200000], Current vocabulary size: [81714]
o.d.m.w.w.VocabConstructor - Sequences checked: [300000], Current vocabulary size: [101096]
o.d.m.w.w.VocabConstructor - Sequences checked: [400000], Current vocabulary size: [114385]
o.d.m.w.w.VocabConstructor - Sequences checked: [500000], Current vocabulary size: [124574]
o.d.m.w.w.VocabConstructor - Sequences checked: [600000], Current vocabulary size: [133721]
o.d.m.w.w.VocabConstructor - Sequences checked: [700000], Current vocabulary size: [141550]
o.d.m.w.w.VocabConstructor - Sequences checked: [800000], Current vocabulary size: [148473]
o.d.m.w.w.VocabConstructor - Sequences checked: [900000], Current vocabulary size: [154309]
o.d.m.w.w.VocabConstructor - Sequences checked: [1000000], Current vocabulary size: [159155]
o.d.m.w.w.VocabConstructor - Sequences checked: [1100000], Current vocabulary size: [165002]
o.d.m.w.w.VocabConstructor - Sequences checked: [1200000], Current vocabulary size: [170055]
o.d.m.w.w.VocabConstructor - Sequences checked: [1300000], Current vocabulary size: [173734]
o.d.m.w.w.VocabConstructor - Sequences checked: [1400000], Current vocabulary size: [177505]
o.d.m.w.w.VocabConstructor - Sequences checked: [1500000], Current vocabulary size: [180998]
o.d.m.w.w.VocabConstructor - Sequences checked: [1600000], Current vocabulary size: [184453]
o.d.m.w.w.VocabConstructor - Sequences checked: [1700000], Current vocabulary size: [188174]
o.d.m.w.w.VocabConstructor - Sequences checked: [1800000], Current vocabulary size: [191190]
o.d.m.w.w.VocabConstructor - Sequences checked: [1900000], Current vocabulary size: [193617]
o.d.m.w.w.VocabConstructor - Sequences checked: [2000000], Current vocabulary size: [196193]
o.d.m.w.w.VocabConstructor - Sequences checked: [2100000], Current vocabulary size: [199192]
o.d.m.w.w.VocabConstructor - Sequences checked: [2102373], Current vocabulary size: [230046]
o.d.m.s.SequenceVectors - Building learning algorithms:
o.d.m.s.SequenceVectors -           building ElementsLearningAlgorithm: [SkipGram]
o.d.m.s.SequenceVectors - Starting learning process...
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [458706];  Lines vectorized so far: [100001]; learningRate: [0.024437475932808646]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [941856];  Lines vectorized so far: [200000]; learningRate: [0.023845059365288954]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [1459716];  Lines vectorized so far: [300000]; learningRate: [0.023209778728284825]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [1958088];  Lines vectorized so far: [400000]; learningRate: [0.022598386825438545]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [2429485];  Lines vectorized so far: [500000]; learningRate: [0.02202049704168255]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [2907800];  Lines vectorized so far: [600000]; learningRate: [0.021433934606120315]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3399539];  Lines vectorized so far: [700000]; learningRate: [0.020830453692405732]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3871432];  Lines vectorized so far: [800000]; learningRate: [0.02025184149429337]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [4343644];  Lines vectorized so far: [900001]; learningRate: [0.019672627079799044]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [4796386];  Lines vectorized so far: [1000000]; learningRate: [0.019117208184706903]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [5286368];  Lines vectorized so far: [1100000]; learningRate: [0.018516821755578252]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [5804380];  Lines vectorized so far: [1200000]; learningRate: [0.017880918051523132]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [6368634];  Lines vectorized so far: [1300000]; learningRate: [0.017188808302824717]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [6813828];  Lines vectorized so far: [1400000]; learningRate: [0.016642903445359646]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [7230385];  Lines vectorized so far: [1500000]; learningRate: [0.01613187930416068]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [7752621];  Lines vectorized so far: [1600001]; learningRate: [0.015491371281779916]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [8244951];  Lines vectorized so far: [1700000]; learningRate: [0.01488750033790349]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [8701170];  Lines vectorized so far: [1800000]; learningRate: [0.014328000844231326]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [9131327];  Lines vectorized so far: [1900000]; learningRate: [0.013800380796994994]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [9598024];  Lines vectorized so far: [2000000]; learningRate: [0.013228416282773143]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [10134411];  Lines vectorized so far: [2100000]; learningRate: [0.012570045369583997]
o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [10146010];  Lines vectorized so far: [2102373]; learningRate: [1.0E-4]
o.d.e.i.CreateWord2vec200 - Writing word vectors to full model....
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 40
at org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.listToArray(VocabularyHolder.java:179)
at org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.buildNode(VocabularyHolder.java:89)
at org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.writeFullModel(WordVectorSerializer.java:722)
at org.deeplearning4j.examples.TrainWord2vec.main(TrainWord2vec.java:81)
		</comment>
		<comment id='3' author='KayMKM' date='2016-08-24T07:55:34Z'>
		Aha, that explains a lot. Could you please tell me, what's the idea of training being applied iterative?
		</comment>
		<comment id='4' author='KayMKM' date='2016-08-24T08:27:34Z'>
		Sorry. I don't really understand... The training corpus is too large so I split it to 100 pieces. But I don't know how to train it alternately. I tried it by this way.
		</comment>
		<comment id='5' author='KayMKM' date='2016-08-24T08:35:23Z'>
		No, that's definitely wrong way. How large is your corpus, in gigabytes?
		</comment>
		<comment id='6' author='KayMKM' date='2016-08-24T08:38:06Z'>
		About 6GB.
		</comment>
		<comment id='7' author='KayMKM' date='2016-08-24T08:52:52Z'>
		That's small corpus, don't split it into chunks :)
		</comment>
		<comment id='8' author='KayMKM' date='2016-08-24T08:54:53Z'>
		For standalone w2v  corpus might be considered big somewhere around 50GB for consumer i7 cpu, and beyond 200GB+ for Xeon cpu
		</comment>
		<comment id='9' author='KayMKM' date='2016-08-24T08:57:13Z'>
		All right... I will try again. Thanks a lot!
		</comment>
		<comment id='10' author='KayMKM' date='2016-08-24T09:55:25Z'>
		Feel free to add new issue if you'll have any more problems there.
		</comment>
		<comment id='11' author='KayMKM' date='2019-01-20T20:52:54Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>