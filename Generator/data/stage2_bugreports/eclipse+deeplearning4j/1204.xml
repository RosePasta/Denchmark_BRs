<bug id='1204' author='ericson3000' open_date='2016-02-28T15:30:35Z' closed_time='2016-05-06T23:53:28Z'>
	<summary>Exception in GemmParams when using GravesLSTM and TruncatedBPTT</summary>
	<description>
The Matrix "c" is not in c-order it is in "f"-order. Stack trace is as follows:
Exception in thread "main" java.lang.IllegalArgumentException: c must be f order, offset 0 and have length == c.data.length at org.nd4j.linalg.api.blas.params.GemmParams.&lt;init&gt;(GemmParams.java:59) at org.nd4j.linalg.api.blas.impl.BaseLevel3.gemm(BaseLevel3.java:78) at org.nd4j.linalg.factory.Nd4j.gemm(Nd4j.java:713) at org.deeplearning4j.nn.layers.recurrent.GravesLSTM.activateHelper(GravesLSTM.java:410) at org.deeplearning4j.nn.layers.recurrent.GravesLSTM.rnnActivateUsingStoredState(GravesLSTM.java:533) at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.rnnActivateUsingStoredState(MultiLayerNetwork.java:2242) at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:1748) at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:132) at org.deeplearning4j.optimize.solvers.BaseOptimizer.optimize(BaseOptimizer.java:151) at org.deeplearning4j.optimize.Solver.optimize(Solver.java:52) at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.doTruncatedBPTT(MultiLayerNetwork.java:1299) at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1157) at eric.masterproject.dl4j.lstmView.LSTMAnalysis.trainAndTestModel(LSTMAnalysis.java:93) at eric.masterproject.dl4j.lstmView.LSTMAnalysis.main(LSTMAnalysis.java:42)
The following Configuration was used:
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() .learningRate(0.1) .iterations(20) .seed(123) .list(2) .layer(0, new GravesLSTM.Builder().nIn(FEATURES).nOut(LABELS) .updater(Updater.ADAGRAD) .activation("tanh").weightInit(WeightInit.UNIFORM) .dist(new UniformDistribution(-0.08, 0.08)) .build()) .layer(1, new RnnOutputLayer.Builder(LossFunction.XENT).activation("tanh")         .updater(Updater.ADAGRAD) .nIn(LABELS).nOut(LABELS).weightInit(WeightInit.UNIFORM) .dist(new UniformDistribution(-0.08, 0.08)) .build()) .backpropType(BackpropType.TruncatedBPTT) .tBPTTBackwardLength(10) .tBPTTForwardLength(10) .pretrain(false) .backprop(true) .build();
	</description>
	<comments>
		<comment id='1' author='ericson3000' date='2016-02-28T22:49:47Z'>
		Can you post the version you are using? Both DL4J and ND4J.
		</comment>
		<comment id='2' author='ericson3000' date='2016-03-05T18:22:58Z'>
		&lt;denchmark-code&gt; &lt;nd4j.version&gt;0.4-rc3.8&lt;/nd4j.version&gt;
    &lt;dl4j.version&gt;  0.4-rc3.8&lt;/dl4j.version&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='ericson3000' date='2016-03-05T22:12:31Z'>
		I haven't seen this in practice before (and I've used LSTMs a lot). Can you give me something I can run to reproduce this? The configuration above isn't enough.
		</comment>
		<comment id='4' author='ericson3000' date='2016-03-06T22:16:01Z'>
		package eric.masterproject.dl4j.lstmView;
import java.io.File;
import java.io.IOException;
import org.canova.api.records.reader.SequenceRecordReader;
import org.canova.api.records.reader.impl.CSVSequenceRecordReader;
import org.canova.api.split.FileSplit;
import org.deeplearning4j.datasets.canova.SequenceRecordReaderDataSetIterator;
import org.deeplearning4j.datasets.canova.SequenceRecordReaderDataSetIterator.AlignmentMode;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.nn.conf.BackpropType;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.distribution.UniformDistribution;
import org.deeplearning4j.nn.conf.layers.GravesLSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;
/**


LSTM Test
*/
public class LSTMAnalysis {
private final static int FEATURES = 5;
private final static int LABELS = 1;
private final static double TRAIN_TEST_RATIO = 0.7;
/**


@param args


@throws IOException


@throws InterruptedException
*/
public static void main(String[] args) throws IOException, InterruptedException {
final MultiLayerNetwork model = trainAndTestModel();




//      final ComplexChart2 charting = new ComplexChart2("LSTM/RNN-Model", model, samples);
//      charting.pack();
//      RefineryUtilities.centerFrameOnScreen(charting);
//      charting.setVisible(true);
}
&lt;denchmark-code&gt;/**
 * 
 * @param featuresAndLabels
 * @param timePoint
 */
private static MultiLayerNetwork trainAndTestModel() {

    // 1. setup the model configuration
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
&lt;/denchmark-code&gt;

//              .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
.learningRate(0.1)
.iterations(20)
//              .rmsDecay(0.95)
.seed(123)
//              .regularization(true).l2(0.01)
.list(2)
.layer(0, new GravesLSTM.Builder().nIn(FEATURES).nOut(LABELS)
.updater(Updater.ADAGRAD)
.activation("tanh").weightInit(WeightInit.UNIFORM)
.dist(new UniformDistribution(-0.08, 0.08))
.build())
.layer(1, new RnnOutputLayer.Builder(LossFunction.XENT).activation("tanh")        //MCXENT + softmax for classification
.updater(Updater.ADAGRAD)
.nIn(LABELS).nOut(LABELS).weightInit(WeightInit.UNIFORM)
.dist(new UniformDistribution(-0.08, 0.08))
.build())
//              .backpropType(BackpropType.TruncatedBPTT)
//              .tBPTTBackwardLength(FEATURES)
//              .tBPTTForwardLength(FEATURES)
.pretrain(false)
.backprop(true)
.build();
&lt;denchmark-code&gt;    // 3. instantiate the configurred network
    final MultiLayerNetwork model = new MultiLayerNetwork(conf);
    model.init();

    // 4. set up score listeners (optional)
&lt;/denchmark-code&gt;

//      model.setListeners(new ScoreIterationListener(10),
//      new HistogramIterationListener(10));
model.setListeners(new ScoreIterationListener(2));
&lt;denchmark-code&gt;    try {
        model.fit(getIterator());
    } catch (IOException | InterruptedException e) {
        // TODO Auto-generated catch block
        System.out.println("ERROR while creating the Data iterator!");
        e.printStackTrace();
    }
    return model;
}

/**
 * 
 * @return
 * @throws IOException
 * @throws InterruptedException
 */
private static DataSetIterator getIterator() throws IOException, InterruptedException{

    final File dataRootDir = new File("/Users/eric/Desktop/TestData/");
    final String[] fileFormats = {"csv"};
    final String dataSeperator = ",";

    //use 1 to skip the header
    final int firstDataRow = 1;

    //use 1 so each file is an DataSet, if there are 3 files in our root-dir and numMiniBatch is 3 
    //we will get one DataSet that contains all timeseries
    final int numMiniBatches = 1;

    final SequenceRecordReader featureReader = new CSVSequenceRecordReader(firstDataRow, dataSeperator);
    featureReader.initialize(new FileSplit(dataRootDir, fileFormats));

    //start the label-reader one row after feature-reader so the label represents the next timestep value
    final SequenceRecordReader lableReader = new CSVSequenceRecordReader(firstDataRow+1, dataSeperator);
    lableReader.initialize(new FileSplit(dataRootDir, fileFormats));

    //Because we do regression=true the number of Labels is not used and can be set to anything (here =0). 
    //AlignmentMode.ALIGN_START is needed because our label-series is one timestep shorter than our feature-series.
    //It will move the first label up to the first feature and fill missing trailing values with 0's
    final SequenceRecordReaderDataSetIterator iter =
            new SequenceRecordReaderDataSetIterator(featureReader, lableReader, numMiniBatches, 0, true, AlignmentMode.ALIGN_START);

    return iter;
}
&lt;/denchmark-code&gt;

}
		</comment>
		<comment id='5' author='ericson3000' date='2016-05-06T08:03:10Z'>
		Is this still the case?
		</comment>
		<comment id='6' author='ericson3000' date='2016-05-06T23:53:28Z'>
		Just ran a modified version of this locally on latest version. Not seeing any issues.
		</comment>
		<comment id='7' author='ericson3000' date='2019-01-21T03:53:02Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>