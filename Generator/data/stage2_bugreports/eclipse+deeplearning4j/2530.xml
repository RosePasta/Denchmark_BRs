<bug id='2530' author='maybecan' open_date='2016-12-18T08:11:28Z' closed_time='2018-04-25T10:44:53Z'>
	<summary>CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&amp;lt;unknown&amp;gt;) "result"</summary>
	<description>
I want to train DBN by spark+GPU, according to the spark example-MnistMLPExample, I just changed the MultiLayerConfiguration, as following:
&lt;denchmark-code&gt;MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(12345)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)
            .activation("leakyrelu")
            .weightInit(WeightInit.XAVIER)
            .learningRate(0.02)
            .updater(Updater.NESTEROVS).momentum(0.9)
            .regularization(true).l2(1e-4)
            .list()
            .layer(0, new RBM.Builder().nIn(28 * 28).nOut(500).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(1, new RBM.Builder().nIn(500).nOut(500).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(2, new RBM.Builder().nIn(500).nOut(250).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(3, new RBM.Builder().nIn(250).nOut(100).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(4, new RBM.Builder().nIn(100).nOut(30).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) //encoding stops
            .layer(5, new RBM.Builder().nIn(30).nOut(100).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) //decoding starts
            .layer(6, new RBM.Builder().nIn(100).nOut(250).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(7, new RBM.Builder().nIn(250).nOut(500).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(8, new RBM.Builder().nIn(500).nOut(1000).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(9, new OutputLayer.Builder(LossFunctions.LossFunction.RMSE_XENT).activation("softmax").nIn(1000).nOut(10).build())
            .pretrain(true).backprop(true)
            .build();
&lt;/denchmark-code&gt;

Then I got an error:
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77() "result"
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77() "result"
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77() "result"
Failed on [10122043392] -&gt; [9788597760], size: [640], direction: [3], result: [77]
Failed on [9788597760] -&gt; [10122027008], size: [640], direction: [3], result: [77]
Failed on [9815857664] -&gt; [35758252544], size: [2000], direction: [3], result: [77]
Failed on [11080302592] -&gt; [11077156864], size: [1568000], direction: [3], result: [77]
Failed on [34978004992] -&gt; [11073486848], size: [1568000], direction: [1], result: [77]
Failed on [9815908864] -&gt; [34984092160], size: [2000], direction: [3], result: [77]
Some relevant hardward/software specs
GPU: Tesla K20C
CUDA: 7.5
nvidia-smi:
NVIDIA-SMI 361.77                 Driver Version: 361.77                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GT 710      Off  | 0000:03:00.0     N/A |                  N/A |
| N/A   39C    P0    N/A /  N/A |    304MiB /  2001MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K20c          Off  | 0000:05:00.0     Off |                  Off |
| 30%   36C    P8    15W / 225W |      0MiB /  5062MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
	</description>
	<comments>
		<comment id='1' author='maybecan' date='2016-12-18T09:32:39Z'>
		I need full output log, and full source code.
		</comment>
		<comment id='2' author='maybecan' date='2016-12-18T09:57:31Z'>
		source code:
&lt;denchmark-code&gt;package org.deeplearning4j.mlp;

import com.beust.jcommander.JCommander;
import com.beust.jcommander.Parameter;
import com.beust.jcommander.ParameterException;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.RBM;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.parallelism.ParallelWrapper;
import org.deeplearning4j.spark.api.TrainingMaster;
import org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer;
import org.deeplearning4j.spark.impl.paramavg.ParameterAveragingTrainingMaster;
//import org.nd4j.jita.conf.CudaEnvironment;
//import org.nd4j.jita.conf.CudaEnvironment;
import org.nd4j.linalg.api.buffer.DataBuffer;
import org.nd4j.linalg.api.buffer.util.DataTypeUtil;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.List;

public class DeepAutoEncoderExample {
    private static final Logger log = LoggerFactory.getLogger(MnistMLPExample.class);

    @Parameter(names = "-useSparkLocal", description = "Use spark local (helper for testing/running without spark submit)", arity = 1)
    private boolean useSparkLocal = true;

    @Parameter(names = "-batchSizePerWorker", description = "Number of examples to fit each worker with")
    private int batchSizePerWorker = 16;

    @Parameter(names = "-numEpochs", description = "Number of epochs for training")
    private int numEpochs = 1;

    public static void main(String[] args) throws Exception {
        new DeepAutoEncoderExample().entryPoint(args);
    }

    protected void entryPoint(String[] args) throws Exception {
    
        //Handle command line arguments
        JCommander jcmdr = new JCommander(this);
        try {
            jcmdr.parse(args);
        } catch (ParameterException e) {
            //User provides invalid input -&gt; print the usage info
            jcmdr.usage();
            try { Thread.sleep(500); } catch (Exception e2) { }
            throw e;
        }

        SparkConf sparkConf = new SparkConf();
        if (useSparkLocal) {
            sparkConf.setMaster("local[*]");
        }
        sparkConf.setAppName("DL4J Spark DBN Example");
        JavaSparkContext sc = new JavaSparkContext(sparkConf);

        //Load the data into memory then parallelize
        //This isn't a good approach in general - but is simple to use for this example
        DataSetIterator iterTrain = new MnistDataSetIterator(batchSizePerWorker, true, 12345);
        DataSetIterator iterTest = new MnistDataSetIterator(batchSizePerWorker, true, 12345);
       
        List&lt;DataSet&gt; trainDataList = new ArrayList&lt;&gt;();
        List&lt;DataSet&gt; testDataList = new ArrayList&lt;&gt;();
        while (iterTrain.hasNext()) {
            trainDataList.add(iterTrain.next());
        }
        while (iterTest.hasNext()) {
            testDataList.add(iterTest.next());
        }

        JavaRDD&lt;DataSet&gt; trainData = sc.parallelize(trainDataList);

        //----------------------------------
        //Create network configuration and conduct network training
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(12345)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)
            .activation("leakyrelu")
            .weightInit(WeightInit.XAVIER)
            .learningRate(0.02)
            .updater(Updater.NESTEROVS).momentum(0.9)
            .regularization(true).l2(1e-4)
            .list()
            .layer(0, new RBM.Builder().nIn(28 * 28).nOut(500).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(1, new RBM.Builder().nIn(500).nOut(500).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(2, new RBM.Builder().nIn(500).nOut(250).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(3, new RBM.Builder().nIn(250).nOut(100).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(4, new RBM.Builder().nIn(100).nOut(30).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) //encoding stops
            .layer(5, new RBM.Builder().nIn(30).nOut(100).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) //decoding starts
            .layer(6, new RBM.Builder().nIn(100).nOut(250).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(7, new RBM.Builder().nIn(250).nOut(500).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(8, new RBM.Builder().nIn(500).nOut(1000).lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())
            .layer(9, new OutputLayer.Builder(LossFunctions.LossFunction.RMSE_XENT).activation("softmax").nIn(1000).nOut(10).build())
            .pretrain(false).backprop(true)
            .build();

        //Configuration for Spark training: see http://deeplearning4j.org/spark for explanation of these configuration options
        TrainingMaster tm = new ParameterAveragingTrainingMaster.Builder(batchSizePerWorker)    //Each DataSet object: contains (by default) 32 examples
            .averagingFrequency(5)
            .workerPrefetchNumBatches(2)            //Async prefetching: 2 examples per worker
            .batchSizePerWorker(batchSizePerWorker)
            .build();

        //Create the Spark network
        SparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(sc, conf, tm);

        //Execute training:
        for (int i = 0; i &lt; numEpochs; i++) {
            sparkNet.fit(trainData);
            log.info("Completed Epoch {}", i);
        }

        log.info("***** Example Complete *****");
    }
}
&lt;/denchmark-code&gt;

output log:
&lt;denchmark-code&gt;SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/pyt/.m2/repository/ch/qos/logback/logback-classic/1.1.2/logback-classic-1.1.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/pyt/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]


Peer access [0] -&gt; [1] isn't possible
Peer access [1] -&gt; [0] isn't possible
o.d.s.i.p.ParameterAveragingTrainingMaster - Starting training of split 1 of 31. workerMiniBatchSize=16, averagingFreq=5, Configured for 24 workers
[Stage 2:=========================&gt;                              (11 + 13) / 24]CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
CUDA error at /skymind/libnd4j/blas/cuda/NativeOps.cu:5384 code=77(&lt;unknown&gt;) "result" 
Failed on [139881920934256] -&gt; [35058803712], size: [50176], direction: [0], result: [77]
Failed on [139881717393856] -&gt; [35544194048], size: [50176], direction: [0], result: [77]
Failed on [139882858185648] -&gt; [35543747584], size: [50176], direction: [0], result: [77]
Failed on [139881382231136] -&gt; [35547088896], size: [50176], direction: [0], result: [77]
Failed on [139881314633600] -&gt; [35171799040], size: [50176], direction: [0], result: [77]
Failed on [139882991327328] -&gt; [35360693248], size: [50176], direction: [0], result: [77]
Failed on [10483269632] -&gt; [10692853760], size: [5857016], direction: [3], result: [77]
Failed on [10483269632] -&gt; [10698752000], size: [5857016], direction: [3], result: [77]
Failed on [139881987100336] -&gt; [35342817280], size: [50176], direction: [0], result: [77]

Process finished with exit code 1
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='maybecan' date='2016-12-18T12:20:55Z'>
		Interesting.
Could you please limit your env to just one GPU (K20c)? That could be done using cuda env variable CUDA_VISIBLE_DEVICES
&lt;denchmark-link:https://docs.nvidia.com/cuda/cuda-c-programming-guide/#env-vars&gt;https://docs.nvidia.com/cuda/cuda-c-programming-guide/#env-vars&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='maybecan' date='2018-04-25T10:44:53Z'>
		Obsolete - RBMs were removed in 1.0.0-alpha
		</comment>
		<comment id='5' author='maybecan' date='2018-09-22T19:13:48Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>