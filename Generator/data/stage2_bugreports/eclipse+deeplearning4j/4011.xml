<bug id='4011' author='daanvdn' open_date='2017-09-06T06:45:11Z' closed_time='2018-05-10T06:48:15Z'>
	<summary>ND4J_FALLBACK not being picked up by spark workers</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

When running  the following issue presents itself (as discussed with &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 ):


ND4J_FALLBACK is enabled in spark_env.sh  (ND4J_FALLBACK=1) of spark driver


upon spark-submit, the driver logs indicate that fallback mode is enabled:


&lt;denchmark-code&gt;17/09/05 11:16:53 INFO Reflections: Reflections took 508 ms to scan 161 urls, producing 128489 keys and 145961 values
17/09/05 11:16:53 INFO NativeOpsHolder: Number of threads used for NativeOps: 8
17/09/05 11:16:53 INFO Reflections: Reflections took 227 ms to scan 1 urls, producing 31 keys and 227 values
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                 ND4J_FALLBACK environment variable is detected!
                 Performance will be slightly reduced

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

17/09/05 11:16:54 INFO Nd4jBlas: Number of threads used for BLAS: 8
17/09/05 11:16:54 INFO DefaultOpExecutioner: Backend used: [CPU]; OS: [Linux]
17/09/05 11:16:54 INFO DefaultOpExecutioner: Cores: [32]; Memory: [57.5GB];
17/09/05 11:16:54 INFO DefaultOpExecutioner: Blas vendor: [OPENBLAS]
17/09/05 11:16:54 INFO Reflections: Reflections took 354 ms to scan 1 urls, producing 396 keys and 1573 values
&lt;/denchmark-code&gt;


At the point where SparkDl4jMultiLayer starts fitting the hdfs paths, the call to org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate causes the application to crash.
The logs of the spark-submit process contains stackraces like the following:

&lt;denchmark-code&gt;ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1504086973654_0049_01_000003 on host: slave4. Exit status: 134. Diagnostics: Exception from container-launch: 
org.apache.hadoop.util.Shell$ExitCodeException: /bin/bash: line 1: 52944 Aborted                 (core dumped) /opt/java/jdk1.8.0_112/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms51200m -Xmx51200m -Djava.io.tmpdir=/export/data/ndmhce-landc015-1/hadoop_data/hdfs/tmp/nm-local-dir/usercache/daan_vandennest/appcache/application_1504086973654_0049/container_1504086973654_0049_01_000003/tmp '-Dspark.driver.port=47766' '-Dspark.rpc.message.maxSize=1024' '-Dspark.akka.frameSize=2000' -Dspark.yarn.app.container.log.dir=/opt/hadoop-2.2.0/logs/userlogs/application_1504086973654_0049/container_1504086973654_0049_01_000003 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.56.42.97:47766 --executor-id 2 --hostname slave4 --cores 4 --app-id application_1504086973654_0049 --user-class-path file:/export/data/ndmhce-landc015-1/hadoop_data/hdfs/tmp/nm-local-dir/usercache/daan_vandennest/appcache/application_1504086973654_0049/container_1504086973654_0049_01_000003/__app__.jar &gt; /opt/hadoop-2.2.0/logs/userlogs/application_1504086973654_0049/container_1504086973654_0049_01_000003/stdout 2&gt; /opt/hadoop-2.2.0/logs/userlogs/application_1504086973654_0049/container_1504086973654_0049_01_000003/stderr

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:464)
	at org.apache.hadoop.util.Shell.run(Shell.java:379)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:589)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:283)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:79)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745) 
&lt;/denchmark-code&gt;


When digging deeper the hadoop logs of my slaves all signal an error relating to libopenblas

&lt;denchmark-code&gt;#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f05fc98fae9, pid=47028, tid=0x00007f0608dde700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_112-b15) (build 1.8.0_112-b15)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.112-b15 mixed mode linux-amd64 )
# Problematic frame:
# C  [libopenblas.so.0+0x10f1ae9]  sgemm_itcopy_SANDYBRIDGE+0x89
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /export/data/ndmhce-landc015-1/hadoop_data/hdfs/tmp/nm-local-dir/usercache/daan_vandennest/appcache/application_1504086973654_0048/container_1504086973654_0048_01_000003/hs_err_pid47028.log
[thread 139663661188864 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
&lt;/denchmark-code&gt;

This seems to indicate that the workers are not using the ND4J fallback
potentially related to: &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/issues/1355&gt;deeplearning4j/nd4j#1355&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version: 0.9.1
platform information: Centos Linux 6.3
hadoop version: 2.2.0
spark version: 1.6.3

Previously is was on dl4j-0.7.1 + spark 1.6.2 and this issue did not occur in that setup
	</description>
	<comments>
		<comment id='1' author='daanvdn' date='2017-09-06T12:10:26Z'>
		This issue might be fixed in OpenBLAS 0.2.20. Could you try it out by including the following dependencies to your pom.xml file? Thanks!
&lt;dependency&gt;
    &lt;groupId&gt;org.bytedeco.javacpp-presets&lt;/groupId&gt;
    &lt;artifactId&gt;openblas&lt;/artifactId&gt;
    &lt;version&gt;0.2.20-1.3.4-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.bytedeco.javacpp-presets&lt;/groupId&gt;
    &lt;artifactId&gt;openblas&lt;/artifactId&gt;
    &lt;version&gt;0.2.20-1.3.4-SNAPSHOT&lt;/version&gt;
    &lt;classifier&gt;linux-x86_64&lt;/classifier&gt;
&lt;/dependency&gt;
		</comment>
		<comment id='2' author='daanvdn' date='2017-09-06T13:44:11Z'>
		hi &lt;denchmark-link:https://github.com/saudet&gt;@saudet&lt;/denchmark-link&gt;
 ,  I'm assuming I'll have to compile those dependencies from source right? (can't pull them from maven, which makes sense, seeing they're SNAPSHOT versions)
		</comment>
		<comment id='3' author='daanvdn' date='2017-09-06T23:52:35Z'>
		No, they are there:
&lt;denchmark-link:https://oss.sonatype.org/content/repositories/snapshots/org/bytedeco/javacpp-presets/openblas/0.2.20-1.3.4-SNAPSHOT/&gt;https://oss.sonatype.org/content/repositories/snapshots/org/bytedeco/javacpp-presets/openblas/0.2.20-1.3.4-SNAPSHOT/&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='daanvdn' date='2017-09-06T23:53:10Z'>
		Or they should be anyway, please check back a bit later...
		</comment>
		<comment id='5' author='daanvdn' date='2017-09-07T07:48:28Z'>
		yeah, I just got a mvn build failure because it can't find the linux-x86_64artifact, so I'll try again later
		</comment>
		<comment id='6' author='daanvdn' date='2017-12-01T01:32:17Z'>
		OpenBLAS also has a fallback mode of sorts. I've added documentation for this here:
&lt;denchmark-link:https://deeplearning4j.org/native#fallback-mode&gt;https://deeplearning4j.org/native#fallback-mode&lt;/denchmark-link&gt;

If that doesn't work either, that might indicate something wrong with how environment variables are passed around by Spark...
		</comment>
		<comment id='7' author='daanvdn' date='2018-05-10T06:48:15Z'>
		I'm assuming the crash has since been fixed upstream in OpenBLAS, but please let me know if this isn't the case! Thanks
		</comment>
		<comment id='8' author='daanvdn' date='2018-09-22T03:24:08Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>