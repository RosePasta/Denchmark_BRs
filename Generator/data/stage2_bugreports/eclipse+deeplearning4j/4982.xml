<bug id='4982' author='jef5ez' open_date='2018-04-25T21:15:57Z' closed_time='2018-04-27T17:48:22Z'>
	<summary>Non-deterministic behavior with same inputs to simple NN</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Feeding the same input to a simple Conv2d -&gt; DenseLayer network doesn't produce the same output every time.
&lt;denchmark-link:https://gist.github.com/jef5ez/ebd4eee8e83266f2c3e55ddbe9c38bd2&gt;https://gist.github.com/jef5ez/ebd4eee8e83266f2c3e55ddbe9c38bd2&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version: 1.0.0-SNAPSHOT
latest jars in my m2 seem to be from around 20180425.122408
platform information (OS, etc): Ubuntu 16.04
CUDA version: 8.0
NVIDIA driver version: 384.111

	</description>
	<comments>
		<comment id='1' author='jef5ez' date='2018-04-25T21:19:11Z'>
		so that's cuda + cudnn used, right?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Apr 25, 2018 at 14:16 jef5ez ***@***.***&gt; wrote:
 Issue Description

 Feeding the same input to a simple Conv2d -&gt; DenseLayer network doesn't
 produce the same output every time.
 https://gist.github.com/jef5ez/ebd4eee8e83266f2c3e55ddbe9c38bd2
 Version Information

    - Deeplearning4j version: 1.0.0-SNAPSHOT
    latest jars in my m2 seem to be from around 20180425.122408
    - platform information (OS, etc): Ubuntu 16.04
    - CUDA version: 8.0
    - NVIDIA driver version: 384.111

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/4982&gt;, or mute
 the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ALru_3TDUYeNWOcrNg7qf03HUAR62wqCks5tsOejgaJpZM4TkKxf&gt;
 .



		</comment>
		<comment id='2' author='jef5ez' date='2018-04-25T21:26:53Z'>
		Correct, cudnn 6.0
		</comment>
		<comment id='3' author='jef5ez' date='2018-04-25T21:32:48Z'>
		Can you please disable cudnn dependency, and run without it? Just to see what happens
		</comment>
		<comment id='4' author='jef5ez' date='2018-04-25T21:45:59Z'>
		Same outputs, just with the cudnn missing warning. I can probably run on another machine in a few minutes as well.
		</comment>
		<comment id='5' author='jef5ez' date='2018-04-25T21:55:57Z'>
		So only appears to happen on my 750Ti and not on my Titan
&lt;denchmark-link:https://gist.github.com/jef5ez/47cef22001be32e760d073f37696555d&gt;https://gist.github.com/jef5ez/47cef22001be32e760d073f37696555d&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='jef5ez' date='2018-04-25T21:56:55Z'>
		Ok, i'll try to reproduce it on my box shortly
		</comment>
		<comment id='7' author='jef5ez' date='2018-04-25T22:02:03Z'>
		my cuda version is 8.0.44 on the 750Ti box but 8.0.61 on the titan. could be part of the problem
		</comment>
		<comment id='8' author='jef5ez' date='2018-04-25T22:02:57Z'>
		We’ll see shortly.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 On Apr 25, 2018, at 15:02, jef5ez ***@***.***&gt; wrote:

 my cuda version is 8.0.44 on the 750Ti box but 8.0.61 on the titan. could be part of the problem

 —
 You are receiving this because you were assigned.
 Reply to this email directly, view it on GitHub &lt;https://github.com/deeplearning4j/deeplearning4j/issues/4982#issuecomment-384448281&gt;, or mute the thread &lt;https://github.com/notifications/unsubscribe-auth/ALru_6R9L0s6fEiCqSfdErHyStHAvsuOks5tsPJpgaJpZM4TkKxf&gt;.



		</comment>
		<comment id='9' author='jef5ez' date='2018-04-25T22:35:01Z'>
		updated drivers to 390 and cuda to 8.0.61 but same result on the baby card
		</comment>
		<comment id='10' author='jef5ez' date='2018-04-25T22:58:37Z'>
		Does not reproduce with 1.0.0-alpha using nd4j-cuda-8.0 on GTX 780.
both native and cuda consistently give "-0.7103".
		</comment>
		<comment id='11' author='jef5ez' date='2018-04-26T15:22:31Z'>
		yeah also works fine for me on 0.9.1 and 1.0.0-alpha
		</comment>
		<comment id='12' author='jef5ez' date='2018-04-27T03:51:11Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;

Here is a &lt;denchmark-link:https://gist.github.com/RobAltena/8d1591cd6204cc7cb01b4872bde06802&gt;java unit test&lt;/denchmark-link&gt;
 for the issue. You can plug that in production (if this is an issue with the current build).
		</comment>
		<comment id='13' author='jef5ez' date='2018-04-27T04:03:57Z'>
		Yes, reproducible for me. Thank you.
		</comment>
		<comment id='14' author='jef5ez' date='2018-04-27T04:43:05Z'>
		Now that the test is in, it should be on the list to be fixed for future releases. Thanks for reporting the issue.
		</comment>
		<comment id='15' author='jef5ez' date='2018-04-27T04:47:21Z'>
		Test is yet to be decomposed down to nd4j level :)
		</comment>
		<comment id='16' author='jef5ez' date='2018-04-27T04:56:24Z'>
		Looks like an edge case issue. Fails for me for nOut=1, passes for nOut=2
Edit: and also fails without workspaces, but seems less likely to occur without vs. with.
		</comment>
		<comment id='17' author='jef5ez' date='2018-04-27T06:38:20Z'>
		Isolated here: &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/issues/2860&gt;deeplearning4j/nd4j#2860&lt;/denchmark-link&gt;

Will close this once the fix for that is confirmed in the context of DL4J output.
		</comment>
		<comment id='18' author='jef5ez' date='2018-04-27T17:48:22Z'>
		Issue fixed on current master.
Thanks for highlighting.
		</comment>
		<comment id='19' author='jef5ez' date='2018-09-22T09:24:06Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>