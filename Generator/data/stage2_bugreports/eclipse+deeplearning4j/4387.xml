<bug id='4387' author='ryanzpn' open_date='2017-12-08T07:27:32Z' closed_time='2017-12-08T11:50:42Z'>
	<summary>load keras model fail</summary>
	<description>
I was trying to load keras h5 model by using deeplearning4j-modelimport, but it failed.
I downloaded deeplearning4j master, compiled and installed. The keras version is 2.0.4 and is based on tensorflow.
I loaded the model with this api:
&lt;denchmark-code&gt;KerasModelImport.importKerasSequentialModelAndWeights()
&lt;/denchmark-code&gt;

It is a sequential cnn model, with an embedding layer after the input. It just like this:
    model = Sequential()
    model.add(Embedding(VOCAB_SIZE, 100, input_length=MAX_TEXT_LEN, embeddings_initializer='he_normal'))
    #
    model.add(Conv1D(1, 3, activation='relu'))
    model.add(MaxPooling1D(3))
    #
    model.add(Conv1D(1, 3, activation='relu'))
    model.add(MaxPooling1D(3))
    #
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    
    # compile the model
    opt = SGD(lr=0.1)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])
It's application is nlp for antispam, the training data is like this:
&lt;denchmark-code&gt;1       [55, 45, 325, 326, 18, 74, 189, 54, 313, 18, 193, 287, 23, 114, 114, 1088, 873, 997, 918]
&lt;/denchmark-code&gt;

the first column is label, the second column is index for chars not words. padding will be used before feed into the model
the error message is listed below:
&lt;denchmark-code&gt;java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.lang.Integer
        at org.deeplearning4j.nn.modelimport.keras.layers.convolutional.KerasConvolutionUtils.getDilationRate(KerasConvolutionUtils.java:94)
        at org.deeplearning4j.nn.modelimport.keras.layers.convolutional.KerasConvolution1D.&lt;init&gt;(KerasConvolution1D.java:83)
        at org.deeplearning4j.nn.modelimport.keras.utils.KerasLayerUtils.getKerasLayerFromConfig(KerasLayerUtils.java:187)
        at org.deeplearning4j.nn.modelimport.keras.KerasModel.prepareLayers(KerasModel.java:185)
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.&lt;init&gt;(KerasSequentialModel.java:98)
        at org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.&lt;init&gt;(KerasSequentialModel.java:60)
        at org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.buildSequential(KerasModelBuilder.java:120)
        at org.deeplearning4j.nn.modelimport.keras.KerasModelImport.importKerasSequentialModelAndWeights(KerasModelImport.java:158)
        at com.hello.world.antispam.learn.KerasToJava.main(KerasToJava.java:14)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ryanzpn' date='2017-12-08T08:34:31Z'>
		&lt;denchmark-link:https://github.com/ryanzpn&gt;@ryanzpn&lt;/denchmark-link&gt;
 thanks for reporting this, definitely a bug. It tries to read  parameters and fails, but it seems you don't even specify that.
Is the input data to this model freely available, i.e. can you share your full script? I think this would make a good integration test / full example.
		</comment>
		<comment id='2' author='ryanzpn' date='2017-12-08T08:53:58Z'>
		Here is the script for generating the input data and corresponding dict. The cnn model is used to detecting meaningless text, eg. 'fpqegaldjhalgvlaj0000022....', it can be easily generated. In my case, text often contains chinese, but it doesn't matter.
&lt;denchmark-code&gt;#! /usr/bin/env python

import json

def parse(line):
    try:
        return json.loads(line)
    except:
        return None

def main():
    char_dict = {}

    with open(FILENAME) as infile:
        for line in infile:
            obj = parse(line)
            if obj is None:
                continue

            text = obj['text']
            if 0 == len(text):
                continue

            oh = []

            for i in range(len(text)):
                if text[i] not in char_dict:
                    char_dict[text[i]] = len(char_dict) + 1

                oh.append(char_dict[text[i]])

            # input training data for the model
            #print(str(obj['result']) + "\t" + str(oh))

    # dict file 
    #with open(DICT_FILE_NAME, 'w') as outfile:
    #    outfile.write(json.dumps(char_dict))

if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

And here is the code for training and evaluating.
&lt;denchmark-code&gt;import json
import time

from keras.models import load_model
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.pooling import MaxPooling1D
from keras.optimizers import Adam, SGD 

MAX_TEXT_LEN = 200
VOCAB_SIZE = 11000

def current_millis():
    return int(time.time())
 
def parse(line):
    try:
        return json.loads(line)
    except:
        return None

def make_xy(FILENAME):
    labels = []
    encoded_docs = []

    with open(FILENAME) as infile:
        for line in infile:
            segs = line.split('\t')
            if 2 != len(segs):
                continue
    
            labels.append(int(segs[0]) - 1)
        
            tmp_oh = segs[1][1:-2].split(',')    
            tail = min(MAX_TEXT_LEN, len(tmp_oh))
            encoded_docs.append([int(tmp_oh[i]) for i in range(tail)])

    return pad_sequences(encoded_docs, maxlen=MAX_TEXT_LEN, padding='post'), labels

def compile_model():
    # define the model
    model = Sequential()
    model.add(Embedding(VOCAB_SIZE, 100, input_length=MAX_TEXT_LEN, embeddings_initializer='he_normal'))
    #
    model.add(Conv1D(1, 3, activation='relu'))
    model.add(MaxPooling1D(3))
    #
    model.add(Conv1D(1, 3, activation='relu'))
    model.add(MaxPooling1D(3))
    #
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    
    # compile the model
    opt = SGD(lr=0.1)
    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])

    return model

def train(model, x_train, y_train):
    model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)
    return model

def summary(model):
    return model.summary()

def evaluate(model, x_test, y_test):
    return model.evaluate(x_test, y_test, verbose=0)

def make_xy_from_audit(audit_file, oh_dict):
    labels = []
    encoded_docs = []

    with open(audit_file) as infile:
        for line in infile:
            obj = parse(line)
            if obj is None:
                continue

            text = obj['text']
            if 0 == len(text):
                continue

            oh = []
            flag = True
            tail = min(len(text), MAX_TEXT_LEN)
            
            for i in range(tail):
                if text[i] not in oh_dict:
                    flag = False
                    break
                else:
                    oh.append(oh_dict[text[i]])

            if flag:
                labels.append(int(obj['result']) - 1)
                encoded_docs.append(oh)

    return pad_sequences(encoded_docs, maxlen=MAX_TEXT_LEN, padding='post'), labels

def load_oh_dict(FILENAME):
    oh_dict = {}
    with open(FILENAME) as infile:
        for line in infile:
            oh_dict = json.loads(line)
            break
    return oh_dict

def model_save(model, save_dir='./'):
    filename = str(current_millis()) + ".h5"
    model.save(save_dir + filename) 

def main():
    oh_dict = load_oh_dict(DICT_FILE_NAME)
    print(len(oh_dict))

    x_train, y_train = make_xy(DATA_FOR_TRAINING)
    x_test, y_test = make_xy_from_audit(NEW_DATA_FOR_EVALUATING)
    print(len(x_test))
    print(len(y_test))

    model = compile_model()
    #
    model = train(model, x_train, y_train)
    #
    smry = summary(model)
    print(smry)
    #
    loss, accuracy = evaluate(model, x_test, y_test)
    print('Accuracy: %f' % (accuracy * 100))
    # save model
    model_save(model)

if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

Hope this may help~
		</comment>
		<comment id='3' author='ryanzpn' date='2017-12-08T10:29:28Z'>
		&lt;denchmark-link:https://github.com/ryanzpn&gt;@ryanzpn&lt;/denchmark-link&gt;
 I have fixed your issue in a branch, but now have identified a new problem: the embedding layer doesn't properly connect to conv1d upon import. I need to add a preprocessor for this, will keep you updated.
		</comment>
		<comment id='4' author='ryanzpn' date='2017-12-08T11:50:42Z'>
		should be fixed now &lt;denchmark-link:https://github.com/ryanzpn&gt;@ryanzpn&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/pull/4388&gt;#4388&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='ryanzpn' date='2018-09-24T02:43:58Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>