<bug id='3202' author='AlexDBlack' open_date='2017-04-07T06:53:35Z' closed_time='2017-04-07T06:59:28Z'>
	<summary>ParallelWrapper - continuing training not working correctly</summary>
	<description>
    @Test
    public void testContinueTraining() throws Exception {

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(12345)
                .regularization(true).l2(0.0005)
                .learningRate(0.1)
                .weightInit(WeightInit.XAVIER)
                .updater(Updater.ADAM).adamMeanDecay(0.9).adamVarDecay(0.999)
                .list()
                .layer(0, new ConvolutionLayer.Builder(5, 5).nIn(1).nOut(10).activation(Activation.IDENTITY).build())
                .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2,2).stride(2,2).build())
                .layer(2, new DenseLayer.Builder().activation(Activation.RELU).nOut(100).build())
                .layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).nOut(10).activation(Activation.SOFTMAX).build())
                .setInputType(InputType.convolutionalFlat(28,28,1))
                .build();

        MultiLayerNetwork net = new MultiLayerNetwork(conf);
        net.init();

        //Set up data
        List&lt;DataSet&gt; data1 = new ArrayList&lt;&gt;();

        DataSetIterator iter = new MnistDataSetIterator(32, true, 12345);
        for( int i=0; i&lt;2*3*10; i++ ){
            data1.add(iter.next());
        }

        List&lt;DataSet&gt; data2 = new ArrayList&lt;&gt;();
        for( int i=0; i&lt;2*3*2; i++ ){
            data2.add(iter.next());
        }

        //Do some initial training
        ParallelWrapper pw1 = getPW(net);
        pw1.setListeners(new ScoreIterationListener(1));
        pw1.fit(new ExistingDataSetIterator(data1));

        MultiLayerNetwork net1 = (MultiLayerNetwork)pw1.getModel();

        //Save and load the model
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        ModelSerializer.writeModel(net1, baos, true);
        byte[] bytes = baos.toByteArray();
        ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
        MultiLayerNetwork net2 = ModelSerializer.restoreMultiLayerNetwork(bais, true);

        //At this point: pw1 and pw2 should have identical models
        ParallelWrapper pw2 = getPW(net2);

        assertEquals(net1.params(), net2.params());
        assertEquals(net1.getUpdater().getStateViewArray(), net2.getUpdater().getStateViewArray());
        assertEquals(net1.getLayerWiseConfigurations().getIterationCount(), net2.getLayerWiseConfigurations().getIterationCount());

        System.out.println("Fitting PW1");
        pw1.fit(new ExistingDataSetIterator(data2));

        System.out.println("Fitting PW2");
        pw2.setListeners(new ScoreIterationListener(1));
        pw2.fit(new ExistingDataSetIterator(data2));
    }

    public static ParallelWrapper getPW(MultiLayerNetwork net){
        return new ParallelWrapper.Builder(net)
                .prefetchBuffer(0)
                .workers(2)
                .averagingFrequency(3)
                .reportScoreAfterAveraging(false)
                .useLegacyAveraging(false)
                .build();
    }
These two should be the same (perhaps with a small amount of random variability)
&lt;denchmark-code&gt;Fitting PW1
o.d.o.l.ScoreIterationListener - Score at iteration 60 is 0.21184506149179833
o.d.o.l.ScoreIterationListener - Score at iteration 61 is 0.5661765250671615
o.d.o.l.ScoreIterationListener - Score at iteration 62 is 0.3122041064924461
o.d.o.l.ScoreIterationListener - Score at iteration 62 is 0.3622308858031376
o.d.o.l.ScoreIterationListener - Score at iteration 64 is 0.2964624604238272
o.d.o.l.ScoreIterationListener - Score at iteration 64 is 0.23689279970086657
o.d.o.l.ScoreIterationListener - Score at iteration 66 is 0.2754616718081668
o.d.o.l.ScoreIterationListener - Score at iteration 66 is 0.6093583981780245
o.d.o.l.ScoreIterationListener - Score at iteration 68 is 0.2605746573827326
o.d.o.l.ScoreIterationListener - Score at iteration 69 is 0.2791835955666789
o.d.o.l.ScoreIterationListener - Score at iteration 70 is 0.503278662772234
o.d.o.l.ScoreIterationListener - Score at iteration 70 is 0.18504373885178518
Fitting PW2
o.d.o.l.ScoreIterationListener - Score at iteration 0 is 2.3647792901185127
o.d.o.l.ScoreIterationListener - Score at iteration 1 is 2.2599990002863284
o.d.o.l.ScoreIterationListener - Score at iteration 2 is 2.189428063949927
o.d.o.l.ScoreIterationListener - Score at iteration 3 is 2.1645925560471877
o.d.o.l.ScoreIterationListener - Score at iteration 4 is 1.9060159220782842
o.d.o.l.ScoreIterationListener - Score at iteration 4 is 1.950356322992359
o.d.o.l.ScoreIterationListener - Score at iteration 6 is 2.1959433134600643
o.d.o.l.ScoreIterationListener - Score at iteration 7 is 2.2312168654009823
o.d.o.l.ScoreIterationListener - Score at iteration 8 is 2.095845878516587
o.d.o.l.ScoreIterationListener - Score at iteration 9 is 2.1418557734294366
o.d.o.l.ScoreIterationListener - Score at iteration 10 is 2.0283166455774606
o.d.o.l.ScoreIterationListener - Score at iteration 10 is 1.9856916581013055
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2017-04-07T06:58:34Z'>
		This issue is duplicate of this one: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/3152&gt;https://github.com/deeplearning4j/deeplearning4j/issues/3152&lt;/denchmark-link&gt;

And it's already fixed in my branch.
		</comment>
		<comment id='2' author='AlexDBlack' date='2017-04-07T06:59:28Z'>
		No worries - hit this today, didn't realize it was a known issue already.
		</comment>
		<comment id='3' author='AlexDBlack' date='2017-04-07T07:00:17Z'>
		It's easy fix. If you want that on master - i can apply it there as well. Just params/updater states should be copied at replicated models init.
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-09-30T07:03:05Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>