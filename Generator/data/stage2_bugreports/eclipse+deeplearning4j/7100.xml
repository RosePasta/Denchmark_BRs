<bug id='7100' author='AlexDBlack' open_date='2019-01-31T23:18:33Z' closed_time='2019-02-01T11:32:06Z'>
	<summary>SameDiff: Issues with placeholder gradients?</summary>
	<description>
As reported in gitter:
&lt;denchmark-link:https://github.com/SchmaR&gt;@SchmaR&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;I think that I've found a bug in executing the backward pass of a SameDiff graph. If the graph contains a placeholder variable, I can not calculate a gradient for it. SDVariable.getGradient().getArr(); returns null.

I also noticed an unintuitive behavior/naming when asking for the gradient of an operation like add, which throws java.lang.IllegalStateException: Cannot get array for SDVariable "cast_1": no array has been defined, and array shape cannot be calculated. I'm not sure if cast_1 is the correct variable name for the backward pass of add.

I'm using the current snapshot (nd4j-api-1.0.0-20190131.150707-14381.jar) on a Ubuntu 18.10 64 bit machine and running the code on an Intel i7-7700HQ CPU.

Here you can find three failing test cases: https://gist.github.com/SchmaR/d5647285b7f8d7ddcbb6d76a6dfb978d
&lt;/denchmark-code&gt;

Aha! Link: &lt;denchmark-link:https://skymindai.aha.io/features/ND4J-38&gt;https://skymindai.aha.io/features/ND4J-38&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2019-02-01T11:32:06Z'>
		Turns out this is just an issue of unclear/not properly documented behaviour.
The behaviour for the particular method (execBackwards(map)) is correct - it calculates the gradients for the VARIABLE type SDVariables only (i.e., the trainable parameter SDVariables), not the placeholders/constants etc (unless doing so is required to calculate the variable gradients). In this case, no variables -&gt; nothing to calculate (using that method).
I've added the logging and better javadoc to this method to clarify what's going on. That will be merged soon.
To force calculation of gradients for placeholders etc, you can use execBackwards(Map,List) to specify the gradient variables to use. Here's an example:
&lt;denchmark-code&gt;    @Test
    public void sameDiffPlaceholderGrad(){
        INDArray x = Nd4j.ones(2,2);
        INDArray y = Nd4j.ones(2,2);

        SameDiff sd = SameDiff.create();

        SDVariable xSd = sd.placeHolder("x", x.shape());
        SDVariable ySd = sd.placeHolder("y", y.shape());

        SDVariable add = ySd.add("add", xSd);

        Map&lt;String, INDArray&gt; placeholders = new HashMap&lt;&gt;();
        placeholders.put("x", x);
        placeholders.put("y", y);
        sd.createGradFunction();    //Otherwise: xSd.gradient() etc won't be defined
        sd.execBackwards(placeholders, Arrays.asList(xSd.gradient().getVarName(), ySd.gradient().getVarName()));
        INDArray xGradientEnforced = add.getGradient().getArr(true);
        assertNotNull(xGradientEnforced);
    }
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='AlexDBlack' date='2019-03-03T12:02:48Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>