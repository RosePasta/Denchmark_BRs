<bug id='3192' author='kepricon' open_date='2017-04-05T13:07:11Z' closed_time='2018-04-25T22:44:52Z'>
	<summary>ParallelWrapper gpu memory allocation issue</summary>
	<description>
please refer vgg16 &lt;denchmark-link:https://gist.github.com/kepricon/60084a744824e77715dd8dbdca29de76&gt;network configuration&lt;/denchmark-link&gt;

below are about cudaEnvironment and ParallelWraper
&lt;denchmark-code&gt;CudaEnvironment.getInstance().getConfiguration()
                .allowMultiGPU(true)
                .setMaximumDeviceCache(6L * 1024L * 1024L * 1024L)
                .setMaximumHostCache(12L * 1024L * 1024L * 1024L)
                .setNumberOfGcThreads(4);
        Nd4j.create(1);
        Nd4j.getMemoryManager().togglePeriodicGc(true);
        Nd4j.getMemoryManager().setAutoGcWindow(50);
        Nd4j.getMemoryManager().setOccasionalGcFrequency(0);
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;ParallelWrapper pw = new ParallelWrapper.Builder&lt;&gt;(model)
                        .prefetchBuffer(7)
                        .reportScoreAfterAveraging(true)
                        .averagingFrequency(10)
                        .useLegacyAveraging(false)
                        .useMQ(true)
                        .workers(7)  // set 7 out of 8 devices
                        .build();

                pw.fit(iter);
&lt;/denchmark-code&gt;

Memory allocation to each gpus seems different from my expectation
and after averaging it seems never going down.
the below is a capture of nvidia-smi.
iteration 10
&lt;denchmark-code&gt;Wed Apr  5 21:35:09 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.20                 Driver Version: 375.20                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-SXM2...  On   | 0000:06:00.0     Off |                    0 |
| N/A   38C    P0    46W / 300W |   3911MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-SXM2...  On   | 0000:07:00.0     Off |                    0 |
| N/A   42C    P0    50W / 300W |   6521MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P100-SXM2...  On   | 0000:0A:00.0     Off |                    0 |
| N/A   41C    P0    47W / 300W |   6033MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P100-SXM2...  On   | 0000:0B:00.0     Off |                    0 |
| N/A   41C    P0    49W / 300W |   6105MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla P100-SXM2...  On   | 0000:85:00.0     Off |                    0 |
| N/A   40C    P0    51W / 300W |   6103MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla P100-SXM2...  On   | 0000:86:00.0     Off |                    0 |
| N/A   43C    P0    46W / 300W |   6519MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla P100-SXM2...  On   | 0000:89:00.0     Off |                    0 |
| N/A   42C    P0    46W / 300W |   6267MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla P100-SXM2...  On   | 0000:8A:00.0     Off |                    0 |
| N/A   40C    P0    41W / 300W |   6001MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     75520    C   java                                          3847MiB |
|    1     75520    C   java                                          6457MiB |
|    2     75520    C   java                                          5969MiB |
|    3     75520    C   java                                          6041MiB |
|    4     75520    C   java                                          6039MiB |
|    5     75520    C   java                                          6455MiB |
|    6     75520    C   java                                          6203MiB |
|    7     75520    C   java                                          5937MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

iteration 50
&lt;denchmark-code&gt;Wed Apr  5 21:38:17 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.20                 Driver Version: 375.20                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-SXM2...  On   | 0000:06:00.0     Off |                    0 |
| N/A   38C    P0    46W / 300W |   4735MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-SXM2...  On   | 0000:07:00.0     Off |                    0 |
| N/A   43C    P0    50W / 300W |   7841MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P100-SXM2...  On   | 0000:0A:00.0     Off |                    0 |
| N/A   42C    P0    54W / 300W |   7895MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P100-SXM2...  On   | 0000:0B:00.0     Off |                    0 |
| N/A   42C    P0    43W / 300W |   8291MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla P100-SXM2...  On   | 0000:85:00.0     Off |                    0 |
| N/A   42C    P0    52W / 300W |   7873MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla P100-SXM2...  On   | 0000:86:00.0     Off |                    0 |
| N/A   45C    P0    54W / 300W |   7687MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla P100-SXM2...  On   | 0000:89:00.0     Off |                    0 |
| N/A   44C    P0    55W / 300W |   8275MiB / 16308MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla P100-SXM2...  On   | 0000:8A:00.0     Off |                    0 |
| N/A   42C    P0    47W / 300W |   7363MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     75520    C   java                                          4657MiB |
|    1     75520    C   java                                          7763MiB |
|    2     75520    C   java                                          7817MiB |
|    3     75520    C   java                                          8213MiB |
|    4     75520    C   java                                          7795MiB |
|    5     75520    C   java                                          7609MiB |
|    6     75520    C   java                                          8197MiB |
|    7     75520    C   java                                          7285MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

iteration 70
&lt;denchmark-code&gt;Wed Apr  5 21:41:25 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.20                 Driver Version: 375.20                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-SXM2...  On   | 0000:06:00.0     Off |                    0 |
| N/A   39C    P0    46W / 300W |  10133MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-SXM2...  On   | 0000:07:00.0     Off |                    0 |
| N/A   43C    P0    44W / 300W |   8349MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla P100-SXM2...  On   | 0000:0A:00.0     Off |                    0 |
| N/A   43C    P0    54W / 300W |   8305MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla P100-SXM2...  On   | 0000:0B:00.0     Off |                    0 |
| N/A   42C    P0    44W / 300W |   8405MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla P100-SXM2...  On   | 0000:85:00.0     Off |                    0 |
| N/A   42C    P0    52W / 300W |   8143MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla P100-SXM2...  On   | 0000:86:00.0     Off |                    0 |
| N/A   45C    P0    46W / 300W |   8107MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla P100-SXM2...  On   | 0000:89:00.0     Off |                    0 |
| N/A   45C    P0    55W / 300W |   8407MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla P100-SXM2...  On   | 0000:8A:00.0     Off |                    0 |
| N/A   43C    P0    48W / 300W |   8213MiB / 16308MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     75520    C   java                                         10049MiB |
|    1     75520    C   java                                          8265MiB |
|    2     75520    C   java                                          8221MiB |
|    3     75520    C   java                                          8321MiB |
|    4     75520    C   java                                          8059MiB |
|    5     75520    C   java                                          8023MiB |
|    6     75520    C   java                                          8323MiB |
|    7     75520    C   java                                          8129MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='kepricon' date='2017-04-06T03:51:52Z'>
		I have this problem too， it seems to be  that nd4j-cuda memory  release is dependent on   jvm GC  ,but GC too slowly  and  if you set GC frequency too highly the program also very slowly, seems to have no way to come！
		</comment>
		<comment id='2' author='kepricon' date='2017-04-06T03:54:26Z'>
		&lt;denchmark-link:https://github.com/xuenhappy&gt;@xuenhappy&lt;/denchmark-link&gt;
 Complaining about it useless for both of us  it's a known problem being worked on  I'm not sure what else you want. Unless you have extra actual valuable information to provide just wait for the fix. If you're eager enough, volunteer to test when it's out.
		</comment>
		<comment id='3' author='kepricon' date='2017-04-06T04:04:19Z'>
		It's not a complaint, it's just an explanation of nd4j-cuda's release of memory that is totally dependent on the GC， and Need yourself to balance your own good GC frequency and CUDA memory
		</comment>
		<comment id='4' author='kepricon' date='2017-04-06T04:05:35Z'>
		Indeed. That's why we're building workspaces :D. Feel free to ask questions about the current work in the mean time. We've heard from people loud and clear what the problems are ;). There are some other issues on GC/workspaces as well. Eg: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/3184&gt;https://github.com/deeplearning4j/deeplearning4j/issues/3184&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='kepricon' date='2017-04-06T07:09:08Z'>
		Guys, this issue isn't about GC :)
It's weird issue of device_0, that consumes lots of memory, despite it's not used for training. Only holds original model and does replication.
We've actually worked out reasons behind this behavior, but i still want to make sure device_0 doesn't hold two model copies. That's why i've asked to file this issue :)
		</comment>
		<comment id='6' author='kepricon' date='2017-04-06T07:10:01Z'>
		p.s. it's high amount of assigned device cache was causing ooms. Just if anyone is interested.
		</comment>
		<comment id='7' author='kepricon' date='2017-05-06T16:27:18Z'>
		On current master CPU fallback for averaging was introduced, so issue should be solved. Please try &amp; confirm or decline
		</comment>
		<comment id='8' author='kepricon' date='2018-04-25T22:44:52Z'>
		Issue considered fixed long ago.
		</comment>
		<comment id='9' author='kepricon' date='2018-09-22T17:14:06Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>