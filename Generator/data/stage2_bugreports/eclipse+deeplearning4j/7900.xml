<bug id='7900' author='montardon' open_date='2019-06-12T13:38:37Z' closed_time='2019-06-25T07:23:22Z'>
	<summary>Out of memory in training a model</summary>
	<description>
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/3281536/outOfMemory.txt&gt;outOfMemory.txt&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I import an untrained  Keras model of ResNet20 on Cifar10.
I train my model on 200 epochs.
After about 90 epochs, I get an out of memory error.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version: 1.0.0-beta4
platform information (OS, etc): Ubuntu 18.0.2
CUDA version, if used: CUDA 10.0
NVIDIA driver version, if in use: 418.67

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

Training code
&lt;denchmark-link:https://gist.github.com/montardon/17b9962718592aa595c891f001a836fe&gt;https://gist.github.com/montardon/17b9962718592aa595c891f001a836fe&lt;/denchmark-link&gt;

Model
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/files/3281531/model_cifar10_ResNet20v1.zip&gt;model_cifar10_ResNet20v1.zip&lt;/denchmark-link&gt;

Give path to model as program argument.
	</description>
	<comments>
		<comment id='1' author='montardon' date='2019-06-12T14:56:50Z'>
		I used YourKit Java Profiler while training the model. Heap memory keeps growing.
&lt;denchmark-link:https://user-images.githubusercontent.com/238749/59361957-fc29f480-8d32-11e9-9e0e-abb166457e06.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/238749/59361987-08ae4d00-8d33-11e9-871b-45f8630067cb.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='montardon' date='2019-06-12T15:05:16Z'>
		That's expected behavior. heap grows -&gt; gc kicks in -&gt; heap shrinks -&gt; heap grows.
Problem is somewhere else. We'll find out where.
		</comment>
		<comment id='3' author='montardon' date='2019-06-23T11:03:39Z'>
		So, all out of suddent that's not a real leak.
Problem appears only if gc pressure is small enough, and full gc is never performed. In this case thread (and workspace attached to that thread) is never removed by gc, and memory stacks up.
Once gc kicks in - everything is released properly.
Problem appeared in beta4 due to 2 changes

periodic gc calls were disabled by default
we've reduced amount of on-heap objects used by nd4j objects.

Obvious temporary workaround - add System.gc() call within your epoch loop. Or enable periodic gc.
		</comment>
		<comment id='4' author='montardon' date='2019-06-23T11:05:01Z'>
		Another workaround - wrap your iterator with AsyncShieldDataSetIterator.  But i'd recommend not to, and stick to other two workarounds.
		</comment>
		<comment id='5' author='montardon' date='2019-06-24T03:10:24Z'>
		It's also possible that the system is having a hard time deallocating native memory for some reason. This typically happens on Windows, but in any case, increasing "org.bytedeco.javacpp.maxretries" to something crazy like 1000 should avoid giving up too fast:
&lt;denchmark-link:http://bytedeco.org/javacpp/apidocs/org/bytedeco/javacpp/Pointer.html#maxRetries&gt;http://bytedeco.org/javacpp/apidocs/org/bytedeco/javacpp/Pointer.html#maxRetries&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='montardon' date='2019-06-24T12:35:49Z'>
		I found this dump in IntelliJ project tree. May be it can help you.
&lt;denchmark-link:https://github.com/eclipse/deeplearning4j/files/3320798/dl4j-memory-crash-dump-1560283133783_1.txt&gt;dl4j-memory-crash-dump-1560283133783_1.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='montardon' date='2019-06-24T12:52:31Z'>
		Few comments above i've already explained what's wrong and what are the possible workarounds
		</comment>
	</comments>
</bug>