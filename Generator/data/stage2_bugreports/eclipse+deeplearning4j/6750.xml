<bug id='6750' author='AlexDBlack' open_date='2018-11-22T10:07:49Z' closed_time='2018-11-23T09:55:41Z'>
	<summary>DL4J Spark training: Batch norm variance may (rarely) become negative during training</summary>
	<description>
Batch norm mean/variance is usually updated (locally) by just modifying the parameters during forward pass when training==true.
This is fine for single node training, but doesn't work at all for distributed training.
Thus, we push the parameter changes into the updates vector and all is well.
However, when the variance estimate becomes small relative to the threshold, we can have a situation where varEstimate - threshold &lt; 0 which can obviously break things.
In principle, we can also have update stacking in this situation: i.e., N nodes change the variance estimate simultaneously, thus we have varEstimate - n*threshold &lt; 0.
One solution: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/6749&gt;https://github.com/deeplearning4j/deeplearning4j/issues/6749&lt;/denchmark-link&gt;

Another (quicker) solution: reparameterize the variance "parameter" to something else, like log variance (which can be positive or negative).
The choice of reparameterization obviously matters - a badly chosen parameterization may impact convergence (as we'll need to take very large - or very small - steps to get to where we wan to be).
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-12-23T10:04:19Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>