<bug id='1556' author='fac2003' open_date='2016-05-17T18:26:39Z' closed_time='2016-06-04T15:00:07Z'>
	<summary>core dump in native code with LSTM and rc3.9</summary>
	<description>
I am experiencing reproducible core dumps when running a LSTM training similar to the GravesLSTMCharModellingExample sample. I am happy to share the code and the data if you need it to track down the issue.
&lt;denchmark-code&gt;
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x000000012cf8b6e9, pid=85398, tid=5891
#
# JRE version: Java(TM) SE Runtime Environment (8.0_40-b27) (build 1.8.0_40-b27)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.40-b25 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# C  [libnd4j.dylib+0x236e9]  _ZN5shape3TAD4initEPiS1_i+0x249
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /Users/fac2003/IdeaProjects/git/dl4c/hs_err_pid85398.log
#
&lt;/denchmark-code&gt;

Here's the relevant content of hs_err:
&lt;denchmark-code&gt;
Stack: [0x000070000011a000,0x000070000021a000],  sp=0x0000700000218400,  free space=1017k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libnd4j.dylib+0x236e9]  shape::TAD::init(int*, int*, int)+0x249
C  [libnd4j.dylib+0x279ef]  functions::reduce::ReduceFunction&lt;float&gt;::exec(float*, int*, float*, float*, int*, int*, int)+0xff
C  [libnd4j.dylib+0x63aa]  NativeOps::execReduceFloat(long long*, int, long long, long long, long long, long long, long long, long long, int)+0x5a
C  [libjnind4j.dylib+0xe6e9]  Java_org_nd4j_nativeblas_NativeOps_execReduceFloat___3JIJJJJJJI+0xb9
J 1461  org.nd4j.nativeblas.NativeOps.execReduceFloat([JIJJJJJJI)V (0 bytes) @ 0x000000010d7dfa31 [0x000000010d7df940+0xf1]
J 1986 C2 org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(Lorg/nd4j/linalg/api/ops/Accumulation;[I)Lorg/nd4j/linalg/api/ndarray/INDArray; (1441 bytes) @ 0x000000010d9f5938 [0x000000010d9f4ec0+0xa78]

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
J 1461  org.nd4j.nativeblas.NativeOps.execReduceFloat([JIJJJJJJI)V (0 bytes) @ 0x000000010d7df9b7 [0x000000010d7df940+0x77]
J 1986 C2 org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(Lorg/nd4j/linalg/api/ops/Accumulation;[I)Lorg/nd4j/linalg/api/ndarray/INDArray; (1441 bytes) @ 0x000000010d9f5938 [0x000000010d9f4ec0+0xa78]
J 1985 C2 org.nd4j.linalg.api.ndarray.BaseNDArray.sum([I)Lorg/nd4j/linalg/api/ndarray/INDArray; (18 bytes) @ 0x000000010d9d9dfc [0x000000010d9d9b20+0x2dc]
j  org.deeplearning4j.nn.layers.recurrent.LSTMHelpers.backpropGradientHelper(Lorg/deeplearning4j/nn/conf/NeuralNetConfiguration;Lorg/nd4j/linalg/api/ndarray/INDArray;Lorg/nd4j/linalg/api/ndarray/INDArray;Lorg/nd4j/linalg/api/ndarray/INDArray;Lorg/nd4j/linalg/api/ndarray/INDArray;ZILorg/deeplearning4j/nn/layers/recurrent/FwdPassReturn;ZLjava/lang/String;Ljava/lang/String;Ljava/lang/String;)Lorg/deeplearning4j/berkeley/Pair;+1567
j  org.deeplearning4j.nn.layers.recurrent.GravesLSTM.backpropGradientHelper(Lorg/nd4j/linalg/api/ndarray/INDArray;ZI)Lorg/deeplearning4j/berkeley/Pair;+127
j  org.deeplearning4j.nn.layers.recurrent.GravesLSTM.backpropGradient(Lorg/nd4j/linalg/api/ndarray/INDArray;)Lorg/deeplearning4j/berkeley/Pair;+4
j  org.deeplearning4j.nn.multilayer.MultiLayerNetwork.calcBackpropGradients(Lorg/nd4j/linalg/api/ndarray/INDArray;Z)Lorg/deeplearning4j/berkeley/Pair;+330
j  org.deeplearning4j.nn.multilayer.MultiLayerNetwork.backprop()V+3
j  org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore()V+38
j  org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore()Lorg/deeplearning4j/berkeley/Pair;+12
j  org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize()Z+14
j  org.deeplearning4j.optimize.Solver.optimize()V+19

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='fac2003' date='2016-05-17T18:29:38Z'>
		So, that's Mac OS X, nd4-native backend, right?
		</comment>
		<comment id='2' author='fac2003' date='2016-05-17T19:09:17Z'>
		Correct.
		</comment>
		<comment id='3' author='fac2003' date='2016-05-17T19:30:24Z'>
		We'll need to see your source code and pom.xml. Please use &lt;denchmark-link:http://gist.github.com&gt;http://gist.github.com&lt;/denchmark-link&gt;
 to upload them and post links to them here
		</comment>
		<comment id='4' author='fac2003' date='2016-05-17T20:12:56Z'>
		I've shared the small repo on BitBucket if you want to have a look (with raver119, I can add more users). The class is TrainLSTMFromBits. You can run it with one arg: data/latex/all.tex, it fails for me usually after iteration 40, but I have seen failures as far as iteration 71
		</comment>
		<comment id='5' author='fac2003' date='2016-05-17T20:14:34Z'>
		Can i have link to that repo?
		</comment>
		<comment id='6' author='fac2003' date='2016-05-17T20:16:35Z'>
		&lt;denchmark-link:https://bitbucket.org/campagnelaboratory/dl4c&gt;https://bitbucket.org/campagnelaboratory/dl4c&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='fac2003' date='2016-05-17T20:26:59Z'>
		"after iteration 40" sounds suspicious... I'll take a look later, thanks.
		</comment>
		<comment id='8' author='fac2003' date='2016-05-17T20:29:45Z'>
		I ran this for hours with rc3.8 without a glitch.
		</comment>
		<comment id='9' author='fac2003' date='2016-05-19T13:49:53Z'>
		In case anybody cares, I am running this on:
java version "1.8.0_65"
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)
		</comment>
		<comment id='10' author='fac2003' date='2016-05-19T18:30:38Z'>
		I was able to reproduce this with the GravesLSTMCharModelingExample with the following changes to hyperparameters:
int exampleLength = 300;
and
.learningRate(0.01)
I then get:
&lt;denchmark-code&gt;o.d.o.l.ScoreIterationListener - Score at iteration 414 is 149.3247007444239
o.d.o.l.ScoreIterationListener - Score at iteration 415 is 144.7411355886625
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x000000012f1a56e9, pid=63580, tid=5891
#
# JRE version: Java(TM) SE Runtime Environment (8.0_40-b27) (build 1.8.0_40-b27)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.40-b25 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# C  [libnd4j.dylib+0x236e9]  shape::TAD::init(int*, int*, int)+0x249
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /Users/fac2003/IdeaProjects/git/dl4j-0.4-examples/hs_err_pid63580.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

Process finished with exit code 134
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='fac2003' date='2016-05-20T00:37:34Z'>
		&lt;denchmark-link:https://github.com/fac2003&gt;@fac2003&lt;/denchmark-link&gt;
 thanks, I'll be looking into this today.
		</comment>
		<comment id='12' author='fac2003' date='2016-05-20T03:48:04Z'>
		Ok, so with those hyperparameters on the LSTM example: No issue for me (Windows 8.1); I ran through the full example.
So it may be platform specific.
&lt;denchmark-link:https://github.com/nyghtowl&gt;@nyghtowl&lt;/denchmark-link&gt;
 you are still on OSX, right? Could you try running this, see if it occurs for you?
		</comment>
		<comment id='13' author='fac2003' date='2016-05-20T04:01:11Z'>
		I tried this on two different macs (a laptop, a mac pro desktop), both running 10.11.4. Same errors on these two machines.
		</comment>
		<comment id='14' author='fac2003' date='2016-05-20T04:04:17Z'>
		One question though: does it always occur at the exact same iteration? Or does it occur at different times during training?
		</comment>
		<comment id='15' author='fac2003' date='2016-05-20T04:10:05Z'>
		Different steps with the same random seed, but I use multithreading, which could impact the iteration by timing differences.
		</comment>
		<comment id='16' author='fac2003' date='2016-05-20T05:06:30Z'>
		I also got core dump on my Mac for GravesLSTM but Linux was fine. &lt;denchmark-link:https://github.com/eraly&gt;@eraly&lt;/denchmark-link&gt;
 can you run this on your machine tomorrow and confirm if you are getting the same behavior?
		</comment>
		<comment id='17' author='fac2003' date='2016-05-20T05:08:56Z'>
		Could we try to narrow down a cause as well? I'd like to get a test case we
can put in to nd4j.
On Fri, May 20, 2016 at 2:06 PM, Melanie Warrick &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;

wrote:

I also got core dump on my Mac for GravesLSTM but Linux was fine. @eraly
https://github.com/eraly can you run this on your machine tomorrow and
confirm if you are getting the same behavior?
â€”
You are receiving this because you are subscribed to this thread.
Reply to this email directly or view it on GitHub
https://github.com/deeplearning4j/deeplearning4j/issues/1556#issuecomment-220518188

		</comment>
		<comment id='18' author='fac2003' date='2016-05-20T05:19:49Z'>
		J 1461  org.nd4j.nativeblas.NativeOps.execReduceFloat([JIJJJJJJI)V (0 bytes) @ 0x000000010d7df9b7 [0x000000010d7df940+0x77]
J 1986 C2 org.nd4j.linalg.cpu.nativecpu.ops.NativeOpExecutioner.exec(Lorg/nd4j/linalg/api/ops/Accumulation;[I)Lorg/nd4j/linalg/api/ndarray/INDArray; (1441 bytes) @ 0x000000010d9f5938 [0x000000010d9f4ec0+0xa78]
J 1985 C2 org.nd4j.linalg.api.ndarray.BaseNDArray.sum([I)Lorg/nd4j/linalg/api/ndarray/INDArray; (18 bytes) @ 0x000000010d9d9dfc [0x000000010d9d9b20+0x2dc]
So, it's a sum(0) - that's the only sum along dimensions that are done in LSTMs. They are always on f order, too.
Running the full nd4j test suite on a mac is probably a good start, in case we already have something that picks this up.
		</comment>
		<comment id='19' author='fac2003' date='2016-05-22T13:23:45Z'>
		I have the same problem in linux:
&lt;denchmark-link:https://gist.github.com/fcecconi/c9913857d0bec00959ef4cd48acbc8dc&gt;https://gist.github.com/fcecconi/c9913857d0bec00959ef4cd48acbc8dc&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='fac2003' date='2016-05-23T09:20:18Z'>
		Pull &lt;denchmark-link:https://github.com/deeplearning4j/libnd4j/pull/207&gt;deeplearning4j/libnd4j#207&lt;/denchmark-link&gt;
 should fix this.
		</comment>
		<comment id='21' author='fac2003' date='2016-06-03T20:37:40Z'>
		&lt;denchmark-link:https://github.com/fac2003&gt;@fac2003&lt;/denchmark-link&gt;
 can you try this again when we release 3.10 is out? Or if you are impatient you could always build from source? Thanks!
		</comment>
		<comment id='22' author='fac2003' date='2016-06-06T19:02:41Z'>
		I am trying this with 3.10 and it seems to hold up so far (one hour into training with 8 threads). I'll write only if I get a failure.
		</comment>
		<comment id='23' author='fac2003' date='2019-01-21T00:53:14Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>