<bug id='3927' author='shengc' open_date='2017-08-23T22:52:49Z' closed_time='2018-05-04T09:31:48Z'>
	<summary>flatten layer too restrictive</summary>
	<description>
let's say if we have training data like this
[brown fox jumps], [fox jumps over], [jumps over lazy], [over lazy dog]
I want to convert each word into its corresponding word vectors, and then concatenate these vectors to represent the training data. What I can do is I provide vector of the integers to an embedding layer whose weights are the real word vectors. Assuming size of the word vector is 100, so the output of the embedding layer for a training data would be 3 * 100. Now I want to flatten this matrix into a vector of size 300 * 1 before feeding it to the next layer just as I plan. Here comes the problem, dl4j does not provide a flatten layer out of the box, like Keras does. The preprocessor of dl4j layer only cares about cnn and rnn layers, but in my case, obviously this is a feed forward layer.  I am thinking Is it possible to add a convolutional layer right after the embedding layer to do the flattening ? If yes, how should I do it ? Thanks!
	</description>
	<comments>
		<comment id='1' author='shengc' date='2017-08-24T12:08:52Z'>
		&lt;denchmark-link:https://github.com/shengc&gt;@shengc&lt;/denchmark-link&gt;
 you're right in that we don't have a dl4j preprocessor in place that directly mimics what keras does, however for keras model import we do have a  layer that does what you want, see:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/KerasFlatten.java#L68-L90&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/KerasFlatten.java#L68-L90&lt;/denchmark-link&gt;

Also, in the same module we also have a more general Reshape preprocessor, see:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/preprocessors/ReshapePreprocessor.java&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/preprocessors/ReshapePreprocessor.java&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 maybe we should move this  preprocessor to the respective core module? Supporting  in general could also be of value.
		</comment>
		<comment id='2' author='shengc' date='2017-08-24T14:56:11Z'>
		&lt;denchmark-link:https://github.com/maxpumperla&gt;@maxpumperla&lt;/denchmark-link&gt;
 Thanks for the tip. It looks like  is not in 0.9.1 yet ? Also, how should I use it ? Does the below snippet make sense to you ?
val map = conf.getInputPreProcessors
// probably dl4j prohibits setting preprocessor on a vertex, so I will just set it on a layer.
map.put("hidden", new ReshapePreprocessor(Array(3, 100), Array(1, 300)))
conf.setInputPreProcessors(map)
val graph = new ComputationGraph(conf.build())
EDIT: In that class, at Line 87, why compare two arrays with !=? That seems straight wrong....
if (targetShape != output.shape) 
		</comment>
		<comment id='3' author='shengc' date='2017-08-25T15:18:50Z'>
		Ok I managed to let it work by hacking ReshapeProcessor. However the model behaves in a bizarre way in that the model score keeps bouncing back and forth, which seems that it has a hard time to converge. The prediction result is terrible, and by no means can be compared with the model for which I did all the heavy lifting to generate the concatenated word vectors as input. I don't think dl4j supports the notion of flattening in its current shape, and some superficial hack like what I did does not help. That being said, one of the reasons I chose dl4j is its ability to import the model of Keras. What I described can be easily achieved by writing a few keras code, but I dont think the model would be able to be imported into dl4j, which seems to be a deal breaker to me.
		</comment>
		<comment id='4' author='shengc' date='2018-05-04T09:31:48Z'>
		&lt;denchmark-link:https://github.com/shengc&gt;@shengc&lt;/denchmark-link&gt;
 embedding layer now produces 3d output reliably after import, so this should work:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/ebed91e9ab4d07a2599c38c322258cb8931f51e1/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/core/KerasFlatten.java#L89-L91&gt;https://github.com/deeplearning4j/deeplearning4j/blob/ebed91e9ab4d07a2599c38c322258cb8931f51e1/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/core/KerasFlatten.java#L89-L91&lt;/denchmark-link&gt;

Let me know if you need anything else.
		</comment>
		<comment id='5' author='shengc' date='2018-09-22T05:24:20Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>