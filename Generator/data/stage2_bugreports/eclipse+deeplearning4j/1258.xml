<bug id='1258' author='treo' open_date='2016-03-11T09:10:53Z' closed_time='2016-05-06T08:02:04Z'>
	<summary>Saved/Reloaded Network uses a lot less memory (rc3.8)</summary>
	<description>
I'm saving and reloading my network as described in &lt;denchmark-link:http://deeplearning4j.org/modelpersistence&gt;http://deeplearning4j.org/modelpersistence&lt;/denchmark-link&gt;
.
When the network is created the first time, it takes up a lot of memory (6GB used average, some times spiking to 10GB). But when the same network is reloaded (i.e. coefficients and updater coming from a file), then it uses a lot less memory (2.5GB on average, with spikes up to 3.5GB).
Why does it take so much more memory when the network is freshly created?
	</description>
	<comments>
		<comment id='1' author='treo' date='2016-03-11T10:02:59Z'>
		Well, that could be JVM GC, or memleak.
There's one issue opened that's hints possible memleak somewhere in pipeline.
After 3.9 codebase gets stable we'll look into this.
		</comment>
		<comment id='2' author='treo' date='2016-03-11T22:44:16Z'>
		See also: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/1021&gt;https://github.com/deeplearning4j/deeplearning4j/issues/1021&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='treo' date='2016-03-11T23:28:14Z'>
		I'm not quite sure they are related, as I continue training after reloading the network, and it does not continue to grow in memory consumption, like it seems to do when the network is first created.
		</comment>
		<comment id='4' author='treo' date='2016-05-01T12:23:55Z'>
		Is this still the case?
		</comment>
		<comment id='5' author='treo' date='2016-05-06T08:02:04Z'>
		Closing due to inactivity. The underlying issues appear to be fixed.
		</comment>
		<comment id='6' author='treo' date='2019-01-21T03:53:04Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>