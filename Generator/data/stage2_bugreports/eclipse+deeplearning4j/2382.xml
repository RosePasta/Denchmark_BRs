<bug id='2382' author='KonceptGeek' open_date='2016-11-28T01:51:23Z' closed_time='2016-11-28T19:40:59Z'>
	<summary>SkipGram NullPointerException</summary>
	<description>
I'm getting the following NullPointerException when calling Word2Vec.fit() after reloading the model.
&lt;denchmark-code&gt;Exception in thread "VectorCalculationsThread 0" Exception in thread "VectorCalculationsThread 3" java.lang.RuntimeException: java.lang.NullPointerException
	at org.deeplearning4j.models.sequencevectors.SequenceVectors$VectorCalculationsThread.run(SequenceVectors.java:1100)
Caused by: java.lang.NullPointerException
	at org.deeplearning4j.models.embeddings.learning.impl.elements.SkipGram.learnSequence(SkipGram.java:167)
	at org.deeplearning4j.models.sequencevectors.SequenceVectors.trainSequence(SequenceVectors.java:303)
	at org.deeplearning4j.models.sequencevectors.SequenceVectors$VectorCalculationsThread.run(SequenceVectors.java:1075)
java.lang.RuntimeException: java.lang.NullPointerException
	at org.deeplearning4j.models.sequencevectors.SequenceVectors$VectorCalculationsThread.run(SequenceVectors.java:1100)
Caused by: java.lang.NullPointerException
	at org.deeplearning4j.models.embeddings.learning.impl.elements.SkipGram.learnSequence(SkipGram.java:167)
	at org.deeplearning4j.models.sequencevectors.SequenceVectors.trainSequence(SequenceVectors.java:303)
	at org.deeplearning4j.models.sequencevectors.SequenceVectors$VectorCalculationsThread.run(SequenceVectors.java:1075)
&lt;/denchmark-code&gt;

The model is loaded using the following command:
&lt;denchmark-code&gt;Word2Vec word2Vec = WordVectorSerializer.readWord2VecModel(new File(modelPath), true);
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='KonceptGeek' date='2016-11-28T02:03:15Z'>
		FYI, looks like batches.get() is returning null.
The following code in the function iterateSample is executed:
&lt;denchmark-code&gt;if (batches.get() == null) {
    batches.set(new ArrayList&lt;Aggregate&gt;());
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='KonceptGeek' date='2016-11-28T07:01:16Z'>
		Show full model configuration please.
		</comment>
		<comment id='3' author='KonceptGeek' date='2016-11-28T09:36:28Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 following was the configuration used:
&lt;denchmark-code&gt;word2Vec = new Word2Vec.Builder()
                .minWordFrequency(10)
                .layerSize(200)
                .learningRate(0.025)
                .minLearningRate(0.0001)
                .windowSize(5)
                .sampling(0.001)
                .seed(1)
                .workers(8)
                .negativeSample(5)
                .iterations(5)
                .stopWords(new ArrayList&lt;String&gt;())
                .elementsLearningAlgorithm(new SkipGram&lt;VocabWord&gt;())
                .useHierarchicSoftmax(false)
                .allowParallelTokenization(false)
                .tokenizerFactory(tokenizerFactory)
                .iterate(word2VecContentIterator)
                .build();
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='KonceptGeek' date='2016-11-28T09:37:43Z'>
		Thanks, i'll try to reproduce it.
		</comment>
		<comment id='5' author='KonceptGeek' date='2016-11-28T10:03:28Z'>
		Please take a look into this test, does it looks close enough to what you trying to do?
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/70ff36760fbb863cd7622fead582d75935098ef8/deeplearning4j-nlp-parent/deeplearning4j-nlp-uima/src/test/java/org/deeplearning4j/models/word2vec/Word2VecTests.java#L333-L333&gt;https://github.com/deeplearning4j/deeplearning4j/blob/70ff36760fbb863cd7622fead582d75935098ef8/deeplearning4j-nlp-parent/deeplearning4j-nlp-uima/src/test/java/org/deeplearning4j/models/word2vec/Word2VecTests.java#L333-L333&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='KonceptGeek' date='2016-11-28T10:07:31Z'>
		TL/DR: We train model, using negSampling. Saving it. Restoring. Attaching new iter/tokenizer, and do training.
For me in both cases i don't see any exceptions.
		</comment>
		<comment id='7' author='KonceptGeek' date='2016-11-28T10:08:57Z'>
		Here's my output: &lt;denchmark-link:https://gist.github.com/raver119/6ce0250d1b0b83356e7113c844116925&gt;https://gist.github.com/raver119/6ce0250d1b0b83356e7113c844116925&lt;/denchmark-link&gt;

For Fit 3 i get only one line reported, because iterations = 1 by default, since it's reduced vectors were restored.
		</comment>
		<comment id='8' author='KonceptGeek' date='2016-11-28T10:10:19Z'>
		Pretty close, except I'm using default batch size and parallel tokenization is set to false. Another thing, after the reloading of the model, the number of documents provided to the iterator can be less than the batch size. For example, if the batch size is 512, it is possible that a CollectionSentenceIterator was created with list of size 50.
		</comment>
		<comment id='9' author='KonceptGeek' date='2016-11-28T10:15:42Z'>
		Batch size doesn't matter here, it's basically just a variable that defines runtime partitioning for the corpus AND acts as trigger for op batches aggregation. So even batchSize of 64 or 1 will work fine, just will be a bit slower.
		</comment>
		<comment id='10' author='KonceptGeek' date='2016-11-28T10:17:39Z'>
		Another thing, this exception does not occur all the time. Following is the current log of 2 consecutive fit calls:
&lt;denchmark-code&gt;2016-11-28 10:09:49,941 [pool-4-thread-1] INFO  o.d.m.s.SequenceVectors - Starting learning process...
Exception in thread "VectorCalculationsThread 14" java.lang.RuntimeException: java.lang.NullPointerException
        at org.deeplearning4j.models.sequencevectors.SequenceVectors$VectorCalculationsThread.run(SequenceVectors.java:1100)
Caused by: java.lang.NullPointerException
2016-11-28 10:10:22,340 [pool-4-thread-1] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3706980];  Lines vectorized so far: [47365]; learningRate: [1.0E-4]
2016-11-28 10:10:22,340 [pool-4-thread-1] INFO  o.d.m.s.SequenceVectors - Time spent on training: 32399 ms
2016-11-28 10:10:23,392 [pool-4-thread-1] INFO  o.d.m.s.SequenceVectors - Starting learning process...
2016-11-28 10:10:56,848 [pool-4-thread-1] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [3877685];  Lines vectorized so far: [49925]; learningRate: [1.0E-4]
2016-11-28 10:10:56,848 [pool-4-thread-1] INFO  o.d.m.s.SequenceVectors - Time spent on training: 33456 ms
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='KonceptGeek' date='2016-11-28T10:18:48Z'>
		That's awesome, show full output log please!
		</comment>
		<comment id='12' author='KonceptGeek' date='2016-11-28T10:22:02Z'>
		Aha, i probably have idea what's wrong.
Is there ANY chance, that your second corpus you're using for training after model restored, contains sentences that have not a single word matching first corpus?
		</comment>
		<comment id='13' author='KonceptGeek' date='2016-11-28T10:27:11Z'>
		The full output is pretty huge, but i just took out some of the lines from it: &lt;denchmark-link:https://gist.github.com/KonceptGeek/20f8683dc5199e23af9ea76a3df3ca13&gt;https://gist.github.com/KonceptGeek/20f8683dc5199e23af9ea76a3df3ca13&lt;/denchmark-link&gt;

With regards to the 2nd corpus not have sentences containing a single word in the vocab: Yes, that is totally possible. This test corpus that I am using has approximately 15 million documents. 500k of which were used to build the vocab and initial training, and then chunks of the remaining are being used to update the weights of the model.
		</comment>
		<comment id='14' author='KonceptGeek' date='2016-11-28T10:28:36Z'>
		Right. Bug found and fixed then, will merge it later today, and it'll be included into 0.7.1 release coming out soon.
Thanks for highlighting this one.
		</comment>
		<comment id='15' author='KonceptGeek' date='2016-11-28T10:34:50Z'>
		Any recommended solution for handling this right now on my end?
		</comment>
		<comment id='16' author='KonceptGeek' date='2016-11-28T10:37:49Z'>
		I'd suggest to add 2 dummy words manually to each document/sentence with custom tokenizer, i.e. "this" word.
As fast workaround that should work.
But main problem here is small vocab you have after initial training. I'd suggest to build proper vocab at initial training, and in subsequent training you won't suffer anymore.
		</comment>
		<comment id='17' author='KonceptGeek' date='2016-11-28T19:40:59Z'>
		Fix is merged now.
		</comment>
		<comment id='18' author='KonceptGeek' date='2019-01-20T10:09:06Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>