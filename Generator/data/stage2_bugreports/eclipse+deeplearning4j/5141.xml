<bug id='5141' author='AlexDBlack' open_date='2018-05-11T11:58:53Z' closed_time='2018-05-16T05:43:34Z'>
	<summary>ParallelInference tests failing</summary>
	<description>
Passes on native backend ok.
Windows 10, Titan X, CUDA 9.1
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/39923296-6c6bd44e-5566-11e8-98cc-08bbbe9b4860.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-05-12T01:48:30Z'>
		Looks like: the testInferenceX tests are simply flaky - there's inherent randomness in terms of the disribution of tasks between threads, and sometimes the tests will randomly fail as a result.
The testInputMasking failure looks legitimate, however.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-05-12T03:54:23Z'>
		testInferenceX failures handled here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5145&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5145&lt;/denchmark-link&gt;

testInputMasking failure has a low probability of occurring, but the failure when it does occur looks like a bug.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-05-14T11:41:58Z'>
		So: It turns out that this doesn't have anything to do with parallel inference... this is sufficient to reproduce it:
&lt;denchmark-code&gt;    @Test
    public void batch1(){

        int nIn = 10;
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .activation(Activation.TANH)
                .seed(12345)
                .list()
                .layer(new LSTM.Builder().nIn(nIn).nOut(5).build())
                .layer(new GlobalPoolingLayer(PoolingType.AVG))
                .layer(new OutputLayer.Builder().nIn(5).nOut(5).build())
                .build();

        MultiLayerNetwork net = new MultiLayerNetwork(conf);
        net.init();

        double[] d = new double[]{0.023977466, 0.021927578, 0.5638201, 0.4765249, 0.16738842, 0.998578, 0.7989319, 0.048469864, 0.04674433, 0.4631205, 0.055117667, 0.5074715, 0.6624276, 0.27351934, 0.23996872, 0.81527346, 0.37917703, 0.22173452, 0.05470153, 0.4437007, 0.542513, 0.6791057, 0.93314016, 0.7195189, 0.17417517, 0.8054955, 0.7770213, 0.44565812, 0.6900083, 0.38784736};
        INDArray arr = Nd4j.create(d, new int[]{3,10,1}, new int[]{1,3,30}, 0);

        INDArray outBatch = net.output(arr);
        for( int i=0; i&lt;3; i++ ){
            INDArray inSingle = arr.get(NDArrayIndex.interval(i,i,true), NDArrayIndex.all(), NDArrayIndex.all());
            INDArray outSingle = net.output(inSingle);

            INDArray outBatchSubset = outBatch.get(NDArrayIndex.interval(i,i,true), NDArrayIndex.all());

            assertEquals(outSingle, outBatchSubset);
        }
    }
&lt;/denchmark-code&gt;

Maybe global pooling edge case?
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-05-14T12:32:19Z'>
		Yep, definitely not related to ParallelInference at all... it's an ND4J reduction edge case:
&lt;denchmark-code&gt;    @Test
    public void testMeanEdgeCase(){
        INDArray arr = Nd4j.rand(new int[]{3,10,1}, 12345).dup('f');
        INDArray arr2 = arr.mean(2);

        INDArray exp = arr.get(NDArrayIndex.all(), NDArrayIndex.all(), NDArrayIndex.point(0));

        assertEquals(exp, arr2);
    }
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='AlexDBlack' date='2018-05-14T12:38:50Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 does that edge case impact global pooling? I'm seeing some prediction discrepancies for imported keras models in global pooling, maybe that's related?!
		</comment>
		<comment id='6' author='AlexDBlack' date='2018-05-15T00:07:45Z'>
		&lt;denchmark-link:https://github.com/maxpumperla&gt;@maxpumperla&lt;/denchmark-link&gt;
 yes, it was/is impacting global pooling - average pooling with sequence length == 1 and minibatch &gt; 1
		</comment>
		<comment id='7' author='AlexDBlack' date='2018-05-16T05:43:34Z'>
		Confirmed fixed and passing.
		</comment>
		<comment id='8' author='AlexDBlack' date='2018-09-22T01:24:18Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>