<bug id='1230' author='Habitats' open_date='2016-03-04T12:26:10Z' closed_time='2016-03-08T11:31:25Z'>
	<summary>Evaluation object reporting wrong counts for TP, FP, TN, FN</summary>
	<description>
For a binary classification with two outputs, eihter [1,0] or [0,1], the following outputs are generated from the Evalution object:
&lt;denchmark-code&gt;The following confusion matrix:
                  Predicted Class
                       0     1 Total
Actual Class     0  1500     2  1502
                 1    68   131   199
             Total  1568   133

And the following counts:
    TP     FP     FN     TN 
  1631     70     70   1631 

i.e. --&gt; TP == TN == (confusionTP + confusionTN) and FN == FP == (confusionFP + confusionFN)
&lt;/denchmark-code&gt;

The relevant lines in Evaluation are 138-168, in the eval() method. The method increments two counters for every given example, when it really should be incrementing _one _of them.
Problem is present both in 3.8 and 3.9-SNAPSHOT.
	</description>
	<comments>
		<comment id='1' author='Habitats' date='2016-03-07T11:07:26Z'>
		Referring to this: &lt;denchmark-link:http://scikit-learn.org/stable/modules/model_evaluation.html&gt;http://scikit-learn.org/stable/modules/model_evaluation.html&lt;/denchmark-link&gt;

"In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario."
So yes, the values returned by the truePositives() and trueNegatives() etc methods are wrong, in so far as they return the sum of truePositives/negatives/etc for all classes.
The problem here is that in the multiclass classification, there is no single true/false positive/negative values - these binary metrics are only defined for each class not for the classification problem as a whole. I'm thinking that having truePositives()/trueNegatives()/etc methods that return a double doesn't make sense in the context of multiclass classification. Perhaps instead these methods should return true positives/negatives of the binary problems per class (i.e., a list or map).
Now, for binary it does make sense to return a double, so the alternative is to make these methods a special case for binary only (I don't really like this, but perhaps it'd be fine if we threw an exception for non-binary evaluation). However we also have the issue of deciding which (0 or 1) is the positive class? (Do we hard-code this to class 0 by conversion?)
		</comment>
		<comment id='2' author='Habitats' date='2016-03-07T14:54:08Z'>
		
Perhaps instead these methods should return true positives/negatives of the binary problems per class (i.e., a list or map).

I think this is the most sensible solution. This information is already present in the confusion matrix, but at the moment they need to be extracted manually.
Also, why are these methods even returning doubles in the first place? In which scenario are they something other than discrete counts?
		</comment>
		<comment id='3' author='Habitats' date='2016-03-07T22:39:02Z'>
		Right. They are discrete counts, so double doesn't make sense even in the binary case (int does though).
I'll change those truePositive()/Negative()/etc methods to return a map instead.
		</comment>
		<comment id='4' author='Habitats' date='2016-03-07T23:07:49Z'>
		That would be neat!
		</comment>
		<comment id='5' author='Habitats' date='2016-03-08T08:45:19Z'>
		Any timeframe, btw?
		</comment>
		<comment id='6' author='Habitats' date='2019-01-21T09:37:22Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>