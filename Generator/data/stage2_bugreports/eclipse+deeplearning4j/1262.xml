<bug id='1262' author='zzyxzz' open_date='2016-03-11T14:28:52Z' closed_time='2016-03-13T10:35:46Z'>
	<summary>Word2Vec always stops working after a few hours runnning</summary>
	<description>
After some point, the program will be extremely slow and waiting for some signal. Then this state will last for a very long time until I kill the process. It runs on cluster node with text corpus around 2G in size.
The word2vec program runs on cluster node using jdk1.8.0_60 -Xmx40g and environment:
Linux 2.6.32-358.14.1.el6.x86_64 &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/1&gt;#1&lt;/denchmark-link&gt;
 SMP Tue Jul 16 14:24:33 CDT 2013 x86_64 x86_64 x86_64 GNU/Linux
Intel(R) Xeon(R) CPU E5-2407 0 @ 2.20GHz
MemTotal:       32771548 kB
Parameters of word2vec: context window 5; batch size: 50; layer size: 800
The POM file : &lt;denchmark-link:https://gist.github.com/zzyxzz/e15705c8ae6b06d81538&gt;https://gist.github.com/zzyxzz/e15705c8ae6b06d81538&lt;/denchmark-link&gt;

THe Source code: &lt;denchmark-link:https://gist.github.com/zzyxzz/bd3589bf5fe398400b35&gt;https://gist.github.com/zzyxzz/bd3589bf5fe398400b35&lt;/denchmark-link&gt;

The Jstack trace: &lt;denchmark-link:https://gist.github.com/zzyxzz/a153e17ae35a0572d039&gt;https://gist.github.com/zzyxzz/a153e17ae35a0572d039&lt;/denchmark-link&gt;

The full log info: &lt;denchmark-link:https://gist.github.com/zzyxzz/aa5d8d2c42d49860d218&gt;https://gist.github.com/zzyxzz/aa5d8d2c42d49860d218&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='zzyxzz' date='2016-03-11T18:24:54Z'>
		Below are my implementations of tokenizer:
&lt;denchmark-link:https://gist.github.com/zzyxzz/5d58e0f25be302ce84b8&gt;https://gist.github.com/zzyxzz/5d58e0f25be302ce84b8&lt;/denchmark-link&gt;

&lt;denchmark-link:https://gist.github.com/zzyxzz/f0b76c37d593ee89ce83&gt;https://gist.github.com/zzyxzz/f0b76c37d593ee89ce83&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='zzyxzz' date='2016-03-12T21:23:07Z'>
		I've got few "test" datasets. The closes one to yours is 1.5GB.
Used dataset is properly preprocessed prior to use: 1 sentence per line, everything is stemmed in advance, etc
Also, I've used a bit different settings: 150 layerSize and 2 iterations, since the goal is to check how time varies from the beginning to the end, and not blas performance on huge vectors.
From the beginning to the end of vectorization  time is close to be the same it varies within 20-30 seconds per 100k lines. That mostly depends on number of words in sentence.
Also, as i can see with JVisualVM profiler, memory use is almost constant, gc activity is low.
85% of userspace time is consumed by blas axpy/dot calls.
I just don't see any degradation related to nd4 or seqvec/w2v here.
Here's the screenshots from profiler:
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/12250879/13725393/4145e1d4-e8b1-11e5-8d6e-4c6dc5386634.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/12250879/13725394/48902364-e8b1-11e5-9648-51710a6af4e1.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://cloud.githubusercontent.com/assets/12250879/13725396/4d7cd2b4-e8b1-11e5-9a04-12dc31b61f3c.png&gt;&lt;/denchmark-link&gt;

So, right now i'm definitely unable to confirm/reproduce reported issue with rc3.8.
I'll do some digging after rc3.9 becomes stable, though.
		</comment>
		<comment id='3' author='zzyxzz' date='2016-03-13T10:35:46Z'>
		I've launched w2v tonight, with higher params, and still unable to reproduce your issue.
02:26:18.880 [main] INFO  o.d.m.w.wordstore.VocabConstructor - Sequences checked: [20388448], Current vocabulary size: [540857]
05:33:48.487 [main] INFO  o.d.m.s.SequenceVectors - Epoch: [1]; Words vectorized so far: [290374642];  Lines vectorized so far: [40444556]; learningRate: [0.01]
Time between reports was variating in the same boundaries as before.
		</comment>
		<comment id='4' author='zzyxzz' date='2019-01-21T08:37:58Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>