<bug id='5537' author='AlexDBlack' open_date='2018-06-09T04:08:03Z' closed_time='2018-06-12T08:42:13Z'>
	<summary>DL4J: CuDNN LSTM helper dropout use incorrect?</summary>
	<description>
setDropoutDescriptor is costly (should be called only once):
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/883f23cb4f3bcbd23387e79b4aaa48313f2ec963/deeplearning4j/deeplearning4j-cuda/src/main/java/org/deeplearning4j/nn/layers/recurrent/CudnnLSTMHelper.java#L456-L457&gt;https://github.com/deeplearning4j/deeplearning4j/blob/883f23cb4f3bcbd23387e79b4aaa48313f2ec963/deeplearning4j/deeplearning4j-cuda/src/main/java/org/deeplearning4j/nn/layers/recurrent/CudnnLSTMHelper.java#L456-L457&lt;/denchmark-link&gt;

Furthermore, dropout is (or should be?) handled outside of the LSTM helper, as per the other layers.
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-06-12T08:42:13Z'>
		Turns out: the dropout descriptor is only used  layers, not within layers, hence is not double applied. However, the initialization should only be applied once, which as been fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5560&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5560&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='AlexDBlack' date='2018-09-21T19:59:22Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>