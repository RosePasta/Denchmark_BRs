<bug id='4964' author='ospavel' open_date='2018-04-23T22:40:14Z' closed_time='2018-04-30T06:00:14Z'>
	<summary>Complex TransferLearning graph - no updates of first layer</summary>
	<description>
DL4J 1.0.0-alpha, Native CPU
I create rnn model, train it and expand by adding second inputs with rnn layers. I use LastTimeStepVertex because of rnn of different branches use different timeseries length and I need to merge them.
The problem is that during training the first lstm layer of second branch (added) have no changes of Update and Ratio (seen in UI).
The code
&lt;denchmark-code&gt;FineTuneConfiguration fineTuneConf = new FineTuneConfiguration.Builder()
        .seed(1234567)
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .updater(new Adam(0.1)).l2(1e-5).activation(Activation.TANH).weightInit(WeightInit.XAVIER).build();

ComputationGraphConfiguration graph1Conf = new NeuralNetConfiguration.Builder()
        .seed(12345)
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .updater(new Adam(0.1)).l2(1e-5).activation(Activation.TANH).weightInit(WeightInit.XAVIER)        .trainingWorkspaceMode(WorkspaceMode.SEPARATE).inferenceWorkspaceMode(WorkspaceMode.SINGLE)
        .graphBuilder()
        .addInputs("M1_In")
        .addLayer("M1_0", new GravesLSTM.Builder().nIn(43).nOut(8).build(), "M1_In")
        .addLayer("M1_1", new GravesLSTM.Builder().nIn(8).nOut(8).build(), "M1_0")
        .addVertex("M1_Last", new LastTimeStepVertex("M1_In"), "M1_1")
        .addLayer("M1_Out", new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(8).nOut(2).build(), "M1_Last")
        .setOutputs("M1_Out")
        .pretrain(false).backprop(true).build();

ComputationGraph graph1 = new ComputationGraph(graph1Conf);
graph1.init();

// ... here is training of model 1

ComputationGraph graph = new @TransferLearning.GraphBuilder(graph1)
        .fineTuneConfiguration(fineTuneConf)
        .setFeatureExtractor("M1_Last")
        .removeVertexKeepConnections("M1_Out")
        .addInputs("M2_In")
        .addLayer("M2_0", new GravesLSTM.Builder().nIn(43).nOut(8).build(), "M2_In")
        .addLayer("M2_1", new GravesLSTM.Builder().nIn(8).nOut(8).build(), "M2_0")
        .addVertex("M2_Last", new LastTimeStepVertex("M2_In"), "M2_1")
        .addLayer("Merge", new DenseLayer.Builder().nIn(16).nOut(4).build(), "M1_Last", "M2_Last")
        .addLayer("Output", new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).weightInit(WeightInit.XAVIER).nIn(4).nOut(2).build(), "Merge")
        .setOutputs("Output")
        .build();

// ... here is training of model
&lt;/denchmark-code&gt;

Model structure and no updates for layer M2_0
&lt;denchmark-link:https://camo.githubusercontent.com/91bec4d205962945a3ab3c7d575a152f23fabdb71f5a16ca2709240b0d7d0044/687474703a2f2f6f6936372e74696e797069632e636f6d2f32347931636d642e6a7067&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='ospavel' date='2018-04-23T22:42:04Z'>
		&lt;denchmark-link:https://github.com/treo&gt;@treo&lt;/denchmark-link&gt;
, I sent test data into PM
		</comment>
		<comment id='2' author='ospavel' date='2018-04-28T13:22:57Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;

It looks like this is a legitimate bug. We currently abort calculating more gradients once the first frozen layer according to the topological sort is found (&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L2220-L2235&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L2220-L2235&lt;/denchmark-link&gt;
)
In this configuration this results in the following iteration order (Names as in the screenshot above):
Output, Merge, Merge-merge, M2_Last, M1_Last, M2_1, M1_1, M2_0, M1_0, M2_In, M1_in
Due to short circuiting rules, gradient propagation stops at the first frozen layer (1.0.0-alpha):
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/deeplearning4j-1.0.0-alpha/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1985&gt;https://github.com/deeplearning4j/deeplearning4j/blob/deeplearning4j-1.0.0-alpha/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/graph/ComputationGraph.java#L1985&lt;/denchmark-link&gt;

In this case that is M1_1. Therefore there is no gradient calculated for M2_0. Now with the changes in master, the short circuit rules have changed a bit, and just resetting the hitFrozen Flag on each iteration through the vertices seems to solve the issue.
But I'm not quite sure if that is really the proper solution, or if the topological sort should result in a different ordering if parts of the graph are frozen.
		</comment>
		<comment id='3' author='ospavel' date='2018-09-22T06:24:30Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>