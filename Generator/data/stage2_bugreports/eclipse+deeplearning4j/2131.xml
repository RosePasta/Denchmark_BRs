<bug id='2131' author='AlexanderSerbul' open_date='2016-09-25T13:56:53Z' closed_time='2016-10-05T10:38:38Z'>
	<summary>XOR simple nn model fits on 0.5.0 and does not fit on 0.6.0</summary>
	<description>
Good day!
Windows 10, java 7, eclipse mars.
Updated to 0.6.0 and simple example XOR nn model does not works.
Maven configs for 0.5 and 0.6:
&lt;denchmark-link:https://gist.github.com/AlexanderSerbul/0b44ad2d59e5fc70bd2101bab9b9f1d7&gt;https://gist.github.com/AlexanderSerbul/0b44ad2d59e5fc70bd2101bab9b9f1d7&lt;/denchmark-link&gt;

&lt;denchmark-link:https://gist.github.com/AlexanderSerbul/838a20f7e7007e017d6f9bf0c22387e7&gt;https://gist.github.com/AlexanderSerbul/838a20f7e7007e017d6f9bf0c22387e7&lt;/denchmark-link&gt;

Simple full test code:
&lt;denchmark-link:https://gist.github.com/AlexanderSerbul/3322efe6b493bea920fdc738aaa1ec60&gt;https://gist.github.com/AlexanderSerbul/3322efe6b493bea920fdc738aaa1ec60&lt;/denchmark-link&gt;

Output of code, working on 0.5 and not working on 0.6:
&lt;denchmark-link:https://gist.github.com/AlexanderSerbul/05dd336b5a6dc95ca9489805e8eb62d1&gt;https://gist.github.com/AlexanderSerbul/05dd336b5a6dc95ca9489805e8eb62d1&lt;/denchmark-link&gt;

And this code also works OK on 0.5 and does not work on 0.6, too: &lt;denchmark-link:https://gist.github.com/AlexanderSerbul/5747cb9286b282063b91f05876b0822b&gt;https://gist.github.com/AlexanderSerbul/5747cb9286b282063b91f05876b0822b&lt;/denchmark-link&gt;

Thank you!
	</description>
	<comments>
		<comment id='1' author='AlexanderSerbul' date='2016-09-27T12:05:25Z'>
		Hi, I observed that when using the "sigmoid" activation function for the output layer (as found in the basic XorExample), it does not reach 100%, while it does when switching to "softmax" function.
		</comment>
		<comment id='2' author='AlexanderSerbul' date='2016-10-05T10:38:38Z'>
		Fixed here: &lt;denchmark-link:https://github.com/eclipse/deeplearning4j-examples/pull/248&gt;eclipse/deeplearning4j-examples#248&lt;/denchmark-link&gt;

Turns out the good behaviour on 0.5.0 was due to an incorrect implementation of negative log likelihood when using sigmoid activation function. That has since been fixed with the loss function rewrite as of 0.6.0.
The "bad" performance with 0.6.0 is due to the fact that the loss and activation functions are mismatched. With NLL + sigmoid, the muli(labels) means that there is no score/gradient for the label = 0 case. Consequently, both outputs are driven to 1.0 for all inputs/labels - for that loss/activation function combo, that is indeed a local minima. Correct config is softmax + mcxent/nll, or sigmoid + binary xent (former is preferred).
		</comment>
		<comment id='3' author='AlexanderSerbul' date='2019-01-20T17:57:00Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>