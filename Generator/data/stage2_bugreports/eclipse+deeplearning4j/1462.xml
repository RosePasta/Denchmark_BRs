<bug id='1462' author='crockpotveggies' open_date='2016-04-28T19:02:35Z' closed_time='2016-08-12T09:33:08Z'>
	<summary>All iteration scores are NaN</summary>
	<description>
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/1445295/14897836/7d66934c-0d38-11e6-9c25-66ef2df15e3f.png&gt;&lt;/denchmark-link&gt;

I'm not able to view the score vs. iteration on the latest master when using either regularization(true) and regularization(false). I'm using the EarlyStoppingTrainer on a dataset of &lt;10k black and white JPEGs and here are my configs:
Convolutional net: &lt;denchmark-link:https://gist.github.com/crockpotveggies/d342bbb8a614fc88286bb8e75a8454e1&gt;https://gist.github.com/crockpotveggies/d342bbb8a614fc88286bb8e75a8454e1&lt;/denchmark-link&gt;

Training implementation: &lt;denchmark-link:https://gist.github.com/crockpotveggies/54fd6c86bccc4822ddf2f1c8f0a3d42c&gt;https://gist.github.com/crockpotveggies/54fd6c86bccc4822ddf2f1c8f0a3d42c&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='crockpotveggies' date='2016-04-28T20:23:48Z'>
		Confirmed it's not related to the UI. Put in a quick ScoreIterationListener for the following:
&lt;denchmark-code&gt;o.d.o.l.ScoreIterationListener - Score at iteration 0 is NaN
o.d.o.l.ScoreIterationListener - Score at iteration 1 is NaN
&lt;/denchmark-code&gt;

This particular net model was working just fine in RC 3.8. Not sure if it's related to the fact that I'm now using an EarlyStoppingTrainer.
		</comment>
		<comment id='2' author='crockpotveggies' date='2016-05-01T12:17:22Z'>
		Could you try this again with recent changes?
		</comment>
		<comment id='3' author='crockpotveggies' date='2016-05-02T17:58:14Z'>
		&lt;denchmark-link:https://cloud.githubusercontent.com/assets/1445295/14962667/db100ac2-1054-11e6-9012-20d94a3b82f1.png&gt;&lt;/denchmark-link&gt;

Still getting NaN. I fixed an exception at runtime that told me that dropConnect() was incorrectly set to true which I thought may have fixed the problem. However, it still persists. What debugging information can I provide?
		</comment>
		<comment id='4' author='crockpotveggies' date='2016-05-02T18:53:11Z'>
		&lt;denchmark-link:https://github.com/agibsonccc&gt;@agibsonccc&lt;/denchmark-link&gt;
 also noticed you tagged it as GUI but I'm seeing this in  as well
		</comment>
		<comment id='5' author='crockpotveggies' date='2016-05-06T16:46:52Z'>
		The problem causing &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/issues/897#issuecomment-217370843&gt;deeplearning4j/nd4j#897 (comment)&lt;/denchmark-link&gt;
 also appears related to this problem as well.
		</comment>
		<comment id='6' author='crockpotveggies' date='2016-08-12T09:33:07Z'>
		Issue is pretty outdated, so closing it now.
		</comment>
		<comment id='7' author='crockpotveggies' date='2016-09-07T15:23:40Z'>
		I've got the same issue that of Score at iteration X is NaN and cannot find any fix. How did you manage to overcome this issue?
		</comment>
		<comment id='8' author='crockpotveggies' date='2017-01-13T20:30:49Z'>
		I also got the same issue that Score at iteration X is NaN. I search with Google, but still have not found a solution.
		</comment>
		<comment id='9' author='crockpotveggies' date='2017-01-13T20:33:50Z'>
		For future users viewing this issue, documentation is going to be your best friend here:
&lt;denchmark-link:https://deeplearning4j.org/troubleshootingneuralnets&gt;https://deeplearning4j.org/troubleshootingneuralnets&lt;/denchmark-link&gt;

&lt;denchmark-link:https://deeplearning4j.org/visualization&gt;https://deeplearning4j.org/visualization&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='crockpotveggies' date='2018-04-02T16:10:32Z'>
		Facing the same issue here (not UI related). I did have a look at all hints and related bugs. I have the same issue wit hthe ScoreIterationListener.
Class ScoreIterationListener methid iterationDone says: Score at iteration 0 is NaN
This holds true for subsequent iterations and does not go away (tried until 30000).
What I did so far is

reviewed network config, reduced number of nodes in the net layers
ensured input features / outputs are between -1 and 1: did apply normalizers, etc.
exchange GPU with CPU backend
I read all (hope I really got all!) defects around NaN scores on a model. Am I missing something obvious here?

Network looks as follows:
final int lstmLayerSize = 10; // reduced this. was 250, 150 and 50
final int tbpttLength = 30; //also reduced this just to play around
return new NeuralNetConfiguration.Builder()
.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
.iterations(iterations)
.learningRate(learningRate).seed(12345)
.weightInit(WeightInit.XAVIER).updater(Updater.RMSPROP).rmsDecay(0.95).list()
.layer(0,
new GravesLSTM.Builder().nIn(numInputs).nOut(lstmLayerSize)
.activation(Activation.SIGMOID).build())
.layer(1,
new GravesLSTM.Builder().nIn(lstmLayerSize).nOut(lstmLayerSize / 2)
.activation(Activation.SIGMOID).build())
.layer(2,
new RnnOutputLayer.Builder(LossFunctions.LossFunction.MSE)
.activation(Activation.SIGMOID)
.nIn(lstmLayerSize / 2).nOut(numOutputs).build())
.backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(tbpttLength)
.tBPTTBackwardLength(tbpttLength).pretrain(false).backprop(true)
.build();
		</comment>
		<comment id='11' author='crockpotveggies' date='2018-04-02T16:12:42Z'>
		&lt;denchmark-link:https://github.com/sascha08-15&gt;@sascha08-15&lt;/denchmark-link&gt;
 your problem isn't a bug or something. your net just doesn't learn, and math goes off somewhere internally. Check those 2 links above in this issue.
		</comment>
		<comment id='12' author='crockpotveggies' date='2018-04-02T16:13:23Z'>
		p.s. hey &lt;denchmark-link:https://github.com/crockpotveggies&gt;@crockpotveggies&lt;/denchmark-link&gt;
 ! :)
		</comment>
		<comment id='13' author='crockpotveggies' date='2018-04-02T16:40:31Z'>
		Also suggest giving &lt;denchmark-link:https://github.com/deeplearning4j/Arbiter&gt;https://github.com/deeplearning4j/Arbiter&lt;/denchmark-link&gt;
 a shot here since you have a few hyperparameters that could benefit from a search.
		</comment>
	</comments>
</bug>