<bug id='4393' author='gagatust' open_date='2017-12-10T16:21:18Z' closed_time='2018-04-25T23:41:34Z'>
	<summary>Nd4j perfomance</summary>
	<description>
I ran the test to compare the performance java vs nd4j with small N
Test &lt;denchmark-link:https://github.com/deeplearning4j/libnd4j/issues/158&gt;deeplearning4j/libnd4j#158&lt;/denchmark-link&gt;

Log, N = 1000
&lt;denchmark-code&gt;double[]...
time: 38
NDArray...
time: 366
&lt;/denchmark-code&gt;

Log, N = 100
&lt;denchmark-code&gt;double[]...
time: 6
NDArray...
time: 377
&lt;/denchmark-code&gt;

Log, N = 10
&lt;denchmark-code&gt;double[]...
time: 2
NDArray...
time: 217
&lt;/denchmark-code&gt;

Win7 64 Pro, i5-3470, 16 Gb.
&lt;dependency&gt;
     &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
     &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;
     &lt;version&gt;0.9.1&lt;/version&gt;
&lt;/dependency&gt;

	</description>
	<comments>
		<comment id='1' author='gagatust' date='2017-12-10T16:24:06Z'>
		Problem is: when we're calling any reduce-to-number operation, we're creating NDArray for result. And in case of tiny arrays - array creation takes way more time then operation itself.
For N = 100, operation takes 18% of time. And the rest of time was spent on array creation/assign of initial value/ etc.
		</comment>
		<comment id='2' author='gagatust' date='2017-12-10T19:13:52Z'>
		Another part of the problem (minor in this case, though) was in too low element &amp; tad thresholds set by default. This is fixed now.
		</comment>
		<comment id='3' author='gagatust' date='2018-09-22T17:13:36Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>