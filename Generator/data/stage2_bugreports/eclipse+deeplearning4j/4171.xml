<bug id='4171' author='eypros' open_date='2017-10-13T14:19:34Z' closed_time='2018-07-25T09:01:23Z'>
	<summary>Different learning policies are not applicable to transfer-learning example</summary>
	<description>
I modified the code of EditLastLayerOthersFrozen to add a different learning policy. At first Inverse and then Step. The problem is I am getting an

IllegalStateException: Layer "Layer not named" learning rate policy decay rate (lrPolicyDecayRate) must be set to use learningRatePolicy.

which is the result (as I found out) of not correctly propagating all parameters from FineTuneConfiguration to the new network in TransferLearning. While lrPolicyDecayRate takes the value in FineTuneConfiguration it's propagated further. Other parameters are propagated like lrPolicyPower and lrPolicySteps are correctly propagated.
P.S. Maybe a common terminology in FineTuneConfiguration and NeuralNetConfiguration might be more appropriate. The former uses learningRateDecayPolicy where the latter uses learningRatePolicy to express the same thing I guess.
&lt;denchmark-code&gt;
package org.deeplearning4j.examples.transferlearning.vgg16;

import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.examples.transferlearning.vgg16.dataHelpers.FlowerDataSetIterator;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.LearningRatePolicy;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.WorkspaceMode;
import org.deeplearning4j.nn.conf.distribution.NormalDistribution;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.graph.ComputationGraph;
import org.deeplearning4j.nn.modelimport.keras.InvalidKerasConfigurationException;
import org.deeplearning4j.nn.modelimport.keras.UnsupportedKerasConfigurationException;
import org.deeplearning4j.nn.transferlearning.FineTuneConfiguration;
import org.deeplearning4j.nn.transferlearning.TransferLearning;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.zoo.PretrainedType;
import org.deeplearning4j.zoo.ZooModel;
import org.deeplearning4j.zoo.model.VGG16;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.slf4j.Logger;

import java.io.IOException;

/**
 * @author susaneraly on 3/9/17.
 *
 * We use the transfer learning API to construct a new model based of org.deeplearning4j.transferlearning.vgg16
 * We will hold all layers but the very last one frozen and change the number of outputs in the last layer to
 * match our classification task.
 * In other words we go from where fc2 and predictions are vertex names in org.deeplearning4j.transferlearning.vgg16
 *  fc2 -&gt; predictions (1000 classes)
 *  to
 *  fc2 -&gt; predictions (5 classes)
 * The class "FitFromFeaturized" attempts to train this same architecture the difference being the outputs from the last
 * frozen layer is presaved and the fit is carried out on this featurized dataset.
 * When running multiple epochs this can save on computation time.
 */
public class EditLastLayerOthersFrozen3Issue {
    private static final Logger log = org.slf4j.LoggerFactory.getLogger(EditLastLayerOthersFrozen3Issue.class);

    protected static final int numClasses = 5;
    protected static final long seed = 12345;

    private static final int trainPerc = 80;
    private static final int batchSize = 15;
    private static final String featureExtractionLayer = "fc2";
    private static double lr = 30e-5;
    private static double decayRate = 0.9;//2.0;
    private static double power = 3.0;
    private static Double steps = 5.0;

    public static void main(String [] args) throws UnsupportedKerasConfigurationException, IOException, InvalidKerasConfigurationException {

        ZooModel zooModel =
            new VGG16();
        ComputationGraph pretrainedNet = (ComputationGraph) zooModel.initPretrained(PretrainedType.IMAGENET);
        System.out.println("Workspace in initiation: " + pretrainedNet.getConfiguration().getTrainingWorkspaceMode());

        log.info(pretrainedNet.summary());

        //Decide on a fine tune configuration to use.
        //In cases where there already exists a setting the fine tune setting will
        //  override the setting for all layers that are not "frozen".
        FineTuneConfiguration fineTuneConf = new FineTuneConfiguration.Builder()
            .learningRate(lr)
//            .learningRatePolicy(LearningRatePolicy.Inverse)
//            .lrPolicyPower(power)
//            .lrPolicyDecayRate(decayRate)
            .learningRatePolicy(LearningRatePolicy.Step)
            .lrPolicyPower(power)
            .lrPolicyDecayRate(decayRate)
            .lrPolicySteps(steps)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
            .updater(Updater.NESTEROVS)
            .seed(seed)
            .build();

        //Construct a new model with the intended architecture and print summary
        ComputationGraph netTransfer = new TransferLearning.GraphBuilder(pretrainedNet)
            .setWorkspaceMode(WorkspaceMode.SEPARATE)
            .fineTuneConfiguration(fineTuneConf)
            .setFeatureExtractor(featureExtractionLayer) //the specified layer and below are "frozen"
            .removeVertexKeepConnections("predictions") //replace the functionality of the final vertex
            .addLayer("predictions",
                new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                    .nIn(4096).nOut(numClasses)
                    .weightInit(WeightInit.DISTRIBUTION)
                    .dist(new NormalDistribution(0,0.2*(2.0/(4096+numClasses)))) //This weight init dist gave better results than Xavier
                    .activation(Activation.SOFTMAX).build(),
                "fc2")
            .build();
        System.out.println("Workspace before: " + netTransfer.getConfiguration().getTrainingWorkspaceMode());

        System.out.println("Workspace much after: " + netTransfer.getConfiguration().getTrainingWorkspaceMode());

        log.info(netTransfer.summary());

        //Dataset iterators
        FlowerDataSetIterator.setup(batchSize,trainPerc);
        DataSetIterator trainIter = FlowerDataSetIterator.trainIterator();
        DataSetIterator testIter = FlowerDataSetIterator.testIterator();

        Evaluation eval;
        eval = netTransfer.evaluate(testIter);
        log.info("Eval stats BEFORE fit.....");
        log.info(eval.stats() + "\n");
        testIter.reset();

        int iter = 0;
        while(trainIter.hasNext()) {
            netTransfer.fit(trainIter.next());
            if (iter % 10 == 0) {
                log.info("Evaluate model at iter "+iter +" ....");
                eval = netTransfer.evaluate(testIter);
                log.info(eval.stats());
                testIter.reset();
            }
            iter++;
        }

        log.info("Model build complete");
    }
}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j 0.9.1
platform information Win7
CUDA no

	</description>
	<comments>
		<comment id='1' author='eypros' date='2017-10-15T00:27:22Z'>
		Anything related to updaters has changed a lot on the master branch (and this issue is thus probably fixed in a way). Would it be possible to retest with the latest code?
		</comment>
		<comment id='2' author='eypros' date='2017-10-16T12:03:58Z'>
		I pulled the latest version today and tried it out but the exception persists.
		</comment>
		<comment id='3' author='eypros' date='2017-10-16T22:43:08Z'>
		There is no lrPolicyDecayRate in FineTuneConfiguration anymore. Could you update your code as well?
		</comment>
		<comment id='4' author='eypros' date='2017-10-17T10:30:48Z'>
		I switch to 0.9.2-SNAPSHOT to check the new code (wasn't that the idea?) but in FineTuneConfiguration I can still use lrPolicyDecayRate. Is there any other way to check the latest version?
		</comment>
		<comment id='5' author='eypros' date='2017-10-17T10:34:21Z'>
		Where?
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.java&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.java&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='eypros' date='2018-07-25T09:01:23Z'>
		Closing as outdated - LR schedules were moved to updater configuration a long time ago, and can also be used with transfer learning.
		</comment>
		<comment id='7' author='eypros' date='2018-09-21T13:59:06Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>