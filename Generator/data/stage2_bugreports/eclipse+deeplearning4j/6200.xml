<bug id='6200' author='GlassyWing' open_date='2018-08-18T06:01:53Z' closed_time='2018-08-20T10:41:08Z'>
	<summary>Strange behavior with SameDiff</summary>
	<description>
I want to calcuate the loss (=out[0, 0, 0] + out[1, 0, 0]) to get grad of 'out' ,and I use the way as follow:
// out's shape (2, 2, 2)
// calculate loss: out[0, 0, 0] + out[1, 0, 0]

 SDVariable sum = sd.scalar("sum", 0); // &lt;---

for (int i = 0; i &lt; 2; i++) {
    SDVariable out_i = out.get(SDIndex.point(i));

    sum.add(out_i.get(SDIndex.point(0), SDIndex.point(0)));
}

sd.execBackwards();

System.out.println(sd.grad("out").getArr()); 
That throws exception on sd.execBackwards():
org.nd4j.linalg.exception.ND4JIllegalStateException: No gradient found for add

	at org.nd4j.autodiff.samediff.SameDiff$3.define(SameDiff.java:9523)
	at org.nd4j.autodiff.samediff.SameDiff.defineFunction(SameDiff.java:9344)
	at org.nd4j.autodiff.samediff.SameDiff.defineFunction(SameDiff.java:9328)
	at org.nd4j.autodiff.samediff.SameDiff.createGradFunction(SameDiff.java:9439)
	at org.nd4j.autodiff.samediff.SameDiff.execBackwards(SameDiff.java:9406)
	at CRFTest.testCalculateRealPathScore03(CRFTest.java:76)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
But when I change to the following way, that works everything.
// out's shape (2, 2, 2)
// calculate loss: out[0, 0, 0] + out[1, 0, 0]

SDVariable[] sum = new SDVariable[seqLen]; // &lt;---

for (int i = 0; i &lt; 2; i++) {
    SDVariable out_i = out.get(SDIndex.point(i));

    sum[i] = out_i.get(SDIndex.point(0), SDIndex.point(0));
}

sum[0].add(sum[1]);  // &lt;---

sd.execBackwards();

System.out.println(sd.grad("out").getArr());
so I know there must be error(s) for var 'sum', but I can't to locate it.
	</description>
	<comments>
		<comment id='1' author='GlassyWing' date='2018-08-20T10:40:22Z'>
		OK, so first thing: you have run into an issue, but it's not the one you thing you have :)
Basically, you have multiple outputs - both instances of sum.add(out_i.get(SDIndex.point(0), SDIndex.point(0))); are independent scalar variables.
That's why your second case works: you have only one output - (i.e., a single scalar loss) after adding.
Now, for solutions: don't create that scalar zero "sum" variable - you don't need it. Just add the values. There's a bunch of ways to do that - the simplest is just out.sum(). Or use indexing if you want, but again, it's unnecessary. Or use indexing + mergeAdd.
		</comment>
		<comment id='2' author='GlassyWing' date='2018-08-20T10:41:08Z'>
		Closing in favor of this issue: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/6214&gt;https://github.com/deeplearning4j/deeplearning4j/issues/6214&lt;/denchmark-link&gt;

Basically the fix is "detect this and throw a useful exception"
		</comment>
		<comment id='3' author='GlassyWing' date='2018-09-21T09:21:29Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>