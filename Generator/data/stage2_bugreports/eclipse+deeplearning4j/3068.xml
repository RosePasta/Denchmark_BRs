<bug id='3068' author='vlovsky' open_date='2017-03-18T05:23:13Z' closed_time='2018-04-27T23:21:32Z'>
	<summary>Review/fix applying updaters for layerwise pretraining</summary>
	<description>
&lt;denchmark-link:https://github.com/vlovsky/deeplearning4j/blob/1f8af820c29cc5567a2c5eaa290f094c4d1492a7/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/updater/LayerUpdater.java#L85-L86&gt;https://github.com/vlovsky/deeplearning4j/blob/1f8af820c29cc5567a2c5eaa290f094c4d1492a7/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/updater/LayerUpdater.java#L85-L86&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='vlovsky' date='2017-03-19T23:49:03Z'>
		Looks like it's intended to skip updating of pretrain parameters when doing backprop.
It does look brittle: if any parameters involved in layerwise pretraining that don't match that that particular key will not be correctly handled.
Edit: though now that I think about it, we have tests for exactly that issue for the VAE implementation - so presumably this case is handled correctly elsewhere. Either way, it has to be checked, and possibly removed.
		</comment>
		<comment id='2' author='vlovsky' date='2018-04-27T20:43:13Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 is this issue still viable?
		</comment>
		<comment id='3' author='vlovsky' date='2018-04-27T23:21:32Z'>
		Yeah, fixed long ago
		</comment>
		<comment id='4' author='vlovsky' date='2018-09-22T07:24:14Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>