<bug id='3000' author='crockpotveggies' open_date='2017-03-08T16:45:26Z' closed_time='2017-03-09T18:19:21Z'>
	<summary>Setting CUDA grid size creates NaNs</summary>
	<description>
Setting CUDA grid size to 512 degrades network scores into NaNs. Using CUDA 8 on master branch.
Usage: CudaEnvironment.getInstance().getConfiguration().setMaximumGridSize(cudaGridSize)
Result:
&lt;denchmark-code&gt;[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 0 is 8.521245163904679
[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 1 is 8.438281933109966
[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2 is NaN
[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 3 is NaN
&lt;/denchmark-code&gt;

If grid size is larger than 1024, network performs normally.
	</description>
	<comments>
		<comment id='1' author='crockpotveggies' date='2017-03-08T16:55:22Z'>
		In other words, something goes wrong, if number of blocks is too small.
		</comment>
		<comment id='2' author='crockpotveggies' date='2017-03-08T19:20:52Z'>
		Apparently, it's not Col2Im/Im2Col, not a Reduce/IndexReduce and not a Broadcast.
		</comment>
		<comment id='3' author='crockpotveggies' date='2017-03-09T18:19:21Z'>
		After investigation we have strong feeling it's just numerical underflow/overflow, and grid size is just booster here.  Using DOUBLE datatype fixes issue.
		</comment>
		<comment id='4' author='crockpotveggies' date='2018-10-02T08:21:37Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>