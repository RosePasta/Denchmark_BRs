<bug id='3778' author='firasdib' open_date='2017-08-02T15:34:12Z' closed_time='2017-08-03T09:47:55Z'>
	<summary>Gradient sharing causes crash - not enough memory</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I have around 70-80gb free ram and get the following error after just a few iterations:
&lt;denchmark-code&gt;2017-08-02 17:30:29,364 INFO  Starting MultiLayerNetwork with WorkspaceModes set to [training: SINGLE; inference: SINGLE]
2017-08-02 17:30:29,580 INFO  Setting threshold to [0.001]
2017-08-02 17:30:29,627 INFO  Model construction done!
2017-08-02 17:30:29,627 INFO  Training model...
2017-08-02 17:30:29,701 INFO  Epoch 1
2017-08-02 17:30:29,701 INFO  Using workspaceMode SINGLE for training
2017-08-02 17:30:29,705 INFO  Creating asynchronous prefetcher...
2017-08-02 17:30:29,712 INFO  Starting ParallelWrapper training round...
2017-08-02 17:30:34,519 INFO  Switched to threshold encoding
2017-08-02 17:30:34,729 INFO  Switched to threshold encoding
2017-08-02 17:30:34,746 INFO  Score at iteration 0 is 1.5946775922883951
2017-08-02 17:30:34,746 INFO  ETL: 1208 ms; iteration 1; iteration time: 0 ms; samples/sec: Infinity; batches/sec: Infinity;
2017-08-02 17:30:34,749 INFO  Score at iteration 1 is 1.5932513723482096
2017-08-02 17:30:34,749 INFO  ETL: 3596 ms; iteration 1; iteration time: 0 ms; samples/sec: Infinity; batches/sec: Infinity;
2017-08-02 17:30:37,050 INFO  Score at iteration 2 is 1.594224120150944
2017-08-02 17:30:37,050 INFO  Score at iteration 2 is 1.593331957827946
2017-08-02 17:30:39,472 INFO  Score at iteration 4 is 1.597569609652897
2017-08-02 17:30:39,472 INFO  Score at iteration 4 is 1.5947050104249918
2017-08-02 17:30:41,903 INFO  Score at iteration 6 is 1.5962342271913492
2017-08-02 17:30:41,903 INFO  Score at iteration 6 is 1.594462657939335
2017-08-02 17:30:47,874 ERROR Uncaught exception: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.nd4j.linalg.exception.ND4JIllegalStateException: Not enough memory to handle update: [17955936 bytes required]. Please increase memory amount for GradientsAccumulator
2017-08-02 17:30:50,072 ERROR Uncaught exception
java.lang.RuntimeException: org.nd4j.linalg.exception.ND4JIllegalStateException: Not enough memory to handle update: [17955936 bytes required]. Please increase memory amount for GradientsAccumulator
	at org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulator.registerConsumers(EncodedGradientsAccumulator.java:143)
	at org.deeplearning4j.parallelism.ParallelWrapper.fit(ParallelWrapper.java:529)
	at se.lth.cs.nlp.EntityRecognizer.NeuralNetwork.NeuralNetwork.train(NeuralNetwork.java:121)
	at se.lth.cs.nlp.EntityRecognizer.EntityRecognizer.&lt;init&gt;(EntityRecognizer.java:72)
	at se.lth.cs.nlp.EntityRecognizer.EntityRecognizer.main(EntityRecognizer.java:86)
Caused by: org.nd4j.linalg.exception.ND4JIllegalStateException: Not enough memory to handle update: [17955936 bytes required]. Please increase memory amount for GradientsAccumulator
	at org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulator.receiveUpdate(EncodedGradientsAccumulator.java:474)
	at org.deeplearning4j.optimize.solvers.accumulation.EncodingHandler.sendMessage(EncodingHandler.java:195)
	at org.deeplearning4j.optimize.solvers.accumulation.EncodingHandler.broadcastUpdates(EncodingHandler.java:207)
	at org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulator.storeUpdate(EncodedGradientsAccumulator.java:439)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:69)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1766)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1715)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1818)
	at org.deeplearning4j.parallelism.trainer.DefaultTrainer.fit(DefaultTrainer.java:207)
	at org.deeplearning4j.parallelism.trainer.DefaultTrainer.run(DefaultTrainer.java:331)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version 0.9.0
Linux

	</description>
	<comments>
		<comment id='1' author='firasdib' date='2017-08-02T18:49:47Z'>
		Please show your NN configuration &amp; PW configuration.
		</comment>
		<comment id='2' author='firasdib' date='2017-08-02T19:15:34Z'>
		NN
NeuralNetConfiguration.Builder()
                    .miniBatch(true)
                    .trainingWorkspaceMode(WorkspaceMode.SINGLE)
                    .inferenceWorkspaceMode(WorkspaceMode.SINGLE)
                    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                    .iterations(1) // Not the same as epoch. Should probably only ever be 1.
                    .seed(RNG_SEED)
                    .activation(Activation.RELU) // Recommended by FOFE-NER
                    .weightInit(WeightInit.XAVIER)
                    .updater(Adam.builder()
                            .beta1(ADAM_MEAN_DECAY) // Default value
                            .beta2(ADAM_VAR_DECAY) // Default value
                            .build())
                    .learningRateDecayPolicy(LearningRatePolicy.Schedule)
                    .learningRateSchedule(lrSchedule)
                    .regularization(true)
                    .l2(1E-3)
                    .list()
                    .layer(0, new DenseLayer.Builder()
                            .nIn(inputSize)
                            .nOut(512)
                            .build())
                    .layer(1, new DenseLayer.Builder()
                            .nIn(512)
                            .nOut(512)
                            .dropOut(0.9)
                            .build())
                    .layer(2, new DenseLayer.Builder()
                            .nIn(512)
                            .nOut(512)
                            .dropOut(0.9)
                            .build())
                    .layer(3, new DenseLayer.Builder()
                            .nIn(512)
                            .nOut(512)
                            .dropOut(0.9)
                            .build())
                    .layer(4, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
                            .activation(Activation.SOFTMAX)
                            .nIn(512)
                            .nOut(outputSize)
                            .build())
                    .pretrain(false)
                    .backprop(true)
                    .build();
PW
new ParallelWrapper.Builder&lt;&gt;(model)
                        .prefetchBuffer(2)
                        .workers(2)
                        .trainerFactory(new SymmetricTrainerContext())
                        .trainingMode(ParallelWrapper.TrainingMode.CUSTOM)
                        .gradientsAccumulator(new EncodedGradientsAccumulator(NUMBER_OF_CPUS, 1E-3))
                        .workspaceMode(WorkspaceMode.SINGLE)
                        .build();
		</comment>
		<comment id='3' author='firasdib' date='2017-08-02T19:44:11Z'>
		What's inputSize value, and outputSize value too please.
		</comment>
		<comment id='4' author='firasdib' date='2017-08-02T19:47:58Z'>
		12600 input, 5 output
		</comment>
		<comment id='5' author='firasdib' date='2017-08-03T09:47:55Z'>
		Should be fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/3780&gt;https://github.com/deeplearning4j/deeplearning4j/pull/3780&lt;/denchmark-link&gt;

Now if number of updates &gt; 1/16, we immediately fall back to bitmap encoding, instead of doing that on next round.
		</comment>
		<comment id='6' author='firasdib' date='2017-08-14T00:06:32Z'>
		Got this again with 0.9.1:
&lt;denchmark-code&gt;2017-08-14 02:04:42,829 ERROR Uncaught exception: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.nd4j.linalg.exception.ND4JIllegalStateException: Not enough memory to handle update: [30675092 bytes required]. Please increase memory amount for GradientsAccumulator
2017-08-14 02:04:45,249 ERROR Uncaught exception
java.lang.RuntimeException: org.nd4j.linalg.exception.ND4JIllegalStateException: Not enough memory to handle update: [30675092 bytes required]. Please increase memory amount for GradientsAccumulator
	at org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulator.registerConsumers(EncodedGradientsAccumulator.java:170)
	at org.deeplearning4j.parallelism.ParallelWrapper.fit(ParallelWrapper.java:534)
	at se.lth.cs.nlp.EntityRecognizer.NeuralNetwork.NeuralNetwork.train(NeuralNetwork.java:109)
	at se.lth.cs.nlp.EntityRecognizer.EntityRecognizer.&lt;init&gt;(EntityRecognizer.java:106)
	at se.lth.cs.nlp.EntityRecognizer.EntityRecognizer.main(EntityRecognizer.java:124)
Caused by: org.nd4j.linalg.exception.ND4JIllegalStateException: Not enough memory to handle update: [30675092 bytes required]. Please increase memory amount for GradientsAccumulator
	at org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulator.receiveUpdate(EncodedGradientsAccumulator.java:503)
	at org.deeplearning4j.optimize.solvers.accumulation.EncodingHandler.sendMessage(EncodingHandler.java:206)
	at org.deeplearning4j.optimize.solvers.accumulation.EncodingHandler.broadcastUpdates(EncodingHandler.java:218)
	at org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulator.storeUpdate(EncodedGradientsAccumulator.java:468)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:69)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1780)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1729)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1832)
	at org.deeplearning4j.parallelism.trainer.DefaultTrainer.fit(DefaultTrainer.java:209)
	at org.deeplearning4j.parallelism.trainer.DefaultTrainer.run(DefaultTrainer.java:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='firasdib' date='2018-09-25T13:27:26Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>