<bug id='3079' author='AlexDBlack' open_date='2017-03-21T05:13:12Z' closed_time='2017-03-21T10:36:52Z'>
	<summary>Incorrect config warning with GlobalPooling</summary>
	<description>
With 0.8.0, the following configuration results in incorrect configuration warnings:
&lt;denchmark-code&gt;        ComputationGraphConfiguration conf = new NeuralNetConfiguration.Builder()
            .updater(Updater.ADAM).adamMeanDecay(0.9).adamVarDecay(0.999)
            .learningRate(0.01)
            .graphBuilder()
            .addInputs("in")
            .addLayer("blstm", new GravesBidirectionalLSTM.Builder()
                .nIn(inputSize).nOut(layerSize)
                .activation(Activation.TANH)
                .build(), "in")
            .addLayer("pool", new GlobalPoolingLayer.Builder().build(), "blstm")
            .addLayer("out", new OutputLayer.Builder()
                .nIn(layerSize).nOut(outputSize)
                .activation(Activation.SIGMOID)
                .lossFunction(LossFunctions.LossFunction.XENT)
                .build(), "pool")
            .setOutputs("out")
            .build();
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;o.d.n.c.l.LayerValidation - Layer "Layer not named" momentum has been set but will not be applied unless the updater is set to NESTEROVS.
o.d.n.c.l.LayerValidation - Layer "Layer not named" rho is set but will not be applied unless the updater is set to ADADELTA.
o.d.n.c.l.LayerValidation - Layer "Layer not named" rmsdecay is set but will not be applied unless the updater is set to RMSPROP.
&lt;/denchmark-code&gt;

However, removing the global pooling layer seems to result in no incorrect warnings being printed.
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-10-01T12:17:55Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>