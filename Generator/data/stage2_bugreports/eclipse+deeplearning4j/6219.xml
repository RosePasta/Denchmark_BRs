<bug id='6219' author='mtothak' open_date='2018-08-20T15:38:05Z' closed_time='2019-11-10T14:33:23Z'>
	<summary>code=4(cudaErrorLaunchFailure) "result" on Windows Server 2016 / NVIDIA Titan V</summary>
	<description>
Hi!
I opened this issue as I still have have problems running on a cuda backend
(might be related to &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/5708&gt;https://github.com/deeplearning4j/deeplearning4j/issues/5708&lt;/denchmark-link&gt;
) :
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2017 NVIDIA Corporation
Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017
Cuda compilation tools, release 9.0, V9.0.176
[INFO ] 2018-08-20 16:54:26.465 [main] Nd4jBackend - Loaded [JCublasBackend] backend
[INFO ] 2018-08-20 16:54:31.794 [main] NativeOpsHolder - Number of threads used for NativeOps: 32
[INFO ] 2018-08-20 16:54:32.606 [main] Nd4jBlas - Number of threads used for BLAS: 0
[INFO ] 2018-08-20 16:54:32.622 [main] DefaultOpExecutioner - Backend used: [CUDA]; OS: [Windows Server 2016]
[INFO ] 2018-08-20 16:54:32.622 [main] DefaultOpExecutioner - Cores: [12]; Memory: [21,3GB];
[INFO ] 2018-08-20 16:54:32.622 [main] DefaultOpExecutioner - Blas vendor: [CUBLAS]
[INFO ] 2018-08-20 16:54:32.622 [main] CudaExecutioner - Device Name: [TITAN V]; CC: [7.0]; Total/free memory: [12884901888]
[INFO ] 2018-08-20 16:54:32.872 [main] MultiLayerNetwork - Starting MultiLayerNetwork with WorkspaceModes set to [training: ENABLED; inference: ENABLED], cacheMode set to [NONE]
Error message:
&lt;denchmark-link:https://gist.github.com/mtothak/ffdf2cf429cc9eef0e9a17a43213a0fb&gt;https://gist.github.com/mtothak/ffdf2cf429cc9eef0e9a17a43213a0fb&lt;/denchmark-link&gt;

Files:
pom.xml
&lt;denchmark-link:https://gist.github.com/mtothak/7176e67a69e22f07aca3e8226770b3ea&gt;https://gist.github.com/mtothak/7176e67a69e22f07aca3e8226770b3ea&lt;/denchmark-link&gt;

Iterator
&lt;denchmark-link:https://gist.github.com/mtothak/75998b02170f72e0f4a37c9dc6ccd35f&gt;https://gist.github.com/mtothak/75998b02170f72e0f4a37c9dc6ccd35f&lt;/denchmark-link&gt;

BILSTMClassifier
&lt;denchmark-link:https://gist.github.com/mtothak/b0c1a77335c73f8e93a182763a20ea94&gt;https://gist.github.com/mtothak/b0c1a77335c73f8e93a182763a20ea94&lt;/denchmark-link&gt;

Runner
&lt;denchmark-link:https://gist.github.com/mtothak/e953fb09b8c77ad11fd6c5a004d971e1&gt;https://gist.github.com/mtothak/e953fb09b8c77ad11fd6c5a004d971e1&lt;/denchmark-link&gt;

DummyData
&lt;denchmark-link:https://gist.github.com/mtothak/e2739d537028e5cdfd52c8bb0637b33e&gt;https://gist.github.com/mtothak/e2739d537028e5cdfd52c8bb0637b33e&lt;/denchmark-link&gt;

It runs ok with (1.0.0-SNAPSHOT or 1.0.0-beta2):
&lt;nd4j.backend&gt;nd4j-native-platform&lt;/nd4j.backend&gt;
but with (1.0.0-SNAPSHOT or 1.0.0-beta2):
&lt;nd4j.backend&gt;nd4j-cuda-9.0-platform&lt;/nd4j.backend&gt;
I get the error explained above...
Thank you for you help!
lg Markus
	</description>
	<comments>
		<comment id='1' author='mtothak' date='2018-08-21T17:44:49Z'>
		Hi!
I investigated some network configurations and it seems that the hidden layer inputs and outputs have to be a multiple of 8. At least with such a configuration it worked in my case...
lg Markus
		</comment>
		<comment id='2' author='mtothak' date='2018-08-27T04:50:15Z'>
		I'd propose to update to CUDA 9.2 in this case. That might be the issue.
		</comment>
	</comments>
</bug>