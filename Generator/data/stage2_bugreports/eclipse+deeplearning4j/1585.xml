<bug id='1585' author='jsteggink' open_date='2016-05-22T20:22:12Z' closed_time='2016-06-03T19:54:15Z'>
	<summary>Consistent NaN crashes</summary>
	<description>
Since version 3.9 I have been having trouble training my GravesLSTM network. It seems to be related to the amount of data. I have tried many mini batch sizes, all sorts of network configurations. With regularization, without, all sorts of learning rates, gradientNormalization, they have no influence, it will always crash at the same score iteration. Depending on the size of data the crashpoint will shift from the first to a subsequent epoch. The larger the training data, the further in the epochs it will crash. In total I have 120.000 rows, 92 input classes, 3 output classes, and the sequence length is 30. With these data it crashes in the second epoch.
Exception:
&lt;denchmark-link:https://gist.github.com/jsteggink/89e7c36a877f2eb641c756bdc373521d&gt;https://gist.github.com/jsteggink/89e7c36a877f2eb641c756bdc373521d&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jsteggink' date='2016-05-22T20:25:31Z'>
		&lt;denchmark-link:https://cloud.githubusercontent.com/assets/978411/15456302/141fdf50-206c-11e6-8394-2f85b791dfb3.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='jsteggink' date='2016-05-22T20:26:02Z'>
		It maybe some bug within HistogramListener itself. Have you tried to disable it, and just use ScoreIterationListener to see if that's the case?
But still, obviously HistogramListener shouldn't crash.
		</comment>
		<comment id='3' author='jsteggink' date='2016-05-22T20:27:48Z'>
		Without the HistogramIterationListener it's the same issue. I can run it without and send the exception if yout want, so you can narrow down the issue. However it will take a while, since I will have to start the training again.
		</comment>
		<comment id='4' author='jsteggink' date='2016-05-22T20:28:38Z'>
		Yes, do that please if that's possible.
		</comment>
		<comment id='5' author='jsteggink' date='2016-05-22T21:02:49Z'>
		Sorry, I forgot, no exception, just NaN at the same iteration it crashes with the HistogramIterationListener.
o.d.o.l.ScoreIterationListener - Score at iteration 1341 is 11.537639902156043
c.s.n.w.t.WordbreakerTrainingIterator - batch: 143 numCharacters: 92 maxWordLength: 30
o.d.o.l.ScoreIterationListener - Score at iteration 1342 is 11.06500806641667
c.s.n.w.t.WordbreakerTrainingIterator - batch: 144 numCharacters: 92 maxWordLength: 30
o.d.o.l.ScoreIterationListener - Score at iteration 1343 is 11.03507276386018
c.s.n.w.t.WordbreakerTrainingIterator - batch: 145 numCharacters: 92 maxWordLength: 30
o.d.o.l.ScoreIterationListener - Score at iteration 1344 is NaN
c.s.n.w.t.WordbreakerTrainingIterator - batch: 146 numCharacters: 92 maxWordLength: 30
o.d.o.l.ScoreIterationListener - Score at iteration 1345 is NaN
c.s.n.w.t.WordbreakerTrainingIterator - batch: 147 numCharacters: 92 maxWordLength: 30
o.d.o.l.ScoreIterationListener - Score at iteration 1346 is NaN
c.s.n.w.t.WordbreakerTrainingIterator - batch: 148 numCharacters: 92 maxWordLength: 30
o.d.o.l.ScoreIterationListener - Score at iteration 1347 is NaN
		</comment>
		<comment id='6' author='jsteggink' date='2016-05-23T00:38:37Z'>
		Check your input data and labels at that iteration.
Also: More UI output, just before it starts with the NaN would be good... parameter histograms and gradient mean magnitudes for each layer would be good to help diagnose this.
		</comment>
		<comment id='7' author='jsteggink' date='2016-05-23T02:47:37Z'>
		Might be related to &lt;denchmark-link:https://github.com/deeplearning4j/libnd4j/issues/191&gt;deeplearning4j/libnd4j#191&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='jsteggink' date='2016-05-23T10:28:15Z'>
		I don't think it is the data itself. The data has a random order every time I initialise the data set iterator, it happens in the the third epoch, so it already has seen all the data twice.
I debugged the training and it seems it all goes wrong here
&lt;denchmark-link:https://cloud.githubusercontent.com/assets/978411/15468249/74e3f688-20e3-11e6-89f5-08a75a0fe101.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='jsteggink' date='2016-05-23T18:24:19Z'>
		I'm not exactly sure at what point it happens, but just before I get NaN scores I have this gradient:
{0_W=[[0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [   �,    �,    �,    �], [   �,    �,    �,    �], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [   �,    �,    �,    �], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, -0,00, -0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [-0,00, -0,00, 0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, -0,00, 0,00, 0,00], [-0,00, -0,00, 0,00, 0,00], [0,00, 0,00, 0,00, -0,00], [-0,00, 0,00, 0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, -0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, -0,00, 0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, -0,00, -0,00, 0,00], [-0,00, -0,00, 0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, -0,00, 0,00, 0,00], [-0,00, -0,00, 0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, -0,00, 0,00], [0,00, 0,00, -0,00, -0,00], [-0,00, -0,00, 0,00, -0,00], [0,00, -0,00, -0,00, -0,00], [0,00, 0,00, -0,00, -0,00], [0,00, 0,00, -0,00, -0,00], [0,00, -0,00, 0,00, 0,00], [-0,00, 0,00, -0,00, 0,00], [0,00, -0,00, 0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, 0,00, -0,00, 0,00], [-0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, 0,00, -0,00], [0,00, 0,00, 0,00, 0,00], [0,00, -0,00, 0,00, -0,00], [0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, -0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, 0,00, 0,00, -0,00], [-0,00, 0,00, 0,00, 0,00], [0,00, 0,00, -0,00, 0,00], [0,00, -0,00, -0,00, 0,00], [-0,00, 0,00, 0,00, -0,00], [0,00, 0,00, -0,00, 0,00], [0,00, 0,00, -0,00, 0,00], [-0,00, 0,00, 0,00, -0,00], [-0,00, -0,00, -0,00, 0,00], [0,00, -0,00, -0,00, 0,00], [-0,00, 0,00, -0,00, -0,00], [0,00, 0,00, 0,00, -0,00], [-0,00, 0,00, 0,00, -0,00], [-0,00, -0,00, 0,00, -0,00], [0,00, -0,00, -0,00, 0,00], [0,00, 0,00, 0,00, -0,00], [0,00, 0,00, 0,00, 0,00], [-0,00, 0,00, -0,00, 0,00], [-0,00, -0,00, -0,00, 0,00], [0,00, -0,00, 0,00, 0,00]], 0_RW=[-0,00, 0,00, 0,00, 0,00, 0,00, 0,00, 0,00], 0_b=[0,00, -0,00, -0,00, -0,00], 1_W=[-0,00, 0,00, 0,00, 0,00], 1_RW=[-0,00, 0,00, 0,00, 0,00, 0,00, 0,00, 0,00], 1_b=[0,00, -0,00, -0,00, -0,00], 2_W=[0,00, -0,00, -0,00], 2_b=[-0,00, -0,00, 0,00]}
		</comment>
		<comment id='10' author='jsteggink' date='2016-05-23T18:48:58Z'>
		&lt;denchmark-link:https://github.com/jsteggink&gt;@jsteggink&lt;/denchmark-link&gt;
 What os are you on? And this is with nd4j-native or cuda?
		</comment>
		<comment id='11' author='jsteggink' date='2016-05-23T19:06:38Z'>
		This is on Windows with nd4j-native
		</comment>
		<comment id='12' author='jsteggink' date='2016-05-23T19:37:40Z'>
		Thanks. I am on a mac and linux. I can run on my end and see what happens. But if I can't reproduce the issue the other guys (who have windows) will look into it :)
		</comment>
		<comment id='13' author='jsteggink' date='2016-05-23T19:52:09Z'>
		&lt;denchmark-link:https://github.com/eraly&gt;@eraly&lt;/denchmark-link&gt;
 I just ran the code on Linux (Ubuntu 14.04) and I have no problem at all. So I guess it's Windows related. Don't know why I hadn't tried that before.
		</comment>
		<comment id='14' author='jsteggink' date='2016-05-23T20:02:05Z'>
		&lt;denchmark-link:https://github.com/jsteggink&gt;@jsteggink&lt;/denchmark-link&gt;
 But I get the feeling we will still want to figure out what the problem was.
		</comment>
		<comment id='15' author='jsteggink' date='2016-05-23T20:12:46Z'>
		I agree, something libnd4j related probably?
		</comment>
		<comment id='16' author='jsteggink' date='2016-05-23T20:14:11Z'>
		Eh. I can't really tell yet. I am running through on my mac fine so far. Raver will probably try to reproduce it on his laptop.
Edit: I ran through the whole example on my mac fine.
		</comment>
		<comment id='17' author='jsteggink' date='2016-05-24T16:23:29Z'>
		Hey guys.  I get a similar issue with the � symbols appearing in my training data causing the same NaN issue.  The �'s only appear in the data set AFTER i call dataset.normalizeZeroMeanZeroUnitVariance()
There are no � symbols in my data if I don't run dataset.normalizeZeroMeanZeroUnitVariance()
EDIT: Not totally sure if this is related to this issue, I am going to do more testing and I'll post what I find.
		</comment>
		<comment id='18' author='jsteggink' date='2016-05-24T17:44:12Z'>
		@DevinCalado You should use standard scaler instead.
import org.nd4j.linalg.dataset.api.iterator.StandardScaler;
StandardScaler scaler = new StandardScaler();
scaler.fit(iterator);
		</comment>
		<comment id='19' author='jsteggink' date='2016-05-24T17:51:32Z'>
		&lt;denchmark-link:https://github.com/eraly&gt;@eraly&lt;/denchmark-link&gt;
 I don't understand why the standard scalar will work instead but I'll give it a try when I get home. Maybe I don't fully understand why normalizeZeroMeanZeroUnitVariance is giving bad symbols.    I'll post an update after I try what you suggested. Cheers!
		</comment>
		<comment id='20' author='jsteggink' date='2016-05-24T18:21:27Z'>
		@DevinCalado Basically because it is a method we are going to deprecate. You have to do a transform after the fit on every batch too. If it doesn't work file a separate issue.
		</comment>
		<comment id='21' author='jsteggink' date='2016-05-24T19:02:29Z'>
		&lt;denchmark-link:https://github.com/eraly&gt;@eraly&lt;/denchmark-link&gt;
 Okay thanks for the advice. I'll follow up if it is a separate issue.
		</comment>
		<comment id='22' author='jsteggink' date='2016-06-03T19:54:15Z'>
		Resolved. Closing.
		</comment>
		<comment id='23' author='jsteggink' date='2019-01-21T01:53:00Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>