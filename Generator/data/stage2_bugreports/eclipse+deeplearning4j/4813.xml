<bug id='4813' author='AlexDBlack' open_date='2018-03-16T23:45:06Z' closed_time='2018-03-27T04:37:04Z'>
	<summary>ParallelInference: no support for input masks? + check batching functionality</summary>
	<description>
Use case: sequence to sequence model with variable length time series.
We need input masks (either provided by the user, or inferred automatically by the model) in order for the LastTimeStepVertex to function as expected when examples are batched together.
So 3 things here:

Provide method that allows user to provide an input masks
Ensure that (for variable-length RNNs) input masks are inferred automatically
Check that for variable input size models (RNN, CNNs) stacking works properly (i.e., doesn't try to stack incompatible inputs)

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-03-24T07:07:12Z'>
		Confirming that there are some issues here - ParallelInference can't handle variable length time series (see testParallelInferenceVariableLengthTS() added here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4836&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4836&lt;/denchmark-link&gt;
)
Edit: unsurprisingly, the same problem occurs for variable size CNN architectures (some fully convolutional CNNs like YOLO and segmentation nets can support different input sizes)
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-03-24T10:11:09Z'>
		Depending on the case, we have a few options here.
For the CNN case, really the only option is "don't batch if the shapes are different". In some architectures we might be able to do something more intelligent, but I can't see this working reliably in general.
For the RNN case, the naive solution ("don't batch if the shapes are different") will work well (at least if we ignore performance).
"Batch and extract the relevant output" will be much better for performance.
Simplest case: Single input RNN
(a) with RNN (3d) output: infer masks automatically (like DataSet merge). Then simply use input size to work out relevant subset of the output.
{b} with 2d output {i.e., net includes global pooling): infer input masks, otherwise no need to do anything special for the output
Complex case: multi-input RNN (seq2seq model, etc)
Here, it's unclear how we could infer what the "real" output is, and what the masked time steps are.
In the short term, we may simply want to use the "don't batch on different shape" strategy. Long term, we can probably do something a bit more intelligent - but it will take some careful thinking to ensure it works robustly in general.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-09-23T01:28:12Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>