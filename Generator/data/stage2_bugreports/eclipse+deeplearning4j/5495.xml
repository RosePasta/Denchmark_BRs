<bug id='5495' author='borkox' open_date='2018-06-06T17:57:29Z' closed_time='2018-06-07T12:12:16Z'>
	<summary>Array workspace validation failed: Array of type ACTIVATION_GRAD should be in workspace "WS_LAYER_ACT_1" but is in workspace "WS_LAYER_WORKING_MEM"</summary>
	<description>
I have an exception:
&lt;denchmark-code&gt;2018-06-06 20:54:38 ERROR o.d.r.l.a.AsyncThread - Thread crashed: org.nd4j.linalg.workspace.ND4JWorkspaceException: Array workspace validation failed: Array of type ACTIVATION_GRAD should be in workspace "WS_LAYER_ACT_1" but is in workspace "WS_LAYER_WORKING_MEM"
java.lang.IllegalStateException: Backprop: array (ACTIVATION_GRAD) workspace validation failed (vertex 0 - class: ConvolutionLayer) - array is defined in incorrect workspace
	at org.deeplearning4j.nn.graph.ComputationGraph.validateArrayWorkspaces(ComputationGraph.java:1708)
	at org.deeplearning4j.nn.graph.ComputationGraph.calcBackpropGradients(ComputationGraph.java:2435)
	at org.deeplearning4j.nn.graph.ComputationGraph.computeGradientAndScore(ComputationGraph.java:1319)
	at org.deeplearning4j.rl4j.network.ac.ActorCriticCompGraph.gradient(ActorCriticCompGraph.java:73)
	at org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CThreadDiscrete.calcGradient(A3CThreadDiscrete.java:106)
	at org.deeplearning4j.rl4j.learning.async.a3c.discrete.A3CThreadDiscrete.calcGradient(A3CThreadDiscrete.java:28)
	at org.deeplearning4j.rl4j.learning.async.AsyncThreadDiscrete.trainSubEpoch(AsyncThreadDiscrete.java:123)
	at org.deeplearning4j.rl4j.learning.async.AsyncThread.run(AsyncThread.java:82)
Caused by: org.nd4j.linalg.workspace.ND4JWorkspaceException: Array workspace validation failed: Array of type ACTIVATION_GRAD should be in workspace "WS_LAYER_ACT_1" but is in workspace "WS_LAYER_WORKING_MEM"
	at org.nd4j.linalg.workspace.BaseWorkspaceMgr.validateArrayLocation(BaseWorkspaceMgr.java:221)
	at org.deeplearning4j.nn.workspace.LayerWorkspaceMgr.validateArrayLocation(LayerWorkspaceMgr.java:66)
	at org.deeplearning4j.nn.graph.ComputationGraph.validateArrayWorkspaces(ComputationGraph.java:1699)
	... 7 more
&lt;/denchmark-code&gt;

I am doing an actor critic (own application, not from examples) and I am using own implementation ActorCriticFactoryCompGraph which is copy/paste from ActorCriticFactoryCompGraphStdConv but with changed numbers(stride, kernel, etc)
	</description>
	<comments>
		<comment id='1' author='borkox' date='2018-06-06T17:58:21Z'>
		Show us source code please. It's impossible to be helpful without it.
		</comment>
		<comment id='2' author='borkox' date='2018-06-06T18:40:13Z'>
		This is the actor critic factory
package com.rltrader.rl.a3c;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Value;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.ComputationGraphConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.WorkspaceMode;
import org.deeplearning4j.nn.conf.inputs.InputType;
import org.deeplearning4j.nn.conf.layers.*;
import org.deeplearning4j.nn.conf.preprocessor.CnnToFeedForwardPreProcessor;
import org.deeplearning4j.nn.conf.preprocessor.FeedForwardToRnnPreProcessor;
import org.deeplearning4j.nn.conf.preprocessor.RnnToCnnPreProcessor;
import org.deeplearning4j.nn.graph.ComputationGraph;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.TrainingListener;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.rl4j.network.ac.ActorCriticCompGraph;
import org.deeplearning4j.rl4j.network.ac.ActorCriticFactoryCompGraph;
import org.deeplearning4j.rl4j.network.ac.ActorCriticLoss;
import org.deeplearning4j.rl4j.util.Constants;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.learning.config.Adam;
import org.nd4j.linalg.learning.config.IUpdater;
import org.nd4j.linalg.lossfunctions.LossFunctions;


@Value
public class ActorCriticFactory implements ActorCriticFactoryCompGraph {


    Configuration conf;

    public ActorCriticCompGraph buildActorCritic(int shapeInputs[], int numOutputs) {

        if (shapeInputs.length == 1)
            throw new AssertionError("Impossible to apply convolutional layer on a shape == 1");

        int h = (((shapeInputs[1] - 1) / 1 + 1) - 1) / 1 + 1;
        int w = (((shapeInputs[2] - 4) / 2 + 1) - 2) / 2 + 1;

        ComputationGraphConfiguration.GraphBuilder confB =
                new NeuralNetConfiguration.Builder()
                        .seed(Constants.NEURAL_NET_SEED)
                        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                        .updater(conf.getUpdater() != null ? conf.getUpdater() : new Adam())
                        .weightInit(WeightInit.XAVIER)
                        .l2(conf.getL2()).graphBuilder()
                        .addInputs("input");

        confB.addLayer("0", new ConvolutionLayer
                .Builder(1, 4)
                .nIn(shapeInputs[0])
                .nOut(16)
                .stride(1, 2)
                .activation(Activation.RELU)
                .build(), "input");

        confB.addLayer("1", new ConvolutionLayer
                .Builder(1, 2)
                .nIn(16)
                .nOut(32)
                .stride(1, 2)
                .activation(Activation.RELU)
                .build(), "0");

        confB.addLayer("2", new DenseLayer
                .Builder()
                .nIn(w * h * 32)
                .nOut(256)
                .activation(Activation.RELU)
                .build(), "1");

        if (conf.isUseLSTM()) {
            confB.addLayer("3", new LSTM
                    .Builder()
                    .nIn(256)
                    .nOut(256)
                    .activation(Activation.TANH)
                    .build(), "2");

            confB.addLayer("value", new RnnOutputLayer
                    .Builder(LossFunctions.LossFunction.MSE)
                    .activation(Activation.IDENTITY)
                    .nIn(256)
                    .nOut(1)
                    .build(), "3");

            confB.addLayer("softmax", new RnnOutputLayer
                    .Builder(new ActorCriticLoss())
                    .activation(Activation.SOFTMAX)
                    .nIn(256)
                    .nOut(numOutputs)
                    .build(), "3");
        } else {
            confB.addLayer("value", new OutputLayer.Builder(LossFunctions.LossFunction.MSE).activation(Activation.IDENTITY)
                    .nIn(256).nOut(1).build(), "2");

            confB.addLayer("softmax", new OutputLayer.Builder(new ActorCriticLoss()).activation(Activation.SOFTMAX)
                    .nIn(256).nOut(numOutputs).build(), "2");
        }

        confB.setOutputs("value", "softmax");

        if (conf.isUseLSTM()) {
            confB.inputPreProcessor("0", new RnnToCnnPreProcessor(shapeInputs[1], shapeInputs[2], shapeInputs[0]));
            confB.inputPreProcessor("2", new CnnToFeedForwardPreProcessor(h, w, 32));
            confB.inputPreProcessor("3", new FeedForwardToRnnPreProcessor());
        } else {
            confB.setInputTypes(InputType.convolutional(shapeInputs[1], shapeInputs[2], shapeInputs[0]));
        }

        ComputationGraphConfiguration cgconf = confB
                .pretrain(false)
                .backprop(true)
                .build();
        ComputationGraph model = new ComputationGraph(cgconf);
        model.init();
        if (conf.getListeners() != null) {
            model.setListeners(conf.getListeners());
        } else {
            model.setListeners(new ScoreIterationListener(Constants.NEURAL_NET_ITERATION_LISTENER));
        }

        return new ActorCriticCompGraph(model);
    }


    @AllArgsConstructor
    @Builder
    @Value
    public static class Configuration {

        double l2;
        IUpdater updater;
        TrainingListener[] listeners;
        boolean useLSTM;
    }

}
I noticed that if I set useLSTM=false the exception disappears.
		</comment>
		<comment id='3' author='borkox' date='2018-06-06T18:45:36Z'>
		I am not sure if this code is enough. In order to isolate and build a failing test case I have to build completely new project. Is it possible to investigate the bug by the code I gave, or you need more ?
		</comment>
		<comment id='4' author='borkox' date='2018-06-07T00:06:19Z'>
		Hopefully that will be enough. I'll have to see if I can reproduce it locally.
If not I'll ask for more details.
		</comment>
		<comment id='5' author='borkox' date='2018-06-07T11:35:26Z'>
		Thanks for reporting. I've managed to fix it here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/5506&gt;https://github.com/deeplearning4j/deeplearning4j/pull/5506&lt;/denchmark-link&gt;

That should (hopefully) be merged soon; you can then access the fix via snapshots: &lt;denchmark-link:https://deeplearning4j.org/snapshots&gt;https://deeplearning4j.org/snapshots&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='borkox' date='2018-06-07T13:46:45Z'>
		Thanks, I will test this later with SNAPSHOT versions.
		</comment>
		<comment id='7' author='borkox' date='2018-09-21T20:59:50Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>