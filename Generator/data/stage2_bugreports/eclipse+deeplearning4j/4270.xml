<bug id='4270' author='crockpotveggies' open_date='2017-11-08T22:35:52Z' closed_time='2018-04-25T23:37:25Z'>
	<summary>ParallelWrapper odd param updates issue (gradient sharing)</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

When training a seq2seq autoencoder for denoising, training on a single device yields the following training stats:
&lt;denchmark-link:https://user-images.githubusercontent.com/1445295/32578293-89da5f02-c491-11e7-948e-0f1315df367e.png&gt;&lt;/denchmark-link&gt;

In the training process above, the score converges at around 0.09.
However, when using ParallelWrapper a strange updates pattern emerges. There seem to be a couple issues with the PW approach:
Sometimes update parameter ratios display frequent NaN patterns when using a threshold of 1e-3
&lt;denchmark-link:https://user-images.githubusercontent.com/1445295/32578389-e2de7160-c491-11e7-8c51-3fdce9318873.png&gt;&lt;/denchmark-link&gt;

When using a threshold of 1e-4 the score never converges lower than 0.25
I'm not quite sure why that's the case. Either the single device training is overfitting (not likely considering dropout is used...?) or there's an issue with how the parameter updates are handled.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version 0.9.1
CUDA 8.0
Titan X Pascal

	</description>
	<comments>
		<comment id='1' author='crockpotveggies' date='2017-11-08T23:29:35Z'>
		Attaching implementation:
&lt;denchmark-code&gt;        ParallelWrapper wrapper = new ParallelWrapper.Builder(net)
                .prefetchBuffer(24)
                .workers(Nd4j.getAffinityManager().getNumberOfDevices())
                .trainerFactory(new SymmetricTrainerContext())
                .trainingMode(ParallelWrapper.TrainingMode.CUSTOM)
                .gradientsAccumulator(new EncodedGradientsAccumulator(2, 1e-4))
                .build();
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='crockpotveggies' date='2017-11-10T07:03:53Z'>
		
https://deeplearning4j.org/distributed
http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf

As explained here, gradients sharing works via accumulation of gradients. Only gradients with module greater or equal to threshold are shared, others are accumulated locally, and re-evaluated on next iteration(s). So, for each individual update step there's only few values within gradients update message: +threshold, -threshold and 0.0. Plus, if that's a dense update message, there's also -threshold/2 value possible. So, 0.0s in gradients aren't that unexpected. That's the part of the idea: we share only gradients that are big enough.
		</comment>
		<comment id='3' author='crockpotveggies' date='2018-09-22T17:13:40Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>