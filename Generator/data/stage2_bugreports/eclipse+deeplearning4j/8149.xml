<bug id='8149' author='iltie165' open_date='2019-08-26T06:43:22Z' closed_time='2019-10-29T06:01:24Z'>
	<summary>Deconvolution2D with ConvolutionMode.Same output is 0</summary>
	<description>
When I build a simple model with Deconvolution2D layer and set ConvolutionMode = 'Same',
the model output is always zero regardless of input.
Here is my model config:
&lt;denchmark-code&gt;ComputationGraphConfiguration config = new NeuralNetConfiguration.Builder()
                .seed(123)
                .weightInit(new NormalDistribution(0, 0.02))
                .updater(new Adam(0.0002, 0.5, Adam.DEFAULT_ADAM_BETA2_VAR_DECAY, Adam.DEFAULT_ADAM_EPSILON))
                .graphBuilder()
                .addInputs("ramdon-z")
                .appendLayer("pj-layer", new DenseLayer.Builder()
                        .nIn(this.inputDim)
                        .nOut(4 * 4 * 512)
                        .activation(Activation.IDENTITY)
                        .build())
                .appendLayer("decv512-layer", new Deconvolution2D.Builder()
                        .kernelSize(5, 5)
                        .stride(2, 2)
                        .convolutionMode(ConvolutionMode.Same)
                        .nIn(512)
                        .nOut(256)
                        .activation(Activation.IDENTITY)
                        .build(), new FeedForwardToCnnPreProcessor(4, 4, 512))
                .appendLayer("img-output-layer", new Deconvolution2D.Builder()
                        .kernelSize(5, 5)
                        .stride(2, 2)
                        .convolutionMode(ConvolutionMode.Same)
                        .nIn(256)
                        .nOut(128)
                        .activation(Activation.IDENTITY)
                        .build())
                .setOutputs("img-output-layer")
                .build();
&lt;/denchmark-code&gt;

and my input is a random Array:
&lt;denchmark-code&gt;INDArray randomZ = Nd4j.rand(new long[]{16, 100}, new 
          org.nd4j.linalg.api.rng.distribution.impl.NormalDistribution(0, Math.exp(-1 / Math.PI)));
       
Map&lt;String, INDArray&gt; res = generator.model.feedForward(randomZ, true);

&lt;/denchmark-code&gt;

The output of "img-output-layer" is always zero.
os: mac osx
dl4j version: 1.0.0-beta4
	</description>
	<comments>
		<comment id='1' author='iltie165' date='2019-08-26T20:37:21Z'>
		I don't think this is a bug.  You're starting with a small number (your random) then multiplying by three other small numbers (initialized weights).
If you try using just one deconv layer, you get a nonzero output (although quite small, with a large number of zeros).
ComputationGraphConfiguration config = new NeuralNetConfiguration.Builder()
        .seed(123)
        .weightInit(new NormalDistribution(0, 0.02))
        .updater(new Adam(0.0002, 0.5, Adam.DEFAULT_ADAM_BETA2_VAR_DECAY, Adam.DEFAULT_ADAM_EPSILON))
        .graphBuilder()
        .addInputs("ramdon-z")
        .appendLayer("pj-layer", new DenseLayer.Builder()
                .nIn(100)
                .nOut(4 * 4 * 512)
                .activation(Activation.IDENTITY)
                .build())
        .appendLayer("img-output-layer", new Deconvolution2D.Builder()
                .kernelSize(5, 5)
                .stride(2, 2)
                .convolutionMode(ConvolutionMode.Same)
                .nIn(512)
                .nOut(128)
                .activation(Activation.IDENTITY)
                .build(), new FeedForwardToCnnPreProcessor(4, 4, 512))
        .setOutputs("img-output-layer")
        .build();
Now if this happens after training, it is definitely a bug.
		</comment>
		<comment id='2' author='iltie165' date='2019-08-27T01:48:47Z'>
		
I don't think this is a bug. You're starting with a small number (your random) then multiplying by three other small numbers (initialized weights).
If you try using just one deconv layer, you get a nonzero output (although quite small, with a large number of zeros).
ComputationGraphConfiguration config = new NeuralNetConfiguration.Builder()
        .seed(123)
        .weightInit(new NormalDistribution(0, 0.02))
        .updater(new Adam(0.0002, 0.5, Adam.DEFAULT_ADAM_BETA2_VAR_DECAY, Adam.DEFAULT_ADAM_EPSILON))
        .graphBuilder()
        .addInputs("ramdon-z")
        .appendLayer("pj-layer", new DenseLayer.Builder()
                .nIn(100)
                .nOut(4 * 4 * 512)
                .activation(Activation.IDENTITY)
                .build())
        .appendLayer("img-output-layer", new Deconvolution2D.Builder()
                .kernelSize(5, 5)
                .stride(2, 2)
                .convolutionMode(ConvolutionMode.Same)
                .nIn(512)
                .nOut(128)
                .activation(Activation.IDENTITY)
                .build(), new FeedForwardToCnnPreProcessor(4, 4, 512))
        .setOutputs("img-output-layer")
        .build();
Now if this happens after training, it is definitely a bug.

Thank for you reply.
Indeed one layer can output nonzero value, and there is large zero values. But the output is very strange, according to 'deconv' algorithm.
If not set ConvolutionMode = Same, you can find the layer output can be pretty good.
At last, I have tried the same network with 'tensorflow'.  Output is nonzero.
		</comment>
		<comment id='3' author='iltie165' date='2019-08-27T02:59:55Z'>
		This is an odd choice of configuration, dense before deconv.
Not sure yet if there is a legitimate issue here or not, but 98304 of 131072 output values are zero, and that's not just numerical underflow (still happens with larger input + larger weight init). That seems suspicious, given the way deconv works.
&lt;denchmark-link:https://gist.github.com/AlexDBlack/3d0238d4444e437ea3c263376f9df561&gt;https://gist.github.com/AlexDBlack/3d0238d4444e437ea3c263376f9df561&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='iltie165' date='2019-08-27T03:28:35Z'>
		
This is an odd choice of configuration, dense before deconv.
Not sure yet if there is a legitimate issue here or not, but 98304 of 131072 output values are zero, and that's not just numerical underflow (still happens with larger input + larger weight init). That seems suspicious, given the way deconv works.
https://gist.github.com/AlexDBlack/3d0238d4444e437ea3c263376f9df561

At the beginning this issue appeared when I build a DCGAN. I find no matter how training the generator, it always output zero.
With debugging the code, I find when set 'ConvolutionMode = Same' 'deconv' output become strange. But I am not familiar with C++, so I may not be able to solve the problem.  Sorry.
		</comment>
		<comment id='5' author='iltie165' date='2019-08-27T03:49:04Z'>
		I think this has to do with differences in how padding is interpreted.  See &lt;denchmark-link:https://colab.research.google.com/drive/10D8JbXKvC5UVfTOOyCuQX53f6cozdvjr&gt;this colab&lt;/denchmark-link&gt;
 vs:
@Test
    public void deconv2dTest(){
        SameDiff sd = SameDiff.create();
        SDVariable input = sd.constant(Nd4j.linspace(0, 256, 256).reshape(1, 4, 8, 8));
        SDVariable weight = sd.var("W", new UniformInitScheme('c', 256), DataType.FLOAT,
                5, 5, 2, 4);
        SDVariable out = sd.cnn.deconv2d(input, weight, DeConv2DConfig.builder().kH(5).kW(5).sH(2).sW(2).isSameMode(true).build());

        INDArray outArr = out.eval();

        System.out.println(outArr);
        System.out.println(Arrays.toString(outArr.shape()));
    }
Gives:
&lt;denchmark-code&gt;[[[[  -42.9209,  -74.8325,  -43.1695,  -25.7057,  -17.6494,  -16.4760,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [  -42.7457,   -4.4296,  -43.1091,  -17.5173,  -15.6857,  -29.0854,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [  -44.9090,  -78.0450,  -45.1576,  -26.7143,  -18.6145,  -17.6672,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [  -41.6534,  -22.6532,  -41.9391,  -19.7356,  -15.0543,  -27.2701,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [  -26.6816,  -52.2985,  -26.7939,  -31.3898,  -25.7956,  -20.1823,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [  -43.5764,  -14.2697,  -43.8698,  -22.0389,  -25.1268,  -10.3894,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]], 

  [[   54.4223,  -36.6817,   54.7970,  -26.8508,   36.5192,    4.6110,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [   18.8667,  -30.2205,   18.9997,   -5.2288,   14.1767,   20.8869,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [   57.4202,  -37.7387,   57.7950,  -27.5827,   38.4198,    4.7517,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [   21.9030,  -14.8757,   21.9774,   -4.9474,   14.1493,   13.6490,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [   34.3398,    3.0361,   34.5293,    1.9770,   26.0766,   16.2576,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [    8.1951,   -4.5432,    8.1975,   11.5045,    6.4680,   18.2789,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
[1, 2, 16, 16]
&lt;/denchmark-code&gt;

It looks like the Nd4j op does the deconv, and then pads around it, where the tf op doesn't.
		</comment>
		<comment id='6' author='iltie165' date='2019-08-27T05:04:40Z'>
		
        SameDiff sd = SameDiff.create();
        SDVariable input = sd.constant(Nd4j.linspace(0, 256, 256).reshape(1, 4, 8, 8));
        SDVariable weight = sd.var("W", new UniformInitScheme('c', 256), DataType.FLOAT,
                5, 5, 2, 4);
        SDVariable out = sd.cnn.deconv2d(input, weight, DeConv2DConfig.builder().kH(5).kW(5).sH(2).sW(2).isSameMode(true).build());

        INDArray outArr = out.eval();

        System.out.println(outArr);
        System.out.println(Arrays.toString(outArr.shape()));

So, it may be the reason which more than one 'deconv' ops with padding output always zeros.
&lt;denchmark-code&gt;    @Test
    public void testDeconv2d(){
        SameDiff sd = SameDiff.create();
        SDVariable input = sd.constant(Nd4j.linspace(0, 256, 256).reshape(1, 4, 8, 8));
        SDVariable weight = sd.var("W", new UniformInitScheme('c', 256), DataType.FLOAT,
                5, 5, 2, 4);
        SDVariable out = sd.cnn.deconv2d(input, weight, DeConv2DConfig.builder().kH(5).kW(5).sH(2).sW(2).isSameMode(true).build());

        SDVariable weightAfterDeconvWithPadding = sd.var("W2", new UniformInitScheme('c', 256), DataType.FLOAT,
                5, 5, 1, 2);
        SDVariable outAfterDeconvWithPadding = sd.cnn.deconv2d(out, weightAfterDeconvWithPadding,
                DeConv2DConfig.builder().kH(5).kW(5).sH(2).sW(2).isSameMode(true).build());

        INDArray outArr = out.eval();

        INDArray outArrAfterDeconv = outAfterDeconvWithPadding.eval();

        System.out.println(outArr);
        System.out.println(Arrays.toString(outArr.shape()));

        System.out.println(String.format("double deconv output max: %f, min: %f, std: %f",
                outArrAfterDeconv.maxNumber().floatValue(),
                outArrAfterDeconv.aminNumber().floatValue(),
                outArrAfterDeconv.stdNumber().floatValue()));
    }
&lt;/denchmark-code&gt;

Gives:
&lt;denchmark-code&gt;double deconv output max: 0.000000, min: 0.000000, std: 0.000000
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='iltie165' date='2019-10-29T06:01:23Z'>
		Recently fixed here: &lt;denchmark-link:https://github.com/KonduitAI/deeplearning4j/pull/12&gt;KonduitAI#12&lt;/denchmark-link&gt;

Same issue as: &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/8298&gt;#8298&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>