<bug id='3191' author='BartKeulen' open_date='2017-04-04T22:35:13Z' closed_time='2017-05-04T12:55:01Z'>
	<summary>Wrong Z value returned by MultiLayerNetwork.computeZ</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

When computing the z values of a model using the &lt;denchmark-link:https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html#computeZ-org.nd4j.linalg.api.ndarray.INDArray-boolean-&gt;computeZ&lt;/denchmark-link&gt;
 function it returns the wrong values. The problem can be viewed with this &lt;denchmark-link:https://gist.github.com/BartKeulen/00ef00291338f1ad662089f62439a374&gt;gist&lt;/denchmark-link&gt;
.

Expected: The z-value is calculated using the activations of the previous layer.
Encountered: The z-value is calculated using the z-values of the previous layer.

The gist outputs the z-values for all four layers and the preOutput value. The z-value for the fourth layer should be similar to the preOutput value, this is not the case. The z-value of the third layer is not correct either, at the bottom of the script I have added some code that calculates the correct z-values. Using different activation functions for the hidden layer does not change the values return by computeZ.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version: 0.8.0
OS: Ubuntu 16.04.1 LTS
Java version: 1.8.0_121

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

At the bottom of the gist I have included a script for computing the correct z-values. If you agree with this method I can create a pull request (first time so didn't want to do it immediately).
	</description>
	<comments>
		<comment id='1' author='BartKeulen' date='2017-04-04T22:49:11Z'>
		side note: &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 see that gist? That's what i'm afraid of...
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/3183&gt;https://github.com/deeplearning4j/deeplearning4j/issues/3183&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='BartKeulen' date='2017-04-06T04:24:46Z'>
		Yeah, computeZ is wrong - it should be passing activation functions (not pre-out) to the next layer.
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java#L639&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java#L639&lt;/denchmark-link&gt;

We'll need to call both preOut (to return) and activate (to do the forward pass) on each layer. That's inefficient but necessary, as in some cases (like LSTM) activations don't cleanly decompose into activationFn(preOut).
		</comment>
		<comment id='3' author='BartKeulen' date='2017-05-04T12:55:01Z'>
		Fixed here: will be merged soon.
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/3358&gt;https://github.com/deeplearning4j/deeplearning4j/pull/3358&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='BartKeulen' date='2017-05-10T23:46:41Z'>
		Of note - snapshots are available so you can get the fix now.
		</comment>
		<comment id='5' author='BartKeulen' date='2018-09-28T07:38:20Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>