<bug id='3351' author='sethah' open_date='2017-05-02T20:19:25Z' closed_time='2017-07-13T09:17:49Z'>
	<summary>Frozen layers are disregarded when training with Spark</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I am trying to do transfer learning with Spark. In the code below, I have loaded a model which has the following structure:
"fc1 (input)" -&gt; "fc2" -&gt; "predictions"
I intend to only train the predictions.
    val tm = new ParameterAveragingTrainingMaster.Builder(1)
        .averagingFrequency(param.averagingFrequency)
        .workerPrefetchNumBatches(2)
        .batchSizePerWorker(param.batchSizePerWorker)
        .rddTrainingApproach(RDDTrainingApproach.Export)
        .build()

      val fineTuneConf = new FineTuneConfiguration.Builder()
        .learningRate(param.learningRate)
        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
        .updater(Updater.NESTEROVS)
        .momentum(param.momentum)
        .regularization(true)
        .l2(param.regularization)
        .seed(seed)
        .build()
      val restoredNetwork = new TransferLearning.GraphBuilder(restored)
        .fineTuneConfiguration(fineTuneConf)
        .setFeatureExtractor("fc2")
        .build()

      val model = new SparkComputationGraph(sc, restoredNetwork, tm)
The parameters for "fc2" should not change - they should be frozen - but still, they do. I have also tested this same code, but NOT using the Spark trainer and it works as expected.
The problem, AFAICT, is this: when the computation graph is broadcast, only its configuration and its parameters get serialized. Then, the workers reconstruct the graph from the config and params. However, in TransferLearning.GraphBuilder.build() the newConfig does not reflect that some of the layers are frozen. The newGraph, however, does have the frozen layers. This explains why it works locally, but not on the Spark executors.
I have not quite figured out how to fix this, though it seems as simple as adding the frozen layers to the configuration in the build() method. At this point, I'm relatively confident this is a bug, but definitely possible I've overlooked something.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version: 0.8.0
platform information (OS, etc): Mac OSX Sierra 10.12.4

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

I can look into this, though I won't be upset if someone else wants to fix it faster.
	</description>
	<comments>
		<comment id='1' author='sethah' date='2017-05-02T20:21:55Z'>
		cc &lt;denchmark-link:https://github.com/huitseeker&gt;@huitseeker&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='sethah' date='2017-05-02T22:37:40Z'>
		It's a bug. We wanted to keep the conf devoid of references to frozen layers. We might still want to. Although not sure how we want to work around this right now.
		</comment>
		<comment id='3' author='sethah' date='2017-07-12T06:31:23Z'>
		Fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/3653&gt;https://github.com/deeplearning4j/deeplearning4j/pull/3653&lt;/denchmark-link&gt;

Issue will be closed once that PR is merged.
		</comment>
		<comment id='4' author='sethah' date='2018-09-26T02:57:39Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>