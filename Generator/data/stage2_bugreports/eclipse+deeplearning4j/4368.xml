<bug id='4368' author='AlexDBlack' open_date='2017-12-05T05:45:52Z' closed_time='2017-12-11T08:46:44Z'>
	<summary>Transfer learning: 0 dropout doesn't remove/override existing layer dropout</summary>
	<description>
&lt;denchmark-code&gt;            netToTest = new TransferLearning.Builder(net)
                    .fineTuneConfiguration(new FineTuneConfiguration.Builder()
                            .dropOut(0.0)
                    .build()).build();
&lt;/denchmark-code&gt;

The original dropout is still present on the existing layers. Reason: .dropOut(0.0) does dropOut((IDropOut)null) hence it's treated as equivalent to "not set"
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2017-12-05T10:09:21Z'>
		This looks to impact more than just dropout: it also impacts weight noise (drop connect, etc) - and by that logic, I expect it'll impact bunch of others stuff too...
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-09-23T23:43:50Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>