<bug id='8082' author='ch1010' open_date='2019-08-09T01:30:49Z' closed_time='2019-08-21T13:47:26Z'>
	<summary>Wrong searchSize in GridSearchCandidateGenerator</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

The searchSize calculated in GridSearchCandidateGenerator.initialize() is wrong
I have four ParameterSpaces:
&lt;denchmark-code&gt;ParameterSpace&lt;Double&gt; learningRateHyperparam = new DiscreteParameterSpace&lt;&gt;(0.003, 0.005, 0.01, 0.05);
ParameterSpace&lt;Integer&gt; layerSizeHyperparam1 = new DiscreteParameterSpace&lt;&gt;(32, 64, 96, 128);
ParameterSpace&lt;Integer&gt; layerSizeHyperparam2 = new DiscreteParameterSpace&lt;&gt;(32, 64, 96, 128);
ParameterSpace&lt;Double&gt; dropoutHyperparam = new DiscreteParameterSpace&lt;Double&gt;(0.8, 0.9);
&lt;/denchmark-code&gt;

But the searchSize is 16, which is 4 (from layerSizeHyperparam1) x 4 (from layerSizeHyperparam2).
AdamSpace(learningRateHyperparam) and DropoutSpace are not considered.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Deeplearning4j version: 1.0.0-beta4
&lt;denchmark-h:h4&gt;Additional Information&lt;/denchmark-h&gt;

The full structure of MultiLayerSpace:
&lt;denchmark-code&gt;protected MultiLayerSpace getNetworkSpace() {
        ParameterSpace&lt;Double&gt; learningRateHyperparam = new DiscreteParameterSpace&lt;&gt;(0.003, 0.005, 0.01, 0.05);
        ParameterSpace&lt;Integer&gt; layerSizeHyperparam1 = new DiscreteParameterSpace&lt;&gt;(32, 64, 96, 128);
        ParameterSpace&lt;Integer&gt; layerSizeHyperparam2 = new DiscreteParameterSpace&lt;&gt;(32, 64, 96, 128);
        ParameterSpace&lt;Double&gt; dropoutHyperparam = new DiscreteParameterSpace&lt;Double&gt;(0.8, 0.9);

        return new MultiLayerSpace.Builder()
                .seed(SEED)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .updater(new AdamSpace(learningRateHyperparam))
                .weightInit(WeightInit.XAVIER)
                .l2(0.0001)
                .addLayer(new DenseLayerSpace.Builder()
                        .nIn(mInputNum)
                        .nOut(layerSizeHyperparam1)
                        .build())
                .addLayer(new BatchNormalizationSpace.Builder()
                        .nOut(layerSizeHyperparam1)
                        .activation(Activation.RELU)
                        .build())
                .addLayer(new DropoutLayerSpace.Builder()
                        .dropOut(dropoutHyperparam)
                        .build())
                .addLayer(new DenseLayerSpace.Builder()
                        .nOut(layerSizeHyperparam2)
                        .build())
                .addLayer(new BatchNormalizationSpace.Builder()
                        .nOut(layerSizeHyperparam2)
                        .activation(Activation.RELU)
                        .build())
                .addLayer(new DropoutLayerSpace.Builder()
                        .dropOut(dropoutHyperparam)
                        .build())
                .addLayer(new OutputLayerSpace.Builder()
                        .nOut(mOutputNum)
                        .activation(Activation.SOFTMAX)
                        .lossFunction(LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY)
                        .build())
                .numEpochs(mHyperEpochCount)
                .build();
}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ch1010' date='2019-08-21T13:47:55Z'>
		Thanks for reporting - fixed here, will be merged to eclipse/deeplearning4j master soon.
&lt;denchmark-link:https://github.com/SkymindIO/deeplearning4j/pull/141&gt;SkymindIO#141&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>