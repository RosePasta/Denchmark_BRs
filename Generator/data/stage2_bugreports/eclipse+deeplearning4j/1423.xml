<bug id='1423' author='Ander-MZ' open_date='2016-04-21T16:13:15Z' closed_time='2020-10-06T04:46:08Z'>
	<summary>Unable to perform regression with SparkDl4jMultiLayer when fitting with method fit(SparkContext, RDD&amp;lt;LabelPoint&amp;gt;)</summary>
	<description>
If you create a network of type SparkDl4jMultiLayer, if you want to perform a regression (single output neuron) you may encounter an IndexOutOfBounds exception when fitting the network.
If you fit the network using the method with signature fit(RDD) everything goes smoothly. However, if you instead use the fit(SparkContext, RDD) method, you can NOT perform the aforementioned regression. Somehow, when fitting through this method, the network assumes you are going to perform a classification, and thus expects at least 2 output neurons.
The exception comes in class org.nd4j.linalg.util.FeatureUtil, method toOutcomeVector, where the outputs array is of size 1 (because of OutputLayer.nOut = 1), but the network thinks there are 2 outputs, and tries to set the non-existent second output of the array.
Minimum Working Example (Based on RegressionSum.java from examples repository):
&lt;denchmark-code&gt;package main;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.mllib.regression.LabeledPoint;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer;

import org.apache.spark.mllib.linalg.Vectors;

import java.io.Serializable;
import java.util.*;

public class TestEx implements Serializable {

    private static final Random rand = new Random();


    public static void main(String[] args) {

        SparkConf sparkConf = new SparkConf();
        sparkConf.setMaster("local[*]");
        sparkConf.setAppName("Sum");

        int samples = 1000;

        JavaSparkContext sc = new JavaSparkContext(sparkConf);

        JavaRDD&lt;DataSet&gt; ds = sc.parallelize(getTrainingData_DataSet(samples));
        JavaRDD&lt;LabeledPoint&gt; labeled_points = sc.parallelize(getTrainingData_LabeledPoint(samples));

        final int iterations = 1;
        final int nInput = 2; // Number of input variables
        final int nHidden = 10; // Number of neurons on hidden layers
        final int nOutput = 1; // Number of output neurons

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(12345)
                .iterations(iterations)
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .learningRate(1e-1)
                .l1(0.01).regularization(true).l2(1e-3)
                .list(2)
                .layer(0, new DenseLayer.Builder().nIn(nInput).nOut(nHidden)
                        .activation("tanh")
                        .weightInit(WeightInit.XAVIER)
                        .build())
                .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
                        .weightInit(WeightInit.XAVIER)
                        .activation("identity")
                        .nIn(nHidden).nOut(nOutput).build())
                .backprop(true).pretrain(false)
                .build();

        MultiLayerNetwork net = new MultiLayerNetwork(conf);
        net.init();
        net.setUpdater(null); 

        //Create Spark network
        SparkDl4jMultiLayer sparkNetwork = new SparkDl4jMultiLayer(sc, net);

        try {
            sparkNetwork.fitDataSet(ds); // &lt;---- Success!
            System.out.println("Success on DataSet!");
        } catch (Exception ex){
            ex.printStackTrace();
        }

        try {
            sparkNetwork.fit(sc,labeled_points); // &lt;---- Exception here!
        } catch (Exception ex){
            ex.printStackTrace();
        }

    }

    // Using LabaledPoint
    private static List&lt;LabeledPoint&gt; getTrainingData_LabeledPoint(int nSamples){

        final int MIN_RANGE = 0;
        final int MAX_RANGE = 1;

        double [] sum = new double[nSamples];
        double [] input1 = new double[nSamples];
        double [] input2 = new double[nSamples];
        for (int i= 0; i&lt; nSamples; i++) {
            input1[i] = MIN_RANGE + (MAX_RANGE - MIN_RANGE) * rand.nextDouble();
            input2[i] =  MIN_RANGE + (MAX_RANGE - MIN_RANGE) * rand.nextDouble();
            sum[i] = input1[i] + input2[i];
        }

        List&lt;LabeledPoint&gt; points = new ArrayList&lt;&gt;();

        for(int i = 0; i &lt; nSamples ; i++){
            points.add(new LabeledPoint(sum[i],Vectors.dense(input1[i],input2[i])));
        }

        return points;
    }

    // Using DataSet
    private static List&lt;DataSet&gt; getTrainingData_DataSet(int nSamples){

        final int MIN_RANGE = 0;
        final int MAX_RANGE = 1;

        double [] sum = new double[nSamples];
        double [] input1 = new double[nSamples];
        double [] input2 = new double[nSamples];
        for (int i= 0; i&lt; nSamples; i++) {
            input1[i] = MIN_RANGE + (MAX_RANGE - MIN_RANGE) * rand.nextDouble();
            input2[i] =  MIN_RANGE + (MAX_RANGE - MIN_RANGE) * rand.nextDouble();
            sum[i] = input1[i] + input2[i];
        }
        INDArray inputNDArray1 = Nd4j.create(input1, new int[]{nSamples,1});
        INDArray inputNDArray2 = Nd4j.create(input2, new int[]{nSamples,1});
        INDArray inputNDArray = Nd4j.hstack(inputNDArray1,inputNDArray2);
        INDArray outPut = Nd4j.create(sum, new int[]{nSamples, 1});
        DataSet dataSet = new DataSet(inputNDArray, outPut);
        List&lt;DataSet&gt; listDs = dataSet.asList();
        Collections.shuffle(listDs, TestEx.rand);
        return listDs;

    }

}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Ander-MZ' date='2020-10-06T04:46:08Z'>
		Closing stale issue: cleanup.
		</comment>
	</comments>
</bug>