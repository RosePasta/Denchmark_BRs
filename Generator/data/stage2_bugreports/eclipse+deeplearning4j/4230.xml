<bug id='4230' author='elifaj' open_date='2017-10-31T19:17:31Z' closed_time='2018-10-05T06:26:15Z'>
	<summary>Errors running distributed Word2Vec on EMR/YARN</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I'm getting errors running SparkWord2Vec in distributed mode on our AWS EMR cluster (1 master, 2 core nodes). It works fine using Spark local mode. I've tested with &lt;denchmark-link:https://github.com/deeplearning4j/dl4j-examples/blob/r119_spark/dl4j-spark-examples/dl4j-spark/src/main/java/org/deeplearning4j/nlp/DistributedWord2VecExample.java&gt;https://github.com/deeplearning4j/dl4j-examples/blob/r119_spark/dl4j-spark-examples/dl4j-spark/src/main/java/org/deeplearning4j/nlp/DistributedWord2VecExample.java&lt;/denchmark-link&gt;
 which I merged into my fork of the dl4j-examples master branch: &lt;denchmark-link:https://github.com/elifaj/dl4j-examples&gt;https://github.com/elifaj/dl4j-examples&lt;/denchmark-link&gt;
.
Here is my spark-submit:
&lt;denchmark-code&gt;#!/usr/bin/env bash
spark-submit \
--master yarn \
--name DL4J-Word2Vec \
--class "org.deeplearning4j.nlp.DistributedWord2VecExample" \
--executor-memory 10G \
--conf "spark.driver.memory=10G" \
--conf "spark.locality.wait=0" \
dl4j-spark-0.9.1-bin.jar \
--text "hdfs://ip-172-31-33-21.ec2.internal:8020/user/hadoop/raw_sentences.txt" \
-x false \
2&gt; ./sparkstderr.log &amp;
&lt;/denchmark-code&gt;

I see a lot of these in my driver log and Spark job never completes.
&lt;denchmark-code&gt;17/10/31 18:56:19 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 8, ip-172-31-44-159.ec2.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1509065004176_0020_01_000002 on host: ip-172-31-44-159.ec2.internal. Exit status: 50. Diagnostics: Exception from container-launch.
Container id: container_1509065004176_0020_01_000002
Exit code: 50
Stack trace: ExitCodeException exitCode=50:
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:582)
        at org.apache.hadoop.util.Shell.run(Shell.java:479)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)
        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
&lt;/denchmark-code&gt;

I'm seeing this in my executor log:
&lt;denchmark-code&gt;17/10/31 18:56:18 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[VoidParameterServer messages handling thread,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.nd4j.parameterserver.distributed.VoidParameterServer.lambda$init$0(VoidParameterServer.java:273)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.nd4j.parameterserver.distributed.training.impl.CbowTrainer.startTraining(CbowTrainer.java:46)
        at org.nd4j.parameterserver.distributed.training.impl.CbowTrainer.startTraining(CbowTrainer.java:27)
        at org.nd4j.parameterserver.distributed.messages.requests.CbowRequestMessage.processMessage(CbowRequestMessage.java:52)
        at org.nd4j.parameterserver.distributed.messages.Frame.processMessage(Frame.java:202)
        at org.nd4j.parameterserver.distributed.VoidParameterServer.handleMessage(VoidParameterServer.java:426)
        at org.nd4j.parameterserver.distributed.VoidParameterServer.lambda$init$0(VoidParameterServer.java:268)
        ... 1 more
17/10/31 18:56:18 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[VoidParameterServer messages handling thread,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.nd4j.parameterserver.distributed.VoidParameterServer.lambda$init$0(VoidParameterServer.java:273)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at org.nd4j.parameterserver.distributed.training.impl.CbowTrainer.startTraining(CbowTrainer.java:46)
        at org.nd4j.parameterserver.distributed.training.impl.CbowTrainer.startTraining(CbowTrainer.java:27)
        at org.nd4j.parameterserver.distributed.messages.requests.CbowRequestMessage.processMessage(CbowRequestMessage.java:52)
        at org.nd4j.parameterserver.distributed.messages.Frame.processMessage(Frame.java:202)
        at org.nd4j.parameterserver.distributed.VoidParameterServer.handleMessage(VoidParameterServer.java:426)
        at org.nd4j.parameterserver.distributed.VoidParameterServer.lambda$init$0(VoidParameterServer.java:268)
        ... 1 more
&lt;/denchmark-code&gt;

When I try to set shardAddresses and networkMask, I get the following in my driver log:
&lt;denchmark-code&gt;Exception in thread "main" java.lang.AssertionError: assertion failed: copyAndReset must return a zero value copy
        at scala.Predef$.assert(Predef.scala:170)
        at org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:168)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at java.io.ObjectStreamClass.invokeWriteReplace(ObjectStreamClass.java:1118)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1136)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
        at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)
        at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)
        at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)
        at org.apache.spark.SparkContext.clean(SparkContext.scala:2287)
        at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)
        at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:369)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
        at org.apache.spark.rdd.RDD.map(RDD.scala:369)
        at org.apache.spark.api.java.JavaRDDLike$class.map(JavaRDDLike.scala:93)
        at org.apache.spark.api.java.AbstractJavaRDDLike.map(JavaRDDLike.scala:45)
        at org.deeplearning4j.spark.models.sequencevectors.SparkSequenceVectors.fitSequences(SparkSequenceVectors.java:253)
        at org.deeplearning4j.spark.models.word2vec.SparkWord2Vec.fitSentences(SparkWord2Vec.java:77)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;java_version = 1.8
spark_version = 2.2.0
scala_version = 2.11
scala_binary_version = 2.11
nd4j_version = 0.9.1
nd4j_backend = nd4j-native-platform
dl4j_version = 0.9.1
datavec_version = 0.9.1
rl4j_version = 0.9.1
dl4j_spark_version = 0.9.1_spark_2
datavec_spark_version = 0.9.1_spark_2
jcommander_version = 1.72
aws_sdk_version = 1.11.109
jackson_version = 2.6.6
logback_version = 1.1.7
scala_logging_version = 3.5.0
slf4j_version = 1.7.25

AWS EMR 5.9.0
Hadoop 2.7.3
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='elifaj' date='2017-10-31T19:52:38Z'>
		Show your current w2v config &amp; voidconfiguration please
		</comment>
		<comment id='2' author='elifaj' date='2017-10-31T20:05:35Z'>
		Hi raver119. I'm using an example from one of your old branches that wasn't merged. Apologize for the broken links above. They should work now but here are the snippets:
&lt;denchmark-code&gt;VoidConfiguration paramServerConfig = VoidConfiguration.builder()
            //.networkMask("172.16.0.0/12")
            //.shardAddresses(Arrays.asList("172.31.8.139:48381"))
            .numberOfShards(2)
            .ttl(4)
            .build();
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;TokenizerFactory t = new DefaultTokenizerFactory();
t.setTokenPreProcessor(new CommonPreprocessor());
VocabCacheExporter exporter = new VocabCacheExporter();

SparkWord2Vec word2Vec = new SparkWord2Vec.Builder(paramServerConfig)
            .setTokenizerFactory(t)
            .setLearningAlgorithm(new SparkCBOW())
            .setModelExporter(exporter)
            .layerSize(113)
            .epochs(1)
            .workers(1)
            .useHierarchicSoftmax(false)
            .negativeSampling(10)
            .build();
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='elifaj' date='2018-10-05T06:24:16Z'>
		Dear,
I have faced to similar issue when training model using spark on AWS EMR cluster. Is there any workaround for this issue?
Thank you all.
		</comment>
		<comment id='4' author='elifaj' date='2018-10-05T06:26:14Z'>
		There's no workaround. Our sparkw2v doesn't support yarn atm.
		</comment>
		<comment id='5' author='elifaj' date='2018-10-05T07:26:35Z'>
		
There's no workaround. Our sparkw2v doesn't support yarn atm.

thanks for letting me know and your quick response.
		</comment>
		<comment id='6' author='elifaj' date='2018-11-04T07:45:02Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>