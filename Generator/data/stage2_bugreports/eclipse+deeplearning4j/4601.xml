<bug id='4601' author='AlexDBlack' open_date='2018-02-02T09:15:47Z' closed_time='2018-02-03T01:46:36Z'>
	<summary>Workspace crash on input array migration</summary>
	<description>
Requires the following PR to reproduce:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4600&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4600&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;    @Test
    public void debugMigrateCrash() {
        WorkspaceMode ws = WorkspaceMode.SINGLE;

        Nd4j.getRandom().setSeed(123);
        ComputationGraphConfiguration conf = new NeuralNetConfiguration.Builder()
                .weightInit(WeightInit.XAVIER)
                .inferenceWorkspaceMode(WorkspaceMode.NONE)
                .trainingWorkspaceMode(ws)
                .graphBuilder()
                .addInputs("in")
                .addLayer("0", new DenseLayer.Builder().nIn(4).nOut(3).activation(Activation.TANH).build(), "in")
                .addLayer("1", new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(
                                LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(3)
                                .build(),"0")
                .setOutputs("1")
                .build();


        ComputationGraph network = new ComputationGraph(conf);
        network.init();

        DataSetIterator iter = new IrisDataSetIterator(50, 150);

        //iter = new AsyncShieldDataSetIterator(iter);    //Async shield: no more JVM crash

        for (int i = 0; i &lt; 100; i++) {
            log.info("Fit " + i);
            network.fit(iter);
        }
    }
&lt;/denchmark-code&gt;

JVM crash log:
&lt;denchmark-link:https://gist.github.com/AlexDBlack/29cdf35346304a57f87e640033e858fe&gt;https://gist.github.com/AlexDBlack/29cdf35346304a57f87e640033e858fe&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-02-02T19:27:52Z'>
		Are you sure it's not specific for this Iterator implementation? There's obvious use-after-reset possible internally, which can cause weird things, like that crash.
i.e. curr in BaseDataFetcher gets allocated in spilled area, and reset just resets cursor. So on hasMore()/next() call the same curr instance gets sent through the pipeline, despite pointer was invalidated.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-02-02T20:03:45Z'>
		Mmm, no. that's not the case.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-02-02T20:35:41Z'>
		Stale pointer in labels field of output layer. Details are in pm.
		</comment>
		<comment id='4' author='AlexDBlack' date='2018-02-03T01:46:36Z'>
		Confirmed that this seems to be due to migrating old / out of scope array.
Fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4600&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4600&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='AlexDBlack' date='2018-09-23T15:26:10Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>