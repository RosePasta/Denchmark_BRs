<bug id='2295' author='liulhdarks' open_date='2016-11-10T01:43:34Z' closed_time='2016-11-10T05:30:50Z'>
	<summary>java.lang.IllegalArgumentException when use other OptimizationAlgorithm except STOCHASTIC_GRADIENT_DESCENT in RNN.</summary>
	<description>
&lt;denchmark-h:h2&gt;Code&lt;/denchmark-h&gt;

====================================
WordVectors wordVectors = WordVectorSerializer.loadGoogleModel(new File(word2vecModelPath), false, true);
        ConversionUbuntuMultiDataSetIterator iter = new ConversionUbuntuMultiDataSetIterator(wordVectors, convPath, 40);
        ConversionUbuntuCorpus corpus = iter.corpus;
        int featureVecSize = wordVectors.lookupTable().layerSize();
        int outputVecSize = iter.getOutputDimesion();
        int numHiddenNodes = 368;
        int outputDimension = corpus.getIndexMap().size();
        ComputationGraphConfiguration configuration = new NeuralNetConfiguration.Builder()
	        .weightInit(WeightInit.XAVIER)
	        .learningRate(0.001)
	        .updater(Updater.RMSPROP)
	        .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)
	        .iterations(1)
	        .seed(123)
	        .graphBuilder()
	        .addInputs("termVecIn", "convOut")
	        .setInputTypes(InputType.recurrent(featureVecSize), 
	            InputType.recurrent(outputVecSize))
	        .addLayer("encoder", new GravesLSTM.Builder().nIn(featureVecSize).nOut(numHiddenNodes)
	            .activation("softsign").build(),"termVecIn")
	        .addVertex("lastTimeStep", new LastTimeStepVertex("termVecIn"), "encoder")
            .addVertex("duplicateTimeStep", new DuplicateToTimeSeriesVertex("convOut"), "lastTimeStep")
            .addLayer("decoder", new GravesLSTM.Builder().nIn(outputVecSize + numHiddenNodes).nOut(numHiddenNodes)
                .activation("softsign").build(), "convOut", "duplicateTimeStep")
	        .addLayer("output", new RnnOutputLayer.Builder().nIn(numHiddenNodes).nOut(outputDimension)
	            .activation("softmax").lossFunction(LossFunctions.LossFunction.MCXENT).build(), "decoder")
	        .setOutputs("output")
	        .pretrain(false).backprop(true)
	        .build();
==============================================================
Exception in thread "main" java.lang.IllegalArgumentException: Length must be &gt;= 1
at org.nd4j.linalg.api.buffer.BaseDataBuffer.(BaseDataBuffer.java:440)
at org.nd4j.linalg.api.buffer.DoubleBuffer.(DoubleBuffer.java:52)
at org.nd4j.linalg.api.buffer.factory.DefaultDataBufferFactory.createDouble(DefaultDataBufferFactory.java:228)
at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1282)
at org.nd4j.linalg.factory.Nd4j.createBuffer(Nd4j.java:1252)
at org.nd4j.linalg.api.ndarray.BaseNDArray.(BaseNDArray.java:249)
at org.nd4j.linalg.cpu.nativecpu.NDArray.(NDArray.java:112)
at org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.create(CpuNDArrayFactory.java:248)
at org.nd4j.linalg.factory.Nd4j.create(Nd4j.java:4369)
at org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.toFlattened(CpuNDArrayFactory.java:503)
at org.nd4j.linalg.factory.Nd4j.toFlattened(Nd4j.java:1707)
at org.deeplearning4j.nn.gradient.DefaultGradient.gradient(DefaultGradient.java:73)
at org.deeplearning4j.optimize.solvers.BaseOptimizer.setupSearchState(BaseOptimizer.java:298)
at org.deeplearning4j.optimize.solvers.BaseOptimizer.optimize(BaseOptimizer.java:174)
at org.deeplearning4j.optimize.Solver.optimize(Solver.java:51)
at org.deeplearning4j.nn.graph.ComputationGraph.fit(ComputationGraph.java:719)
at org.deeplearning4j.demo.conversion.ubuntu.ConversionUbuntuDemo.main(ConversionUbuntuDemo.java:84)
	</description>
	<comments>
		<comment id='1' author='liulhdarks' date='2016-11-10T05:30:50Z'>
		Fixed here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/2296&gt;https://github.com/deeplearning4j/deeplearning4j/pull/2296&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='liulhdarks' date='2019-01-20T17:01:00Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>