<bug id='4942' author='rothn' open_date='2018-04-18T04:03:02Z' closed_time='2019-02-07T12:33:16Z'>
	<summary>doc2vec / Paragraph Vectors Uses word2vec loss function</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;


In the current DL4J implementation, paragraph vectors does not use the doc2vec loss function
In fact, it is trained in what seems to be exactly the same way as word2vec
Specifically, in this paper: https://cs.stanford.edu/~quocle/paragraph_vector.pdf

The parameter "h" should be constructed from a mathematical concatenation (or average, as DL4J uses) of the context words in W and the paragraph vector from D
This is mentioned in the second paragraph of section 2.2 and clarified in the third paragraph
Currently, DL4J uses equation 1 without modification



This results in an improperly trained model. In many cases, users may simply be better off by averaging word vectors instead of inferring a paragraph vector since the word vectors were not trained w.r.t. a paragraph vector and thus the result of a prediction task could be arbitrary.
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j 0.9.1
All platforms

Aha! Link: &lt;denchmark-link:https://skymindai.aha.io/features/ND4J-83&gt;https://skymindai.aha.io/features/ND4J-83&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='rothn' date='2018-04-18T04:31:16Z'>
		I need to check that. As I remember impl was based on some python implementation.

Thanks for highlighting.

With best regards,
Vyacheslav K.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 On Apr 18, 2018, at 07:03, Nicholas Roth ***@***.***&gt; wrote:

 Issue Description

 In the current DL4J implementation, paragraph vectors does not use the doc2vec loss function
 In fact, it is trained in what seems to be exactly the same way as word2vec
 Specifically, in this paper: https://cs.stanford.edu/~quocle/paragraph_vector.pdf
 The parameter "h" should be constructed from a mathematical concatenation (or average, as DL4J uses) of the context words in W and the paragraph vector for D
 This is mentioned in the second paragraph of section 2.2 and clarified in the third paragraph
 Currently, DL4J uses equation 1 without modification
 This results in an improperly trained model. In many cases, users may simply be better off by averaging word vectors instead of inferring a paragraph vector since the word vectors were not trained w.r.t. a paragraph vector and thus the result of a prediction task could be arbitrary.

 Version Information

 Please indicate relevant versions, including, if relevant:

 Deeplearning4j 0.9.1
 All platforms
 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub, or mute the thread.



		</comment>
		<comment id='2' author='rothn' date='2018-04-18T04:48:03Z'>
		If you're going back into the implementation, it may also be worth noting
that mathematical concatenation has been shown to work better than an
average.

Also, I really like this implementation (in Python) from the Machine
Learning Cookbook:
&lt;denchmark-link:https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py&gt;https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py&lt;/denchmark-link&gt;


In this case, the lines that DL4J seems to be missing are 109 and 112
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Apr 17, 2018 at 9:31 PM, raver119 ***@***.***&gt; wrote:
 I need to check that. As I remember impl was based on some python
 implementation.

 Thanks for highlighting.

 With best regards,
 Vyacheslav K.

 &gt; On Apr 18, 2018, at 07:03, Nicholas Roth ***@***.***&gt;
 wrote:
 &gt;
 &gt; Issue Description
 &gt;
 &gt; In the current DL4J implementation, paragraph vectors does not use the
 doc2vec loss function
 &gt; In fact, it is trained in what seems to be exactly the same way as
 word2vec
 &gt; Specifically, in this paper: https://cs.stanford.edu/~
 quocle/paragraph_vector.pdf
 &gt; The parameter "h" should be constructed from a mathematical
 concatenation (or average, as DL4J uses) of the context words in W and the
 paragraph vector for D
 &gt; This is mentioned in the second paragraph of section 2.2 and clarified
 in the third paragraph
 &gt; Currently, DL4J uses equation 1 without modification
 &gt; This results in an improperly trained model. In many cases, users may
 simply be better off by averaging word vectors instead of inferring a
 paragraph vector since the word vectors were not trained w.r.t. a paragraph
 vector and thus the result of a prediction task could be arbitrary.
 &gt;
 &gt; Version Information
 &gt;
 &gt; Please indicate relevant versions, including, if relevant:
 &gt;
 &gt; Deeplearning4j 0.9.1
 &gt; All platforms
 &gt; —
 &gt; You are receiving this because you are subscribed to this thread.
 &gt; Reply to this email directly, view it on GitHub, or mute the thread.
 &gt;

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/4942#issuecomment-382257079&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AEcxtzdRYB5Re7ZqPuW-IlGm8pavVTdYks5tpsGygaJpZM4TZY5H&gt;
 .



		</comment>
		<comment id='3' author='rothn' date='2018-06-15T22:38:21Z'>
		?
		</comment>
		<comment id='4' author='rothn' date='2019-02-07T12:33:16Z'>
		Sorry for the delay, forgot to post answer here :(
You're fererring to PV-DM, which is basically CBOW.
Here's the piece of code where label gets sent to cpp together with context words for averaging:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/0be227a98302585ccb229e5275967f947a1a3a3e/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/learning/impl/sequence/DM.java#L128-L161&gt;https://github.com/deeplearning4j/deeplearning4j/blob/0be227a98302585ccb229e5275967f947a1a3a3e/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/embeddings/learning/impl/sequence/DM.java#L128-L161&lt;/denchmark-link&gt;

And here's the piece of code where actual averaging happens:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/38626210a2bf61d06bc14123f0121cacba4752fa/libnd4j/include/ops/declarable/helpers/cpu/sg_cb.cpp#L547-L570&gt;https://github.com/deeplearning4j/deeplearning4j/blob/38626210a2bf61d06bc14123f0121cacba4752fa/libnd4j/include/ops/declarable/helpers/cpu/sg_cb.cpp#L547-L570&lt;/denchmark-link&gt;

And it was like that since day 0.
Idea of concatenation is nice, and we'll consider adding that as option.
Once again, sorry for late response.
		</comment>
		<comment id='5' author='rothn' date='2019-03-09T13:07:35Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>