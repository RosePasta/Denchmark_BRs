<bug id='3064' author='crockpotveggies' open_date='2017-03-17T19:18:04Z' closed_time='2018-04-25T22:42:21Z'>
	<summary>cuDNN CUDNN_STATUS_EXECUTION_FAILED on certain batch/GC windows</summary>
	<description>
&lt;denchmark-code&gt;[main] ERROR java.lang.Throwable - Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: java.lang.IllegalStateException: MemcpyAsync H2H failed: [140405908257904] -&gt; [1112606498304]
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.parallelism.ParallelWrapper.fit(ParallelWrapper.java:403)
[main] ERROR java.lang.Throwable - 	at ai.bernie.train.ClassifierTraining.run(ClassifierTraining.java:242)
[main] ERROR java.lang.Throwable - 	at ai.bernie.train.ParallelOptimization.main(ParallelOptimization.java:121)
[main] ERROR java.lang.Throwable - Caused by: java.lang.RuntimeException: java.lang.IllegalStateException: MemcpyAsync H2H failed: [140405908257904] -&gt; [1112606498304]
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.parallelism.ParallelWrapper$Trainer.waitTillRunning(ParallelWrapper.java:814)
[main] ERROR java.lang.Throwable - 	at org.deeplearning4j.parallelism.ParallelWrapper.fit(ParallelWrapper.java:401)
[main] ERROR java.lang.Throwable - 	... 2 more
[main] ERROR java.lang.Throwable - Caused by: java.lang.IllegalStateException: MemcpyAsync H2H failed: [140405908257904] -&gt; [1112606498304]
&lt;/denchmark-code&gt;

See gist for full output: &lt;denchmark-link:https://gist.github.com/crockpotveggies/e20d56c57c523982f5acbb37620b447a&gt;https://gist.github.com/crockpotveggies/e20d56c57c523982f5acbb37620b447a&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='crockpotveggies' date='2017-03-17T19:36:20Z'>
		Interestingly, tuning both cache and GC frequency alleviated memory errors:
&lt;denchmark-code&gt;    CudaEnvironment.getInstance().getConfiguration()
      // key option enabled
      .allowMultiGPU(true)
      .allowCrossDeviceAccess(true)
      // we're allowing larger memory caches
      .setMaximumDeviceCache(6L * 1024L * 1024L * 1024L)
      .setMaximumHostCache(8L * 1024L * 1024L * 1024L)
      // cross-device access is used for faster model averaging over pcie
      .allowFallbackFromDevice(false)
      .setNumberOfGcThreads(6);

    Nd4j.getMemoryManager().togglePeriodicGc(true);
    Nd4j.getMemoryManager().setAutoGcWindow(50);
    Nd4j.getMemoryManager().setOccasionalGcFrequency(0);
&lt;/denchmark-code&gt;

I reduced the AutoGcWindow to a small value of 50. I also had to reduce my batch size to 16. It appears GC cannot keep up with GPU execution.
		</comment>
		<comment id='2' author='crockpotveggies' date='2017-03-17T19:41:22Z'>
		Updated title of issue since this appears to be happening on both ParallelWrapper and single device training. On ParallelWrapper it manifests as an H2H memory sync issue. On single device it often manifests as CUDNN_STATUS_EXECUTION_FAILED.
Some guesses here are that GC window/cache/batch size are affecting this.
		</comment>
		<comment id='3' author='crockpotveggies' date='2017-03-17T20:28:20Z'>
		I made GC window, batch size, and device cache easily tunable. Here's a table of these params, and the resulting error:



ParallelWrapper
Batch size
GC Window
Device Cache
NaNs
Error




no
16
110
6
n/a
Can't allocate [HOST] memory: 9437184; threadId: 1


no
16
110
7
yes
CUDNN_STATUS_EXECUTION_FAILED


no
12
110
7
yes
CUDNN_STATUS_EXECUTION_FAILED


no
24
110
7
yes
CUDA error at /home/justin/Projects/libnd4j/blas/cuda/NativeOps.cu:4851 code=77() "result"


no
24
110
7
n/a
CUDNN_STATUS_EXECUTION_FAILED


no
24
300
7
yes
CUDNN_STATUS_EXECUTION_FAILED


no
24
80
6
yes
CUDNN_STATUS_EXECUTION_FAILED


no
24
80
3
yes
CUDNN_STATUS_EXECUTION_FAILED


yes
24
80
8
n/a
CUDNN_STATUS_INTERNAL_ERROR


yes
24
80
6
n/a
CUDNN_STATUS_NOT_INITIALIZED


no
24
80
4
n/a
CUDNN_STATUS_EXECUTION_FAILED


no
16
110
4
yes
Failed on [140523549164464] -&gt; [1118420707328], size: [5852], direction: [0], result: [77]


no
16
30
4
yes
no errors, but all scores NaN


no
16
50
4
yes
no errors, but all scores NaN


no
16
80
4
yes
no errors, but all scores NaN



		</comment>
		<comment id='4' author='crockpotveggies' date='2017-03-17T23:05:12Z'>
		Does this happen only when using cuDNN? No issues without?
		</comment>
		<comment id='5' author='crockpotveggies' date='2017-03-17T23:35:15Z'>
		Yes only happens when cuDNN is enabled. When disabled, NaN scores disappear. I've also tried enabling/disabling double precision with no change in outcome.
		</comment>
		<comment id='6' author='crockpotveggies' date='2017-03-18T00:03:16Z'>
		I mean, the MemcpyAsync errors too only happen with cuDNN?
		</comment>
		<comment id='7' author='crockpotveggies' date='2017-03-18T00:12:31Z'>
		Correct, MemcpyAsync only happens with cuDNN
		</comment>
		<comment id='8' author='crockpotveggies' date='2017-03-18T08:56:20Z'>
		Sounds like gc race condition? Something gets deallocated prematurely?
		</comment>
		<comment id='9' author='crockpotveggies' date='2017-03-19T00:28:09Z'>
		Agree is it possible cuDNN moves fast enough to trigger a race condition?
What about GC?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sat, Mar 18, 2017 at 1:56 AM raver119 ***@***.***&gt; wrote:
 Sounds like gc race condition? Something gets deallocated prematurely?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/3064#issuecomment-287527256&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABYNrwR2JLrMk8x7gluZs7FyGExrri-zks5rm5xCgaJpZM4Mg_yd&gt;
 .



		</comment>
		<comment id='10' author='crockpotveggies' date='2017-03-19T13:02:12Z'>
		Let's try to find something easy to debug here. Are you having any issues with the MultiGpuLenetMnistExample sample? It seems to be working fine here...
		</comment>
		<comment id='11' author='crockpotveggies' date='2017-03-20T14:29:17Z'>
		Negative I haven't been having issues with the example. I spent some time
on the problem this morning and the mystery deepens. cuDNN doesn't seem to
be loading into my local environment at all after a reboot, even after
including the deeplearning4j-cuda-8.0 dependency and doing a clean rebuild.

Is it possible this is due to some sort of local environment issue?
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Sun, Mar 19, 2017 at 6:02 AM Samuel Audet ***@***.***&gt; wrote:
 Let's try to find something easy to debug here. Are you having any issues
 with the MultiGpuLenetMnistExample sample? It seems to be working fine
 here...

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/3064#issuecomment-287615192&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABYNr9txbapaU5HQprqPAHGcaHev1_s1ks5rnSdkgaJpZM4Mg_yd&gt;
 .



		</comment>
		<comment id='12' author='crockpotveggies' date='2017-03-20T14:38:06Z'>
		Ah sorry, I hit send too early. It is NOT my local environment, I had
double precision enabled.

Changing the GC window to 15 simplifies the problem. the
CUDNN_INTERNAL_ERROR goes away and I'm left with the following:

```
[main] INFO CLSTAGE2-0.1LR-48BATCH - Beginning parallel training....
CUDA error at /home/justin/Projects/libnd4j/blas/cuda/NativeOps.cu:4841
code=77(&lt;unknown&gt;) "result"
CUDA error at /home/justin/Projects/libnd4j/blas/cuda/NativeOps.cu:4822
code=77(&lt;unknown&gt;) "result"
CUDA error at /home/justin/Projects/libnd4j/blas/cuda/NativeOps.cu:4841
code=77(&lt;unknown&gt;) "result"
CUDA error at /home/justin/Projects/libnd4j/blas/cuda/NativeOps.cu:4801
code=77(&lt;unknown&gt;) "result"
Failed on [140258331191840] -&gt; [1118419890176], size: [5852], direction:
[0], result: [77]
[ParallelWrapper trainer 1] WARN org.nd4j.jita.handler.impl.CudaZeroHandler
- Out of [DEVICE] memory, host memory will be used instead: deviceId: [1],
requested bytes: [48]
```

It appears it can't allocate memory fast enough. Possibly a race condition?
My network is much more complex than the MNIST example, so there's a lot
more gears turning...

On Mon, Mar 20, 2017 at 7:29 AM Justin Long &lt;justinelgenlong@gmail.com&gt;
wrote:
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Negative I haven't been having issues with the example. I spent some time
 on the problem this morning and the mystery deepens. cuDNN doesn't seem to
 be loading into my local environment at all after a reboot, even after
 including the deeplearning4j-cuda-8.0 dependency and doing a clean rebuild.

 Is it possible this is due to some sort of local environment issue?

 On Sun, Mar 19, 2017 at 6:02 AM Samuel Audet ***@***.***&gt;
 wrote:

 Let's try to find something easy to debug here. Are you having any issues
 with the MultiGpuLenetMnistExample sample? It seems to be working fine
 here...

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/3064#issuecomment-287615192&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABYNr9txbapaU5HQprqPAHGcaHev1_s1ks5rnSdkgaJpZM4Mg_yd&gt;
 .




		</comment>
		<comment id='13' author='crockpotveggies' date='2017-03-20T14:40:07Z'>
		This thread isn't going anywhere, just wastes your time. Give me some code that i can run locally, and i'll find &amp; fix issue.
		</comment>
		<comment id='14' author='crockpotveggies' date='2017-03-20T14:44:30Z'>
		You can use the same repo I've sent you previously. Just change your run
params to this:

`java -Xmx24g -cp build/libs/train-triplet-embeddings-0.1-SNAPSHOT-all.jar
ai.bernie.train.ParallelOptimization true true false 0.1 48 2 ADAM 0.001
XAVIER 25 2 /home/YOUR_DATA_PATH_HERE/`

Note the 25 above is GC window, and the 2 is device cache (I've
experimented with different values)
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Mar 20, 2017 at 7:40 AM raver119 ***@***.***&gt; wrote:
 This thread isn't going anyway, just wastes your time. Give me some code i
 can run locally, and i'll find &amp; fix issue.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/3064#issuecomment-287779365&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABYNr1kNo5yHaCbbFW6m7NsMpvLjzkKNks5rno_WgaJpZM4Mg_yd&gt;
 .



		</comment>
		<comment id='15' author='crockpotveggies' date='2017-03-20T14:44:58Z'>
		what is "device cache" in this case?
		</comment>
		<comment id='16' author='crockpotveggies' date='2017-03-20T14:45:45Z'>
		from ParallelOptimization.java
&lt;denchmark-code&gt;CudaEnvironment.getInstance().getConfiguration()
      // key option enabled
      .allowMultiGPU(true)
      .allowCrossDeviceAccess(true)
      // we're allowing larger memory caches
      .setMaximumDeviceCache(deviceCache * 1024L * 1024L * 1024L)
      .setMaximumHostCache(12L * 1024L * 1024L * 1024L)
      // cross-device access is used for faster model averaging over pcie
      .allowFallbackFromDevice(false)
      .setNumberOfGcThreads(6);
&lt;/denchmark-code&gt;

		</comment>
		<comment id='17' author='crockpotveggies' date='2017-03-20T14:50:21Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 just realized your copy of the repo doesn't have same configurations. feel free to pull from BitBucket
		</comment>
		<comment id='18' author='crockpotveggies' date='2017-03-21T00:02:44Z'>
		I ran  and here's some output: &lt;denchmark-link:https://gist.github.com/crockpotveggies/bee9ff1e9091bd82adb5f3dd1556bd7b&gt;https://gist.github.com/crockpotveggies/bee9ff1e9091bd82adb5f3dd1556bd7b&lt;/denchmark-link&gt;

		</comment>
		<comment id='19' author='crockpotveggies' date='2017-03-21T00:34:54Z'>
		That's interesting. Literally second time in my life i see cuda-memcheck gives something valuable in output :)
		</comment>
		<comment id='20' author='crockpotveggies' date='2017-03-21T00:50:05Z'>
		Awesome. It's replicable too:
&lt;denchmark-code&gt;========= Invalid __global__ read of size 4
=========     at 0x00000c50 in maxwell_scudnn_128x64_medium_nn
=========     by thread (47,0,0) in block (0,0,0)
=========     Address 0x1046cbffff8 is out of bounds
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so.1 (cuLaunchKernel + 0x2c5) [0x209035]
=========     Host Frame:/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10 [0x564e51]
=========     Host Frame:/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10 [0x5829f3]
=========     Host Frame:/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10 [0x30346e]
=========     Host Frame:/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10 [0x107dfe]
=========     Host Frame:/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10 [0x1099e7]
=========     Host Frame:/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10 [0xd0ec3]
=========     Host Frame:/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10 (cudnnConvolutionForward + 0x806) [0x41af6]
=========     Host Frame:/home/justin/.javacpp/cache/train-triplet-embeddings-0.1-SNAPSHOT-all.jar/org/bytedeco/javacpp/linux-x86_64/libjnicudnn.so (Java_org_bytedeco_javacpp_cudnn_cudnnConvolutionForward + 0x2ef) [0x1ecaf]
=========     Host Frame:[0x7fdb79017a14]
=========
========= Program hit cudaErrorLaunchFailure (error 4) due to "unspecified launch failure" on CUDA API call to cudaStreamSynchronize.
CUDA error at /home/justin/Projects/libnd4j/blas/cuda/NativeOps.cu:4841 code=4(cudaErrorLaunchFailure) "result"
=========     Saved host backtrace up to driver entry point at error
=========     Host Frame:/usr/lib/x86_64-linux-gnu/libcuda.so.1 [0x2f4e33]
=========     Host Frame:/home/justin/.javacpp/cache/train-triplet-embeddings-0.1-SNAPSHOT-all.jar/org/nd4j/nativeblas/linux-x86_64/libnd4jcuda.so [0x410e2e]
=========     Host Frame:/home/justin/.javacpp/cache/train-triplet-embeddings-0.1-SNAPSHOT-all.jar/org/nd4j/nativeblas/linux-x86_64/libnd4jcuda.so (_ZN9NativeOps17streamSynchronizeEPv + 0xc) [0x165c4c]
=========     Host Frame:/home/justin/.javacpp/cache/train-triplet-embeddings-0.1-SNAPSHOT-all.jar/org/nd4j/nativeblas/linux-x86_64/libjnind4jcuda.so (Java_org_nd4j_nativeblas_Nd4jCuda_00024NativeOps_streamSynchronize + 0x87) [0x3e0f7]
=========     Host Frame:[0x7fdb79658384]
=========
&lt;/denchmark-code&gt;

		</comment>
		<comment id='21' author='crockpotveggies' date='2017-03-21T05:51:35Z'>
		Ah yes by default cuDNN uses a lot of memory. You should try the NO_WORKSPACE AlgoMode:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/search?utf8=%E2%9C%93&amp;q=AlgoMode&gt;https://github.com/deeplearning4j/deeplearning4j/search?utf8=%E2%9C%93&amp;q=AlgoMode&lt;/denchmark-link&gt;

		</comment>
		<comment id='22' author='crockpotveggies' date='2017-03-21T17:17:21Z'>
		Confirmed NO_WORKSPACE AlgoMode seems to have alleviated the issue. Let me
try this with a long-running process and report back.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Mar 20, 2017 at 10:51 PM Samuel Audet ***@***.***&gt; wrote:
 Ah yes by default cuDNN uses a lot of memory. You should try the
 NO_WORKSPACE AlgoMode:

 https://github.com/deeplearning4j/deeplearning4j/search?utf8=%E2%9C%93&amp;q=AlgoMode

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/3064#issuecomment-287983799&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABYNr2JmUbUHqyCz7hfuZw972Csdk2Tjks5rn2V2gaJpZM4Mg_yd&gt;
 .



		</comment>
		<comment id='23' author='crockpotveggies' date='2017-03-21T19:14:08Z'>
		Long-running process has been going for more than an hour now with no
issues. Next question is: does an underlying issue still exist? I.e.:
should I be able to use fastest algo mode with my current setup (2xTitanX
GPU, 64GB RAM)?

On Tue, Mar 21, 2017 at 10:17 AM Justin Long &lt;justinelgenlong@gmail.com&gt;
wrote:
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


 Confirmed NO_WORKSPACE AlgoMode seems to have alleviated the issue. Let me
 try this with a long-running process and report back.

 On Mon, Mar 20, 2017 at 10:51 PM Samuel Audet ***@***.***&gt;
 wrote:

 Ah yes by default cuDNN uses a lot of memory. You should try the
 NO_WORKSPACE AlgoMode:

 https://github.com/deeplearning4j/deeplearning4j/search?utf8=%E2%9C%93&amp;q=AlgoMode

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/3064#issuecomment-287983799&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABYNr2JmUbUHqyCz7hfuZw972Csdk2Tjks5rn2V2gaJpZM4Mg_yd&gt;
 .




		</comment>
		<comment id='24' author='crockpotveggies' date='2017-03-22T02:39:52Z'>
		Maybe, how much more memory does it allocate? Maybe it's having to allocate that memory on the host, and we're triggering bugs in cuDNN because they don't expect people to do that :)
		</comment>
		<comment id='25' author='crockpotveggies' date='2017-03-22T02:51:37Z'>
		In total or per call? It fails randomly with different values. Sometimes it
will try to allocate 28MB and fail. Other times it's as small as 248 bytes.
I'm not sure how I can track total allocation without a profiler.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Tue, Mar 21, 2017 at 7:40 PM Samuel Audet ***@***.***&gt; wrote:
 Maybe, how much more memory does it allocate? Maybe it's having to
 allocate that memory on the host, and we're triggering bugs in cuDNN
 because they don't expect people to do that :)

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 &lt;https://github.com/deeplearning4j/deeplearning4j/issues/3064#issuecomment-288281470&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ABYNr-z8zt6gzYlEj3xeqZeRGL_U9RtVks5roIoKgaJpZM4Mg_yd&gt;
 .



		</comment>
		<comment id='26' author='crockpotveggies' date='2017-03-22T02:58:16Z'>
		I meant, what is the difference in total allocated bytes between when using and when not using NO_WORKSPACE? Anyway, if cuDNN is trying to allocate on the host, you'll get warnings in the log, for example:
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-cuda/src/main/java/org/deeplearning4j/nn/layers/convolution/CudnnConvolutionHelper.java#L157&gt;https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-cuda/src/main/java/org/deeplearning4j/nn/layers/convolution/CudnnConvolutionHelper.java#L157&lt;/denchmark-link&gt;

		</comment>
		<comment id='27' author='crockpotveggies' date='2017-03-22T03:19:38Z'>
		I do not get any kind of warnings with NO_WORKSPACE. With fastest algorithm
it gives me errors in various order, but usually the host runs out of
memory before the device.
		</comment>
		<comment id='28' author='crockpotveggies' date='2017-03-28T23:49:37Z'>
		Do we want to wait until workspaces are ready from &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 before closing this?
		</comment>
		<comment id='29' author='crockpotveggies' date='2018-04-25T22:42:21Z'>
		I think this issue isn't relevant for quite some time now.
		</comment>
		<comment id='30' author='crockpotveggies' date='2018-09-22T18:13:34Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>