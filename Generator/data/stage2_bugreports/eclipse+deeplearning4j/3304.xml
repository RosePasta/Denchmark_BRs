<bug id='3304' author='Alcalol' open_date='2017-04-25T19:33:20Z' closed_time='2017-05-08T11:50:46Z'>
	<summary>CUDA out of memory crash during parallel training</summary>
	<description>
When using parallel trainer (CUDA) to train 10 networks one after another, GPU memory does not seem to clear but rather uses GPU memory in a cumulative fashion.  So just as an example if each network requires 1024mb of memory to process, the GPU memory usage would start at 1024 for network 1, then 2048 for network 2 etc etc until memory is full and the application crashes.  This information was gained by studying sensor information in GPU-Z.
It might be worth noting that this crash does not happen while doing single GPU (non parallel) training, however I cannot say for sure if this is just delaying the inevitable by lowering memory requirement per cycle or if the issue does not apply.
I am currently using CUDA version 8.0.61 (also tried 8.0.44), and version 0.8.0 of DL4j (2.11?) and ND4J on a windows 10 machine consisting of 2x GTX 780 cards with SLi mode turned off.  My nvidia graphics driver version is 381.89.
Please find following the full error log, as well as the various classes in my code which creates and trains the network.
error log
&lt;denchmark-link:https://gist.github.com/Alcalol/974771e6d3bb6ae4202c2cc4d22a79a3&gt;https://gist.github.com/Alcalol/974771e6d3bb6ae4202c2cc4d22a79a3&lt;/denchmark-link&gt;

Network creater class
&lt;denchmark-link:https://gist.github.com/Alcalol/1c24f5176cecc09d30c5b7eb70db8de0&gt;https://gist.github.com/Alcalol/1c24f5176cecc09d30c5b7eb70db8de0&lt;/denchmark-link&gt;

trainer class
&lt;denchmark-link:https://gist.github.com/Alcalol/64fced41384f9d30db38b7f5868c2d73&gt;https://gist.github.com/Alcalol/64fced41384f9d30db38b7f5868c2d73&lt;/denchmark-link&gt;

main class
&lt;denchmark-link:https://gist.github.com/Alcalol/5c276ffa3024cdf63f965e43375b5303&gt;https://gist.github.com/Alcalol/5c276ffa3024cdf63f965e43375b5303&lt;/denchmark-link&gt;

Thanks
	</description>
	<comments>
		<comment id='1' author='Alcalol' date='2017-05-08T11:50:46Z'>
		Right, that looks like flaw of nd4j memory cache implementation. That's bad news.
There's good news though: at current snapshot, with workspaces enabled that shouldn't be an issue anymore, since it doesn't rely on cache for individual INDArray allocations.
		</comment>
		<comment id='2' author='Alcalol' date='2018-09-28T15:26:42Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>