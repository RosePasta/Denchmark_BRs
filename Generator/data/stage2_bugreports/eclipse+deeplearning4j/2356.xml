<bug id='2356' author='fac2003' open_date='2016-11-22T18:43:44Z' closed_time='2016-11-25T12:49:47Z'>
	<summary>Dropout on GPU does not scale to large training sets</summary>
	<description>
This issue is related to the change in 0.7.0 to migrate dropout/ random number generation to the GPU (see prior issue that triggered these changes at &lt;denchmark-link:https://github.com/deeplearning4j/nd4j/issues/1202&gt;deeplearning4j/nd4j#1202&lt;/denchmark-link&gt;
).
I can confirm the speed improvement for dropout in 0.7.0 on GPU when the dataset is very small, but see a huge penalty when training with thousands of minibatches:
Training with one minibatch per epoch:
0.7.0 without dropout:
11:57:03.177 [main] INFO o.c.dl.framework.tools.TrainModel - 3 epoch, 12s, 15.01 epoch/m, 4.00 s/epoch [15.01 epoch/m, 4.00 s/epoch]; 0% done, 1788973h 10m 48s to end
11:57:16.384 [main] INFO o.c.dl.framework.tools.TrainModel - 7 epoch, 25s, 16.64 epoch/m, 3.60 s/epoch [18.13 epoch/m, 3.31 s/epoch]; 0% done, 1881657h 58m 26s to end
0.7.0, dropout-rate 0.15
11:56:11.062 [main] INFO o.c.dl.framework.tools.TrainModel - 2 epoch, 12s, 9.37 epoch/m, 6.41 s/epoch [9.37 epoch/m, 6.41 s/epoch]; 0% done, 2547154h 12m 44s to end
11:56:22.518 [main] INFO o.c.dl.framework.tools.TrainModel - 4 epoch, 24s, 9.89 epoch/m, 6.07 s/epoch [10.47 epoch/m, 5.73 s/epoch]; 0% done, 2895523h 46m 43s to end
without dropout:
This performance is much better than 0.6.0 where we saw dropout 4 times slower on GPU.
Training with many minibatches per epoch:
0.7.0. Training with 2765 minibatches per epoch
With dropout, full dataset, 0.15: -- 0.96% GPU utilization
12:03:38.713 [main] INFO o.c.dl.framework.tools.TrainModel - 2,048 mini-batch, 5m 39s, 6.04 mini-batch/s, 165.67 ms/mini-batch; 74% done, 1m 58s to end
12:05:40.904 [main] INFO o.c.dl.framework.tools.TrainModel - 1 epoch, 7m 41s, 7.80 epoch/h, 7.69 m/epoch [7.80 epoch/h, 7.69 m/epoch]; 0% done, 137644753h 51m 36s to end
No dropout, full dataset: 44% GPU utilization
12:55:18.055 [main] INFO o.c.dl.framework.tools.TrainModel - 2,048 mini-batch, 38s, 53.13 mini-batch/s, 18.82 ms/mini-batch; 74% done, 13s to end
12:55:34.114 [main] INFO o.c.dl.framework.tools.TrainModel - 1 epoch, 54s, 1.10 epoch/m, 54.61 s/epoch [1.10 epoch/m, 54.61 s/epoch]; 0% done, 16287470h 24m 30s to end
This is much worse than 0.6.0 on GPU. I am training a feedforward network with this architecture:
Number of parameters in layer 0: 243048
Number of parameters in layer 1: 629415
Number of parameters in layer 2: 265600
Number of parameters in layer 3: 112320
Number of parameters in layer 4: 47425
Number of parameters in layer 5: 352
Number of parameters in layer 6: 176
Total number of network parameters: 1298336
	</description>
	<comments>
		<comment id='1' author='fac2003' date='2016-11-22T19:01:07Z'>
		Will you be able to build libnd4j from sources if/when i'll provide you with custom rng setup branch?
		</comment>
		<comment id='2' author='fac2003' date='2016-11-22T19:04:08Z'>
		The workstation is not configured for building from source at this time, but I am thinking about how to share code and since data to help you reproduce this.
		</comment>
		<comment id='3' author='fac2003' date='2016-11-22T19:05:56Z'>
		Now i think more about that, i could just provide you with simple java file, that calls for dropout on different length manually, just to confirm issue. And if it works fine - i'll use it for problem tuning.
		</comment>
		<comment id='4' author='fac2003' date='2016-11-22T19:14:12Z'>
		Sure, could do this. I have a suspicion the million params could have a
role. I can test with a network 10 times smaller to see if this still
happens.
On Tue, Nov 22, 2016, 14:06 raver119 &lt;denchmark-link:mailto:notifications@github.com&gt;notifications@github.com&lt;/denchmark-link&gt;
 wrote:

Now i think more about that, i could just provide you with simple java
file, that calls for dropout on different length manually, just to confirm
issue. And if it works fine - i'll use it for problem tuning.
â€”
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub
https://github.com/deeplearning4j/deeplearning4j/issues/2356#issuecomment-262334128,
or mute the thread
https://github.com/notifications/unsubscribe-auth/ABpfl8R8BH0QoyovCv-wyMLpVXfJyDJfks5rAz0dgaJpZM4K5zBG
.


Fabien Campagne, PhD -- &lt;denchmark-link:http://campagnelab.org&gt;http://campagnelab.org&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;                                      @FabienCampagne
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://twitter.com/FabienCampagne&gt;https://twitter.com/FabienCampagne&lt;/denchmark-link&gt;

Assistant Professor, Dept. of Physiology and Biophysics
&lt;denchmark-code&gt;                     Institute for Computational Biomedicine
&lt;/denchmark-code&gt;

Associate Director,   Biomedical Informatics Core,
&lt;denchmark-code&gt;                  Clinical Translational Science Center
&lt;/denchmark-code&gt;

Weill Medical College of Cornell University
phone:  (646)-962-5613  1305 York Avenue
fax:    (646)-962-0383  Box 140
New York, NY 10021
Learn about Data Analysis with MetaR. &lt;denchmark-link:http://metaR.campagnelab.org&gt;http://metaR.campagnelab.org&lt;/denchmark-link&gt;

&lt;denchmark-link:http://metar.campagnelab.org/&gt;http://metar.campagnelab.org/&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='fac2003' date='2016-11-22T19:16:59Z'>
		Nah, don't bother.
I'll create simple test that checks time for dropout that uses current rng impl, and 0.6.0 dropout, which had no rng technically at all. And we'll see what's up there. If it'll be reproducible using such a trick - i'll just use it for fututre checks. Shame on me, i hadn't thought about this earlier.
		</comment>
		<comment id='6' author='fac2003' date='2016-11-22T20:59:03Z'>
		Damn, yes.
CPU performance with current implementation is better then legacy, and GPU performance is worse.
		</comment>
		<comment id='7' author='fac2003' date='2016-11-24T19:06:14Z'>
		So, time for some numbers.
0.7.0 contains bug, which causes low performance for rng with cuda backend.
Here's timing for DropOutInverted applied to different length inputs:
&lt;denchmark-code&gt;13:32:12,837 INFO  ~ Length: 100; Legacy time: 712 us, Current time: 1047 us; Legacy NPE: 7128 ns; Current NPE: 10473
13:32:12,840 INFO  ~ Length: 1000; Legacy time: 302 us, Current time: 1076 us; Legacy NPE: 302 ns; Current NPE: 1076
13:32:12,844 INFO  ~ Length: 10000; Legacy time: 449 us, Current time: 1265 us; Legacy NPE: 44 ns; Current NPE: 126
13:32:12,852 INFO  ~ Length: 100000; Legacy time: 358 us, Current time: 3706 us; Legacy NPE: 3 ns; Current NPE: 37
13:32:12,881 INFO  ~ Length: 1000000; Legacy time: 474 us, Current time: 28159 us; Legacy NPE: 0 ns; Current NPE: 28
13:32:13,186 INFO  ~ Length: 10000000; Legacy time: 1484 us, Current time: 275466 us; Legacy NPE: 0 ns; Current NPE: 27
13:32:16,216 INFO  ~ Length: 100000000; Legacy time: 10167 us, Current time: 2752678 us; Legacy NPE: 0 ns; Current NPE: 27
&lt;/denchmark-code&gt;

Here's time for fixed version:
&lt;denchmark-code&gt;22:03:17,459 INFO  ~ Length: 100; Legacy time: 1354 us, Current time: 67 us; Legacy NPE: 13549 ns; Current NPE: 674
22:03:17,462 INFO  ~ Length: 1000; Legacy time: 468 us, Current time: 61 us; Legacy NPE: 468 ns; Current NPE: 61
22:03:17,465 INFO  ~ Length: 10000; Legacy time: 363 us, Current time: 58 us; Legacy NPE: 36 ns; Current NPE: 5
22:03:17,467 INFO  ~ Length: 100000; Legacy time: 268 us, Current time: 79 us; Legacy NPE: 2 ns; Current NPE: 0
22:03:17,467 INFO  ~ Length: 1000000; Legacy time: 332 us, Current time: 295 us; Legacy NPE: 0 ns; Current NPE: 0
22:03:17,488 INFO  ~ Length: 10000000; Legacy time: 1388 us, Current time: 2540 us; Legacy NPE: 0 ns; Current NPE: 0
22:03:17,677 INFO  ~ Length: 100000000; Legacy time: 10928 us, Current time: 37934 us; Legacy NPE: 0 ns; Current NPE: 0
&lt;/denchmark-code&gt;

And here's current time for cpu:
&lt;denchmark-code&gt;22:04:09,353 INFO  ~ Length: 100; Legacy time: 375 us, Current time: 10 us; Legacy NPE: 3754 ns; Current NPE: 100
22:04:09,353 INFO  ~ Length: 1000; Legacy time: 53 us, Current time: 53 us; Legacy NPE: 53 ns; Current NPE: 53
22:04:09,354 INFO  ~ Length: 10000; Legacy time: 303 us, Current time: 437 us; Legacy NPE: 30 ns; Current NPE: 43
22:04:09,370 INFO  ~ Length: 100000; Legacy time: 14918 us, Current time: 1243 us; Legacy NPE: 149 ns; Current NPE: 12
22:04:09,476 INFO  ~ Length: 1000000; Legacy time: 97865 us, Current time: 8173 us; Legacy NPE: 97 ns; Current NPE: 8
22:04:10,401 INFO  ~ Length: 10000000; Legacy time: 839071 us, Current time: 84868 us; Legacy NPE: 83 ns; Current NPE: 8
22:04:20,163 INFO  ~ Length: 100000000; Legacy time: 8719919 us, Current time: 1041980 us; Legacy NPE: 87 ns; Current NPE: 10
&lt;/denchmark-code&gt;

TL/DR: Current implementation for CPU is approx 10 times faster. And for CUDA it's faster for smaller inputs (&lt;10m elements) and slower for large inputs (&gt;10m elements). However, slowdown isn't that bad as for 0.7.0.
		</comment>
		<comment id='8' author='fac2003' date='2016-11-24T19:38:01Z'>
		Also, on next major release i'll provide stateful RNG, it should reduce time needed for op by few more microseconds. But it'll require some work on cpu side, so that'll happen a bit later.
		</comment>
		<comment id='9' author='fac2003' date='2016-11-25T12:49:47Z'>
		Fix is merged to master, works fine on linux and windows wrt TDR.
		</comment>
		<comment id='10' author='fac2003' date='2019-01-20T10:09:16Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>