<bug id='3961' author='thhart' open_date='2017-08-28T17:25:14Z' closed_time='2017-09-16T07:13:21Z'>
	<summary>Probable memory problem in 0.9.1</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

I have a memory problem in the current snapshot release 0.9.1. I am using a big data set of 32000 images to train a CNN. If I run with GPU backend in the second epoch the memory is exhausted. To confirm the problem I have downloaded a clean snapshot of deeplearning4j-examples and run org.deeplearning4j.examples.convolution.LenetMnistExample.
I am not right sure if the MNIST dataset and also my original set with a batch size of 8 is simply to big but what strives me is the fact that one epoch is running fine. Shouldn't there be a memory cleanup after the epoch?
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;


Deeplearning4j version: 0.9.1
Linux 64 Bit (Ubuntu 16.4)
CUDA version: 0.8.0
NVIDIA driver version: 375.83
GPU: GTX 1070 8 GB

&lt;denchmark-h:h4&gt;How to reproduce&lt;/denchmark-h&gt;


Download deeplearning4j-examples from github
Enable CUDA in pom.xml
Edit dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java and enable 16 epochs
mvn install
./runexamples.sh with example LenetMnist

&lt;denchmark-h:h4&gt;Contribution&lt;/denchmark-h&gt;

Please let me know anything I should report or add here.
&lt;denchmark-h:h4&gt;Complete output&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;
====== org.deeplearning4j.examples.convolution.LenetMnistExample

o.d.e.c.LenetMnistExample - Load data....
o.d.e.c.LenetMnistExample - Build model....
o.n.l.f.Nd4jBackend - Loaded [JCublasBackend] backend
o.n.n.NativeOpsHolder - Number of threads used for NativeOps: 32
o.n.l.a.o.e.DefaultOpExecutioner - Backend used: [CUDA]; OS: [Linux]
o.n.l.a.o.e.DefaultOpExecutioner - Cores: [8]; Memory: [3.5GB];
o.n.l.a.o.e.DefaultOpExecutioner - Blas vendor: [CUBLAS]
o.n.l.j.o.e.CudaExecutioner - Device name: [GeForce GTX 1070]; CC: [6.1]; Total/free memory: [8506769408]
o.d.n.m.MultiLayerNetwork - Starting MultiLayerNetwork with WorkspaceModes set to [training: NONE; inference: SEPARATE]
[...]
o.d.o.l.ScoreIterationListener - Score at iteration 1521 is 0.04100004196399555
o.d.o.l.ScoreIterationListener - Score at iteration 1522 is 0.04616239726726625
o.d.o.l.ScoreIterationListener - Score at iteration 1523 is 0.023948549212878367
o.d.o.l.ScoreIterationListener - Score at iteration 1524 is 0.03301683067697266
o.d.o.l.ScoreIterationListener - Score at iteration 1525 is 0.1154119633322653
o.d.o.l.ScoreIterationListener - Score at iteration 1526 is 0.01834671576793227
Exception in thread "main" java.lang.OutOfMemoryError: Cannot allocate new FloatPointer(1): totalBytes = 257, physicalBytes = 6G
	at org.bytedeco.javacpp.FloatPointer.&lt;init&gt;(FloatPointer.java:76)
	at org.bytedeco.javacpp.FloatPointer.&lt;init&gt;(FloatPointer.java:41)
	at org.nd4j.linalg.jcublas.blas.JcublasLevel3.sgemm(JcublasLevel3.java:107)
	at org.nd4j.linalg.api.blas.impl.BaseLevel3.gemm(BaseLevel3.java:57)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.mmuli(BaseNDArray.java:3011)
	at org.nd4j.linalg.api.ndarray.BaseNDArray.mmul(BaseNDArray.java:2812)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:357)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.preOutput(ConvolutionLayer.java:248)
	at org.deeplearning4j.nn.layers.convolution.ConvolutionLayer.activate(ConvolutionLayer.java:392)
	at org.deeplearning4j.nn.layers.AbstractLayer.activate(AbstractLayer.java:309)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.activationFromPrevLayer(MultiLayerNetwork.java:789)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForwardToLayer(MultiLayerNetwork.java:929)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.computeGradientAndScore(MultiLayerNetwork.java:2224)
	at org.deeplearning4j.optimize.solvers.BaseOptimizer.gradientAndScore(BaseOptimizer.java:174)
	at org.deeplearning4j.optimize.solvers.StochasticGradientDescent.optimize(StochasticGradientDescent.java:60)
	at org.deeplearning4j.optimize.Solver.optimize(Solver.java:53)
	at org.deeplearning4j.nn.multilayer.MultiLayerNetwork.fit(MultiLayerNetwork.java:1245)
	at org.deeplearning4j.examples.convolution.LenetMnistExample.main(LenetMnistExample.java:139)
Caused by: java.lang.OutOfMemoryError: Physical memory usage is too high: physicalBytes = 6G &gt; maxPhysicalBytes = 6G
	at org.bytedeco.javacpp.Pointer.deallocator(Pointer.java:576)
	at org.bytedeco.javacpp.Pointer.init(Pointer.java:121)
	at org.bytedeco.javacpp.FloatPointer.allocateArray(Native Method)
	at org.bytedeco.javacpp.FloatPointer.&lt;init&gt;(FloatPointer.java:68)
	... 17 more

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='thhart' date='2017-08-28T18:16:44Z'>
		Can you please try to run with trainingWorkspaceMode set to SINGLE or SEPARATE?
		</comment>
		<comment id='2' author='thhart' date='2017-08-28T21:29:31Z'>
		Same problem only a bit later.
		</comment>
		<comment id='3' author='thhart' date='2017-08-29T08:08:51Z'>
		Today I tested the same example and it fails also in the CPU back-end, so it is not related to GPU finally. Please let me know if you need a memory snapshot.
		</comment>
		<comment id='4' author='thhart' date='2017-08-29T09:55:03Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/thhart&gt;@thhart&lt;/denchmark-link&gt;
    hi, guys i also encouter same memory problem,   the cases i run is vgg16 model,  i have try a lot ,   it seems as long as the trained model is large enough ,  the memory problem will appear after some epochs.  i run vgg16 model in spark standalone cluster.
but i can confirm that the javacpp maxMemory size is enough  in my case i set it to 40G ,
i strongly feel that the javacpp has some memory leak when face large memory consume
		</comment>
		<comment id='5' author='thhart' date='2017-09-16T07:13:21Z'>
		I have checked again deeply and it is really related to only low memory available or not garbage collected enough. So it is not a leak for sure.
		</comment>
		<comment id='6' author='thhart' date='2018-09-24T18:26:22Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>