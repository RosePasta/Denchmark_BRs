<bug id='4534' author='emc5ud' open_date='2018-01-22T16:47:22Z' closed_time='2018-05-07T14:17:30Z'>
	<summary>Unet: Error importing model for training with Conv2D as last layer (0.9.2-SNAPSHOT)</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Hi so I have a model in keras that looks like this.
def get_unet():
    conv_params = dict(activation='relu', border_mode='same')
    merge_params = dict(mode='concat', concat_axis=1)
    inputs = Input((8, 256, 256))
    conv1 = Convolution2D(32, 3, 3, **conv_params)(inputs)
    conv1 = Convolution2D(32, 3, 3, **conv_params)(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Convolution2D(64, 3, 3, **conv_params)(pool1)
    conv2 = Convolution2D(64, 3, 3, **conv_params)(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = Convolution2D(128, 3, 3, **conv_params)(pool2)
    conv3 = Convolution2D(128, 3, 3, **conv_params)(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = Convolution2D(256, 3, 3, **conv_params)(pool3)
    conv4 = Convolution2D(256, 3, 3, **conv_params)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

    conv5 = Convolution2D(512, 3, 3, **conv_params)(pool4)
    conv5 = Convolution2D(512, 3, 3, **conv_params)(conv5)

    up6 = merge_l([UpSampling2D(size=(2, 2))(conv5), conv4], **merge_params)
    conv6 = Convolution2D(256, 3, 3, **conv_params)(up6)
    conv6 = Convolution2D(256, 3, 3, **conv_params)(conv6)

    up7 = merge_l([UpSampling2D(size=(2, 2))(conv6), conv3], **merge_params)
    conv7 = Convolution2D(128, 3, 3, **conv_params)(up7)
    conv7 = Convolution2D(128, 3, 3, **conv_params)(conv7)

    up8 = merge_l([UpSampling2D(size=(2, 2))(conv7), conv2], **merge_params)
    conv8 = Convolution2D(64, 3, 3, **conv_params)(up8)
    conv8 = Convolution2D(64, 3, 3, **conv_params)(conv8)

    up9 = merge_l([UpSampling2D(size=(2, 2))(conv8), conv1], **merge_params)
    conv9 = Convolution2D(32, 3, 3, **conv_params)(up9)
    conv9 = Convolution2D(32, 3, 3, **conv_params)(conv9)

    conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)

    optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)
    model = Model(input=inputs, output=conv10)
    model.compile(optimizer=optimizer,
                  loss='binary_crossentropy',
                  metrics=['accuracy', jaccard_coef, jaccard_coef_int])
    return model
And saved like this
model = get_unet()

#train model on lots of images....

model.save('unet.h5')
I would like to try loading this trained model into dl4j. I tried to do this with
  val unet = KerasModelImport.importKerasModelAndWeights("src/main/resources/unet.h5")
However I got the following error:
&lt;denchmark-code&gt;Exception in thread "main" java.lang.IllegalStateException: Invalid input type (layer index = -1, layer name="convolution2d_19_loss"): expected FeedForward input type. Got: InputTypeConvolutional(h=256,w=256,d=1)

&lt;/denchmark-code&gt;

Any ideas? This model works as expected in keras. I had to use the snapshot build so that the merge layers will work in dl4j. Not sure why it doesn't like that the first layer is convolutional.
keras==1.0.0
theano==0.9.0
inside keras.json
&lt;denchmark-code&gt;{
    "image_dim_ordering": "th",
    "epsilon": 1e-07,
    "floatx": "float32",
    "backend": "theano"
}
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='emc5ud' date='2018-01-22T18:51:48Z'>
		thanks for reporting. I'll have a look into this
		</comment>
		<comment id='2' author='emc5ud' date='2018-01-24T18:04:19Z'>
		So as I shot in the dark I changed my backend to tensorflow and used a more recent version of Keras.
My new evironment:
tensorflow-gpu==1.4
keras==1.2.2
I also played around with switching the dimension ordering to be 'tf' (channels last) instead of 'th' (channels first). No luck. I know in the past I have imported a Unet successfully into dl4j with my snaphot, so I'll keep experimenting until I figure out something that works.
		</comment>
		<comment id='3' author='emc5ud' date='2018-01-24T19:42:42Z'>
		I feel a little silly for not trying this first, but saving the model weights and architecture separately and importing them that way works.
val unet = KerasModelImport.importKerasModelAndWeights("src/main/resources/unet.json",
    "src/main/resources/unet_weights.h5")
I want to say I tried this originally when I was using theano as a backend, but maybe I didn't do it right. Anyways, there still remains an issue in dl4j if you save the model and weights into a single h5 file as I was doing earlier, so I won't close this yet.
		</comment>
		<comment id='4' author='emc5ud' date='2018-01-24T20:59:35Z'>
		&lt;denchmark-link:https://github.com/emc5ud&gt;@emc5ud&lt;/denchmark-link&gt;
 would you be so kind to provide some sort of self-contained example I can test with? i.e. either the  file or, better yet, a minimal keras example script that reproduces your error.
Just from the looks of it, the problem is not that it doesn't like the first layer, it's more that it doesn't like the last. Note that it tries to build a loss layer from the convnet with index 19. DL4J has the concept of output and loss layers that are somewhat special.
Two ideas:
a) test with
val unet = KerasModelImport.importKerasModelAndWeights("src/main/resources/unet.h5", enforceTrainingConfig = false) so that it doesn't try to load the training config (a start)
b) would you mind adding a Dense layer at the very end and try as you did previously? I suspect that would work with 0.9.2-SNAPSHOT.
If you actually do need training of the imported model in DL4J, I need to add a CnnLossLayer in the keras import code for you (for which I'd need a little time).
		</comment>
		<comment id='5' author='emc5ud' date='2018-01-25T09:20:31Z'>
		&lt;denchmark-link:https://github.com/emc5ud&gt;@emc5ud&lt;/denchmark-link&gt;
 the reason this works is that when you store weights and architecture separately, no training config is stored (or loaded). So this solution is equivalent to my suggestion a) above.
Can you confirm, please?
		</comment>
		<comment id='6' author='emc5ud' date='2018-01-25T11:48:02Z'>
		&lt;denchmark-link:https://github.com/emc5ud&gt;@emc5ud&lt;/denchmark-link&gt;
 at the very least would you mind telling me how you define  in your script?
		</comment>
		<comment id='7' author='emc5ud' date='2018-01-25T15:07:42Z'>
		Hi, sorry for not responding sooner.
1.) Your suggestion to set enforceTrainingConfig = false worked. For this problem I don't need to do any further training in dl4j but I'm willing to continue to troubleshoot to see if it is possible to in the future.
2.) Here is a sample python script to load and save the untrained model. &lt;denchmark-link:https://gist.github.com/emc5ud/3fd5d22a63bc65e918083a3eff3fdcad&gt;gist&lt;/denchmark-link&gt;

3.) I copied this model definition from someone else, and he  defined merge_l as  from keras.engine.topology import merge as merge_l.
As far as I know
&lt;denchmark-code&gt;from keras.engine.topology import merge_l
&lt;/denchmark-code&gt;

is equivalent to
&lt;denchmark-code&gt;from keras.layers import merge
&lt;/denchmark-code&gt;

Maybe at one point he was trying out both so he had to rename the one from  keras.engine.topology. Who knows. I think he was using an older version of Keras so maybe that is why he didn't use the merge inside keras.layers. I have tried using both flavors of merge and the same error pops up.
		</comment>
		<comment id='8' author='emc5ud' date='2018-01-25T16:56:59Z'>
		thanks! alright, I'm currently building end to end tests to cover your unet use case, see here: &lt;denchmark-link:https://github.com/deeplearning4j/dl4j-test-resources/pull/57&gt;deeplearning4j/dl4j-test-resources#57&lt;/denchmark-link&gt;

I'll keep you posted. the training case is definitely something we should support. (I leave this open and slightly change the title for clarity)
		</comment>
		<comment id='9' author='emc5ud' date='2018-09-22T04:24:40Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>