<bug id='4631' author='mboyanov' open_date='2018-02-07T15:50:58Z' closed_time='2018-02-12T08:00:25Z'>
	<summary>Unidirectional LSTM masking when importing model from keras</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue&lt;/denchmark-h&gt;

My goal is to add support for input masking in ParallelInference. I've setup a very basic model to test it - it's basically a calculator for three digits. The input is (batch_size,3) and the output is the sum of the three digits. For example [7 1 3] should equal 11. However, I have an embedding layer in the middle because I want to make sure that it will work for my NLP use cases later. The model is as follows:
&lt;denchmark-code&gt;def getModel():
    input = Input(shape=(xs.shape[1],))
    emb = Embedding(10, 1, mask_zero=True,embeddings_initializer=RandomUniform(0.0, .05))(input)
    rnn = LSTM(1, activation= "linear", use_bias=True, kernel_initializer=RandomUniform(0.0, .05), recurrent_initializer=RandomUniform(0.0, .05))(emb)
    out = Dense(1, activation="linear", use_bias=True, kernel_initializer=RandomUniform(0.0, 0.05))(rnn)
    return Model(input, out)
&lt;/denchmark-code&gt;

I can also upload it somewhere if it helps.
I'm importing the model and running two experiments: one without masks and one with.
The first one has input [1 1 1] and no masks and it finds the correct answer 3.
The second one has input [1 1 1] with mask [0 0 1], so expected answer is 1, but actual answer is 3.6. Upon debugging I found that the input mask array is null for the LSTM layer.
Note that if I override the mask just before the null check in LSTMHelpers it provides the correct answer.:
&lt;denchmark-code&gt;...
            INDArray mask = Nd4j.zeros(10,3);
            for (int i =0; i&lt;10 ; i++) {
                for (int j = 2; j&lt;3; j++ ) {
                    mask.putScalar(i, j, 1);
                }
            }
            maskArray=mask;
            if (maskArray != null) { //this one is null if not overriden
....
&lt;/denchmark-code&gt;

(The input is actually a batch of 10 identical examples Nd4j.ones(10,1, 3))
&lt;denchmark-h:h4&gt;Version Information&lt;/denchmark-h&gt;

Please indicate relevant versions, including, if relevant:

Deeplearning4j version 0.9.2-SNAPSHOT
platform information (OS, etc) Ubuntu 17.04

&lt;denchmark-h:h4&gt;Contributing&lt;/denchmark-h&gt;

I'm open to contributing, but I need further clarification about the masking mechanism for normal/unidirectional lstms in dl4j.
	</description>
	<comments>
		<comment id='1' author='mboyanov' date='2018-02-07T23:23:34Z'>
		Regarding masking - what specifically do you want/need to know?
Note that there's feedForward and output methods that support mask arrays, which should really be all you need here?
&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/fd981d74c33d55ced3584bb3ee3f7bf3ea06f099/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java#L1058&gt;https://github.com/deeplearning4j/deeplearning4j/blob/fd981d74c33d55ced3584bb3ee3f7bf3ea06f099/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java#L1058&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/fd981d74c33d55ced3584bb3ee3f7bf3ea06f099/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java#L2044&gt;https://github.com/deeplearning4j/deeplearning4j/blob/fd981d74c33d55ced3584bb3ee3f7bf3ea06f099/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java#L2044&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='mboyanov' date='2018-02-08T06:25:14Z'>
		I'm using one of the output methods, but it's not working as expected - it's not setting the mask for the lstm layer. It seems to be intentional because of this javadoc:
&lt;denchmark-code&gt;Pair&lt;INDArray, MaskState&gt; org.deeplearning4j.nn.api.Layer.feedForwardMaskArray(INDArray maskArray, MaskState currentMaskState, int minibatchSize)

Feed forward the input mask array, setting in in the layer as appropriate. This allows different layers to handle masks differently - for example, bidirectional RNNs and normal RNNs operate differently with masks (the former sets activations to 0 outside of the data present region (and keeps the mask active for future layers like dense layers), whereas normal RNNs don't zero out the activations/errors )instead relying on backpropagated error arrays to handle the variable length case.
This is also used for example for networks that contain global pooling layers, arbitrary preprocessors, etc.
&lt;/denchmark-code&gt;

What I don't understand is how masking works with  "relying on backpropagated error arrays to handle the variable length case". Especially, since I'm only using the model for inference, so backpropagation and errors don't make any sense in my context.
		</comment>
		<comment id='3' author='mboyanov' date='2018-02-08T23:48:39Z'>
		I was under the impression that this had been changed (even if the javadoc hadn't). Apparently not - the following simple test does not mask the activations on forward pass -
&lt;denchmark-code&gt;        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .list()
            .layer(0, new LSTM.Builder().nIn(3).nOut(3).build())
            .build();

        MultiLayerNetwork net = new MultiLayerNetwork(conf);
        net.init();

        INDArray in = Nd4j.linspace(1,9,9).reshape('f', 1, 3, 3);
        INDArray mask = Nd4j.create(new double[]{1,1,0});
        INDArray out = net.output(in, true, mask, null);
        System.out.println(out);
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;[[[0.24,  0.26,  0.31],  
  [0.01,  0.00,  0.00],  
  [0.20,  0.05,  0.01]]]
&lt;/denchmark-code&gt;

That should give a set of 0s for the masked step...
I'll mark this as a bug and get it fixed (probably) soon.
		</comment>
		<comment id='4' author='mboyanov' date='2018-02-09T08:37:29Z'>
		Looking at this again, and the motivation: the current implementation appears to be correct.
Motivation is described here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/recurrent/LSTM.java#L214-L219&gt;https://github.com/deeplearning4j/deeplearning4j/blob/c06924b5378a5fb52677d3ddc9be9140f09784ef/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/recurrent/LSTM.java#L214-L219&lt;/denchmark-link&gt;

Basically, if we always apply masks, we end up breaking some situations like sequence to sequence models...
Consider the input mask [1,0,0], no output mask, and structure in -&gt; (time distributed dense) -&gt; LSTM -&gt; RNN output (this is basically part of a seq2seq model)
The effective network structure (where x is masked out, o is present/not masked)
oxx     Input
oxx     Dense
ooo    LSTM
ooo    RnnOutput
If we masked at the LSTM forward pass, activations going into RnnOutput layer would be wrong.
But yours is a legitimate use case. What I think I'll do here: add a new layer that applies masks to activations on forward pass.
		</comment>
		<comment id='5' author='mboyanov' date='2018-02-09T10:54:44Z'>
		Thanks, I think I understand the problem now:
Sometimes the decoder needs to continue generating even if no input is supplied.
		</comment>
		<comment id='6' author='mboyanov' date='2018-02-12T08:00:25Z'>
		I have added the mask layer here: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/pull/4647&gt;https://github.com/deeplearning4j/deeplearning4j/pull/4647&lt;/denchmark-link&gt;

That should give you the ability to do what you want regarding the masked activations.
		</comment>
		<comment id='7' author='mboyanov' date='2018-02-12T08:42:14Z'>
		I haven't looked at the merge request too carefully, but I'm under the impression that the MaskLayer will apply the mask before/after a given layer.
However, in the case of a RNN cell, the internal memory will still be updated for the masked timesteps?
		</comment>
		<comment id='8' author='mboyanov' date='2018-02-13T02:08:26Z'>
		The mask layer will zero out the masked steps for both the activations and activation gradients passing through it. Will the RNN internal memory cell state be updated for the masked steps? Yes, and it should be. For situations like a mask of 111000, the backpropagated gradients from the last 3 steps will be 0 (from label mask, or mask layer if you want), and consequently the internal memory cell state doesn't matter (literally, we'll get the exact same gradients whether they are zeroed or not). Gradient checks verify that also.
		</comment>
		<comment id='9' author='mboyanov' date='2018-02-13T06:40:24Z'>
		You're thinking about the case of the decoder. Consider the encoder with mask 000111. In this case the internal memory state shouldn't be updated in the first 3 timesteps.
		</comment>
		<comment id='10' author='mboyanov' date='2018-09-23T11:26:17Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>