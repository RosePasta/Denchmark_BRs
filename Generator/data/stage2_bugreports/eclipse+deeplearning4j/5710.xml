<bug id='5710' author='TheKingDiamond' open_date='2018-06-26T10:52:43Z' closed_time='2018-07-31T12:05:10Z'>
	<summary>NullPointerException when serializing Word2Vec model</summary>
	<description>
(EDIT: I am using version 0.9.1 of DL4J, but have checked and this issue persists in 1.0.0-beta.)
If a Word2Vec model has useUnknown set to true and the vocabulary has been limited (to a size smaller than the number of unique words in the text corpus), then the writeWordVectors(WeightLookupTable lookupTable, OutputStream stream) method of WordVectorSerializer throws a NullPointerException when it attempts to execute the last iteration of the for loop (specifically at line 418).
The UNK string will not generally have an index value contiguous with the words that have been included in the vocabulary - and thus in the idxMap. The index values are used as the keys in the idxMap and this is where the issue occurs. The following example should help to illustrate the issue:
If we have a vocabulary of restricted to a size of 2 (from an original size much larger than this) and are using UNK, then we may have something like the following in the idxMap (simplified):
Element [0]: Key = 0, Value = "the"
Element [1]: Key = 1, Value = "an"
Element [2]: Key = 11356, Value = "UNK"
The call to vocabCache.elementAtIndex(x) will execute successfully for the first two words. However, the for loop is configured to run a final time to get the UNK value. The issue is that  now x = 2, but the key for UNK is actually 11356. Thus, a NullPointerException is thrown as no element exists in the idxMap with a key of '2'.
	</description>
	<comments>
		<comment id='1' author='TheKingDiamond' date='2018-06-27T01:00:37Z'>
		Can you show source code please, that reproduces this problem?
		</comment>
		<comment id='2' author='TheKingDiamond' date='2018-06-27T07:57:48Z'>
		&lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 Sure thing, the following should cause the error provided you use a sensible text file as the corpus (i.e. at least a few unique words, such that UNK will be used when the vocab is limited):
&lt;denchmark-code&gt;String modelOutputPath = **some path**;
String corpusPath = **some path containing a text file**;

SentenceIterator iter = new BasicLineIterator(corpusPath);
TokenizerFactory t = new DefaultTokenizerFactory();
t.setTokenPreProcessor(new CommonPreprocessor());
        
Word2Vec vec = new Word2Vec.Builder()
        .minWordFrequency(1)
        .epochs(1)
        .layerSize(300)
        .limitVocabularySize(1) // Limit the vocab size to 2 words
        .windowSize(5)
        .allowParallelTokenization(true)
        .batchSize(512)
        .learningRate(0.025)
        .minLearningRate(0.0001)
        .negativeSample(0.0)
        .sampling(0.0)
        .useAdaGrad(false)
        .useHierarchicSoftmax(true)
        .iterations(1)
        .useUnknown(true) // Using UNK with limited vocab size causes the issue
        .seed(42)
        .iterate(iter)
        .workers(4)
        .tokenizerFactory(t).build();
        
System.out.println("Starting to train the model...");
vec.fit();
System.out.println("Model trained!");
        
System.out.println("Writing model to file...");
WordVectorSerializer.writeWord2VecModel(vec, new File(modelOutputPath)); // NullPointerException thrown here
System.out.println("Model written to file successfully!");
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='TheKingDiamond' date='2018-07-31T12:05:10Z'>
		Fixed. THanks for highlighting this problem
		</comment>
		<comment id='4' author='TheKingDiamond' date='2018-09-12T10:13:03Z'>
		Hi &lt;denchmark-link:https://github.com/raver119&gt;@raver119&lt;/denchmark-link&gt;
 , I'm taking a look at the changes in 1.0.0-beta2.
It seems that the case is now that all the indices for the words in the object returned from iterator.nextSequence() (VocabConstructor, line 246) are -1. These indices are carried through the rest of the program, which leads to a totally new issue in that limiting the vocabulary size is no longer possible (as the removal of words works by removing elements with indices greater than the specified limit). Please let me know if I should create a new issue for this defect, or if we can use this one instead!
Many thanks!
		</comment>
		<comment id='5' author='TheKingDiamond' date='2018-09-12T19:55:18Z'>
		Yes please, file an issue. It was quite a while ago, but i think -1 means "element not found in vocabulary".
		</comment>
		<comment id='6' author='TheKingDiamond' date='2018-09-12T19:55:44Z'>
		Are you sure there's no delta between vocab and actual data fed into training pipeline?
		</comment>
		<comment id='7' author='TheKingDiamond' date='2018-10-12T20:33:16Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
		<comment id='8' author='TheKingDiamond' date='2018-11-17T15:06:13Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>