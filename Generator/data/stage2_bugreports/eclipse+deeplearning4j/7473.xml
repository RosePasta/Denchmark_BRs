<bug id='7473' author='AlexDBlack' open_date='2019-04-08T09:24:35Z' closed_time='2019-04-10T10:55:28Z'>
	<summary>Libnd4j: layer_norm (broadcasting?) fails with mixed orders</summary>
	<description>
layer_norm op provides different output when the result array is not the same order as the input array.
This appears to be the underlying cause of this issue: &lt;denchmark-link:https://github.com/deeplearning4j/deeplearning4j/issues/7443&gt;https://github.com/deeplearning4j/deeplearning4j/issues/7443&lt;/denchmark-link&gt;

Given layer norm is a simple op, it can only be standardize or broadcasting that is the problem here...
The C in, C out case is fine. As is the F in, F out case. But C in, F out fails, as does F in, C out.
&lt;denchmark-code&gt;    @Test
    public void testLayerNormMixedOrders(){
        Nd4j.getRandom().setSeed(12345);
        INDArray input = Nd4j.rand(DataType.DOUBLE, 3, 8).dup('f');
        INDArray gain = Nd4j.rand(DataType.DOUBLE, 1, 8).dup('f');
        INDArray bias = Nd4j.rand(DataType.DOUBLE, 1, 8).dup('f');

        INDArray outFF = Nd4j.create(DataType.DOUBLE, new long[]{3,8}, 'f');
        INDArray outCC = Nd4j.create(DataType.DOUBLE, new long[]{3,8}, 'c');
        INDArray outFC = Nd4j.create(DataType.DOUBLE, new long[]{3,8}, 'c');
        INDArray outCF = Nd4j.create(DataType.DOUBLE, new long[]{3,8}, 'f');

        //F in, F out case
        Nd4j.exec(DynamicCustomOp.builder("layer_norm")
                .addInputs(input, gain, bias)
                .addOutputs(outFF)
                .addIntegerArguments(1) //Axis
                .build());

        //C in, C out case
        Nd4j.exec(DynamicCustomOp.builder("layer_norm")
                .addInputs(input.dup('c'), gain.dup('c'), bias.dup('c'))
                .addOutputs(outCC)
                .addIntegerArguments(1) //Axis
                .build());

        assertEquals(outFF, outCC);       //OK

        //C in, F out case
        outFF.assign(0);
        Nd4j.exec(DynamicCustomOp.builder("layer_norm")
                .addInputs(input.dup('c'), gain.dup('c'), bias.dup('c'))
                .addOutputs(outCF)
                .addIntegerArguments(1) //Axis
                .build());
        assertEquals(outCC, outCF);       //Fails here

        //F in, C out case
        outFF.assign(0);
        Nd4j.exec(DynamicCustomOp.builder("layer_norm")
                .addInputs(input, gain, bias)
                .addOutputs(outFC)
                .addIntegerArguments(1) //Axis
                .build());
        assertEquals(outCC, outFC);       //Fails here
    }
&lt;/denchmark-code&gt;

First failure (CF case):
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/55713256-a172d480-5a33-11e9-84bd-9340be244c22.png&gt;&lt;/denchmark-link&gt;

Second failure (FC case):
&lt;denchmark-link:https://user-images.githubusercontent.com/2360237/55713287-b5b6d180-5a33-11e9-9d1a-15fce5a3ded6.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2019-04-08T11:07:48Z'>
		I've narrowed it down to applyTrueBroadcast with a given output target.
The following test (in cpp) fails:
&lt;denchmark-code&gt;TEST_F(DeclarableOpsTests15, Test_apply_true_scalar_1) {
    auto x = NDArrayFactory::create&lt;double&gt;('f', {2, 3}, {1., 2., 3., 4., 5., 6.});
    auto y = NDArrayFactory::create&lt;double&gt;('f', {2, 3}, {5., 4., 3., 2., 1., 0.});
    auto z = NDArrayFactory::create&lt;double&gt;('c', {2, 3});

    auto exp = x - y;

    x.applyTrueBroadcast(nd4j::BroadcastOpsTuple::Subtract(), &amp;y, &amp;z, true);

    ASSERT_EQ(exp, z);
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='AlexDBlack' date='2019-04-09T16:11:00Z'>
		Hi, Paul
Thanks for problem narrowing.
Initially it has been assumed that broadcast op should not be applied to arrays with same shape.
However since such situations occur in your tests we simply provided redirection in broadcast op to pairwise op in corresponding cases. Changes are already in master.
		</comment>
		<comment id='3' author='AlexDBlack' date='2019-04-09T18:03:16Z'>
		Hi &lt;denchmark-link:https://github.com/shyrma&gt;@shyrma&lt;/denchmark-link&gt;

It looks like I overcooked the test while trying to simplify it.
TEST_F(DeclarableOpsTests15, Test_apply_broadcast_2) {
    auto x = NDArrayFactory::create&lt;double&gt;('f', {2, 3}, {1., 2., 3., 4., 5., 6.});
    auto y = NDArrayFactory::create&lt;double&gt;('f', {1, 3}, {5., 4., 3.});
    auto z = NDArrayFactory::create&lt;double&gt;('c', {2, 3});

    auto exp = x - y;
    x.applyTrueBroadcast(nd4j::BroadcastOpsTuple::Subtract(), &amp;y, &amp;z, true);

    ASSERT_EQ(exp, z);
}
This test uses shapes which do require broadcasting, and it does still fail.
		</comment>
		<comment id='4' author='AlexDBlack' date='2019-04-10T10:15:30Z'>
		Hi, Paul
Yes, that was bug in NDArray::applyBroadcast - we gave same input tad-shapeInfo both for input and output arrays, ant it was wrong, now 2 shapeInfos are given: input tad-shapeInfo for input array and output tad-shapeInfo for output array.
Thanks for catching this.
Corresponding pr is already merged.
		</comment>
		<comment id='5' author='AlexDBlack' date='2019-04-10T10:55:24Z'>
		Original layer norm issue is now confirmed passing, thanks &lt;denchmark-link:https://github.com/shyrma&gt;@shyrma&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/treo&gt;@treo&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='AlexDBlack' date='2019-05-10T12:07:31Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>