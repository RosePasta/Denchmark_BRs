<bug id='4915' author='ghost(ghost)' open_date='2018-04-10T02:33:54Z' closed_time='2018-04-17T14:58:00Z'>
	<summary>How to have an input_length for Embedding layers?</summary>
	<description>
&lt;denchmark-h:h4&gt;Issue Description&lt;/denchmark-h&gt;

Hello, I am studying DL4J embedding layers and I have a question about the length of input sequences.  In Keras (sorry for the cross-referencing), we can do this:
&lt;denchmark-code&gt;inputs = Input(shape=(32, ))
embedding = Embedding(max_features, 128, input_length=32)(inputs)
&lt;/denchmark-code&gt;

The output will be a 2D matrix with shape (32, 128).  How do we do this in DL4J?
	</description>
	<comments>
		<comment id='1' author='ghost(ghost)' date='2018-04-11T08:29:26Z'>
		@mobiusinversion we don't support this (yet). I'll have a look at Embedding layers for import soon.
		</comment>
		<comment id='2' author='ghost(ghost)' date='2018-04-12T00:18:08Z'>
		I should add that if you are using time series inputs (not sure if that's what you want here), you can do .setInputType(InputType.recurrent(x)) and it'll work in a time distributed manner.
		</comment>
		<comment id='3' author='ghost(ghost)' date='2018-04-12T09:48:00Z'>
		&lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
  @mobiusinversion exactly, so in principle we can already do this, but I have to tell our import module to translate Keras'  argument into a recurrent  with time series length equal to .
		</comment>
		<comment id='4' author='ghost(ghost)' date='2018-04-16T22:56:22Z'>
		Thanks much for having a lock &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/maxpumperla&gt;@maxpumperla&lt;/denchmark-link&gt;
.  Regarding recurrent nets, and time series.  My understanding is that recurrent implies hidden / shared state.  My understanding of &lt;denchmark-link:https://github.com/keras-team/keras/issues/3110&gt;what is happening in Keras embeddings&lt;/denchmark-link&gt;
 is that these embeddings are IID, with no shared state between them.  In this way, an embedding of 5 integers with a max value of 40, into 2 dimensions, becomes a 5 x 40 one hot matrix which then is embedded into a 5 x 2 matrix, which can then be used by a CNN to do classification (without state between classifications, just a typical feedforward design).   The embeddings can be trained as part of training the model, and &lt;denchmark-link:https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html&gt;then even reused elsewhere as a kind of transfer learning&lt;/denchmark-link&gt;
.  Does this sound right to you?  And your thoughts?
		</comment>
		<comment id='5' author='ghost(ghost)' date='2018-04-17T14:58:00Z'>
		@mobiusinversion this is fixes now with &lt;denchmark-link:https://github.com/eclipse/deeplearning4j/issues/4899&gt;#4899&lt;/denchmark-link&gt;
.
If you have input_length = x and minibatch size = mb then keras will embed an input of shape (mb, x) into (mb, output_dim, x) where the input entries are labels from a vocabulary of size input_dim.
		</comment>
		<comment id='6' author='ghost(ghost)' date='2018-04-18T00:56:49Z'>
		Thanks &lt;denchmark-link:https://github.com/maxpumperla&gt;@maxpumperla&lt;/denchmark-link&gt;
 &amp; &lt;denchmark-link:https://github.com/AlexDBlack&gt;@AlexDBlack&lt;/denchmark-link&gt;
!
		</comment>
		<comment id='7' author='ghost(ghost)' date='2018-04-18T00:58:37Z'>
		&lt;denchmark-link:https://github.com/maxpumperla&gt;@maxpumperla&lt;/denchmark-link&gt;
, however, I was wondering, can we do this directly in DL4J without Keras import?  That's what I was mainly interested in, plus supporting Keras 2.0 is fantastic.
		</comment>
		<comment id='8' author='ghost(ghost)' date='2018-09-22T22:28:01Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>