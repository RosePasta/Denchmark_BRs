<bug id='5944' author='AlexDBlack' open_date='2018-07-21T13:11:33Z' closed_time='2018-07-25T08:43:49Z'>
	<summary>DL4J Spark: Possible memory leak on each fit operation?</summary>
	<description>
Unconfirmed. In recent experiments, I could fit once for as many minibatches as I wanted.
However, once I started doing multiple sequential fit ops, I was running into a NPE.
Need to look into this further.
	</description>
	<comments>
		<comment id='1' author='AlexDBlack' date='2018-07-23T12:17:37Z'>
		Update: Not reprodicible locally with Resnet50 and aeron buffer set in range of default to 128MB.
May depend on number of workers, or require that workers are remote (not local); this was seen with 22 workers on cluster under some circumstances.
		</comment>
		<comment id='2' author='AlexDBlack' date='2018-07-25T08:43:49Z'>
		I've been running tests for a couple of days, on 4-22 workers, on the same network configuration that I've used previously. I've been unable to reproduce this - everything has run flawlessly.
So either it was an intermittent isue, or it has been fixed in some of the more recent PRs for Spark training.
As I can't reproduce this, I'm closing this issue.
		</comment>
		<comment id='3' author='AlexDBlack' date='2018-09-21T13:59:12Z'>
		This thread has been automatically locked since there has not been any recent activity after it was closed. Please open a new issue for related bugs.
		</comment>
	</comments>
</bug>