<bug id='43' author='timbrucks' open_date='2018-02-16T14:05:34Z' closed_time='2018-02-26T12:22:39Z'>
	<summary>Issue training with DeepVoice3 model with LJSpeech Data</summary>
	<description>
Thanks for your excellent implementation of Deep Voice 3. I am attempting to retrain a DeepVoice3 model using the LJSpeech data. My interest in training a new model is that I want to make some small model parameter changes in order to enable fine-tuning using some Spanish data that I have.
As a first step I tried to retrain the baseline model and I have run into some issues.
With my installation, I have been able to successfully synthesize using the pre-trained DeepVoice3 model with git commit &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/43579764f35de6b8bac2b18b52a06e4e11b705b2&gt;4357976&lt;/denchmark-link&gt;
 as your instructions indicate. That synthesized audio sounds very much like the samples linked from the instructions page.
However, I am trying to train now with the  (commit &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/48d1014799add9c296235f89714168e868c2468d&gt;48d1014&lt;/denchmark-link&gt;
, dated Feb 7). I am using the LJSpeech data set downloaded from the link you provided. I have run the pre-processing and training steps as indicated in your instructions. I am using the default preset parameters for deepvoice3_ljspeech.
I have let the training process run for a while. When I synthesize using the checkpoint saved at 210K iterations, the alignment is bad and the audio is very robotic and mostly unintelligible.
&lt;denchmark-link:https://user-images.githubusercontent.com/17724266/36311054-9e087ea8-12ef-11e8-8f1d-dbbb91e3de90.png&gt;&lt;/denchmark-link&gt;

When I synthesize using the checkpoint saved at 700K iterations, the alignment is better (but not great); the audio is improved but still robotic and choppy.
&lt;denchmark-link:https://user-images.githubusercontent.com/17724266/36311069-ac429698-12ef-11e8-868f-05e1d016c419.png&gt;&lt;/denchmark-link&gt;

I can post the synthesized wav files via dropbox if you are interested. I expected to have good alignment and audio at 210K iterations as that is what the pretrained model used.
Any ideas what has changed between git commits &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/43579764f35de6b8bac2b18b52a06e4e11b705b2&gt;4357976&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/48d1014799add9c296235f89714168e868c2468d&gt;48d1014&lt;/denchmark-link&gt;
 that could have caused this issue? When I diff the two commits, I see some changes in audio.py, some places where support for multi-voice has been added, and some other changes I do not yet understand. There are some additions to hparams.py, but I only noticed one difference: in the current commit, masked_loss_weight defaults to 0.5, but in the prior commit the default was 0.0.
I have just started a new training run with masked_loss_weight set to 0.0. In the meantime, do you have thoughts on anything else that might be causing the issues I am seeing?
	</description>
	<comments>
		<comment id='1' author='timbrucks' date='2018-02-16T15:28:49Z'>
		Thank you for the detailed report. If everything goes good, you should get reasonable results with 200k steps, so there must be something wrong in your case. I would like to investigate the problem but I am currently in a business trip and won’t have time for a week. I believe I didn’t change anything performance critical but I might miss something important... masked_loss_weight was not a sensible parameter from my experience.
		</comment>
		<comment id='2' author='timbrucks' date='2018-02-19T07:55:07Z'>
		&lt;denchmark-link:https://github.com/timbrucks&gt;@timbrucks&lt;/denchmark-link&gt;
 : Same problem here. Maybe it is related to &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/issues/38&gt;#38&lt;/denchmark-link&gt;
 ?
		</comment>
		<comment id='3' author='timbrucks' date='2018-02-19T10:13:09Z'>
		Could anyone try to revert &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/421e8b72b975ea285283e852a47ae87a329db51f&gt;421e8b7&lt;/denchmark-link&gt;
 and see if it works?
		</comment>
		<comment id='4' author='timbrucks' date='2018-02-19T12:45:52Z'>
		I have reverted to &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/421e8b72b975ea285283e852a47ae87a329db51f&gt;421e8b7&lt;/denchmark-link&gt;
 and started a training run. I will let you know what happens.
		</comment>
		<comment id='5' author='timbrucks' date='2018-02-19T15:34:27Z'>
		Hi,
This not relevant to the problem you have. I tried to do a synthesizing but it failed. I used the same instructions as in &lt;denchmark-link:url&gt;https://github.com/r9y9/deepvoice3_pytorch/tree/43579764f35de6b8bac2b18b52a06e4e11b705b2&lt;/denchmark-link&gt;

But I get a error.
&lt;denchmark-link:https://user-images.githubusercontent.com/8535277/36385385-70e02720-15b8-11e8-90c0-50cd36517bb8.png&gt;&lt;/denchmark-link&gt;

Please help.
		</comment>
		<comment id='6' author='timbrucks' date='2018-02-19T15:42:56Z'>
		Looks like dup of &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/issues/37&gt;#37&lt;/denchmark-link&gt;
. Also please do not comment on the unrelated issue. Create a new one if necessary.
		</comment>
		<comment id='7' author='timbrucks' date='2018-02-19T15:45:22Z'>
		Ops, I`m new to git hub. That issue is the same as mine. Thanks for the support :)
		</comment>
		<comment id='8' author='timbrucks' date='2018-02-20T08:34:14Z'>
		&lt;denchmark-link:https://github.com/r9y9&gt;@r9y9&lt;/denchmark-link&gt;
 : Also reverted to &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/421e8b72b975ea285283e852a47ae87a329db51f&gt;421e8b7&lt;/denchmark-link&gt;
 and training... will let you know asap...
No changes, eval still doesn't deliver any results (see alignment).
Training Alignment (Ave):
&lt;denchmark-link:https://user-images.githubusercontent.com/10852644/36431569-40089ce0-1658-11e8-83a1-47b53d1c09a1.png&gt;&lt;/denchmark-link&gt;

Eval Alignment at same (60000 steps):
&lt;denchmark-link:https://user-images.githubusercontent.com/10852644/36431600-4a34713a-1658-11e8-9cb3-915d0412cda5.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='timbrucks' date='2018-02-20T16:49:43Z'>
		I allowed my training process for &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/421e8b72b975ea285283e852a47ae87a329db51f&gt;421e8b7&lt;/denchmark-link&gt;
 to run up to 400k iterations. The sample WAV files produced in the checkpoints folder are pretty good (a little more reverb / metallic sound than expected) and the alignment is pretty good as well. Here is the alignment (from the "alignment_ave" dir) at 300k iterations for the utterance "that an anti-Castro organization had maintained offices there for a period ending early in 1962"
&lt;denchmark-link:https://user-images.githubusercontent.com/17724266/36436024-cee26278-1628-11e8-98f8-ca85cb4b2238.png&gt;&lt;/denchmark-link&gt;

Here is a zip of the predicted WAV file at 300k iterations:
&lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/files/1740995/step000300000_predicted.wav.zip&gt;step000300000_predicted.wav.zip&lt;/denchmark-link&gt;

However, if I use the stored checkpoint to synthesize that same utterance, the alignment looks like this and the WAV file produced is unintelligible.
&lt;denchmark-link:https://user-images.githubusercontent.com/17724266/36436521-199f4ce4-162a-11e8-90f8-c48d110eb1c2.png&gt;&lt;/denchmark-link&gt;

So at this point I am wondering if the issue is something in the synthesis process ...
		</comment>
		<comment id='10' author='timbrucks' date='2018-02-21T08:30:03Z'>
		Hi, &lt;denchmark-link:https://github.com/homink&gt;@homink&lt;/denchmark-link&gt;
, I am wondering if you hit the issue while working on &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/pull/44&gt;#44&lt;/denchmark-link&gt;
. Could you give us some insight?
		</comment>
		<comment id='11' author='timbrucks' date='2018-02-21T13:16:09Z'>
		As an additional experiment, I reset my git commit to &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/43579764f35de6b8bac2b18b52a06e4e11b705b2&gt;4357976&lt;/denchmark-link&gt;
 and attempted to train the LJSpeech model. I stopped the training at ~370K iterations. I see similar behavior as mentioned above: the predicted WAV files sounds good, but the synthesized result is much lower quality.
This is the command I used, which based on the looking at hparams.py for that specific commit seems like the correct way to get the preset parameters for LJSpeech.
python train.py --hparams "use_preset=True,builder=deepvoice3" --data-root &lt;path to the ljspeech feats produced by preprocess.py&gt; --checkpoint-dir &lt;path checkpoint dir&gt; --log-event-path &lt;path log dir&gt;
I again suspected something was amiss with the synthesis process. But with the same environment / setup, when I synthesize using the pre-trained LJSpeech model, the output sounds great - just like the samples provided.
At this point, all I can think of is that somehow I hosed up the pre-processing of the LJSpeech data ... maybe I ran pre-processing when I had the git pointed at a later commit? So I reran the preprocess step and have just started another training run using commit &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/43579764f35de6b8bac2b18b52a06e4e11b705b2&gt;4357976&lt;/denchmark-link&gt;
.
Any other ideas?
		</comment>
		<comment id='12' author='timbrucks' date='2018-02-21T15:22:59Z'>
		Hi All,
I believe the quality drop is really tough to figure out. I guess 2 things - random seed and silence of audio - would cause such degradation. For random seed, If I checked correctly, I was not able to find random seed initialization, which will be differently set up and will yield different training outputs. Silence of audio in the beginning and in the end also could affect model training. Here is what I had experienced so far.
I only tried JSUT and NIKL using two commits: &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/tree/aeed2258a9b1111669a793ae161cfe0a7396fdf0&gt;aeed2&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/tree/ed38dd204f8fea4ae7df5f5e4a8c64429321c0b0&gt;ed38d&lt;/denchmark-link&gt;
. I only checked the prediction wav files which were produced on training. These wav files sounded gradually better as iteration went by. For JSUT, most of these wav files made good sound with high iteration around 370K. But, for NIKL, a few wav files made good sound with high iteration around 400K. I thought this would be caused by some parameter tuning and I didn't investigate further. Also, untrimmed audio in the beginning and in the end, especially in NIKL, was really bad even with the high iteration. Thus I trimmed and left margin about 100 msec in the beginning and in the end. Details are &lt;denchmark-link:https://github.com/homink/speech.ko&gt;here&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='13' author='timbrucks' date='2018-02-23T14:33:53Z'>
		I was able to successfully train a model using &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/421e8b72b975ea285283e852a47ae87a329db51f&gt;421e8b7&lt;/denchmark-link&gt;
. My first attempt (described above) did not work, but after rerunning the pre-processing the results are much better, very similar to the sample results. Somehow I must have hosed up the pre-processing step (not sure how?).
I will now try to train using the recent commit &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/48d1014799add9c296235f89714168e868c2468d&gt;48d1014&lt;/denchmark-link&gt;
 (dated Feb 7).
		</comment>
		<comment id='14' author='timbrucks' date='2018-02-24T07:35:29Z'>
		Finaly I'm back. I will do some experiments locally soon. At the moment I am guessing &lt;denchmark-link:https://github.com/r9y9/wavenet_vocoder/issues/22&gt;r9y9/wavenet_vocoder#22&lt;/denchmark-link&gt;
 is the bug I have to fix, which was introduced at &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/421e8b72b975ea285283e852a47ae87a329db51f&gt;421e8b7&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='15' author='timbrucks' date='2018-02-24T13:38:47Z'>
		The results of training with with &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/48d1014799add9c296235f89714168e868c2468d&gt;48d1014&lt;/denchmark-link&gt;
 (now that I have good pre-processed data) were better. One of the synthesized utterances is bad, but several were good. I will give this latest commit a try.
		</comment>
		<comment id='16' author='timbrucks' date='2018-02-24T15:02:32Z'>
		OK, now I can reproduce. Looking into it...
		</comment>
		<comment id='17' author='timbrucks' date='2018-02-24T20:14:47Z'>
		Can confirm the bug with &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/2987b7640dc4601664e9366c7a7b5ac700543356&gt;2987b76&lt;/denchmark-link&gt;
 — sound quality after 210k iterations on LJSpeech is far from examples at &lt;denchmark-link:https://r9y9.github.io/deepvoice3_pytorch/&gt;r9y9.github.io/deepvoice3_pytorch&lt;/denchmark-link&gt;

Will &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/commit/18bd61d09059cc63f9906b125b5352acb6001038&gt;18bd61d&lt;/denchmark-link&gt;
 fix it? Were VCTK and Nyanko models affected too?
		</comment>
		<comment id='18' author='timbrucks' date='2018-02-25T00:13:50Z'>
		No, the bug persists. I'm looking into it.
		</comment>
		<comment id='19' author='timbrucks' date='2018-02-25T08:02:27Z'>
		A little progress:
diff --git a/audio.py b/audio.py
index 0decdbc..53fa56c 100644
--- a/audio.py
+++ b/audio.py
@@ -45,7 +45,7 @@ def inv_spectrogram(spectrogram):
 
 def melspectrogram(y):
     D = _lws_processor().stft(preemphasis(y)).T
-    S = _amp_to_db(_linear_to_mel(np.abs(D))) - hparams.ref_level_db
+    S = _amp_to_db(_linear_to_mel(np.abs(D)))  # - hparams.ref_level_db
     if not hparams.allow_clipping_in_normalization:
         assert S.max() &lt;= 0 and S.min() - hparams.min_level_db &gt;= 0
     return _normalize(S)
@@ -69,18 +69,15 @@ def _linear_to_mel(spectrogram):
 
 
 def _build_mel_basis():
-    assert hparams.fmax &lt;= hparams.sample_rate // 2
-    return librosa.filters.mel(hparams.sample_rate, hparams.fft_size,
-                               fmin=hparams.fmin, fmax=hparams.fmax,
-                               n_mels=hparams.num_mels)
+    return librosa.filters.mel(hparams.sample_rate, hparams.fft_size, n_mels=hparams.num_mels)
 
 
 def _amp_to_db(x):
-    return 20 * np.log10(x + 0.01)
+    return 20 * np.log10(np.maximum(1e-5, x))
 
 
 def _db_to_amp(x):
-    return np.maximum(np.power(10.0, x * 0.05) - 0.01, 0.0)
+    return np.power(10.0, x * 0.05)
with this I can get reasonable quality after 100k steps. Will look into it further.
		</comment>
		<comment id='20' author='timbrucks' date='2018-02-25T11:16:19Z'>
		I think &lt;denchmark-link:https://github.com/r9y9/deepvoice3_pytorch/pull/46&gt;#46&lt;/denchmark-link&gt;
 should fix this. I'm training a model from scratch to confirm if it actually fixes the problem.
		</comment>
		<comment id='21' author='timbrucks' date='2018-02-25T17:01:42Z'>
		&lt;denchmark-link:https://github.com/r9y9&gt;@r9y9&lt;/denchmark-link&gt;
 With Tacotron 2 I've been able to get reasonable quality with 20k iterations using the log of the clipped magnitudes instead of the representation with the data normalized to [0, 1].
		</comment>
		<comment id='22' author='timbrucks' date='2018-02-26T12:24:42Z'>
		Sorry for the bug. I think I fixed the problem. Feel free to reopen if you still see the bug.
		</comment>
		<comment id='23' author='timbrucks' date='2018-02-26T15:46:36Z'>
		Based on my tests, I think you fixed the problem. Thanks!
		</comment>
	</comments>
</bug>