<bug id='1085' author='zburning' open_date='2019-12-28T07:49:23Z' closed_time='2020-01-16T22:50:57Z'>
	<summary>Different tensor contexts in nlp.optimizer.lamb</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

In the update() function in lamb.py:
&lt;denchmark-code&gt;# preprocess grad
   grad *= self.rescale_grad
   if self.clip_gradient is not None:
       grad = clip(grad, -self.clip_gradient, self.clip_gradient)
&lt;/denchmark-code&gt;

The rescale_grad should be as_in_context(grad.context)? Otherwise it will raise illegal memory access in multi-gpu training
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

(Paste the complete error message, including stack trace.)
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

(If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

(Paste the commands you ran that produced the error.)




&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;





&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
&lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python

# paste outputs here
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='zburning' date='2019-12-29T19:05:51Z'>
		Since mxnet has lamb optimizer available in 1.6, we shall just reuse that one and remove the one in gluonnlp.
		</comment>
		<comment id='2' author='zburning' date='2019-12-30T09:52:45Z'>
		Related: &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/issues/1019&gt;#1019&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='zburning' date='2020-01-16T22:51:29Z'>
		[API] Drop LAMB optimizer from GluonNLP in favor of MXNet version &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/1116&gt;#1116&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>