<bug id='1015' author='carter54' open_date='2019-11-20T02:31:59Z' closed_time='2019-11-20T08:20:28Z'>
	<summary>prev_len in gpt.py</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

the gpt code in



gluon-nlp/scripts/text_generation/model/gpt.py


         Line 258
      in
      5e11334






 prev_len = states[0].shape[1] 





seems to have a mistake.
I might be wrong, but I think the code prev_len = states[0].shape[1] means to return the length of previous key/value matrix (or the previous input tokens) . However it returns the number of multi-head (12 for the 124M gpt2 model).
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

NA
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

run scripts/text_generation/sequence_sampling.py script and print the output of prev_len = states[0].shape[1] at line 258 in scripts/text_generation/model/gpt.py.
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;


change to prev_len = states[0].shape[2]

	</description>
	<comments>
		<comment id='1' author='carter54' date='2019-11-20T04:20:07Z'>
		Then the positional embedding is calculated for the wrong positions



gluon-nlp/scripts/text_generation/model/gpt.py


        Lines 262 to 265
      in
      5e11334






 data_pos = mx.nd.arange(prev_len, prev_len + seq_len, ctx=data.context, dtype=np.float32) 



 data_pos = mx.nd.broadcast_axes(mx.nd.expand_dims(data_pos, axis=0), 



 axis=0, size=batch_size) 



 out = self._embed(data) + self._pos_embed(data_pos) 





		</comment>
		<comment id='2' author='carter54' date='2019-11-20T04:33:44Z'>
		Added a fix to &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/1010&gt;#1010&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 please help to review.
		</comment>
		<comment id='3' author='carter54' date='2019-11-20T04:47:40Z'>
		Yes, it's a bug and it should be states[0].shape[2]
		</comment>
		<comment id='4' author='carter54' date='2019-11-20T04:47:59Z'>
		&lt;denchmark-link:https://github.com/carter54&gt;@carter54&lt;/denchmark-link&gt;
 Really appreciate for pointing out this!
		</comment>
		<comment id='5' author='carter54' date='2019-11-20T07:42:32Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 happy to help! May I ask a question not related to this bug. Is there any method to accelerate the inference speed of gpt2 model? Can HybridBlock help? or quantization possibly?
		</comment>
		<comment id='6' author='carter54' date='2019-11-20T07:45:08Z'>
		Hybridization will definitely help. Quantization also helps but Iâ€™m not sure how to do that in MXNet. Another solution is to dump the Json file and use TVM for inference.
		</comment>
		<comment id='7' author='carter54' date='2019-11-20T08:17:41Z'>
		&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 Thanks for your reply~ I read the TVM tutorial, most of the examples are within image processing area. Did you try TVM on gpt2 model before? Is there any examples and what range of the acceleration ratio can be expected? Thank you~
		</comment>
	</comments>
</bug>