<bug id='1233' author='DushyantaDhyani' open_date='2020-05-18T19:24:42Z' closed_time='2020-07-30T08:01:31Z'>
	<summary>TypeError: can't pickle SwigPyObject objects when pickling SentencepieceTokenizer</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

As stated &lt;denchmark-link:https://github.com/google/sentencepiece/issues/387&gt;here&lt;/denchmark-link&gt;
 , pickling sentencepiece tokenizer returns an error.
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

Traceback (most recent call last): File "tokenizer_repro.py", line 10, in &lt;module&gt; pickle.dump(tokenizer, writer) TypeError: can't pickle SwigPyObject objects
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;from gluonnlp.data import SentencepieceTokenizer
from mxnet import gluon
import pickle
url = "http://repo.mxnet.io/gluon/dataset/vocab/test-0690baed.bpe"
f = gluon.utils.download(url, overwrite=True)
tokenizer = SentencepieceTokenizer(f)
with open("sptokenizer.pickle", "wb") as writer:
    pickle.dump(tokenizer, writer)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;

As mentioned in the issue above, changing _SentencepieceProcessor as below fixes the issue
&lt;denchmark-code&gt;class _SentencepieceProcessor:
    def __init__(self, path):
        try:
            import sentencepiece
        except ImportError:
            raise ImportError(
                'sentencepiece is not installed. You must install sentencepiece '
                'in order to use the Sentencepiece tokenizer and detokenizer. '
                'You can refer to the official installation guide '
                'in https://github.com/google/sentencepiece#installation')
        self._vocab_file = path
        self._processor = sentencepiece.SentencePieceProcessor()
        self._processor.Load(path)

    def __len__(self):
        return len(self._processor)

    @property
    def tokens(self):
        return [self._processor.id_to_piece(i) for i in range(len(self))]

    def __getstate__(self):
        state = self.__dict__.copy()
        state["_processor"] = None
        state["_vocab_file"] = None
        return state, self._vocab_file

    def __setstate__(self, d):
        self.__dict__, self.vocab_file = d
        self._processor = sentencepiece.SentencePieceProcessor()
        self._processor.Load(self.vocab_file)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='DushyantaDhyani' date='2020-07-30T08:01:22Z'>
		&lt;denchmark-link:https://github.com/DushyantaDhyani&gt;@DushyantaDhyani&lt;/denchmark-link&gt;
 This is solved in the numpy version. We added pickle test here: 

To try out the new version, use the following code:
pip install -U --pre "mxnet&gt;=2.0.0b20200716" -f https://dist.mxnet.io/python --user
pip install git+https://github.com/dmlc/gluon-nlp.git@numpy --user
import mxnet as mx
mx.npx.set_np()
from gluonnlp.models import get_backbone
model_cls, cfg, tokenizer, local_params_path, others = get_backbone('google_albert_base_v2')
print(tokenizer)
model = model_cls.from_cfg(cfg)
model.load_parameters(local_params_path)
Out:
&lt;denchmark-code&gt;SentencepieceTokenizer(
   model_path = /Users/xjshi/.mxnet/models/nlp/google_albert_base_v2/spm-65999e5d.model
   lowercase = True, nbest = 0, alpha = 0.0
   vocab = Vocab(size=30000, unk_token="&lt;unk&gt;", pad_token="&lt;pad&gt;", cls_token="[CLS]", sep_token="[SEP]", mask_token="[MASK]")
)

&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>