<bug id='905' author='liusy182' open_date='2019-08-28T06:47:59Z' closed_time='2019-09-11T20:52:51Z'>
	<summary>KeyError: '&amp;lt;pad&amp;gt;' if run /scripts/word_embeddings/evaluate_pretrained.py with flag `analog-max-vocab-size`</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

run /scripts/word_embeddings/evaluate_pretrained.py with flag analog-max-vocab-size throws exception.
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "evaluate_pretrained.py", line 216, in &lt;module&gt;
    vocab.set_embedding(token_embedding_)
  File "/Users/siyuanl/private/gluon-nlp/src/gluonnlp/vocab/vocab.py", line 412, in set_embedding
    new_idx_to_vec[1:, col_start:col_end] = embs[self._idx_to_token[1:]]
  File "/Users/siyuanl/private/gluon-nlp/src/gluonnlp/embedding/token_embedding.py", line 637, in __getitem__
    indices = [self._token_to_idx[token] for token in tokens]
  File "/Users/siyuanl/private/gluon-nlp/src/gluonnlp/embedding/token_embedding.py", line 637, in &lt;listcomp&gt;
    indices = [self._token_to_idx[token] for token in tokens]
KeyError: '&lt;pad&gt;'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;cd scripts/word_embeddings

python evaluate_pretrained.py --gpu 0  --embedding-name glove --embedding-source glove.42B.300d --logdir results --analogy-max-vocab-size 300000 --analogy-datasets GoogleAnalogyTestSet BiggerAnalogyTestSet
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;

It looks like the reason is because &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/word_embeddings/evaluate_pretrained.py#L208&gt;enforce_max_size&lt;/denchmark-link&gt;
 trims off certain tokens such as  which results to the error above.
What should be the correct way to run this script?
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
&lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python

# paste outputs here
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='liusy182' date='2019-08-28T11:29:01Z'>
		Thanks for reporting this issue liusy182. The reason was that  function incorrectly overwrote an internal TokenEmbedding datastructure, breaking the use of 's embedding for . When this script was written, modifying TokenEmbedding's internals was required. Based on &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/750&gt;#750&lt;/denchmark-link&gt;
 we can now use proper API to make the required changes.
In general however, vocab = nlp.Vocab(nlp.data.count_tokens(tokens)) can be replaced with vocab = nlp.Vocab(nlp.data.count_tokens(tokens), unknown_token=token_embedding_.unknown_token, padding_token=None, bos_token=None, eos_token=None). I'm opening a PR for these two changes.
Unfortunately this error slipped through the tests. I'm also extending the testcase.
		</comment>
	</comments>
</bug>