<bug id='992' author='AaronYALai' open_date='2019-10-28T23:46:58Z' closed_time='2019-11-13T02:23:40Z'>
	<summary>[Inconsistency] Non-deterministic behavior of pre-trained BERT within mx.autograd.record() after calling uuid.uuid1()</summary>
	<description>
Sample snippets to reproduce the inconsistency issue:
&lt;denchmark-code&gt;import random
import mxnet as mx
import numpy as np

import gluonnlp as nlp
import uuid

# mx.__version__ '1.5.0' (mxnet-cu100mkl)
# np.__version__ '1.17.3'
# nlp.__version__ '0.8.1'
# environment: source activate mxnet_p36 (Amazon Deep Learning AMI)
# machine: EC2 p3.8xl


def run_bert(context=[mx.gpu()]):
    seed = 0
    mx.random.seed(seed)
    random.seed(seed)
    np.random.seed(seed)

    # Execute this line makes BERT non-deterministic inside mx.autograd.record()
    # uid = uuid.uuid1()

    bert_model, bert_vocab = nlp.model.get_model(
        name='bert_12_768_12',
        dataset_name='book_corpus_wiki_en_uncased',
        pretrained=True,
        ctx=context,
        use_pooler=True,
        use_decoder=False,
        use_classifier=False,
        dropout=0.1,
        embed_dropout=0.1)

    # sample inputs
    ti = mx.nd.array([[2, 22100, 2080, 2629, 2072, 2475, 2912, 7685, 4160, 2078,
                       2629, 4430, 2581, 6895,  3501, 2546, 2629, 2480, 17299, 3],
                      [2, 2064, 1045, 2689, 2000, 2151, 12485, 1029, 2835, 1999,
                       6421, 2575, 2683, 12521, 18139, 2575, 2581, 16068, 2475, 3]],
                     dtype=int, ctx=context[0])
    tt = mx.nd.zeros((2, 20), dtype=int, ctx=context[0])
    vl = mx.nd.array([20, 20], ctx=context[0])

    # BERT forward, outside and inside autograd.record()
    out1 = [a.sum().asscalar() for a in bert_model(ti, tt, vl)]

    with mx.autograd.record():
        # Inconsistent if call "uuid.uuid1()"
        out2 = [a.sum().asscalar() for a in bert_model(ti, tt, vl)]

    print('Outside autograd:', out1)
    print('Inside autograd:', out2)


if __name__ == '__main__':
    run_bert()
&lt;/denchmark-code&gt;

Only when un-commenting and executing "uuid.uuid1()", the "out2" will have different values when you run the script again and again.
Suspect the it messed up with internal states used by gluonnlp BERT implementation.
Thanks in advance for taking care of this issue!
	</description>
	<comments>
		<comment id='1' author='AaronYALai' date='2019-10-28T23:55:27Z'>
		If you could reproduce the non-deterministic behavior with a simple model / example, that would be helpful.
For example, you can try out the different components that the Bert model is made of and pinpoint which one is causing the issue.
Thank you!
		</comment>
		<comment id='2' author='AaronYALai' date='2019-10-29T00:50:48Z'>
		&lt;denchmark-link:https://github.com/AaronYALai&gt;@AaronYALai&lt;/denchmark-link&gt;
 I'm not familiar with the inner mechanism of uuid. It looks that is may have interfered the random state of the random number generator.
		</comment>
		<comment id='3' author='AaronYALai' date='2019-10-29T18:07:08Z'>
		&lt;denchmark-link:https://github.com/AaronYALai&gt;@AaronYALai&lt;/denchmark-link&gt;
 In fact, I'm not able to reproduce the inconsistency problem using the script. Maybe it's because I'm using 1.6
		</comment>
		<comment id='4' author='AaronYALai' date='2019-10-29T19:18:41Z'>
		To reproduce the issue, I've followed this process:


Launch a new GPU instance (ex. p3.2xl) with Deep Learning AMI (Ubuntu 16.04) Version 24.3 (ami-05931d11d2bf831c3)


Once get into the machine, setup the environment:


&lt;denchmark-code&gt;source activate mxnet_p36
pip install --upgrade pip
pip install mxnet-cu100mkl==1.5.0
pip install gluonnlp
&lt;/denchmark-code&gt;


Uncomment the uuid.uuid1() line and run the test program above twice that can have inconsistent outputs

		</comment>
		<comment id='5' author='AaronYALai' date='2019-10-29T19:25:49Z'>
		uuid.uuid1 queries random.getrandbits so it may have changed the internal states of random. This means that somewhere in the code there is dependency on the states in random.
To avoid querying the random, you can specify clock_seq argument when calling uuid1.
Ref: `uuid.uuid1` implementation
```
def uuid1(node=None, clock_seq=None):
    """Generate a UUID from a host ID, sequence number, and the current time.
    If 'node' is not given, getnode() is used to obtain the hardware
    address.  If 'clock_seq' is given, it is used as the sequence number;
    otherwise a random 14-bit sequence number is chosen."""
# When the system provides a version-1 UUID generator, use it (but don't
# use UuidCreate here because its UUIDs don't conform to RFC 4122).
_load_system_functions()
if _generate_time_safe is not None and node is clock_seq is None:
    uuid_time, safely_generated = _generate_time_safe()
    try:
        is_safe = SafeUUID(safely_generated)
    except ValueError:
        is_safe = SafeUUID.unknown
    return UUID(bytes=uuid_time, is_safe=is_safe)

global _last_timestamp
import time
nanoseconds = int(time.time() * 1e9)
# 0x01b21dd213814000 is the number of 100-ns intervals between the
# UUID epoch 1582-10-15 00:00:00 and the Unix epoch 1970-01-01 00:00:00.
timestamp = int(nanoseconds/100) + 0x01b21dd213814000
if _last_timestamp is not None and timestamp &lt;= _last_timestamp:
    timestamp = _last_timestamp + 1
_last_timestamp = timestamp
if clock_seq is None:
    import random
    clock_seq = random.getrandbits(14) # instead of stable storage
time_low = timestamp &amp; 0xffffffff
time_mid = (timestamp &gt;&gt; 32) &amp; 0xffff
time_hi_version = (timestamp &gt;&gt; 48) &amp; 0x0fff
clock_seq_low = clock_seq &amp; 0xff
clock_seq_hi_variant = (clock_seq &gt;&gt; 8) &amp; 0x3f
if node is None:
    node = getnode()
return UUID(fields=(time_low, time_mid, time_hi_version,
                    clock_seq_hi_variant, clock_seq_low, node), version=1)

&lt;/details&gt;

		</comment>
		<comment id='6' author='AaronYALai' date='2019-10-29T19:30:10Z'>
		It's unclear why such query would affect the consistency of the results. We should raise an issue in mxnet to investigate further.
		</comment>
		<comment id='7' author='AaronYALai' date='2019-10-29T20:49:39Z'>
		&lt;denchmark-link:https://github.com/AaronYALai&gt;@AaronYALai&lt;/denchmark-link&gt;
 It's related to the cudnn dropout in MXNet, minimal reproducible example: &lt;denchmark-link:https://gist.github.com/sxjscience/93512080231e6faa1f874fa3eba87b78&gt;https://gist.github.com/sxjscience/93512080231e6faa1f874fa3eba87b78&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='AaronYALai' date='2019-10-31T18:10:49Z'>
		&lt;denchmark-link:https://github.com/AaronYALai&gt;@AaronYALai&lt;/denchmark-link&gt;
 Here's the monkey patch approach to solve this problem (disable the cudnn-dropout):
import random
import mxnet as mx


def apply_monkey_patch_dropout(mxnet_lib):
    class Dropout(mx.gluon.HybridBlock):
        def __init__(self, rate, axes=(), **kwargs):
            super(Dropout, self).__init__(**kwargs)
            self._rate = rate
            self._axes = axes

        def hybrid_forward(self, F, x):
            if self._rate &gt; 0:
                return F.Dropout(x, p=self._rate, axes=self._axes, name='fwd', cudnn_off=True)
            else:
                copy = F.identity
                return copy(x)

        def __repr__(self):
            s = '{name}(p = {_rate}, axes={_axes})'
            return s.format(name=self.__class__.__name__,
                            **self.__dict__)
    mxnet_lib.gluon.nn.Dropout = Dropout
apply_monkey_patch_dropout(mx)

import numpy as np
import gluonnlp as nlp
import uuid

# mx.__version__ '1.5.0' (mxnet-cu100mkl)
# np.__version__ '1.17.3'
# nlp.__version__ '0.8.1'
# environment: source activate mxnet_p36 (Amazon Deep Learning AMI)
# machine: EC2 p3.8xl


def run_bert(context=[mx.gpu()]):
    seed = 0
    mx.random.seed(seed)
    random.seed(seed)
    np.random.seed(seed)

    # Execute this line makes BERT non-deterministic inside mx.autograd.record()
    uid = uuid.uuid1()

    bert_model, bert_vocab = nlp.model.get_model(
        name='bert_12_768_12',
        dataset_name='book_corpus_wiki_en_uncased',
        pretrained=True,
        ctx=context,
        use_pooler=True,
        use_decoder=False,
        use_classifier=False,
        dropout=0.1,
        embed_dropout=0.1)

    # sample inputs
    ti = mx.nd.array([[2, 22100, 2080, 2629, 2072, 2475, 2912, 7685, 4160, 2078,
                       2629, 4430, 2581, 6895,  3501, 2546, 2629, 2480, 17299, 3],
                      [2, 2064, 1045, 2689, 2000, 2151, 12485, 1029, 2835, 1999,
                       6421, 2575, 2683, 12521, 18139, 2575, 2581, 16068, 2475, 3]],
                     dtype=int, ctx=context[0])
    tt = mx.nd.zeros((2, 20), dtype=int, ctx=context[0])
    vl = mx.nd.array([20, 20], ctx=context[0])

    # BERT forward, outside and inside autograd.record()
    out1 = [a.sum().asscalar() for a in bert_model(ti, tt, vl)]

    with mx.autograd.record():
        # Inconsistent if call "uuid.uuid1()"
        out2 = [a.sum().asscalar() for a in bert_model(ti, tt, vl)]

    print('Outside autograd:', out1)
    print('Inside autograd:', out2)


if __name__ == '__main__':
    run_bert()
		</comment>
		<comment id='9' author='AaronYALai' date='2019-10-31T18:30:32Z'>
		This PR in MXNet should fix the problem then. &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/16532&gt;apache/incubator-mxnet#16532&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='AaronYALai' date='2019-11-13T02:23:40Z'>
		Let me close this one since it's mainly related to MXNet.
		</comment>
	</comments>
</bug>