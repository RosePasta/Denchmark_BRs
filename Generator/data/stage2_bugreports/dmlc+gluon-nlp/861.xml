<bug id='861' author='YouhuiBai' open_date='2019-08-03T13:36:05Z' closed_time='2019-12-13T05:37:00Z'>
	<summary>Bert backward propagation order isn't correct ??</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

Hi, I downloaded Bert source code from &lt;denchmark-link:https://gluon-nlp.mxnet.io/model_zoo/bert/index.html&gt;Bert&lt;/denchmark-link&gt;
, and ran successfully at a single machine with one GPU. Then I expanded the code and made it be with the support of Horovod, modified source code &lt;denchmark-link:https://gitee.com/YouhuiBai/Bert&gt;here&lt;/denchmark-link&gt;
, only changed the , then I ran it with 2 servers with one GPU per machine, and profiled the process with &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/docs/timeline.rst&gt;Horovod timeline&lt;/denchmark-link&gt;
, found that there was no overlap between backward computation and communication, namely, it begins to transfer after the completion of backward, cause the order of transfer is exactly the same with forward computation order.
The earliest operators completed from mxnet engine will be transferred first by Horovod, but the transfer order is the same with forward's order, so I think the backward completion order from mxnet engine is not correct. Can anyone give some suggestions?
Thanks.
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

I used the latest code from &lt;denchmark-link:https://gluon-nlp.mxnet.io/model_zoo/bert/index.html&gt;Bert&lt;/denchmark-link&gt;
 and made it be with the support of horovod too, the same result I can get.
&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;


I changed the Bert computation graph from static to dynamic, but it didn't matter.
I tracked to mxnet imperative computation graph, the node ids indicate that the backward order is the reverse of forward, then the operators are sent to mxnet engine to execute with dependency.

But the Bert from google-research with tensorflow and horovod does have overlap between backward and communication. And the image-classification models from &lt;denchmark-link:https://github.com/horovod/horovod/blob/master/examples/mxnet_imagenet_resnet50.py&gt;here&lt;/denchmark-link&gt;
 also have overlap with mxnet and horovod.
&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

gluonnlp 0.7.1, mxnet 1.5.0, horovod 0.16.1, openmpi 3.1.2
&lt;denchmark-h:h1&gt;paste outputs here&lt;/denchmark-h&gt;

For example, you can see that the transformer1 is transferred after transformer0, but the transformer1 is computed and completed first in backward propagation in theory.
&lt;denchmark-link:https://user-images.githubusercontent.com/27176645/62412480-886ede80-b635-11e9-8583-6ab603bb55d3.png&gt;&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='YouhuiBai' date='2019-08-03T16:43:31Z'>
		This looks like a bug. Would you mind providing the complete hvd timeline log?
		</comment>
		<comment id='2' author='YouhuiBai' date='2019-08-04T00:40:55Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 Of course, the log is &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/files/3464459/timeline_Bert.txt&gt;Horovod timeline for Bert&lt;/denchmark-link&gt;
,  you can modify the extension name from  to  to open  with chrome.
The setups are:

Bert using RTE dataset and running 50 iterations
Single machine with 2 GPUs, the communication way between GPUs is PCIe, Horovod organizes these two GPU as a ring

You can see that from the first message prepared to the last message, it takes about 30 ms per iteration, means that the backward computation time cost is less than 30 ms, it's unreasonable.
Thanks a lot
		</comment>
		<comment id='3' author='YouhuiBai' date='2019-08-18T01:31:53Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 Hi Haibin, any suggestions?
		</comment>
		<comment id='4' author='YouhuiBai' date='2019-08-19T16:31:32Z'>
		Hi &lt;denchmark-link:https://github.com/YouhuiBai&gt;@YouhuiBai&lt;/denchmark-link&gt;
 can you update your horovod version? It was fixed by this PR: &lt;denchmark-link:https://github.com/horovod/horovod/commit/0febb667bd0f511de61d2f2d7a667d8e9d30f821&gt;horovod/horovod@0febb66&lt;/denchmark-link&gt;

where the allreduce operation is scheduled by priority
Although the priority currently is defined by the parameter name, which does not really reflect the dependency in a general graph..
		</comment>
		<comment id='5' author='YouhuiBai' date='2019-08-20T01:27:10Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 Thank you,  I see, this is a way to fix this problem by pass a prioritized parameter to mxnet engine, the engine will schedule the operations depended on variables by priority, but why is the keys' order out from the engine different from the order of backward propagation when there is no priority? I think the problem lies in the default scheduling method of mxnet engine, and try to solve it, if you have some experience, please share to me.
		</comment>
		<comment id='6' author='YouhuiBai' date='2019-08-20T22:48:52Z'>
		The idea is that we want to schedule layers that are closer to the input earlier, if when multiple tensors are available for allreduce. This paper talks about the priority based approach: &lt;denchmark-link:https://www.sysml.cc/doc/2019/75.pdf&gt;https://www.sysml.cc/doc/2019/75.pdf&lt;/denchmark-link&gt;

Are you trying to improve the allreduce strategy in mxnet/horovod?
		</comment>
		<comment id='7' author='YouhuiBai' date='2019-08-24T10:00:51Z'>
		Hi Haibin, thanks and yes, I am trying to improve the allreduce strategy in mxnet with horovod, I tried to analyze the computation and communication order of image-classification models and NLP models in mxnet+horovod, then found that the transferring order for Bert is strange. I read this paper few months ago and it is based on the parameter server typology, but there is a little difference when it comes to allreduce-based typology. I have some suspects and am trying to verify them these days.
		</comment>
		<comment id='8' author='YouhuiBai' date='2019-09-11T23:55:40Z'>
		I think there's a bug with the newly added sort logic: &lt;denchmark-link:https://github.com/horovod/horovod/blob/3999eca9c85550564444c61db55591e86c584ea6/horovod/mxnet/__init__.py#L101-L102&gt;https://github.com/horovod/horovod/blob/3999eca9c85550564444c61db55591e86c584ea6/horovod/mxnet/__init__.py#L101-L102&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/YouhuiBai&gt;@YouhuiBai&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='YouhuiBai' date='2019-09-16T12:38:17Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 Sorry I forgot to check gmail last days, I used horovod with version 0.16.1, there is no  in . I found that this is because of global gradient clipping, namely, when all gradients are completed then compute the clipping  in , if the clipping is followed by communication, of course there is no overlap between backward computation and communication. And it doesn't matter to the priority in Mxnet engine, the priority will only impact the order of operators in  at the same moment. Thanks a lot!!
		</comment>
		<comment id='10' author='YouhuiBai' date='2019-12-13T05:37:00Z'>
		np. closing it for now
		</comment>
	</comments>
</bug>