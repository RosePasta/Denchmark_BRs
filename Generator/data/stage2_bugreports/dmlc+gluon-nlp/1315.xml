<bug id='1315' author='ZiyueHuang' open_date='2020-08-25T14:35:13Z' closed_time='2020-08-28T11:53:25Z'>
	<summary>Bugs for run_electra.py at master branch</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Bug for Horovod support&lt;/denchmark-h&gt;

At &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L418&gt;https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L418&lt;/denchmark-link&gt;
,  depends on the data batch, and thus could be different across the horovod processes (note that Horovod will launch a process for each GPU card), meaning that the parameters maintained in each horovod process (since we use ) might diverge.
We can normalize the loss by batch_size on each worker before trainer.allreduce_grads(), to avoid syncing num_samples_per_update across all workers.
&lt;denchmark-h:h3&gt;Bug for MLM loss&lt;/denchmark-h&gt;

In the current implementation, the MLM loss actually doesn't take into account masked_weights.
At &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L383-L384&gt;https://github.com/dmlc/gluon-nlp/blob/master/scripts/pretraining/run_electra.py#L383-L384&lt;/denchmark-link&gt;
,  should be replaced by , please see the doc of  at &lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/19010/files&gt;https://github.com/apache/incubator-mxnet/pull/19010/files&lt;/denchmark-link&gt;
 or the example usage in bert at &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/v0.10.x/scripts/bert/pretraining_utils.py#L416-L418&gt;https://github.com/dmlc/gluon-nlp/blob/v0.10.x/scripts/bert/pretraining_utils.py#L416-L418&lt;/denchmark-link&gt;

Note that
mlm_scores's shape: (batch_size, num_masked_positions, vocab_size)
unmasked_tokens's shape: (batch_size, num_masked_positions)
masked_weights's shape: (batch_size, num_masked_positions)
If we comment out  and add two lines before/after &lt;denchmark-link:https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/loss.py#L408&gt;https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/loss.py#L408&lt;/denchmark-link&gt;
:  and , running the current script run_electra.py we will get
&lt;denchmark-code&gt;loss.shape before weighting  (64, 19, 1)  sample_weight.shape  (1216,)
loss.shape after weighting  (64, 19, 1216)
&lt;/denchmark-code&gt;

note that 1216 = 64 x 19
cc &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ZheyuYe&gt;@ZheyuYe&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

(Paste the complete error message, including stack trace.)
&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

(If you developed your own code, please provide a short script that reproduces the error. For existing examples, please provide link.)
&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

(Paste the commands you ran that produced the error.)




&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;





&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
&lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python

# paste outputs here
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='ZiyueHuang' date='2020-08-25T14:58:43Z'>
		There is indeed a Bug for mlm loss here, thank you very patiently and carefully to revise the training script and find this bug out. As Far as I can see, this MLM bug will lead to a steep increase in calculations which need to be fixed.
		</comment>
		<comment id='2' author='ZiyueHuang' date='2020-08-25T15:00:06Z'>
		&lt;denchmark-link:https://github.com/ZheyuYe&gt;@ZheyuYe&lt;/denchmark-link&gt;
 Thanks for the response, I will work on the fix later.
		</comment>
		<comment id='3' author='ZiyueHuang' date='2020-08-25T17:04:53Z'>
		That might also explain why it's slower. Thanks for the observation! We need to refactor the pretraining script to make it more structured.
		</comment>
	</comments>
</bug>