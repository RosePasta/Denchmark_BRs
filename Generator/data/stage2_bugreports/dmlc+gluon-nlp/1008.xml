<bug id='1008' author='hymzoque' open_date='2019-11-14T09:38:41Z' closed_time='2020-10-26T22:48:09Z'>
	<summary>GPT2BPETokenizer produce strange symbol</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I ran code piece from
&lt;denchmark-link:https://gluon-nlp.mxnet.io/model_zoo/bert/index.html&gt;https://gluon-nlp.mxnet.io/model_zoo/bert/index.html&lt;/denchmark-link&gt;

and the GPT2BPETokenizer produce a strange symbol Ġ
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

In [1]: import gluonnlp as nlp; import mxnet as mx;
...: model, vocab = nlp.model.get_model('roberta_12_768_12', dataset_name='openwebtext_ccnew
...: s_stories_books_cased', use_decoder=False);
...: tokenizer = nlp.data.GPT2BPETokenizer();
...: text = [vocab.bos_token] + tokenizer('Hello world!') + [vocab.eos_token];
...: seq_encoding = model(mx.nd.array([vocab[text]]))
...:
...:
In [2]: print(text)
['&lt;s&gt;', 'Hello', 'Ġworld', '!', '&lt;/s&gt;']
command and paste the outputs below:
	</description>
	<comments>
		<comment id='1' author='hymzoque' date='2019-11-14T10:01:00Z'>
		Thank you for reporting the bug &lt;denchmark-link:https://github.com/hymzoque&gt;@hymzoque&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 are you aware of this issue? Could it be related to &lt;denchmark-link:https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py#L38-L56&gt;https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py#L38-L56&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='hymzoque' date='2019-11-14T10:02:47Z'>
		Also, currently GPT2BPETokenizer hardcodes the vocabulary. However, for example to add XLM-R &lt;denchmark-link:https://github.com/pytorch/fairseq/tree/master/examples/xlmr&gt;https://github.com/pytorch/fairseq/tree/master/examples/xlmr&lt;/denchmark-link&gt;
 we need to add support for a separate vocabulary.
		</comment>
		<comment id='3' author='hymzoque' date='2019-11-14T11:36:57Z'>
		I think that is how it encodes space character. As far as I can tell, it doesn't lead to any error. For instance, if you apply nlp.data.GPT2BPEDetokenizer on the tokenized text, you successfully retrieve the original text.
		</comment>
		<comment id='4' author='hymzoque' date='2019-11-14T14:42:46Z'>
		Yes, that's working as intended. (cc &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
)
		</comment>
		<comment id='5' author='hymzoque' date='2019-11-14T15:16:15Z'>
		We are now considering to revise the GPT2Tokenzier to make it more similar to &lt;denchmark-link:https://github.com/sxjscience/gluonnlp-gpt2&gt;https://github.com/sxjscience/gluonnlp-gpt2&lt;/denchmark-link&gt;
. For this issue, it’s not a bug because the tokenizer in GPT2 is based on bytes.
		</comment>
		<comment id='6' author='hymzoque' date='2019-11-14T15:59:03Z'>
		&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 is there an open issue which I can follow for supporting separate vocabulary for GPT2BPETokenizer as mentioned by &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
 ? Or is it on the backburner since there is only one trained dataset for both GPT and RoBERTa?
		</comment>
		<comment id='7' author='hymzoque' date='2019-11-15T02:01:11Z'>
		Before closing this issue, at the very least we have to fix the example to use the GPT2BPEDetokenizer instead of simply looking up tokens in the vocab.
&lt;denchmark-link:https://github.com/zeeshansayyed&gt;@zeeshansayyed&lt;/denchmark-link&gt;
 let's open a second issue to track it. It's a real issue as other models use the same tokenizer with a separate vocab.
		</comment>
		<comment id='8' author='hymzoque' date='2020-10-26T22:28:51Z'>
		&lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
 Should we close this?
		</comment>
		<comment id='9' author='hymzoque' date='2020-10-26T22:48:09Z'>
		Fixed in master
		</comment>
	</comments>
</bug>