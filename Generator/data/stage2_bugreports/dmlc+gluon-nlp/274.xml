<bug id='274' author='leezu' open_date='2018-08-13T15:18:54Z' closed_time='2018-08-15T06:17:34Z'>
	<summary>Sharded dataloader causing CI hangs</summary>
	<description>
Recently many jobs on CI are running into deadlocks and must be manually killed. This morning I killed a few jobs that ran more than 12 hours. I observe that all of them hang in the sharded dataloader tests. &lt;denchmark-link:https://github.com/szhengac&gt;@szhengac&lt;/denchmark-link&gt;
 do you have any idea what could be the reason?
From &lt;denchmark-link:http://ci.mxnet.io/blue/organizations/jenkins/gluon-nlp/detail/PR-246/10/pipeline&gt;http://ci.mxnet.io/blue/organizations/jenkins/gluon-nlp/detail/PR-246/10/pipeline&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;tests/unittest/train/test_dataloader.py::test_sharded_data_loader Sending interrupt signal to process
&lt;/denchmark-code&gt;

Also

http://ci.mxnet.io/blue/organizations/jenkins/gluon-nlp/detail/PR-233/25/pipeline/
http://ci.mxnet.io/blue/organizations/jenkins/gluon-nlp/detail/PR-246/12/pipeline/

	</description>
	<comments>
		<comment id='1' author='leezu' date='2018-08-13T15:42:06Z'>
		Does script have the same problem? Is it possible that it is due to some
env setup changes? I think we don’t have such problems before.
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, 13 Aug 2018 at 23:18, Leonard Lausen ***@***.***&gt; wrote:
 Recently many jobs on CI are running into deadlocks and must be manually
 killed. This morning I killed a few jobs that ran more than 12 hours. I
 observe that all of them hang in the sharded dataloader tests. @szhengac
 &lt;https://github.com/szhengac&gt; do you have any idea what could be the
 reason?

 From
 http://ci.mxnet.io/blue/organizations/jenkins/gluon-nlp/detail/PR-246/10/pipeline

 tests/unittest/train/test_dataloader.py::test_sharded_data_loader Sending interrupt signal to process
 After 10s process did not stop

 Also
 http://ci.mxnet.io/blue/organizations/jenkins/gluon-nlp/detail/PR-233/25/pipeline/

 tests/unittest/train/test_dataloader.py::test_sharded_data_loaderSending interrupt signal to process
 Terminated
 script returned exit code 143

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#274&gt;, or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/ADxs1IBTPmgDAcLB9hafy4GStWuI8iaZks5uQZjfgaJpZM4V6tWg&gt;
 .



		</comment>
		<comment id='2' author='leezu' date='2018-08-13T16:46:50Z'>
		I'm not aware of any change in environment. It may be due to some other issue, but for some reason CI always shows that it was working on the sharded dataloader test when being killed.
		</comment>
		<comment id='3' author='leezu' date='2018-08-13T17:29:37Z'>
		I think the hang also occurs during the test_transformer scripts tests as it relies on the ShardedDataloader. ci.mxnet.io/blue/organizations/jenkins/gluon-nlp/detail/PR-275/2/pipeline/22
I disabled that for now too.
		</comment>
		<comment id='4' author='leezu' date='2018-08-13T18:33:41Z'>
		CI works again after disabling both. The test should be enabled again before 0.4
		</comment>
		<comment id='5' author='leezu' date='2018-08-14T00:49:14Z'>
		I think this is due to the recent change in DataLoader (&lt;denchmark-link:https://github.com/apache/incubator-mxnet/pull/11908&gt;#11908&lt;/denchmark-link&gt;
). The shared DataLoader inherits the DataLoader, and the change possibly incurs some inconsistency. The workaround is to copy the original DataLoader to shared Dataloader instead of using inheritance.
		</comment>
		<comment id='6' author='leezu' date='2018-08-14T01:16:05Z'>
		&lt;denchmark-link:https://github.com/zhreshold&gt;@zhreshold&lt;/denchmark-link&gt;
 may have some ideas.
		</comment>
		<comment id='7' author='leezu' date='2018-08-14T01:23:45Z'>
		&lt;denchmark-link:https://github.com/szhengac&gt;@szhengac&lt;/denchmark-link&gt;
 is correct, the latest changes in #11908 was not correctly handled by _ShardedMultiWorkerIter, therefore the workers are never actually terminated.
		</comment>
	</comments>
</bug>