<bug id='544' author='szhengac' open_date='2019-01-09T07:30:27Z' closed_time='2019-02-01T12:15:32Z'>
	<summary>Transformer BLEU of the current code base is worse than previous one</summary>
	<description>
I recently retrained the transformer, and found that it will diverge with the default setting. The divergence issue is caused by use_layer_norm_before_dropout=True and is fixed in &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/543&gt;#543&lt;/denchmark-link&gt;
. However, the final testing BLEU is still worse than the previous result. Compare to the version in &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/413&gt;#413&lt;/denchmark-link&gt;
, the testing BLEU is about 0.8 worse.
	</description>
	<comments>
		<comment id='1' author='szhengac' date='2019-01-09T07:31:23Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='szhengac' date='2019-01-09T17:38:05Z'>
		Any idea where the difference is coming from?
		</comment>
		<comment id='3' author='szhengac' date='2019-01-09T18:19:44Z'>
		I checked the part of code of model and didn't find error except the one mentioned in &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/543&gt;#543&lt;/denchmark-link&gt;
. Comparing the logs, the outputs of bucket sampler are different. But I haven't got time looking into it. This may be caused by the change to the bucket sampler or the data processing. I paste the output of sampler as follows:
Current code base:
FixedBucketSampler:
sample_num=4493328, batch_num=59321
key=[(4, 5), (7, 8), (11, 13), (16, 18), (22, 24), (29, 31), (38, 40), (48, 50), (60, 63), (74, 78), (92, 96), (113, 118), (138, 144), (168, 175), (205, 213), (248, 258), (301, 312), (363, 377), (439,
455), (529, 548)]
cnt=[15338, 57067, 248232, 511919, 747010, 820181, 816207, 571262, 380721, 193794, 90070, 32452, 6865, 1445, 517, 171, 57, 11, 7, 2]
batch_size=[629, 362, 233, 166, 123, 94, 72, 57, 45, 36, 29, 24, 19, 16, 13, 10, 8, 7, 6, 4]
2019-01-05 16:04:49,638 - root - Valid Batch Sampler:
FixedBucketSampler:
sample_num=3000, batch_num=376
key=[3, 4, 5, 6, 7, 9, 11, 14, 17, 20, 25, 30, 36, 44, 53, 64, 77, 93, 111, 134]
cnt=[13, 12, 14, 24, 53, 110, 141, 254, 253, 275, 411, 377, 321, 328, 207, 109, 61, 24, 7, 6]
batch_size=[80, 64, 51, 42, 36, 28, 23, 18, 15, 12, 10, 8, 7, 6, 5, 4, 3, 2, 2, 1]
2019-01-05 16:04:49,647 - root - Test Batch Sampler:
FixedBucketSampler:
sample_num=3003, batch_num=394
key=[6, 7, 8, 9, 10, 12, 13, 15, 18, 21, 25, 29, 35, 41, 49, 58, 70, 83, 99]
cnt=[16, 20, 35, 43, 46, 141, 86, 167, 261, 257, 355, 308, 422, 306, 223, 192, 93, 27, 5]
batch_size=[41, 36, 32, 28, 25, 21, 19, 17, 14, 12, 10, 8, 7, 6, 5, 4, 3, 3, 2]
Previous one:
FixedBucketSampler:
sample_num=4493328, batch_num=61430
key=[(5, 6), (8, 9), (12, 14), (17, 19), (23, 25), (30, 32), (39, 41), (49, 51), (61, 64), (75, 79), (93, 97), (114, 119), (139, 145), (169, 176), (206, 214), (249, 259), (302, 313), (364, 378), (440, 456), (530, 549)]
cnt=[15338, 57067, 248232, 511919, 747010, 820181, 816207, 571262, 380721, 193794, 90070, 32452, 6865, 1445, 517, 171, 57, 11, 7, 2]
batch_size=[510, 319, 214, 156, 117, 90, 70, 55, 44, 35, 29, 23, 19, 16, 13, 10, 8, 7, 6, 4]
2019-01-09 14:13:53,198 - root - Valid Batch Sampler:
FixedBucketSampler:
sample_num=3000, batch_num=400
key=[4, 5, 6, 7, 8, 10, 12, 15, 18, 21, 26, 31, 37, 45, 54, 65, 78, 94, 112, 135]
cnt=[13, 12, 14, 24, 53, 110, 141, 254, 253, 275, 411, 377, 321, 328, 207, 109, 61, 24, 7, 6]
batch_size=[61, 51, 42, 36, 32, 25, 21, 17, 14, 12, 10, 8, 7, 5, 4, 4, 3, 2, 2, 1]
2019-01-09 14:13:53,204 - root - Test Batch Sampler:
FixedBucketSampler:
sample_num=3003, batch_num=400
key=[7, 8, 9, 10, 11, 13, 14, 16, 19, 22, 26, 30, 36, 42, 50, 59, 71, 84, 100]
cnt=[16, 20, 35, 43, 46, 141, 86, 167, 261, 257, 355, 308, 422, 306, 223, 192, 93, 27, 5]
batch_size=[36, 32, 28, 25, 23, 19, 18, 16, 13, 11, 10, 8, 7, 6, 5, 4, 3, 3, 2]
		</comment>
		<comment id='4' author='szhengac' date='2019-01-10T20:37:11Z'>
		Does it mean there's also some changes in the data processing pipeline?
		</comment>
		<comment id='5' author='szhengac' date='2019-01-11T04:53:50Z'>
		Comparing the keys, it seems each sequence lose one token.
		</comment>
	</comments>
</bug>