<bug id='1134' author='xinyu-intel' open_date='2020-02-01T12:51:20Z' closed_time='2020-02-03T12:41:03Z'>
	<summary>Cannot reproduce BERT MRPC accuracy on mxnet1.6.0rc1&amp;2</summary>
	<description>
Cannot the &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/commit/5fe8d7b7f5770d82e4f3bfb489a67999d9442ce6#diff-4aa9944e2504738fdac28fee1d391628R124&gt;reported accuracy&lt;/denchmark-link&gt;
 with the master branch of GluonNLP and a pre-release build of MXNet.
&lt;denchmark-h:h2&gt;Build MXNet:&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;git checkout 1.6.0.rc2 #(or rc1)
    make \
        DEV=0                                     \
        ENABLE_TESTCOVERAGE=0                     \
        USE_BLAS=openblas                         \
        USE_MKLDNN=0                              \
        USE_CUDA=1                                \
        USE_CUDA_PATH=/usr/local/cuda             \
        USE_CUDNN=1                               \
        USE_CPP_PACKAGE=0                         \
        USE_DIST_KVSTORE=1                        \
        USE_SIGNAL_HANDLER=1                      \
        -j$(nproc)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;finetune log&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;root@04056391b12a:/workspace/gluon-nlp/scripts/bert# python3 finetune_classifier.py --batch_size 32 --lr 2e-5 --epochs 5 --gpu 0 --seed 27 --task_name MRPC --warmup_ratio 0.1
INFO:root:20:37:55 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=10, lr=2e-05, max_len=128, model_parameters=None, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, seed=27, task_name='MRPC', training_steps=None, warmup_ratio=0.1)
INFO:root:20:38:01 processing dataset...
INFO:root:20:38:04 Now we are doing BERT classification training on gpu(0)!
INFO:root:20:38:04 training steps=573
INFO:root:20:38:07 [Epoch 1 Batch 10/119] loss=0.6059, lr=0.0000032, metrics:accuracy:0.7125,f1:0.8321
INFO:root:20:38:08 [Epoch 1 Batch 20/119] loss=0.6329, lr=0.0000067, metrics:accuracy:0.6969,f1:0.8210
INFO:root:20:38:10 [Epoch 1 Batch 30/119] loss=0.5927, lr=0.0000102, metrics:accuracy:0.7010,f1:0.8225
INFO:root:20:38:11 [Epoch 1 Batch 40/119] loss=0.6142, lr=0.0000137, metrics:accuracy:0.6938,f1:0.8111
INFO:root:20:38:12 [Epoch 1 Batch 50/119] loss=0.6124, lr=0.0000172, metrics:accuracy:0.6900,f1:0.7995
INFO:root:20:38:14 [Epoch 1 Batch 60/119] loss=0.5779, lr=0.0000199, metrics:accuracy:0.6983,f1:0.8040
INFO:root:20:38:15 [Epoch 1 Batch 70/119] loss=0.5541, lr=0.0000195, metrics:accuracy:0.7019,f1:0.8050
INFO:root:20:38:17 [Epoch 1 Batch 80/119] loss=0.5493, lr=0.0000191, metrics:accuracy:0.7078,f1:0.8065
INFO:root:20:38:18 [Epoch 1 Batch 90/119] loss=0.5324, lr=0.0000188, metrics:accuracy:0.7127,f1:0.8088
INFO:root:20:38:20 [Epoch 1 Batch 100/119] loss=0.4922, lr=0.0000184, metrics:accuracy:0.7190,f1:0.8111
INFO:root:20:38:21 [Epoch 1 Batch 110/119] loss=0.4427, lr=0.0000180, metrics:accuracy:0.7272,f1:0.8162
INFO:root:20:38:23 Now we are doing evaluation on dev with gpu(0).
INFO:root:20:38:23 [Batch 10/51] loss=0.4181, metrics:accuracy:0.8250,f1:0.8852
INFO:root:20:38:23 [Batch 20/51] loss=0.5037, metrics:accuracy:0.8187,f1:0.8816
INFO:root:20:38:24 [Batch 30/51] loss=0.4967, metrics:accuracy:0.8000,f1:0.8717
INFO:root:20:38:24 [Batch 40/51] loss=0.4722, metrics:accuracy:0.8000,f1:0.8704
INFO:root:20:38:25 [Batch 50/51] loss=0.5662, metrics:accuracy:0.7850,f1:0.8613
INFO:root:20:38:25 validation metrics:accuracy:0.7868,f1:0.8621
INFO:root:20:38:25 Time cost=1.77s, throughput=229.98 samples/s
INFO:root:20:38:26 params saved in: ./output_dir/model_bert_MRPC_0.params
INFO:root:20:38:26 Time cost=22.49s
INFO:root:20:38:28 [Epoch 2 Batch 10/119] loss=0.3253, lr=0.0000172, metrics:accuracy:0.8688,f1:0.8966
INFO:root:20:38:29 [Epoch 2 Batch 20/119] loss=0.3138, lr=0.0000169, metrics:accuracy:0.8719,f1:0.9042
INFO:root:20:38:31 [Epoch 2 Batch 30/119] loss=0.2757, lr=0.0000165, metrics:accuracy:0.8797,f1:0.9104
INFO:root:20:38:32 [Epoch 2 Batch 40/119] loss=0.2706, lr=0.0000161, metrics:accuracy:0.8833,f1:0.9130
INFO:root:20:38:34 [Epoch 2 Batch 50/119] loss=0.2650, lr=0.0000157, metrics:accuracy:0.8866,f1:0.9145
INFO:root:20:38:35 [Epoch 2 Batch 60/119] loss=0.2459, lr=0.0000153, metrics:accuracy:0.8889,f1:0.9178
INFO:root:20:38:37 [Epoch 2 Batch 70/119] loss=0.2139, lr=0.0000149, metrics:accuracy:0.8950,f1:0.9225
INFO:root:20:38:38 [Epoch 2 Batch 80/119] loss=0.2327, lr=0.0000145, metrics:accuracy:0.8956,f1:0.9231
INFO:root:20:38:40 [Epoch 2 Batch 90/119] loss=0.2622, lr=0.0000141, metrics:accuracy:0.8933,f1:0.9215
INFO:root:20:38:41 [Epoch 2 Batch 100/119] loss=0.2276, lr=0.0000138, metrics:accuracy:0.8954,f1:0.9233
INFO:root:20:38:43 [Epoch 2 Batch 110/119] loss=0.2377, lr=0.0000134, metrics:accuracy:0.8962,f1:0.9232
INFO:root:20:38:44 Now we are doing evaluation on dev with gpu(0).
INFO:root:20:38:44 [Batch 10/51] loss=0.3335, metrics:accuracy:0.8750,f1:0.9153
INFO:root:20:38:45 [Batch 20/51] loss=0.3119, metrics:accuracy:0.8812,f1:0.9191
INFO:root:20:38:45 [Batch 30/51] loss=0.4895, metrics:accuracy:0.8542,f1:0.8997
INFO:root:20:38:45 [Batch 40/51] loss=0.4089, metrics:accuracy:0.8531,f1:0.8976
INFO:root:20:38:46 [Batch 50/51] loss=0.3950, metrics:accuracy:0.8550,f1:0.8990
INFO:root:20:38:46 validation metrics:accuracy:0.8554,f1:0.8991
INFO:root:20:38:46 Time cost=1.74s, throughput=234.35 samples/s
INFO:root:20:38:47 params saved in: ./output_dir/model_bert_MRPC_1.params
INFO:root:20:38:47 Time cost=21.17s
INFO:root:20:38:49 [Epoch 3 Batch 10/119] loss=0.1351, lr=0.0000126, metrics:accuracy:0.9643,f1:0.9744
INFO:root:20:38:50 [Epoch 3 Batch 20/119] loss=0.1336, lr=0.0000122, metrics:accuracy:0.9634,f1:0.9736
INFO:root:20:38:52 [Epoch 3 Batch 30/119] loss=0.0653, lr=0.0000119, metrics:accuracy:0.9672,f1:0.9763
INFO:root:20:38:53 [Epoch 3 Batch 40/119] loss=0.1611, lr=0.0000115, metrics:accuracy:0.9593,f1:0.9702
INFO:root:20:38:55 [Epoch 3 Batch 50/119] loss=0.0723, lr=0.0000111, metrics:accuracy:0.9632,f1:0.9731
INFO:root:20:38:56 [Epoch 3 Batch 60/119] loss=0.0905, lr=0.0000107, metrics:accuracy:0.9642,f1:0.9740
INFO:root:20:38:58 [Epoch 3 Batch 70/119] loss=0.0998, lr=0.0000103, metrics:accuracy:0.9649,f1:0.9742
INFO:root:20:39:00 [Epoch 3 Batch 80/119] loss=0.1052, lr=0.0000099, metrics:accuracy:0.9658,f1:0.9746
INFO:root:20:39:01 [Epoch 3 Batch 90/119] loss=0.1333, lr=0.0000095, metrics:accuracy:0.9654,f1:0.9744
INFO:root:20:39:02 [Epoch 3 Batch 100/119] loss=0.1174, lr=0.0000091, metrics:accuracy:0.9658,f1:0.9746
INFO:root:20:39:04 [Epoch 3 Batch 110/119] loss=0.0648, lr=0.0000088, metrics:accuracy:0.9670,f1:0.9755
INFO:root:20:39:05 Now we are doing evaluation on dev with gpu(0).
INFO:root:20:39:06 [Batch 10/51] loss=0.4370, metrics:accuracy:0.8750,f1:0.9123
INFO:root:20:39:06 [Batch 20/51] loss=0.5114, metrics:accuracy:0.8875,f1:0.9217
INFO:root:20:39:06 [Batch 30/51] loss=0.6030, metrics:accuracy:0.8625,f1:0.9054
INFO:root:20:39:07 [Batch 40/51] loss=0.5415, metrics:accuracy:0.8688,f1:0.9091
INFO:root:20:39:07 [Batch 50/51] loss=0.5222, metrics:accuracy:0.8675,f1:0.9081
INFO:root:20:39:07 validation metrics:accuracy:0.8701,f1:0.9097
INFO:root:20:39:07 Time cost=1.72s, throughput=237.49 samples/s
INFO:root:20:39:08 params saved in: ./output_dir/model_bert_MRPC_2.params
INFO:root:20:39:08 Time cost=21.12s
INFO:root:20:39:10 [Epoch 4 Batch 10/119] loss=0.0717, lr=0.0000080, metrics:accuracy:0.9773,f1:0.9825
INFO:root:20:39:11 [Epoch 4 Batch 20/119] loss=0.0759, lr=0.0000076, metrics:accuracy:0.9761,f1:0.9827
INFO:root:20:39:13 [Epoch 4 Batch 30/119] loss=0.0419, lr=0.0000072, metrics:accuracy:0.9781,f1:0.9844
INFO:root:20:39:15 [Epoch 4 Batch 40/119] loss=0.0484, lr=0.0000069, metrics:accuracy:0.9801,f1:0.9853
INFO:root:20:39:16 [Epoch 4 Batch 50/119] loss=0.0318, lr=0.0000065, metrics:accuracy:0.9823,f1:0.9871
INFO:root:20:39:18 [Epoch 4 Batch 60/119] loss=0.0364, lr=0.0000061, metrics:accuracy:0.9837,f1:0.9881
INFO:root:20:39:20 [Epoch 4 Batch 70/119] loss=0.0524, lr=0.0000057, metrics:accuracy:0.9843,f1:0.9887
INFO:root:20:39:21 [Epoch 4 Batch 80/119] loss=0.0461, lr=0.0000053, metrics:accuracy:0.9847,f1:0.9889
INFO:root:20:39:23 [Epoch 4 Batch 90/119] loss=0.0142, lr=0.0000049, metrics:accuracy:0.9856,f1:0.9896
INFO:root:20:39:24 [Epoch 4 Batch 100/119] loss=0.0698, lr=0.0000045, metrics:accuracy:0.9858,f1:0.9896
INFO:root:20:39:26 [Epoch 4 Batch 110/119] loss=0.0215, lr=0.0000041, metrics:accuracy:0.9868,f1:0.9903
INFO:root:20:39:27 Now we are doing evaluation on dev with gpu(0).
INFO:root:20:39:27 [Batch 10/51] loss=0.4604, metrics:accuracy:0.8875,f1:0.9204
INFO:root:20:39:28 [Batch 20/51] loss=0.4895, metrics:accuracy:0.8875,f1:0.9204
INFO:root:20:39:28 [Batch 30/51] loss=0.8420, metrics:accuracy:0.8583,f1:0.9000
INFO:root:20:39:29 [Batch 40/51] loss=0.5393, metrics:accuracy:0.8625,f1:0.9022
INFO:root:20:39:29 [Batch 50/51] loss=0.5371, metrics:accuracy:0.8675,f1:0.9059
INFO:root:20:39:29 validation metrics:accuracy:0.8676,f1:0.9056
INFO:root:20:39:29 Time cost=1.78s, throughput=228.63 samples/s
INFO:root:20:39:30 params saved in: ./output_dir/model_bert_MRPC_3.params
INFO:root:20:39:30 Time cost=21.96s
INFO:root:20:39:32 [Epoch 5 Batch 10/119] loss=0.0172, lr=0.0000034, metrics:accuracy:0.9938,f1:0.9950
INFO:root:20:39:33 [Epoch 5 Batch 20/119] loss=0.0403, lr=0.0000030, metrics:accuracy:0.9906,f1:0.9927
INFO:root:20:39:35 [Epoch 5 Batch 30/119] loss=0.0427, lr=0.0000026, metrics:accuracy:0.9917,f1:0.9936
INFO:root:20:39:36 [Epoch 5 Batch 40/119] loss=0.0460, lr=0.0000022, metrics:accuracy:0.9913,f1:0.9934
INFO:root:20:39:38 [Epoch 5 Batch 50/119] loss=0.0076, lr=0.0000019, metrics:accuracy:0.9924,f1:0.9942
INFO:root:20:39:40 [Epoch 5 Batch 60/119] loss=0.0457, lr=0.0000015, metrics:accuracy:0.9921,f1:0.9941
INFO:root:20:39:41 [Epoch 5 Batch 70/119] loss=0.0045, lr=0.0000011, metrics:accuracy:0.9932,f1:0.9949
INFO:root:20:39:42 [Epoch 5 Batch 80/119] loss=0.0227, lr=0.0000007, metrics:accuracy:0.9931,f1:0.9948
INFO:root:20:39:44 [Epoch 5 Batch 90/119] loss=0.0273, lr=0.0000003, metrics:accuracy:0.9931,f1:0.9948
INFO:root:20:39:45 Finish training step: 573
INFO:root:20:39:45 Now we are doing evaluation on dev with gpu(0).
INFO:root:20:39:45 [Batch 10/51] loss=0.5129, metrics:accuracy:0.8750,f1:0.9123
INFO:root:20:39:46 [Batch 20/51] loss=0.5087, metrics:accuracy:0.8812,f1:0.9170
INFO:root:20:39:46 [Batch 30/51] loss=0.8643, metrics:accuracy:0.8542,f1:0.8980
INFO:root:20:39:47 [Batch 40/51] loss=0.5841, metrics:accuracy:0.8594,f1:0.9007
INFO:root:20:39:47 [Batch 50/51] loss=0.6210, metrics:accuracy:0.8625,f1:0.9030
INFO:root:20:39:47 validation metrics:accuracy:0.8627,f1:0.9028
INFO:root:20:39:47 Time cost=1.72s, throughput=236.88 samples/s
INFO:root:20:39:48 params saved in: ./output_dir/model_bert_MRPC_4.params
INFO:root:20:39:48 Time cost=17.98s
INFO:root:20:39:49 Best model at epoch 2. Validation metrics:accuracy:0.8701,f1:0.9097
INFO:root:20:39:49 Now we are doing testing on test with gpu(0).
INFO:root:20:39:56 Time cost=6.93s, throughput=249.47 samples/s
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/TaoLv&gt;@TaoLv&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zburning&gt;@zburning&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='xinyu-intel' date='2020-02-01T13:56:59Z'>
		Hi &lt;denchmark-link:https://github.com/xinyu-intel&gt;@xinyu-intel&lt;/denchmark-link&gt;
 I am looking into it.  I think most likely it's due to the different mxnet and gluonnlp commits...The training on small glue datasets has high variance. From my side, I think acc 87 is around the average so that it's acceptable.
		</comment>
		<comment id='2' author='xinyu-intel' date='2020-02-01T13:59:51Z'>
		&lt;denchmark-link:https://github.com/zburning&gt;@zburning&lt;/denchmark-link&gt;
 yh, I do think so, it's hard to reproduce the same accuracy. Can you please send me a copy of MRPC-base and squad1.1-base params to me? I'm going to evaluate the int8 quantization accuracy.
		</comment>
		<comment id='3' author='xinyu-intel' date='2020-02-01T14:09:05Z'>
		&lt;denchmark-link:https://github.com/xinyu-intel&gt;@xinyu-intel&lt;/denchmark-link&gt;
 Sure, are you using &lt;denchmark-link:mailto:xinyu1.chen@intel.com&gt;xinyu1.chen@intel.com&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='4' author='xinyu-intel' date='2020-02-01T14:10:39Z'>
		&lt;denchmark-link:https://github.com/zburning&gt;@zburning&lt;/denchmark-link&gt;
 you can send me through &lt;denchmark-link:mailto:xinyulux@gmail.com&gt;xinyulux@gmail.com&lt;/denchmark-link&gt;
. Thx:)
		</comment>
		<comment id='5' author='xinyu-intel' date='2020-02-01T16:41:07Z'>
		The reported results are achieved by mxnet1.6.0rc1 commit ad1ff3aa8532f2f7e42a732f0d35dfb0574ca05c, and gluonnlp master branch commit &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/commit/7f52dc05367b4640833abf2d767c98e4719395af&gt;7f52dc0&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='xinyu-intel' date='2020-02-01T19:01:00Z'>
		MRPC's variance is a bit high. Are we reproducing fine-tuning just to get a checkpoint? I have a SST-2 checkpoint at &lt;denchmark-link:https://dist-bert.s3.amazonaws.com/demo/finetune/sst.params&gt;https://dist-bert.s3.amazonaws.com/demo/finetune/sst.params&lt;/denchmark-link&gt;

The target is to compare the acc before and after the calibration, right?
		</comment>
		<comment id='7' author='xinyu-intel' date='2020-02-01T23:53:43Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 yes, you are right. I'll try your params.
		</comment>
	</comments>
</bug>