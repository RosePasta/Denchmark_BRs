<bug id='1034' author='rich-junwang' open_date='2019-12-03T01:04:51Z' closed_time='2019-12-05T02:48:29Z'>
	<summary>Error when using fp16 trainer</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I use fp16 trainer in fp16_utils.py to train model. I got the following error.
&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

File "/home/ec2-user/project/src/deep/utils/fp16_utils.py", line 179, in step
overflow = self._scaler.has_overflow(self.fp32_trainer._params)
File "/home/ec2-user/project/src/deep/utils/fp16_utils.py", line 195, in has_overflow
is_not_finite += mx.nd.contrib.isnan(grad).sum()
File "/apollo/env/project/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py", line 217, in iadd
return op.broadcast_add(self, other, out=self)
File "", line 56, in broadcast_add
File "/apollo/env/project/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py", line 92, in _imperative_invoke
ctypes.byref(out_stypes)))
File "/apollo/env/project/lib/python3.6/site-packages/mxnet/base.py", line 253, in check_call
raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [22:57:10] /opt/brazil-pkg-cache/packages/DeepMXNet/DeepMXNet-1.5.x.1353.0/AL2012/generic-flavor/src/src/io/../operator/elemwise_op_common.h:135: Check failed: assign(&amp;dattr, vec.at(i)): Incompatible attr in node  at 1-th input: expected float16, got float32
	</description>
	<comments>
	</comments>
</bug>