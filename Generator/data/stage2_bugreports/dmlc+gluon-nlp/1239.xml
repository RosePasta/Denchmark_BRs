<bug id='1239' author='haven-jeon' open_date='2020-06-08T07:55:52Z' closed_time='2020-07-12T01:09:44Z'>
	<summary>BPE's  default alpha with sentencepiece</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

As BPE-dropout is applied to sentencepiece recently, it can be tokenized based on sampling.
alpha = 1 is for optimally training not for inference.
Default alpha=1 isn't appropriate because most users and models provided by gluonnlp expect to deterministically tokenize.
&lt;denchmark-link:https://github.com/google/sentencepiece/issues/371&gt;google/sentencepiece#371&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;path = gluon.utils.download('https://kobert.blob.core.windows.net/models/kogpt2/tokenizer/kogpt2_news_wiki_ko_cased_818bfa919d.spiece')
tok = nlp.data.SentencepieceTokenizer(path)
tok('안녕하세요.')  
['▁', '안', '녕', '하', '세', '요', '.']
tok = nlp.data.SentencepieceTokenizer(path, 0, 0.5)                                                                                                                                              
tok('안녕하세요.')                                                                                                                                                                               
['▁', '안', '녕', '하', '세요', '.']
tok('안녕하세요.')                                                                                                                                                                               
['▁안', '녕', '하', '세요', '.']
tok('안녕하세요.')                                                                                                                                                                               
['▁안녕', '하', '세요', '.']
tok('안녕하세요.')                     
tok = nlp.data.SentencepieceTokenizer(path, num_best=0, alpha=0)
tok('안녕하세요.')                                                                                                                                                                               
['▁안녕하세요', '.']
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='haven-jeon' date='2020-06-08T17:46:45Z'>
		agreed. feel free to propose a PR to update this.
		</comment>
		<comment id='2' author='haven-jeon' date='2020-06-10T07:11:17Z'>
		Nice observation, I've fixed it in the new version: See &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/1225&gt;#1225&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>