<bug id='724' author='eric-haibin-lin' open_date='2019-05-23T06:46:27Z' closed_time='2020-10-15T20:16:10Z'>
	<summary>PrefetcherIter with worker_type='process' may hang</summary>
	<description>
I noticed that using PrefetcherIter with worker_type='process' may leave the prefetching process hang. Specifically, if I do
&lt;denchmark-code&gt;kill $PARENT_PID
&lt;/denchmark-code&gt;

I will observe that the parent process exits, and that child process still runs. And the child process runs till &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/src/gluonnlp/data/stream.py#L265-L271&gt;https://github.com/dmlc/gluon-nlp/blob/master/src/gluonnlp/data/stream.py#L265-L271&lt;/denchmark-link&gt;
 and block there because the parent never calls .
I'm a bit confused because the process is already marked as daemon=True, but it does not exit.
	</description>
	<comments>
		<comment id='1' author='eric-haibin-lin' date='2019-05-23T06:46:48Z'>
		&lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
 have u seen this before? Probably  sends , which is not handled and therefore  is not called (?)
		</comment>
		<comment id='2' author='eric-haibin-lin' date='2019-05-23T13:15:14Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 did you confirm the child processes exit correctly if you don't send TERM to the parent process but exit it via Python?
		</comment>
		<comment id='3' author='eric-haibin-lin' date='2019-05-23T13:17:03Z'>
		Maybe &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/blob/master/src/gluonnlp/data/stream.py#L265-L271&gt;https://github.com/dmlc/gluon-nlp/blob/master/src/gluonnlp/data/stream.py#L265-L271&lt;/denchmark-link&gt;
 catches the KeyboardInterrupt exception?
		</comment>
		<comment id='4' author='eric-haibin-lin' date='2019-05-24T01:20:53Z'>
		Yes if I press control+c it exits successfully
		</comment>
		<comment id='5' author='eric-haibin-lin' date='2019-05-24T01:22:08Z'>
		I was experimenting with SIGTERM  because that's the way openmpi kills a worker, which might be an issue if we use horovod for distributed training
		</comment>
	</comments>
</bug>