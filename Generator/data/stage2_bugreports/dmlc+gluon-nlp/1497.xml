<bug id='1497' author='rondogency' open_date='2021-01-19T21:44:23Z' closed_time='2021-01-22T05:03:45Z'>
	<summary>BERT pretraining loss and accuracy is not stabilized</summary>
	<description>
&lt;denchmark-h:h2&gt;Description&lt;/denchmark-h&gt;

I am using 8 node Horovod BERT pretraining on gluonnlp==0.10.0, currently running for 10k steps and found that on each run the final loss and accuracy after 10k steps are not stabilized and mlm loss varies a lot.
So I am wondering if there is a stable way to generate loss and accuracy for pretraining
&lt;denchmark-h:h3&gt;Error Message&lt;/denchmark-h&gt;

There is 4 runs logs
&lt;denchmark-code&gt;[1,0]&lt;stdout&gt;:[step 10000]#011mlm_loss=2.76460#011mlm_acc=51.87#011nsp_loss= 0.08#011nsp_acc=96.88#011throughput=6.6K tks/s#011lr=0.0002000 time=0.57, latency=574.0 ms/step
[1,0]&lt;stdout&gt;:[step 10000]#011mlm_loss=1.66976#011mlm_acc=69.06#011nsp_loss= 0.07#011nsp_acc=96.88#011throughput=7.3K tks/s#011lr=0.0002000 time=0.54, latency=541.5 ms/step
[1,0]&lt;stdout&gt;:[step 10000]#011mlm_loss=2.61692#011mlm_acc=51.15#011nsp_loss= 0.02#011nsp_acc=100.00#011throughput=6.6K tks/s#011lr=0.0002000 time=0.53, latency=530.5 ms/step
[1,0]&lt;stdout&gt;:[step 10000]#011mlm_loss=2.25963#011mlm_acc=57.22#011nsp_loss= 0.11#011nsp_acc=93.75#011throughput=6.5K tks/s#011lr=0.0002000 time=0.55, latency=547.6 ms/step
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;To Reproduce&lt;/denchmark-h&gt;

I am using Sagemaker to trigger Horovod, and shared config will be
&lt;denchmark-code&gt;    instance_count = 8
    instance_type = "ml.p3dn.24xlarge"
    SM_DATA_ROOT = '/opt/ml/input/data/train'
    hyperparameters={
        "data": '/'.join([SM_DATA_ROOT, 'bert/train']),
        "data_eval": '/'.join([SM_DATA_ROOT, 'bert/eval']),
        "ckpt_dir": '/'.join([SM_DATA_ROOT, 'ckpt_dir']),
        "comm_backend": "smddp",
        "model": "bert_24_1024_16",
        "total_batch_size": instance_count * 256,
        "total_batch_size_eval": instance_count * 256,
        "max_seq_length": 128,
        "max_predictions_per_seq": 20,
        'log_interval': 1,
        "lr": 0.0002,
        "num_steps": 10000,
        'warmup_ratio': 1,
        "raw": '',
    }
    distribution = {'mpi': {'enabled': True, "custom_mpi_options": "-verbose --NCCL_DEBUG=INFO"}}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Steps to reproduce&lt;/denchmark-h&gt;

(Paste the commands you ran that produced the error.)




&lt;denchmark-h:h2&gt;What have you tried to solve it?&lt;/denchmark-h&gt;





&lt;denchmark-h:h2&gt;Environment&lt;/denchmark-h&gt;

We recommend using our script for collecting the diagnositc information. Run the following command and paste the outputs below:
&lt;denchmark-code&gt;curl --retry 10 -s https://raw.githubusercontent.com/dmlc/gluon-nlp/master/tools/diagnose.py | python

# paste outputs here
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='rondogency' date='2021-01-20T20:46:36Z'>
		At the moment there isn't a good way to enforce reproducibility when using distributed training due to the different compute orders. While in the current setting complete reproducibility is hard to obtain, potential ideas for reducing variance is to 1) initialize once and store the initial random weight, and then always train from that set of weights, and 2) enforce sample ordering in data loader by processing and storing processed samples beforehand.
		</comment>
		<comment id='2' author='rondogency' date='2021-01-21T16:18:47Z'>
		&lt;denchmark-link:https://github.com/rondogency&gt;@rondogency&lt;/denchmark-link&gt;
 Has the training been stabilized after you have fixed the random seed?
		</comment>
		<comment id='3' author='rondogency' date='2021-01-22T04:57:13Z'>
		&lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 thanks for the answer!
&lt;denchmark-link:https://github.com/sxjscience&gt;@sxjscience&lt;/denchmark-link&gt;
 yes, I am using what we have in gluoncv &lt;denchmark-link:https://github.com/dmlc/gluon-cv/blob/master/gluoncv/utils/random.py&gt;https://github.com/dmlc/gluon-cv/blob/master/gluoncv/utils/random.py&lt;/denchmark-link&gt;
, now the loss is more stable now (less variance from multiple runs)
suggest to add same thing to gluonnlp and allow user to pass random seed to bert run_pretraining.py script
		</comment>
		<comment id='4' author='rondogency' date='2021-01-22T05:03:45Z'>
		Thanks, we will ensure that all scripts use this set_seed function: 


gluon-nlp/src/gluonnlp/utils/misc.py


        Lines 187 to 191
      in
      09f3435






 def set_seed(seed): 



 import mxnet as mx 



 mx.random.seed(seed) 



 np.random.seed(seed) 



 random.seed(seed) 





I'll close this issue for now as it should have been solved in the master version.
Feel free to reopen.
		</comment>
	</comments>
</bug>