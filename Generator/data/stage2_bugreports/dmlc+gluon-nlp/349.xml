<bug id='349' author='eric-haibin-lin' open_date='2018-09-26T05:51:23Z' closed_time='2018-09-28T13:05:14Z'>
	<summary>Transformer script doesn't run with py3</summary>
	<description>
&lt;denchmark-code&gt;MXNET_GPU_MEM_POOL_TYPE=Round python3 train_transformer.py --dataset WMT2014BPE                        -$
src_lang en --tgt_lang de --batch_size 2700                        --optimizer adam --num_accumulated 16 --lr 2.0 --warmup_steps 4000                        --save_dir transformer_$
n_de_u512 --epochs 30 --gpus 0,1,2,3,4,5,6,7 --scaled                        --average_start 5 --num_buckets 20 --bucket_scheme exp --bleu 13a --log_interval 10 2&gt;&amp;1 | tee base.log
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;2018-09-26 05:38:28,761 - root - Namespace(average_checkpoint=False, average_start=5, batch_size=2700, beam_size=4, bleu='13a', bucket_ratio=0.0, bucket_scheme='exp', dataset='WMT2$
14BPE', dropout=0.1, epochs=30, epsilon=0.1, full=False, gpus='0,1,2,3,4,5,6,7', hidden_size=2048, log_interval=10, lp_alpha=0.6, lp_k=5, lr=2.0, magnitude=3.0, num_accumulated=16,
num_averages=5, num_buckets=20, num_heads=8, num_layers=6, num_units=512, optimizer='adam', save_dir='transformer_en_de_u512', scaled=True, src_lang='en', src_max_len=-1, test_batc$
_size=256, tgt_lang='de', tgt_max_len=-1, warmup_steps=4000.0)
All Logs will be saved to transformer_en_de_u512/train_transformer.log
Load cached data from /home/ubuntu/transformer/scripts/nmt/cached/WMT2014BPE_en_de_-1_-1_train.npz
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py", line 650, in read_array
    array = pickle.load(fp, **pickle_kwargs)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xfa in position 8: ordinal not in range(128)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train_transformer.py", line 304, in &lt;module&gt;
    = load_translation_data(dataset=args.dataset, src_lang=args.src_lang, tgt_lang=args.tgt_lang)
  File "train_transformer.py", line 260, in load_translation_data
    data_train_processed = load_cached_dataset(common_prefix + '_train')
  File "train_transformer.py", line 167, in load_cached_dataset
    return ArrayDataset(np.array(dat['src_data']), np.array(dat['tgt_data']))
  File "/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py", line 235, in __getitem__
    pickle_kwargs=self.pickle_kwargs)
  File "/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py", line 656, in read_array
    "to numpy.load" % (err,))
UnicodeError: Unpickling a python object failed: UnicodeDecodeError('ascii', b'\x07 \x00\x00ls\x00\x00\xfab\x00\x00k]\x00\x00`p\x00\x00fi\x00\x00\x03\x00\x00\x00', 8, 9, 'ordinal n$
t in range(128)')
You may need to pass the encoding= option to numpy.load
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='eric-haibin-lin' date='2018-09-26T06:31:24Z'>
		&lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 What is your default system encoding format? You can use the following code:
import sys 
sys.defaultencoding()
Also, did you use py2 to create the cached processed dataset?
		</comment>
		<comment id='2' author='eric-haibin-lin' date='2018-09-26T17:20:48Z'>
		Yes I used py2 to create the cached processed dataset and realized that I don't have mxnet installed for py2. That's why I switched.
I can run the code with py2 for a while after I installed mxnet, and then I encountered:
&lt;denchmark-code&gt;2018-09-26 06:36:20,327 - root - [Epoch 0 Batch 5920/7679] loss=7.3885, ppl=1617.2688, throughput=114.00K wps, wc=5780.56K
2018-09-26 06:37:11,299 - root - [Epoch 0 Batch 6080/7679] loss=7.3100, ppl=1495.1115, throughput=113.51K wps, wc=5785.66K
2018-09-26 06:38:01,756 - root - [Epoch 0 Batch 6240/7679] loss=7.2699, ppl=1436.3710, throughput=114.49K wps, wc=5776.66K
2018-09-26 06:38:52,638 - root - [Epoch 0 Batch 6400/7679] loss=7.2225, ppl=1369.8938, throughput=113.76K wps, wc=5788.13K
2018-09-26 06:39:43,664 - root - [Epoch 0 Batch 6560/7679] loss=7.2048, ppl=1345.8282, throughput=113.12K wps, wc=5772.16K
2018-09-26 06:40:33,210 - root - [Epoch 0 Batch 6720/7679] loss=7.1519, ppl=1276.5605, throughput=116.81K wps, wc=5787.53K
2018-09-26 06:41:25,057 - root - [Epoch 0 Batch 6880/7679] loss=7.1224, ppl=1239.4275, throughput=111.62K wps, wc=5787.31K
2018-09-26 06:42:14,961 - root - [Epoch 0 Batch 7040/7679] loss=7.1001, ppl=1212.0946, throughput=115.85K wps, wc=5781.56K
2018-09-26 06:43:04,671 - root - [Epoch 0 Batch 7200/7679] loss=7.0639, ppl=1168.9431, throughput=116.21K wps, wc=5777.01K
2018-09-26 06:43:56,381 - root - [Epoch 0 Batch 7360/7679] loss=7.0212, ppl=1120.1717, throughput=111.99K wps, wc=5791.21K
2018-09-26 06:44:47,374 - root - [Epoch 0 Batch 7520/7679] loss=7.0003, ppl=1096.9319, throughput=113.43K wps, wc=5783.88K
Traceback (most recent call last):
  File "train_transformer.py", line 632, in &lt;module&gt;
    train()
  File "train_transformer.py", line 577, in train
    bpe=bpe)
  File "/home/ubuntu/transformer/scripts/nmt/bleu.py", line 209, in compute_bleu
    'references and translation should have format of list(list(str)) ' \
AssertionError: references and translation should have format of list(list(str)) and list(str), respectively, when toknized is False.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='eric-haibin-lin' date='2018-09-26T18:03:46Z'>
		The UnicodeDecodeError problem seems due to the incompatibility of np.load between py2 and py3. If you use py3 to create the processed dataset, I think the error would disappear.
For the second problem, can you print out the type format of references and translations?
		</comment>
		<comment id='4' author='eric-haibin-lin' date='2018-09-26T18:06:36Z'>
		Should we alert the py2 users in code?
		</comment>
		<comment id='5' author='eric-haibin-lin' date='2018-09-26T21:39:36Z'>
		&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train_transformer.py", line 632, in &lt;module&gt;
    train()
  File "train_transformer.py", line 577, in train
    bpe=bpe)
  File "/home/ubuntu/transformer/scripts/nmt/bleu.py", line 210, in compute_bleu
    'and list(str), respectively, when toknized is False.', reference_corpus_list[0][0], translation_corpus[0])
AssertionError: ('references and translation should have format of list(list(str)) and list(str), respectively, when toknized is False.', u'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', u'Die Kommission ist die Union.')
&lt;/denchmark-code&gt;

I guess it's just another encoding error. I haven't applied sys.defaultencoding() yet
		</comment>
		<comment id='6' author='eric-haibin-lin' date='2018-09-27T03:39:04Z'>
		Yes, the second problem is also due to the encoding error. In py3, the bytes class isn't considered a string anymore.
		</comment>
		<comment id='7' author='eric-haibin-lin' date='2018-09-27T17:52:10Z'>
		How about consistently using UTF-8 by default? I believe all our Datasets default to UTF-8 encoding, even when read with Py2, so this would be consistent with the rest of the library?
		</comment>
	</comments>
</bug>