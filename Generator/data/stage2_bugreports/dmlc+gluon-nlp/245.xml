<bug id='245' author='leezu' open_date='2018-08-01T05:26:41Z' closed_time='2018-08-17T04:36:15Z'>
	<summary>Unify bptt_batchify interface between LanguageModelStream and LanguageModelDataset</summary>
	<description>
Currently there are at least the following discrepancies:

LanguageModelStream.bptt_batchify() returns a mask

The mask can be recovered by mask = data != vocab[vocab.padding_token]
We can either change the LanguageModelDataset.bptt_batchify() interface to also return a mask or use above formula to recover there mask where neede.
I vote for above formula, as it explicitly defines the mask and may be clearer to users. Users anyways need a line mask = mask.as_in_context(ctx) which could then just be replaced by mask = data != vocab[vocab.padding_token]. However, I'd also be ok with chaning the LanguageModelDataset.bptt_batchify().
LanguageModelStream.bptt_batchify() is smart and never includes &lt;eos&gt; in the data batch but only in the target  batch.
LanguageModelDataset.bptt_batchify() instead assumes the data doesn't contain &lt;bos&gt; but only eos.
See https://github.com/dmlc/gluon-nlp/blob/master/gluonnlp/data/stream.py#L415



&lt;denchmark-link:https://github.com/cgraywang&gt;@cgraywang&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='leezu' date='2018-08-01T06:00:44Z'>
		I think it would be good to make the interface and skip behavior consistent. On that, I think it's a good time to bring up again &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/128&gt;#128&lt;/denchmark-link&gt;
 (which was reverted in &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/134&gt;#134&lt;/denchmark-link&gt;
). The interface would become clean in two ways:

corpora are just corpora. they are arbitrary text and they don't have to have specific format for samples
batchify is a clean way to abstract the BPTT logic, as the function abstraction is already used such as Stack() and Pad(). If we follow this abstraction, we won't need to maintain separate functions for stream and dataset.

		</comment>
		<comment id='2' author='leezu' date='2018-08-01T18:46:33Z'>
		Based on offline discussion with Leo &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;
  , I think it is reasonable to align the bptt_batchify return value between datastream and corpus dataset. Then let's remove the mask as the return value, and only preserve data and target as the return value. Thanks &lt;denchmark-link:https://github.com/leezu&gt;@leezu&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='leezu' date='2018-08-01T18:52:56Z'>
		&lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/246&gt;#246&lt;/denchmark-link&gt;
 serves to illustrate the changes suggested here to unify the API (not taking into account &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/128&gt;#128&lt;/denchmark-link&gt;
, which indeed should be revisited, thanks &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
). &lt;denchmark-link:https://github.com/szha&gt;@szha&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/eric-haibin-lin&gt;@eric-haibin-lin&lt;/denchmark-link&gt;
 please double check and raise any concerns. I'm happy to adapt the code there if you prefer any other way to align the API.
		</comment>
		<comment id='4' author='leezu' date='2018-08-17T04:36:09Z'>
		Fixed by &lt;denchmark-link:https://github.com/dmlc/gluon-nlp/pull/282&gt;#282&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>