<bug id='1119' author='panchul' open_date='2020-10-08T17:48:05Z' closed_time='2020-10-23T13:12:56Z'>
	<summary>Bug with gpu support in pytorchserver</summary>
	<description>
/kind bug
What steps did you take and what happened:
Attempting to reproducing docs/samples/pytorch pytorchserver demo. Installed pytorchserver as per Readme in the python/pytorchserver, and ran the inference template as per Readme in docs/samples/pytorch/.
On a cluster with gpus, receiving an error from pytorchserver, location: /kfserving/python/pytorchserver/pytorchserver/model.py", line 67:
&lt;denchmark-code&gt;$ python3 -m pytorchserver --model_dir `pwd` --model_name pytorchmodel --model_class_name Net
[I 201008 17:15:32 storage:35] Copying contents of /home/azureuser/kfserving/docs/samples/pytorch to local
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/model.pt to pytorchmodel/model.pt
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/pytorch.yaml to pytorchmodel/pytorch.yaml
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/pytorchmodel to pytorchmodel/pytorchmodel
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/README.md to pytorchmodel/README.md
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/data to pytorchmodel/data
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/input.json to pytorchmodel/input.json
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/pytorch_gpu.yaml to pytorchmodel/pytorch_gpu.yaml
[I 201008 17:15:32 storage:205] Linking: /home/azureuser/kfserving/docs/samples/pytorch/cifar10.py to pytorchmodel/cifar10.py
[I 201008 17:15:34 kfserver:88] Registering model: pytorchmodel
[I 201008 17:15:34 kfserver:77] Listening on port 8080
[I 201008 17:15:34 kfserver:79] Will fork 0 workers
[I 201008 17:15:34 process:126] Starting 6 processes
[E 201008 17:18:28 web:1792] Uncaught exception POST /v1/models/pytorchmodel:predict (127.0.0.1)
    HTTPServerRequest(protocol='http', host='localhost:8080', method='POST', uri='/v1/models/pytorchmodel:predict', version='HTTP/1.1', remote_ip='127.0.0.1')
    Traceback (most recent call last):
      File "/home/azureuser/kfserving/python/pytorchserver/pytorchserver/model.py", line 67, in predict
        inputs = torch.tensor(request["instances"]).to(self.device)
      File "/anaconda/envs/py37_default/lib/python3.7/site-packages/torch/cuda/__init__.py", line 185, in _lazy_init
        "Cannot re-initialize CUDA in forked subprocess. " + msg)
    RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File "/anaconda/envs/py37_default/lib/python3.7/site-packages/tornado/web.py", line 1701, in _execute
        result = method(*self.path_args, **self.path_kwargs)
      File "/anaconda/envs/py37_default/lib/python3.7/site-packages/kfserving/handlers/http.py", line 58, in post
        response = model.predict(request)
      File "/home/azureuser/kfserving/python/pytorchserver/pytorchserver/model.py", line 70, in predict
        "Failed to initialize Torch Tensor from inputs: %s, %s" % (e, inputs))
    TypeError: Failed to initialize Torch Tensor from inputs: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, []
[E 201008 17:18:28 web:2250] 500 POST /v1/models/pytorchmodel:predict (127.0.0.1) 21.34ms
&lt;/denchmark-code&gt;

What did you expect to happen:
I expected the inference run through and print the results.
Anything else you would like to add:
&lt;denchmark-code&gt;$ nvidia-smi
Thu Oct  8 17:31:39 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |
| N/A   52C    P0    71W / 149W |    358MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   1489345      C   python3                           355MiB |
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

Environment:

Istio Version:
Knative Version:
KFServing Version:
Kubeflow version:
Kfdef:[k8s_istio/istio_dex/gcp_basic_auth/gcp_iap/aws/aws_cognito/ibm]
Minikube version: 1.13.1
Kubernetes version: (use kubectl version): 1.17.11 server, 1.19.2 client
OS (e.g. from /etc/os-release):  Ubuntu 18.04.4 LTS

	</description>
	<comments>
		<comment id='1' author='panchul' date='2020-10-08T17:48:12Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/docs
0.68


area/inference
1.00



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kfserving&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='panchul' date='2020-10-08T17:48:13Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/inference
1.00


area/docs
0.68



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kfserving&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='3' author='panchul' date='2020-10-08T22:39:18Z'>
		&lt;denchmark-link:https://github.com/panchul&gt;@panchul&lt;/denchmark-link&gt;
 I think that's multi processing issue, if you set  that forces to use 1 worker. We recently made a fix to default #worker to 1.
		</comment>
		<comment id='4' author='panchul' date='2020-10-13T16:48:20Z'>
		
that forces to use 1 worker

Thanks.
		</comment>
	</comments>
</bug>