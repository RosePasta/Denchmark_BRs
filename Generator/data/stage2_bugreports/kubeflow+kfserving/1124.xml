<bug id='1124' author='Peilun-Li' open_date='2020-10-09T18:52:19Z' closed_time='2020-12-03T14:15:01Z'>
	<summary>Unexpected (false-positive) model ready status when using batcher</summary>
	<description>
/kind bug
Usually when deploying kfserving endpoint, the inferenceservices won't be marked as ready=True (can be monitored through kubectl get inferenceservices or python sdk KFServing.get(watch=True)) until the model gets ready (i.e., warmup finished through load function). However, this is not the case when using batcher: the inferenceservices will be marked as ready as soon as the batcher sidecar is ready and all containers running, before the model warmup finished. So in case there's a bug in model load function, if using batcher, we'll get false-positive and thus can't get the correct ready status.
What steps did you take and what happened:
[A clear and concise description of what the bug is.]
Here I created a simple custom model docker (model will never be ready) and uploaded to lplrobert/kfserving-not-ready
&lt;denchmark-code&gt;# Dockerfile
FROM python:3.6-alpine

WORKDIR /app

COPY app.py /app

RUN apk update
RUN apk add make automake gcc g++ subversion python3-dev libffi-dev openssl-dev

RUN pip install kfserving==0.4.0

CMD ["python", "app.py"]
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# app.py
import kfserving
import time

class KFServingSampleModel(kfserving.KFModel):
    def __init__(self, name):
        super().__init__(name)
        self.ready = False

    def load(self):
        time.sleep(60)
        raise RuntimeError("Model load failed!")

    def predict(self, request):
        return request

if __name__ == "__main__":
    model = KFServingSampleModel("kfserving-custom-model")
    model.load()
    kfserving.KFServer(workers=1).start([model])
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;# custom.yaml
apiVersion: serving.kubeflow.org/v1alpha2
kind: InferenceService
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: kfserving-custom-model
spec:
  default:
    predictor:
      batcher:
        maxBatchSize: 32
        maxLatency: 5000
        timeout: 60
      custom:
        name: kfserving-custom-model
        container:
          image: lplrobert/kfserving-not-ready
&lt;/denchmark-code&gt;

If we apply above CRD (with batcher enabled) and watch at kubectl get inferenceservices, we'll see kfserving-custom-model gets ready pretty quickly
&lt;denchmark-code&gt;NAME                              URL                                                              READY   DEFAULT TRAFFIC   CANARY TRA
FFIC   AGE
kfserving-custom-model            http://kfserving-custom-model.namespace.example.com            True    100
       15s
&lt;/denchmark-code&gt;

Then become unready only after waiting for a few (&gt;2) minutes when the pod restarted and failed
&lt;denchmark-code&gt;NAME                              URL                                                              READY   DEFAULT TRAFFIC   CANARY TRA
FFIC   AGE
kfserving-custom-model                                                                             False
       2m15s
&lt;/denchmark-code&gt;

Then become ready again quickly when the pod retry again
&lt;denchmark-code&gt;NAME                              URL                                                              READY   DEFAULT TRAFFIC   CANARY TRA
FFIC   AGE
kfserving-custom-model            http://kfserving-custom-model.namespace.example.com            True    100
       2m27s
&lt;/denchmark-code&gt;

And after that, the ready status will change back and forth when the pod fails and restarts. In that case, we'll be misled to think the model works fine at first and can't be alerted until waiting for quite a long time.
While in comparison, if we apply the above CRD with batcher section removed. The inferenceservice will never be in ready status, and thus we know at first there's a bug somewhere.
What did you expect to happen:
If using batcher, inferenceservice should also return ready=True only if model warmup succeed, rather than when all containers are running.
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:

Istio Version:
Knative Version:
KFServing Version: 0.4.0
Kubeflow version:
Kfdef:[k8s_istio/istio_dex/gcp_basic_auth/gcp_iap/aws/aws_cognito/ibm]
Minikube version:
Kubernetes version: (use kubectl version):
OS (e.g. from /etc/os-release):

	</description>
	<comments>
		<comment id='1' author='Peilun-Li' date='2020-10-09T18:52:27Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/inference
0.99


area/engprod
0.99



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kfserving&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='Peilun-Li' date='2020-10-09T18:52:27Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/inference
0.99


area/engprod
0.99



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kfserving&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='3' author='Peilun-Li' date='2020-10-10T01:33:58Z'>
		/assign &lt;denchmark-link:https://github.com/zhangrongguo&gt;@zhangrongguo&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='Peilun-Li' date='2020-10-10T01:33:59Z'>
		&lt;denchmark-link:https://github.com/yuzisun&gt;@yuzisun&lt;/denchmark-link&gt;
: GitHub didn't allow me to assign the following users: zhangrongguo.
Note that only &lt;denchmark-link:https://github.com/orgs/kubeflow/people&gt;kubeflow members&lt;/denchmark-link&gt;
, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.
For more information please see &lt;denchmark-link:https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github&gt;the contributor guide&lt;/denchmark-link&gt;


In response to this:

/assign @zhangrongguo

Instructions for interacting with me using PR comments are available here.  If you have questions or suggestions related to my behavior, please file an issue against the kubernetes/test-infra repository.

		</comment>
		<comment id='5' author='Peilun-Li' date='2020-10-16T01:31:38Z'>
		&lt;denchmark-link:https://github.com/Peilun-Li&gt;@Peilun-Li&lt;/denchmark-link&gt;
  Is there any more detail? could you put the container log here?
		</comment>
		<comment id='6' author='Peilun-Li' date='2020-10-16T01:47:46Z'>
		
@Peilun-Li Is there any more detail? could you put the container log here?

The main container won't output anything until the runtime error (as can be inferred from the app.py)
batcher sidecar log:
&lt;denchmark-code&gt;{"level":"info","ts":1602812236.449479,"logger":"entrypoint","msg":"Starting","Port":"9082"}
{"level":"info","ts":1602812236.4495263,"logger":"entrypoint","msg":"Start Consume"}
2020/10/16 01:37:16.450 [I]  http server Running on http://:9082
&lt;/denchmark-code&gt;

inferenceservices status:
at first ready
&lt;denchmark-code&gt;    Conditions:
    Last Transition Time:  2020-10-16T01:37:20Z
    Status:                True
    Type:                  DefaultPredictorReady
    Last Transition Time:  2020-10-16T01:37:20Z
    Status:                True
    Type:                  Ready
    Last Transition Time:  2020-10-16T01:37:20Z
    Status:                True
    Type:                  RoutesReady
&lt;/denchmark-code&gt;

fallback to fail after 2 min
&lt;denchmark-code&gt;    Reason:                RevisionFailed
    Status:                False
    Type:                  DefaultPredictorReady
    Last Transition Time:  2020-10-16T01:39:18Z
    Message:               Failed to reconcile predictor
    Reason:                PredictorHostnameUnknown
    Status:                False
    Type:                  Ready
    Last Transition Time:  2020-10-16T01:39:18Z
    Message:               Failed to reconcile predictor
    Reason:                PredictorHostnameUnknown
    Status:                False
    Type:                  RoutesReady
&lt;/denchmark-code&gt;

Some noticable behavior:

The batcher won't be restarted during the CrashLoopBackOff.
The inferenceservice will get ready=True status immediately once all containers are running (pod ready 4/4)
The inferenceservice will get ready=False status immediately once main container throws error and restarts (pod ready 3/4 CrashLoopBackOff)

It seems like the batcher is not actually waiting until model is ready, through the HealthHandler.
		</comment>
		<comment id='7' author='Peilun-Li' date='2020-10-16T02:12:24Z'>
		can you put the description of the pod when the service is crash? kubectl describe pods -n $ns $podname
		</comment>
		<comment id='8' author='Peilun-Li' date='2020-10-16T02:14:46Z'>
		

@Peilun-Li Is there any more detail? could you put the container log here?

The main container won't output anything until the runtime error (as can be inferred from the app.py)
batcher sidecar log:
{"level":"info","ts":1602812236.449479,"logger":"entrypoint","msg":"Starting","Port":"9082"}
{"level":"info","ts":1602812236.4495263,"logger":"entrypoint","msg":"Start Consume"}
2020/10/16 01:37:16.450 [I]  http server Running on http://:9082

inferenceservices status:
at first ready
    Conditions:
    Last Transition Time:  2020-10-16T01:37:20Z
    Status:                True
    Type:                  DefaultPredictorReady
    Last Transition Time:  2020-10-16T01:37:20Z
    Status:                True
    Type:                  Ready
    Last Transition Time:  2020-10-16T01:37:20Z
    Status:                True
    Type:                  RoutesReady

fallback to fail after 2 min
    Reason:                RevisionFailed
    Status:                False
    Type:                  DefaultPredictorReady
    Last Transition Time:  2020-10-16T01:39:18Z
    Message:               Failed to reconcile predictor
    Reason:                PredictorHostnameUnknown
    Status:                False
    Type:                  Ready
    Last Transition Time:  2020-10-16T01:39:18Z
    Message:               Failed to reconcile predictor
    Reason:                PredictorHostnameUnknown
    Status:                False
    Type:                  RoutesReady

Some noticable behavior:

The batcher won't be restarted during the CrashLoopBackOff.
The inferenceservice will get ready=True status immediately once all containers are running (pod ready 4/4)
The inferenceservice will get ready=False status immediately once main container throws error and restarts (pod ready 3/4 CrashLoopBackOff)

It seems like the batcher is not actually waiting until model is ready, through the HealthHandler.

If you remove the batch section, the service will turn to false too?
		</comment>
		<comment id='9' author='Peilun-Li' date='2020-10-16T02:18:37Z'>
		
If you remove the batch section, the service will turn to false too?

If removing batch section, the service will never be in ready=True status (forever false from the very beginning)
		</comment>
		<comment id='10' author='Peilun-Li' date='2020-10-16T02:22:14Z'>
		
If removing batch section, the service will never be in ready=True status (forever false from the very beginning)

emm, I know what you are meaning now.  Thanks your report, I will fix this.
		</comment>
		<comment id='11' author='Peilun-Li' date='2020-10-16T02:22:39Z'>
		Here's the pod description
&lt;denchmark-code&gt;$ kubectl describe pods kfserving-custom-model-predictor-default-k5t6h-deployment-x2c6z
Name:         kfserving-custom-model-predictor-default-k5t6h-deployment-x2c6z
Namespace:    example_namespace
Priority:     0
Node:         ip-100-120-28-190.us-west-2.compute.internal/100.120.28.190
Start Time:   Thu, 15 Oct 2020 19:16:42 -0700
Labels:       app=kfserving-custom-model-predictor-default-k5t6h
              component=predictor
              controller-tools.k8s.io=1.0
              endpoint=default
              istio.io/rev=default
              model=kfserving-custom-model
              pod-template-hash=b7b599575
              security.istio.io/tlsMode=istio
              service.istio.io/canonical-name=kfserving-custom-model-predictor-default
              service.istio.io/canonical-revision=kfserving-custom-model-predictor-default-k5t6h
              serving.knative.dev/configuration=kfserving-custom-model-predictor-default
              serving.knative.dev/configurationGeneration=1
              serving.knative.dev/revision=kfserving-custom-model-predictor-default-k5t6h
              serving.knative.dev/revisionUID=1ebb5a12-5ae0-4b73-877b-c33bbd8b8fb7
              serving.knative.dev/service=kfserving-custom-model-predictor-default
              serving.kubeflow.org/inferenceservice=kfserving-custom-model
Annotations:  autoscaling.knative.dev/class: kpa.autoscaling.knative.dev
              autoscaling.knative.dev/minScale: 1
              autoscaling.knative.dev/target: 1
              internal.serving.kubeflow.org/batcher: true
              internal.serving.kubeflow.org/batcher-max-batchsize: 32
              internal.serving.kubeflow.org/batcher-max-latency: 5000
              internal.serving.kubeflow.org/batcher-timeout: 60
              kubernetes.io/limit-ranger: LimitRanger plugin set: memory request for container queue-proxy; cpu, memory limit for container queue-proxy
              kubernetes.io/psp: eks.privileged
              serving.knative.dev/creator: system:serviceaccount:kfserving-system:default
              sidecar.istio.io/status:
                {"version":"d36ff46d2def0caba37f639f09514b17c4e80078f749a46aae84439790d2b560","initContainers":["istio-init"],"containers":["istio-proxy"]...
Status:       Running
IP:           100.120.20.116
IPs:
  IP:           100.120.20.116
Controlled By:  ReplicaSet/kfserving-custom-model-predictor-default-k5t6h-deployment-b7b599575
Init Containers:
  istio-init:
    Container ID:  docker://9362953cf972fa077761243a0ec7e2f9228f6288f5a94a05d2d86c0eabd653e2
    Image:         docker.io/istio/proxyv2:1.6.1
    Image ID:      docker-pullable://istio/proxyv2@sha256:84e3afe9b4404ca94fd2e6e0277c642eb29b8b37ca46deff49dbe1f5e1b7fdc3
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Args:
      istio-iptables
      -p
      15001
      -z
      15006
      -u
      1337
      -m
      REDIRECT
      -i
      *
      -x

      -b
      *
      -d
      15090,15021,15020
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Thu, 15 Oct 2020 19:16:44 -0700
      Finished:     Thu, 15 Oct 2020 19:16:45 -0700
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     10m
      memory:  10Mi
    Environment:
      DNS_AGENT:
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-75kkk (ro)
Containers:
  user-container:
    Container ID:  docker://5427f0c6a0b8695db71d6d9d62a1e8792fa0d5d56efc0e56c25d5c6990499bc1
    Image:         index.docker.io/lplrobert/kfserving-not-ready@sha256:6f0c491ad90c32b51ab13106c032c6aa8ad8050ab6317b7a59360a02539ff851
    Image ID:      docker-pullable://lplrobert/kfserving-not-ready@sha256:6f0c491ad90c32b51ab13106c032c6aa8ad8050ab6317b7a59360a02539ff851
    Port:          9082/TCP
    Host Port:     0/TCP
    State:         Terminated
      Reason:      Error
      Message:     Traceback (most recent call last):
  File "app.py", line 18, in &lt;module&gt;
    model.load()
  File "app.py", line 11, in load
    raise RuntimeError("Model load failed!")
RuntimeError: Model load failed!

      Exit Code:  1
      Started:    Thu, 15 Oct 2020 19:17:47 -0700
      Finished:   Thu, 15 Oct 2020 19:18:48 -0700
    Last State:   Terminated
      Reason:     Error
      Message:    Traceback (most recent call last):
  File "app.py", line 18, in &lt;module&gt;
    model.load()
  File "app.py", line 11, in load
    raise RuntimeError("Model load failed!")
RuntimeError: Model load failed!

      Exit Code:    1
      Started:      Thu, 15 Oct 2020 19:16:46 -0700
      Finished:     Thu, 15 Oct 2020 19:17:46 -0700
    Ready:          False
    Restart Count:  1
    Limits:
      cpu:     1
      memory:  2Gi
    Requests:
      cpu:     1
      memory:  2Gi
    Environment:
      PORT:                      9082
      K_REVISION:                kfserving-custom-model-predictor-default-k5t6h
      K_CONFIGURATION:           kfserving-custom-model-predictor-default
      K_SERVICE:                 kfserving-custom-model-predictor-default
      K_INTERNAL_POD_NAME:       kfserving-custom-model-predictor-default-k5t6h-deployment-x2c6z (v1:metadata.name)
      K_INTERNAL_POD_NAMESPACE:  example_namespace (v1:metadata.namespace)
    Mounts:
      /var/log from knative-var-log (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-75kkk (ro)
  queue-proxy:
    Container ID:   docker://0c23786875850a012f44b711504b0736b67bc805fd24078852caa3a62ca62faf
    Image:          gcr.io/knative-releases/knative.dev/serving/cmd/queue@sha256:713bd548700bf7fe5452969611d1cc987051bd607d67a4e7623e140f06c209b2
    Image ID:       docker-pullable://gcr.io/knative-releases/knative.dev/serving/cmd/queue@sha256:713bd548700bf7fe5452969611d1cc987051bd607d67a4e7623e140f06c209b2
    Ports:          8022/TCP, 9090/TCP, 9091/TCP, 8012/TCP
    Host Ports:     0/TCP, 0/TCP, 0/TCP, 0/TCP
    State:          Running
      Started:      Thu, 15 Oct 2020 19:16:46 -0700
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:      25m
      memory:   128Mi
    Readiness:  exec [/ko-app/queue -probe-period 0] delay=0s timeout=10s period=1s #success=1 #failure=3
    Environment:
      SERVING_NAMESPACE:                      example_namespace
      SERVING_SERVICE:                        kfserving-custom-model-predictor-default
      SERVING_CONFIGURATION:                  kfserving-custom-model-predictor-default
      SERVING_REVISION:                       kfserving-custom-model-predictor-default-k5t6h
      QUEUE_SERVING_PORT:                     8012
      CONTAINER_CONCURRENCY:                  0
      REVISION_TIMEOUT_SECONDS:               60
      SERVING_POD:                            kfserving-custom-model-predictor-default-k5t6h-deployment-x2c6z (v1:metadata.name)
      SERVING_POD_IP:                          (v1:status.podIP)
      SERVING_LOGGING_CONFIG:                 {
                                                "level": "info",
                                                "development": false,
                                                "outputPaths": ["stdout"],
                                                "errorOutputPaths": ["stderr"],
                                                "encoding": "json",
                                                "encoderConfig": {
                                                  "timeKey": "ts",
                                                  "levelKey": "level",
                                                  "nameKey": "logger",
                                                  "callerKey": "caller",
                                                  "messageKey": "msg",
                                                  "stacktraceKey": "stacktrace",
                                                  "lineEnding": "",
                                                  "levelEncoder": "",
                                                  "timeEncoder": "iso8601",
                                                  "durationEncoder": "",
                                                  "callerEncoder": ""
                                                }
                                              }
      SERVING_LOGGING_LEVEL:
      SERVING_REQUEST_LOG_TEMPLATE:
      SERVING_REQUEST_METRICS_BACKEND:        prometheus
      TRACING_CONFIG_BACKEND:                 none
      TRACING_CONFIG_ZIPKIN_ENDPOINT:
      TRACING_CONFIG_STACKDRIVER_PROJECT_ID:
      TRACING_CONFIG_DEBUG:                   false
      TRACING_CONFIG_SAMPLE_RATE:             0.1
      USER_PORT:                              9082
      SYSTEM_NAMESPACE:                       knative-serving
      METRICS_DOMAIN:                         knative.dev/internal/serving
      DOWNWARD_API_LABELS_PATH:               /etc/podinfo/labels
      SERVING_READINESS_PROBE:                {"tcpSocket":{"port":9082,"host":"127.0.0.1"},"successThreshold":1}
      ENABLE_PROFILING:                       false
      SERVING_ENABLE_PROBE_REQUEST_LOG:       false
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-75kkk (ro)
  batcher:
    Container ID:  docker://90e50d6742bb149167289e4ae8fda221b36acb56be6de8bf911e05c018d2663b
    Image:         kfserving/batcher:v0.4.0
    Image ID:      docker-pullable://kfserving/batcher@sha256:5260a674f6241c4eead71a32813a6b7fca082827cf1a014ef6627357ba115477
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Args:
      --max-batchsize
      32
      --max-latency
      5000
      --timeout
      60
    State:          Running
      Started:      Thu, 15 Oct 2020 19:16:46 -0700
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  1Gi
    Requests:
      cpu:        1
      memory:     1Gi
    Environment:  &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-75kkk (ro)
  istio-proxy:
    Container ID:  docker://b0f41818bc02a3eaa1b55afdef06e5be0e4dc1e4cd9fd9c12a9bf70aaf4ed9fb
    Image:         docker.io/istio/proxyv2:1.6.1
    Image ID:      docker-pullable://istio/proxyv2@sha256:84e3afe9b4404ca94fd2e6e0277c642eb29b8b37ca46deff49dbe1f5e1b7fdc3
    Port:          15090/TCP
    Host Port:     0/TCP
    Args:
      proxy
      sidecar
      --domain
      $(POD_NAMESPACE).svc.cluster.local
      --serviceCluster
      kfserving-custom-model-predictor-default-k5t6h.$(POD_NAMESPACE)
      --proxyLogLevel=warning
      --proxyComponentLogLevel=misc:error
      --trust-domain=cluster.local
      --concurrency
      2
    State:          Running
      Started:      Thu, 15 Oct 2020 19:16:47 -0700
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     2
      memory:  1Gi
    Requests:
      cpu:      100m
      memory:   128Mi
    Readiness:  http-get http://:15021/healthz/ready delay=1s timeout=1s period=2s #success=1 #failure=30
    Environment:
      JWT_POLICY:                    third-party-jwt
      PILOT_CERT_PROVIDER:           istiod
      CA_ADDR:                       istiod.istio-system.svc:15012
      POD_NAME:                      kfserving-custom-model-predictor-default-k5t6h-deployment-x2c6z (v1:metadata.name)
      POD_NAMESPACE:                 example_namespace (v1:metadata.namespace)
      INSTANCE_IP:                    (v1:status.podIP)
      SERVICE_ACCOUNT:                (v1:spec.serviceAccountName)
      HOST_IP:                        (v1:status.hostIP)
      CANONICAL_SERVICE:              (v1:metadata.labels['service.istio.io/canonical-name'])
      CANONICAL_REVISION:             (v1:metadata.labels['service.istio.io/canonical-revision'])
      PROXY_CONFIG:                  {"proxyMetadata":{"DNS_AGENT":""}}

      ISTIO_META_POD_PORTS:          [
                                         {"name":"user-port","containerPort":9082,"protocol":"TCP"}
                                         ,{"name":"http-queueadm","containerPort":8022,"protocol":"TCP"}
                                         ,{"name":"http-autometric","containerPort":9090,"protocol":"TCP"}
                                         ,{"name":"http-usermetric","containerPort":9091,"protocol":"TCP"}
                                         ,{"name":"queue-port","containerPort":8012,"protocol":"TCP"}
                                     ]
      ISTIO_META_APP_CONTAINERS:     [
                                         user-container,
                                         queue-proxy,
                                         batcher
                                     ]
      ISTIO_META_CLUSTER_ID:         Kubernetes
      ISTIO_META_INTERCEPTION_MODE:  REDIRECT
      ISTIO_METAJSON_ANNOTATIONS:    {"autoscaling.knative.dev/class":"kpa.autoscaling.knative.dev","autoscaling.knative.dev/minScale":"1","autoscaling.knative.dev/target":"1","internal.serving.kubeflow.org/batcher":"true","internal.serving.kubeflow.org/batcher-max-batchsize":"32","internal.serving.kubeflow.org/batcher-max-latency":"5000","internal.serving.kubeflow.org/batcher-timeout":"60","kubernetes.io/limit-ranger":"LimitRanger plugin set: memory request for container queue-proxy; cpu, memory limit for container queue-proxy","kubernetes.io/psp":"eks.privileged","serving.knative.dev/creator":"system:serviceaccount:kfserving-system:default"}

      ISTIO_META_WORKLOAD_NAME:      kfserving-custom-model-predictor-default-k5t6h-deployment
      ISTIO_META_OWNER:              kubernetes://apis/apps/v1/namespaces/example_namespace/deployments/kfserving-custom-model-predictor-default-k5t6h-deployment
      ISTIO_META_MESH_ID:            cluster.local
      DNS_AGENT:
      ISTIO_KUBE_APP_PROBERS:        {}
    Mounts:
      /etc/istio/pod from istio-podinfo (rw)
      /etc/istio/proxy from istio-envoy (rw)
      /var/lib/istio/data from istio-data (rw)
      /var/run/secrets/istio from istiod-ca-cert (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-75kkk (ro)
      /var/run/secrets/tokens from istio-token (rw)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  knative-var-log:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  &lt;unset&gt;
  default-token-75kkk:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-75kkk
    Optional:    false
  istio-envoy:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  &lt;unset&gt;
  istio-data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  &lt;unset&gt;
  istio-podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      metadata.labels -&gt; labels
      metadata.annotations -&gt; annotations
  istio-token:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  43200
  istiod-ca-cert:
    Type:        ConfigMap (a volume populated by a ConfigMap)
    Name:        istio-ca-root-cert
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  online=true
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
                 online:NoSchedule
Events:
  Type     Reason     Age                  From                                                   Message
  ----     ------     ----                 ----                                                   -------
  Normal   Scheduled  2m18s                default-scheduler                                      Successfully assigned example_namespace/kfserving-custom-model-predictor-default-k5t6h-deployment-x2c6z to ip-100-120-28-190.us-west-2.compute.internal
  Normal   Pulling    2m17s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Pulling image "docker.io/istio/proxyv2:1.6.1"
  Normal   Pulled     2m16s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Successfully pulled image "docker.io/istio/proxyv2:1.6.1"
  Normal   Started    2m16s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Started container istio-init
  Normal   Created    2m16s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Created container istio-init
  Normal   Pulled     2m14s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Container image "kfserving/batcher:v0.4.0" already present on machine
  Normal   Pulling    2m14s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Pulling image "docker.io/istio/proxyv2:1.6.1"
  Normal   Started    2m14s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Started container batcher
  Normal   Created    2m14s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Created container batcher
  Normal   Pulled     2m14s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Container image "gcr.io/knative-releases/knative.dev/serving/cmd/queue@sha256:713bd548700bf7fe5452969611d1cc987051bd607d67a4e7623e140f06c209b2" already present on machine
  Normal   Started    2m14s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Started container queue-proxy
  Normal   Created    2m14s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Created container queue-proxy
  Normal   Pulled     2m13s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Successfully pulled image "docker.io/istio/proxyv2:1.6.1"
  Normal   Created    2m13s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Created container istio-proxy
  Normal   Started    2m13s                kubelet, ip-100-120-28-190.us-west-2.compute.internal  Started container istio-proxy
  Normal   Started    73s (x2 over 2m14s)  kubelet, ip-100-120-28-190.us-west-2.compute.internal  Started container user-container
  Normal   Created    73s (x2 over 2m15s)  kubelet, ip-100-120-28-190.us-west-2.compute.internal  Created container user-container
  Normal   Pulled     73s (x2 over 2m15s)  kubelet, ip-100-120-28-190.us-west-2.compute.internal  Container image "index.docker.io/lplrobert/kfserving-not-ready@sha256:6f0c491ad90c32b51ab13106c032c6aa8ad8050ab6317b7a59360a02539ff851" already present on machine
  Warning  BackOff    12s                  kubelet, ip-100-120-28-190.us-west-2.compute.internal  Back-off restarting failed container
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>