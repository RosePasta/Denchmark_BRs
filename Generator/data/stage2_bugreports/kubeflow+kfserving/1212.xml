<bug id='1212' author='iavinas' open_date='2020-11-17T04:36:25Z' closed_time='2020-11-30T06:14:05Z'>
	<summary>Multiple Worker on Pytorch GPU models</summary>
	<description>
/kind bug
What steps did you take and what happened:
[A clear and concise description of what the bug is.]
I want more GPU utilization for my Pytorch inference models using kfserving's Pytorchserver. I hardcoded workers=2 in KFServing's code and built docker again but it is not working. I am getting an error message
TypeError: Failed to initialize Torch Tensor from inputs: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, []
Is there any workaround to use to get more workers in Pytorchserver?
Currently, the model is using 1GB out of 16 GB GPU. The model is also using very less CPU.
What did you expect to happen:
More workers and better performance.
Anything else you would like to add:
[Miscellaneous information that will assist in solving the issue.]
Environment:

Istio Version:
Knative Version:
KFServing Version: 0.4
Kubeflow version:
Kfdef:[k8s_istio/istio_dex/gcp_basic_auth/gcp_iap/aws/aws_cognito/ibm]
Minikube version:
Kubernetes version: (use kubectl version):
OS (e.g. from /etc/os-release):

	</description>
	<comments>
		<comment id='1' author='iavinas' date='2020-11-17T12:10:55Z'>
		&lt;denchmark-link:https://github.com/iavinas&gt;@iavinas&lt;/denchmark-link&gt;
 KFServing is deprecating the existing pytorchserver, we prefer using torchserve instead and here are the torchserve &lt;denchmark-link:https://github.com/kubeflow/kfserving/tree/master/docs/samples/custom/torchserve&gt;custom examples&lt;/denchmark-link&gt;
 which I think should work with 0.4 as well, the native integration is still working in progress. &lt;denchmark-link:https://github.com/jagadeeshi2i&gt;@jagadeeshi2i&lt;/denchmark-link&gt;
 Do you have an example for pytorch GPU with multi workers?
		</comment>
		<comment id='2' author='iavinas' date='2020-11-17T16:50:11Z'>
		&lt;denchmark-link:https://github.com/iavinas&gt;@iavinas&lt;/denchmark-link&gt;
 Please refer &lt;denchmark-link:https://github.com/pytorch/serve/blob/master/docs/configuration.md&gt;https://github.com/pytorch/serve/blob/master/docs/configuration.md&lt;/denchmark-link&gt;
 for adding workers to config.properties . Refer &lt;denchmark-link:https://github.com/pytorch/serve/blob/master/README.md#start-torchserve-to-serve-the-model&gt;worker scaling&lt;/denchmark-link&gt;
 on how the workers are scaled.
		</comment>
	</comments>
</bug>