<bug id='1142' author='rtrobin' open_date='2020-10-19T13:45:51Z' closed_time='2020-10-27T07:21:11Z'>
	<summary>Custom image with pytorch or tensorflow model can't deploy on kfserving, but can deploy successfully as a normal pod.</summary>
	<description>
/kind bug
What steps did you take and what happened:
I build an image prediction flask app using pytorch. Gunicorn is used to launch multiple pytorch model instances to speedup inference procedure. It works fine on local machine, using docker run command.
When I deploy it on KFServing, the pod is successfully deployed. But python processes are repeatedly rebooted. The service can't run successfully. Also, It can be deployed successfully as a normal pod.
I record a gif to show my case.
&lt;denchmark-link:https://user-images.githubusercontent.com/8224989/96459059-3e7f1200-1254-11eb-94ee-1ad9dfe27526.gif&gt;&lt;/denchmark-link&gt;

What did you expect to happen:
Anything else you would like to add:
I pushed all my test code in my repo(&lt;denchmark-link:https://github.com/rtrobin/codeMixed/tree/master/server&gt;https://github.com/rtrobin/codeMixed/tree/master/server&lt;/denchmark-link&gt;
). By running , you can build my test docker. Also I pushed my docker into docker hub. You can pull it by  as you wish.
The kfserving yaml file is shown below:
apiVersion: serving.kubeflow.org/v1alpha2
kind: InferenceService
metadata:
  name: codemixed
spec:
  default:
    predictor:
      custom:
        container:
          image: rtrobin/server:test
          env:
            - name: WORKER
              value: "2"
          resources:
            limits:
              nvidia.com/gpu: "1"
            requests:
              nvidia.com/gpu: "1"

The normal pod delpoy yaml file is shown below:
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  restartPolicy: Never
  containers:
  - image: rtrobin/server:test
    env:
      - name: WORKER
        value: "2"
    name: torchmodel
    resources:
      limits:
        nvidia.com/gpu: 1
The server code is shown below:
import io
import json

import torch
from torchvision import models
import torchvision.transforms as transforms
from PIL import Image
from flask import Flask, jsonify, request


app = Flask(__name__)
imagenet_class_index = json.load(open('./imagenet_class_index.json'))
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = models.densenet121(pretrained=False).to(device)
model.eval()


def transform_image(image_bytes):
    my_transforms = transforms.Compose([transforms.Resize(255),
                                        transforms.CenterCrop(224),
                                        transforms.ToTensor(),
                                        transforms.Normalize(
                                            [0.485, 0.456, 0.406],
                                            [0.229, 0.224, 0.225])])
    image = Image.open(io.BytesIO(image_bytes))
    return my_transforms(image).unsqueeze(0)


def get_prediction(image_bytes):
    tensor = transform_image(image_bytes=image_bytes).to(device)
    with torch.no_grad():
        outputs = model.forward(tensor)
        _, y_hat = outputs.max(1)
        predicted_idx = str(y_hat.item())
    return imagenet_class_index[predicted_idx]


@app.route('/predict', methods=['POST'])
def predict():
    if request.method == 'POST':
        file = request.files['file']
        img_bytes = file.read()
        class_id, class_name = get_prediction(image_bytes=img_bytes)
        return jsonify({'class_id': class_id, 'class_name': class_name})


if __name__ == '__main__':
    app.run()
The gunicorn config file is shown below:
# gunicorn.conf

import os

workers = int(os.environ.get('WORKER', '4'))
bind = '0.0.0.0:' + os.environ.get('PORT', '5000')
daemon = 'false'
worker_connections = 2000
The dockerfile is shown below:
FROM pytorch/pytorch:1.6.0-cuda10.1-cudnn7-runtime

LABEL maintainer="robintang.116@gmail.com"

RUN pip install flask gunicorn gevent \
        --no-cache-dir \
        -i https://mirrors.aliyun.com/pypi/simple

ENV LANG C.UTF-8

COPY ./ /workspace/server
WORKDIR /workspace/server

ENV PORT=5000
ENV WORKER=4

CMD exec gunicorn -c gunicorn.py main:app
Environment:

Istio Version:
Knative Version:
KFServing Version: 0.4.0
Kubeflow version:
Kfdef:[k8s_istio/istio_dex/gcp_basic_auth/gcp_iap/aws/aws_cognito/ibm]
Minikube version:
Kubernetes version: (use kubectl version):
OS (e.g. from /etc/os-release):

	</description>
	<comments>
		<comment id='1' author='rtrobin' date='2020-10-19T13:46:03Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/inference
0.64



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kfserving&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='2' author='rtrobin' date='2020-10-19T13:46:03Z'>
		Issue-Label Bot is automatically applying the labels:



Label
Probability




area/inference
0.64



Please mark this comment with  or  to give our bot feedback!
Links: &lt;denchmark-link:https://github.com/marketplace/issue-label-bot&gt;app homepage&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://label-bot-prod.mlbot.net/data/kubeflow/kfserving&gt;dashboard&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/hamelsmu/MLapp&gt;code&lt;/denchmark-link&gt;
 for this bot.
		</comment>
		<comment id='3' author='rtrobin' date='2020-10-24T15:03:19Z'>
		We have a custom image with tensorflow model. It acts very like this: it can successfully run as a normal pod, but fail to run as inferenceservice. It reports error shown below:
 W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at save_srestore_v2_ops.cc:184 : Invalid argument: resources/models/merged_model_cx_sub/variables/variables.data-00000-of-00001; Bad address
It is confused what the difference is between deploy as pod and inferenceservice? And why it acts differently.
		</comment>
		<comment id='4' author='rtrobin' date='2020-10-27T07:21:11Z'>
		Ref to discussion &lt;denchmark-link:https://github.com/kubeflow/kfserving/issues/1074#issuecomment-717040382&gt;here&lt;/denchmark-link&gt;
, the pod runs OOM. Adjusting memory limit could fix the issue.
		</comment>
	</comments>
</bug>