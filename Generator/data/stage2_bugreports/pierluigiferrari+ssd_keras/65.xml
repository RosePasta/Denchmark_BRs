<bug id='65' author='nunotrancoso' open_date='2018-02-07T16:12:53Z' closed_time='2018-02-09T09:53:38Z'>
	<summary>weight sampling example</summary>
	<description>
Hi, i've tried to follow your weight sampling example, but i'm stuck with this error:
ValueError: Dimension 0 in both shapes must be equal, but are 3 and 324 for 'Assign_49' (op: 'Assign') with input shapes: [3,3,512,12], [324,512,3,3].
At first i thought it was because i was trying to adapt the example to SDD512, but a clean copy/paste of the original example ends up with same error. It looks like the vector is backwards, but i'm not yet fluent enough in Python/Keras/Tensorflow to actually figure where it might be happening ...
I wasn't using the original ipynb, just a local file, but should be pretty much the same as it was a plain copy/paste...
&lt;denchmark-code&gt;import h5py
import numpy as np
import shutil

from tensor_sampling_utils import sample_tensors

# TODO: Set the path for the source weights file you want to load.

weights_source_path = 'VGG_coco_SSD_300x300_iter_400000.h5'

# TODO: Set the path and name for the destination weights file
#       that you want to create.

# *** New class number ***
num_classes=2

weights_destination_path = 'VGG_coco_SSD_300x300_iter_400000_subsampled_'+str(num_classes)+'_classes.h5'

# Make a copy of the weights file.
shutil.copy(weights_source_path, weights_destination_path)

# Load both the source weights file and the copy we made.
# We will load the original weights file in read-only mode so that we can't mess up anything.
weights_source_file = h5py.File(weights_source_path, 'r')
weights_destination_file = h5py.File(weights_destination_path)

classifier_names = ['conv4_3_norm_mbox_conf',
                    'fc7_mbox_conf',
                    'conv6_2_mbox_conf',
                    'conv7_2_mbox_conf',
                    'conv8_2_mbox_conf',
                    'conv9_2_mbox_conf']

# *** Output the original model shape ***

conv4_3_norm_mbox_conf_kernel = weights_source_file[classifier_names[0]][classifier_names[0]]['kernel:0']
conv4_3_norm_mbox_conf_bias = weights_source_file[classifier_names[0]][classifier_names[0]]['bias:0']

print("Shape of the '{}' weights:".format(classifier_names[0]))
print()
print("kernel:\t", conv4_3_norm_mbox_conf_kernel.shape)
print("bias:\t", conv4_3_norm_mbox_conf_bias.shape)

# *** Preset paramas for the new model ***

# Number of classes that the model was trained for
n_classes_source = 81
# Number of classes we want to re-train for
# n_classes_dest = 2
# We add 1 because 'backgound' is a class too, 0 to be more precise
# classes_of_interest = list(range(n_classes_dest+1))

# The subset of classes we want to extract from our source model
# 0 SHOULD carried over to the new model.

classes_of_interest=[0,3,8]

subsampling_indices = []
for i in range(int(324/n_classes_source)):
    indices = np.array(classes_of_interest) + i * n_classes_source
    subsampling_indices.append(indices)
subsampling_indices = list(np.concatenate(subsampling_indices))

print(subsampling_indices)

for name in classifier_names:
    # Get the trained weights for this layer from the source HDF5 weights file.
    kernel = weights_source_file[name][name]['kernel:0'].value
    bias = weights_source_file[name][name]['bias:0'].value

    # Get the shape of the kernel. We're interested in sub-sampling
    # the last dimension, 'o'.
    height, width, in_channels, out_channels = kernel.shape
    
    # Compute the indices of the elements we want to sub-sample.
    # Keep in mind that each classification predictor layer predicts multiple
    # bounding boxes for every spatial location, so we want to sub-sample
    # the relevant classes for each of these boxes.
    subsampling_indices = []
    for i in range(int(out_channels/n_classes_source)):
        indices = np.array(classes_of_interest) + i * n_classes_source
        subsampling_indices.append(indices)
    subsampling_indices = list(np.concatenate(subsampling_indices))
    
    # Sub-sample the kernel and bias.
    # The `sample_tensors()` function used below provides extensive
    # documentation, so don't hesitate to read it if you want to know
    # what exactly is going on here.
    new_kernel, new_bias = sample_tensors(weights_list=[kernel, bias],
                                          sampling_instructions=[height, width, in_channels, subsampling_indices],
                                          axes=[[3]], # The one bias dimension corresponds to the last kernel dimension.
                                          init=['gaussian', 'zeros'],
                                          mean=0.0,
                                          stddev=0.005)
    
    # Delete the old weights from the destination file.
    del weights_destination_file[name][name]['kernel:0']
    del weights_destination_file[name][name]['bias:0']
    # Create new datasets for the sub-sampled weights.
    weights_destination_file[name][name].create_dataset(name='kernel:0', data=new_kernel)
    weights_destination_file[name][name].create_dataset(name='bias:0', data=new_bias)

# *** Print the new model shape ***

conv4_3_norm_mbox_conf_kernel = weights_destination_file[classifier_names[0]][classifier_names[0]]['kernel:0']
conv4_3_norm_mbox_conf_bias = weights_destination_file[classifier_names[0]][classifier_names[0]]['bias:0']

print("Shape of the '{}' weights:".format(classifier_names[0]))
print()
print("kernel:\t", conv4_3_norm_mbox_conf_kernel.shape)
print("bias:\t", conv4_3_norm_mbox_conf_bias.shape)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='nunotrancoso' date='2018-02-07T16:38:10Z'>
		Did you set the correct number of classes when you created the model into which you tried to load the sub-sampled weights? You can see in the error that the number of output channels of the two tensors already doesn't match (12 vs 324). It seems as if you tried to load the sub-sampled weights into a model with 80 classes.
		</comment>
		<comment id='2' author='nunotrancoso' date='2018-02-07T16:55:14Z'>
		Think so, on the SDD300 one it was actually all that i changed from your ipynb, ie, the number of classses, to match the ones in the new model... At first i thought i might be loading the model and not just the weights, but nope.
&lt;denchmark-code&gt;from keras.optimizers import Adam
from keras import backend as K
from keras.models import load_model

from keras_ssd300 import ssd_300
from keras_ssd_loss import SSDLoss
from keras_layer_AnchorBoxes import AnchorBoxes
from keras_layer_L2Normalization import L2Normalization
from ssd_box_encode_decode_utils import decode_y, decode_y2
from ssd_batch_generator import BatchGenerator

img_height = 300 # Height of the input images
img_width = 300 # Width of the input images
img_channels = 3 # Number of color channels of the input images
subtract_mean = [123, 117, 104] # The per-channel mean of the images in the dataset

# The color channel order in the original SSD is BGR, so we should set this to `True`,
# but weirdly the results are better without swapping.
swap_channels = True

# Number of positive classes in the new model
n_classes = 2

# The anchor box scaling factors used in the original SSD300 for the MS COCO datasets.
scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]

# The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets.
# scales = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05]

# The anchor box aspect ratios used in the original SSD300; the order matters
aspect_ratios_per_layer=[[1.0, 2.0, 0.5],
                        [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                        [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                        [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                        [1.0, 2.0, 0.5],
                        [1.0, 2.0, 0.5]]
two_boxes_for_ar1 = True
steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.
offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.
limit_boxes = False # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries
variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation
coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids', 'corners', or 'minmax' format, see documentation
normalize_coords = True

# *** Build the Keras  model ***

K.clear_session() # Clear previous models from memory.

model = ssd_300(image_size=(img_height, img_width, img_channels),
                n_classes=n_classes,
                l2_regularization=0.0005,
                scales=scales,
                aspect_ratios_per_layer=aspect_ratios_per_layer,
                two_boxes_for_ar1=two_boxes_for_ar1,
                steps=steps,
                offsets=offsets,
                limit_boxes=limit_boxes,
                variances=variances,
                coords=coords,
                normalize_coords=normalize_coords,
                subtract_mean=subtract_mean,
                divide_by_stddev=None,
                swap_channels=swap_channels)

print("Model built")



# *** Load the sub-sampled weights into the model ***

# Load the weights that we've just created via sub-sampling.
weights_path = 'VGG_coco_SSD_300x300_iter_400000_subsampled_2_classes.h5'
model.load_weights(weights_path, by_name=True)
print("Weights file loaded : ", weights_path)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='nunotrancoso' date='2018-02-07T17:24:08Z'>
		In any case, one of those two tensors still represents 80 classes. I can't tell from the error message whether it's the kernel tensor of the model you created or the kernel of the weights file that you're trying to load into the model, but either the weights you're trying to load weren't actually sub-sampled to 2 classes or the model you're trying to load them into wasn't created with 2 classes.
I would recommend that you check the shapes of all classifier layers after the sub-sampling following the example in the tutorial where it does this check for the 'conv4_3_norm_mbox_conf' layer. Do the same for the other five classifier layers to see whether all shapes are correct. The number of output channels should be either 12 or 18 for all classifier layers.
Next, check the shapes of the weight of all the classifier layers for the model into which you want to load the weights. You can do that by inspecting model.get_layer('layer_name').get_weights(), which returns a list of Numpy arrays containing the weights for the respective layer.
Somewhere along the way will be the answer to what's going on.
Also, if you post code, please make sure it's formatted properly. What you posted above is an absolute pain in the ass to read 🙂. I also don't understand why you don't use the .ipynb. You said you used a "local" file instead, but there's nothing non-local about the Jupyter notebook. I know you said you did a plain cop-and-paste, but it's still a potential source of error.
		</comment>
		<comment id='4' author='nunotrancoso' date='2018-02-09T09:10:37Z'>
		As a follow up, on a "gut feeling" added a .flush() after the write and... it just started working as it should. My guess is that the Python shell signals the script execution as "done" while some background I/O might actually still be ongoing. On a ipynb it wouldn't make a difference because the UPDATED file probably still is in the process cache, but if you're running it as two separate scripts back to back, the second one might start reading the file before the first actually ends it's update.
&lt;denchmark-code&gt;# Create new datasets for the sub-sampled weights.
weights_destination_file[name][name].create_dataset(name='kernel:0', data=new_kernel)
weights_destination_file[name][name].create_dataset(name='bias:0', data=new_bias)
# Flush buffers just in case
weights_destination_file.flush()
&lt;/denchmark-code&gt;

Also added this little bit to the box drawing code, just a little check in case we subsampled to less classes than the original:
&lt;denchmark-code&gt;for box in y_true[i]:
    class_id = box[0]
    # Check if the index still exists in our new class list
    if class_id&lt;len(classes) :
&lt;/denchmark-code&gt;

Sorry about the code format, thought the code tags would do, like would, but seems not :)
As for why not use ipynb, well, eventually i'll end up using parts of the code on my own stuff so it's just an early decoupling. While i see ipynb's value as a showcase tool, for me it's just a PITA because if/when i want to use of some of the code, it's copy/paste time, and as you mentioned, potential source of error...
Anyway, thx for your work on this implementation of SSD for keras, it's heaven sent :) Any plans to extended it by say, adding classifier options like MobileNet/Inceptionv3?
p.s. just figured how the code formating actually works here, updating...
		</comment>
		<comment id='5' author='nunotrancoso' date='2018-02-09T20:39:43Z'>
		Thanks a lot for pointing this out! So it was actually a bug, at least kind of. People using the notebook just didn't run into it I suppose. I added the flush statement to the Jupyter notebook so that this issue can't happen anymore.
I also fixed another bug by the way, random sampling threw an error. The sample_tensors() function worked fine, but the code snipped in the Jupyter notebook had a bug when you wanted to use random sampling instead of providing an explicit list of indices. Now it works.
As for expanding to other base network architectures: That is very much the plan, especially the two you mentioned (MobileNet and InceptionV3), but also other, even more recent architectures.
		</comment>
		<comment id='6' author='nunotrancoso' date='2018-04-12T03:05:07Z'>
		ValueError: Dimension 2 in both shapes must be equal, but are 300 and
		</comment>
		<comment id='7' author='nunotrancoso' date='2018-04-12T03:05:47Z'>
		ValueError: Dimension 2 in both shapes must be equal, but are 300 and 3. Shapes are [3,3,300,64] and [3,3,3,64]. for 'Assign' (op: 'Assign') with input shapes: [3,3,300,64], [3,3,3,64].
		</comment>
		<comment id='8' author='nunotrancoso' date='2018-04-13T12:26:43Z'>
		Hello, thanks for this excellent work. we run the ssd300 successfully, but when we run the ssd512 with your pre-trained weights,  this issue occurred:

The full description as follow:
&lt;denchmark-link:https://user-images.githubusercontent.com/22444809/38734575-b589bfac-3f58-11e8-8695-9e83a78a6561.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/22444809/38734598-c764f5e8-3f58-11e8-903a-b898e32e968c.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/22444809/38734610-ce7c47be-3f58-11e8-81a2-da1d1bac94ae.png&gt;&lt;/denchmark-link&gt;

We checked the weights you supplied and nothing is wrong... the shape of weights is [3, 3, 512, 84] but the error tells the shape is inverted.
We set load_weights(skip_mismatch=True), it shows that all of conv_loc and conv_conf have the shape problem, but we are confused about how it happened...
&lt;denchmark-link:https://user-images.githubusercontent.com/22444809/38734526-92f95ede-3f58-11e8-8380-d4528c5482d4.png&gt;&lt;/denchmark-link&gt;

Looking forward to your reply :)
		</comment>
		<comment id='9' author='nunotrancoso' date='2018-04-13T13:14:10Z'>
		&lt;denchmark-link:https://github.com/yf704475209&gt;@yf704475209&lt;/denchmark-link&gt;
 It looks like you're trying to load weights that were sub-sampled to 1 class into a model that was built to predict 20 classes. You have to build the model with the right number of classes.
		</comment>
		<comment id='10' author='nunotrancoso' date='2018-05-06T14:59:01Z'>
		Were you able to fix this &lt;denchmark-link:https://github.com/yf704475209&gt;@yf704475209&lt;/denchmark-link&gt;
? I'm getting the exact same error - trying to train a 1 class detector. I have double checked that the number of classes used when subsampling () the weights is the same number as defined in the model ( - because the  constructor adds 1 to it).
		</comment>
		<comment id='11' author='nunotrancoso' date='2018-05-06T15:44:08Z'>
		&lt;denchmark-link:https://github.com/shivam6294&gt;@shivam6294&lt;/denchmark-link&gt;
 are we talking about SSD300 or SSD512?
		</comment>
		<comment id='12' author='nunotrancoso' date='2018-05-06T16:13:32Z'>
		Hey &lt;denchmark-link:https://github.com/pierluigiferrari&gt;@pierluigiferrari&lt;/denchmark-link&gt;
! I'm training a SSD512 model
		</comment>
		<comment id='13' author='nunotrancoso' date='2018-05-06T16:27:43Z'>
		&lt;denchmark-link:https://github.com/shivam6294&gt;@shivam6294&lt;/denchmark-link&gt;
 you probably forgot to set the list of classifier layers correctly for SSD512. Note that SSD512 has one more predictor layer than SSD300 ('conv10_2_mbox_conf'). The list of classifier layers for SSD512 must be set as follows:
&lt;denchmark-code&gt;classifier_names = ['conv4_3_norm_mbox_conf',
                    'fc7_mbox_conf',
                    'conv6_2_mbox_conf',
                    'conv7_2_mbox_conf',
                    'conv8_2_mbox_conf',
                    'conv9_2_mbox_conf',
                    'conv10_2_mbox_conf']
&lt;/denchmark-code&gt;

In hindsight, &lt;denchmark-link:https://github.com/yf704475209&gt;@yf704475209&lt;/denchmark-link&gt;
 probably had the same problem.
		</comment>
		<comment id='14' author='nunotrancoso' date='2018-05-06T18:52:00Z'>
		Oh, I did set the classifier_names appropriately in the weight sampling script (same as above). I also set the predictor_sizes in the train script as:
&lt;denchmark-code&gt;predictor_sizes = [
        model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],
        model.get_layer('fc7_mbox_conf').output_shape[1:3],
        model.get_layer('conv6_2_mbox_conf').output_shape[1:3],
        model.get_layer('conv7_2_mbox_conf').output_shape[1:3],
        model.get_layer('conv8_2_mbox_conf').output_shape[1:3],
        model.get_layer('conv9_2_mbox_conf').output_shape[1:3],
        model.get_layer('conv10_2_mbox_conf').output_shape[1:3]
    ]
&lt;/denchmark-code&gt;

Also ensured that the scales and aspect ratios match that of the SSD512:
&lt;denchmark-code&gt;# The anchor box scaling factors used in the original SSD512
SCALES = [0.07, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1.05]

# The anchor box aspect ratios used in the original SSD512; the order matters
ASPECT_RATIOS = [[1.0, 2.0, 0.5],
                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                 [1.0, 2.0, 0.5],
                 [1.0, 2.0, 0.5]]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='nunotrancoso' date='2018-05-06T19:06:18Z'>
		&lt;denchmark-link:https://github.com/shivam6294&gt;@shivam6294&lt;/denchmark-link&gt;
 then I'm afraid I'm unable to reproduce your error. If I set the classifier names list as posted above, sub-sample some trained SSD512 weights to one positive class, and try to load the resulting sub-sampled weights into a newly built SSD512, loading the weights works fine - no errors over here.
		</comment>
		<comment id='16' author='nunotrancoso' date='2020-01-16T09:07:18Z'>
		&lt;denchmark-link:https://github.com/shivam6294&gt;@shivam6294&lt;/denchmark-link&gt;
  did you solve the Problem?
		</comment>
	</comments>
</bug>