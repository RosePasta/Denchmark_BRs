<bug id='491' author='hellotheregeneral' open_date='2020-08-06T19:04:06Z' closed_time='2020-08-13T12:50:19Z'>
	<summary>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</summary>
	<description>
Hi, I tried to fine tune the pretrained model in google colab.
To install TTS requirements I had to do before
pip install segments
Then the requirements got installed properly.
But the problem came when I finetuned the model with:
!python train.py --config_path config.json --restore_path ../best_model.pth.tar
It said:

TRAINING (2020-08-06 18:37:04)
! Run is removed from ../ljspeech-August-06-2020_06+36PM-6d6dca0
Traceback (most recent call last):
File "train.py", line 677, in 
main(args)
File "train.py", line 592, in main
global_step, epoch)
File "train.py", line 184, in train
alignments, alignment_lengths, text_lengths)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 722, in _call_impl
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/TTS-0.0.3+6d6dca0-py3.6.egg/TTS/layers/losses.py", line 231, in forward
ga_loss = self.criterion_ga(alignments, input_lens, alignment_lens)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 722, in _call_impl
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/TTS-0.0.3+6d6dca0-py3.6.egg/TTS/layers/losses.py", line 145, in forward
ga_masks = self._make_ga_masks(ilens, olens).to(att_ws.device)
File "/usr/local/lib/python3.6/dist-packages/TTS-0.0.3+6d6dca0-py3.6.egg/TTS/layers/losses.py", line 141, in _make_ga_masks
ga_masks[idx, :olen, :ilen] = self._make_ga_mask(ilen, olen, self.sigma)
File "/usr/local/lib/python3.6/dist-packages/TTS-0.0.3+6d6dca0-py3.6.egg/TTS/layers/losses.py", line 155, in _make_ga_mask
return 1.0 - torch.exp(-(grid_y / ilen - grid_x / olen) ** 2 / (2 * (sigma ** 2)))
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

I tried putting in train.py below import torch
torch.set_default_tensor_type('torch.cuda.FloatTensor')
but it gave:

TRAINING (2020-08-06 18:47:53)
! Run is removed from ../ljspeech-August-06-2020_06+47PM-6d6dca0
Traceback (most recent call last):
File "train.py", line 677, in 
main(args)
File "train.py", line 592, in main
global_step, epoch)
File "train.py", line 171, in train
text_input, text_lengths, mel_input, speaker_ids=speaker_ids)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 722, in _call_impl
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/TTS-0.0.3+6d6dca0-py3.6.egg/TTS/models/tacotron2.py", line 71, in forward
encoder_outputs = self.encoder(embedded_inputs, text_lengths)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py", line 722, in _call_impl
result = self.forward(*input, **kwargs)
File "/usr/local/lib/python3.6/dist-packages/TTS-0.0.3+6d6dca0-py3.6.egg/TTS/layers/tacotron2.py", line 75, in forward
batch_first=True)
File "/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py", line 244, in pack_padded_sequence
_VF._pack_padded_sequence(input, lengths, batch_first)
RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor

	</description>
	<comments>
		<comment id='1' author='hellotheregeneral' date='2020-08-06T19:40:55Z'>
		I have the same issue. I've looked at &lt;denchmark-link:https://github.com/mozilla/TTS/issues/473&gt;#473&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/mozilla/TTS/pull/476&gt;#476&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='2' author='hellotheregeneral' date='2020-08-06T19:56:03Z'>
		Using grid_x, grid_y = torch.meshgrid(torch.arange(olen, device='cuda'), torch.arange(ilen, device='cuda'))
in the file losses.py which is in usr - local - lib - python 3.6 - dist-packages - TTS-0[more numbers] - TTS - layers - losses.py
seems to have solved the problem
		</comment>
		<comment id='3' author='hellotheregeneral' date='2020-08-13T12:50:19Z'>
		I think this is fixed in dev branch.
		</comment>
	</comments>
</bug>