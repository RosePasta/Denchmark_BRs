<bug id='2875' author='JSAustin' open_date='2020-08-08T00:07:56Z' closed_time='2020-10-02T20:50:26Z'>
	<summary>Significant Amount of Idle Time Not Part of Training or Validation Steps</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When running in AWS using multi-GPU machines, we begin to notice that time spent between epochs becomes very significant. For our case, we spend 17 seconds without GPU processing between epochs that isn't part of either train or validation time. We don't have other custom functions running, so it isn't anything else beyond PyTorch lightning's other functions. When we go to 8 GPUs, this 17 seconds starts to become very significant (25% of the total time). With 1 GPU, it's not much of a factor. We considered it was model saving, but it also happens if we skip validation on some epochs.
We are using PyTorch Lightning 0.8.5 with ddp and PyTorch 1.5.0. In case it matters, our dataloaders load images each mini-batch from Lustre and aren't limited by bandwidth.
	</description>
	<comments>
		<comment id='1' author='JSAustin' date='2020-08-08T00:08:33Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='JSAustin' date='2020-08-08T02:54:06Z'>
		&lt;denchmark-link:https://github.com/JSAustin&gt;@JSAustin&lt;/denchmark-link&gt;
 can you provide a part of code which can be used to reproduce this issue? Have you tried the same thing with DataParallel instead of DDP?
		</comment>
		<comment id='3' author='JSAustin' date='2020-08-08T15:48:23Z'>
		&lt;denchmark-link:https://github.com/JSAustin&gt;@JSAustin&lt;/denchmark-link&gt;
 Can you also mention if you are using a logger? (one of lightning's or otherwise)
		</comment>
		<comment id='4' author='JSAustin' date='2020-08-13T08:58:35Z'>
		This is due to some reinitializing of the datasets at the beginning of each validation epoch. The time grows with the number of workers used. Especially with small datasets and ddp training this accumulates to a significant amount time, since N workers are spawned for each GPU.
A solution is suggested here:
&lt;denchmark-link:https://github.com/pytorch/pytorch/issues/15849&gt;pytorch/pytorch#15849&lt;/denchmark-link&gt;

Which has to be modified like this to work with newer versions of Pytorch:
(also pytorch lightning 0.8.5 and pytorch 1.5.1)
class _RepeatSampler(object):
    """ Sampler that repeats forever.
    Args:
        sampler (Sampler)
    """
    def __init__(self, sampler):
        self.sampler = sampler

    def __iter__(self):
        while True:
            yield from iter(self.sampler)

class ContinuousDataLoader(torch.utils.data.DataLoader):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._DataLoader__initialized = False
        self.batch_sampler = _RepeatSampler(self.batch_sampler)
        self._DataLoader__initialized = True
        self.iterator = super().__iter__()

    def __len__(self):
        return len(self.batch_sampler.sampler)

    def __iter__(self):
        for i in range(len(self)):
            yield next(self.iterator)
		</comment>
		<comment id='5' author='JSAustin' date='2020-08-26T17:27:38Z'>
		&lt;denchmark-link:https://github.com/JSAustin&gt;@JSAustin&lt;/denchmark-link&gt;
 any updates on this one?
		</comment>
		<comment id='6' author='JSAustin' date='2020-08-27T23:27:34Z'>
		Yes, sorry. I was using both Tensorboard and Comet. I can confirm it also happens if I just use Comet. Haven't yet tried no logger. Providing the code is hard. It's proprietary. Easier to answer questions. I'm going to look at what monoelh mentioned and see if that fixes it.
Thanks!
		</comment>
		<comment id='7' author='JSAustin' date='2020-09-02T16:02:27Z'>
		I have the same problem here. The idle time between epochs are about 20s seconds and scale up with num_workers for dataloader.
		</comment>
		<comment id='8' author='JSAustin' date='2020-09-22T20:53:02Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 are you able to repro?
		</comment>
		<comment id='9' author='JSAustin' date='2020-09-24T15:38:12Z'>
		&lt;denchmark-link:https://github.com/edenafek&gt;@edenafek&lt;/denchmark-link&gt;
 not yet, None of my runs face this issue right now.
		</comment>
		<comment id='10' author='JSAustin' date='2020-10-02T20:50:25Z'>
		closing this for now. feel free to reopen if needed!
		</comment>
	</comments>
</bug>