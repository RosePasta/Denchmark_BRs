<bug id='2286' author='hjalmarlucius' open_date='2020-06-19T21:38:11Z' closed_time='2020-07-05T11:17:23Z'>
	<summary>example_input_array dtype</summary>
	<description>
Currently assumed that example_input_array dtype to be equal to model dtype. This is not necessarily correct - e.g. if input is a vector of INT.



pytorch-lightning/pytorch_lightning/core/memory.py


         Line 192
      in
      7dc58bd






 input_ = apply_to_collection(input_, torch.Tensor, lambda x: x.type(model.dtype)) 





	</description>
	<comments>
		<comment id='1' author='hjalmarlucius' date='2020-06-23T17:10:20Z'>
		Hi, I don't understand. Does it throw an error or does it display nothing? Could you clarify?
I don't think we can very accurately define the "input shape" for anything other than tensors.
For this reason we exclude things like dicts from the overview, because it is not very practical to visualize this in a table.
		</comment>
		<comment id='2' author='hjalmarlucius' date='2020-06-24T02:11:09Z'>
		Hi, currently the model is run with input_ as an input. If the model expects a tensor of INTs then it will crash if floats come. I encountered this issue when pretraining an ALBERT-like model. This receives word embeddings as inputs, which have to be integers as they're going into a nn.Embedding
		</comment>
		<comment id='3' author='hjalmarlucius' date='2020-06-24T13:38:31Z'>
		okay I see, so we should not change the dtype as given by example_input_array.
I can't recall why I added this conversion, maybe  it was because of amp and the half conversions. I'll have a closer look, thanks for bringing it up.
		</comment>
		<comment id='4' author='hjalmarlucius' date='2020-06-24T13:39:20Z'>
		as a workaround until it is fixed, cast your input to int before feeding to the embedding layer, or don't use the example_input_array.
		</comment>
	</comments>
</bug>