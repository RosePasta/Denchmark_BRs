<bug id='2771' author='KeeeeiZ' open_date='2020-07-31T07:49:14Z' closed_time='2020-10-29T23:43:05Z'>
	<summary>Speed Drastically Decrease Under Horovod</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

If the backend is set to horovod, training speed will drop drastically.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
I run the training job in docker, and the Dockerfile is shown below:
FROM nvidia/cuda:10.1-devel-ubuntu18.04

# TensorFlow version is tightly coupled to CUDA and cuDNN so it should be selected carefully
ENV TENSORFLOW_VERSION=2.1.0
ENV PYTORCH_VERSION=1.5.1
ENV TORCHVISION_VERSION=0.5.0
ENV CUDNN_VERSION=7.6.5.32-1+cuda10.1
ENV NCCL_VERSION=2.4.8-1+cuda10.1
ENV MXNET_VERSION=1.6.0

# Python 3.6 is supported by Ubuntu Bionic out of the box
ARG python=3.6
ENV PYTHON_VERSION=${python}

# Set default shell to /bin/bash
SHELL ["/bin/bash", "-cu"]

RUN apt-get update &amp;&amp; apt-get install -y --allow-downgrades --allow-change-held-packages --no-install-recommends \
        build-essential \
        cmake \
        g++-4.8 \
        git \
        curl \
        vim \
        wget \
        ca-certificates \
        libcudnn7=${CUDNN_VERSION} \
        libnccl2=${NCCL_VERSION} \
        libnccl-dev=${NCCL_VERSION} \
        libjpeg-dev \
        libpng-dev \
        python${PYTHON_VERSION} \
        python${PYTHON_VERSION}-dev \
        python${PYTHON_VERSION}-distutils \
        librdmacm1 \
        libibverbs1 \
        ibverbs-providers

RUN ln -s /usr/bin/python${PYTHON_VERSION} /usr/bin/python

RUN curl -O https://bootstrap.pypa.io/get-pip.py &amp;&amp; \
    python get-pip.py &amp;&amp; \
    rm get-pip.py

# Install TensorFlow, Keras, PyTorch and MXNet
RUN pip install future typing
RUN pip install numpy \
        tensorflow-gpu==${TENSORFLOW_VERSION} \
        h5py
        # keras \

# RUN pip install https://download.pytorch.org/whl/cu101/torch-${PYTORCH_VERSION}-$(python -c "import wheel.pep425tags as w; print('-'.join(w.get_supported(None)[0][:-1]))")-linux_x86_64.whl \
# https://download.pytorch.org/whl/cu101/torchvision-${TORCHVISION_VERSION}-$(python -c "import wheel.pep425tags as w; print('-'.join(w.get_supported(None)[0][:-1]))")-linux_x86_64.whl
RUN pip install mxnet-cu101==${MXNET_VERSION}
COPY backbones backbones
RUN cd backbones &amp;&amp; pip install -r requirements.txt


# Install Open MPI
RUN mkdir /tmp/openmpi &amp;&amp; \
    cd /tmp/openmpi &amp;&amp; \
    wget https://www.open-mpi.org/software/ompi/v4.0/downloads/openmpi-4.0.0.tar.gz &amp;&amp; \
    tar zxf openmpi-4.0.0.tar.gz &amp;&amp; \
    cd openmpi-4.0.0 &amp;&amp; \
    ./configure --enable-orterun-prefix-by-default &amp;&amp; \
    make -j $(nproc) all &amp;&amp; \
    make install &amp;&amp; \
    ldconfig &amp;&amp; \
    rm -rf /tmp/openmpi

# Install Horovod, temporarily using CUDA stubs
RUN ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs &amp;&amp; \
    HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_WITH_MXNET=1 \
         pip install --no-cache-dir horovod &amp;&amp; \
    ldconfig

# Install OpenSSH for MPI to communicate between containers
RUN apt-get install -y --no-install-recommends openssh-client openssh-server &amp;&amp; \
    mkdir -p /var/run/sshd

# Allow OpenSSH to talk to containers without asking for confirmation
RUN cat /etc/ssh/ssh_config | grep -v StrictHostKeyChecking &gt; /etc/ssh/ssh_config.new &amp;&amp; \
    echo "    StrictHostKeyChecking no" &gt;&gt; /etc/ssh/ssh_config.new &amp;&amp; \
    mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config

# Download examples
RUN apt-get install -y --no-install-recommends subversion &amp;&amp; \
    svn checkout https://github.com/horovod/horovod/trunk/examples &amp;&amp; \
    rm -rf /examples/.svn

WORKDIR "/examples"
My GPUs include 4 NVIDIA P100.
My training script is in /backbones, and if I run the script below, it trains smoothly:
export CUDA_VISIBLE_DEVICES=0,1,2,3
python backbones/tasks/mask_lm/trainer.py \
--data_dirs $DATA_DIR \
--bert_config_file $CONFIG_FILE \
--vocab_file $VOCAB_FILE \
--max_epochs=30 \
--gpus=1 \
--progress_bar_refresh_rate 1 \
--checkpoint_callback=True \
--val_check_interval 1.0 \
--default_root_dir=$OUTPUT_DIR \
--gradient_clip_val=5.0 \
--batch_size=32 \
--distributed_backend=ddp \
--accumulate_grad_batches 16 \
--lr 1e-4 \
--weight_decay 0.01 \
--workers 1 \
--max_length 128
If I run the script below which is using horovod, the speed will be about 1/3 of the one using ddp:
export CUDA_VISIBLE_DEVICES=0,1,2,3
horovodrun -np 4 python backbones/tasks/mask_lm/trainer.py \
--data_dirs $DATA_DIR \
--bert_config_file $CONFIG_FILE \
--vocab_file $VOCAB_FILE \
--max_epochs=30 \
--progress_bar_refresh_rate 1 \
--checkpoint_callback=True \
--val_check_interval 1.0 \
--default_root_dir=$OUTPUT_DIR \
--gradient_clip_val=5.0 \
--batch_size=32 \
--accumulate_grad_batches 16 \
--lr 1e-4 \
--weight_decay 0.01 \
--workers 1 \
--max_length 128 \
--distributed_backend horovod
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The training speed under horovod will at least be around the one under ddp, not 1/3 or 1/4.
	</description>
	<comments>
		<comment id='1' author='KeeeeiZ' date='2020-07-31T07:50:11Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='KeeeeiZ' date='2020-07-31T14:15:44Z'>
		&lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
 mind check this one?
		</comment>
		<comment id='3' author='KeeeeiZ' date='2020-07-31T14:20:24Z'>
		Hey &lt;denchmark-link:https://github.com/KeeeeiZ&gt;@KeeeeiZ&lt;/denchmark-link&gt;
, can you run  and provide the output here?  My guess is that you're not using NCCL.
Note that HOROVOD_GPU_OPERATIONS=NCCL is unreleased (coming in v0.20.0).  For now, you should use HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_GPU_BROADCAST=NCCL instead.  That is likely the source of the issue.
		</comment>
		<comment id='4' author='KeeeeiZ' date='2020-08-04T17:31:22Z'>
		How about using PL docker image - &lt;denchmark-link:https://hub.docker.com/repository/docker/pytorchlightning/pytorch_lightning&gt;https://hub.docker.com/repository/docker/pytorchlightning/pytorch_lightning&lt;/denchmark-link&gt;

with all already installed libraries...
		</comment>
		<comment id='5' author='KeeeeiZ' date='2020-08-06T07:34:16Z'>
		
Hey @KeeeeiZ, can you run horovodrun --check-build and provide the output here? My guess is that you're not using NCCL.
Note that HOROVOD_GPU_OPERATIONS=NCCL is unreleased (coming in v0.20.0). For now, you should use HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_GPU_BROADCAST=NCCL instead. That is likely the source of the issue.

The output is as below:
&lt;denchmark-code&gt;Horovod v0.19.5:

Available Frameworks:
    [X] TensorFlow
    [X] PyTorch
    [X] MXNet

Available Controllers:
    [X] MPI
    [X] Gloo

Available Tensor Operations:
    [ ] NCCL
    [ ] DDL
    [ ] CCL
    [X] MPI
    [X] Gloo
&lt;/denchmark-code&gt;

Just like what you mentioned, NCCL is not available.
I rebuilt the docker image with HOROVOD_GPU_ALLREDUCE=NCCL and HOROVOD_GPU_BROADCAST=NCCL, now the performance is as expected. Thanks a lot!
But since my final goal is to run horovod distributedly, I also tried the training script on two machines with a total of 8 GPUs according to the instructions on
&lt;denchmark-link:https://github.com/horovod/horovod#running-horovod&gt;https://github.com/horovod/horovod#running-horovod&lt;/denchmark-link&gt;
. The script works fine if the docker image is not built with  and . After I rebuilt the image, the script gave the log below, and I am not sure if the problem is due to configs of  horovod or pytorch-lighting:
&lt;denchmark-code&gt;[1,6]&lt;stderr&gt;:Traceback (most recent call last):
[1,6]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 224, in &lt;module&gt;
[1,6]&lt;stderr&gt;:    main()
[1,6]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 220, in main
[1,6]&lt;stderr&gt;:    trainer.fit(model)
[1,6]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,6]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,6]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,6]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,6]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,6]&lt;stderr&gt;:    synchronize(handle)
[1,6]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,6]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,6]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
[1,7]&lt;stderr&gt;:Traceback (most recent call last):
[1,7]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 224, in &lt;module&gt;
[1,7]&lt;stderr&gt;:    main()
[1,7]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 220, in main
[1,7]&lt;stderr&gt;:    trainer.fit(model)
[1,7]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,7]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,7]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,7]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,7]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,7]&lt;stderr&gt;:    synchronize(handle)
[1,7]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,7]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,7]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
[1,3]&lt;stderr&gt;:Traceback (most recent call last):
[1,3]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 219, in &lt;module&gt;
[1,3]&lt;stderr&gt;:    main()
[1,3]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 215, in main
[1,3]&lt;stderr&gt;:    trainer.fit(model)
[1,3]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,3]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,3]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,3]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,3]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,3]&lt;stderr&gt;:    synchronize(handle)
[1,3]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,3]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,3]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
[1,0]&lt;stderr&gt;:Traceback (most recent call last):
[1,0]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 219, in &lt;module&gt;
[1,0]&lt;stderr&gt;:    main()
[1,0]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 215, in main
[1,0]&lt;stderr&gt;:    trainer.fit(model)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,0]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,0]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,0]&lt;stderr&gt;:    synchronize(handle)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,0]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,0]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
[1,2]&lt;stderr&gt;:Traceback (most recent call last):
[1,2]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 219, in &lt;module&gt;
[1,2]&lt;stderr&gt;:    main()
[1,2]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 215, in main
[1,2]&lt;stderr&gt;:    trainer.fit(model)
[1,2]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,2]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,2]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,2]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,2]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,2]&lt;stderr&gt;:    synchronize(handle)
[1,2]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,2]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,2]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
[1,1]&lt;stderr&gt;:Traceback (most recent call last):
[1,1]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 219, in &lt;module&gt;
[1,1]&lt;stderr&gt;:    main()
[1,1]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 215, in main
[1,1]&lt;stderr&gt;:    trainer.fit(model)
[1,1]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,1]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,1]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,1]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,1]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,1]&lt;stderr&gt;:    synchronize(handle)
[1,1]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,1]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,1]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
[1,5]&lt;stderr&gt;:Traceback (most recent call last):
[1,5]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 224, in &lt;module&gt;
[1,5]&lt;stderr&gt;:    main()
[1,5]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 220, in main
[1,5]&lt;stderr&gt;:    trainer.fit(model)
[1,5]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,5]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,5]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,5]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,5]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,5]&lt;stderr&gt;:    synchronize(handle)
[1,5]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,5]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,5]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
[1,4]&lt;stderr&gt;:Traceback (most recent call last):
[1,4]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 224, in &lt;module&gt;
[1,4]&lt;stderr&gt;:    main()
[1,4]&lt;stderr&gt;:  File "backbones/tasks/mask_lm/trainer.py", line 220, in main
[1,4]&lt;stderr&gt;:    trainer.fit(model)
[1,4]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 1000, in fit
[1,4]&lt;stderr&gt;:    results = self.horovod_train(model)
[1,4]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 308, in horovod_train
[1,4]&lt;stderr&gt;:    hvd.broadcast_parameters(model.state_dict(), root_rank=0)
[1,4]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/__init__.py", line 481, in broadcast_parameters
[1,4]&lt;stderr&gt;:    synchronize(handle)
[1,4]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/horovod/torch/mpi_ops.py", line 505, in synchronize
[1,4]&lt;stderr&gt;:    mpi_lib.horovod_torch_wait_and_clear(handle)
[1,4]&lt;stderr&gt;:RuntimeError: Mismatched BROADCAST CPU/GPU device selection: One rank specified device CPU, but another rank specified device GPU.
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[28623,1],6]
  Exit code:    1
&lt;/denchmark-code&gt;

My start up command is:
horovodrun -np 8 -H ip1:4,ip2:4 -p 12345 python backbones/tasks/mask_lm/trainer.py
and the newer Dockerfile is just the old one with two new lines in the beginning:
ENV HOROVOD_GPU_ALLREDUCE=NCCL
ENV HOROVOD_GPU_BROADCAST=NCCL
		</comment>
		<comment id='6' author='KeeeeiZ' date='2020-08-06T07:44:38Z'>
		
How about using PL docker image - https://hub.docker.com/repository/docker/pytorchlightning/pytorch_lightning
with all already installed libraries...

Thanks for the link. Is this image a superset of the one on &lt;denchmark-link:https://hub.docker.com/r/horovod/horovod&gt;https://hub.docker.com/r/horovod/horovod&lt;/denchmark-link&gt;
 ? I am not sure what major differences it would make if I rebuild my image with just changing the beginning of the Dockerfile from  to .
		</comment>
		<comment id='7' author='KeeeeiZ' date='2020-08-06T10:35:28Z'>
		it is not based on Horovod docker image but CUDA and installed Horovod together with other Pl requirements
		</comment>
		<comment id='8' author='KeeeeiZ' date='2020-08-06T17:11:16Z'>
		Hey &lt;denchmark-link:https://github.com/KeeeeiZ&gt;@KeeeeiZ&lt;/denchmark-link&gt;
, is it possible that one of the hosts is running with the new image, while the other is running with the old image?  That error message suggests that one machine is using NCCL and another is not.
		</comment>
		<comment id='9' author='KeeeeiZ' date='2020-08-10T15:57:32Z'>
		
Hey @KeeeeiZ, is it possible that one of the hosts is running with the new image, while the other is running with the old image? That error message suggests that one machine is using NCCL and another is not.

Thanks for the tip &lt;denchmark-link:https://github.com/tgaddair&gt;@tgaddair&lt;/denchmark-link&gt;
  ! My training script is now running without bugs. But the log below shows that inifiniband device is not found. (My primary host machine's name is translation 1)
&lt;denchmark-code&gt;[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO Bootstrap : Using [0]lo:127.0.0.1&lt;0&gt;
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO NET/IB : No device found.
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO NET/Socket : Using [0]lo:127.0.0.1&lt;0&gt;
[1,0]&lt;stdout&gt;:NCCL version 2.4.8+cuda10.1
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO Bootstrap : Using [0]lo:127.0.0.1&lt;0&gt;
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO Bootstrap : Using [0]lo:127.0.0.1&lt;0&gt;
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO Bootstrap : Using [0]lo:127.0.0.1&lt;0&gt;
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO NET/IB : No device found.
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO NET/IB : No device found.
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO NET/IB : No device found.
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO NET/Socket : Using [0]lo:127.0.0.1&lt;0&gt;
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO NET/Socket : Using [0]lo:127.0.0.1&lt;0&gt;
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO NET/Socket : Using [0]lo:127.0.0.1&lt;0&gt;
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO Setting affinity for GPU 0 to 0fff
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO Setting affinity for GPU 2 to 0fff
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO Setting affinity for GPU 1 to 0fff
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO Setting affinity for GPU 3 to 0fff
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO Channel 00 :    0   1   2   3
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO Ring 00 : 3[3] -&gt; 0[0] via direct shared memory
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO Ring 00 : 1[1] -&gt; 2[2] via direct shared memory
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO Ring 00 : 0[0] -&gt; 1[1] via direct shared memory
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO Ring 00 : 2[2] -&gt; 3[3] via direct shared memory
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees disabled
[1,3]&lt;stdout&gt;:translation-1:1749:1957 [3] NCCL INFO comm 0x7fc76c2fb0f0 rank 3 nranks 4 cudaDev 3 nvmlDev 2 - Init COMPLETE
[1,1]&lt;stdout&gt;:translation-1:1747:1958 [1] NCCL INFO comm 0x7f24b82faf90 rank 1 nranks 4 cudaDev 1 nvmlDev 0 - Init COMPLETE
[1,0]&lt;stdout&gt;:translation-1:1746:1951 [0] NCCL INFO comm 0x7faefc3097f0 rank 0 nranks 4 cudaDev 0 nvmlDev 3 - Init COMPLETE
[1,2]&lt;stdout&gt;:translation-1:1748:1959 [2] NCCL INFO comm 0x7f04602fb010 rank 2 nranks 4 cudaDev 2 nvmlDev 1 - Init COMPLETE
&lt;/denchmark-code&gt;

My guess is I need to configure IB manually and the following instruction is what I found after searching since my system is Ubuntu.
&lt;denchmark-link:https://techcommunity.microsoft.com/t5/azure-compute/configuring-infiniband-for-ubuntu-hpc-and-gpu-vms/ba-p/1221351&gt;https://techcommunity.microsoft.com/t5/azure-compute/configuring-infiniband-for-ubuntu-hpc-and-gpu-vms/ba-p/1221351&lt;/denchmark-link&gt;

I am not sure I need to install all this stuff both inside docker and on the host machine, or one of them.
I have read your replies under other questions and thought you are quite familiar with NCCL, horovod, pytorch-lighting and so on. So if it is OK, I will just continue to ask here. Thanks a lot.
		</comment>
		<comment id='10' author='KeeeeiZ' date='2020-08-10T16:59:32Z'>
		Hey &lt;denchmark-link:https://github.com/KeeeeiZ&gt;@KeeeeiZ&lt;/denchmark-link&gt;
, have you gone through the &lt;denchmark-link:https://github.com/Mellanox/nv_peer_memory&gt;nv_peer_memory&lt;/denchmark-link&gt;
 walkthrough to install the kernel module for GPUDirect?  This should be installed in the host, not the container.  That should be sufficient to make use of your Infiniband card for RDMA in NCCL.
		</comment>
		<comment id='11' author='KeeeeiZ' date='2020-08-26T11:45:41Z'>
		
Hey @KeeeeiZ, have you gone through the nv_peer_memory walkthrough to install the kernel module for GPUDirect? This should be installed in the host, not the container. That should be sufficient to make use of your Infiniband card for RDMA in NCCL.

I finally managed to start the script with all the environmental requirements including IB interfaces set. But, when I try to run the task on 2 machines, I got infinitely stuck with the log
&lt;denchmark-code&gt;...
[1,0]&lt;stdout&gt;:translation-0:4325:4536 [0] NCCL INFO Ring 00 : 4 -&gt; 0 [receive] via NET/IB/0
[1,1]&lt;stdout&gt;:translation-0:4326:4538 [1] NCCL INFO comm 0x7fd5bc3124c0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE
[1,4]&lt;stdout&gt;:translation-1:5597:5808 [0] NCCL INFO Ring 00 : 0 -&gt; 4 [receive] via NET/IB/0
[1,0]&lt;stdout&gt;:translation-0:4325:4536 [0] NCCL INFO Ring 00 : 0 -&gt; 4 [send] via NET/IB/0
[1,0]&lt;stdout&gt;:translation-0:4325:4536 [0] NCCL INFO Trees [0] -1-&gt;0-&gt;1/4/-1
[1,0]&lt;stdout&gt;:translation-0:4325:4536 [0] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees enabled up to size 149999
[1,4]&lt;stdout&gt;:translation-1:5597:5808 [0] NCCL INFO Trees [0] 0-&gt;4-&gt;5/-1/-1
[1,0]&lt;stdout&gt;:translation-0:4325:4536 [0] NCCL INFO comm 0x7effcc34dd10 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE
[1,4]&lt;stdout&gt;:translation-1:5597:5808 [0] NCCL INFO comm 0x7fbf40322de0 rank 4 nranks 8 cudaDev 0 nvmlDev 1 - Init COMPLETE
[1,0]&lt;stdout&gt;:translation-0:4325:4536 [0] NCCL INFO Launch mode Parallel
&lt;/denchmark-code&gt;

I guess you might be familiar with the line Launch mode Parallel since I found other issues with the same problem.
My starting script is NCCL_PROTO=^LL NCCL_DEBUG=INFO NCCL_SOCKET_IFNAME=eth0 NCCL_P2P_DISABLE=1 /bin/bash backbones/tasks/mask_lm/scripts/horovod_run_gpu.sh(my IB interface is eth1 on both of my machines)
I have tried many combinations of NCCL_xxx configs but it does not work as the other issues I saw.
And also, the LOG shows that I am using eth0 which is not my IB interface.
&lt;denchmark-code&gt;[1,1]&lt;stdout&gt;:translation-0:11145:11354 [1] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth0:10.0.1.74&lt;0&gt;
[1,3]&lt;stdout&gt;:translation-0:11147:11349 [3] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth0:10.0.1.74&lt;0&gt;
[1,6]&lt;stdout&gt;:translation-1:11633:11839 [2] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth0:10.0.1.76&lt;0&gt;
[1,7]&lt;stdout&gt;:translation-1:11634:11842 [3] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth0:10.0.1.76&lt;0&gt;
[1,5]&lt;stdout&gt;:translation-1:11632:11841 [1] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth0:10.0.1.76&lt;0&gt;
[1,4]&lt;stdout&gt;:translation-1:11631:11840 [0] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth0:10.0.1.76&lt;0&gt;
&lt;/denchmark-code&gt;

If I add the config NCCL_SOCKET_IFNAME=eth1, the program will be stuck with the log showing my route to eth1(my IB interface) failed, as the following:
&lt;denchmark-code&gt;[1,3]&lt;stdout&gt;:translation-0:12299:12502 [3] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth1:172.16.1.31&lt;0&gt;
[1,2]&lt;stdout&gt;:translation-0:12298:12510 [2] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth1:172.16.1.31&lt;0&gt;
[1,1]&lt;stdout&gt;:translation-0:12297:12511 [1] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth1:172.16.1.31&lt;0&gt;
[1,6]&lt;stdout&gt;:translation-1:12677:12885 [2] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth1:172.16.1.7&lt;0&gt;
[1,4]&lt;stdout&gt;:translation-1:12675:12884 [0] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth1:172.16.1.7&lt;0&gt;
[1,7]&lt;stdout&gt;:translation-1:12678:12883 [3] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth1:172.16.1.7&lt;0&gt;
[1,5]&lt;stdout&gt;:translation-1:12676:12886 [1] NCCL INFO NET/IB : Using [0]mlx4_0:1/RoCE ; OOB eth1:172.16.1.7&lt;0&gt;
...
...
[1,6]&lt;stdout&gt;:translation-1:12677:12885 [2] include/socket.h:390 NCCL WARN Connect to 172.16.1.31&lt;51817&gt; failed : No route to host
[1,6]&lt;stdout&gt;:translation-1:12677:12885 [2] NCCL INFO bootstrap.cc:100 -&gt; 2
[1,6]&lt;stdout&gt;:translation-1:12677:12885 [2] NCCL INFO bootstrap.cc:326 -&gt; 2
[1,6]&lt;stdout&gt;:translation-1:12677:12885 [2] NCCL INFO init.cc:695 -&gt; 2
[1,6]&lt;stdout&gt;:translation-1:12677:12885 [2] NCCL INFO init.cc:951 -&gt; 2
[1,5]&lt;stdout&gt;:
[1,5]&lt;stdout&gt;:translation-1:12676:12886 [1] include/socket.h:390 NCCL WARN Connect to 172.16.1.31&lt;51817&gt; failed : No route to host
[1,5]&lt;stdout&gt;:translation-1:12676:12886 [1] NCCL INFO bootstrap.cc:100 -&gt; 2
[1,5]&lt;stdout&gt;:translation-1:12676:12886 [1] NCCL INFO bootstrap.cc:326 -&gt; 2
[1,5]&lt;stdout&gt;:translation-1:12676:12886 [1] NCCL INFO init.cc:695 -&gt; 2
[1,5]&lt;stdout&gt;:translation-1:12676:12886 [1] NCCL INFO init.cc:951 -&gt; 2
&lt;/denchmark-code&gt;

I do not know if I should

set NCCL_SOCKET_IFNAME=eth0 and try to dig into Launch mode Parallel problem
or
set NCCL_SOCKET_IFNAME=eth1 and try to solve the routing problem

		</comment>
		<comment id='12' author='KeeeeiZ' date='2020-10-22T21:38:27Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>