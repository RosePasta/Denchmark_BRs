<bug id='3678' author='ddrevicky' open_date='2020-09-27T11:17:14Z' closed_time='2020-10-05T03:28:47Z'>
	<summary>`EvalResult` does not clone logged tensors</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Tensors passed to EvalResult.log() are not cloned by the object, it keeps references to the original tensors. This may become a problem in a somewhat exotic case when the user stores the calculated metric value as a model's property and logs this property as in result.log(self.my_metric).
Then when averaging metrics across batches for the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L444&gt;entire epoch&lt;/denchmark-link&gt;
, the array of batch metrics contains multiple copies of a reference to the same tensor , the average is therefore incorrect.
This is probably only a problem in the situation so not sure how important it is or whether we want to fix it. A fix like  could also conflict with the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/step_result.py#L728&gt;enable_graph&lt;/denchmark-link&gt;
 option in .
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

See this &lt;denchmark-link:https://colab.research.google.com/drive/1cmKyET8aDNzaFeZppznlCt5QzMXmaqN0?usp=sharing&gt;colab notebook&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;class LitClassifier(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(28 * 28, 10)
        self.my_metric = torch.tensor(0.0) # User wants to store the current value for some reason
...
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.my_metric =  calculate_my_metric()

        result = pl.EvalResult()
        result.log("my_metric", self.my_metric)     # Logged value contains value of self.my_metric in the laste validation step
        result.log("my_metric_detached", self.my_metric.clone().detach())  # Logged value contains average of values produced each step
        return result
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The EvalResult object clones the tensor passed to result.log()
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Colab:

CUDA:

GPU:
available:         False
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0+cu101
pytorch-lightning: 0.9.1rc4
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ddrevicky' date='2020-09-27T11:18:01Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='ddrevicky' date='2020-09-27T23:32:50Z'>
		Thanks for reporting.
&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 do we need to clone everything?
This may become expensive with many metrics?
		</comment>
		<comment id='3' author='ddrevicky' date='2020-10-05T03:28:47Z'>
		yeah, please clone yourself... we can't really do these types of clones since they can OOM
		</comment>
	</comments>
</bug>