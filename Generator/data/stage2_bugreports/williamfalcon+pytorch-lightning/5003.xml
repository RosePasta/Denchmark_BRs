<bug id='5003' author='stachu86' open_date='2020-12-07T19:44:48Z' closed_time='2020-12-16T21:06:55Z'>
	<summary>AttributeError: 'float' object has no attribute 'clone' - when logging float values with sync_dist=True and ddp</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;self.log('d_acc_dummy' , 100.0, on_step=True, on_epoch=True, sync_dist=True) 
&lt;/denchmark-code&gt;

results in
&lt;denchmark-code&gt;/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/step_result.py in log(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, tbptt_reduce_fx, tbptt_pad_token, enable_graph, sync_dist, sync_dist_op, sync_dist_group, sync_fn)
    138             is_dist_initialized = torch.distributed.is_available() and torch.distributed.is_initialized()
    139             # TODO: Find a way to make the reduction only once, so we don't need to clone.
--&gt; 140             value = value.clone() if is_dist_initialized else value
    141             value = sync_fn(value, group=sync_dist_group, reduce_op=sync_dist_op)
    142 

AttributeError: 'float' object has no attribute 'clone'
&lt;/denchmark-code&gt;

Float is allowed type (from step_result.py)
&lt;denchmark-code&gt; if sync_dist and isinstance(value, (torch.Tensor, numbers.Number)):
            is_dist_initialized = torch.distributed.is_available() and torch.distributed.is_initialized()
            # TODO: Find a way to make the reduction only once, so we don't need to clone.
            value = value.clone() if is_dist_initialized else value
            value = sync_fn(value, group=sync_dist_group, reduce_op=sync_dist_op)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Please reproduce using [the BoringModel and post here]&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1_3ZyOfmvdYl5zGEaF-WBcYd1aG0w4k9y?usp=sharing&gt;Simple gan example&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Logging of float values
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     True
pyTorch_version:   1.7.0+cu101
pytorch-lightning: 1.0.8
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='stachu86' date='2020-12-07T19:45:34Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='stachu86' date='2020-12-07T20:07:38Z'>
		I think you shall be using Tensor, &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='3' author='stachu86' date='2020-12-07T20:13:16Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 not metric package related, logging related.
But I do agree that manually casting the number to tensor should get it working for now, but we should probably do this internally.
		</comment>
		<comment id='4' author='stachu86' date='2020-12-09T15:37:24Z'>
		value = torch.tensor(value, device=.... dtype=...)
value = value.clone()
should be sufficient, but perhaps the conversion can be done earlier and cloning should not be necessary in case value was a float. &lt;denchmark-link:https://github.com/stachu86&gt;@stachu86&lt;/denchmark-link&gt;
 are you interested in working on this?
		</comment>
		<comment id='5' author='stachu86' date='2020-12-09T23:10:06Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  yes, it looks to me like a good first contribution. I was little discouraged by the  comment that suggests somebody will  rewrite this soon, but from my understanding  is necessary here because  operates in-place. However, isn't  would be more appropriate in the logging context?
		</comment>
	</comments>
</bug>