<bug id='2971' author='s-rog' open_date='2020-08-14T03:50:00Z' closed_time='2020-08-14T06:42:42Z'>
	<summary>TensorBoardLogger not saving hparams without metrics</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

log_hyperparams for TensorBoardLogger saves no data with default metrics=None, only hparam entries/names show up in sidebar
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
&lt;denchmark-code&gt;import pytorch_lightning as pl
logger = pl.loggers.TensorBoardLogger("./test_logs")
test_dict = {"test":0}
logger.log_hyperparams(test_dict)                      ## no data saved
logger.log_hyperparams(test_dict, test_dict)           ## works
logger.log_metrics(test_dict)                          ## works
logger.experiment.add_hparams(test_dict, test_dict)    ## works but saves in a different events file
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

hparams data is saved and viewable via tensorboard with default args
&lt;denchmark-h:h3&gt;Proposed solution&lt;/denchmark-h&gt;

For default hparams logging without metrics, add a placeholder metric? I can do a PR if this is appropriate.
	</description>
	<comments>
		<comment id='1' author='s-rog' date='2020-08-14T06:00:31Z'>
		&lt;denchmark-code&gt;logger.log_hyperparams({"hp_1":1,"hp_2":2}, {"test_m":0})
### training
logger.log_hyperparams({"hp_1":10,"hp_2":20}, {"test_m":1})
&lt;/denchmark-code&gt;

by calling log_hyperparams again at the end of training with the same metric name, the value can be updated... however it does mess up the graph for said metric (it sets the step to 0)... so there would have to be a dedicated metric for hparams
(calling log_hyperparams again cannot update the hparams)
code to reproduce the messed up graph:
&lt;denchmark-code&gt;import pytorch_lightning as pl
logger = pl.loggers.TensorBoardLogger("./test_logs")
logger.log_metrics({"metric":0.0}, step=0)
logger.log_metrics({"metric":0.5}, step=1)
logger.log_metrics({"metric":1.0}, step=2)
logger.log_hyperparams({"hp_1":10,"hp_2":20}, {"metric":1.0})
&lt;/denchmark-code&gt;

similar behavior can be seen in pytorch summarywriter:
&lt;denchmark-code&gt;from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter(log_dir="./test_logs")
writer.add_scalar("metric", 0.0, global_step=0)
writer.add_scalar("metric", 0.5, global_step=1)
writer.add_scalar("metric", 1.0, global_step=2)
writer.add_hparams({"hp_1":10,"hp_2":20}, {"metric": 1.0})
&lt;/denchmark-code&gt;

and the summarywriter docs states that the hparams metric key should be unique as well
		</comment>
		<comment id='2' author='s-rog' date='2020-08-14T06:39:47Z'>
		This is duplicate of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2406&gt;#2406&lt;/denchmark-link&gt;
 . The issue we have there, is that usually you log these params at the end, but we cannot guarantee them to be logged for crashes then.
		</comment>
		<comment id='3' author='s-rog' date='2020-08-14T06:42:42Z'>
		My bad, will move discussions over there
		</comment>
	</comments>
</bug>