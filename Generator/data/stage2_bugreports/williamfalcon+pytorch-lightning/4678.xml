<bug id='4678' author='JunhaoWang' open_date='2020-11-15T00:32:46Z' closed_time='2020-12-11T21:37:48Z'>
	<summary>MisconfigurationException: ddp2 only works in SLURM or via torchelastic with the WORLD_SIZE, LOCAL_RANK, GROUP_RANK flags</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

pytorch_lightning.utilities.exceptions.MisconfigurationException: ddp2 only works in SLURM or via torchelastic with the WORLD_SIZE, LOCAL_RANK, GROUP_RANK flags
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Using ddp2 and 16bit training on 8 GPUs on Slurm machine causes above error. I have installed torchelastic but that didn't help.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- available:         True
- version:           9.2
Packages:
- numpy:             1.19.1
- pyTorch_debug:     True
- pyTorch_version:   1.7.0+cu92
- pytorch-lightning: 1.0.4
- tqdm:              4.50.2
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.4
- version:           #166-Ubuntu SMP Wed Nov 14 20:09:47 UTC 2018

	</description>
	<comments>
		<comment id='1' author='JunhaoWang' date='2020-11-15T00:33:31Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='JunhaoWang' date='2020-11-15T09:56:23Z'>
		&lt;denchmark-link:https://github.com/JunhaoWang&gt;@JunhaoWang&lt;/denchmark-link&gt;
 mind share how you executed/call the training script?
		</comment>
		<comment id='3' author='JunhaoWang' date='2020-11-15T17:04:53Z'>
		Thanks &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 I started an interactive Slurm job and called it by

python main.py

		</comment>
		<comment id='4' author='JunhaoWang' date='2020-11-16T08:38:17Z'>
		let me sync with &lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 but for the elastic call, you shall use a &lt;denchmark-link:https://pytorch.org/elastic/0.2.1/quickstart.html&gt;similar call&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;python -m torchelastic.distributed.launch --nnodes=NUM_NODES script.py
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='JunhaoWang' date='2020-11-16T10:18:24Z'>
		hey &lt;denchmark-link:https://github.com/JunhaoWang&gt;@JunhaoWang&lt;/denchmark-link&gt;
 what are you trying to do? Are you just trying to run the code within a slurm interactive instance?
Just in case our documentation offers information on running parallel slurm jobs:
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/slurm.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/slurm.html&lt;/denchmark-link&gt;

Which version of PL are you using?
		</comment>
		<comment id='6' author='JunhaoWang' date='2020-12-11T21:37:48Z'>
		Closing this for now, feel free to reopen if needed!
		</comment>
	</comments>
</bug>