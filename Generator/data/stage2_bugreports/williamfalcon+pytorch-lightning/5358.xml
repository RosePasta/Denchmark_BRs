<bug id='5358' author='costantinoai' open_date='2021-01-05T12:01:57Z' closed_time='2021-01-13T12:53:20Z'>
	<summary>'RuntimeError: No rendezvous handler for env://' with multi-gpu</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I get an error
'RuntimeError: No rendezvous handler for env://'
when I run my model with multiple GPU.
Below the code and the traceback:
&lt;denchmark-code&gt;trainer = pl.Trainer(gpus = -1,
                     accelerator='ddp',
                     check_val_every_n_epoch=10, 
                    # precision=16,
                    # auto_scale_batch_size='binsearch',
                     callbacks=[checkpoint_callback],
                     max_epochs = 1)
&lt;/denchmark-code&gt;


GPU available: True, used: True
TPU available: None, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

trainer.fit(model)

initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
Traceback (most recent call last):
File "", line 1, in 
trainer.fit(model)
File "C:\Users\45027900\Anaconda3\envs\PyTorch\lib\site-packages\pytorch_lightning\trainer\trainer.py", line 470, in fit
results = self.accelerator_backend.train()
File "C:\Users\45027900\Anaconda3\envs\PyTorch\lib\site-packages\pytorch_lightning\accelerators\ddp_accelerator.py", line 152, in train
results = self.ddp_train(process_idx=self.task_idx, model=model)
File "C:\Users\45027900\Anaconda3\envs\PyTorch\lib\site-packages\pytorch_lightning\accelerators\ddp_accelerator.py", line 252, in ddp_train
self.init_ddp_connection(
File "C:\Users\45027900\Anaconda3\envs\PyTorch\lib\site-packages\pytorch_lightning\accelerators\accelerator.py", line 153, in init_ddp_connection
self.ddp_plugin.init_ddp_connection(
File "C:\Users\45027900\Anaconda3\envs\PyTorch\lib\site-packages\pytorch_lightning\plugins\ddp_plugin.py", line 90, in init_ddp_connection
torch_distrib.init_process_group(
File "C:\Users\45027900\Anaconda3\envs\PyTorch\lib\site-packages\torch\distributed\distributed_c10d.py", line 433, in init_process_group
rendezvous_iterator = rendezvous(
File "C:\Users\45027900\Anaconda3\envs\PyTorch\lib\site-packages\torch\distributed\rendezvous.py", line 82, in rendezvous
raise RuntimeError("No rendezvous handler for {}://".format(result.scheme))
RuntimeError: No rendezvous handler for env://

The error is not present if I set
gpus = 1
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.7.1
OS (e.g., Linux): Windows 10
How you installed PyTorch (conda, pip, source): conda
Build command you used (if compiling from source): conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch
Python version: 3.8.5
CUDA/cuDNN version: 11.0
GPU models and configuration: 2 * Quadro RTX 6000
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='costantinoai' date='2021-01-05T12:02:38Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='costantinoai' date='2021-01-05T12:05:10Z'>
		Also, I don't know if it is related, but when I check the GPU performance during training (with the flag GPU = 1) using windows task manager, I can see only 1-2% used in the GPU, and 45-50% in the CPU. Is this a normal behaviour?
		</comment>
		<comment id='3' author='costantinoai' date='2021-01-06T08:09:50Z'>
		&lt;denchmark-link:https://github.com/costantinoai&gt;@costantinoai&lt;/denchmark-link&gt;
 mind share what PL version are you using? also, do you have and full example to reproduce?
		</comment>
		<comment id='4' author='costantinoai' date='2021-01-06T08:16:57Z'>
		Hi &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 ,
Thanks for your reply.
PL version is 1.1.2.
I do have an example of the full code on colab, but I would rather not post it publicly.
How can I share it with you?
		</comment>
		<comment id='5' author='costantinoai' date='2021-01-07T00:37:20Z'>
		Hi, you can ping me on slack if you want. It's probably an issue with passing the argument gpus=-1 to the subprocess script. I bet if you set gpus=n where n is the number of gpus, it will work. We just have to support -1 for ddp.
		</comment>
		<comment id='6' author='costantinoai' date='2021-01-07T01:11:33Z'>
		Ok, thanks. I‚Äôll try setting n and see what happens. I‚Äôll send you the Colab link on slack if I still have issues.

Really appreciated!

Get Outlook for iOS&lt;&lt;denchmark-link:https://aka.ms/o0ukef&gt;https://aka.ms/o0ukef&lt;/denchmark-link&gt;
&gt;
&lt;denchmark-link:#&gt;‚Ä¶&lt;/denchmark-link&gt;


________________________________
From: Adrian W√§lchli &lt;notifications@github.com&gt;
Sent: Thursday, January 7, 2021 11:37:33 AM
To: PyTorchLightning/pytorch-lightning &lt;pytorch-lightning@noreply.github.com&gt;
Cc: Andrea Costantino &lt;andrea.costantino@mq.edu.au&gt;; Mention &lt;mention@noreply.github.com&gt;
Subject: Re: [PyTorchLightning/pytorch-lightning] 'RuntimeError: No rendezvous handler for env://' with multi-gpu (#5358)


Hi, you can ping me on slack if you want. It's probably an issue with passing the argument gpus=-1 to the script. I bet if you set gpus=n where n is the number of gpus, it will work. We just have to support -1 for ddp.

‚Äî
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub&lt;#5358 (comment)&gt;, or unsubscribe&lt;https://github.com/notifications/unsubscribe-auth/AOCXNCMM3CA3OQJSM2ZAP5DSYT643ANCNFSM4VU7DUZQ&gt;.

		</comment>
		<comment id='7' author='costantinoai' date='2021-01-07T10:11:46Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 still got the same problem after setting gpus = 2. I reached you on twitter (I don't have a slack account).
Thanks!
		</comment>
		<comment id='8' author='costantinoai' date='2021-01-07T11:36:02Z'>
		In summary after private conversation with &lt;denchmark-link:https://github.com/costantinoai&gt;@costantinoai&lt;/denchmark-link&gt;


ddp not supported on windows platform (yet)
script needs guard around entry point (if __name__ == "__main__")

if these requirements are not met, we see the No rendezvous handler for env://' or similar exceptions.
		</comment>
	</comments>
</bug>