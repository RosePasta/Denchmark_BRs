<bug id='3364' author='sgondala' open_date='2020-09-05T19:43:54Z' closed_time='2020-09-05T20:04:04Z'>
	<summary>load_from_checkpoint not initializing right when using transformers</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I created a LightningModel which is a wrapper over a hugging face transformer. I was able to train the model and save it to disk. However, load_from_checkpoint doesn't seem to pass the right parameters to LightningModel init, due to which the function fails with an error.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Check code sample
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

Model
&lt;denchmark-code&gt;class BertBasedClassifier(LightningModule):
    def __init__(self, model_name='distilbert-base-uncased', num_labels=4):
        super().__init__()
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=num_labels)
&lt;/denchmark-code&gt;

load_from_checkpoint
&lt;denchmark-code&gt;model = BertBasedClassifier.load_from_checkpoint('checkpoints/full_yahoo_answers_classifier_baseline_without_wandb/lightning_logs/version_1/checkpoints/epoch=0.ckpt') 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I've debuged the error a bit, and it's because the the right parameters  aren't being sent to BertBasedClassifier __init__(). The parameters that go to the model are
&lt;denchmark-code&gt;Model name  {}
num_labels  4
&lt;/denchmark-code&gt;

The parameters that should go to model are
&lt;denchmark-code&gt;Model name 'distilbert-base-uncased'
num_labels 10
&lt;/denchmark-code&gt;

I've tried explicitly passing parameters to as a part of load_from_checkpoint That didn't work either.
&lt;denchmark-code&gt;model = BertBasedClassifier.load_from_checkpoint('checkpoints/full_yahoo_answers_classifier_baseline_without_wandb/lightning_logs/version_1/checkpoints/epoch=0.ckpt', num_labels=10)
&lt;/denchmark-code&gt;

Parameters that model gets are
&lt;denchmark-code&gt;Model name  {'num_labels': 10}
num_labels  10
&lt;/denchmark-code&gt;

How to resolve the model name issue? Or can I first create a model normally and then load weights into it?
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti
GeForce RTX 2080 Ti


available:         True
version:           10.2


Packages:

numpy:             1.19.1
pyTorch_debug:     False
pyTorch_version:   1.6.0
pytorch-lightning: 0.9.0
tensorboard:       2.2.0
tqdm:              4.48.2


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #100~16.04.1-Ubuntu SMP Wed Apr 22 23:56:30 UTC 2020



	</description>
	<comments>
		<comment id='1' author='sgondala' date='2020-09-05T19:44:39Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='sgondala' date='2020-09-05T20:04:04Z'>
		Looks like I didn't use self.save_hyperparameters() right. Using that fixed the issue.
		</comment>
	</comments>
</bug>