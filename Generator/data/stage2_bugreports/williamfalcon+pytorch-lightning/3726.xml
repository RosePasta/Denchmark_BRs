<bug id='3726' author='celsofranssa' open_date='2020-09-29T17:44:10Z' closed_time='2020-09-29T21:26:28Z'>
	<summary>Hydra Hyperparameter Optimization</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

After update PL from 0.8.x to 0.9.0, I started to face the following error when passing a configuration file via hydra:
Running in fast_dev_run mode: will run a full train, val and test loop using a single batch
[2020-09-29 17:24:44,547][lightning][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using a single batch
GPU available: True, used: True
[2020-09-29 17:24:44,589][lightning][INFO] - GPU available: True, used: True
TPU available: False, using: 0 TPU cores
[2020-09-29 17:24:44,589][lightning][INFO] - TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
[2020-09-29 17:24:44,589][lightning][INFO] - CUDA_VISIBLE_DEVICES: [0]
/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
  warnings.warn(*args, **kwargs)
Traceback (most recent call last):
  File "/home/celso/projects/semantic_code_search/source/semantic_code_search.py", line 236, in &lt;module&gt;
    dev_run()
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/hydra/main.py", line 24, in decorated_main
    strict=strict,
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/hydra/_internal/utils.py", line 174, in run_hydra
    overrides=args.overrides,
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/hydra/_internal/hydra.py", line 86, in run
    job_subdir_key=None,
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/hydra/plugins/common/utils.py", line 109, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/celso/projects/semantic_code_search/source/semantic_code_search.py", line 45, in dev_run
    trainer.fit(model)
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py", line 48, in wrapped_fn
    result = fn(self, *args, **kwargs)
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1073, in fit
    results = self.accelerator_backend.train(model)
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_backend.py", line 51, in train
    results = self.trainer.run_pretrain_routine(model)
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1169, in run_pretrain_routine
    self.logger.save()
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py", line 27, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/loggers/tensorboard.py", line 212, in save
    save_hparams_to_yaml(hparams_file, self.hparams)
  File "/home/celso/projects/venvs/semantic_code_search/lib/python3.7/site-packages/pytorch_lightning/core/saving.py", line 357, in save_hparams_to_yaml
    if OmegaConf.is_config(hparams):
AttributeError: type object 'OmegaConf' has no attribute 'is_config'
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I'm starting the training as follows:
@hydra.main(config_path="configs/config.yaml")
def run(cfg):

    # logger
    tb_logger = pl_loggers.TensorBoardLogger(cfg.logs.path, name='funcom_exp')

    # checkpoint callback
    checkpoint_callback = ModelCheckpoint(
        filepath=cfg.checkpoint.path + "joint_encoder-java-{epoch:02d}",
        monitor='avg_val_loss')

    model = JointEncoder(config=cfg)

    trainer = Trainer(
        fast_dev_run=True,
        max_epochs=cfg.train.max_epochs,
        gpus=1,
        logger=tb_logger,
        checkpoint_callback=checkpoint_callback
    )

    # training
    trainer.fit(model)

    # testing
    trainer.test()
and my PL model is created as in the code snippet below:
class JointEncoder(LightningModule):

    def __init__(self, config):
        super(JointEncoder, self).__init__()
        self.config = config
        ...
Any direction on that?
	</description>
	<comments>
		<comment id='1' author='celsofranssa' date='2020-09-29T21:26:28Z'>
		I think the bug was due to the difference in versions of hyda and omegaconf.
I just updated to the latest version.
		</comment>
	</comments>
</bug>