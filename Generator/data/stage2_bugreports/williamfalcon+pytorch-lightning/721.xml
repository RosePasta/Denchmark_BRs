<bug id='721' author='ChristofHenkel' open_date='2020-01-21T15:17:52Z' closed_time='2020-03-02T23:14:48Z'>
	<summary>Tqdm progress bar error</summary>
	<description>
When running one epoch with train and val dataloader, as soon as validation is started the progressbar will create a new line for each iteration. I have this bug in pycharm as well as kaggle kernels.  Below a typical example. 80% runs smoothly, as soon as validation starts a new line for each tqdm iteration is started
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.
Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Epoch 1:  80%|████████  | 1216/1520 [09:01&lt;02:08,  2.36batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
Validating:   0%|          | 0/304 [00:00&lt;?, ?batch/s]
Epoch 1:  80%|████████  | 1217/1520 [09:01&lt;01:44,  2.90batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
Epoch 1:  80%|████████  | 1218/1520 [09:02&lt;01:26,  3.48batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
Epoch 1:  80%|████████  | 1219/1520 [09:02&lt;01:14,  4.05batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
Epoch 1:  80%|████████  | 1220/1520 [09:02&lt;01:05,  4.58batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
Epoch 1:  80%|████████  | 1221/1520 [09:02&lt;00:59,  5.04batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
Epoch 1:  80%|████████  | 1222/1520 [09:02&lt;00:54,  5.42batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
Epoch 1:  80%|████████  | 1223/1520 [09:02&lt;00:51,  5.72batch/s, batch_nb=1215, gpu=0, loss=0.649, train_loss=0.616, v_nb=0]
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130
OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect
Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
Nvidia driver version: 418.87.00
cuDNN version: Could not collect
Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] pytorch-lightning==0.5.3.2
[pip] pytorchcv==0.0.50
[pip] torch==1.2.0
[pip] torchaudio==0.3.0
[pip] torched==0.11
[pip] torchfile==0.1.0
[pip] torchvision==0.4.0
[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi
[conda] pytorchcv                 0.0.50                   pypi_0    pypi
[conda] torch                     1.2.0                    pypi_0    pypi
[conda] torchaudio                0.3.0                    pypi_0    pypi
[conda] torched                   0.11                     pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchvision               0.4.0                    pypi_0    pypi
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ChristofHenkel' date='2020-01-21T19:29:02Z'>
		Indeed, this looks quite annoying in such an otherwise amazing CLI user experience!
Happens to me only on Colab though with everything working as expected locally :/
I'm not very familiar with the code base, but it seems that the tqdm progressbar for &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/06242c200a318a37d1f882c786e60354ec04533f/pytorch_lightning/trainer/evaluation_loop.py#L296&gt;test/val&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/06242c200a318a37d1f882c786e60354ec04533f/pytorch_lightning/trainer/trainer.py#L819&gt;train&lt;/denchmark-link&gt;
 has a bit different set of parameters on creation.
A quick search lands on a &lt;denchmark-link:https://stackoverflow.com/questions/41707229/tqdm-printing-to-newline&gt;similar issue on SO&lt;/denchmark-link&gt;
 that suggests initializing tqdm with  and .
I do not exactly understand how that supposed to fix the issue, but as according to the &lt;denchmark-link:https://tqdm.github.io/docs/tqdm/#9595init9595&gt;tqdm docs&lt;/denchmark-link&gt;
  is set by default, that makes me think it may have something to do with the initial position value.
		</comment>
		<comment id='2' author='ChristofHenkel' date='2020-01-21T19:56:16Z'>
		I think this is a tqdm issue, since I've seen it across a variety of code that uses tqdm. I've mostly seen it when my terminal isn't wide enough to fit the progress bar plus all of the printed quantities.
		</comment>
		<comment id='3' author='ChristofHenkel' date='2020-01-22T10:23:36Z'>
		I am afraid that we cannot do much TQDM, I have experienced the printing bar on a new line even in other projects and it is typically when (another) process move errstream cursor or print anything else to stdout
		</comment>
		<comment id='4' author='ChristofHenkel' date='2020-01-24T22:42:51Z'>
		I am not sure this is tqdm, as I don't use it (using my own progress bar). Slightly different circumstances - things work fine and suddenly the progress bar - trn, val or test - creates a new line on each call. My current theory is that this is an Ubuntu terminal problem - but I have yet to prove it.
s
btw - happy to donate my prog bar code - it simply does the job of the moving bar, and takes a leadin and a leadout string to print before and after. Unicode terminals only, only tested on Ubuntu
		</comment>
		<comment id='5' author='ChristofHenkel' date='2020-01-25T11:46:13Z'>
		btw, probably similar to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/330&gt;#330&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='ChristofHenkel' date='2020-02-01T10:38:29Z'>
		&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/765&gt;#765&lt;/denchmark-link&gt;
 seems also relevant.
		</comment>
		<comment id='7' author='ChristofHenkel' date='2020-03-02T23:14:47Z'>
		I will close this in favour of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/765&gt;#765&lt;/denchmark-link&gt;
 so pls let's continue the discussion there... 
		</comment>
	</comments>
</bug>