<bug id='1468' author='siddk' open_date='2020-04-12T22:32:36Z' closed_time='2020-04-19T11:03:41Z'>
	<summary>Mixing hparams and arguments in LightningModule.__init__() crashes load_from_checkpoint()</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Right now, if you initialize a Lightning Module with a mixture of a Namespace (hparams) as well as additional arguments (say to a Dataset), load_from_checkpoint can't recover.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Create a LightningModule as follows:
class Model(pl.LightningModule):
      def __init__(self, hparams, train_dataset, val_dataset):
              self.hparams = hparams
              self.train_dset, self.val_dset = train_dataset, val_dataset
              ...
Run training, then try to restore from checkpoint, via:
nn = Model.restore_from_checkpoint(&lt;PATH&gt;, train_dataset=None, val_dataset=None)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Ideally, you'd just be able to pass in the additional arguments (as above) and everything would work.
	</description>
	<comments>
		<comment id='1' author='siddk' date='2020-04-12T22:33:18Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='siddk' date='2020-04-13T09:08:22Z'>
		Very interesting issue. I think the whole restore_from_checkpoint could use a bit better documentation. I think you are trying to use checkpoints for something that was not the intention.
I think what you want is run the model without training (this is why the train and val datasets are empty). In this case I would try falling back to plain torch solution, such as saving and reusing the state_dict:
&lt;denchmark-code&gt;torch.save(nn.state_dict(), SAVE_PATH)
loaded_model = nn(hparams)
loaded_model.load_state_dict(torch.load(SAVE_PATH))
loaded_model.eval()
print("Model's state_dict:")
for param_tensor in loaded_model.state_dict():
  print(param_tensor, "\t", loaded_model.state_dict()[param_tensor].size())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='siddk' date='2020-04-13T15:59:15Z'>
		That makes sense for this usecase --&gt; but say I want to pause/resume training later on (the intended usecase of restore from checkpoint). Here, I really want to be able to leverage the fact that Lightning remembers my hyperparameters, and I want to be able to just pass in the additional arguments (like the datasets I've constructed).
		</comment>
		<comment id='4' author='siddk' date='2020-04-13T16:04:49Z'>
		This is the behavior in 0.7.3
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html#lightningmodule-class&gt;https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html#lightningmodule-class&lt;/denchmark-link&gt;

# or load passing whatever args the model takes to load
MyLightningModule.load_from_checkpoint(
    'path/to/checkpoint.ckpt',
    learning_rate=0.1,
    layers=2,
    pretrained_model=some_model
)
		</comment>
		<comment id='5' author='siddk' date='2020-04-13T16:32:15Z'>
		This doesn't seem to be true if I explicitly pass a hparams argument... only if I break out each of the arguments and pass them to the init() method.
		</comment>
		<comment id='6' author='siddk' date='2020-04-13T16:35:00Z'>
		share a colab? this should be true on 0.7.3.
For now you can do Trainer(PATH, **hparams)
		</comment>
		<comment id='7' author='siddk' date='2020-04-13T20:19:19Z'>
		&lt;denchmark-link:https://colab.research.google.com/drive/1WmuZfOQxyi4nF_YvjFcJrcpVhNvuCXST&gt;https://colab.research.google.com/drive/1WmuZfOQxyi4nF_YvjFcJrcpVhNvuCXST&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='siddk' date='2020-04-16T04:46:35Z'>
		I can reproduce this with that script locally as well on &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/e3001a092913514d65547d4e912382525cfedad2&gt;e3001a0&lt;/denchmark-link&gt;

Gives me the following traceback
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "pl_loadfromcheckpoint.py", line 40, in &lt;module&gt;
    new_model = Model.load_from_checkpoint('save.ckpt', c='this is', d='a test')
  File "/home/henry/Coding/pytorch-lightning/pytorch_lightning/core/lightning.py", line 1509, in load_from_checkpoint
    model = cls._load_model_state(checkpoint, *args, **kwargs)
  File "/home/henry/Coding/pytorch-lightning/pytorch_lightning/core/lightning.py", line 1541, in _load_model_state
    model = cls(*model_args)
TypeError: __init__() missing 2 required positional arguments: 'c' and 'd'
&lt;/denchmark-code&gt;

It seems to be due to this if statement here which stops it taking into account anymore arguments if there's a hparam in the checkpoint
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1540&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1540&lt;/denchmark-link&gt;

EDIT: PRed &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1505&gt;#1505&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>