<bug id='1442' author='alexeykarnachev' open_date='2020-04-10T12:52:39Z' closed_time='2020-04-10T15:43:07Z'>
	<summary>Failed to configure_optimizers from dictionary without lr_scheduler field presented</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Optimizer is failed to be configured from the dictionary without lr_sheduler field.
Consider an example of the Module configure_optimizers method:
        def configure_optimizers(self):
            config = {
                'optimizer': torch.optim.SGD(params=self.parameters(), lr=1e-03)
            }
            return config
Then, we run a simple trainer:
    trainer_options = dict(default_save_path=tmpdir, max_epochs=1)
    trainer = Trainer(**trainer_options)
    _ = trainer.fit(model)
And we fail with an error:
&lt;denchmark-code&gt;UnboundLocalError: local variable 'lr_schedulers' referenced before assignment
&lt;/denchmark-code&gt;

I believe, that the reason is that lr_schedulers local variable is not determined here:



pytorch-lightning/pytorch_lightning/trainer/optimizers.py


        Lines 36 to 42
      in
      8dd9b80






 # single dictionary 



 elif isinstance(optim_conf, dict): 



 optimizer = optim_conf["optimizer"] 



 lr_scheduler = optim_conf.get("lr_scheduler", []) 



 if lr_scheduler: 



 lr_schedulers = self.configure_schedulers([lr_scheduler]) 



 return [optimizer], lr_schedulers, [] 





I think, it could be fixed like this:
        # single dictionary
        elif isinstance(optim_conf, dict):
            optimizer = optim_conf["optimizer"]
            lr_scheduler = optim_conf.get("lr_scheduler", [])
            if lr_scheduler:
                lr_schedulers = self.configure_schedulers([lr_scheduler])
            else:
                lr_schedulers = []
            return [optimizer], lr_schedulers, []
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Create a simple Module with configure_optimizers which looks like above.
Run the fit Trainer method with the model.
See error

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-link:https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc&gt;https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I suppose, that such the configuration: {"optimizer": ...}, without "lr_scheduler" must be a valid one, and this error must not be occurred.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

OS: Linux
architecture: 64bit
processor: x86_64
python: 3.7.6
version: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/97&gt;#97&lt;/denchmark-link&gt;
~16.04.1-Ubuntu SMP Wed Apr 1 03:03:31 UTC 2020
pytorch-lightning: 0.7.3rc1
	</description>
	<comments>
		<comment id='1' author='alexeykarnachev' date='2020-04-10T13:21:59Z'>
		yeah, agreed that dict should work without the scheduler. mind submitting a PR?
		</comment>
	</comments>
</bug>