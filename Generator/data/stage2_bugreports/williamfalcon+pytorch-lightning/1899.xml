<bug id='1899' author='binshengliu' open_date='2020-05-20T06:25:58Z' closed_time='2020-06-23T15:21:25Z'>
	<summary>Incorrect number of batches when multiple test loaders are used and test_percent_check is specified</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When there are multiple test dataloaders and test_percent_check is specified. The estimated total batches are incorrect and progress bar doesn't show properly.
For example, when I specify two dataloaders each of which has 100 batches and test_percent_check=0.1. The expected total batches are 200*0.1=20. But actually, 40 batches are run.
At this line, num_batches is the global number of batches and will be assigned to self.num_test_batches. 


pytorch-lightning/pytorch_lightning/trainer/data_loading.py


         Line 243
      in
      3459a54






 num_batches = int(num_batches * percent_check) 





while in the evaluation loop, max_batches is regarded as the number of batches for one data loader.



pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py


         Line 262
      in
      3459a54






 if batch_idx &gt;= max_batches: 





&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Return multiple dataloaders from test_dataloaders()
Specify test_percent_check.
Run trainer.test()
Observe expected_batches * num_loaders be run. The progress bar also fails to show progress after expected_batches as it exceeds its specified total steps.

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Run correct number of batches.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:
available:         False
version:           10.2


Packages:

numpy:             1.18.4
pyTorch_debug:     False
pyTorch_version:   1.5.0
pytorch-lightning: 0.7.6
tensorboard:       2.2.0
tqdm:              4.45.0


System:

OS:                Linux
architecture:

64bit



processor:
python:            3.7.6
version:           #1 SMP Debian 4.19.118-2 (2020-04-29)



	</description>
	<comments>
		<comment id='1' author='binshengliu' date='2020-05-20T12:33:46Z'>
		Just had a look at this. The problem is in the trainer as you say, not the progress bar.
There are two loops, the outer runs through the number of dataloaders and the inner loop runs through each.
so the max batches should be the number of batches to run in each dataloader, not totally.
We can easily fix this.
There should really be a test. There seems to be no test that checks that *_percent_check works with the correct amount of data. we should definitely have these tests.
		</comment>
		<comment id='2' author='binshengliu' date='2020-05-20T22:19:23Z'>
		Same case might be hapenning with val_dataloaders. max_batches should be a list I suggest.
		</comment>
		<comment id='3' author='binshengliu' date='2020-05-21T04:25:02Z'>
		That's true yes, I agree, because they could have different length.
		</comment>
		<comment id='4' author='binshengliu' date='2020-05-21T17:53:56Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Anyone working on this or should I submit a PR? Need this to be fixed for a personal project debugging and testing.
		</comment>
		<comment id='5' author='binshengliu' date='2020-05-21T17:59:23Z'>
		if you like, that would help us a lot .)
I could help with the tests if you need help :)
		</comment>
	</comments>
</bug>