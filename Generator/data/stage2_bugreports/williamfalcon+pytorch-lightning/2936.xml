<bug id='2936' author='import-antigravity' open_date='2020-08-12T18:38:40Z' closed_time='2020-10-06T03:15:52Z'>
	<summary>Trainer "optimizers" attribute is None when saving checkpoint and callbacks list is not empty</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm training a GAN and I'm running a few custom callbacks as well. When the model attempts to save at the end of the first epoch, it crashes. Here's the very strange thing: I have the exact same code in a Jupyter notebook and the error doesn't occur.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
The bug does not occur when the callbacks list passed into the trainer is empty. None of the callbacks I'm using have anything to do with saving checkpoints, they're all for logging certain things about the model. Enabling any one of them causes the error. Running the exact same code in Jupyter results in no crashes.
Stack trace:
&lt;denchmark-code&gt;Traceback (most recent call last):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà-| 98.33% [590/600 00:05&lt;00:00 loss: -0.558, v_num: 1, d_loss: -1.120, g_loss: -0.016]
  File "mnist-dense-gan-convergence.py", line 55, in &lt;module&gt;
    main(args)
  File "mnist-dense-gan-convergence.py", line 45, in main
    trainer.fit(gan)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1044, in fit
    results = self.run_pretrain_routine(model)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1213, in run_pretrain_routine
    self.train()
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 370, in train
    self.run_training_epoch()
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 502, in run_training_epoch
    self.check_checkpoint_callback(should_check_val)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 513, in check_checkpoint_callback
    [c.on_validation_end(self, self.get_model()) for c in checkpoint_callbacks]
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 513, in &lt;listcomp&gt;
    [c.on_validation_end(self, self.get_model()) for c in checkpoint_callbacks]
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py", line 12, in wrapped_fn
    return fn(*args, **kwargs)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 309, in on_validation_end
    self._do_check_save(filepath, current, epoch)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 346, in _do_check_save
    self._save_model(filepath)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 168, in _save_model
    self.save_function(filepath, self.save_weights_only)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py", line 268, in save_checkpoint
    checkpoint = self.dump_checkpoint(weights_only)
  File "/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py", line 350, in dump_checkpoint
    for i, optimizer in enumerate(self.optimizers):
TypeError: 'NoneType' object is not iterable
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

Here is the relevant part of my setup code:
inception_callback = GANInceptionScorer(classifier, logits=True, sample_size=1000, input_shape=(-1, 1, 28, 28))

log_dir = os.path.abspath('../logs/mnist-dense-gan-convergence')

params = ParameterMatrixCallback()

callbacks = [
    GANProgressBar(),
    GANTensorboardImageView(),
    params,
    inception_callback
]

trainer_args = {
        'max_epochs': 100,
        'default_root_dir': log_dir,
        'callbacks': callbacks,
        'progress_bar_refresh_rate': 0
    }

    print(log_dir)
    try:
        trainer = Trainer(gpus=1, **trainer_args)
    except MisconfigurationException:
        trainer = Trainer(**trainer_args)

    trainer.fit(gan)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Please copy and paste the output from our
&lt;denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py&gt;environment collection script&lt;/denchmark-link&gt;

(or fill out the checklist below manually).
You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;

and the same code in Jupyter:
inception_callback = GANInceptionScorer(classifier, logits=True, sample_size=1000, input_shape=(-1, 1, 28, 28))

log_dir = os.path.abspath('../logs/mnist-gan-dense')

params = ParameterMatrixCallback()

trainer_args = {
    'max_epochs': 200, 
    'callbacks': [GANProgressBar(), GANTensorboardImageView(n=4), params, inception_callback],
    'progress_bar_refresh_rate': 0, 
    'default_root_dir': log_dir
}

t = Trainer(**trainer_args)

PyTorch Version (e.g., 1.0): 1.3.1
OS (e.g., Linux): macOS
How you installed PyTorch (conda, pip, source): conda
Python version: 3.7
Any other relevant information: pytorch-lightning 0.8.5

	</description>
	<comments>
		<comment id='1' author='import-antigravity' date='2020-08-12T18:41:55Z'>
		Additional info, here are the relevant methods in my GAN class:
class GAN(LightningModule, ABC):
   ...

    @abstractmethod
    def g_optimizer(self) -&gt; Optimizer:
        pass

    @abstractmethod
    def d_optimizer(self) -&gt; Optimizer:
        pass

    def configure_optimizers(self):
        return self.g_optimizer(), self.d_optimizer()

class MnistGanDense(GAN):
    ...

    def g_optimizer(self) -&gt; Optimizer:
        return optim.RMSprop(self.G.parameters(), self.hparams['learning_rate'])

    def d_optimizer(self) -&gt; Optimizer:
        return optim.RMSprop(self.D.parameters(), self.hparams['learning_rate'])
		</comment>
		<comment id='2' author='import-antigravity' date='2020-08-14T01:51:49Z'>
		could you try 0.9.0rc12?
		</comment>
		<comment id='3' author='import-antigravity' date='2020-08-14T02:34:34Z'>
		Is there a way to do that with conda?
		</comment>
		<comment id='4' author='import-antigravity' date='2020-08-14T06:44:05Z'>
		inside your Conda environment you could also install it with pip
		</comment>
		<comment id='5' author='import-antigravity' date='2020-08-15T11:47:42Z'>
		Inside conda you can always install with pip:
&lt;denchmark-code&gt;pip install pytorch-lightning==0.9.0rc13
&lt;/denchmark-code&gt;

If this is still an issue, happy to reopen
		</comment>
		<comment id='6' author='import-antigravity' date='2020-09-01T21:41:55Z'>
		This is still a problem for me. I updated to 0.9.1rc1 and still get this error. Here is my trace.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train_unet.py", line 270, in &lt;module&gt;
    trainer.save_checkpoint(args.save_checkpoint_path)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py", line 275, in
 save_checkpointe
    checkpoint = self.dump_checkpoint(weights_only)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py", line 360, in
 dump_checkpoint
    for i, optimizer in enumerate(self.optimizers):
TypeError: 'NoneType' object is not iterable
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='import-antigravity' date='2020-09-06T20:04:11Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 could you open this again? I'm still getting the error as well
		</comment>
		<comment id='8' author='import-antigravity' date='2020-09-30T06:35:44Z'>
		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 didn't we recently make optimizers init to an empty list instead of None? I think this should solve the problem. Could you check?
		</comment>
		<comment id='9' author='import-antigravity' date='2020-09-30T06:44:51Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 yes its an empty list now. But the code for lightning model defined above has optimizers defined, so am not sure yet what's the issue there.
&lt;denchmark-link:https://github.com/import-antigravity&gt;@import-antigravity&lt;/denchmark-link&gt;
 mind check this on master?
		</comment>
		<comment id='10' author='import-antigravity' date='2020-10-02T19:11:37Z'>
		&lt;denchmark-link:https://github.com/deekshadangwal&gt;@deekshadangwal&lt;/denchmark-link&gt;
 mind share full sample code so we can reproduce your issue?
		</comment>
	</comments>
</bug>