<bug id='1903' author='annemariet' open_date='2020-05-20T15:17:01Z' closed_time='2020-06-26T13:56:56Z'>
	<summary>`num_training_batches` can be 0 if `train_percent_check` is too small</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

if train_percent_check is set to a tiny value, then num_training_batches can be 0. Then no training happens, and no validation either, but no warning is raised and epochs are normally iterated over.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Train on any dataset using limit sufficiently small (eg 0.01).
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

I think there is a simple fix, in data_loading.py, adding a line after l. 171
&lt;denchmark-code&gt;            (l.171) self.num_training_batches = int(self.num_training_batches * self.train_percent_check)
            self.num_training_batches = max(1, self.num_training_batches)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Training at least on 1 batch to have a quick check.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- available:         False
- version:           None
Packages:
- numpy:             1.18.4
- pyTorch_debug:     False
- pyTorch_version:   1.5.0
- pytorch-lightning: 0.7.6rc3
- tensorboard:       2.2.1
- tqdm:              4.46.0
System:
- OS:                Linux
- architecture:
- 64bit
- ELF
- processor:         x86_64
- python:            3.8.2
- version:           #864-Microsoft Thu Nov 07 15:22:00 PST 2019

	</description>
	<comments>
		<comment id='1' author='annemariet' date='2020-05-20T16:34:15Z'>
		sounds reasonable.
btw there is also a Trainer arg "fast_dev_run" if you want to do a quick check with one batch per training/validation dataloader.
		</comment>
		<comment id='2' author='annemariet' date='2020-05-20T16:35:38Z'>
		Okay, thanks for the tip, indeed that would fit my needs here!
		</comment>
	</comments>
</bug>