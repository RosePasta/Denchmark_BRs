<bug id='5572' author='Toyhom' open_date='2021-01-19T14:10:32Z' closed_time='2021-01-19T14:38:56Z'>
	<summary>When overwriting the setup() function in the LightningDataModule, it is forced to add an unnecessary parameter.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;When overwriting the setup() function in the LightningDataModule, it is forced to add an unnecessary parameter.&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0):1.7
OS (e.g., Linux):Windows
How you installed PyTorch (conda, pip, source):pip
Python version:3.6.12
CUDA/cuDNN version:11.0
GPU models and configuration:
Any other relevant information: def setup(self)

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I have to add another argument to setup(self) for it to work, such as setup(self,a), which I won't actually use at all.
	</description>
	<comments>
		<comment id='1' author='Toyhom' date='2021-01-19T14:36:44Z'>
		the other parameter refers to stage which can be 'fit'(called with trainer.fit) or 'test'(called with trainer.test). Using this you can avoid loading both train &amp; val and test data all at once.
something like:
def setup(self, stage):
    if stage == 'fit':  # will be 'fit' when calling trainer.fit()
        # load train &amp; val data only
    elif stage == 'test':  # will be 'test' when calling trainer.test()
        # load test data only
		</comment>
		<comment id='2' author='Toyhom' date='2021-01-19T14:38:56Z'>
		Thank you for your answer.
		</comment>
	</comments>
</bug>