<bug id='4937' author='swethmandava' open_date='2020-12-01T16:18:46Z' closed_time='2021-01-07T16:42:59Z'>
	<summary>Timing one training batch correctly</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm trying to time a training batch (model forward, backward and optimizer step) by doing the following:
&lt;denchmark-code&gt;    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
        self.train_time_epoch_list.append(time.time() - self.t0) #Measures ~time for forward + backward + optimizer_step

    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):
        self.t0 = time.time()
&lt;/denchmark-code&gt;

As a sanity check, I tried to confirm with profiler=True set in my trainer.
&lt;denchmark-code&gt;Profiler Report

Action              	|  Mean duration (s)	|  Total time (s) 
-----------------------------------------------------------------
on_fit_start        	|  0.0002028      	|  0.0002028      
on_train_start      	|  0.018464       	|  0.018464       
on_epoch_start      	|  0.00025359     	|  0.00025359     
on_train_epoch_start	|  2.099e-05      	|  2.099e-05      
get_train_batch     	|  0.039579       	|  1.0686         
on_batch_start      	|  3.1986e-05     	|  0.00086362     
on_train_batch_start	|  2.5267e-05     	|  0.0006822      
training_step_end   	|  1.5809e-05     	|  0.00042684     
model_forward       	|  0.085158       	|  2.2993         
model_backward      	|  0.17356        	|  4.6862         
on_after_backward   	|  2.5937e-05     	|  0.00070029     
optimizer_step      	|  0.2901         	|  7.8326         
on_before_zero_grad 	|  2.1495e-05     	|  0.00058038     
on_batch_end        	|  0.00023385     	|  0.0063139      
on_train_batch_end  	|  0.0010522      	|  0.028408       
on_validation_start 	|  0.00022554     	|  0.00067661     
on_validation_epoch_start	|  1.7601e-05     	|  5.2804e-05     
on_validation_batch_start	|  0.00020385     	|  0.00061156     
validation_step_end 	|  4.1274e-05     	|  0.00012382     
on_validation_batch_end	|  0.00060834     	|  0.001825       
on_validation_epoch_end	|  3.8694e-05     	|  0.00011608     
on_validation_end   	|  10.084         	|  30.253         
on_epoch_end        	|  2.2369e-05     	|  2.2369e-05     
on_train_epoch_end  	|  0.001022       	|  0.001022       
on_train_end        	|  0.00019247     	|  0.00019247     

[0.6943173408508301, 0.2949495315551758, 0.2607576847076416, 0.2683448791503906, 0.2595827579498291, 0.26221323013305664, 0.27995753288269043, 0.2629232406616211, 5.543228387832642, 0.26635313034057617, 0.25763630867004395, 0.2739405632019043, 0.32945823669433594, 0.3239474296569824, 0.3061668872833252, 0.3066873550415039, 13.223016262054443, 0.30397772789001465, 0.29555439949035645, 0.2981913089752197, 0.2988457679748535, 0.29936695098876953, 0.29789280891418457, 0.29648828506469727, 12.333884477615356, 0.29997682571411133, 0.30209898948669434]

&lt;/denchmark-code&gt;

The list numbers returned from call back hook and profiler don't add up. ie from profiler, each batch should approximately take
0.085158  (fprop) + 0.17356 (bprop) + 0.2901 (optim step).  &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/1d3724a87800f00c5e2af4539312c8802763f512/pytorch_lightning/trainer/training_loop.py#L562&gt;Confirming here&lt;/denchmark-link&gt;
 that it is what is being measured with the hooks. Am I missing something?
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

expected fprop + bprop + optim_step to add up to callback hook time
	</description>
	<comments>
		<comment id='1' author='swethmandava' date='2020-12-01T18:18:13Z'>
		note optimizer step contains backward because of the closure
		</comment>
		<comment id='2' author='swethmandava' date='2020-12-02T17:43:46Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 is there any way to skip optimizer_step based on the calculated gradients in backward??
		</comment>
		<comment id='3' author='swethmandava' date='2020-12-02T17:51:56Z'>
		I discussed it recently with &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
, there is a way to make the profiler step more accurate there to not include both backward and step but that needs a bit more refactoring with the experimental LightningOptimizer wrapper we recently introduced.
		</comment>
		<comment id='4' author='swethmandava' date='2020-12-02T22:55:22Z'>
		actually, in master it should already be good with optimizer step not including backward :)
		</comment>
		<comment id='5' author='swethmandava' date='2020-12-03T17:40:12Z'>
		


pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 685 to 698
      in
      0b653b8






 if self.automatic_optimization: 



 



 def train_step_and_backward_closure(): 



 result = self.training_step_and_backward( 



 split_batch, 



 batch_idx, 



 opt_idx, 



 optimizer, 



 self.trainer.hiddens 



         ) 



 return None if result is None else result.loss 



 



 # optimizer step 



 self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure) 








pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 477 to 480
      in
      0b653b8






 def optimizer_step(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure, *args, **kwargs): 



 # optimizer step lightningModule hook 



 if isinstance(optimizer, LightningOptimizer): 



 optimizer.step(closure=train_step_and_backward_closure) 








pytorch-lightning/pytorch_lightning/core/optimizer.py


        Lines 188 to 205
      in
      0b653b8






 trainer = self._trainer 



 optimizer = self._optimizer 



 



 if make_optimizer_step: 



 if trainer.on_tpu: 



 with trainer.profiler.profile(profiler_name): 



 xm.optimizer_step(optimizer, optimizer_args={'closure': closure, **kwargs}) 



 



 elif trainer.amp_backend is not None: 



 trainer.precision_connector.backend.optimizer_step( 



 trainer, optimizer, closure) 



 



 else: 



 with trainer.profiler.profile(profiler_name): 



 optimizer.step(closure=closure, *args, **kwargs) 



 



 # perform zero grad 



 optimizer.zero_grad() 





I don't think that is the case on master. Also, I see a bug here. optimizer.zero_grad() should not be here. Did I miss something??
		</comment>
		<comment id='6' author='swethmandava' date='2020-12-03T17:44:09Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 can you check?
		</comment>
		<comment id='7' author='swethmandava' date='2020-12-04T06:29:38Z'>
		Hey &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 , why do you think optimizer.step() should be there ?
We just called optimiser.step(), so we have to zero_grad.
Best regards,
T.C
		</comment>
		<comment id='8' author='swethmandava' date='2020-12-04T20:14:06Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;


why do you think optimizer.step() should be there?

optimizer.step() is at the correct place. The only problem here is that backward is called within the closure, so it's not possible to skip optimizer.step() or do something else with it after gradients are calculated.

We just called optimizer.step(), so we have to zero_grad.

for optimizer.zero_grad() I have explained the bug here: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4982&gt;#4982&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='swethmandava' date='2020-12-10T12:52:32Z'>
		Hey &lt;denchmark-link:https://github.com/swethmandava&gt;@swethmandava&lt;/denchmark-link&gt;
,
Any update on this one ? Could you use the update Simple Profiler released on 1.1.0 ?
Best regards,
T.C
		</comment>
		<comment id='10' author='swethmandava' date='2021-01-07T16:42:59Z'>
		Feel free to reopen if needed!
		</comment>
	</comments>
</bug>