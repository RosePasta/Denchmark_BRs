<bug id='3778' author='carmocca' open_date='2020-10-01T22:03:07Z' closed_time='2020-10-04T21:10:26Z'>
	<summary>training_step log requires that tbptt_reduce_fx is also set</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

training_step log requires that tbptt_reduce_fx is also set.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

def training_step(self, batch, batch_idx):
    ...
    self.log("train_loss", loss, on_step=False, on_epoch=True, sync_dist=True)
    self.log(
        "foo",
        torch.tensor(self.current_epoch),
        on_step=False,
        on_epoch=True,
        reduce_fx=max,
        #tbptt_reduce_fx=max, ERROR when commented
    )
    return loss

def validation_step(self, batch, batch_idx):
    # No issues here
    self.log(
        "bar",
        torch.tensor(self.global_step),
        on_step=False,
        on_epoch=True,
        reduce_fx=max,
    )
Error:
&lt;denchmark-code&gt;time_outputs = [{'tr_loss': tensor(6.2107), 'foo': tensor(0), 'minimize': tensor(6.2107)}]

    @classmethod
    def reduce_across_time(cls, time_outputs):
        # auto-reduce across time for tbptt
        meta = time_outputs[0]['meta']
    
        # in 1.0 the results have 'extra'. Once we deprecate 0.10.0 we may not need this
        if 'extra' in time_outputs[0]:
            [x.pop('extra', None) for x in time_outputs]
    
        result = cls()
        result = recursive_gather(time_outputs, result)
        recursive_stack(result)
    
        for k, value in result.items():
            if k in ['meta', 'extra']:
                continue
    
            # pick the reduce fx
            if k in ['checkpoint_on', 'early_stop_on', 'minimize']:
                tbptt_reduce_fx = torch.mean
            else:
                tbptt_reduce_fx = meta[k]['tbptt_reduce_fx']
&gt;           result[k] = tbptt_reduce_fx(value)
E           RuntimeError: Can only calculate the mean of floating types. Got Long instead.

../../venv/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:423: RuntimeError
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

That tbptt_reduce_fx is not required
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Master @ commit &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/8be002ccc7c2e8371ab426ea07c953f72747269e&gt;8be002c&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='carmocca' date='2020-10-02T13:01:25Z'>
		good catch. yeah, that's a bug.
want to submit a PR?
		</comment>
		<comment id='2' author='carmocca' date='2020-10-02T14:31:12Z'>
		I am not familiar with the tbptt logic, so maybe its better someone else does it.
I'll create a PR with the failing test so somebody can work on it.
		</comment>
		<comment id='3' author='carmocca' date='2020-12-22T23:48:14Z'>
		What the hell is this tbptt, this looks crazy specific, why is this necessary? Why not keep things simple?
		</comment>
	</comments>
</bug>