<bug id='2470' author='Laksh1997' open_date='2020-07-02T16:05:46Z' closed_time='2020-07-28T20:31:16Z'>
	<summary>WandB Logger always resumes even when `Trainer(resume_from_checkpoing=None)`</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 As requested, bug above in the title.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
model = CoolSystem.load_from_checkpoint(checkpoint)
logger = WandbLogger()
trainer = Trainer(resume_from_checkpoint=None)
trainer.fit(model)
The above resumes the wandb run.
PL 0.8.4

PyTorch Version (e.g., 1.0): 1.6 nightly
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): Conda
Build command you used (if compiling from source):
Python version: 3.7
CUDA/cuDNN version:
GPU models and configuration: V100
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='Laksh1997' date='2020-07-05T06:10:27Z'>
		&lt;denchmark-link:https://github.com/Laksh1997&gt;@Laksh1997&lt;/denchmark-link&gt;
 I simply cannot reproduce this, I tried in several ways, online, offline, with and without checkpoints etc.. Did you set any environment variables related to WandB?
If possible, could you post a full code sample or Google colab?
Thanks
		</comment>
		<comment id='2' author='Laksh1997' date='2020-07-28T20:31:16Z'>
		Closing this. &lt;denchmark-link:https://github.com/Laksh1997&gt;@Laksh1997&lt;/denchmark-link&gt;
 please reopen if still relevant.
		</comment>
	</comments>
</bug>