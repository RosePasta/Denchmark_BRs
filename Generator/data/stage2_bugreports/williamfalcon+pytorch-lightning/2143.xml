<bug id='2143' author='ZhaofengWu' open_date='2020-06-11T02:44:58Z' closed_time='2020-10-05T02:18:50Z'>
	<summary>Fix checkpoint warning for floats</summary>
	<description>
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1819&gt;#1819&lt;/denchmark-link&gt;
 added this warning when saving checkpoints. 


Is there a reason why native Python floats aren't supported? I think it's actually quite common especially when libraries e.g. scipy are involved when computing the metrics.
It says checkpoint not saved, but actually checkpoints are still saved. This check doesn't actually change the saving logic besides giving that warning.

	</description>
	<comments>
		<comment id='1' author='ZhaofengWu' date='2020-06-11T08:32:28Z'>
		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 mind have a look :]
		</comment>
		<comment id='2' author='ZhaofengWu' date='2020-06-11T08:58:32Z'>
		The reason it is still working is due to the fact that it is being converted to a torch.tensor later on:



pytorch-lightning/pytorch_lightning/callbacks/model_checkpoint.py


        Lines 176 to 181
      in
      bd49b07






 if not isinstance(current, torch.Tensor): 



 rank_zero_warn( 



 f'{current} is supposed to be a torch.Tensor. Saving checkpoint may not work correctly. ' 



 f'HINT: check the value of {self.monitor} in your validation loop', RuntimeWarning 



     ) 



 current = torch.tensor(current) 





so in that sense they are supported. I guess the warning is there due to:

If you are returning something other than a native float, like a dict, you will get a non-informative error (RuntimeError: Could not infer dtype of dict). The warning could hopefully help you figure out what is going on. So the only way we can guarantee that this will work, is if you return a torch.Tensor.
The PR that implemented this, tried to fix some ddp bugs. I cannot completely figure out how this is related to ddp right now, but I guess that again torch.Tensor is the only type that guarantee that this works in ddp mode.

		</comment>
		<comment id='3' author='ZhaofengWu' date='2020-06-11T18:15:12Z'>
		If types such as floats are supported, should they be excluded from the check and don't show warnings? And IMHO the warning message saying checkpoints won't be saved while they actually are is not desirable.
		</comment>
		<comment id='4' author='ZhaofengWu' date='2020-06-11T18:18:13Z'>
		sounds reasonable, mind draft a PR? üê∞
		</comment>
		<comment id='5' author='ZhaofengWu' date='2020-07-28T15:59:10Z'>
		&lt;denchmark-link:https://github.com/ZhaofengWu&gt;@ZhaofengWu&lt;/denchmark-link&gt;
 Checking in :)
		</comment>
		<comment id='6' author='ZhaofengWu' date='2020-07-29T05:21:30Z'>
		Ah sorry I overlooked this. I might not have the time recently to do this. Sorry :(
		</comment>
		<comment id='7' author='ZhaofengWu' date='2020-09-28T08:20:03Z'>
		You can assign this to me :).
		</comment>
		<comment id='8' author='ZhaofengWu' date='2020-09-28T10:03:59Z'>
		Logging floats actually doesn't work with the current version.
I tried logging a metric with different dtypes (tensor, float, int, dict, list) and it only works with tensors. See this &lt;denchmark-link:https://colab.research.google.com/drive/1XpeQQuLSXOaaM_RxmPZtHpBLXB0U6nNT#scrollTo=PnBye6lrwH-P&gt;colab&lt;/denchmark-link&gt;
.
For the other types it fails even before reaching the  code because of reduction of the per validation step results at the end of a validation epoch in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/step_result.py#L874&gt;step_result.py&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;    873 def weighted_mean(result, weights):
--&gt; 874     weights = weights.to(result.device)
    875     numerator = torch.dot(result.float(), weights.transpose(-1, 0).float())
    876     result = numerator / weights.sum().float()

AttributeError: 'list' object has no attribute 'device'
&lt;/denchmark-code&gt;

For example if in 2 validation steps the logged metric is 42.0 and 43.0 (floats) the collected metric values to be reduced are [42.0, 43.0] and they do not have the device method.
I think this issue is connected with &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3678&gt;#3678&lt;/denchmark-link&gt;
  and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2636&gt;#2636&lt;/denchmark-link&gt;
.
Basically, shouldn't we ensure that the metrics that user logs fulfill some basic properties? Then later in the program we can assume that they are valid and avoid later extra checks like above in this issue:

tensor (clone and detach the tensor if possible to deal with #3678) or convertible to tensor (in which case convert straight away)
non nan/inf (to deal with #2636)
scalar (perhaps only for a monitored metric since logging 2D tensors is a desired functionality #3697 )

		</comment>
		<comment id='9' author='ZhaofengWu' date='2020-10-02T17:29:43Z'>
		
For example if in 2 validation steps the logged metric is 42.0 and 43.0 (floats) the collected metric values to be reduced are [42.0, 43.0] and they do not have the device method.

then you need to make a tensor which has those attributes... :]
		</comment>
		<comment id='10' author='ZhaofengWu' date='2020-10-03T10:16:21Z'>
		Yeah :) Just found out &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3540&gt;this PR&lt;/denchmark-link&gt;
 which could fix float logging when merged.
		</comment>
	</comments>
</bug>