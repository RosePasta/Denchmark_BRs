<bug id='2139' author='anthonytec2' open_date='2020-06-10T15:38:26Z' closed_time='2020-06-11T01:26:32Z'>
	<summary>OmegaConf save to hparams</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I have updated to the latest version 0.8.0rc1 and wanted to test out the new OmegaConf support. I can pass a OmegaConf object into my model, although saving to hparams says that OmegaConf is an unsupported type. I maybe doing this wrong, but I followed  &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html&gt;updated docs&lt;/denchmark-link&gt;
 which shows this should be supported
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "test_n.py", line 105, in &lt;module&gt;
    mnist_model = MNISTModel(cfg)
  File "test_n.py", line 26, in __init__
    self.hparams = hparams
  File "/Users/a.bisulco/robotics/.venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 620, in __setattr__
    object.__setattr__(self, name, value)
  File "/Users/a.bisulco/robotics/.venv/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1682, in hparams
    self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)
  File "/Users/a.bisulco/robotics/.venv/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1657, in save_hyperparameters
    self._set_hparams(hp)
  File "/Users/a.bisulco/robotics/.venv/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1667, in _set_hparams
    raise ValueError(f'Unsupported config type of {type(hp)}.')
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import os

import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision import transforms
import pytorch_lightning as pl


class MNISTModel(pl.LightningModule):
    def __init__(self, hparams):
        super(MNISTModel, self).__init__()
        # not the best model...
        # self.hparams = hparams
        self.hparams = hparams
        self.l1 = torch.nn.Linear(28 * 28, 10)

    def forward(self, x):
        # called with self(x)
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_nb):
        # REQUIRED
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {"train_loss": loss}
        return {"loss": loss, "log": tensorboard_logs}

    def validation_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self(x)
        return {"val_loss": F.cross_entropy(y_hat, y)}

    def validation_epoch_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        tensorboard_logs = {"val_loss": avg_loss}
        return {"val_loss": avg_loss, "log": tensorboard_logs}

    def test_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self(x)
        return {"test_loss": F.cross_entropy(y_hat, y)}

    def test_epoch_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x["test_loss"] for x in outputs]).mean()
        logs = {"test_loss": avg_loss}
        return {"test_loss": avg_loss, "log": logs, "progress_bar": logs}

    def configure_optimizers(self):
        # REQUIRED
        # can return multiple optimizers and learning_rate schedulers
        # (LBFGS it is automatically supported, no need for closure function)
        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)

    def train_dataloader(self):
        # REQUIRED
        return DataLoader(
            MNIST(
                os.getcwd(), train=True, download=True, transform=transforms.ToTensor()
            ),
            batch_size=32,
        )

    def val_dataloader(self):
        # OPTIONAL
        return DataLoader(
            MNIST(
                os.getcwd(), train=True, download=True, transform=transforms.ToTensor()
            ),
            batch_size=32,
        )

    def test_dataloader(self):
        # OPTIONAL
        return DataLoader(
            MNIST(
                os.getcwd(), train=False, download=True, transform=transforms.ToTensor()
            ),
            batch_size=32,
        )


# train!
train_loader = DataLoader(
    MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()),
    batch_size=32,
)
cfg = OmegaConf.create({"lr": 0.001})
mnist_model = MNISTModel(cfg)

# most basic trainer, uses good defaults (1 gpu)
trainer = pl.Trainer(gpus=0, max_epochs=1, logger=None)
trainer.fit(mnist_model)

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The object should be saved to the hparams object.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:
available:         False
version:           None


Packages:

numpy:             1.17.0
pyTorch_debug:     False
pyTorch_version:   1.4.0
pytorch-lightning: 0.8.0rc1
tensorboard:       2.1.1
tqdm:              4.45.0


System:

OS:                Darwin
architecture:

64bit



processor:         i386
python:            3.6.9
version:           Darwin Kernel Version 18.7.0: Sat Oct 12 00:02:19 PDT 2019; root:xnu-4903.278.12~1/RELEASE_X86_64



	</description>
	<comments>
		<comment id='1' author='anthonytec2' date='2020-06-11T01:26:32Z'>
		Solved this issue, for OmegaConf support you should upgrade to OmegaConf-2.0.0. I was using 1.4.1 previously. Possibly should make a note of this in the docs
		</comment>
	</comments>
</bug>