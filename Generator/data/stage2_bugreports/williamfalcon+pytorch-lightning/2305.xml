<bug id='2305' author='tridao' open_date='2020-06-20T21:15:03Z' closed_time='2020-10-01T11:15:29Z'>
	<summary>Why raise error "cannot infer num_classes when target is all zero" in accuracy metric</summary>
	<description>
Why does the accuracy metric raise error when target is all zero?
(&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.8.1/pytorch_lightning/metrics/functional/classification.py#L222&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/0.8.1/pytorch_lightning/metrics/functional/classification.py#L222&lt;/denchmark-link&gt;
)
I think it's still reasonable to compute accuracy even when the target in the minibatch happens to be all zero.
	</description>
	<comments>
		<comment id='1' author='tridao' date='2020-06-20T21:15:40Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='tridao' date='2020-06-23T21:32:47Z'>
		I think it is not meant to compute this accuracy over the minibatch, but rather over the whole data in e.g. validation_epoch_end, and then it should not happen that all targets are 0. If you still want it, you can pass in e.g. num_classes=1 and then the error will not be raised. That's my understanding.
&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 correct me if I'm wrong :)
		</comment>
		<comment id='3' author='tridao' date='2020-06-24T05:49:09Z'>
		almost &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 :)
&lt;denchmark-link:https://github.com/tridao&gt;@tridao&lt;/denchmark-link&gt;
 when you have only one class, in your targets your sups will be zero for the other class and you will divide by zero since accuracy is generally defined by sum(tps)/sum(sups).
Since we don't want to divide by zero we explicitly excluded that case.
If you set num_classes to 1, you bypass that, but in some cases you may end up with zero division error / NaN
		</comment>
		<comment id='4' author='tridao' date='2020-06-24T21:23:08Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Ideally, accuracy should work with a minibatch too so that we can just import it from pl and directly use to track this metric during training or validation for each batch. Also, what's  here &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
  ??
		</comment>
		<comment id='5' author='tridao' date='2020-06-24T21:26:02Z'>
		Yes, this is the issue I'm running into. I'm doing binary classification, and calling accuracy on minibatches. After a while, I'll get a minibatch where the target is all zero, and it errors.
I think this is a common enough situation.
		</comment>
		<comment id='6' author='tridao' date='2020-06-24T21:28:34Z'>
		Then the solution is to pass in the number of classes, right?
		</comment>
		<comment id='7' author='tridao' date='2020-06-24T21:35:32Z'>
		I don't think there's division by zero.
After removing the raise RuntimeError in the accuracy implementation this code works just fine:
accuracy(torch.tensor([0, 1]), torch.tensor([0, 0]))
# tensor(0.5000)
So I'm not seeing the reason for raising error.
		</comment>
		<comment id='8' author='tridao' date='2020-06-24T21:41:13Z'>
		Sorry, there's division by zero, but it's handled automatically after taking the mean.
If reduction='none' (per class accuracy):
accuracy(torch.tensor([0, 1]), torch.tensor([0, 0]), reduction='none')
# tensor([0.5000,     nan])
Maybe the error should only be raised if reduction='none'?
		</comment>
		<comment id='9' author='tridao' date='2020-07-30T07:34:25Z'>
		&lt;denchmark-link:https://github.com/tridao&gt;@tridao&lt;/denchmark-link&gt;
 to follow up on this, can you maybe create a PR that changes this to a warning maybe and ping me on it?
		</comment>
		<comment id='10' author='tridao' date='2020-08-24T04:37:49Z'>
		I think this is actually a more major issue than you were originally considering. During the initial sanity check, it's happened to me that the small sampled data has a uniform class and there is an error thrown. The accuracy function is thus not only being called with a full dataset. I had to switch to the slower sklearn metrics module to solve this problem. It's an easy fix and high priority for your users given this erratic behavior.
		</comment>
		<comment id='11' author='tridao' date='2020-08-24T07:35:57Z'>
		&lt;denchmark-link:https://github.com/sauhaardac&gt;@sauhaardac&lt;/denchmark-link&gt;
 As of now, I agree with you, would you make this a warning or drop completely?
		</comment>
		<comment id='12' author='tridao' date='2020-08-26T06:16:20Z'>
		I would drop it completely as other frameworks don't have this kind of a
warning and users don't seem to be complaining about it.
ᐧ
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Mon, Aug 24, 2020 at 12:36 AM Justus Schock ***@***.***&gt; wrote:
 @sauhaardac &lt;https://github.com/sauhaardac&gt; As of now, I agree with you,
 would you make this a warning or drop completely?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#2305 (comment)&gt;,
 or unsubscribe
 &lt;https://github.com/notifications/unsubscribe-auth/ACTSCR3TYFYDAPTVLDTZIMDSCIJ6XANCNFSM4ODRQJNA&gt;
 .



		</comment>
		<comment id='13' author='tridao' date='2020-09-01T09:19:10Z'>
		Same problem here, you need to specify the num_classes
		</comment>
	</comments>
</bug>