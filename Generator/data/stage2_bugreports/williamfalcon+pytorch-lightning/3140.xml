<bug id='3140' author='dycw' open_date='2020-08-25T01:51:26Z' closed_time='2020-08-25T02:00:27Z'>
	<summary>Unable to load checkpoint; __init__'s missing positional arguments</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
Create a very simple module with named arguments (of type inspect.Parameter.POSITIONAL_OR_KEYWORD), train it with a custom model checkpoint, and then try reload it.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import pytorch_lightning as pl
import torch


class Foo(pl.LightningModule):
    def __init__(self, m, n):
        super().__init__()
        self.m = m
        self.n = n
        self.linear = torch.nn.Linear(m, n)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters())

    def forward(self, x):
        return self.linear(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        return {"loss": torch.nn.functional.mse_loss(self(x), y)}

    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(
                torch.randn(size=(100, 5)),
                torch.randn(size=(100, 2)),
            )
        )

    def validation_step(self, batch, batch_idx):
        x, y = batch
        return {"val_loss": torch.nn.functional.mse_loss(self(x), y)}

    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(
                torch.randn(size=(50, 5)),
                torch.randn(size=(50, 2)),
            )
        )

    def validation_epoch_end(self, outputs):
        loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        return {"val_loss": loss, "log": {"val_loss": loss}}


foo = Foo(m=5, n=2)
trainer = pl.Trainer(
    max_epochs=10,
    checkpoint_callback=pl.callbacks.ModelCheckpoint(
        "models/test_foo/{epoch}")
trainer.fit(foo)

Foo.load_from_checkpoint("models/test_foo/epoch=9.ckpt")
This outputs:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-62-8b8f67e5e924&gt; in &lt;module&gt;
----&gt; 1 Foo.load_from_checkpoint("models/epoch=9_val_loss=9.653e-01.ckpt")

~/miniconda3/envs/dts/lib/python3.8/site-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, tags_csv, *args, **kwargs)
    167         checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)
    168 
--&gt; 169         model = cls._load_model_state(checkpoint, *args, **kwargs)
    170         return model
    171 

~/miniconda3/envs/dts/lib/python3.8/site-packages/pytorch_lightning/core/saving.py in _load_model_state(cls, checkpoint, *cls_args, **cls_kwargs)
    203         if len(cls_spec.args) &lt;= 1 and not cls_spec.kwonlyargs:
    204             cls_args, cls_kwargs = [], {}
--&gt; 205         model = cls(*cls_args, **cls_kwargs)
    206         # load the state_dict on the model automatically
    207         model.load_state_dict(checkpoint['state_dict'])

TypeError: __init__() missing 1 required positional argument: 'n'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I should be able to load the model, or, be missing both its named parameters. After all, load_from_checkpoint allows *args and **kwargs to be passed to the initialization. So why, here, am I getting a value of m but not n?
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - GeForce GT 710
        - available:         True
        - version:           10.2
* Packages:
        - numpy:             1.19.1
        - pyTorch_debug:     False
        - pyTorch_version:   1.6.0
        - pytorch-lightning: 0.8.5
        - tensorboard:       2.2.1
        - tqdm:              4.48.2
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - ELF
        - processor:         x86_64
        - python:            3.8.5
        - version:           #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='dycw' date='2020-08-25T01:52:05Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='dycw' date='2020-08-25T02:00:26Z'>
		I just stumbled upon &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.5/hyperparameters.html&gt;https://pytorch-lightning.readthedocs.io/en/0.8.5/hyperparameters.html&lt;/denchmark-link&gt;
, and figured this out myself. Oops.
		</comment>
	</comments>
</bug>