<bug id='3422' author='rebryk' open_date='2020-09-09T15:14:17Z' closed_time='2020-10-03T18:05:32Z'>
	<summary>DDP doesn't work properly with CUDA_VISIBLE_DEVICE</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

DDP trainer doesn't work properly when CUDA_VISIBLE_DEVICE is set.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Set CUDA_VISIBLE_DEVICE=1,2
Run DDP trainer with 2 GPUs
The main process will use available_gpus[self.trainer.local_rank] that is equal to 1
The second process will use GPU process_idx that is again equal to 1
Thus both processes will use the same single GPU, instead of both

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Training should use both GPUs.
&lt;denchmark-h:h3&gt;Suggestion&lt;/denchmark-h&gt;

Looks like the following if-statement causes the problem:



pytorch-lightning/pytorch_lightning/accelerators/ddp_backend.py


         Line 204
      in
      5b4db52






 if is_master: 





Why do we use int(available_gpus[self.trainer.local_rank]) instead of simple process_idx?
As far as I understand, the master process should always use GPU 0, which is equal to the first GPU in the CUDA_VISIBLE_DEVICE list. Please, correct me if I am wrong.
	</description>
	<comments>
		<comment id='1' author='rebryk' date='2020-09-09T15:15:02Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
	</comments>
</bug>