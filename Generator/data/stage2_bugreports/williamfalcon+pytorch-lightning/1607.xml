<bug id='1607' author='GuardedAirplane' open_date='2020-04-26T06:03:49Z' closed_time='2020-07-04T07:37:03Z'>
	<summary>Tensorboard loss graph differs from command-line output when using accumulated gradients</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Tensorboard loss graph differs from command-line output when using accumulated gradients
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Run a model with accumulated gradients
Compare printed loss to tensorboard loss
See error

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The loss displayed via tensorboard should agree with the command-line output.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce RTX 2070 with Max-Q Design
- available:         True
- version:           10.2
Packages:
- numpy:             1.18.1
- pyTorch_debug:     False
- pyTorch_version:   1.5.0
- pytorch-lightning: 0.7.3
- tensorboard:       2.2.1
- tqdm:              4.45.0
System:
- OS:                Linux
- architecture:
- 64bit
- ELF
- processor:         x86_64
- python:            3.8.2
- version:           #41158678979119.10~9593806-Ubuntu SMP Mon Apr 13 17:50:40 UTC

	</description>
	<comments>
		<comment id='1' author='GuardedAirplane' date='2020-06-25T07:12:49Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>