<bug id='1846' author='jamesjjcondon' open_date='2020-05-15T12:04:31Z' closed_time='2020-05-15T12:34:06Z'>
	<summary>Summing multiple losses with single machine ddp</summary>
	<description>
Hi,
I'm summing together multiple different losses using ddp on a single machine, 2 gpus.
I've been struggling to reduce my loss to zero as a sanity check on a subset of my images.
Is there something I should be calling to synchronise loss across gpus?
I've done this with MNIST no worries.
My model output is a dictionary with 8 components and I'm calling F.nll_loss on each of them before summing together. (One training example consists of 4 images and each example can have zero, 1 or 2 classes)
&lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;

Both my training and validation steps are like:
&lt;denchmark-code&gt;x, y = batch
out = self.forward(x)
loss1 = F.nll_loss(out['CC'][:,0], y['L-CC']['be'])
loss2 = F.nll_loss(out['CC'][:,1], y['R-CC']['ben'])
loss3 = F.nll_loss(out['CC'][:,2], y['L-CC']['ca']) 
loss4 = F.nll_loss(out['CC'][:,3], y['R-CC']['ca'])
loss5 = F.nll_loss(out['MLO'][:,0], y['L-MLO']['ben'])
loss6 = F.nll_loss(out['MLO'][:,1], y['R-MLO']['ben'])
loss7 = F.nll_loss(out['MLO'][:,2], y['L-MLO']['ca'])
loss8 = F.nll_loss(out['MLO'][:,3], y['R-MLO']['ca'])
            
lossCa = loss3 + loss4 + loss7 + loss8
lossb = loss1 + loss2 + loss5 + loss6
        
train_loss = lossCa + lossb
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;What have you tried?&lt;/denchmark-h&gt;

I've tried each of them following: (before sum)
&lt;denchmark-code&gt;losses = [lossLCCb, lossRCCb, lossLCCca, lossRCCca, lossLMLOb, lossRMLOb, lossLMLOca, lossRMLOca]

for loss in losses:
     loss = dist.all_reduce(loss)
     loss /= dist.get_world_size()
&lt;/denchmark-code&gt;

and after sum
&lt;denchmark-code&gt;dist.all_reduce(train_loss)
train_loss /= dist.get_world_size()
&lt;/denchmark-code&gt;

Neither make any difference.
&lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;


OS: ubuntu 1804
Packaging - pip
torch                  1.5.0
torchvision            0.6.0
PL Version - happens with both 0.7.1 and 0.7.2

Any tips / thoughts much appreciated. Cheers.
	</description>
	<comments>
		<comment id='1' author='jamesjjcondon' date='2020-05-15T12:05:12Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='jamesjjcondon' date='2020-05-15T12:34:05Z'>
		in ddp you don't need to dist all reduce. each process is independent of each other.
just return:
{'loss': train_loss} from the training_step.
What syncs in DDP are the gradients... each process calculates its own loss and gradients
		</comment>
	</comments>
</bug>