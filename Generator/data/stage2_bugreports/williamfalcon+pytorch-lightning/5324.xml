<bug id='5324' author='Molaire' open_date='2021-01-02T01:55:27Z' closed_time='2021-01-02T14:14:44Z'>
	<summary>Using DataModule cause memory leak</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/13Z5xjfsBAQgwFdnwyiess_CelfLmIGPs?usp=sharing&gt;https://colab.research.google.com/drive/13Z5xjfsBAQgwFdnwyiess_CelfLmIGPs?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

No tensor should be allocated outside trainer.fit and trainer.test, as to be able to train models sequentially with the same subprocess.
&lt;denchmark-h:h3&gt;Actual behavior&lt;/denchmark-h&gt;

If many models are trained sequentially and they all use a datamodule, it will crash because of a memory leak.
Also: the bug persists if (train, val test) Datasets are instantiated in the setup() method of the DataModule.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

You can get the script and run it with:
`* CUDA:
- GPU:
- Tesla T4
- available:         True
- version:           10.1

Packages:

numpy:             1.19.4
pyTorch_debug:     True
pyTorch_version:   1.7.0+cu101
pytorch-lightning: 1.1.2
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



	</description>
	<comments>
		<comment id='1' author='Molaire' date='2021-01-02T14:15:00Z'>
		It's hogwash, I'll ask help in Slack
		</comment>
	</comments>
</bug>