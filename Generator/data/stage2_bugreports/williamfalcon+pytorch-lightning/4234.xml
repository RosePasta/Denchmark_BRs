<bug id='4234' author='pbmstrk' open_date='2020-10-19T14:34:45Z' closed_time='2020-10-20T16:33:19Z'>
	<summary>Values logged in test_epoch_end not returned when calling test()</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When calling test(), if values are logged only in the test_epoch_end method they are not returned. This leads to the following somewhat inconsistent behaviour:

Values logged only in step method -&gt; appear in list returned by test().
Values logged in step and epoch_end -&gt; both appear in list returned by test().
Values logged only in epoch_end -&gt; values do not appear.

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1C1UiT815EAP6CGZcumhQ_JP8RMVBdFpX?usp=sharing&gt;https://colab.research.google.com/drive/1C1UiT815EAP6CGZcumhQ_JP8RMVBdFpX?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

If values are logged only in epoch_end they should be returned by test().
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

PL version: 1.0.2
see colab.
	</description>
	<comments>
		<comment id='1' author='pbmstrk' date='2020-10-19T14:54:35Z'>
		&lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 mind have look? :]
		</comment>
		<comment id='2' author='pbmstrk' date='2020-10-19T16:25:32Z'>
		Thanks for the report &lt;denchmark-link:https://github.com/pbmstrk&gt;@pbmstrk&lt;/denchmark-link&gt;
! Fairly simple but with a couple of ways to fix this.
The issue arises because when we log metrics, we do not include any custom logged epoch end metrics. The reason why this is included when something happens within the  function is because we actually complete the loop &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/logger_connector.py#L174&gt;here&lt;/denchmark-link&gt;
 which reduces step metrics per dataloader.
At &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/logger_connector.py#L197&gt;this&lt;/denchmark-link&gt;
 line we add everything in callback_metrics which includes our custom logged metric in .
Solution 1
Add the below lines in the step metrics calculation in logger_connector.py, replacing the simple continue operation:
if len(self.callback_metrics) &gt; 0:
    self.eval_loop_results.append(self.callback_metrics)
continue
The issue with this solution is that if we had multiple data-loaders, our custom metric will be added to every result for each data-loader. This is currently already the case if there is another metric already that is being reduced per dataloader.
will return with multi dl:
[
    {
        'custom_metric': 0.4
    },
    {
        'custom_metric': 0.4
    }
]
Solution 2
Add the below lines after the step metrics calculation, creating a separate entry in the return Results List for custom metrics. I.e add after the dataloader metrics loop:
                if len(self.callback_metrics) &gt; 0:
                    self.eval_loop_results.append(self.callback_metrics)
will return:
[
    {
        'y/dl1': 1.4e-4,
    },
    {
        'x/dl2': 1.8e-3,
    },
    {
        'custom_metric': 0.4
    }
]
cc &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='pbmstrk' date='2020-10-20T09:25:13Z'>
		Will implement solution 1, as this reflects what we see if there are step logging.
		</comment>
	</comments>
</bug>