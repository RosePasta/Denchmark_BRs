<bug id='1214' author='level14taken' open_date='2020-03-23T13:01:44Z' closed_time='2020-04-09T12:44:26Z'>
	<summary>can't get mnist tpu colab notebook to work with tpu</summary>
	<description>
I ended up with following error when trying to run &lt;denchmark-link:https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3&gt;mnist_colab&lt;/denchmark-link&gt;
 with tpu,
&lt;denchmark-code&gt;Exception in device=TPU:4: [Errno 2] No such file or directory: '/content/lightning_logs/version_0/checkpoints/epoch=0.ckpt' Traceback (most recent call last): File "/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py", line 119, in _start_fn fn(gindex, *args)
&lt;/denchmark-code&gt;

I've restarted the session after lightning import
I've set the num_tpu_cores=8
any ideas on what should be done?
	</description>
	<comments>
		<comment id='1' author='level14taken' date='2020-03-23T13:02:27Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='level14taken' date='2020-03-23T14:18:54Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 hey can you check this please
		</comment>
		<comment id='3' author='level14taken' date='2020-03-25T18:45:45Z'>
		Try adding a CheckpointCallback that actually saves checkpoints (save_top_k-parameter):
checkpoint_callback = ModelCheckpoint( filepath=args.decoder_model_dir, save_top_k=1, verbose=True, monitor='val_loss', mode='min', prefix='' )
and add it to your trainer:
Trainer(..., checkpoint_callback=checkpoint_callback)
It did work for me this way.
Also you might need to set precision to 32 bit as this seems to be broken right now (or at least has been last week ;) )
		</comment>
		<comment id='4' author='level14taken' date='2020-03-27T12:37:59Z'>
		&lt;denchmark-link:https://github.com/jwallat&gt;@jwallat&lt;/denchmark-link&gt;
 good points, mind draft a PR with forcing 32 precision for TPU and stop trying to import missing checkpoint is there was not requested (no CheckPointing)?
		</comment>
		<comment id='5' author='level14taken' date='2020-04-09T12:00:53Z'>
		Trying to get info about float16 support for pytorch TPU? &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;

for the record:

https://discuss.pytorch.org/t/pytorch-tpu-support/25504/10
pytorch/xla#1330 and pytorch/xla#1489

		</comment>
		<comment id='6' author='level14taken' date='2020-04-09T12:32:22Z'>
		lightning already supports tpu 16 bit &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='level14taken' date='2020-04-09T12:44:26Z'>
		Feel free to reopen if the issue stays ðŸ¤–
		</comment>
	</comments>
</bug>