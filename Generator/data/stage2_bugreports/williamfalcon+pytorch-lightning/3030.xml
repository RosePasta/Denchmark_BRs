<bug id='3030' author='manipopopo' open_date='2020-08-18T05:21:13Z' closed_time='2020-08-18T23:28:36Z'>
	<summary>Incorrect default cuda device when using single gpu other than cuda:0</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The default cuda is not set properly to the trainer.root_gpu in single-GPU mode. The tensors created with device='cuda' will be placed on the incorrect gpu, and the dataloader will acquire memory on the incorrect gpu when pin_memory=True.
Maybe we'll need to add
torch.cuda.set_device(self.trainer.root_gpu) to 


pytorch-lightning/pytorch_lightning/accelerators/gpu_backend.py


         Line 24
      in
      5dfc7b1






 class GPUBackend(object): 





as DDPBackend did:



pytorch-lightning/pytorch_lightning/accelerators/ddp_backend.py


         Line 195
      in
      5dfc7b1






 torch.cuda.set_device(self.trainer.root_gpu) 





&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Running the following code will get
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import pytorch_lightning as pl
import torch
from torch import nn
from torch.utils import data


class Dataset(data.Dataset):

  def __getitem__(self, item):
    return torch.zeros(1)

  def __len__(self):
    return 5


class Model(pl.LightningModule):

  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.x = nn.Parameter(torch.zeros(1))

  def forward(self, *args, **kwargs):
    return self.x

  def training_step(self, *args, **kwargs):
    return self.x + torch.zeros(1, device='cuda')  # RuntimeError.

  def train_dataloader(self):
    return data.DataLoader(Dataset(), num_workers=1, pin_memory=True)

  def configure_optimizers(self):
    return torch.optim.SGD(self.parameters(), 1.0)


if __name__ == '__main__':
  trainer = pl.Trainer(gpus=[1], num_sanity_val_steps=0, max_epochs=1)
  model = Model()
  trainer.fit(model)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

No RuntimeError occurs.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:
available:
version:


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0
pytorch-lightning: 0.9.0rc16
tensorboard:       2.3.0
tqdm:              4.48.2


System:

OS:                Windows
architecture:

64bit
WindowsPE


processor:
python:            3.7.3
version:           10.0.18362



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='manipopopo' date='2020-08-18T06:05:33Z'>
		Directly related to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3016&gt;#3016&lt;/denchmark-link&gt;
 I believe, perhaps a duplicate? Thanks for bringing this up 
		</comment>
		<comment id='2' author='manipopopo' date='2020-08-18T21:39:12Z'>
		&lt;denchmark-link:https://github.com/nateraw&gt;@nateraw&lt;/denchmark-link&gt;
 looked at &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3016&gt;#3016&lt;/denchmark-link&gt;
, that is slightly different. Limiting my PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3042&gt;#3042&lt;/denchmark-link&gt;
 to fixing this particular bug.
		</comment>
	</comments>
</bug>