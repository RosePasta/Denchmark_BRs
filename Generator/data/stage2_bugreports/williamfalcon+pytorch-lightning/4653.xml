<bug id='4653' author='Vozf' open_date='2020-11-13T09:39:10Z' closed_time='2020-11-13T11:00:50Z'>
	<summary>accumulate_grad_batches ignores last batches in epoch if number of steps is not divisible by accumulate_grad_batches?</summary>
	<description>
Suppose I have accumulate_grad_batches=256 and number of steps in my epoch is 260. Loss is updated only on step number 256 every epoch. I suppose it means that the last 4 batches grads are ignored. Is that correct?
	</description>
	<comments>
		<comment id='1' author='Vozf' date='2020-11-13T10:31:27Z'>
		I suppose we do not ignore for last batches. Can you share a minimal example if it's not working?
		</comment>
		<comment id='2' author='Vozf' date='2020-11-13T10:32:54Z'>
		So what is done with last 6 batches? Is it aggreagated over 6 batches instead of asked 256?
		</comment>
		<comment id='3' author='Vozf' date='2020-11-13T10:36:18Z'>
		We call .backward and optimizer.step optimizer.zero_grad() for the last 4 batches.
		</comment>
		<comment id='4' author='Vozf' date='2020-11-13T10:44:52Z'>
		So first you accumulate 256 batches and call backward and then you accumulate 4 batches and call backward, correct?
		</comment>
		<comment id='5' author='Vozf' date='2020-11-13T10:59:08Z'>
		Yep we accumulate 256 if possible and accumulate the rest of batches if it's not divisible by 256
		</comment>
		<comment id='6' author='Vozf' date='2020-11-13T11:00:49Z'>
		Ok, thanks for clarification
		</comment>
	</comments>
</bug>