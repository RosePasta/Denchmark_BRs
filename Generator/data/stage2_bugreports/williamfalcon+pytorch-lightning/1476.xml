<bug id='1476' author='rmrao' open_date='2020-04-13T17:57:32Z' closed_time='2020-04-20T12:03:53Z'>
	<summary>Learning rate scheduler should step after each optimizer step</summary>
	<description>
&lt;denchmark-h:h2&gt;ğŸ› Bug&lt;/denchmark-h&gt;

I'm not sure that this is a bug or if it is a deliberate design decision, but right now the learning rate schedule gets updated at every "step" which actually corresponds to every forward pass. I think a more standard implementation would have the learning rate scheduler "step" interval correspond to being updated every backwards pass. This has caused me a lot of problems with instability as I did not realize that using standard learning rate warmups of say 16000 steps would actually only warm up for 1000 steps if I set accumulate_grad_batches=16.
	</description>
	<comments>
		<comment id='1' author='rmrao' date='2020-04-13T18:00:27Z'>
		good point. it should be every backward pass as you mention.
		</comment>
		<comment id='2' author='rmrao' date='2020-04-13T18:00:37Z'>
		mind submitting  PR?
		</comment>
		<comment id='3' author='rmrao' date='2020-04-13T18:06:12Z'>
		Sure, will do.
		</comment>
	</comments>
</bug>