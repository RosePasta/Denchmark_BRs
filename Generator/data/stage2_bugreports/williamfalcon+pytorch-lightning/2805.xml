<bug id='2805' author='sooheon' open_date='2020-08-03T07:24:17Z' closed_time='2020-10-19T22:36:09Z'>
	<summary>WandbLogger(log_model=True) seems to have no effect</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When initializing WandbLogger with log_model=True, user expects the logger to save the best k checkpoints to wandb on train end.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Initialize wandb_logger = WandbLogger(log_model=True)
Initialize trainer = pl.Trainer(logger=wandb_logger, early_stop_callback=True)
trainer.train(model, tdl, vdl)
When training finishes, check the wandb dashboard -- there are no associated files or artifacts with the run.

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

Be logged into wandb
import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn.functional as F
import wandb
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
from sklearn.datasets import make_classification
from torch.utils.data import DataLoader


class LitModel(pl.LightningModule):

    def __init__(self):
        super().__init__()
        self.l1 = torch.nn.Linear(20, 2)

    def forward(self, x):
        return torch.relu(self.l1(x))

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        return {'loss': loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.0005)

    def validation_step(self, batch, batch_idx):
        return self.training_step(batch, batch_idx)

    def validation_epoch_end(self, outputs):
        losses = []
        for m in outputs:
            losses.append(m['loss'])
        return {'log': {'val_loss': torch.stack(losses).mean()}}


X, y = make_classification(n_samples=1000)
X = X.astype(np.float32)
y = y.astype(np.long)
dataset = list(zip(X, y))

dl = DataLoader(dataset[:800])
vdl = DataLoader(dataset[800:])

wandb_logger = WandbLogger(log_model=True)
checkpointer = ModelCheckpoint(save_top_k=3)
trainer = pl.Trainer(logger=wandb_logger,
                     checkpoint_callback=checkpointer,
                     max_epochs=10)

model = LitModel()

trainer.fit(model, dl, vdl)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

WandbLogger utilizes the checkpoint callback, and saves the best model weights to wandb, using &lt;denchmark-link:https://www.wandb.com/artifacts&gt;artifacts&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
	- GPU:
		- Tesla V100-SXM2-16GB
		- Tesla V100-SXM2-16GB
		- Tesla V100-SXM2-16GB
		- Tesla V100-SXM2-16GB
	- available:         True
	- version:           10.1
* Packages:
	- numpy:             1.18.5
	- pyTorch_debug:     False
	- pyTorch_version:   1.6.0
	- pytorch-lightning: 0.8.5
	- tensorboard:       1.15.0
	- tqdm:              4.48.0
* System:
	- OS:                Linux
	- architecture:
		- 64bit
		-
	- processor:
	- python:            3.7.7
	- version:           #1 SMP Debian 4.19.118-2+deb10u1 (2020-06-07)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

In general, it's a little unclear how trainer.default_root_dir, ModelCheckpoint.filepath and WandbLogger.save_dir interact. I can create a separate ModelCheckpoint callback, and after the training ends upload to wandb:
wandb_logger = WandbLogger(log_model=True)
checkpointer = ModelCheckpoint(save_top_k=3)
trainer = pl.Trainer(logger=wandb_logger,
                     checkpoint_callback=checkpointer,
                     max_epochs=10)

model = LitModel()

trainer.fit(model, dl, vdl)
artifact = wandb.Artifact(f'{wandb_logger.experiment.name}_model', type='model')
for path, val_loss in checkpointer.best_k_models.items():
    artifact.add_file(path)
wandb_logger.experiment.log_artifact(artifact)
However, this is now "experiment code" outside of the trainer abstraction. Ideally this should be handled by the wandblogger and checkpointer in conjunction, and gracefully handle early sigint and so on.
	</description>
	<comments>
		<comment id='1' author='sooheon' date='2020-08-03T07:25:12Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='sooheon' date='2020-08-03T07:32:55Z'>
		&lt;denchmark-link:https://github.com/borisdayma&gt;@borisdayma&lt;/denchmark-link&gt;
 mind have look? 
		</comment>
		<comment id='3' author='sooheon' date='2020-08-03T07:41:45Z'>
		&lt;denchmark-link:https://github.com/sooheon&gt;@sooheon&lt;/denchmark-link&gt;
 If you go to the file menu on wandb dashboard do you see your model?
&lt;denchmark-link:https://user-images.githubusercontent.com/715491/89157836-c3e70600-d532-11ea-9580-95de9771c279.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='sooheon' date='2020-08-03T07:44:33Z'>
		&lt;denchmark-link:https://github.com/borisdayma&gt;@borisdayma&lt;/denchmark-link&gt;
 I do not. Just some config files.
&lt;denchmark-link:https://user-images.githubusercontent.com/302945/89158096-8fae2c80-d5a8-11ea-82ff-04d9ed407c57.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='sooheon' date='2020-08-03T08:59:22Z'>
		It's because the save dir is just the root dir of the logger. The ModelCheckpoint callback actually determines the final checkpoint dir. By default it will create a path of the form save_dir / logger name / logger version. But the trainer could also specify weights_save_dir, or the model checkpoint callback can be configured to save it in a custom location different from the logger. Ultimately, the logger does not know where the checkpoints will be saved.
The current log_model argument we have here is wrong. It cannot work like that. We need a creative solution :)
		</comment>
		<comment id='6' author='sooheon' date='2020-08-05T01:23:16Z'>
		If I were to try to add this Artifact creation and upload code to some pl.Trainer or pl.LightningModule method(s), what is the right place for it?
&lt;denchmark-code&gt;artifact = wandb.Artifact(f'{wandb_logger.experiment.name}_model', type='model')
for path, val_loss in checkpointer.best_k_models.items():
    artifact.add_file(path)
wandb_logger.experiment.log_artifact(artifact)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='7' author='sooheon' date='2020-08-05T01:32:42Z'>
		if you want it in your LightningModule, then the on_train_end hook, or better make a callback class and add these lines in the on_train_end callback method. Then you can reuse it in other models and training scripts
Hooks:
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hooks.html#general-hooks&gt;https://pytorch-lightning.readthedocs.io/en/latest/hooks.html#general-hooks&lt;/denchmark-link&gt;

Callback:
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/callbacks.html#callback-base&gt;https://pytorch-lightning.readthedocs.io/en/latest/callbacks.html#callback-base&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='sooheon' date='2020-08-05T04:12:01Z'>
		Thanks, with the following I'm getting artifacts uploaded properly if training ends as normal (i.e. hitting max_epochs). However, if I interrupt training with Ctrl-C, the files are added to the artifact but fail to upload to wandb. I have also tried the same code in on_keyboard_interrupt, but the result is the same.
class WandbArtifactCallback(pl.Callback):
    def on_train_end(self, trainer, pl_module):
        run = trainer.logger.experiment
        print(f'Ending run: {run.id}')
        artifact = wandb.Artifact(f'{run.id}_model', type='model')
        for path, val_loss in trainer.checkpoint_callback.best_k_models.items():
            print(f'Adding artifact: {path}')
            artifact.add_file(path)
        run.log_artifact(artifact)
		</comment>
		<comment id='9' author='sooheon' date='2020-08-05T04:22:27Z'>
		This is probably related to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2059&gt;#2059&lt;/denchmark-link&gt;

		</comment>
		<comment id='10' author='sooheon' date='2020-10-15T01:19:23Z'>
		I checked and all checkpoints seem to be saved correctly.
See &lt;denchmark-link:https://colab.research.google.com/drive/1AUPRCyeSN7miViMMU5bfmbpyLvgHadKu?usp=sharing&gt;this colab&lt;/denchmark-link&gt;

As for interrupting training with Ctrl + C, this seems to be already filed as &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2059&gt;#2059&lt;/denchmark-link&gt;
 so if there are no more comments related to checkpoints I¬†would suggest to close this issue.
		</comment>
	</comments>
</bug>