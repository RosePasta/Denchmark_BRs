<bug id='2114' author='sshleifer' open_date='2020-06-08T15:15:12Z' closed_time='2020-06-14T15:36:47Z'>
	<summary>load_from_checkpoint: checkpoint[ 'module_arguments'] KeyError</summary>
	<description>
After training, I load my best checkpoint and run trainer.test.
This fails with the following error in v 0.76.
Have people encountered this before? My unit tests, which don't call finetune.py through the command line, do not encounter this issue.
Thanks in advance! Happy to make a reproducible example if this is a new/unknown bug.
    model = model.load_from_checkpoint(checkpoints[-1])
  File "/home/shleifer/miniconda3/envs/nb/lib/python3.7/site-packages/pytorch_lightni
ng/core/lightning.py", line 1563, in load_from_checkpoint
    checkpoint[CHECKPOINT_KEY_MODULE_ARGS].update(kwargs)
=&gt;
KeyError: 'module_arguments'
model is a pl.Module checkpoints[-1] was saved by it, with the save_weights_only=True kwarg specified.
	</description>
	<comments>
		<comment id='1' author='sshleifer' date='2020-06-08T15:45:04Z'>
		try use the last dev version 0.8
&lt;denchmark-code&gt;pip3 install --upgrade git+https://github.com/PyTorchLightning/pytorch-lightning.git
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='sshleifer' date='2020-06-08T16:26:39Z'>
		Tried that, get better traceback but no solution:
&lt;denchmark-code&gt;KeyError: 'Trying to restore training state but checkpoint contains only the model. This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.'
&lt;/denchmark-code&gt;

If I just want to run eval on a pl.Module should I avoid making a trainer?
My ckpt has 3 keys: ['state_dict, epoch, global_step]
		</comment>
		<comment id='3' author='sshleifer' date='2020-06-09T07:18:47Z'>
		When you only save the weights you need to instantiate the model with its parameters first and then load the state_dict with the weights into it
		</comment>
		<comment id='4' author='sshleifer' date='2020-06-09T11:11:40Z'>
		&lt;denchmark-link:https://github.com/sshleifer&gt;@sshleifer&lt;/denchmark-link&gt;
 mind trying 0.8.0rc1?
		</comment>
		<comment id='5' author='sshleifer' date='2020-06-10T22:33:01Z'>
		this can be some back-compatibility issue as we moved the params there and back...
&lt;denchmark-link:https://github.com/sshleifer&gt;@sshleifer&lt;/denchmark-link&gt;
 Mind draft a PR about the past key names for saved params?
		</comment>
		<comment id='6' author='sshleifer' date='2020-06-12T16:40:39Z'>
		shall be fixed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2160&gt;#2160&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/sshleifer&gt;@sshleifer&lt;/denchmark-link&gt;
 mind have a look?
		</comment>
		<comment id='7' author='sshleifer' date='2020-09-22T23:51:15Z'>
		still having issues when loading a checkpoint
When I manually examine the checkpoint saved by lightning it only contains following keys:
['epoch', 'global_step', 'pytorch-lightning_version', 'checkpoint_callback_best_model_score',
'checkpoint_callback_best_model_path',  'optimizer_states',  'lr_schedulers', 'state_dict']
so when I try using Module.load_from_checkpoint it fails because the parameters are not present.
OmegaConf is used to instantiate the module like this: lm = Module(**config.lightning_module_conf)
pytorch_lightning version 0.9.0
		</comment>
		<comment id='8' author='sshleifer' date='2020-10-29T10:57:56Z'>
		Still an issue at version 1.0.4, module_arguments are not present in the checkpoint.
		</comment>
		<comment id='9' author='sshleifer' date='2020-10-30T00:20:06Z'>
		&lt;denchmark-link:https://github.com/FluidSense&gt;@FluidSense&lt;/denchmark-link&gt;
 did you call  in LightningModule's ?
		</comment>
		<comment id='10' author='sshleifer' date='2020-10-30T08:31:15Z'>
		&lt;denchmark-link:https://github.com/wolterlw&gt;@wolterlw&lt;/denchmark-link&gt;
 No, you're right! That was the issue. I discovered the pull requests regarding updating the docs regarding checkpoint saving last night, and there I discovered that I was missing that line.
		</comment>
	</comments>
</bug>