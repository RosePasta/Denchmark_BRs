<bug id='939' author='helldragger' open_date='2020-02-25T11:51:32Z' closed_time='2020-02-27T20:54:07Z'>
	<summary>logger is NoneType hence doesn't have any experiment or other functionality in a lightning module</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When trying to use the logging abilities of lightning, I hit a wall, the default and tensorboard loggers both seem to stay uninitialized when calling trainer.fit(model), resulting in crashes everytime I try to log something.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Create a lightning module as such
&lt;denchmark-code&gt;class SimpleRegressor(pl.LightningModule):
    ...
&lt;/denchmark-code&gt;

Use the logger anywhere to get this kind of stacktrace:
&lt;denchmark-code&gt;d:\Documents\projects\MetaWatch\MetaWatch\notebooks\audio-video-interest\simple_regressor.py in configure_optimizers(self)
    105         #see https://pytorch-lightning.readthedocs.io/en/latest/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers
    106         # REQUIRED
--&gt; 107         self.logger.experiment.add_hparams({'hidden_layer_size':self.hidden_layer_size,
    108                                             'linear_layer_size':self.linear_layer_size,
    109                                             'lstm_layers':self.lstm_layers})

AttributeError: 'NoneType' object has no attribute 'experiment'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import pytorch_lightning as pl

class SimpleRegressor(pl.LightningModule):
    def __init__(self, cuda=False):
        super(SimpleRegressor, self).__init__()
        self.logger.experiment.add_hparams({'hidden_layer_size':1})
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

To log as described in the documentation.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Microsoft Windows 10 Pro    
GCC version: Could not collect  
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 970
Nvidia driver version: 441.12
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.18.1
[pip3] pytorch-lightning==0.6.0
[pip3] tinynumpy==1.2.1
[pip3] torch==1.4.0
[pip3] torchvision==0.4.1
[conda] Could not collect
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='helldragger' date='2020-02-25T11:52:17Z'>
		Hey, thanks for your contribution! Great first issue!
		</comment>
		<comment id='2' author='helldragger' date='2020-02-25T19:04:09Z'>
		Thanks for the issue! The intended way to acheive this is through a hook. When __init__ is called on the LightningModule, the loggers won't have been created yet. I don't think there's any way to change that so we should update the docs to use a hook instead of __init__.
&lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
 any other thoughts on this?
		</comment>
		<comment id='3' author='helldragger' date='2020-02-26T21:00:09Z'>
		it also doesn't work in other functions, I tried in the training step, in the configure_optimizers too
		</comment>
		<comment id='4' author='helldragger' date='2020-02-27T09:09:05Z'>
		Ok, most of these work on master (i.e. if you install from github) for me - except configure_optimizers.
I've opened a PR (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/964&gt;#964&lt;/denchmark-link&gt;
) which fixes that and cleans up the docs a bit.
		</comment>
	</comments>
</bug>