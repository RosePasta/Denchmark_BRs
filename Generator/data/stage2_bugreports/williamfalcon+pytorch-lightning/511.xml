<bug id='511' author='suvojit-0x55aa' open_date='2019-11-15T06:03:47Z' closed_time='2019-11-21T05:57:08Z'>
	<summary>Model checkpoint is not working</summary>
	<description>
Describe the bug
Model checkpoint is not working, even with explicit checkpoint callback.
To Reproduce
Steps to reproduce the behavior:
This is the settings I'm using. hparams.checkpoint_path is actually a dir like './weights'
&lt;denchmark-code&gt;checkpoint_callback = ModelCheckpoint(
        filepath=hparams.checkpoint_path,
        save_best_only=True,
        verbose=True,
        monitor='val_loss',
        mode='min',
        prefix='')

    model = Net3DMMSTN(hparams)
    trainer = Trainer(
        early_stop_callback=None,
        track_grad_norm=2,
        checkpoint_callback=checkpoint_callback,
        print_nan_grads=True,
        weights_summary='full',
        default_save_path=hparams.checkpoint_path,
        max_nb_epochs=hparams.max_nb_epochs,
        gpus=hparams.gpus,
        fast_dev_run=hparams.dev_run)
&lt;/denchmark-code&gt;

It's not saving the model weights.
Even leaving the checkpoint_callback in default mode it' not saving the weights.
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots
If applicable, add screenshots to help explain your problem.
Desktop (please complete the following information):

OS: Linux
os kernel version: 21~18.04.1-Ubuntu SMP Mon Oct 7 04:51:28 UTC 2019
os release version: 5.0.0-1021-gcp
os platform: Linux-5.0.0-1021-gcp-x86_64-with-Ubuntu-18.04-bionic
pytorch-lightning : 0.5.3
PyTorch Version : 1.3.0
== cuda libs  ===================================================
/usr/local/cuda-10.0/doc/man/man7/libcudart.7
/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-10.0/lib64/libcudart_static.a
/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130

	</description>
	<comments>
		<comment id='1' author='suvojit-0x55aa' date='2019-11-15T06:55:00Z'>
		
Is the run completely finishing? The default callback saves at the end of an epoch.
You have val_loss set. Are you sure you're logging it correctly? Check for an error message like Can save best model...

		</comment>
		<comment id='2' author='suvojit-0x55aa' date='2019-11-15T07:04:30Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
  yes the run completely finishes
My two function are setup like this:
&lt;denchmark-code&gt;def validation_step(self, batch, batch_nb):
        input, label = batch
        sel, mask, alpha, predgrid, _ = self.forward(input)
        loss, l1, l2, l3, l4 = self.criterion(sel, label, alpha, predgrid)
        self.last_val_images = input

        log_dict = {
            'valid/loss': loss,
            'valid/euclidean_loss': l1,
            'valid/sse_loss': l2,
            'valid/siamese_loss': l3,
            'valid/symmetry_loss': l4
        }

        return {'val_loss': loss, 'log': log_dict}

def validation_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        log_dict = {
            'valid/loss':
            avg_loss,
            'valid/euclidean_loss':
            torch.stack(
                [x['log']['valid/euclidean_loss'] for x in outputs]).mean(),
            'valid/sse_loss':
            torch.stack([x['log']['valid/sse_loss'] for x in outputs]).mean(),
            'valid/siamese_loss':
            torch.stack(
                [x['log']['valid/siamese_loss'] for x in outputs]).mean(),
            'valid/symmetry_loss':
            torch.stack(
                [x['log']['valid/symmetry_loss'] for x in outputs]).mean()
        }

        return {'avg_val_loss': avg_loss, 'log': log_dict}
&lt;/denchmark-code&gt;

Is this the correct way to do it ?
		</comment>
		<comment id='3' author='suvojit-0x55aa' date='2019-11-15T07:56:36Z'>
		I'm on my phone right now so I can't verify, but try changing         monitor='val_loss', to         monitor='avg_val_loss',
		</comment>
		<comment id='4' author='suvojit-0x55aa' date='2019-11-18T13:42:51Z'>
		&lt;denchmark-link:https://github.com/jeffling&gt;@jeffling&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  making the change worked but it throws up the following error.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/stn_env/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/home/ubuntu/miniconda3/envs/stn_env/lib/python3.7/site-packages/tensorboard/summary/writer/event_file_writer.py", line 211, in run
    self._record_writer.write(data)
  File "/home/ubuntu/miniconda3/envs/stn_env/lib/python3.7/site-packages/tensorboard/summary/writer/record_writer.py", line 39, in write
    self._writer.write(header + header_crc + data + footer_crc)
  File "/home/ubuntu/miniconda3/envs/stn_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 474, in write
    self.fs.append(self.filename, file_content, self.binary_mode)
  File "/home/ubuntu/miniconda3/envs/stn_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 139, in append
    self._write(filename, file_content, "ab" if binary_mode else "a")
  File "/home/ubuntu/miniconda3/envs/stn_env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 143, in _write
    with io.open(filename, mode, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: b'/home/ubuntu/3DMMasSTN-Pytorch/weights/lightning_logs/version_0/tf/events.out.tfevents.1574084352.gpunew9.4187.0'
&lt;/denchmark-code&gt;

My  is actually a dir like 
Is there some way to save it in  directory ? Also &lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/docs/Trainer/Checkpointing.md#model-saving&gt;according to the docs&lt;/denchmark-link&gt;
 model should check point automatically without and explicit

option in the trainer.
		</comment>
		<comment id='5' author='suvojit-0x55aa' date='2019-11-18T14:23:55Z'>
		Turns out changing,
&lt;denchmark-code&gt;def validation_end(self, outputs):
        ...
        return {'avg_val_loss': avg_loss, 'log': log_dict}
&lt;/denchmark-code&gt;

to
&lt;denchmark-code&gt;def validation_end(self, outputs):
        ...
        return {'val_loss': avg_loss, 'log': log_dict}
&lt;/denchmark-code&gt;

starts checkpointing without explicitly passing the checkpoint_callback option. This should be added to docs.
		</comment>
		<comment id='6' author='suvojit-0x55aa' date='2019-11-19T21:14:38Z'>
		So for the checkpoint_callback to work, we don't need to pass it to the trainer?
		</comment>
		<comment id='7' author='suvojit-0x55aa' date='2019-11-21T05:57:05Z'>
		@Jiequannnnnnnnnn yes
		</comment>
	</comments>
</bug>