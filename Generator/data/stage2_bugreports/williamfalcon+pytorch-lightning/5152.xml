<bug id='5152' author='rohitgr7' open_date='2020-12-15T22:47:14Z' closed_time='2021-01-10T18:56:28Z'>
	<summary>Allow configuring optimizer_step based on the gradients.</summary>
	<description>
As of now, there is no way to skip optimizer_step based on the gradients after they are calculated since the backward resides inside the closure and this closure is passed to the optimizer.step. Also if training_step returns None it still calls optimizer_step due to the closure which I think it should not if there are no pending accumulated gradients.



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 707 to 718
      in
      fde972f






 def train_step_and_backward_closure(): 



 result = self.training_step_and_backward( 



 split_batch, 



 batch_idx, 



 opt_idx, 



 optimizer, 



 self.trainer.hiddens 



     ) 



 return None if result is None else result.loss 



 



 # optimizer step 



 self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure) 








pytorch-lightning/pytorch_lightning/core/optimizer.py


        Lines 127 to 136
      in
      fde972f






 if trainer.on_tpu: 



 with trainer.profiler.profile(profiler_name): 



 xm.optimizer_step(optimizer, optimizer_args={'closure': closure, **kwargs}) 



 



 elif trainer.amp_backend is not None: 



 trainer.precision_connector.backend.optimizer_step(trainer, optimizer, closure) 



 



 else: 



 with trainer.profiler.profile(profiler_name): 



 optimizer.step(closure=closure, *args, **kwargs) 





&lt;denchmark-h:h2&gt;Alternate option&lt;/denchmark-h&gt;


Use manual optimization
zero_grad after backward the gradients and let optimizer_step happen which will have no effect.

&lt;denchmark-h:h2&gt;Expected behavior&lt;/denchmark-h&gt;

Would like to have this support with automatic optimization with no hacky way (2nd point above)
	</description>
	<comments>
		<comment id='1' author='rohitgr7' date='2020-12-16T09:38:44Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 mind have a look :]
		</comment>
		<comment id='2' author='rohitgr7' date='2021-01-08T14:36:37Z'>
		Hey &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
,
After brainstorming with &lt;denchmark-link:https://github.com/carmocca&gt;@carmocca&lt;/denchmark-link&gt;
, we feel there is no much we can do in  due to the design of Pytorch Optimizer.
&lt;denchmark-code&gt;    @torch.no_grad()
    def step(self, closure=None):
        """Performs a single optimization step.

        Arguments:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
&lt;/denchmark-code&gt;

We could remove the closure from the optimizer, but it would break some users training when their optimizer need the closure.
We can add in the doc manual_optimization could be a way to have better control there.
What are your thoughts ?
		</comment>
		<comment id='3' author='rohitgr7' date='2021-01-10T18:56:28Z'>
		cool üëç ... manual optimization to the rescue.
		</comment>
	</comments>
</bug>