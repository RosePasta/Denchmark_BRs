<bug id='4829' author='simonrouse9461' open_date='2020-11-24T07:06:21Z' closed_time='2020-12-07T10:42:23Z'>
	<summary>"MisconfigurationException: PyTorch XLA not installed." Even though PyTorch XLA is installed</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm using exactly the same command to install pytorch xla as shown in the doc:
curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev
However, I still got this error:
&lt;denchmark-code&gt;GPU available: False, used: False
TPU available: True, using: 1 TPU cores
training on 1 TPU cores
---------------------------------------------------------------------------
MisconfigurationException                 Traceback (most recent call last)
&lt;ipython-input-11-1f9f6fbe4f6c&gt; in &lt;module&gt;()
----&gt; 1 test_x(tmpdir)

2 frames
/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/tpu_accelerator.py in setup(self, model)
     61         # TODO: Move this check to Trainer __init__ or device parser
     62         if not TPU_AVAILABLE:
---&gt; 63             raise MisconfigurationException('PyTorch XLA not installed.')
     64 
     65         # see: https://discuss.pytorch.org/t/segfault-with-multiprocessing-queue/81292/2

MisconfigurationException: PyTorch XLA not installed.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1dz4NY233VoBv9SIb22JnrWiqYBHFpxCm?usp=sharing&gt;https://colab.research.google.com/drive/1dz4NY233VoBv9SIb22JnrWiqYBHFpxCm?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

run through the colab notebook in the link above
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

no error
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Note: Bugs with code are solved faster ! Colab Notebook should be made public !


IDE: Please, use our python bug_report_model.py template.


Colab Notebook: Please copy and paste the output from our environment collection script (or fill out the checklist below manually).


You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;


CUDA:

GPU:
available:         False
version:           None


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.8.0a0+4ed7f36
pytorch-lightning: 1.0.7
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='simonrouse9461' date='2020-11-24T07:07:05Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='simonrouse9461' date='2020-11-24T16:53:08Z'>
		&lt;denchmark-link:https://github.com/simonrouse9461&gt;@simonrouse9461&lt;/denchmark-link&gt;
 can you make your notebook public?
		</comment>
		<comment id='3' author='simonrouse9461' date='2020-11-24T16:57:58Z'>
		
@simonrouse9461 can you make your notebook public?

Sorry, I forgot to check. It's now public.
		</comment>
		<comment id='4' author='simonrouse9461' date='2020-11-24T18:29:03Z'>
		when I changed the time here to 20 sec, it worked.



pytorch-lightning/pytorch_lightning/utilities/xla_device_utils.py


         Line 44
      in
      fb0278a






 proc.join(10) 





not sure if this is the real issue. cc &lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='simonrouse9461' date='2020-11-24T22:33:28Z'>
		Hmm I tried to ran the notebook and it worked just fine for me
		</comment>
		<comment id='6' author='simonrouse9461' date='2020-11-25T01:17:02Z'>
		
when I changed the time here to 20 sec, it worked.



pytorch-lightning/pytorch_lightning/utilities/xla_device_utils.py


         Line 44
      in
      fb0278a






 proc.join(10) 





not sure if this is the real issue. cc @lezwon

It might be the issue. The behavior is not consistent every time. Have a PR in works which changes this to 20 secs. &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 maybe we could quickly make this change for now and see if the behavior is more consistent.
		</comment>
		<comment id='7' author='simonrouse9461' date='2020-11-25T06:00:42Z'>
		Yep checked again. It's working now with 10 sec. I believe it has to do something with the availability.
		</comment>
		<comment id='8' author='simonrouse9461' date='2020-11-27T10:05:03Z'>
		I was also facing the same issue in my code (it occurs randomly and bothered me quite a bit). Changing it to 20 secs has resolved the issue in my case.
		</comment>
		<comment id='9' author='simonrouse9461' date='2020-11-27T14:51:14Z'>
		How do you change it to 20 seconds in the source file?
		</comment>
		<comment id='10' author='simonrouse9461' date='2020-11-27T18:04:41Z'>
		Facing the same issue but changing to 20 secs (or some other larger value) does not solve the issue for me. Also, it's failing right away, so don't see how this could be a timing issue for me...
Unfortunately, I can't share the notebook but following the instructions as per the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/tpu.html&gt;readme&lt;/denchmark-link&gt;
.
However, the issue does seem to reside in this multiprocessing wrapper as this device_type = XLADeviceUtils._fetch_xla_device_type(device) (Line 82 from xla_device_utils) does evaluate to TPU for me..
(One a side note: I don't quite understand where _is_device_tpu() is filling the queue from the mp wrapper)
		</comment>
		<comment id='11' author='simonrouse9461' date='2020-11-27T18:23:59Z'>
		
How do you change it to 20 seconds in the source file?

&lt;denchmark-link:https://github.com/armandnaessens&gt;@armandnaessens&lt;/denchmark-link&gt;
 there are multiple ways, one way on colab you can do:
from pytorch_lightning.utilities.xla_device_utils import XLADeviceUtils
then hover over XLADeviceUtils it will give you an option to View Source, click on it, update the value there from 10 to 20, save it, restart the notebook and run again.
		</comment>
		<comment id='12' author='simonrouse9461' date='2020-11-27T18:27:26Z'>
		AFAIR the reason why we do it this way is because if someone installs XLA on a device with no TPU, then just checking XLA installation to verify a TPU device exist or not is not correct. We need to check the device by calling xm.xla_device(), since pytorch doesn't have any functionality to check TPU device just like for GPU (torch.cuda.is_available()). Anyone got suggestions how this can be done in a better way if possible??
		</comment>
		<comment id='13' author='simonrouse9461' date='2020-11-27T19:26:28Z'>
		Changing it to 20 didn't work for me. I then changed it to 100, and it worked. Is there a more robust way to fix it than setting a magic number?
		</comment>
		<comment id='14' author='simonrouse9461' date='2020-11-27T19:29:42Z'>
		cc &lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 any suggestion or an alternative to check TPU availability here?
		</comment>
		<comment id='15' author='simonrouse9461' date='2020-11-28T01:08:19Z'>
		There was initially no timeout for checking the TPU availability. However, in some cases this resulted in lightning hanging during imports, so a timeout of 10 was added.  Kaggle/Colab TPU initialization time differs, due to which I suspect we are facing this issue. One way to deal with this might be to lazy check for TPU device. i.e only during .fit. Rest of the times we can just check if XLA is available.
		</comment>
		<comment id='16' author='simonrouse9461' date='2020-11-29T08:40:02Z'>
		This may be fixed in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4309&gt;#4309&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='17' author='simonrouse9461' date='2020-11-29T17:01:41Z'>
		I've been using TPUs a lot and can confirm this error only happens occasionally in colab, and I just restart the runtime until the TPUs have loaded.
&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 I think lazy checking would be a good idea like you say because it's often only a matter of seconds, but if the TPUs haven't loaded before  is called, perhaps there is a way to print a "warning: waiting for TPUs to start" and wait? Somehow having it wait at that point vs. throwing an error is probably ideal incase time was spent preprocessing and building variables before . But if that's not possible a more descriptive error on what happened would let people know it's less of an error and more of just needing to wait for the TPUs to start.
		</comment>
		<comment id='18' author='simonrouse9461' date='2020-11-30T10:18:43Z'>
		el free to re-open if needed üê∞
		</comment>
		<comment id='19' author='simonrouse9461' date='2020-11-30T18:52:05Z'>
		opening this because it needs to be fixed. Came up again on forums.
&lt;denchmark-link:https://forums.pytorchlightning.ai/t/misconfigurationexception-pytorch-xla-not-installed/429&gt;https://forums.pytorchlightning.ai/t/misconfigurationexception-pytorch-xla-not-installed/429&lt;/denchmark-link&gt;

		</comment>
		<comment id='20' author='simonrouse9461' date='2020-12-03T03:25:33Z'>
		&lt;denchmark-link:https://github.com/simonrouse9461&gt;@simonrouse9461&lt;/denchmark-link&gt;
 can you try the master branch and let us know if the issue is still occurring?
		</comment>
		<comment id='21' author='simonrouse9461' date='2020-12-07T03:00:03Z'>
		I got this error again on Colab today.
		</comment>
		<comment id='22' author='simonrouse9461' date='2020-12-07T05:41:41Z'>
		&lt;denchmark-link:https://github.com/ayushm-agrawal&gt;@ayushm-agrawal&lt;/denchmark-link&gt;
 did you try with the master branch?
		</comment>
		<comment id='23' author='simonrouse9461' date='2020-12-07T10:42:23Z'>
		&lt;denchmark-link:https://github.com/simonrouse9461&gt;@simonrouse9461&lt;/denchmark-link&gt;
 I have run the notebook you shared and have not observed any issue/error...
in general, I would recommend using a stable version like 1.6 or 1.7 not nightly builds
pls fee free to reopen if you have problems also on stable XLA versions 
		</comment>
		<comment id='24' author='simonrouse9461' date='2020-12-08T00:04:45Z'>
		I had the same problem on Colab + XLA 1.7.
Changing the join() timeout to 100 as &lt;denchmark-link:https://github.com/simonrouse9461&gt;@simonrouse9461&lt;/denchmark-link&gt;
 said got rid of the issue: slowly, the messages like
&lt;denchmark-code&gt;INIT TPU local core: 6, global rank: 6 with XLA_USE_BF16=None
INFO:lightning:INIT TPU local core: 6, global rank: 6 with XLA_USE_BF16=None
&lt;/denchmark-code&gt;

started appearing, slowly and one at a time (up to 8).
With the default join timeout, running my training script on Colab TPUs was a hit or miss. To overcome this, i put my invocation in a while true; do py train.py &amp;&amp; exit 0 ; done loop just to find that moment when the TPU becomes readily available... and it was only possible when setting tpu_cores=1 in the Trainer, because setting it to 8 would just never work.
		</comment>
		<comment id='25' author='simonrouse9461' date='2020-12-08T00:32:32Z'>
		
@ayushm-agrawal did you try with the master branch?

I tried installing on Colab using the following command.
pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade
Got this error
&lt;denchmark-code&gt;  Cloning https://github.com/PytorchLightning/pytorch-lightning.git (to revision master) to /tmp/pip-req-build-cekbn4x_
  Running command git clone -q https://github.com/PytorchLightning/pytorch-lightning.git /tmp/pip-req-build-cekbn4x_
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
ERROR: Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.6/dist-packages/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /tmp/tmpsw5o0jxw Check the logs for full command output.```
&lt;/denchmark-code&gt;

		</comment>
		<comment id='26' author='simonrouse9461' date='2020-12-08T01:04:51Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
  I meet this error on colab TPU .
I install the packages following the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/tpu.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/tpu.html&lt;/denchmark-link&gt;
:
&lt;denchmark-code&gt;!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev
&lt;/denchmark-code&gt;

		</comment>
		<comment id='27' author='simonrouse9461' date='2020-12-08T11:25:42Z'>
		Btw, a quick way for anyone getting this issue to change the join timeout is the following:
from pytorch_lightning.utilities import xla_device_utils
!sed 's/join(10)/join(100)/' \
    {xla_device_utils.__file__} \
    -i # IN-PLACE
		</comment>
	</comments>
</bug>