<bug id='606' author='awaelchli' open_date='2019-12-08T00:30:41Z' closed_time='2019-12-09T18:32:50Z'>
	<summary>Early Stopping kicks in at min_epochs + 2 instead of min_epochs</summary>
	<description>
&lt;denchmark-h:h3&gt;Describe the bug&lt;/denchmark-h&gt;

I was working on a fix for &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/524&gt;#524&lt;/denchmark-link&gt;
 and found that early stopping starts to kick in at epoch 3 despite min_epochs = 1.
&lt;denchmark-h:h4&gt;To Reproduce&lt;/denchmark-h&gt;

run basic_examples/gpu_template.py and log the callback calls every epoch.
&lt;denchmark-h:h4&gt;Expected behavior&lt;/denchmark-h&gt;

When setting min_epochs=n (counting from 1), we should evaluate early stopping at the end of epoch n.
&lt;denchmark-h:h4&gt;Proposed fix:&lt;/denchmark-h&gt;

I propose to change &lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/58cc6e13b9999161a526b83063c983750e6553b1/pytorch_lightning/trainer/training_loop.py#L322&gt;this&lt;/denchmark-link&gt;
 line in the training loop:

to



Why the "-1"? The epoch variable in the training loop starts at 0, but the Trainer argument min_epochs starts counting at 1.


Why the "&gt;="? The early stop check is done at the end of each epoch, hence the epoch counter will be = to min_epochs after min_epochs have passed.


Desktop (please complete the following information):

OS: Linux
Version: master

	</description>
	<comments>
		<comment id='1' author='awaelchli' date='2019-12-09T12:49:31Z'>
		submit the PR for this?
		</comment>
		<comment id='2' author='awaelchli' date='2019-12-09T13:20:42Z'>
		yep. was waiting for an approval.
		</comment>
	</comments>
</bug>