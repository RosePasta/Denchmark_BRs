<bug id='4788' author='jw3126' open_date='2020-11-20T14:12:42Z' closed_time='2021-01-19T14:05:00Z'>
	<summary>Issue with ddp + gradient checkpointing</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

From &lt;denchmark-link:https://forums.pytorchlightning.ai/t/gradient-checkpointing-ddp-nan/398/&gt;the forum&lt;/denchmark-link&gt;
. The &lt;denchmark-link:https://gist.github.com/jw3126/29cbb8177a9f4acfb66fc3a3ed7cbf24&gt;model&lt;/denchmark-link&gt;
 runs fine on one gpu, but loss becomes  on multiple gpus.
This seems to be an issue when combining gradient checkpointing and ddp.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run the following with  and 
&lt;denchmark-link:https://gist.github.com/jw3126/29cbb8177a9f4acfb66fc3a3ed7cbf24&gt;https://gist.github.com/jw3126/29cbb8177a9f4acfb66fc3a3ed7cbf24&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Loss should be finite independently of gpus=1 or gpus=4
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - Tesla K80
                - Tesla K80
                - Tesla K80
                - Tesla K80
        - available:         True
        - version:           10.1
* Packages:
        - numpy:             1.18.5
        - pyTorch_debug:     True
        - pyTorch_version:   1.7.0+cu101
        - pytorch-lightning: 1.0.5
        - tqdm:              4.51.0
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - ELF
        - processor:         x86_64
        - python:            3.6.9
        - version:           #32~18.04.1-Ubuntu SMP Tue Oct 6 10:03:22 UTC 2020
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='jw3126' date='2020-11-20T14:13:29Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='jw3126' date='2020-11-22T11:18:06Z'>
		I've been able to reproduce your results; with gpus=1 it trains stabily, and with gpus=4 it collapses to nan.
I don't know anything about gradient checkpointing, but some time ago I was able to fix my own instability issues by setting amsgrad=True or eps=1e-2 for Adam: I tested this on your code but that didn't help.
Just putting an egg into the basket üôÇ
		</comment>
		<comment id='3' author='jw3126' date='2020-12-11T15:27:06Z'>
		I too would love to see gradient checkpointing working with lightning and ddp. FairScale seems to use gradient checkpointing with ddp in their tests successfully, so it should be possible.
&lt;denchmark-link:https://github.com/facebookresearch/fairscale/blob/66b2b5145ef9799f64e8056858c43609ec936bea/tests/nn/pipe/test_checkpoint_ddp.py&gt;https://github.com/facebookresearch/fairscale/blob/66b2b5145ef9799f64e8056858c43609ec936bea/tests/nn/pipe/test_checkpoint_ddp.py&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='jw3126' date='2020-12-13T07:39:49Z'>
		I was able to use DDP and Pytorch gradient checkpointing using the following:
&lt;denchmark-code&gt;ddp = pytorch_lightning.plugins.ddp_plugin.DDPPlugin(find_unused_parameters=False)
trainer = pytorch_lightning.Trainer(plugins=[ddp])
trainer.fit(model)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='jw3126' date='2021-01-12T12:15:20Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>