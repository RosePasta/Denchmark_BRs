<bug id='5264' author='Shreeyak' open_date='2020-12-25T08:18:12Z' closed_time='2021-01-01T13:49:55Z'>
	<summary>Using GPU1,2 on a 3-GPU system causes crash with DDP</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

On a system with 3 GPUs, if I pass gpus=[1,2] to the Trainer, it causes an error.
Note: GPU0, if visible, will always be used, even if the trainer is passed other gpus ids.

If all 3 gpus are visible and Trainer is give gpus=[2], then gpu2 will be used for training, but resources are still consumed on gpu0.
If I pass gpus=[0,2], gpu1 still gets loaded and the process hangs.
The same way, if I pass gpus=[1,2], gpu0 is still loaded and the below error is thrown:

&lt;denchmark-code&gt;Traceback (most recent call last):
  File ".../seg_lapa/train.py", line 77, in main
    trainer.fit(model, datamodule=dm)
  File ".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File ".../lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File ".../lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 303, in ddp_train
    self.barrier('ddp_setup')
  File ".../lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 180, in barrier
    torch_distrib.barrier(group=self.ddp_plugin.data_parallel_group)
  File ".../lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File ".../seg_lapa/train.py", line 77, in main
    trainer.fit(model, datamodule=dm)
  File ".../lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File ".../lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File ".../lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 303, in ddp_train
    self.barrier('ddp_setup')
  File ".../lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 180, in barrier
    torch_distrib.barrier(group=self.ddp_plugin.data_parallel_group)
  File ".../lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel&lt;/denchmark-h&gt;

Error happens with 3 GPUs minimum, per my testing. Cannot use Colab.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Have at least 3 gpus visible and pass 2 gpus other than gpu0 in DDP mode.
Execute this gist (based on ): &lt;denchmark-link:https://gist.github.com/Shreeyak/0e57d5b227546f08d003e9cdf80e3a49&gt;https://gist.github.com/Shreeyak/0e57d5b227546f08d003e9cdf80e3a49&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;export CUDA_VISIBLE_DEVICES=0,1,2
wget https://gist.githubusercontent.com/Shreeyak/0e57d5b227546f08d003e9cdf80e3a49/raw/2073f1a477fc6528cf4aeabc409ab7e582d1ec4a/bug_report_model.py
python bug_report_model.py
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I would expect behaviour similar to if I'd made only the gpus I want visible before running the code.
I.e., it gpus=1,2 is passed, gpu0 should not be loaded at all and all training should run on gpu1,2 only.
And the script should neither hang nor crash.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Note: Bugs with code are solved faster ! Colab Notebook should be made public !


IDE: Please, use our python bug_report_model.py template.


CUDA:

GPU:

GeForce RTX 3090
GeForce RTX 3090
GeForce RTX 3090


available:         True
version:           11.0



Packages:

numpy:             1.19.4
pyTorch_debug:     False
pyTorch_version:   1.7.1+cu110
pytorch-lightning: 1.1.2
tqdm:              4.54.1



System:

OS:                Linux
architecture:

64bit
ELF


processor:         x86_64
python:            3.8.0
version:           #64~18.04.1-Ubuntu SMP Wed Dec 9 17:11:11 UTC 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I'm not seeing this error with DP. DP behaves as expected.
	</description>
	<comments>
		<comment id='1' author='Shreeyak' date='2020-12-25T17:29:10Z'>
		Hi, this is unfortunately an error that occurs only with torch 1.7. I can confirm it works fine with torch 1.6.
		</comment>
		<comment id='2' author='Shreeyak' date='2020-12-26T04:05:16Z'>
		Made some progress debugging this. By setting NCCL_DEBUG="INFO"
I see that initializing the distributed looks fine:
&lt;denchmark-code&gt;cvg-ws-aw:96742:96931 [2] NCCL INFO comm 0x7f6d4c001060 rank 1 nranks 2 cudaDev 2 busId 42000 - Init COMPLETE
cvg-ws-aw:96696:96930 [1] NCCL INFO comm 0x7fc4bc001060 rank 0 nranks 2 cudaDev 1 busId a000 - Init COMPLETE

&lt;/denchmark-code&gt;

But then later on I get this warning in the output:
&lt;denchmark-code&gt;cvg-ws-aw:96696:96936 [1] init.cc:573 NCCL WARN Duplicate GPU detected : rank 1 and rank 2 both on CUDA device a000
&lt;/denchmark-code&gt;

putting this here for documentation purposes and will continue to debug ..., and as previously mentioned happens with pytorch &gt;= 1.7 only!
		</comment>
		<comment id='3' author='Shreeyak' date='2020-12-26T06:15:35Z'>
		It seems the issue is fixed in torch 1.8. &lt;denchmark-link:https://github.com/Shreeyak&gt;@Shreeyak&lt;/denchmark-link&gt;
 can you please confirm that installing torch 1.8 solves it for you too?
&lt;denchmark-code&gt;pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html --upgrade
&lt;/denchmark-code&gt;

Or the alternative is to use pytorch &lt; 1.7.
Let me know if this is an acceptable solution.
		</comment>
		<comment id='4' author='Shreeyak' date='2021-01-01T13:48:25Z'>
		Returned to this today. Interestingly, I got an error this time (same env as last time, torch 1.7):
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "bug_report_model.py", line 154, in &lt;module&gt;
  File "/home/shrek/work/cleargrasp2-pycharm-remote/pytorch-lightning-segmentation-lapa/scripts/bug_report_model.py", line 154, in &lt;module&gt;
    test_run()
    test_run()
  File "/home/shrek/work/cleargrasp2-pycharm-remote/pytorch-lightning-segmentation-lapa/scripts/bug_report_model.py", line 148, in test_run
  File "bug_report_model.py", line 148, in test_run
    trainer.fit(model, train_data, val_data)
    trainer.fit(model, train_data, val_data)
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
    results = self.accelerator_backend.train()
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 152, in train
    results = self.ddp_train(process_idx=self.task_idx, model=model)
    results = self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 303, in ddp_train
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 303, in ddp_train
    self.barrier('ddp_setup')
    self.barrier('ddp_setup')
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 180, in barrier
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_accelerator.py", line 180, in barrier
    torch_distrib.barrier(group=self.ddp_plugin.data_parallel_group)
    torch_distrib.barrier(group=self.ddp_plugin.data_parallel_group)
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
  File "/home/shrek/miniconda3/envs/cleargrasp/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1960, in barrier
    work = _default_pg.barrier()
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='Shreeyak' date='2021-01-01T13:49:55Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Thank you! pytorch 1.8 did solve the issue.
Since it's a bug due to pytorch, can't do anything else but use a different version :)
		</comment>
	</comments>
</bug>