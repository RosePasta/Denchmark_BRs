<bug id='5073' author='VinhLoiIT' open_date='2020-12-11T02:16:06Z' closed_time='2020-12-16T06:44:31Z'>
	<summary>TensorRunningAccum does not reset as it assumed to be</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I looked into the source code in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/7e8673debd2f8bc8ae639063c4aee71f774fc304/pytorch_lightning/trainer/supporters.py#L51&gt;this file&lt;/denchmark-link&gt;
 and found this snippet that does not make sense to me.
    def reset(self) -&gt; None:
        """Empty the accumulator."""
        self = TensorRunningAccum(self.window_length)
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I made a short code snippet to verify this behavior as follows.
from pytorch_lightning.trainer.supporters import TensorRunningAccum

running_accum = TensorRunningAccum(window_length=10)
running_accum.current_idx = 100     # just want to change the value
print(id(running_accum))
print(running_accum.current_idx)

running_accum.reset()
print(running_accum.current_idx)    # current_idx is expected to be 0
print(id(running_accum))
And the result is
&lt;denchmark-code&gt;139686191621984
100
100
139686191621984
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='VinhLoiIT' date='2020-12-11T22:22:09Z'>
		Yep, seems like a bug. I guess the original author wanted to do:
def reset(self):
    self.__init__(self.window_length)
Want to open a PR?
		</comment>
	</comments>
</bug>