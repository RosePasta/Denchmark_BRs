<bug id='1997' author='jeremyjordan' open_date='2020-05-29T02:01:15Z' closed_time='2020-05-30T05:40:20Z'>
	<summary>TestTube version attribute does not perform as documented</summary>
	<description>
&lt;denchmark-h:h3&gt;üêõ Bug&lt;/denchmark-h&gt;

The docs for TestTube logger state:

If version is not specified the logger inspects the save for existing versions, then automatically assigns the next available version.

However, this does not happen.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;&gt;&gt;&gt; from pytorch_lightning.loggers import TestTubeLogger
&gt;&gt;&gt; logger = TestTubeLogger('tb_logs', name='my_model')
&gt;&gt;&gt; logger.version
&gt;&gt;&gt; assert logger.version is None
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I would expect the TestTube logger performs something similar to:
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/tensorboard.py#L177&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/tensorboard.py#L177&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='jeremyjordan' date='2020-05-29T20:42:09Z'>
		Since the experiment is not initialized in the constructor no version is assigned. Either the docs needs to be updated or the experiment should be initialized within the constructor.
		</comment>
		<comment id='2' author='jeremyjordan' date='2020-05-29T21:00:00Z'>
		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 mind update docs?
		</comment>
		<comment id='3' author='jeremyjordan' date='2020-05-29T22:03:28Z'>
		I would push for an implementation similar to the Tensorboard logger rather than simply updating the docs. The reason is that the ModelCheckpoint checks the logger version when the Trainer is initialized (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1504/files#diff-78066f556babcb77bb4490eca66c7ec1L50&gt;https://github.com/PyTorchLightning/pytorch-lightning/pull/1504/files#diff-78066f556babcb77bb4490eca66c7ec1L50&lt;/denchmark-link&gt;
 - I created this issue based on observing the problem in that PR) and right now (in my PR) it is saving checkpoints to  which is less than ideal.
		</comment>
		<comment id='4' author='jeremyjordan' date='2020-05-29T23:06:18Z'>
		I checked with master only and it's working fine there.
		</comment>
	</comments>
</bug>