<bug id='3053' author='yukw777' open_date='2020-08-19T20:11:59Z' closed_time='2020-08-20T11:19:12Z'>
	<summary>load_from_checkpoint() doesn't work when a LightningModule inherits from typing.Generic</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When a LightningModule with saved hyperparameters inherits from , hyperparameters saved in the checkpoint file are not loaded automatically, causing an error. When  calls  to gather the list of arguments of the LightningModule that inherits from ,  returns  instead of the actual arguments, because  implements an empty  (the execution path ends up here: &lt;denchmark-link:https://github.com/python/cpython/blob/3.8/Lib/inspect.py#L2324&gt;https://github.com/python/cpython/blob/3.8/Lib/inspect.py#L2324&lt;/denchmark-link&gt;
). As a result, PL filters out all the saved hyperparameters from the checkpoint, which results in an error when trying to instantiate the LightningModule. I'd assume this would happen when a LightningModule inherits from any class that implements  such as .
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Create a LightningModule that inherits from typing.Generic with some hyperparameters, fit it, then try to load it from a checkpoint.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import torch
import torch.nn.functional as F
import pytorch_lightning as pl

from typing import Generic, TypeVar
from torch.utils.data import DataLoader

T = TypeVar("T")


class GenericLitClassifier(Generic[T], pl.LightningModule):
    def __init__(self, dim):
        super().__init__()
        self.l1 = torch.nn.Linear(28 * 28, dim)
        self.save_hyperparameters()

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_nb):
        x, y = batch
        loss = F.cross_entropy(self(x), y)
        tensorboard_logs = {"train_loss": loss}
        return {"loss": loss, "log": tensorboard_logs}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)


class LitClassifier(GenericLitClassifier[str]):
    pass


class Dataset:
    def __getitem__(self, idx):
        return torch.ones(1, 784), 1

    def __len__(self):
        return 5


train_loader = DataLoader(Dataset(), batch_size=2)
model = LitClassifier(10)
trainer = pl.Trainer(max_epochs=5)
trainer.fit(model, train_loader)

for path, _ in trainer.checkpoint_callback.best_k_models.items():
    lm = LitClassifier.load_from_checkpoint(path)
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Even when a LightningModule inherits from any class that implements __new__() (e.g. typing.Generic), its hyperparameters should be loaded automatically from a checkpoint.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
        - available:         False
        - version:           None
* Packages:
        - numpy:             1.19.1
        - pyTorch_debug:     False
        - pyTorch_version:   1.5.1
        - pytorch-lightning: 0.8.5
        - tensorboard:       2.3.0
        - tqdm:              4.48.2
* System:
        - OS:                Darwin
        - architecture:
                - 64bit
                - 
        - processor:         i386
        - python:            3.7.7
        - version:           Darwin Kernel Version 18.7.0: Mon Apr 27 20:09:39 PDT 2020; root:xnu-4903.278.35~1/RELEASE_X86_64
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
	</comments>
</bug>