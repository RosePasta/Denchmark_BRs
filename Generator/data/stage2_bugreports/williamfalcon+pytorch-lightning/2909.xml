<bug id='2909' author='siahuat0727' open_date='2020-08-11T03:30:02Z' closed_time='2020-08-11T11:38:30Z'>
	<summary>load_from_checkpoint: TypeError: __init__() missing 1 required positional argument</summary>
	<description>
&lt;denchmark-h:h2&gt;‚ùì Questions and Help&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;What is your question?&lt;/denchmark-h&gt;

load_from_checkpoint: TypeError: init() missing 1 required positional argument
I have read the issues before, but the things different is my LightningModule is inherited from my self-defined LightningModule.
How to solve this problem or what is the best practice better suited to my needs?
&lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;

To reproduce the error:
import os
import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision import transforms
import pytorch_lightning as pl
from pytorch_lightning import Trainer

from argparse import Namespace

class _LitModel(pl.LightningModule):

    def __init__(self, hparams):
        super().__init__()
        if isinstance(hparams, dict):
            hparams = Namespace(**hparams)
        self.hparams = hparams
        self.l1 = torch.nn.Linear(28 * 28, hparams.classes)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        return {'val_loss': loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        return {'val_loss': avg_loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

class LitModel(_LitModel):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument('--classes', type=int, default=10)
parser.add_argument('--checkpoint', type=str, default=None)
hparams = parser.parse_args()

mnist_train = MNIST(os.getcwd(), train=True, download=True,
                    transform=transforms.ToTensor())
mnist_train = DataLoader(mnist_train, num_workers=1)
mnist_val = MNIST(os.getcwd(), train=False, download=False,
                  transform=transforms.ToTensor())
mnist_val = DataLoader(mnist_val, num_workers=1)

# A bit weird here. I just want to show `load_from_checkpoint` will fail.
if hparams.checkpoint is None:
    model = LitModel(hparams)
else:
    model = LitModel.load_from_checkpoint(hparams.checkpoint)

trainer = Trainer(max_epochs=2, limit_train_batches=2,
                  limit_val_batches=2, progress_bar_refresh_rate=0)
trainer.fit(model, mnist_train, mnist_val)
&lt;denchmark-h:h4&gt;Error msg&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Traceback (most recent call last):
  File "main.py", line 64, in &lt;module&gt;
    model = LitModel.load_from_checkpoint(hparams.checkpoint)
  File "/home/siahuat0727/.local/lib/python3.8/site-packages/pytorch_lightning/core/saving.py", line 138, in load_from_checkpoint
    model = cls._load_model_state(checkpoint, *args, **kwargs)
  File "/home/siahuat0727/.local/lib/python3.8/site-packages/pytorch_lightning/core/saving.py", line 174, in _load_model_state
    model = cls(*cls_args, **cls_kwargs)
  File "main.py", line 46, in __init__
    super().__init__(*args, **kwargs)
TypeError: __init__() missing 1 required positional argument: 'hparams'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;How to run to get the error&lt;/denchmark-h&gt;

$ python3 main.py 
$ python3 main.py --checkpoint lightning_logs/version_0/checkpoints/epoch\=1.ckpt
&lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;


OS: Linux
Packaging: pip
Version 0.9.0rc12

	</description>
	<comments>
		<comment id='1' author='siahuat0727' date='2020-08-11T07:18:51Z'>
		Did you try to call self.save_hyperparameters() in _LitModel?
Because it looks like hparams were not saved to checkpoint.
		</comment>
		<comment id='2' author='siahuat0727' date='2020-08-11T07:54:33Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;

Hihi, the result is the same.
It works if I directly use  instead of . So I think that's sth about inheritance.
		</comment>
		<comment id='3' author='siahuat0727' date='2020-08-11T07:56:41Z'>
		&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html&lt;/denchmark-link&gt;


Anything assigned to self.hparams will also be saved automatically.

		</comment>
		<comment id='4' author='siahuat0727' date='2020-08-11T08:37:39Z'>
		&lt;denchmark-link:https://github.com/siahuat0727&gt;@siahuat0727&lt;/denchmark-link&gt;
 I can confirm this is a bug. I fixed it and reduced your example to a minimal test case, so it won't break in the future. Thanks for providing a easy to reproduce script!
		</comment>
		<comment id='5' author='siahuat0727' date='2020-08-11T11:38:30Z'>
		Great job. Thanks!!
		</comment>
		<comment id='6' author='siahuat0727' date='2020-11-11T04:45:10Z'>
		The same issue appears with version 1.0.5 (0.9.0 is fine). Can you help with it?
(also track_grad_norm doesn't work in 0.9.0, so I have to switch to 1.0.5...)
		</comment>
		<comment id='7' author='siahuat0727' date='2020-11-12T18:30:36Z'>
		
The same issue appears with version 1.0.5 (0.9.0 is fine). Can you help with it?
(also track_grad_norm doesn't work in 0.9.0, so I have to switch to 1.0.5...)

Bump
		</comment>
		<comment id='8' author='siahuat0727' date='2020-11-14T03:39:35Z'>
		bump for version 1.0.6 as well
		</comment>
		<comment id='9' author='siahuat0727' date='2020-11-18T10:16:11Z'>
		same problem here on 1.0.4
		</comment>
		<comment id='10' author='siahuat0727' date='2020-11-18T14:39:14Z'>
		Apparently the problem is that checkpoint['hparams_name'] is empty. Maybe the problem is in the saving of the module when it is inherited?
		</comment>
		<comment id='11' author='siahuat0727' date='2020-11-18T17:23:35Z'>
		What solved it for me is that instead of passing the hparams, you can pass them as kwargs. So in your class use:
&lt;denchmark-code&gt;class my_pl_module(LightningModule):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.save_hyperparameters()
&lt;/denchmark-code&gt;

Still a bug though cause the hparams method is not yet deprecated.
		</comment>
		<comment id='12' author='siahuat0727' date='2020-11-18T17:38:44Z'>
		&lt;denchmark-link:https://github.com/stathius&gt;@stathius&lt;/denchmark-link&gt;
 yes, the "old" hparams method is not yet deprecated but it simply has conceptual flaws in terms of typing, that cannot be fixed as in a "bugfix". The solution we came up with here is to simply decouple two things:

Saving hyperparameters into the checkpoint
making hyperparameters accessible through a convenient self.hparams "namespace".

And the code you posted is exactly doing that, and this is the recommended way today.
		</comment>
	</comments>
</bug>