<bug id='1161' author='sneiman' open_date='2020-03-16T18:09:55Z' closed_time='2020-03-30T16:13:35Z'>
	<summary>multi-gpu ddp calls validation and testing loops too many times</summary>
	<description>
When using ddp with multiple gpus, each validation and test loop is called with the entire validation dataset for each gpu.
Expected behavior is that the dataset is divided appropriately across the gpus.
I am using current master (cloned Mar 14), Ubuntu 19.10, Cuda 10.1, python 3.7.5, pytorch 1.4, venv environment.
The problem appears to be in auto_add_sampler() in data_loading.py. It does not create a DistributedSampler for validation or test datasets.
	</description>
	<comments>
		<comment id='1' author='sneiman' date='2020-03-16T21:24:51Z'>
		Latest pull - 1 hour ago, no longer this behavior. Closing.
		</comment>
		<comment id='2' author='sneiman' date='2020-03-17T00:22:10Z'>
		Sorry - this issue still exists in some configurations. My proposed fix is not the total picture. Still investigating - will provide reproducible example.
		</comment>
		<comment id='3' author='sneiman' date='2020-03-17T03:29:44Z'>
		Testing underway. Will make PR tomorrow.
		</comment>
		<comment id='4' author='sneiman' date='2020-03-17T23:18:43Z'>
		Dont want to clutter up PR world if no one is interested in this. Let me know ...
		</comment>
		<comment id='5' author='sneiman' date='2020-03-18T21:54:08Z'>
		that sounds a good contribution to me... mind send a PR?
Any suggestion &lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
?
in a technical note when you refer some master state pls use coit hash as there can be multiple commits each day...
		</comment>
		<comment id='6' author='sneiman' date='2020-03-18T22:05:28Z'>
		will do on both pr, and hash ref
		</comment>
	</comments>
</bug>