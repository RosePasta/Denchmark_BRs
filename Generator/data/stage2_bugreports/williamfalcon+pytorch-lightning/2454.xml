<bug id='2454' author='fkodom' open_date='2020-07-01T17:03:45Z' closed_time='2020-07-02T10:43:13Z'>
	<summary>Why am I receiving '_GeneratorContextManager' error when using APEX with 'precision=16'?</summary>
	<description>
&lt;denchmark-h:h2&gt;Issue&lt;/denchmark-h&gt;

I'm converting an existing PyTorch model to PyTorch-Lightning, and running into an issue with mixed-precision training that I've never seen before.  Training works perfectly for 32-bit precision.  When using APEX and 16-bit precision, I get the following error message:
&lt;denchmark-code&gt;GPU available: True, used: True
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
Using APEX 16bit precision.
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=ddp
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Epoch 1:   0%|                                                                                                                                                                                                                        | 0/2 [00:00&lt;?, ?it/s]Traceback (most recent call last):
  File "train.py", line 119, in &lt;module&gt;
    train(
  File "train.py", line 79, in train
    pl.Trainer(
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 917, in fit
    self.spawn_ddp_children(model)
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 449, in spawn_ddp_children
    self.ddp_train(local_rank, model, is_master=True)
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 538, in ddp_train
    self.run_pretrain_routine(model)
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1100, in run_pretrain_routine
    self.train()
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 370, in train
    self.run_training_epoch()
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 452, in run_training_epoch
    batch_output = self.run_training_batch(batch, batch_idx)
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 625, in run_training_batch
    opt_closure_result = self.optimizer_closure(
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 805, in optimizer_closure
    model_ref.backward(self, closure_loss, optimizer, opt_idx)
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/pytorch_lightning/core/hooks.py", line 189, in backward
    loss.backward()
AttributeError: '_GeneratorContextManager' object has no attribute 'backward'
Exception ignored in: &lt;function tqdm.__del__ at 0x7f8985f97d30&gt;
Traceback (most recent call last):
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/tqdm/std.py", line 1086, in __del__
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/tqdm/std.py", line 1293, in close
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/tqdm/std.py", line 1471, in display
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/tqdm/std.py", line 1089, in __repr__
  File "/home/fkodom/miniconda3/envs/detr/lib/python3.8/site-packages/tqdm/std.py", line 1433, in format_dict
TypeError: cannot unpack non-iterable NoneType object
&lt;/denchmark-code&gt;

The heart of the issue appears to be this line:
&lt;denchmark-code&gt;AttributeError: '_GeneratorContextManager' object has no attribute 'backward'
&lt;/denchmark-code&gt;

It's possible that this is a bug with APEX.  However, I have several other Lightning projects that work fine with MP training.  There's nothing fancy about the model -- just a CNN training on MS-COCO.  Is this a known issue, or is there something I might be doing wrong?
	</description>
	<comments>
		<comment id='1' author='fkodom' date='2020-07-01T17:04:40Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='fkodom' date='2020-07-01T17:25:31Z'>
		I think it's fixed here &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2411&gt;#2411&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='fkodom' date='2020-07-02T08:52:22Z'>
		yep, this is a duplicate of my issue (mentioned above) and is already fixed!
		</comment>
		<comment id='4' author='fkodom' date='2020-07-02T10:43:13Z'>
		&lt;denchmark-link:https://github.com/fkodom&gt;@fkodom&lt;/denchmark-link&gt;
 please update to 0.8.4!
		</comment>
	</comments>
</bug>