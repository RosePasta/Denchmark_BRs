<bug id='2351' author='Uroc327' open_date='2020-06-24T20:54:04Z' closed_time='2020-06-25T14:06:21Z'>
	<summary>Model validation code is not called</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

My defined methods for validation_step as well as validation_epoch_end do not seem to get called.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Just call the provided code sample. Python should show the NotImplementedError. Instead the model completes 'successfully'.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader


class Dataset(torch.utils.data.IterableDataset):
    def __init__(self):
        super().__init__()

    def __iter__(self):
        def get_sample():
            for _ in range(5):
                yield torch.randn(20)
        return get_sample()

    def __len__(self):
        return 5

class Model(pl.LightningModule):
    def __init__(self):
        super().__init__()

        self.enc = nn.Linear(20, 10)
        self.dec = nn.Linear(10, 20)

    def forward(self, x):
        x = self.enc(x)
        x = F.relu(x)
        x = self.dec(x)
        return x

    def training_step(self, batch, batchIdx):
        x = self.forward(batch)
        return {'loss': torch.mean(x)}

    def validation_step(self, batch, batchIdx):
        raise NotImplementedError()
        x = self.forward(batch)
        return {'val_loss': torch.mean(x)}

    def validation_epoch_end(self, outputs):
        return {'val_loss': torch.mean(torch.stack([x['val_loss'] for x in outputs]))}

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters())

if __name__ == '__main__':
    trainer = pl.Trainer(num_sanity_val_steps=0)

    net = Model()
    dataset = Dataset()

    trainer.fit(net, train_dataloader=DataLoader(dataset, batch_size=8, num_workers=0), val_dataloaders=DataLoader(dataset, batch_size=8, num_workers=0))
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Instead, I'd expect the code sample above to fail.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Collecting environment information...
PyTorch version: 1.5.1+cu101
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Gentoo Base System release 2.7
GCC version: (Gentoo 9.3.0 p1) 9.3.0
CMake version: version 3.17.3

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce GT 730
Nvidia driver version: 440.82
cuDNN version: /opt/cuda/targets/x86_64-linux/lib/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.19.0
[pip3] pytorch-lightning==0.8.1
[pip3] torch==1.5.1+cu101
[pip3] torchvision==0.6.1+cu101
[conda] Could not collect
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='Uroc327' date='2020-06-25T11:10:24Z'>
		As a note, the validation code is called twice when leaving out the num_sanity_val_steps=0 parameter. After the two sanity runs, it is not called anymore.
		</comment>
		<comment id='2' author='Uroc327' date='2020-06-25T14:06:20Z'>
		Your dataset is wrong. That's causing the issue.
&lt;denchmark-code&gt;DataLoader(dataset, batch_size=8, num_workers=0, drop_last=True)
&lt;/denchmark-code&gt;

since your batch size and dataset length don't work out well.
		</comment>
		<comment id='3' author='Uroc327' date='2020-06-25T14:10:02Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 Changing the dataset to produce 128 samples does not change this either:
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader


class Dataset(torch.utils.data.IterableDataset):
    def __init__(self):
        super().__init__()

    def __iter__(self):
        def get_sample():
            for _ in range(128):
                yield torch.randn(20)
        return get_sample()

    def __len__(self):
        return 128

class Model(pl.LightningModule):
    def __init__(self):
        super().__init__()

        self.enc = nn.Linear(20, 10)
        self.dec = nn.Linear(10, 20)

    def forward(self, x):
        x = self.enc(x)
        x = F.relu(x)
        x = self.dec(x)
        return x

    def training_step(self, batch, batchIdx):
        x = self.forward(batch)
        return {'loss': torch.mean(x)}

    def validation_step(self, batch, batchIdx):
        raise NotImplementedError()
        x = self.forward(batch)
        return {'val_loss': torch.mean(x)}

    def validation_epoch_end(self, outputs):
        return {'val_loss': torch.mean(torch.stack([x['val_loss'] for x in outputs]))}

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters())

if __name__ == '__main__':
    trainer = pl.Trainer(num_sanity_val_steps=0)

    net = Model()
    dataset = Dataset()

    trainer.fit(net, train_dataloader=DataLoader(dataset, batch_size=8, num_workers=0), val_dataloaders=DataLoader(dataset, batch_size=8, num_workers=0))
&lt;denchmark-code&gt;&gt; python main.py
GPU available: True, used: False
TPU available: False, using: 0 TPU cores

  | Name | Type   | Params
--------------------------------
0 | enc  | Linear | 210   
1 | dec  | Linear | 220   
/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
Epoch 1000:   6%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                                                                                                                                                                                                                                     | 16/256 [00:00&lt;00:00, 1252.80it/s, loss=-2894.485, v_num=7]
&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='Uroc327' date='2020-06-25T15:05:59Z'>
		Also changing the  option does not make any difference (on 128 samples).
Could we please reopen this, &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 ?
		</comment>
	</comments>
</bug>