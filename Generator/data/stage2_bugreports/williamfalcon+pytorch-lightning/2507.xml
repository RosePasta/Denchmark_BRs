<bug id='2507' author='aeryen' open_date='2020-07-05T03:41:09Z' closed_time='2020-07-08T23:21:06Z'>
	<summary>Training with DataParallel (DP) is broken</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Currently, the distributed training backend DataParallel (DP) seems to be broken. Using DP will result in error
TypeError: zip argument #1 must support iteration. Below is the last few lines of the call stack:
&lt;denchmark-code&gt;  File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py", line 66, in forward
    return self.gather(outputs, self.output_device)
  File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 168, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 68, in gather
    res = gather_map(outputs)
  File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 62, in gather_map
    for k in out))
  File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 62, in &lt;genexpr&gt;
    for k in out))
  File "/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/parallel/scatter_gather.py", line 63, in gather_map
    return type(out)(map(gather_map, zip(*outputs)))
TypeError: zip argument #1 must support iteration
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run the following code will always result in this issue.
&lt;denchmark-code&gt;from pytorch_lightning import Trainer, seed_everything
from pl_examples.models.lightning_template import LightningTemplateModel

seed_everything(234)

def main():
    # model = LightningTemplateModel(**vars(args))
    model = LightningTemplateModel()

    trainer = Trainer(
        gpus=2,
        num_nodes=1,
        distributed_backend='dp',
        )

    trainer.fit(model)


if __name__ == '__main__':
    main()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The trainer should initialize correctly and start training.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- Tesla V100-SXM2-16GB
- Tesla V100-SXM2-16GB
- Tesla V100-SXM2-16GB
- Tesla V100-SXM2-16GB
- available:         True
- version:           10.2
Packages:
- numpy:             1.18.5
- pyTorch_debug:     False
- pyTorch_version:   1.7.0.dev20200702
- pytorch-lightning: 0.8.5-dev
- tensorboard:       2.2.2
- tqdm:              4.47.0
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.7
- version:           #30~18.04.1-Ubuntu SMP Mon Jun 22 15:48:21 UTC 2020

	</description>
	<comments>
		<comment id='1' author='aeryen' date='2020-07-05T04:01:01Z'>
		From what I can tell, this issue seems to be caused by LightningDataParallel calling gather on outputs from validation_step. Gather expects a list of tensors but received a list of dicts.
		</comment>
		<comment id='2' author='aeryen' date='2020-07-08T17:52:52Z'>
		This might be related to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1935&gt;#1935&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='aeryen' date='2020-07-08T23:21:06Z'>
		&lt;denchmark-link:https://github.com/nsarang&gt;@nsarang&lt;/denchmark-link&gt;
 oh you are totally right, sorry didn't these before. I'm closing this issue as it's a duplicate of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1904&gt;#1904&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>