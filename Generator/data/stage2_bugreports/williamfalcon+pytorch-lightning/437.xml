<bug id='437' author='s-rog' open_date='2019-10-28T06:57:53Z' closed_time='2019-10-29T10:49:54Z'>
	<summary>PackedSequence inputs gets converted to tuples of tensors</summary>
	<description>
Describe the bug
dataloaders returning 4 tensors (attributes of packed sequence) when collate_fn returns packed sequences
To Reproduce
def collate_seq(batch):
    x = rnn.pack_sequence([item[0] for item in batch], enforce_sorted=False)
    y = rnn.pack_sequence([item[1] for item in batch], enforce_sorted=False)
    return x, y
Expected behavior
the x packed sequence gets passed to forward()
	</description>
	<comments>
		<comment id='1' author='s-rog' date='2019-10-28T09:53:58Z'>
		when are you calling this function?

we can look into this.
in the meantime you can collate in the beginning of training_step

		</comment>
		<comment id='2' author='s-rog' date='2019-10-28T12:45:05Z'>
		I implemented the latter solution as a workaround in both train and validation step and can confirm it works.
I used that collate_seq function as my collate_fn in train and val dataloaders. both x and y packed sequences are being passed into the model's functions as a tuple of (4) tensors. I've looked into the lightning source code but I haven't been able to find the source of the forced tensor conversion.
Edit: my dataset returns a variable length tensor for each sequence, so the collate function receives a list of tensors to be packed
		</comment>
		<comment id='3' author='s-rog' date='2019-10-29T08:02:21Z'>
		Yeah, i think the assumption in lightning is that you pack the sentence in the beginning of training_step.
I usually pack and unpack right before going into the RNN.
def forward(self, x):
  x = pack(x)
  out = rnn(x) 
  x = unpack(x)
Happy to reopen if this is an actual issue.
		</comment>
		<comment id='4' author='s-rog' date='2019-10-29T15:40:32Z'>
		In my use case both the inputs and labels have to be packed so I have to repeat the packing in both training and validation steps instead of doing it once in collate.
Since the workaround works fine (for my use case), this isn't an urgent issue. It really comes down to a design choice, though forcibly converting inputs is probably not a good idea, and might break certain things down the line. Not to mention breaks something that works in native pytorch.
If no changes are to be made, perhaps the docs should reflect that packed sequences cannot be used as inputs?
		</comment>
		<comment id='5' author='s-rog' date='2019-10-29T15:49:27Z'>
		&lt;denchmark-link:https://github.com/s-rog&gt;@s-rog&lt;/denchmark-link&gt;
 that would be very helpful. Mind adding to docs and submitting a PR?
A better alternative would be a simple RNN example in the /examples folder documenting this.
		</comment>
		<comment id='6' author='s-rog' date='2019-11-06T07:19:52Z'>
		I looked into it again and it seems like this might be the source of the issue? &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 do you think this needs to be fixed? should be trivial to add another elif for packed sequences before tuple
&lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/dp_mixin.py#L34&gt;https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/dp_mixin.py#L34&lt;/denchmark-link&gt;

Reason being that x evaluates to true (somehow) in the following snippet
import torch.nn.utils.rnn as rnn
import torch

a = torch.tensor([0, 1, 2])
b = torch.tensor([1, 2, 3])
c = rnn.pack_sequence([a, b], enforce_sorted=False)
x = isinstance(c, tuple)
		</comment>
		<comment id='7' author='s-rog' date='2019-11-06T12:41:09Z'>
		ummm. yeah, submit the PR!
		</comment>
	</comments>
</bug>