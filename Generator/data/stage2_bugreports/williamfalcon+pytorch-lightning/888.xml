<bug id='888' author='Laksh1997' open_date='2020-02-18T00:04:36Z' closed_time='2020-03-07T00:06:07Z'>
	<summary>transfer_batch_to_gpu not moving List[torch.tensor] and like datastructures to GPU</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

trainer.transfer_batch_to_gpu(torch.randn(3, 3), 0) gives:
&lt;denchmark-code&gt;tensor([[-1.0600,  0.3306,  1.1276],
        [ 1.0012, -0.2687, -0.5493],
        [-0.5619, -1.7161, -0.8625]], device='cuda:0')
&lt;/denchmark-code&gt;

But
trainer.transfer_batch_to_gpu([ torch.randn(3, 3) ], 0) gives
&lt;denchmark-code&gt;[tensor([[ 1.3631,  0.3408, -1.1074],
         [-1.1176, -0.8056,  0.2937],
         [-0.4235,  0.7321, -0.8811]])]
&lt;/denchmark-code&gt;

which is not the expected behaviour as we are supposed to move all tensors in the data structure onto GPU.
Same issue occurs for tuple and dictionary datastructures.
Also - would it be possible to get gpu transfer for argparse.Namespace datastructures, or class datastructures?
IE if I make a class like so:
&lt;denchmark-code&gt;class Batch(object):
    def __init__(self, 
                 src_input_ids,
                 tgt_input_ids,
                 src_attn_mask,
                 tgt_attn_mask,
                 src_seg_ids,
                 tgt_labels
                ):
        self.__dict__.update(locals())
&lt;/denchmark-code&gt;

it would be cool if I could directly pass this in with gpu transfer.
Thanks a lot !
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Just copy one of the examples to get the Trainer class.
Then do trainer=Trainer(...) and run the above code
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Python 3.6
Pytorch 1.3.1
Cuda 10.1
PytorchLightning master (as of today)
	</description>
	<comments>
		<comment id='1' author='Laksh1997' date='2020-02-22T17:08:19Z'>
		I cannot reproduce this on master. For me
trainer.transfer_batch_to_gpu([ torch.randn(3, 3) ], 0)
returns
&lt;denchmark-code&gt;[tensor([[ 1.6940, -0.8183,  0.6075],
        [ 0.5152,  0.2925, -0.3388],
        [ 0.9064,  0.2509,  0.7852]], device='cuda:0')]
&lt;/denchmark-code&gt;

as expected. Same with tuples and dicts. Was it perhaps fixed in the meantime? Could you try again &lt;denchmark-link:https://github.com/Laksh1997&gt;@Laksh1997&lt;/denchmark-link&gt;
?
		</comment>
		<comment id='2' author='Laksh1997' date='2020-03-07T00:06:07Z'>
		we can reopen if it's still an issue
		</comment>
	</comments>
</bug>