<bug id='3973' author='singkwan' open_date='2020-10-08T04:23:55Z' closed_time='2020-10-08T18:29:49Z'>
	<summary>Metrics return unexpected results in 0.10.0rc1</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

There is a chance i dont understand well how it works, but both sklearn, functional and tensor metrics seem to not behave expectedly, specifically precision and recall
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I used a small dummy example for y_true and y_pred for a 2 class classification problem
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

import pytorch_lightning.metrics as plmetrics
import pytorch_lightning as pl
import torch

# Dummy data 
y_pred = torch.Tensor([1,0,1,0,1,1])
y_true = torch.Tensor([0,1,1,0,0,0])
## PL scikit learn test
plsk_accuracy = pl.metrics.sklearns.Accuracy()
plsk_precision = pl.metrics.sklearns.Precision()
plsk_recall = pl.metrics.sklearns.Recall()

accuracy = plsk_accuracy(y_pred, y_true)
precision = plsk_precision(y_pred, y_true)
recall = plsk_recall(y_pred, y_true)

print("PL scikit metrics precision: {}, recall: {}, accuracy: {}".format(precision, recall, accuracy))
PL scikit metrics precision: 0.375, recall: 0.375, accuracy: 0.3333333432674408
# Test for class based metrics
pl_accuracy = plmetrics.classification.Accuracy(num_classes=2)
pl_precision = plmetrics.classification.Precision(num_classes=2)
pl_recall = plmetrics.classification.Recall(num_classes=2)

accuracy = pl_accuracy(y_pred, y_true)
precision = pl_precision(y_pred, y_true)
recall = pl_recall(y_pred, y_true)

print("PL class metrics precision: {}, recall: {}, accuracy: {}".format(precision, recall, accuracy))
PL class metrics precision: 0.3333333432674408, recall: 0.3333333432674408, accuracy: 0.3333333432674408
# Normal scikit test
from sklearn.metrics import accuracy_score, precision_score, recall_score

sk_recall = recall_score(y_true.to("cpu").numpy(), y_pred.to("cpu").numpy())
sk_precision = precision_score(y_true.to("cpu").numpy(), y_pred.to("cpu").numpy())
sk_accuracy = accuracy_score(y_true.to("cpu").numpy(), y_pred.to("cpu").numpy())
print("precision: {}, recall: {}, accuracy: {}".format(sk_precision, sk_recall, sk_accuracy))
precision: 0.25, recall: 0.5, accuracy: 0.3333333333333333
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I expect that all 3 would yield the same precision, recall and accuracy numbers, specifically they should match scikit-learns results in the toy example above
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla V100-SXM2-16GB


available:         True
version:           10.1


Packages:

numpy:             1.18.1
pyTorch_debug:     False
pyTorch_version:   1.5.0
pytorch-lightning: 0.10.0rc1
tqdm:              4.42.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.7.7
version:           #1 SMP Wed Jun 24 19:07:39 UTC 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='singkwan' date='2020-10-08T04:24:44Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='singkwan' date='2020-10-08T05:55:02Z'>
		I have a observed a similar behaviour in 0.9.0 version. However, looking at source code, the implementation is surely correct
		</comment>
		<comment id='3' author='singkwan' date='2020-10-08T08:57:27Z'>
		Thanks for the report! By default sklearn uses a binary average (reports score for the positive class). This is different to the default of PL accuracy metric 'micro'. Setting this average to be the same fixes the test:
import pytorch_lightning as pl
import pytorch_lightning.metrics as plmetrics
import torch

# Dummy data
y_pred = torch.Tensor([1, 0, 1, 0, 1, 1])
y_true = torch.Tensor([0, 1, 1, 0, 0, 0])

## PL scikit learn test
plsk_accuracy = pl.metrics.sklearns.Accuracy()
plsk_precision = pl.metrics.sklearns.Precision(average='micro')
plsk_recall = pl.metrics.sklearns.Recall(average='micro')

accuracy = plsk_accuracy(y_pred, y_true)
precision = plsk_precision(y_pred, y_true)
recall = plsk_recall(y_pred, y_true)

print("PL scikit metrics precision: {}, recall: {}, accuracy: {}".format(precision, recall, accuracy))

# Test for class based metrics
pl_accuracy = plmetrics.classification.Accuracy(num_classes=2)
pl_precision = plmetrics.classification.Precision(num_classes=2, class_reduction='micro')
pl_recall = plmetrics.classification.Recall(num_classes=2, class_reduction='micro')

accuracy = pl_accuracy(y_pred, y_true)
precision = pl_precision(y_pred, y_true)
recall = pl_recall(y_pred, y_true)

print("PL class metrics precision: {}, recall: {}, accuracy: {}".format(precision, recall, accuracy))
&lt;denchmark-code&gt;PL scikit metrics precision: 0.3333333432674408, recall: 0.3333333432674408, accuracy: 0.3333333432674408
PL class metrics precision: 0.3333333432674408, recall: 0.3333333432674408, accuracy: 0.3333333432674408
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='singkwan' date='2020-10-08T09:03:35Z'>
		Thanks &lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 ! but what setting should i put for it to match scikit-learns defaults?

This i believe should be the accurate results for the metrics on the positive class = 1 ?
		</comment>
		<comment id='5' author='singkwan' date='2020-10-08T09:12:35Z'>
		&lt;denchmark-link:https://github.com/singkwan&gt;@singkwan&lt;/denchmark-link&gt;
 you can set the  argument to . This will give you the score of the individual classes:
&lt;denchmark-code&gt;pl_precision = pl.metrics.classification.Precision(num_classes=2, class_reduction='none')
pl_precision(y_pred, y_true)
tensor([0.5000, 0.2500])
&lt;/denchmark-code&gt;

then you just chooses the last :]
		</comment>
		<comment id='6' author='singkwan' date='2020-10-08T09:21:13Z'>
		Thanks a lot, this works, and super quick response! Not a bug, sorry!
Thinking about it, any point in making this callout more explicit in the docs? (or could just be me who was confused haha)
		</comment>
		<comment id='7' author='singkwan' date='2020-10-08T09:27:48Z'>
		&lt;denchmark-link:https://github.com/singkwan&gt;@singkwan&lt;/denchmark-link&gt;
 we are in the process of revamping the hole metrics class based interface (if you update to master right now  is the only classification metric present) so we can probably do something about that in the process :]
Please close issue if you feel it is solved.
		</comment>
	</comments>
</bug>