<bug id='4693' author='ananyahjha93' open_date='2020-11-16T12:38:57Z' closed_time='2020-11-17T18:58:09Z'>
	<summary>trainer.test(datamodule=dm) stores reference to wrong checkpoint</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When finetuning from saved weights in bolts, trainer.test() picks up reference to checkpoints which have already been deleted or not yet created.
Checkpoint created using default trainer options, no callbacks added from the user's side.
&lt;denchmark-h:h2&gt;Please reproduce using [the BoringModel and post here]&lt;/denchmark-h&gt;

Not sure how to reproduce fine-tuning from a checkpoint using the boring model.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


clone bolts using git clone https://github.com/PyTorchLightning/pytorch-lightning-bolts.git
cd to pl_bolts/models/self_supervised/swav/
wget 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/checkpoints/swav_stl10.pth.tar'
python swav_finetuner.py --ckpt swav_stl10.pth.tar --dataset stl10 --batch_size 256 --gpus 1 --learning_rate 0.1

Latest saved checkpoint is say 'epoch=33.ckpt' but line 712 in trainer.py looks for other saved checkpoints which might be epochs before or after the one present in checkpoints folder.
&lt;denchmark-code&gt;  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 712, in test
    results = self.__test_using_best_weights(ckpt_path, test_dataloaders)
&lt;/denchmark-code&gt;

Error:
&lt;denchmark-code&gt;FileNotFoundError: [Errno 2] No such file or directory: '/home/jovyan/pytorch_lightning_bolts/pl_bolts/models/self_supervised/swav/lightning_logs/version_3/checkpoints/epoch=7.ckpt'
FileNotFoundError: [Errno 2] No such file or directory: '/home/jovyan/pytorch_lightning_bolts/pl_bolts/models/self_supervised/swav/lightning_logs/version_3/checkpoints/epoch=21.ckpt'
FileNotFoundError: [Errno 2] No such file or directory: '/home/jovyan/pytorch_lightning_bolts/pl_bolts/models/self_supervised/swav/lightning_logs/version_3/checkpoints/epoch=37.ckpt'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

trainer.test(datamodule=dm) should pickup the reference to the correct checkpoint saved in lightning_logs/version_x/checkpoints
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

PyTorch Lightning version 1.0.4+ (tested with both 1.0.4 and 1.0.6)
bolts from master

PyTorch Version (e.g., 1.0): 1.6
OS (e.g., Linux): linux
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.7
CUDA/cuDNN version:
GPU models and configuration: V100s
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ananyahjha93' date='2020-11-16T13:02:15Z'>
		you please say how the checkpoint was created? and what Pl version was used?
		</comment>
		<comment id='2' author='ananyahjha93' date='2020-11-16T13:06:33Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 added
		</comment>
	</comments>
</bug>