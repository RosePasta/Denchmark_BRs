<bug id='1464' author='lizhitwo' open_date='2020-04-12T05:23:58Z' closed_time='2020-06-29T01:36:47Z'>
	<summary>EarlyStopping checkpointed state is lagging one epoch behind</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Currently EarlyStopping's state is updated after the checkpoint callback, so &lt;denchmark-link:https://github.com/PyTorchLightning/PyTorch-Lightning/blob/master/pytorch_lightning/trainer/training_io.py#L297&gt;what is being saved here&lt;/denchmark-link&gt;
 is last epoch's state.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

This is somewhat related to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1463&gt;#1463&lt;/denchmark-link&gt;
 so I am going to use the same code.
Steps to reproduce the behavior:
Install using pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade
&lt;denchmark-code&gt;import os
import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
import torchvision.transforms as transforms

import pytorch_lightning as pl

class CoolSystem(pl.LightningModule):

    def __init__(self):
        super(CoolSystem, self).__init__()
        # not the best model...
        self.l1 = torch.nn.Linear(28 * 28, 10)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_nb):
        # REQUIRED
        x, y = batch
        y_hat = self.forward(x)
        return {'loss': F.cross_entropy(y_hat, y)}

    def validation_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self.forward(x)
        return {'val_loss': F.cross_entropy(y_hat, y)}

    def validation_epoch_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        return {'val_loss': avg_loss}

    def configure_optimizers(self):
        # REQUIRED
        # can return multiple optimizers and learning_rate schedulers
        return torch.optim.Adam(self.parameters(), lr=0.02)

    def train_dataloader(self):
        # REQUIRED
        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)

    def val_dataloader(self):
        # OPTIONAL
        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)

from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

model = CoolSystem()

checkpoint_callback = ModelCheckpoint(
    filepath='./model_ckpt/whatever_the_name_is_gonna_be_auto_chosen',
    save_top_k=-1,
    verbose=True,
    monitor='val_loss',
    mode='auto'
)

class EarlyStoppingPrinting(EarlyStopping):

    def on_train_start(self, trainer, pl_module):
        print('EarlyStoppingPrinting before on_train_start')
        print('self.wait = ', self.wait)
        super().on_train_start(trainer, pl_module)
        print('EarlyStoppingPrinting after on_train_start')
        print('self.wait = ', self.wait)

    def on_epoch_end(self, trainer, pl_module):
        ret = super().on_epoch_end(trainer, pl_module)
        if self.wait:
            print('Early stopping patience: %d/%d' % (self.patience-self.wait, self.patience))
        return ret


early_stopping = EarlyStoppingPrinting(
    monitor='val_loss',
    patience=5,
    verbose=True,
    mode='auto'
)

trainer = Trainer(max_nb_epochs=1000, train_percent_check=0.1, 
                  checkpoint_callback=checkpoint_callback, 
                  early_stop_callback=early_stopping)

trainer.fit(model)
&lt;/denchmark-code&gt;

Let the model train until convergence. And then reload the model and see how it continues:
&lt;denchmark-code&gt;trainer = Trainer(max_nb_epochs=1000, train_percent_check=0.1, 
                  checkpoint_callback=None, 
                  resume_from_checkpoint = 'model_ckpt/_ckpt_epoch_7.ckpt',
                  early_stop_callback=early_stopping)
trainer.fit(model)
&lt;/denchmark-code&gt;

The early_stopping callback would print:
&lt;denchmark-code&gt;EarlyStoppingPrinting before on_train_start
self.wait =  4
...
&lt;/denchmark-code&gt;

and keeps training.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The early_stopping callback should print:
&lt;denchmark-code&gt;EarlyStoppingPrinting before on_train_start
self.wait =  5
...
&lt;/denchmark-code&gt;

and should not be trained again at all since self.wait &gt;= self.patience.
If the model is loaded from an interrupted save, then it should still train after resuming, but with corrected self.wait.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

This is ran on Google colab.
&lt;denchmark-link:https://colab.research.google.com/drive/1ZdiFf6ksNpgsqOdSKM6lMO0yIhqpnTHD&gt;https://colab.research.google.com/drive/1ZdiFf6ksNpgsqOdSKM6lMO0yIhqpnTHD&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Somewhat related to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1463&gt;#1463&lt;/denchmark-link&gt;
.
	</description>
	<comments>
		<comment id='1' author='lizhitwo' date='2020-04-16T02:27:59Z'>
		&lt;denchmark-link:https://github.com/lizhitwo&gt;@lizhitwo&lt;/denchmark-link&gt;
 thanks for this very detailed bug report! looking into it...
at a high level, i think that we should keep the concern of the callback state contained within the callback itself. we can follow the pytorch convention of having methods for  and . the trainer can just call those methods rather than reaching in and saving individual attributes. (this more so addresses &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1463&gt;#1463&lt;/denchmark-link&gt;
 but i plan to fix both in a single PR)
		</comment>
		<comment id='2' author='lizhitwo' date='2020-04-16T03:37:36Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 do you know why we need ? why can't we just set the values in ?
edit: going to remove on_train_start
		</comment>
		<comment id='3' author='lizhitwo' date='2020-04-17T03:03:00Z'>
		the core problem for this issue is the  runs on  which occurs before  runs during . the checkpoint callback is not run again after early stopping halts training. the checkpoint includes a state dict of the early stopping values, and as &lt;denchmark-link:https://github.com/lizhitwo&gt;@lizhitwo&lt;/denchmark-link&gt;
 points out the last saved callback contains the early stopping state of the second to last epoch.


we could move the checkpoint callback to also run during on_epoch_end, but this might not always be desired (eg. if you run validation multiple times per epoch and want all checkpoints).


we could also just write the checkpoint callback to re-run at the end of an epoch, but not sure how we want to handle saving the k best models in this case


cc &lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
 any suggestions? i'm not a huge fan of either of these approaches
		</comment>
		<comment id='4' author='lizhitwo' date='2020-04-19T21:10:18Z'>
		I would like to propose that we do the following:

when we trigger early stopping, check to see if a checkpoint callback is being used
if so, create a new checkpoint that following the standard naming scheme, plus "_early_stopping" at the end (eg. ckpt_epoch_7_early_stopping.ckpt)

&lt;denchmark-link:https://github.com/lizhitwo&gt;@lizhitwo&lt;/denchmark-link&gt;
 would this be a suitable solution for you needs?
		</comment>
		<comment id='5' author='lizhitwo' date='2020-04-20T02:39:40Z'>
		This would be correct only when the training ends normally. When the user hits Ctrl+C to interrupt, the previous checkpoints that are already saved are still lagging one epoch behind.
I would also advise against putting more undocumented checkpoint naming conventions into Lightning. I currently am already confused why Lightning overrides my checkpoint name unless I put e.g. {epoch} in them.
Question: is there a reason why early stopping callback can't be split into two, and the status update moved before checkpoint? You can update its state before checkpoint, and checkpoint, and then query its state to decide if early-stopping should be performed using e.g. a should_early_stop property or something.
		</comment>
		<comment id='6' author='lizhitwo' date='2020-04-24T03:30:39Z'>
		yeah you're right, we need a better solution here. i think we can also improve how the checkpoint naming is done but i'll leave that for a separate issue.

Question: is there a reason why early stopping callback can't be split into two, and the status update moved before checkpoint?

it's a good question. this is a bit challenging because our checkpoint callback is currently set up to run every time we iterate over the validation set. this can happen once per epoch, multiple times per epoch, or once every n epochs, depending on how the user has defined various arguments in their Trainer.
here's how i think we should proceed:

add a new argument to the early stopping callback which specifies whether we monitor a value from the training_step output or validation_step output (this is cleaner than guessing imo)
if we're monitoring the training_step, perform a check after each batch (eg. on_batch_end) which always runs before the checkpoint callback
if we're monitoring the validation_step, perform a check after running through the validation set (eg. on_validation_end) but before the checkpoint callback runs
switch from having early stopping return a value to instead set a Trainer attribute

cc &lt;denchmark-link:https://github.com/lizhitwo&gt;@lizhitwo&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 want to weigh in if this is a good plan?
		</comment>
		<comment id='7' author='lizhitwo' date='2020-04-24T09:52:34Z'>
		I think that in some cases you want to monitor both train and valid...

switch from having early stopping return a value to instead set a Trainer attribute

do you mean a Trainer state like we discussed several times before?
I think that there is one more thing we shall think about and it specifies the "unit" for evaluation if the e.g. patience od epoch or batch/step
cc: &lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='lizhitwo' date='2020-04-24T13:25:14Z'>
		
I think that in some cases you want to monitor both train and valid...

could you provide an example? in 90% of the cases the user is going to want to monitor something like val_loss. it seems very odd to condition the early stopping criterion on something which bridges both train and valid

do you mean a Trainer state like we discussed several times before?

i'm thinking of setting an attribute.
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L366&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L366&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;if self.enable_early_stop:
    if (met_min_epochs and met_min_steps) or self.fast_dev_run:
        should_stop = self.early_stop_callback.on_epoch_end(self, self.get_model())
        # stop training
        stop = should_stop and met_min_epochs
        if stop:
            self.run_training_teardown()
            return
&lt;/denchmark-code&gt;

to
&lt;denchmark-code&gt;if self.should_stop:  # the checks against met_min_epochs and met_min_steps happen in the callback
    return 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='9' author='lizhitwo' date='2020-04-24T16:29:14Z'>
		I think this works, as long as

an attribute is written to either the trainer or the early stopping callback, either by one of the trainer's method or one of the callback's method
there is no checkpointing between "this attribute writing is done" and "the early stopping callback has the ability to update this value"
the attribute is preserved when checkpointing

then no matter when the checkpoint is done, the early-stopping's state is clean.
Although this would break compatibility with older code, so maybe set the attribute inside Trainer according to early stopping's return value, instead of in early stopping callback which may be overridden a lot?
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I think monitoring both train and val is better left to users, since they need to specify how the criteria is computed anyway. They can choose to compute it during either val or train and add it to the log in one of them.
		</comment>
		<comment id='10' author='lizhitwo' date='2020-04-24T16:33:28Z'>
		By the way, the newest version of Lightning has an additional bug: the early stopping is evaluated twice per epoch, &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L459&gt;here&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L369&gt;here&lt;/denchmark-link&gt;
. You can run my code again to see that the  decreases by 2 each epoch.
		</comment>
		<comment id='11' author='lizhitwo' date='2020-04-24T21:47:08Z'>
		yes that was a behavioral regression introduced by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1528&gt;#1528&lt;/denchmark-link&gt;
 i will fix it as well, thanks for catching it! we clearly need better tests to spot these errors
		</comment>
		<comment id='12' author='lizhitwo' date='2020-04-24T21:48:48Z'>
		&lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;
 submit asap so we can get it in 0.7.4? &lt;denchmark-link:https://github.com/lizhitwo&gt;@lizhitwo&lt;/denchmark-link&gt;
 thanks for catching that!
		</comment>
		<comment id='13' author='lizhitwo' date='2020-04-24T21:50:28Z'>
		actually... this may not be a trivial fix
		</comment>
		<comment id='14' author='lizhitwo' date='2020-04-24T21:52:34Z'>
		in (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L459&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L459&lt;/denchmark-link&gt;
) we need to check it whenever we save weights. In this case, if this is true, then we need to stop training but make sure we do all the rest of the actions we need to call.
In (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L369&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L369&lt;/denchmark-link&gt;
) we actually want to exit.
But anyhow, &lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;
 if you figure this out let's get this into 0.7.4
		</comment>
		<comment id='15' author='lizhitwo' date='2020-04-24T22:17:19Z'>
		yes i'm hoping to get this all wrapped up this weekend, it's a bit of a tricky one
		</comment>
	</comments>
</bug>