<bug id='5384' author='celsofranssa' open_date='2021-01-06T15:25:33Z' closed_time='2021-01-09T13:55:57Z'>
	<summary>Value interpolation with hydra composition</summary>
	<description>
I am using hydra composition with the following structure:
├── configs
    │   ├── config.yaml
    │   ├── data
    │   │   ├── dataset_01.yaml
    │   │   └── dataset_02.yaml
    │   └── model
    │       ├── bert.yaml
    │       └── gpt.yaml

config.yaml

defaults:
  - model: bert
  - data: dataset_01

...

data/dataset_01.yaml

# @package _group_

name: "dataset_01"

train:
  path: "../resources/datasets/dataset_01/train.jsonl"
  num_samples: 1257391

test:
  path: "../resources/datasets/dataset_01/test.jsonl"
  num_samples: 71892

val:
  path: "../resources/datasets/dataset_01/val.jsonl"
  num_samples: 73805

model/bert.yaml

# @package _group_

name: "bert"

encoder: "source.encoder.BertEncoder.BertEncoder"

encoder_hparams:
  architecture: "bert-base-uncased"

lr: 1e-7

tokenizer:
  architecture: "bert-base-uncased"

predictions:
  path: "../resources/predictions/bert_${data.name}_predictions.pt"
-entry point:
@hydra.main(config_path="configs/", config_name="config.yaml")
def perform_tasks(hparams):

    model = MyModel(hparams.model)

if __name__ == '__main__':
    perform_tasks()
However, I am having the following error when interpolating the dataset.name:
Traceback (most recent call last):
  File "xCoFormer.py", line 153, in perform_tasks
    fit(hparams)
  File "xCoFormer.py", line 88, in fit
    trainer.fit(model, datamodule=dm)
  File "/home/celso/projects/venvs/xCoFormer/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 470, in fit
    results = self.accelerator_backend.train()
  File "/home/celso/projects/venvs/xCoFormer/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 65, in train
    self.trainer.train_loop.setup_training(model)
  File "/home/celso/projects/venvs/xCoFormer/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 147, in setup_training
    self.trainer.logger.save()
  File "/home/celso/projects/venvs/xCoFormer/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py", line 39, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/celso/projects/venvs/xCoFormer/lib/python3.7/site-packages/pytorch_lightning/loggers/tensorboard.py", line 221, in save
    save_hparams_to_yaml(hparams_file, self.hparams)
  File "/home/celso/projects/venvs/xCoFormer/lib/python3.7/site-packages/pytorch_lightning/core/saving.py", line 366, in save_hparams_to_yaml
    OmegaConf.save(hparams, fp, resolve=True)
omegaconf.errors.ConfigKeyError: str interpolation key 'data.name' not found
This happening only in my PL project. In another project, interpolations like that resolves normally.
Is there any way around this error?
	</description>
	<comments>
		<comment id='1' author='celsofranssa' date='2021-01-06T18:37:39Z'>
		A minimal example showing the expected behavior was provided in &lt;denchmark-link:https://github.com/celsofranssa/comp_interpolation&gt;https://github.com/celsofranssa/comp_interpolation&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='celsofranssa' date='2021-01-07T04:28:24Z'>
		I ran your minimal example and it produced the file you were expecting "if all goes well".
The stack trace from your bug report contains pytorch-lightning, I think you accidentally fixed your issue by taking it out of the equation.
Keep working on your minimal example, you will probably need to bring a minimal PL training loop.
		</comment>
		<comment id='3' author='celsofranssa' date='2021-01-07T04:53:04Z'>
		What I tried to demonstrate is that in a project that does not use PL, the interpolation occurred normally (like in the minimal example above which is not a PL project). Even though I distributed the DictConfig among the different parts of the system (hparams.mode for model module and hparams.data for the dataset module).
However, when using the same logic in a project using PL, this same interpolation did not work. The error occurred mainly on the following lines:



pytorch-lightning/pytorch_lightning/core/saving.py


        Lines 364 to 366
      in
      cc62435






 if OmegaConf.is_config(hparams): 



 with fs.open(config_yaml, "w", encoding="utf-8") as fp: 



 OmegaConf.save(hparams, fp, resolve=True) 





where the interpolation about  ${data.name} could not be resolved.
		</comment>
		<comment id='4' author='celsofranssa' date='2021-01-07T04:59:12Z'>
		Gotcha, I missed the fact that this was showing the expected behavior.
It will help if you provide a minimal repro that demonstrates the problem you are seeing.
add PL back and to reproduce the exception.
One advantage is that by doing it, in many cases you find the problem for yourself: You already saw that by creating a minimal repro (as I asked you in Stack overflow and on the the OmegaConf comment) - you reached the conclusion that the issue is related to PL.
The second advantage is that it makes it easy for someone else to debug the exact problem you are seeing.
You basically help them to help you.
		</comment>
		<comment id='5' author='celsofranssa' date='2021-01-07T12:55:30Z'>
		Ok &lt;denchmark-link:https://github.com/omry&gt;@omry&lt;/denchmark-link&gt;
,
we now have two minimal examples:

expected behavior (without using PL): https://github.com/celsofranssa/comp_interpolation/tree/main
version with this issue (using PL): https://github.com/celsofranssa/comp_interpolation/tree/pytorch-lightning

		</comment>
		<comment id='6' author='celsofranssa' date='2021-01-07T15:45:53Z'>
		Hey &lt;denchmark-link:https://github.com/omry&gt;@omry&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/celsofranssa&gt;@celsofranssa&lt;/denchmark-link&gt;
 and al.
Sorry for the delay, I meant to solve this bug for some time now.
PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/5406&gt;#5406&lt;/denchmark-link&gt;
 should fix it. Would you mind give it a try with your examples.
Best,
T.C
		</comment>
		<comment id='7' author='celsofranssa' date='2021-01-07T18:38:35Z'>
		Hello &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
,
Sure, it will be a pleasure to test. What should I do to test?
		</comment>
		<comment id='8' author='celsofranssa' date='2021-01-15T21:53:16Z'>
		Hello,
Any update on this?
		</comment>
		<comment id='9' author='celsofranssa' date='2021-01-15T22:18:34Z'>
		You can check-out PTL from GitHub and see if it works for you now.
		</comment>
		<comment id='10' author='celsofranssa' date='2021-01-15T23:18:58Z'>
		Even though installing the PL from sources, it generates the following error:
Traceback (most recent call last):
  File "/home/celso/projects/comp_interpolation/main.py", line 17, in my_app
    trainer.fit(model)
  File "/home/celso/projects/comp_interpolation/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 509, in fit
    results = self.accelerator_backend.train()
  File "/home/celso/projects/comp_interpolation/venv/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py", line 56, in train
    self.trainer.setup_trainer(self.trainer.model)
  File "/home/celso/projects/comp_interpolation/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 443, in setup_trainer
    self.logger.save()
  File "/home/celso/projects/comp_interpolation/venv/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py", line 39, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/celso/projects/comp_interpolation/venv/lib/python3.7/site-packages/pytorch_lightning/loggers/tensorboard.py", line 234, in save
    save_hparams_to_yaml(hparams_file, self.hparams)
  File "/home/celso/projects/comp_interpolation/venv/lib/python3.7/site-packages/pytorch_lightning/core/saving.py", line 384, in save_hparams_to_yaml
    hparams = apply_to_collection(hparams, DictConfig, to_container)
  File "/home/celso/projects/comp_interpolation/venv/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py", line 49, in apply_to_collection
    return function(data, *args, **kwargs)
omegaconf.errors.ConfigKeyError: str interpolation key 'data.name' not found

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.

Process finished with exit code 1
that can be reproduced by running &lt;denchmark-link:https://github.com/celsofranssa/comp_interpolation/tree/pytorch-lightning&gt;this example&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>