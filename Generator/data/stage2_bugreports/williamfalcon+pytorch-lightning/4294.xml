<bug id='4294' author='angadkalra' open_date='2020-10-21T21:29:20Z' closed_time='2020-12-07T01:50:43Z'>
	<summary>Segmentation Fault when training 3D CNN using 4 GPUs with batch_size=8</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Trying to train 3D CNN using 4 GPUs, batch_size = 8, and num_workers &gt;= 4 (ddp backend). I'm using a GCP VM with 16 cores and 60GB memory. The data is stored on a mounted disk and is roughly 3 TB.
I can successfully train using 2 GPUs, batch_size=4, and num_workers=4, but whenever I try increasing number of GPUs, batch size, and num_workers, a segmentation fault get's thrown. It's not the same segfault stack trace everytime, but the image below is most common. I tried increasing cores and memory to 32/120 and it still errors out. The memory used doesn't stay stable either. I see the buff/cache increasing to about 85% of available mem, then segfault gets thrown shortly after.
I followed the PyTorch Lightning website carefully and made sure the code didn't have anything that could cause slow down or memory leak. I followed all the tips for fast performance, multi-gpu training, etc. that's listed on docs website.
One more observation:
To test my code, I run training for 3 epochs using about 60 training examples and 60 validation. The test run works perfectly fine with batch_size=8, 4 GPUs, and num_workers=4. When I switch to normal training mode with full dataset, it errors out. I've noticed that anything more than 64 examples causes segfault.
&lt;denchmark-link:https://user-images.githubusercontent.com/8902328/96787719-1653f800-13a7-11eb-96a8-ba8994e3c8fa.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.6.0+cu101
OS (e.g., Linux): Linux (Debian)
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.7.6
CUDA/cuDNN version: 10.1
GPU models and configuration: 4 x Nvidia Tesla V100
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;


Data is stored as .npy files

	</description>
	<comments>
		<comment id='1' author='angadkalra' date='2020-10-21T21:30:12Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='angadkalra' date='2020-10-21T22:38:00Z'>
		&lt;denchmark-link:https://github.com/angadkalra&gt;@angadkalra&lt;/denchmark-link&gt;
 can you try running with num_workers=0. For a batch size of 8 it should be fine.
If that works, there might be something in your dataset which is not compatible with having multiple processes.
		</comment>
		<comment id='3' author='angadkalra' date='2020-10-22T02:37:34Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 Thanks for the response.
Looks like it's working with num_workers=0. Takes about 15s/it.
Are you saying that there is a corrupt file that is causing one of the workers to die during getitem? Or something in my Dataset class that doesn't work well with multiple processes?
		</comment>
		<comment id='4' author='angadkalra' date='2020-10-22T17:53:49Z'>
		If it works for the entire dataset with num_workers=0, there might be something which doesn't work with multiple processes.
		</comment>
		<comment id='5' author='angadkalra' date='2020-10-23T19:38:51Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 I ran with num_workers=0 on entire dataset and it worked fine. I then started eliminating one preprocessing step at a time to figure out what didn't work when using num_workers &gt; 0. It ended up being a function that pads a tensor to a specific shape. I changed from using numpy.pad to using torch.nn.functional.pad. However, I still get a segfault when running with num_workers &gt; 0. This is the traceback:
&lt;denchmark-link:https://user-images.githubusercontent.com/8902328/97046602-7a53f900-152c-11eb-9386-a801ff1ff1ff.png&gt;&lt;/denchmark-link&gt;

I increased my machine size so definitely not a memory error. GPU memory isn't close to being full either. I notice that seg fault occurs more often when using a bigger image (i.e 224x224 instead of 64x64).
		</comment>
		<comment id='6' author='angadkalra' date='2020-10-29T20:49:26Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 any ideas here?
		</comment>
		<comment id='7' author='angadkalra' date='2020-10-30T20:41:50Z'>
		Currently, I have 320x128x128 tensors (float16) working (batch_size=8, num_workers=6). I notice that when I use shuffle=True in dataloader I get segfault after 70 batch iters. Also, when I try to increase to 192x192 or 224x224, segfault. One thing that seems to help is clearing system buffer/cache before training.
		</comment>
		<comment id='8' author='angadkalra' date='2020-11-29T22:50:11Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>