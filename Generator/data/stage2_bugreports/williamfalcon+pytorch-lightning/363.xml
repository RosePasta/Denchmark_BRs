<bug id='363' author='tshrjn' open_date='2019-10-13T08:05:21Z' closed_time='2019-10-14T11:00:46Z'>
	<summary>on_epoch_* callbacks should only be run for 1 process</summary>
	<description>
&lt;denchmark-h:h3&gt;Bug Description:&lt;/denchmark-h&gt;

Anything done in on_epoch_* (like on_epoch_end or on_epoch_start) callbacks happens n number of times when doing distributed training i.e. ddp . Process rank check should be added.
To Reproduce
Use ddp for training and create the on_epoch_end callback method with a print statement like: print(self.current_epoch)
Expected behavior
Each epoch is supposed to printed only once.
	</description>
	<comments>
		<comment id='1' author='tshrjn' date='2019-10-14T11:00:46Z'>
		on_epoch needs to be called on every process because you might want to do something which affects the model on every process.
		</comment>
	</comments>
</bug>