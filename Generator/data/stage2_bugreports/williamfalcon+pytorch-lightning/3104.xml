<bug id='3104' author='dalmia' open_date='2020-08-22T19:24:01Z' closed_time='2020-10-06T17:54:38Z'>
	<summary>TPU available: true when there are no TPUs</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am using a DGX machine (and so, no TPUs), but on initiating Trainer, it logs TPU available: True. This ends up returning Missing XLA configuration when I run my script.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

Simply running the following lines on my machine:
&gt;&gt; trainer = pl.Trainer(gpus=[0])                                                                                                                 
GPU available: True, used: True
TPU available: True, using: 0 TPU cores
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&gt;&gt; trainer = pl.Trainer(gpus=[0])                                                                                                                 
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - Tesla V100-SXM2-32GB
        - available:         True
        - version:           10.2
* Packages:
        - numpy:             1.18.2
        - pyTorch_debug:     False
        - pyTorch_version:   1.6.0
        - pytorch-lightning: 0.9.0
        - tensorboard:       2.2.0
        - tqdm:              4.45.0
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - 
        - processor:         x86_64
        - python:            3.6.9
        - version:           #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='dalmia' date='2020-08-24T23:10:11Z'>
		sounds like some misconfiguration issue, are interested in sending a PR? üê∞
		</comment>
		<comment id='2' author='dalmia' date='2020-08-28T01:39:04Z'>
		Sure. I realized that the bug is in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/8ebf4fe1739aae04c14ddb3ad572a57775018673/pytorch_lightning/trainer/distrib_parts.py&gt;this&lt;/denchmark-link&gt;
 script.
Specifically:
&lt;denchmark-code&gt;try:
    import torch_xla.core.xla_model as xm
except ImportError:
    XLA_AVAILABLE = False
else:
    XLA_AVAILABLE = True
&lt;/denchmark-code&gt;

So, if the environment has  installed but no TPU, then this error is thrown. If I use an environment without , it works fine. So, is this something that should be fixed in the codebase or something that the user should take care of? &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='dalmia' date='2020-08-28T07:35:19Z'>
		yes, we had the XLA detection as a temporal solution as we did not expect someone would install XLA without having TPU...
so, pls send a PR, I think that we have this patter in several files...
		</comment>
		<comment id='4' author='dalmia' date='2020-10-01T18:35:37Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>