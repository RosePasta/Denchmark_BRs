<bug id='3101' author='VirajBagal' open_date='2020-08-22T10:13:05Z' closed_time='2020-08-29T12:55:16Z'>
	<summary>TypeError: Can't instantiate abstract class GraphAEStage1 with abstract methods forward</summary>
	<description>
Below is my LightningModule. GraphEncoderStage1 and GraphDecoderStage1 in the __init__   are the usual nn.Modules with their own __init__ and forward methods. When I instantiate GraphAEStage1 as
model = GraphAEStage1(config, layer_id = 0)
I get the following error:

TypeError: Can't instantiate abstract class GraphAEStage1 with abstract methods forward

Please help.
class GraphAEStage1(pl.LightningModule):
  def __init__(self, config, layer_id):
    super(GraphAEStage1, self).__init__()
    
    if layer_id==0:
      self.encoder = GraphEncoderStage1(config['poc_max_degree'], config['input_dim'], config['hidden_feat_dim'][layer_id])
      self.decoder = GraphDecoderStage1(config['poc_max_degree'], config['input_dim'], config['hidden_feat_dim'][layer_id])
    else:
      self.encoder = GraphEncoderStage1(config['poc_max_degree'], config['hidden_feat_dim'][layer_id-1], config['hidden_feat_dim'][layer_id])
      self.decoder = GraphDecoderStage1(config['poc_max_degree'], config['hidden_feat_dim'][layer_id-1], config['hidden_feat_dim'][layer_id])

    # Weight sharing
    
    for i in range(max_degree):
      self.decoder.neigh_weights[i] = nn.Parameter(torch.t(self.encoder.neigh_weights[i]))
    
    self.decoder.self_weights = nn.Parameter(torch.t(self.encoder.self_weights))

  def training_step(self, batch, batch_idx):

    loss, tf_node = self.shared_step(batch)
    result = pl.TrainResult(minimize=loss)
    return result.log('train_loss', loss, prog_bar = True, logger = True, on_step = True, on_epoch = True,
                          reduce_fx = torch.mean), tf_node

  def validation_step(self, batch, batch_idx):

    loss, tf_node = self.shared_step(batch)
    result = pl.EvalResult(checkpoint_on=loss)
    return result.log('val_loss', loss, prog_bar = True, logger = True, on_step = True, on_epoch = True,
                          reduce_fx = torch.mean), tf_node                


  def shared_step(self, batch):

    node, adj, deg, mask = batch
    tf_node, avg_adj = self.encoder(node, adj, deg, mask)
    recon_node, recon_adj = self.decoder(tf_node, deg)

    node_loss = nn.functional.mse_loss(recon_node, node)
    adj_loss = nn.functional.mse_loss(recon_adj, avg_adj)
    loss = node_loss + adj_loss

    return loss, tf_node

  def configure_optimizers(self):

    optimizer = torch.optim.Adam(self.parameters(), lr=config['lr'])
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = config['patience'], factor = config['factor'])
    return [optimizer], [scheduler]
	</description>
	<comments>
		<comment id='1' author='VirajBagal' date='2020-08-22T10:13:46Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='VirajBagal' date='2020-08-29T13:22:50Z'>
		Hey &lt;denchmark-link:https://github.com/VirajBagal&gt;@VirajBagal&lt;/denchmark-link&gt;
, did you find the reason for the error here??
		</comment>
		<comment id='3' author='VirajBagal' date='2020-08-30T05:19:16Z'>
		Hey &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 , the following helped me. So I closed the issue. Thanks for asking!

LightningModule is like nn.Module only so you need to replace def shared_step(self, batch) with def forward(self, batch) then you can use either self(batch) or self.forward(batch) instead of self.shared_step(batch).

		</comment>
	</comments>
</bug>