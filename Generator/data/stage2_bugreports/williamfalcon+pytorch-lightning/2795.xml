<bug id='2795' author='xiadingZ' open_date='2020-08-02T03:34:39Z' closed_time='2020-08-15T12:38:20Z'>
	<summary>EvalResult doesn't do mean_of_gpus if using TensorMetric</summary>
	<description>
I want to use new AccuracyMetric, it can automatically sync in ddp, but it doesn't divide by word_size. In manually mode, I can  divide it by word_size by hand in validation_epoch_end. But if I use EvalResult, how to do this? It only do mean  across batches, but no across gpus.
This is original code:
&lt;denchmark-code&gt;    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()
        avg_acc = avg_acc / self.trainer.world_size

&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='xiadingZ' date='2020-08-02T19:27:36Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 mind have a look?
		</comment>
		<comment id='2' author='xiadingZ' date='2020-08-10T09:35:12Z'>
		&lt;denchmark-link:https://github.com/xiadingZ&gt;@xiadingZ&lt;/denchmark-link&gt;
 this was solved in a recent PR (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2568&gt;#2568&lt;/denchmark-link&gt;
). You can now set the  when you construct the metric, and it will calculate the mean and not the sum.
		</comment>
	</comments>
</bug>