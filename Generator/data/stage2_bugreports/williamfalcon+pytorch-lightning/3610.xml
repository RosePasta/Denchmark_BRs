<bug id='3610' author='BrianPugh' open_date='2020-09-22T17:43:53Z' closed_time='2020-09-23T15:33:22Z'>
	<summary>TensorBoardLogger log_hyperparameters fails when explicitly specified.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

TensorBoardLogger log_hyperparameters fails with TypeError: log_hyperparams() missing 1 required positional argument: 'params'  when explicitly specified. It behaves correctly when not specified (even though its the default logger).
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

See code snippet:
&lt;denchmark-code&gt;import pytorch_lightning as pl
from pytorch_lightning import Trainer

# This is fine, even though TensorBoardLogger is the default logger.
trainer = Trainer()

# This breaks when `fit` is called, see stacktrace
trainer = Trainer(logger=pl.loggers.tensorboard.TensorBoardLogger)

trainer.fit(model, train_loader, val_loader)
&lt;/denchmark-code&gt;

Stacktrace (I removed some Hydra calls from the top of the trace that don't contribute anything). This doesn't happen when logger is not explicitly specified in the Trainer constructor.
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "train.py", line 65, in train
    trainer.fit(model, train_loader, val_loader)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 301, in fit
    results = self.accelerator_backend.train()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/gpu_backend.py", line 50, in train
    self.trainer.train_loop.setup_training(model)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 124, in setup_training
    self.trainer.logger.log_hyperparams(ref_model.hparams)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py", line 27, in wrapped_fn
    return fn(*args, **kwargs)
TypeError: log_hyperparams() missing 1 required positional argument: 'params'
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Ran into issue on release 0.9.0 as well as current master &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/ba01ec9dbf3bdac1a26cfbbee67631c3ff1fadad&gt;ba01ec9&lt;/denchmark-link&gt;


CUDA:
- GPU:
- GeForce GTX 1070
- available:         True
- version:           10.1
Packages:
- numpy:             1.18.5
- pyTorch_debug:     False
- pyTorch_version:   1.6.0
- pytorch-lightning: 0.9.1rc3
- tqdm:              4.46.0
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.7
- version:           #1 SMP Sun Mar 8 14:34:03 CDT 2020

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I'm explicitly specifying this because I have my own subclass of tensorboard's SummaryWriter that I would like TensorBoardLogger to use. I'm doing this by subclassing TensorBoardLogger and copying the experiment method, replacing the SummaryWriter with my own.
	</description>
	<comments>
		<comment id='1' author='BrianPugh' date='2020-09-22T17:44:40Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='BrianPugh' date='2020-09-22T17:59:54Z'>
		follow up: with the default logger (not explicitly specified), hparams are not logged. I'm using the newer hparams api where the hparams attribute is not explicitly set.
&lt;denchmark-link:https://user-images.githubusercontent.com/14318576/93919518-aaf10a80-fcc2-11ea-8df9-e39e00a6f560.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='BrianPugh' date='2020-09-22T19:00:19Z'>
		&lt;denchmark-link:https://github.com/teddykoker&gt;@teddykoker&lt;/denchmark-link&gt;
 take a look?
		</comment>
		<comment id='4' author='BrianPugh' date='2020-09-23T00:47:50Z'>
		&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2974&gt;#2974&lt;/denchmark-link&gt;
 btw this has already been merged into master, but hasn't made it into release yet, current release implementation is broken.
		</comment>
		<comment id='5' author='BrianPugh' date='2020-09-23T02:54:26Z'>
		
Ran into issue on release 0.9.0 as well as current master ba01ec9

		</comment>
		<comment id='6' author='BrianPugh' date='2020-09-23T03:36:07Z'>
		You need to initialize a logger instance
Trainer(logger=pl.loggers.tensorboard.TensorBoardLogger(save_dir))
		</comment>
		<comment id='7' author='BrianPugh' date='2020-09-23T15:33:22Z'>
		Closing, everything works as it should. See &lt;denchmark-link:https://colab.research.google.com/drive/1bvsyanpnMwf3RZerfybcH6w-cFNuduUw?usp=sharing&gt;https://colab.research.google.com/drive/1bvsyanpnMwf3RZerfybcH6w-cFNuduUw?usp=sharing&lt;/denchmark-link&gt;
 for an example of using your own tensorboard logger.
		</comment>
		<comment id='8' author='BrianPugh' date='2020-09-26T21:12:39Z'>
		
Closing, everything works as it should. See https://colab.research.google.com/drive/1bvsyanpnMwf3RZerfybcH6w-cFNuduUw?usp=sharing for an example of using your own tensorboard logger.

I wrote this to turn tensorboard on:

Hyperparams are still not logged, like there &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3610#issuecomment-696884730&gt;#3610 (comment)&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='BrianPugh' date='2020-09-26T22:33:15Z'>
		To be clear, I am still having the hyperparams issues as mentioned by &lt;denchmark-link:https://github.com/vladimirbelyaev&gt;@vladimirbelyaev&lt;/denchmark-link&gt;
 , but the whole constructor thing was totally user error on my part. That said, I shouldn't have brought up 2 separate issues in a single github issue.
		</comment>
	</comments>
</bug>