<bug id='2421' author='thschaaf' open_date='2020-06-29T17:39:18Z' closed_time='2020-06-30T12:14:09Z'>
	<summary>testing gets stuck when num_workers is set to value &amp;gt;0 in tests/base/model_utilities.py</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

While executing bash .run_local_tests.sh the test hangs frequently (but not always) if parallel data loading is enabled in tests/base/model_utilities.py by setting num_workers to a value larger than 0. If an manual keyboard interrupt (CTRL-c) is done the test continues with a "PASSED" message. This is an issue that does not always occur which could be an indicator that the test-frame work is waiting for the process to finish, but does not realize that the process has been completed.
The same issue occurs during CI testing with the result that time is exceeded.
The issue does not happen for  in tests/base/model_utilities.py; which is the reason for the change in PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2307&gt;#2307&lt;/denchmark-link&gt;
 to set this value to zero. This is an unsatisfying solution since it excludes the case of using multiple workers in dataloaders from testing, and risk that someone might inadvertently set the value to a different.
It is unclear of this is caused by an issue with Pythorch-Lightning, Pytorch, or the testing framework.
This issues serves to inform others what to expect when num_workers is set to a value &gt;0, and that this needs to be investigated. It might need someone wth good understanding of the pytest framework and concurrency issues in that framework.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

set num_workers=3 in  tests/base/model_utilities.py
Run bash .run_local_tests.sh or pytest -v -x tests/trainer/test_dataloaders.py (might need to run several times)
The pytest -v -x tests/trainer/test_dataloaders.py should finish successfully in 10-15 seconds but often does not
If test gets stuck doing a CRTL-c continues the testing and ends them successfully

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

This issue also happens in the CI environments.
My environment:
/Users/thomas.schaaf/virtualenv/Python37/pytorch-lightning/lib/python3.7/site-packages/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'

CUDA:

GPU:
available:         False
version:           None


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.5.1
pytorch-lightning: 0.8.2-dev
tensorboard:       2.2.2
tqdm:              4.46.1


System:

OS:                Darwin
architecture:

64bit



processor:         i386
python:            3.7.7
version:           Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

This issue came up during PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2307&gt;#2307&lt;/denchmark-link&gt;
 and was discussed with &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
.
My pytest version:
`pytest -V
This is pytest version 5.4.3, imported from /Users/thomas.schaaf/virtualenv/Python37/pytorch-lightning/lib/python3.7/site-packages/pytest/init.py
setuptools registered plugins:
pytest-flake8-1.0.6 at /Users/thomas.schaaf/virtualenv/Python37/pytorch-lightning/lib/python3.7/site-packages/pytest_flake8.py
pytest-cov-2.10.0 at /Users/thomas.schaaf/virtualenv/Python37/pytorch-lightning/lib/python3.7/site-packages/pytest_cov/plugin.py`
	</description>
	<comments>
		<comment id='1' author='thschaaf' date='2020-06-29T17:56:09Z'>
		this has been resolved in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2358&gt;#2358&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='thschaaf' date='2020-06-29T18:43:17Z'>
		the reason for setting num_workers=0 is getting Windows builds up to speed, otherwise, it took 10x longer...
		</comment>
		<comment id='3' author='thschaaf' date='2020-06-30T13:59:34Z'>
		
the reason for setting num_workers=0 is getting Windows builds up to speed, otherwise, it took 10x longer...

&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 : I changed  to 0 because with  pytest got stuck on my Mac and also on Ubuntu machines during CI. However the behavior is not 100% deterministic; meaning the testing sometimes did not get stuck. My suspicion is that this is caused by the test framework. Not sure if Windows is especially susceptible for this issue, but other platforms are affected as well.
I just pulled the latest code version and tested if the issue disappeared on my Mac by setting num_workers=3 and running the test pytest -v -x tests/trainer/test_dataloaders.py. Unfortunately the test got stuck with 3 workers, but passes consistently in less than 10 seconds with num_workers=0.
The ActivityMonitor does not show that CPU cycles are used from 10 concurrent Python processes when the test gets stuck.
&lt;denchmark-link:https://user-images.githubusercontent.com/42753790/86133497-5de00c80-bab6-11ea-89fd-ad0fcf69db4f.png&gt;&lt;/denchmark-link&gt;

This seems to be not a Windows only issue. Setting num_workers=0 certainly made the tests pass for now, but affects what gets tested. To me it does not seem resolved, but maybe it should not be labeled as a "Bug"?
		</comment>
	</comments>
</bug>