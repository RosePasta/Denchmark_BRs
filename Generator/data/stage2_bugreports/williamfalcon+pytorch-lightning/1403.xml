<bug id='1403' author='OliverCWY' open_date='2020-04-07T14:41:41Z' closed_time='2020-04-10T13:20:15Z'>
	<summary>TPU error: RAM full, page stopped responding and slower than GPU on google colab</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Open lightning_mnist_tpu.ipynb
Run the code

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The code runs normally and faster than GPU.
&lt;denchmark-h:h3&gt;Error&lt;/denchmark-h&gt;


The webpage stopped responding soon after running the trainer, on several devices such as PC, phone and puffin browser, with Ram reaching 100% on PC. (both GPU and TPU)
Iteration speed for TPU calculations is ~30 it/s while iteration speed for GPU is &gt;90 it/s.

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Running the demo notebook Lightning-demo.ipynb on TPU solved the first error but the iteration speed is still slower for TPU, with perpare_data added.
	</description>
	<comments>
		<comment id='1' author='OliverCWY' date='2020-04-07T14:42:22Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='OliverCWY' date='2020-04-08T11:23:47Z'>
		&lt;denchmark-link:https://github.com/OliverCWY&gt;@OliverCWY&lt;/denchmark-link&gt;
 may you share a link to the notebook?
		</comment>
		<comment id='3' author='OliverCWY' date='2020-04-08T11:59:27Z'>
		
@OliverCWY may you share a link to the notebook?

Sorry I did not make it clear that I was using the official TPU demo notebook &lt;denchmark-link:https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3&gt;https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='OliverCWY' date='2020-04-08T12:48:01Z'>
		well for me it is falling on TQDM error:
&lt;denchmark-code&gt;Exception in device=TPU:0: 'tqdm_notebook' object has no attribute 'leave'
  File "/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py", line 247, in close
    if self.leave:
AttributeError: 'tqdm_notebook' object has no attribute 'leave'
AttributeError: 'tqdm_notebook' object has no attribute 'leave'
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py", line 119, in _start_fn
    fn(gindex, *args)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 505, in tpu_train
    self.run_pretrain_routine(model)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 850, in run_pretrain_routine
    self.val_progress_bar.close()
  File "/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py", line 247, in close
    if self.leave:
AttributeError: 'tqdm_notebook' object has no attribute 'leave'
Exception in device=TPU:1: 'tqdm_notebook' object has no attribute 'leave'
Exception in device=TPU:7: 'tqdm_notebook' object has no attribute 'leave'
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/OliverCWY&gt;@OliverCWY&lt;/denchmark-link&gt;
 may you share your error?
		</comment>
		<comment id='5' author='OliverCWY' date='2020-04-08T13:32:43Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 There are no error messages for me, despite for a lot of warnings. The webpage stopped responding even when I set .
One possible reason is that the tqdm progress bar reloads for every update without freeing the memory, but the problem only exists in the TPU demo notebook. When I copy the codes into the demo notebook (&lt;denchmark-link:https://colab.research.google.com/drive/1IqfISTenqy50Fq8DafCmm8KfUf9JssJF&gt;https://colab.research.google.com/drive/1IqfISTenqy50Fq8DafCmm8KfUf9JssJF&lt;/denchmark-link&gt;
), everything is fine except for the iteration speed.
		</comment>
		<comment id='6' author='OliverCWY' date='2020-04-08T20:00:54Z'>
		
well for me it is falling on TQDM error:
Exception in device=TPU:0: 'tqdm_notebook' object has no attribute 'leave'
  File "/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py", line 247, in close
    if self.leave:
AttributeError: 'tqdm_notebook' object has no attribute 'leave'
AttributeError: 'tqdm_notebook' object has no attribute 'leave'
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py", line 119, in _start_fn
    fn(gindex, *args)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py", line 505, in tpu_train
    self.run_pretrain_routine(model)
  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 850, in run_pretrain_routine
    self.val_progress_bar.close()
  File "/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py", line 247, in close
    if self.leave:
AttributeError: 'tqdm_notebook' object has no attribute 'leave'
Exception in device=TPU:1: 'tqdm_notebook' object has no attribute 'leave'
Exception in device=TPU:7: 'tqdm_notebook' object has no attribute 'leave'

@OliverCWY may you share your error?

If you restart the runtime, tqdm error should go away.
		</comment>
		<comment id='7' author='OliverCWY' date='2020-04-09T03:28:08Z'>
		I tried to profile a efficientnet_es model on cifar10. It is taking ~500+ seconds for forward propagation but only ~47 seconds for back prop. Also, it is taking over 15 mins to run 1 epoch which doesn't seem right. This was over 6 epochs.
		</comment>
		<comment id='8' author='OliverCWY' date='2020-04-09T12:31:09Z'>
		I used lightning on colab for other models and they all had this problem.
		</comment>
		<comment id='9' author='OliverCWY' date='2020-04-09T12:31:23Z'>
		
I used lightning on colab for other models and they all had this problem.

For GPU as well
		</comment>
		<comment id='10' author='OliverCWY' date='2020-04-09T12:36:02Z'>
		colabs are slow with a low refresh frequency.
set the tqdm freq refresh to 10 or something more than 1
		</comment>
		<comment id='11' author='OliverCWY' date='2020-04-09T12:38:37Z'>
		can you share the colabs?
we have speed benchmarks in CI, and lightning is a few seconds slower than pure pytorch because of the loggers and tqdm bar but not slower by much (ie: if you added tensorboard to your code it would be as slow).
this is likely because you‚Äôre not putting something on GPU or something like that
		</comment>
		<comment id='12' author='OliverCWY' date='2020-04-09T13:05:20Z'>
		Just tested on colab... it works fine
&lt;denchmark-link:https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3#scrollTo=kr8cql-aaKnC&gt;https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3#scrollTo=kr8cql-aaKnC&lt;/denchmark-link&gt;

		</comment>
		<comment id='13' author='OliverCWY' date='2020-04-10T03:50:09Z'>
		
Just tested on colab... it works fine
https://colab.research.google.com/drive/1-_LKx4HwAxl5M6xPJmqAAu444LTDQoa3#scrollTo=kr8cql-aaKnC

The memory used by the iframes in google colab reaches 600+MB after the 29th epoch and continues to increase so probably setting the refresh requency does not actually address the problem.
And I am actually referring to the speed using different devices with pytorch-lightning. Running on TPU is significantly slower than running on GPU or even CPU.
Using a single layer of nn.Linear:
TPU:
&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/78960209-ef775800-7b1f-11ea-9860-cec670e836de.png&gt;&lt;/denchmark-link&gt;

CPU:
&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/78960212-f43c0c00-7b1f-11ea-8152-388b537b2aa1.png&gt;&lt;/denchmark-link&gt;

GPU(P100):
&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/78960225-028a2800-7b20-11ea-8244-66f6dff0c5a4.png&gt;&lt;/denchmark-link&gt;

With more layers:
TPU:
&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/78960243-0f0e8080-7b20-11ea-8ba2-0e42d4392d4f.png&gt;&lt;/denchmark-link&gt;

CPU:
&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/78960248-10d84400-7b20-11ea-8ad5-c53bc77e7631.png&gt;&lt;/denchmark-link&gt;

GPU(P100):
&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/78960252-15046180-7b20-11ea-8b29-5746be9416f3.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='14' author='OliverCWY' date='2020-04-10T13:20:15Z'>
		speed fixed on 0.7.3
The RAM issue is a colab issue not a PL issue. Crash the ram using cell 1 or upgrade to PRO
		</comment>
		<comment id='15' author='OliverCWY' date='2020-04-11T08:16:45Z'>
		
speed fixed on 0.7.3
The RAM issue is a colab issue not a PL issue. Crash the ram using cell 1 or upgrade to PRO

&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 Sorry but I don't think the problem is solved.
Just tested on colab:
&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/79038865-6f302000-7c0f-11ea-8353-7d4dfeb3faa9.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/79038883-92f36600-7c0f-11ea-9ae7-ad2b7727ea68.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/25003495/79038885-9850b080-7c0f-11ea-943c-8cc64476d141.png&gt;&lt;/denchmark-link&gt;

I am referring to the memory used by my browser running colab when saying "RAM full", so it is not the problem of the backend.
Thank you for you patience.
		</comment>
		<comment id='16' author='OliverCWY' date='2020-06-27T11:00:41Z'>
		I am afraid this does not work for me either, hence I also don't think that the problem is solved.
I have tried all the versions, given in the notebook.
Additionally, I have also tried it with the version 20200516. That version is given in the official colab TPU MNIST example notebook which does not use pytorch-lightening, ie 20200516. A reference is below in NB2.
The summary of the results are:
"1.5" : wont run at all
"20200325" hangs in the final epoch (with 10 epochs in the 10th, with 3 epochs in the 3rd)
"nightly" crashes with : Exception: process 0 terminated with signal SIGABRT
"20200516" hangs after one epoch
I have tried this several times over the last few days. With the exception of the nightly all these results have always been the same.
NB1:
Locally I am on a Mac, not sure whether this makes a difference.
My terminal gives this

uname -a
Darwin osx-lhind6957 18.7.0 Darwin Kernel Version 18.7.0: Mon Apr 27 20:09:39 PDT 2020; root:xnu-4903.278.35~1/RELEASE_X86_64 x86_64

NB2:
The links for that official colab TPU MNIST example notebook which does not use pytorch lightning are here:
&lt;denchmark-link:https://cloud.google.com/tpu/docs/colabs?hl=de&gt;https://cloud.google.com/tpu/docs/colabs?hl=de&lt;/denchmark-link&gt;

&lt;denchmark-link:https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb?authuser=1#scrollTo=sPJVqAKyml5W&gt;https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb?authuser=1#scrollTo=sPJVqAKyml5W&lt;/denchmark-link&gt;

(The official notebook which does not use pytorch lightning has no problem and runs through with 20200516)
		</comment>
	</comments>
</bug>