<bug id='840' author='versatran01' open_date='2020-02-14T14:48:07Z' closed_time='2020-02-26T21:55:19Z'>
	<summary>Handle abstract loader that doesn't have a dataset member</summary>
	<description>
&lt;denchmark-h:h1&gt;Feature / Bug (?)&lt;/denchmark-h&gt;

I have an abstract loader that chains multiple pytorch loaders when training on video sequences. Each small loader contains one sequence and the chained loader just use itertools.chain to chain them together.
I cannot put all data in a single loader because it does not make sense to read images from two different sequences.
I cannot use an IterableDataset because that would still have the same problem.



pytorch-lightning/pytorch_lightning/trainer/data_loading.py


         Line 58
      in
      06242c2






 if EXIST_ITER_DATASET and isinstance(self.get_train_dataloader().dataset, IterableDataset): 





This assumes that a loader must have a dataset field, which I felt is too restrictive. I suggest putting a check to see if the loader has a dataset member before doing this check. My workaround, for now, is just to declare a dataset member in my ChainedLoader to be None.
&lt;denchmark-h:h1&gt;Fix&lt;/denchmark-h&gt;

Since this kind of abstract loader does not have an explicit handle to a dataset, it should belong to the concept of an IterableDataset, which the user specifies the number of batches rather than a percentage. So a simple fix would be to put a check of hasattr somewhere in the same line.
	</description>
	<comments>
	</comments>
</bug>