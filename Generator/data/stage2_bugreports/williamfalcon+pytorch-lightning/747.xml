<bug id='747' author='sneiman' open_date='2020-01-25T06:06:04Z' closed_time='2020-02-03T20:22:55Z'>
	<summary>model reference after ddp does not have trained parameters</summary>
	<description>
The model reference after ddp training does not appear to be a copy of the trained model - but the unaltered model as initialized. Expected behavior is that the model reference following the return from trainer.fit() is the trained model.
I am using lightning 0.6, pytorch 1.4, CUDA 10.2, python 3.6.8, Ubuntu 18.04.2LTS. 14 core, 7 gpu standalone node, no virtual env, no SLURM.
I train the model as always with the following code snippet:
&lt;denchmark-code&gt;        os.environ['OMP_NUM_THREADS']   = '5'
        os.environ['KMP_AFFINITY']      = "granularity=fine,compact,1,0"
        os.environ['KMP_BLOCKTIME']     = '1'
        os.environ['MASTER_PORT']       = str(args.port+int(log_dec))

        model                           = gaze1(netname, log_path, args.gpus, args.batch_size, args.report_flags, args.log_flags)
        model.desc                      = net_desc
        trainer                         = Trainer(max_epochs=args.epochs, gpus=args.gpus, show_progress_bar=False,  gradient_clip_val=5, default_save_path=log_path, num_sanity_val_steps=0, early_stop_callback=None, distributed_backend='ddp', weights_summary=None)
        model.trainer                   = trainer
        trainer.fit(model)
        # trainer.test()
        trainer.test(model)

&lt;/denchmark-code&gt;

The first problem I ran into is that a call to trainer.test() fails as noted elsewhere. So, used trainer.test(model). This produced the same results as expected for an untrained model - though validation statistics are good, and prior experience shows that the test and validation statistics are very similar.
I did a few obvious tests - and it is clear that the parameters of model post trainer.fit() have not been altered, though the parameters on each gpu have been.
Note that trainer.test(model) reloads each gpu as if from scratch.
Note that for some reason no checkpoints are being created - just FYI, as I have not investigated this yet.
	</description>
	<comments>
		<comment id='1' author='sneiman' date='2020-01-26T15:12:44Z'>
		Hello, could you please edit the description - what is the problem, what is happening and what is the expected behaviour not just paste a link to a script... Thx
		</comment>
		<comment id='2' author='sneiman' date='2020-01-26T16:19:09Z'>
		Sorry - I did fill this out properly and did not paste a link - not sure what happened. Will rewrite now.
		</comment>
		<comment id='3' author='sneiman' date='2020-01-26T22:47:40Z'>
		I simplifed the code that produces this - I wanted to eliminate some things I was only doing in the model on gpu 0 which cluttered the code.
I ran this both in dp and ddp.
dp works fine, ddp trains fine - including ongoing statistics - but cannot run trainer.test() - it thinks model is None, and cannot run trainer.test(model) as it resubmits the entire job - and APPARENTLY loses the trained parameters in the restart - test results are like a random network.
I have attached the model for your perusal. I have included a utility module 'sn_utils.py' which has some screen messaging functions, and I did not include the dataset. If you want to try to run it, happy to send it also.
&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 mentioned in 746  to change back end to dp and run test that way. Happy to try it - how do I change backend for test only? ?
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/files/4114698/gaze_03c_ddp.py.txt&gt;gaze_03c_ddp.py.txt&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/files/4114701/sn_utils.py.txt&gt;sn_utils.py.txt&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='sneiman' date='2020-01-26T23:25:48Z'>
		I did try changing the backend directly after training with
&lt;denchmark-code&gt;trainer.use_ddp = False
trainer.use_dp = True
&lt;/denchmark-code&gt;

Fails as originally described for both trainer.test().
trainer.test(model) fails with"
&lt;denchmark-code&gt;    train_sampler       = torch.utils.data.distributed.DistributedSampler(self.data.trn_dataset, num_replicas=self.num_gpus)
  File "/home/seth/.local/lib/python3.6/site-packages/torch/utils/data/distributed.py", line 34, in __init__
    rank = dist.get_rank()
  File "/home/seth/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 564, in get_rank
    _check_default_pg()
  File "/home/seth/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py", line 193, in _check_default_pg
    "Default process group is not initialized"
AssertionError: Default process group is not initialized
&lt;/denchmark-code&gt;

I did try not using a DistributedSampler with the test data set - but the failure is on training. Again, it seems like the whole job is being restarted.
Happy to try anything else you wish.
		</comment>
		<comment id='5' author='sneiman' date='2020-01-27T14:31:48Z'>
		I don't think the ddp model is saving checkpoints - which would explain the reloading issue, I expect. I have no indication of why checkpoints are not being saved - there is nothing in the default log path. I will continue to investigate.
		</comment>
		<comment id='6' author='sneiman' date='2020-01-27T21:15:35Z'>
		Pretty sure I've identified the cause of this.
I have been using the default callback_checkpoint, which fails as noted above. If I specify a model checkpoint -
&lt;denchmark-code&gt;        checkpoint_callback = ModelCheckpoint(filepath=log_path, verbose=True, monitor='val_loss', mode='min', prefix='gaze_03c_ddp_')
&lt;/denchmark-code&gt;

... essentially identical to the default as detailed in the docs - things work as expected. trainer() has lost track of the model, but trainer.test(model) successfully loads the checkpoint from the exact location passed in ModelCheckpoint() (not from deeper in the log structure as is typical) and runs test() on that trained model. Yay!
I suspect the root of this has been identified in  &lt;denchmark-link:url&gt;https://github.com/PyTorchLightning/pytorch-lightning/issues/734&lt;/denchmark-link&gt;
.
To &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
's question wrt testing - seems like something is being missed wrt default checkpoint_callback.
I think this should stay open until 734 resolved and testing covers default checkpoint_callback. Not sure if this affects more than ddp.
		</comment>
		<comment id='7' author='sneiman' date='2020-01-29T19:41:13Z'>
		&lt;denchmark-link:https://github.com/sneiman&gt;@sneiman&lt;/denchmark-link&gt;
 good catch. Mind submitting a PR? Would love the fix
		</comment>
		<comment id='8' author='sneiman' date='2020-01-29T23:28:30Z'>
		Happy to dig in. I have a ton of experience building good code - but essentially none on working on an open source project. So, I will need a little guidance how this is supposed to go ...
Do I simply work on the master, and the do a PR? Or do a PR and work on the fork?
And then what? I'm a noob in the protocol for this. A grey haired, long time programming noob! ;)
		</comment>
		<comment id='9' author='sneiman' date='2020-02-01T20:09:33Z'>
		This issue has combined 2 items:
First is the fact that the trainer declared by the user appears to have lost track of the model when using ddp.
Second is in 734 - about losing track of where checkpoints go.
These combined for 2 kinds of 'failures' when calling trainer.test() using ddp. One where trainer.test() fails, and one where trainer.test(model) fails when using a default checkpoint_callback.
I have been working on the first. In essence the call sequence can't find a model that has test_step and test_end defined. I believe I know what's happening and would like some feedback before I submit a PR.
Of course, when using ddp, the useful versions of both the trainer and model are all on gpus. There is nothing gathered back to the original instantiations in the master process. Of course, they could be gathered back - but this essentially means copying the trainer and model from device_ids[0] back to the master process, so that trainer.test() behaves as expected.
It seems to me that any realistic version of this is effectively identical to simply calling trainer.test(model). Which already works fine.
Of course, this a fairly expensive proposition - basically everything gets flushed off the gpus and the job is restarted.
For my purposes, the only useful thing would be to avoid this restarting - which is easily accomplished by adding a flag to Trainer() to run test when training exits.
I have tried this - and it works fine. It uses the parameters of the last epoch - not necessarily the optimum.
So ... I don't have a suggestion on how to actually get the right trainer and model back to master process that is better than simply calling trainer.test(model), and don't think we need one. I personally could use a flag to run trainer.test() after trainer.fit(model) to avoid all the restarting.
Thoughts and criticisms welcome.
seth
PS I will look into the checkpoint location issue separately
		</comment>
		<comment id='10' author='sneiman' date='2020-02-01T20:33:58Z'>
		Btw, pls avoid using #number, it is automatically linked to the issue/PR of the same number so it may make your comment misleading...
		</comment>
		<comment id='11' author='sneiman' date='2020-02-01T20:47:24Z'>
		got it. thx
		</comment>
		<comment id='12' author='sneiman' date='2020-02-03T20:22:55Z'>
		I have not been able to find a way to get a trained model reference back to the trainer.model property in the spawning process when using ddp. I have found a way to get a model with the right parameters back to the spawning process. But no way to get it into trainer.model without altering user's code - which is what I believe the objective is.
So closing this - if anyone has a good idea on how to do this, I will be happy to pursue it.
		</comment>
	</comments>
</bug>