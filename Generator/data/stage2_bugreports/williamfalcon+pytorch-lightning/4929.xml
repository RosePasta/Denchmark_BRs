<bug id='4929' author='veritas9872' open_date='2020-12-01T09:13:16Z' closed_time='2020-12-11T21:24:28Z'>
	<summary>Learning rate scheduling interval on LambdaLR scheduler cannot be set to "step".</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hello. I am trying to use LambdaLR learning rate scheduler with lr updates at every step with my custom function.
However, I have found that even with scheduler={'interval': 'step'}, the update occurs at each epoch, not each training step.
&lt;denchmark-code&gt;def configure_optimizers(self):
    def func(step: int, max_steps=max_steps):
        return (1 - (step / max_steps)) ** 0.9
    scheduler = optim.lr_scheduler.LambdaLR(lr_lambda=func)
    return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'interval': 'step'}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Apologies for the small snippet but the project is very complex and I think this is the relevant part.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Learning rate should decrease at every training step after backpropagation.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Ubuntu16.04 LTS on Docker. PyTorch Lightning v.1.0.6 and 1.0.8. Pytorch version 1.6.0
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I think the usage of step-wise iteration could be better documented as to how the arguments are parsed and utilized.
	</description>
	<comments>
		<comment id='1' author='veritas9872' date='2020-12-02T01:47:58Z'>
		I have also found that ExponentialLR also changes learning rate only at the epoch level.
From this, I think that the {'interval': 'step'} may not be functioning properly.
		</comment>
		<comment id='2' author='veritas9872' date='2020-12-02T17:32:25Z'>
		can you reproduce this with &lt;denchmark-link:https://colab.research.google.com/drive/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing&gt;https://colab.research.google.com/drive/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='veritas9872' date='2020-12-03T18:14:27Z'>
		Facing the same issue with OneCycleLR. Looks like the {'interval': 'step'} thing is indeed not working properly.
		</comment>
		<comment id='4' author='veritas9872' date='2020-12-04T21:22:50Z'>
		I think you have to pass the interval inside a dict to the lr_scheduler key. In your example, if you try
def configure_optimizers(self):
    def func(step: int, max_steps=max_steps):
        return (1 - (step / max_steps)) ** 0.9
    scheduler = optim.lr_scheduler.LambdaLR(lr_lambda=func)
    return {
        'optimizer': optimizer, 
        'lr_scheduler': {
            'scheduler': scheduler, 
            'interval': 'step'
        }
    }
it should work as expected.
		</comment>
		<comment id='5' author='veritas9872' date='2020-12-11T21:24:28Z'>
		Closing for now. Feel free to reopen if not solved!
		</comment>
	</comments>
</bug>