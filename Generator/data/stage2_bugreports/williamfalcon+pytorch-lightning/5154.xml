<bug id='5154' author='aiyolo' open_date='2020-12-16T02:20:43Z' closed_time='2020-12-16T09:36:03Z'>
	<summary>Too much ports can't be released after ddp-sharded training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I just tried the new feature ddp-shared training, everything goes well except that there are too much ports remained opened after training. Is the phenomenon normal?
&lt;denchmark-link:https://user-images.githubusercontent.com/12464091/102296250-e9c3f300-3f87-11eb-98ce-0c0bc2c49c73.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;...
trainer = Trainer(max_epochs=opt.epochs, gpus=2, accelerator='ddp', plugins='ddp_sharded')
trainer.fit(model, train_loader, val_loader)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

ports should be released right after training is finished
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Note: Bugs with code are solved faster ! Colab Notebook should be made public !


IDE: Please, use our python bug_report_model.py template.


Colab Notebook: Please copy and paste the output from our environment collection script (or fill out the checklist below manually).


You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;


PyTorch Version (e.g., 1.0):
OS (e.g., Linux):
How you installed PyTorch (conda, pip, source):
Build command you used (if compiling from source):
Python version:
CUDA/cuDNN version:
GPU models and configuration:
Any other relevant information:

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='aiyolo' date='2020-12-16T07:44:59Z'>
		do you also have hanging processes in the background, or just the blocked ports? (check with  or similar)
could this be a problem with ddp alone? or is it really only with fairscale/sharded? cc &lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='aiyolo' date='2020-12-16T09:09:37Z'>
		&lt;denchmark-code&gt;(base) ‚ûú  ~ pgrep python
840
851
864
....
31307
31342
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;(base) ‚ûú  ~ lsof -i
COMMAND     PID   USER   FD   TYPE    DEVICE SIZE/OFF NODE NAME
....
node       4138 aiyolo   19u  IPv4 870494046      0t0  TCP localhost:35723-&gt;localhost:38164 (ESTABLISHED)
python     4579 aiyolo   12u  IPv4 571538969      0t0  TCP localhost:49729 (LISTEN)
python     4579 aiyolo   14u  IPv4 571538971      0t0  TCP localhost:40373 (LISTEN)
python     4579 aiyolo   16u  IPv4 571538973      0t0  TCP localhost:51103 (LISTEN)
python     4579 aiyolo   18u  IPv4 571538975      0t0  TCP localhost:33943 (LISTEN)
python     4579 aiyolo   23u  IPv4 571538979      0t0  TCP localhost:35083 (LISTEN)
....
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;(base) ‚ûú  ~ nvidia-smi
Wed Dec 16 16:56:29 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |
| 71%   84C    P2   249W / 250W |   5778MiB / 11176MiB |     98%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |
| 21%   54C    P2    61W / 250W |    689MiB / 11178MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1363      G   /usr/lib/xorg/Xorg                            40MiB |
|    0      1486      G   /usr/bin/gnome-shell                          47MiB |
|    0     31342      C   ...gy/miniconda3/envs/pytorch36/bin/python  5677MiB | # not my process
+-----------------------------------------------------------------------------+
&lt;/denchmark-code&gt;

hope it can help understanding my question.
		</comment>
		<comment id='3' author='aiyolo' date='2020-12-16T09:36:03Z'>
		It seems that the opened ports could be normally released when performing a standard mnist train, so the phenomenon may be caused by something else.
		</comment>
	</comments>
</bug>