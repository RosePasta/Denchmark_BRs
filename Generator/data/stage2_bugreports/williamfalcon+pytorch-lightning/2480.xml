<bug id='2480' author='HHousen' open_date='2020-07-03T04:08:46Z' closed_time='2020-07-09T11:11:08Z'>
	<summary>For versions &amp;gt;0.8.2 learning rate is zero for last epoch (potentially a logging bug)</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Version 0.8.2 and above changed the behavior of either my learning rate scheduler or the WandbLogger logger. I am using a linear warmup and decay scheduler. However, the learning rate graph produced by the LearningRateLogger is as shown below ever since version 0.8.2:
&lt;denchmark-link:https://user-images.githubusercontent.com/11785397/86428929-fc898a80-bcbb-11ea-85b1-2a42f4c54cfd.png&gt;&lt;/denchmark-link&gt;

The period where the learning rate is zero corresponds to the last epoch of training as you can see below:
&lt;denchmark-link:https://user-images.githubusercontent.com/11785397/86428981-22af2a80-bcbc-11ea-90ff-bf4418dfe090.png&gt;&lt;/denchmark-link&gt;

This graph raises another issue. The first epoch appears to take twice as many steps as the second and third epoch. I specified max_epochs=3. During training, each epoch takes the same amount of time, so this seems like a logging issue.
Note that the above graphs are for a model that had its training stopped early. So the last epoch is slightly shorter than the second to last. This is not the issue.
Both of these issues (the 0 learning rate and the twice-as-long epoch) do not exist in version 0.8.1, and both graphs look as they should.
These issues could be caused by the logger or they might actually occur and be logged correctly. I have looked through the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/releases&gt;changelog&lt;/denchmark-link&gt;
 and I am guessing that these bugs are caused by "Changed epoch indexing from 0 instead of 1" (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2289&gt;#2289&lt;/denchmark-link&gt;
). I also may be relying on the fact that epoch indexing started at 1 somewhere in my code, but I do not believe this to be the case.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Reproducing this problem may be difficult since I can't provide the script and data I used. I used the WandbLogger logger and LearningRateLogger callback. I trained with 1400 warmup steps and accumulate_grad_batches set to 2.
I can provide additional code samples or information that you may need.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

def lr_lambda_func(current_step, num_warmup_steps, num_training_steps):
    if current_step &lt; num_warmup_steps:
        return float(current_step) / float(max(1, num_warmup_steps))
    return max(
        0.0,
        float(num_training_steps - current_step)
        / float(max(1, num_training_steps - num_warmup_steps)),
    )

t_total = int(len(self.train_dataloader_object) * self.hparams.max_epochs // self.hparams.accumulate_grad_batches)

lr_lambda = partial(
    lr_lambda_func,
    num_warmup_steps=self.hparams.warmup_steps
    * self.hparams.accumulate_grad_batches,
    num_training_steps=t_total,
)

scheduler = LambdaLR(optimizer, lr_lambda, -1)
scheduler_dict = {"scheduler": scheduler, "interval": "step"}
return ([optimizer], [scheduler_dict])
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The learning rate should warmup and decay in versions greater than 0.8.2 the same way it does in versions less than 0.8.2. Each epoch should be the same number of steps.
The below graphs highlight the expected behavior. They are from a different model so they are not directly comparable, but their shape is as expected since they were captured from a model trained with pytorch_lightning version 0.8.1.
&lt;denchmark-link:https://user-images.githubusercontent.com/11785397/86430600-b5ea5f00-bcc0-11ea-8c07-b78670f1ef7d.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/11785397/86430629-c8649880-bcc0-11ea-8aca-85cdde47a0d2.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla P100-PCIE-16GB


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.5.1+cu101
pytorch-lightning: 0.8.4
tensorboard:       2.2.2
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Wed Feb 19 05:26:34 PST 2020



	</description>
	<comments>
		<comment id='1' author='HHousen' date='2020-07-03T04:09:41Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='HHousen' date='2020-07-03T05:36:59Z'>
		it would be good to know whether this can be observed with the other loggers as well. Could you run your example also with TensorboardLogger?
		</comment>
		<comment id='3' author='HHousen' date='2020-07-03T15:06:33Z'>
		Hey! I believe problem lies in configure_accumulated_gradients() when accumulate_grad_batches is integer, scheduler is set to use it from current_epoch=1, but Trainer starts from current_epoch=0, so trainer.accumulate_grad_batches = self.scheduling.get(self.epochs[i]) sets accumulate_grad_batches to default value (1) for this epoch.
		</comment>
		<comment id='4' author='HHousen' date='2020-07-03T15:10:10Z'>
		Update: When accumulate_grad_batches is an integer, Scheduler gets {1: accumulate_grad_batches} as input, and then scheduling.update({0: 1}) inserts "default" 1 for first epoch.
		</comment>
		<comment id='5' author='HHousen' date='2020-07-03T15:18:41Z'>
		&lt;denchmark-link:https://github.com/HHousen&gt;@HHousen&lt;/denchmark-link&gt;
 You could do workaround and set  in pl.Trainer (I did so), but you might have problems with restoring from checkpoint, as
&lt;denchmark-code&gt;n_accum = 1 if self.accumulate_grad_batches is None else self.accumulate_grad_batches
expected_steps = self.num_training_batches / n_accum
&lt;/denchmark-code&gt;

in restore_training_state will try to use dict in division.
		</comment>
		<comment id='6' author='HHousen' date='2020-07-03T19:56:20Z'>
		&lt;denchmark-link:https://github.com/szymonzareba&gt;@szymonzareba&lt;/denchmark-link&gt;
 Yep, setting  to  fixed this problem (I create my  like so: ). Both the learning rate and epoch graphs are now correct. It seems like your reasoning is correct.
		</comment>
		<comment id='7' author='HHousen' date='2020-07-03T20:20:50Z'>
		&lt;denchmark-link:https://github.com/HHousen&gt;@HHousen&lt;/denchmark-link&gt;
 mind send a PR?
		</comment>
		<comment id='8' author='HHousen' date='2020-07-03T20:51:23Z'>
		Sure
		</comment>
		<comment id='9' author='HHousen' date='2020-07-03T22:45:38Z'>
		&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2490&gt;#2490&lt;/denchmark-link&gt;
 fixes this. There was a problem with the  test. See &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2490&gt;#2490&lt;/denchmark-link&gt;
 for more information.
		</comment>
	</comments>
</bug>