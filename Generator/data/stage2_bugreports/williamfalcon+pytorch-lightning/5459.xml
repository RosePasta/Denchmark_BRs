<bug id='5459' author='YannDubs' open_date='2021-01-11T11:14:10Z' closed_time='2021-01-18T13:31:18Z'>
	<summary>[BUG] Logging in a callback does not work with multiple optimizers</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/5063&gt;#5063&lt;/denchmark-link&gt;
 is not solved when logging in callbacks (&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 ). Specifically,  returns the following
File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py", line 240, in auto_reduce_results_on_epoch_end
    opt_outputs = epoch_metrics[opt_idx]
KeyError: 0
when:

there are multiple optimizers
a callback calls pl_module.log

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1JoHFDQx1vad6th2IhNSxL0m2n_1HzQro?usp=sharing&gt;https://colab.research.google.com/drive/1JoHFDQx1vad6th2IhNSxL0m2n_1HzQro?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

the bug appears in stable or master (see notebook)
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I'm using 2 optimizers and  which does not work as it logs the loss in the callback. (Note that there are other issues with this callback and multiple optimizers &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4955&gt;#4955&lt;/denchmark-link&gt;
 )
	</description>
	<comments>
		<comment id='1' author='YannDubs' date='2021-01-11T23:33:42Z'>
		&lt;denchmark-link:https://github.com/akihironitta&gt;@akihironitta&lt;/denchmark-link&gt;
 please take a look!
		</comment>
	</comments>
</bug>