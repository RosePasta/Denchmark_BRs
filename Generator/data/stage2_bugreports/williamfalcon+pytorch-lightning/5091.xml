<bug id='5091' author='ORippler' open_date='2020-12-11T15:12:27Z' closed_time='2021-01-05T10:02:00Z'>
	<summary>Trainer.test() in combination with resume_from_checkpoint is broken</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When passing resume_from_checkpoint to Trainer, and then training (e.g. call to trainer.fit()), the state used for trainer.test() is always the checkpoint initially given to resume_from_checkpoint, and never the newer, better one.
&lt;denchmark-code&gt;trainer = Trainer(resume_from_checkpoint="path_to_ckpt") # pass ckpt to Trainer for resuming
trainer.fit() # do some fine-tuning/resume training
trainer.test() # should make use of "best" checkpoint, however uses ckpt passed to resume_from_checkpoint
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1ABXnUP10QUqHeUQmFy-FX26cV2w1JILA?usp=sharing&gt;https://colab.research.google.com/drive/1ABXnUP10QUqHeUQmFy-FX26cV2w1JILA?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

After fine-tuning, the best model state is looked up internally as introduced by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2190&gt;#2190&lt;/denchmark-link&gt;
 before running on the test dataset.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     True
pyTorch_version:   1.7.0+cu101
pytorch-lightning: 1.1.0
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

A hotfix is to manually set trainer.resume_from_checkpoint = None between calls to trainer.fit() and trainer.test().
&lt;denchmark-code&gt;trainer = Trainer(resume_from_checkpoint="path_to_ckpt") # pass ckpt to Trainer for resuming
trainer.fit()
trainer.resume_from_checkpoint = None
trainer.test()
&lt;/denchmark-code&gt;

The cause behind the issue is that  is performed internally by calling to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L796&gt;Trainer.fit()&lt;/denchmark-link&gt;
 for all configurations.
Long term, the checkpoint passed by  should most likely be consumed internally (i.e. reset to ) after the state is restored. Alternatively, one could make use of the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L793&gt;Trainer.testing&lt;/denchmark-link&gt;
 attribute to limit the utilization of  by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/12cb9942a18f9e9f6b7cdf6329abed0afdcd329e/pytorch_lightning/trainer/connectors/checkpoint_connector.py#L63-L64&gt;CheckpointConnector&lt;/denchmark-link&gt;
 to the training state only.
	</description>
	<comments>
		<comment id='1' author='ORippler' date='2020-12-11T21:03:20Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 thoughts?
		</comment>
		<comment id='2' author='ORippler' date='2020-12-16T11:19:50Z'>
		Hey &lt;denchmark-link:https://github.com/ORippler&gt;@ORippler&lt;/denchmark-link&gt;
,
Thanks for reporting the bug. Would you mind making the notebook public ?
Waiting for you to do so, I will try to reproduce the bug locally and update I am manage to.
Best regards,
Thomas Chaton.
		</comment>
		<comment id='3' author='ORippler' date='2020-12-16T11:23:01Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;

Link should be updated.
Cheers!
		</comment>
	</comments>
</bug>