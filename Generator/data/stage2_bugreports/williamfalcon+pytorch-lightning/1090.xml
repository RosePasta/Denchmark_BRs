<bug id='1090' author='nikhilno1' open_date='2020-03-08T10:56:55Z' closed_time='2020-03-08T12:40:34Z'>
	<summary>Training on TPU stuck at "Waiting to connect to client mesh master (300 seconds) localhost:54541"</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am training GPT2 model on TPU but training is getting stuck with following as the last line:
tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:54541
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I have followed all steps as outlined in &lt;denchmark-link:https://github.com/mgrankin/ru_transformers/tree/master/tpu&gt;https://github.com/mgrankin/ru_transformers/tree/master/tpu&lt;/denchmark-link&gt;
 to train a GPT2 model on TPU on Google Cloud.  As mentioned there, I was able to successfully run MNIST example without any issue

But when I ran the full training which is on a small dataset (10MB) just to make sure it runs successfully, the training is getting stuck with above line and doesn't proceed further. When I press Ctrl-C, I can see it is waiting in socket polling. I have tried restarting the TPU but same problem is observed.
Steps to reproduce the behavior:

Run the fit.sh present in the repo here: https://github.com/mgrankin/ru_transformers after all the necessary configuration.

&lt;denchmark-h:h4&gt;Logs&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/files/4303096/TPU.Hang.log&gt;TPU Hang.log&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Training should complete successfully.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;Collecting environment information...
PyTorch version: 1.5.0a0+65bad41
Is debug build: No
CUDA used to build PyTorch: None

OS: Debian GNU/Linux 9 (stretch)
GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] numpydoc==0.9.1
[pip] torch==1.5.0a0+65bad41
[pip] torch-xla==0.8+98a2790
[pip] torchvision==0.6.0a0+b6f28ec
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.0.14           py36ha843d7b_0  
[conda] mkl_random                1.1.0            py36hd6b4f25_0  
[conda] torch                     1.5.0a0+65bad41           &lt;pip&gt;
[conda] torch-xla                 0.8+98a2790               &lt;pip&gt;
[conda] torchvision               0.6.0a0+b6f28ec           &lt;pip&gt;


```### Additional context

This is my first time using TPU for training.
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='nikhilno1' date='2020-03-08T10:57:33Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='nikhilno1' date='2020-03-08T12:18:32Z'>
		Is it the case that training is actually completing but command doesn't return, which is what I am used to seeing?
		</comment>
		<comment id='3' author='nikhilno1' date='2020-03-08T12:40:34Z'>
		I am seeing the pytorch_model.bin getting created so which means training was successful. Closing the issue.
		</comment>
	</comments>
</bug>