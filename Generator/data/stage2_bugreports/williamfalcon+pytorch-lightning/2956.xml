<bug id='2956' author='lezwon' open_date='2020-08-13T17:37:50Z' closed_time='2020-08-13T22:57:24Z'>
	<summary>'NoneType' object has no attribute 'lower'  while training on TPU</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

distributed_backend is not set to 'tpu' which breaks it here: line 


pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py


         Line 412
      in
      2c935d0






 if self.distributed_backend.lower() not in ['ddp_spawn', 'ddp_cpu', 'tpu']: 





distributed_backend has to be explicitly specified in Trainer params to have it working which is misleading from the docs: &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/tpu.html#distributed-backend-with-tpu&gt;https://pytorch-lightning.readthedocs.io/en/latest/tpu.html#distributed-backend-with-tpu&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
&lt;denchmark-link:https://www.kaggle.com/lezwon/lightning-mnist-tpu&gt;https://www.kaggle.com/lezwon/lightning-mnist-tpu&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Should automatically set distributed_backend and train successfully.
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Related to :
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2698#issuecomment-671665202&gt;#2698 (comment)&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2812#issuecomment-668659590&gt;#2812 (comment)&lt;/denchmark-link&gt;

	</description>
	<comments>
	</comments>
</bug>