<bug id='1537' author='Deanamic' open_date='2020-04-20T17:53:29Z' closed_time='2020-09-22T22:29:59Z'>
	<summary>Output not reduced in DP</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I believe there is a discrepancy in the training and validation epoch ends when training in parallel gpus. I cannot release the code I'm working with as it is private, but I'll try and reproduce if I can.
The bug is that the output in validation epochs is not reduced, we can verify it &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0203938af8f69a19b7e0264f18e03d543d86e0e9/pytorch_lightning/trainer/evaluation_loop.py#L262&gt;in the source of the validation&lt;/denchmark-link&gt;
, and we can also see that all references to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0203938af8f69a19b7e0264f18e03d543d86e0e9/pytorch_lightning/trainer/logging.py#L183&gt;this function&lt;/denchmark-link&gt;
 are inside an  statement.
This can lead to bugs which are hard to detect without looking at the source code. For example,  you can calculate the mean of the loss at , but the same code depending on the number of gpus, validation samples and batchsize might lead to bugs in .
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Given 4 gpus, 10 validation samples and a batchsize of 4. The last validation step will only have two outputs. Then:
Running torch.stack([x['loss'] for x in outputs]).mean() works in training_epoch_end as outputs are reduced before reaching training_epoch_end.
Running torch.stack([x['loss'] for x in outputs]).mean() will fail in validaition_epoch_end as outputs are not reduced, and the first dimention of the output coming from the last batch differs. so torch.stack will fail
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I expect that both validation and training epochs have the same behaviour, or if not there has to be a clear warning.
	</description>
	<comments>
		<comment id='1' author='Deanamic' date='2020-04-20T22:55:54Z'>
		they should have the same behavior. mind putting together a colab to illustrate? mnist should be enough no?
		</comment>
		<comment id='2' author='Deanamic' date='2020-04-21T03:46:12Z'>
		&lt;denchmark-link:https://gist.github.com/Deanamic/d0aa1bdbed83f288d7365327046ceb97&gt;This code ilustrates the issue&lt;/denchmark-link&gt;

If you run with

2 gpus,
batch size:2
dataloader size: 3

The outputs from the 2 training/validation steps will have dimension 2 and 1 in each epoch.
For training, it's OK as it is reduced. We verify that the exception occurs in the validation epoch end, having the sanity run disabled.
On the other hand, if we run with a batch size of 3 the problem will not happen.I belive this bug only will happen if there is a gpu with no data in a run, i.e. len(val_dataloader)%batch_size &lt; n_gpusWhich make this bug very hard to diagnose and many times unnoticed.
The error is the following:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "dpbug.py", line 75, in &lt;module&gt;  
    main()  
  File "dpbug.py", line 72, in main  
    val_dataloaders = val_dl)  
  File "/cluster/home/dzhu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer  /trainer.py", line 702, in fit  
    self.dp_train(model)  
  File "/cluster/home/dzhu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer  /distrib_parts.py", line 538, in dp_train  
    self.run_pretrain_routine(model)  
  File "/cluster/home/dzhu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer  /trainer.py", line 865, in run_pretrain_routine  
    self.train()
  File "/cluster/home/dzhu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 363, in train
    self.run_training_epoch()
  File "/cluster/home/dzhu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py", line 457, in run_training_epoch
    self.run_evaluation(test_mode=self.testing)
  File "/cluster/home/dzhu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 372, in run_evaluation
    eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)
  File "/cluster/home/dzhu/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py", line 316, in _evaluate
    eval_results = model.validation_epoch_end(outputs)
  File "dpbug.py", line 44, in validation_epoch_end
    loss = torch.stack([x['loss'] for x in outputs]).mean()
RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 1 and 2 in dimension 1 at /tmp/pip-req-build-4baxydiv/aten/src/THC/generic/THCTensorMath.cu:71
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='Deanamic' date='2020-04-21T04:07:24Z'>
		QQ: Why is the reason the calls to  are only for training.
I think the fix is probably adding a call to  &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/bd168819f2e6433a8b0f0d497eda785199cf65cf/pytorch_lightning/trainer/evaluation_loop.py#L262&gt;here&lt;/denchmark-link&gt;
. but I haven't looked too deep into the issue. I'm having quite a bit of work right now, so hopefully someone else will look into it
		</comment>
		<comment id='4' author='Deanamic' date='2020-04-21T19:45:31Z'>
		&lt;denchmark-link:https://github.com/Deanamic&gt;@Deanamic&lt;/denchmark-link&gt;
 mind send a PR? :]
		</comment>
		<comment id='5' author='Deanamic' date='2020-04-21T20:49:35Z'>
		I won't be able to work on this until approximately mid May, so hopefully someone can work on this. Else I'll do it when I'm available
		</comment>
		<comment id='6' author='Deanamic' date='2020-05-03T09:14:36Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/Deanamic&gt;@Deanamic&lt;/denchmark-link&gt;
 : I can have a look at this if you do not mind :-)
		</comment>
		<comment id='7' author='Deanamic' date='2020-07-02T12:50:59Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='8' author='Deanamic' date='2020-08-17T21:26:28Z'>
		&lt;denchmark-link:https://github.com/jbschiratti&gt;@jbschiratti&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

I am curious the status of this issue. Could this be fixed in the near future?
I am in the process of migration from Pytorch to lightning and I ran into the same issue outlined above when I changed batch_size.
Thanks so much!
		</comment>
		<comment id='9' author='Deanamic' date='2020-08-17T21:50:36Z'>
		&lt;denchmark-link:https://github.com/junwen-austin&gt;@junwen-austin&lt;/denchmark-link&gt;
 would you be interested in sending a fix for this issue? :]
		</comment>
		<comment id='10' author='Deanamic' date='2020-08-17T22:01:02Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 I'd love to but my background is data science and do not have lots of coding experience in DDP. I am using Lightning for my data science project at work and am really impressed by the design and philosophy of Lightning 
		</comment>
		<comment id='11' author='Deanamic' date='2020-08-19T09:35:45Z'>
		I didn't know this bug was still open, the fix shouldn't involve coding with DDP as pytorch should handle everything. I could try fix that now, but I wondered if there is any way to test multiple GPUs as I do not have access to that.

I think the fix is probably adding a call to self.reduce_distributed_output() here. but I haven't looked too deep into the issue. I'm having quite a bit of work right now, so hopefully someone else will look into it

This should probably fix the issue, but would require further testing
		</comment>
		<comment id='12' author='Deanamic' date='2020-09-15T19:46:11Z'>
		&lt;denchmark-link:https://github.com/jbschiratti&gt;@jbschiratti&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/junwen-austin&gt;@junwen-austin&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/Deanamic&gt;@Deanamic&lt;/denchmark-link&gt;
 anyone wants to take this over?
		</comment>
		<comment id='13' author='Deanamic' date='2020-09-16T18:35:04Z'>
		This appears to be an issue on both training_epoch_end() as well as validation_epoch_end(), as there is now no reduction for either. This means that DP basically doesn't work, looking into.
		</comment>
		<comment id='14' author='Deanamic' date='2020-09-16T19:26:29Z'>
		Using results objects seems to fix the issue, but it is still a problem when returning dicts. Not sure if you are done with refactoring DP &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;

		</comment>
		<comment id='15' author='Deanamic' date='2020-09-22T22:29:59Z'>
		ok... this is not a bug.
The batch sizes are too small and you're not dropping the last batch.
What ends up happening is that one GPU processes 2 items and the second only 1 item (because your batch size is 3).
And thus your outputs aren't the same length... this is not something we can automate... so you have to either check for that in your code or make sure your batch size is always a multiple of GPUs.
&lt;denchmark-code&gt;from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import pytorch_lightning as pl
from pytorch_lightning import Trainer

class Data(Dataset):
    def __init__(self, sz):
        self.sz = sz

    def __len__(self):
        return self.sz

    def __getitem__(self, idx):
        return {'x' : torch.tensor(idx, dtype = torch.float32),
                'y': torch.tensor(0, dtype = torch.float32)}

class Model(pl.LightningModule):
    def __init__(self):
        super(Model, self).__init__()
        self.criterion = nn.MSELoss()
        self.layer = nn.Linear(1,1)

    def forward(self, x):
        return self.layer(x)

    def training_step(self, batch, batch_idx):
        output = self.forward(batch['x'].view(-1, 1))
        loss = self.criterion(batch['y'].view(-1), output.view(-1))
        return {'loss': loss}

    def training_epoch_end(self, outputs):
        ## This is ok, outputs have been reduced
        loss = torch.stack([x['loss'] for x in outputs]).mean()
        return {'log': {'train_loss' : loss}}

    def validation_step(self, batch, batch_idx):
        output = self.forward(batch['x'].view(-1, 1))
        loss = self.criterion(batch['y'].view(-1), output.view(-1))
        
        r = {'loss': loss}
        print(r)
        return r

    def validation_epoch_end(self, outputs):
        import pdb; pdb.set_trace()
        ## This is not ok, outputs have not been reduced
        loss = torch.stack([x['loss'] for x in outputs]).mean()
        return {'log': {'val_loss' : loss}}

    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.parameters(),
                                    lr = 0.001)
        return optimizer



def main(backend, ngpu):
    ngpu = 2
    b_size = 2
    gpus = list(range(ngpu))
    train_dl = DataLoader(Data(3), batch_size = b_size)

    # Batch size 2 on 2 gpus -&gt; second epoch a gpu will be empty
    # output is reduced in training epoch end, so it's fine but validation
    # epoch end will run into an exception
    # Batch size on 3 will run properly
    val_dl = DataLoader(Data(3), batch_size = b_size)

    net = Model()
    trainer = Trainer(gpus = gpus,
                      max_epochs = 2,
                      distributed_backend = backend,
                      num_sanity_val_steps=0)
    trainer.fit(net,
                train_dataloader = train_dl,
                val_dataloaders = val_dl)


if __name__ == '__main__':
    main('dp', 2)
&lt;/denchmark-code&gt;

		</comment>
	</comments>
</bug>