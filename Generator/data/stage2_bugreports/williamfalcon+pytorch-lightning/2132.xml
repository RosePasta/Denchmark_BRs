<bug id='2132' author='sshleifer' open_date='2020-06-09T15:04:49Z' closed_time='2020-10-29T06:10:43Z'>
	<summary>ddp: fp16 misaligned address</summary>
	<description>
Versions:
&lt;denchmark-code&gt;torch==1.5
pytorch-lightning==0.8.0rc1
cuda=10.1
&lt;/denchmark-code&gt;

In both O1 and O2 mode and the new ddp trainer.fit fails with the following traceback. This does not happen with ddp_spawn.
fp16 O2 mode:
  File "finetune.py", line 791, in &lt;module&gt;
    main(args)
  File "finetune.py", line 723, in main
    trainer: pl.Trainer = generic_train(model, args, early_stopping_callback=True)
  File "/home/shleifer/transformers_fork/examples/lightning_base.py", line 428, in generic_train
    trainer.fit(model)
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 890, in fit
    self.spawn_ddp_children(model)
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py", line 396, in spawn_ddp_children
    self.ddp_train(local_rank, model, is_master=True)
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py", line 472, in ddp_train
    self.run_pretrain_routine(model)
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/trainer.py", line 1050, in run_pretrain_routine
    self.train()
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 363, in train
    self.run_training_epoch()
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 445, in run_training_epoch
    _outputs = self.run_training_batch(batch, batch_idx)
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 621, in run_training_batch
    loss, batch_output = optimizer_closure()
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/trainer/training_loop.py", line 599, in optimizer_closure
    model_ref.backward(self, closure_loss, optimizer, opt_idx)
  File "/home/shleifer/pytorch-lightning/pytorch_lightning/core/hooks.py", line 155, in backward
    scaled_loss.backward()
  File "/home/shleifer/.conda/envs/nb/lib/python3.7/contextlib.py", line 119, in __exit__
    next(self.gen)
  File "/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/handle.py", line 123, in scale_loss
    optimizer._post_amp_backward(loss_scaler)
  File "/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/_process_optimizer.py", line 190, in post_backward_with_master_weights
    models_are_masters=False)
  File "/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/scaler.py", line 119, in unscale
    self.unscale_python(model_grads, master_grads, scale)
  File "/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/scaler.py", line 89, in unscale_python
    self.dynamic)
  File "/home/shleifer/.conda/envs/nb/lib/python3.7/site-packages/apex/amp/scaler.py", line 9, in scale_check_overflow_python
    cpu_sum = float(model_grad.float().sum())
RuntimeError: CUDA error: misaligned address
feel free to cross post if this is a pytorch issue
	</description>
	<comments>
		<comment id='1' author='sshleifer' date='2020-06-09T16:35:02Z'>
		&lt;denchmark-link:https://github.com/mcarilli&gt;@mcarilli&lt;/denchmark-link&gt;
 i think this error is on Apex though not native amp.
		</comment>
		<comment id='2' author='sshleifer' date='2020-10-22T02:24:18Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>