<bug id='4265' author='aabrol' open_date='2020-10-20T18:28:50Z' closed_time='2020-12-25T06:18:21Z'>
	<summary>slurm auto re-queue inconsistency</summary>
	<description>
Hi! I submitted a slurm job-array with pytorch lightning functionality. I used the suggested signal (#SBATCH --signal=SIGUSR1@90) and set distributed_backend to 'ddp' in the Trainer call. I did notice successful auto-resubmission this morning whenever my jobs were pre-emptied; however, I now notice that several of them have not completed and are not queued either. Wondering if this has been reported by someone earlier and any clue why this could happen? Is there a maximum to the number of times the jobs would be re-queued or other slurm rules that may prevent requeuing, etc.? My jobs might have been pre-emptied several times as I was running them on the low priority "non-capped" queue so as to occupy maximum number of gpus whenever they become available (already using my quota of high/medium priority queues). Thanks in advance!
	</description>
	<comments>
		<comment id='1' author='aabrol' date='2020-10-20T18:32:10Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='aabrol' date='2020-10-23T04:57:59Z'>
		Another issue I noticed is that re-queuing is not working as expected. Here are details relevant to this issue:
Training doesn't seem to resume from the "checkpoint saved on slurm-preemption" as number of epochs on tensorboard seem to well exceed max_epcohs for the training and there are clear breaks in the epoch plot as well as the performance metric plots (while plotting horizontal axis in relative/wall mode in tensorboard).
As clear from the trainer call in submitted code below, I am saving/logging by the PTL defaults (path/logger). A specific job is always logged to the same (originally created) lightning_logs/version_num directory every time it is preempted, however a separate "events.out.tfevents." file seems to be created each time this job is preemptied. Additionally, lightning_logs/version_num/checkpoints lists several checkpoints [e.g. epoch=37.ckpt  epoch=41-v0.ckpt  epoch=4.ckpt  epoch=9.ckpt].
I was hoping to be able to resume training from checkpoint created on pre-emption, and any points explaining the behavior I observe would be appreciated. Thanks!
Here is the slurm script "slurm_run_train.sh" submitted as "sbatch --array=[0-9] slurm_run_train.sh".
&lt;denchmark-code&gt;#!/bin/bash
#SBATCH -N 1 
#SBATCH -n 1
#SBATCH -p qGPUM
#SBATCH --gres=gpu:1
#SBATCH -c 8
#SBATCH --mem-per-cpu=4000
#SBATCH -t 7200
#SBATCH -J xyz
#SBATCH -e ./slurmlogs/err%A-%a.csv
#SBATCH -o ./slurmlogs/out%A-%a.csv
#SBATCH -A xxyyzz
#SBATCH --oversubscribe 
#SBATCH --mail-type=ALL
#SBATCH --mail-user=xyz@xyz.edu
#SBATCH --signal=SIGUSR1@90

sleep 7s
source activate /home/users/xyz/anaconda3/envs/chumps/
python run_train.py
sleep 7s
&lt;/denchmark-code&gt;

Here is relevant part of run_train.py
&lt;denchmark-code&gt;if __name__ == '__main__':
    rep = int(os.environ['SLURM_ARRAY_TASK_ID'])
    cfg = Config( ... )
    train(cfg)
&lt;/denchmark-code&gt;

Here is the called train method:
&lt;denchmark-code&gt;def train(cfg: Config):
    from xyz import xyznet as Model
    epochs = 100
    model = Model(cfg)
    trainer = Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=epochs, distributed_backend='ddp')
    trainer.fit(model)
    trainer.test() 
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='aabrol' date='2020-12-18T04:09:18Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>