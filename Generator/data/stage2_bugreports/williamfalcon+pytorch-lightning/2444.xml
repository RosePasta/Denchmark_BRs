<bug id='2444' author='NumesSanguis' open_date='2020-07-01T08:42:24Z' closed_time='2020-08-28T07:07:44Z'>
	<summary>self.hparam silently removes params that are not serializable</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Following the approach found under &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html#lightningmodule-hyperparameters&gt;hyperparameters in the docs&lt;/denchmark-link&gt;
, step 3, I passed a dict with parameters to my .
In the  printing  shows all contents I passed.
However, in the function , some hparams are gone.
This might be related to that not all param values are YAML serializable, and therefore automatically removed? Because the 2 removed params are "criterion": torch.nn.BCELoss() and "optimizer": partial(optim.Adam, lr=0.001).
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Run the following script:

from functools import partial
import torch
import torch.optim as optim
from torch.utils.data import Dataset
import pytorch_lightning as pl
from pytorch_lightning import Trainer

# partial to give all params, except the data
hparams = {
    "criterion": torch.nn.BCELoss(),  # F.cross_entropy(),  # loss function
    "optimizer": partial(optim.Adam, lr=0.001),  # (lr=0.001),
    # "learning_rate": 0.001,
    "filters": 64,
    "layers": 2
}

class EmptyDataset(Dataset):
    def __init__(self, transform=None):
        pass
    
    def __len__(self):
        return 32
    
    def __getitem__(self, idx):
        return {"input": np.array([1]), "output": "nothing"}

class LitLake(pl.LightningModule):
    def __init__(self, hparams: dict, transforms: dict = None):
        super().__init__()
        
        self.hparams = hparams
        print("self.hparams\n", self.hparams)
        
    def forward(self, x):
        pass
    
    def training_step(self, batch, batch_idx):
        """
        Lightning calls this inside the training loop with the data from the training dataloader
        passed in as `batch`.
        """
        # forward pass
        x, y = batch
        y_hat = self(x)
        loss = self.hparams["criterion"](y_hat, y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}
        
    def configure_optimizers(self):
        print("self.hparams\n", self.hparams)
        optimizer = self.hparams["optimizer"](self.parameters())
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
        return [optimizer], [scheduler]
    
    def train_dataloader(self):
         return DataLoader(EmptyDataset(), batch_size=4, num_workers=1)

model = LitLake(hparams=hparams)
# most basic trainer, uses good defaults
trainer = Trainer()  # gpus=1, num_nodes=1
trainer.fit(model)  # KeyError: 'optimizer'

See error

Script output (CLICK ME)

self.hparams
 "criterion": BCELoss()
"filters":   64
"layers":    2
"optimizer": functools.partial(&lt;class 'torch.optim.adam.Adam'&gt;, lr=0.001)
GPU available: True, used: False
TPU available: False, using: 0 TPU cores
self.hparams
 "filters": 64
"layers":  2
Traceback (most recent call last):
  File "lightning_hparams_bug.py", line 61, in &lt;module&gt;
    trainer.fit(model)  # KeyError: 'optimizer'
  File "/home/*user*/anaconda3/envs/onseilake/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 965, in fit
    self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)
  File "/home/*user*/anaconda3/envs/onseilake/lib/python3.7/site-packages/pytorch_lightning/trainer/optimizers.py", line 18, in init_optimizers
    optim_conf = model.configure_optimizers()
  File "lightning_hparams_bug.py", line 51, in configure_optimizers
    optimizer = self.hparams["optimizer"](self.parameters())
KeyError: 'optimizer'


&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Either:

self.hparams keeping the non-serializable parameters (will give problems with loading?)
Throw an error explaining why those param values are not acceptable and how to approach it, instead of silently removing them.

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.5.0
OS (e.g., Linux): Ubuntu 18.04
How you installed PyTorch (conda, pip, source): conda
Build command you used (if compiling from source): -
Python version: 3.7.6
CUDA/cuDNN version: 10.2
GPU models and configuration: 1x GeForce GTX 1080 Ti
Any other relevant information: -

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Bug reproduced by &lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='NumesSanguis' date='2020-07-01T19:13:22Z'>
		Happens to me too. I'm also waiting for this fix, since i really want to use the new feature to put anything in hparams.
		</comment>
		<comment id='2' author='NumesSanguis' date='2020-07-02T16:33:19Z'>
		&lt;denchmark-link:https://github.com/dscarmo&gt;@dscarmo&lt;/denchmark-link&gt;
 you can take it over and send a PR :] or I ll check it tomorrow...
		</comment>
		<comment id='3' author='NumesSanguis' date='2020-08-04T19:29:05Z'>
		Need to add warning about this.
		</comment>
		<comment id='4' author='NumesSanguis' date='2020-08-18T23:14:11Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 this should be fixed with PR, right?
		</comment>
		<comment id='5' author='NumesSanguis' date='2020-08-18T23:19:45Z'>
		&lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
 yes the linked PR solves this.
		</comment>
	</comments>
</bug>