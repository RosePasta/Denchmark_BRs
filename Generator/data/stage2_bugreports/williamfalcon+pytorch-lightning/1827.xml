<bug id='1827' author='williamFalcon' open_date='2020-05-14T01:44:01Z' closed_time='2020-09-22T21:18:17Z'>
	<summary>Automatic batch_size finder does not work with 16 bit and in ddp</summary>
	<description>
The recursive .fit() call in this module means it can't be used with ddp interactive.
It also cannot be used with 16-bit since the scaler is not enabled and saving checkpoint fails.
	</description>
	<comments>
		<comment id='1' author='williamFalcon' date='2020-05-14T01:44:29Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='williamFalcon' date='2020-05-14T10:34:07Z'>
		I have trouble reproducing your error. If I run pl_examples/basic_examples/gpu_template.py with --use_16bit --distributed_backend ddp and have set auto_scale_batch_size='power in trainer construction I do not get an error.
		</comment>
		<comment id='3' author='williamFalcon' date='2020-05-14T12:21:29Z'>
		add â€”gpus 2
		</comment>
		<comment id='4' author='williamFalcon' date='2020-06-08T11:37:05Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 is this still relevant?
		</comment>
		<comment id='5' author='williamFalcon' date='2020-09-16T17:12:15Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 could you reproduce the error?
		</comment>
	</comments>
</bug>