<bug id='3016' author='kmistry-wx' open_date='2020-08-17T14:02:11Z' closed_time='2020-09-18T12:23:01Z'>
	<summary>Memory allocated on gpu:0 when using torch.cuda.empty_cache()</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Pytorch lightning calls torch.cuda.empty_cache() at times, e.g. at the end of the training loop. When the trainer is set to run on GPUs other than gpu:0, it still allocates memory on gpu:0 when running torch.cuda.empty_cache(). Apparently this is the initial device context, but it can be avoided. For example,
&lt;denchmark-code&gt;with torch.cuda.device('cuda:1'):
    torch.cuda.empty_cache()
&lt;/denchmark-code&gt;

If the cache is emptied in this way, it will not allocate memory on any other gpu other than the one specified
This seems to be the same issue as in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/458&gt;#458&lt;/denchmark-link&gt;
, but was never resolved and is still an issue.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Create a pl.Trainer with gpus=[1]
Fit a model on gpu:1
torch.cuda.empty_cache() runs in run_training_teardown at the end of the training loop
nvidia-smi shows memory usage on gpu:0
If gpu:0 already had high memory allocation because of another job, then it will throw a CUDA out of memory error

&lt;denchmark-code&gt;.../pytorch_lightning/trainer/training_loop.py in run_training_teardown(self)
   1153             model = self.get_model()
   1154             model.cpu()
-&gt; 1155             torch.cuda.empty_cache()
   1156 
   1157     def training_forward(self, batch, batch_idx, opt_idx, hiddens):

.../torch/cuda/memory.py in empty_cache()
     84     """
     85     if is_initialized():
---&gt; 86         torch._C._cuda_emptyCache()
     87 
     88 

RuntimeError: CUDA error: out of memory
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;trainer = Trainer(gpus=[1])
trainer.fit(task, train_dataloader, val_dataloader)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Only gpu:1 should be used when training this model.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

GeForce RTX 2080 Ti
GeForce GTX 1080 Ti


available:         True
version:           10.1


Packages:

numpy:             1.18.1
pyTorch_debug:     False
pyTorch_version:   1.5.0
pytorch-lightning: 0.9.0rc12
tensorboard:       2.2.1
tqdm:              4.46.0


System:

OS:                Linux
architecture:

64bit
ELF


processor:         x86_64
python:            3.8.3
version:           18.04.1-Ubuntu



	</description>
	<comments>
		<comment id='1' author='kmistry-wx' date='2020-08-17T14:02:49Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='kmistry-wx' date='2020-08-17T17:18:04Z'>
		Mind submitting a PR for this?
		</comment>
		<comment id='3' author='kmistry-wx' date='2020-08-18T07:00:47Z'>
		I remember that there was a bug in pytorch, which always created a cuda context (and thus allocated memory) on the default gpu (which is the first one). Not sure if this is also the issue here though
		</comment>
		<comment id='4' author='kmistry-wx' date='2020-08-19T23:36:41Z'>
		yeah, i think this is a pytorch bug... will investigate after 0.9
		</comment>
		<comment id='5' author='kmistry-wx' date='2020-09-16T17:23:37Z'>
		&lt;denchmark-link:https://github.com/kmistry-wx&gt;@kmistry-wx&lt;/denchmark-link&gt;
 does the issue persist?
		</comment>
		<comment id='6' author='kmistry-wx' date='2020-09-18T08:10:28Z'>
		&lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
 After upgrading (PyTorch 1.5.0 -&gt; 1.6.0, Pytorch Lightning 0.9.0rc12 -&gt; 0.9.0), I can no longer reproduce. A trainer run on gpu:1 seems to only use resources from gpu:1 now
		</comment>
	</comments>
</bug>