<bug id='4414' author='ClaartjeBarkhof' open_date='2020-10-28T16:43:29Z' closed_time='2020-11-17T19:46:26Z'>
	<summary>GPU setting not working, tensors not converted to CUDA: "RuntimeError: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am trying to fit a model GPU. I set:
&lt;denchmark-code&gt;accelerator="dp",
gpus=2
&lt;/denchmark-code&gt;

or
&lt;denchmark-code&gt;accelerator=None,
gpus=1
&lt;/denchmark-code&gt;

I am sure there are GPUs available (4 in total) and it says it uses them:
&lt;denchmark-code&gt;GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
--------------------
ARGPARSE GPU defaults are set to:
n_gpus_default: 2
distributed_backend_default: dp
accelerator_default: dp
log_gpu_memory_default: all
--------------------
number of GPUs available 4
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

Can't reproduce in Colab, since it is a multi-GPU problem.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

When I run my code with GPUs, one or multiple, dataloader returns CPU instead of CUDA tensors.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/ClaartjeBarkhof/code-thesis/blob/master/NewsVAE/NewsVAE.py&gt;My code can be found on GitHub. The main file is NewsVAE.py&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - GeForce GTX 1080 Ti
                - GeForce GTX 1080 Ti
                - GeForce GTX 1080 Ti
                - GeForce GTX 1080 Ti
        - available:         True
        - version:           10.0
* Packages:
        - numpy:             1.19.1
        - pyTorch_debug:     False
        - pyTorch_version:   1.4.0
        - pytorch-lightning: 1.0.4
        - tqdm:              4.49.0
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - 
        - processor:         
        - python:            3.6.12
        - version:           #1 SMP Debian 4.19.152-1 (2020-10-18)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ClaartjeBarkhof' date='2020-10-29T08:01:23Z'>
		The CUDA error I can resolve by adding the following to my DataModule:
&lt;denchmark-code&gt;    def transfer_batch_to_device(self, batch, device):
        print("### DEVICE CHECK", device)
        for k in batch.keys():
            batch[k] = batch[k].to(device)
        return batch
&lt;/denchmark-code&gt;

I am a bit confused, as I thought Pytorch Lightning would handle these things for me.
Now another issue is that Pytorch Lightning seems to ignore my training_step? When I print stuff in there it is not printed, while I run a fast_dev_run and it says it has completed a few training batches:
&lt;denchmark-code&gt;Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/2 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/2 [00:00&lt;?, ?it/s] 
Epoch 0:  50%|#####     | 1/2 [00:00&lt;00:00,  3.89it/s]
Epoch 0:  50%|#####     | 1/2 [00:00&lt;00:00,  3.87it/s, loss=nan, v_num=skim]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='ClaartjeBarkhof' date='2020-10-29T21:41:46Z'>
		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 maybe you can have look?
		</comment>
		<comment id='3' author='ClaartjeBarkhof' date='2020-10-30T10:00:05Z'>
		&lt;denchmark-link:https://github.com/ClaartjeBarkhof&gt;@ClaartjeBarkhof&lt;/denchmark-link&gt;
 I'll try to look into this.
I got a couple of questions here to start with:

I am trying to fit a model GPU. I set:
accelerator="dp",
gpus=2
or
accelerator=None,
gpus=1

Does that mean, you also try single-GPU and it does not work?

Can't reproduce in Colab, since it is a multi-GPU problem.

That's True, but could you maybe try to get a small reproduction script outside colab for me? Unfortunately, I am not that familiar with your code and this a minimal example for reproduction would help a lot.
I am asking, since locally I cannot reproduce your behaviour with my code.

I am a bit confused, as I thought Pytorch Lightning would handle these things for me.
With that you are correct. We usually handle this with very good defaults. This is also the reason, why the reproduction script would be so important for us.

		</comment>
		<comment id='4' author='ClaartjeBarkhof' date='2020-11-17T19:46:26Z'>
		feel free to reopen with an example, so we can try to reproduce!
		</comment>
	</comments>
</bug>