<bug id='461' author='zide05' open_date='2019-11-05T11:11:03Z' closed_time='2019-12-04T12:38:15Z'>
	<summary>error when constructing own model module</summary>
	<description>
&lt;denchmark-h:h3&gt;Common bugs:&lt;/denchmark-h&gt;

Describe the bug
when construct the module "SummarizerModule"  myself, i got error:
Traceback (most recent call last):
File "/remote-home/yrchen/anaconda3/envs/pytorch1.2_p37/lib/python3.7/site-packages/pytorch_lightning/root_module/decorators.py", line 15, in _get_data_loader
value = getattr(self, attr_name)
File "/remote-home/yrchen/anaconda3/envs/pytorch1.2_p37/lib/python3.7/site-packages/torch/nn/modules/module.py", line 591, in getattr
type(self).name, name))
AttributeError: 'SummarizerModule' object has no attribute '_lazy_val_dataloader'
the 'SummarizerModule'  is defined as :
class SummarizerModule(pl.LightningModule):
"""PyTorch Lightning system.
"""
&lt;denchmark-code&gt;def __init__(self, summarizer, token_indexer,
             loaders_params: dict, optimizer_params: dict, args):
    super().__init__()
    self.summarizer = summarizer
    self._optimizer_params = optimizer_params
    self._loaders_params = loaders_params
    self._token_indexer = token_indexer
    self.args = args

def forward(self, article, article_length,
            prev_input, prev_input_length):
    return self.summarizer(article, article_length,
                           prev_input, prev_input_length)

def training_step(self, batch, batch_nb):
    article_data, prev_input_data, gold_summary = batch
    article, article_length = article_data
    prev_input, prev_input_length = prev_input_data

    infered_summary = self.forward(article, article_length,
                                   prev_input, prev_input_length)
    return {'loss': sequence_nll(infered_summary, gold_summary)}

def validation_step(self, batch, batch_nb):
    return self.training_step(batch, batch_nb)

def validation_end(self, outputs):
    # OPTIONAL
    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
    return {'avg_val_loss': avg_loss}

def configure_optimizers(self):
    return torch.optim.Adam(self.summarizer.parameters(),
                            **self._optimizer_params)

@pl.data_loader
def train_dataloader(self):
    train_set = DCADataset(split="train", n_agents=3,
                           token_indexer=self._token_indexer, args=self.args)
    return DataLoader(train_set, **self._loaders_params, shuffle=True)

@pl.data_loader
def val_dataloader(self):
    val_set = DCADataset(split="val", n_agents=3,
                         token_indexer=self._token_indexer, args=self.args)
    return DataLoader(val_set, **self._loaders_params, shuffle=False)
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='zide05' date='2019-12-04T12:38:15Z'>
		&lt;denchmark-link:https://github.com/zide05&gt;@zide05&lt;/denchmark-link&gt;
 that error means you likely have a bug in your dataloaders. If this is still an issue I'll reopen.
		</comment>
	</comments>
</bug>