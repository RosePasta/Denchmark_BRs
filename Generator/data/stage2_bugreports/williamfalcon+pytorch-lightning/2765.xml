<bug id='2765' author='edenlightning' open_date='2020-07-30T14:55:50Z' closed_time='2020-08-16T15:19:58Z'>
	<summary>Add a test case for running trainer.test without trainer.fit on DDP</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Running trainer.test(model) using DDp without running trainer.fit hangs.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import pytorch_lightning as pl
import torch
from torch.utils.data import DataLoader, Dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples=100, dim=5):
        self.num_samples = num_samples
        self.dim = dim
    def __len__(self):
        return self.num_samples
    def __getitem__(self, item):
        x = torch.rand(self.dim)
        y = x.sum()
        return x, y
class Model(pl.LightningModule):
    def __init__(self):
        super(Model, self).__init__()
        self.layer = torch.nn.Linear(5, 1)
    def forward(self, x):
        y = self.layer(x)
        return y
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=0.1)
        return optimizer
    def train_dataloader(self):
        return DataLoader(
            dataset=RandomDataset(num_samples=100, dim=5),
            batch_size=32,
        )
    def test_dataloader(self):
        return DataLoader(
            dataset=RandomDataset(num_samples=64, dim=5),
            batch_size=8
        )
    def training_step(self, batch, batch_idx, optimizer_idx=0):
        x, y = batch
        x = x.view(-1, 5)
        y = y.view(-1, 1)
        y_dash = self(x)
        loss = ((y - y_dash) ** 2).sum()
        return {'loss': loss, 'log': {'train_loss': loss / x.size(0)}}
    def test_step(self, batch, batch_idx, dataloader_idx=0):
        return self.training_step(batch, batch_idx)
    def test_epoch_end(self, outputs):
        loss = torch.stack([log['loss'] for log in outputs]).mean()
        return {'test_loss': loss}
if __name__ == '__main__':
    model = Model()
    trainer = pl.Trainer(
        max_steps=20,
        amp_level='O1',
        gpus=2,
        precision=16,
        distributed_backend='ddp'
    )
    # comment below / remove comment below
    # trainer.fit(model)
    trainer.test(model)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Should be able to run test with DDP.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.5
PL: 0.8.5

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='edenlightning' date='2020-07-31T01:11:06Z'>
		I am experiencing the same issue with my program.
		</comment>
		<comment id='2' author='edenlightning' date='2020-07-31T02:56:33Z'>
		Same problem here. I cannot use trainer.test in ddp mode without running trainer.fit first.
		</comment>
	</comments>
</bug>