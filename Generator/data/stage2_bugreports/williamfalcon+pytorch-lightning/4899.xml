<bug id='4899' author='emilemathieu' open_date='2020-11-29T21:36:31Z' closed_time='2020-11-30T21:23:43Z'>
	<summary>[LoggerCollection.log_hyperparams] [no optional argument metrics]</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hello ! :)
The following method &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/1.0.8/pytorch_lightning/loggers/tensorboard.py#L149&gt;TensorBoardLogger.log_hyperparams&lt;/denchmark-link&gt;
 has an optional argument  yet &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/1.0.8/pytorch_lightning/loggers/base.py#L353&gt;LoggerCollection.log_hyperparams&lt;/denchmark-link&gt;
 doesn't, yielding an error when relying on several loggers including  and using this optional argument.
Is that behaviour expected? Shouldn't all loggers have this optional argument?
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

logger1 = pl.loggers.TensorBoardLogger(save_dir="logs")
logger2 = pl.loggers.CSVLogger(save_dir="logs")
pl.Trainer(logger=[logger1, logger2], etc)
Yields TypeError: log_hyperparams() got an unexpected keyword argument 'metrics'.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:
available:         False
version:           10.2


Packages:

numpy:             1.19.4
pyTorch_debug:     True
pyTorch_version:   1.7.0
pytorch-lightning: 1.0.6
tqdm:              4.53.0


System:

OS:                Linux
architecture:

64bit
ELF


processor:         x86_64
python:            3.8.6
version:           #1 SMP Tue Aug 11 16:36:14 UTC 2020



	</description>
	<comments>
		<comment id='1' author='emilemathieu' date='2020-11-30T08:57:24Z'>
		&lt;denchmark-link:https://github.com/emilemathieu&gt;@emilemathieu&lt;/denchmark-link&gt;
 I definitly see the problem, but does it happen while calling  or do you call  yourself?
		</comment>
		<comment id='2' author='emilemathieu' date='2020-11-30T18:54:41Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 I have
def test_epoch_end(self, outputs):
    self.logger.log_hyperparams(self.hparams, metrics={'hp_metric': self.hp_metric})
to get the hyperparameters in tensorboard, which get called with result = trainer.test(datamodule=data_module).
		</comment>
		<comment id='3' author='emilemathieu' date='2020-11-30T21:05:47Z'>
		&lt;denchmark-link:https://github.com/emilemathieu&gt;@emilemathieu&lt;/denchmark-link&gt;
 so tensorboard is the odd one here as it is the only logger that has the extra  argument. Therefore, instead of changing the remaining loggers to comply with tensorboard, I would advice that you just call that logger directly:

Assuming here that tensorboard is the first logger you initialize with.
		</comment>
		<comment id='4' author='emilemathieu' date='2020-11-30T21:23:43Z'>
		Thanks &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
! You're right it makes more sense to do so :)
I'm closing the issue.
		</comment>
	</comments>
</bug>