<bug id='5224' author='PiotrDabkowski' open_date='2020-12-21T21:10:58Z' closed_time='2021-01-08T21:13:13Z'>
	<summary>enable_pl_optimizer causes optimizers to not be restored properly</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

enable_pl_optimizer (default!) causes optimizers to not be restored properly from the checkpoint specified by resume_from_checkpoint.
&lt;denchmark-h:h2&gt;BoringModel Colab Reproduction&lt;/denchmark-h&gt;

The model is trained for 3 epochs and saved in a checkpoint. The checkpoint is then restored and further trained for 1 epoch (with different values of enable_pl_optimizer), the training loss is printed at each step.
The setup where enable_pl_optimizer=True shows a huge loss spike after the first optimizer step, suggesting that the optimizer is not restored properly.
&lt;denchmark-link:https://colab.research.google.com/drive/1lHYXm4MpnmXwPZTcPem4D4wwwU5vJhHc?usp=sharing&gt;https://colab.research.google.com/drive/1lHYXm4MpnmXwPZTcPem4D4wwwU5vJhHc?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

PL Optimizers are restored such that there is no huge loss spike after restore, just like when enable_pl_optimizer=False.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

See Colab.
	</description>
	<comments>
		<comment id='1' author='PiotrDabkowski' date='2020-12-21T21:11:36Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='PiotrDabkowski' date='2020-12-21T22:56:30Z'>
		Hi, I think you can check &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4655&gt;#4655&lt;/denchmark-link&gt;
. I think these problems are related. &lt;denchmark-link:https://github.com/PiotrDabkowski&gt;@PiotrDabkowski&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='PiotrDabkowski' date='2020-12-21T23:23:16Z'>
		The loaded optimiser is restored as the initial one.
		</comment>
		<comment id='4' author='PiotrDabkowski' date='2020-12-21T23:57:32Z'>
		Possibly, but not sure. Does disabling pl_optimizer fix &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4655&gt;#4655&lt;/denchmark-link&gt;
 as well?
		</comment>
		<comment id='5' author='PiotrDabkowski' date='2020-12-22T08:16:28Z'>
		Hey &lt;denchmark-link:https://github.com/PiotrDabkowski&gt;@PiotrDabkowski&lt;/denchmark-link&gt;
,
enable_pl_optimizer has been reset to False by default.
We will look into this bug.
Thanks,
T.C
		</comment>
		<comment id='6' author='PiotrDabkowski' date='2020-12-22T10:07:59Z'>
		I didn't try to disable enable_pl_optimizer, &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 is there anything I should take care of when enable_pl_optimizer=False? I use ddp to train on Slurm cluster.
		</comment>
		<comment id='7' author='PiotrDabkowski' date='2020-12-23T08:51:26Z'>
		Hey @hyw1994,
I checked your notebook ! You are entirely right.
I will look into today as it is high priority to re-enable LightningOptimizer :)
Best regards,
T.C
		</comment>
	</comments>
</bug>