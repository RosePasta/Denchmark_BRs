<bug id='5539' author='kapsner' open_date='2021-01-16T08:38:35Z' closed_time='2021-01-18T17:03:42Z'>
	<summary>Why are losses different when logging from '_step' (with on_epoch=True) compared to logging from '_epoch_end'?</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When logging losses from {prefix}_step with self.log("{prefix}_loss", loss, on_step=False, on_epoch=True), they are different from losses logged in {prefix}_epoch_end, using
avg_loss = torch.stack([x["loss"] for x in outputs]).mean()
self.log(
      name="loss/" + prefix,
      value=avg_loss,
      prog_bar=False,
      logger=True,
      on_step=False,
      on_epoch=True
  )
&lt;denchmark-link:https://user-images.githubusercontent.com/44973495/104807213-87a41b00-57dd-11eb-8a2b-286f8e398319.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1Sz9kgGuMWxcAPOZ7SPcm4XYhsoWgwOFt?usp=sharing&gt;https://colab.research.google.com/drive/1Sz9kgGuMWxcAPOZ7SPcm4XYhsoWgwOFt?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run the code from the link (I copied it into a .py-file and ran it from commandline) and see csv-file with logged losses (under ./default/version_{int}/metrics.csv).
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

{prefix}_loss (logged from {prefix}_step) and loss/{prpefix} (logged from {prefix}_epoch_end) should be equal.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- available:         False
- version:           10.2
Packages:
- numpy:             1.19.5
- pyTorch_debug:     False
- pyTorch_version:   1.7.1
- pytorch-lightning: 1.1.4
- tqdm:              4.56.0
System:
- OS:                Linux
- architecture:
- 64bit
- ELF
- processor:         x86_64
- python:            3.8.5
- version:           #1 SMP Tue Jun 23 12:58:10 UTC 2020

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

The differences in losses seem marginally using this boring model, however, to me it is unclear, why this happens.
From my understanding, there should be no differences at all when logging losses from steps with on_epoch=True or from epoch_ends.
	</description>
	<comments>
		<comment id='1' author='kapsner' date='2021-01-16T08:39:17Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='kapsner' date='2021-01-16T11:18:14Z'>
		check the answer here: &lt;denchmark-link:https://forums.pytorchlightning.ai/t/understanding-different-values-of-training-validation-loss-in-callback-metrics-dictionary/568/2?u=goku&gt;https://forums.pytorchlightning.ai/t/understanding-different-values-of-training-validation-loss-in-callback-metrics-dictionary/568/2?u=goku&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='kapsner' date='2021-01-16T12:14:44Z'>
		Thanks for sharing this post (and sorry for missing it beforehand).
Do I understand it correctly, that the loss calculated by weighted mean from steps (train_loss_epoch from the forum link) is 'more trustworthy' than the unweighted avg_loss from epoch_ends?
		</comment>
		<comment id='4' author='kapsner' date='2021-01-16T12:17:54Z'>
		ideally yes.. let's say batch_sizes are [64, 64, 64, 7]... then doing an average isn't actually correct.
		</comment>
		<comment id='5' author='kapsner' date='2021-01-16T15:51:40Z'>
		I have now tried to reproduce the weighted averaging in {prefix}_epoch_ends, but I am still not able to reproduce the numbers that are logged from steps.
The boring model above was updated as follows to calculate weighted means on epoch ends:
def _shared_epoch_end(self, outputs, prefix):
    # concat batch sizes
    batch_sizes = torch.stack([torch.tensor(
        x["batch_size"],
        dtype=torch.int16
    ) for x in outputs])

    # concat losses
    losses = torch.stack([x["loss"] for x in outputs])

    # calculating weighted average loss
    avg_loss = torch.sum(losses * batch_sizes) / torch.sum(batch_sizes)

    self.log(
        name="loss/" + prefix,
        value=avg_loss,
        prog_bar=False,
        logger=True,
        on_step=False,
        on_epoch=True
    )

def training_step(self, batch, batch_idx):
    output = self.layer(batch)
    loss = self.loss(batch, output)
    self.log(
        name="train_loss",
        value=loss,
        prog_bar=False,
        logger=True,
        on_step=False,
        on_epoch=True
    )
    return {"loss": loss, "batch_size": batch.shape[0]}

def training_step_end(self, training_step_outputs):
    return training_step_outputs

def training_epoch_end(self, outputs) -&gt; None:
    self._shared_epoch_end(outputs, "train")

def validation_step(self, batch, batch_idx):
    output = self.layer(batch)
    loss = self.loss(batch, output)
    self.log(
        name="valid_loss",
        value=loss,
        prog_bar=False,
        logger=True,
        on_step=False,
        on_epoch=True
    )
    return {"loss": loss, "batch_size": batch.shape[0]}

def validation_epoch_end(self, outputs) -&gt; None:
    self._shared_epoch_end(outputs, "valid")

def test_step(self, batch, batch_idx):
    output = self.layer(batch)
    loss = self.loss(batch, output)
    return {"loss": loss, "batch_size": batch.shape[0]}

def test_epoch_end(self, outputs) -&gt; None:
    self._shared_epoch_end(outputs, "test")
However, losses calculated from {prefix}_step and {prefix}_epoch_end still differ slightly:
&lt;denchmark-link:https://user-images.githubusercontent.com/44973495/104816373-0c615a00-581b-11eb-8026-060224108f6f.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='kapsner' date='2021-01-16T18:02:52Z'>
		&lt;denchmark-link:https://github.com/kapsner&gt;@kapsner&lt;/denchmark-link&gt;
 : Is there any  which may cause this subtle differences? just a thought.
		</comment>
		<comment id='7' author='kapsner' date='2021-01-16T18:09:23Z'>
		Well, in the boring model, seed_everything and deterministic=TRUE is set, so I assume, there should be no randomness. Maybe I am overseeing something
		</comment>
		<comment id='8' author='kapsner' date='2021-01-17T11:08:33Z'>
		&lt;denchmark-link:https://github.com/kapsner&gt;@kapsner&lt;/denchmark-link&gt;
 mind share a reproducible colab notebook with updated code? will check.
		</comment>
		<comment id='9' author='kapsner' date='2021-01-17T11:57:36Z'>
		&lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 Thanks in advance, I have updated the &lt;denchmark-link:https://colab.research.google.com/drive/1Sz9kgGuMWxcAPOZ7SPcm4XYhsoWgwOFt?usp=sharing&gt;notebook&lt;/denchmark-link&gt;
 accordingly.
		</comment>
		<comment id='10' author='kapsner' date='2021-01-17T18:05:55Z'>
		couldn't find the solution but after experimenting I found that the issue is with torch.dot here:



pytorch-lightning/pytorch_lightning/core/step_result.py


         Line 1077
      in
      6926b84






 numerator = torch.dot(result.float(), weights.transpose(-1, 0).float()) 





if you do  which is equivalent to  here, then it's giving the correct result up to last decimal, but with  there is some issue. Tried &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/7064#issuecomment-385161862&gt;this solution&lt;/denchmark-link&gt;
 too but it's not working for me.
		</comment>
		<comment id='11' author='kapsner' date='2021-01-18T04:51:06Z'>
		I suppose torch.dot flattens both tensors before doing the product.
Is there something left to fix or was it resolved now by reproducing the weighted average?
		</comment>
		<comment id='12' author='kapsner' date='2021-01-18T08:53:10Z'>
		Hey &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
,
Batch_size is attached there: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py#L341&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py#L341&lt;/denchmark-link&gt;

I don't remember adding a test for weighted average. We should add a test to make sure it works fine.
Best,
T.C
		</comment>
		<comment id='13' author='kapsner' date='2021-01-18T09:16:03Z'>
		there is no problem with batch_size here... the problem is with dot product torch.dot as I explained above.. most of the time it's correct but sometimes the results are not correct upto last precision. For eg. if your batch_size=1 then torch.dot(losses, batches) == losses.sum() (should be)... but sometimes results with torch.dot are like 12345.0 which actually should be 12345.25000(torch.sum)... which leads to these minor differences in the final results.
		</comment>
		<comment id='14' author='kapsner' date='2021-01-18T17:02:58Z'>
		Hey &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
,
What I meant is: We should add a test for make sure we properly reduced with weighted_mean independently of torch.dot.
Best,
T.C
		</comment>
		<comment id='15' author='kapsner' date='2021-01-18T17:32:43Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 even if we do that with some random tensors maybe, then still we can't ensure that it will pass all the time because it just happens very rarely. Maybe we should not use  at all in  function? WDYT?
		</comment>
	</comments>
</bug>