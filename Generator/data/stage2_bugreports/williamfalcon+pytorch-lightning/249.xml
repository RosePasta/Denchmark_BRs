<bug id='249' author='awaelchli' open_date='2019-09-25T13:11:17Z' closed_time='2019-09-26T17:20:56Z'>
	<summary>UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars</summary>
	<description>
Describe the bug
Not sure if this is a bug. It shows me this warning at the beginning of training:
/home/adrian/research/envs/research/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
To Reproduce
The minimal MNIST example from the docs has this problem when trained on multiple GPUs. Attached the Python script:
&lt;denchmark-code&gt;import os
import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
import torchvision.transforms as transforms

import pytorch_lightning as pl


class CoolModel(pl.LightningModule):

    def __init__(self):
        super(CoolModel, self).__init__()
        # not the best model...
        self.l1 = torch.nn.Linear(28 * 28, 10)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_nb):
        # REQUIRED
        x, y = batch
        y_hat = self.forward(x)
        return {'loss': F.cross_entropy(y_hat, y)}

    def validation_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self.forward(x)
        return {'val_loss': F.cross_entropy(y_hat, y)}

    def validation_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        return {'avg_val_loss': avg_loss}

    def test_step(self, batch, batch_nb):
        # OPTIONAL
        x, y = batch
        y_hat = self.forward(x)
        return {'test_loss': F.cross_entropy(y_hat, y)}

    def test_end(self, outputs):
        # OPTIONAL
        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
        return {'avg_test_loss': avg_loss}

    def configure_optimizers(self):
        # REQUIRED
        return [torch.optim.Adam(self.parameters(), lr=0.02)]

    @pl.data_loader
    def tng_dataloader(self):
        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)

    @pl.data_loader
    def val_dataloader(self):
        # OPTIONAL
        # can also return a list of val dataloaders
        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)

    @pl.data_loader
    def test_dataloader(self):
        # OPTIONAL
        # can also return a list of test dataloaders
        return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)


if __name__ == '__main__':
    from pytorch_lightning import Trainer

    trainer = Trainer(
        gpus=[2, 3],
        distributed_backend='dp',
    )

    model = CoolModel()
    trainer.fit(model)


&lt;/denchmark-code&gt;

Expected behavior
There are no scalars involved in the forward pass, so the warning does not make sense and should not be shown.
Desktop (please complete the following information):

OS: Ubuntu 18.04
Version: the latest pip install

	</description>
	<comments>
		<comment id='1' author='awaelchli' date='2019-10-14T10:30:30Z'>
		I'm using the newest pip version 0.5.2.1 and I'm still getting this warning (0.5.1.3 also), do I have to toggle it somehow?
		</comment>
		<comment id='2' author='awaelchli' date='2020-05-02T13:16:12Z'>
		It happens also to me.
		</comment>
		<comment id='3' author='awaelchli' date='2020-05-02T13:17:26Z'>
		update  to 0.7.5
		</comment>
		<comment id='4' author='awaelchli' date='2020-05-02T13:22:38Z'>
		I'm currently on cutting edge.
When manually adding. Unsqueeze to the loss it vanishes of course
		</comment>
		<comment id='5' author='awaelchli' date='2020-05-02T13:29:05Z'>
		weird, can you post a colab with a minimal example? we added auto squeezing in dp for this. (i assume this is dp).
but on a much greater note, use DDP instead of DP as DP use is discouraged in general (by pytorch and us)
		</comment>
	</comments>
</bug>