<bug id='5136' author='indigoviolet' open_date='2020-12-14T19:23:34Z' closed_time='2021-01-05T02:54:50Z'>
	<summary>fast_dev_run should set max_steps</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Trainer(fast_dev_run=True) should set max_steps accordingly, but max_steps is None
&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1PFoozULca2zFtw0Ljor5V8AAGsGqe1Y4#scrollTo=quj4LUDgmFvj&gt;https://colab.research.google.com/drive/1PFoozULca2zFtw0Ljor5V8AAGsGqe1Y4#scrollTo=quj4LUDgmFvj&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

max_steps should be set to 1 if fast_dev_run is True else fast_dev_run
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Note: Bugs with code are solved faster ! Colab Notebook should be made public !


IDE: Please, use our python bug_report_model.py template.


Colab Notebook: Please copy and paste the output from our environment collection script (or fill out the checklist below manually).


You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     True
pyTorch_version:   1.7.0+cu101
pytorch-lightning: 1.1.0
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='indigoviolet' date='2020-12-14T19:24:24Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='indigoviolet' date='2020-12-15T21:40:21Z'>
		
max_steps should be set to 1 if fast_dev_run is True else fast_dev_run

Do you mean?
max_steps should be set to 1 if fast_dev_run is True else max_steps
also val_check_interval, check_val_every_n_epoch be set to 1.0 and 1 respectively as well??
in the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#fast-dev-run&gt;video here&lt;/denchmark-link&gt;
 it says,  won't create any logs or save checkpoints but unfortunately it does both. Not sure what's the intended behavior.
		</comment>
		<comment id='3' author='indigoviolet' date='2020-12-16T04:23:09Z'>
		
fast_dev_run won't create any logs or save checkpoints but unfortunately it does both

I noticed it too, it's a bug (I tried to explain here &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4629&gt;#4629&lt;/denchmark-link&gt;
). it should really just be a test that the training loop runs. logging is still executed to some extend but it should not get sent to the actual logger object, no files should be saved. The motivation is to be able to debug the script without polluting the working dir with tons of files :)
		</comment>
		<comment id='4' author='indigoviolet' date='2020-12-16T21:42:13Z'>
		yeah, I believe a simple fix would be to disable loggers, checkpoint callback, earlystopping, ... maybe more in the init itself.
		</comment>
		<comment id='5' author='indigoviolet' date='2020-12-16T21:53:06Z'>
		Will that fix the original bug filed here - ie. max_steps not being set, which can break the optimizers/schedulers that refer to it?
		</comment>
		<comment id='6' author='indigoviolet' date='2020-12-17T15:09:03Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 ^^
		</comment>
		<comment id='7' author='indigoviolet' date='2020-12-17T15:12:26Z'>
		
which can break the optimizers/schedulers that refer to it?

&lt;denchmark-link:https://github.com/indigoviolet&gt;@indigoviolet&lt;/denchmark-link&gt;
 mind explain this a bit more, what do you mean here?
		</comment>
		<comment id='8' author='indigoviolet' date='2020-12-17T19:14:14Z'>
		I meant that the lightning module might need to refer to self.trainer.max_steps to configure its optimizers/schedulers, and this breaks in fast_dev_run.
		</comment>
		<comment id='9' author='indigoviolet' date='2020-12-17T19:20:57Z'>
		yeah then max_steps should be set correctly too in such a case. But fast_dev_run is meant to debug, I don't think anyone should set it to a value &gt; max_steps (even if someone does, it will stop at max_steps only) so scheduler might get a higher number of iterations &gt; fast_dev_run, but it won't break in such a case too.
		</comment>
	</comments>
</bug>