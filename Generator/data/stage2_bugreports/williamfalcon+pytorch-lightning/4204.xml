<bug id='4204' author='denadai2' open_date='2020-10-17T15:57:03Z' closed_time='2020-11-13T12:04:42Z'>
	<summary>loss=None and no logs when automatic_optimization=False</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I think there is a bug when automatic_optimization=False. The loss=None (


pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 336
      in
      72f1976






 loss=untouched_loss, 




) and this means that all the checkpoint_callbacks cannot work. There is no way to set the loss.
I also add that in the documentation (&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/optimizers.html#manual-optimization&gt;https://pytorch-lightning.readthedocs.io/en/latest/optimizers.html#manual-optimization&lt;/denchmark-link&gt;
) the training_step does not return anything. However, if it does not return anything, all the logs do not work because of: 
.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

There should be a way to set the loss, and the behaviour when nothing is returned in training_step should be clear.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - GeForce RTX 2080 Ti
                - GeForce RTX 2080 Ti
        - available:         True
        - version:           10.2
* Packages:
        - numpy:             1.19.1
        - pyTorch_debug:     False
        - pyTorch_version:   1.6.0
        - pytorch-lightning: 1.0.2
        - tqdm:              4.48.2
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - ELF
        - processor:         x86_64
        - python:            3.6.9
        - version:           #26-Ubuntu SMP Mon Jun 24 09:32:08 UTC 2019
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='denadai2' date='2020-10-17T15:57:44Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='denadai2' date='2020-10-22T15:33:23Z'>
		alsp &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4295&gt;#4295&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='denadai2' date='2020-11-01T23:39:51Z'>
		Thanks &lt;denchmark-link:https://github.com/denadai2&gt;@denadai2&lt;/denchmark-link&gt;
! I'll modify the doc example to report loss value, if you care about logging your loss values (which in most cases is yes!)
		</comment>
		<comment id='4' author='denadai2' date='2020-11-02T11:11:20Z'>
		&lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 actually, it's not only about the doc. It's also that if the loss is nan, pytorch lightning skips to write ALL the logged variables because of: 

		</comment>
		<comment id='5' author='denadai2' date='2020-11-02T11:12:10Z'>
		To be more clear, this step is skipped: 


pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 687
      in
      72f1976






 self.log_training_step_metrics(opt_closure_result, batch_callback_metrics, batch_log_metrics) 





		</comment>
		<comment id='6' author='denadai2' date='2020-11-02T12:04:55Z'>
		Thanks &lt;denchmark-link:https://github.com/denadai2&gt;@denadai2&lt;/denchmark-link&gt;
 just to confirm the logic is as below:
You've overridden the training step and set automatic_optimization to false.
Your training_step function logs metrics now using self.log, but never returns a loss (you may have multiple losses or something)
You would like other metrics aside from the loss to be logged in training, but because of this line:



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 721
      in
      f40d086






 if self._curr_step_result is None: 





We never get to this part of the code:



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 725 to 730
      in
      f40d086






 batch_outputs = self._process_closure_result( 



 batch_callback_metrics=batch_callback_metrics, 



 batch_log_metrics=batch_log_metrics, 



 batch_outputs=batch_outputs, 



 opt_idx=opt_idx, 



 ) 





I think custom logged metrics in callbacks is the only thing that will not be logged for now (there is a major refactor coming which should fix this &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4439&gt;#4439&lt;/denchmark-link&gt;
 ), and for now you'll need to log using  within the lightning module functions. Let me know if this is a solution that solves your issue!
		</comment>
		<comment id='7' author='denadai2' date='2020-11-03T18:08:02Z'>
		Currently blocked on &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4495&gt;#4495&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='denadai2' date='2020-11-04T14:41:48Z'>
		Hi
I'm not sure if my problem is related to this bug it seems so.
because I wanted to do backward and step in the middle of training_step so I set the automatic_optimization to False. I'm logging in the training_step and validation_step as follows:
&lt;denchmark-code&gt;def training_step(self, batch, batch_idx):
    ...
   self.log('train_loss', loss.view(1,).item(), prog_bar=True)
   self.log('train_ci', train_cindex, prog_bar=True)
   return loss

def validation_step(self, batch, batch_idx):
   ...
   self.log('val_loss', eval_loss.view(1,).item(), prog_bar=True)
   self.log('val_ci'  , eval_cindex, prog_bar=True)
   return eval_loss
&lt;/denchmark-code&gt;

It works quite well until it reaches a specific epoch and throws this error (the batch size and the dataset size are the same):
&lt;denchmark-code&gt;Epoch 49:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                  | 1/2 [00:00&lt;00:00,  2.34it/s, loss=nan, v_num=54, train_loss=3.51, train_ci=0.938, val_loss=3.58, val_ci=0.689]Traceback (most recent call last):
  File "trainer_main.py", line 45, in &lt;module&gt;
    trainer.fit(model, tcga_dm)
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 440, in fit
    results = self.accelerator_backend.train()
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 54, in train
    results = self.train_or_test()
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 68, in train_or_test
    results = self.trainer.train()
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 485, in train
    self.train_loop.run_training_epoch()
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 565, in run_training_epoch
    self.trainer.logger_connector.log_train_step_metrics(batch_output)
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py", line 536, in log_train_step_metrics
    self.log_metrics(metrics, grad_norm_dic)
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py", line 79, in log_metrics
    metrics.update(grad_norm_dic)
TypeError: 'NoneType' object is not iterable
Exception ignored in: &lt;function tqdm.__del__ at 0x7f34fe50eca0&gt;
Traceback (most recent call last):
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/tqdm/std.py", line 1122, in __del__
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/tqdm/std.py", line 1335, in close
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/tqdm/std.py", line 1514, in display
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/tqdm/std.py", line 1125, in __repr__
  File "/home/asalimi/.conda/envs/mytorch/lib/python3.8/site-packages/tqdm/std.py", line 1475, in format_dict
TypeError: cannot unpack non-iterable NoneType object
&lt;/denchmark-code&gt;

I checked the loss and eval_loss to not be None and they are not None. it seems the error happens right after the return loss.
also I think a documentation is needed to specify how to log and return values in training_step and validation_step. this was a bit confusing for me.
		</comment>
		<comment id='9' author='denadai2' date='2020-11-04T20:41:51Z'>
		after some debugging I solved the Error by changing the training_step to this:
&lt;denchmark-code&gt;def training_step(self, batch, batch_idx):
    ...
   self.log('loss', loss.view(1,).item(), prog_bar=True, logger=True)
   self.log('train_ci', train_cindex, prog_bar=True, logger=True)
&lt;/denchmark-code&gt;

However now although I set the logger to True, the  and  aren't logged in the tensorboard. and also I cannot see them in the progress bar.
&lt;denchmark-link:https://github.com/SeanNaren&gt;@SeanNaren&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4439&gt;#4439&lt;/denchmark-link&gt;
 didn't solve this problem
		</comment>
		<comment id='10' author='denadai2' date='2020-11-07T16:33:56Z'>
		&lt;denchmark-link:https://github.com/asalimih&gt;@asalimih&lt;/denchmark-link&gt;
 try returning the loss at the end of training_step. This temporanly solves the bug I pointed out.
		</comment>
		<comment id='11' author='denadai2' date='2020-11-07T20:24:21Z'>
		
@asalimih try returning the loss at the end of training_step. This temporanly solves the bug I pointed out.

&lt;denchmark-link:https://github.com/denadai2&gt;@denadai2&lt;/denchmark-link&gt;
 the first error I explained &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4204#issuecomment-721771876&gt;here&lt;/denchmark-link&gt;
 had the  line. after I removed the line it didn't throw error but now metrics wouldn't be logged.
		</comment>
		<comment id='12' author='denadai2' date='2020-11-09T17:34:02Z'>
		We're currently in a deep dive into automatic_optimization=False behaviour after a lot of different bugs have appeared in different edge case situations. Please have a look at &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4485&gt;#4485&lt;/denchmark-link&gt;

The logging changes will hopefully make logs a little clearer, but in terms of actual functionality for automatic_optimization we're in the process of debugging.
&lt;denchmark-link:https://github.com/asalimih&gt;@asalimih&lt;/denchmark-link&gt;
 could you reproduce the bug with this? &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/denadai2&gt;@denadai2&lt;/denchmark-link&gt;
 just finishing the final logging refactor here: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4552&gt;#4552&lt;/denchmark-link&gt;

Once we figure out some of the functionality issues with automatic_optimization, i'll circle back here.
		</comment>
		<comment id='13' author='denadai2' date='2020-11-09T17:38:02Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>