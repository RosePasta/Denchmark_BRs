<bug id='3756' author='chrismaliszewski' open_date='2020-09-30T23:22:46Z' closed_time='2020-10-02T06:37:58Z'>
	<summary>type object got multiple values for keyword argument 'loss'</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The error appears when TrainReport has minimize param set and loss log added at the same time with prog_bar=True
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;def training_step(self, batch, batch_idx):
        loss = self(batch)
        result = pl.TrainResult(minimize=loss)
        result.log("loss", loss, prog_bar=True)

        return result
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Where the problem is&lt;/denchmark-h&gt;

I followed the code and it comes to the problem with the ProgressBar callback inside progress.py line 339 -&gt; trainer.py line 884 (return dict(**ref_model.get_progress_bar_dict(), **self.progress_bar_metrics)) which returns
&lt;denchmark-code&gt;ref_model.get_progress_bar_dict()
Out[4]: {'loss': '0.692', 'v_num': 9}
self.progress_bar_metrics
Out[5]: {'loss': 0.6924866437911987}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Not sure. At least the error message should be a bit clearer since a user does not create two loss logs but just one.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:
available:         False
version:           None


Packages:

numpy:             1.19.1
pyTorch_debug:     False
pyTorch_version:   1.6.0
pytorch-lightning: 0.9.0
tqdm:              4.49.0


System:

OS:                Windows
architecture:

64bit
WindowsPE


processor:         Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
python:            3.8.5
version:           10.0.18362



	</description>
	<comments>
		<comment id='1' author='chrismaliszewski' date='2020-10-01T09:02:59Z'>
		I believe TrainResult always shows the value passed to the minimize argument in the progress bar. So, you probably don't need to use result.log to show the loss in the progress bar.
		</comment>
		<comment id='2' author='chrismaliszewski' date='2020-10-01T20:39:09Z'>
		That's true but I think the problem is the error's message.
As I said, a user may not realise why the error appears since they created just one log loss nor that it comes from prog_bar=True. Just saying.
The only idea that I have would be to remove showing minimize loss in progress bar by default and let users set loss log to be shown with prog_bar. That would eliminate the problem. Don't know that's something others would want though.
		</comment>
		<comment id='3' author='chrismaliszewski' date='2020-10-02T04:16:39Z'>
		Thank you for your feedback.
Lightning believes almost all of users may want to show the training loss in the progress bar to be able to check/iterate the model quickly (like in the situation of overfitting the small batch) without the need to use loggers or something extra, so it is done by default and let the users use result.log or self.log for the other metrics or the various losses to show in the progress bar or in the loggers by epoch or step level.
NOTE: self.log is feature from the master branch.
Docs:

https://pytorch-lightning.readthedocs.io/en/latest/new-project.html#logging
https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#logging-methods

		</comment>
		<comment id='4' author='chrismaliszewski' date='2020-10-02T06:37:58Z'>
		Got it. I'll close the issue. It'll be for other's people reference if they have the same problem.
		</comment>
	</comments>
</bug>