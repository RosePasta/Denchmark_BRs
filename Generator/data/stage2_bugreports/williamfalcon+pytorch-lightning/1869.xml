<bug id='1869' author='s-rog' open_date='2020-05-18T01:00:26Z' closed_time='2020-05-18T06:07:31Z'>
	<summary>0.7.6 breaks model checkpoint</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

training crashes when model checkpoint is triggered
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Identical code works in 0.7.5, hparam types used are int, float, str, bool
hparams is generated via test tube and saved in the module as self.hparams
&lt;denchmark-code&gt;-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 19, in _wrap
    fn(i, *args)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py", line 389, in ddp_train
    self.run_pretrain_routine(model)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 1015, in run_pretrain_routine
    self.train()
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 347, in train
    self.run_training_epoch()
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 452, in run_training_epoch
    self.call_checkpoint_callback()
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py", line 790, in call_checkpoint_callback
    self.checkpoint_callback.on_validation_end(self, self.get_model())
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py", line 10, in wrapped_fn
    return fn(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 241, in on_validation_end
    self._do_check_save(filepath, current, epoch)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 275, in _do_check_save
    self._save_model(filepath)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 142, in _save_model
    self.save_function(filepath)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py", line 260, in save_checkpoint
    checkpoint = self.dump_checkpoint()
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py", line 355, in dump_checkpoint
    f' not {checkpoint["hparams_type"]}'
ValueError: ('The acceptable hparams type is dict or argparse.Namespace,', ' not TTNamespace')
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='s-rog' date='2020-05-18T01:27:32Z'>
		try master?
		</comment>
		<comment id='2' author='s-rog' date='2020-05-18T06:07:31Z'>
		yep 0.7.7 dev fixed it, cheers.
		</comment>
	</comments>
</bug>