<bug id='1185' author='christabella' open_date='2020-03-18T17:35:46Z' closed_time='2020-03-19T07:27:31Z'>
	<summary>No `loss` in `training_step()` return when logging image to TensorBoard.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

RuntimeError: No  value in the dictionary returned from .
This only happens when I try to log images, as &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.7.1/experiment_reporting.html#log-metrics&gt;suggested in the docs&lt;/denchmark-link&gt;
.
This is fine:
&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        ...
        return {"loss": loss, "log": tensorboard_logs}
&lt;/denchmark-code&gt;

However, this is not:
&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        ...
        loader = self.val_dataloader()
        image = plot_from_loader_to_tensor(loader,
                                           self,
                                           i=self.hparams["vis_i"])
        self.logger.experiment.add_image('train_image', image,
                                         self.trainer.global_step)
        return {"loss": loss, "log": tensorboard_logs}
&lt;/denchmark-code&gt;

But the weirdest thing is that when debugging this, output in process_outputs() ends up having the return dictionary of the validation step...
&lt;denchmark-code&gt;def validation_step(self, batch, batch_idx):
        ...
        return {"val_loss": loss, "log": tensorboard_logs}
&lt;/denchmark-code&gt;

which I then fixed by replacing val_loss with loss, so now the code runs but the TensorBoard logs are switched around (train is logged as val and vice versa):
&lt;denchmark-code&gt;def validation_step(self, batch, batch_idx):
        ...
        return {"loss": loss, "log": tensorboard_logs}
&lt;/denchmark-code&gt;

So this is probably a strong hint of why the error came up in the first place, because for some reason validation_step() is being called in the beginning of training?
The error trace:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "/u/71/irwantc1/unix/.conda/envs/ANP_RNN_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/logging.py", line 108, in process_output
    if k not in ['progress_bar', 'log', 'hiddens']:
KeyError: 'loss'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
...
  File "/u/71/irwantc1/unix/.conda/envs/ANP_RNN_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 703, in training_forward
    output = self.process_output(output, train=True)
  File "/u/71/irwantc1/unix/.conda/envs/ANP_RNN_torch/lib/python3.8/site-packages/pytorch_lightning/trainer/logging.py", line 108, in process_output
    if k not in ['progress_bar', 'log', 'hiddens']:
RuntimeError: No `loss` value in the dictionary returned from `model.training_step()`.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Should not happen
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.13.6
GCC version: Could not collect
CMake version: version 3.14.3

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.16.3
[pip3] torch==1.1.0
[pip3] torchvision==0.2.2.post3
[conda] torch                     1.1.0                    pypi_0    pypi
[conda] torchvision               0.2.2.post3              pypi_0    pypi
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

N/A
	</description>
	<comments>
		<comment id='1' author='christabella' date='2020-03-18T17:36:25Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='christabella' date='2020-03-19T07:27:31Z'>
		I think it‚Äôs because i was running Trainer(distributed_backend=‚Äòdp‚Äô) on single-node single-GPU, removed that and it seems fine!
		</comment>
	</comments>
</bug>