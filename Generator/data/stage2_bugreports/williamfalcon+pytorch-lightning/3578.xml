<bug id='3578' author='carmocca' open_date='2020-09-21T00:54:24Z' closed_time='2020-09-25T12:18:07Z'>
	<summary>Incorrect "Saving latest checkpoint" warning</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

"Saving latest checkpoint..." warning appears regardless of whether a ModelCheckpoint exists or save_last is set to True



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 167 to 169
      in
      a71d62d






 # Save latest checkpoint 



 rank_zero_warn('Saving latest checkpoint..') 



 self.check_checkpoint_callback(should_check_val=False, force_save=True) 








pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 196 to 204
      in
      a71d62d






 def check_checkpoint_callback(self, should_check_val, force_save=False): 



 model = self.trainer.get_model() 



 



 # when no val loop is present or fast-dev-run still need to call checkpoints 



 # TODO bake this logic into the checkpoint callback 



 should_activate = not is_overridden('validation_step', model) and not should_check_val 



 if should_activate or force_save: 



 checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)] 



         [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks] 





This might confuse an user to think the last checkpoint got saved when it did not.
&lt;denchmark-h:h2&gt;Proposed change:&lt;/denchmark-h&gt;

def check_checkpoint_callback(self, should_check_val, force_save=False):
    model = self.trainer.get_model()

    # when no val loop is present or fast-dev-run still need to call checkpoints
    # TODO bake this logic into the checkpoint callback
    should_activate = not is_overridden('validation_step', model) and not should_check_val
    if should_activate or force_save:
        checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)]
        if any(c.save_last for c in checkpoint_callbacks):
            rank_zero_warn('Saving latest checkpoint..')
        [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks]
	</description>
	<comments>
		<comment id='1' author='carmocca' date='2020-09-21T01:15:09Z'>
		why not just remove the log line from training_loop and defer logging about saving the latest checkpoint to be within the checkpoint callback? that seems simpler to me
		</comment>
		<comment id='2' author='carmocca' date='2020-09-21T01:33:25Z'>
		Because the logic to save last is inside of on_validation_end so it would appear after the first validation run
		</comment>
		<comment id='3' author='carmocca' date='2020-09-21T12:25:59Z'>
		
save_last is set to True

it was meant to save the checkpoint if someone interrupts the training.

regardless of whether a ModelCheckpoint exists

yea, should not log if no ModelCheckpoint is used.
		</comment>
		<comment id='4' author='carmocca' date='2020-09-21T15:21:39Z'>
		Thanks for the issue &lt;denchmark-link:https://github.com/carmocca&gt;@carmocca&lt;/denchmark-link&gt;
 . Mind sending a PR?
		</comment>
		<comment id='5' author='carmocca' date='2020-09-21T17:35:42Z'>
		Done!
		</comment>
	</comments>
</bug>