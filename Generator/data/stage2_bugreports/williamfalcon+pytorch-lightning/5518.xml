<bug id='5518' author='cemanil' open_date='2021-01-14T18:30:25Z' closed_time='2021-01-15T02:09:49Z'>
	<summary>Empth "outputs" argument in on_train_batch_end() method of Callback</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The "outputs" argument of the 'on_train_batch_end' method of a lightning Callback seems to be empty, unless training_epoch_end() is implemented in the lightning model.
I'm looking for a way to process the outputs of training_step() in a callback. If I'm not mistaken, the "outputs" argument of the on_train_batch_end() of a lightning callback is meant for use cases like this. If I don't implement the training_epoch_end() method in my lightning model, the "outputs" argument is consistently an empty list. Implementing training_epoch_end() does fill the "outputs" argument with the output of training_step(), but I'd like to avoid this, as keeping track of all the training_step outputs for an entire epoch might be memory intensive.
&lt;denchmark-h:h2&gt;Reproducing the issue&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

The following link &lt;denchmark-link:https://colab.research.google.com/drive/1YD0LFvdrlDZKygUP2cDXDFpyUZ0Kcg75?usp=sharing&gt;BoringModel&lt;/denchmark-link&gt;
 contains the behaviour I'm referring to.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The "outputs" argument of the on_train_batch_end() method of a lightning callback is an empty list if one comments out train_epoch_end() in the lightning model.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.19.5
pyTorch_debug:     True
pyTorch_version:   1.7.0+cu101
pytorch-lightning: 1.1.4
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

If this is a feature rather than a bug, how do you recommend we use the outputs  of training_step in a callback without having to track all training_step outputs for an entire epoch?
	</description>
	<comments>
		<comment id='1' author='cemanil' date='2021-01-15T02:09:49Z'>
		duplicate of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/5517&gt;#5517&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>