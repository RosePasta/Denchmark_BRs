<bug id='4953' author='rakhimovv' open_date='2020-12-02T20:09:05Z' closed_time='2020-12-07T19:31:56Z'>
	<summary>manual_optimization does not work with ddp</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Can't run ddp with manual optimization. Fails on the second batch with a error:
RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the forwardfunction. Please make sure model parameters are not shared across multiple concurrent forward-backward passes2) Reused parameters in multiple reentrant backward passes. For example, if you use multiplecheckpoint functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases yet.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Change optimization to manual in basic gan bolt.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Do not fail when n_gpus &gt; 1
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla V100-SXM2-16GB
Tesla V100-SXM2-16GB
Tesla V100-SXM2-16GB
Tesla V100-SXM2-16GB


available:         True
version:           10.2


Packages:

numpy:             1.19.4
pyTorch_debug:     True
pyTorch_version:   1.7.0
pytorch-lightning: 1.0.8
tqdm:              4.54.0


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.7.9
version:           #1 SMP Tue Sep 10 10:50:19 EDT 2019



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

To have manual optimization working with GANs in multi-gpu regime is very useful applicaiton.
	</description>
	<comments>
		<comment id='1' author='rakhimovv' date='2020-12-02T20:09:55Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='rakhimovv' date='2020-12-02T21:36:43Z'>
		possibly related &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4917&gt;#4917&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='rakhimovv' date='2020-12-03T10:17:15Z'>
		hey &lt;denchmark-link:https://github.com/rakhimovv&gt;@rakhimovv&lt;/denchmark-link&gt;
! We're seeing incorrect behaviour with DDP/manual optimization because backward DDP hooks are not being called correctly within the training step, thus meaning gradients are not being reduced per process.
Our current short term solution is to assert to prevent users from using DDP/manual optimization by enforcing an assert. It will probably take some time for us to come up with a elegant fix for this!
		</comment>
		<comment id='4' author='rakhimovv' date='2020-12-03T12:24:44Z'>
		Heu &lt;denchmark-link:https://github.com/rakhimovv&gt;@rakhimovv&lt;/denchmark-link&gt;
,
I noticed this bug too with pytorch 1.7.
Can you try 1.6 ?
Best,
T.C
		</comment>
	</comments>
</bug>