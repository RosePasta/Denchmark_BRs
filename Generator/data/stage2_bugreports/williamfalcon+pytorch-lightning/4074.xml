<bug id='4074' author='dschaehi' open_date='2020-10-11T12:36:26Z' closed_time='2020-10-11T13:11:58Z'>
	<summary>Logging on step does not work anymore</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Logging on step does not seem to work properly.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run the following MNIST example.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import os

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from torchvision.datasets import MNIST


class MNISTDataModule(pl.LightningDataModule):
    def __init__(self, batch_size=32):
        super().__init__()
        self.batch_size = batch_size

    # When doing distributed training, Datamodules have two optional arguments for
    # granular control over download/prepare/splitting data:

    # OPTIONAL, called only on 1 GPU/machine
    def prepare_data(self):
        MNIST(os.getcwd(), train=True, download=True)
        MNIST(os.getcwd(), train=False, download=True)

    # OPTIONAL, called for every GPU/machine (assigning state is OK)
    def setup(self, stage):
        # transforms
        transform = transforms.Compose(
            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
        )
        # split dataset
        if stage == "fit":
            mnist_train = MNIST(os.getcwd(), train=True, transform=transform)
            self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])
        if stage == "test":
            self.mnist_test = MNIST(os.getcwd(), train=False, transform=transform)

    # return the dataloader for each split
    def train_dataloader(self):
        mnist_train = DataLoader(self.mnist_train, batch_size=self.batch_size)
        return mnist_train

    def val_dataloader(self):
        mnist_val = DataLoader(self.mnist_val, batch_size=self.batch_size)
        return mnist_val

    def test_dataloader(self):
        mnist_test = DataLoader(self.mnist_test, batch_size=self.batch_size)
        return mnist_test


class LitAutoEncoder(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3)
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28)
        )

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        embedding = self.encoder(x)
        return embedding

    def training_step(self, batch, batch_idx):
        # training_step defined the train loop. It is independent of forward
        x, y = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = F.mse_loss(x_hat, x)
        # Logging to TensorBoard by default
        self.log("train_loss", loss, on_step=True)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer


from pytorch_lightning.loggers import CSVLogger


logger = CSVLogger("csv_logs")

# init model
model = LitAutoEncoder()

# init data
dm = MNISTDataModule(batch_size=512)

# train
trainer = pl.Trainer(max_epochs=1, logger=logger)
trainer.fit(model, dm)

# test
trainer.test(datamodule=dm)

import pandas as pd

pd.read_csv("csv_logs/default/version_0/metrics.csv")
&lt;/denchmark-code&gt;

Output:



train_loss
epoch
step




0.790777
0
49


0.614327
0
99


0.582780
0
149


0.594851
0
199


0.545873
0
249



&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The loss should be recorded at each step, but is recorded every 50 steps instead:
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.6.9
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): conda
Python version: 3.7.9
CUDA/cuDNN version: not used for the above example
GPU models and configuration: not used for the above example

	</description>
	<comments>
		<comment id='1' author='dschaehi' date='2020-10-11T12:54:41Z'>
		Hi &lt;denchmark-link:https://github.com/dschaehi&gt;@dschaehi&lt;/denchmark-link&gt;
 this is the intended behavior. This is b/c of a trainer flag called  which is default to 50 as logging every step in the loggers might be resource expensive.
		</comment>
		<comment id='2' author='dschaehi' date='2020-10-11T13:11:58Z'>
		By bad! Thanks for clarification, &lt;denchmark-link:https://github.com/ydcjeff&gt;@ydcjeff&lt;/denchmark-link&gt;
. I'll close this issue.
		</comment>
	</comments>
</bug>