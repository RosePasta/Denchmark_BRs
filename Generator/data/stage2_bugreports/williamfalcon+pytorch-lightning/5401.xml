<bug id='5401' author='Vijayabhaskar96' open_date='2021-01-07T10:37:01Z' closed_time='2021-01-07T14:58:40Z'>
	<summary>TypeError caused due to arguments not found at the end of TPU training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When using TPU for training if you pass arguments when initializing the model, the training runs fine and completes all the epochs but at the end lightning for some reason tries to load the checkpoint back, here when loading the arguments are not found for some reason causing the error.
&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel&lt;/denchmark-h&gt;

Reproduced in the BoringModel, with lr passed during model initialization.
&lt;denchmark-link:https://colab.research.google.com/drive/1CJ1WFQBvfe0-1oFV6BYl-8cAmvzjQQl_?usp=sharing&gt;https://colab.research.google.com/drive/1CJ1WFQBvfe0-1oFV6BYl-8cAmvzjQQl_?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Just change the runtime to TPU and run the notebook.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Load the arguments passed before properly.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Tested on both Colab and Kaggle.
	</description>
	<comments>
		<comment id='1' author='Vijayabhaskar96' date='2021-01-07T12:17:44Z'>
		In:
class BoringModel(LightningModule):

    def __init__(self,lr):
        super().__init__()
        self.layer = torch.nn.Linear(32, 2)
        self.lr = lr
Could you add save_hyperparameters()? This will cache your parameters for saving/loading the checkpoints:
class BoringModel(LightningModule):

    def __init__(self,lr):
        super().__init__()
        self.save_hyperparameters()
        self.layer = torch.nn.Linear(32, 2)
        self.lr = lr
		</comment>
		<comment id='2' author='Vijayabhaskar96' date='2021-01-07T13:08:58Z'>
		It worked!
		</comment>
	</comments>
</bug>