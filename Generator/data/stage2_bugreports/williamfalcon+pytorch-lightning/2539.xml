<bug id='2539' author='YuxianMeng' open_date='2020-07-07T11:30:39Z' closed_time='2020-07-10T01:28:12Z'>
	<summary>TPU fp16 requires apex installed</summary>
	<description>
When I tried to use precision=16 on TPU, pytorch-lightning is trying to find amp, which is unnecessary.
The backtrace is
&lt;denchmark-code&gt;GPU available: False, used: False
TPU available: True, using: 8 TPU cores
Traceback (most recent call last):
  File "bert_ner/light/fp16_debug.py", line 16, in &lt;module&gt;
    trainer = pl.Trainer(tpu_cores=8, precision=16)
  File "/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 607, in __init__
    self.init_amp()
  File "/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/auto_mix_precision.py", line 27, in init_amp
    "You set `use_amp=True` but do not have apex installed."
ModuleNotFoundError: You set `use_amp=True` but do not have apex installed.Install apex first using this guide and rerun with use_amp=True:https://github.com/NVIDIA/apex#linux his run will NOT use 16 bit precision
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
build a whatever Trainer in TPU and use fp16
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;import pytorch_lightning as pl

trainer = pl.Trainer(tpu_cores=8, precision=16)
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Should have nothing error.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.5.0):
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): conda
Build command you used (if compiling from source):
Python version:
CUDA/cuDNN version:
GPU models and configuration:
Any other relevant information: actually I directly use pytorch-xla-1.5 docker on Google Cloud

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='YuxianMeng' date='2020-07-07T11:31:35Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='YuxianMeng' date='2020-07-07T12:26:33Z'>
		If you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.
I could get this working using nightly version of pytorch:
&lt;denchmark-code&gt;pl.Trainer(precision=16, tpu_cores=8)
&gt;&gt;&gt;GPU available: False, used: False
&gt;&gt;&gt;TPU available: True, using: 8 TPU cores
&gt;&gt;&gt;Using native 16bit precision.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='3' author='YuxianMeng' date='2020-07-08T03:23:51Z'>
		
If you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.
I could get this working using nightly version of pytorch:
pl.Trainer(precision=16, tpu_cores=8)
&gt;&gt;&gt;GPU available: False, used: False
&gt;&gt;&gt;TPU available: True, using: 8 TPU cores
&gt;&gt;&gt;Using native 16bit precision.


Thanks for the quick reply. But &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/apex.html&gt;the document&lt;/denchmark-link&gt;
 does not point out that I must have nightly version of pytorch installed or have apex installed when training on TPU with fp16. Maybe it's better to revise that part of document?
		</comment>
		<comment id='4' author='YuxianMeng' date='2020-07-08T09:56:58Z'>
		Yes, I agree that from the documentation it would look like it is only a requirement for gpu training. I guess that the specific requirement for TPU is to have pytorch version 1.6 or higher.
		</comment>
	</comments>
</bug>