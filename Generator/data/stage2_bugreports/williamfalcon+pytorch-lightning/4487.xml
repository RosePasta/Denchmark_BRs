<bug id='4487' author='marcimarc1' open_date='2020-11-02T19:56:36Z' closed_time='2020-11-03T09:41:49Z'>
	<summary>using autograd in neural network calculation breaks the validation step</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi,
just a small bug report, maybe this can be a feature in the future.
Assuming you want to have a derivative in your neural network as output,
the option: torch.set_grad_enabled(False), which is activated during validation, breaks the PyTorch lightning module.
So if you want to train and validate a model like this (see Code) in pytorch ligthning, a workaround is to  torch.set_grad_enabled(True) at the beginning of the validation step.
&lt;denchmark-code&gt;import torch 

class Feedforward(torch.nn.Module):
        def __init__(self, input_size, hidden_size):
            super(Feedforward, self).__init__()
            self.input_size = input_size
            self.hidden_size  = hidden_size
            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)
            self.relu = torch.nn.ReLU()
            self.fc2 = torch.nn.Linear(self.hidden_size, 1)
     
        
        def forward(self, x):
            hidden = self.fc1(x)
            relu = self.relu(hidden)
            output = self.fc2(relu)
            output = output.sum()
            output =torch.autograd.grad(outputs=output, inputs=x, retain_graph=True, create_graph=True)
            return output[0]

test_input = torch.rand((10,3),requires_grad=True)
test_output = torch.rand((10,3))


model = Feedforward(3,10)
optim = torch.optim.Adam(model.parameters())
optim.zero_grad()
loss_fn = torch.nn.L1Loss()
model.train()
out = model(test_input)
loss = loss_fn(out, test_output)
loss.backward()
optim.step() 

&lt;/denchmark-code&gt;

I dont know if this behavior is intendet and if you want to find a workaround, but I leave this here as a Feature Request/ Bug Report.
	</description>
	<comments>
		<comment id='1' author='marcimarc1' date='2020-11-02T19:57:17Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='marcimarc1' date='2020-11-03T09:41:50Z'>
		Dear &lt;denchmark-link:https://github.com/marcimarc1&gt;@marcimarc1&lt;/denchmark-link&gt;
,
Yes, autograd is blocked in validation_step for performance reason. As most people just do metric computation, they don't need gradients to be computed. I hope it makes sense.
Best regards,
T.C
		</comment>
	</comments>
</bug>