<bug id='1682' author='jeremyjordan' open_date='2020-05-01T03:30:17Z' closed_time='2020-06-01T15:00:35Z'>
	<summary>Comet logger cannot be pickled after creating an experiment</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:
initialize the logger object (works fine)
&lt;denchmark-code&gt;from pytorch_lightning.loggers import CometLogger
import tests.base.utils as tutils
from pytorch_lightning import Trainer
import pickle

model, _ = tutils.get_default_model()
logger = CometLogger(save_dir='test')
pickle.dumps(logger)
&lt;/denchmark-code&gt;

initialize a Trainer object with the logger (works fine)
&lt;denchmark-code&gt;trainer = Trainer(
    max_epochs=1,
    logger=logger
)
pickle.dumps(logger)
pickle.dumps(trainer)
&lt;/denchmark-code&gt;

access the experiment attribute which creates the OfflineExperiment object (fails)
&lt;denchmark-code&gt;logger.experiment
pickle.dumps(logger)
&gt;&gt; TypeError: can't pickle _thread.lock objects
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

We should be able to pickle loggers for distributed training.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- available:         False
- version:           None
Packages:
- numpy:             1.18.1
- pyTorch_debug:     False
- pyTorch_version:   1.4.0
- pytorch-lightning: 0.7.5
- tensorboard:       2.1.0
- tqdm:              4.42.0
System:
- OS:                Darwin
- architecture:
- 64bit
-
- processor:         i386
- python:            3.7.6
- version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64

	</description>
	<comments>
		<comment id='1' author='jeremyjordan' date='2020-05-01T07:41:13Z'>
		&lt;denchmark-link:https://github.com/ceyzaguirre4&gt;@ceyzaguirre4&lt;/denchmark-link&gt;
 pls ^^
		</comment>
		<comment id='2' author='jeremyjordan' date='2020-05-18T15:16:44Z'>
		I don't know if it can help or if it is the right place, but a similar error occurswhen running in ddp mode with the WandB logger.
WandB uses a lambda function at some point.
Does the logger have to pickled ? Couldn't it log only on rank 0 at epoch_end ?
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "../train.py", line 140, in &lt;module&gt;
    main(args.gpus, args.nodes, args.fast_dev_run, args.mixed_precision, project_config, hparams)
  File "../train.py", line 117, in main
    trainer.fit(model)
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 751, in fit
    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 149, in start_processes
    process.start()
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/multiprocessing/context.py", line 283, in _Popen
    return Popen(process_obj)
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/clear/fbartocc/miniconda3/envs/Depth_env/lib/python3.8/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.&lt;locals&gt;.&lt;lambda&gt;'
&lt;/denchmark-code&gt;

also related:
&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1704&gt;#1704&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='jeremyjordan' date='2020-05-18T18:38:38Z'>
		I had the same error as &lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;
 . This happened when I added the   and additional  in , as explained here &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html&gt;https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html&lt;/denchmark-link&gt;

&lt;denchmark-code&gt;trainer = pl.Trainer.from_argparse_args(hparams, logger=logger, callbacks=[PrinterCallback(), ])
&lt;/denchmark-code&gt;

I could make the problem go away by directly overwriting the members of Trainer
&lt;denchmark-code&gt;trainer = pl.Trainer.from_argparse_args(hparams)
trainer.logger = logger
trainer.callbacks.append(PrinterCallback())
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='jeremyjordan' date='2020-05-20T21:10:05Z'>
		Same issue as &lt;denchmark-link:https://github.com/F-Barto&gt;@F-Barto&lt;/denchmark-link&gt;
 using a wandb logger across 2 nodes with .
		</comment>
		<comment id='5' author='jeremyjordan' date='2020-05-21T20:36:03Z'>
		same issue when using wandb logger with ddp
		</comment>
		<comment id='6' author='jeremyjordan' date='2020-05-26T20:49:15Z'>
		same here.. &lt;denchmark-link:https://github.com/joseluisvaz&gt;@joseluisvaz&lt;/denchmark-link&gt;
 your workaround doesn't solve the callback issue.. when I try to add a callback like this it is simply being ignored :/ but adding it the Trainer init call normally works.. so I'm pretty sure the error is thrown by the logger (I'm using TB) not the callbacks.
		</comment>
		<comment id='7' author='jeremyjordan' date='2020-05-29T09:02:03Z'>
		Same issue, using wandb logger with 8 gpus in an AWS p2.8xlarge machine
		</comment>
		<comment id='8' author='jeremyjordan' date='2020-07-12T21:41:54Z'>
		With CometLogger, I get this error only when the experiment name is declared. If it is not declared, I get no issue.
		</comment>
	</comments>
</bug>