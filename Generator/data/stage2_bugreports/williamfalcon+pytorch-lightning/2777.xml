<bug id='2777' author='hadarishav' open_date='2020-07-31T11:23:00Z' closed_time='2020-10-02T21:00:45Z'>
	<summary>On multi GPU Model picks up certain parameters from last run on Grid search</summary>
	<description>
I am doing a Grid Search over a set of hyperparameters. I do this by applying loops and re-initializing the model every time. However, the subsequent model seems to be picking up something from the previous runs because their performance changes as opposed to when I try only that particular setting of hyperparameters. Any idea what's happening here?
	</description>
	<comments>
		<comment id='1' author='hadarishav' date='2020-07-31T14:11:43Z'>
		mind share an example?
		</comment>
		<comment id='2' author='hadarishav' date='2020-08-02T11:51:33Z'>
		Turns out there was some problem with how I was handling the data. I will close this issue now. Thanks!
		</comment>
		<comment id='3' author='hadarishav' date='2020-08-03T09:52:51Z'>
		I am sorry for repeatedly opening and closing the issue. But I really wanted to be sure of the problem. So the problem was not really with how I was handling data. It is to do with lightning. If on multi gpu, I repeatedly run the same code on different files, the model picks something up from the previous runs in the lightning logs folder. If I delete that folder and do a subsequent run I get a certain performance, however when I let that folder be and do a subsequent run, the performance changes (becomes better). Here's the code:
&lt;denchmark-code&gt;if __name__ == "__main__":

  ctr = 0

  ctr += 1
  config = {
  'PRE_TRAINED_MODEL_NAME': 'bert-base-cased',
  'batch_size': 16,
  'max_len': 200,
  'abuse_classes': 1,
  'bert_dropout': 0.3,
  'fc_dropout': 0.3,
  'device': torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),
  'num_epochs': 10,
  'lr': 2e-5,
  'wd': 1e-4,
  'filename': ''

  }

  df_train = pd.read_csv("train.csv")
  df_val = pd.read_csv("val.csv")
  df_test = []

  model = Abuse_lightning(0, df_train,df_val,df_test, config)
  path = os.path.join(os.getcwd(), 'runs/lightning_logs/version_'+str(ctr)+'/checkpoints/')
  checkpoint_callback = ModelCheckpoint(
    save_top_k=1,
    filepath = path,
    verbose=True,
    monitor='val_loss',
    mode='min')

  early_stop_callback = EarlyStopping(
   monitor='val_loss',
   min_delta=0.00,
   patience=3,
   verbose=False,
   mode='min')
  
  trainer = pl.Trainer(gpus = 4, progress_bar_refresh_rate=0, max_epochs= config['num_epochs'], checkpoint_callback=checkpoint_callback, distributed_backend="ddp") #, distributed_backend="ddp"
  trainer.fit(model)
&lt;/denchmark-code&gt;

Now if I let the lightning_logs folder be, I get the following performance on 2 consecutive runs:
1st run -
DATALOADER:0 TEST RESULTS
{'kendall': 0.060517550840717894,
'loss': tensor(1.3496),
'pearson': 0.09995590153171233,
'spearman': 0.0893974875437095}
2nd run -
DATALOADER:0 TEST RESULTS
{'kendall': 0.1193280845525937,
'loss': tensor(0.9547),
'pearson': 0.20193530230375267,
'spearman': 0.17886979100553788}
If I delete the lightning_logs folder, I get the following performance on 2 consecutive runs:
1st run -
DATALOADER:0 TEST RESULTS
{'kendall': 0.060517550840717894,
'loss': tensor(1.3496),
'pearson': 0.09995590153171233,
'spearman': 0.0893974875437095}
2nd run -
DATALOADER:0 TEST RESULTS
{'kendall': 0.033294763979888835,
'loss': tensor(1.0082),
'pearson': 0.08154190029041658,
'spearman': 0.04883269827105484}
As you can see, in the former case, the model performance is better in the 2nd run.
Hope this helps!
		</comment>
		<comment id='4' author='hadarishav' date='2020-09-20T01:44:28Z'>
		did you make your runs deterministic, e.g., pytorch_lightning.seed_everything ? Let's make sure this is the case.
In your two consecutive runs, what is the value of ctr ?
		</comment>
		<comment id='5' author='hadarishav' date='2020-09-22T09:08:07Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 I did set the seed.
ctr was mainly for when gridsearch was done in a loop, so it would get updated each time.
However for debugging purpose, in the code I shared above I had removed the loops, and executed the code repeatedly myself. so ctr remained same for each run.
In a new execution I would believe that data from past execution is not taken.
		</comment>
		<comment id='6' author='hadarishav' date='2020-09-22T21:31:09Z'>
		&lt;denchmark-link:https://github.com/hadarishav&gt;@hadarishav&lt;/denchmark-link&gt;
 ok I think I see now, so ctr is the name of the version folder in your code. If that stays constant over multiple runs, then Lightning will resume from the last checkpoint because the version folder already contains a checkpoint. Is that what's happening?
		</comment>
		<comment id='7' author='hadarishav' date='2020-09-22T21:48:07Z'>
		I just checked, and on master, even if the checkpoint path already exists, it creates multiple versions with a suffix -v1, -v2, etc.
If I restart, it always starts with the same val loss never continues from before, and it also saves new checkpoints. So could it be this is already fixed? which version are you using now?
		</comment>
		<comment id='8' author='hadarishav' date='2020-10-02T21:00:45Z'>
		closing this. please reopen if needed.
		</comment>
	</comments>
</bug>