<bug id='3840' author='itsikad' open_date='2020-10-04T09:38:08Z' closed_time='2020-10-04T12:17:01Z'>
	<summary>Incorrect batch size tracking in training and validation steps</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Batch sizes are tracked both in training and evaluation loops to reduce the Train/Eval results on epoch end.
In both cases len(batch) is used to find the current batch_size which is incorrect, for example MNIST loader will return 2 since batch = batch_data, batch_target.
Training loop:



pytorch-lightning/pytorch_lightning/trainer/training_loop.py


         Line 1026
      in
      b40de54






 training_step_output.track_batch_size(len(split_batch)) 





Evaluation loop:



pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py


         Line 339
      in
      b40de54






 output.track_batch_size(len(batch)) 





&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Match the actual batch_size
	</description>
	<comments>
		<comment id='1' author='itsikad' date='2020-10-04T11:27:39Z'>
		duplicate &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3668&gt;#3668&lt;/denchmark-link&gt;
??
		</comment>
		<comment id='2' author='itsikad' date='2020-10-04T12:16:55Z'>
		Thanks &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
! I absolutely missed this one when I searched. Closing the issue.
		</comment>
	</comments>
</bug>