<bug id='2210' author='Nilanshrajput' open_date='2020-06-16T14:17:55Z' closed_time='2020-08-03T19:57:59Z'>
	<summary>[metrics] AUROC Metric cant handle batch with single class samples</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

During calculation of AUROC for a batch, if all samples of particular batch are of same class, it raises following error
&lt;denchmark-code&gt;`/usr/local/lib/python3.6/dist-packages/pytorch_lightning/metrics/functional/classification.py in roc(pred, target, sample_weight, pos_label)
    548 
    549     if fps[-1] &lt;= 0:
--&gt; 550         raise ValueError("No negative samples in targets, false positive value should be meaningless")
    551 
    552     fpr = fps / fps[-1]

ValueError: No negative samples in targets, false positive value should be meaningless`
&lt;/denchmark-code&gt;

This is tensor for labels : tensor([1., 1., 1., 1., 1., 1.]) for this batch all samples were of same class,
The metric being used : pytorch_lightning.metrics.classification.AUROC
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I don't know much about the calculation of auroc but it possible to batch have a single class only, and the metric should somehow handle this
	</description>
	<comments>
		<comment id='1' author='Nilanshrajput' date='2020-06-17T17:25:23Z'>
		In such case it shall be 1
&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='Nilanshrajput' date='2020-06-18T08:45:47Z'>
		Well this is to be consistent with sklearn:
&lt;denchmark-code&gt;&gt;&gt;&gt; roc_auc_score([1, 1, 1, 1, 1], [0.9, 0.8, 0.5, 0.7])
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/Users/justusschock/Workspace/conda/envs/leg/lib/python3.7/site-packages/sklearn/utils/validation.py", line 73, in inner_f
    return f(**kwargs)
  File "/Users/justusschock/Workspace/conda/envs/leg/lib/python3.7/site-packages/sklearn/metrics/_ranking.py", line 393, in roc_auc_score
    sample_weight=sample_weight)
  File "/Users/justusschock/Workspace/conda/envs/leg/lib/python3.7/site-packages/sklearn/metrics/_base.py", line 77, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
  File "/Users/justusschock/Workspace/conda/envs/leg/lib/python3.7/site-packages/sklearn/metrics/_ranking.py", line 223, in _binary_roc_auc_score
    raise ValueError("Only one class present in y_true. ROC AUC score "
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.
&lt;/denchmark-code&gt;

And I also think it should be handled that way
		</comment>
		<comment id='3' author='Nilanshrajput' date='2020-06-18T08:50:42Z'>
		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 we calculate metrics after each validation step, and we cannot insure them to be of a single class, this way training would never complete due to possible error in every step. I think sklearn metric is designed keeping in mind to calculate auroc over a complete dataset not a batch.
		</comment>
		<comment id='4' author='Nilanshrajput' date='2020-06-18T10:24:06Z'>
		Yes, and we basically do the same.
This implementation just provides the implementation. In the next version, we will provide an automated evaluation plan that caches multiple batches. In the meantime you can return them and aggregate them in at_epoch_end for metric computation.
I designed it that way on purpose, since an AUROC (or a ROC in general) is not descriptive in a single batch
		</comment>
		<comment id='5' author='Nilanshrajput' date='2020-06-18T10:28:35Z'>
		Oh, that sounds right, currently, i just increased the validation batch size very much to decrease such probability of such cases:D, but I will add it to val epoch end, Thanks &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='Nilanshrajput' date='2020-07-07T04:06:06Z'>
		What I am doing now is using the following code for aggregation:
class Metrics(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.trues = []
        self.preds = []
        self.metrics = [
            M.Precision(num_classes),
            M.Recall(num_classes),
            M.AUROC(),
            M.Accuracy(),
            # M.AveragePrecision(pos_label=0),
            # M.AveragePrecision(pos_label=1),
        ]

    def forward(self, y_pred, y_true, prefix='train'):
        self.trues.append(y_true.cpu())
        self.preds.append(y_pred.detach().cpu())

        trues = torch.cat(self.trues, dim=0)
        preds = torch.cat(self.preds, dim=0)

        results = {}
        if len(trues.unique()) != 1:
            preds = nn.functional.softmax(preds, dim=-1)
            for metric in self.metrics:
                results.update({f"{prefix}_{metric.name}": metric(preds.cpu(), trues.cpu())})
        return results

    def reset(self):
        self.trues = []
        self.preds = []
		</comment>
	</comments>
</bug>