<bug id='4389' author='robogast' open_date='2020-10-27T13:03:32Z' closed_time='2021-01-19T12:58:33Z'>
	<summary>DDP init hangs with num_nodes &amp;gt;= 2 (with wrong global ranks?)</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

practically, I'm doing the following:
srun python train.py --num-nodes 2 --gpus 4
Which results in the following hanging state, where multiple processes are assigned the same global rank:
&lt;denchmark-code&gt;GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
&lt;/denchmark-code&gt;

If the ddp init is handled by slurm (by manually setting --tasks-per-node=4), global ranks are initiated correctly, but I get a rendezvous error:
File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 172, in _env_rendezvous_handler
    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)
ValueError: host not found: Name or service not known
(but I'm not sure that is related, it might also be an error on my end)

PyTorch Version: 1.6
OS: Linux
How you installed PyTorch: conda
How you installed PyTorch-Lightning: pip from source (pytorch-lightning-1.0.4rc1)
Python version: 3.8
CUDA/cuDNN/NCCL version: 10.1.243 / 7.6.5.32 / 2.6.4

	</description>
	<comments>
		<comment id='1' author='robogast' date='2020-10-28T01:42:15Z'>
		Same question here.
I tried trainer = pl.Trainer(gpus=[4,7]), and then the output was
&lt;denchmark-code&gt;LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [4,7]
initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/2
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='robogast' date='2020-10-28T21:03:48Z'>
		&lt;denchmark-link:https://github.com/Xiaohan-Wang&gt;@Xiaohan-Wang&lt;/denchmark-link&gt;
 your error is not related to this issue here. what you report we fixed it in Lightning 1.0.4 (latest).
&lt;denchmark-link:https://github.com/robogast&gt;@robogast&lt;/denchmark-link&gt;
 I know DDP a bit but unfortunately not much about SLURM.  "host not found: Name or service not known" sounds suspiciously like master address / port are not set. Can you confirm you have set them correctly?
my suspicion is that it sees the same node twice somehow.
Was it working fine before 1.0.4?
		</comment>
		<comment id='3' author='robogast' date='2020-10-29T15:33:38Z'>
		I fixed the latter issue by moving to a newer NCCL toolchain, so that was probably some config error on my end.
The former issue still stands though, also with my updated NCCL.
Edit: It seems I just got lucky a couple of times with the init? I still run into this problem, see comment below.
		</comment>
		<comment id='4' author='robogast' date='2020-10-30T18:18:43Z'>
		Same problem here -- let me know if I can help debugging. Thanks!
&lt;denchmark-code&gt;GPU available: True, used: True
TPU available: False, using: 0 TPU cores
...
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Using native 16bit precision.
initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='robogast' date='2020-11-02T12:54:54Z'>
		About the latter issue, where I got:
File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 172, in _env_rendezvous_handler
    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)
ValueError: host not found: Name or service not known
I added a print statement just above this LOC, and I got the following:
master_addr r29n5,r33n6,r34n1, master_port 22220, world_size 16, start_daemon False, timeout 0:30:00
So the reason it doesn't work is that the os.environ['MASTER_ADDR'] is set to the address of multiple nodes.
I can hotfix it for myself with a master_addr = master_addr.split(',')][0]in rendezvous.py, but I hope someone can figure out why these addresses are wrongly set anyway (my guess is probably somewhere in accelerators/accelerator.py or accelerators/ddp_slurm_accelerator.py?)
Edit 1:
By the way:
In accelerator.py, in def init_ddp_connection there is an unused function arg is_slurm_managing_tasks, perhaps this was forgotten to be passed on in some manner or form?
Edit 2:
For completion, I'll post a slightly bigger section of the error trace I get:
Traceback (most recent call last):
  File "/home/robertsc/hpc-generative-models/reproducibility/pytorch/vq-vae-2-pytorch/vqvae/3d/train.py", line 121, in &lt;module&gt;
    main(args)
  File "/home/robertsc/hpc-generative-models/reproducibility/pytorch/vq-vae-2-pytorch/vqvae/3d/train.py", line 105, in main
    trainer.fit(model, datamodule)
  File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 440, in fit
    results = self.accelerator_backend.train()
  File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_slurm_accelerator.py", line 59, in train
    self.ddp_train(process_idx=self.task_idx, model=model)
  File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/accelerators/ddp_slurm_accelerator.py", line 136, in ddp_train
    self.init_ddp_connection(
  File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 212, in init_ddp_connection
    torch_distrib.init_process_group(
  File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 422, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/robertsc/.conda/envs/pytorch1.6/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 174, in _env_rendezvous_handler
    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)
ValueError: host not found: Name or service not known
		</comment>
		<comment id='6' author='robogast' date='2020-12-18T04:09:16Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
		<comment id='7' author='robogast' date='2021-01-18T00:52:19Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
		<comment id='8' author='robogast' date='2021-01-18T02:29:22Z'>
		&lt;denchmark-link:https://github.com/robogast&gt;@robogast&lt;/denchmark-link&gt;
 see here someone figured it out, the fix is as you say to split the names: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/5533&gt;#5533&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>