<bug id='2600' author='p-wein' open_date='2020-07-13T13:26:48Z' closed_time='2020-09-15T09:07:28Z'>
	<summary>Trainer flag overfit_batches does not overwrite train dataloaders shuffle flag</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Setting the trainer flag overfit_batches (e.g. =10) does not overwrite the shuffle flag set in the training dataloader, even though the warning reads:
UserWarning: You requested to overfit but enabled training dataloader shuffling. We are turning it off for you.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Create lightning module with method train_dataloader with flag shuffle=True:

&lt;denchmark-code&gt;   def train_dataloader(self) -&gt; loading.DataLoader:
        dataset = ProstateX(train=True)
        batch_transforms, gpu_transforms, sample_transforms = self.get_transformations()
        dataloader = loading.DataLoader(dataset,
                                        batch_size=self.hparams.tr_batch_size,
                                        batch_transforms=batch_transforms,
                                        shuffle=True,
                                        sample_transforms= sample_transforms,
                                        gpu_transforms=gpu_transforms,
                                        pseudo_batch_dim=True,
                                        num_workers=self.hparams.num_workers)
        return dataloader 
&lt;/denchmark-code&gt;

( I use a rising dataloader, bug should also occur with pytorch dataloaders though)

Create main.py with:

&lt;denchmark-code&gt;mymodel = model.Model3D(cfg)
trainer = pl.Trainer(gpus=1, precision=16, overfit_batches=10)
trainer.fit(mymodel)`
&lt;/denchmark-code&gt;


Run main.py
Find out that your model does not converge.
set shuffle=False when creating Dataloader in train_dataloader
See that your model converges after some epochs.

(Or log the samples loaded by the dataloader and check if they are the same each epoch.)
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Either model also converges with shuffle=True, since warning says that it got overwritten (assuming model converges with shuffle=False) or at least warning should read that user has to change shuffle to False.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce GTX 1080 Ti
- available:         True
- version:           10.1
Packages:
- numpy:             1.19.0
- pyTorch_debug:     False
- pyTorch_version:   1.7.0.dev20200705+cu101
- pytorch-lightning: 0.8.5
- tensorboard:       2.2.2
- tqdm:              4.47.0
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.7
- version:           #109-Ubuntu SMP Fri Jun 19 11:33:10 UTC 2020

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='p-wein' date='2020-07-13T13:27:53Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='p-wein' date='2020-09-12T11:39:18Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
		<comment id='3' author='p-wein' date='2020-09-28T13:45:33Z'>
		I am seeing the same issue when using . &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/859ec92da51ff56b49e3e1f1beefa75767edc175/pytorch_lightning/trainer/trainer.py#L132&gt; From a comment in the code, &lt;/denchmark-link&gt;
I believe that option is to be removed in 1.0.0, but is it worth it to fix it anyways? The same code will do fix the issue just checking  instead.
		</comment>
	</comments>
</bug>