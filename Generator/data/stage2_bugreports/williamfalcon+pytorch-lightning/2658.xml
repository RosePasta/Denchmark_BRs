<bug id='2658' author='samikhenissi' open_date='2020-07-21T15:19:37Z' closed_time='2020-09-15T17:53:57Z'>
	<summary>Pytorch lightning switched to cpu in the middle of training. How can I debug this?</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/20474484/88071206-8bfabe80-cb41-11ea-8043-c09c57ef00f8.png&gt;&lt;/denchmark-link&gt;

So I am training a model and suddenly PyTorch lightning started using the CPU. I checked the GPU status and it is working normally.
I have never seen this before. Can Pytorch start using CPU in the middle of training (at the next epoch)?
The weird thing is the next epoch was using the gpu.
EDIT: Some asked how I noticed it is not using GPU.
I noticed the difference by looking at the it/s displayed in the verbose. It dropped to 13 it/s from 130 in one epoch and then rebounded to 150 it/s. I immediately took a look at the CUDA usage and It was very low.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

This is the training loop: Pretty standard
`    model = MatrixFactorizationAttention(att_mf_config)
&lt;denchmark-code&gt;train_loader = sample_generator.instance_a_train_loader_attention(att_mf_config['batch_size'])
val_loader = sample_generator.instance_val_loader_attention
# wandb_logger.log_hyperparams(att_mf_config)
early_stop_callback = EarlyStopping(
    monitor='HR',
    min_delta=0.01,
    patience=10,
    verbose=True,
    mode='max'
)

trainer = pl.Trainer(gpus=1, max_epochs=150, precision=16,checkpoint_callback=False,logger = False,early_stop_callback=early_stop_callback)
trainer.fit(model, train_dataloader=train_loader, val_dataloaders=val_loader)
&lt;/denchmark-code&gt;

`
EDIT: apart from this training loop, I am using a standard PyTorch model with a regular classification pipeline.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Win 10 with pytorch lightning running on pycharm
Cuda 10.2
GPU 2060 super
pytorch: nightly version
	</description>
	<comments>
		<comment id='1' author='samikhenissi' date='2020-07-21T15:20:38Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='samikhenissi' date='2020-07-21T18:09:48Z'>
		Weird, mind share the code in colab or something?
		</comment>
		<comment id='3' author='samikhenissi' date='2020-07-21T19:44:29Z'>
		How did you come to the conclusion that it is switching to CPU? The screenshot certainly does not give us this information.
		</comment>
		<comment id='4' author='samikhenissi' date='2020-07-21T19:46:45Z'>
		You probably saw it in tensorboard? Is it one entire epoch?
		</comment>
		<comment id='5' author='samikhenissi' date='2020-07-21T20:22:45Z'>
		
How did you come to the conclusion that it is switching to CPU? The screenshot certainly does not give us this information.

In the verbose, you can see the number of iterations per second. It dropped to 13 it/s from 130 it/s. I also monitored the CUDA memory.
		</comment>
		<comment id='6' author='samikhenissi' date='2020-07-21T20:44:46Z'>
		Can we test this with a callback that checks the device like so:
from pytorch_lightning import Callback

class DeviceCallback(Callback):

    def on_batch_start(self, trainer, pl_module):
        assert next(pl_module.parameters()).device.type == "cuda"

... 
trainer = Trainer(..., callbacks=[DeviceCallback()])
If this assertion does not fail and your iterations decrease, it might be a data loading issue.
		</comment>
		<comment id='7' author='samikhenissi' date='2020-07-22T15:49:58Z'>
		Thank you. It did not fail indeed when the speed decreased. Any idea what may cause this behavior or how to investigate the data loading issue?
I create the trainloader from a dataframe using a standard method.
def instance_a_train_loader_attention(self,batch_size):
        users, items, ratings = self.train_ratings['userId'].tolist(), self.train_ratings['itemId'].tolist(), self.train_ratings['rating'].tolist()
        items_rated_byuser, users_rated_item = self.train_ratings['item_sample'].tolist(), self.train_ratings['user_sample'].tolist()
        dataset = UserItemRatingDatasetAttention(user_tensor=torch.LongTensor(users),
                                        item_tensor=torch.LongTensor(items),
                                        target_tensor=torch.FloatTensor(ratings),
                                        items_rated_user=torch.LongTensor(items_rated_byuser),
                                        users_rated_item_tensor=torch.LongTensor(users_rated_item))
        return DataLoader(dataset, batch_size=batch_size, shuffle=True)
		</comment>
		<comment id='8' author='samikhenissi' date='2020-07-29T13:58:07Z'>
		Anyone who wants to reproduce the issue here is the code: &lt;denchmark-link:https://github.com/samikhenissi/pl_neumf&gt;https://github.com/samikhenissi/pl_neumf&lt;/denchmark-link&gt;

I am not sure if it is a hardware issue from my part or a bug. The speed just randomly drops for one iteration and goes back up again. The callback showed that I am still using the GPU for the model but with very low speed.
		</comment>
		<comment id='9' author='samikhenissi' date='2020-07-29T16:54:42Z'>
		Can reproduce using this code, but don't know yet what's causing it. My observations so far:

it also happens on cpu (if you set gpus=0)
does not look to be related to logging, row_log_interval, log_save_interval, progress bar refresh rate
does not look like a data loading issue
in my case, the it/s display does not drop, I only see interruptions directly looking at the iteration counter.
observed on master too

		</comment>
		<comment id='10' author='samikhenissi' date='2020-09-15T17:53:57Z'>
		I guess this was resolved, otherwise feel free to reopen... üê∞
		</comment>
	</comments>
</bug>