<bug id='3233' author='carmocca' open_date='2020-08-27T21:56:50Z' closed_time='2020-09-03T20:07:50Z'>
	<summary>auto_scale_batch_size not working with datamodule</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The Trainer expects the LightningModule to have self.batch_size (see scale_batch_size() in training_tricks.py). However, if one is using the new LightningDataModule, that should be the class with self.batch_size defined.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

assert hasattr(lightning_data_module, "batch_size")
trainer = Trainer(auto_scale_batch_size=True)
trainer.fit(lightning_module, datamodule=lightning_data_module)
pytorch_lightning.utilities.exceptions.MisconfigurationException: Field batch_size not found in both `model` and `model.hparams`
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

auto_scale_batch_size should work using LightningDataModule
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* Packages:
	- numpy:             1.18.5
	- pyTorch_debug:     False
	- pyTorch_version:   1.6.0
	- pytorch-lightning: 0.9.1rc1
	- tensorboard:       2.2.0
	- tqdm:              4.48.2
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='carmocca' date='2020-08-27T23:26:56Z'>
		I knew this bug report will eventually come :) same will happen for auto_lr_find.
We need to generalize our lightning_hasattr and getattr helper functions to include the datamodule. In total, we have
&lt;denchmark-code&gt;model.attribute_name
model.hparams.attribute_name
model.dm.attribute_name
&lt;/denchmark-code&gt;

all of these should be considered by both lr_find and auto_scale_batch_size
		</comment>
		<comment id='2' author='carmocca' date='2020-08-28T14:56:05Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  So currently there is no solution to this issue?
		</comment>
		<comment id='3' author='carmocca' date='2020-08-28T15:00:09Z'>
		no, you need to set it manually, sorry.
But I'll try to make a PR this weekend if not today. Should be a relatively easy fix unless I missed something.
		</comment>
		<comment id='4' author='carmocca' date='2020-08-28T15:22:29Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 I completely agree that it should be a simple fix, since all the attribute getting/setting is handled by our own  ect functions.
		</comment>
		<comment id='5' author='carmocca' date='2020-09-03T20:47:58Z'>
		&lt;denchmark-link:https://github.com/carmocca&gt;@carmocca&lt;/denchmark-link&gt;
 As you may already know, we fixed this. Just a note in case you didn't see it, you now need to call .tune() instead of .fit():
trainer.tune(lightning_module, datamodule=lightning_data_module)
This is to better distinguish the training from the tuning step. However, it may be subject to change since there are some refactors happening right now.
		</comment>
		<comment id='6' author='carmocca' date='2020-09-04T07:38:41Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;

In which version  will be officially introduced?
		</comment>
		<comment id='7' author='carmocca' date='2020-09-04T21:01:02Z'>
		If it stays, in v1.0
		</comment>
		<comment id='8' author='carmocca' date='2020-09-05T02:01:15Z'>
		I am not familiar with the tuning interface. Few questions:
Right now, I can use auto_scale_batch_size by passing the option to the trainer and calling fit. This does the auto scale procedure and then starts training
If I understand correctly, when tune is released the following will be the necessary:
&lt;denchmark-code&gt;should_auto_scale_bs = # comes from the user
trainer = Trainer(auto_scale_batch_size=should_auto_scale_bs)

if should_auto_scale_bs:
    trainer.tune(...)
trainer.fit(...)
&lt;/denchmark-code&gt;

Im assuming tune doesnt run fit automatically when it is finished.
What would happen if the code ran trainer.tune(...) outside of the if and auto_scale_batch_size was False?
Also, shouldn't auto_scale_batch_size be a parameter of tune instead of Trainer? (Maybe it is, I just don't know where is the tune discussion).
		</comment>
		<comment id='9' author='carmocca' date='2020-09-05T03:33:33Z'>
		I do not know how to answer these questions, &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 added the tune method so it would be best to ask him how he sees it being used in the future.
		</comment>
		<comment id='10' author='carmocca' date='2020-09-05T09:34:06Z'>
		&lt;denchmark-link:https://github.com/carmocca&gt;@carmocca&lt;/denchmark-link&gt;
 I think the process in the future will be:
&lt;denchmark-code&gt;trainer = Trainer(auto_scale_batch_size = should_auto_scale_bs)
trainer.tune(model) # will do nothing if should_auto_scale_bs=False
trainer.fit(model)
&lt;/denchmark-code&gt;

it should still be easy for the user to use these features. The moving of the tuning from fit to tune is to disentangle the hyperparameter tuning from the actual optimization of the network. This will make it easier for us in the future to implement more tuning algorithms.
		</comment>
		<comment id='11' author='carmocca' date='2020-10-23T21:03:21Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 , I have the same issue happening with me, my code is:
&lt;denchmark-code&gt; # Reading and intilaizing the Trainer
    trainer_config = config.pop('trainer_config')
    trainer = Trainer(
        **trainer_config,
        callbacks=callbacks,
        logger=loggers,
        checkpoint_callback=checkpoint_callback,
        auto_scale_batch_size=(dataset_config['batch_size'] is None)
    )

    # optimizing batch size if batch_size is none
    if dataset_config['batch_size'] is None:
        print("Batch Size is None attempting to tune batch size")
        # tuner = Tuner(trainer)
        # optimal_batch_size = tuner.scale_batch_size(
        #     model, mode='power',
        #     batch_arg_name='batch_size',
        #     datamodule=dataset)
        optimal_batch_size = trainer.tune(model, datamodule=dataset)
        print(f"Found best batch size to be: {optimal_batch_size}")
        dataset.batch_size = optimal_batch_size
&lt;/denchmark-code&gt;

I tracked the issue and I think there is a problem in logic from &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L186&gt;parsing.py Line186&lt;/denchmark-link&gt;
 to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L193&gt;parsing.py Line193&lt;/denchmark-link&gt;

The problem is that I have a hparams attribute in my model (I don't know where that came from),  but it doesn't contain a batch size attribute, the batch_size is an attribute that is contained in the datamodule, if &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L192&gt;this if condition&lt;/denchmark-link&gt;
 is executed then I think there would not be a problem
		</comment>
	</comments>
</bug>