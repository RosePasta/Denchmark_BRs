<bug id='5410' author='sjgosai' open_date='2021-01-07T20:12:24Z' closed_time='2021-01-18T14:11:16Z'>
	<summary>LightningModule models using `setup` don't load checkpoints properly.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Using  methods within a  does not allow proper checkpoint loading using the  method. Furthermore, even if  and  are used to manually load a checkpoint, the  seems to always invoke  with  is called, thereby overwriting the loaded parameters. There are ways to get around these issues, but they don't fit nicely into the recommended PTL style standards, as I interpreted them. This issue has been filed as a bug on the recommendation in this &lt;denchmark-link:https://forums.pytorchlightning.ai/t/loading-checkpoints-when-models-built-using-a-setup-block/530/3?u=sjgosai&gt;forum post&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel&lt;/denchmark-h&gt;

I have reproduced this bug with a simple model in this &lt;denchmark-link:https://colab.research.google.com/drive/1weAyerRPpe5jQOD0s-yzymmF2czFx18Q?usp=sharing&gt;Colab Notebook&lt;/denchmark-link&gt;
.
&lt;denchmark-link:https://colab.research.google.com/drive/1weAyerRPpe5jQOD0s-yzymmF2czFx18Q?usp=sharing&gt;https://colab.research.google.com/drive/1weAyerRPpe5jQOD0s-yzymmF2czFx18Q?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Use the above Colab notebook to reproduce, noting that cell [5] is expected to fail. Otherwise Notebook runs top-to-bottom.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Ideally, code in cell [5] should not error. Additionally, trainer should not reset model Parameters with fit is called (e.g., in cell [7]).
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

This Colab Notebook includes a pip install of PTL in the top cell. Otherwise, it uses the default Colab environment settings.
&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Please refer to &lt;denchmark-link:https://forums.pytorchlightning.ai/t/loading-checkpoints-when-models-built-using-a-setup-block/530&gt;this forum post&lt;/denchmark-link&gt;
 for more information.
	</description>
	<comments>
		<comment id='1' author='sjgosai' date='2021-01-07T20:13:06Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='sjgosai' date='2021-01-08T21:23:05Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 mind taking a look?
		</comment>
		<comment id='3' author='sjgosai' date='2021-01-09T07:31:15Z'>
		&lt;denchmark-link:https://github.com/sjgosai&gt;@sjgosai&lt;/denchmark-link&gt;
 Hi, what you observe is not surprising, considering that you for some reason moved model definition to setup? setup is not meant for this purpose. The convenience function  that we provide does not just load the model weights, it  the model and then loads the weights. However, your model does not contain any layers when instantiated, so loading the tensors on non-existing layers will of course fail. And setup is a hook, it is not meant to be called manually. Since you change the model definition inside the setup hook, the side effects you observe are expected.
If, for some reason you really need to use setup to build your model, then you also have to put the torch.load call inside of that method, or in any hook that gets called after that. But the best is to just move the layer definition into the constructor, it is anyway equivalent to what you have in the code there. Hope this helps.
		</comment>
		<comment id='4' author='sjgosai' date='2021-01-10T20:29:44Z'>
		Hi &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
, thanks for taking a look. Yes, I totally agree that my observations aren't surprising.
In my particular use-case my data module and model module need to communicate before nn.Modules are assigned and data-related classes are constructed, the example I gave was meant only to be illustrative. I've also seen an example of using class inheritance to construct models, and here I can imagine a place for model construction within . Additionally, to me, the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#setup&gt;docs&lt;/denchmark-link&gt;
 suggest layer building in  should be considered reasonable.
I also have tried to use Trainer(resume_from_checkpoint='/tmp/pytorch-example/model.ckpt'), which appears to first run setup then load the checkpoint as is desired. However, the downside here is restoring the full training, which is undesirable when doing transfer learning. That said, the apparent compatibility with resume_from_checkpoint further makes me believe standard model loading methods should consistently work (or fail) when setup is used for model modules.
If users should not expect setup blocks to play well with recommended checkpoint loading methods, clarification in the docs would be helpful for the appropriate usage of the hook and its limitations.
		</comment>
		<comment id='5' author='sjgosai' date='2021-01-10T21:06:01Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Follow up, if I were to introduce  calls into , are there any side-effects to look out for? This seems like a reasonable strategy.
		</comment>
		<comment id='6' author='sjgosai' date='2021-01-10T22:54:38Z'>
		
That said, the apparent compatibility with resume_from_checkpoint further makes me believe standard model loading methods should consistently work (or fail) when setup is used for model modules.

I don't understand this sentence. Where is the inconstency?
I think it's fine to build layers in setup.
The load_from_checkpoint does two things at the same time, instantiate the Python object and then load tensor values. If the checkpoint contains weights but the nn.Module has no matching layers, it will error unless the strict option is False. Here is an example:
import torch
import torch.nn as nn

from pytorch_lightning import LightningModule


class FullModule(LightningModule):

    def __init__(self):
        super().__init__()
        self.layer = nn.Linear(3, 4)


class EmptyModule(LightningModule):
    pass


full = FullModule()
empty = EmptyModule()
torch.save({"state_dict": full.state_dict()}, "full.ckpt")
torch.save({"state_dict": empty.state_dict()}, "empty.ckpt")


# PyTorch
# -------

# works
empty.load_state_dict(torch.load("empty.ckpt")["state_dict"])

# error: Unexpected key(s) in state_dict: "layer.weight", "layer.bias".
# empty.load_state_dict(torch.load("full.ckpt")["state_dict"])

# works
empty.load_state_dict(torch.load("full.ckpt")["state_dict"], strict=False)


# Lightning
# ---------

# error: Unexpected key(s) in state_dict: "layer.weight", "layer.bias".
# EmptyModule.load_from_checkpoint("full.ckpt")

# works
empty = EmptyModule.load_from_checkpoint("full.ckpt", strict=False)
This gives you a few options. You can restore your Module with hyperparameters from the checkpoint using the load_from_checkpoint function and the strict=False option, even though it doesn't have any submodules (it will not attempt to load any weights). Then you can either use the Trainer flag to restore the weights and trainer state (optimizers etc.), or if that is not desired simply call torch.load in setup after the layers are built. That would require passing the checkpoint file path into the model.
Another alternative is to build the model in the on_load_checkpoint() hook, if passing the checkpoint path into the model is not desired.
In terms of usability, we can think about giving the user more options when resuming the training. For example:
Trainer(resume_from_checkpoint={"file": "x/y/z.ckpt", optimizers=False, schedulers=False, global_step=100, ...})
What do you think of that?

However, the downside here is restoring the full training

Partially true yes, but you can still change/override the Trainer arguments as you like.
		</comment>
		<comment id='7' author='sjgosai' date='2021-01-11T11:10:05Z'>
		Hey &lt;denchmark-link:https://github.com/sjgosai&gt;@sjgosai&lt;/denchmark-link&gt;
,
To add on &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 answers.
If your model depends on data attributes, I think it is better practices to pass them the  class directly. You shouldn't need to dataset to be created to re-create your model.
&lt;denchmark-code&gt;MyModel(*args, **kwargs, **datamodule.hyper_parameters)
&lt;/denchmark-code&gt;

Best,
T.C
		</comment>
		<comment id='8' author='sjgosai' date='2021-01-11T11:10:26Z'>
		Hey &lt;denchmark-link:https://github.com/sjgosai&gt;@sjgosai&lt;/denchmark-link&gt;
, If it answers your question, please close this issue.
		</comment>
		<comment id='9' author='sjgosai' date='2021-01-11T18:17:58Z'>
		
I don't understand this sentence. Where is the inconstancy?

&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
, Both  and  use PTL generated checkpoints. One option works by default with  the other doesn't by default (your example clearly shows why). The inconsistency, from my perspective, is that the when using , the order of operations is effectively , , then  but when using the classmethod it's  then . However, I guess you guys are considering everything in the  block to be ephemeral and specific to the current "Training State". I think, if this is the view, it should be clear in the docs for the  hook. However, I still think the  hook is useful for dynamically built models which we want to persist outside the context of the current Training State, but am happy to see things your way :).

You shouldn't need to dataset to be created to re-create your model.

&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
, I agree that you don't need to see a dataset to re-create a trained model, but to there are cases where you would want to create an initial model for training after you build your dataset, which itself may depend on some attribute of the model. I see  as an easy way to deal with this kind of interaction, and these communicated attributes can be saved using the  hook for future restoration. I would prefer to avoid duplicating model code to have one version to init and train a model and another to load model weights.

In terms of usability, we can think about giving the user more options when resuming the training.

&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Yes, this would be helpful, I think. Additionally, is there some way to skip the  hook when running ? If I run two datasets in the same session or manually load weights with torch code, I will always run into problems if  is constantly resetting layers.
&lt;denchmark-code&gt;trainer.fit(model, data1)
trainer.fit(model, data2) # setup called again, layers reinitialized
&lt;/denchmark-code&gt;


Another alternative is to build the model in the on_load_checkpoint() hook, if passing the checkpoint path into the model is not desired.

You are correct that passing checkpoint paths to the model isn't ideal. However, I don't always load a model from a checkpoint because sometimes I will be training from scratch, so I'm not sure this is viable.
		</comment>
		<comment id='10' author='sjgosai' date='2021-01-18T04:39:16Z'>
		Hi, &lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;
 I wanted to flag that I have posted followup.
		</comment>
		<comment id='11' author='sjgosai' date='2021-01-18T05:10:52Z'>
		In reply to your first paragraph, the two ways are inconsistent by design. load_from_checkpoint is a function, not a method. It cannot restore the state of a trainer or in any way influence at which point the weights are loaded. We cannot call setup() during the load_from_checkpoint function, because there is no Trainer.
So what do we need to do? We need to load the model before setup is running?

I think, if this is the view, it should be clear in the docs for the setup hook.

The docs for setup state:
"Called at the beginning of fit and test. This is a good hook when you need to build models dynamically or adjust something about them."
I think this implies that we need to call load after setup, otherwise the newly modified layers would stay uninitialized?
		</comment>
		<comment id='12' author='sjgosai' date='2021-01-18T05:58:35Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Thanks for your response and willingness to discuss with me.

load_from_checkpoint is a function, not a method.

Sorry for the confusion in my abuse of the term "method", I was referring to it as such as it's defined as a python classmethod &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/6926b849372fe8f7bc6d5fa8c9eb3ba856645534/pytorch_lightning/core/saving.py#L56&gt;here&lt;/denchmark-link&gt;
 that gets inherited through LightningModule.

In reply to your first paragraph, the two ways are inconsistent by design.

Got it.

I think this implies that we need to call load after setup, otherwise the newly modified layers would stay uninitialized?

I agree, this order seems like the right thing to do. However, there can be the case where you want to run a trainer multiple times using model that has already initialized dynamic weights with setup once (i.e., fit a model on two datasets in succession), and you want to re-use these pre-trained weights. I'm suggesting that the user have the option to skip setup when fit is called.
		</comment>
		<comment id='13' author='sjgosai' date='2021-01-18T13:39:15Z'>
		Ok, I understand now. You need a way to skip certain hooks. Running setup as it is done currently is fine, however, you run into trouble when you fit multiple times, then you want to prevent calling setup a second time.
We can think about how best to add such a feature. Something like model.skip_hooks = ["setup", ...] and Trainer looking up this list everytime it needs to call a hook.
In the meantime, to get your work going, I suggest the following hacks:

in your current setup, you can do the following:

def setup(self, stage):
    if self.has_setup:
        return
    # do your thing
    ...
    self.has_setup = True

delattr(model, "setup") removes the hook completely

		</comment>
		<comment id='14' author='sjgosai' date='2021-01-18T14:11:16Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 Thank you! This makes a lot of sense. And I think this sort of functionality could be generally useful in many cases :). I will close this issue.
		</comment>
	</comments>
</bug>