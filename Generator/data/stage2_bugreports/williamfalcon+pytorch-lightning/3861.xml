<bug id='3861' author='Vichoko' open_date='2020-10-05T02:44:44Z' closed_time='2020-10-08T02:58:58Z'>
	<summary>EvalResult checkpoint_on parameter only minimizes. What about maximizable metrics?</summary>
	<description>
EvalResult checkpoint_on parameter, for example uses the val_loss metric. This metric is optimized when its minimized. But some metrics, like accuracy or recall, are optimized when maximized.
How do I tell the EvalResult logger to maximize this watched metric? When i intend to checkpoint on a maximizable metric, like the accuracy, in my case.
As stated &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.step_result.html&gt;on the docs&lt;/denchmark-link&gt;
, there is no way to do this.
&lt;denchmark-code&gt;def test_step(self, batch, batch_idx):
    loss = ...
    acc = ...
    result = EvalResult(checkpoint_on=acc)
    result.log('val_loss', loss)
    result.log('val_acc', acc)
    return result
&lt;/denchmark-code&gt;


Parameters
    early_stop_on
    (Optional[Tensor]) – Metric to early stop on. Should be a one element tensor if combined with default EarlyStopping. If this result is returned by validation_step(), the specified value will be averaged across all steps.

    checkpoint_on
    (Optional[Tensor]) – Metric to checkpoint on. Should be a one element tensor if combined with default checkpoint callback. If this result is returned by validation_step(), the specified value will be averaged across all steps.

    hiddens
    (Optional[Tensor]) –


	</description>
	<comments>
		<comment id='1' author='Vichoko' date='2020-10-05T17:27:55Z'>
		set  in &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/callbacks.html#model-checkpointing&gt;ModelCheckpoint&lt;/denchmark-link&gt;
??
		</comment>
		<comment id='2' author='Vichoko' date='2020-10-05T22:37:08Z'>
		Is not clear how ModelCheckpoint callback on Trainer instance interacts with EvalResult checkpoint system as they are 2 different checkpoint systems. In my case, I tried both defining a ModelChekpoint callback with max mode and with the EvalResult checkpoint_on param, and neither worked with val_accuracy (the metric I'm trying to checkpoint on).
Luckily, the workaround I figured out was to define the error rate metric as 1 - accuracy, and track that metric instead as it's optimized when minimized.
Something like this:
def test_step(self, batch, batch_idx):
    loss = ...
    acc = ...
    error_rate = 1 - acc
    result = EvalResult(checkpoint_on=error_rate)
    result.log('val_error_rate, error_rate)
    result.log('val_loss', loss)
    result.log('val_acc', acc)
    return result
		</comment>
		<comment id='3' author='Vichoko' date='2020-10-08T02:59:22Z'>
		EvalResult is deprecated.
Second, remember that min = -max
		</comment>
		<comment id='4' author='Vichoko' date='2020-10-08T05:26:34Z'>
		Thanks for the response &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;

Did something replace EvalResult and TrainResult? What would be the best practice to log metrics and checkpointing now?
		</comment>
		<comment id='5' author='Vichoko' date='2020-10-13T14:11:23Z'>
		&lt;denchmark-link:https://github.com/Vichoko&gt;@Vichoko&lt;/denchmark-link&gt;

The Metric class was updated in PL v1.0 and has similar logging functionality, except for checkpointing, for which you'd rather use checkpoint callback. I didn't tested it myself yet though.
		</comment>
		<comment id='6' author='Vichoko' date='2020-10-14T18:10:31Z'>
		Thank you for the clarification &lt;denchmark-link:https://github.com/itsikad&gt;@itsikad&lt;/denchmark-link&gt;
 ^^
		</comment>
	</comments>
</bug>