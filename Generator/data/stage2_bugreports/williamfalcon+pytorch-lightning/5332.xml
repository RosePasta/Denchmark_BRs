<bug id='5332' author='chris-clem' open_date='2021-01-02T12:23:32Z' closed_time='2021-01-04T09:05:22Z'>
	<summary>Unexpected global_steps/ accelerator behaviour</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I observed some, imo,  unexpected behaviour when experimenting with different combinations of gpus/ num_processes and accelerator trainer args.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

I use the following &lt;denchmark-link:https://gist.github.com/chris-clem/7d3c04d3132c2e9b47c9c74c39910d4e&gt;bug_report_model.py&lt;/denchmark-link&gt;
. I adapted the one from pl to take , , and  args, only use fake train_data, and train for ten epochs.

When running it with the default args (accelerator=None, gpus=0, num_processes=1), trainer.global_step=640 as expected:

&lt;denchmark-code&gt;.venv ‚ùØ python -W ignore bug_report_model.py
GPU available: True, used: False
TPU available: None, using: 0 TPU cores
Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:00&lt;00:00, 969.92it/s, loss=718]
trainer.accelerator_backend=&lt;pytorch_lightning.accelerators.cpu_accelerator.CPUAccelerator object at 0x7f8e39ea7dc0&gt;
trainer.gpus=0
trainer.num_processes=1
trainer.global_step=640
&lt;/denchmark-code&gt;


When running it with  --num_processes 2 --accelerator ddp_cpu, trainer.global_step=0 which is not expected:

&lt;denchmark-code&gt;.venv ‚ùØ python -W ignore bug_report_model.py --num_processes 2 --accelerator ddp_cpu
GPU available: True, used: False
TPU available: None, using: 0 TPU cores
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00&lt;00:00, 305.89it/s, loss=45.2]
trainer.accelerator_backend=&lt;pytorch_lightning.accelerators.ddp_cpu_spawn_accelerator.DDPCPUSpawnAccelerator object at 0x7f325f6e6df0&gt;
trainer.gpus=0
trainer.num_processes=2
trainer.global_step=0
&lt;/denchmark-code&gt;


When running it with --gpus 2, trainer.accelerator_backend is DDPSpawnAccelerator and trainer.global_step=0 which is not expected:

&lt;denchmark-code&gt;.venv ‚ùØ python -W ignore bug_report_model.py --gpus 2
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00&lt;00:00, 284.00it/s, loss=0.159]
trainer.accelerator_backend=&lt;pytorch_lightning.accelerators.ddp_spawn_accelerator.DDPSpawnAccelerator object at 0x7fa00deafe50&gt;
trainer.gpus=2
trainer.num_processes=2
trainer.global_step=0
&lt;/denchmark-code&gt;


When running it with  --gpus 2 --accelerator ddp, trainer.global_step=320 which makes sense due to ddp, but changes logging behaviour and max_steps calculations:

&lt;denchmark-code&gt;.venv ‚ùØ python -W ignore bug_report_model.py --gpus 2 --accelerator ddp
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00&lt;00:00, 233.42it/s, loss=0.19]
trainer.accelerator_backend=&lt;pytorch_lightning.accelerators.ddp_accelerator.DDPAccelerator object at 0x7f1b88743370&gt;
trainer.gpus='0,1'
trainer.num_processes=2
trainer.global_step=320
trainer.accelerator_backend=&lt;pytorch_lightning.accelerators.ddp_accelerator.DDPAccelerator object at 0x7fb45ae2ee80&gt;
trainer.gpus=2
trainer.num_processes=2
trainer.global_step=320
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Regarding 2.:  trainer.global_step should be 320 or 640?
Regarding 3.:  trainer.accelerator_backend should be DDPAccelerator   as this is the recommended one for multi GPU training?
Regarding 4.: Would it be possible and/ or make sense to have total_steps that multplies global_steps and gpus that is used for max_steps and logging?
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;* CUDA:
        - GPU:
                - GeForce RTX 2080 Ti
                - GeForce RTX 2080 Ti
                - GeForce RTX 2080 Ti
                - GeForce RTX 2080 Ti
        - available:         True
        - version:           10.2
* Packages:
        - numpy:             1.19.4
        - pyTorch_debug:     False
        - pyTorch_version:   1.7.1
        - pytorch-lightning: 1.1.2
        - tqdm:              4.55.0
* System:
        - OS:                Linux
        - architecture:
                - 64bit
                - ELF
        - processor:         x86_64
        - python:            3.8.6
        - version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='chris-clem' date='2021-01-03T21:05:12Z'>
		Hi, these are all expected behaviour. In all modes that spawn children processes (ddp_spawn, ddp_cpu (spawn)), the main process does not do any training, therefore trainer.global_step = 0. If you want to access global_step, you need to do it in the code that runs in training, i.e. in training_step or similar, then you will see it has a meaningful value.
And in ddp (no spawn) max_steps, batch_size etc. are settings per gpu.
		</comment>
		<comment id='2' author='chris-clem' date='2021-01-04T09:05:19Z'>
		Hey &lt;denchmark-link:https://github.com/chris-clem&gt;@chris-clem&lt;/denchmark-link&gt;
,
As this behaviour is expected and &lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
  provided a great answer, I will be closing this issue.
Best regards,
T.C
		</comment>
	</comments>
</bug>