<bug id='3426' author='davidwhealey' open_date='2020-09-09T17:13:40Z' closed_time='2020-09-09T20:44:07Z'>
	<summary>Model checkpoint not saving hyperparameters correctly</summary>
	<description>
When using the ModelCheckpoint, my hyperparameters are not being saved with the checkpoints.  So I get an AttributeError when attempting to load from checkpoints.
To reproduce:
&lt;denchmark-code&gt;import pytorch_lightning as pl
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision import transforms
import torch
import torch.nn.functional as F
import argparse
from bunch import Bunch

import pytorch_lightning as pl
class LitModel(pl.LightningModule):

    def __init__(self, args):
        super().__init__()
        self.l1 = torch.nn.Linear(28 * 28, 10)
        print('args:', args)
        print(args.to_print)

    def forward(self, x):
        return torch.relu(self.l1(x.view(x.size(0), -1)))

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        return pl.TrainResult(loss)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)

    
train_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))
checkpoint_callback = pl.callbacks.ModelCheckpoint(
        os.path.join(os.getcwd(), 'chkpts'),
        save_top_k=1,
        verbose=True,
        monitor='loss',
        mode='min'
    )
trainer = pl.Trainer(checkpoint_callback=checkpoint_callback,
                    train_percent_check=0.1,
                    val_percent_check=0,
                    max_epochs=1)

hparams = argparse.Namespace()
hparams.to_print = 'foo'
model = LitModel(hparams)

trainer.fit(model, train_loader)

mod = LitModel.load_from_checkpoint(ckpt_path)
&lt;/denchmark-code&gt;

Produces the following Error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-38-868d05212321&gt; in &lt;module&gt;
     49 trainer.fit(model, train_loader)
     50 
---&gt; 51 mod = LitModel.load_from_checkpoint(ckpt_path)

~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, *args, **kwargs)
    151         checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)
    152 
--&gt; 153         model = cls._load_model_state(checkpoint, *args, strict=strict, **kwargs)
    154         return model
    155 

~/miniconda3/envs/camtraps/lib/python3.6/site-packages/pytorch_lightning/core/saving.py in _load_model_state(cls, checkpoint, strict, *cls_args, **cls_kwargs)
    188             cls_args, cls_kwargs = [], {}
    189 
--&gt; 190         model = cls(*cls_args, **cls_kwargs)
    191         # load the state_dict on the model automatically
    192         model.load_state_dict(checkpoint['state_dict'], strict=strict)

&lt;ipython-input-38-868d05212321&gt; in __init__(self, args)
     15         self.l1 = torch.nn.Linear(28 * 28, 10)
     16         print('args:', args)
---&gt; 17         print(args.to_print)
     18 
     19     def forward(self, x):

AttributeError: 'dict' object has no attribute 'to_print'
&lt;/denchmark-code&gt;

The print statements indicate that args is an empty dict when attempting to load from checkpoint.
When inspecting the checkpoint
&lt;denchmark-code&gt;ckpt_path = os.path.join(os.getcwd(), '_ckpt_epoch_0.ckpt')
ckpt = torch.load(ckpt_path)
print(ckpt.keys())
&lt;/denchmark-code&gt;

I get the following:
&lt;denchmark-code&gt;dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'checkpoint_callback_best_model_score', 'checkpoint_callback_best_model_path', 'optimizer_states', 'lr_schedulers', 'state_dict'])
&lt;/denchmark-code&gt;

My understanding is there should be a hyper_parameters in the checkpoint.
System:

PyTorch Version 1.3.1
pytorch-lightning: 0.9.0 installed conda
OS: Ubuntu 18.04
Python 3.6

	</description>
	<comments>
		<comment id='1' author='davidwhealey' date='2020-09-09T17:14:23Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='davidwhealey' date='2020-09-09T19:29:10Z'>
		you need to call self.save_hyperparameters() in __init__ to make it work.
		</comment>
		<comment id='3' author='davidwhealey' date='2020-09-09T20:44:04Z'>
		Worked, thank you!
		</comment>
	</comments>
</bug>