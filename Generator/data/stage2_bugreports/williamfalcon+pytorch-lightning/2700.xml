<bug id='2700' author='ibeltagy' open_date='2020-07-25T00:37:19Z' closed_time='2020-12-02T13:05:12Z'>
	<summary>Checkpointing is broken on TPUs</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Pytorch/XLA saves checkpoints using the following syntax which is not supported in pytorch-lightning.
&lt;denchmark-code&gt;import torch_xla.core.xla_model as xm
xm.save()
&lt;/denchmark-code&gt;

It is a little tricky to support because  has a barrier inside it and it checks for rank=0 while  doesn't. This means  should be called only on the process with rank=0 (which pytorch-lighting does) but  should be called by all processes (or it will wait forever at the barrier). This means pytorch-lightning code that checks for the rank (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.8.5/pytorch_lightning/callbacks/model_checkpoint.py#L265-L269&gt;here&lt;/denchmark-link&gt;
 will need to be switched off on TPUs.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


Train any model on TPUs using PyTorch/XLA with ptl.Trainer(checkpoint_callback=[ModelCheckpoint(...)], num_tpu_cores=8)
Wait until the model saves one checkpoint then kill the process
Try to load the saved checkpoint with ptl.Trainer(resume_from_checkpoint='path_to_saved_checkpoint', num_tpu_cores=8)
See error

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Loading checkpoint successfully.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

pytorch-lightning==v0.8.5
&lt;denchmark-h:h3&gt;Additional Context&lt;/denchmark-h&gt;

Thanks to &lt;denchmark-link:https://github.com/matt-peters&gt;@matt-peters&lt;/denchmark-link&gt;
 for finding the bug and suggesting the solution mention below.
	</description>
	<comments>
		<comment id='1' author='ibeltagy' date='2020-07-25T12:10:01Z'>
		&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 mind have look?
		</comment>
		<comment id='2' author='ibeltagy' date='2020-07-25T14:24:21Z'>
		sure :]
		</comment>
		<comment id='3' author='ibeltagy' date='2020-07-25T15:33:26Z'>
		Thanks, &lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
. You might want to check this fix here &lt;denchmark-link:https://github.com/ibeltagy/pytorch-lightning/commit/a5c8d182329a3be88f17f952bee9cc063116c515&gt;ibeltagy@a5c8d18&lt;/denchmark-link&gt;
 which works but I don't like it. I also tried calling the functions inside  &lt;denchmark-link:https://github.com/pytorch/xla/blob/master/torch_xla/core/xla_model.py#L782&gt;here&lt;/denchmark-link&gt;
 in the main process only without the barrier but everything hangs, maybe because the processes go out of sync.
		</comment>
		<comment id='4' author='ibeltagy' date='2020-07-25T20:20:24Z'>
		&lt;denchmark-link:https://github.com/ibeltagy&gt;@ibeltagy&lt;/denchmark-link&gt;
 Nice work :] I'll check out your solution and try and back with a fix on this.
		</comment>
		<comment id='5' author='ibeltagy' date='2020-07-26T14:24:52Z'>
		&lt;denchmark-link:https://github.com/ibeltagy&gt;@ibeltagy&lt;/denchmark-link&gt;
 I am able to reload the checkpoint successfully, however, the training fails due to some xla device issue. Is it the same error you face? could you share a notebook reproducing this issue?
		</comment>
		<comment id='6' author='ibeltagy' date='2020-07-27T16:49:01Z'>
		Loading the checkpoint fails only fails when loading without a TPU device available, as  will write out XLA tensors instead of pytorch tensors.  This a common workflow to train on TPU, but then move to a CPU or GPU for further processing.  As a result, I don't think it's possible to reproduce with a notebook.  moves everything to CPU before saving to avoid this problem.  I opened a PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2726&gt;#2726&lt;/denchmark-link&gt;
 that includes a fix.
		</comment>
	</comments>
</bug>