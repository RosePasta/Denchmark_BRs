<bug id='2976' author='invisprints' open_date='2020-08-14T10:09:20Z' closed_time='2020-09-22T22:00:25Z'>
	<summary>How to use ReduceLROnPlateau methon in matster branch version?</summary>
	<description>
&lt;denchmark-h:h4&gt;What is your question?&lt;/denchmark-h&gt;

I find the log design has changed a lot between version 0.8.5 and master branch &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/tree/0c264689cb566582ac47333d8b7192d656e19440&gt;0c26468&lt;/denchmark-link&gt;

I got the error message when I follow the &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#logging-from-a-lightningmodule&gt;docs logging-from-a-lightningmodule&lt;/denchmark-link&gt;
 to modify the log code.
error message:
&lt;denchmark-code&gt;MisconfigurationException: ReduceLROnPlateau conditioned on metric val_loss which is not available. Available metrics are: val_early_stop_on,val_checkpoint_on,epoch,checkpoint_on. Condition can be set using `monitor` key in lr scheduler dict
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code&lt;/denchmark-h&gt;

def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr = 0.01)
    scheduler = ReduceLROnPlateau(optimizer, patience=10)
    return [optimizer], [scheduler]

def validation_step(self, batch, batch_nb):
    x, y = batch
    
    y_hat = self(x)    
    loss = F.l1_loss(y, y_hat)
    result = pl.EvalResult()
    result.log('val_step_loss', loss)
    return result

def validation_epoch_end(self, outputs):
    avg_loss = outputs.val_step_loss.mean()
    result = pl.EvalResult()
    result.log('val_loss', avg_loss)
    return result
I wonder if I should defined validation_epoch_end like above, if there are any example about how to use ReduceLROnPlateau in a right way?
&lt;denchmark-h:h4&gt;What's your environment?&lt;/denchmark-h&gt;


OS: Ubuntu 18.04
Packaging pip
Version 0.9.0 rc 12 master branch 0c26468

	</description>
	<comments>
		<comment id='1' author='invisprints' date='2020-08-14T19:54:08Z'>
		it's a bug I think since callback_metrics contains only val_early_stop_on and val_checkpoint_on.



pytorch-lightning/pytorch_lightning/core/step_result.py


        Lines 727 to 733
      in
      5bce06c






 def get_callback_metrics(self) -&gt; dict: 



 result = { 



 'val_early_stop_on': self.early_stop_on, 



 'val_checkpoint_on': self.checkpoint_on 



     } 



 



 return result 








pytorch-lightning/pytorch_lightning/trainer/training_loop.py


        Lines 1254 to 1263
      in
      5bce06c






 if lr_scheduler['reduce_on_plateau']: 



 monitor_key = lr_scheduler['monitor'] 



 monitor_val = self.callback_metrics.get(monitor_key) 



 if monitor_val is None: 



 avail_metrics = ','.join(list(self.callback_metrics.keys())) 



 raise MisconfigurationException( 



 f'ReduceLROnPlateau conditioned on metric {monitor_key}' 



 f' which is not available. Available metrics are: {avail_metrics}.' 



 ' Condition can be set using `monitor` key in lr scheduler dict' 



         ) 





		</comment>
		<comment id='2' author='invisprints' date='2020-08-15T10:51:56Z'>
		yeah, that's a bug with the new version. adding a fix now.
What should we condition it on? i guess the clear thing maybe is on the checkpoint key?
result = EvalResult(checkpoint_on=the_thing_to_lr_reduce_on)
this does assume the scheduler currently adjusts using val loop and not train loop?
		</comment>
		<comment id='3' author='invisprints' date='2020-08-15T12:10:50Z'>
		result = EvalResult(checkpoint_on=the_thing_to_lr_reduce_on)
in that case, if some assigns checkpoint_on to be a metric EvalResult(checkpoint_on=some_metric_tensor) and ReduceLROnPlateau to monitor on val_loss then there might be a conflict here.
		</comment>
		<comment id='4' author='invisprints' date='2020-08-15T12:18:03Z'>
		the point is that when using ReduceLROnPlateau there is no longer "val_loss"... it will monitor whatever the value of checkpoint_on is
		</comment>
		<comment id='5' author='invisprints' date='2020-08-15T12:28:40Z'>
		val_loss is just an example. My point here is if checkpoint_on=metric1 and monitor=metric2 for ReduceLROnPlateau, in such case it's a conflict. We should not force it to monitor metric1 for ReduceLROnPlateau.
		</comment>
		<comment id='6' author='invisprints' date='2020-08-15T13:06:43Z'>
		haha. i think you're still missing the point.
the keyword 'monitor' does not have an effect when using evalresults... instead, the ReduceLROnPlateau will look at whatever is on the checkpoint_on
You could set monitor='jiraffe' for ReduceLROnPlateau and it won't matter.
Lightning will use whatever is in checkpoint_on=X
		</comment>
		<comment id='7' author='invisprints' date='2020-08-15T14:14:24Z'>
		yeah, maybe I am missing something here ðŸ˜… but not sure how this will work if we take checkpoint_on value for ReduceLROnPlateau:
model_checkpoint = ModelCheckpoint(monitor='val_loss')

...

def validation_epoch_end(self, outputs):
    val_loss = ...
    val_recall = ...
    res = pl.EvalResult(checkpoint_on=val_loss) # not familier with EvalResult but I guess checkpoint_on will be used for modelcheckpoint
    res.log('val_recall', val_recall)
    return res

def configure_optimizers(self):
    optimizer = ...
    scheduler = {'scheduler': ReduceLROnPlateau(), 'interval': 'epoch', monitor: 'val_recall'}
    return [optimizer], [scheduler]
		</comment>
		<comment id='8' author='invisprints' date='2020-08-15T14:42:59Z'>
		again... monitor has NO effect anywhere... with the results object. doesnâ€™t matter what callback uses the word
		</comment>
		<comment id='9' author='invisprints' date='2020-08-16T11:58:37Z'>
		Had the same question so arrived here.
&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
, I think what &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
  means, is that there might be cases where someone wish to use ReduceOnPlatue on metric1 and to save checkpoint on metric2.
i.e, I wish to use ReduceOnPlatue on train_loss (to allow the network to (over)fit in case the lr is not low enough) and use checkpoint_on='val_acc', to save the best model along the training routine.
		</comment>
		<comment id='10' author='invisprints' date='2020-08-16T12:02:56Z'>
		ok, then we should allow feeding the values logged as options to the callbacks?
		</comment>
		<comment id='11' author='invisprints' date='2020-08-16T12:21:28Z'>
		Not sure myself (I'm not sure of the implementation details).
Would love if you could maybe tag someone who could answer that better.
Ido
		</comment>
		<comment id='12' author='invisprints' date='2020-08-16T12:30:48Z'>
		haha ok. yeah, i think thatâ€™s the sensible option since this allows any metric to be monitored by any callback in the future
		</comment>
		<comment id='13' author='invisprints' date='2020-08-16T12:39:40Z'>
		yeah, will solve issues with ModelCheckpoint too. Also, why even add a checkpoint_on/early_stop_on as a parameter there? can't we just use log itself with checkpoint=True/False just like on_epoch/on_step, if we feed logs in the callbacks? Just a suggestion.
		</comment>
		<comment id='14' author='invisprints' date='2020-08-16T12:43:53Z'>
		because we need a single, unique value to checkpoint/early stop on. with log we canâ€™t enforce that. and also if you want to change what to ckpt on during training, this approach allows that.
ie: in some tasks, i may use loss for a while, but switch to a metric after a certain number of epochs
		</comment>
		<comment id='15' author='invisprints' date='2020-08-16T12:57:17Z'>
		then maybe take the first value in case if multiple checkpoint values are passed with log and raise a warning there?
result.log('metric1', metric1_val, checkpoint=True)
result.log('metric2', metric2_val, checkpoint=True)
take metric1_val for checkpoint.
		</comment>
		<comment id='16' author='invisprints' date='2020-08-16T13:19:08Z'>
		let's go with the current approach in the API and iterate on it if it causes issues.
		</comment>
		<comment id='17' author='invisprints' date='2020-08-18T05:35:04Z'>
		I am still confused about how to use ReduceLROnPlateau, are there any simple examples to explain it?
		</comment>
		<comment id='18' author='invisprints' date='2020-08-18T06:35:22Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 I checked the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3004&gt;#3004&lt;/denchmark-link&gt;
 and find that the monitor seems can only record dicts in , but not ï¼Œ Here is the code running in Colab (modified from &lt;denchmark-link:https://github.com/nateraw/pytorch-lightning/blob/colab-tuts/notebooks/01_mnist_hello_world.ipynb&gt;MNIST Hello World&lt;/denchmark-link&gt;
).
class LitMNIST(pl.LightningModule):
    
    def __init__(self, data_dir='./', hidden_size=64, learning_rate=2e-4):

        super().__init__()

        # Set our init args as class attributes
        self.data_dir = data_dir
        self.hidden_size = hidden_size
        self.learning_rate = learning_rate

        # Hardcode some dataset specific attributes
        self.num_classes = 10
        self.dims = (1, 28, 28)
        channels, width, height = self.dims
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        # Define PyTorch model
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(channels * width * height, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, self.num_classes)
        )

    def forward(self, x):
        x = self.model(x)
        return F.log_softmax(x, dim=1)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y)
        result = pl.TrainResult(loss)
        result.log('train_loss', loss)
        return result

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = F.nll_loss(logits, y)
        preds = torch.argmax(logits, dim=1)
        acc = accuracy(preds, y)
        result = pl.EvalResult(checkpoint_on=loss)

        # Calling result.log will surface up scalars for you in TensorBoard
        result.log('val_loss', loss, prog_bar=True)
        result.log('val_acc', acc, prog_bar=True)
        return result

    def test_step(self, batch, batch_idx):
        # Here we just reuse the validation_step for testing
        return self.validation_step(batch, batch_idx)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)
        scheduler = {'scheduler': lr_scheduler, 'interval': 'step', 'monitor': 'val_loss'}
        return [optimizer], [scheduler]

    ####################
    # DATA RELATED HOOKS
    ####################

    def prepare_data(self):
        # download
        MNIST(self.data_dir, train=True, download=True)
        MNIST(self.data_dir, train=False, download=True)

    def setup(self, stage=None):

        # Assign train/val datasets for use in dataloaders
        if stage == 'fit' or stage is None:
            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)
            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])

        # Assign test dataset for use in dataloader(s)
        if stage == 'test' or stage is None:
            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)

    def train_dataloader(self):
        return DataLoader(self.mnist_train, batch_size=32)

    def val_dataloader(self):
        return DataLoader(self.mnist_val, batch_size=32)

    def test_dataloader(self):
        return DataLoader(self.mnist_test, batch_size=32)

model = LitMNIST()
trainer = pl.Trainer(gpus=1, max_epochs=3, fast_dev_run=False, progress_bar_refresh_rate=20)
trainer.fit(model)
		</comment>
		<comment id='19' author='invisprints' date='2020-08-25T03:40:46Z'>
		&lt;denchmark-link:https://github.com/invisprints&gt;@invisprints&lt;/denchmark-link&gt;
 in case you haven't figured it out, the note sections in this doc would be helpful for you: &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/stable/lightning-module.html#configure-optimizers&gt;https://pytorch-lightning.readthedocs.io/en/stable/lightning-module.html#configure-optimizers&lt;/denchmark-link&gt;

So your configure_optimizers() should be something like:
def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)
        # reduce every epoch (default)
        scheduler = {
            'scheduler': lr_scheduler, 
            'reduce_on_plateau': True,
            # val_checkpoint_on is val_loss passed in as checkpoint_on
            'monitor': 'val_checkpoint_on'
        }
        return [optimizer], [scheduler]
		</comment>
		<comment id='20' author='invisprints' date='2020-08-25T05:50:57Z'>
		&lt;denchmark-link:https://github.com/yukw777&gt;@yukw777&lt;/denchmark-link&gt;
 isn't  specific to checkpoints? I mean one must set the  monitor equal to what the checkpoint monitor is? They can't be different?
		</comment>
		<comment id='21' author='invisprints' date='2020-08-25T06:40:16Z'>
		&lt;denchmark-link:https://github.com/yukw777&gt;@yukw777&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/rohitgr7&gt;@rohitgr7&lt;/denchmark-link&gt;
 yeap, I mean if I want to monitor something different from checkpoint_on monitor, what should I do? Because in &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/ccc923cbb01af7776f2472056b264758a6745871/tests/base/deterministic_model.py#L459&gt;test code&lt;/denchmark-link&gt;
, it seems we can monitor anything we want, and it works in training step but not in val step.
		</comment>
		<comment id='22' author='invisprints' date='2020-08-25T19:06:16Z'>
		Decided to rewrite my code to new API, still didn't get how to use ReduceLROnPlateau :(
		</comment>
		<comment id='23' author='invisprints' date='2020-08-25T19:14:30Z'>
		I got it
    def training_step(self, batch, batch_idx):
        loss = self.calc_loss(batch)

        res = pl.TrainResult(loss)
        res.log("train_loss", loss, prog_bar=True)

        return res

    def validation_step(self, batch, batch_idx):
        loss = self.calc_loss(batch)

        res = pl.EvalResult(checkpoint_on=loss)
        res.log("val_loss", loss, prog_bar=True)
        return res

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(params=self.parameters(),
                                     lr=self.hparams.lr,
                                     weight_decay=self.hparams.l2_norm)

        lr_scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.9, verbose=True)

        scheduler = {
            'scheduler': lr_scheduler,
            'reduce_on_plateau': True,
            # val_checkpoint_on is val_loss passed in as checkpoint_on
            'monitor': 'val_checkpoint_on'
        }
        return [optimizer], [scheduler]
		</comment>
		<comment id='24' author='invisprints' date='2020-09-16T18:16:13Z'>
		&lt;denchmark-link:https://github.com/invisprints&gt;@invisprints&lt;/denchmark-link&gt;
 did your question get answered?
		</comment>
		<comment id='25' author='invisprints' date='2020-09-17T01:38:42Z'>
		No, what I want is we can monitor anything we want, not just val_checkpoint_on
		</comment>
		<comment id='26' author='invisprints' date='2020-09-17T15:23:35Z'>
		marking as duplicate.
		</comment>
	</comments>
</bug>