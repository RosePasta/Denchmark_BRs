<bug id='3149' author='ShomyLiu' open_date='2020-08-25T08:55:54Z' closed_time='2020-08-26T10:08:03Z'>
	<summary>Data DistributedSampler Error  when using Multi-GPU setting (with ddp).</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Hi,
I have converted my pure PyTorch code into pytorch-lightning code, however,  the pl code would be crashed when using multi-gpus settings, while the code runs successfully when I set gpus=1.
My task is a binary classification task, and the error happens in AUC-ROC-score computing using sklearn:
&lt;denchmark-code&gt;File "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/ranking.py", line 256, in _binary_roc_auc_score                                                    raise ValueError("Only one class present in y_true. ROC AUC score "
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Code sample&lt;/denchmark-h&gt;

the trainer:
&lt;denchmark-code&gt;trainer = pl.Trainer(gpus=opt.gpu_num, distributed_backend='ddp')
&lt;/denchmark-code&gt;

the core part in model.py: Note that I need to collect the outputs of all validation-steps first, and then to compute the metric in the validation_epoch_end.
&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        label_list = batch[0]
        data = batch[1:]
        score = self(data)
        loss = F.cross_entropy(score, label_list.max(1)[1])
        result = pl.TrainResult(loss)
        result.log('train_loss', loss, sync_dist=True)
        return result

    def validation_step(self, batch, batch_idx):
        label_list = batch[0]
        data = batch[1:]
        score = self(data)
        loss = F.cross_entropy(score, label_list.max(1)[1])
        result = pl.EvalResult(loss)
        result.label_list = label_list.squeeze(1).tolist()
        result.pred_list = score.squeeze(1).tolist()
        return result

    def validation_epoch_end(self, val_outputs):

        labels = [i for k in val_outputs.label_list for i in k]
        preds = [i for k in val_outputs.pred_list for i in k]
    
        res = cal_metric(all_labels, all_preds)
        result = pl.EvalResult(checkpoint_on=torch.tensor(res['group_auc']))
        result.log_dict(res, prog_bar=True, sync_dist=True)
        return result

&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce GTX 1080 Ti
- GeForce GTX 1080 Ti
- available:         True
- version:           10.2
Packages:
- numpy:             1.17.4
- pyTorch_debug:     False
- pyTorch_version:   1.6.0
- pytorch-lightning: 0.9.0
- tensorboard:       2.0.2
- tqdm:              4.48.2
System:
- OS:                Linux
- architecture:
- 64bit
- ELF
- processor:         x86_64
- python:            3.6.9
- version:           #110-Ubuntu SMP Tue Jun 23 02:39:32 UTC 2020

So  what might be the reason that leads to the error in validation_epoch_end in multi-gpus setting. (it goes well in gpus=1).
Thanks.
	</description>
	<comments>
		<comment id='1' author='ShomyLiu' date='2020-08-25T13:03:23Z'>
		Updated
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

I have checked that the batch_data (i.e., the label and data),  and found out that the batch data is unexpected and disturbed, which would lead to there are all zeros in some y_true and cause the error.  (In fact, there should be at least one 1 in the label.
Hence the reason might be the  in . &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1567&gt;#1567&lt;/denchmark-link&gt;
.  Following is my data-loader:
&lt;denchmark-code&gt;def collate_fn(batch):
    data = zip(*batch)
    data = [torch.LongTensor(d) for d in data]
    return data
train_dataloader = DataLoader(train_data,
                                  opt.batch_size,
                                  shuffle=True,
                                  collate_fn=collate_fn)
test_dataloader = DataLoader(test_data,
                                 opt.batch_size,
                                 shuffle=False,
                                 collate_fn=collate_fn)
&lt;/denchmark-code&gt;

So, should I define the distributedsampler?
		</comment>
		<comment id='2' author='ShomyLiu' date='2020-08-25T13:45:24Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 mind have look? 
		</comment>
		<comment id='3' author='ShomyLiu' date='2020-08-25T14:21:35Z'>
		&lt;denchmark-h:h2&gt;Updated2: (BTW, I'm new to DDP)&lt;/denchmark-h&gt;

when I disabled the auto-sampler in pl using replace_sampler_ddp=False, the code can run without errors, however, if there is no distributed sampler, the data in different GPUs is the same.
		</comment>
		<comment id='4' author='ShomyLiu' date='2020-08-25T14:32:35Z'>
		&lt;denchmark-link:https://github.com/ShomyLiu&gt;@ShomyLiu&lt;/denchmark-link&gt;
 yes without the ddp sampler data on different GPUs is same. What you'd need is some kind of stratified splitting across GPUs (or a larger batchsize).
While we cannot do anything on the stratification side, we will provide custom aggregation cross different batches for several metrics, once &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2528&gt;#2528&lt;/denchmark-link&gt;
 is done.
But this is not an error on our side, this only caused to your data scenario.
		</comment>
		<comment id='6' author='ShomyLiu' date='2020-08-25T17:50:03Z'>
		you can try out some kind of stratified DistributedSampler. If you search if online you'll find one. Then just add that sampler and set replace_ddp_sampler=False.
		</comment>
		<comment id='8' author='ShomyLiu' date='2020-12-15T01:53:56Z'>
		I had similar problems. I think the type of batch argument the collate_fn function receives changed from a list of examples to examples in my case. I do not know why this bug exists, but it seems only with Pytorch Lightning (after googling)..
		</comment>
	</comments>
</bug>