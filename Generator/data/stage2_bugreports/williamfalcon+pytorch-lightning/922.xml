<bug id='922' author='srush' open_date='2020-02-23T20:09:13Z' closed_time='2020-02-25T03:23:26Z'>
	<summary>Init'ing Dataloader calls get_train_dataloader</summary>
	<description>
It seems like the code of initializing the dataloader calls into getting the dataloader.



pytorch-lightning/pytorch_lightning/trainer/data_loading.py


         Line 69
      in
      c00a8a1






 if EXIST_ITER_DATASET and isinstance(self.get_train_dataloader().dataset, IterableDataset): 





This means that all the effort for wrapping get_dataloader to sync through barriers for multi-gpu / tpu is not used on this first call (and results in a crash).
	</description>
	<comments>
		<comment id='1' author='srush' date='2020-02-23T20:14:29Z'>
		ummm good point. I think we need to simplify the get_XXX_dataloader() calls. The lazy loading decorator does make it harder for people to debug their data loading issues.
&lt;denchmark-link:https://github.com/neggert&gt;@neggert&lt;/denchmark-link&gt;
 do you remember the original reason we added the decorator? maybe it's time to remove it and simplify this logic?
		</comment>
		<comment id='2' author='srush' date='2020-02-23T20:35:20Z'>
		&lt;denchmark-link:https://github.com/ethanwharris&gt;@ethanwharris&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/jakubczakon&gt;@jakubczakon&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/MattPainter01&gt;@MattPainter01&lt;/denchmark-link&gt;

any thoughts?
		</comment>
		<comment id='3' author='srush' date='2020-02-23T20:55:31Z'>
		I always assumed the decorator was to stop multiple instantiation - there was some old bug where data loading threads would hang around after each epoch because new data loaders were created and the old threads just carried on - having said that I can't find the issue anywhere
The IterableDataset stuff at the moment is a bit fragile (mostly hard coded type checks), there might be better ways to deal with it that simplify the above
If there's some way we can remove the decorator but still only create the dataloader once then that would be a big usability improvement :)
		</comment>
		<comment id='4' author='srush' date='2020-02-23T21:13:36Z'>
		agreed. that was the original reason. basically we could refactor to make sure we only call it at time of the epoch beginning.
i think we needed it before to determine length and some other reasons
		</comment>
		<comment id='5' author='srush' date='2020-02-23T22:02:02Z'>
		I'm stuck on a couple issues here actually that I can't unwind.
Main Issue: I don't really understand the semantics of train_dataloader in ddp / tpu training. Is it supposed to be called by only (a) with rank 0 or (b) with all ranks. I would prefer (a) but I need to know so I don't call barriers internally. If it is (b) then I do need to do that, but the semantics are more clear. My assumption had been (b) which works for DDP for me (TPU I'm still stuck).
Side Issue: It's very difficult to determine ordering. I had been calling training_data() from configure_optimizer, but doing that seems to preempt everything and lead to strange behavior.
A related issue to this is that loading the data set 8x times on TPU blows up the limited amount of RAM to Colab allows for. It would be nice to avoid this issue.
		</comment>
	</comments>
</bug>