<bug id='1702' author='xinfangliu' open_date='2020-05-02T05:57:03Z' closed_time='2020-06-26T14:22:47Z'>
	<summary>CUDA out of memory</summary>
	<description>
I set Cuda to 5
INFO:lightning:CUDA_VISIBLE_DEVICES: [5]
however, there always a small part of the memory in gpu 0, when gpu 0 has not enough memory, error is produced.
&lt;denchmark-link:https://user-images.githubusercontent.com/35248527/80856418-6c5ba480-8c7c-11ea-97fd-1ded85ee0e5b.png&gt;&lt;/denchmark-link&gt;

in the img you can see that two processes is running on gpu5,6 and 2 * 577M is running gpu0.
I want the processes to only use what GPU I set.
	</description>
	<comments>
		<comment id='1' author='xinfangliu' date='2020-05-02T15:19:45Z'>
		m24.py
&lt;denchmark-link:https://user-images.githubusercontent.com/35248527/80868176-6214c700-8ccb-11ea-982b-ed3b6c584e9f.png&gt;&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='xinfangliu' date='2020-05-10T06:40:31Z'>
		I have this same issue as well, it happens regardless of memory pinning
		</comment>
		<comment id='3' author='xinfangliu' date='2020-05-10T08:42:03Z'>
		I am using this as a temporary fix
&lt;denchmark-code&gt;if __name__ == "__main__":
    args = parse_arguments()
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, args.gpus))
    args.gpus = list(range(len(args.gpus)))
    ...
    trainer = pl.Trainer(gpus=args.gpus)
&lt;/denchmark-code&gt;

However, if I call torch.cuda.device_count() beforehand like so, the issue still occurs. There might be something that occurred when device_count is called.
&lt;denchmark-code&gt;    if len(args.gpus) != torch.cuda.device_count():
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, args.gpus))
        args.gpus = list(range(len(args.gpus)))
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='xinfangliu' date='2020-06-26T14:07:42Z'>
		you shall user batch side finder
		</comment>
		<comment id='5' author='xinfangliu' date='2020-06-26T14:08:17Z'>
		can you try running on master? and also decrease your batch size?
		</comment>
		<comment id='6' author='xinfangliu' date='2020-06-26T14:22:47Z'>
		
can you try running on master? and also decrease your batch size?

It has no matter with the batch size, I  solved the problem with torch.cuda.set_device(6)
		</comment>
	</comments>
</bug>