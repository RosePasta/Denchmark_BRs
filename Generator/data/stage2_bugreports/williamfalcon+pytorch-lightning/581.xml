<bug id='581' author='YonyBresler' open_date='2019-12-03T23:27:20Z' closed_time='2020-03-26T14:45:37Z'>
	<summary>GPU usage decreasing with epochs</summary>
	<description>
I'm having a similar issue to &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/373&gt;#373&lt;/denchmark-link&gt;
 with gradual slowing down while training.
The model works fine but gradually with each epoch GPU utilization is decreasing, and the time needed to complete it increases.
&lt;denchmark-link:https://user-images.githubusercontent.com/24940683/70098507-1d8cde80-15fa-11ea-88ce-f8f473bfd540.png&gt;&lt;/denchmark-link&gt;

GPU memory usage is also slowly decreasing at the same rate, so I don't think there's a memory leak, and i/o usage is roughly constant at 1200 K/s, well below the sustained speed my HD will support. CPU usage is 20-40%, total CPU memory is &lt;30%, and GPU usage, power and temperature are all in regime where there should be no throttling, so I don't think it's a hardware issue.
Stopping the run, letting things cool down, and restarting with loaded weights doesn't revert speed, and continues with the same slower speed.
I don't have these issues if I replace lightning with a basic training loop.
Has there been any other indication as to what might be causing this?
	</description>
	<comments>
		<comment id='1' author='YonyBresler' date='2019-12-04T16:52:32Z'>
		On closer look it seems I/O does increase as epochs roll on, trying to figure out what is writing to disk so much since aside from default logging my model doesn't write anything, and epochs are long so weights shouldn't be an issue either.
		</comment>
		<comment id='2' author='YonyBresler' date='2019-12-04T16:57:46Z'>
		maybe the tensorboard logger is doing that? itâ€™s the only thing that would write
		</comment>
		<comment id='3' author='YonyBresler' date='2019-12-04T17:01:38Z'>
		Thanks, the writes aren't to the local folder during an epoch, but it's running within an nvidia-docker, so there's some filesystem writes but it's a bit obscured where exactly.
		</comment>
		<comment id='4' author='YonyBresler' date='2019-12-04T21:41:14Z'>
		The I/O issue seemed to be an issue between tqdm and screen, running the model outside of screen eliminates the high IO load, back to normal &lt; 1M/s.
The gradual slowing with epochs remains, so I think there's another issue, I'll try to test more to narrow it down, but if you have any suggestions, I'm happy to follow up if I can.
		</comment>
		<comment id='5' author='YonyBresler' date='2019-12-04T22:06:28Z'>
		got it. you can turn off thr prog bar in the trainer as an arg
		</comment>
		<comment id='6' author='YonyBresler' date='2019-12-05T15:47:26Z'>
		Avoiding screen seems to solve the I/O issue, but to be safe I restarted with turned off progress bar, there is now almost zero writing to disk, yet GPU usage drops to 30% during epochs (starts at 70%!), though interestingly during validation step usage jumps back up to 60%, so it must be something related to back-propogation (since I evaluate the same loss during validation).
To be clear, restarting from a saved run with version=X will resume at the reduced 30% utilization.
Any other ideas for what I can test? I'm testing lightning for us to use it, other than this issue everything else works great, would like to resolve it so we can migrate.
		</comment>
		<comment id='7' author='YonyBresler' date='2019-12-08T23:30:51Z'>
		I have also experienced the problem of training slowing per epoch. I found a few causes to be aware of ...
Pytorch tensors will accumulate history so you have to be careful to NOT create a tensor outside of training/val/test loops that accumulate history, or to append to evergrowing lists unnecessarily. See this for discussion: &lt;denchmark-link:https://discuss.pytorch.org/t/training-gets-slow-down-by-each-batch-slowly/4460&gt;https://discuss.pytorch.org/t/training-gets-slow-down-by-each-batch-slowly/4460&lt;/denchmark-link&gt;

This was not my problem, unfortunately.
In the above discussion, the recommendation is also made to periodically make this call:
torch.cuda.empty_cache()
I put this in on_epoch_start() - and almost all of my problems disappeared.
However, I still have an important amount of this when using torch.optim.lr_scheduler.ReduceLROnPlateau(). Each time the learning rate is reduced the epoch time goes up by about 2 seconds in my case. The increase starts following the first time lr is reduced, persists, and increases again each time the lr is reduced.
I have not tried any other lr schedulers, and am just starting to track down if there is something in ReduceLROnPlateau() or lightning. If I learn any more, I will post here.
		</comment>
		<comment id='8' author='YonyBresler' date='2019-12-08T23:55:05Z'>
		empty cache will make your code really slow... we make sure to release all tensors, so look where you store any tensors to make sure you do too
		</comment>
		<comment id='9' author='YonyBresler' date='2019-12-09T00:05:09Z'>
		Got it. Have removed - def a few seconds quicker per epoch. And do not see the slowdown EXCEPT with lr scheduler as above. I looked at the scheduler code and nothing jumps out. All tensors are properly managed. thx
		</comment>
		<comment id='10' author='YonyBresler' date='2019-12-09T00:07:09Z'>
		Thanks for the info, I'll also try to see if I have any tensors stored outside that loop that are getting appended to and see if that resolves this issue
		</comment>
		<comment id='11' author='YonyBresler' date='2019-12-09T15:03:36Z'>
		The pytorch optimizer documentation says that optimizers should be constructed after moving the model to gpu. Inspection of dp_mixin.py shows them being constructed before, however. Could this be the problem?
		</comment>
		<comment id='12' author='YonyBresler' date='2019-12-13T16:46:54Z'>
		I have leaned into this a bit, and I am reasonably sure that this issue is in lighting somewhere. I don't know where, unfortunately.
What I did was take the exact same model, and constructed a trainer for it. Sort of baby lightning.  I get essentially the same results as with lightning, but do not see the slowdown. I use the same data, and data loaders, the same logging - though directly to tensorboard using tensorboardX. Even the same call structure with the training loop calling into the model for training_step, validation_step, etc.
So, it's not in pytorch and it's not in logging. I did not use tqdm, but it is hard to imagine that's the problem.
So - sorry I don't have and answer, but I feel pretty confident that this is not the model, or pytorch, or tensorboard.
s
		</comment>
		<comment id='13' author='YonyBresler' date='2019-12-16T14:58:04Z'>
		I have also done a similar exercise and the code does not slow down with epochs when using it with my own training loop. Hope this can be further investigated and resolved
		</comment>
		<comment id='14' author='YonyBresler' date='2020-01-04T14:12:02Z'>
		hi &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 , I'm experiencing a similar GPU utilization issue, dropping from 20% to 10% in ~200k steps. Is there any recommended way to set up performance profiling in pytorch-lighting?
		</comment>
		<comment id='15' author='YonyBresler' date='2020-01-04T15:16:26Z'>
		After setting logger=False, the issue has been mitigated. Previously the save_dir of TestTubeLogger was set to a path in a shared folder, hence the high-frequency logging file I/O becomes the bottleneck.
		</comment>
		<comment id='16' author='YonyBresler' date='2020-01-16T21:41:25Z'>
		setting logger to False did not work for me. still losing ~1-1.5 secs per epoch. does NOT appear to be related to lr scheduler at this point.
		</comment>
		<comment id='17' author='YonyBresler' date='2020-01-18T00:16:36Z'>
		My issues are also not related to scheduler, logger or hardware.
My best guess: Something in the model is saving gradients that aren't needed, though I can't figure out what. Trying another (simpler) model on the same machine and libraries shows no slowdown.
		</comment>
		<comment id='18' author='YonyBresler' date='2020-01-21T12:50:45Z'>
		&lt;denchmark-link:https://github.com/YonyBresler&gt;@YonyBresler&lt;/denchmark-link&gt;
 any updates?
		</comment>
		<comment id='19' author='YonyBresler' date='2020-01-21T18:24:26Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 haven't been able to narrow down the issue, beyond the following:
It's not logging or hardware related, but with a complex model, something is causing a slowdown per epoch during training, likely some graph history being accumulated, as suggested above.
Resuming from a saved run will 'remember' the slowdown, changing from lightning to a simple custom training loop eliminates the issue, and using a simpler model with lightning doesn't have this issue on the same hardware &amp; software stack.
Unfortunately I can't paste my complex model here, but I'll try to look through a debugger and see if I can dig up anything else, but it sounds like &lt;denchmark-link:https://github.com/sneiman&gt;@sneiman&lt;/denchmark-link&gt;
 is running into the exact same issue as I am, so I hope it should be reproducible.
		</comment>
		<comment id='20' author='YonyBresler' date='2020-01-28T04:22:48Z'>
		FWIW - this is not model related, as I have run the exact same model with my own training loop without seeing any loss of performance. I have been doing some work comparing dp and ddp, and my current effort shows the loss of performance ONLY with dp.
		</comment>
		<comment id='21' author='YonyBresler' date='2020-03-07T00:27:58Z'>
		how's this going?
dp is very slow btw compared with ddp
		</comment>
		<comment id='22' author='YonyBresler' date='2020-03-07T15:20:44Z'>
		I haven't seen is issue in .60 or master.
		</comment>
		<comment id='23' author='YonyBresler' date='2020-03-26T14:45:37Z'>
		Any update here? feel free t reopen if needed... ðŸ¤–
		</comment>
		<comment id='24' author='YonyBresler' date='2020-10-22T02:55:31Z'>
		&lt;denchmark-link:https://github.com/sneiman&gt;@sneiman&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
  Did you find any solutions for this issue? I tried all those things you mentioned above but GPU utilization gradually drop down every iterations.
		</comment>
	</comments>
</bug>