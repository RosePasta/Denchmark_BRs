<bug id='1562' author='kevinc13' open_date='2020-04-22T16:31:41Z' closed_time='2020-05-12T04:08:08Z'>
	<summary>test_dataloader called during check_testing_model_configuration</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When testing a model that has been loaded using the  Trainer flag (note:  has  been called yet in this case), the  method &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/c1c6e3b6c988c690f9e2cd5726da4ab3402dbd03/pytorch_lightning/trainer/trainer.py#L1026&gt;calls&lt;/denchmark-link&gt;
 , which results in undefined attribute errors if the test dataloader depends on stuff set during  (which is called at the beginning of )
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Initialize a Trainer like trainer = Trainer(model, resume_from_checkpoint="...", ...) (make sure .fit is not called yet)
Run trainer.test()
If test dataloader depends on prepare_data, errors will be thrown that trace back to check_testing_model_configuration

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

This seems like It could be fixed with a simple change--remove the  call from this &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/c1c6e3b6c988c690f9e2cd5726da4ab3402dbd03/pytorch_lightning/trainer/trainer.py#L1026&gt;line&lt;/denchmark-link&gt;
 and replace  with  call . I can submit a PR if this solution is acceptable.
	</description>
	<comments>
	</comments>
</bug>