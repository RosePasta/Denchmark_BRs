<bug id='2635' author='ibeltagy' open_date='2020-07-17T19:38:29Z' closed_time='2020-07-28T20:29:47Z'>
	<summary>Loss value in the progress bar is wrong when `accumulate_grad_batches &amp;gt; 1`</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

The loss value reported in the progress bar is the_correct_loss_value / accumulate_grad_batches, so this value is wrong when accumulate_grad_batches &gt; 1.
This is happening because &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.8.5/pytorch_lightning/trainer/training_loop.py#L799&gt;here&lt;/denchmark-link&gt;
 the loss is divided by , then &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.8.5/pytorch_lightning/trainer/training_loop.py#L663&gt;here&lt;/denchmark-link&gt;
 the running loss is the  of these losses.
To fix this, either remove the first line (no division by accumulate_grad_batches) or replace mean with sum in the second line.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;


Train any model with accumulate_grad_batches=1 and note the loss value reported in the progress bar
Train the same model with accumulate_grad_batches=2 and half the batch size,  now the loss value in the progress bar will be half the value from step 1.

&lt;denchmark-h:h3&gt;Expected Behaviour&lt;/denchmark-h&gt;

The loss in steps (1) and (2) should be the same
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

pytorch-lightning==v0.8.5
	</description>
	<comments>
		<comment id='1' author='ibeltagy' date='2020-07-19T20:02:40Z'>
		duplicate of &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2569&gt;#2569&lt;/denchmark-link&gt;
?
		</comment>
	</comments>
</bug>