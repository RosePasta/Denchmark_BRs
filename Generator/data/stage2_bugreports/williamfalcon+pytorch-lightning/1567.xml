<bug id='1567' author='ashwinb' open_date='2020-04-23T01:11:54Z' closed_time='2020-04-26T14:57:16Z'>
	<summary>Samplers are auto-added in DDP with no mechanism to override</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Lightning automatically adds DistributedSampler when you turn on ddp, ddp2 or TPU: 


pytorch-lightning/pytorch_lightning/trainer/data_loading.py


         Line 86
      in
      17f58d2






 def auto_add_sampler(self, dataloader: DataLoader, train: bool) -&gt; DataLoader: 





This seems to be a recent change.
This is surprising behavior and not always something that's warranted. For example, it is common (at least in several of our large scale vision trainers) for each worker to read a specific partition of a large warehouse table. In this case, the automatic addition of the DistributedSampler will only provide access to a portion of the loaded data, which is unintended.
Worse, there's no mechanism at all to override this.
&lt;denchmark-h:h3&gt;Possible fixes&lt;/denchmark-h&gt;


At the very least, provide some way to override this functionality
If the dataset is iterable-style, never auto-add a Sampler

	</description>
	<comments>
		<comment id='1' author='ashwinb' date='2020-04-23T01:12:33Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='ashwinb' date='2020-04-23T01:19:55Z'>
		on master we have a flag to disable this
		</comment>
		<comment id='3' author='ashwinb' date='2020-04-23T01:22:17Z'>
		Ah, thanks &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
. What about my comment around "if the dataset is iterable-style, never auto-add a distributed sampler" -- how would that even work when you don't have indexes for the dataset?
		</comment>
		<comment id='4' author='ashwinb' date='2020-04-23T01:58:58Z'>
		&lt;denchmark-link:https://github.com/tullie&gt;@tullie&lt;/denchmark-link&gt;
 suggested defaulting to false. Maybe we should do that?  i guess turned off is the intuitive behavior?
		</comment>
		<comment id='5' author='ashwinb' date='2020-04-23T02:01:25Z'>
		‚Äú If the dataset is iterable-style, never auto-add a Sampler‚Äù
If we had the flag to default off, then this would be taken care of no? that way we can only add when user wants it?
&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
 thoughts?
		</comment>
		<comment id='6' author='ashwinb' date='2020-04-23T02:12:05Z'>
		Yes default=off works ‚Äî and I would prefer that.
&lt;denchmark-link:#&gt;‚Ä¶&lt;/denchmark-link&gt;


Sent from my iPhone

On Apr 22, 2020, at 7:01 PM, William Falcon &lt;notifications@github.com&gt; wrote:

Ôªø

‚Äú If the dataset is iterable-style, never auto-add a Sampler‚Äù

If we had the flag to default off, then this would be taken care of no? that way we can only add when user wants it?

@srush&lt;https://github.com/srush&gt; thoughts?

‚Äî
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub&lt;#1567 (comment)&gt;, or unsubscribe&lt;https://github.com/notifications/unsubscribe-auth/AAAEXPXRVJRCECH2RMTO2PTRN6OQHANCNFSM4MOUPIXA&gt;.

		</comment>
		<comment id='7' author='ashwinb' date='2020-04-23T09:43:39Z'>
		Yeah I still think the add sampler argument is confusing for anyone that has used pytorch before. There's an expectation that when you explicitly pass a sampler into the dataloader it will use that sampler.
One way to make it automatic while keeping it intuitive to the user would be to add a LightningDataLoader which automatically does things like this. That way the user is buying into a new type of data loader with different expectations when specifying the type.
The "never add sampler when iterable-style dataset" rule makes sense for this specific issue but i'm concerned it doesn't solve the underlying surprising behavior.
		</comment>
		<comment id='8' author='ashwinb' date='2020-04-23T22:51:59Z'>
		+1 on default. I like the idea of the LightningDataLoader potentially, but it's a new abstraction so I'd recommend be very stingy with those :)
		</comment>
		<comment id='9' author='ashwinb' date='2020-04-24T01:12:30Z'>
		yeah, i don't love adding new abstractions.I really  want to keep it pure pytorch or we'll just end up with another hard to read library haha.
so we all agree that default=False is best for this flag?
&lt;denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors&gt;@PyTorchLightning/core-contributors&lt;/denchmark-link&gt;
 thoughts?
&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;
?
I do like the ability to set gpus=2 and everything just work though. I'm torn but leaning on default=False
		</comment>
		<comment id='10' author='ashwinb' date='2020-04-24T07:19:20Z'>
		I wouldn't introduce another data loader here. To make this work, the user would always have to use this, whereas now he can use a lightning-independent torch loader or even a custom one.
		</comment>
		<comment id='11' author='ashwinb' date='2020-04-24T12:15:14Z'>
		&lt;denchmark-link:https://twitter.com/PyTorchLightnin/status/1253656336329519108?s=20&gt;https://twitter.com/PyTorchLightnin/status/1253656336329519108?s=20&lt;/denchmark-link&gt;

		</comment>
		<comment id='12' author='ashwinb' date='2020-04-26T14:57:13Z'>
		ok, in 0.7.4 we made these changes:

If dataset is iterable we don't mess with samplers.
Added a flag to enable adding auto_samplers.
The flag is set to True by default. So far this seems to be the consensus, but by 0.7.5 we will know if this was the right choice or not based on the number of GH issues and complaints we hear.
If it turns out to cause more problems than its worth, we will default the flag to False.

Thank you all for bringing this up!
		</comment>
		<comment id='13' author='ashwinb' date='2020-04-26T16:23:58Z'>
		Thank you!
		</comment>
	</comments>
</bug>