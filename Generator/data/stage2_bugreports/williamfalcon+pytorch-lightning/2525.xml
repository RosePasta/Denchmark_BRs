<bug id='2525' author='blakeliu' open_date='2020-07-06T08:22:20Z' closed_time='2020-08-03T21:52:50Z'>
	<summary>How to disable Detected KeyboardInterrupt</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

I use pycharm
Enter F5 key or click pycharm debug

&lt;denchmark-code&gt;Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 27/35 [00:12&lt;00:03,  2.08it/s, loss=6.617, v_num=16, train_loss=5.91]/home/blake/anaconda3/envs/torch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  warnings.warn(*args, **kwargs)
Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 27/35 [00:13&lt;00:03,  2.06it/s, loss=6.617, v_num=16, train_loss=5.91]
See you again!
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;    trainer = Trainer(logger=tb_logger,  
                      gpus=cfg.NUM_GPUS,  
                      max_epochs=cfg.SOLVER.MAX_STEPS,  
                      checkpoint_callback=checkpoint_callback,  
                      distributed_backend = 'dp',  
                      resume_from_checkpoint= None if cfg.MODEL.LANDMARK_DET.PRE_TRAIN_DIR == 'None' else cfg.MODEL.LANDMARK_DET.PRE_TRAIN_DIR,  
                      num_sanity_val_steps=0)  
    #start training  
    trainer.fit(train_model) 
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- GeForce GTX 1080
- GeForce GTX 1080
- available:         True
- version:           10.0
Packages:
- numpy:             1.18.1
- pyTorch_debug:     False
- pyTorch_version:   1.4.0+cu100
- pytorch-lightning: 0.8.4-dev
- tensorboard:       2.2.2
- tqdm:              4.46.1
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.7
- version:           #41~16.04.1-Ubuntu SMP Wed Oct 10 20:16:04 UTC 2018

	</description>
	<comments>
		<comment id='1' author='blakeliu' date='2020-07-06T08:23:16Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='blakeliu' date='2020-07-10T00:47:23Z'>
		&lt;denchmark-link:https://github.com/blakeliu&gt;@blakeliu&lt;/denchmark-link&gt;
  try:
&lt;denchmark-code&gt;pip install pytorch-lightning==0.8.5rc1
&lt;/denchmark-code&gt;

We can re-open if it is not fixed.
For what it's worth, i use pycharm and don't run into this.
		</comment>
		<comment id='3' author='blakeliu' date='2020-07-10T07:08:22Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;

Thanks for your advise.
&lt;denchmark-code&gt;git clone https://github.com/PyTorchLightning/pytorch-lightning.git
git checkout 0.8.5
&lt;/denchmark-code&gt;

I debug pytorch-lightning/pl_examples/basic_examples/gpu_template.py  by pycharm.
&lt;denchmark-code&gt;def run_cli():
    # ------------------------
    # TRAINING ARGUMENTS
    # ------------------------
    # these are project-wide arguments
    root_dir = os.path.dirname(os.path.realpath(__file__))
    parent_parser = ArgumentParser(add_help=False)

    # each LightningModule defines arguments relevant to it
    parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)
    parser = Trainer.add_argparse_args(parser)
    parser.set_defaults(gpus=1, num_sanity_val_steps=0)
    args = parser.parse_args()

    # ---------------------
    # RUN TRAINING
    # ---------------------
    main(args)
&lt;/denchmark-code&gt;

Got the same  results.
&lt;denchmark-code&gt;PU available: True, used: True
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
TPU available: False, using: 0 TPU cores
CUDA_VISIBLE_DEVICES: [0]
CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type        | Params | In sizes   | Out sizes 
--------------------------------------------------------------------
0 | c_d1      | Linear      | 39 M   | [2, 784]   | [2, 50000]
1 | c_d1_bn   | BatchNorm1d | 100 K  | [2, 50000] | [2, 50000]
2 | c_d1_drop | Dropout     | 0      | [2, 50000] | [2, 50000]
3 | c_d2      | Linear      | 500 K  | [2, 50000] | [2, 10]   

  | Name      | Type        | Params | In sizes   | Out sizes 
--------------------------------------------------------------------
0 | c_d1      | Linear      | 39 M   | [2, 784]   | [2, 50000]
1 | c_d1_bn   | BatchNorm1d | 100 K  | [2, 50000] | [2, 50000]
2 | c_d1_drop | Dropout     | 0      | [2, 50000] | [2, 50000]
3 | c_d2      | Linear      | 500 K  | [2, 50000] | [2, 10]   
Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 937/1095 [00:18&lt;00:03, 50.36it/s, loss=0.163, v_num=13]/home/blake/PycharmProjects/dl/pytorch-lightning/pytorch_lightning/utilities/distributed.py:25: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  warnings.warn(*args, **kwargs)
Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 937/1095 [00:18&lt;00:03, 49.67it/s, loss=0.163, v_num=13]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='blakeliu' date='2020-07-10T11:18:21Z'>
		&lt;denchmark-link:https://github.com/jeremyjordan&gt;@jeremyjordan&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='blakeliu' date='2020-07-10T16:04:29Z'>
		I'm sorry there's not much information in the issue. What is the expected behavior? Why do you wish to disable the catch for a KeyboardInterrupt?
		</comment>
		<comment id='6' author='blakeliu' date='2020-08-03T21:52:50Z'>
		&lt;denchmark-link:https://github.com/blakeliu&gt;@blakeliu&lt;/denchmark-link&gt;
 feel free to reopen if needed and explain the problem and what needs fixing.
		</comment>
	</comments>
</bug>