<bug id='125' author='bighuang624' open_date='2019-08-16T08:01:44Z' closed_time='2019-08-17T02:45:16Z'>
	<summary>AttributeError: 'xxx' object has no attribute 'tng_dataloader'</summary>
	<description>
Describe the bug
I modified the template to implement my model and task. In the process, I ran it several times and correctly. However, when I revised the evaluation metrics, the program suddenly started to report errors: AttributeError: 'DSANet' object has no attribute 'tng_dataloader'. However, I've defined the def tng_dataloader(self) method. Although I delete what I have written and make sure all is right, the error still exists.
Code
The error report:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "single_cpu_trainer.py", line 112, in &lt;module&gt;
    main(hyperparams)
  File "single_cpu_trainer.py", line 83, in main
    trainer.fit(model)
  File "E:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\pytorch_lightning\models\trainer.py", line 567, in fit
    self.__run_pretrain_routine(model)
  File "E:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\pytorch_lightning\models\trainer.py", line 742, in __run_pretrain_routine
    self.get_dataloaders(ref_model)
  File "E:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\pytorch_lightning\models\trainer.py", line 469, in get_dataloaders
    self.tng_dataloader = model.tng_dataloader
  File "E:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\torch\nn\modules\module.py", line 591, in __getattr__
    type(self).__name__, name))
AttributeError: 'DSANet' object has no attribute 'tng_dataloader'
&lt;/denchmark-code&gt;

The code:
class DSANet(LightningModule):
    # ...
    def __dataloader(self, train):
        # init data generators
        set_type = 'train' if train else 'test'
        dataset = MTSFDataset(window=self.hparams.window, horizon=self.hparams.horizon, data_name=self.hparams.data_name, set_type=set_type, data_dir=self.hparams.data_dir)
        # when using multi-node we need to add the datasampler
        train_sampler = None
        batch_size = self.hparams.batch_size
        try:
            if self.on_gpu:
                train_sampler = DistributedSampler(dataset, rank=self.trainer.proc_rank)
                batch_size = batch_size // self.trainer.world_size # scale batch size
        except Exception as e:
            pass
        should_shuffle = train_sampler is None
        loader = DataLoader(
            dataset=dataset,
            batch_size=batch_size,
            shuffle=should_shuffle,
            sampler=train_sampler
        )
        return loader

    @pl.data_loader
    def tng_dataloader(self):
        print('tng data loader called')
        return self.__dataloader(train=True)

    @pl.data_loader
    def val_dataloader(self):
        print('val data loader called')
        return self.__dataloader(train=False)

    @pl.data_loader
    def test_dataloader(self):
        print('test data loader called')
        return self.__dataloader(train=False)
I can make a new repo and upload my code if it is needed.
Expected behavior
Run correctly.
Screenshots
command line:
&lt;denchmark-link:https://user-images.githubusercontent.com/18595460/63152280-0a9bd180-c03e-11e9-8c0d-0cbf9191e852.png&gt;&lt;/denchmark-link&gt;

Environment:

OS: windows 10.0, 17134
conda version (no venv): conda 4.7.11
PyTorch version: 1.2.0
Lightning version: 0.4.6
Test-tube version: 0.6.9

Additional context
What have you tried:  I used print and found that it seemed to be something wrong around  loader = DataLoader( dataset=dataset, batch_size=batch_size, shuffle=should_shuffle, sampler=train_sampler ). And I found that val_dataloader and test_dataloader are also None when printing info about my class. However, I still don't know why.
	</description>
	<comments>
		<comment id='1' author='bighuang624' date='2019-08-16T11:27:42Z'>
		the bug seems to be that Lightning isn’t showing the exception inside your loader. the dataloader code has an exception but Lightning shows it as a missing loader.
might have to add a try catch to the decorator in lightning.
wrap your loader call in a try catch to see if the exception shows up
		</comment>
		<comment id='2' author='bighuang624' date='2019-08-17T02:45:15Z'>
		
the bug seems to be that Lightning isn’t showing the exception inside your loader. the dataloader code has an exception but Lightning shows it as a missing loader.
might have to add a try catch to the decorator in lightning.
wrap your loader call in a try catch to see if the exception shows up

Yes, it works. Thanks for your help.
		</comment>
		<comment id='3' author='bighuang624' date='2020-04-13T18:33:12Z'>
		&lt;denchmark-link:https://github.com/bighuang624&gt;@bighuang624&lt;/denchmark-link&gt;
  Can you put the code with the try catch in the decorator file? Because i am having the same error and I am not able to fix it. Please do confirm if the file is the one in the core folder of pytorch lightning? Thanks in advance.
		</comment>
		<comment id='4' author='bighuang624' date='2020-04-14T02:36:26Z'>
		
@bighuang624 Can you put the code with the try catch in the decorator file? Because i am having the same error and I am not able to fix it. Please do confirm if the file is the one in the core folder of pytorch lightning? Thanks in advance.

Sorry. As it was an early problem, I can't guarantee that what I remember is right. But I think I just added a try catch around
&lt;denchmark-code&gt;loader = DataLoader(
            dataset=dataset,
            batch_size=batch_size,
            shuffle=should_shuffle,
            sampler=train_sampler
        )
&lt;/denchmark-code&gt;

and found the problem was from my own code. I didn't revise the decorator file.
		</comment>
		<comment id='5' author='bighuang624' date='2020-04-14T15:55:59Z'>
		&lt;denchmark-link:https://github.com/bighuang624&gt;@bighuang624&lt;/denchmark-link&gt;
  Thank you. Well, this is the error I am getting btw,
&lt;denchmark-code&gt;(Python36) C:\Program Files (x86)\DSAnet\DSANet&gt;python single_cpu_trainer.py
Traceback (most recent call last):
  File "single_cpu_trainer.py", line 14, in &lt;module&gt;
    from model import DSANet
  File "C:\Program Files (x86)\DSAnet\DSANet\model.py", line 158, in &lt;module&gt;
    class DSANet(LightningModule):
  File "C:\Program Files (x86)\DSAnet\DSANet\model.py", line 369, in DSANet
    @ptl.data_loader
AttributeError: module 'pytorch_lightning' has no attribute 'data_loader'
&lt;/denchmark-code&gt;

I tried to fix it, but I am not able to. Initially I had trouble with the versions, now I am getting these kind of bugs. Any help would be appreciated. Thanks.
		</comment>
		<comment id='6' author='bighuang624' date='2020-09-16T10:13:19Z'>
		
@bighuang624 Thank you. Well, this is the error I am getting btw,
(Python36) C:\Program Files (x86)\DSAnet\DSANet&gt;python single_cpu_trainer.py
Traceback (most recent call last):
  File "single_cpu_trainer.py", line 14, in &lt;module&gt;
    from model import DSANet
  File "C:\Program Files (x86)\DSAnet\DSANet\model.py", line 158, in &lt;module&gt;
    class DSANet(LightningModule):
  File "C:\Program Files (x86)\DSAnet\DSANet\model.py", line 369, in DSANet
    @ptl.data_loader
AttributeError: module 'pytorch_lightning' has no attribute 'data_loader'

I tried to fix it, but I am not able to. Initially I had trouble with the versions, now I am getting these kind of bugs. Any help would be appreciated. Thanks.

&lt;denchmark-link:https://github.com/Josepharol&gt;@Josepharol&lt;/denchmark-link&gt;
 have you managed to fix it?
		</comment>
		<comment id='7' author='bighuang624' date='2020-09-17T02:10:26Z'>
		&lt;denchmark-link:https://github.com/lorrp1&gt;@lorrp1&lt;/denchmark-link&gt;
 Thanks for reaching out. My python version is 3.6.10 and other requirements are as per the author's guidance. If you have older versions of other packages, please uninstall and reinstall, then it should work. It would be better if you have a very small dataset to test your code.
Hope this helps.
		</comment>
	</comments>
</bug>