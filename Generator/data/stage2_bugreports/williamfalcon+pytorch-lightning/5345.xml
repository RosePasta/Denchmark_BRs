<bug id='5345' author='bnaman50' open_date='2021-01-04T05:38:29Z' closed_time='2021-01-05T15:54:02Z'>
	<summary>Error with `load_from_checkpoint`</summary>
	<description>
I am trying to fine-tune a language model and facing some issues with loading the model from the saved checkpoint.
I have defined my own model which takes in  as the input as is suggested in the documentation &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html&gt;here&lt;/denchmark-link&gt;
.
&lt;denchmark-code&gt;class FineTuner(pl.LightningModule):
    def __init__(self, hparams):  # pass all the arguments
        super(FineTuner, self).__init__()
        self.hparams = hparams
&lt;/denchmark-code&gt;

After I am done with the fine-tuning, I try to load the model from saved checkpoint in a separate file as follows -
&lt;denchmark-code&gt;from model_file import FineTuner
model = FineTuner.load_from_checkpoint(ckpt_path)
&lt;/denchmark-code&gt;

but it gives me the following error -
AttributeError: 'dict' object has no attribute 'tokenizer_name'
I tried to look into the source code and found that the error is happening at this &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/saving.py#L192&gt;line&lt;/denchmark-link&gt;
.
I believe the error is occurring because the aforementioned line in  file expects a  which is different from my model definition.
It would be great if you could solve this issue as I have seen many others are also having similar issues with model loading.
&lt;denchmark-h:h3&gt;For others who might still be looking for the answer, here is the workaround that I am using right now&lt;/denchmark-h&gt;

(but would still like to use the cleaner official version) -
&lt;denchmark-code&gt;ckpt_path = abs_path('./path/to/ckpt/file')
checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)
model = my_model(checkpoint['hyper_parameters']) #(better solution than explicitly providing all the hyperparameters)
model.load_state_dict(checkpoint['state_dict'])
&lt;/denchmark-code&gt;

If one were to look at the source code, you would notice that they are kind of doing the same steps but with some extra stuff ;)
But hey, who does not like one-liners.
I am using version version 1.0.4.
	</description>
	<comments>
		<comment id='1' author='bnaman50' date='2021-01-04T05:39:10Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='bnaman50' date='2021-01-04T08:34:57Z'>
		Hey &lt;denchmark-link:https://github.com/bnaman50&gt;@bnaman50&lt;/denchmark-link&gt;
,
Thanks for reporting this behaviour.
For now, you could change your FineTuner class as follow:
&lt;denchmark-code&gt;class FineTuner(...):
    def __init__(self, *args, arg_1, ...,arg_n, kwarg_1=..., ..., kwarg_n=...,  **kwargs):
&lt;/denchmark-code&gt;

So extra parameters doesn't break the loading.
Best,
T.X
		</comment>
		<comment id='3' author='bnaman50' date='2021-01-04T10:11:55Z'>
		You are seeing this error because you use the old way of setting hyperparameters. The docs warn about deprecation. Generally speaking, you want to replace the line  with the new  and use the constructor format suggested by &lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='4' author='bnaman50' date='2021-01-05T15:54:02Z'>
		closing this. feel free to reopen of needed.
		</comment>
	</comments>
</bug>