<bug id='3752' author='nathanpainchaud' open_date='2020-09-30T19:46:37Z' closed_time='2020-10-05T14:13:24Z'>
	<summary>Default reduction always applied by `Metric`, even when requesting `'none'` reduction</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

Metric reduction doesn't behave the same between the functional and class API when using reduction='none'. The functional API applies no reduction as expected, but the class API seems to apply the default reduction regardless.
I haven't investigated the code yet to find the specific cause of the bug, so I'm not sure how widespread this bug is, but I've encountered it using both the DiceCoefficient and my own implementation of the differentiable dice, inheriting from TensorMetric.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Given a pair of pred and target, I get the following behavior with 3 class + background segmentation data:
&gt;&gt;&gt; from pytorch_lightning.metrics import DiceCoefficient
&gt;&gt;&gt; from pytorch_lightning.metrics.functional import dice_score
&gt;&gt;&gt; DiceCoefficient(reduction="none")(pred, target)
tensor(0.0800)
&gt;&gt;&gt; dice_score(pred, target, reduction="none")
tensor([0.0876, 0.0937, 0.0586], device='cuda:0')
where I would have expected both version to give the same result.
The class API seems to apply the default reduction of 'elementwise_mean' even though I requested 'none', since:
&gt;&gt;&gt; dice_score(x_hat, x, reduction="none").mean()
tensor(0.0800, device='cuda:0')
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Reduction behavior should be consistent between class and functional API, and to behave like the current functional API.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

I just now installed Lightning from Git to ensure that it's not a bug that's already been solved since the last release.

CUDA:
- GPU: TITAN Xp
- available:         True
- version:           10.2
Packages:
- numpy:             1.19.2
- pyTorch_debug:     False
- pyTorch_version:   1.6.0
- pytorch-lightning: 0.9.1rc4
- tqdm:              4.49.0
System:
- OS:                Linux
- architecture: 64bit, ELF
- processor:         x86_64
- python:            3.8.5
- version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020

	</description>
	<comments>
		<comment id='1' author='nathanpainchaud' date='2020-10-05T13:44:51Z'>
		Hi &lt;denchmark-link:https://github.com/nathanpainchaud&gt;@nathanpainchaud&lt;/denchmark-link&gt;
, running your code example on master produces the correct result (your issue was probably solved by PR &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3517&gt;#3517&lt;/denchmark-link&gt;
). Could you please try upgrading?
		</comment>
		<comment id='2' author='nathanpainchaud' date='2020-10-05T14:13:24Z'>
		Hi &lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
. I can confirm that master now runs fine on my end as well. Thanks for the follow up!
		</comment>
	</comments>
</bug>