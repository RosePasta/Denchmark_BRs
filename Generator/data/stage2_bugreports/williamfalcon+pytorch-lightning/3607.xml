<bug id='3607' author='navrudh' open_date='2020-09-22T13:54:25Z' closed_time='2020-12-25T06:18:20Z'>
	<summary>LightningDataModule seems to do some dataloader operations on CPU, which was not the case with LightningModule loader methods</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

While using LightningDataModule as lit_model(datamodule=datamodule) the models waits for some time using 1 CPU core before beginging training, and periodically stops training (every 50 train steps) GPU util goes 0% and 1 CPU core is in use. This behaviour continues till training finishes.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

Create a lightning model which takes a datamodule as input. __init__ containsthis.datamodule=datamodule
In the LightningDataModule I'm using PyTorch's UCF101 dataset

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

class UCF101DataModule(LightningDataModule):
    def setup(self, stage=None):
        if stage == "fit" or stage is None:
            self.train_dataset = datasets.UCF101(
                UCF101_ROOT_PATH,
                UCF101_ANNO_PATH,
                frames_per_clip=5,
                step_between_clips=30,
                num_workers=UCF101_WORKERS,
                train=True,
                fold=self.fold,
            )

    def train_dataloader(self):
        print("Train Dataloader Called")
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            num_workers=DATALOADER_WORKERS,
            collate_fn=custom_collate,
            shuffle=True,
        )

class LitModel(LightningModule):
    def __init__(datamodule):
        self.datamodule = datamodule
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Training loop should start immediately
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

GeForce RTX 2080 Ti


available:         True
version:           10.2


Packages:

numpy:             1.18.1
pyTorch_debug:     False
pyTorch_version:   1.6.0
pytorch-lightning: 0.9.0
tqdm:              4.46.0


System:

OS:                Linux
architecture:

64bit
ELF


processor:         x86_64
python:            3.8.3
version:          113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020



	</description>
	<comments>
		<comment id='1' author='navrudh' date='2020-09-22T13:55:16Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='navrudh' date='2020-09-22T14:13:09Z'>
		&lt;denchmark-link:https://github.com/nateraw&gt;@nateraw&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='navrudh' date='2020-09-22T14:26:25Z'>
		
(every 50 train steps)

&lt;denchmark-link:https://github.com/navrudh&gt;@navrudh&lt;/denchmark-link&gt;
 That sounds suspiciously like the . Can you confirm you are not doing any expensive custom logging?  Try reducing row_log_interval if you're not sure and see what happens
		</comment>
		<comment id='4' author='navrudh' date='2020-09-29T10:58:44Z'>
		Logged values: train_loss, val_loss, images (using self.logger.experiment.add_image roughly every 100 steps).
Tried Trainer(row_log_interval=1, ...) and am still observing the same behaviour.
Removed image logging and still observed this behaviour.
		</comment>
		<comment id='5' author='navrudh' date='2020-09-29T13:45:53Z'>
		How many workers are you using? If the training stops and gpu usage goes to 0, it means your dataloading can't keep up. Increase the number of workers. Make sure the cpu is not the bottleneck.
		</comment>
		<comment id='6' author='navrudh' date='2020-09-29T18:10:55Z'>
		I'm using number of workers = 12.
Also, if I instead move the code to the val_loader, train_loader methods of LightningModule training starts straightaway without this delay.
		</comment>
		<comment id='7' author='navrudh' date='2020-10-06T04:17:01Z'>
		&lt;denchmark-link:https://github.com/navrudh&gt;@navrudh&lt;/denchmark-link&gt;
 I don't fully understand how to reproduce this. Can you provide your training script and LightningModule?
&lt;denchmark-link:https://colab.research.google.com/drive/1JPqcv8hexwdByACFN5S8UyPi5G4Ielxq?usp=sharing&gt;https://colab.research.google.com/drive/1JPqcv8hexwdByACFN5S8UyPi5G4Ielxq?usp=sharing&lt;/denchmark-link&gt;

		</comment>
		<comment id='8' author='navrudh' date='2020-10-06T18:02:49Z'>
		I've updated the code. I took a model from the internet since I can't share the model that I'm currently using.
		</comment>
		<comment id='9' author='navrudh' date='2020-10-06T19:49:17Z'>
		
I've updated the code.

thanks. where can I find it?
		</comment>
		<comment id='10' author='navrudh' date='2020-10-07T15:04:29Z'>
		Sorry, here's the link &lt;denchmark-link:https://colab.research.google.com/drive/1e69KUDeZbNA-eEE76EK0MaiVhOGJsxmY?usp=sharing&gt;https://colab.research.google.com/drive/1e69KUDeZbNA-eEE76EK0MaiVhOGJsxmY?usp=sharing&lt;/denchmark-link&gt;

But I'm not getting the same issue on colab.
		</comment>
		<comment id='11' author='navrudh' date='2020-12-18T04:09:19Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>