<bug id='5312' author='haven-jeon' open_date='2020-12-31T02:59:52Z' closed_time='2021-01-02T13:29:03Z'>
	<summary>Sequential model parallel with multi nodes regarding --num_nodes</summary>
	<description>
&lt;denchmark-h:h2&gt;Sequential model parallel with multi nodes regarding --num_nodes&lt;/denchmark-h&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel&lt;/denchmark-h&gt;

8 MP with 2 DP as follow.
# node 0 
MASTER_ADDR=XXX.XXX.XXX.1 MASTER_PORT=7874 NODE_RANK=0 python train.py --gpus 8 --accelerator ddp  ....  --use_ddp_sequential --num_nodes 2

# node 1
MASTER_ADDR=XXX.XXX.XXX.1 MASTER_PORT=7874 NODE_RANK=1 python train.py --gpus 8 --accelerator ddp  ....  --use_ddp_sequential --num_nodes 2
It produces error as follow.
&lt;denchmark-code&gt;File "/home/hey/.local/lib/python3.7/site-packages/pytorch_lightning/plugins/ddp_sequential_plugin.py", line 233, in _infer_check_num_gpus
    "Pipe currently only supports splitting the module onto all available GPUs"
&lt;/denchmark-code&gt;

So I edited _infer_check_num_gpus as follow.
&lt;denchmark-link:https://github.com/haven-jeon/pytorch-lightning/commit/c96f14470576356c059710ac6288fc6d9a1e22f6&gt;haven-jeon@c96f144&lt;/denchmark-link&gt;

After that, learning proceeds normally.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


PyTorch Version (e.g., 1.0): 1.6.0
OS (e.g., Linux): CentOS 7
How you installed PyTorch (conda, pip, source):  pip
Build command you used (if compiling from source): pip install -U .
Python version: 3.7
CUDA/cuDNN version: 10.1
GPU models and configuration: P40
Any other relevant information:

	</description>
	<comments>
		<comment id='1' author='haven-jeon' date='2020-12-31T03:00:36Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='haven-jeon' date='2020-12-31T12:05:17Z'>
		&lt;denchmark-link:https://github.com/haven-jeon&gt;@haven-jeon&lt;/denchmark-link&gt;
 mind send it as PR? :]
		</comment>
		<comment id='3' author='haven-jeon' date='2021-01-01T02:05:50Z'>
		
@haven-jeon mind send it as PR? :]

Sure! ðŸ˜ƒ
		</comment>
	</comments>
</bug>