<bug id='4665' author='antoinebrl' open_date='2020-11-13T18:50:39Z' closed_time='2020-12-25T06:18:24Z'>
	<summary>Missing output directory when ddp is activated</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug Description&lt;/denchmark-h&gt;

Thanks for the intensive work you put into this library! I might have found a bug recording logging when DDP is used.
When I run my experiments on 1 GPU everything works fine and all generated files are where they should be. However, when I activate DDP there is no lightning_logs and therefore nothing is saved.
&lt;denchmark-h:h3&gt;Reproduce&lt;/denchmark-h&gt;

This is the minimal code to reproduce
import os
import pytorch_lightning as pl
import torchvision
from torchvision.datasets import MNIST
from torchvision import transforms
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

class MyModel(pl.LightningModule):
    def __init__(self, num_classes):
        super().__init__()
        self.save_hyperparameters()
        self.classifier = torchvision.models.resnet18(pretrained=True)
        self.classifier.fc = nn.Linear(self.classifier.fc.in_features, self.hparams.num_classes)

    def forward(self, x):
        x = torch.cat([x, x, x], dim=1)
        return self.classifier(x)

    def training_step(self, batch, batch_nb):
        x, y = batch
        loss = F.cross_entropy(self(x), y)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.02)

def main():
    model = MyModel(num_classes=10)
    dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
    loader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True, num_workers=2)
    trainer = pl.Trainer(gpus=[0,1], accelerator="ddp", max_epochs=2)
    trainer.fit(model, loader)

if __name__ == '__main__':
    main()
This code produces this execution trace:
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
Missing logger folder: /workspace/tmp/lightning_logs              &lt;---- This should not happen!!
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
  | Name       | Type   | Params
--------------------------------------
0 | classifier | ResNet | 11 M
Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [01:25&lt;00:00, 21.94it/s, loss=0.086, v_num=0]
The folder lightning_logs is not created and checkpoints are not saved. If the folder exists before I run the script there is no problem.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

We should expect the logger to behave the same with or without DDP. Which means it should create the destination directory so it can save everything there.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;user@machine# python collect_env_details.py
* CUDA:
        - GPU:
                - Tesla V100-SXM2-16GB
                - Tesla V100-SXM2-16GB
                - Tesla V100-SXM2-16GB
                - Tesla V100-SXM2-16GB
                - Tesla V100-SXM2-16GB
                - Tesla V100-SXM2-16GB
                - Tesla V100-SXM2-16GB
                - Tesla V100-SXM2-16GB
        - available:         True
        - version:           10.2
* Packages:
        - numpy:                1.18.5
        - pyTorch_debug:  False
        - pyTorch_version: 1.6.0
        - pytorch-lightning: 1.0.6
        - tqdm:                   4.48.2
* System:
        - OS:                Linux
        - architecture:
                - 64bit
        - processor:         x86_64
        - python:            3.6.10
        - version:           #119-Ubuntu SMP Tue Sep 8 12:30:01 UTC 2020
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='antoinebrl' date='2020-11-13T18:51:23Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='antoinebrl' date='2020-11-16T00:38:21Z'>
		I ran your script, and the warning is there but the lightning_logs folder along with checkpoints are all created normally.
Though I am running on master, mind giving that a try?
		</comment>
		<comment id='3' author='antoinebrl' date='2020-11-16T10:50:45Z'>
		Thanks for running the code! I tried with v1.0.7rc0 (pip install git+https://github.com/PyTorchLightning/pytorch-lightning.git@1.0.7rc0) and it works as you said. The warning message is still here but the folder is created and checkpoints are saved.
		</comment>
		<comment id='4' author='antoinebrl' date='2020-11-16T12:47:12Z'>
		Awesome! This was probably fixed as part of our logging refactors which hasn't made it into release yet.
		</comment>
		<comment id='5' author='antoinebrl' date='2020-11-16T14:03:02Z'>
		Should the warning message be investigated ?
		</comment>
		<comment id='6' author='antoinebrl' date='2020-11-16T15:58:03Z'>
		&lt;denchmark-link:https://github.com/tchaton&gt;@tchaton&lt;/denchmark-link&gt;
 is this warning intended behavior? I don't think I've seen this warning before the refactors.
		</comment>
		<comment id='7' author='antoinebrl' date='2020-12-18T04:09:14Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>