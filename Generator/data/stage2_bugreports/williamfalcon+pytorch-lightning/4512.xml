<bug id='4512' author='jonas-ricker' open_date='2020-11-04T12:36:44Z' closed_time='2021-01-12T12:15:23Z'>
	<summary>Profiler is not reset after calling trainer.tune() with auto_lr_find=True</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When using SimpleProfiler together with the Learning Rate Finder, only the timings from the call to train within the Learning Rate Finder are logged. For the actual training, no timings are recorded.
&lt;denchmark-h:h2&gt;Please reproduce using [the BoringModel and post here]&lt;/denchmark-h&gt;

&lt;denchmark-link:https://colab.research.google.com/drive/1FUOW-A7gJk1fuR7ODQdMSCQkrkgamtru?usp=sharing&gt;https://colab.research.google.com/drive/1FUOW-A7gJk1fuR7ODQdMSCQkrkgamtru?usp=sharing&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Profiler is reset after trainer.tune() is called OR the profiler is not active at all during tuning.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

Tesla T4


available:         True
version:           10.1


Packages:

numpy:             1.18.5
pyTorch_debug:     False
pyTorch_version:   1.6.0+cu101
pytorch-lightning: 0.10.0
tqdm:              4.41.1


System:

OS:                Linux
architecture:

64bit



processor:         x86_64
python:            3.6.9
version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

Beside this issue, I'm wondering if calling the LR Finder manually like it's suggested &lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lr_finder.html&gt;here&lt;/denchmark-link&gt;
 () might lead to some problems compared to . In the  class there are several things happening before  is called. However, as far as I know the first method is necessary to obtain e.g. the plot.
	</description>
	<comments>
		<comment id='1' author='jonas-ricker' date='2020-11-04T15:06:03Z'>
		Can confirm I can reproduce this, just need some time to fix :) Here is the script that I've built out of the provided colab:
import os

from torch.utils.data import DataLoader, Dataset
import pytorch_lightning as pl
from pytorch_lightning.profiler import SimpleProfiler
import os

from torch.utils.data import DataLoader, Dataset

import pytorch_lightning as pl
from pytorch_lightning.profiler import SimpleProfiler

tmpdir = os.getcwd()


class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len


import torch
from pytorch_lightning import LightningModule


class BoringModel(LightningModule):

    def __init__(self):
        super().__init__()
        self.layer = torch.nn.Linear(32, 2)
        self.lr = 0.1

    def forward(self, x):
        return self.layer(x)

    def loss(self, batch, prediction):
        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))

    def training_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        return {"loss": loss}

    def training_step_end(self, training_step_outputs):
        return training_step_outputs

    def training_epoch_end(self, outputs) -&gt; None:
        torch.stack([x["loss"] for x in outputs]).mean()

    def validation_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        return {"x": loss}

    def validation_epoch_end(self, outputs) -&gt; None:
        torch.stack([x['x'] for x in outputs]).mean()

    def test_step(self, batch, batch_idx):
        output = self.layer(batch)
        loss = self.loss(batch, output)
        self.log('fake_test_acc', loss)
        return {"y": loss}

    def test_epoch_end(self, outputs) -&gt; None:
        torch.stack([x["y"] for x in outputs]).mean()

    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.layer.parameters(), lr=self.lr)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
        return [optimizer], [lr_scheduler]


def test_x(tmpdir):
    # init model
    num_samples = 10000
    model = BoringModel()
    train = RandomDataset(32, num_samples)
    train = DataLoader(train, batch_size=32)
    val = RandomDataset(32, num_samples)
    val = DataLoader(val, batch_size=32)
    test = RandomDataset(32, num_samples)
    test = DataLoader(test, batch_size=32)
    # Initialize a trainer
    trainer = pl.Trainer(
        max_epochs=1,
        progress_bar_refresh_rate=1,
        auto_lr_find=True,
        profiler=SimpleProfiler()
    )

    # determine best learning rate
    trainer.tune(model, train, val)

    # Train the model ‚ö°
    trainer.fit(model, train, val)


test_x(tmpdir)
		</comment>
		<comment id='2' author='jonas-ricker' date='2020-11-17T16:46:54Z'>
		&lt;denchmark-link:https://github.com/SkafteNicki&gt;@SkafteNicki&lt;/denchmark-link&gt;
 mind taking a look?
		</comment>
		<comment id='3' author='jonas-ricker' date='2021-01-05T10:28:54Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>