<bug id='4691' author='FrankXinqi' open_date='2020-11-16T03:57:06Z' closed_time='2020-11-16T05:14:49Z'>
	<summary>batch size behaves strange for ddp</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I am running SimCLR code. I notice the batch size plays an important role for contrastive learning, so I want to increase batch size.
I can run batch_size=64, on a single 2080Ti. When I use 4 GPUs by setting distributed_backend='ddp'. I still have to set batch_size=64, since each GPU is fully occupied by this setting when I checked by nvidia-smi. To be more clear, if I increase batch_size, there will be an error for CUDA out of memory.
However, from what I understand I should be able to set batch_size = 4*64 or so.
&lt;denchmark-h:h2&gt;Please reproduce using [the BoringModel and post here]&lt;/denchmark-h&gt;

I use the SimCLR example provided by pl.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

I should be able to set batch_size = 4*64 or so.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

I am using 1.0.2 version from conda-forge.  I understand the lastest version is .5 or .6, but I am pretty sure there are other bugs, so using the latest version I even cannot start train SimCLR.
Note: Bugs with code are solved faster ! Colab Notebook should be made public !

IDE: PyCharm on Ubuntu

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='FrankXinqi' date='2020-11-16T03:57:47Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='FrankXinqi' date='2020-11-16T05:14:49Z'>
		ddp batch size is per GPU so you are already getting 64*4
		</comment>
	</comments>
</bug>