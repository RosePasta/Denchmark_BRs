<bug id='4324' author='L4zyy' open_date='2020-10-23T16:59:47Z' closed_time='2020-11-01T00:04:26Z'>
	<summary>Hanging when importing pytorch_lightning on google cloud vm.</summary>
	<description>
&lt;denchmark-h:h2&gt;‚ùì Questions and Help&lt;/denchmark-h&gt;

Hi,
I was trying to use pytorch-lightning with TPU on google cloud virtual machine and the virtual machine is created by this command:
&lt;denchmark-code&gt;gcloud compute instances create tpu-vm \
	   --machine-type=n1-standard-4 \
	   --image-project=ml-images \
	   --image-family=torch-xla \
	   --boot-disk-size=200GB \
   --scopes=cloud-platform
&lt;/denchmark-code&gt;

When I try to import pytorch_lightning, it hanged all the time. I tried on both jupyter notebook and python in terminal, the results are the same. When I KeyboardInterrupt the code, it returns the logs below. It seems that it has something to do with the multiprocessing part. I have tried different installation methods but still have this problem. Could you help me to solve this problem? Any ideas or help would be much appreciated! Thanks!
&lt;denchmark-code&gt;---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
&lt;ipython-input-1-702a026384b9&gt; in &lt;module&gt;
      1 import torch_xla.core.xla_model as xm
----&gt; 2 import pytorch_lightning as pl
      3 # from pytorch_lightning import Trainer, seed_everything

/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/__init__.py in &lt;module&gt;
     54     # We are not importing the rest of the lightning during the build process, as it may not be compiled yet
     55 else:
---&gt; 56     from pytorch_lightning.core import LightningDataModule, LightningModule
     57     from pytorch_lightning.callbacks import Callback
     58     from pytorch_lightning.trainer import Trainer

/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/core/__init__.py in &lt;module&gt;
     14 
     15 from pytorch_lightning.core.datamodule import LightningDataModule
---&gt; 16 from pytorch_lightning.core.lightning import LightningModule
     17 
     18 __all__ = [

/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py in &lt;module&gt;
     44 
     45 
---&gt; 46 TPU_AVAILABLE = XLADeviceUtils.tpu_device_exists()
     47 
     48 if TPU_AVAILABLE:

/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/utilities/xla_device_utils.py in tpu_device_exists()
     88         """
     89         if XLADeviceUtils.TPU_AVAILABLE is None and TORCHXLA_AVAILABLE:
---&gt; 90             XLADeviceUtils.TPU_AVAILABLE = pl_multi_process(XLADeviceUtils._is_device_tpu)()
     91         return XLADeviceUtils.TPU_AVAILABLE

/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/utilities/xla_device_utils.py in wrapper(*args, **kwargs)
     41         proc = Process(target=inner_f, args=(queue, func,), kwargs=kwargs)
     42         proc.start()
---&gt; 43         proc.join()
     44         return queue.get()
     45 

/anaconda3/envs/ldetr/lib/python3.6/multiprocessing/process.py in join(self, timeout)
    122         assert self._parent_pid == os.getpid(), 'can only join a child process'
    123         assert self._popen is not None, 'can only join a started process'
--&gt; 124         res = self._popen.wait(timeout)
    125         if res is not None:
    126             _children.discard(self)

/anaconda3/envs/ldetr/lib/python3.6/multiprocessing/popen_fork.py in wait(self, timeout)
     48                     return None
     49             # This shouldn't block if wait() returned successfully.
---&gt; 50             return self.poll(os.WNOHANG if timeout == 0.0 else 0)
     51         return self.returncode
     52 

/anaconda3/envs/ldetr/lib/python3.6/multiprocessing/popen_fork.py in poll(self, flag)
     26             while True:
     27                 try:
---&gt; 28                     pid, sts = os.waitpid(self.pid, flag)
     29                 except OSError as e:
     30                     # Child process not yet created. See #1731717

KeyboardInterrupt: 
&lt;/denchmark-code&gt;

	</description>
	<comments>
		<comment id='1' author='L4zyy' date='2020-10-24T03:32:41Z'>
		&lt;denchmark-link:https://github.com/lezwon&gt;@lezwon&lt;/denchmark-link&gt;
 this is the utilities function you recently added, right? Maybe you need to put a timeout on the join()?
		</comment>
		<comment id='2' author='L4zyy' date='2020-10-25T02:06:33Z'>
		Sure. I'll add a timeout. However, I'm curious to know why the process hangs. &lt;denchmark-link:https://github.com/L4zyy&gt;@L4zyy&lt;/denchmark-link&gt;
 mind executing this in the console and letting us know the output?
from pytorch_lightning.utilities.xla_device_utils import XLADeviceUtils
XLADeviceUtils._is_device_tpu()
		</comment>
		<comment id='3' author='L4zyy' date='2020-10-25T02:11:05Z'>
		
Sure. I'll add a timeout. However, I'm curious to know why the process hangs. @L4zyy mind executing this in the console and letting us know the output?
from pytorch_lightning.utilities.xla_device_utils import XLADeviceUtils
XLADeviceUtils._is_device_tpu()

It will hang every time I import pytorch_lightning. When I ran the first line of this code, I got hang and when I interupt it with Ctrl C, I got:
&lt;denchmark-code&gt;Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/__init__.py", line 56, in &lt;module&gt;
    from pytorch_lightning.core import LightningDataModule, LightningModule
  File "/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/core/__init__.py", line 16, in &lt;module&gt;
    from pytorch_lightning.core.lightning import LightningModule
  File "/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 46, in &lt;module&gt;
    TPU_AVAILABLE = XLADeviceUtils.tpu_device_exists()
  File "/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/utilities/xla_device_utils.py", line 90, in tpu_device_exists
    XLADeviceUtils.TPU_AVAILABLE = pl_multi_process(XLADeviceUtils._is_device_tpu)()
  File "/anaconda3/envs/ldetr/lib/python3.6/site-packages/pytorch_lightning/utilities/xla_device_utils.py", line 43, in wrapper
    proc.join()
  File "/anaconda3/envs/ldetr/lib/python3.6/multiprocessing/process.py", line 124, in join
    res = self._popen.wait(timeout)
  File "/anaconda3/envs/ldetr/lib/python3.6/multiprocessing/popen_fork.py", line 50, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/anaconda3/envs/ldetr/lib/python3.6/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='L4zyy' date='2020-10-25T02:15:57Z'>
		Sorry about that. Try this one. It does not import Lightning.
import torch_xla.core.xla_model as xm
device = xm.xla_device()
xm.xla_device_hw(device)
		</comment>
		<comment id='5' author='L4zyy' date='2020-10-25T02:32:35Z'>
		
Sorry about that. Try this one. It does not import Lightning.
import torch_xla.core.xla_model as xm
device = xm.xla_device()
xm.xla_device_hw(device)

I find out the reason of hanging. I forgot to export the TPU_IP_ADDRESS before importing. When I forgot to export the variable, the device = ... line will hang and can not be interupted. After I export the variable, I can get device normally and the third line will return 'TPU'. Also I can successfully import pytorch_lightning after I export the variable.
		</comment>
		<comment id='6' author='L4zyy' date='2020-10-25T02:34:30Z'>
		that's awesome üëç Glad it worked :)
		</comment>
		<comment id='7' author='L4zyy' date='2020-10-25T03:11:36Z'>
		It seems we don't have the TPU_IP_ADDRESS anywhere in our codebase. Is it something we should document, or this is a system configuration with xla that the user has to do?
		</comment>
		<comment id='8' author='L4zyy' date='2020-10-25T03:16:50Z'>
		It's present in &lt;denchmark-link:https://cloud.google.com/tpu/docs/tutorials/resnet-pytorch?hl=tr#create_and_configure_the_pytorch_environment&gt;cloud docs&lt;/denchmark-link&gt;
. Maybe we could add it to our docs too?
		</comment>
		<comment id='9' author='L4zyy' date='2020-10-25T08:44:33Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 I was wondering if we should lazy load . Maybe only when the user selects to train on TPU's. That way we can ensure the user is able to load lightning without running into errors as such.
		</comment>
		<comment id='10' author='L4zyy' date='2020-10-25T16:45:58Z'>
		Yes, I think that would be the best :)
		</comment>
	</comments>
</bug>