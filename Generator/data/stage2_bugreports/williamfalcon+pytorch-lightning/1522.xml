<bug id='1522' author='Jonas-Jaeger' open_date='2020-04-17T23:15:19Z' closed_time='2020-04-19T03:07:16Z'>
	<summary>Performance drop when activating gradient clipping</summary>
	<description>
Hello all,
I experienced a substantial drop in computation time when activating gradient clipping (by passing a non-zero value to the keyword argument gradient_clip_val when initializing the Trainer).
I noticed that in the current implementation of the clipping_gradient method in pytorch-lightning/trainer/training_tricks.py redundant computations are made by first computing the 2-norm and second squaring this result, which could be shortened by computing the sum of squares directly. This saves one square root and squaring operation per parameter set.
Best,
Jonas
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;cuda:
	GPU:
	available:           False
	version:             None
packages:
	numpy:               1.18.1
	pyTorch_debug:       False
	pyTorch_version:     1.4.0
	pytorch-lightning:   0.7.4-dev
	tensorboard:         2.2.1
	tqdm:                4.45.0
system:
	OS:                  Darwin
	architecture:
		64bit
		
	processor:           i386
	python:              3.8.2
	version:             Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I trained a relatively small (two-layered) MLP on MNIST; perhaps this performance drop does not become that apparent when training on larger network architectures.
	</description>
	<comments>
		<comment id='1' author='Jonas-Jaeger' date='2020-04-17T23:15:56Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='Jonas-Jaeger' date='2020-04-17T23:20:13Z'>
		Unfortunately, it seems that I do not have push access to this repository to push the proposed fix and create a pull request for this issue.
		</comment>
		<comment id='3' author='Jonas-Jaeger' date='2020-04-17T23:42:13Z'>
		fork the repo
make the fix
submit a PR
:)
		</comment>
	</comments>
</bug>