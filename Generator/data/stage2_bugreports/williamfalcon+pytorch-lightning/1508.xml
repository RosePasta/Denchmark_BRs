<bug id='1508' author='zlenyk' open_date='2020-04-16T13:30:39Z' closed_time='2020-04-21T18:38:16Z'>
	<summary>Batch is not split in 'dp' mode when dataloader output is not a tensor</summary>
	<description>
My dataloader is returning a list of lists (for multi-label classification) for labels and a tensor of images for each batch. When I'm using DataParallel mode, labels are not getting split into "sub-batches" and I'm getting all the labels on each GPU. Is there a way to implement this splitting also for non-tensors?
&lt;denchmark-code&gt;class CustomDataset(Dataset):
...
    def collate(self, batch):
        images, labels = list(zip(*batch))
        return torch.stack(images), [ label_set for label_set in labels ]

class LitModel(pl.LightningModule):
...
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.model(x)
        print(f'Val loss {y.shape}, {y_hat.shape}' )
&lt;/denchmark-code&gt;

This example will print y to have length of full batch and y_hat to have length the same as the length of x tensor (which is smaller here). x tensor got split correctly, while y didn't.
Is it the issue of lightning or maybe DataParallel module?
	</description>
	<comments>
		<comment id='1' author='zlenyk' date='2020-04-16T13:31:18Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='zlenyk' date='2020-04-17T14:04:57Z'>
		It is the expected behavior of the DataParallel module.
When something is not a Tensor it will duplicate it for all GPUs.
		</comment>
		<comment id='3' author='zlenyk' date='2020-04-17T18:46:12Z'>
		&lt;denchmark-link:https://github.com/BartekRoszak&gt;@BartekRoszak&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/zlenyk&gt;@zlenyk&lt;/denchmark-link&gt;

I believe that is the expected behaviour. Lightning uses PyTorch's scatter to distribute the input across GPUs &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L11&gt;https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/scatter_gather.py#L11&lt;/denchmark-link&gt;

PyTorch's scatter will recurse into nested lists and treat each Tensor at the bottom as the data batch. So it's treating each of your label as the whole batch.
The best way to work around this would be to covert it to a tensor. torch.tensor([ label_set for label_set in labels ]) should work fine
		</comment>
		<comment id='4' author='zlenyk' date='2020-04-20T18:58:15Z'>
		I see, thank you guys for the answers.
		</comment>
	</comments>
</bug>