<bug id='760' author='fdelrio89' open_date='2020-01-28T15:36:34Z' closed_time='2020-02-26T23:34:53Z'>
	<summary>Test metrics not logging to Comet after training</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When testing a model with Trainer.test metrics are not logged to Comet if the model was previously trained using Trainer.fit. While training metrics are logged correctly.
&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;    comet_logger = CometLogger()
    trainer = Trainer(logger=comet_logger)
    model = get_model()

    trainer.fit(model) # Metrics are logged to Comet
    trainer.test(model) # No metrics are logged to Comet
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Test metrics should also be logged in to Comet.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;- PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti
GPU 4: GeForce GTX 1080 Ti
GPU 5: GeForce GTX 1080 Ti
GPU 6: GeForce GTX 1080 Ti
GPU 7: GeForce GTX 1080 Ti

Nvidia driver version: 418.67
cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.6.1

Versions of relevant libraries:
[pip3] numpy==1.16.4
[pip3] pytorch-lightning==0.6.0
[pip3] torch==1.3.0
[pip3] torchvision==0.4.1
[conda] Could not collect
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

I believe the issue is caused because at the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/deffbaba7ffb16ff57b56fe65f62df761f25fbd6/pytorch_lightning/trainer/training_loop.py#L366&gt;end of the training routine&lt;/denchmark-link&gt;
,  is called. This in turn calls  inside the logger and the  object doesn't expect to send more information after this.
An alternative is to create another Trainer object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the ExistingExperiment object form the Comet SDK, but the solution seems a little hacky and the CometLogger currently doesn't support this kind of experiment.
	</description>
	<comments>
		<comment id='1' author='fdelrio89' date='2020-02-11T17:36:13Z'>
		Did you find a solution?
Mind submitting a PR?
&lt;denchmark-link:https://github.com/fdelrio89&gt;@fdelrio89&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='fdelrio89' date='2020-02-13T21:36:24Z'>
		I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.
I solved it by getting the experiment key and creating another logger and trainer with it.
&lt;denchmark-code&gt;    comet_logger = CometLogger()
    trainer = Trainer(logger=comet_logger)
    model = get_model()

    trainer.fit(model)

    experiment_key = comet_logger.experiment.get_key()
    comet_logger = CometLogger(experiment_key=experiment_key)
    trainer = Trainer(logger=comet_logger)

    trainer.test(model)
&lt;/denchmark-code&gt;

For this to work, I had to modify the CometLogger class to accept the experiment_key and create a CometExistingExperiment from the Comet SDK when this param is present.
&lt;denchmark-code&gt;class CometLogger(LightningLoggerBase):
     ...

    @property
    def experiment(self):
        ...

        if self.mode == "online":
            if self.experiment_key is None:
                self._experiment = CometExperiment(
                    api_key=self.api_key,
                    workspace=self.workspace,
                    project_name=self.project_name,
                    **self._kwargs
                )
            else:
                self._experiment = CometExistingExperiment(
                    api_key=self.api_key,
                    workspace=self.workspace,
                    project_name=self.project_name,
                    previous_experiment=self.experiment_key,
                    **self._kwargs
                )
        else:
            ...

        return self._experiment
&lt;/denchmark-code&gt;

I can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it &lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='fdelrio89' date='2020-02-17T11:18:50Z'>
		&lt;denchmark-link:https://github.com/williamFalcon&gt;@williamFalcon&lt;/denchmark-link&gt;
 Any progress on this Issue? I am facing the same problem.
		</comment>
		<comment id='4' author='fdelrio89' date='2020-02-17T11:21:16Z'>
		&lt;denchmark-link:https://github.com/fdelrio89&gt;@fdelrio89&lt;/denchmark-link&gt;
 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the  directly in the logger object itself, instead of having to re-instantiate the logger.
		</comment>
		<comment id='5' author='fdelrio89' date='2020-02-18T21:27:32Z'>
		&lt;denchmark-link:https://github.com/xssChauhan&gt;@xssChauhan&lt;/denchmark-link&gt;
 good idea, I just submitted a PR (&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/892&gt;#892&lt;/denchmark-link&gt;
) considering this. Thanks!
		</comment>
		<comment id='6' author='fdelrio89' date='2020-02-26T23:34:53Z'>
		I assume that it was fixed by &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/892&gt;#892&lt;/denchmark-link&gt;

if you have some other problems feel free to reopen or create a new... 
		</comment>
		<comment id='7' author='fdelrio89' date='2020-04-19T06:51:52Z'>
		Actually I'm still facing the problem.
		</comment>
		<comment id='8' author='fdelrio89' date='2020-04-19T09:11:27Z'>
		&lt;denchmark-link:https://github.com/dvirginz&gt;@dvirginz&lt;/denchmark-link&gt;
 are you using the latest master? may you provide a minimal example?
		</comment>
		<comment id='9' author='fdelrio89' date='2020-04-19T09:21:55Z'>
		
@dvirginz are you using the latest master? may you provide a minimal example?

You are right, sorry.
After building from source it works.
		</comment>
		<comment id='10' author='fdelrio89' date='2020-07-14T13:43:08Z'>
		I should probably open a new issue, but it happens with Weights &amp; Biases logger too. I haven't had the time to delve deep into it yet.
		</comment>
	</comments>
</bug>