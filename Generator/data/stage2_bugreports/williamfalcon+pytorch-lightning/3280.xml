<bug id='3280' author='hyukyu' open_date='2020-08-31T06:32:08Z' closed_time='2020-09-11T14:55:59Z'>
	<summary>Error in transfer_batch_to_device when None type is in the batch</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

There should be no torchtext pre-installed
Run the sample code

&lt;denchmark-h:h4&gt;Code sample&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;from torch.utils.data import DataLoader
import pytorch_lightning as pl


def collate_fn(batch):
    return batch


class MyDataModule(pl.LightningDataModule):
    def __init__(self):
        super().__init__()

    def prepare_data(self):
        pass

    def setup(self, stage):
        self.train = [{"input": torch.randn(1,2), "output": None}]

    def train_dataloader(self):
        return DataLoader(self.train, batch_size=1, collate_fn=collate_fn)


class MyModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(2, 1)

    def forward(self, x):
        return self.linear(x)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters())

    def training_step(self, batch, batch_idx):
        x = batch[0]["input"]
        y = batch[0]["output"]
        loss = self(x)
        result = pl.TrainResult(loss)
        result.log('train_loss', loss, on_epoch=True)
        return result


def main():
    # Dataset
    data_module = MyDataModule()

    # Model
    model = MyModel()

    # Train
    trainer = pl.Trainer(max_steps=1)
    trainer.fit(model, datamodule=data_module)


if __name__ == "__main__":
    main()
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The above code runs fine if the package torchtext is installed. However, the code raises following error if torchtext is not available and I believe this inconsistency is a bug.
&lt;denchmark-code&gt; File "python3.6/site-packages/pytorch_lightning/utilities/apply_func.py", line 122, in batch_to
    return data.to(device, **kwargs)
AttributeError: 'NoneType' object has no attribute 'to'
python-BaseException
&lt;/denchmark-code&gt;

I think this line of the &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/f46318ebfeb785a659c49091a6871584ccde3ee1/pytorch_lightning/utilities/apply_func.py#L24&gt;code&lt;/denchmark-link&gt;
 is the cause of the problem
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:

GPU:

TITAN Xp
TITAN Xp
TITAN Xp
TITAN Xp
TITAN Xp
TITAN Xp
TITAN Xp
TITAN Xp


available:         True
version:           10.2


Packages:

numpy:             1.18.4
pyTorch_debug:     False
pyTorch_version:   1.6.0
pytorch-lightning: 0.9.0
tensorboard:       2.2.1
tqdm:              4.46.0


System:

OS:                Linux
architecture:

64bit
ELF


processor:         x86_64
python:            3.6.8
version:           #81~16.04.1-Ubuntu SMP Tue Nov 26 16:34:21 UTC 2019



	</description>
	<comments>
		<comment id='1' author='hyukyu' date='2020-08-31T06:32:47Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='hyukyu' date='2020-09-01T13:36:08Z'>
		Thanks! Mind sending us a PR with the fix?
		</comment>
		<comment id='3' author='hyukyu' date='2020-09-11T00:08:43Z'>
		&lt;denchmark-link:https://github.com/awaelchli&gt;@awaelchli&lt;/denchmark-link&gt;
 mind taking a look?
		</comment>
		<comment id='4' author='hyukyu' date='2020-09-11T07:08:23Z'>
		I submitted a fix.
&lt;denchmark-link:https://github.com/hyukyu&gt;@hyukyu&lt;/denchmark-link&gt;
 Be aware, torchtext will remove/deprecate this Batch object soon. So in the future you should switch to their new dataloading pipeline that integrates better with PyTorch and Lightning.
		</comment>
	</comments>
</bug>