<bug id='4442' author='nathanpainchaud' open_date='2020-10-30T15:49:28Z' closed_time='2020-12-25T06:18:22Z'>
	<summary>Random CUDA OOM error when starting SLURM jobs</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When submitting jobs to SLURM, some jobs (around 1-2% of them) will randomly encounter a CUDA OOM error during the setup prior to training. I can confirm it's not an issue with the configuration of the job vs hardware itself, since I can resubmit the exact same job script and it will work. I also know that my resource consumption is not even close to the limit, since the jobs that do work use ~10Gb on 32Gb GPUs.
I already checked with nvidia-smi that the GPUs I get allocated are empty at the start of the training, confirming that it's not a problem with the cluster:
echo $(nvidia-smi --query-gpu=memory.used --format=csv,noheader) # At the beginning of the script
# Output: 0 MiB
Here is the output I get when my jobs crash:
CometLogger will be initialized in online mode
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Using native 16bit precision.
Traceback (most recent call last):
  File "my_code.py", line XX, in XX
    trainer.fit(model)
  File "my_virtualenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 424, in fit
    self.accelerator_backend.setup(model)
  File "my_virtualenv/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py", line 36, in setup
    model.cuda(self.trainer.root_gpu)
  File "my_virtualenv/lib/python3.8/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py", line 124, in cuda
    return super().cuda(device=device)
  File "my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 463, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 359, in _apply
    module._apply(fn)
  File "my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 359, in _apply
    module._apply(fn)
  File "my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 359, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 381, in _apply
    param_applied = fn(param)
  File "my_virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py", line 463, in &lt;lambda&gt;
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
&lt;denchmark-h:h2&gt;Please reproduce using [the BoringModel and post here]&lt;/denchmark-h&gt;

Unfortunately, given the nature of the bug, it's not something I can reproduce with the BoringModel.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Here are any of the SLURM parameters that could be relevant (only excludes logging and job time):
#SBATCH --gres=gpu:v100l:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=48000M
The related non-default parameter to the trainer and dataloaders are the following:
Trainer(..., benchmark=true, precision=16, ...)

# Inside the lightning module
DataLoader(..., shuffle=True, num_workers=7, pin_memory=self.on_gpu, ...)
From experience, I set num_workers to 1 lower than the number of CPUs I request. On my local machine, that's never caused me any issues.
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

SLURM jobs should not encounter random CUDA OOM error when configured with the necessary ressources.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

PyTorch and CUDA are provided by the cluster's manager, so that they are optimized for the hardware. PyTorch Lightning I installed myself using pip.

CUDA:

GPU:

Tesla V100-SXM2-32GB


available:         True
version:           10.2


Packages:

numpy:             1.19.1
pyTorch_debug:     True
pyTorch_version:   1.7.0
pytorch-lightning: 1.0.4
tqdm:              4.50.2


System:

OS:                Linux
architecture:

64bit
ELF


processor:
python:            3.8.0
version:           #1 SMP Tue Feb 4 23:02:59 UTC 2020



&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

The PyTorch version reported here is 1.7, but I updated it just yesterday. I encountered the reported bug using both PyTorch 1.6 and 1.7.
EDIT: Fixed mistakes when detailing trainer &amp; dataloader arguments.
	</description>
	<comments>
		<comment id='1' author='nathanpainchaud' date='2020-11-16T10:54:52Z'>
		I encounter similar issues:

for some, but not all jobs (about 50%) I get an OOM error right at the beginning of training.
If a job doesn't immediately crash, memory usage is about ~75% (about 17/24 GB).

&lt;denchmark-link:https://github.com/nathanpainchaud&gt;@nathanpainchaud&lt;/denchmark-link&gt;
  is not an argument to , I assume you mean it to be the argument to your ?
		</comment>
		<comment id='2' author='nathanpainchaud' date='2020-11-16T15:41:21Z'>
		
@nathanpainchaud num_workers is not an argument to pl.Trainer, I assume you mean it to be the argument to your DataLoader?

Yeah, my bad. I'll edit the post to make it clear.
		</comment>
		<comment id='3' author='nathanpainchaud' date='2020-12-18T04:09:17Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
		<comment id='4' author='nathanpainchaud' date='2021-01-01T23:00:13Z'>
		Had a similar issue, and tried different torch versions (1.4 ,1.5, 1.7)
		</comment>
	</comments>
</bug>