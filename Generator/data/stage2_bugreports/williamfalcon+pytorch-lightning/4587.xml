<bug id='4587' author='david-waterworth' open_date='2020-11-09T08:14:58Z' closed_time='2020-11-09T22:49:41Z'>
	<summary>Segmentation fault on import with RTX 3090</summary>
	<description>
I've just installed an RTX 3090 in my DL machine. I have the following dependencies installed
Driver Version: 455.32
Cuda Version: 11.0
Python Version: 3.8.6
Pytorch Version: 1.7.0+cu11  (pip install torch==1.7.0+cu110 torchvision==0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html)
After initially having issues (installed cuda-11-1 rather than cuda-11-0) torch appears to work. I ran a  CIFAR10 example on GPU, I verified with nvidia-smi that it was indeed running on the GPU.
I installed the latest pytorch-lightning, i.e. pip install git+https://github.com/PytorchLightning/pytorch-lightning.git@master --upgrade but I cannot import, i.e.
&lt;denchmark-code&gt;&gt;&gt;&gt; import pytorch_lightning as pl
Segmentation fault (core dumped)
&lt;/denchmark-code&gt;

Should this work? If so is there anything I've done wrong? If not, is there anything I can do to assist in tracking down the issue. Ironically I've just discovered that the CRF decoder is quicker on CPU (at least cf my older 1080Ti) so I don't need the GPU backend but atm I crashing on import is problematic ;)
	</description>
	<comments>
		<comment id='1' author='david-waterworth' date='2020-11-09T11:18:33Z'>
		Hi &lt;denchmark-link:https://github.com/david-waterworth&gt;@david-waterworth&lt;/denchmark-link&gt;
 so Lighting imports failed on both CUDA 11 and 11.1?
		</comment>
		<comment id='2' author='david-waterworth' date='2020-11-09T20:52:15Z'>
		&lt;denchmark-link:https://github.com/david-waterworth&gt;@david-waterworth&lt;/denchmark-link&gt;
 could you please confirm that the very same import works with ?
		</comment>
		<comment id='3' author='david-waterworth' date='2020-11-09T21:10:51Z'>
		&lt;denchmark-link:https://github.com/ydcjeff&gt;@ydcjeff&lt;/denchmark-link&gt;
 it may have but at that stage I was still getting the dependencies correct - there's no pre-built torch binary for 11.1 (afaik).
&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 yes  imports fine and I can run a pure torch training loop on my 3090. I'm going to clone  and see where it's crashing. There's a chance I have an env variable or something pointing to the wrong place, although I would have thought that would impact  as well as 
		</comment>
		<comment id='4' author='david-waterworth' date='2020-11-09T21:26:18Z'>
		I deleted my virtual environment and reinstalled. It now no longer crashes. Instead I'm getting 'CUDA error: no kernel image is available for execution on the device' which is the error I saw earlier using torch when I had the wrong version of pytorch / cuda. Not sure why at this stage but pl is finding the wrong cuda by the looks of things - I'll dig further - it's probably an issue with my installation.
		</comment>
		<comment id='5' author='david-waterworth' date='2020-11-09T21:51:42Z'>
		I ended up installing the nightly version of pytorch via  which installed  (based on &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/45028&gt;pytorch/pytorch#45028&lt;/denchmark-link&gt;
).
It's working, despite the messages:
torchvision 0.8.1 requires torch==1.7.0, but you'll have torch 1.8.0.dev20201106+cu110 which is incompatible.
pytorch-lightning 1.0.5 requires torch&lt;1.8,&gt;=1.3, but you'll have torch 1.8.0.dev20201106+cu110 which is incompatible.
looks like there are still some torch issues to iron out - and cuda 11.1 is needed to unlock the full potential of the card.
		</comment>
		<comment id='6' author='david-waterworth' date='2020-11-09T22:49:38Z'>
		&lt;denchmark-link:https://github.com/david-waterworth&gt;@david-waterworth&lt;/denchmark-link&gt;
 I am sorry for such complication, but it seems that it was not Pl problem in the end, away the HW specific issue would be quite hard to debug/reproduce locally as these kinds of cards are not available on cloud hosting...
From my experience, actual Torch binaries on PyPI are building with CUDA 10.1 and 10.2 so you may consider building your one package from the source...
So I assume this one as resolved as it is not PL issue, feel free to re-open if needed 
		</comment>
		<comment id='7' author='david-waterworth' date='2020-11-09T23:29:46Z'>
		&lt;denchmark-link:https://github.com/Borda&gt;@Borda&lt;/denchmark-link&gt;
 yes I agree, I'm not sure what the best approach at the moment is. I'll try building torch 1.7 against CUDA 11.1 from source. Although if I need torch 1.8 I'll have to wait for downstream packages to update their dependencies - both PL, AllenNLP and others require torch &lt;= 1.7 at the moment
		</comment>
		<comment id='8' author='david-waterworth' date='2020-11-09T23:55:49Z'>
		We are working on adding support for nightly (1.8) as soon as possible unless it will require some significant changes on the PL side...
cc: &lt;denchmark-link:https://github.com/edenlightning&gt;@edenlightning&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='david-waterworth' date='2020-11-10T00:33:32Z'>
		Thanks, it's now working with pip install torch==1.7.0+cu110 torchvision==0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html
I think some packages with requires in their dependencies replace 1.7.0+cu110 with 1.7.0 or even 1.6.0 so I've found I need to install this last.
		</comment>
	</comments>
</bug>