<bug id='1711' author='jxchen01' open_date='2020-05-03T05:37:59Z' closed_time='2020-07-27T06:38:23Z'>
	<summary>model load error when the module is defined with extrat positional auguments</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.7.5/pytorch_lightning/core/lightning.py#L1552&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/0.7.5/pytorch_lightning/core/lightning.py#L1552&lt;/denchmark-link&gt;

When using ddp, this function will throw an error if the pl module is defined with extra positional arguments.
&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:


define a pl module with extra positional arguments:
'''
class My_Classifier(pl.LightningModule):
def init(self, model_params: dict, train_params: dict) -&gt; None:
'''


Train the model with ddp


At some point, a model reloading is triggered


Then, this function will have an error:


&lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.7.5/pytorch_lightning/core/lightning.py#L1552&gt;https://github.com/PyTorchLightning/pytorch-lightning/blob/0.7.5/pytorch_lightning/core/lightning.py#L1552&lt;/denchmark-link&gt;

model = cls(*model_args, *args, **kwargs)
TypeError: init() missing 2 required positional arguments: 'model_params' and 'train_params'
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

Either prevent using extra positional arguments when using ddp, or support more args others than  hparams
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- Tesla V100-SXM2-32GB
- available:         True
- version:           10.1
Packages:
- numpy:             1.18.1
- pyTorch_debug:     False
- pyTorch_version:   1.4.0
- pytorch-lightning: 0.7.5
- tensorboard:       2.1.1
- tqdm:              4.45.0
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.7.7
- version:           #55-Ubuntu SMP Wed May 15 14:27:21 UTC 2019

	</description>
	<comments>
		<comment id='1' author='jxchen01' date='2020-05-03T05:38:40Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='jxchen01' date='2020-05-03T09:52:12Z'>
		&lt;denchmark-link:https://github.com/cmpute&gt;@cmpute&lt;/denchmark-link&gt;
 mind have a look?
		</comment>
		<comment id='3' author='jxchen01' date='2020-05-03T14:31:38Z'>
		In my understanding, right now the pl module must have no argument or only hparam argument except for other optional arguments when it's used in DDP, because lightning handles only the hparams when loading temporary checkpoint: 


pytorch-lightning/pytorch_lightning/core/lightning.py


        Lines 1553 to 1555
      in
      e6b34ef






 if hparams: 



 kwargs.update(hparams=hparams) 



 model = cls(*args, **kwargs) 





which is called from



pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py


         Line 400
      in
      e6b34ef






 loaded_model = original_model.__class__.load_from_checkpoint(path) 





To solve this problem, constructor parameters in lightning module should also be saved into the temporary checkpoint and loaded when DDP reconstruct the module. But this will be an additional feature. My personal preference is to wrap all parameters needed into the hparam argument.
		</comment>
		<comment id='4' author='jxchen01' date='2020-05-19T02:29:08Z'>
		I also got stuck in the similar issue.. I was trying to make customizable batch size for train/test/val dataloaders among other things(the goal was to reuse core code with different training setups) so that I can re-use the code with minimum changes to core training code.
Although not a clean solution, I ended up using setters to set these custom configs, and calling them before Trainer.fit(my_model)
for eg:-
&lt;denchmark-code&gt;class MyModel(LightningModule):
    def __init__(self):
        &lt;your_code_here&gt;

    def set_train_batch_size(self, train_batch_size):
        self.train_batch_size = train_batch_size

#Now use this self.train_batch_size in your train dataloader

#your model loading code will look simpler now
model_1 = MyModel.load_from_checkpoint(&lt;path_to_the_checkpointed_model&gt;)

&lt;/denchmark-code&gt;

Please let us know if there is better way of doing this.
		</comment>
		<comment id='5' author='jxchen01' date='2020-05-19T02:37:48Z'>
		you can add more arguments to the load call... take a look at the docs
		</comment>
		<comment id='6' author='jxchen01' date='2020-05-19T02:44:33Z'>
		
load
sorry, should have given more information, my load call is encapsulated in the prepare_data()

my goal is to have a Model class which i can create different instances of, and then use different configs such as datasources/batchsize etc among others, without changing the Model class, but controlling these things when i initialize the object from the class
		</comment>
		<comment id='7' author='jxchen01' date='2020-07-18T06:21:22Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
		</comment>
	</comments>
</bug>