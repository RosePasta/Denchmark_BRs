<bug id='2188' author='xiadingZ' open_date='2020-06-15T03:10:15Z' closed_time='2020-08-03T18:44:12Z'>
	<summary>[hparams] save_hyperparameters doesn't save kwargs</summary>
	<description>
&lt;denchmark-h:h2&gt;‚ùì Questions and Help&lt;/denchmark-h&gt;

when I use hyperparemeters like docs:
&lt;denchmark-code&gt;class LitMNIST(LightningModule):

    def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):
        super().__init__()
        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint
        self.save_hyperparameters()

&lt;/denchmark-code&gt;

model checkpoint doesn't save args in kwargs. But kwargs is important. Args such as num_frames, img_size, img_std ... must be used in creating dataloader, but it will be tedious if writes them in __init__ explicitly .  it can make code clean if hides them in kwargs.
Before I use hparams, it's ok. But now it's not recommended to use hparams,  is there any good idea to  deal with this problem?
	</description>
	<comments>
		<comment id='1' author='xiadingZ' date='2020-06-15T03:24:08Z'>
		if don't use hparams, it will put all args of model, dataset, dataloader... in a LightnModule' s __init__ method, and save_hyperparameters  doesn't save args in kwargs. It this really a good idea?
		</comment>
		<comment id='2' author='xiadingZ' date='2020-06-15T17:21:33Z'>
		Could you please share your model example?
		</comment>
		<comment id='3' author='xiadingZ' date='2020-06-16T02:50:51Z'>
		
Could you please share your model example?

&lt;denchmark-code&gt;class LitMNIST(LightningModule):

    def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):
        super().__init__()
        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint
        self.save_hyperparameters()
        self.kwargs = kwargs
        ...
    
    def train_dataloader(self):
        img_size = self.kwargs['img_size']
        ...
&lt;/denchmark-code&gt;

I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size
		</comment>
		<comment id='4' author='xiadingZ' date='2020-06-16T14:34:35Z'>
		
I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size

I see, we need to ignore  from the model hparams saving...
&lt;denchmark-link:https://github.com/xiadingZ&gt;@xiadingZ&lt;/denchmark-link&gt;
 mind adding PR with a test for this case and I ll finish it with a patch?
		</comment>
		<comment id='5' author='xiadingZ' date='2020-06-19T00:48:13Z'>
		&lt;denchmark-code&gt;Traceback (most recent call last):
  File "./training.py", line 101, in &lt;module&gt;
    main(hparam_trial)
  File "./training.py", line 86, in main
    model = module(hparams, fold_train, fold_val, data_dir+img_dir)
  File "../main/module.py", line 18, in __init__
    self.hparams = hparams
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 638, in __setattr__
    object.__setattr__(self, name, value)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1695, in hparams
    self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1662, in save_hyperparameters
    cand_names = [k for k, v in init_args.items() if v == hp]
  File "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py", line 1662, in &lt;listcomp&gt;
    cand_names = [k for k, v in init_args.items() if v == hp]
  File "/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py", line 1479, in __nonzero__
    f"The truth value of a {type(self).__name__} is ambiguous. "
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&lt;/denchmark-code&gt;

I'm guessing this is why 0.8.0 causes this error, it's trying to save all args (including dataframes in my case) outside of hparams?
Edit: &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2250&gt;#2250&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>