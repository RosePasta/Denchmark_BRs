<bug id='5034' author='LearnedVector' open_date='2020-12-09T01:25:33Z' closed_time='2021-01-17T18:34:59Z'>
	<summary>Error when using NVIDIA APEX with self.trainer.reinit_scheduler_properties</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

There is an error with the self.trainer.reinit_scheduler_properties class when using nvidia apex found &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/ee9b3fe5744ff7b908b02483fea17f68c6e228fd/pytorch_lightning/plugins/apex.py#L36&gt;here&lt;/denchmark-link&gt;
.
After some digging into the code, the error is thrown because  returns false &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/c2e6e68c7ed1725563f8c84d8cf8b04ae528fde7/pytorch_lightning/trainer/optimizers.py#L145&gt;here&lt;/denchmark-link&gt;
, which causes the idx to not be initialized before calling .
I suspect this may be something to do with this line of code  found &lt;denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/ee9b3fe5744ff7b908b02483fea17f68c6e228fd/pytorch_lightning/plugins/apex.py#L35&gt;here&lt;/denchmark-link&gt;
 changing some properties of the optimizers making the scheduler optimizer, and optimizer not equal to each other.
&lt;denchmark-code&gt;[1,0]&lt;stderr&gt;:Traceback (most recent call last):
[1,0]&lt;stderr&gt;:  File "lightning_train.py", line 253, in &lt;module&gt;
[1,0]&lt;stderr&gt;:    main(args)
[1,0]&lt;stderr&gt;:  File "lightning_train.py", line 169, in main
[1,0]&lt;stderr&gt;:    trainer.fit(asr)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py", line 424, in fit
[1,0]&lt;stderr&gt;:    self.accelerator_backend.setup(model)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/horovod_accelerator.py", line 84, in setup
    def reinit_scheduler_properties(self, optimizers: list, schedulers: list):
[1,0]&lt;stderr&gt;:    model = self.trainer.precision_connector.connect(model)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/connectors/precision_connector.py", line 78, 
in connect
[1,0]&lt;stderr&gt;:    model, optimizers = self.backend.connect(model, self.trainer.optimizers)
[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/plugins/apex.py", line 32, in connect
[1,0]&lt;stderr&gt;:    self.trainer.reinit_scheduler_properties(optimizers, self.trainer.lr_schedulers)[1,0]&lt;stderr&gt;:  File "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/optimizers.py", line 153, in reinit_scheduler_properties
[1,0]&lt;stderr&gt;:    scheduler.__class__.__mro__[idx].__init__(scheduler, optimizer)
[1,0]&lt;stderr&gt;:UnboundLocalError: local variable 'idx' referenced before assignment
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Run apex and fp16 using pytorch 1.4.0 (which doesn't support native AMP).
&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The model should train fine without error. When I replace the if scheduler.optimizer == optimizer: with if True:, the code runs and the model trains fine using fp16 and nvidai apex.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;


CUDA:
- GPU:
- Tesla V100-FHHL-16GB
- Tesla V100-FHHL-16GB
- Tesla V100-FHHL-16GB
- Tesla V100-FHHL-16GB
- Tesla V100-FHHL-16GB
- Tesla V100-FHHL-16GB
- Tesla V100-FHHL-16GB
- Tesla V100-FHHL-16GB
- available:         True
- version:           10.1
Packages:
- numpy:             1.19.4
- pyTorch_debug:     False
- pyTorch_version:   1.4.0
- pytorch-lightning: 1.0.0
- tqdm:              4.54.1
System:
- OS:                Linux
- architecture:
- 64bit
-
- processor:         x86_64
- python:            3.6.9
- version:           #79~16.04.1-Ubuntu SMP Tue Nov 12 14:01:10 UTC 2019

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='LearnedVector' date='2021-01-10T17:06:08Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>