<bug id='902' author='srush' open_date='2020-02-20T05:28:31Z' closed_time='2020-02-25T03:23:27Z'>
	<summary>TPU Training Issues and Feature Requests</summary>
	<description>
TPU training is really cool. A couple issues that came up in training.


There is no longer a call to optimizer_step this is needed for setting learning rates that change over each step. Not sure how to fix this except by maybe having another "post_optimizer" call, which might be a better place for this style of stepping.


According to this tutorial https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md clip_grad_norm_ is very slow in XLA. The give a suggestion for a replacement function call. It would be nice if lightning just called this instead.


Nice to have: Point to that speed tutorial in the docs so people don't get stuck.


When using DDP, we have been using proc_rank to decide when to print debug statements. Might be nice to have a value that works with DDP and TPU to abstract away from using this term.


	</description>
	<comments>
		<comment id='1' author='srush' date='2020-02-25T03:19:40Z'>
		&lt;denchmark-link:https://github.com/srush&gt;@srush&lt;/denchmark-link&gt;

can the .where also work when not on TPUs. The docs suggest that it's always a faster way of clipping grads?
		</comment>
		<comment id='2' author='srush' date='2020-02-25T03:48:00Z'>
		Yeah, think that .where is likely always better.
		</comment>
	</comments>
</bug>