<bug id='3205' author='LearnedVector' open_date='2020-08-26T20:16:41Z' closed_time='2020-08-29T04:14:43Z'>
	<summary>OneCycleLR scheduler looks wrong</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm using the optim.lr_scheduler.OneCycleLR and it seems to be cycling up and down every 1k iteration. anybody understand why? here is my implementation...
&lt;denchmark-code&gt;    def configure_optimizers(self):
        self.optimizer = optim.AdamW(self.model.parameters(), self.args.learning_rate)

        ds_len = self.train_dataset.__len__()
        total_bs = self.args.batch_size*self.args.gpus
        self.scheduler = optim.lr_scheduler.OneCycleLR(
                                        self.optimizer, max_lr=self.args.learning_rate,
                                        anneal_strategy='linear', div_factor=100,
                                        steps_per_epoch=int((ds_len/total_bs)),
                                        epochs=self.args.epochs)

        sched = {
            'scheduler': self.scheduler,
            'interval': 'step',
        }
        return [self.optimizer], [sched]
&lt;/denchmark-code&gt;

And here is what it looks like...
&lt;denchmark-link:https://user-images.githubusercontent.com/8495552/91351509-0906fc80-e7ae-11ea-9de1-4800cec85f5c.png&gt;&lt;/denchmark-link&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

Steps to reproduce the behavior:

use the optimizer configuration code above
use a tensorboard logger
in training step, log the learning rate to tensorboard

&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):
        loss = self.step(batch)
        lr = self.scheduler.get_last_lr()
        logs = {'loss': loss, 'lr': lr[0].detach().cpu().numpy()}
        return {'loss': loss, 'log': logs}
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

The one cycle learning rate should be a single cycle where it goes up and down. Not sure what is happening here.
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Please copy and paste the output from our
&lt;denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py&gt;environment collection script&lt;/denchmark-link&gt;

(or fill out the checklist below manually).
You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;


PyTorch Version (e.g., 1.0): 1.6
OS (e.g., Linux): Linux
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.7.0
CUDA/cuDNN version: 10.2
GPU models and configuration: GeForce RTX 2080 Ti

	</description>
	<comments>
		<comment id='1' author='LearnedVector' date='2020-08-26T20:17:21Z'>
		Hi! thanks for your contribution!, great first issue!
		</comment>
		<comment id='2' author='LearnedVector' date='2020-08-26T21:59:07Z'>
		&lt;denchmark-link:https://github.com/LearnedVector&gt;@LearnedVector&lt;/denchmark-link&gt;
 are you using DDP or DP for training? Does the same issue occur if you provide  param instead?
		</comment>
		<comment id='3' author='LearnedVector' date='2020-08-26T22:11:58Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
 I am using . and yes if i provide the  same issue is occurring!
		</comment>
		<comment id='4' author='LearnedVector' date='2020-08-26T22:17:54Z'>
		&lt;denchmark-link:https://github.com/LearnedVector&gt;@LearnedVector&lt;/denchmark-link&gt;
 can you print out these values in the code and provide them here.

ds_len
batch_size
gpu count
epoch count

		</comment>
		<comment id='5' author='LearnedVector' date='2020-08-26T22:24:20Z'>
		&lt;denchmark-link:https://github.com/ananyahjha93&gt;@ananyahjha93&lt;/denchmark-link&gt;
  Sure thing! Thanks for taking time to help.
&lt;denchmark-code&gt;ds_len: 982746
batch_size: 80
gpu_count: 8
epoch_count: 50
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='LearnedVector' date='2020-08-26T22:30:16Z'>
		no worries, let me see if I can replicate this quickly
		</comment>
		<comment id='7' author='LearnedVector' date='2020-08-29T04:14:43Z'>
		This is fixed. there was a leftover piece of code in my lightning module where I called self.scheduler.step(val_loss) in the validation steps. This caused the onecycle to reset. Thanks for looking into this!
		</comment>
		<comment id='8' author='LearnedVector' date='2020-08-31T18:44:25Z'>
		&lt;denchmark-link:https://github.com/LearnedVector&gt;@LearnedVector&lt;/denchmark-link&gt;
 oh ok, yeah that explains why I wasn't able to replicate this
		</comment>
	</comments>
</bug>