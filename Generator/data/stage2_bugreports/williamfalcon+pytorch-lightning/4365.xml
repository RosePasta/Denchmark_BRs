<bug id='4365' author='ClaartjeBarkhof' open_date='2020-10-26T08:16:16Z' closed_time='2021-01-07T21:35:26Z'>
	<summary>Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

When initialising a PytorchLightning trainer and fitting it, I get the following error:
&lt;denchmark-code&gt;Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.
Abort trap: 6
&lt;/denchmark-code&gt;

My train code:
&lt;denchmark-code&gt;import pytorch_lightning as pl
from typing import List, Dict
import torch
from EncoderDecoderShareVAE import EncoderDecoderShareVAE
import NewsVAEArguments
from NewsData import NewsData
from typing import Tuple


class NewsVAE(pl.LightningModule):
    """
    This Pytorch Lightning Module serves as a wrapper for training the EncoderDecoderShareVAE,
    which is a VAE composed of two checkpoints connected by a latent space.
    """

    def __init__(self, roberta_ckpt_name: str = "roberta-base"):
        super().__init__()

        # Encoder Decoder model
        self.encoder_decoder = EncoderDecoderShareVAE(roberta_ckpt_name)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)
        return optimizer

    def forward(self, batch_inputs: Dict[str, torch.Tensor]) -&gt; Tuple[float, float]:
        kl_loss, recon_loss = self.encoder_decoder(batch_inputs)
        return kl_loss, recon_loss

    def training_step(self, article_batch: List[str], batch_idx: int) -&gt; Dict:
        kl_loss, recon_loss = self(article_batch)
        loss = kl_loss + recon_loss  # TODO: add beta term
        logs = {'loss': loss, 'kl_loss': kl_loss, 'recon_loss': recon_loss}
        return {'loss': loss, 'logs': logs}


def main(args):
    news_data = NewsData(args.dataset_name, args.tokenizer_name,
                         batch_size=args.batch_size, num_workers=args.num_workers)
    news_vae = NewsVAE()

    trainer = pl.Trainer()
    trainer.fit(news_vae, news_data.dataloaders['train'])  
&lt;/denchmark-code&gt;

I use the Huggingface datasets library to create a dataset and make a Pytorch data loader out of it.
&lt;denchmark-code&gt;import torch as torch
from torch.utils.data import DataLoader
from transformers import RobertaTokenizerFast, logging as transformers_logging  # type: ignore
from datasets import load_dataset, list_datasets, load_from_disk # type: ignore
from typing import Optional, List, Dict, Union
import os
from collections import OrderedDict

transformers_logging.set_verbosity_warning()

DATASETS_PROPERTIES = {
    'cnn_dailymail': {
        'article_col': 'article',
        'splits': ['train', 'validation', 'test'],
    },
    'ag_news': {
        'article_col': 'text',
        'splits': ['train', 'test']
    }
}

TOKENIZER_PROPERTIES = {
    'roberta': {
        'class': RobertaTokenizerFast,
        'ckpt': 'roberta-base'
    }
}


class NewsData:
    """
    A class to handle news data preparation, downloading and loading.
    """
    def __init__(self, dataset_name: str, tokenizer_name: str,
                 batch_size: int = 8, num_workers: int = 8):
        # DATASET PROPERTIES
        self.dataset_name = dataset_name
        self.splits = DATASETS_PROPERTIES[dataset_name]['splits']
        self.article_column = DATASETS_PROPERTIES[dataset_name]['article_col']

        # TOKENIZER
        self.tokenizer = TOKENIZER_PROPERTIES[tokenizer_name]['class'].from_pretrained(
            TOKENIZER_PROPERTIES[tokenizer_name]['ckpt'])

        # ENCODE DATASET PATHS
        file_encoded_dataset = '{}-{}'.format(dataset_name, tokenizer_name)
        data_dir = '../Data/'
        file_path_encoded_dataset = data_dir + file_encoded_dataset

        # LOAD PROCESSED FROM DISK
        if file_encoded_dataset in os.listdir(data_dir):
            print("Encoded this one before, loading from disk: {}".format(file_path_encoded_dataset))
            self.dataset = load_from_disk(data_dir+file_encoded_dataset)

        # LOAD &amp; PROCESS DATA
        else:
            print("Did not encode this one before, loading and processing...")
            name = '3.0.0' if dataset_name == 'cnn_dailymail' else None
            assert self.dataset_name in list_datasets(), "Currently only supporting datasets from Huggingface"
            self.dataset = load_dataset(dataset_name, name=name)
            self.change_article_col_name()

            self.dataset = self.dataset.map(self.convert_to_features, batched=True)
            print(self.dataset.column_names)
            # Not the article itself since it cant be turned into torch tensor
            columns = ['attention_mask', 'input_ids']
            self.dataset.set_format(type='torch', columns=columns)
            print("Saving processed dataset to disk: {}".format(file_path_encoded_dataset))
            self.dataset.save_to_disk(file_path_encoded_dataset)

        # PREPARE DATA LOADERS
        self.dataloaders = {split: DataLoader(self.dataset[split],
                                              collate_fn=self.collate_fn,
                                              batch_size=batch_size,
                                              num_workers=num_workers) for split in self.splits}

    def collate_fn(self, examples: List[Dict[str, List[int]]]) -&gt; Dict[str, torch.Tensor]:
        """
        A function that assembles a batch. This is where padding is done, since it depends on
        the maximum sequence length in the batch.

        :param examples: list of truncated, tokenised &amp; encoded sequences
        :return: padded_batch (batch x max_seq_len)
        """

        # Combine the tensors into a padded batch
        padded_batch: Dict[str, torch.Tensor] = self.tokenizer.pad(examples, return_tensors='pt')

        return padded_batch

    def change_article_col_name(self) -&gt; None:
        """
        Changes the article text column name to 'article' for consistency.
        """
        article_col_name = DATASETS_PROPERTIES[self.dataset_name]['article_col']
        if article_col_name != 'article':
            self.dataset = self.dataset.map(lambda example: {'article': example[article_col_name]},
                                            remove_columns=[article_col_name])

    def convert_to_features(self, data_batch: OrderedDict) -&gt; OrderedDict:
        """
        Truncates and tokenises &amp; encodes a batch of text samples.

        -&gt;  Note: does not pad yet, this will be done in the DataLoader to allow flexible
            padding according to the longest sequence in the batch.

        :param data_batch: batch of text samples
        :return: encoded_batch: batch of samples with the encodings with the defined tokenizer added
        """
        # TODO: check whether this adds start and end of sentence tokens
        encoded_batch = self.tokenizer(data_batch['article'], truncation=True)

        return encoded_batch


if __name__ == "__main__":
    data = NewsData('cnn_dailymail', 'roberta')

    for batch_idx, batch in enumerate(data.dataloaders['train']):
        print("Batch {}".format(batch_idx))
        print("Batch consists of {}".format(batch.keys()))
        print("Of shapes {}".format([batch[key].shape for key in batch.keys()]))
        break
&lt;/denchmark-code&gt;

&lt;denchmark-h:h2&gt;Please reproduce using the BoringModel and post here&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;To Reproduce&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Expected behavior&lt;/denchmark-h&gt;

&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Note: Bugs with code are solved faster ! Colab Notebook should be made public !


IDE: Please, use our python bug_report_model.py template.


Colab Notebook: Please copy and paste the output from our environment collection script (or fill out the checklist below manually).


You can get the script and run it with:
&lt;denchmark-code&gt;wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
# For security purposes, please check the contents of collect_env_details.py before running it.
python collect_env_details.py
&lt;/denchmark-code&gt;


PyTorch Version (e.g., 1.0): 1.4.0.
OS (e.g., Linux): MacOS
How you installed PyTorch (conda, pip, source): pip
Build command you used (if compiling from source):
Python version: 3.6.12.
CUDA/cuDNN version: -
GPU models and configuration: -
Any other relevant information:

&lt;denchmark-code&gt;* CUDA:
        - GPU:
        - available:         False
        - version:           None
* Packages:
        - numpy:             1.19.1
        - pyTorch_debug:     False
        - pyTorch_version:   1.4.0
        - pytorch-lightning: 1.0.2
        - tqdm:              4.50.2
* System:
        - OS:                Darwin
        - architecture:
                - 64bit
                - 
        - processor:         i386
        - python:            3.6.12
        - version:           Darwin Kernel Version 19.5.0: Tue May 26 20:41:44 PDT 2020; root:xnu-6153.121.2~2/RELEASE_X86_64
&lt;/denchmark-code&gt;

&lt;denchmark-h:h3&gt;Additional context&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='ClaartjeBarkhof' date='2020-11-17T16:43:45Z'>
		&lt;denchmark-link:https://github.com/justusschock&gt;@justusschock&lt;/denchmark-link&gt;
 mind taking a look?
		</comment>
		<comment id='2' author='ClaartjeBarkhof' date='2020-11-18T13:16:14Z'>
		Hi &lt;denchmark-link:https://github.com/ClaartjeBarkhof&gt;@ClaartjeBarkhof&lt;/denchmark-link&gt;
,
Sorry for getting back so late. Does the issue still persist?
		</comment>
		<comment id='3' author='ClaartjeBarkhof' date='2020-12-31T18:18:50Z'>
		This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!
		</comment>
	</comments>
</bug>