<bug id='3348' author='harpone' open_date='2020-09-04T11:27:18Z' closed_time='2020-09-04T15:10:19Z'>
	<summary>Model resetting every 600 iterations?</summary>
	<description>
&lt;denchmark-h:h2&gt;üêõ Bug&lt;/denchmark-h&gt;

I'm getting a weird model param reset (maybe, not sure) every 600 iterations and haven't been able to pinpoint the reason... happens both on 2xV100 GPUs and v3-8 TPUs.
&lt;denchmark-link:https://user-images.githubusercontent.com/5112840/92233947-db126e00-eeb9-11ea-8c1f-7c3cd64db155.png&gt;&lt;/denchmark-link&gt;

Does anyone happen to know if something mystic is taking place at iteration 600? I'm using fairly default args and trainer = Trainer.from_argparse_args(args).
&lt;denchmark-h:h3&gt;Environment&lt;/denchmark-h&gt;

Happens both on PTL 0.9.0 and 0.9.1rc1. Probably not an environment issue, since the CUDA and XLA environments are quite different... I'm hoping someone will recognize the magical number 600.
	</description>
	<comments>
		<comment id='1' author='harpone' date='2020-09-04T12:11:31Z'>
		Update: actually this seems to be due to Weights and Biases logging! I'm looking at the stdout loss and it's not blowing up... investigating...
		</comment>
		<comment id='2' author='harpone' date='2020-09-04T15:10:19Z'>
		This seems to not be an issue with lightning, closing for now.
		</comment>
		<comment id='3' author='harpone' date='2020-09-07T10:07:39Z'>
		OK it's actually not wandb, since I'm getting this with tensorboard as well...
		</comment>
		<comment id='4' author='harpone' date='2020-09-07T17:59:53Z'>
		Could you try to verify with a different model? Or potentially share your code? We want to make sure this isn‚Äôt some weird behavior with your particular model.
		</comment>
		<comment id='5' author='harpone' date='2020-09-08T09:48:26Z'>
		Can't share code unfortunately because there's lots of dependencies and it's mixed with other company code... but I made some progress. Turns out the number of dataloader workers screw things up really bad for all logging basically (at least W&amp;B and Tensorboard). Here's some experiments with 1, 21, and 24 workers (I was using 24 workers before, which resulted in the "reset" at iter=600, but really that's just 24 * 25 so I think 25 is the "magic number" here):
&lt;denchmark-link:https://user-images.githubusercontent.com/5112840/92460298-79f7dc80-f1d0-11ea-92b4-547385d9f5a8.png&gt;&lt;/denchmark-link&gt;

So all is OK with 1 worker, but clearly there's some weird pseudo-random logging going on that depends on the number of workers (cumulative values until step = num_workers, then reset or something).
I was logging the above accuracies by calling self.trainer.logger.experiment.log({'global_step': self.global_step, 'loss_xent_manual': loss_xent, 'acc_manual': acc}), but similarly weird stuff happens with the Result classes.
I'm doing more or less this at train step:
&lt;denchmark-code&gt;    def training_step(self, batch, batch_idx):

        xs, ys = batch

        _, heads_out = self(xs)

        loss_dct = self.compute_losses(heads_out, ys)
        loss = loss_dct['loss']

        result = pl.TrainResult(minimize=loss)
        result.log('loss_trn', loss_dct['loss'], sync_dist=True)
        if self.args.get('classifier_head', False):
            result.log('loss_xent_trn', loss_dct['loss_xent'], sync_dist=True)
            result.log('acc_trn', loss_dct['acc'], sync_dist=True)
        if self.args.get('conv_head', False):
            result.log('loss_sim_trn', loss_dct['loss_sim'], sync_dist=True)

        if 1:

            # manual loss_xent and acc:
            targets = ys['class_id'].to(dtype=torch.int64)
            preds = torch.argmax(heads_out['classifier_head'], dim=-1).to(targets.dtype)
            acc = torch.eq(targets, preds).float().mean().item()
            loss_xent = self.loss_class_fn(heads_out['classifier_head'], targets).item()

            self.trainer.logger.experiment.log({'global_step': self.global_step,
                                                'loss_xent_manual': loss_xent,
                                                'acc_manual': acc})

        return result
&lt;/denchmark-code&gt;

and a similar validation step.
I'm using distributed_backend = 'ddp' in the Trainer args.
I'll go through an entire train/log step line by line with a debugger.
		</comment>
		<comment id='6' author='harpone' date='2020-09-08T10:06:49Z'>
		Actually it's not the logging - this happens at every training_step even if I just print the acc above... what's going on? Can't I use more than 1 dataloader worker??
		</comment>
		<comment id='7' author='harpone' date='2020-09-08T10:56:54Z'>
		OK it was actually this issue: &lt;denchmark-link:https://github.com/pytorch/pytorch/issues/5059&gt;pytorch/pytorch#5059&lt;/denchmark-link&gt;

So I had 24 dataloaders, each seeded exactly the same way =&gt; I got 24 identical minibatches, and the model quickly overfit to those, and resulted in major drop in acc/ increase in loss at iter=25!
I haven't actually used seeds in training at all... do I really need to seed_everything?
		</comment>
		<comment id='8' author='harpone' date='2020-09-08T14:34:56Z'>
		Are you using randomly generated data? DDP should split your data among the accelerators if it is a standard pytorch Dataset. It is very important to seed everything when using DDP as each accelerator will initialize it's own model, and they should have the same random initialization to work best.
		</comment>
		<comment id='9' author='harpone' date='2020-09-08T16:56:03Z'>
		Not randomly generated data, but note that there's 24 dataloader workers per each accelerator, which are all initialized with the same seed...
BTW this is potentially a very dangerous bug - I noticed it only because I was using such a high number of workers. If one used e.g. 4 or less, it wouldn't really show up at all, except probably as somewhat deteriorated training performance...
So I really suggest some safeguards when using seed_everything.
		</comment>
		<comment id='10' author='harpone' date='2020-09-08T16:58:04Z'>
		Wouldn't it be better to do one all_reduce step for model params before training starts to make sure all accelerators have models with same weights? I wouldn't really want to seed_everything in the first place...
		</comment>
		<comment id='11' author='harpone' date='2020-09-09T08:05:15Z'>
		or something like checking if the worker_init_fn is set when using seed_everything, like
&lt;denchmark-code&gt;loader = torch.utils.data.DataLoader(...)
if loader.worker_init_fn is None:
    (log a warning)
&lt;/denchmark-code&gt;

Adding the seeding worker_init_fn could probably be possible too, but that might override someone's worker_init_fn:s that do something else...
		</comment>
		<comment id='12' author='harpone' date='2020-09-09T14:00:06Z'>
		I am still trying to understand your issue. Looking through the PyTorch code, they &lt;denchmark-link:https://github.com/pytorch/pytorch/blob/ac79c874cefee2f8bc1605eed9a924d80c0b3542/torch/utils/data/dataloader.py#L715-L720&gt;increment the base seed by 1&lt;/denchmark-link&gt;
 when instantiating each dataloader
		</comment>
		<comment id='13' author='harpone' date='2020-09-09T15:43:59Z'>
		hmm yeah that's right... but I was using 'ddp', not 'ddp_spawn'... I guess 'ddp' uses the fork method to distribute the training, so the seed seems to be shared across the dataloader workers if you don't use a worker_init_fn that will explicitly.
Anyway, adding the worker_init_fn to do the seed increment explicitly definitely fixes the issue completely.
		</comment>
	</comments>
</bug>