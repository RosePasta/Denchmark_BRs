<bug id='401' author='kyoungrok0517' open_date='2019-10-21T08:22:36Z' closed_time='2019-10-21T09:53:29Z'>
	<summary>"RuntimeError: Address already in use" when running multiple multi-gpu training (DDP).</summary>
	<description>
Describe the bug
I see "RuntimeError: Address already in use" error message if I try to run two multi-gpu training session (using ddp) at the same time.
To Reproduce
Run two multi-gpu training session at the same time.
Expected behavior
Able to run two multi-gpu training session at the same time.

&lt;denchmark-link:https://user-images.githubusercontent.com/1051900/67188345-15505980-f427-11e9-823c-08cafbd0289b.png&gt;&lt;/denchmark-link&gt;

Desktop (please complete the following information):

OS: Ubuntu 18.04
Pytorch 1.3.0
CUDA 10.1

	</description>
	<comments>
		<comment id='1' author='kyoungrok0517' date='2019-10-21T09:24:51Z'>
		you have to set master_port yourself if running 2 ddp sessions on the same machien at the same time. This is a PyTorch limitation. in all the other ddp settings you wouldnâ€™t have to worry about this.
&lt;denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/Trainer/Distributed%20training/#multi-node&gt;https://pytorch-lightning.readthedocs.io/en/latest/Trainer/Distributed%20training/#multi-node&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='kyoungrok0517' date='2019-10-21T09:53:29Z'>
		Great. Thanks!
		</comment>
	</comments>
</bug>