<bug id='447' author='dehoyosb' open_date='2019-10-31T23:07:33Z' closed_time='2019-11-03T10:47:52Z'>
	<summary>Problems loading model from metrics</summary>
	<description>
Describe the bug
Hello! Good Day.
It appears im having a similar issue. I have the next model:
&lt;denchmark-code&gt;class Predictor(pl.LightningModule):
    def __init__(self, 
                nb_layers,
                 nb_lstm_units = 100, 
                 input_dim = 10, 
                 batch_size = 256, 
                 bilstm = False, 
                 dropout = 0.2, 
                 hidden_size = 100,
                 encoder_discrete = [],
                 embedding_dim = 10):
        
        super(Predictor, self).__init__()
  
       
        self.input_dim = input_dim
        self.nb_lstm_layers = nb_layers
        self.nb_lstm_units = nb_lstm_units
        self.batch_size = batch_size
        self.bilstm = bilstm
        self.dropout = dropout
        self.hidden_size = hidden_size
        self.unique_length_encoders = encoder_discrete
        self.embedding_dim = embedding_dim
        self.loss = RMSELoss()
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)
        
        self.__build_model()
        
    def __build_model(self):
       
        self.embedding = nn.ModuleList([nn.Embedding(num_embeddings = length + 1, 
                                         embedding_dim = self.embedding_dim,
                                         padding_idx = 0) for length in self.unique_length_encoders])

        # Design LSTM
        self.lstm = torch.nn.LSTM(input_size = self.input_dim,
                                  hidden_size = self.nb_lstm_units,
                                  num_layers = self.nb_lstm_layers,
                                  batch_first=True,
                                  dropout = self.dropout if self.dropout and self.nb_lstm_layers &gt; 1 else 0,
                                  bidirectional = self.bilstm).to(DEVICE)

        self.dense1 = torch.nn.Linear(self.nb_lstm_units*2 if self.bilstm else self.nb_lstm_units, 
                                      1).to(DEVICE)
        
        self.init_weigths()
        
    def init_hidden(self):
        hidden = torch.randn(self.nb_lstm_layers*2 if self.bilstm else self.nb_lstm_layers, 
                               self.batch_size, self.nb_lstm_units).to(DEVICE)
        cell = torch.randn(self.nb_lstm_layers*2 if self.bilstm else self.nb_lstm_layers, 
                               self.batch_size, self.nb_lstm_units).to(DEVICE)

        return (hidden, cell)
    
    def init_weigths(self):
        
        for name, param in self.lstm.named_parameters():
            if 'weight_hh' in name:
                torch.nn.init.orthogonal_(param)
            elif 'weight_ih' in name:
                torch.nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                torch.nn.init.zeros_(param)
        torch.nn.init.xavier_uniform_(self.dense1.weight)
        print('weigths initializer: done!')

    def forward(self, X_discrete, X_continuous, X_lengths):
        
        (self.hidden, self.cell) = self.init_hidden()
        
        for i, emb in enumerate(self.embedding):
            if i == 0:
                X_discrete_emb = emb(X_discrete[:,:,i])
            else:
                X_discrete_emb = torch.cat((X_discrete_emb, emb(X_discrete[:,:,i])), 2)
        
        X = torch.cat((X_discrete_emb,X_continuous), 2)
        
        X = pack_padded_sequence(X, X_lengths, batch_first=True, enforce_sorted=False)

        X, (self.hidden, self.cell) = self.lstm(X, (self.hidden, self.cell))
        
        X, _ = pad_packed_sequence(X, batch_first=True, padding_value=0)
        
        hidden = self.hidden.view(self.nb_lstm_layers, 2, -1,
                                  self.nb_lstm_units)[-1] if self.bilstm else self.hidden[-1]
        hidden = hidden.contiguous()
       
        hidden = hidden.view(-1, self.nb_lstm_units*2 if self.bilstm else self.nb_lstm_units)
       
        hidden = self.dense1(hidden)
        
        return hidden

    def my_loss(self, y_hat, y):
        return self.loss(y_hat, y)

    def training_step(self, batch, batch_nb):
        (x_discrete, x_continuous, lengths, y) = batch
        y_hat = self.forward(x_discrete, x_continuous, lengths)
        loss = self.my_loss(y_hat, y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def validation_step(self, batch, batch_nb):
        (x_discrete, x_continuous, lengths, y) = batch
        y_hat = self.forward(x_discrete, x_continuous, lengths)
        
        return {'val_loss': self.my_loss(y_hat, y)}

    def validation_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        tensorboard_logs = {'val_loss': avg_loss}
        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}

    def configure_optimizers(self):
        return [torch.optim.Adam(self.parameters(), lr=0.0001, weight_decay=5e-4, amsgrad = True)]

    @pl.data_loader
    def train_dataloader(self):
        return DataLoader(train_dataset, batch_size = self.batch_size, shuffle = True, collate_fn = my_collate)
    @pl.data_loader
    def val_dataloader(self):
        return DataLoader(val_dataset, batch_size = self.batch_size, shuffle = True, collate_fn = my_collate)
&lt;/denchmark-code&gt;

I have already trained the model, and i wasnt able to make inference because of not enough GPU memory.
Therefore a restarted my kernel, and decided to load the model again for inference with the next few lines of code:
&lt;denchmark-code&gt;model = Predictor.load_from_metrics(
                                weights_path='/lightning_logs/version_50/checkpoints/_ckpt_epoch_50.ckpt',
                                tags_csv="/lightning_logs/version_50/meta_tags.csv",
                            )
&lt;/denchmark-code&gt;

Im getting the next error:
&lt;denchmark-code&gt;---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-68-aa919fee611f&gt; in &lt;module&gt;
      1 model = HitsPredictor.load_from_metrics(
      2                                 weights_path='/home/daniel/trivago_challenge2/lightning_logs/version_50/checkpoints/_ckpt_epoch_50.ckpt',
----&gt; 3                                 tags_csv="/home/daniel/trivago_challenge2/lightning_logs/version_50/meta_tags_old.csv",
      4                             )
      5 

/opt/conda/envs/nlp/lib/python3.7/site-packages/pytorch_lightning/root_module/root_module.py in load_from_metrics(cls, weights_path, tags_csv)
    152 
    153         # load the state_dict on the model automatically
--&gt; 154         model = cls(hparams)
    155         model.load_state_dict(checkpoint['state_dict'])
    156 

&lt;ipython-input-67-161a6c83168a&gt; in __init__(self, nb_layers, nb_lstm_units, input_dim, batch_size, bilstm, dropout, hidden_size, encoder_discrete, embedding_dim)
     46 
     47         # Function that builds the actual model
---&gt; 48         self.__build_model()
     49 
     50     def __build_model(self):

&lt;ipython-input-67-161a6c83168a&gt; in __build_model(self)
     67                                   batch_first=True,
     68                                   # Appliying dropout only if we have more than 1 layer in the network
---&gt; 69                                   dropout = self.dropout if self.dropout and self.nb_lstm_layers &gt; 1 else 0,
     70                                   # Bidirectional if i choose so in the input.
     71                                   bidirectional = self.bilstm).to(DEVICE)

TypeError: '&gt;' not supported between instances of 'Namespace' and 'int'

&lt;/denchmark-code&gt;

For training i did the next:
&lt;denchmark-code&gt;model = Predictor(nb_layers = 3,
                      nb_lstm_units= 200,
                      input_dim = 71,
                      batch_size = 8192,
                      bilstm = False,
                      dropout = 0.2,
                      hidden_size = 100, 
                      encoder_discrete = [arr.shape[0] for arr in (encoder_discrete_variables.categories_ + 
                                                                   encoder_paths_ids.categories_)],
                      embedding_dim = 10)

trainer = pl.Trainer(max_nb_epochs=50, train_percent_check=1, gpus=[0], track_grad_norm=2, gradient_clip_val=0.5)

trainer.fit(model)
&lt;/denchmark-code&gt;

From what is have been testing, it seems that my file meta_data.csv file only has the headers key and value, but it didnt save any thing regarding the input parameters that i chose at the beggining of training. So, the first question is why it didnt save them? and the second one is: is there a way in which i can load those parameters in any way, so i can use the model?. This model training took almost 18 hours, and i dont have time to train it again right now.
Any help would be much appreciated
	</description>
	<comments>
		<comment id='1' author='dehoyosb' date='2019-11-01T15:06:24Z'>
		The loader assumes that you've followed the standard convention and passed your hyperparameters to the model constructor as a single . Take a look at &lt;denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pl_examples/basic_examples/lightning_module_template.py&gt;the examples&lt;/denchmark-link&gt;
 to see how this works.
In short, set your model constructor up to take one argument, which is the output of parse_args.
class MyModel(pl.LightningModule):
    def __init__(self, hparams):
        super(LightningTemplateModel, self).__init__()
        self.hparams = hparams
    ...

args = parser.parse_args()
model = MyModel(args)
		</comment>
	</comments>
</bug>