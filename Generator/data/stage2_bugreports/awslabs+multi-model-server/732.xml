<bug id='732' author='miguelvr' open_date='2019-01-23T16:59:11Z' closed_time='2019-03-01T22:06:32Z'>
	<summary>Batching</summary>
	<description>
Hey guys,
I'm setting up a face detection service and am currently using your SSD example as a baseline.
I've noticed that you have an assert here
&lt;denchmark-link:https://github.com/awslabs/mxnet-model-server/blob/aa8f8a3473e6f06780ffd362fb8722a65affd380/examples/model_service_template/mxnet_model_service.py#L51&gt;https://github.com/awslabs/mxnet-model-server/blob/aa8f8a3473e6f06780ffd362fb8722a65affd380/examples/model_service_template/mxnet_model_service.py#L51&lt;/denchmark-link&gt;

checking if the batch size is equal to 1
Does this mean I can not process multiple files at the same time?
More precisely, I would like to run an MTCNN service that detects all the faces in an image and then calll a Face Embedding service that computes all the embeddings of said faces.
Is it possible to batch the faces detected and compute the embeddings in one go?
I would like to know if it is just a matter of overloading the correct methods, ot if MMS does something clever in the background.
Thanks,
Miguel
	</description>
	<comments>
		<comment id='1' author='miguelvr' date='2019-01-23T17:27:11Z'>
		Following. Would be nice to have &lt;denchmark-link:https://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching&gt;a similar description &lt;/denchmark-link&gt;
of how batching is handled like for tensorflow serving.
		</comment>
		<comment id='2' author='miguelvr' date='2019-01-23T17:30:36Z'>
		&lt;denchmark-link:https://github.com/miguelvr&gt;@miguelvr&lt;/denchmark-link&gt;

MMS support automatic batch the HTTP request. You can use management API to register a model with batch support: &lt;denchmark-link:https://github.com/awslabs/mxnet-model-server/blob/master/docs/management_api.md#register-a-model&gt;https://github.com/awslabs/mxnet-model-server/blob/master/docs/management_api.md#register-a-model&lt;/denchmark-link&gt;

If you enabled batch mode for your model, your custom service code will receive batched request. It's your service code's responsibility to batch then and send the batch request to engine. The example code didn't implement batch.
The the MTCNN case, it's different from batch feature the MMS is supporting.
MMS didn't restrict what you can do in your service code. You can achieve what you need by override the handle function.
		</comment>
		<comment id='3' author='miguelvr' date='2019-01-23T17:43:44Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 thanks for the quick response! That's what I suspected.
It would be nice however to have this feature highlighted somewhere as it is common practice with most model servers
		</comment>
		<comment id='4' author='miguelvr' date='2019-01-23T17:58:50Z'>
		
@miguelvr
MMS support automatic batch the HTTP request. You can use management API to register a model with batch support: https://github.com/awslabs/mxnet-model-server/blob/master/docs/management_api.md#register-a-model
If you enabled batch mode for your model, your custom service code will receive batched request. It's your service code's responsibility to batch then and send the batch request to engine. The example code didn't implement batch.
The the MTCNN case, it's different from batch feature the MMS is supporting.
MMS didn't restrict what you can do in your service code. You can achieve what you need by override the handle function.

&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 If we use the model-archiver utility to package our models, how should we set batch_size and max_batch_delay? Via the management API? If so, it is peculiar that they are not available for setting by the model archiver.
		</comment>
		<comment id='5' author='miguelvr' date='2019-01-23T18:22:03Z'>
		&lt;denchmark-link:https://github.com/radao&gt;@radao&lt;/denchmark-link&gt;

In current implementation, management API is the only way to set batch size.
Exposing more options in model-archiver tool is on our road map. This feature will come together with new model archive format spec.
We are working on document about batch support and will provide example models that support batch.
		</comment>
		<comment id='6' author='miguelvr' date='2019-01-25T07:24:06Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 should we add an example show casing how to use batched requests in your request handler code, and showcasing the performance boost?
&lt;denchmark-link:https://github.com/vdantu&gt;@vdantu&lt;/denchmark-link&gt;
 as an FYI.
		</comment>
		<comment id='7' author='miguelvr' date='2019-01-25T07:40:00Z'>
		&lt;denchmark-link:https://github.com/lupesko&gt;@lupesko&lt;/denchmark-link&gt;
 that would be very helpful
		</comment>
		<comment id='8' author='miguelvr' date='2019-01-25T15:47:47Z'>
		I think this is definitely needed. I can look into writing an example with
batching.

Thanks,
Vamshi
&lt;denchmark-link:#&gt;â€¦&lt;/denchmark-link&gt;





		</comment>
		<comment id='9' author='miguelvr' date='2019-01-25T18:46:13Z'>
		We do have a story for working on this in our team's internal backlog, will try to prioritize it for one of the upcoming sprints.
		</comment>
		<comment id='10' author='miguelvr' date='2019-01-30T01:22:05Z'>
		&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;
 hi, may I ask how to change the model batch_size after starting the mms in the docker? when the mms starts in the docker, the model registered automatically, if I post a register request like:
&lt;denchmark-code&gt;curl -v -X POST "http://localhost:8086/models?url=resnet.mar&amp;model_name=image_retrieval&amp;batch_size=30"
&lt;/denchmark-code&gt;

It'll return a message: "message": "Model image_retrieval is already registered.".
If I DELETE the register model, and register model with command:
&lt;denchmark-code&gt;curl -v -X POST "http://localhost:8086/models?initial_workers=1&amp;synchronous=true&amp;url=resnet.mar"
&lt;/denchmark-code&gt;

The model registered, but still can't change the batch_size, even I PUT a request:
&lt;denchmark-code&gt;curl -v -X PUT "http://localhost:8086/models/resnet?batch_size=30&amp;synchronous=true"
&lt;/denchmark-code&gt;

it'll return:
&lt;denchmark-code&gt;*   Trying ::1...
* Connected to localhost (::1) port 8086 (#0)
&gt; PUT /models/image_retrieval?batch_size=30&amp;synchronous=true HTTP/1.1
&gt; Host: localhost:8086
&gt; User-Agent: curl/7.49.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; content-type: application/json
&lt; x-request-id: 2bf616da-bab8-40d7-86d6-a3893a55c4cd
&lt; content-length: 33
&lt; connection: keep-alive
&lt; 
{
  "status": "Workers scaled"
}
* Connection #0 to host localhost left intact
&lt;/denchmark-code&gt;

		</comment>
		<comment id='11' author='miguelvr' date='2019-01-30T01:48:48Z'>
		&lt;denchmark-link:https://github.com/JustinhoCHN&gt;@JustinhoCHN&lt;/denchmark-link&gt;
, could you please share what is the output of  after you tried to update its batch_size with POST request? I am trying to see what leads you to conclude that batch size is not updated.
Also, as a side note - you can set batch_size during register call (POST to /models endpoint). Consecutive calls to scale workers (PUT to /models/&lt;model_name&gt;) cannot change size of batch so you don't need that one.
		</comment>
		<comment id='12' author='miguelvr' date='2019-01-30T02:26:04Z'>
		&lt;denchmark-link:https://github.com/ddavydenko&gt;@ddavydenko&lt;/denchmark-link&gt;
 Thank you for your quick reply. Here is the return info after the POST request (I just post the request , not after DELETE the register model).
&lt;denchmark-code&gt;curl -v -X POST "http://localhost:8086/models?url=resnet.mar&amp;model_name=image_retrieval&amp;batch_size=30"
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;*   Trying ::1...
* Connected to localhost (::1) port 8086 (#0)
&gt; POST /models?url=resnet.mar&amp;model_name=image_retrieval&amp;batch_size=30 HTTP/1.1
&gt; Host: localhost:8086
&gt; User-Agent: curl/7.49.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 400 Bad Request
&lt; content-type: application/json
&lt; x-request-id: 921e4a3e-c882-47e1-907f-caeb1e0e525b
&lt; content-length: 112
&lt; 
{
  "code": 400,
  "type": "BadRequestException",
  "message": "Model image_retrieval is already registered."
}
* Connection #0 to host localhost left intact
&lt;/denchmark-code&gt;

Here's curl "http://localhost:8086/models/image_retrieval" output:
&lt;denchmark-code&gt;{
  "modelName": "image_retrieval",
  "modelUrl": "resnet.mar",
  "runtime": "python",
  "minWorkers": 1,
  "maxWorkers": 1,
  "batchSize": 1,
  "maxBatchDelay": 100,
  "workers": [
    {
      "id": "9000",
      "startTime": "2019-01-30T01:16:36.394Z",
      "status": "READY",
      "gpu": true,
      "memoryUsage": 1950052352
    }
  ]
}
&lt;/denchmark-code&gt;

If I DELETE the register model and POST again:
&lt;denchmark-code&gt;curl -v -X POST "http://localhost:8086/models?initial_workers=1&amp;batch_size=30&amp;synchronous=true&amp;url=resnet.mar&amp;model_name=image_retrieval"
&lt;/denchmark-code&gt;

&lt;denchmark-code&gt;*   Trying ::1...
* Connected to localhost (::1) port 8086 (#0)
&gt; POST /models?initial_workers=1&amp;batch_size=30&amp;synchronous=true&amp;url=resnet.mar&amp;model_name=image_retrieval HTTP/1.1
&gt; Host: localhost:8086
&gt; User-Agent: curl/7.49.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 404 Not Found
&lt; content-type: application/json
&lt; x-request-id: 116d61f2-f362-4006-8a2d-a106e977b2ab
&lt; content-length: 120
&lt; 
{
  "code": 404,
  "type": "ModelNotFoundException",
  "message": "Load failed... Deregistered model image_retrieval"
}
* Connection #0 to host localhost left intact
&lt;/denchmark-code&gt;

So I want to register again, I can only POST curl -v -X POST "http://localhost:8086/models?initial_workers=1&amp;synchronous=true&amp;url=resnet.mar without batch_size and other argument.
&lt;denchmark-code&gt;{
  "modelName": "resnet",
  "modelUrl": "resnet.mar",
  "runtime": "python",
  "minWorkers": 1,
  "maxWorkers": 1,
  "batchSize": 1,
  "maxBatchDelay": 100,
  "workers": [
    {
      "id": "9002",
      "startTime": "2019-01-30T02:22:44.951Z",
      "status": "READY",
      "gpu": true,
      "memoryUsage": 1854517248
    }
  ]
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='miguelvr' date='2019-01-30T03:47:35Z'>
		Ok, I think what's happening when you DELETE model (unregister it by posting DELETE to /models/&lt;model_name&gt;) is that MMS actually deletes its model archive (.mar file) as well. You can confirm that by checking what is left in /opt/ml/model folder (or whatever is in your model_store setting) right after DELETE call.
So I think that is why your register attempt right after DELETE call fails. In order to do it cleanly, I suggest you DELETE model after container bounced and then register it by referring to url of the model where it actually exists (maybe on S3) so that MMS would have a strong link to where it could download model from. During this same POST call you can specify batch_size param.
I admit this is not the best experience for MMS users, but hopefully this workaround could be sufficient for now. Please let me know if this approach works for you.
		</comment>
		<comment id='14' author='miguelvr' date='2019-01-30T04:46:36Z'>
		After some more digging around and reproducing issue I can confirm this is actually a bug with MMS. The call to register model with batch_size param works only if there is no synchronous parameter specified. So after DELETE of the model call like this: curl -X POST "localhost:8081/models?url=resnet50_ssd.mar&amp;model_name=resnet50&amp;batch_size=8&amp;initial_workers=4" works fine, but call like this curl -X POST "localhost:8081/models?url=resnet50_ssd.mar&amp;model_name=resnet50&amp;batch_size=8&amp;initial_workers=4&amp;synchronous=true"  fails with { "code": 404, "type": "ModelNotFoundException", "message": "Load failed... Deregistered model resnet50" }
We will prioritize this as a fix in near future. In the meantime you can use async approach to register model in order to change its batch size.
		</comment>
		<comment id='15' author='miguelvr' date='2019-01-30T05:01:55Z'>
		&lt;denchmark-link:https://github.com/ddavydenko&gt;@ddavydenko&lt;/denchmark-link&gt;
 Great! I'll try your suggestion and will let you know my result later. You guys are so effiecient! Thanks again!
		</comment>
		<comment id='16' author='miguelvr' date='2019-01-30T06:12:24Z'>
		so I remove the synchronous params from the POST request, the model registered successfully, but the model seems not loading:
&lt;denchmark-code&gt;{
  "modelName": "image_retrieval",
  "modelUrl": "resnet_triplet.mar",
  "runtime": "python",
  "minWorkers": 1,
  "maxWorkers": 1,
  "batchSize": 30,
  "maxBatchDelay": 100,
  "workers": [
    {
      "id": "9002",
      "startTime": "2019-01-30T06:08:29.942Z",
      "status": "UNLOADING",
      "gpu": true,
      "memoryUsage": 0
    }
  ]
}
&lt;/denchmark-code&gt;

status: unloading. MMS didn't start the model service, and didn't use any gpu memory. How to start the worker and make it load the model again? &lt;denchmark-link:https://github.com/ddavydenko&gt;@ddavydenko&lt;/denchmark-link&gt;

		</comment>
		<comment id='17' author='miguelvr' date='2019-01-30T07:14:45Z'>
		Hm, this might be even bigger issue than initially thought. Let us investigate on our side and comment on findings within a day or two. Sorry for the inconvenience :(

â€”
Thanks,
Denis
&lt;denchmark-link:#&gt;â€¦&lt;/denchmark-link&gt;


 On Jan 29, 2019, at 10:12 PM, Justin.h ***@***.***&gt; wrote:

 so I remove the synchronous params from the POST request, the model registered successfully, but the model seems not loading:

 {
   "modelName": "image_retrieval",
   "modelUrl": "resnet_triplet.mar",
   "runtime": "python",
   "minWorkers": 1,
   "maxWorkers": 1,
   "batchSize": 30,
   "maxBatchDelay": 100,
   "workers": [
     {
       "id": "9002",
       "startTime": "2019-01-30T06:08:29.942Z",
       "status": "UNLOADING",
       "gpu": true,
       "memoryUsage": 0
     }
   ]
 }
 status: unloading. MMS didn't start the model service, and didn't use any gpu memory. How to start the worker and make it load the model again? @ddavydenko

 â€”
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or mute the thread.


		</comment>
		<comment id='18' author='miguelvr' date='2019-01-30T10:26:54Z'>
		I would also like to understand the behavior of MMS batching as it is not clear from the little description provided.
When setting a specific value X for batch size, does it mean the model server will only accept batch sizes of X or is it up to X, with X being an upper bound?
My goal is to have some servers accepting a variable number of samples at the same time, depending of what is available to be processed, so having an upper bound would be useful while having a fixed batch size, not so much.
		</comment>
		<comment id='19' author='miguelvr' date='2019-01-30T16:07:47Z'>
		&lt;denchmark-link:https://github.com/JustinhoCHN&gt;@JustinhoCHN&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/ddavydenko&gt;@ddavydenko&lt;/denchmark-link&gt;
 : I am unable to recreate the issue with initial workers and batch size configured. I did the following

Downloaded the squeezenet_v1.1.mar from MMS model-zoo into /tmp/model-store folder
Started MMS as follows

&lt;denchmark-code&gt;mxnet-model-server --model-store /tmp/model-store
&lt;/denchmark-code&gt;


Ran the following register command

&lt;denchmark-code&gt;curl -X POST localhost:8081/models?url=squeezenet_v1.1.mar\&amp;batch_size=30\&amp;max_batch_delay=40\&amp;initial_workers=1\&amp;synchronous=true
&lt;/denchmark-code&gt;

It seems to load fine
&lt;denchmark-code&gt;curl localhost:8081/models/squeezenet_v1.1

{
  "modelName": "squeezenet_v1.1",
  "modelVersion": "1.0",
  "modelUrl": "squeezenet_v1.1.mar",
  "engine": "MXNet",
  "runtime": "python",
  "minWorkers": 1,
  "maxWorkers": 1,
  "batchSize": 30,
  "maxBatchDelay": 40,
  "workers": [
    {
      "id": "9000",
      "startTime": "2019-01-30T07:45:28.133Z",
      "status": "READY",
      "gpu": false,
      "memoryUsage": 131043328
    }
  ]
}
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://github.com/JustinhoCHN&gt;@JustinhoCHN&lt;/denchmark-link&gt;
 : If you are seeing that the model is in "UNLOADING" state, that means your model wasn't loaded properly. Look at the  in the logs folder. It would have logs of what the issue is. Or, share your log file so that we can also take a look and try to help out.
&lt;denchmark-link:https://github.com/miguelvr&gt;@miguelvr&lt;/denchmark-link&gt;
 : We are in the process of writing a document. It will be out shortly. But, for now let me try to answer your questions here,

When setting a specific value X for batch size, does it mean the model server will only accept batch sizes of X or is it up to X, with X being an upper bound?

Short Answer : It is upto X messages. You get to choose the batch_size and the max_batch_delay for every model you register. Currently there is no way to configure these two params at startup. The max_batch_delay timer for a particular model starts when the model server receives the first inference request for that model and it keeps getting rescheduled as long as there are inference requests in the queue. If you receive X messages before max_batch_delay time expires, then the timer is switched off and the backend worker is given all the X messages. If you receive X-delta messages before max_batch_delay timeout, then the backend worker is given only X-delta messages.
		</comment>
		<comment id='20' author='miguelvr' date='2019-01-31T09:29:39Z'>
		&lt;denchmark-link:https://github.com/vdantu&gt;@vdantu&lt;/denchmark-link&gt;
 I'm afraid it didn't work, I tried again and still the same. BTW I'm using Docker, maybe you should try the docker version. Here's the log: &lt;denchmark-link:https://github.com/vdantu&gt;@vdantu&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ddavydenko&gt;@ddavydenko&lt;/denchmark-link&gt;

&lt;denchmark-link:https://github.com/awslabs/mxnet-model-server/files/2816521/mms_log.log&gt;mms_log.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='21' author='miguelvr' date='2019-01-31T15:49:32Z'>
		I tried with docker container 1.0.1 CPU as well, and it works.  Could you
let us know what container are you using? Also, can you share your log file?

Thanks,
Vamshi
&lt;denchmark-link:#&gt;â€¦&lt;/denchmark-link&gt;


On Thu, Jan 31, 2019, 1:29 AM Justin.h ***@***.*** wrote:
 @vdantu &lt;https://github.com/vdantu&gt; I'm afraid it didn't work, I tried
 again and still the same. BTW I'm using Docker, maybe you should try the
 docker version.

 â€”
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#732 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AiiLNEad2GrYAnehtDV6UpUp2L7dyA4Mks5vIreEgaJpZM4aPNsV&gt;
 .



		</comment>
		<comment id='22' author='miguelvr' date='2019-02-06T02:29:15Z'>
		&lt;denchmark-link:https://github.com/vdantu&gt;@vdantu&lt;/denchmark-link&gt;
  Sorry for the late reply, these days are Chinese lunar new year, happy Chinese new year! I'm using the 1.0.0 GPU docker container, here's my log: &lt;denchmark-link:https://github.com/awslabs/mxnet-model-server/files/2816521/mms_log.log&gt;mms_log.log&lt;/denchmark-link&gt;

		</comment>
		<comment id='23' author='miguelvr' date='2019-02-10T08:41:54Z'>
		hi &lt;denchmark-link:https://github.com/ddavydenko&gt;@ddavydenko&lt;/denchmark-link&gt;
 , any progress for now?
&lt;denchmark-link:https://github.com/vdantu&gt;@vdantu&lt;/denchmark-link&gt;
 , I tried the 1.0.1 CPU container, still the same problem that mentioned above, can you provide your command detail?
I ran the docker container like:
&lt;denchmark-code&gt;sudo docker run -itd --name mms -p 8087:8087  -p 8088:8088 -v \
/home/huzhihao/projects/image_retrieval_for_supply_chain/models/:/models \
awsdeeplearningteam/mxnet-model-server:1.0.1-mxnet-cpu \
mxnet-model-server --start \
--mms-config /models/config.properties \
--model-store=/models \
--models image_retrieval=resnet_triplet.mar
&lt;/denchmark-code&gt;

I'm using shared volume which contains config.properties and .mar file, if I register the model with:
&lt;denchmark-code&gt;curl -v -X POST localhost:8088/models?url=resnet_triplet.mar\&amp;model_name=image_retrieval\&amp;batch_size=30\&amp;max_batch_delay=40\&amp;initial_workers=1\&amp;synchronous=true
&lt;/denchmark-code&gt;


"Load failed... Deregistered model image_retrieval" 404 error raised.

If I register the model with:
&lt;denchmark-code&gt;curl -v -X POST localhost:8088/models?url=resnet_triplet.mar\&amp;model_name=image_retrieval\&amp;batch_size=30\&amp;max_batch_delay=40\&amp;initial_workers=1
&lt;/denchmark-code&gt;


"Model image_retrieval is already registered."

If I delete the model and register again with:
&lt;denchmark-code&gt;curl -v -X POST localhost:8088/models?url=resnet_triplet.mar\&amp;model_name=image_retrieval\&amp;batch_size=30\&amp;max_batch_delay=40\&amp;initial_workers=1
&lt;/denchmark-code&gt;

logs will output:
&lt;denchmark-code&gt;AssertionError: Batch is not supported.
Load model failed: image_retrieval, error: Worker died.
W-9044-image_retrieval State change WORKER_STARTED -&gt; WORKER_STOPPED
Retry worker: 9044 in 8 seconds.
&lt;/denchmark-code&gt;

		</comment>
		<comment id='24' author='miguelvr' date='2019-02-11T04:42:02Z'>
		&lt;denchmark-link:https://github.com/JustinhoCHN&gt;@JustinhoCHN&lt;/denchmark-link&gt;
 : I went through the logs you had sent. So the example models in our model zoo don't support batching by default.
. But, in the mean time to unblock you,

You are getting this error from your custom service code, here.
You have to modify the preprocess, inference and post process code to handle a list of requests.

MMS sends a list of inputs to the preprocess, seen &lt;denchmark-link:https://github.com/awslabs/mxnet-model-server/blob/master/examples/model_service_template/mxnet_model_service.py#L99&gt;here as parameter named batch&lt;/denchmark-link&gt;
. Your model code should use this list of input requests to preprocess and send it to the inference method.
		</comment>
		<comment id='25' author='miguelvr' date='2019-03-02T02:37:50Z'>
		Thank you &lt;denchmark-link:https://github.com/vdantu&gt;@vdantu&lt;/denchmark-link&gt;
 and mms team, that's what we want, it really helps! Few days ago I stuck in how to handle number of requests which less than a batch, your padding method solved this problem! Thanks again!
		</comment>
		<comment id='26' author='miguelvr' date='2019-03-02T04:23:22Z'>
		&lt;denchmark-link:https://github.com/JustinhoCHN&gt;@JustinhoCHN&lt;/denchmark-link&gt;
 : Thats awesome :) . Really glad that the batching work we did helped. I could think of two options for variable batch size

Padding with 0's and ignoring this result on return path, or
Re-bind the network at every inference request.

Since this logic is in the inference path , i.e., re-bind would have to be done for every batch of request that comes in, it can quickly become very inefficient :) . So I chose to go with "padding with 0's" solution.
		</comment>
		<comment id='27' author='miguelvr' date='2019-03-06T08:01:44Z'>
		
@radao
In current implementation, management API is the only way to set batch size.
Exposing more options in model-archiver tool is on our road map. This feature will come together with new model archive format spec.
We are working on document about batch support and will provide example models that support batch.

&lt;denchmark-link:https://github.com/frankfliu&gt;@frankfliu&lt;/denchmark-link&gt;

Is there a way to contribute to configuring the batch size through the model spec? I'd be happy to help.
		</comment>
		<comment id='28' author='miguelvr' date='2019-03-06T18:11:14Z'>
		&lt;denchmark-link:https://github.com/erandagan&gt;@erandagan&lt;/denchmark-link&gt;
 , awesome to see your activities, keep 'em coming! :)
As for this particular question - could you please clarify what exactly you are trying to achieve? Describing it from user perspective would probably be the best way.
		</comment>
		<comment id='29' author='miguelvr' date='2019-03-06T18:38:01Z'>
		&lt;denchmark-link:https://github.com/ddavydenko&gt;@ddavydenko&lt;/denchmark-link&gt;
 Sure.
We're deploying a (set of) MMS servers using K8s, and it would be more friendly to operate if we could configure the batch settings directly within the model, rather than POSTing the model along with it's settings after deployment.
This makes things a lot easier from an operational standpoint, as POSTing to the model server is actually mutating the server's state. So, currently, if the server were to crash and K8s would spawn a new server pod, we'd have to reconfigure that pod again as it would boot into a "wrong" state. In contrast, if the batch size was set in the model archive, the server would boot directly into the desired state.
		</comment>
		<comment id='30' author='miguelvr' date='2019-03-13T17:14:30Z'>
		&lt;denchmark-link:https://github.com/erandagan&gt;@erandagan&lt;/denchmark-link&gt;
 : Thats a good feature to have as well. Could you have share a short writeup on what changes you were proposing?
IMO, there is a model section in the model-archive (&lt;denchmark-link:https://github.com/awslabs/mxnet-model-server/blob/master/model-archiver/model_archiver/manifest_components/model.py&gt;https://github.com/awslabs/mxnet-model-server/blob/master/model-archiver/model_archiver/manifest_components/model.py&lt;/denchmark-link&gt;
). This could be a good place for "max_batch_size", "max_batch_delay".
Frontend "Model.java" already has a copy of the ModelArchive, which contains Manifest with this Model.
		</comment>
		<comment id='31' author='miguelvr' date='2019-03-14T07:32:59Z'>
		&lt;denchmark-link:https://github.com/vdantu&gt;@vdantu&lt;/denchmark-link&gt;
 I'll follow up on &lt;denchmark-link:https://github.com/awslabs/multi-model-server/issues/770&gt;#770&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>