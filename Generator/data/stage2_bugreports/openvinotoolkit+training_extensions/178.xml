<bug id='178' author='FengYen-Chang' open_date='2019-11-12T08:36:21Z' closed_time='2019-11-15T01:17:01Z'>
	<summary>Failed resume on INT8 w/ 60% of sparsity model</summary>
	<description>
Describe the bug
Missing some keys when resuming the model.
Resnet50:
RuntimeError: Error(s) when loading model parameters:
        Missing key(s):
                "module.module.conv1.pre_ops.0.op._mask",
                "module.module.conv1.pre_ops.0.op.uniform",
                "module.module.layer1.0.conv1.pre_ops.0.op._mask",
                "module.module.layer1.0.conv1.pre_ops.0.op.uniform",
                "module.module.layer1.0.conv2.pre_ops.0.op._mask",
...
Inception_v3:
RuntimeError: Error(s) when loading model parameters:
        Missing key(s):
                "module.module.Conv2d_1a_3x3.conv.pre_ops.0.op._mask",
                "module.module.Conv2d_1a_3x3.conv.pre_ops.0.op.uniform",
                "module.module.Conv2d_2a_3x3.conv.pre_ops.0.op._mask",
                "module.module.Conv2d_2a_3x3.conv.pre_ops.0.op.uniform",
                "module.module.Conv2d_2b_3x3.conv.pre_ops.0.op._mask",
...
Steps to Reproduce
Following README, download the pre-trained INT8 w/ 60% of sparsity model, resnet50 and inception_v3, and then follow the below command resumes the model and convert to .onnx.
Resnet50:
python3 main.py -m test --config=configs/sparsity_quantization/inceptionV3_imagenet_sparsity_int8.json --resume=inceptionV3_imagenet_sparsity_int8.pth --to-onnx=resnet50_sparse_int8.onnx
Inception_v3:
python3 main.py -m test --config=configs/sparsity_quantization/inceptionV3_imagenet_sparsity_int8.json --resume=inceptionV3_imagenet_sparsity_int8.pth --to-onnx=inceptionV3_sparse_int8.onnx
Environment:

OS: Linux Ubuntu 16.04
Framework version: PyTorch 1.3.1
Python version: 3.6.7
OpenVINO version: 2019 R3.1
CUDA/cuDNN version: 10.1
GPU model and memory: 11GB * 2

	</description>
	<comments>
		<comment id='1' author='FengYen-Chang' date='2019-11-12T08:55:16Z'>
		&lt;denchmark-link:https://github.com/AlexKoff88&gt;@AlexKoff88&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='FengYen-Chang' date='2019-11-12T10:07:55Z'>
		mobilenetV2_imagenet_sparsity_int8 and resnet18_imagenet_bin_xnor also have this problem. Could you confirm this?
		</comment>
		<comment id='3' author='FengYen-Chang' date='2019-11-12T10:50:44Z'>
		cc'ed &lt;denchmark-link:https://github.com/ljaljushkin&gt;@ljaljushkin&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://github.com/vshampor&gt;@vshampor&lt;/denchmark-link&gt;
. Could you please answer?
		</comment>
		<comment id='4' author='FengYen-Chang' date='2019-11-12T13:40:18Z'>
		Greetings, &lt;denchmark-link:https://github.com/FengYen-Chang&gt;@FengYen-Chang&lt;/denchmark-link&gt;
 !
It seems that some of the sparsity-compressed checkpoints available by links in the README were produced using an older version of NNCF; as such, these won't be able to be resumed from to continue NNCF-enabled finetuning because they are missing some vital module fields required for the RB-sparsity algorithm to work.
The checkpoints do, however, contain the binary sparsity masks themselves, and therefore the checkpoints may be used to convert the model to ONNX format using your command lines if you apply a following workaround:
In the .json config files corresponding to the problematic .pth files, replace the entire JSON sub-tree referring to "rb_sparsity" compression algorithm with a tree specifying the "const_sparsity" algorithm, i.e. the JSON subtree of this kind:
&lt;denchmark-code&gt;        {
            "algorithm": "rb_sparsity",
            "params": {
                "sparsity_init": 0.01,
                "sparsity_target": 0.61,
                "sparsity_steps": 5,
                "sparsity_training_steps": 10
            }
        },
&lt;/denchmark-code&gt;

should be replaced by:
&lt;denchmark-code&gt;        {
            "algorithm": "const_sparsity"
        },
&lt;/denchmark-code&gt;

You should be able to run the command lines you specified without trouble then. We will update the .pth checkpoints within the next NNCF release so that they may be used without the workaround above.
		</comment>
		<comment id='5' author='FengYen-Chang' date='2019-11-13T03:40:43Z'>
		Hi &lt;denchmark-link:https://github.com/vshampor&gt;@vshampor&lt;/denchmark-link&gt;
,
Thanks a lot. I have converted resnet50, inceptionV3, mobilenetV2 to ONNX. But the XNOR resnet18 still can't convert. Below is the error message.
RuntimeError: Error(s) when loading model parameters:
        Skipped key(s):
                "module.module.layer1.0.conv1.pre_ops.1.op.threshold",
                "module.module.layer1.0.conv2.pre_ops.1.op.threshold",
                "module.module.layer1.1.conv1.pre_ops.1.op.threshold",
                "module.module.layer1.1.conv2.pre_ops.1.op.threshold",
                "module.module.layer2.0.conv1.pre_ops.1.op.threshold",
                "module.module.layer2.0.conv2.pre_ops.1.op.threshold",
                "module.module.layer2.1.conv1.pre_ops.1.op.threshold",
                "module.module.layer2.1.conv2.pre_ops.1.op.threshold",
                "module.module.layer3.0.conv1.pre_ops.1.op.threshold",
                "module.module.layer3.0.conv2.pre_ops.1.op.threshold",
                "module.module.layer3.1.conv1.pre_ops.1.op.threshold",
                "module.module.layer3.1.conv2.pre_ops.1.op.threshold",
                "module.module.layer4.0.conv1.pre_ops.1.op.threshold",
                "module.module.layer4.0.conv2.pre_ops.1.op.threshold",
                "module.module.layer4.1.conv1.pre_ops.1.op.threshold",
                "module.module.layer4.1.conv2.pre_ops.1.op.threshold". 
        Unexpected key(s):
                "module.layer1.0.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer1.0.conv2.pre_ops.1.op.scale_is_initialized",
                "module.layer1.1.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer1.1.conv2.pre_ops.1.op.scale_is_initialized",
                "module.layer2.0.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer2.0.conv2.pre_ops.1.op.scale_is_initialized",
                "module.layer2.1.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer2.1.conv2.pre_ops.1.op.scale_is_initialized",
                "module.layer3.0.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer3.0.conv2.pre_ops.1.op.scale_is_initialized",
                "module.layer3.1.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer3.1.conv2.pre_ops.1.op.scale_is_initialized",
                "module.layer4.0.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer4.0.conv2.pre_ops.1.op.scale_is_initialized",
                "module.layer4.1.conv1.pre_ops.1.op.scale_is_initialized",
                "module.layer4.1.conv2.pre_ops.1.op.scale_is_initialized". 
        Missing key(s):
                "module.module.layer1.0.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer1.0.conv2.pre_ops.1.op.scale_initialized",
                "module.module.layer1.1.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer1.1.conv2.pre_ops.1.op.scale_initialized",
                "module.module.layer2.0.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer2.0.conv2.pre_ops.1.op.scale_initialized",
                "module.module.layer2.1.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer2.1.conv2.pre_ops.1.op.scale_initialized",
                "module.module.layer3.0.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer3.0.conv2.pre_ops.1.op.scale_initialized",
                "module.module.layer3.1.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer3.1.conv2.pre_ops.1.op.scale_initialized",
                "module.module.layer4.0.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer4.0.conv2.pre_ops.1.op.scale_initialized",
                "module.module.layer4.1.conv1.pre_ops.1.op.scale_initialized",
                "module.module.layer4.1.conv2.pre_ops.1.op.scale_initialized".
Not sure it was a version issue or not, but the node name duplicate module.
		</comment>
		<comment id='6' author='FengYen-Chang' date='2019-11-13T11:55:01Z'>
		Greetings, &lt;denchmark-link:https://github.com/FengYen-Chang&gt;@FengYen-Chang&lt;/denchmark-link&gt;
 !
This is once again an issue with the mismatch between exported .pth checkpoint formats and current NNCF state, sorry about that. Still, you can convert the model to .onnx: try using the --weights key instead of --resume when specifying the source .pth checkpoint to be converted from to the scripts.
		</comment>
		<comment id='7' author='FengYen-Chang' date='2019-11-13T15:46:53Z'>
		&lt;denchmark-link:https://github.com/vshampor&gt;@vshampor&lt;/denchmark-link&gt;
 Thanks, it works for me. So, the  is mainly for training and the  just re-load the parameters. Do I get it right?
And I also got some import errors when resuming the binarize models, and I make some workaround.
  File "main.py", line 94, in main
    from examples.classification.binarization_worker import main_worker_binarization
ImportError: No module named 'examples.classification'
and
  File "/tmp/openvino_training_extensions/pytorch_toolkit/nncf/examples/classification/binarization_worker.py", line 35, in &lt;module&gt;
    from examples.classification.main import create_dataloaders, AverageMeter, accuracy, get_lr, validate
ImportError: No module named 'examples.classification'
Replace it by
from binarization_worker import main_worker_binarization
and
from main import create_dataloaders, AverageMeter, accuracy, get_lr, validate
Not sure it is an issue or not, just let you know. Thanks for your help, thanks.
		</comment>
		<comment id='8' author='FengYen-Chang' date='2019-11-14T08:27:03Z'>
		&lt;denchmark-link:https://github.com/FengYen-Chang&gt;@FengYen-Chang&lt;/denchmark-link&gt;
 ,  does strict checks on loaded model checkpoint parameters vs. what is required in the model instantiated inside PyTorch, while  just does best-effort parameter loading.
 is indispensable while doing compression fine-tune training starting from a full-precision uncompressed model; on the other hand,  is used for continuing training with the same config/training script if for some reason the training process had been interrupted, and also during evaluation runs in the  mode with one of the example checkpoints available via README.md links, to be sure that we evaluate the same model that is being instantiated inside PyTorch.
We aim to make all published checkpoints possible to be evaluated via --resume, but sadly, this is not yet the case due to mismatches between Python model code when the published checkpoints were originally trained and current NNCF state; still, --weights workaround works for the most part.
		</comment>
	</comments>
</bug>