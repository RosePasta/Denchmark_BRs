<bug id='281' author='dandanli814' open_date='2020-04-23T10:01:21Z' closed_time='2020-11-19T14:26:14Z'>
	<summary>error when converting onnx to openvion</summary>
	<description>
Thank you for your work.
When after finetuning the model using NNCF, I want to transfer the .pth model to onnx and then convert onnx to openvion, an error occurred, as follows.
First of all, when I was finetuning, the h and w are scaled to a fixed scale like [b, c, 64, 1024]. But when I want to inference, the input shape is [b, c, 64, -1],  -1 means that the width will maintain the same scaling factor with height.
So based on that, first I modified your code &lt;denchmark-link:https://github.com/opencv/openvino_training_extensions/blob/develop/pytorch_toolkit/nncf/nncf/compression_method_api.py#L166&gt;https://github.com/opencv/openvino_training_extensions/blob/develop/pytorch_toolkit/nncf/nncf/compression_method_api.py#L166&lt;/denchmark-link&gt;
 as following:
with torch.no_grad():
torch.onnx.export(model, tuple(input_tensor_list),
filename, verbose=True, input_names = ['input'],
dynamic_axes={'input':[3]})
Then I use your code to transfer my .pth model to onnx. Then I run this:
python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model ./results/my_int8.onnx --output_dir ./results/
But an error occurs as follows:
Model Optimizer arguments:
Common parameters:
- Path to the Input Model:      ./results/my_int8.onnx
- Path for generated IR:        ./results/
- IR output name:       my_int8
- Log level:    ERROR
- Batch:        Not specified, inherited from the model
- Input layers:         Not specified, inherited from the model
- Output layers:        Not specified, inherited from the model
- Input shapes:         Not specified, inherited from the model
- Mean values:  Not specified
- Scale values:         Not specified
- Scale factor:         Not specified
- Precision of IR:      FP32
- Enable fusing:        True
- Enable grouped convolutions fusing:   True
- Move mean values to preprocess section:       False
- Reverse input channels:       False
ONNX specific parameters:
Model Optimizer version:        2020.1.0-61-gd349c3ba4a
[ ERROR ]  Shape [  1 128   4  -1] is not fully defined for output 0 of "1385". Use --input_shape with positive integers to override model input shapes.
[ ERROR ]  Cannot infer shapes or values for node "1385".
[ ERROR ]  Not all output shapes were inferred or fully defined for node "1385".
For more information please refer to Model Optimizer FAQ (&lt;denchmark-link:https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html&gt;https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html&lt;/denchmark-link&gt;
), question &lt;denchmark-link:https://github.com/openvinotoolkit/training_extensions/pull/40&gt;#40&lt;/denchmark-link&gt;
.
[ ERROR ]
[ ERROR ]  It can happen due to bug in custom shape infer function &lt;function Elementwise... at 0x7f3b17b0c488&gt;.
[ ERROR ]  Or because the node inputs have incorrect values/shapes.
[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).
[ ERROR ]  Run Model Optimizer with --log_level=DEBUG for more information.
[ ERROR ]  Exception occurred during running replacer "REPLACEMENT_ID" (&lt;class 'extensions.middle.PartialInfer.PartialInfer'&gt;): Stopped shape/value propagation at "1385" node.
For more information please refer to Model Optimizer FAQ (&lt;denchmark-link:https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html&gt;https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimizer_FAQ.html&lt;/denchmark-link&gt;
), question &lt;denchmark-link:https://github.com/openvinotoolkit/training_extensions/issues/38&gt;#38&lt;/denchmark-link&gt;
.
I don't know where the problem is. I hope your can help me. Thank you.
	</description>
	<comments>
		<comment id='1' author='dandanli814' date='2020-04-23T12:13:46Z'>
		&lt;denchmark-link:https://github.com/AlexKoff88&gt;@AlexKoff88&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/vshampor&gt;@vshampor&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ljaljushkin&gt;@ljaljushkin&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='dandanli814' date='2020-04-23T12:31:25Z'>
		Greetings, &lt;denchmark-link:https://github.com/dandanli814&gt;@dandanli814&lt;/denchmark-link&gt;
 !
Sadly, AFAIK Model Optimizer does not support dynamic model inputs when generating IR. If you really want your OpenVINO-powered model to infer inputs with different shapes in a target application, you should instead use Inference Engine's model reshaping mechanisms each time your input shape is changed. See &lt;denchmark-link:https://docs.openvinotoolkit.org/latest/_docs_IE_DG_ShapeInference.html&gt;https://docs.openvinotoolkit.org/latest/_docs_IE_DG_ShapeInference.html&lt;/denchmark-link&gt;
 and general OpenVINO documentation for more details.
I would further suggest that you first try to enable your model execution with OpenVINO using static input shapes in ONNX (if dynamic inputs are not critical for your model or scenario), and then, once the static input model is working, experiment with reshaping.
		</comment>
		<comment id='3' author='dandanli814' date='2020-04-23T12:40:03Z'>
		OK, I will try the static input model first, thank you very much.
		</comment>
	</comments>
</bug>