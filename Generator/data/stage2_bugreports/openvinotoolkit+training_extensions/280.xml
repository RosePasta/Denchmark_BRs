<bug id='280' author='minhphai' open_date='2020-04-22T03:38:19Z' closed_time='2020-11-19T14:40:33Z'>
	<summary>face-recognition focal loss issue</summary>
	<description>
Command-line:
python train.py 
--train_data_root /dataset/vgg_face2/train/ 
--train_list /vgg_face2/train_list.txt 
--train_landmarks /dataset/vgg_face2/bb_landmark/ 
--val_data_root /dataset/lfw/ 
--val_list /home /lfw/pairs.txt 
--val_landmarks /dataset/lfw/lfw_landmark.txt 
--train_batch_size 200 
--snap_prefix mobilenet_256 
--lr 0.35 
--embed_size 256 
--model mobilenetv2 
--device 4
Environment:

OS: Linux Ubuntu 16.04
Framework version: PyTorch
Python version:3.5
OpenVINO version:3
CUDA/cuDNN version: 10.1
GPU model and memory:

I got this issue while trying to train the face recognition model:
W0422 10:34:10.326288 191361 am_softmax.py:81] /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:48: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.
Traceback (most recent call last):
File "train.py", line 278, in 
main()
File "train.py", line 274, in main
train(args)
File "train.py", line 185, in train
loss_sm = softmax_criterion(sm_outputs, label)
File "/.local/lib/python3.5/site-packages/torch/nn/modules/module.py", line 547, in call
result = self.forward(*input, **kwargs)
File "/face-recog/openvino_training_extensions/pytorch_toolkit/face_recognition/losses/am_softmax.py", line 81, in forward
torch.lt(torch.masked_select(phi_theta, index).view(-1, 1).repeat(1, h_theta.shape[1]) - cos_theta, 0)
RuntimeError: expected device cuda:4 and dtype Byte but got device cuda:4 and dtype Bool
	</description>
	<comments>
		<comment id='1' author='minhphai' date='2020-11-19T14:38:45Z'>
		Hi! The version of code you use is outdated. Please take a look at the latest &lt;denchmark-link:https://github.com/openvinotoolkit/training_extensions/tree/develop/pytorch_toolkit/object_reidentification/face_recognition&gt;face recognition toolkit&lt;/denchmark-link&gt;
.
		</comment>
	</comments>
</bug>