<bug id='487' author='sbhttchryy' open_date='2020-10-15T09:10:32Z' closed_time='2020-10-15T16:01:40Z'>
	<summary>CUDA out of memory error when finetuning on the xlm-roberta-large-squad2</summary>
	<description>
Hello developers, when I am trying to finetune a model with my own set of annotated QA pairs, I keep running into the following error:
'RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.92 GiB total capacity; 11.55 GiB already allocated; 7.25 MiB free; 11.60 GiB reserved in total by PyTorch)'
The code that I tried to run is the following:
`
from haystack.reader.farm import FARMReader
import logging
import subprocess
import time
logging.basicConfig(filename='/data/home/krystian/haystack/src/example.log',level=logging.DEBUG)
logger = logging.getLogger(name)
reader = FARMReader(model_name_or_path = "deepset/xlm-roberta-large-squad2", use_gpu= True)
train_data = "/data/home/res"
reader.train(data_dir=train_data, train_filename="annotated.json", use_gpu=True, n_epochs=1, save_dir="test_model")`
The traceback is as follows:
Traceback (most recent call last): File "/data/home/krystian/haystack/src/models/models/finetuned_models/QA_XLM_R_ES.py", line 16, in &lt;module&gt; reader.train(data_dir=train_data, train_filename="annotated.json", use_gpu=True, n_epochs=1, save_dir="test_model") File "/data/home/krystian/haystack/haystack/reader/farm.py", line 221, in train self.inferencer.model = trainer.train() File "/data/home/krystian/test_env/lib/python3.7/site-packages/farm/train.py", line 288, in train logits = self.model.forward(**batch) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 155, in forward outputs = self.parallel_apply(replicas, inputs, kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 165, in parallel_apply return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)]) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply output.reraise() File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/_utils.py", line 395, in reraise raise self.exc_type(msg) RuntimeError: Caught RuntimeError in replica 0 on device 0. Original Traceback (most recent call last): File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker output = module(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/farm/modeling/adaptive_model.py", line 399, in forward sequence_output, pooled_output = self.forward_lm(**kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/farm/modeling/adaptive_model.py", line 443, in forward_lm sequence_output, pooled_output = self.language_model(**kwargs, output_all_encoded_layers=False) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/farm/modeling/language_model.py", line 694, in forward attention_mask=padding_mask, File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 762, in forward output_hidden_states=output_hidden_states, File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 439, in forward output_attentions, File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 371, in forward hidden_states, attention_mask, head_mask, output_attentions=output_attentions, File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 315, in forward hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 250, in forward attention_probs = self.dropout(attention_probs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__ result = self.forward(*input, **kwargs) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 54, in forward return F.dropout(input, self.p, self.training, self.inplace) File "/data/home/krystian/test_env/lib/python3.7/site-packages/torch/nn/functional.py", line 936, in dropout else _VF.dropout(input, p, training)) RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 11.92 GiB total capacity; 11.55 GiB already allocated; 7.25 MiB free; 11.60 GiB reserved in total by PyTorch) 
Could you please help me here?
Edit1: I changed my model to "distilbert-base-uncased-distilled-squad" and it does not have a problem there. Is it just a problem with the model I chose then?
	</description>
	<comments>
		<comment id='1' author='sbhttchryy' date='2020-10-15T11:05:23Z'>
		&lt;denchmark-link:https://github.com/sbhttchryy&gt;@sbhttchryy&lt;/denchmark-link&gt;
 consider that DistilBERT is lighter than XLM-Roberta Large. It is possible that your GPUÂ memory is not enough on this large model.
Have you tried to reduce  when you call ?
NB: Probably you can also try with ahotrod/electra_large_discriminator_squad2_512 and ahotrod/albert_xxlargev1_squad2_512 to see whether it raise CUDA out of memory or not.
		</comment>
		<comment id='2' author='sbhttchryy' date='2020-10-15T12:15:05Z'>
		Reducing down the batch_size did not help.
It does not raise the CUDA error for the ahotrod/electra_large_discriminator_squad2_512 and the ahotrod/albert_xxlargev1_squad2_512.  However, I need something for German.Could you refer me such a model?
		</comment>
		<comment id='3' author='sbhttchryy' date='2020-10-15T13:00:25Z'>
		
I need something for German.Could you refer me such a model?

No, unfortunately I've never tried German models. However, some members of Haystack team can certainly help you find the best solution for German language.
		</comment>
		<comment id='4' author='sbhttchryy' date='2020-10-15T13:59:31Z'>
		Hey &lt;denchmark-link:https://github.com/sbhttchryy&gt;@sbhttchryy&lt;/denchmark-link&gt;
 the XLM-R large is rather large : )
We can fit batch_size of 4 on a 16GB V100. So your 12GB should accommodate a batch size of 3, which is possibly not enough for training...
When you have enough German QA data, you should either use multiGPU to increase the batch size (be aware that the memory distribution across GPUs is not optimal because of pytorch issues/settings) or use gradient accumulation across batches.
One additional question: Do you have your own German data or do you use MLQA or XQuAD?
		</comment>
		<comment id='5' author='sbhttchryy' date='2020-10-15T16:01:37Z'>
		I have my own German data
		</comment>
		<comment id='6' author='sbhttchryy' date='2020-10-15T17:16:08Z'>
		Did lowering the batch size to 3 help getting XLM-r large to run?
We also continued training this XLM model on the available German data, but we are unsure if this really performs better than the XLM trained only on english SQuAD. So we did not upload it to the transformers modelhub. &lt;denchmark-link:https://gist.github.com/Timoeller/cc315f236ad20c51009b2a060caddaa6&gt;Here&lt;/denchmark-link&gt;
 is a gist describing how you can access it.
		</comment>
	</comments>
</bug>