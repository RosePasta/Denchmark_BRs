<bug id='508' author='sb2202' open_date='2020-10-20T08:49:10Z' closed_time='2020-10-20T12:17:53Z'>
	<summary>On using the finetuning script, all my GPUs are not getting used.</summary>
	<description>
When using the fine-tuning script on my annotated question-answer pair json file, none of the GPUs are being used, even though the use_gpu=True in `reader.train(data_dir=train_data, train_filename="dev-v2.0.json", use_gpu=True, n_epochs=1, save_dir="my_model").
I have 4 Tesla K-80 GPUs,  CUDA Version: 10.1, OS: Ubuntu 16.04.6 LTS, Pytorch: torch.version: 1.6.0.
Alongside, is there any tips on the number of epochs to train, keeping in mind the current train loss?
`
	</description>
	<comments>
		<comment id='1' author='sb2202' date='2020-10-20T11:17:29Z'>
		Hey &lt;denchmark-link:https://github.com/sb2202&gt;@sb2202&lt;/denchmark-link&gt;
 ,
What do you see in the logs? There should be a line similar to this one showing how many GPUs got detected:
04/28/2020 14:39:27 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None

Alongside, is there any tips on the number of epochs to train, keeping in mind the current train loss?

This really depends on a few factors (base model, size of your dataset, diff of your domain to the initial one of the base model) . However, in most scenarios, we were fine with 1-3 epochs. I'd recommend watching the metrics on the dev set to see if you are overfitting or continuing training still helps.
		</comment>
		<comment id='2' author='sb2202' date='2020-10-20T11:54:15Z'>
		Hi &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
, it is exactly this:
04/28/2020 14:39:27 - INFO - farm.utils - device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None.
Same thing is happening for the evaluation scripts as well, as in none of the GPUs are getting used.
		</comment>
		<comment id='3' author='sb2202' date='2020-10-20T11:56:40Z'>
		In that case, it is very likely a problem with your PyTorch installation / environment.
Can you try to run:
&lt;denchmark-code&gt;import torch
torch.cuda.is_available()
&lt;/denchmark-code&gt;

		</comment>
		<comment id='4' author='sb2202' date='2020-10-20T11:59:56Z'>
		Yes, it is false.
		</comment>
		<comment id='5' author='sb2202' date='2020-10-20T12:03:12Z'>
		Ok, then the issue is really related to PyTorch / Cuda and not to Haystack.
Sometimes this happens if you install the wrong version of PyTorch. You can try to fix this by re-installing like advised here &lt;denchmark-link:https://pytorch.org/get-started/locally/&gt;https://pytorch.org/get-started/locally/&lt;/denchmark-link&gt;

An alternative would be CUDA or Nvidia drivers, but I suppose you were already running other jobs successfully on your GPU and nvidia-smi displays everything correctly.
		</comment>
		<comment id='6' author='sb2202' date='2020-10-20T12:09:48Z'>
		Thank you &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
. I will try the reinstall and let you know.
		</comment>
		<comment id='7' author='sb2202' date='2020-10-20T12:17:53Z'>
		Thank you &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
 . It works.
		</comment>
	</comments>
</bug>