<bug id='96' author='tholor' open_date='2020-05-06T09:49:33Z' closed_time='2020-08-01T19:04:08Z'>
	<summary>dev_split in FARMReader.train() for very small datasets</summary>
	<description>

Hello there,
Trying to train the model gave me the following error:
Traceback (most recent call last):
File "C:/Users/andre/Desktop/Haystack_QA/haystack/questionAnswer.py", line 18, in
reader.train(data_dir='training_data', train_filename="answers.json", use_gpu=True, n_epochs=1, save_dir='models/trained_model')
File "C:\Users\andre\Desktop\Haystack_QA\haystack\haystack\reader\farm.py", line 160, in train
data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False)
File "C:\Users\andre\Desktop\Haystack_QA\haystack\venv\lib\site-packages\farm\data_handler\data_silo.py", line 105, in init
self._load_data()
File "C:\Users\andre\Desktop\Haystack_QA\haystack\venv\lib\site-packages\farm\data_handler\data_silo.py", line 223, in _load_data
self._create_dev_from_train()
File "C:\Users\andre\Desktop\Haystack_QA\haystack\venv\lib\site-packages\farm\data_handler\data_silo.py", line 365, in _create_dev_from_train
train_dataset, dev_dataset = self.random_split_ConcatDataset(self.data["train"], lengths=[n_train, n_dev])
File "C:\Users\andre\Desktop\Haystack_QA\haystack\venv\lib\site-packages\farm\data_handler\data_silo.py", line 397, in random_split_ConcatDataset
assert idx_dataset &gt;= 1, "Dev_split ratio is too large, there is no data in train set. "
AssertionError: Dev_split ratio is too large, there is no data in train set. Please lower dev_split = 0.1


Setting dev_split to 0.1 did not help.
Setting the arguments dev_filename=None, dev_split=0.0 made it work.


Used dataset: 50 q&amp;a pairs in total for 3 documents

Originally posted by &lt;denchmark-link:https://github.com/Krak91&gt;@Krak91&lt;/denchmark-link&gt;
 in &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/95&gt;#95&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='tholor' date='2020-05-06T09:58:17Z'>
		This seems related to the small size of training data + the multiprocessing we do during preprocessing. For small datasets like this we could
a) disable multiprocessing / adjust chunksizes
b) adjust the splitting method in FARM (&lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/2f28310aa0d5e969aff8125c7ce1d7b487aa6f9f/farm/data_handler/data_silo.py#L376&gt;here&lt;/denchmark-link&gt;
).
c) make the error message more explicit and point to the cross-validation method once implemented in haystack (this will be more reliable on small datasets)
		</comment>
		<comment id='2' author='tholor' date='2020-07-18T18:30:22Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 14 days if no further activity occurs.
		</comment>
	</comments>
</bug>