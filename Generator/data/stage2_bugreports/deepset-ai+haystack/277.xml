<bug id='277' author='nsankar' open_date='2020-07-31T16:07:56Z' closed_time='2020-11-03T03:54:23Z'>
	<summary>DensePassageRetriever exception for use_amp=O1</summary>
	<description>
I installed nvidia apex in colab and created a retriever  with use_amp='O1' parameter as cited below.
from haystack.retriever.dense import DensePassageRetriever retriever = DensePassageRetriever(document_store=document_store, embedding_model="dpr-bert-base-nq",do_lower_case=True, use_gpu=True, use_amp='O1') document_store.update_embeddings(retriever)
Upon executing it, I get  the following error as below that TypeError: 'HFBertEncoder' object is not iterable`
07/31/2020 16:03:10 - INFO - haystack.retriever.dpr_utils -   Loading saved model from models/dpr/checkpoint/retriever/single/nq/bert-base-encoder.cp
07/31/2020 16:03:11 - INFO - haystack.retriever.dense -   Loaded encoder params:  {'do_lower_case': True, 'pretrained_model_cfg': 'bert-base-uncased', 'encoder_model_type': 'hf_bert', 'pretrained_file': None, 'projection_dim': 0, 'sequence_length': 256}
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.
Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
&lt;denchmark-h:h2&gt;loss_scale             : dynamic&lt;/denchmark-h&gt;

TypeError                                 Traceback (most recent call last)
 in ()
1 from haystack.retriever.dense import DensePassageRetriever
----&gt; 2 retriever = DensePassageRetriever(document_store=document_store, embedding_model="dpr-bert-base-nq",do_lower_case=True, use_gpu=True, use_amp='O1')
3
4
5
1 frames
/usr/local/lib/python3.6/dist-packages/haystack/retriever/dense.py in _prepare_model(self, encoder, saved_state, prefix)
176                 raise ImportError("Please install apex from &lt;denchmark-link:https://www.github.com/nvidia/apex&gt;https://www.github.com/nvidia/apex&lt;/denchmark-link&gt;
 to use fp16 training.")
177
--&gt; 178             encoder, _ = amp.initialize(encoder, None, opt_level=self.use_amp)
179
180         encoder.eval()
TypeError: 'HFBertEncoder' object is not iterable
â”†Issue is synchronized with this &lt;denchmark-link:https://deepset-ai.atlassian.net/browse/HS-31&gt;Jira Task&lt;/denchmark-link&gt;
 by &lt;denchmark-link:https://www.unito.io/learn-more&gt;Unito&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='nsankar' date='2020-08-01T16:47:30Z'>
		Hey &lt;denchmark-link:https://github.com/nsankar&gt;@nsankar&lt;/denchmark-link&gt;
,
Unfortunately Amp is not supported yet. The current DPR code is pretty basic and only addresses basic inference. We are currently working on the training part in &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/273&gt;#273&lt;/denchmark-link&gt;
 and will do some refactoring of the class. Will have a look at AMP then, too.
		</comment>
		<comment id='2' author='nsankar' date='2020-11-02T07:40:24Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 14 days if no further activity occurs.
		</comment>
	</comments>
</bug>