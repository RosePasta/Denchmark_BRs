<bug id='666' author='gabinguo' open_date='2020-12-08T16:53:51Z' closed_time='2021-01-22T09:24:49Z'>
	<summary>DensePassageRetriever update embeddings out of memory issue</summary>
	<description>
Describe the bug
The code I run:
&lt;denchmark-code&gt;from haystack.document_store.elasticsearch import ElasticsearchDocumentStore
from haystack.retriever.dense import DensePassageRetriever

document_store = ElasticsearchDocumentStore(host="localhost", username="", password="", index="wikipedia", embedding_dim=768)
retriever = DensePassageRetriever(document_store=document_store,
        query_embedding_model="facebook/dpr-question_encoder-single-nq-base",
        passage_embedding_model="facebook/dpr-ctx_encoder-single-nq-base",
        max_seq_len_query=64,
        max_seq_len_passage=256,
        batch_size=16,
        embed_title=True,
        use_fast_tokenizers=True,
        use_gpu=True)
document_store.update_embeddings(retriever)
&lt;/denchmark-code&gt;

I have indexed the whole Wikipedia dump in Elasticsearch already, and then I wanted to save the word embeddings in the ES as well...  I run the code above.  In the beginning, it was fine and output some normal ES loggings like this:
&lt;denchmark-link:https://user-images.githubusercontent.com/27879119/101514173-48ccab00-397d-11eb-9091-a3d96b9e30d0.png&gt;&lt;/denchmark-link&gt;

Till it failed when it exceeded the memory size...
Error message
The process has been killed automatically when it exceeds the memory size. Seems like it didn't process it in batches...
** Question **
Could I index a large dump like this in batches without exceeding the memory..?
System:

OS: Linux (48GB of ram)
GPU/CPU: GPU (GTX 1080Ti)
Haystack version (commit or version number):  0.5.0
DocumentStore: ES
Reader: Not there yet...
Retriever: DPR

	</description>
	<comments>
		<comment id='1' author='gabinguo' date='2020-12-08T18:23:43Z'>
		Hey &lt;denchmark-link:https://github.com/gabinguo&gt;@gabinguo&lt;/denchmark-link&gt;
,
Thanks for reporting this. It is a known limitation (see &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/601&gt;#601&lt;/denchmark-link&gt;
) and we will introduce a batch mode for  soon. As a temporary workaround (that we also use internally) you can create the embeddings first in batches, attach them to the document and then call write_documents(). See a rough sketch here: &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/601#issuecomment-729469535&gt;#601 (comment)&lt;/denchmark-link&gt;

Let me know if you have further questions on this workaround. Otherwise, I will keep you updated and ping you here once we implemented the batch_mode ...
		</comment>
		<comment id='2' author='gabinguo' date='2020-12-08T18:30:52Z'>
		Two side notes:

Do you split your Wikipedia articles into smaller chunks? If not, you will have problems creating meaningful embeddings with DPR.
If you want to skip the creation of embeddings yourself, you could also consider using the wiki dataset from the DPR authors that already include their precomputed embeddings (facebookresearch/DPR@3d99998).

		</comment>
		<comment id='3' author='gabinguo' date='2020-12-08T18:41:18Z'>
		
Hey @gabinguo,
Thanks for reporting this. It is a known limitation (see #601) and we will introduce a batch mode for update_embeddings soon. As a temporary workaround (that we also use internally) you can create the embeddings first in batches, attach them to the document and then call write_documents(). See a rough sketch here: #601 (comment)
Let me know if you have further questions on this workaround. Otherwise, I will keep you updated and ping you here once we implemented the batch_mode ...


Two side notes:

Do you split your Wikipedia articles into smaller chunks? If not, you will have problems creating meaningful embeddings with DPR.
If you want to skip the creation of embeddings yourself, you could also consider using the wiki dataset from the DPR authors that already include their precomputed embeddings (facebookresearch/DPR@3d99998).


Thanks for replying : )
Ok nice, that would be a do-able solution for now. And can't wait for the updated update_embeddings function  : )
As for the side notes,

Yea, I did split them into small chunks (paragraph-level indexing).
Thanks for mentioning this dataset. But I need to run some benchmarks with some defined datasets : \

		</comment>
		<comment id='4' author='gabinguo' date='2021-01-14T16:40:43Z'>
		Hi &lt;denchmark-link:https://github.com/gabinguo&gt;@gabinguo&lt;/denchmark-link&gt;
, the PR &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/733&gt;#733&lt;/denchmark-link&gt;
 should significantly reduce memory consumption by introducing a buffer. Could you try this out and let us know if it helps?
		</comment>
		<comment id='5' author='gabinguo' date='2021-01-19T18:15:47Z'>
		
Hi @gabinguo, the PR #733 should significantly reduce memory consumption by introducing a buffer. Could you try this out and let us know if it helps?

Thanks a lot for your great work. I will test on this and post the result when possible.
		</comment>
		<comment id='6' author='gabinguo' date='2021-01-22T09:24:49Z'>
		Hi &lt;denchmark-link:https://github.com/gabinguo&gt;@gabinguo&lt;/denchmark-link&gt;
, this should now be resolved with &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/733&gt;#733&lt;/denchmark-link&gt;
. Please feel free to reopen this thread if you still face any issues.
		</comment>
	</comments>
</bug>