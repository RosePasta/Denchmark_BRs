<bug id='302' author='kandloic' open_date='2020-08-10T19:04:18Z' closed_time='2020-08-20T13:43:38Z'>
	<summary>Division by zero error when fine-tuning model</summary>
	<description>

When running the &lt;denchmark-link:https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb#scrollTo=Wui9d_VKXXDm&gt;notebook&lt;/denchmark-link&gt;
 example on how to fine-tune the model with our own data, I get an error when I am fine-tuning the model.
A clear and concise description of what the bug is.
Error message
ZeroDivisionError                         Traceback (most recent call last)
&lt;ipython-input-5-14a6de8ed200&gt; in &lt;module&gt;()
      2 train_data = "data/squad20"
      3 # train_data = "PATH/TO_YOUR/TRAIN_DATA"
----&gt; 4 reader.train(data_dir=train_data, train_filename="dev-v2.0.json", use_gpu=True, n_epochs=1, save_dir="my_model")

4 frames
/usr/local/lib/python3.6/dist-packages/haystack/reader/farm.py in train(self, data_dir, train_filename, dev_filename, test_file_name, use_gpu, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes)
    184         # 2. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them
    185         # and calculates a few descriptive statistics of our datasets
--&gt; 186         data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False, max_processes=num_processes)
    187 
    188         # Quick-fix until this is fixed upstream in FARM:

/usr/local/lib/python3.6/dist-packages/farm/data_handler/data_silo.py in __init__(self, processor, batch_size, distributed, automatic_loading, max_multiprocessing_chunksize, max_processes, caching, cache_path)
    103             # In most cases we want to load all data automatically, but in some cases we rather want to do this
    104             # later or load from dicts instead of file (https://github.com/deepset-ai/FARM/issues/85)
--&gt; 105             self._load_data()
    106 
    107     @classmethod

/usr/local/lib/python3.6/dist-packages/farm/data_handler/data_silo.py in _load_data(self, train_dicts, dev_dicts, test_dicts)
    205             train_file = self.processor.data_dir / self.processor.train_filename
    206             logger.info("Loading train set from: {} ".format(train_file))
--&gt; 207             self.data["train"], self.tensor_names = self._get_dataset(train_file)
    208         else:
    209             logger.info("No train set is being loaded")

/usr/local/lib/python3.6/dist-packages/farm/data_handler/data_silo.py in _get_dataset(self, filename, dicts)
    142             num_dicts=num_dicts,
    143             max_processes=self.max_processes,
--&gt; 144             max_chunksize=self.max_multiprocessing_chunksize,
    145         )
    146 

/usr/local/lib/python3.6/dist-packages/farm/utils.py in calc_chunksize(num_dicts, min_chunksize, max_chunksize, max_processes)
     46 def calc_chunksize(num_dicts, min_chunksize=4, max_chunksize=2000, max_processes=128):
     47     num_cpus = min(mp.cpu_count() - 1 or 1, max_processes)  # -1 to keep a CPU core free for the main process
---&gt; 48     dicts_per_cpu = np.ceil(num_dicts / num_cpus)
     49     # automatic adjustment of multiprocessing chunksize
     50     # for small files (containing few dicts) we want small chunksize to ulitize all available cores but never less

ZeroDivisionError: division by zero
System:

OS: MacOS Catalina Version 10.15.6
GPU/CPU: Google Collab GPU runtime
Haystack version (commit or version number): Commit 5186d2d
Reader: FARMreader

	</description>
	<comments>
		<comment id='1' author='kandloic' date='2020-08-11T07:48:17Z'>
		Thanks for raising this issue, I could reproduce it.
We have created a fix for this in &lt;denchmark-link:https://github.com/deepset-ai/haystack/pull/303&gt;#303&lt;/denchmark-link&gt;
. Could you try running the code again on newest master?
The underlying problem is that FARM.QAInferencer and FARM.data_silo have different behaviors for the max_processes or num_processes arguments. We will unify these args in FARM in the next release.
		</comment>
		<comment id='2' author='kandloic' date='2020-08-11T12:28:18Z'>
		Hi, I tried running the code again but this time, I got a different error.
100%|██████████| 10186398/10186398 [00:02&lt;00:00, 3895273.34B/s]
Preprocessing Dataset data/squad20/dev-v2.0.json:  20%|██        | 241/1204 [00:37&lt;02:30,  6.39 Dicts/s]
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-3-14a6de8ed200&gt; in &lt;module&gt;()
      2 train_data = "data/squad20"
      3 # train_data = "PATH/TO_YOUR/TRAIN_DATA"
----&gt; 4 reader.train(data_dir=train_data, train_filename="dev-v2.0.json", use_gpu=True, n_epochs=1, save_dir="my_model")

4 frames
/usr/local/lib/python3.6/dist-packages/farm/data_handler/data_silo.py in random_split_ConcatDataset(self, ds, lengths)
    397                             f"train/dev split: {lengths}")
    398 
--&gt; 399         assert idx_dataset &gt;= 1, "Dev_split ratio is too large, there is no data in train set. " \
    400                              f"Please lower dev_split = {self.processor.dev_split}"
    401 

AssertionError: Dev_split ratio is too large, there is no data in train set. Please lower dev_split = 0.1
		</comment>
		<comment id='3' author='kandloic' date='2020-08-11T12:39:52Z'>
		: )
Thanks for continued reporting.
This is a quite unique combination of dataset (SQuAD dev set) + few cores (colab just gives 2 cores) available. It should be fixed for now.
I guess you want to train a model on your custom data? Or what is your use-case?
		</comment>
		<comment id='4' author='kandloic' date='2020-08-11T13:37:04Z'>
		I didn't have a specific use-case. I came across this project yesterday and I was just exploring the GC Collab notebook to understand how it works.
		</comment>
		<comment id='5' author='kandloic' date='2020-08-14T10:04:28Z'>
		Hey &lt;denchmark-link:https://github.com/kandloic&gt;@kandloic&lt;/denchmark-link&gt;
, is the issue fixed and can we close this?
		</comment>
		<comment id='6' author='kandloic' date='2020-08-14T11:50:55Z'>
		Yes, thanks for your help!
		</comment>
	</comments>
</bug>