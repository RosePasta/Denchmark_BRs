<bug id='425' author='tholor' open_date='2020-09-23T12:42:00Z' closed_time='2020-10-05T14:02:56Z'>
	<summary>Different answers from Transformers vs FARMReader for deepset/roberta-base-squad2</summary>
	<description>

Posing the same question to FARM and TransformersReader with the same model gives different answers.
While this is to some extent expected due to different postprocessing (see &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/248&gt;#248&lt;/denchmark-link&gt;
), the difference seems to be more substantial at the moment.
While we can reproduce reported eval metrics for deepset/roberta-base-squad2 with FARM(F1&gt;0.81), the metrics from eval with transformers are significantly off (F1 &lt; 0.70).
Expected behavior

Same model input  (input_ids etc)
Same logits output of model
Different postprocessing
Quite similar eval performance in a range of 2-3 %

Additional context
I started to debug and found that the model input already differs.
To Reproduce
from haystack.reader.farm import FARMReader
from haystack.reader.transformers import TransformersReader
from haystack import Document

#squad id 56de11154396321400ee25aa
question =  "What were the origins of the Raouliii family?"
context= "Several families of Byzantine Greece were of Norman mercenary origin during the period of the Comnenian Restoration, when Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d\u0027Aulps, and that group of Albanian clans known as the Maniakates were descended from Normans who served under George Maniaces in the Sicilian expedition of 1038."
docs = [Document(text=context)]
 
reader = FARMReader("deepset/roberta-base-squad2")
res = reader.predict(question=question, documents=docs)
print(res)
# returns: 
#{'question': 'What were the origins of the Raouliii family?', 'no_ans_gap': 2.385634422302246, 'answers': [{'answer': 'The Raoulii were descended from an Italo-Norman named Raoul', 'score': 9.49970817565918, 'probability': 0.7662871102539097, 'context': 's were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierr', 'offset_start': 46, 'offset_end': 105, 'offset_start_in_doc': 185, 'offset_end_in_doc': 244, 'document_id': '10fb7110-f827-4cbb-9a97-76f72c369304'}]}

reader2 = TransformersReader("deepset/roberta-base-squad2",use_gpu=-1)
res = reader2.predict(question=question, documents=docs)
print(res)
# returns: 
# {'question': 'What were the origins of the Raouliii family?', 'answers': [{'answer': None, 'score': 0.7205391153693199, 'probability': 0.5225016380517181, 'context': None, 'offset_start': 0, 'offset_end': 0, 'document_id': None, 'meta': None}, {'answer': 'an Italo-Norman named Raoul,', 'context': "eeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 217, 'offset_end': 245, 'probability': 0.41548100113868713, 'score': None, 'document_id': '10fb7110-f827-4cbb-9a97-76f72c369304', 'meta': None}, {'answer': 'an Italo-Norman', 'context': "eeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, an", 'offset_start': 217, 'offset_end': 232, 'probability': 0.0026169593911617994, 'score': None, 'document_id': '10fb7110-f827-4cbb-9a97-76f72c369304', 'meta': None}, {'answer': 'an Italo-Norman named Raoul,', 'context': "eeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 217, 'offset_end': 245, 'probability': 0.0022485838271677494, 'score': None, 'document_id': '10fb7110-f827-4cbb-9a97-76f72c369304', 'meta': None}]}
The input_ids and tokens in FARM:
&lt;denchmark-code&gt;tensor([[    0, 12196,    58,     5, 18863,     9,     5, 15642,  5156, 31917,
         #   284,   116,     2,     2, 30453,   337, 27505, 28694,  1116,  1409,
         #   329, 43212,   571,  5314,  1755, 17341,  1116,   282, 16803,  2089,
         #   438, 19985, 43211, 37460,   627, 28030,  1116,   627,   175, 19596,
         #   811,  7110, 18620,     6, 14746,  1409,   329, 43212,   991, 46220,
         # 17341, 32632,   995, 16507, 23286,  2379,   260,  5557, 41410,     4,
         #   627,   763,  5156,  4132, 17341, 45091,  6228,  7761,   260,  8632,
         #   139,    12,   282, 16803, 26442,   763,  5156,     6,   627, 13713,
         #  7085,  1588,  1999,   242, 17341, 45091,  6228,  7761,   102,   642,
         # 41307,   417,   108,  6695,  3275,     6,   463,  6025, 13839,  1116,
         #   337,  7384,   811,  3998,  1253,  6421,   281,   627,   397, 20082,
         #  1626, 17341, 45091,  6228,  7761, 42258,  1253,  8155, 33091,  5087,
         #  1899, 26875,   397,   118, 11667,   179,   627, 15335, 35373,  3463,
         #  9700,  7469,  1116,   698,  3170,     4,     2,     1,     1,     1, ... ]

=&gt; tokens:
['&lt;s&gt;', 'what', 'Ġwere', 'Ġthe', 'Ġorigins', 'Ġof', 'Ġthe', 'Ġra', 'oul', 'iii', 'Ġfamily', '?', '&lt;/s&gt;', '&lt;/s&gt;', 'sever', 'al', 'Ġfamilies', 'Ġof', 'Ġby', 'z', 'antine', 'Ġg', 'ree', 'ce', 'Ġwere', 'Ġof', 'Ġnorm', 'an', 'Ġmercenary', 'Ġorigin', 'Ġduring', 'Ġthe', 'Ġperiod', 'Ġof', 'Ġthe', 'Ġcom', 'nen', 'ian', 'Ġrestoration', ',', 'Ġwhen', 'Ġby', 'z', 'antine', 'Ġem', 'perors', 'Ġwere', 'Ġseeking', 'Ġout', 'Ġwestern', 'Ġeuro', 'pe', 'an', 'Ġwarriors', '.', 'Ġthe', 'Ġra', 'oul', 'ii', 'Ġwere', 'Ġdescended', 'Ġfrom', 'Ġan', 'Ġit', 'alo', '-', 'n', 'orman', 'Ġnamed', 'Ġra', 'oul', ',', 'Ġthe', 'Ġpet', 'ral', 'ip', 'ha', 'e', 'Ġwere', 'Ġdescended', 'Ġfrom', 'Ġa', 'Ġpier', 're', 'Ġd', "'", 'aul', 'ps', ',', 'Ġand', 'Ġthat', 'Ġgroup', 'Ġof', 'Ġal', 'ban', 'ian', 'Ġclans', 'Ġknown', 'Ġas', 'Ġthe'...
&lt;/denchmark-code&gt;

The input_ids and tokens in Transformers:
&lt;denchmark-code&gt;tensor([[    0, 12196,    58,     5, 18863,     9,     5, 15642,  5156, 31917,
         #   284,   116,     2,     2, 30453,   337, 27505, 28694,  1116,  1409,
         #   329, 43212,   571,  5314,  1755, 17341,  1116,   282, 16803,  2089,
         #   438, 19985, 43211, 37460,   627, 28030,  1116,   627,   175, 19596,
         #   811,  7110, 18620,     6, 14746,  1409,   329, 43212,   991, 46220,
         # 17341, 32632,   995, 16507, 23286,  2379,   260,  5557, 41410,     4,
         #   627,   763,  5156,  4132, 17341, 45091,  6228,  7761,   260,  8632,
         #   139,    12,   282, 16803, 26442,   763,  5156,     6,   627, 13713,
         #  7085,  1588,  1999,   242, 17341, 45091,  6228,  7761,   102,   642,
         # 41307,   417,   108,  6695,  3275,     6,   463,  6025, 13839,  1116,
         #   337,  7384,   811,  3998,  1253,  6421,   281,   627,   397, 20082,
         #  1626, 17341, 45091,  6228,  7761, 42258,  1253,  8155, 33091,  5087,
         #  1899, 26875,   397,   118, 11667,   179,   627, 15335, 35373,  3463,
         #  9700,  7469,  1116,   698,  3170,     4,     2, 1, .....]

=&gt;tokens:
  ['&lt;s&gt;', 'what', 'Ġwere', 'Ġthe', 'Ġorigins', 'Ġof', 'Ġthe', 'Ġra', 'oul', 'iii', 'Ġfamily', '?', '&lt;/s&gt;', '&lt;/s&gt;', 'sever', 'al', 'fam', 'ilies', 'of', 'by', 'z', 'antine', 'g', 'ree', 'ce', 'were', 'of', 'n', 'orman', 'mer', 'c', 'enary', 'origin', 'during', 'the', 'period', 'of', 'the', 'com', 'nen', 'ian', 'rest', 'oration', ',', 'when', 'by', 'z', 'antine', 'em', 'perors', 'were', 'seeking', 'out', 'western', 'euro', 'pe', 'an', 'war', 'riors', '.', 'the', 'ra', 'oul', 'ii', 'were', 'desc', 'ended', 'from', 'an', 'ital', 'o', '-', 'n', 'orman', 'named', 'ra', 'oul', ',', 'the', 'pet', 'ral', 'ip', 'ha', 'e', 'were', 'desc', 'ended', 'from', 'a', 'p', 'ierre', 'd', "'", 'aul', 'ps', ',', 'and', 'that', 'group', 'of'...
&lt;/denchmark-code&gt;

It seems that the tokenization of the passage in transformers is wrong as it doesn't respect the whitespace before words anymore ('fam', 'ilies', instead of 'Ġfamilies'). I think this is related to this part of  &lt;denchmark-link:https://github.com/huggingface/transformers/blob/master/src/transformers/data/processors/squad.py#L110&gt;https://github.com/huggingface/transformers/blob/master/src/transformers/data/processors/squad.py#L110&lt;/denchmark-link&gt;

System:

OS: Ubuntu 18.04
GPU/CPU: CPU
Haystack version (commit or version number): 0.4.0

	</description>
	<comments>
		<comment id='1' author='tholor' date='2020-09-23T16:47:21Z'>
		Seems related to &lt;denchmark-link:https://github.com/huggingface/transformers/pull/4615&gt;huggingface/transformers#4615&lt;/denchmark-link&gt;

		</comment>
		<comment id='2' author='tholor' date='2020-09-25T15:54:38Z'>
		Created a PR &lt;denchmark-link:https://github.com/huggingface/transformers/pull/7387&gt;huggingface/transformers#7387&lt;/denchmark-link&gt;

		</comment>
		<comment id='3' author='tholor' date='2020-10-05T14:02:56Z'>
		PR got merged. We will have it fixed with the next release of transformers + haystack.
FYI &lt;denchmark-link:https://github.com/melaniebeck&gt;@melaniebeck&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>