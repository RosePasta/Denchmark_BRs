<bug id='476' author='antoniolanza1996' open_date='2020-10-10T09:30:09Z' closed_time='2020-11-12T10:28:41Z'>
	<summary>deepset/roberta-base-squad2 seems to work not as it should</summary>
	<description>
As advised in your &lt;denchmark-link:https://haystack.deepset.ai/en/docs/readermd#Choosing-the-Right-Model&gt;documentation&lt;/denchmark-link&gt;
, I was testing  but I've noted that its performance is much lower than I expected.
Hence, I have run evaluation on SQuAD 2.0 dev set and I have noted that my EM and F1 values are different than yours reported &lt;denchmark-link:https://huggingface.co/deepset/roberta-base-squad2&gt;here&lt;/denchmark-link&gt;
. In particular, with this script (using HF Transformers ):
&lt;denchmark-code&gt;python transformers/examples/question-answering/run_squad.py \
  --model_type roberta \
  --model_name_or_path 'deepset/roberta-base-squad2' \
  --do_eval \
  --do_lower_case \
  --predict_file 'dev-v2.0.json' \
  --output_dir 'output' \
  --overwrite_output_dir \
  --version_2_with_negative
&lt;/denchmark-code&gt;

I've obtained 'exact': 61.020803503748, 'f1': 64.14229786629926 which are much lower than yours ("exact": 78.49743114629833, "f1": 81.73092721240889).
Considering the high number of HF model downloads (i.e. more than 3.5MLN) and the importance of this architecture in QA, probably we have to better investigate on it. What do you think?
NB: I've encountered similar problems also with deepset/xlm-roberta-large-squad2 model. Here my eval values are 'exact': 64.67615598416576, 'f1': 77.27580544355429, yours "exact": 79.45759285774446, "f1": 83.79259828925511,.
	</description>
	<comments>
		<comment id='1' author='antoniolanza1996' date='2020-10-10T11:51:01Z'>
		Yep, this is a known issue in transformers. See &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/248&gt;#248&lt;/denchmark-link&gt;
 for details. I created a fix here &lt;denchmark-link:https://github.com/huggingface/transformers/pull/7387&gt;huggingface/transformers#7387&lt;/denchmark-link&gt;

In the meantime you can use the FARMreader. The issue should only be present in the TransformersReader...
		</comment>
		<comment id='2' author='antoniolanza1996' date='2020-10-10T14:04:23Z'>
		Ok, makes sense. Indeed, I have tried to install Transformers from master and the problem is solved, as you mentioned in the PR.
FYI, with Transformers installed from master, I have run the same eval on deepset/xlm-roberta-large-squad2 and now the situation is worse than before:

Transformers installed from master: 'exact': 60.11959090373114, 'f1': 76.13129575803934
Transformers installed from pip: 'exact': 64.67615598416576, 'f1': 77.27580544355429
Your reported metrics in the model card: "exact": 79.45759285774446, "f1": 83.79259828925511

		</comment>
		<comment id='3' author='antoniolanza1996' date='2020-10-10T15:53:11Z'>
		Hm..ok, then there's probably a different problem with xlm-r ðŸ¤” . Have you observed anything suspicious that could help narrowing it down? Do you see the same bad performance with the FARMreader?
		</comment>
		<comment id='4' author='antoniolanza1996' date='2020-10-10T20:47:29Z'>
		No, I have never tried this model in Haystack yet.
Before to use it directly on Haystack, I wanted to run SQuAD 2.0 evaluation just to be sure that all is working well but I have come up with this values mismatch.
Meanwhile, I have quickly run your test at &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/425&gt;#425&lt;/denchmark-link&gt;
 , just changing string model.

FARMReader output:

{'question': 'What were the origins of the Raouliii family?', 'no_ans_gap': 4.4801517724990845, 'answers': [{'answer': 'Italo-Norman', 'score': 3.0138206481933594, 'probability': 0.5930835934414579, 'context': "ng out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, a", 'offset_start': 69, 'offset_end': 81, 'offset_start_in_doc': 220, 'offset_end_in_doc': 232, 'document_id': 'e882d78e-1078-49e5-982b-bd7ba38e7b85'}]}

TransformersReader output:

{'question': 'What were the origins of the Raouliii family?', 'answers': [{'answer': None, 'score': 0.37284865975379944, 'probability': 0.5116494120268487, 'context': None, 'offset_start': 0, 'offset_end': 0, 'document_id': None, 'meta': None}, {'answer': 'Italo-Norman named', 'context': "ing out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 220, 'offset_end': 238, 'probability': 0.18642432987689972, 'score': None, 'document_id': 'e882d78e-1078-49e5-982b-bd7ba38e7b85', 'meta': None}, {'answer': 'Raoulii were descended from an Italo-Norman named', 'context': "en Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 189, 'offset_end': 238, 'probability': 0.16388101875782013, 'score': None, 'document_id': 'e882d78e-1078-49e5-982b-bd7ba38e7b85', 'meta': None}, {'answer': 'Raoulii were descended from an Italo-Norman named', 'context': "en Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 189, 'offset_end': 238, 'probability': 0.15468387305736542, 'score': None, 'document_id': 'e882d78e-1078-49e5-982b-bd7ba38e7b85', 'meta': None}, {'answer': 'Italo-Norman named Raoul,', 'context': "ing out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 220, 'offset_end': 245, 'probability': 0.06070682406425476, 'score': None, 'document_id': 'e882d78e-1078-49e5-982b-bd7ba38e7b85', 'meta': None}]}
		</comment>
		<comment id='5' author='antoniolanza1996' date='2020-10-11T09:35:34Z'>
		&lt;denchmark-link:https://github.com/antoniolanza1996&gt;@antoniolanza1996&lt;/denchmark-link&gt;
 Some days back, I also noticed this difference but then I realised difference in default values for  and . I am not sure about prediction and scores. But count of results differ due to two parameters:  of  and  of .
If we update them we get following results -
FARMReader
&lt;denchmark-code&gt;reader = FARMReader(model_name_or_path="deepset/roberta-base-squad2", use_gpu=False, top_k_per_candidate=4, top_k_per_sample=4, context_window_size=70)
res = reader.predict(question=question, documents=docs)
print(res)

# returns:
# {'question': 'What were the origins of the Raouliii family?', 'no_ans_gap': 2.385634422302246, 'answers': [{'answer': 'The Raoulii were descended from an Italo-Norman named Raoul', 'score': 9.49970817565918, 'probability': 0.7662871102539097, 'context': 'iors. The Raoulii were descended from an Italo-Norman named Raoul, the', 'offset_start': 6, 'offset_end': 65, 'offset_start_in_doc': 185, 'offset_end_in_doc': 244, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'}, {'answer': 'Italo-Norman named Raoul', 'score': 8.893800735473633, 'probability': 0.752450581998683, 'context': 'were descended from an Italo-Norman named Raoul, the Petraliphae were ', 'offset_start': 23, 'offset_end': 47, 'offset_start_in_doc': 220, 'offset_end_in_doc': 244, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'}, {'answer': 'The Raoulii were descended from an Italo-Norman', 'score': 8.449728965759277, 'probability': 0.7419667772750598, 'context': 'n warriors. The Raoulii were descended from an Italo-Norman named Raou', 'offset_start': 12, 'offset_end': 59, 'offset_start_in_doc': 185, 'offset_end_in_doc': 232, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'}, {'answer': 'Italo-Norman', 'score': 7.843822479248047, 'probability': 0.7272030139817564, 'context': 'oulii were descended from an Italo-Norman named Raoul, the Petraliphae', 'offset_start': 29, 'offset_end': 41, 'offset_start_in_doc': 220, 'offset_end_in_doc': 232, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'}]}
&lt;/denchmark-code&gt;

TransformersReader
&lt;denchmark-code&gt;reader2 = TransformersReader(model="deepset/roberta-base-squad2", use_gpu=-1, return_no_answers=False)
res = reader2.predict(question=question, documents=docs)
print(res)

# returns:
# {'question': 'What were the origins of the Raouliii family?', 'answers': [{'answer': 'an Italo-Norman named Raoul,', 'context': "eeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 217, 'offset_end': 245, 'probability': 0.41548100113868713, 'score': None, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43', 'meta': None}, {'answer': 'an Italo-Norman', 'context': "eeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, an", 'offset_start': 217, 'offset_end': 232, 'probability': 0.0026169593911617994, 'score': None, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43', 'meta': None}, {'answer': 'an Italo-Norman named Raoul,', 'context': "eeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 217, 'offset_end': 245, 'probability': 0.002248584059998393, 'score': None, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43', 'meta': None}, {'answer': 'descended from an Italo-Norman named Raoul,', 'context': "emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 202, 'offset_end': 245, 'probability': 0.002219543792307377, 'score': None, 'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43', 'meta': None}]}
&lt;/denchmark-code&gt;

Raw values of prediction from FARM is -
&lt;denchmark-code&gt;[{
	'task': 'qa',
	'predictions': [{
		'question': 'What were the origins of the Raouliii family?',
		'id': '68dd141e-b950-4842-9a2b-79d8da093c43',
		'ground_truth': [],
		'answers': [{
			'score': 9.49970817565918,
			'probability': None,
			'answer': 'The Raoulii were descended from an Italo-Norman named Raoul',
			'offset_answer_start': 185,
			'offset_answer_end': 244,
			'context': 'iors. The Raoulii were descended from an Italo-Norman named Raoul, the',
			'offset_context_start': 179,
			'offset_context_end': 249,
			'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'
		}, {
			'score': 8.893800735473633,
			'probability': None,
			'answer': 'Italo-Norman named Raoul',
			'offset_answer_start': 220,
			'offset_answer_end': 244,
			'context': 'were descended from an Italo-Norman named Raoul, the Petraliphae were ',
			'offset_context_start': 197,
			'offset_context_end': 267,
			'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'
		}, {
			'score': 8.449728965759277,
			'probability': None,
			'answer': 'The Raoulii were descended from an Italo-Norman',
			'offset_answer_start': 185,
			'offset_answer_end': 232,
			'context': 'n warriors. The Raoulii were descended from an Italo-Norman named Raou',
			'offset_context_start': 173,
			'offset_context_end': 243,
			'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'
		}, {
			'score': 7.843822479248047,
			'probability': None,
			'answer': 'Italo-Norman',
			'offset_answer_start': 220,
			'offset_answer_end': 232,
			'context': 'oulii were descended from an Italo-Norman named Raoul, the Petraliphae',
			'offset_context_start': 191,
			'offset_context_end': 261,
			'document_id': '68dd141e-b950-4842-9a2b-79d8da093c43'
		}],
		'no_ans_gap': 2.385634422302246
	}]
}]
&lt;/denchmark-code&gt;

And from Transformer is -
&lt;denchmark-code&gt;[{
	'score': 0.41548100113868713,
	'start': 217,
	'end': 245,
	'answer': 'an Italo-Norman named Raoul,'
}, {
	'score': 0.0026169593911617994,
	'start': 217,
	'end': 232,
	'answer': 'an Italo-Norman'
}, {
	'score': 0.002248584059998393,
	'start': 217,
	'end': 245,
	'answer': 'an Italo-Norman named Raoul,'
}, {
	'score': 0.002219543792307377,
	'start': 202,
	'end': 245,
	'answer': 'descended from an Italo-Norman named Raoul,'
}]
&lt;/denchmark-code&gt;

		</comment>
		<comment id='6' author='antoniolanza1996' date='2020-10-11T19:45:44Z'>
		Hi &lt;denchmark-link:https://github.com/lalitpagaria&gt;@lalitpagaria&lt;/denchmark-link&gt;
 ,
thank you for your detailed report. Even though you try to setup  and  as close as possible (e.g. both cannot reply with a ""), predicted answers could be different (read &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/248&gt;#248&lt;/denchmark-link&gt;
 for more detail).
However, I have opened this issue for  and, as mentioned by &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
 , the problem should be solved in the next Transformers release (and after this release, we can use this model on TransformersReader).
Now, we shifted to problems of another model (), as you can see in &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/476#issuecomment-706553939&gt;this comment&lt;/denchmark-link&gt;
. Have you tried this? Any feedback?
		</comment>
		<comment id='7' author='antoniolanza1996' date='2020-10-12T13:30:43Z'>
		&lt;denchmark-link:https://github.com/antoniolanza1996&gt;@antoniolanza1996&lt;/denchmark-link&gt;


Now, we shifted to problems of another model (deepset/xlm-roberta-large-squad2), as you can see in this comment. Have you tried this? Any feedback?

I tried this model with same data got this -
FARM
&lt;denchmark-code&gt;{'question': 'What were the origins of the Raouliii family?', 'no_ans_gap': 4.4801517724990845, 'answers': [{'answer': 'Italo-Norman', 'score': 3.0138206481933594, 'probability': 0.5930835934414579, 'context': 'oulii were descended from an Italo-Norman named Raoul, the Petraliphae', 'offset_start': 29, 'offset_end': 41, 'offset_start_in_doc': 220, 'offset_end_in_doc': 232, 'document_id': '500e58b4-7036-4492-935d-2386326045bb'}, {'answer': 'The Raoulii were descended from an Italo-Norman', 'score': 2.8849358558654785, 'probability': 0.5891897797893366, 'context': 'n warriors. The Raoulii were descended from an Italo-Norman named Raou', 'offset_start': 12, 'offset_end': 59, 'offset_start_in_doc': 185, 'offset_end_in_doc': 232, 'document_id': '500e58b4-7036-4492-935d-2386326045bb'}, {'answer': 'Raoulii were descended from an Italo-Norman', 'score': 2.827178716659546, 'probability': 0.5874411817628595, 'context': 'warriors. The Raoulii were descended from an Italo-Norman named Raoul,', 'offset_start': 14, 'offset_end': 57, 'offset_start_in_doc': 189, 'offset_end_in_doc': 232, 'document_id': '500e58b4-7036-4492-935d-2386326045bb'}, {'answer': 'Italo-Norman named Raoul', 'score': 1.8918513059616089, 'probability': 0.5588463675455848, 'context': 'were descended from an Italo-Norman named Raoul, the Petraliphae were ', 'offset_start': 23, 'offset_end': 47, 'offset_start_in_doc': 220, 'offset_end_in_doc': 244, 'document_id': '500e58b4-7036-4492-935d-2386326045bb'}]}
&lt;/denchmark-code&gt;

Transformer with 3.1.0 version
&lt;denchmark-code&gt;{'question': 'What were the origins of the Raouliii family?', 'answers': [{'answer': 'Italo-Norman named', 'context': "ing out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 220, 'offset_end': 238, 'probability': 0.18642432987689972, 'score': None, 'document_id': '500e58b4-7036-4492-935d-2386326045bb', 'meta': None}, {'answer': 'Raoulii were descended from an Italo-Norman named', 'context': "en Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 189, 'offset_end': 238, 'probability': 0.16388101875782013, 'score': None, 'document_id': '500e58b4-7036-4492-935d-2386326045bb', 'meta': None}, {'answer': 'Raoulii were descended from an Italo-Norman named', 'context': "en Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 189, 'offset_end': 238, 'probability': 0.15468387305736542, 'score': None, 'document_id': '500e58b4-7036-4492-935d-2386326045bb', 'meta': None}, {'answer': 'Italo-Norman named Raoul,', 'context': "ing out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 220, 'offset_end': 245, 'probability': 0.06070682406425476, 'score': None, 'document_id': '500e58b4-7036-4492-935d-2386326045bb', 'meta': None}]}
&lt;/denchmark-code&gt;

Transformer with top of master branch ie 3.3.1
&lt;denchmark-code&gt;{'question': 'What were the origins of the Raouliii family?', 'answers': [{'answer': 'Italo-Norman named', 'context': "ing out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 220, 'offset_end': 238, 'probability': 0.18642432987689972, 'score': None, 'document_id': 'fc497f70-86c4-4974-96be-2d4c093197e7', 'meta': None}, {'answer': 'Raoulii were descended from an Italo-Norman named', 'context': "en Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 189, 'offset_end': 238, 'probability': 0.16388101875782013, 'score': None, 'document_id': 'fc497f70-86c4-4974-96be-2d4c093197e7', 'meta': None}, {'answer': 'Raoulii were descended from an Italo-Norman named', 'context': "en Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that", 'offset_start': 189, 'offset_end': 238, 'probability': 0.15468387305736542, 'score': None, 'document_id': 'fc497f70-86c4-4974-96be-2d4c093197e7', 'meta': None}, {'answer': 'Italo-Norman named Raoul,', 'context': "ing out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group ", 'offset_start': 220, 'offset_end': 245, 'probability': 0.06070682406425476, 'score': None, 'document_id': 'fc497f70-86c4-4974-96be-2d4c093197e7', 'meta': None}]}
&lt;/denchmark-code&gt;

As you see transformer putting more emphasis on named keyword.
BTW &lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
 I think your fix (&lt;denchmark-link:https://github.com/huggingface/transformers/pull/7387&gt;huggingface/transformers#7387&lt;/denchmark-link&gt;
) is not getting applied on  model as it use different tokeniser .
&lt;denchmark-code&gt;reader2 = TransformersReader(model="deepset/xlm-roberta-large-squad2", use_gpu=-1, return_no_answers=False)
print(reader2.model.tokenizer)
# &lt;transformers.tokenization_xlm_roberta.XLMRobertaTokenizer object at 0x7f1b48415710&gt;
&lt;/denchmark-code&gt;

		</comment>
		<comment id='8' author='antoniolanza1996' date='2020-10-13T06:41:48Z'>
		
BTW @tholor I think your fix (huggingface/transformers#7387) is not getting applied on xlm-roberta-large-squad2 model as it use different tokeniser XLMRobertaTokenizer.

Yes, that's intended as the fix should not be necessary for XLM-Roberta as its tokenizer is based on the sentencepiece tokenizer. I will need to dig deeper there.

Before to use it directly on Haystack, I wanted to run SQuAD 2.0 evaluation just to be sure that all is working well but I have come up with this values mismatch.

&lt;denchmark-link:https://github.com/antoniolanza1996&gt;@antoniolanza1996&lt;/denchmark-link&gt;
 Can you try running squad eval in Haystack or FARM so that we can isolate if its only a problem in transformers or something more general?
You can do it in Haystack like this:
&lt;denchmark-code&gt;reader = FARMReader(...)
reader.eval_on_file(data_dir="my_dir", test_filename="squad-dev.json", device= "cuda")
&lt;/denchmark-code&gt;

Or in FARM like you see at the bottom of this example script: &lt;denchmark-link:https://github.com/deepset-ai/FARM/blob/master/examples/question_answering.py&gt;https://github.com/deepset-ai/FARM/blob/master/examples/question_answering.py&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='antoniolanza1996' date='2020-10-13T13:39:35Z'>
		I have run this snippet:
! pip install git+https://github.com/deepset-ai/haystack.git #also tried with `pip install farm-haystack` with same results
! wget https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/master/dataset/dev-v2.0.json

from haystack.reader.farm import FARMReader
reader = FARMReader(model_name_or_path = "deepset/xlm-roberta-large-squad2", use_gpu = True, max_seq_len = 256)
reader.eval_on_file(data_dir="", test_filename="dev-v2.0.json", device="cuda")
#Output:
{
'EM': 0.8094837025183189,
'f1': 0.852492779607721,
'top_n_accuracy': 0.9785226985597575
}
Here, results are better than others reported in my &lt;denchmark-link:https://github.com/deepset-ai/haystack/issues/476#issuecomment-706553939&gt;comment above&lt;/denchmark-link&gt;
.
Furthermore, I have also run  with  (which is the default value on &lt;denchmark-link:https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py#L548-L554&gt;run_squad script&lt;/denchmark-link&gt;
) and here results are slightly better:
{
'EM': 0.8104101743451528,
'f1': 0.8531653662348317,
'top_n_accuracy': 0.977259327886802
}
Even though the results of these 2 tests appear to be slightly different than those reported in HF model card, it is really weird.
However, I have also noted that  could truly affect the results.
Indeed, in the 2 tests above,  (&lt;denchmark-link:https://github.com/deepset-ai/haystack/blob/master/haystack/reader/farm.py#L47&gt;ref&lt;/denchmark-link&gt;
). Then, I have tried with  and the output is showed here:
{
'EM': 0.5089699317779837,
'f1': 0.5089699317779837,
'top_n_accuracy': 0.9785226985597575
}
		</comment>
		<comment id='10' author='antoniolanza1996' date='2020-10-13T15:28:18Z'>
		Okay, then we now that this is rather something in the Transformers pipeline than in the model itself.

Even though the results of these 2 tests appear to be slightly different than those reported in HF model card, it is really weird.

A small difference is expected as the eval metrics in Haystack are token based whereas the official SQuAD script uses word based metrics.

However, I have also noted that no_ans_boost could truly affect the results.

Yes, this is also expected I would say. With no_ans_boost=100 you basically always predict "no_answer" and the dev set here has probably 50% "no_answer samples"
		</comment>
		<comment id='11' author='antoniolanza1996' date='2020-10-14T07:13:36Z'>
		Ok, I've opened an issue on Transformers repo, probably they can better figure out where is the problem.
&lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
 FYI, &lt;denchmark-link:https://github.com/huggingface/transformers/issues/7774&gt;huggingface/transformers#7774&lt;/denchmark-link&gt;
.
Regarding to this issue, I'm going to close it as RoBERTa model works well. Thanks
		</comment>
		<comment id='12' author='antoniolanza1996' date='2020-10-14T11:50:40Z'>
		&lt;denchmark-link:https://github.com/tholor&gt;@tholor&lt;/denchmark-link&gt;
 I've better investigated on my RoBERTa benchmarks (using ) and I've noted that when the answer is at the beginning of the passage, model isn't able to extract it.
Indeed, I've seen the issue opened by &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 (&lt;denchmark-link:https://github.com/deepset-ai/FARM/issues/552&gt;deepset-ai/FARM#552&lt;/denchmark-link&gt;
)  and I've tried his examples.
This error happens also with &lt;denchmark-link:https://huggingface.co/deepset/electra-base-squad2&gt;ELECTRA-Base&lt;/denchmark-link&gt;
, but not with other models (e.g. &lt;denchmark-link:https://huggingface.co/twmkn9/bert-base-uncased-squad2&gt;BERT&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://huggingface.co/deepset/minilm-uncased-squad2&gt;MiniLM&lt;/denchmark-link&gt;
, &lt;denchmark-link:https://huggingface.co/twmkn9/albert-base-v2-squad2&gt;ALBERT&lt;/denchmark-link&gt;
).
Can you please merge the fix proposed in &lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/564&gt;deepset-ai/FARM#564&lt;/denchmark-link&gt;
?
Instead, If we've to wait for the official merge for some days, can you advise me some hacks to quickly overcome this problem?
NB: Regarding to our tests on SQuAD 2.0 dev set discussed yesterday, they didn't figure out this behaviour probably because in the SQuAD 2.0 dev set there aren't examples like those (this is just my guess).
		</comment>
		<comment id='13' author='antoniolanza1996' date='2020-10-14T13:29:53Z'>
		Hey &lt;denchmark-link:https://github.com/antoniolanza1996&gt;@antoniolanza1996&lt;/denchmark-link&gt;
 the PR you linked (&lt;denchmark-link:https://github.com/deepset-ai/FARM/pull/564&gt;deepset-ai/FARM#564&lt;/denchmark-link&gt;
) only fixes the answer at beginning for label creation.
We do need another PR for making the predictions right, as stated in &lt;denchmark-link:https://github.com/deepset-ai/FARM/issues/552&gt;deepset-ai/FARM#552&lt;/denchmark-link&gt;
. Fixing this bug is in our current sprint, so it should take no more than a week (next sprint starts next Wednesday).
If that is taking too long, I can point you to some suspicions that I have and you could contribute to farm or just create a temporary hack for yourself.
		</comment>
		<comment id='14' author='antoniolanza1996' date='2020-10-14T14:38:58Z'>
		Hi &lt;denchmark-link:https://github.com/Timoeller&gt;@Timoeller&lt;/denchmark-link&gt;
 ,

the PR you linked (deepset-ai/FARM#564) only fixes the answer at beginning for label creation.

Yes, I though that your issueÂ and PR were related. But now that you point this out to me, I checked it out better and in fact I was wrong, sorry for that.

Fixing this bug is in our current sprint, so it should take no more than a week (next sprint starts next Wednesday).

Good to hear. I think both FARM and Haystack users are waiting for this fix considering the impact this bug can have on the results.
However, even though you have written that QA doesn't predict correctly neither for BERT nor for RoBERTa type QA models, I've noted that for BERT there are no problems. The problems come up only with RoBERTa-Base and ELECTRA-Base. Even with RoBERTa-Large and ELECTRA-Large is ok.
from haystack.reader.farm import FARMReader
models = ["deepset/roberta-base-squad2","deepset/electra-base-squad2", "twmkn9/bert-base-uncased-squad2","twmkn9/albert-base-v2-squad2","deepset/minilm-uncased-squad2","ahotrod/electra_large_discriminator_squad2_512","phiyodr/roberta-large-finetuned-squad2"]
readers = []
for model in models:
  reader = FARMReader(model_name_or_path = model, use_gpu = True)
  readers.append(reader)

question = "What is the largest city in Germany?"
context = "Berlin is the capital and largest city of Germany by both area and population."
for model,reader in zip(models,readers):
  prediction = reader.predict_on_texts(question,[context],5)
  print(f"Model {model}: {[answer['answer'] for answer in prediction['answers']]}")
Output:
&lt;denchmark-code&gt;Model deepset/roberta-base-squad2: ['Germany by both area and population.']
Model deepset/electra-base-squad2: ['capital']
Model twmkn9/bert-base-uncased-squad2: ['Berlin']
Model twmkn9/albert-base-v2-squad2: ['Berlin']
Model deepset/minilm-uncased-squad2: ['Berlin']
Model ahotrod/electra_large_discriminator_squad2_512: ['Berlin']
Model phiyodr/roberta-large-finetuned-squad2: ['Berlin']
&lt;/denchmark-code&gt;

		</comment>
		<comment id='15' author='antoniolanza1996' date='2020-10-14T15:04:10Z'>
		Mhh those are interesting findings. Could you add this to the bug report in FARM please?
Another thought: the base roberta model is by us and the large roberta by ahotrod - there might have been differences in training (we trained our models in FARM, ahotrod likely trained his in transformers). So it might not be an issue of roberta/electra base only but more related to FARM QA training. We will have to investigate different models during fixing this bug.
		</comment>
		<comment id='16' author='antoniolanza1996' date='2020-10-14T15:40:18Z'>
		
So it might not be an issue of roberta/electra base only but more related to FARM QA training. We will have to investigate different models during fixing this bug.

Indeed it could be. Ok, I'll wait for your news on fixing this bug, thanks.
		</comment>
		<comment id='17' author='antoniolanza1996' date='2020-11-12T10:04:28Z'>
		Hi &lt;denchmark-link:https://github.com/brandenchan&gt;@brandenchan&lt;/denchmark-link&gt;
 ,
finally I was able to test RoBERTa model.
With  the problem is also solved on my side. Thank you.
FYI, in &lt;denchmark-link:https://haystack.deepset.ai/docs/latest/readermd&gt;Haystack documentation&lt;/denchmark-link&gt;
 it's still reported the older model.
However,  still doesn't predict  (test yourself the snippet in this comment &lt;denchmark-link:https://github.com/deepset-ai/FARM/issues/552#issuecomment-708487987&gt;deepset-ai/FARM#552 (comment)&lt;/denchmark-link&gt;
). Is it somehow related to RoBERTa model error?
		</comment>
		<comment id='18' author='antoniolanza1996' date='2020-11-12T10:28:41Z'>
		
With deepset/roberta-base-squad2-v2 the problem is also solved on my side. Thank you.
FYI, in Haystack documentation it's still reported the older model.

Ok that's great. HuggingFace recently introduced model versioning in &lt;denchmark-link:https://github.com/huggingface/transformers/releases/tag/v3.5.0&gt;their latest release&lt;/denchmark-link&gt;
. We've decided to go along with this meaning that  now contains both versions of the model. Strangely enough, the model card hasn't updated yet, but I am quite sure that the new model weights are uploaded.

However, deepset/electra-base-squad2 still doesn't predict 'Berlin' (test yourself the snippet in this comment deepset-ai/FARM#552 (comment)). Is it somehow related to RoBERTa model error?

We trained an ELECTRA model after the fix but somehow it still wasn't predicting first token so we suspected it was a model specific issue. As a result, we haven't updated deepset/electra-base-squad2. If you need to use the ELECTRA model, then feel free to open a new issue and we will also look into that! But for now, I will be closing this issue.
Thanks for your dedicated bug chasing on this!
		</comment>
		<comment id='19' author='antoniolanza1996' date='2020-11-12T10:59:50Z'>
		With HuggingFace model versioning, are you planning to permanently delete deepset/roberta-base-squad2-v2 in the next days and leave only deepset/roberta-base-squad2 (with both versions)?
I'm asking this so that I can adapt my use-case with the correct model string.
		</comment>
		<comment id='20' author='antoniolanza1996' date='2020-11-18T13:56:13Z'>
		Hi &lt;denchmark-link:https://github.com/antoniolanza1996&gt;@antoniolanza1996&lt;/denchmark-link&gt;
, yes we will be deleting deepset/roberta-base-squad2-v2 in the coming days. deepset/roberta-base-squad2 already contains both!
		</comment>
	</comments>
</bug>