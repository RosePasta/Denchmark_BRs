<bug id='1186' author='annxingyuan' open_date='2019-02-01T22:11:19Z' closed_time='2020-02-21T19:20:45Z'>
	<summary>Possible memory leak in executeAsync.</summary>
	<description>
I'm using the latest tfjs (0.14.1) and calling executeAsync on the converted universal sentence encoder lite model. When I log tf.memory().numTensors before and after executeAsync, I see that 11 new tensors are created. We can't wrap executeAsync in a tidy because it's asynchronous. Would you mind looking into the possibility that there is a memory leak in executeAsync?
I created a codepen that demonstrates the issue: &lt;denchmark-link:https://codepen.io/annxingyuan/pen/LqyGzM&gt;https://codepen.io/annxingyuan/pen/LqyGzM&lt;/denchmark-link&gt;

Thanks!
	</description>
	<comments>
		<comment id='1' author='annxingyuan' date='2019-02-02T21:01:22Z'>
		Also confirmed this is happening in 0.15.0.
		</comment>
		<comment id='2' author='annxingyuan' date='2019-04-23T17:45:31Z'>
		Re-opening as I am experiencing a very similar issue on version 1.0.4 of tfjs.
Here is a reproduction with universal sentence encoder. &lt;denchmark-link:https://codepen.io/tafsiri/pen/BEOENm?editors=0012&gt;https://codepen.io/tafsiri/pen/BEOENm?editors=0012&lt;/denchmark-link&gt;
 Digging into the code a little bit on my local machine suggests that it is the executeAsync call. I also get the same number of tensors leaked as the original issue.
		</comment>
		<comment id='3' author='annxingyuan' date='2019-04-23T17:45:37Z'>
		cc &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;

		</comment>
		<comment id='4' author='annxingyuan' date='2019-12-13T16:22:17Z'>
		Facing the similar issue on tfjs 1.2.10 , After the executeAsync  increases by 54.
Any update on when will the fix be available? &lt;denchmark-link:https://github.com/nsthorat&gt;@nsthorat&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;

		</comment>
		<comment id='5' author='annxingyuan' date='2019-12-13T16:28:41Z'>
		cc &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='annxingyuan' date='2020-07-28T20:01:03Z'>
		I observed potential memory leak in executeAsync in the latest release. Can we consider re-open this issue?
Since my model is more of a differential equation simulation step, there are a lot of control flow ops in the model. It is hard to create a minimal reproducible code snippet, but my code repo, &lt;denchmark-link:https://github.com/alvinsunyixiao/itp-websim/tree/alvin/mem&gt;https://github.com/alvinsunyixiao/itp-websim/tree/alvin/mem&lt;/denchmark-link&gt;
, can be easily tested by running  in the root.
The python code for the TF model is defined &lt;denchmark-link:https://github.com/alvinsunyixiao/itp-websim/blob/alvin/mem/model/spresso_tf.py&gt;here&lt;/denchmark-link&gt;
. This script also conveniently saves the model into TFSavedModel format. The inference where the memory leak was observed is in &lt;denchmark-link:https://github.com/alvinsunyixiao/itp-websim/blob/alvin/mem/src/Spresso.js#L160-L188&gt;this function&lt;/denchmark-link&gt;
. I have tried to log the number of used tensors through  in a variety of locations, and it seems like the number of tensors grows (nondeterministically) every time  is called.
		</comment>
		<comment id='7' author='annxingyuan' date='2020-08-01T13:25:38Z'>
		&lt;denchmark-link:https://github.com/alvinsunyixiao&gt;@alvinsunyixiao&lt;/denchmark-link&gt;
 meanwhile they check and provide fix. Please try using following.
The way to clean any unused tensors in async code is to wrap the code that creates them between a startScope() and an endScope() call.
tf.engine().startScope()
// do your thing
tf.engine().endScope()
Hope it helps
		</comment>
		<comment id='8' author='annxingyuan' date='2020-12-12T09:37:45Z'>
		
@alvinsunyixiao meanwhile they check and provide fix. Please try using following.
The way to clean any unused tensors in async code is to wrap the code that creates them between a startScope() and an endScope() call.
tf.engine().startScope()
// do your thing
tf.engine().endScope()
Hope it helps

&lt;denchmark-link:https://github.com/Nk911&gt;@Nk911&lt;/denchmark-link&gt;

im facing a problem with this startScpoe and endScpoe,
so in my code,
i need to set this first before anything else,
async componentDidMount() {
await tf.ready();
this.model = await mobilenet.load()
this.knnClass = await knnClassifier.create()
}
and for the other things, already wrapped inside startScope and endScope,
and return 0 or 1 numTensor,
the problem is, everytime i refresh the page, or go to another page and come back to TF page,
numTensor increase by 72, everytime i opened the page,
am i doing it wrong ? or its a bug ?
		</comment>
	</comments>
</bug>