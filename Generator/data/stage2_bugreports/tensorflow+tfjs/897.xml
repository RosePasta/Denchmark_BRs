<bug id='897' author='t-mullen' open_date='2018-11-10T06:57:28Z' closed_time='2018-11-15T21:15:29Z'>
	<summary>Calling executeAsync more than once fails</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

0.13.4
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

Chrome 70
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

Any calls after the first to executeAsync will throw the error  Error: Tensor is disposed.. This happens when using the same input tensor, or a new one.
I'm using a pre-trained object detection model converted using tfjs-converter. The model works fine, but only once.
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

const MODEL_URL = '/coco_model/web_model/tensorflowjs_model.pb';
const WEIGHTS_URL = '/coco_model/web_model/weights_manifest.json';

tf.loadFrozenModel(MODEL_URL, WEIGHTS_URL).then(async (model) =&gt; {
    const image_tensor = tf.expandDims(tf.fromPixels(inputVideoElement), 0)
    const result = await model.executeAsync({ image_tensor });
    const result2 = await model.executeAsync({ image_tensor }); // &lt;- ERROR
})
	</description>
	<comments>
		<comment id='1' author='t-mullen' date='2018-11-10T07:04:16Z'>
		My guess is that some intermediate tensors are being disposed of - but I'm not sure how to prevent that. I can't use tidy() around executeAsync(), nor can I call keep() on tensors within a loaded model.
Here's the model used: &lt;denchmark-link:http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz&gt;http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz&lt;/denchmark-link&gt;

And conversion command:
&lt;denchmark-code&gt;tensorflowjs_converter --input_format=tf_frozen_model --output_node_names='num_detections,detection_classes,detection_boxes,detection_scores' --saved_model_tags=serve coco_model/frozen_inference_graph.pb coco_model/web_model
&lt;/denchmark-code&gt;

		</comment>
		<comment id='2' author='t-mullen' date='2018-11-10T22:09:48Z'>
		After digging through Github, I've found that sticking await model.load() before every call to model. executeAsync will avoid the issue. Unfortunately model.load() takes more than twice as long as the actual execution... I shouldn't need to reload the whole model every time, right?
		</comment>
		<comment id='3' author='t-mullen' date='2018-11-11T00:44:44Z'>
		I can confirm that this error doesn't occur in version 0.13.1, but does in 0.13.4. Definitely a regression.
		</comment>
		<comment id='4' author='t-mullen' date='2018-11-15T18:38:35Z'>
		&lt;denchmark-link:https://github.com/t-mullen&gt;@t-mullen&lt;/denchmark-link&gt;
 looks like the weights tensor might have been disposed prematurely, can you confirm if it works on 0.13.3? I notice this only happens on 0.13.4. Thanks!
		</comment>
		<comment id='5' author='t-mullen' date='2018-11-16T17:23:08Z'>
		Works, thanks! üëç
		</comment>
	</comments>
</bug>