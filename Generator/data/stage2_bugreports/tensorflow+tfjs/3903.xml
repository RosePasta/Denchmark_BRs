<bug id='3903' author='vladmandic' open_date='2020-09-10T13:39:49Z' closed_time='2020-10-25T01:50:36Z'>
	<summary>TFJS converter mismanages EfficientDet models</summary>
	<description>
None of the new EfficientDet saved models on TFHub can be used in TFJS.
Actual convert completes ok and creates tfjs_graph_model:
tensorflowjs_converter --input_format=tf_hub --output_format=tfjs_graph_model --signature_name="serving_default" https://tfhub.dev/tensorflow/efficientdet/d0/1 .
But when loading model with tf.loadGraphModel, it fails with
Cannot read property 'children' of undefined
Traced to function below where nodes[nodeName] ends up as undefined.
class OperationMapper {
  ...
  mapFunction(functionDef) {
    ..
    const allNodes = Object.keys(nodes);
    allNodes.forEach(key =&gt; {
      const node = nodes[key];
      node.inputNames.forEach(name =&gt; {
        const [nodeName,] = getNodeNameAndIndex(name);
        node.inputs.push(nodes[nodeName]);
        nodes[nodeName].children.push(node);
      });
    });
Tried adding a null check for node[nodeName] there and then it loads model without errors,
but it just fails later as it cannot recognize input on call to executeAsync
Cannot compute the outputs [Preprocessor/ResizeToRange/cond/resize/Squeeze] from the provided inputs [preprocessor_resizetorange_cond_resize_expanddims_preprocessor_unstack]. Consider providing the following inputs: [].
Reproduction: Convert model from saved model using tensorflowjs_converter and try to load it using tf.loadGraphModel.
Environment: Ubuntu 20.04, Node 14.9.0, TFJS 2.3.0, Browser Chrome 85
	</description>
	<comments>
		<comment id='1' author='vladmandic' date='2020-09-10T17:46:59Z'>
		what model are you using ?
		</comment>
		<comment id='2' author='vladmandic' date='2020-09-10T17:54:51Z'>
		I don't understand - it's in the tensorflowjs_converter command line I've posted.
But it's the same with all EfficientDet models on TFHub.
		</comment>
		<comment id='3' author='vladmandic' date='2020-09-10T18:19:53Z'>
		Apologies , I see now , &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/tafsiri&gt;@tafsiri&lt;/denchmark-link&gt;
 do you guys know if we support  models in TFJS ?
		</comment>
		<comment id='4' author='vladmandic' date='2020-09-14T00:59:25Z'>
		Btw, there's something weird with   from &lt;denchmark-link:https://tfhub.dev/tensorflow/efficientdet/d0/1&gt;https://tfhub.dev/tensorflow/efficientdet/d0/1&lt;/denchmark-link&gt;
 since when I go to the source at &lt;denchmark-link:https://github.com/google/automl/tree/master/efficientdet&gt;https://github.com/google/automl/tree/master/efficientdet&lt;/denchmark-link&gt;
, download  and convert them to , I can then convert  to  and load it in TFJS without issues.
Also, saved_model files created this way can be read without issues using saved_model_cli and summarize_graph while using same commands fails on saved_models from TFHub!


Download and unpack checkpoint from https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco2/efficientdet-d0.tar.gz


Convert checkpoint to saved_model and frozen_model using model_inspect.py provided in the https://github.com/google/automl git repository


Convert saved_model or frozen_model to tfjs_graph_model using tensorflowjs_convert


&lt;denchmark-code&gt;
python model_inspect.py --runmode=saved_model --model_name=efficientdet-d0 --ckpt_path=d0/ckpt --saved_model_dir=d0/saved --tensorrt=FP16

saved_model_cli show --dir=d0/saved
  The given SavedModel contains the following tag-sets: 'serve'

summarize_graph --in_graph="./d0/saved/efficientdet-d0_frozen.pb"
  Found 1 possible inputs: (name=image_files, type=string(7), shape=[?])
  Found 1 possible outputs: (name=detections, op=Pack)

# option#1: convert from saved_model to tfjs_graph_model

tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model --strip_debug_ops=* --control_flow_v2=True d0/saved d0/graph
  Writing weight file d0/graph/model.json...

# option#2: convert from frozen_model to tfjs_graph_model

tensorflowjs_converter --input_format=tf_frozen_model --output_format=tfjs_graph_model --strip_debug_ops=* --control_flow_v2=True --output_node_names=detections d0/saved/efficientdet-d0_frozen.pb d0/graph-frozen
  Writing weight file d0/graph-frozen/model.json...
&lt;/denchmark-code&gt;

I still can't use the model since I'm now running into
&lt;denchmark-code&gt;Error: the input tensors shape does not match
&lt;/denchmark-code&gt;

coming from Pack operation when calling executeAsync.
Which doesn't make much sense since I'm providing input in the shape of [1, width, height, 3] (as specified in model docs) and model expects input [-1, -1, -1, -1].
		</comment>
		<comment id='5' author='vladmandic' date='2020-09-15T16:15:42Z'>
		&lt;denchmark-link:https://github.com/vladmandic&gt;@vladmandic&lt;/denchmark-link&gt;
 Thank you for report the issue. We have identified the bug related to the model execution for TFHub conversion.
&lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/3917&gt;#3917&lt;/denchmark-link&gt;

For saved/frozen model failure, your code show the model input is string, which is different from your image data input for the prediction.
		</comment>
		<comment id='6' author='vladmandic' date='2020-09-15T17:30:57Z'>
		Thanks &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 !
I'll test it as soon as it's in master.
Btw, I've created a standalone Gist which tests 8 different variations/sources of the same  model using both  to load  and  to load converted :
&lt;denchmark-link:https://gist.github.com/vladmandic/a7cf75109b7b48f8914a5b18da5c498f&gt;https://gist.github.com/vladmandic/a7cf75109b7b48f8914a5b18da5c498f&lt;/denchmark-link&gt;

This fix should help with scenarios (4) and (5), but there seems to be an issue with int32 vs uint8 which causes failure on execution of a saved_model in tfjs (didn't test tfjs_graph_model yet).
I've found your earlier fix at &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/2981&gt;#2981&lt;/denchmark-link&gt;
 which patches  by mapping  to , but:

Same is also needed in tfjs-node/saved_model:mapTFDtypeToJSDtype() (and possibly in other places)
During model execution, model expects to receive uint8, but receives int32 and fails with:
error: Session fail to run with error: Expects arg[0] to be uint8 but int32 is provided
So I'm not sure that simply mapping uint8 to int32 is a fix?

Also, on a separate note, why does tensorflowjs_convert mangle model signature?
This is not only for EfficientDet-D0 model, but pretty much all of converted models.
For example, output of tfnode.node.getMetaGraphsFromSavedModel for loaded saved_model is descriptive:
&lt;denchmark-code&gt;inputs: { input_tensor: { dtype: 'uint8', name: 'serving_default_input_tensor:0', shape: [Array] } },
outputs: {
  detection_anchor_indices: { dtype: 'float32', name: 'StatefulPartitionedCall:0', shape: [Array] },
  detection_boxes: { dtype: 'float32', name: 'StatefulPartitionedCall:1', shape: [Array] },
  detection_classes: { dtype: 'float32', name: 'StatefulPartitionedCall:2', shape: [Array] },
  detection_multiclass_scores: { dtype: 'float32', name: 'StatefulPartitionedCall:3', shape: [Array] },
  detection_scores: { dtype: 'float32', name: 'StatefulPartitionedCall:4', shape: [Array] },
  num_detections: { dtype: 'float32', name: 'StatefulPartitionedCall:5', shape: [Array] },
  raw_detection_boxes: { dtype: 'float32', name: 'StatefulPartitionedCall:6', shape: [Array] },
  raw_detection_scores: { dtype: 'float32', name: 'StatefulPartitionedCall:7', shape: [Array] }
}
&lt;/denchmark-code&gt;

While output of model.executor._signature or model.outputs for loaded tfjs_graph_model is cryptic:
&lt;denchmark-code&gt;inputs: { 'input_tensor:0': { name: 'input_tensor:0', dtype: 'DT_UINT8', tensorShape: { dim: [ { size: '1' }, { size: '-1' }, { size: '-1' }, { size: '3' }, [length]: 4 ] } } },
outputs: {
  'Identity_4:0': { name: 'Identity_4:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, { size: '100' }, [length]: 2 ] } },
  'Identity_6:0': { name: 'Identity_6:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, { size: '49104' }, { size: '4' }, [length]: 3 ] } },
  'Identity_2:0': { name: 'Identity_2:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, { size: '100' }, [length]: 2 ] } },
  'Identity_7:0': { name: 'Identity_7:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, { size: '49104' }, { size: '90' }, [length]: 3 ] } },
  'Identity:0': { name: 'Identity:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, { size: '100' }, [length]: 2 ] } },
  'Identity_3:0': { name: 'Identity_3:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, { size: '100' }, { size: '90' }, [length]: 3 ] } },
  'Identity_1:0': { name: 'Identity_1:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, { size: '100' }, { size: '4' }, [length]: 3 ] } },
  'Identity_5:0': { name: 'Identity_5:0', dtype: 'DT_FLOAT', tensorShape: { dim: [ { size: '1' }, [length]: 1 ] } }
}
&lt;/denchmark-code&gt;

I understand both uint8 mapping and model signature names are separate issues from current one, do you want me to open separate issues for those?
		</comment>
		<comment id='7' author='vladmandic' date='2020-09-17T04:54:22Z'>
		yes, please open another issue, the int8 support is for node direct saved model execution. thanks.
		</comment>
		<comment id='8' author='vladmandic' date='2020-09-17T12:01:09Z'>
		I've created separate issues &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/3942&gt;#3942&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/3930&gt;#3930&lt;/denchmark-link&gt;
.
As you've noted, work for core issue here is done under (still in progress) &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/3917&gt;#3917&lt;/denchmark-link&gt;

		</comment>
		<comment id='9' author='vladmandic' date='2020-10-14T15:50:58Z'>
		possible code fix under &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/3917&gt;#3917&lt;/denchmark-link&gt;
 is pending for a month - any updates?
		</comment>
		<comment id='10' author='vladmandic' date='2020-10-22T17:47:53Z'>
		&lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/3917&gt;#3917&lt;/denchmark-link&gt;
 is merged , closing this issue.Thank you
		</comment>
		<comment id='11' author='vladmandic' date='2020-10-22T22:08:48Z'>
		&lt;denchmark-link:https://github.com/rthadur&gt;@rthadur&lt;/denchmark-link&gt;
 Please don't close the issue before it's confirmed as resolved.
in this case, &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/3917&gt;#3917&lt;/denchmark-link&gt;
 has been superseeded by &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/4100&gt;#4100&lt;/denchmark-link&gt;
 which will be merged into master soon.
and then it's not certain it fixes this issue - it needs to be verified.
(and yes, i've verified that &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/3917&gt;#3917&lt;/denchmark-link&gt;
 DOES NOT resolve this issue on its own).
		</comment>
		<comment id='12' author='vladmandic' date='2020-10-24T00:33:11Z'>
		now that &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/4100&gt;#4100&lt;/denchmark-link&gt;
 is committed, i've tried it out and no luck
fresh build from &lt;denchmark-link:https://github.com/tensorflow/tfjs/commit/cbaa09e5558f1eae6610704c2889222f9db4ea7b&gt;master&lt;/denchmark-link&gt;

tested with  () and  () environments - both  exactly the same as in the original report
&lt;denchmark-code&gt;TypeError: Cannot read property 'children' of undefined
    at operation_mapper.js:299
    at Array.forEach (&lt;anonymous&gt;)
    at operation_mapper.js:296
    at Array.forEach (&lt;anonymous&gt;)
    at OperationMapper.mapFunction (operation_mapper.js:294)
    at operation_mapper.js:129
    at Array.reduce (&lt;anonymous&gt;)
    at OperationMapper.transformGraph (operation_mapper.js:128)
    at GraphModel.loadSync (graph_model.js:123)
    at GraphModel.load (graph_model.js:105)
&lt;/denchmark-code&gt;

		</comment>
		<comment id='13' author='vladmandic' date='2020-10-25T01:50:36Z'>
		I've updated tensorflowjs pip packages and re-converted EfficientDet models using tensorflowjs_convert --input_format tf_hub ... and it seems that EfficientDet converted saved_models are now working.
There is another issue with CenterNet models, but I'll open a new issue for that.
I'm closing this issue.
		</comment>
		<comment id='14' author='vladmandic' date='2020-10-25T01:50:38Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/3903&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/3903&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>