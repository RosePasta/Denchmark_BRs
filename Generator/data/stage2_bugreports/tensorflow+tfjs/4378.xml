<bug id='4378' author='huningxin' open_date='2020-12-09T06:37:36Z' closed_time='2021-01-21T04:03:55Z'>
	<summary>GPU memory leak of tf.signal.stft</summary>
	<description>
Please make sure that this is a bug. As per our
GitHub Policy,
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow.js): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow.js installed from (npm or script link): &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.7.0"&gt; &lt;/script&gt;
TensorFlow.js version (use command below): 2.7.0
Browser version: Chrome 87.0.4280.88
Tensorflow.js Converter Version: N/A

Describe the current behavior
There is GPU memory leak after use tf.signal.stft. If use it continuously, it leads to crash the Chrome GPU process.
Describe the expected behavior
There is no GPU memory leak after use tf.signal.stft.
Standalone code to reproduce the issue
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/CodePen/any notebook.
function hanningWindow(N) {
  return tf.tidy(() =&gt; {
    const window = new Float32Array(N);
    for (let i = 0; i &lt; N - 1; ++i) {
      window[i] = 0.5*(1 - Math.cos(6.283185307179586*i/(N-1)));
    }
    return tf.sqrt(window);
  });
}

console.log(tf.engine().memory());
// e.g. {unreliable: false, numBytesInGPU: 0, numBytesInGPUAllocated: 0, numBytesInGPUFree: 0, numTensors: 0, …}

const N = 100; // Reproduce the GPU process crash by setting large number, e.g. 10000 
for (let i = 0; i &lt; N; ++i) {
  const win = 320;
  const fft = 320;
  const hop = 160;
  const input = tf.zeros([1760]);
  const output = tf.signal.stft(input, win, hop, fft, hanningWindow);
  input.dispose();
  output.dispose();
}

console.log(tf.engine().memory());
// e.g. {unreliable: false, numBytesInGPU: 2560000, numBytesInGPUAllocated: 2694080, numBytesInGPUFree: 134080, numTensors: 0, …}
The GPU memory leak can be reproduced by using WebGL backend. The CPU backend doesn't have this issue.
Other info / logs Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
	</description>
	<comments>
		<comment id='1' author='huningxin' date='2020-12-09T17:18:53Z'>
		&lt;denchmark-link:https://github.com/huningxin&gt;@huningxin&lt;/denchmark-link&gt;
 thanks for reporting, the intermediate tensors seem to have be disposed, but the bytesInGPU has increased, &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;
 can you confirm this is normal? Is the numBytesInGPUAllocated counting the texture size?
		</comment>
		<comment id='2' author='huningxin' date='2020-12-15T00:58:23Z'>
		Thanks &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 for your comments. If you'd like to reproduce the Chrome GPU process crash, you can simply set the  to a large number, say 100000, in above &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/4378#issue-760048997&gt;sample code&lt;/denchmark-link&gt;
.
		</comment>
		<comment id='3' author='huningxin' date='2020-12-17T00:29:33Z'>
		&lt;denchmark-link:https://github.com/huningxin&gt;@huningxin&lt;/denchmark-link&gt;
 I notice that the  output is tight to the size , which is also not disposed after stft call.
The other thing you can choose to do to set the texture dispose threshold flag , here is modified code:
    function hanningWindow(N) {
      const window = new Float32Array(N);
      for (let i = 0; i &lt; N - 1; ++i) {
        window[i] = 0.5 * (1 - Math.cos(6.283185307179586 * i / (N - 1)));
      }
      return tf.sqrt(window);
    }

    console.log(tf.engine().memory());
    // e.g. {unreliable: false, numBytesInGPU: 0, numBytesInGPUAllocated: 0, numBytesInGPUFree: 0, numTensors: 0, …}
    // aggressive gpu texture disposal starts at 100M
    tf.env().set("WEBGL_DELETE_TEXTURE_THRESHOLD", 100 * 1024 * 1024);
    const N = 10000; // Reproduce the GPU process crash by setting large number, e.g. 10000 
    for (let i = 0; i &lt; N; ++i) {
      const win = 320;
      const fft = 320;
      const hop = 160;
      const input = tf.zeros([1760]);
      //const ident = (length) =&gt; tf.ones([length]).as1D();
      const output = tf.tidy(() =&gt; {
        return tf.signal.stft(input, win, hop, fft, hanningWindow);
      })
      input.dispose();
      output.dispose();
    }

    console.log(tf.engine().memory());
		</comment>
		<comment id='4' author='huningxin' date='2020-12-17T01:18:00Z'>
		Thanks for your reply, &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 !

@huningxin I notice that the hanningWindow output is tight to the size N, which is also not disposed after stft call.

I understand the output of hanningWindow is used by tf.signal.stft internally. I suppose it would be tf.singla.stft's responsibility to dispose it. Is that correct?

The other thing you can choose to do to set the texture dispose threshold flag WEBGL_DELETE_TEXTURE_THRESHOLD, here is modified code:

I tried the modified code. The numBytesInGPU can still go higher than WEBGL_DELETE_TEXTURE_THRESHOLD, 100 * 1024 * 1024. e.g. after run the loop several times, the output of console.log(tf.engine().memory()); is:
&lt;denchmark-code&gt;{unreliable: false, numBytesInGPU: 130560000, numBytesInGPUAllocated: 130560000, numBytesInGPUFree: 0, numTensors: 0, …}numBytes: 0numBytesInGPU: 130560000numBytesInGPUAllocated: 130560000numBytesInGPUFree: 0numDataBuffers: 0numTensors: 0unreliable: false__proto__: Object
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='huningxin' date='2020-12-17T17:53:01Z'>
		&lt;denchmark-link:https://github.com/huningxin&gt;@huningxin&lt;/denchmark-link&gt;
 I think you are right that, the op is wrapped in the  and , intermediate tensor should have been disposed, which is also verified through the tensor count from the result.
I curious about the size exceeding the limit as well, possibly there are some large intermediate tensor get allocated, which cause the memory to exceed the limit, maybe the disposal of texture is done before the tensor creation. &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;

		</comment>
		<comment id='6' author='huningxin' date='2020-12-28T16:16:57Z'>
		I'm investigating. This is the first bad commit: &lt;denchmark-link:https://github.com/tensorflow/tfjs/commit/c537c813ba8234a909d1e0849abfd95998a8fb5b&gt;c537c81&lt;/denchmark-link&gt;

		</comment>
		<comment id='7' author='huningxin' date='2021-01-21T04:03:55Z'>
		I verified with tf.js 2.8.5 release that the memory leak issue has been fixed. Thanks much &lt;denchmark-link:https://github.com/pyu10055&gt;@pyu10055&lt;/denchmark-link&gt;
 and &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;
 !
		</comment>
		<comment id='8' author='huningxin' date='2021-01-21T04:03:56Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/4378&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/4378&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>