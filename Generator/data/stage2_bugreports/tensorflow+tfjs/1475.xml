<bug id='1475' author='JacopoMangiavacchi' open_date='2019-04-01T17:39:49Z' closed_time='2019-04-02T00:29:15Z'>
	<summary>Performance issues inferencing model in the Electron main process</summary>
	<description>
To get help from the community, we encourage using Stack Overflow and the &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow.js&gt;tensorflow.js&lt;/denchmark-link&gt;
 tag.
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

"@tensorflow/tfjs": "^1.0.3",
"@tensorflow/tfjs-node": "^1.0.2"
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

Electron 3.0.13
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

Experiencing performance differences porting javascript.js code from browser to main Electron process.
Previously I got very good performance inferencing an object detection model (the standard mobilenet-coco-ssd model) on the browser process of an electron app but as I've to load the model from filesystem I moved the code to the main electron process and the inference call is now much slower then before.
I'm loading now the same model from filesystem and I'm proxing the image I want to inference to the electron main process using IpcRendererProxy.  I'm not experiencing any particular delays on proxing the image and the result back to the browser but the final executeAsync call is taking much more time now.
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

	</description>
	<comments>
		<comment id='1' author='JacopoMangiavacchi' date='2019-04-01T22:35:04Z'>
		I figure it out that this is not a Tensorflow.js issue but a more general problem about blocking the Electron main process with long running, cpu intensive operations.
Anyway let me better explain my problem here: I need to load a model using the file: protocol in an Electron app.
I see I can create a worker from the Electron main process and call this using a network interface from the browser but I just wonder if there's a better way, in the specific context of an Electron App, to directly use a model in the browser loading/proxying the model files through IpcRendererProxy.
		</comment>
		<comment id='2' author='JacopoMangiavacchi' date='2019-04-02T00:29:15Z'>
		Fixed this implementing a custom IOHandler to simply proxying the model files (json+bin) using IpcRendererProxy and then load and use the model directly in the browser.
		</comment>
	</comments>
</bug>