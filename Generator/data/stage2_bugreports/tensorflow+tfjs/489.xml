<bug id='489' author='ubertao' open_date='2018-07-04T00:09:25Z' closed_time='2018-12-30T21:19:45Z'>
	<summary>tf.io.loadWeights doesn't work with local file in nodejs</summary>
	<description>
To get help from the community, check out our &lt;denchmark-link:https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs&gt;Google group&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

tfjs 0.11.7
tfjs-node 0.1.7
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

node v8.7.0
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

Got the following error when trying to load local weights file:
(node:73865) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 2): ReferenceError: fetch is not defined
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

const fs = require('fs');
const tf = require('@tensorflow/tfjs');
require('@tensorflow/tfjs-node');

(async function(){
    let manifest = JSON.parse(fs.readFileSync('weights.json'));
    let weights = await tf.io.loadWeights(manifest, 'file://./weights');
})();
	</description>
	<comments>
		<comment id='1' author='ubertao' date='2018-07-04T22:27:22Z'>
		&lt;denchmark-link:https://github.com/ubertao&gt;@ubertao&lt;/denchmark-link&gt;
 Why do you need to load weights individually (i.e., separately from the model architecture)? Can you load the entire model using tf.loadModel(), then get the weights from the model using Model.getWeights()?
		</comment>
		<comment id='2' author='ubertao' date='2018-07-05T10:52:56Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 The model was trained somewhere else, so far I only have access to the weights.
		</comment>
		<comment id='3' author='ubertao' date='2018-07-25T23:28:43Z'>
		+1 on this. I cannot write tests for my models without being able to load the weights from a local file
		</comment>
		<comment id='4' author='ubertao' date='2018-08-18T15:52:18Z'>
		I've had the same issue when using a library which loads the weights, rather than the full model, using that API.
Looks like the library uses an inline IO handler, which delegates to fetch, rather than using the IO Router?
&lt;denchmark-link:https://github.com/chrisdonahue&gt;@chrisdonahue&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/ubertao&gt;@ubertao&lt;/denchmark-link&gt;
 I did manage to workaround this by stubbing a  global which reads from the filesystem
&lt;denchmark-code&gt;global.fetch = async (file) =&gt; {
   return { arrayBuffer: () =&gt; fs.readFileSync(file) }
}
&lt;/denchmark-code&gt;

		</comment>
		<comment id='5' author='ubertao' date='2018-08-19T02:17:50Z'>
		&lt;denchmark-link:https://github.com/jthomas&gt;@jthomas&lt;/denchmark-link&gt;
 That work around shouldn't be necessary. Are you importing (requiring) both &lt;denchmark-link:https://github.com/orgs/tensorflow/teams/tfjs&gt;@tensorflow/tfjs&lt;/denchmark-link&gt;
 and @tensorflow/tfjs-node? If the latter is not imported, the module that handles the native file system won't be available.
		</comment>
		<comment id='6' author='ubertao' date='2018-08-22T16:37:14Z'>
		&lt;denchmark-link:https://github.com/caisq&gt;@caisq&lt;/denchmark-link&gt;
 Unless I've missing something in the code, it didn't seem to me that the  method uses the IOHandler system to read files?
Let me give you the background, I'm using this library (&lt;denchmark-link:https://github.com/justadudewhohacks/face-api.js&gt;face-api.js&lt;/denchmark-link&gt;
) which &lt;denchmark-link:https://github.com/justadudewhohacks/face-api.js/blob/47a54b57b7b376cba157824f2b8d7f6cdf9bc2b3/src/commons/loadWeightMap.ts#L44-L46&gt;loads weights&lt;/denchmark-link&gt;
 directly using the same snippet as this issue.
&lt;denchmark-code&gt;const manifest = await (await fetch(manifestUri)).json()
return tf.io.loadWeights(manifest, modelBaseUri)
&lt;/denchmark-code&gt;

Looking at the  code, &lt;denchmark-link:https://github.com/tensorflow/tfjs-core/blob/master/src/io/weights_loader.ts#L130-L138&gt;it reads&lt;/denchmark-link&gt;
 the manifest for all the paths and then calls . This inline function uses fetch to retrieve the URLs directly, rather than going through the IOHandler, which causes the issue above?
&lt;denchmark-code&gt;export async function loadWeightsAsArrayBuffer(
    fetchURLs: string[], requestOptions?: RequestInit): Promise&lt;ArrayBuffer[]&gt; {
  // Create the requests for all of the weights in parallel.
  const requests = fetchURLs.map(fetchURL =&gt; fetch(fetchURL, requestOptions));
  const responses = await Promise.all(requests);
  const buffers =
      await Promise.all(responses.map(response =&gt; response.arrayBuffer()));
  return buffers;
}
&lt;/denchmark-code&gt;

The @tensorflow/tfjs-node module doesn't appear to overwrite or extend these functions. Have I missed something in the code?
		</comment>
		<comment id='7' author='ubertao' date='2018-08-23T13:45:03Z'>
		James,

That's correct - tf.io.loadWeights doesn't use IOHandlers under the hood.
It works only for http-based cases because it uses fetch.

What are you trying to achieve? Depending on the answer, I may be able to
suggest solutions.

Best,
Shanqing
&lt;denchmark-link:#&gt;…&lt;/denchmark-link&gt;


On Wed, Aug 22, 2018 at 12:37 PM James Thomas ***@***.***&gt; wrote:
 @caisq &lt;https://github.com/caisq&gt; Unless I've missing something in the
 code, it didn't seem to me that the tf.io.loadWeights method uses the
 IOHandler system to read files?

 Let me give you the background, I'm using this library (face-api.js
 &lt;https://github.com/justadudewhohacks/face-api.js&gt;) which loads weights
 &lt;https://github.com/justadudewhohacks/face-api.js/blob/47a54b57b7b376cba157824f2b8d7f6cdf9bc2b3/src/commons/loadWeightMap.ts#L44-L46&gt;
 directly using the same snippet as this issue.

 const manifest = await (await fetch(manifestUri)).json()
 return tf.io.loadWeights(manifest, modelBaseUri)

 Looking at the tf.io.loadWeights code, it reads
 &lt;https://github.com/tensorflow/tfjs-core/blob/master/src/io/weights_loader.ts#L130-L138&gt;
 the manifest for all the paths and then calls loadWeightsAsArrayBuffer.
 This inline function uses fetch to retrieve the URLs directly, rather than
 going through the IOHandler, which causes the issue above?

 export async function loadWeightsAsArrayBuffer(
     fetchURLs: string[], requestOptions?: RequestInit): Promise&lt;ArrayBuffer[]&gt; {
   // Create the requests for all of the weights in parallel.
   const requests = fetchURLs.map(fetchURL =&gt; fetch(fetchURL, requestOptions));
   const responses = await Promise.all(requests);
   const buffers =
       await Promise.all(responses.map(response =&gt; response.arrayBuffer()));
   return buffers;
 }

 The @tensorflow/tfjs-node module doesn't appear to overwrite or extend
 these functions. Have I missed something in the code?

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 &lt;#489 (comment)&gt;,
 or mute the thread
 &lt;https://github.com/notifications/unsubscribe-auth/AQC5fmuY3ZsFsrxRmS49kToITJ5mpDmcks5uTYjLgaJpZM4VByIK&gt;
 .


-- 
---
Shanqing Cai
Software Engineer
Google
cais@google.com

		</comment>
		<comment id='8' author='ubertao' date='2018-08-23T15:49:25Z'>
		I can't speak for James but my use case is that I want to be able to write tests for my model which load model weights from a locally-stored manifest.json and compare to known input/output pairs from Python.
Currently I have an extremely undesirable workaround where I save the output of tf.io.loadWeights to a JSON file in the browser. Then, I load that JSON file in the test and pass the result to my model, where it would otherwise expect the result of tf.io.loadWeights. This shouldn't be necessary and the JSON files are huge because floating point values are serialized as strings.
Local testing of models has already helped me identify very subtle failure cases and should be supported.
		</comment>
		<comment id='9' author='ubertao' date='2018-12-30T21:19:45Z'>
		This is now fixed in the latest release (0.2.1) of @tensorflow/tfjs-node and @tensorflow/tfjs-node-gpu
		</comment>
	</comments>
</bug>