<bug id='1967' author='Neuroforge' open_date='2019-09-04T06:10:20Z' closed_time='2019-09-05T01:37:39Z'>
	<summary>Loading a Converted TensorFlow Model</summary>
	<description>
To get help from the community, we encourage using Stack Overflow and the &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow.js&gt;tensorflow.js&lt;/denchmark-link&gt;
 tag.
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

1.2.6
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

A converted model appears to not work correctly. The mean value from the TensorMap appears to be 0 and has a shape of [0], despite the input being [1,96,192,3]. In Python the input has the same dimensions as this javascript object.
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

I have a working model in Tensorflow that works fine with Python. It is now a saved model and attempted to convert to TensorflowJS.
A converted model appears to not work correctly. The mean value from the TensorMap appears to be 0 and has a shape of [0], despite the input being [1,96,192,3].
The conversion was done as follows....
&lt;denchmark-code&gt;tensorflowjs_converter --input_format=tf_saved_model ~/Projects/models/model_0 ~/Projects/modelsjs/model_0
&lt;/denchmark-code&gt;

This works fine and it also appears to load ok. However, when it comes to predicting, errors are thrown and any advice would be appreciated.
&lt;denchmark-code&gt;&lt;script&gt;
    async function handleButtonClick(){

       for(var i=0; i&lt;1;i++)
       {
           var t0 = performance.now();
           console.log("Loading - model_"+i);
           var inputTensor = tf.tensor4d(input);
           var model = await tf.loadGraphModel('/modelsjs/model_'+i+'/model.json');
           var poutput = model.predict(inputTensor);
       }
&lt;/script&gt;
&lt;/denchmark-code&gt;

The error appears as follows.
&lt;denchmark-code&gt;graph_model.ts:213 Uncaught (in promise) Error: The model contains control flow or dynamic shape ops, please use executeAsync method
    at t.execute_ (graph_model.ts:213)
    at t.predict (graph_model.ts:169)
    at (index):157
    at engine.ts:156
    at t.scopedRun (engine.ts:167)
    at t.tidy (engine.ts:153)
    at Object.t.tidy (environment.ts:186)
    at handleButtonClick ((index):156)
&lt;/denchmark-code&gt;

As per the above error, the prediction was attempted using executeAsync but produces the error relevant to this question.
&lt;denchmark-code&gt;&lt;script&gt;
    async function handleButtonClick(){

       for(var i=0; i&lt;1;i++)
       {
           var t0 = performance.now();
           console.log("Loading - model_"+i);
           var inputTensor = tf.tensor4d(input);
           var model = await tf.loadGraphModel('/modelsjs/model_'+i+'/model.json');
           console.log("Load model_" + i + "took " + (t1 - t0) + " milliseconds.");
           const res = await model.executeAsync(inputTensor);
       }
&lt;/script&gt;
&lt;/denchmark-code&gt;

The Error appears as follows. And appears to be related to the $mean value from the Tensormap. This value is [0]
&lt;denchmark-code&gt;broadcast_util.ts:81 Uncaught (in promise) Error: Operands could not be broadcast together with shapes 1,12,24,64 and 0.
    at un (broadcast_util.ts:81)
    at new kn (batchnorm_packed_gpu.ts:32)
    at t.batchNormalization (backend_webgl.ts:869)
    at Bt.engine.runKernel.$x (batchnorm.ts:344)
    at engine.ts:206
    at t.scopedRun (engine.ts:167)
    at t.runKernel (engine.ts:202)
    at $a (batchnorm.ts:343)
    at batchNorm (operation.ts:46)
    at xy (normalization_executor.ts:31)
&lt;/denchmark-code&gt;

&lt;denchmark-link:https://user-images.githubusercontent.com/1474186/64229868-64513680-cf2e-11e9-95ad-29e5d013f5e4.png&gt;&lt;/denchmark-link&gt;

Setting 'strict' to true, the model still loads and does not throw and error.
&lt;denchmark-code&gt;var model = await 
        tf.loadGraphModel('/modelsjs/model_0/model.json', {onProgress:onProgressCallback, strict:true});

&lt;/denchmark-code&gt;

Appears to be similar to - &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/1583&gt;#1583&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/dsmilkov&gt;@dsmilkov&lt;/denchmark-link&gt;
 perhaps you have some insight here. I have tried your previous suggestion of adding the tags and signature fields.
I also tried converting using the --saved_mode_tags and the --signature_name values.
&lt;denchmark-code&gt;tensorflowjs_converter --saved_model_tags=serve --signature_name=serving_default --input_format=tf_saved_model ~/Projects/models//model_0 ~/Projects/modelsjs/model_0 

&lt;/denchmark-code&gt;

Original model has serve tag.
&lt;denchmark-code&gt;saved_model_cli show --dir ~/Projects/models/model_0
&lt;/denchmark-code&gt;

The given SavedModel contains the following tag-sets:
serve
Update:
When i search for is_training it appears that both appear inside of model.json. Is this relevant, important?
"is_training": {"b": true} and  "is_training": {"b": false},
Setting all instances of is_training to 'true' in the model.json file does not fix the issues either.
	</description>
	<comments>
		<comment id='1' author='Neuroforge' date='2019-09-05T01:37:39Z'>
		Ok. This has been solved. After speaking to the creator of the model, it appears that the model was created in Keras and converted to a Tensorflow SavedModel.  The follow up conversion to TensorflowJS did not work.
However, converting to TensorflowJS from the original Keras models seems to work fine.
		</comment>
	</comments>
</bug>