<bug id='1735' author='ykume' open_date='2019-07-12T06:42:20Z' closed_time='2019-07-17T10:15:46Z'>
	<summary>Conv2D with 1x1 kernel returns incorrect result</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

1.1.2
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

Firefox 68.0, Chrome 75.0.3770.100
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

When the model is converted from Keras, Conv2D with 1x1 kernel returns different result than Keras in a certain condition.
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

&lt;denchmark-h:h5&gt;Python&lt;/denchmark-h&gt;

import numpy as np
import tensorflow.keras as keras

shape = (192, 192, 1)
inputs = keras.Input(shape)
outputs = []

x = inputs
x = keras.layers.Conv2D(24, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False, 
                        kernel_initializer=keras.initializers.RandomNormal(seed=42), name='conv1')(x)
outputs.append(x)
x = keras.layers.Conv2D(24, kernel_size=(1,1), strides=(1,1), padding='same', use_bias=False, 
                        kernel_initializer=keras.initializers.RandomNormal(seed=42), name='conv2')(x)
outputs.append(x)

model = keras.models.Model(inputs=inputs, outputs=outputs)
model.save('simple_convx2.h5')

x = np.arange(shape[0] * shape[1]) / (shape[0] * shape[1])
x = x.reshape(shape)[np.newaxis,:,:,:]
ys = model.predict(x)
for i, y in enumerate(ys):
  yf = y.flatten()
  print("out{}: min {}, max {}, mean: {}".format(i+1, np.min(y), np.max(y), np.mean(y)))
  print("min idx: {}".format(np.argmin(y)))
  print("max idx: {}".format(np.argmax(y)))
The result of this code is following:
&lt;denchmark-code&gt;out1: min -0.24379302561283112, max 0.27761271595954895, mean: 0.00583014078438282
min idx: 884696
max idx: 880083
out2: min -0.09378751367330551, max 0.10990175604820251, mean: 0.005884638521820307
min idx: 875542
max idx: 880095
&lt;/denchmark-code&gt;

&lt;denchmark-h:h5&gt;Convert to TensorFlow.js&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;tensorflowjs_converter --input_format keras simple_convx2.h5 simple_convx2
&lt;/denchmark-code&gt;

&lt;denchmark-h:h5&gt;TensorFlow.js&lt;/denchmark-h&gt;

tf.loadLayersModel('./simple_convx2/model.json')
  .then(pretrainedModel =&gt; {
    const model = pretrainedModel;
    const shape = [1, 192, 192, 1];
    const x = new Float32Array(shape[1] * shape[2]);
    for (var i = 0; i &lt; x.length; i++) { x[i] = i / x.length; }
    
    const tx = tf.tensor(x, shape);
    const ys = model.predict(tx);
    for (var i = 0; i &lt; ys.length; i++) {
      const ty = ys[i];
      const y = ty.dataSync();
      let min = y[0];
      let max = y[0];
      let total = 0;
      for (var j = 0; j &lt; y.length; j++) {
        min = Math.min(min, y[j]);
        max = Math.max(max, y[j]);
        total += y[j];
      }
      const mean = total / y.length;
      console.log("out" + (i+1) + ": min " + min + ", max " + max + ", mean: " + mean);
      for (var j = 0; j &lt; y.length; j++) {
        if (y[j] == min) { console.log("min idx: " + j); }
      }
      for (var j = 0; j &lt; y.length; j++) {
        if (y[j] == max) { console.log("max idx: " + j); }
      }
    }
    tf.dispose(tx);
    tf.dispose(ys);
  });
The result of the code is following:
&lt;denchmark-code&gt;out1: min -0.24379301071166992, max 0.27761271595954895, mean: 0.005830140475020938
min idx: 884696
max idx: 880083
out2: min -0.09930852800607681, max 0.11706908047199249, mean: 0.005957962305450471
min idx: 880102
max idx: 880071
&lt;/denchmark-code&gt;

If one of following, the Keras/tfjs results seem to be same.

Another kernel size like 3x3
Small input size like 128x128x1
Small number of filters like 16
CPU backend

tf.ENV.set('WEBGL_PACK', false); does not affect the issue.
	</description>
	<comments>
		<comment id='1' author='ykume' date='2019-07-13T00:57:55Z'>
		Simplified version.
&lt;denchmark-h:h5&gt;Python&lt;/denchmark-h&gt;

import numpy as np
import tensorflow.keras as keras

shape = (192, 192, 1)
inputs = keras.Input(shape)
c1 = keras.layers.Conv2D(24, kernel_size=(3,3), strides=(1,1), padding='same', use_bias=False, 
                        kernel_initializer=keras.initializers.Constant(1), name='conv1')(inputs)
c2 = keras.layers.Conv2D(24, kernel_size=(1,1), strides=(1,1), padding='same', use_bias=False, 
                        kernel_initializer=keras.initializers.Constant(1), name='conv2')(c1)
model = keras.models.Model(inputs=inputs, outputs=c2)
x = np.ones(shape)[np.newaxis,:,:,:]
y = model.predict(x).flatten()
print(y[4580:4590])
c = 0
for yv in y:
  if not (yv == 96 or yv == 144 or yv == 216): c += 1
print("{}/{},{}".format(c, len(y), np.sum(y.astype(np.float64))))
Result:
&lt;denchmark-code&gt;[144. 144. 144. 144.  96.  96.  96.  96.  96.  96.]
0/884736,189778176.0
&lt;/denchmark-code&gt;

&lt;denchmark-h:h5&gt;TensorFlow.js&lt;/denchmark-h&gt;

&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.1.2/dist/tf.min.js"&gt;&lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;script&gt;
      tf.setBackend('webgl');
      const shape =[192, 192, 1];
      const input = tf.input({shape: shape});
      const conv1 = tf.layers.conv2d({filters: 24, kernelSize:[3,3], strides:[1,1], padding:'same', useBias:false, 
                                      kernelInitializer:tf.initializers.constant({value: 1})}).apply(input);
      const conv2 = tf.layers.conv2d({filters: 24, kernelSize:[1,1], strides:[1,1], padding:'same', useBias:false, 
                                      kernelInitializer:tf.initializers.constant({value: 1})}).apply(conv1);
      const model = tf.model({inputs: input, outputs: conv2});
      const x = new Float32Array(shape[0] * shape[1] * shape[2]);
      for (let i = 0; i &lt; x.length; i++) { x[i] = 1; }
      shape.unshift(1);
      const tx = tf.tensor(x, shape);
      const ty = model.predict(tx);
      const y = ty.dataSync();
      console.log(y.slice(4580, 4590));
      let c = t = 0;
      for (let i = 0; i &lt; y.length; i++) {
        t += y[i];
        if (!(y[i] == 96 || y[i] == 144 || y[i] == 216)) { c++; } 
      }
      console.log(c + "/" + y.length + "," + t);
      tf.dispose([tx, ty]);
    &lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
Result:
&lt;denchmark-code&gt;Float32Array(10) [ 144, 144, 144, 144, 100, 100, 100, 100, 100, 100 ]
432888/884736,187181328
&lt;/denchmark-code&gt;

I think it may not be a performance issue but rather a bug.
		</comment>
		<comment id='2' author='ykume' date='2019-07-14T06:28:53Z'>
		I think I've found a workaround.
Using Conv2DTranspose instead of Conv2D (with transposed weights) seems to work correctly.
		</comment>
		<comment id='3' author='ykume' date='2019-07-15T13:42:13Z'>
		Hi &lt;denchmark-link:https://github.com/ykume&gt;@ykume&lt;/denchmark-link&gt;
 - I'm looking into this issue. Which device are you using for testing? For what it's worth, I'm not able to reproduce on my 2018 MacBook Pro.
		</comment>
		<comment id='4' author='ykume' date='2019-07-15T22:19:35Z'>
		Hi, &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;
. Thank you for your response.
I am testing on Windows 10 1809 with Intel UHD Graphics 620. The driver version is 23.20.16.4973.
And I've tested on other PCs, and found that the code returns correct result on other PCs.

Windows 10 1903, NVIDIA GeForce RTX 2070, driver version 25.21.14.1634
Windows 10 1809, Intel UHD Graphics 600, driver version 23.20.16.4936

So I think the issue may depend on GPU.
		</comment>
		<comment id='5' author='ykume' date='2019-07-17T11:39:20Z'>
		Thank you for the quick fix! I'm looking forward to the next release.
		</comment>
	</comments>
</bug>