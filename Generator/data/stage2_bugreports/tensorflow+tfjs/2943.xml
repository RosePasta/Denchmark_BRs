<bug id='2943' author='vpontis' open_date='2020-03-23T04:38:25Z' closed_time='2020-04-23T17:38:41Z'>
	<summary>Bug with expo-camera recording video + Tensor Flow</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

&lt;denchmark-code&gt;    "@tensorflow-models/posenet": "^2.2.1",
    "@tensorflow/tfjs": "^1.7.0",
    "@tensorflow/tfjs-react-native": "^0.2.3",
&lt;/denchmark-code&gt;

&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

iOS 13, iPhone 11 Pro
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

There is a bug with Tensor Flow while expo-camera is recording so that Tensor Flow does not get new image data to process or render to the screen.
I am using the Tensor Flow PoseModel to track the user's pose. When I turn on recording with expo-camera, Tensor Flow no longer receives an updated image tensor so the screen looks stuck.
I think there is a problem with getting the correct WebGL texture from expo-camera while the camera is recording. But I am new to both Tensor Flow and WebGL so I haven't figured out the issue.
&lt;denchmark-link:https://youtu.be/xg0Ln2GWYJI&gt;YouTube Video That Shows Bug&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

This repo gives a minimal reproduction: &lt;denchmark-link:https://github.com/vpontis/expo-camera-tensor-flow-bug&gt;https://github.com/vpontis/expo-camera-tensor-flow-bug&lt;/denchmark-link&gt;

cc &lt;denchmark-link:https://github.com/tafsiri&gt;@tafsiri&lt;/denchmark-link&gt;
 who it looks like wrote the camera code for react-native and might have an idea here. You've done a really awesome job, by the way. Thank you so much for this library!
	</description>
	<comments>
		<comment id='1' author='vpontis' date='2020-03-23T20:25:45Z'>
		I suspect expo doesn't support updating the camera texture while recording to a file. Something you could try is getting the cameraTexture after you start recording. I think this would involve not using CameraWithTensorStream but making your own component that uses camera directly. I'll take a closer look at this but I don't think there is a quick fix here.
Does the recording turn out as you expect?
		</comment>
		<comment id='2' author='vpontis' date='2020-03-23T22:32:06Z'>
		&lt;denchmark-link:https://github.com/tafsiri&gt;@tafsiri&lt;/denchmark-link&gt;
 thanks for the quick reply.
I'll try what you suggested -- getting the cameraTexture after I start recording.
I'll let you know what I end up figuring out :)
		</comment>
		<comment id='3' author='vpontis' date='2020-03-26T06:56:54Z'>
		&lt;denchmark-link:https://github.com/tafsiri&gt;@tafsiri&lt;/denchmark-link&gt;
 I talked with the Expo team and it looks like the issue is from here: &lt;denchmark-link:https://github.com/expo/expo/blob/2a73695021431dc52e46030d5b28954accaaf50a/packages/expo-camera/ios/EXCamera/EXCamera.m#L498&gt;https://github.com/expo/expo/blob/2a73695021431dc52e46030d5b28954accaaf50a/packages/expo-camera/ios/EXCamera/EXCamera.m#L498&lt;/denchmark-link&gt;

The expo-camera module uses AVSession for iOS which does not allow both buffering image data for processing and writing to a video file.
As a workaround, I am periodically taking snapshots of the glView. It's not a perfect solution, but it should work.
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Another question: Is there any way to tweak the performance characteristics of the model other than the &lt;denchmark-link:https://github.com/tensorflow/tfjs-models/tree/master/posenet#config-params-in-posenetload&gt;config params in posenet.load()&lt;/denchmark-link&gt;
?
The &lt;denchmark-link:https://www.tensorflow.org/lite/models/pose_estimation/overview&gt;Posenet overview&lt;/denchmark-link&gt;
 gives a benchmark value of 5 ms with GPU on iPhone XS but I am testing on my iPhone 11 Pro and I'm not getting near 5ms (the best I've seen is ~30ms for ).
My config:
const TENSOR_SIZE = {
  width: 152,
  height: 200,
};

    const posenetModel = await posenet.load({
      // Config param information
      // https://github.com/tensorflow/tfjs-models/tree/master/posenet#config-params-in-posenetload
      architecture: 'MobileNetV1',
      outputStride: 16,
      inputResolution: { width: TENSOR_SIZE.width, height: TENSOR_SIZE.height },
      multiplier: 0.5,
      quantBytes: 4,
    });
		</comment>
		<comment id='4' author='vpontis' date='2020-03-26T15:10:43Z'>
		&lt;denchmark-link:https://github.com/vpontis&gt;@vpontis&lt;/denchmark-link&gt;
 thanks for looking into this and following up.
With regard to performance I'd say the other main axis of control you have is what size image tensor you pass in. I think you've got a reasonably small image going in though. How are you measuring it? Are you just isolating the posenet inference or including some of the image resizing etc.
Also the benchmark you point to is for tf-lite, so tfjs inference times for posenet will be higher (I think the last time we tested we saw numbers around ~20ms on iPhone X). You could compare with a browser implementation on the same device to see how much overhead the react native part introduces (there is some serialization penalty due to the react native bridge).
		</comment>
		<comment id='5' author='vpontis' date='2020-04-23T17:38:41Z'>
		Closing this due to lack of activity, feel to reopen. Thank you
		</comment>
	</comments>
</bug>