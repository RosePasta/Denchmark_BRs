<bug id='2785' author='sseppola' open_date='2020-02-23T17:55:08Z' closed_time='2020-03-03T15:41:20Z'>
	<summary>tfjs - trouble running speech commands in React Native CLI app</summary>
	<description>
To get help from the community, we encourage using Stack Overflow and the &lt;denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow.js&gt;tensorflow.js&lt;/denchmark-link&gt;
 tag.
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

"@tensorflow-models/speech-commands": "0.4.2",
"@tensorflow/tfjs": "1.5.2",
"@tensorflow/tfjs-react-native": "0.2.3",
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

"react-native": "0.61.5",
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

I'm having a lot of trouble loading  as it seems to be missing dependencies. I'm fairly certain I initialized the project correctly according to the &lt;denchmark-link:https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native#setting-up-a-react-native-app-with-tfjs-react-native&gt;setup guide&lt;/denchmark-link&gt;

&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

I initialized the project with React Native CLI
&lt;denchmark-code&gt;$ npx react-native init [projectName] --template react-native-template-typescript
&lt;/denchmark-code&gt;

and followed &lt;denchmark-link:https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native#setting-up-a-react-native-app-with-tfjs-react-native&gt;https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native#setting-up-a-react-native-app-with-tfjs-react-native&lt;/denchmark-link&gt;

After the setup the app runs fine on both Android and iOS, however, as soon as I try to load @tensorflow-models/speech-commands I get problems:
First: error: bundling failed: Error: Unable to resolve module utilfromnode_modules/@tensorflow-models/speech-commands/dist/browser_fft_utils.js: util could not be found within the project.
Fixed with yarn add util
Second: error: bundling failed: Error: Unable to resolve module fsfromnode_modules/@tensorflow-models/speech-commands/dist/browser_fft_utils.js: fs could not be found within the project.
This one I have not been able to resolve, and it seems to be triggered by the line: recognizer = speechCommands.create('BROWSER_FFT', 'directional4w');
The closest issue I found to this was from &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/1682&gt;#1682&lt;/denchmark-link&gt;
 in an expo-cli app, and I tried the tip from &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/1682#issuecomment-534231135&gt;#1682 (comment)&lt;/denchmark-link&gt;
 where I  after  without luck.
I'm a bit confused as the naming of the files implies that it runs in the browser, however both util and fs are node.js modules. I'm expecting there's some polyfilling needed to get this running properly.
Here's the relevant code:
import React, {useEffect} from 'react';
import {View, Text} from 'react-native';
import {Audio} from 'expo-av';

import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-react-native';
import * as speechCommands from '@tensorflow-models/speech-commands';

export default function SpeechCommands() {
  useEffect(() =&gt; {
    let recognizer: speechCommands.SpeechCommandRecognizer | undefined;

    const start = async () =&gt; {
      await tf.ready();

      // the following line throws an error:
      recognizer = speechCommands.create('BROWSER_FFT', 'directional4w');
    };

    start();

    return () =&gt; {
      if (recognizer) {
        recognizer.stopListening();
      }
    };
  }, []);

  return (
    &lt;View&gt;
      &lt;Text&gt;SpeechCommands component&lt;/Text&gt;
    &lt;/View&gt;
  );
}
	</description>
	<comments>
		<comment id='1' author='sseppola' date='2020-02-26T19:53:37Z'>
		You are correct on the polyfilling. This was written before react-native support and I believe uses conditional requires in some places for node modules. Metro however tries to load them all. The quickest resolution may be to try something like &lt;denchmark-link:https://github.com/tradle/rn-nodeify&gt;https://github.com/tradle/rn-nodeify&lt;/denchmark-link&gt;
 to shim the various node require statements.
However there is a larger issue that will likely prevent you from using this model. The model is wholly dependent on using the browser based FFT implementation to convert audio into something the model can process. Since this is not present in React Native (AFAIK), you would need some other way to convert the audio into the same representation (and I know of no straightforward way to do thisâ€”though it wouldn't be impossible). I would thus pretty much consider this model browser only.
		</comment>
		<comment id='2' author='sseppola' date='2020-02-27T11:09:22Z'>
		Good tip regarding rn-nodeify!
The larger issue you mention seems tricky as I'm just getting into Tensorflow, could I bounce some ideas off you to decide how to proceed?
As far as I can tell I've got 3 options, ordered by what I think would be easiest:
: Use native models
Given that  is for the browser, would it be possible to do the opposite and load a native ios/android model into  + ?
I'm thinking I could just use the model from the tensorflow light example: &lt;denchmark-link:https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands&gt;https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands&lt;/denchmark-link&gt;

: Use the native Tensorflow libraries with native models
Start from &lt;denchmark-link:https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands&gt;https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands&lt;/denchmark-link&gt;
 and continue native/brownfield app development working with the native Tensorflow library and models directly.
Option C: Convert the audio into the same representation
The two other options seem easier for me, so we can get back to what this involves if necessary.
		</comment>
		<comment id='3' author='sseppola' date='2020-02-27T15:23:49Z'>
		Option A
We don't support converting from tflite models directly. But the example you pointed to has python training scripts that will output a model that you can try to &lt;denchmark-link:https://github.com/tensorflow/tfjs/tree/master/tfjs-converter&gt;convert&lt;/denchmark-link&gt;
. This involves setting up tensorflow and getting familiar with the python side (which may not be a problem). However it will still require you to come up with a way to capture audio in react native and convert it to the representation expected by that model.
Option B
As I understand it, this means using &lt;denchmark-link:https://www.tensorflow.org/lite&gt;tf-lite&lt;/denchmark-link&gt;
 directly in an Android/iOS app developed with the native stacks for those operating systems? If so, this will be the most straightforward assuming you are comfortable with native app development.
Option C
Its like option above, the hard part is getting the audio in react-native and converting it to a model appropriate format.
Other than that I don't know your overall goals, but I would definitely suggest looking at &lt;denchmark-link:https://www.tensorflow.org/lite&gt;tflite&lt;/denchmark-link&gt;
 and seeing if it matches your requirements.
Option A or C would be great for other JavaScript/React Native devs as you could publish a model on npm more tailored to RN! But I can't really estimate the time or complexity involved.
		</comment>
		<comment id='4' author='sseppola' date='2020-02-27T16:38:45Z'>
		Aha, now I'm starting to see some of my misconceptions.
I thought I could reuse the same model between tfjs and tflite, and that @tensorflow/react-native bridged into native, now that I read the &lt;denchmark-link:https://github.com/tensorflow/tfjs/tree/master/tfjs-react-native&gt;description&lt;/denchmark-link&gt;
 again I understand it executes in js!
I'm working on a PoC ios/android app that needs:

to recognize a handful of words (&lt; 10, here the speech_commands is enough)
learn a few custom words (common for all users, not per-user)
adapt to the user's voice/pronunciation
run on-device for extended sessions in the background

As far as I understand now this is possible with both tflite and tfjs, and point 2 and 3 will require "transfer learning" from the existing model.
Can you confirm this? And do you know which of tflite and tfjs would be more battery intensive?
I don't know native development, but I'm probably going to do a PoC in Swift and tflite to get familiarity with the tools, and to find highly applicable guides, tutorials, and examples. I'll then reassess how I can bring this into react-native when I have a better understanding, at this point it seems a bit too complicated to convert the model and figure out the audio capture.
		</comment>
		<comment id='5' author='sseppola' date='2020-03-03T09:54:26Z'>
		&lt;denchmark-link:https://github.com/tafsiri&gt;@tafsiri&lt;/denchmark-link&gt;
 you've been more than helpful, thank you so much :)
Feel free to close the issue, I'm happy to figure out the rest myself
		</comment>
		<comment id='6' author='sseppola' date='2020-03-03T15:41:19Z'>
		Sounds good. To your point 3, transfer learning is what you would want for specific user adaptation. Point 2 may or may not use transfer learning. However you might be able to get pretty far even without transfer learning. I can't really comment on the battery side. All the best with your project!
		</comment>
		<comment id='7' author='sseppola' date='2020-09-11T09:51:31Z'>
		Hey &lt;denchmark-link:https://github.com/sseppola&gt;@sseppola&lt;/denchmark-link&gt;
 did you find a way to reach out to a conclusion for your use case?
&lt;denchmark-code&gt;I'm working on a PoC ios/android app that needs:

1. to recognize a handful of words (&lt; 10, here the speech_commands is enough)
2. learn a few custom words (common for all users, not per-user)
3. adapt to the user's voice/pronunciation
4. run on-device for extended sessions in the background
&lt;/denchmark-code&gt;

If yes, could you please share the process on how to achieve this?
		</comment>
		<comment id='8' author='sseppola' date='2020-09-17T08:35:21Z'>
		Hi &lt;denchmark-link:https://github.com/tyagit&gt;@tyagit&lt;/denchmark-link&gt;

Given the criteria above I gave up on using tfjs in favor of tflite. These are the points I considered:

I had an example to base my code on
tflite has been around longer, so it should be easier to learn/have better support
tflite is more capable than tfjs, and so I considered it more likely to support the personalization features by the time I need it

I copied the files from the tflite speech commands example into my RN project, and manually integrated across the RN bridge it to handle start, stop, and emit inference events. It was quite the learning experience.
Here's some thoughts on how this achieved the criteria:


As far as learning custom words I'm collecting more samples and training a general model periodically. At this point I do not have acceptable accuracy for custom words but I expect to achieve it with time.


I have not done personalization, nor had an urgent need for it yet. Having submitted my own voice samples and trained the general model it seemed it got a little more accurate. I'll probably get back to this at some point, but the performance is good enough as it is.


My app runs fine in the background for extended periods. I expect tflite to be more efficient/faster as it runs native code vs crossing the bridge N times per second to run inference in JS. That said, however, I have not measured it.


		</comment>
	</comments>
</bug>