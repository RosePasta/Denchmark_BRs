<bug id='1046' author='davidsoergel' open_date='2018-12-20T19:14:32Z' closed_time='2019-06-07T17:33:47Z'>
	<summary>Datasets created from preexisting Tensors can be iterated at most once.</summary>
	<description>
The tfjs-data Dataset creation APIs assume that the underlying data (e.g., an array passed to tf.data.array(...) or a url passed to tf.data.csv(...)) can be accessed repeatedly.
However if a Dataset is created from preexisting Tensors (e.g., tf.data.array(foo: Tensor[])) then this assumption does not hold, because those Tensors will be disposed--at least on the first iteration through the Dataset, if not sooner.
So, we need to either:

Prohibit passing Tensors in.
Detect this situation in tf.data.array(...) and automatically apply keep() to the Tensors.  We should carefully document the memory-management consequences in this case, since it might not be clear to the user what's going on.
Accept Tensors, but automatically .dataSync() them.  Like 2), this has consequences / tradeoffs.
I think options 2 and 3 would sort of implicitly / automatically happen if you say tf.data.array(tensors).cache(), once we have the caching stuff hooked up (#1025, #1024).
... more options I haven't thought of?

	</description>
	<comments>
		<comment id='1' author='davidsoergel' date='2018-12-20T21:00:05Z'>
		I think the correct way is for tf.data.array(tensors) to call .clone() on each tensor internally. This does incref, and separates the user-owned tensors from the dataset-owned tensors. Each can dispose independently.
		</comment>
		<comment id='2' author='davidsoergel' date='2018-12-20T21:07:28Z'>
		Code to reproduce this issue:
const a = tf.ones([2, 1]);
const b = tf.ones([2, 1]);
const ds = tf.data.array([a, b])
await ds.forEach(elem =&gt; elem.print());
a.isDisposed // true, should be false.
		</comment>
		<comment id='3' author='davidsoergel' date='2018-12-20T21:30:44Z'>
		+1, that's what I meant by option 2, except that I got confused about clone() vs. keep().  SGTM
		</comment>
		<comment id='4' author='davidsoergel' date='2018-12-26T21:52:30Z'>
		Even with tf.data.array(tensors) calling .clone() on each tensor internally, the created dataset still could only be iterated once, though user-owned tensors are separated. This disables multiple epoches with model.fitDataset().
		</comment>
		<comment id='5' author='davidsoergel' date='2019-01-02T16:26:56Z'>
		Oh good point.  Is this actually the same as &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/1025&gt;#1025&lt;/denchmark-link&gt;
, then?  I.e. when returning a Tensor from the cache, we have to increase the refcount every time because the downstream pipeline will dispose it.
		</comment>
		<comment id='6' author='davidsoergel' date='2019-01-02T18:19:45Z'>
		Can you call clone() upon the creation of a new iterator() from an Array-backed dataset? This way multiple iterations should be fine.
		</comment>
		<comment id='7' author='davidsoergel' date='2019-01-07T14:15:11Z'>
		Is this fixed?
		</comment>
		<comment id='8' author='davidsoergel' date='2019-01-07T18:04:52Z'>
		Yes this is fixed.
		</comment>
		<comment id='9' author='davidsoergel' date='2019-05-20T16:33:47Z'>
		This is not quite fixed.
The solution at &lt;denchmark-link:https://github.com/tensorflow/tfjs-data/blob/master/src/iterators/lazy_iterator.ts#L497&gt;https://github.com/tensorflow/tfjs-data/blob/master/src/iterators/lazy_iterator.ts#L497&lt;/denchmark-link&gt;
 works for , but not for -- i.e., a single Tensor will be cloned, but Tensors within an array or nested structure will not.
I think we should write tf.deepClone() for this.  (Could be in data for now but ultimately belongs in core imho).
		</comment>
	</comments>
</bug>