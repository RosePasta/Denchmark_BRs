<bug id='4062' author='rfourdinier' open_date='2020-10-13T10:55:36Z' closed_time='2020-11-10T18:16:52Z'>
	<summary>Error when executing most of the models with TFJS 1.X and 2.X</summary>
	<description>
Hi, i have an error i cannot get rid of with TFJS.
First of all, I am getting these errors since the first time i have been trying to use TFJS, except in a few cases that I will explain:
The errors always happen when I am executing the model with these lines:
&lt;denchmark-code&gt;this.breakerModel = await tf.loadFrozenModel('./assets/converted/eee/model.json');
//or tf.loadGraphModel depending on tfjs version

async detectVideo(video, model){
  
  const image_tensor = tf.expandDims(tf.browser.fromPixels(video), 0);
  image_tensor.print();

  await model.executeAsync({ image_tensor }).then(result =&gt; { 
 //or model.executeAsync({ input_tensor }) depending on model used

    console.log(result);
  });
}
&lt;/denchmark-code&gt;

The function raising all the following errors is 'model.executeAsync(input)'.
'video' in this code snippet refers to HTMLVideoElement from webcam or hard-coded video file.
The errors are the following:
=== With TFJS 1.X and models trained with Python TF Object Detection API (TF 1.15.X) ===

with 'ssd_mobilenet_v1_coco_11_06_2017' ---&gt; no error, working perfectly
with 'ssd_mobilenet_v1_fpn_2018_07_03', 'faster_rcnn_inception_v2_coco_2018_01_28', 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03' ---&gt;

core.js:6241 ERROR Error: Uncaught (in promise): TypeError: Cannot read property '0' of undefined TypeError: Cannot read property '0' of undefined at tf-core.esm.js:17 at ei (tf-core.esm.js:17) at tf-core.esm.js:17 at tf-core.esm.js:17 at Array.map (&lt;anonymous&gt;) at Za (tf-core.esm.js:17) at tf-core.esm.js:17 at tf-core.esm.js:17 at o.getAndSaveBinary (tf-core.esm.js:17) at o.runWebGLProgram (tf-core.esm.js:17) at resolvePromise (zone-evergreen.js:798) at zone-evergreen.js:705 at rejected (tslib.es6.js:72) at ZoneDelegate.invoke (zone-evergreen.js:364) at Object.onInvoke (core.js:41667) at ZoneDelegate.invoke (zone-evergreen.js:363) at Zone.run (zone-evergreen.js:123) at zone-evergreen.js:857 at ZoneDelegate.invokeTask (zone-evergreen.js:399) at Object.onInvokeTask (core.js:41645)
When i switch from default backend 'webgl' to 'cpu' with  'await tf.setBackend('cpu')', the models are executing without error but it takes of course too much time.
When i execute the model on an hard-coded image source HTMLImageElement, the model is working perfectly, but if I execute the model from an HTMLImageElement which is extracted from the video canvas i get from my Webcam, it is not working and displays the same error as above.
=== With TFJS 1.X and models trained with Python TF Object Detection API (TF 2.3.1) ===

with 'efficientdet_d1_coco17_tpu-32' ---&gt;

Error in matMul: inputs must have the same rank of at least 2, got ranks 5 and 2.
When i switch from default backend 'webgl' to 'cpu' with  'await tf.setBackend('cpu')', error persists.
=== With TFJS 2.X and model trained with Python TF Object Detection API (TF 2.3.1) ===

with 'efficientdet_d1_coco17_tpu-32' ---&gt;

ERROR Error: Uncaught (in promise): Error: Argument 'x' passed to 'expandDims' must be a Tensor or TensorLike, but got 'Promise'
When i switch from default backend 'webgl' to 'cpu' with  'await tf.setBackend('cpu')', error persists.
=== With TFJS 2.X and models trained with Python TF Object Detection API (TF 1.15.X) ===

with 'ssd_mobilenet_v1_coco_11_06_2017', 'ssd_mobilenet_v1_fpn_2018_07_03', 'faster_rcnn_inception_v2_coco_2018_01_28', 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03' ---&gt;

   ERROR Error: Uncaught (in promise): Error: Argument 'a' passed to 'less' must be a Tensor or TensorLike, but got 'Promise' Error: Argument 'a' passed to 'less' must be a Tensor or TensorLike, but got 'Promise' at convertToTensor (tensor_util_env.js:84) at less_ (less.js:37) at Module.less (operation.js:40) at Module.executeOp (logical_executor.js:34) at operation_executor.js:63 at engine.js:425 at t.e.scopedRun (engine.js:436) at t.e.tidy (engine.js:423) at Module.tidy (globals.js:166) at operation_executor.js:63 at resolvePromise (zone-evergreen.js:798) at zone-evergreen.js:705 at rejected (tslib.es6.js:72) at ZoneDelegate.invoke (zone-evergreen.js:364) at Object.onInvoke (core.js:41654) at ZoneDelegate.invoke (zone-evergreen.js:363) at Zone.run (zone-evergreen.js:123) at zone-evergreen.js:857 at ZoneDelegate.invokeTask (zone-evergreen.js:399) at Object.onInvokeTask (core.js:41632) defaultErrorLogger @ core.js:6228
When i switch from default backend 'webgl' to 'cpu' with  'await tf.setBackend('cpu')', error persists.
I tried with a few versions of @tensorflow-models/coco-ssd, because of issue &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/3485&gt;#3485&lt;/denchmark-link&gt;
 which error is similar.
I have tried loading the model with tfjs-converter library instead of basic tfjs library but the errors in all the above cases are the same.
I am using ionic and node.js, but I cannot use tfjs-node because of a few not related errors . It should nevertheless work with classic tfjs (not node) version, right ?
This is very strange because it works perfectly for only one specific model and TF/TFJS version, but any others cases raises error.
I have been trying to resolve this problem for almost one month, any suggestion would be great.
Thanks.
	</description>
	<comments>
		<comment id='1' author='rfourdinier' date='2020-10-15T20:27:07Z'>
		Hi &lt;denchmark-link:https://github.com/rfourdinier&gt;@rfourdinier&lt;/denchmark-link&gt;
, thank you the detailed description. Is it possible to share the models you mentioned, it will help us debug. One immediate thing I noticed is that you pass image_tensor as namedTensorMap in this line , is the expected input name image_tensor? Is there a input signature for this model? If there is a signature and the expected input name is different, then the image_tensor will be ignored. Also, you mentioned using node.js, webgl backend only works in Browser.
		</comment>
		<comment id='2' author='rfourdinier' date='2020-10-16T11:13:24Z'>
		Hi, thanks for the reply
I am not sure about the signature but all the models come from TF1/TF2 detection zoo and the only thing I have done about input is using --input_type=image_tensor when converting, not sure if this is what you mean, but if it is not, it is very likely that I have not added a signature myself.
with TF1 I convert to frozen_model with this command:
!python3 export_inference_graph.py --input_type=image_tensor --pipeline_config_path=training/pipeline.config --trained_checkpoint_prefix=training/model.ckpt-5331 --output_directory=frozen_graph #ne pas toucher
and then converting to TFJS format with this command:
!tensorflowjs_converter --input_format=tf_frozen_model --quantization_bytes 1 --saved_model_tags=serve --output_node_names='detection_boxes,detection_classes,detection_scores,num_detections' ./frozen_graph/frozen_inference_graph.pb ./converted_and_quantized11/
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

with TF2 I convert to saved_model with this command:
!python3 exporter_main_v2.py --input_type=image_tensor --trained_checkpoint_dir='training/' --output_directory='inference_graph3/' --pipeline_config_path='training/efficientdet_d1_coco17_tpu-32.config'
and then converting fo TFJS format with this command:
!tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes,detection_classes,detection_scores,num_detections' --saved_model_tags=serve --output_format=tfjs_graph_model ./efficientdet_d1_coco17_tpu-32/saved_model ./converted6/
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

Webgl backend only works in Browser, this is kind of strange because using TF1 I think I succeeded executing the first model as explained above with webgl backend (I have printed it to be sure and set backend to webgl my self again, but maybe it used nevertheless a different backend, it was very fast though)
&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

With the TF2 versions model, for example EfficientDet, I cannot use model.executeAsync({ image_tensor }) but I need to use model.executeAsync({ input_tensor }) instead in order to execute the line (then raising the other error), I don't know if it can help.
I uploaded three models:


The first one 'ssd_mobilenet_v1_coco_11_06_2017', trained with TF 1.x, which raises no error using TFJS 1.x but raises error using TFJS 2.x


The second one 'ssd_mobilenet_v1_fpn_2018_07_03', trained with TF 1.x, which raises error using TFJS 1.x and TFJS 2.x


The third one 'efficientdet_d1_coco17_tpu-32' trained with TF 2.x, which raises error using TFJS 1.x and TFJS 2.x:


You can download the models from here: &lt;denchmark-link:https://gofile.io/d/OTXUKV&gt;https://gofile.io/d/OTXUKV&lt;/denchmark-link&gt;

Thanks
		</comment>
		<comment id='3' author='rfourdinier' date='2020-10-20T21:04:40Z'>
		Hi &lt;denchmark-link:https://github.com/rfourdinier&gt;@rfourdinier&lt;/denchmark-link&gt;
, if you are in a Node.js env, webgl backend doesn't work, because it relies on browser native api to run on gpu. If it works in Node.js env, it is only because the input size is small, so the computation is forwarded to cpu to execute. There is no guarantee the forward happens for every kernel. Here's my investigation results:


ssd_mobilenet_v1_fpn_2018_07_03, can run with TFJS 2.x using cpu backend. Using webgl backend has issue, I've created a separate bug tracking particular for that error. Please track the issue there.


efficient_d1_coco17_cpu-32, cannot unpack the rar file. Can you pack the file as .zip and upload again?


ssd_mobilenet_v1_coco_11_06_2017. You said cpu has no error with TFJS 2.x, right?


In terms of how to use the model to predict, look for the signature field in model.json. You need to provide the input as &lt;denchmark-link:https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/tensor_types.ts#L21&gt;NamedTensorMap&lt;/denchmark-link&gt;
. The name has to be exact same as in the signature field except anything other colon. So if the signature name is input1:0, your named tensor should be input1.
		</comment>
		<comment id='4' author='rfourdinier' date='2020-10-21T10:11:15Z'>
		Hi &lt;denchmark-link:https://github.com/lina128&gt;@lina128&lt;/denchmark-link&gt;
 ,
Thank you for all these explanations, it makes sense!
However, I still cannot execute any of the models I have provided with TFJS 2.X ; My models are executing with TFJS 1.X and 'cpu' backend but with TFJS 2.X an error is raised even with 'cpu' backend, and for all the models.
For example, with "ssd_mobilenet_v1_fpn_2018_07_03" I get the following error:
core.js:6241 ERROR Error: Uncaught (in promise): Error: Argument 'a' passed to 'less' must be a Tensor or TensorLike, but got 'Promise' Error: Argument 'a' passed to 'less' must be a Tensor or TensorLike, but got 'Promise' at convertToTensor (tensor_util_env.js:84) at less_ (less.js:38) at Module.less__op (operation.js:43) at Module.executeOp (logical_executor.js:35) at operation_executor.js:63 at engine.js:314 at Engine.scopedRun (engine.js:324) at Engine.tidy (engine.js:313) at Module.tidy (globals.js:173) at operation_executor.js:63 at resolvePromise (zone-evergreen.js:798) at zone-evergreen.js:705 at rejected (tslib.es6.js:72) at ZoneDelegate.invoke (zone-evergreen.js:364) at Object.onInvoke (core.js:41667) at ZoneDelegate.invoke (zone-evergreen.js:363) at Zone.run (zone-evergreen.js:123) at zone-evergreen.js:857 at ZoneDelegate.invokeTask (zone-evergreen.js:399) at Object.onInvokeTask (core.js:41645)
Uncaught (in promise) Error: Argument 'x' passed to 'clone' must be a Tensor or TensorLike, but got 'Promise' at convertToTensor (tensor_util_env.js:84) at clone_ (clone.js:36) at clone__op (operation.js:43) at cloneTensor (utils.js:120) at Module.executeOp (control_executor.js:97) at operation_executor.js:49 at executeOp (operation_executor.js:89) at GraphExecutor.processStack (graph_executor.js:371) at GraphExecutor.executeWithControlFlow (graph_executor.js:332) at GraphExecutor._executeAsync (graph_executor.js:278)
I installed tfjs@2.6.0 and then imported with:
const tf = require('@tensorflow/tfjs'); require("@tensorflow/tfjs-backend-cpu"); this.breakerModel = await tf.loadGraphModel('./assets/converted/testest/model.json'); await tf.setBackend('cpu'); console.log(tf.getBackend()); tf.ready().then( () =&gt; { this.detectVideo(this.video, this.breakerModel); } );
then in this.detectVideo:
   const image_tensor = tf.expandDims(tf.browser.fromPixels(video), 0); image_tensor.print(); await model.executeAsync( image_tensor ).then( result =&gt; { console.log("execution done"); console.log(result);  } );
Can you please tell me if I use it the right way ?
&lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/3485&gt;#3485&lt;/denchmark-link&gt;
 is about the same issue, but I did not understand how it was solved, I have latest version of coco-ssd and still the same error.
Also, you can find the .zip 'efficient_d1_coco17_cpu-32' model here: &lt;denchmark-link:https://gofile.io/d/Twrm7j&gt;https://gofile.io/d/Twrm7j&lt;/denchmark-link&gt;

Thank you very much for your help.
		</comment>
		<comment id='5' author='rfourdinier' date='2020-10-21T17:12:45Z'>
		Hi &lt;denchmark-link:https://github.com/rfourdinier&gt;@rfourdinier&lt;/denchmark-link&gt;
, how do you get the video as HTMLVideoElement in node.js env?
		</comment>
		<comment id='6' author='rfourdinier' date='2020-10-21T17:23:51Z'>
		Another observation is that if you are going to use the union package, you don't need to import cpu backend additionally. This may cause some problems.
		</comment>
		<comment id='7' author='rfourdinier' date='2020-10-23T10:29:45Z'>
		Hi &lt;denchmark-link:https://github.com/lina128&gt;@lina128&lt;/denchmark-link&gt;
 ,
I have tried not importing cpu backend using union package unfortunately it does not resolve the error.
I am getting the video as HTMLVideoElement with this snippet:
From my webcam:
&lt;denchmark-code&gt;webcam_init() {
this.logTime("Initializing webcam...");

if(navigator.getUserMedia){

  console.log('Android');

  const videoCam = &lt;HTMLVideoElement&gt; document.createElement('video');
  videoCam.setAttribute('id', 'vid');
  videoCam.hidden = true;
  videoCam.height = window.innerHeight * 0.8;
  videoCam.width = window.innerWidth;

  navigator.getUserMedia({
    audio: false, 
    video: { facingMode: "environment" ,
        width: {min: 1280, ideal: 1360, max: 4096},
        height: {min: 675, ideal: 720, max: 2160}
    } 
  },
  function(stream) {
    videoCam.srcObject = stream;
    videoCam.onloadedmetadata = () =&gt; {
      videoCam.play();
     };

  },
  function(err) {
     console.log("The following error occurred: " + err.name);
  }
  );
  videoCam.style.marginTop = '60px';
  document.body.appendChild(videoCam);
  this.video = videoCam;
} 
&lt;/denchmark-code&gt;

&lt;denchmark-h:hr&gt;&lt;/denchmark-h&gt;

If I want to use a hard-coded video file I just add these lines:
In HTML:
    &lt;video id = "myVideo" muted&gt; &lt;source src='../../assets/test/videotest.mp4'&gt; &lt;/video&gt;
In JS:
this.video =&lt;HTMLVideoElement&gt;document.getElementById('myVideo');
Thanks.
		</comment>
		<comment id='8' author='rfourdinier' date='2020-10-23T20:59:58Z'>
		Hi &lt;denchmark-link:https://github.com/rfourdinier&gt;@rfourdinier&lt;/denchmark-link&gt;
, we found the root cause for errors that complaining tensor is a Promise. This PR &lt;denchmark-link:https://github.com/tensorflow/tfjs/pull/4122&gt;#4122&lt;/denchmark-link&gt;
 fixes it., we are going to release the fix in 2.7.0 next Monday. I see that your stack trace has zone.js, which causes issue for async code. zone.js monkey patch all the async calls, and in our code, we check promise using , where the  could be the one before patching. The fix uses a different way to assert promise, we have tested it with an Angular application. Please try it again after 2.7.0 release.
		</comment>
		<comment id='9' author='rfourdinier' date='2020-10-27T15:30:12Z'>
		Hi &lt;denchmark-link:https://github.com/lina128&gt;@lina128&lt;/denchmark-link&gt;
 ,
Updating to TFJS 2.7.0 seems to have resolved my problems!
Now i am able to execute models without raising any error.
Of course I still cannot execute them really fast but it is normal behaviour because I am not using tfjs-node.
Your assistance was greatly appreciated, thank you very much !
		</comment>
		<comment id='10' author='rfourdinier' date='2020-10-27T17:15:08Z'>
		That's great to hear, &lt;denchmark-link:https://github.com/rfourdinier&gt;@rfourdinier&lt;/denchmark-link&gt;
 :)
		</comment>
		<comment id='11' author='rfourdinier' date='2020-11-03T18:10:45Z'>
		This issue has been automatically marked as stale because it has not had recent activity. It will be closed in 7 days if no further activity occurs. Thank you.
		</comment>
		<comment id='12' author='rfourdinier' date='2020-11-10T18:16:51Z'>
		Closing as stale. Please @mention us if this needs more attention.
		</comment>
		<comment id='13' author='rfourdinier' date='2020-11-10T18:16:53Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/4062&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/4062&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>