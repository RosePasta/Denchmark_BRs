<bug id='460' author='caisq' open_date='2018-06-21T18:16:14Z' closed_time='2018-06-22T14:44:24Z'>
	<summary>Model.predict() leaks WebGL memory when numExamples &amp;gt; batchSize</summary>
	<description>
To get help from the community, check out our &lt;denchmark-link:https://groups.google.com/a/tensorflow.org/forum/#!forum/tfjs&gt;Google group&lt;/denchmark-link&gt;
.
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

0.11.7 (latest)
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

Doesn't matter
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

During model.predict() calls, if the number of examples is greater than batchSize, the predict() will be be completed in multiple batches. In this case WebGL memory leak can be seen, whereas there in memory leak if the number of examples is &lt;= batchSize.
See original discussion and reproduction code in:
&lt;denchmark-link:https://groups.google.com/a/tensorflow.org/forum/#!topic/tfjs/PfDR9bh5ttk&gt;https://groups.google.com/a/tensorflow.org/forum/#!topic/tfjs/PfDR9bh5ttk&lt;/denchmark-link&gt;

	</description>
	<comments>
	</comments>
</bug>