<bug id='1167' author='BenjaminWegener' open_date='2019-01-29T14:16:27Z' closed_time='2019-02-04T22:37:57Z'>
	<summary>OOM, increasing number of tensors used</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

{tfjs-core: "0.14.5", tfjs-data: "0.1.7", tfjs-layers: "0.9.2", tfjs-converter: "0.7.2", tfjs: "0.14.2"}
&lt;denchmark-h:h4&gt;Browser version&lt;/denchmark-h&gt;

Google Chrome is up to date
Version 71.0.3578.98 (Official Build) (64-bit) (windows 10)
same in FF
&lt;denchmark-h:h4&gt;Describe the problem or feature request&lt;/denchmark-h&gt;

OOM, increasing number of tensors used in model.fit()? tf.memory() shows increasing number of tensors.
LOG:
{tfjs-core: "0.14.5", tfjs-data: "0.1.7", tfjs-layers: "0.9.2", tfjs-converter: "0.7.2", tfjs: "0.14.2"}
layer_utils.ts:62 _________________________________________________________________
layer_utils.ts:152 Layer (type)                 Output shape              Param #
layer_utils.ts:64 =================================================================
layer_utils.ts:152 conv2d_Conv2D1 (Conv2D)      [null,28,28,8]            208
layer_utils.ts:74 _________________________________________________________________
layer_utils.ts:152 average_pooling2d_AveragePoo [null,14,14,8]            0
layer_utils.ts:74 _________________________________________________________________
layer_utils.ts:152 conv2d_Conv2D2 (Conv2D)      [null,10,10,8]            1608
layer_utils.ts:74 _________________________________________________________________
layer_utils.ts:152 average_pooling2d_AveragePoo [null,5,5,8]              0
layer_utils.ts:74 _________________________________________________________________
layer_utils.ts:152 conv2d_transpose_Conv2DTrans [null,7,7,4]              292
layer_utils.ts:74 _________________________________________________________________
layer_utils.ts:152 conv2d_transpose_Conv2DTrans [null,32,32,1]            1601
layer_utils.ts:74 =================================================================
layer_utils.ts:83 Total params: 3709
layer_utils.ts:84 Trainable params: 3709
layer_utils.ts:85 Non-trainable params: 0
layer_utils.ts:86 _________________________________________________________________
bug.html:117 undefined
256performance warning: READ-usage buffer was read back without waiting on a fence. This caused a graphics pipeline stall.
gpgpu_util.ts:331 WebGL: too many errors, no more errors will be reported to the console for this context.
(...)
bug.html:126 epoch: 1/1000 | loss: 0 | memory: {"unreliable":false,"numBytesInGPU":276980,"numTensors":54,"numDataBuffers":54,"numBytes":277044}
bug.html:126 epoch: 2/1000 | loss: 19.962234497070312 | memory: {"unreliable":false,"numBytesInGPU":539124,"numTensors":88,"numDataBuffers":88,"numBytes":539188}
bug.html:126 epoch: 3/1000 | loss: 17.720489501953125 | memory: {"unreliable":false,"numBytesInGPU":801268,"numTensors":122,"numDataBuffers":122,"numBytes":801332}
bug.html:126 epoch: 4/1000 | loss: 42.162967681884766 | memory: {"unreliable":false,"numBytesInGPU":1063412,"numTensors":156,"numDataBuffers":156,"numBytes":1063476}
&lt;denchmark-h:h4&gt;Code to reproduce the bug / link to feature request&lt;/denchmark-h&gt;

&lt;denchmark-link:https://github.com/tensorflow/tfjs/files/2808397/bug.html.txt&gt;bug.html.txt&lt;/denchmark-link&gt;

-&gt; temporal workaround is to save model, reload page and model (&lt;denchmark-link:https://git.io/fhyIf&gt;https://git.io/fhyIf&lt;/denchmark-link&gt;
)
	</description>
	<comments>
		<comment id='1' author='BenjaminWegener' date='2019-02-04T22:37:57Z'>
		As you are iterating through the data, you should dispose any tensors.
You have:
let batch = getBatch();
var history = await model.fit(batch.batch, batch.valBatch, ...);
Change to:
let batch = tf.tidy(() =&gt; getBatch());
var history = await model.fit(batch.batch, batch.valBatch, ...);
tf.dispose(batch);
		</comment>
		<comment id='2' author='BenjaminWegener' date='2019-02-05T14:16:13Z'>
		thanks for your help!
		</comment>
	</comments>
</bug>