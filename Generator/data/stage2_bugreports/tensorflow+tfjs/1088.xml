<bug id='1088' author='caisq' open_date='2019-01-11T23:32:45Z' closed_time='2019-02-19T15:38:34Z'>
	<summary>tfjs-node-gpu@0.2.3 doesn't make use of GPU when multiple GPUs are present</summary>
	<description>
&lt;denchmark-h:h4&gt;TensorFlow.js version&lt;/denchmark-h&gt;

tfjs-node-gpu 0.2.3
When calling Model.fit() (e.g., in the mnist-node example in tfjs-node) while depending tfjs-node-gpu, nvidia-smi shows 0% GPU volatile usage and baseline memory usage. The training speed appears to be identical to tfjs-node.
	</description>
	<comments>
		<comment id='1' author='caisq' date='2019-02-19T15:38:34Z'>
		it turns out that this is not a bug in tfjs-node-gpu. I can't use my GPU because it has an old CUDA Compute Capability version (5.2), which is lower than the minimum requirement of the libtensorflow in the latest version of tfjs-node (6.0).
		</comment>
	</comments>
</bug>