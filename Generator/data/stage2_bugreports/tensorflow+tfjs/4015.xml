<bug id='4015' author='fawazahmed0' open_date='2020-10-04T14:00:30Z' closed_time='2020-10-07T17:16:04Z'>
	<summary>Screen goes blank on embedding large data with USE model in a browser</summary>
	<description>
System information

Windows 7 Service Pack 1 Build 7601
TensorFlow.js Version: 2.4.0 (Latest)
USE Model: 1.3.2 (Latest)
Browser version: Chrome 85.0.4183.121 (Latest)
GPU: RX550 2GB (Radeon Software 20.2.2 )


The Screen seems to go blank when embedding large data using &lt;denchmark-link:https://www.npmjs.com/package/@tensorflow-models/universal-sentence-encoder&gt;USE model&lt;/denchmark-link&gt;
 in browser, the only way to recover is by hard rebooting the pc

&lt;denchmark-link:https://codepen.io/fawazahmed0/pen/bGpXxbw?editors=1111&gt;CodePen&lt;/denchmark-link&gt;

	</description>
	<comments>
		<comment id='1' author='fawazahmed0' date='2020-10-05T14:36:23Z'>
		This may be a driver issue, assuming you are using webgl it generally shouldn't be able to bring your machine regardless of what is going on in the tab.
To help us understand more about this:

How large is large data?
Which tfjs backend are you using?
Could you post a screenshot of what you see here https://js.tensorflow.org/debug/

To help resolve your ability to use USE:

Can you split your data into smaller groups to pass to the model rather than trying to embed it all at once? You may already be hitting the max texture size for webgl on your browser so you may have to do this anyway.

		</comment>
		<comment id='2' author='fawazahmed0' date='2020-10-05T20:54:45Z'>
		Thanks &lt;denchmark-link:https://github.com/tafsiri&gt;@tafsiri&lt;/denchmark-link&gt;
 for replying, I did tried the above &lt;denchmark-link:https://codepen.io/fawazahmed0/pen/bGpXxbw?editors=1111&gt;CodePen&lt;/denchmark-link&gt;
 in a laptop having integrated Intel HD 5500 Graphics and in that the screen goes blank and then driver restarts with the screen coming alive with a Driver failed notification. But in my pc, the screen never comes back alive, have to hard reboot.


How large is large data?


6000+ text lines


Which tfjs backend are you using?


I did not set any backend, so probably the default one.


Could you post a screenshot of what you see here https://js.tensorflow.org/debug/


&lt;denchmark-link:https://user-images.githubusercontent.com/20347013/95129889-51d0ae80-0779-11eb-904a-b9be9d6af63a.jpg&gt;&lt;/denchmark-link&gt;



Can you split your data into smaller groups


I tried embedding 50 lines at a time, but even that causes this issue, see &lt;denchmark-link:https://codepen.io/fawazahmed0/pen/JjKPEzZ?editors=1111&gt;codepen Batch Mode&lt;/denchmark-link&gt;

Thanks
		</comment>
		<comment id='3' author='fawazahmed0' date='2020-10-05T22:17:59Z'>
		I couldn't understand your use of splice in your codepen. But I forked it so that I could try batching, here is a &lt;denchmark-link:https://codepen.io/tafsiri/pen/zYBOwVB?editors=1111&gt;link to my codepen&lt;/denchmark-link&gt;
, you can adjust the batch size used. It will still take a while to process all your data. You may want to consider doing it once (possibly on node) and saving the embeddings so that you can re-use them.
		</comment>
		<comment id='4' author='fawazahmed0' date='2020-10-06T00:07:30Z'>
		The &lt;denchmark-link:https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/splice&gt;splice&lt;/denchmark-link&gt;
 takes slice of array each time it's called, so similar to batching.

here is a link to my codepen,

I tried your &lt;denchmark-link:https://codepen.io/tafsiri/pen/zYBOwVB?editors=1111&gt;codepen&lt;/denchmark-link&gt;
, even that has the same issue, I will share a gif and that would probably explain the issue, it seems to be a memory leakage issue, observe the GPU memory used(Dedicated):
&lt;denchmark-link:https://user-images.githubusercontent.com/20347013/95144050-4db28a00-0795-11eb-8373-9e88138eacd9.gif&gt;&lt;/denchmark-link&gt;

I had to close the browser tab, otherwise the screen would have gone blank and I couldn't have recorded it.

You may want to consider doing it once (possibly on node) and saving the embeddings so that you can re-use them.

I want to use this model on frontend side, so that might not be possible. I actually raised a similar &lt;denchmark-link:https://github.com/tensorflow/tfjs/issues/3471&gt;issue&lt;/denchmark-link&gt;
 but it was a slowness issue at node side. I am not sure if it has something to do with this one.
Thanks
		</comment>
		<comment id='5' author='fawazahmed0' date='2020-10-06T15:06:48Z'>
		Thanks for the GIF, that is very helpful. In terms of controlling memory usage there are a few things you would need to do.

Disposing the output tensor for each batch of embedded text: You would download the final values from the GPU to CPU and then dispose the tensor that is resident on GPU.
Setting tf.env().set("WEBGL_DELETE_TEXTURE_THRESHOLD", 0); at the top of the program. Disposing the tensors above may not solve the memory issue if many differently sized textures are being allocated to process the different sentence lengths in the input. Setting this flag to zero means that when you dispose the tensor it will actually delete the backing texture as well rather than trying to save it to reuse later.
With the two steps above you should save a bunch of GPU memory. You may still run into this issue with the binary cache and we don't have an escape hatch for that but are discussing options. Try the two steps above and let us know if that helps.

I've &lt;denchmark-link:https://codepen.io/tafsiri/pen/zYBOwVB?editors=1011&gt;updated the codepen&lt;/denchmark-link&gt;
 to include the steps above.
cc &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;
 who suggested using the flag mentioned in step 2.
		</comment>
		<comment id='6' author='fawazahmed0' date='2020-10-06T15:08:54Z'>
		For your node example, I didn't look closely but I think you should also do the embedding in batches and dispose the output tensors periodically if that is possible.
		</comment>
		<comment id='7' author='fawazahmed0' date='2020-10-06T17:07:20Z'>
		Thanks &lt;denchmark-link:https://github.com/tafsiri&gt;@tafsiri&lt;/denchmark-link&gt;
 &lt;denchmark-link:https://github.com/annxingyuan&gt;@annxingyuan&lt;/denchmark-link&gt;
 ,the key was  to solve the issue for me, dispose() function rather slows down the whole embedding process for me, so I had to remove it.
Thanks
		</comment>
		<comment id='8' author='fawazahmed0' date='2020-10-07T17:16:05Z'>
		Are you satisfied with the resolution of your issue?
&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/4015&gt;Yes&lt;/denchmark-link&gt;

&lt;denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&amp;entry.2137816233=https://github.com/tensorflow/tfjs/issues/4015&gt;No&lt;/denchmark-link&gt;

		</comment>
	</comments>
</bug>