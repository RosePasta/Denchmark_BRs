<bug id='1240' author='dsmilkov' open_date='2019-02-14T01:14:09Z' closed_time='2019-02-14T23:31:27Z'>
	<summary>dataset.shuffle() gets stuck in infinite loop</summary>
	<description>
Datasets with shuffle() transform freeze the browser.
Repro:
function* data() {
  for (let i = 0; i &lt; 10; i++) {
    yield i;
  }
}
const d = tf.data.generator(data).shuffle();
const iter = await d.iterator();
console.log(await iter.next());
	</description>
	<comments>
		<comment id='1' author='dsmilkov' date='2019-02-14T01:57:09Z'>
		I think this is because of missing arguments when calling .shuffle(), we should add default value for the arguments.
		</comment>
		<comment id='2' author='dsmilkov' date='2019-02-14T02:00:33Z'>
		Ah nice catch! Let's default it to some reasonable value (~1000). I'm dogfooding this in plain js so it is extremely easy to forget to pass a value. At the least, a user-friendly error message letting me know i need to provide a bufferSize, but I like the default better.
		</comment>
		<comment id='3' author='dsmilkov' date='2019-02-14T03:54:16Z'>
		Actually, since as a first time user we expect shuffle to be "ideal", how about:

if dataset.size is defined, bufferSize defaults to it, so it shuffles the whole dataset.
if dataset.size is missing, throw an error, asking for explicit bufferSize.

		</comment>
		<comment id='4' author='dsmilkov' date='2019-02-14T16:30:38Z'>
		The fact that dataset.size is known means that the dataset fits in CPU memory, so there is no problem to shuffle the whole thing.
		</comment>
		<comment id='5' author='dsmilkov' date='2019-02-14T19:48:56Z'>
		dataset.size may well be known without the whole thing fitting in memory-- maybe not in the current implementation, but certainly in principle.  a) we could be streaming fixed-length records from a large file whose total size is known. b) we might have already done a full pass (e.g., to collect stats) so now we can know how big the dataset is.
Also, what is being shuffled may well be Tensors, so it's GPU memory we're worried about, not only CPU.
In any case I implemented a hybrid solution with a default of 10000 elements.  Happy to discuss both the approach and the best default value of course :)
		</comment>
	</comments>
</bug>